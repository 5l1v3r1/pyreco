__FILENAME__ = conftest
import os, sys

xnet = os.environ.get("XNET", "") # eXclude NETwork tests
try:
    xnet = int(xnet)
except ValueError:
    pass

sys.path.append(os.path.dirname(os.path.abspath(__file__)))


def pytest_funcarg__alarm(request):
    import signal, time, math

    def sighandler(signum, frame):
        __tracebackhide__ = True
        raise RuntimeError("timeout after %s seconds" % (time.time() - stime))

    def cleanup():
        if hasattr(signal, "setitimer"):
            signal.setitimer(signal.ITIMER_REAL, 0)
        else:
            signal.alarm(0)
        signal.signal(signal.SIGALRM, old_handler)

    def alarm(secs):
        if hasattr(signal, "setitimer"):
            signal.setitimer(signal.ITIMER_REAL, secs)
        else:
            signal.alarm(math.ceil(secs))


    request.addfinalizer(cleanup)
    stime = time.time()
    old_handler = signal.signal(signal.SIGALRM, sighandler)

    return alarm


def pytest_configure(config):
    kw = config.getvalue("keyword")
    if "xnet" in kw:
        return
    
    if xnet:
        print "conftest.py: disabling tests marked with keyword xnet."
        print "conftest.py: set environment variable XNET=0 to enable them."
        
        if kw:
            kw = kw+" -xnet"
        else:
            kw = "-xnet"
        config.option.keyword = kw

########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# mwlib documentation build configuration file, created by
# sphinx-quickstart on Mon Nov 14 17:53:19 2011.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = []

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'mwlib'
copyright = u'PediaPress GmbH'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.15'
# The full version, including alpha/beta/rc tags.
release = '0.15'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'mwlibdoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'mwlib.tex', u'mwlib Documentation',
   u'Volker Haas, Ralf Schmitt, Johannes Beigel', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'mwlib', u'mwlib Documentation',
     [u'Volker Haas, Ralf Schmitt, Johannes Beigel'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'mwlib', u'mwlib Documentation',
   u'Volker Haas, Ralf Schmitt, Johannes Beigel', 'mwlib', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

########NEW FILE########
__FILENAME__ = advtree
# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

"""
The parse tree generated by the parser is a 1:1 representation of the mw-markup.
Unfortunally these trees have some flaws if used to geenerate derived documents.

This module seeks to rebuild the parstree
to be:
 * more logical markup
 * clean up the parse tree
 * make it more accessible
 * allow for validity checks
 * implement rebuilding strategies

Usefull Documentation:
http://en.wikipedia.org/wiki/Wikipedia:Don%27t_use_line_breaks
http://meta.wikimedia.org/wiki/Help:Advanced_editing
http://meta.wikimedia.org/wiki/Help:HTML_in_wikitext
"""
import re
import time
from mwlib.parser import Math, Ref, Link, URL, NamedURL # not used but imported
from mwlib.parser import CategoryLink, SpecialLink, Caption, LangLink # not used but imported
from mwlib.parser import ArticleLink, InterwikiLink, NamespaceLink
from mwlib.parser import Item, ItemList,  Node, Table, Row, Cell, Paragraph, PreFormatted
from mwlib.parser import Section, Style, TagNode, Text, Timeline
from mwlib.parser import  ImageLink, Article, Book, Chapter
import copy
from mwlib.log import Log

log = Log("advtree")


def _idIndex(lst, el):
    """Return index of first appeareance of element el in list lst"""
    
    for i, e in enumerate(lst):
        if e is el:
            return i
    raise ValueError('element %r not found' % el)

def debug(method): # use as decorator
    def f(self, *args, **kargs):
        log("\n%s called with %r %r" % (method.__name__, args, kargs))
        log("on %r attrs:%r style:%r" % (self, self.attributes, self.style) )
        p = self
        while p.parent:
            p = p.parent
            log("%r" % p)
        return method(self, *args, **kargs)
    return f


class AdvancedNode:
    """Mixin Class that extends Nodes so they become easier accessible.

    Allows to traverse the tree in any direction and 
    build derived convinience functions
   """

    parent = None # parent element
    isblocknode = False

    def copy(self):
        "return a copy of this node and all its children"
        p = self.parent
        try:
            self.parent = None
            n = copy.deepcopy(self)
        finally:
            self.parent = p
        return n


    def moveto(self, targetnode, prefix=False): #FIXME: bad name. rename to moveBehind, and create method moveBefore
        """Move this node behind the target node.

        If prefix is true, move before the target node.
        """
        
        if self.parent:
            self.parent.removeChild(self)
        tp = targetnode.parent
        idx = _idIndex(tp.children, targetnode)
        if not prefix:
            idx+=1
        tp.children.insert(idx, self)
        self.parent = tp

    def hasChild(self, c):
        """Check if node c is child of self"""
        try:
            _idIndex(self.children, c)
            assert c.parent is self
            return True
        except ValueError:
            return False
        
    def appendChild(self, c):
        self.children.append(c)
        c.parent = self

    def removeChild(self, c):
        self.replaceChild(c, [])
        assert c.parent is None

    def replaceChild(self, c, newchildren = []):
        """Remove child node c and replace with newchildren if given."""

        idx = _idIndex(self.children, c)
        self.children[idx:idx+1] = newchildren

        c.parent = None
        assert not self.hasChild(c)
        for nc in newchildren:
            nc.parent = self

    def getParents(self):
        """Return list of parent nodes up to the root node.

        The returned list starts with the root node.
        """

        parents = []
        n = self.parent
        while n:
            parents.append(n)
            n = n.parent
        parents.reverse()
        return parents

    def getParent(self):
        """Return the parent node"""
        return self.parent

    def getLevel(self):
        """Returns the number of nodes of same class in parents"""
        return [p.__class__ for p in self.getParents()].count(self.__class__)

   
    def getParentNodesByClass(self, klass): #FIXME: rename to getParentsByClass
        """returns parents w/ klass"""
        return [p for p in self.parents if p.__class__ == klass]

    def getChildNodesByClass(self, klass): #FIXME: rename to getChildrenByClass
        """returns all children  w/ klass"""
        return [p for p in self.getAllChildren() if p.__class__ == klass]

    def getAllChildren(self):
        """don't confuse w/ Node.allchildren() which returns allchildren + self"""
        for c in self.children:
            yield c
            for x in c.getAllChildren():
                yield x        
        
    def getSiblings(self):
        """Return all siblings WITHOUT self"""
        return [c for c in self.getAllSiblings() if c is not self]

    def getAllSiblings(self):
        """Return all siblings plus self"""
        if self.parent:
            return self.parent.children
        return []

    def getPrevious(self):
        """Return previous sibling"""
        s = self.getAllSiblings()
        try:
            idx = _idIndex(s,self)
        except ValueError:
            return None
        if idx -1 <0:
            return None
        else:
            return s[idx-1]

    def getNext(self):
        """Return next sibling"""
        s = self.getAllSiblings()
        try:
            idx = _idIndex(s,self)
        except ValueError:
            return None
        if idx+1 >= len(s):
            return None
        else:
            return s[idx+1]

    def getLast(self): #FIXME might return self. is this intended?
        """Return last sibling"""
        s = self.getAllSiblings()
        if s:
            return s[-1]

    def getFirst(self): #FIXME might return self. is this intended?
        """Return first sibling"""
        s = self.getAllSiblings()
        if s:
            return s[0]

    def getLastChild(self):
        """Return last child of this node"""
        if self.children:
            return self.children[-1]

    def getFirstChild(self):
        "Return first child of this node"
        if self.children:
            return self.children[0]

    def getFirstLeaf(self, callerIsSelf=True):
        """Return 'first' child that has no children itself"""
        if self.children:
            if self.__class__ == Section: # first kid of a section is its caption
                if len(self.children) == 1:
                    return None
                else:
                    return self.children[1].getFirstLeaf(callerIsSelf=False)
            else:
                return self.children[0].getFirstLeaf(callerIsSelf=False)
        else:
            if callerIsSelf:
                return None
            else:
                return self

    def getLastLeaf(self, callerIsSelf=True):
        """Return 'last' child that has no children itself"""
        if self.children:
            return self.children[-1].getFirstLeaf(callerIsSelf=False)
        else:
            if callerIsSelf:
                return None
            else:
                return self

    def getAllDisplayText(self, amap = None):
        "Return all text that is intended for display"
        text = []
        if not amap:
            amap = {Text:"caption",
                    Link:"target",
                    URL:"caption",
                    Math:"caption",
                    ImageLink:"caption",
                    ArticleLink:"target",
                    NamespaceLink:"target"}
        skip_on_children = [Link, NamespaceLink]
        for n in self.allchildren():
            access = amap.get(n.__class__, "")
            if access:
                if n.__class__ in skip_on_children and n.children:
                    continue
                text.append( getattr(n, access) )
        alltext = [t for t in text if t]
        if alltext:
            return u''.join(alltext)
        else:
            return ''
    
    def getStyle(self):
        if not self.attributes:
            return {}
        else:
            return self.attributes.get('style', {})


    def _cleanAttrs(self, attrs):

        def ensureInt(val, min_val=1):
            try:
                return max(min_val, int(val))
            except ValueError:
                return min_val

        def ensureUnicode(val):
            if isinstance(val, unicode):
                return val
            elif isinstance(val, str):
                return unicode(val, 'utf-8')
            else:
                try:
                    return unicode(val)
                except:
                    return u''

        def ensureDict(val):
            if isinstance(val, dict):
                return val
            else:
                return {}

        for (key, value) in attrs.items():
            if key in ['colspan', 'rowspan']:
                attrs[key] = ensureInt(value, min_val=1)
            elif key == 'style':
                attrs[key] = self._cleanAttrs(ensureDict(value))
            else:
                attrs[key] = ensureUnicode(value)
        return attrs

    def getAttributes(self):
        """ Return dict with node attributes (e.g. class, style, colspan etc.)"""
        vlist = getattr(self, 'vlist', None)
        if vlist is None:
            self.vlist = vlist = {}
            
        attrs = self._cleanAttrs(vlist)
        return attrs


    def hasClassID(self, classIDs):
        _class = self.attributes.get('class','').split(' ')
        _id = self.attributes.get('id','')
        for classID in classIDs:
            if classID in _class or classID == _id:
                return True
        return False
        
    def isVisible(self):
        """Return True if node is visble. Used to detect hidden elements."""
        if self.style.get('display', '').lower() == 'none':
            return False
        if self.style.get('visibility','').lower() == 'hidden':
            return False
        return True

    
    style = property(getStyle)
    attributes = property(getAttributes)
    visible = property(isVisible)
    
    parents = property(getParents)
    next = property(getNext)
    previous = property(getPrevious)
    siblings = property(getSiblings)
    last = property(getLast)
    first = property(getFirst)
    lastchild = property(getLastChild)
    firstchild = property(getFirstChild)
    


# --------------------------------------------------------------------------
# MixinClasses w/ special behaviour
# -------------------------------------------------------------------------

class AdvancedTable(AdvancedNode):    
    @property 
    def rows(self):
        return [r for r in self if r.__class__ == Row]

    @property 
    def numcols(self):
        max_cols = 0
        for row in self.children:
            cols = sum([cell.attributes.get('colspan', 1) for cell in row.children if not getattr(cell, 'colspanned', False)])
            max_cols = max(max_cols, cols)
        return max_cols

        
class AdvancedRow(AdvancedNode):    
    @property 
    def cells(self):
        return [c for c in self if c.__class__ == Cell]


class AdvancedCell(AdvancedNode):
    @property    
    def colspan(self, attr="colspan"):
        ''' colspan of cell. result is always non-zero, positive int'''
        return self.attributes.get('colspan') or 1

    @property
    def rowspan(self):
        ''' rowspan of cell. result is always non-zero, positive int'''
        return self.attributes.get('rowspan') or 1

class AdvancedSection(AdvancedNode):
    def getSectionLevel(self):
        return 1 + self.getLevel()

class AdvancedImageLink(AdvancedNode):
    isblocknode = property ( lambda s: not s.isInline() )

    @property
    def render_caption(self):
        explicit_caption = bool(getattr(self, 'thumb') or getattr(self, 'frame','') == 'frame')
        is_gallery = len(self.getParentNodesByClass(Gallery)) > 0
        has_children = len(self.children) > 0
        return (explicit_caption or is_gallery) and has_children
    
class AdvancedMath(AdvancedNode):
    @property
    def isblocknode(self):
        if self.caption.strip().startswith("\\begin{align}")  or \
                self.caption.strip().startswith("\\begin{alignat}"):
            return True
        return False

       

# --------------------------------------------------------------------------
# Missing as Classes derived from parser.Style
# -------------------------------------------------------------------------

class Italic(Style, AdvancedNode):
    _tag = "i"

class Emphasized(Style, AdvancedNode):
    _tag = "em"

class Strong(Style, AdvancedNode):
    _tag = "strong"

class DefinitionList(Style, AdvancedNode):
    _tag = "dl"

class DefinitionTerm(Style, AdvancedNode):
    _tag = "dt"

class DefinitionDescription(Style, AdvancedNode):
    _tag = "dd"

class Blockquote(Style, AdvancedNode):
    "margins to left &  right"
    _tag = "blockquote"
    
class Indented(Style, AdvancedNode): # fixme: node is deprecated, now style node ':' always becomes a DefinitionDescription
    "margin to the left"
    def getIndentLevel(self):
        return self.caption.count(":")
    indentlevel = property(getIndentLevel)

class Overline(Style, AdvancedNode):
    _style = "overline"

class Underline(Style, AdvancedNode):
    _style = "u"

class Sub(Style, AdvancedNode):
    _style = "sub"
    _tag = "sub"

class Sup(Style, AdvancedNode):
    _style = "sup"
    _tag = "sup"

class Small(Style, AdvancedNode):
    _style = "small"
    _tag = "small"

class Big(Style, AdvancedNode):
    _style = "big"
    _tag = "big"

class Cite(Style, AdvancedNode):
    _style = "cite"
    _tag = "cite"

class Var(Style, AdvancedNode): 
    _tag = "var"
    _style = "var"



_styleNodeMap = dict( (k._style,k) for k in [Overline, Underline, Sub, Sup, Small, Big, Cite,Var] )

# --------------------------------------------------------------------------
# Missing as Classes derived from parser.TagNode
# http://meta.wikimedia.org/wiki/Help:HTML_in_wikitext
# -------------------------------------------------------------------------


class Source(TagNode, AdvancedNode):
    _tag = "source"

class Code(TagNode, AdvancedNode):
    _tag = "code"

class BreakingReturn(TagNode, AdvancedNode):
    _tag = "br"

class HorizontalRule(TagNode, AdvancedNode):
    _tag = "hr"

class Index(TagNode, AdvancedNode):
    _tag = "index"

class Teletyped(TagNode, AdvancedNode):
    _tag = "tt"

class Reference(TagNode, AdvancedNode):
    _tag = "ref"

class ReferenceList(TagNode, AdvancedNode):
    _tag = "references"

class Gallery(TagNode, AdvancedNode):
    _tag = "gallery"

class Center(TagNode, AdvancedNode):
    _tag = "center"

class Div(TagNode, AdvancedNode):
    _tag = "div"

class Span(TagNode, AdvancedNode): # span is defined as inline node which is in theory correct. 
    _tag = "span"

class Font(TagNode, AdvancedNode):
    _tag = "font"

class Strike(TagNode,AdvancedNode):
    _tag = "strike"
    
# class S(TagNode, AdvancedNode):
#     _tag = "s"
    
class ImageMap(TagNode, AdvancedNode): # defined as block node, maybe incorrect
    _tag = "imagemap"

class Ruby(TagNode, AdvancedNode): 
    _tag = "ruby"

class RubyBase(TagNode, AdvancedNode):
    _tag = "rb"

class RubyParentheses(TagNode, AdvancedNode):
    _tag = "rp"

class RubyText(TagNode, AdvancedNode): 
    _tag = "rt"

class Deleted(TagNode, AdvancedNode): 
    _tag = "del"

class Inserted(TagNode, AdvancedNode): 
    _tag = "ins"

class TableCaption(TagNode, AdvancedNode): 
    _tag = "caption"

class Abbreviation(TagNode, AdvancedNode):
    _tag = "abbr"
    
_tagNodeMap = dict( (k._tag,k) for k in [Source, Code, BreakingReturn, HorizontalRule, Index, Teletyped, Reference, ReferenceList, Gallery, Center, Div, Span, Strike, ImageMap, Ruby, RubyBase, RubyText, Deleted, Inserted, TableCaption, Font, DefinitionList, DefinitionTerm, DefinitionDescription, Abbreviation] )
_styleNodeMap["s"] = Strike # Special Handling for deprecated s style
_tagNodeMap["kbd"] = Teletyped

# --------------------------------------------------------------------------
# BlockNode separation for AdvancedNode.isblocknode
# -------------------------------------------------------------------------

"""
For writers it is usefull to know whether elements are inline (within a paragraph) or not.
We define list for blocknodes, which are used in AdvancedNode as:

AdvancedNode.isblocknode

Image depends on result of Image.isInline() see above

Open Issues: Math, Magic, (unknown) TagNode 

"""
_blockNodes = (Blockquote, Book, Chapter, Article, Section, Paragraph, Div, Center,
               PreFormatted, Cell, Row, Table, Item, BreakingReturn,
               ItemList, Timeline, HorizontalRule, Gallery, Indented, 
               DefinitionList, DefinitionTerm, DefinitionDescription, ReferenceList, Source, ImageMap)

for k in _blockNodes:  
  k.isblocknode = True



# --------------------------------------------------------------------------
# funcs for extending the nodes
# -------------------------------------------------------------------------

def mixIn(pyClass, mixInClass, makeFirst=False):
  if mixInClass not in pyClass.__bases__:
    if makeFirst:
      pyClass.__bases__ = (mixInClass,) + pyClass.__bases__
    else:
      pyClass.__bases__ += (mixInClass,)

def extendClasses(node):
    for c in node.children[:]:
        extendClasses(c)
        c.parent = node

# Nodes we defined above and that are separetly handled in extendClasses
_advancedNodesMap = {Section: AdvancedSection, ImageLink:AdvancedImageLink, 
                     Math:AdvancedMath, Cell:AdvancedCell, Row:AdvancedRow, Table:AdvancedTable}
mixIn(Node, AdvancedNode)
for k, v in _advancedNodesMap.items():
    mixIn(k,v)
    
# --------------------------------------------------------------------------
# Functions for fixing the parse tree
# -------------------------------------------------------------------------

def fixTagNodes(node):
    """Detect known TagNodes and and transfrom to appropriate Nodes"""
    for c in node.children:
        if c.__class__ == TagNode:
            if c.caption in _tagNodeMap:
                c.__class__ = _tagNodeMap[c.caption]
            elif c.caption in ("h1", "h2", "h3", "h4", "h5", "h6"): # FIXME
                # NEED TO MOVE NODE IF IT REALLY STARTS A SECTION
                c.__class__ = Section 
                mixIn(c.__class__, AdvancedSection)
                c.level = int(c.caption[1])
                c.caption = ""
            else:
                log.warn("fixTagNodes, unknowntagnode %r" % c)
        fixTagNodes(c)


def fixStyleNode(node): 
    """
    parser.Style Nodes are mapped to logical markup
    detection of DefinitionList depends on removeNodes
    and removeNewlines
    """
    if not node.__class__ == Style:
        return
    if node.caption == "''": 
        node.__class__ = Emphasized
        node.caption = ""
    elif node.caption=="'''''":
        node.__class__ = Strong
        node.caption = ""
        em = Emphasized("''")
        for c in node.children:
            em.appendChild(c)
        node.children = []
        node.appendChild(em)
    elif node.caption == "'''":
        node.__class__ = Strong
        node.caption = ""
    elif node.caption == ";": 
        node.__class__ = DefinitionTerm
        node.caption = ""
    elif node.caption.startswith(":"): 
        node.__class__ = DefinitionDescription        
        node.indentlevel = len(re.findall('^:+', node.caption)[0])
        node.caption = ""
    elif node.caption == '-':
        node.__class__ = Blockquote
        node.caption = ''
    elif node.caption in _styleNodeMap:
        node.__class__ = _styleNodeMap[node.caption]
        node.caption = ""
    else:
        log.warn("fixStyle, unknownstyle %r" % node)
        return node
    
    return node

def fixStyleNodes(node): 
    if node.__class__ == Style:
        fixStyleNode(node)
    for c in node.children[:]:
        fixStyleNodes(c)


def removeNodes(node):
    """
    the parser generates empty Node elements that do 
    nothing but group other nodes. we remove them here
    """
    if node.__class__ == Node:
        # first child of section groups heading text - grouping Node must not be removed
        if not (node.previous == None and node.parent.__class__ == Section): 
            node.parent.replaceChild(node, node.children)
            
    for c in node.children[:]:
        removeNodes(c)

def removeNewlines(node):
    """
    remove newlines, tabs, spaces if we are next to a blockNode
    """
    if node.__class__ in (PreFormatted, Source):
        return
    
    todo = [node]
    while todo:
        node = todo.pop()
        if node.__class__ is Text and node.caption:
            if not node.caption.strip():
                prev = node.previous or node.parent # previous sibling node or parentnode 
                next = node.next or node.parent.next
                if not next or next.isblocknode or not prev or prev.isblocknode: 
                    np = node.parent
                    node.parent.removeChild(node)    
            node.caption = node.caption.replace("\n", " ")

        for c in node.children:
            if c.__class__ in (PreFormatted, Source):
                continue
            todo.append(c)


def buildAdvancedTree(root): # USE WITH CARE
    """
    extends and cleans parse trees
    do not use this funcs without knowing whether these 
    Node modifications fit your problem
    """
    funs = [extendClasses, fixTagNodes, removeNodes, removeNewlines,
            fixStyleNodes,]
    for f in funs:
        f(root)
        


def _validateParserTree(node, parent=None):
    # helper to assert tree parent link consistency
    if parent is not None:
        _idIndex(parent.children, node) # asserts it occures only once
    for c in node:
        _idIndex(node.children, c) # asserts it occures only once
        assert c in node.children
        _validateParserTree(c, node)


def _validateParents(node, parent=None):
    # helper to assert tree parent link consistency
    if parent is not None:
        assert parent.hasChild(node)
    else:
        assert node.parent is None      
    for c in node:
        assert node.hasChild(c)
        _validateParents(c, node)
        


def getAdvTree(fn):
    from mwlib.dummydb import DummyDB
    from mwlib.uparser import parseString
    db = DummyDB()
    input = unicode(open(fn).read(), 'utf8')
    r = parseString(title=fn, raw=input, wikidb=db)
    buildAdvancedTree(r)
    return r

def simpleparse(raw):    # !!! USE FOR DEBUGGING ONLY !!! 
    import sys
    from mwlib import dummydb, parser
    from mwlib.uparser import parseString
    input = raw.decode('utf8')
    r = parseString(title="title", raw=input, wikidb=dummydb.DummyDB())
    buildAdvancedTree(r)
    parser.show(sys.stdout, r, 0)
    return r


########NEW FILE########
__FILENAME__ = allnodes
import mwlib.parser
import mwlib.advtree

import types

def allnodes():
    all = set()
    names = set()
    for m in (mwlib.parser, mwlib.advtree):
        for x in dir(m):
            if x in names:
                continue
            k = getattr(m, x)
            if type(k) == types.TypeType:
                if issubclass(k, mwlib.parser.Node):
                    all.add(k)
                    names.add(x)
    return all
                

if __name__ == '__main__':
    # EXAMPLE THAT SHOWS HOW TO IDENTIFY MISSING NODES
    from mwlib.parser import Control, Chapter
    my = set((Control, Chapter))
    missing = allnodes() - my
    assert len(missing) == len(allnodes()) -2 
    #print missing

########NEW FILE########
__FILENAME__ = buildzip

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

"""mz-zip - installed via setuptools' entry_points"""

import os, sys, tempfile, shutil, zipfile


def _walk(root):
    retval = []
    for dirpath, dirnames, files in os.walk(root):
        # retval.extend([os.path.normpath(os.path.join(dirpath, x))+"/" for x in dirnames])
        retval.extend([os.path.normpath(os.path.join(dirpath, x)) for x in files])
    retval = [x.replace("\\", "/") for x in retval]
    retval.sort()
    return retval

                     
def zipdir(dirname, output=None, skip_ext=None):
    """recursively zip directory and write output to zipfile.
    @param dirname: directory to zip
    @param output: name of zip file that get's written
    @para skip_ext: skip files with the specified extension
    """
    if not output:
        output = dirname+".zip"

    output = os.path.abspath(output)
    zf = zipfile.ZipFile(output, "w", compression=zipfile.ZIP_DEFLATED)
    for i in _walk(dirname):
        if skip_ext and os.path.splitext(i)[1] == skip_ext:
            continue
        zf.write(i, i[len(dirname)+1:])
    zf.close()



        
def make_zip(output=None, options=None, metabook=None, podclient=None, status=None):
    if output:
        tmpdir = tempfile.mkdtemp(dir=os.path.dirname(output))
    else:
        tmpdir = tempfile.mkdtemp()
        
    try:
        fsdir = os.path.join(tmpdir, 'nuwiki')
        print 'creating nuwiki in %r' % fsdir
        from mwlib.apps.make_nuwiki import make_nuwiki
        make_nuwiki(fsdir, metabook=metabook, options=options, podclient=podclient, status=status)

        if output:
            fd, filename = tempfile.mkstemp(suffix='.zip', dir=os.path.dirname(output))
        else:
            fd, filename = tempfile.mkstemp(suffix='.zip')
        os.close(fd)
        zipdir(fsdir, filename)
        if output:
            os.rename(filename, output)
            filename = output

        if podclient:                
            status(status='uploading', progress=0)
            podclient.post_zipfile(filename)

        return filename

    finally:
        if not options.keep_tmpfiles:
            print 'removing tmpdir %r' % tmpdir
            shutil.rmtree(tmpdir, ignore_errors=True)
        else:
            print 'keeping tmpdir %r' % tmpdir

        if sys.platform in ("linux2", "linux3"):
            from mwlib import linuxmem
            linuxmem.report()


def main():
    from gevent import monkey
    monkey.patch_all(thread=False)

    from mwlib.options import OptionParser
    from mwlib import conf

    parser = OptionParser()
    parser.add_option("-o", "--output", help="write output to OUTPUT")
    parser.add_option("-p", "--posturl", help="http post to POSTURL (directly)")
    parser.add_option("-g", "--getposturl",
        help='get POST URL from PediaPress.com, open upload page in webbrowser',
        action='count',
    )
    parser.add_option('--keep-tmpfiles',                  
        action='store_true',
        default=False,
        help="don't remove  temporary files like images",
    )
    
    parser.add_option("-s", "--status-file",
                      help='write status/progress info to this file')

    options, args = parser.parse_args()
    conf.readrc()
    use_help = 'Use --help for usage information.'
        
                        
    if parser.metabook is None and options.collectionpage is None:
        parser.error('Neither --metabook nor, --collectionpage or arguments specified.\n' + use_help)
    if options.posturl and options.getposturl:
        parser.error('Specify either --posturl or --getposturl.\n' + use_help)
    if not options.posturl and not options.getposturl and not options.output:
        parser.error('Neither --output, nor --posturl or --getposturl specified.\n' + use_help)
    if options.posturl:
        from mwlib.podclient import PODClient
        podclient = PODClient(options.posturl)
    elif options.getposturl:
        if options.getposturl>1:
            serviceurl = 'http://test.pediapress.com/api/collections/'
        else:
            serviceurl = 'http://pediapress.com/api/collections/'
        import webbrowser
        from mwlib.podclient import podclient_from_serviceurl
        podclient = podclient_from_serviceurl(serviceurl)
        pid = os.fork()
        if not pid:
            try:
                webbrowser.open(podclient.redirecturl)
            finally:
                os._exit(0)
        import time
        time.sleep(1)
        try:
            os.kill(pid, 9)
        except:
            pass
              
    else:
        podclient = None
    
    from mwlib import utils,  wiki
    

    filename = None
    status = None
    try:
        env = parser.makewiki()
        assert env.metabook, "no metabook"
            
        from mwlib.status import Status
        status = Status(options.status_file, podclient=podclient, progress_range=(1, 90))
        status(progress=0)
        output = options.output
            
        make_zip(output, options, env.metabook, podclient=podclient, status=status)
            
    except Exception, e:
        if status:
            status(status='error')
        raise
    finally:
        if options.output is None and filename is not None:
            print 'removing %r' % filename
            utils.safe_unlink(filename)

########NEW FILE########
__FILENAME__ = client
"Client to mw-serve"

def main():
    import optparse
    import sys

    from mwlib.client import Client
    import mwlib.myjson as json

    parser = optparse.OptionParser(usage="%prog [OPTIONS] COMMAND [ARGS]")
    default_url = 'http://localhost:8899/'
    parser.add_option('-u', '--url',
        help='URL of HTTP interface to mw-serve (default: %r)' % default_url,
        default=default_url,
    )
    options, args = parser.parse_args()

    if not args:
        parser.error('argument required')

    command = args[0]
    data = {}
    for arg in args[1:]:
        if '=' in arg:
            key, value = [x.strip() for x in arg.split('=', 1)]
        else:
            key = arg.strip()
            value = True
        data[key] = value

    if 'metabook' in data:
        data['metabook'] = open(data['metabook'], 'rb').read()

    client = Client(options.url)
    if not client.request(command, data, is_json=(command != 'download')):
        if client.error is not None:
            sys.exit('request failed: %s' % client.error)
        else:
            sys.exit('request failed: got response code %d\n%r' % (client.response_code, client.response))

    if command=="download":
        fn = 'output'
        open(fn, 'w').write(client.response)
        print 'wrote %d bytes to %r' % (len(client.response), fn)
    else:
        print json.dumps(client.response, indent=4)
    

########NEW FILE########
__FILENAME__ = make_nuwiki

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import os
from mwlib.net import fetch, sapi as mwapi

from mwlib.parse_collection_page import extract_metadata
from mwlib.metabook import get_licenses, parse_collection_page, collection
from mwlib import myjson
import urllib
import gevent
import gevent.pool

class start_fetcher(object):
    progress = None
    
    def __init__(self, **kw):
        self.fetcher = None
        self.__dict__.update(kw)
        self.nfo = {}

    def get_api(self):
        if self.username:
            api = mwapi.mwapi(self.api_url, self.username, self.password)
        else:
            api = mwapi.mwapi(self.api_url)
        api.set_limit()

        if self.username:
            api.login(self.username, self.password, self.domain)
        return api

    def fetch_pages_from_metabook(self,  api):
        fsout = self.fsout
        metabook=self.metabook
        
        fsout.dump_json(metabook=metabook)
        nfo = self.nfo.copy()

        nfo.update({
                'format': 'nuwiki',
                'base_url': self.base_url,
                'script_extension': self.options.script_extension})



        fsout.nfo = nfo

        # fsout.dump_json(nfo=nfo)

        pages = fetch.pages_from_metabook(metabook)
        self.fetcher = fetch.fetcher(api, fsout, pages,
                                     licenses=self.licenses,
                                     status=self.status,
                                     progress=self.progress, 
                                     imagesize=self.options.imagesize,
                                     cover_image=metabook.cover_image,
                                     fetch_images=not self.options.noimages)
        self.fetcher.run()

    def init_variables(self):
        base_url = self.base_url
        options = self.options
        
        if not base_url.endswith("/"):
            base_url += "/"
        api_url = "".join([base_url, "api", options.script_extension])
        if isinstance(api_url,  unicode):
            api_url = api_url.encode("utf-8")
        self.api_url = api_url

        self.username = options.username
        self.password = options.password
        self.domain   = options.domain
        
        self.fsout = fetch.fsoutput(self.fsdir)

    def fetch_collectionpage(self, api):
        cp = self.options.collectionpage
        if cp is None:
            return api

        try:
            cp = unicode(urllib.unquote(str(cp)), "utf-8")
        except Exception:
            pass

        self.nfo["collectionpage"] = cp

        val = api.fetch_pages([cp])
        rawtext = val["pages"].values()[0]["revisions"][0]["*"]
        mb = self.metabook = parse_collection_page(rawtext)
        wikitrust(api.baseurl, mb)

        # XXX: localised template parameter names???
        meta = extract_metadata(rawtext, ("cover-image", "cover-color", "text-color", "editor", "description", "sort_as"))
        mb.editor = meta["editor"]
        mb.cover_image = meta["cover-image"]
        mb.cover_color = meta["cover-color"]
        mb.text_color = meta["text-color"]
        mb.description = meta["description"]
        mb.sort_as = meta["sort_as"]

        p = os.path.join(self.fsout.path, "collectionpage.txt")
        if isinstance(rawtext, unicode):
            rawtext=rawtext.encode("utf-8")
        open(p,"wb").write(rawtext)
        return api


    def run(self):
        self.init_variables()
        
        self.licenses = get_licenses(self.metabook)

        api = self.get_api()
        self.fetch_collectionpage(api)
        self.fetch_pages_from_metabook(api)


def wikitrust(baseurl, metabook):
    if not os.environ.get("TRUSTEDREVS"):
        return


    if not baseurl.startswith("http://en.wikipedia.org/w/"):
        return

    from mwlib import trustedrevs
    tr = trustedrevs.TrustedRevisions()

    for x in metabook.articles():
        if x.revision:
            continue

        try:
            r = tr.getTrustedRevision(x.title)
            x.revision = r["revid"]

            print "chosen trusted revision: title=%-20r age=%6.1fd revid=%10d user=%-20r" % (r["title"], r["age"], r["revid"], r["user"])
        except Exception, err:
            print "error choosing trusted revision for", repr(x.title),  repr(err)

def make_nuwiki(fsdir, metabook, options, podclient=None, status=None):
    id2wiki = {}
    for x in metabook.wikis:
        id2wiki[x.ident] = (x, [])

    for x in metabook.articles():
        assert x.wikiident in id2wiki, "no wikiconf for %r (%s)" % (x.wikiident,  x)
        id2wiki[x.wikiident][1].append(x)

    is_multiwiki = len(id2wiki)>1
    
    if is_multiwiki:
        progress = fetch.shared_progress(status=status)
    else:
        progress = None
        
    fetchers =[]
    for id, (wikiconf, articles) in id2wiki.items():
        if id is None:
            id = ""
            assert not is_multiwiki, "id must be set in multiwiki"

        if not is_multiwiki:
            id = ""
        
        assert "/" not in id, "bad id: %r" % (id,)
        my_fsdir = os.path.join(fsdir, id)
        
        if is_multiwiki:
            my_mb = collection()
            my_mb.items = articles
        else:
            my_mb = metabook

        wikitrust(wikiconf.baseurl, my_mb)

        fetchers.append(start_fetcher(fsdir=my_fsdir, progress=progress, base_url=wikiconf.baseurl, metabook=my_mb, options=options, podclient=podclient, status=status))

    if is_multiwiki:
        if not os.path.exists(fsdir):
            os.makedirs(fsdir)
        open(os.path.join(fsdir, "metabook.json"),  "wb").write(metabook.dumps())
        myjson.dump(dict(format="multi-nuwiki"), open(os.path.join(fsdir, "nfo.json"), "wb"))

    pool = gevent.pool.Pool()
    for x in fetchers:
        pool.spawn(x.run)
    pool.join(raise_error=True)

    import signal
    signal.signal(signal.SIGINT,  signal.SIG_DFL)
    signal.signal(signal.SIGTERM,  signal.SIG_DFL)

########NEW FILE########
__FILENAME__ = render
# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

"""mw-render -- installed via setuptools' entry_points"""

from gevent import monkey
monkey.patch_all(thread=False)

import os
import sys
import errno
import pkg_resources
from mwlib.options import OptionParser
from mwlib import utils, wiki, conf, _locale


def init_tmp_cleaner():
    import tempfile, shutil, time
    tempfile.tempdir = tempfile.mkdtemp(prefix="tmp-%s" % os.path.basename(sys.argv[0]))
    os.environ["TMP"] = os.environ["TEMP"] = os.environ["TMPDIR"] = tempfile.tempdir
    ppid = os.getpid()
    try:
        pid = os.fork()
    except:
        shutil.rmtree(tempfile.tempdir)
        raise

    if pid == 0:
        os.closerange(0, 3)
        os.setpgrp()
        while 1:
            if os.getppid() != ppid:
                try:
                    shutil.rmtree(tempfile.tempdir)
                finally:
                    os._exit(0)
            time.sleep(1)


class Main(object):
    zip_filename = None
    
    def parse_options(self):
        parser = OptionParser()
        a = parser.add_option
        
        a("-o", "--output", help="write output to OUTPUT")
        
        a("-w", "--writer", help='use writer backend WRITER')
        
        a("-W", "--writer-options",
          help='";"-separated list of additional writer-specific options')
        
        a("-e", "--error-file", help='write errors to this file')
        
        a("-s", "--status-file",
          help='write status/progress info to this file')
        
        a("--list-writers", action='store_true',
          help='list available writers and exit')
        
        a("--writer-info", metavar='WRITER',
          help='list information about given WRITER and exit')
        
        a('--keep-zip', metavar='FILENAME',
            help='write ZIP file to FILENAME')
        
        a('--keep-tmpfiles', action='store_true', default=False,
            help="don't remove  temporary files like images")
        
        a('-L', '--language',
            help='use translated strings in LANGUAGE')
        
        options, args = parser.parse_args()
        return options, args, parser
    
    def load_writer(self, name):
        try:
            entry_point = pkg_resources.iter_entry_points('mwlib.writers', name).next()
        except StopIteration:
            sys.exit('No such writer: %r (use --list-writers to list available writers)' % name)
        try:
            return entry_point.load()
        except Exception, e:
            sys.exit('Could not load writer %r: %s' % (name, e))

    def list_writers(self):
        for entry_point in pkg_resources.iter_entry_points('mwlib.writers'):
            try:
                writer = entry_point.load()
                if hasattr(writer, 'description'):
                    description = writer.description
                else:
                    description = '<no description>'
            except Exception, e:
                description = '<NOT LOADABLE: %s>' % e
            print '%s\t%s' % (entry_point.name, description)

    def show_writer_info(self, name):
        writer = self.load_writer(name)
        if hasattr(writer, 'description'):
            print 'Description:\t%s' % writer.description
        if hasattr(writer, 'content_type'):
            print 'Content-Type:\t%s' % writer.content_type
        if hasattr(writer, 'file_extension'):
            print 'File extension:\t%s' % writer.file_extension
        if hasattr(writer, 'options') and writer.options:
            print 'Options (usable in a ";"-separated list for --writer-options):'
            for name, info in writer.options.items():
                param = info.get('param')
                if param:
                    print ' %s=%s:\t%s' % (name, param, info['help'])
                else:
                    print ' %s:\t%s' % (name, info['help'])

    def get_environment(self):
        from mwlib.status import Status
        from mwlib import nuwiki
        
        env = self.parser.makewiki()        
        if (isinstance(env.wiki, (nuwiki.NuWiki, nuwiki.adapt))
            or isinstance(env, wiki.MultiEnvironment)):
            self.status = Status(self.options.status_file, progress_range=(0, 100))
            return env

        from mwlib.apps.buildzip import make_zip
        self.zip_filename = make_zip(output=self.options.keep_zip, options=self.options, metabook=env.metabook, status=self.status)

        if env.images:
            try:
                env.images.clear()
            except OSError, err:
                if err.errno!=errno.ENOENT:
                    raise
                
        env = wiki.makewiki(self.zip_filename)
        self.status = Status(self.options.status_file, progress_range=(34, 100))
        return env
            
        
    def __call__(self):    
        options, args, parser = self.parse_options()
        conf.readrc()

        self.parser = parser
        self.options = options
        
        import tempfile
        from mwlib.writerbase import WriterError
        from mwlib.status import Status

        use_help = 'Use --help for usage information.'


        if options.list_writers:
            self.list_writers()
            return

        if options.writer_info:
            self.show_writer_info(options.writer_info)
            return
        
        if options.output is None:
            parser.error('Please specify an output file with --output.\n' + use_help)

        options.output = os.path.abspath(options.output)

        if options.writer is None:
            parser.error('Please specify a writer with --writer.\n' + use_help)    

        writer = self.load_writer(options.writer)
        writer_options = {}
        if options.writer_options:
            for wopt in options.writer_options.split(';'):
                if '=' in wopt:
                    key, value = wopt.split('=', 1)
                else:
                    key, value = wopt, True
                writer_options[str(key)] = value
        if options.language:
            writer_options['lang'] = options.language
        for option in writer_options.keys():
            if option not in getattr(writer, 'options', {}):
                print 'Warning: unknown writer option %r' % option
                del writer_options[option]

        init_tmp_cleaner()

        self.status = Status(options.status_file, progress_range=(1, 33))
        self.status(progress=0)

        env = None
        try:
            env = self.get_environment()

            try:
                _locale.set_locale_from_lang(env.wiki.siteinfo["general"]["lang"])
            except BaseException, err:
                print "Error: could not set locale", err

            basename = os.path.basename(options.output)
            if '.' in basename:
                ext = '.' + basename.rsplit('.', 1)[-1]
            else:
                ext = ''
            fd, tmpout = tempfile.mkstemp(dir=os.path.dirname(options.output), suffix=ext)
            os.close(fd)
            writer(env, output=tmpout, status_callback=self.status, **writer_options)
            os.rename(tmpout, options.output)
            kwargs = {}
            if hasattr(writer, 'content_type'):
                kwargs['content_type'] = writer.content_type
            if hasattr(writer, 'file_extension'):
                kwargs['file_extension'] = writer.file_extension
            self.status(status='finished', progress=100, **kwargs)
            if options.keep_zip is None and self.zip_filename is not None:
                utils.safe_unlink(self.zip_filename)
        except Exception, e:
            import traceback
            self.status(status='error')
            if options.error_file:
                fd, tmpfile = tempfile.mkstemp(dir=os.path.dirname(options.error_file))
                f = os.fdopen(fd, 'wb')
                if isinstance(e, WriterError):
                    f.write(str(e))
                else:
                    f.write('traceback\n')
                    traceback.print_exc(file=f) 
                f.write("sys.argv=%r\n" % (utils.garble_password(sys.argv),))
                f.close()
                os.rename(tmpfile, options.error_file)
            raise
        finally:
            if env is not None and env.images is not None:
                try:
                    if not options.keep_tmpfiles:
                        env.images.clear()
                except OSError, e:
                    if e.errno!=errno.ENOENT:
                        print 'ERROR: Could not remove temporary images: %s' % e, e.errno

def main():
    return Main()()

########NEW FILE########
__FILENAME__ = serve
import optparse


def serve_ctl():
    parser = optparse.OptionParser(usage="%prog [OPTIONS]")
    parser.add_option('--cache-dir',
        help='cache directory (default: /var/cache/mw-serve/)',
        default='/var/cache/mw-serve/',
    )
    parser.add_option('--purge-cache',
        help='remove cache files that have not been touched for at least HOURS hours',
        metavar='HOURS',
    )

    options, args = parser.parse_args()

    if args:
        parser.error('no arguments supported')

    if options.purge_cache:
        try:
            options.purge_cache = float(options.purge_cache)
        except ValueError:
            parser.error('--purge-cache value must be a positive number')

        from mwlib.serve import purge_cache
        purge_cache(options.purge_cache*60*60, cache_dir=options.cache_dir)


def check_service():
    import sys
    import time

    from mwlib.client import Client
    from mwlib.log import Log
    from mwlib import utils

    log = Log('mw-check-service')

    parser = optparse.OptionParser(usage="%prog [OPTIONS] BASEURL METABOOK")
    default_url = 'http://localhost:8899/'
    parser.add_option('-u', '--url',
        help='URL of HTTP interface to mw-serve (default: %r)' % default_url,
        default=default_url,
    )
    parser.add_option('-w', '--writer',
        help='writer to use for rendering (default: rl)',
        default='rl',
    )
    parser.add_option('--max-render-time',
        help='maximum number of seconds rendering may take (default: 120)',
        default='120',
        metavar='SECONDS',
    )
    parser.add_option('--save-output',
        help='if specified, save rendered file with given filename',
        metavar='FILENAME',
    )
    parser.add_option('-l', '--logfile',
        help='log output to LOGFILE',
    )
    parser.add_option('--report-from-mail',
        help='sender of error mails (--report-recipient also needed)',
        metavar='EMAIL',
    )
    parser.add_option('--report-recipient',
        help='recipient of error mails (--report-from-mail also needed)',
        metavar='EMAIL',
    )
    options, args = parser.parse_args()

    if len(args) != 2:
        parser.error('exactly 2 arguments required')

    base_url = args[0]
    metabook = open(args[1], 'rb').read()

    max_render_time = int(options.max_render_time)

    if options.report_recipient and options.report_from_mail:
        def report(msg):
            utils.report(
                system='mw-check-service',
                subject='mw-check-service error',
                from_email=options.report_from_mail.encode('utf-8'),
                mail_recipients=[options.report_recipient.encode('utf-8')],
                msg=msg,
            )
    else:
        report = log.ERROR

    writer = options.writer

    if options.logfile:
        utils.start_logging(options.logfile)

    client = Client(options.url)

    def check_req(command, **kwargs):
        try:
            success = client.request(command, kwargs, is_json=(command != 'download'))
        except Exception, exc:
            report('request failed: %s' % exc)
            sys.exit(1)

        if success:
            return client.response
        if client.error is not None:
            report('request failed: %s' % client.error)
            sys.exit(1)
        else:
            report('request failed: got response code %d' % client.response_code)
            sys.exit(1)

    start_time = time.time()

    log.info('sending render command')
    response = check_req('render',
        base_url=base_url,
        metabook=metabook,
        writer=writer,
        force_render=True,
    )
    collection_id = response['collection_id']

    while True:
        time.sleep(1)

        if time.time() - start_time > max_render_time:
            report('rendering exceeded allowed time of %d s' % max_render_time)
            sys.exit(2)

        log.info('checking status')
        response = check_req('render_status',
            collection_id=collection_id,
            writer=writer,
        )
        if response['state'] == 'finished':
            break

    log.info('downloading')
    response = check_req('download',
        collection_id=collection_id,
        writer=writer,
    )

    if len(response) < 100:
        report('got suspiciously small file from download: size is %d Bytes' % len(response))
        sys.exit(3)
    log.info('resulting file is %d Bytes' % len(response))

    if options.save_output:
        log.info('saving to %r' % options.save_output)
        open(options.save_output, 'wb').write(response)

    render_time = time.time() - start_time
    log.info('rendering ok, took %fs' % render_time)

########NEW FILE########
__FILENAME__ = argv
class error(Exception):
    pass

def parse(args, spec):    
    needarg = dict()
    
    for x in spec.split():
        if x.endswith("="):
            needarg[x[:-1]]=True
        else:
            needarg[x]=False

    opts=[]
    newargs = []
    
    i=0
    while i<len(args):
        a, v = (args[i].split("=", 1)+[None])[:2]
        if a in needarg:
            if v is None and needarg[a]:
                i += 1
                try:
                    v = args[i]
                except IndexError:
                    raise error("option %s needs an argument" % (a, ))
                
        
            opts.append((a, v))
        else:
            newargs.append(args[i])
            
        i += 1

    return opts, newargs

########NEW FILE########
__FILENAME__ = authors
#! /usr/bin/env python

# Copyright (c) 2007-2011 PediaPress GmbH
# See README.rst for additional licensing information.

import re


class inspect_authors(object):
    ip_rex = re.compile(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$')
    ip6_rex = re.compile(r'^(((?=.*(::))(?!.*\3.+\3))\3?|[\dA-F]{1,4}:)([\dA-F]{1,4}(\3|:\b)|\2){5}(([\dA-F]{1,4}(\3|:\b|$)|\2){2}|(((2[0-4]|1\d|[1-9])?\d|25[0-5])\.?\b){4})\Z', re.I)
    bot_rex = re.compile(r'bot$', re.IGNORECASE)
    ANON = "ANONIPEDITS"

    def __init__(self):
        self.num_anon = 0
        self.authors = set()

    def scan_edits(self, revs):
        authors = self.authors

        for r in revs:
            user = r.get('user', u'')
            if 'anon' in r and (not user or self.ip_rex.match(user) or self.ip6_rex.match(user)):  # anon
                self.num_anon += 1
            elif not user:
                continue
            elif self.bot_rex.search(user) or self.bot_rex.search(r.get('comment', '')):
                continue  # filter bots
            else:
                authors.add(user)

    def get_authors(self):
        """Return names of non-bot, non-anon users for changes of
        given article (before given revision).

        The data that can be used to compute a list of authors is
        limited:
        http://de.wikipedia.org/w/api.php?action=query&prop=revisions&rvlimit=500&
        rvprop=ids|timestamp|flags|comment|user|size&titles=Claude_Bourgelat

        @returns: sorted list of principal authors
        @rtype: list([unicode])
        """

        authors = list(self.authors)
        authors.sort()
        if authors or self.num_anon:
            authors.append("%s:%d" % (self.ANON, self.num_anon))  # append anon
        return authors


def get_authors(revs):
    i = inspect_authors()
    i.scan_edits(revs)
    return i.get_authors()

########NEW FILE########
__FILENAME__ = bookshelf
#!/usr/bin/env python

"""
Helper to list and retrieve all stored books from a wiki
"""
class Bookshelf(object):
    def __init__(self, api):
        self.api = api
        self.coll_bookscategory = 'Category:%s' % self.api.content_query('MediaWiki:Coll-bookscategory')

    def _getCategoryMembers(self, title):
        kwargs = {
            'action':'query',
            'list':'categorymembers',
            'cmtitle': title,# FIXME
            'cmlimit': 500
            }
        res = [] # {ns, title}
        while True:
            r = self.api.do_request(**kwargs)
            res.extend(r["query"].get("categorymembers",[]))
            if "query-continue" in r:
                kwargs["cmcontinue"] = r["query-continue"]["categorymembers"]["cmcontinue"]
            else:
                break
        return res

    def booknames(self):
        "returns a list of all book pages" 
        return [x['title'] for x in self._getCategoryMembers(self.coll_bookscategory)]


if __name__ =='__main__':
    from mwlib.mwapidb import get_api_helper
    b = Bookshelf(get_api_helper("http://en.wikipedia.org/w/"))
    print 'have %d books' % len(b.booknames())

########NEW FILE########
__FILENAME__ = caller
#! /usr/bin/env python

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import sys
import os

def caller(n=2):
    """return caller as string"""
    f = sys._getframe(n)
    return "%s:%s" % (f.f_code.co_filename, f.f_lineno)

def short(n=2):
    """return caller as string"""
    f = sys._getframe(n)
    return "%s:%s" % (os.path.basename(f.f_code.co_filename), f.f_lineno)
    
def callerframe(n=2):
    return sys._getframe(n)

########NEW FILE########
__FILENAME__ = client
import urllib

import mwlib.myjson as json


class Error(Exception): pass
    
class Client(object):
    "HTTP client to mw-serve"

    def __init__(self, url):
        self.url = url

    def request(self, command, args, is_json=True):
        self.error = None
        post_data = dict(args)
        post_data['command'] = command
        f = urllib.urlopen(self.url, urllib.urlencode(post_data))
        self.response = f.read()
        self.response_code = f.getcode()
        if self.response_code != 200:
            raise Error(self.response)
        
        if is_json:
            self.response = json.loads(self.response)
            if 'error' in self.response:
                self.error = self.response['error']
                raise Error(self.error)
            
        return self.response

########NEW FILE########
__FILENAME__ = conf
import sys
from mwlib._conf import confmod

sys.modules[__name__] = confmod(__name__)

########NEW FILE########
__FILENAME__ = dummydb

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

class DummyDB(object):
    def __init__(self, lang="en"):
        from mwlib import siteinfo
        self.siteinfo = siteinfo.get_siteinfo(lang)

    def getURL(self, title, revision=None):
        return None

    def get_siteinfo(self):
        return self.siteinfo

########NEW FILE########
__FILENAME__ = dumpparser
import os
import re

try:
    from xml.etree import cElementTree
except ImportError:
    import cElementTree

ns = '{http://www.mediawiki.org/xml/export-0.3/}'
class Tags:

    # <namespaces><namespace> inside <siteinfo>
    namespace = ns + 'namespaces/' + ns + 'namespace'

    page = ns + 'page'

    # <title> inside <page>
    title = ns + 'title'

    # <revision> inside <page>
    revision = ns + 'revision'

    # <id> inside <revision>
    revid = ns + 'id'

    # <contributor><username> inside <revision>
    username = ns + 'contributor/' + ns + 'username'

    # <text> inside <revision>
    text = ns + 'text'

    # <timestamp> inside <revision>
    timestamp = ns + 'timestamp'

    # <revision><text> inside <page>
    revision_text = ns + 'revision/' + ns + 'text'

    siteinfo = ns + "siteinfo"


class Page(object):
    __slots__ = [
        'title', 'pageid', 'namespace_text',
        'namespace',
        'revid', 'timestamp',
        'username', 'userid',
        'minor', 'comment', 'text'
    ]


    redirect_rex = re.compile(r'^#Redirect:?\s*?\[\[(?P<redirect>.*?)\]\]', re.IGNORECASE)

    @property
    def redirect(self):
        mo = self.redirect_rex.search(self.text)
        if mo:
            return mo.group('redirect').split("|", 1)[0]
        return None

    def __repr__(self):
        text = repr(self.text[:50])
        redir = self.redirect
        if redir:
            text = "Redirect to %s" % repr(redir)
        return 'Page(%s (@%s): %s)' % (repr(self.title), self.timestamp, text)


class DumpParser(object):

    tags = Tags()

    def __init__(self, xmlfilename,
                 ignore_redirects=False):
        self.xmlfilename = xmlfilename
        self.ignore_redirects = ignore_redirects

    def openInputStream(self):
        if self.xmlfilename.lower().endswith(".bz2"):
            f = os.popen("bunzip2 -c %s" % self.xmlfilename, "r")
        elif self.xmlfilename.lower().endswith(".7z"):
            f = os.popen("7z -so x %s" % self.xmlfilename, "r")
        else:
            f = open(self.xmlfilename, "r")        

        return f

    @staticmethod
    def getTag(elem):
        # rough is good enough
        return elem.tag[elem.tag.rindex('}')+1:]

    def handleSiteinfo(self, siteinfo):
        pass
    
        # for nsElem in siteinfo.findall(self.tags.namespace):
        #     try:
        #         self.namespaces[nsElem.text.lower()] = int(nsElem.get('key'))
        #     except AttributeError:
        #         # text is probably None
        #         pass
        
    def __iter__(self):
        f = self.openInputStream()    
        
        elemIter = (el for evt, el in cElementTree.iterparse(f))
        for elem in elemIter:
            if self.getTag(elem) == 'page':
                page = self.handlePageElement(elem)
                if page:
                    yield page
                elem.clear()
            elif self.getTag(elem) == 'siteinfo':
                self.handleSiteinfo(elem)
                elem.clear()
        
        f.close()
    
    def handlePageElement(self, pageElem):
        res = Page()
        lastRevision = None
        for el in pageElem:
            tag = self.getTag(el)
            if tag == 'title':
                title = unicode(el.text)
                res.title = title
            elif tag == 'id':
                res.pageid = int(el.text)
            elif tag == 'revision':
                lastRevision = el

        if lastRevision:
            self.handleRevisionElement(lastRevision, res)

        if self.ignore_redirects and res.redirect:
            return None

        return res

    def handleRevisionElement(self, revElem, res):
        for el in revElem:
            tag = self.getTag(el)
            if tag == 'id':
                res.revid = int(el.text)
            elif tag == 'timestamp':
                res.timestamp = el.text
            elif tag == 'contributor':
                pass
                #res.username, res.userid = self.handleContributorElement(el)
            elif tag == 'minor':
                res.minor = True
            elif tag == 'comment':
                res.comment = unicode(el.text)
            elif tag == 'text':
                res.text = unicode(el.text)
                el.clear()

        return res

    def handleContributorElement(self, conElem):
        username = None
        userid = None
        for el in conElem:
            if self.getTag(el) == 'username':
                username = unicode(el.text)
            elif self.getTag(el) == 'id':
                userid = int(el.text)
        return (username, userid)

########NEW FILE########
__FILENAME__ = expander
#! /usr/bin/env python

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import sys

from mwlib.templ import log

from mwlib.templ.nodes import Node, Variable, Template, show
from mwlib.templ.scanner import tokenize
from mwlib.templ.parser import parse, Parser
from mwlib.templ.evaluate import flatten, Expander, ArgumentList
from mwlib.templ.misc import DictDB, expandstr


def get_templates(raw, title=u""):
    used = set()
    e=Expander('', wikidb=DictDB())
    todo = [parse(raw, replace_tags=e.replace_tags)]
    while todo:
        n = todo.pop()
        if isinstance(n, basestring):
            continue
        
        if isinstance(n, Template) and isinstance(n[0], basestring):
            name = n[0]
            if name.startswith("/"):
                name = title+name
            used.add(name)
            
        todo.extend(n)
        
    return used

def find_template(raw, name, parsed_raw=None):
    """Return Template node with given name or None if there is no such template"""
    
    if not parsed_raw:
        e=Expander('', wikidb=DictDB())
        todo = [parse(raw, replace_tags=e.replace_tags)]
    else:
        todo = parsed_raw
    while todo:
        n = todo.pop()
        if isinstance(n, basestring):
            continue
        if isinstance(n, Template) and isinstance(n[0], basestring):
            if n[0] == name:
                return n
        todo.extend(n)

def get_template_args(template, expander):
    """Return ArgumentList for given template"""
    
    return ArgumentList(template[1],
        expander=expander,
        variables=ArgumentList(expander=expander),
    )
    

if __name__=="__main__":
    d=unicode(open(sys.argv[1]).read(), 'utf8')
    e = Expander(d)
    print e.expandTemplates()

########NEW FILE########
__FILENAME__ = expr
#! /usr/bin/env python

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.
# based on pyparsing example code (SimpleCalc.py)

"""Implementation of mediawiki's #expr template. 
http://meta.wikimedia.org/wiki/ParserFunctions#.23expr:
"""

from __future__ import division

import re
import inspect
import math

class ExprError(Exception):
    pass

def _myround(a,b):
    r=round(a, int(b))
    if int(r)==r:
        return int(r)
    return r


pattern = """
(?:\s+)
|((?:(?:\d+)(?:\.\d+)?
 |(?:\.\d+)))
|(\+|-|\*|/|>=|<=|<>|!=|[a-zA-Z]+|.)
"""

rxpattern = re.compile(pattern, re.VERBOSE | re.DOTALL | re.IGNORECASE)
def tokenize(s):
    res = []
    for (v1,v2) in rxpattern.findall(s):
        if not (v1 or v2):
            continue
        v2=v2.lower()
        if v2 in Expr.constants:
            res.append((v2,""))
        else:
            res.append((v1,v2))
    return res
        
    return [(v1,v2.lower()) for (v1,v2) in rxpattern.findall(s) if v1 or v2]

class uminus: pass
class uplus: pass

precedence = {"(":-1, ")":-1}
functions = {}
unary_ops = set()

def addop(op, prec, fun, numargs=None):
    precedence[op] = prec
    if numargs is None:
        numargs = len(inspect.getargspec(fun)[0])

    if numargs==1:
        unary_ops.add(op)
    
    def wrap(stack):
        assert len(stack)>=numargs
        args = tuple(stack[-numargs:])
        del stack[-numargs:]
        stack.append(fun(*args))

    functions[op] = wrap
        
a=addop
a(uminus, 10, lambda x: -x)
a(uplus, 10, lambda x: x)
a("^", 10, math.pow, 2)
a("not", 9, lambda x:int(not(bool(x))))
a("abs", 9, abs, 1)
a("sin", 9, math.sin, 1)
a("cos", 9, math.cos, 1)
a("asin", 9, math.asin, 1)
a("acos", 9, math.acos, 1)
a("tan", 9, math.tan, 1)
a("atan", 9, math.atan, 1)
a("exp", 9, math.exp, 1)
a("ln", 9, math.log, 1)
a("ceil", 9, lambda x: int(math.ceil(x)))
a("floor", 9, lambda x: int(math.floor(x)))
a("trunc", 9, long, 1)

a("e", 11, lambda x, y: x * 10 ** y)
a("E", 11, lambda x, y: x * 10 ** y)

a("*", 8, lambda x,y: x*y)
a("/", 8, lambda x,y: x/y)
a("div", 8, lambda x,y: x/y)
a("mod", 8, lambda x,y: int(x)%int(y))


a("+", 6, lambda x,y: x+y)
a("-", 6, lambda x,y: x-y)

a("round", 5, _myround)

a("<", 4, lambda x,y: int(x<y))
a(">", 4, lambda x,y: int(x>y))
a("<=", 4, lambda x,y: int(x<=y))
a(">=", 4, lambda x,y: int(x>=y))
a("!=", 4, lambda x,y: int(x!=y))
a("<>", 4, lambda x,y: int(x!=y))
a("=", 4, lambda x,y: int(x==y))

a("and", 3, lambda x,y: int(bool(x) and bool(y)))
a("or", 2, lambda x,y: int(bool(x) or bool(y)))
del a

class Expr(object):
    constants = dict(
        e=math.e,
        pi=math.pi)
    
    def as_float_or_int(self, s):
        try:
            return self.constants[s]
        except KeyError:
            pass
        
        if "." in s:
            return float(s)
        return long(s)
    
    def output_operator(self, op):
        return functions[op](self.operand_stack)
    
    def output_operand(self, operand):
        self.operand_stack.append(operand)
            
    def parse_expr(self, s):
        tokens = tokenize(s)
        if not tokens:
            return ""
        
        self.operand_stack = []
        operator_stack = []
        
        last_operand, last_operator = False, True
        
        for operand, operator in tokens:
            if operand in ("e", "E") and (last_operand or last_operator == ")"):
                operand, operator = operator, operand

            if operand:
                if last_operand:
                    raise ExprError("expected operator")
                self.output_operand(self.as_float_or_int(operand))
            elif operator=="(":
                operator_stack.append("(")
            elif operator==")":
                while 1:
                    if not operator_stack:
                        raise ExprError("unbalanced parenthesis")
                    t = operator_stack.pop()
                    if t=="(":
                        break
                    self.output_operator(t)
            elif operator in precedence:
                if last_operator and last_operator!=")":
                    if operator=='-':
                        operator = uminus
                    elif operator=='+':
                        operator = uplus
                        
                is_unary = operator in unary_ops
                prec = precedence[operator]
                while not is_unary and operator_stack and prec<=precedence[operator_stack[-1]]:
                    p = operator_stack.pop()
                    self.output_operator(p)
                operator_stack.append(operator)
            else:
                raise ExprError("unknown operator: %r" % (operator,))

            last_operand, last_operator = operand, operator
            
            
        while operator_stack:
            p=operator_stack.pop()
            if p=="(":
                raise ExprError("unbalanced parenthesis")
            self.output_operator(p)
            
        if len(self.operand_stack)!=1:
            raise ExprError("bad stack: %s" % (self.operand_stack,))

        return self.operand_stack[-1]

_cache = {}
def expr(s):
    try:
        return _cache[s]
    except KeyError:
        pass
    
    
    r = Expr().parse_expr(s)
    _cache[s] = r
    return r


def main():
    import time
    try:
        import readline  # do not remove. makes raw_input use readline
        readline
    except ImportError:
        pass
  
    while 1:
        input_string = raw_input("> ")
        if not input_string:
            continue
    
        stime = time.time()
        try:
            res=expr(input_string)
        except Exception, err:
            print "ERROR:", err
            import traceback
            traceback.print_exc()
            
            continue
        print res
        print time.time()-stime, "s"

if __name__=='__main__':
    main()
    
        

########NEW FILE########
__FILENAME__ = filequeue
import os
import subprocess
import time
import traceback
import cPickle

from mwlib.log import Log
from mwlib import utils


class FileJobQueuer(object):
    """Write a file for each new job request"""
    
    def __init__(self, queue_dir):
        self.queue_dir = utils.ensure_dir(queue_dir)
        self.log = Log('FileJobQueuer')
    
    def __call__(self, job_type, job_id, args):
        job_file = '%s.job' % os.path.join(self.queue_dir, job_id)
        if os.path.exists(job_file):
            self.log.warn('Job file %r already exists' % job_file)
            return
        
        open(job_file + '.tmp', 'wb').write(cPickle.dumps(args))
        os.rename(job_file + '.tmp', job_file)


class FileJobPoller(object):
    def __init__(self, queue_dir, processing_dir=None, sleep_time=1, max_num_jobs=5):
        self.queue_dir = utils.ensure_dir(queue_dir)
        self.sleep_time = sleep_time
        self.max_num_jobs = max_num_jobs
        self.num_jobs = 0
        self.log = Log('FileJobPoller')
        self.files = []
        
    def _reap_children(self):
        while self.num_jobs>0:
            try:
                if self.num_jobs==self.max_num_jobs:
                    flags = 0
                else:
                    flags = os.WNOHANG
                pid, rc = os.waitpid(-1, flags)
            except OSError, exc:
                self.log.ERROR('waitpid(-1) failed: %s' % exc)
                break
            if (pid, rc) == (0, 0):
                break
            self.num_jobs -= 1
            self.log.info('child %s exited: %s. have %d jobs' % (pid, rc, self.num_jobs))
            
    def run_forever(self):
        self.log.info('running with a max. of %d jobs' % self.max_num_jobs)
        while True:
            try:
                self.poll()
                if not self.files:
                    time.sleep(self.sleep_time)
                
                while self.num_jobs < self.max_num_jobs and self.files:
                    self.start_job(self.files.pop())

                self._reap_children()
            except KeyboardInterrupt:
                while self.num_jobs > 0:
                    os.waitpid(-1, 0)
                    self.num_jobs -= 1
                break
            except Exception, err:
                self.log.error("caught exception: %r" % (err, ))
                traceback.print_exc()
                    
        self.log.info('exit')
    
    def poll(self):
        if self.files:
            return
        
        files = []
        for filename in os.listdir(self.queue_dir):
            if filename.endswith(".tmp"):
                continue
            
            path = os.path.join(self.queue_dir, filename)
            if not os.path.isfile(path):
                continue
            try:
                mtime = os.stat(path).st_mtime
            except Exception, exc:
                self.log.ERROR('Could not stat %r: %s' % (path, exc))
                continue
            files.append((mtime, filename))

        files.sort(reverse=True)
        self.files = [x[1] for x in files]
    
    def start_job(self, filename):
        """Fork, and execute job from given file
        
        @returns: whether a new job as been started
        @rtype: bool
        """

        src = os.path.join(self.queue_dir, filename)
        try:
            args = cPickle.loads(open(src, 'rb').read())
        finally:
            os.unlink(src)
        
        self.log.info('starting job %r' % filename)
        
        pid = os.fork()
        self.num_jobs+=1
        
        if pid != 0:
            return True

        # child process:
        try:
            os.execvp(args[0], args)
        except:
            traceback.print_exc()
        finally:
            self.log.warn('error running %r' % (args,))
            os._exit(10)

########NEW FILE########
__FILENAME__ = htmlcolornames
colorname2rgb_map = {
'aliceblue' : (240, 248, 255),
'antiquewhite' : (250, 235, 215),
'aqua' : (0, 255, 255),
'aquamarine' : (127, 255, 212),
'azure' : (240, 255, 255),
'beige' : (245, 245, 220),
'bisque' : (255, 228, 196),
'black' : (0, 0, 0),
'blanchedalmond' : (255, 235, 205),
'blue' : (0, 0, 255),
'blueviolet' : (138, 43, 226),
'brown' : (165, 42, 42),
'burlywood' : (222, 184, 135),
'cadetblue' : (95, 158, 160),
'chartreuse' : (127, 255, 0),
'chocolate' : (210, 105, 30),
'coral' : (255, 127, 80),
'cornflowerblue' : (100, 149, 237),
'cornsilk' : (255, 248, 220),
'crimson' : (237, 164, 61),
'cyan' : (0, 255, 255),
'darkblue' : (0, 0, 139),
'darkcyan' : (0, 139, 139),
'darkgoldenrod' : (184, 134, 11),
'darkgray' : (169, 169, 169),
'darkgrey' : (169, 169, 169),
'darkgreen' : (0, 100, 0),
'darkkhaki' : (189, 183, 107),
'darkmagenta' : (139, 0, 139),
'darkolivegreen' : (85, 107, 47),
'darkorange' : (255, 140, 0),
'darkorchid' : (153, 50, 204),
'darkred' : (139, 0, 0),
'darksalmon' : (233, 150, 122),
'darkseagreen' : (143, 188, 143),
'darkslateblue' : (72, 61, 139),
'darkslategray' : (47, 79, 79),
'darkslategrey' : (47, 79, 79),
'darkturquoise' : (0, 206, 209),
'darkviolet' : (148, 0, 211),
'deeppink' : (255, 20, 147),
'deepskyblue' : (0, 191, 255),
'dimgray' : (105, 105, 105),
'dimgrey' : (105, 105, 105),
'dodgerblue' : (30, 144, 255),
'firebrick' : (178, 34, 34),
'floralwhite' : (255, 250, 240),
'forestgreen' : (34, 139, 34),
'fuchsia' : (255, 0, 255),
'gainsboro' : (220, 220, 220),
'ghostwhite' : (248, 248, 255),
'gold' : (255, 215, 0),
'goldenrod' : (218, 165, 32),
'gray' : (128, 128, 128),
'grey' : (128, 128, 128),
'green' : (0, 128, 0),
'greenyellow' : (173, 255, 47),
'honeydew' : (240, 255, 240),
'hotpink' : (255, 105, 180),
'indianred' : (205, 92, 92),
'indigo' : (75, 0, 130),
'ivory' : (255, 255, 240),
'khaki' : (240, 230, 140),
'lavender' : (230, 230, 250),
'lavenderblush' : (255, 240, 245),
'lawngreen' : (124, 252, 0),
'lemonchiffon' : (255, 250, 205),
'lightblue' : (173, 216, 230),
'lightcoral' : (240, 128, 128),
'lightcyan' : (224, 255, 255),
'lightgoldenrodyellow' : (250, 250, 210),
'lightgray' : (211, 211, 211),
'lightgrey' : (211, 211, 211),
'lightgreen' : (144, 238, 144),
'lightpink' : (255, 182, 193),
'lightsalmon' : (255, 160, 122),
'lightseagreen' : (32, 178, 170),
'lightskyblue' : (135, 206, 250),
'lightslategray' : (119, 136, 153),
'lightslategrey' : (119, 136, 153),
'lightsteelblue' : (176, 196, 222),
'lightyellow' : (255, 255, 224),
'lime' : (0, 255, 0),
'limegreen' : (50, 205, 50),
'linen' : (250, 240, 230),
'magenta' : (255, 0, 255),
'maroon' : (128, 0, 0),
'mediumaquamarine' : (102, 205, 170),
'mediumblue' : (0, 0, 205),
'mediumorchid' : (186, 85, 211),
'mediumpurple' : (147, 112, 219),
'mediumseagreen' : (60, 179, 113),
'mediumslateblue' : (123, 104, 238),
'mediumspringgreen' : (0, 250, 154),
'mediumturquoise' : (72, 209, 204),
'mediumvioletred' : (199, 21, 133),
'midnightblue' : (25, 25, 112),
'mintcream' : (245, 255, 250),
'mistyrose' : (255, 228, 225),
'moccasin' : (255, 228, 181),
'navajowhite' : (255, 222, 173),
'navy' : (0, 0, 128),
'oldlace' : (253, 245, 230),
'olive' : (128, 128, 0),
'olivedrab' : (107, 142, 35),
'orange' : (255, 165, 0),
'orangered' : (255, 69, 0),
'orchid' : (218, 112, 214),
'palegoldenrod' : (238, 232, 170),
'palegreen' : (152, 251, 152),
'paleturquoise' : (175, 238, 238),
'palevioletred' : (219, 112, 147),
'papayawhip' : (255, 239, 213),
'peachpuff' : (255, 218, 185),
'peru' : (205, 133, 63),
'pink' : (255, 192, 203),
'plum' : (221, 160, 221),
'powderblue' : (176, 224, 230),
'purple' : (128, 0, 128),
'red' : (255, 0, 0),
'rosybrown' : (188, 143, 143),
'royalblue' : (65, 105, 225),
'saddlebrown' : (139, 69, 19),
'salmon' : (250, 128, 114),
'sandybrown' : (244, 164, 96),
'seagreen' : (46, 139, 87),
'seashell' : (255, 245, 238),
'sienna' : (160, 82, 45),
'silver' : (192, 192, 192),
'skyblue' : (135, 206, 235),
'slateblue' : (106, 90, 205),
'slategray' : (112, 128, 144),
'slategrey' : (112, 128, 144),
'snow' : (255, 250, 250),
'springgreen' : (0, 255, 127),
'steelblue' : (70, 130, 180),
'tan' : (210, 180, 140),
'teal' : (0, 128, 128),
'thistle' : (216, 191, 216),
'tomato' : (255, 99, 71),
'turquoise' : (64, 224, 208),
'violet' : (238, 130, 238),
'wheat' : (245, 222, 179),
'white' : (255, 255, 255),
'whitesmoke' : (245, 245, 245),
'yellow' : (255, 255, 0),
'yellowgreen' : (154, 205, 50),
}

########NEW FILE########
__FILENAME__ = imgmap
#! /usr/bin/env python

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

from pyparsing import (Literal, restOfLine, Word, nums, Group, 
                       ZeroOrMore, OneOrMore, And, Suppress, LineStart, 
                       LineEnd, StringEnd, ParseException, Optional, White)

class gob(object): 
    def __init__(self, **kw):
        self.__dict__.update(kw)

    def __repr__(self):
        return "<%s %r>" % (self.__class__.__name__, self.__dict__)

class Poly(gob): pass
class Rect(gob): pass
class Circle(gob): pass
class Comment(gob): pass
class Desc(gob): pass
class Default(gob): pass
class ImageMap(gob): pass

def _makepoly(tokens):
    return Poly(caption=tokens[2].strip(), vertices=list(tokens[1]))

def _makerect(tokens):
    return Rect(caption=tokens[-1].strip(), top_left=tuple(tokens[1]), bottom_right=tuple(tokens[2]))

def _makecomment(tokens):
    return Comment(comment=tokens[1])

def _makecircle(tokens):
    return Circle(caption=tokens[3].strip(), center=tokens[1], radius=tokens[2])

def _makedesc(tokens):
    return Desc(location=tokens[1])

def _makeimagemap(tokens):
    image = None
    for x in tokens:
        if isinstance(x, basestring):
            image = x
            break
    return ImageMap(entries=list(tokens), image=image)

        
comment = (Literal('#')+restOfLine).setParseAction(_makecomment)

integer = Word(nums).setParseAction(lambda s: int(s[0]))
integer_pair = (integer+integer).setParseAction(lambda x: tuple(x))

poly = Literal("poly")+Group(ZeroOrMore(integer_pair))+restOfLine
poly = poly.setParseAction(_makepoly)

rect = Literal("rect")+integer_pair+integer_pair+restOfLine
rect = rect.setParseAction(_makerect)

circle = Literal("circle")+integer_pair+integer+restOfLine
circle = circle.setParseAction(_makecircle)

desc = Literal("desc") + (Literal("top-right")
                          |Literal("bottom-right")
                          |Literal("bottom-left")
                          |Literal("top-left")
                          |Literal("none"))
desc = desc.setParseAction(_makedesc)
default = Literal("default")+restOfLine
default.setParseAction(lambda t: Default(caption=t[1].strip()))


def _makeother(tokens):
    if not tokens[0]:
        return [None]
    return tokens

# we can't use restOfLine.setParseAction(_makeother) as that sets the 
# parse action for any occurence of restOfLine

other = And([restOfLine]).setParseAction(_makeother)
line = Suppress(LineStart()) + (comment | poly | rect | circle | desc | default | other) + Suppress(LineEnd())
imagemap = ZeroOrMore(line) + StringEnd()
imagemap.setParseAction(_makeimagemap)

def ImageMapFromString(s):
    # uhh. damn. can't get pyparsing to parse
    # commands, other lines (i.e. syntax errors strictly speaking)
    # and lines containing only whitespace...
    lines = []
    for x in s.split("\n"):
        x=x.strip()
        if x:
            lines.append(x)
    s="\n".join(lines)

    try:
        return imagemap.parseString(s)[0]
    except ParseException, err:
        return ImageMap(entries=[], image=None)

def main():
    ex="""


Image:Foo.jpg|200px|picture of a foo
poly 131 45 213 41 210 110 127 109 [[Display]]
poly 104 126 105 171 269 162 267 124 [[Keyboard]]
rect 15 95 94 176   [[Foo type A]]
# A comment, this line is ignored
circle 57 57 20    [[Foo type B]]
desc bottom-left
default [[Mainz]]
---dfg-sdfg--sdfg
blubb
"""
    res = ImageMapFromString(ex)
    for x in res.entries:
        print x

if __name__=='__main__':
    main()

########NEW FILE########
__FILENAME__ = infobox_magic
from xml.sax.saxutils import quoteattr
from mwlib import expander
from mwlib.templ import parser

def mark_infobox(self, name, raw):
    
    res = parser.parse(raw, replace_tags=self.replace_tags)
    if not name.lower().startswith("infobox"):
        return res
    print "marking infobox %r" % name
    return (u"<div templatename=%s>\n" % quoteattr(name), res, u"</div>")

def install():
    expander.Expander._parse_raw_template = mark_infobox

########NEW FILE########
__FILENAME__ = l10n
from itertools import dropwhile
import os
import subprocess

ext2lang = {
    '.py': 'Python',
}

def execute(*args):
    popen = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    output, errors = popen.communicate()
    popen.wait()
    if errors:
        raise RuntimeError('Errors while executing %r: %s' % (
            ' '.join(args),
            errors),
        )
    elif popen.returncode != 0:
        raise RuntimeError('Got returncode %d != 0 when executing %r' % (
            popen.returncode, ' '.join(args),
        ))
    return output

def make_messages(locale, domain, version, inputdir,
    localedir='locale',
    extensions=('.py',),
):
    assert os.path.isdir(localedir), 'no directory %s found' % (localedir,)
    assert os.path.isdir(inputdir), 'no directory %s found' % (inputdir,)
    
    languages = []
    if locale == 'all':
        languages = [lang for lang in os.listdir(localedir) if not lang.startswith('.')]
    else:
        languages.append(locale)
    
    for locale in languages:
        print "processing language", locale
        basedir = os.path.join(localedir, locale, 'LC_MESSAGES')
        if not os.path.isdir(basedir):
            os.makedirs(basedir)
        
        pofile = os.path.join(basedir, '%s.po' % domain)
        potfile = os.path.join(basedir, '%s.pot' % domain)
        
        if os.path.exists(potfile):
            os.unlink(potfile)
        
        all_files = []
        for (dirpath, dirnames, filenames) in os.walk(inputdir):
            all_files.extend([(dirpath, f) for f in filenames])
        all_files.sort()
        for dirpath, filename in all_files:
            file_base, file_ext = os.path.splitext(filename)
            if file_ext not in extensions:
                continue
            msgs = execute(
                'xgettext',
                '--default-domain', domain,
                '--language', ext2lang[file_ext],
                '--from-code', 'UTF-8',
                '--output', '-',
                os.path.join(dirpath, filename),
            )
            if os.path.exists(potfile):
                # Strip the header
                msgs = '\n'.join(dropwhile(len, msgs.split('\n')))
            else:
                msgs = msgs.replace('charset=CHARSET', 'charset=UTF-8')
            if msgs:
                open(potfile, 'ab').write(msgs)
        
        if os.path.exists(potfile):
            msgs = execute('msguniq', '--to-code', 'UTF-8', potfile)
            open(potfile, 'wb').write(msgs)
            if os.path.exists(pofile):
                msgs = execute('msgmerge', '--quiet', pofile, potfile)
            open(pofile, 'wb').write(msgs)
            os.unlink(potfile)


def compile_messages(localedir='locale'):
    for dirpath, dirnames, filenames in os.walk(localedir):
        for f in filenames:
            if f.endswith('.po'):
                path = os.path.join(dirpath, f)
                mo_filename = os.path.splitext(path)[0] + '.mo'
                try:
                    execute('msgfmt', '--check-format', '--output-file', mo_filename, path)
                except RuntimeError, exc:
                    print 'Could not compile %r: %s' % (path, exc)

########NEW FILE########
__FILENAME__ = linuxmem
_scale = dict(kB=1024.0,  KB=1024.0,
              mB=1024.0 * 1024.0, MB=1024.0 * 1024.0)

# convert byte o MB
for k, v in _scale.items():
    _scale[k] = v / (1024.0 * 1024.0)


def _readproc(key):
    '''Private.
    '''
    try:
        v = open("/proc/self/status").read()
         # get key line e.g. 'VmRSS:  9999  kB\n ...'
        i = v.index(key)
        v = v[i:].split(None, 3)  # whitespace
        if len(v) < 3:
            return 0.0  # invalid format?
         # convert Vm value to bytes
        return float(v[1]) * _scale[v[2]]
    except:
        return 0.0  # non-Linux?


def memory():
    '''Return memory usage in MB.
    '''
    return _readproc('VmSize:')


def resident():
    '''Return resident memory usage in MB.
    '''
    return _readproc('VmRSS:')


def stacksize():
    '''Return stack size in MB.
    '''
    return _readproc('VmStk:')


def report():
    print "memory used: res=%.1f virt=%.1f" % (resident(), memory())


if __name__ == "__main__":
    report()

########NEW FILE########
__FILENAME__ = log
#! /usr/bin/env python

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import sys
import time

class Stdout(object):
    """late-bound sys.stdout"""
    def write(self, msg):
        sys.stdout.write(msg)

    def flush(self):
        sys.stdout.flush()

class Stderr(object):
    """late-bound sys.stderr"""
    def write(self, msg):
        sys.stderr.write(msg)

    def flush(self):
        sys.stderr.flush()

class Log(object):
    logfile = Stderr()
    timestamp_fmt = '%Y-%m-%dT%H:%M:%S'
    
    def __init__(self, prefix=None, timestamps=True):
        self.timestamps = timestamps
        if prefix is None:
            self._prefix = []
        else:
            if isinstance(prefix, basestring):
                self._prefix = [prefix]
            else:
                self._prefix = prefix

    def __getattr__(self, name):
        return Log([self, name], timestamps=self.timestamps)

    def __nonzero__(self):
        return bool(self._prefix)
    
    def __str__(self):
        return ".".join(str(x) for x in self._prefix if x)
                 
    def __call__(self, msg, *args):
        if not self.logfile:
            return
        
        if not isinstance(msg, str):
            msg = repr(msg)
        
        if args:
            msg = " ".join(([msg] + [repr(x) for x in args]))
        
        s = ''
        if self.timestamps:
            s = '%s ' % time.strftime(self.timestamp_fmt)
        s += "%s >> %s\n" % (".".join(str(x) for x in self._prefix if x), msg)
        self.logfile.write(s)

########NEW FILE########
__FILENAME__ = lrucache

# based on code by Raymond Hettinger
# see http://code.activestate.com/recipes/498245/

import threading
from collections import deque

class lrucache(object):
    def __init__(self, maxsize):
        self.maxsize = maxsize
        self.cache = {}
        self.queue = deque()         # order that keys have been accessed
        self.refcount = {}           # number of times each key is in the access queue
        self.hits = 0
        self.misses = 0
        
    def __getitem__(self, key):
        # get cache entry or compute if not found
        try:
            result = self.cache[key]
            self.hits += 1
            self._record_key(key)
            return result
        except KeyError:
            self.misses += 1
            raise

    def __setitem__(self, key, value):
        self.cache[key] = value
        self._record_key(key)
        
    def _record_key(self, key):
        # localize variable access (ugly but fast)
        queue=self.queue
        cache=self.cache
        _len=len
        refcount=self.refcount
        _maxsize=self.maxsize
        queue_append=self.queue.append
        queue_popleft = self.queue.popleft
        
        # record that this key was recently accessed
        self.queue.append(key)
        self.refcount[key] = self.refcount.get(key, 0) + 1

        # Purge least recently accessed cache contents
        while _len(cache) > _maxsize:
            k = queue_popleft()
            refcount[k] -= 1
            if not refcount[k]:
                del cache[k]
                del refcount[k]

        # Periodically compact the queue by duplicate keys
        if _len(queue) > _maxsize * 4:
            for i in [None] * _len(queue):
                k = queue_popleft()
                if refcount[k] == 1:
                    queue_append(k)
                else:
                    refcount[k] -= 1

class mt_lrucache(lrucache):
    def __init__(self, maxsize):
        lrucache.__init__(self, maxsize)
        self.lock = threading.Lock()
        
    def __getitem__(self, key):
        try:
            self.lock.acquire()
            return lrucache.__getitem__(self, key)
        finally:
            self.lock.release()

    def __setitem__(self, key, val):
        try:
            self.lock.acquire()
            lrucache.__setitem__(self, key, val)
        finally:
            self.lock.release()            

########NEW FILE########
__FILENAME__ = main_trampoline
# nserve/nslave only monkeypatch when imported as __main__ so, we
# monkeypatch here and then call into the respective main function


from gevent import monkey
monkey.patch_all()


def nserve_main():
    from mwlib.nserve import main
    return main()


def nslave_main():
    from mwlib.nslave import main
    return main()


def postman_main():
    from mwlib.postman import main
    return main()

########NEW FILE########
__FILENAME__ = mathml
"""
converts LaTex to Mathml using blahtexml

FIXME: Robustness, error handling, ....
# see integration in MW: 
http://cvs.berlios.de/cgi-bin/viewcvs.cgi/blahtex/blahtex/includes/Math.php?rev=HEAD&content-type=text/vnd.viewcvs-markup

FIXME: replace with texvc which is deistributed with MediaWiki


"""
import sys
import popen2
try:
    import xml.etree.ElementTree as ET
except:
    from elementtree import ElementTree as ET
from xml.parsers.expat import ExpatError


def log(err):
    sys.stderr.write(err + " ")
    pass


def latex2mathml(latex):

    data = "\\displaystyle\n%s\n" %  latex.strip()  
    r, w, e = popen2.popen3('blahtexml --mathml')
    w.write(data)
    w.close()
    errormsg = e.read()
    outmsg = r.read()
    r.close()
    e.close()

    if outmsg:
        # ET has unreadable namespace handling
        # http://effbot.org/zone/element.htm#xml-namespaces
        #ET._namespace_map["http://www.w3.org/1998/Math/MathML"] = 'mathml'
        # remove xmlns declaration
        #outmsg = outmsg.replace('xmlns="http://www.w3.org/1998/Math/MathML"', '')

        outmsg = '<?xml version="1.0" encoding="UTF-8"?>\n' + outmsg
        #print repr(outmsg)
    
        try:
            p =  ET.fromstring(outmsg)
        except ExpatError:
            log("\n\nparsing failed\n\n" )
            log(latex +"\n\n")
            log(data +"\n\n")
            log(errormsg +"\n")
            log(outmsg +"\n")
            return 
            

        tag = "mathml"
        mathml = p.getiterator(tag)
        
        if mathml:
            mathml=mathml[0]
            mathml.set("xmlns","http://www.w3.org/1998/Math/MathML")
            # add annotation with original TeX
            #a = ET.Element("annotation", encoding="TeX")
            #a.text=latex
            #mathml.append(a)
            return mathml
        else:
            log ("an error occured, \n%s\n" % outmsg)
        

if __name__ == '__main__':
    test = "\exp(-\gamma x)"
    print
    print ET.tostring(latex2mathml(test))
    

########NEW FILE########
__FILENAME__ = mathutils
#! /usr/bin/env python

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import os
import tempfile
import shutil
from subprocess import Popen, PIPE

try:
    import xml.etree.ElementTree as ET
except:
    from elementtree import ElementTree as ET

from mwlib import log

log = log.Log('mwlib.mathutils')

def try_system(cmd):
    n = os.path.devnull
    cmd += " >%s 2>%s" % (n, n)
    return os.system(cmd)

texvc_available = not try_system("texvc")
blahtexml_available = not try_system("blahtexml")

def _renderMathBlahtex(latex, output_path, output_mode):
    if not blahtexml_available:
        return None
    cmd = ['blahtexml', '--texvc-compatible-commands']
    if output_mode == 'mathml':
        cmd.append('--mathml')
    elif output_mode == 'png':
        cmd.append('--png')
    else:
        return None

    if output_path:
        try: # for some reason os.getcwd failed at some point. this should be investigated...
            curdir = os.getcwd()
        except:
            curdir = None
        os.chdir(output_path)
    latex = latex.lstrip()
    if not latex:
        return None

    try:
        sub = Popen(cmd, stdout=PIPE, stdin=PIPE, stderr=PIPE)
    except OSError:
        log.error('error with blahtexml. cmd:', repr(' '.join(cmd)))
        if curdir:
            os.chdir(curdir)
        return None

    (result, error) = sub.communicate(latex.encode('utf-8'))
    del sub

    if curdir is not None:
        os.chdir(curdir)
    if result:
        p = ET.fromstring(result)
        if output_mode == 'png':
            r = p.getiterator('png')
            if r:
                png_fn =  r[0].findtext('md5')
                if png_fn:
                    png_fn = os.path.join(output_path, png_fn + '.png')
                    if os.path.exists(png_fn):
                        return png_fn
        elif output_mode == 'mathml':
            mathml = p.getiterator('mathml')
            if mathml:
                mathml = mathml[0]
                mathml.set("xmlns","http://www.w3.org/1998/Math/MathML")
                return mathml
    log.error('error converting math (blahtexml). source: %r \nerror: %r' % (latex, result))
    return None

def _renderMathTexvc(latex, output_path, output_mode='png', resolution_in_dpi=120):
    """only render mode is png"""
    if not texvc_available:
        return None
    cmd = ['texvc', output_path, output_path, latex.encode('utf-8'), "UTF-8", str(resolution_in_dpi)]
    try:
        sub = Popen(cmd, stdout=PIPE, stdin=PIPE, stderr=PIPE)
    except OSError:
        log.error('error with texvc. cmd:', repr(' '.join(cmd)))
        return None
    (result, error) = sub.communicate()
    del sub

    if output_mode == 'png':
        if len(result) >= 32:
            png_fn = os.path.join(output_path, result[1:33] + '.png')
            if os.path.exists(png_fn):
                return png_fn

    log.error('error converting math (texvc). source: %r \nerror: %r' % (latex, result))
    return None

def renderMath(latex, output_path=None, output_mode='png', render_engine='blahtexml', resolution_in_dpi=120):
    """
    @param latex: LaTeX code
    @type latex: unicode

    @param output_mode: one of the values 'png' or 'mathml'. mathml only works
        with blahtexml as render_engine
    @type output_mode: str

    @param render_engine: one of the value 'blahtexml' or 'texvc'
    @type render_engine: str

    @returns: either path to generated png or mathml string
    @rtype: basestring
    """
    if not latex:
        return
    assert output_mode in ("png", "mathml")
    assert render_engine in ("texvc", "blahtexml")
    assert isinstance(latex, unicode), 'latex must be of type unicode'


    if output_mode == 'png' and not output_path:
        log.error('math rendering with output_mode png requires an output_path')
        raise Exception("output path required")

    removeTmpDir = False
    if output_mode == 'mathml' and not output_path:
        output_path = tempfile.mkdtemp()
        removeTmpDir = True
    output_path = os.path.abspath(output_path)
    result = None

    if render_engine == 'blahtexml':
        result = _renderMathBlahtex(latex, output_path=output_path, output_mode=output_mode)
    if result is None and output_mode == 'png':
        result = _renderMathTexvc(latex, output_path=output_path, output_mode=output_mode, resolution_in_dpi=resolution_in_dpi)

    if removeTmpDir:
        shutil.rmtree(output_path)
    return result


if __name__ == "__main__":

    latex = u"\\sqrt{4}=2"

##     latexlist = ["\\sqrt{4}=2",
##                  r"a^2 + b^2 = c^2\,",
##                  r"E = m c^2",
##                  r"\begin{matrix}e^{\mathrm{i}\,\pi}\end{matrix}+1=0\;",
##                  r"1\,\mathrm{\frac{km}{h}} = 0{,}2\overline{7}\,\mathrm{\frac{m}{s}}",
##                  r"""\begin{array}{ccc}
## F^2\sim W&\Leftrightarrow&\frac{F_1^2}{F_2^2}=\frac{W_1}{W_2}\\
## \ln\frac{F_1}{F_2}\,\mathrm{Np}&=&
## \frac{1}{2}\ln\frac{W_1}{W_2}\,\mathrm{Np}\\
## 20\,\lg\frac{F_1}{F_2}\,\mathrm{dB}&=&
## 10\,\lg\frac{W_1}{W_2}\,\mathrm{dB}
## \end{array}
##                  """,
##         ]

    print renderMath(latex,  output_mode='mathml')
    print renderMath(latex,  output_path="/tmp/", output_mode='png')
    print renderMath(latex,  output_path="/tmp/", output_mode='png', render_engine='texvc')

########NEW FILE########
__FILENAME__ = metabook
#! /usr/bin/env python

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import warnings
from collections import deque
from hashlib import md5

import copy

def parse_collection_page(txt):
    from mwlib.parse_collection_page import parse_collection_page
    return parse_collection_page(txt)

class mbobj(object):
    def __init__(self, **kw):
        d = dict(type=self.__class__.__name__)
        
        for k in dir(self.__class__):
            if k.startswith("__"):
                continue
            v = getattr(self.__class__, k)
            if callable(v) or v is None:
                continue
            if isinstance(v, (property, )):
                continue
            
            d[k] = v
            
        self.__dict__.update(copy.deepcopy(d))
        self.__dict__.update(kw)
        self.type = self.__class__.__name__
        
    def __getitem__(self, key):
        warnings.warn("deprecated __getitem__ [%r]" % (key,), DeprecationWarning, 2)
    
        try:
            return getattr(self, str(key))
        except AttributeError:
            raise KeyError(repr(key))
    
    def __setitem__(self, key, val):
        warnings.warn("deprecated __setitem__ [%r]=" % (key, ), DeprecationWarning, 2)
        
        self.__dict__[key]=val

    def __contains__(self,  key):
        warnings.warn("deprecated __contains__ %r in " % (key, ), DeprecationWarning, 2)
        val = getattr(self, str(key), None)
        return val is not None
        
    def get(self, key, default=None):
        warnings.warn("deprecated call get(%r)" % (key, ), DeprecationWarning, 2)
        try:
            val = getattr(self, str(key))
            if val is None:
                return default
            return val
        except AttributeError:
            return default
        
    def _json(self):
        d = dict(type=self.__class__.__name__)
        for k, v in self.__dict__.items():
            if v is not None and not k.startswith("_"):
                d[k]=v
        return d
    
    def __repr__(self):
        return "<%s %r>" % (self.__class__.__name__,  self.__dict__)

class wikiconf(mbobj):
    baseurl = None
    ident = None
    def __init__(self, env=None, pages=None, **kw):
        mbobj.__init__(self, **kw)
        
                 
class collection(mbobj):
    title = None
    subtitle=None
    editor = None
    cover_image = None
    cover_color = None
    text_color = None
    description = None
    sort_as = None

    version = 1
    summary = ""
    items = []
    licenses = []
    wikis = []
    _env = None
    
    def append_article(self, title, displaytitle=None, **kw):
        title = title.strip()
        if displaytitle is not None:
            displaytitle = displaytitle.strip()
        art = article(title=title, displaytitle=displaytitle,  **kw)

        if self.items and isinstance(self.items[-1], chapter):
            self.items[-1].items.append(art)
        else:
            self.items.append(art)
    
    def dumps(self):
        from mwlib import myjson
        return myjson.dumps(self, sort_keys=True, indent=4)
    
    def walk(self,  filter_type=None):
        todo = deque(self.items)
        res = []
        while todo:
            elem = todo.popleft()
            if not filter_type or elem.type == filter_type:
                res.append(elem)
            items = getattr(elem, "items", None)
            if items:
                todo.extendleft(items[::-1])
        return res

    def articles(self):
        return self.walk("article")

    def set_environment(self, env):
        if env.wikiconf:
            self.wikis.append(env.wikiconf)
            
        for x in self.articles():
            if x._env is None:
                x._env = env

    def get_wiki(self, ident=None, baseurl=None):
        assert ident is not None or baseurl is not None
        assert ident is None or baseurl is None

        for wikiconf in self.wikis:
            if ident is not None and wikiconf.ident == ident:
                return wikiconf
            if baseurl is not None and wikiconf.baseurl == baseurl:
                return wikiconf
        return None
                
class source(mbobj):
    name=None
    url=None
    language=None
    base_url=None
    script_extension=None
    locals = None
    system="MediaWiki"
    namespaces = None
    

class interwiki(mbobj):
    local=False

class custom(mbobj):
    title=None
    content=None
    content_type='text/x-wiki'

class article(mbobj):
    title=None
    displaytitle=None
    revision=None
    content_type="text/x-wiki"
    wikiident=None
    _env = None

    @property
    def wiki(self):
        return self._env.wiki

    @property
    def images(self):
        return self._env.images
    
class license(mbobj):
    title=None
    wikitext=None
    
class chapter(mbobj):
    items=[]
    title=u''


# ==============================================================================


def append_article(article, displaytitle, metabook, revision=None):
    metabook.append_article(article, displaytitle, revision=revision)

def get_item_list(metabook, filter_type=None):
    """Return a flat list of items in given metabook
    
    @param metabook: metabook dictionary
    @type metabook: dict
    
    @param filter_type: if set, return only items with this type
    @type filter_type: basestring
    
    @returns: flat list of items
    @rtype: [{}]
    """
    return metabook.walk(filter_type=filter_type)

def calc_checksum(metabook):
    return md5(metabook.dumps()).hexdigest() 
    
def get_licenses(metabook):
    """Return list of licenses
    
    @returns: list of dicts with license info
    @rtype: [dict]
    """
    import re
    from mwlib import utils
    retval = []
    for l in metabook.licenses:
        wikitext = ''

        if l.get('mw_license_url'):
            url = l['mw_license_url']
            if re.match(r'^.*/index\.php.*action=raw', url) and 'templates=expand' not in url:
                url += '&templates=expand'
            wikitext = utils.fetch_url(url,
                ignore_errors=True,
                expected_content_type='text/x-wiki',
            )
            if wikitext:
                try:
                    wikitext = unicode(wikitext, 'utf-8')
                except UnicodeError:
                    wikitext = None
        else:
            wikitext = ''
            if l.get('mw_rights_text'):
                wikitext = l['mw_rights_text']
            if l.get('mw_rights_page'):
                wikitext += '\n\n[[%s]]' % l['mw_rights_page']
            if l.get('mw_rights_url'):
                wikitext += '\n\n' + l['mw_rights_url']
        
        if not wikitext:
            continue

        retval.append(license(title=l.get('name', u'License'),
                              wikitext=wikitext))
    
    return retval 

def make_interwiki(api_entry=None):
    api_entry = api_entry or {}
    d={}
    for k, v in api_entry.items():
        d[str(k)] = v
    return interwiki(**d)

make_metabook  = collection
make_chapter   = chapter
make_source    = source
make_article   = article
make_custom    = custom

########NEW FILE########
__FILENAME__ = myjson
# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

"""custom json encoder/decoder, which can handle metabook objects"""

from mwlib import metabook

try:
    import simplejson as json
except ImportError:
    import json
    from json import loads # protect us against http://pypi.python.org/pypi/python-json/


def object_hook(dct):
    try:
        type = dct["type"]
    except KeyError:
        type = None
        
    if type in ["collection", "article", "chapter", "source", "interwiki",  "license",  "wikiconf", "custom"]:
        klass = getattr(metabook, type)
        d = {}
        for k, v in dct.items():
            d[str(k)] = v
        d["type"] = type
        return klass(**d)
        
    return dct

class mbencoder(json.JSONEncoder):
    def default(self, obj):
        try:
            m = obj._json
        except AttributeError:
            return json.JSONEncoder.default(self, obj)
        
        return m()

def loads(data):
    return json.loads(data, object_hook=object_hook)

def dump(obj, fp, **kw):
    return json.dump(obj, fp, cls=mbencoder, **kw)

def dumps(obj, **kw):
    return json.dumps(obj, cls=mbencoder, **kw)

def load(fp):
    return json.load(fp, object_hook=object_hook)

########NEW FILE########
__FILENAME__ = fetch
#! /usr/bin/env python

# Copyright (c) 2007-2011 PediaPress GmbH
# See README.rst for additional licensing information.

import os, sys, urlparse, urllib2, time, traceback
import gevent, gevent.pool, gevent.coros, gevent.event

import sqlite3dbm
from lxml import etree

from mwlib import utils, nshandling, conf, myjson as json
from mwlib.net import sapi as mwapi


class shared_progress(object):
    status = None
    last_percent = 0.0

    def __init__(self, status=None):
        self.key2count = {}
        self.status = status
        self.stime = time.time()

    def report(self):
        isatty = getattr(sys.stdout, "isatty", None)
        done, total = self.get_count()
        if not total:
            total = 1

        if total < 50:
            percent = done / 5.0
        else:
            percent = 100.0 * done / total

        percent = round(percent, 3)

        needed = time.time() - self.stime
        if needed < 60.0:
            percent *= needed / 60.0

        if percent <= self.last_percent:
            percent = self.last_percent

        self.last_percent = percent

        if isatty and isatty():
            msg = "%s/%s %.2f %.2fs" % (done, total, percent, needed)
            if sys.platform in ("linux2", "linux3"):
                from mwlib import linuxmem
                msg += " %.1fMB" % linuxmem.resident()
            sys.stdout.write("\x1b[K" + msg + "\r")
            sys.stdout.flush()

        if self.status:
            try:
                s = self.status.stdout
                self.status.stdout = None
                self.status(status="fetching", progress=percent)
            finally:
                self.status.stdout = s

    def set_count(self, key, done, total):
        self.key2count[key] = (done, total)
        self.report()

    def get_count(self):
        done = 0
        total = 0
        for (d, t) in self.key2count.values():
            done += d
            total += t
        return done, total


class fsoutput(object):
    def __init__(self, path):
        self.path = os.path.abspath(path)
        assert not os.path.exists(self.path)
        os.makedirs(os.path.join(self.path, "images"))
        self.revfile = open(os.path.join(self.path, "revisions-1.txt"), "wb")
        self.seen = dict()
        self.imgcount = 0
        self.nfo = None

        for storage in ['authors', 'html', 'imageinfo']:
            fn = os.path.join(self.path, storage + '.db')
            db = sqlite3dbm.open(fn, 'n')
            db.conn.execute("PRAGMA synchronous = 0")
            setattr(self, storage, db)

    def set_db_key(self, name, key, value):
        storage = getattr(self, name, None)
        assert storage is not None, 'storage not existant %s' % name
        storage[key] = json.dumps(value)

    def close(self):
        if self.nfo is not None:
            self.dump_json(nfo=self.nfo)
        self.revfile.close()
        self.revfile = None

    def get_imagepath(self, title):
        p = os.path.join(self.path, "images", "%s" % (utils.fsescape(title),))
        self.imgcount += 1
        return p

    def dump_json(self, **kw):
        for k, v in kw.items():
            p = os.path.join(self.path, k + ".json")
            json.dump(v, open(p, "wb"), indent=4, sort_keys=True)

    def write_siteinfo(self, siteinfo):
        self.dump_json(siteinfo=siteinfo)

    def write_excluded(self, excluded):
        self.dump_json(excluded=excluded)

    def write_licenses(self, licenses):
        self.dump_json(licenses=licenses)

    def write_expanded_page(self, title, ns, txt, revid=None):
        rev = dict(title=title, ns=ns, expanded=1)
        if revid is not None:
            rev["revid"] = revid

        header = "\n --page-- %s\n" % json.dumps(rev, sort_keys=True)
        self.revfile.write(header)
        self.revfile.write(txt.encode("utf-8"))
        self.seen[title] = rev


    def write_pages(self, data):
        pages = data.get("pages", {}).values()
        for p in pages:

            title = p.get("title")
            ns = p.get("ns")
            revisions = p.get("revisions")

            if revisions is None:
                continue

            for r in revisions:
                revid = r.get("revid")
                txt = r["*"]
                if revid not in self.seen:
                    rev = dict(title=title, ns=ns)
                    if revid is not None:
                        self.seen[revid] = rev
                        rev["revid"] = revid
                    self.seen[title] = rev

                    header = "\n --page-- %s\n" % json.dumps(rev, sort_keys=True)
                    self.revfile.write(header)
                    self.revfile.write(txt.encode("utf-8"))
                # else:
                #     print "fsoutput: skipping duplicate:", dict(revid=revid, title=title)

    def write_authors(self):
        self.authors.close()

    def write_html(self):
        self.html.close()

    def write_redirects(self, redirects):
        self.dump_json(redirects=redirects)


def splitblocks(lst, limit):
    """Split list lst in blocks of max. limit entries. Return list of blocks."""
    res = []
    start = 0
    while start < len(lst):
        res.append(lst[start:start + limit])
        start += limit
    return res


def getblock(lst, limit):
    """Return first limit entries from list lst and remove them from the list"""

    r = lst[-limit:]
    del lst[-limit:]
    return r


def callwhen(event, fun):
    while 1:
        try:
            event.wait()
            event.clear()
            fun()
        except gevent.GreenletExit:
            raise
        except Exception:
            traceback.print_exc()
            pass


def download_to_file(url, path, temp_path):
    opener = urllib2.build_opener()
    opener.addheaders = [('User-Agent', conf.user_agent)]

    try:
        out = None
        size_read = 0
        f = opener.open(url)
        while 1:
            data = f.read(16384)
            if not data:
                break
            size_read += len(data)
            if out is None:
                out = open(temp_path, "wb")
            out.write(data)

        if out is not None:
            out.close()
            os.rename(temp_path, path)
        # print "GOT", url, size_read

    except Exception, err:
        print "ERROR DOWNLOADING", url, err
        raise


class fetcher(object):
    def __init__(self, api, fsout, pages, licenses,
                 status=None,
                 progress=None,
                 cover_image=None,
                 imagesize=800, fetch_images=True):

        self.dispatch_event = gevent.event.Event()
        self.api_semaphore = gevent.coros.Semaphore(20)

        self.cover_image = cover_image

        self.pages = pages

        self.image_download_pool = gevent.pool.Pool(10)

        self.fatal_error = "stopped by signal"

        self.api = api
        self.api.report = self.report
        self.api_cache = {self.api.apiurl: self.api,}

        self.fsout = fsout
        self.licenses = licenses
        self.status = status
        self.progress = progress or shared_progress(status=status)

        self.imagesize = imagesize
        self.fetch_images = fetch_images

        self.scheduled = set()

        self.count_total = 0
        self.count_done = 0
        self.redirects = {}
        self.cat2members = {}

        self.img_max_retries = 2

        self.title2latest = {}

        self.pages_todo = []
        self.revids_todo = []
        self.imageinfo_todo = []
        self.imagedescription_todo = {}  # base path -> list
        self._nshandler = None

        siteinfo = self.get_siteinfo_for(self.api)
        self.fsout.write_siteinfo(siteinfo)
        self.nshandler = nshandling.nshandler(siteinfo)

        params = mwapi.get_collection_params(api)
        self.__dict__.update(params)

        self.make_print_template = None

        titles, revids = self._split_titles_revids(pages)

        self.pool = gevent.pool.Pool()
        self.refcall_pool = gevent.pool.Pool(1024)

        self._refcall(self.fetch_html, "page", titles)
        self._refcall(self.fetch_html, "oldid", revids)

        self._refcall(self.fetch_used, "titles", titles, True)
        self._refcall(self.fetch_used, "revids", revids, True)

        for t in titles:
            self._refcall(self.expand_templates_from_title, t)

        for r in revids:
            self._refcall(self.expand_templates_from_revid, int(r))

    def expand_templates_from_revid(self, revid):
        res = self.api.do_request(action="query", prop="revisions", rvprop="content", revids=str(revid))
        page = res["pages"].values()[0]

        title = page["title"]
        text = page["revisions"][0]["*"]
        res = self.api.do_request(use_post=True, action="expandtemplates", title=title, text=text).get("expandtemplates", {})

        txt = res.get("*")
        if txt:
            redirect = self.nshandler.redirect_matcher(txt)
            if redirect:
                self.redirects[title] = redirect
                self._refcall(self.expand_templates_from_title, redirect)
                self._refcall(self.fetch_used, "titles", [redirect], True)

            self.fsout.write_expanded_page(title, page["ns"], txt, revid=revid)
            self.get_edits(title, revid)

    def expand_templates_from_title(self, title):
        nsnum, suffix, fqname = self.nshandler.splitname(title)

        if nsnum == 0:
            text = u"{{:%s}}" % title
        else:
            text = u"{{%s}}" % title

        res = self.api.do_request(action="expandtemplates", title=title, text=text)
        txt = res.get("*")
        if txt:
            self.fsout.write_expanded_page(title, nsnum, txt)
            self.get_edits(title, None)

    def run(self):
        self.report()
        dispatch_gr = gevent.spawn(callwhen, self.dispatch_event, self.dispatch)
        try:
            self.pool.join()
        finally:
            dispatch_gr.kill()

        self.finish()

        assert not self.imageinfo_todo
        assert not self.revids_todo
        assert not self.pages_todo

    def extension_img_urls(self, data):
        html = data['text']['*']
        root = etree.HTML(html)

        img_urls = set()
        for img_node in root.xpath('.//img'):
            src = img_node.get('src')
            frags = src.split('/')
            if len(frags):
                fullurl = urlparse.urljoin(self.api.baseurl, src)
                if img_node.get('class') != 'thumbimage' and \
                       ('extensions' in src or 'math' in src):

                    img_urls.add(fullurl)
        return img_urls

    def fetch_html(self, name, lst):
        def fetch(c):
            with self.api_semaphore:
                kw = {name: c}
                res = self.api.do_request(action="parse", redirects="1", **kw)
                res[name] = c

            self.fsout.set_db_key('html', c, res)
            img_urls = self.extension_img_urls(res)
            for url in img_urls:
                fn = url.rsplit('/', 1)[1]
                title = self.nshandler.splitname(fn, defaultns=6)[2]
                self.schedule_download_image(str(url), title)

        self.count_total += len(lst)
        for c in lst:
            self._refcall_noinc(fetch, c)

    def fetch_used(self, name, lst, expanded=False):
        limit = self.api.api_request_limit
        pool = gevent.pool.Pool()
        blocks = splitblocks(lst, limit)
        self.count_total += len(blocks)
        for bl in blocks:
            pool.add(self._refcall_noinc(self.fetch_used_block, name, bl, expanded))
        pool.join()

        if conf.noedits:
            return

        items = self.title2latest.items()
        self.title2latest = {}
        self.count_total += len(items)
        for title, rev in items:
            self._refcall_noinc(self.get_edits, title, rev)

    def fetch_used_block(self, name, lst, expanded):
        kw = {name: lst, "fetch_images": self.fetch_images, "expanded": expanded}
        used = self.api.fetch_used(**kw)

        self._update_redirects(used.get("redirects", []))

        pages = used.get("pages", {}).values()

        revids = set()
        for p in pages:
            tmp = self._extract_attribute(p.get("revisions", []), "revid")
            if tmp:
                latest = max(tmp)
                title = p.get("title", None)
                old = self.title2latest.get(title, 0)
                self.title2latest[title] = max(old, latest)

            revids.update(tmp)

        templates = set()
        images = set()
        for p in pages:
            images.update(self._extract_title(p.get("images", [])))
            templates.update(self._extract_title(p.get("templates", [])))

        if self.cover_image:
            images.add(self.nshandler.get_fqname(self.cover_image, 6))
            self.cover_image = None

        for i in images:
            if i not in self.scheduled:
                self.imageinfo_todo.append(i)
                self.scheduled.add(i)

        for r in revids:
            if r not in self.scheduled:
                self.revids_todo.append(r)
                self.scheduled.add(r)

        for t in templates:
            if t not in self.scheduled:
                self.pages_todo.append(t)
                self.scheduled.add(t)


    def get_siteinfo_for(self, m):
        return m.get_siteinfo()

    def _split_titles_revids(self, pages):
        titles = set()
        revids = set()

        for p in pages:
            if p[1] is not None:
                revids.add(p[1])
            else:
                titles.add(p[0])

        titles = list(titles)
        titles.sort()

        revids = list(revids)
        revids.sort()
        return titles, revids

    def get_edits(self, title, rev):
        inspect_authors = self.api.get_edits(title, rev)
        authors = inspect_authors.get_authors()
        # print "GOT_EDITS:", title, authors
        self.fsout.set_db_key('authors', title, authors)

    def report(self):
        qc = self.api.qccount

        limit = self.api.api_request_limit
        jt = self.count_total + len(self.pages_todo) // limit + len(self.revids_todo) // limit
        jt += len(self.title2latest)

        self.progress.set_count(self, self.count_done + qc,  jt + qc)

    def _add_catmember(self, title, entry):
        try:
            self.cat2members[title].append(entry)
        except KeyError:
            self.cat2members[title] = [entry]

    def _handle_categories(self, data):
        pages = data.get("pages", {}).values()
        for p in pages:
            categories = p.get("categories")
            if not categories:
                continue

            e = dict(title=p.get("title"), ns=p.get("ns"), pageid=p.get("pageid"))

            for c in categories:
                cattitle = c.get("title")
                if cattitle:
                    self._add_catmember(cattitle, e)

    def _find_redirect(self,  data):
        pages = data.get("pages", {}).values()
        targets = []
        for p in pages:

            title = p.get("title")
            revisions = p.get("revisions")

            if revisions is None:
                continue

            for r in revisions:
                txt = r["*"]
                if not txt:
                    continue

                redirect = self.nshandler.redirect_matcher(txt)
                if redirect:
                    self.redirects[title] = redirect
                    targets.append(redirect)

        if targets:
            self.fetch_used("titles", targets)

    def _extract_attribute(self, lst, attr):
        res = []
        for x in lst:
            t = x.get(attr)
            if t:
                res.append(t)

        return res

    def _extract_title(self, lst):
        return self._extract_attribute(lst, "title")

    def _update_redirects(self, lst):
        for x in lst:
            t = x.get("to")
            f = x.get("from")
            if t and f:
                self.redirects[f] = t

    def schedule_download_image(self, url, title):
        key = (url, title)
        if key in self.scheduled:
            return
        self.scheduled.add(key)
        self._refcall(self._download_image, url, title)

    def _download_image(self, url, title):
        path = self.fsout.get_imagepath(title)
        temp_path = (path + u'\xb7').encode("utf-8")
        gr = self.image_download_pool.spawn(download_to_file, url, path, temp_path)
        self.pool.add(gr)

    def fetch_imageinfo(self, titles):
        data = self.api.fetch_imageinfo(titles=titles, iiurlwidth=self.imagesize)
        infos = data.get("pages", {}).values()
        # print infos[0]
        new_basepaths = set()

        for i in infos:
            title = i.get("title")

            ii = i.get("imageinfo", [])
            if not ii:
                continue
            ii = ii[0]
            self.fsout.set_db_key('imageinfo', title, ii)
            thumburl = ii.get("thumburl", None)

            if thumburl is None:  # fallback for old mediawikis
                thumburl = ii.get("url", None)

            # FIXME limit number of parallel downloads
            if thumburl:
                # FIXME: add Callback that checks correct file size
                if thumburl.startswith('/'):
                    thumburl = urlparse.urljoin(self.api.baseurl, thumburl)
                self.schedule_download_image(thumburl, title)

                descriptionurl = ii.get("descriptionurl", "")
                if not descriptionurl:
                    descriptionurl = i.get('fullurl', '')

                if descriptionurl and "/" in descriptionurl:
                    path, localname = descriptionurl.rsplit("/", 1)
                    t = (title, descriptionurl)
                    if path in self.imagedescription_todo:
                        self.imagedescription_todo[path].append(t)
                    else:
                        new_basepaths.add(path)
                        self.imagedescription_todo[path] = [t]

        for path in new_basepaths:
            self._refcall(self.handle_new_basepath, path)

    def _get_nshandler(self):
        if self._nshandler is not None:
            return self._nshandler
        return nshandling.get_nshandler_for_lang('en')  # FIXME

    def _set_nshandler(self, nshandler):
        self._nshandler = nshandler

    nshandler = property(_get_nshandler, _set_nshandler)

    def fetch_image_page(self, titles, api):
        data = api.fetch_pages(titles)

        local_nsname = self.nshandler.get_nsname_by_number(6)

        pages = data.get("pages", {}).values()
        # change title prefix to make them look like local pages
        for p in pages:
            title = p.get("title")
            prefix, partial = title.split(":", 1)
            p["title"] = "%s:%s" % (local_nsname, partial)

            revisions = p.get("revisions", [])
            # the revision id's could clash with some local ids. remove them.
            for r in revisions:
                try:
                    del r["revid"]
                except KeyError:
                    pass

        # XXX do we also need to handle redirects here?
        self.fsout.write_pages(data)

    def handle_new_basepath(self, path):
        api = self._get_mwapi_for_path(path)
        todo = self.imagedescription_todo[path]
        del self.imagedescription_todo[path]

        titles = set([x[0] for x in todo])
        # "-d-" is just some prefix to make the names here not clash with local names
        titles = [t for t in titles if "-d-" + t not in self.scheduled]
        self.scheduled.update(["-d-" + x for x in titles])
        if not titles:
            return

        siteinfo = self.get_siteinfo_for(api)

        ns = nshandling.nshandler(siteinfo)
        nsname = ns.get_nsname_by_number(6)

        local_names = []
        for x in titles:
            partial = x.split(":", 1)[1]
            local_names.append("%s:%s" % (nsname, partial))

        for bl in splitblocks(local_names, api.api_request_limit):
            self._refcall(self.fetch_image_page, bl, api)

        for title in local_names:
            self._refcall(self.get_image_edits, title, api)

    def get_image_edits(self, title, api):
        get_authors = api.get_edits(title, None)
        local_nsname = self.nshandler.get_nsname_by_number(6)
        # change title prefix to make them look like local pages
        prefix, partial = title.split(":", 1)
        title = '%s:%s' % (local_nsname, partial)
        authors = get_authors.get_authors()
        self.fsout.set_db_key('authors', title, authors)

    def _get_mwapi_for_path(self, path):
        urls = mwapi.guess_api_urls(path)
        for url in urls:
            if url in self.api_cache:
                return self.api_cache[url]
        for url in urls:
            try:
                api = mwapi.mwapi(url)
                api.ping()
                api.set_limit()
                self.api_cache[url] = api
                return api
            except Exception:
                # traceback.print_exc()
                continue

        raise RuntimeError("cannot guess api url for %r" % (path,))

    def dispatch(self):
        limit = self.api.api_request_limit

        def fetch_pages(**kw):
            data = self.api.fetch_pages(**kw)
            self._find_redirect(data)
            r = data.get("redirects", [])
            self._update_redirects(r)
            self._handle_categories(data)
            self.fsout.write_pages(data)

        def doit(name, lst):
            while lst and self.api.idle():
                bl = getblock(lst, limit)
                self.scheduled.update(bl)
                kw = {name: bl}
                self._refcall(fetch_pages, **kw)

        while self.imageinfo_todo and self.api.idle():
            bl = getblock(self.imageinfo_todo, limit)
            self.scheduled.update(bl)
            self._refcall(self.fetch_imageinfo, bl)

        doit("revids", self.revids_todo)
        doit("titles", self.pages_todo)

        self.report()


    def _sanity_check(self):
        seen = self.fsout.seen
        for title, revid in self.pages:
            if revid is not None:
                if revid in seen:
                    continue

            n = self.nshandler.get_fqname(title)
            if n in seen or self.redirects.get(n, n) in seen:
                continue
            print "WARNING: %r could not be fetched" % ((title, revid),)
            # raise RuntimeError("%r could not be fetched" % ((title, revid), ))

        seen = self.fsout.seen

    def finish(self):
        self._sanity_check()
        self.fsout.write_redirects(self.redirects)
        self.fsout.write_licenses(self.licenses)
        self.fsout.close()

    def _refcall(self, fun, *args, **kw):
        """Increment refcount, schedule call of fun
        decrement refcount after fun has finished.
        """
        self.count_total += 1
        self.report()
        return self._refcall_noinc(fun, *args, **kw)

    def _refcall_noinc(self, fun, *args, **kw):
        def refcall_fun():
            try:
                fun(*args, **kw)
            finally:
                self.count_done += 1
                self.dispatch_event.set()

        gr = self.refcall_pool.spawn(refcall_fun)
        self.pool.add(gr)
        return gr


def pages_from_metabook(mb):
    articles = mb.articles()
    pages = [(x.title, x.revision) for x in articles]
    return pages

########NEW FILE########
__FILENAME__ = sapi
#! /usr/bin/env python

# Copyright (c) PediaPress GmbH
# See README.rst for additional licensing information.

"""api.php client"""

import urllib, urllib2, urlparse, cookielib, re

try:
    import simplejson as json
except ImportError:
    import json

from mwlib import conf, authors


def loads(s):
    """Potentially remove UTF-8 BOM and call json.loads()"""

    if s and isinstance(s, str) and s[:3] == '\xef\xbb\xbf':
        s = s[3:]
    return json.loads(s)


def merge_data(dst, src):
    todo = [(dst, src)]
    while todo:
        dst, src = todo.pop()
        assert type(dst) == type(src), "cannot merge %r with %r" % (type(dst), type(src))

        if isinstance(dst, list):
            dst.extend(src)
        elif isinstance(dst, dict):
            for k, v in src.items():
                if k in dst:
                    todo.append((dst[k], v))
                else:
                    dst[k] = v


class mwapi(object):
    def __init__(self, apiurl, username=None, password=None):
        self.apiurl = apiurl
        self.baseurl = apiurl  # XXX

        if username:
            passman = urllib2.HTTPPasswordMgrWithDefaultRealm()
            passman.add_password(None, apiurl, username, password)
            auth_handler = urllib2.HTTPBasicAuthHandler(passman)
            self.opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookielib.CookieJar()), auth_handler)
        else:
            self.opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cookielib.CookieJar()))
        self.opener.addheaders = [('User-Agent', conf.user_agent)]
        self.edittoken = None
        self.qccount = 0
        self.api_result_limit = conf.get("fetch", "api_result_limit", 500, int)
        self.api_request_limit = conf.get("fetch", "api_request_limit", 15, int)
        self.max_connections = conf.get("fetch", "max_connections", 20, int)
        self.max_retry_count = conf.get("fetch", "max_retry_count", 2, int)
        self.rvlimit = conf.get("fetch", "rvlimit", 500, int)
        self.limit_fetch_semaphore = None

    def report(self):
        pass

    def set_limit(self, limit=None):
        assert self.limit_fetch_semaphore is None, "limit already set"

        if limit is None:
            limit = self.api_request_limit

        from gevent import coros
        self.limit_fetch_semaphore = coros.Semaphore(limit)

    def __repr__(self):
        return "<mwapi %s at %s>" % (self.apiurl, hex(id(self)))

    def _fetch(self, url):
        f = self.opener.open(url)
        data = f.read()
        f.close()
        return data

    def _build_url(self, **kwargs):
        args = {'format': 'json'}
        args.update(**kwargs)
        for k, v in args.items():
            if isinstance(v, unicode):
                args[k] = v.encode('utf-8')
        q = urllib.urlencode(args)
        q = q.replace('%3A', ':')  # fix for wrong quoting of url for images
        q = q.replace('%7C', '|')  # fix for wrong quoting of API queries (relevant for redirects)

        url = "%s?%s" % (self.apiurl, q)
        return url

    def _request(self, **kwargs):
        url = self._build_url(**kwargs)
        return self._fetch(url)

    def _post(self, **kwargs):
        args = {'format': 'json'}
        args.update(**kwargs)
        for k, v in args.items():
            if isinstance(v, unicode):
                args[k] = v.encode('utf-8')

        headers = {"Content-Type": "application/x-www-form-urlencoded"}
        postdata = urllib.urlencode(args)

        req = urllib2.Request(self.apiurl, postdata, headers)

        res = loads(self._fetch(req))
        return res

    def do_request(self, use_post=False, **kwargs):
        sem = self.limit_fetch_semaphore
        if sem is not None:
            sem.acquire()

        try:
            if use_post:
                return  self._post(**kwargs)
            else:
                return self._do_request(**kwargs)
        finally:
            if sem is not None:
                sem.release()

    def _do_request(self, query_continue=True, merge_data=merge_data, **kwargs):
        last_qc = None
        action = kwargs["action"]
        retval = {}
        todo = kwargs
        while todo is not None:
            kwargs = todo
            todo = None

            data = loads(self._request(**kwargs))
            error = data.get("error")
            if error:
                raise RuntimeError("%s: [fetching %s]" % (error.get("info", ""), self._build_url(**kwargs)))
            merge_data(retval, data[action])

            qc = data.get("query-continue", {}).values()

            if qc and query_continue:
                self.qccount += 1
                self.report()
                kw = kwargs.copy()
                for d in qc:
                    for k, v in d.items():  # dict of len(1)
                        kw[str(k)] = v

                if qc == last_qc:
                    print "warning: cannot continue this query:",  self._build_url(**kw)
                    return retval

                last_qc = qc
                todo = kw

        return retval

    def ping(self):
        return self.do_request(action="query", meta="siteinfo",  siprop="general")

    def get_categorymembers(self, cmtitle):
        return self.do_request(action="query", list="categorymembers", cmtitle=cmtitle,  cmlimit=200)

    def get_siteinfo(self):
        siprop = "general namespaces interwikimap namespacealiases magicwords rightsinfo".split()
        while len(siprop) >= 3:
            try:
                r = self.do_request(action="query", meta="siteinfo", siprop="|".join(siprop))
                return r
            except Exception, err:
                print "ERR:", err
                siprop.pop()
        raise RuntimeError("could not get siteinfo")

    def login(self, username, password, domain=None, lgtoken=None):
        args = dict(action="login",
                    lgname=username.encode("utf-8"),
                    lgpassword=password.encode("utf-8"),
                    format="json")

        if domain is not None:
            args['lgdomain'] = domain.encode('utf-8')

        if lgtoken is not None:
            args["lgtoken"] = lgtoken.encode("utf-8")

        res = self._post(**args)

        login_result = res["login"]["result"]
        if login_result == "NeedToken" and lgtoken is None:
            return self.login(username, password, domain=domain, lgtoken=res["login"]["token"])
        elif login_result == "Success":
            return

        raise RuntimeError("login failed: %r" % res)

    def fetch_used(self, titles=None, revids=None, fetch_images=True, expanded=False):
        if fetch_images:
            if expanded:
                prop = "images"
            else:
                prop = "revisions|templates|images"
        else:
            if expanded:
                prop = ""
            else:
                prop = "revisions|templates"

        kwargs = dict(prop=prop,
                      rvprop='ids',
                      imlimit=self.api_result_limit,
                      tllimit=self.api_result_limit)
        if titles:
            kwargs['redirects'] = 1

        self._update_kwargs(kwargs, titles, revids)
        return self.do_request(action="query", **kwargs)

    def _update_kwargs(self, kwargs, titles, revids):
        assert titles or revids and not (titles and revids), 'either titles or revids must be set'

        if titles:
            kwargs["titles"] = "|".join(titles)
        if revids:
            kwargs["revids"] = "|".join([str(x) for x in revids])

    def upload(self, title, txt, summary):
        if self.edittoken is None:
            res = self.do_request(action="query", prop="info|revisions",  intoken="edit",  titles=title)
            self.edittoken = res["pages"].values()[0]["edittoken"]

        self._post(action="edit", title=title, text=txt, token=self.edittoken, summary=summary,  format="json", bot=True)

    def idle(self):
        sem = self.limit_fetch_semaphore
        if sem is None:
            return True
        return not sem.locked()

    def fetch_pages(self, titles=None, revids=None):
        kwargs = dict(prop="revisions",
                      rvprop='ids|content|timestamp|user',
                      imlimit=self.api_result_limit,
                      tllimit=self.api_result_limit)
        if titles:
            kwargs['redirects'] = 1

        self._update_kwargs(kwargs, titles, revids)

        rev_result = self.do_request(action="query", **kwargs)

        kwargs = dict(prop="categories",
                      cllimit=self.api_result_limit)
        if titles:
            kwargs['redirects'] = 1

        self._update_kwargs(kwargs, titles, revids)
        cat_result = self.do_request(action="query", **kwargs)
        merge_data(rev_result, cat_result)
        return rev_result

    def fetch_imageinfo(self, titles, iiurlwidth=800):
        kwargs = dict(prop="imageinfo|info",
                      iiprop="url|user|comment|url|sha1|size",
                      iiurlwidth=iiurlwidth,
                      inprop='url'
                      )

        self._update_kwargs(kwargs, titles, [])
        return self.do_request(action="query", **kwargs)

    def get_edits(self, title, revision, rvlimit=None):
        rvlimit = rvlimit or self.rvlimit
        kwargs = {
            'titles': title,
            'redirects': 1,
            'prop': 'revisions',
            'rvprop': 'ids|user|flags|comment|size',
            'rvlimit': rvlimit,
            'rvdir': 'older',
        }
        if revision is not None:
            kwargs['rvstartid'] = revision

        # def setrvlimit(res):
        #     print "setting rvlimit to 50 for %s" % (self.baseurl, )
        #     self.rvlimit=50
        #     return res

        # # XXX
        # def retry(err):
        #     if rvlimit <= 50:
        #         return err

        #     kwargs["rvlimit"] = 50

        #     return self.do_request(action="query", **kwargs).addCallback(setrvlimit)

        get_authors = authors.inspect_authors()

        def merge_data(retval, newdata):
            edits = newdata["pages"].values()
            for e in edits:
                revs = e["revisions"]
                get_authors.scan_edits(revs)

        self.do_request(action="query", merge_data=merge_data, **kwargs)
        return get_authors


def guess_api_urls(url):
    """
    @param url: URL of a MediaWiki article
    @type url: str

    @returns: list of possible api.php urls
    @rtype: list
    """
    retval = []
    if isinstance(url, unicode):
        url = url.encode("utf-8")

    try:
        scheme, netloc, path, params, query, fragment = urlparse.urlparse(url)
    except ValueError:
        return retval

    if not (scheme and netloc):
        return retval

    path_prefix = ''
    if '/wiki/' in path:
        path_prefix = path[:path.find('/wiki/')]
    elif '/w/' in path:
        path_prefix = path[:path.find('/w/')]

    prefix = '%s://%s%s' % (scheme, netloc, path_prefix)

    for _path in (path + "/", '/w/', '/wiki/', '/'):
        base_url = '%s%sapi.php' % (prefix, _path)
        if base_url not in retval:
            retval.append(base_url)
    if url.endswith('/index.php'):
        retval.append(url[:-len('index.php')] + 'api.php')
    return retval


def get_collection_params(api):
    return dict()


def main():
    s = mwapi("http://en.wikipedia.org/w/api.php")
    print s.get_categorymembers("Category:Mainz")

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = nserve
#! /usr/bin/env python

"""WSGI server interface to mw-render and mw-zip/mw-post"""

from __future__ import with_statement

import gevent.monkey
if __name__ == "__main__":
    gevent.monkey.patch_all()

import sys, re, StringIO, urllib2, urlparse, traceback, urllib, unicodedata
from hashlib import md5

from gevent import pool, pywsgi

from qs.misc import call_in_loop
from mwlib import myjson as json, log, _version
from mwlib.metabook import calc_checksum
from mwlib.async import rpcclient

log = log.Log('mwlib.serve')


class bunch(object):
    def __init__(self, **kw):
        self.__dict__.update(kw)

    def __repr__(self):
        return "bunch(%s)" % (", ".join(["%s=%r" % (k, v) for k, v in self.__dict__.items()]), )


# -- we try to load all writers here but also keep a list of known writers
# -- these writers do not have to be installed on the machine that's running the server
# -- and we also like to speedup the get_writers method

name2writer = {'odf': bunch(file_extension='odt', name='odf', content_type='application/vnd.oasis.opendocument.text'),
               'rl': bunch(file_extension='pdf', name='rl', content_type='application/pdf'),
               'xhtml': bunch(file_extension='html', name='xhtml', content_type='text/xml'),
               'xl': bunch(file_extension='pdf', name='xl', content_type='application/pdf'),
               'zim': bunch(file_extension='zim', name='zim', content_type='application/zim')}


def get_writers(name2writer):
    import pkg_resources

    for entry_point in pkg_resources.iter_entry_points('mwlib.writers'):
        if entry_point.name in name2writer:
            continue

        try:
            writer = entry_point.load()
            name2writer[entry_point.name] = bunch(name=entry_point.name,
                                                  file_extension=writer.file_extension,
                                                  content_type=writer.content_type)
        except Exception:
            continue

    return name2writer

get_writers(name2writer)

collection_id_rex = re.compile(r'^[a-f0-9]{16}$')


def make_collection_id(data):
    sio = StringIO.StringIO()
    sio.write(str(_version.version))
    for key in (
        'base_url',
        'script_extension',
        'login_credentials',
    ):
        sio.write(repr(data.get(key)))
    mb = data.get('metabook')
    if mb:
        if isinstance(mb, str):
            mb = unicode(mb, 'utf-8')
        mbobj = json.loads(mb)
        sio.write(calc_checksum(mbobj))
        num_articles = len(list(mbobj.articles()))
        sys.stdout.write("new-collection %s\t%r\t%r\n" % (num_articles, data.get("base_url"), data.get("writer")))

    return md5(sio.getvalue()).hexdigest()[:16]

from mwlib import lrucache
busy = dict()
collid2qserve = lrucache.lrucache(4000)


class watch_qserve(object):
    getstats_timeout = 3.0
    sleeptime = 2.0

    def __init__(self, (host, port), busy):
        self.host = host
        self.port = port
        self.busy = busy
        self.ident = (host, port)
        self.prefix = "watch: %s:%s:" % (host, port)
        self.qserve = None

    def log(self, msg):
        print self.prefix, msg

    def _serverproxy(self):
        return rpcclient.serverproxy(host=self.host, port=self.port)

    def _mark_busy(self, is_busy):
        if is_busy and busy[self.ident] != is_busy:
            self.log(is_busy)

        if not is_busy and busy[self.ident]:
            self.log("resuming operation")

        self.busy[self.ident] = is_busy

    def _sleep(self):
        gevent.sleep(self.sleeptime)

    def _getstats(self):
        if self.qserve is None:
            self.qserve = self._serverproxy()
        try:
            with gevent.Timeout(self.getstats_timeout):
                return self.qserve.getstats()
        except gevent.Timeout:
            self.qserve = None
            raise RuntimeError("timeout calling getstats")
        except BaseException:
            self.qserve = None
            raise

    def _iterate(self):
        try:
            stats = self._getstats()
            numrender = stats.get("busy",  {}).get("render", 0)
            if numrender > 10:
                self._mark_busy("system overloaded")
            else:
                self._mark_busy(False)
        except gevent.GreenletExit:
            raise
        except Exception, err:
            self._mark_busy("system down")
            self.log("error in watch_qserve: %s" % (err,))

    def __call__(self):
        self.busy[self.ident] = True
        while 1:
            try:
                self._iterate()
            except gevent.GreenletExit:
                raise
            self._sleep()


def choose_idle_qserve():
    import random
    idle = [k for k, v in busy.items() if not v]
    if not idle:
        return None
    return random.choice(idle)  # XXX probably store number of render jobs in busy


from bottle import request, default_app, post, get, HTTPResponse


@get('<path:re:.*>')
@post('<path:re:.*>')
def dispatch_command(path):
    return Application().dispatch(request)


def get_content_disposition_values(filename, ext):
    if isinstance(filename, str):
        filename = unicode(filename)

    if filename:
        filename = filename.strip()

    if not filename:
        filename = u"collection"

    # see http://code.activestate.com/recipes/251871-latin1-to-ascii-the-unicode-hammer/
    asciifn = unicodedata.normalize("NFKD", filename).encode("ASCII", "ignore")
    asciifn = re.sub("[ ;:\"',]+", " ", asciifn).strip() or "collection"
    asciifn = asciifn.replace(" ", "-")

    return (asciifn, filename.encode("utf-8"))


def get_content_disposition(filename, ext):
    asciifn, utf8fn = get_content_disposition_values(filename, ext)

    r = "inline; filename=%s.%s" % (asciifn, ext)
    if utf8fn and utf8fn != asciifn:
        r += ";filename*=UTF-8''%s.%s" % (urllib.quote(utf8fn), ext)
    return r


class Application(object):
    def __init__(self, default_writer='rl'):
        self.default_writer = default_writer

    def dispatch(self, request):
        try:
            command = request.params['command']
        except KeyError:
            log.error("no command given")
            raise HTTPResponse("no command given", status=400)

        try:
            method = getattr(self, 'do_%s' % command)
        except AttributeError:
            log.error("no such command: %r" % (command, ))
            raise HTTPResponse("no such command: %r" % (command, ), status=400)

        collection_id = request.params.get('collection_id')
        if not collection_id:
            collection_id = self.new_collection(request.params)
            is_new = True
        else:
            is_new = False
            if not self.check_collection_id(collection_id):
                raise HTTPResponse(status=404)

        try:
            qserve = collid2qserve[collection_id]
        except KeyError:
            qserve = choose_idle_qserve()
            if qserve is None:
                return self.error_response("system overloaded. please try again later.", queue_full=1)
            collid2qserve[collection_id] = qserve

        self.qserve = rpcclient.serverproxy(host=qserve[0], port=qserve[1])

        try:
            return method(collection_id, request.params, is_new)
        except Exception, exc:
            print "ERROR while dispatching %r: %s" % (command, dict(collection_id=collection_id, is_new=is_new, qserve=qserve))
            traceback.print_exc()
            if command == "download":
                raise exc

            return self.error_response('error executing command %r: %s' % (
                    command, exc,))

    def error_response(self, error, **kw):
        if isinstance(error, str):
            error = unicode(error, 'utf-8', 'ignore')
        elif not isinstance(error, unicode):
            error = unicode(repr(error), 'ascii')
        return dict(error=error, **kw)

    def check_collection_id(self, collection_id):
        """Return True iff collection with given ID exists"""

        if not collection_id or not collection_id_rex.match(collection_id):
            return False
        return True

    def new_collection(self, post_data):
        collection_id = make_collection_id(post_data)
        return collection_id

    def is_good_baseurl(self, url):
        netloc = urlparse.urlparse(url)[1].split(':')[0].lower()
        if netloc == "localhost" or netloc.startswith("127.0.") or netloc.startswith("192.168."):
            return False
        return True

    def _get_params(self, post_data,  collection_id):
        g = post_data.get
        params = bunch()
        params.__dict__ = dict(
            metabook_data=g('metabook'),
            writer=g('writer', self.default_writer),
            base_url=g('base_url'),
            writer_options=g('writer_options', ''),
            login_credentials=g('login_credentials', ''),
            force_render=bool(g('force_render')),
            script_extension=g('script_extension', ''),
            pod_api_url=post_data.get('pod_api_url', ''),
            language=g('language', ''))

        params.collection_id = collection_id

        return params

    def do_render(self, collection_id, post_data, is_new=False):
        params = self._get_params(post_data,  collection_id=collection_id)
        metabook_data = params.metabook_data
        base_url = params.base_url
        writer = params.writer

        if writer not in name2writer:
            return self.error_response("unknown writer %r" % writer)

        if is_new and not metabook_data:
            return self.error_response('POST argument metabook or collection_id required')
        if not is_new and metabook_data:
            return self.error_response('Specify either metabook or collection_id, not both')

        if base_url and not self.is_good_baseurl(base_url):
            log.bad("bad base_url: %r" % (base_url, ))
            return self.error_response("bad base_url %r. check your $wgServer and $wgScriptPath variables. localhost, 192.168.*.* and 127.0.*.* are not allowed." % (base_url, ))

        log.info('render %s %s' % (collection_id, writer))

        response = {
            'collection_id': collection_id,
            'writer': writer,
            'is_cached': False,
        }

        self.qserve.qadd(channel="makezip", payload=dict(params=params.__dict__), jobid="%s:makezip" % (collection_id, ), timeout=20 * 60)

        self.qserve.qadd(channel="render", payload=dict(params=params.__dict__),
                         jobid="%s:render-%s" % (collection_id, writer),  timeout=20 * 60)

        return response

    def do_render_status(self, collection_id, post_data, is_new=False):
        if is_new:
            return self.error_response('POST argument required: collection_id')

        def retval(**kw):
            return dict(collection_id=collection_id, writer=writer, **kw)

        writer = post_data.get('writer', self.default_writer)
        w = name2writer[writer]

        jobid = "%s:render-%s" % (collection_id, writer)

        res = self.qserve.qinfo(jobid=jobid) or {}
        info = res.get("info", {})
        done = res.get("done", False)
        error = res.get("error", None)

        if error:
            return retval(state="failed", error=error)

        if done:
            more = dict()

            try:
                if res["result"]:
                    more["url"] = res["result"]["url"]
                    more["content_length"] = res["result"]["size"]
                    more["suggested_filename"] = res["result"].get("suggested_filename", "")
            except KeyError:
                pass

            if w.content_type:
                more["content_type"] = w.content_type

            if w.file_extension:
                more["content_disposition"] = get_content_disposition(more.get("suggested_filename", None),
                                                                      w.file_extension)

            return retval(state="finished", **more)

        if not info:
            jobid = "%s:makezip" % (collection_id,)
            res = self.qserve.qinfo(jobid=jobid) or {}

            done = res.get("done", False)
            if not done:
                info = res.get("info", {})
            else:
                info = dict(status="data fetched. waiting for render process..")

        return retval(state="progress", status=info)

    def do_download(self, collection_id, post_data, is_new=False):
        if is_new:
            return self.error_response('POST argument required: collection_id')

        writer = post_data.get('writer', self.default_writer)
        w = name2writer[writer]

        jobid = "%s:render-%s" % (collection_id, writer)
        res = self.qserve.qinfo(jobid=jobid) or {}
        download_url = res["result"]["url"]

        print "fetching", download_url
        f = urllib2.urlopen(download_url)
        info = f.info()

        header = {}

        for h in ("Content-Length",):
            v = info.getheader(h)
            if v:
                print "copy header:", h, v
                header[h] = v

        if w.content_type:
            header["Content-Type"] = w.content_type

        if w.file_extension:
            header['Content-Disposition'] = 'inline; filename=collection.%s' % (w.file_extension.encode('utf-8', 'ignore'))

        def readdata():
            while 1:
                d = f.read(4096)
                if not d:
                    break
                yield d

        return HTTPResponse(output=readdata(), header=header)

    def do_zip_post(self, collection_id, post_data, is_new=False):
        params = self._get_params(post_data, collection_id=collection_id)

        try:
            post_data['metabook']
        except KeyError, exc:
            return self.error_response('POST argument required: %s' % exc)

        pod_api_url = params.pod_api_url
        if pod_api_url:
            result = json.loads(unicode(urllib2.urlopen(pod_api_url, data="any").read(), 'utf-8'))
            post_url = result['post_url'].encode('utf-8')
            response = {
                'state': 'ok',
                'redirect_url': result['redirect_url'].encode('utf-8'),
            }
        else:
            try:
                post_url = post_data['post_url']
            except KeyError:
                return self.error_response('POST argument required: post_url')
            response = {'state': 'ok'}

        log.info('zip_post %s %s' % (collection_id, pod_api_url))
        params.post_url = post_url

        self.qserve.qadd(channel="post",  # jobid="%s:post" % collection_id,
                         payload=dict(params=params.__dict__),
                         timeout=20 * 60)
        return response


def _parse_qs(qs):
    for i, x in enumerate(qs):
        if ":" in x:
            host, port = x.split(":", 1)
            port = int(port)
            qs[i] = (host, port)
        else:
            qs[i] = (x, 14311)


def main():
    # pywsgi.WSGIHandler.log_request = lambda *args, **kwargs: None

    from mwlib import argv
    opts,  args = argv.parse(sys.argv[1:], "--disable-all-writers --qserve= --port= -i= --interface=")
    qs = []
    port = 8899
    interface = "0.0.0.0"
    for o, a in opts:
        if o == "--port":
            port = int(a)
        elif o == "--qserve":
            qs.append(a)
        elif o == "--disable-all-writers":
            name2writer.clear()
        elif o in ("-i", "--interface"):
            interface = a

    print "using the following writers", sorted(name2writer.keys())

    qs += args

    if not qs:
        qs.append("localhost:14311")

    _parse_qs(qs)

    address = interface, port
    server = pywsgi.WSGIServer(address, default_app())

    watchers = pool.Pool()
    for x in qs:
        watchers.spawn(call_in_loop(5.0, watch_qserve(x, busy)))

    try:
        print "listening on %s:%d" % address
        server.serve_forever()
    except KeyboardInterrupt:
        server.stop()
        print "bye."

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = nshandling

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

"""
namespace handling based on data extracted from the siteinfo as
returned by api.php
"""

import re


NS_MEDIA          = -2
NS_SPECIAL        = -1
NS_MAIN           =  0
NS_TALK           =  1
NS_USER           =  2
NS_USER_TALK      =  3
NS_PROJECT        =  4
NS_PROJECT_TALK   =  5
NS_FILE           =  6
NS_IMAGE          =  6
NS_FILE_TALK      =  7
NS_IMAGE_TALK     =  7
NS_MEDIAWIKI      =  8
NS_MEDIAWIKI_TALK =  9
NS_TEMPLATE       = 10
NS_TEMPLATE_TALK  = 11
NS_HELP           = 12
NS_HELP_TALK      = 13
NS_CATEGORY       = 14
NS_CATEGORY_TALK  = 15

class ilink(object):
    url = ""
    prefix = ""
    local = ""
    language = ""

def fix_wikipedia_siteinfo(siteinfo):

    # --- http://code.pediapress.com/wiki/ticket/754

    if u'\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd\ufffd' in [x.get("prefix", u"")[2:] for x in siteinfo.get("interwikimap", [])]:
        print "WARNING: interwikimap contains garbage"
        from mwlib import siteinfo as simod
        en = simod.get_siteinfo("en")
        siteinfo['interwikimap'] = list(en["interwikimap"])


    prefixes = [x['prefix'] for x in siteinfo['interwikimap']]
    for p in "pnb ckb mwl mhr ace krc pcd frr koi gag bjn pfl mrj bjn rue kbd ltg xmf".split():
        
        if p in prefixes:
            return
        siteinfo['interwikimap'].append({
            'prefix': p,
            'language': p,
            'url': 'http://%s.wikipedia.org/wiki/$1' % (p, ),
            'local': '',
        })
    
# TODO: build fast lookup table for use in nshandler.splitname
class nshandler(object):
    def __init__(self, siteinfo):
        assert siteinfo is not None

        if 'general' in siteinfo and siteinfo['general'].get('server', '').endswith(".wikipedia.org") and 'interwikimap' in siteinfo:
            fix_wikipedia_siteinfo(siteinfo)

        self.siteinfo = siteinfo
        try:
            self.capitalize = self.siteinfo['general'].get('case') == 'first-letter'
        except KeyError:
            self.capitalize = True

        p = self.prefix2interwiki = {}
        for k in siteinfo.get("interwikimap", []):
            p[k["prefix"]] = k

        self.set_redirect_matcher(siteinfo)

    def set_redirect_matcher(self, siteinfo):
        self.redirect_matcher = get_redirect_matcher(siteinfo, self)

    def __getstate__(self):
        d=self.__dict__.copy()
        del d['redirect_matcher']
        return d

    def __setstate__(self, d):
        self.__dict__ = d
        self.set_redirect_matcher(self.siteinfo)

    # workaround for a copy.deepcopy bug in python 2.4
    # should be save to return the instance itself without copying
    # since it's basically immutable.
    def __deepcopy__(self, memo):
        return self
    
    def _find_namespace(self, name, defaultns=0):
        name = name.lower().strip()
        namespaces = self.siteinfo["namespaces"].values()
        for ns in namespaces:
            star = ns["*"]
            if star.lower()==name or ns.get("canonical", u"").lower()==name:
                return True, ns["id"], star
            

        aliases = self.siteinfo.get("namespacealiases", [])
        for a in aliases:
            if a["*"].lower()==name:
                nsid = a["id"]
                return True, nsid, self.siteinfo["namespaces"][str(nsid)]["*"]
                
        return False, defaultns, self.siteinfo["namespaces"][str(defaultns)]["*"]

    def get_fqname(self, title, defaultns=0):
        return self.splitname(title, defaultns=defaultns)[2]

    def maybe_capitalize(self, t):
        if self.capitalize:
            return t[0:1].upper() + t[1:]
        return t
    
    def splitname(self, title, defaultns=0):
        if not isinstance(title, unicode):
            title = unicode(title, 'utf-8')

        # if "#" in title:
        #     title = title.split("#")[0]
            
        name = re.sub(r' +', ' ', title.replace("_", " ").strip())
        if name.startswith(":"):
            name = name[1:].strip()
            defaultns = 0
            
        if ":" in name:
            ns, partial_name = name.split(":", 1)
            was_namespace, nsnum, prefix = self._find_namespace(ns, defaultns=defaultns)
            if was_namespace:
                suffix = partial_name.strip()
            else:
                suffix = name
        else:
            prefix = self.siteinfo["namespaces"][str(defaultns)]["*"]
            suffix = name
            nsnum = defaultns

        suffix=suffix.strip(u"\u200e\u200f")
        suffix=self.maybe_capitalize(suffix)
        if prefix:
            prefix += ":"
            
        return (nsnum, suffix, "%s%s" % (prefix,  suffix))

    def get_nsname_by_number(self, ns):
        return self.siteinfo["namespaces"][str(ns)]["*"]
        
    def resolve_interwiki(self, title):
        name = title.replace("_", " ").strip()
        if name.startswith(":"):
            name = name[1:].strip()
        if ":" not in name:
            return None
        prefix, suffix = name.split(":", 1)
        prefix = prefix.strip().lower()
        d = self.prefix2interwiki.get(prefix)
        if d is None:
            return None
        
        suffix = suffix.strip(" _\n\t\r").replace(" ", "_")
        retval = ilink()
        retval.__dict__.update(d)
        retval.url = retval.url.replace("$1", suffix)
        retval.partial = suffix
        return retval
        
def get_nshandler_for_lang(lang):
    if lang is None:
        lang = "de" # FIXME: we currently need this to make the tests happy
        
    # assert lang is not None, "expected some language"
    from mwlib import siteinfo
    si = siteinfo.get_siteinfo(lang)
    if si is None:
        si = siteinfo.get_siteinfo("en")
        assert si, "siteinfo-en not found"
    return nshandler(si)

def get_redirect_matcher(siteinfo, handler=None):
    redirect_str = "#REDIRECT"
    magicwords = siteinfo.get("magicwords")
    if magicwords:
        for m in magicwords:            
            if m['name'] == 'redirect':
                redirect_str = "(?:" + "|".join([re.escape(x) for x in m['aliases']]) + ")"
    redirect_rex = re.compile(r'^%s:?\s*?\[\[(?P<redirect>.*?)\]\]' % redirect_str, re.IGNORECASE)

    if handler is None:
        handler =  nshandler(siteinfo)
    
    def redirect(text):
        mo = redirect_rex.search(text)
        if mo:
            name = mo.group('redirect').split("|", 1)[0]
            name = name.split("#")[0]
            return handler.get_fqname(name)
        return None
    
    return redirect

########NEW FILE########
__FILENAME__ = nslave
#! /usr/bin/env python

if __name__ == "__main__":
    from gevent import monkey
    monkey.patch_all()

import os, sys, time, socket

cachedir = None
cacheurl = None

from mwlib.async import proc
from mwlib.utils import garble_password


# -- find_ip is copied from woof sources
# Utility function to guess the IP (as a string) where the server can
# be reached from the outside. Quite nasty problem actually.

def find_ip():
   # we get a UDP-socket for the TEST-networks reserved by IANA.  It
   # is highly unlikely, that there is special routing used for these
   # networks, hence the socket later should give us the ip address of
   # the default route.  We're doing multiple tests, to guard against
   # the computer being part of a test installation.

    candidates = []
    for test_ip in ["192.0.2.0", "198.51.100.0", "203.0.113.0"]:
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect((test_ip, 80))
        ip_addr = s.getsockname()[0]
        s.close()
        if ip_addr in candidates:
            return ip_addr
        candidates.append(ip_addr)

    return candidates[0]


def get_collection_dir(collection_id):
    return os.path.join(cachedir, collection_id[:2], collection_id)


def system(args, timeout=None):
    stime = time.time()

    retcode, stdout = proc.run_cmd(args, timeout=timeout)

    d = time.time() - stime

    pub_args = garble_password(args)
    msg = []
    a = msg.append
    a("%s %s %r\n" % (retcode, d, pub_args))

    writemsg = lambda: sys.stderr.write("".join(msg))

    if retcode != 0:
        a(stdout)
        a("\n====================\n")

        writemsg()
        lines = ["    " + x for x in stdout[-4096:].split("\n")]
        raise RuntimeError("command failed with returncode %s: %r\nLast Output:\n%s" % (retcode, pub_args,  "\n".join(lines)))

    writemsg()


def _get_args(writer_options=None,
              language=None,
              zip_only=False,
              login_credentials=None,
              **kw):

    args = []

    if login_credentials:
        username, password, domain = (login_credentials.split(":", 3) + [None] * 3)[:3]
        assert username and password, "bad login_credentials"
        args.extend(["--username", username, "--password", password])
        if domain:
            args.extend(["--domain", domain])

    if zip_only:
        return args

    if writer_options:
        args.extend(['--writer-options', writer_options])

    if language:
        args.extend(['--language', language])

    return args


def suggest_filename(metabook_data):
    if not metabook_data:
        return None

    from mwlib import myjson
    mb = myjson.loads(metabook_data)

    def suggestions():
        yield mb.title
        for a in mb.items:
            yield a.title

    for x in suggestions():
        if x and x.strip():
            return x.strip()


class commands(object):
    def statusfile(self):
        host = self.proxy._rpcclient.host
        port = self.proxy._rpcclient.port
        return 'qserve://%s:%s/%s' % (host, port, self.jobid)

    def rpc_makezip(self, params=None):
        def doit(metabook_data=None, collection_id=None, base_url=None, **kw):
            dir = get_collection_dir(collection_id)

            def getpath(p):
                return os.path.join(dir, p)

            zip_path = getpath("collection.zip")
            if os.path.isdir(dir):
                if os.path.exists(zip_path):
                    return
            else:
                os.mkdir(dir)

            metabook_path = getpath("metabook.json")

            args = ["mw-zip", "-o", zip_path, "-m", metabook_path, "--status", self.statusfile()]
            if base_url:
                args.extend(['--config', base_url])

            args.extend(_get_args(zip_only=True, **params))

            if metabook_data:
                f = open(metabook_path, 'wb')
                f.write(metabook_data)
                f.close()

            system(args, timeout=8 * 60.0)

        return doit(**params)

    def rpc_render(self, params=None):
        def doit(metabook_data=None, collection_id=None, base_url=None, writer=None, **kw):
            writer = writer or "rl"
            dir = get_collection_dir(collection_id)

            def getpath(p):
                return os.path.join(dir, p)

            self.qaddw(channel="makezip", payload=dict(params=params), jobid="%s:makezip" % (collection_id, ), timeout=20 * 60)
            outfile = getpath("output.%s" % writer)
            args = ["mw-render",  "-w",  writer, "-c", getpath("collection.zip"), "-o", outfile,  "--status", self.statusfile()]

            args.extend(_get_args(**params))

            system(args, timeout=15 * 60.0)
            os.chmod(outfile, 0644)
            size = os.path.getsize(outfile)
            url = cacheurl + "/%s/%s/output.%s" % (collection_id[:2], collection_id, writer)
            return dict(url=url, size=size,
                        suggested_filename=suggest_filename(metabook_data) or "")

        return doit(**params)


def start_serving_files(cachedir, address, port):
    from gevent.pywsgi import WSGIServer
    from bottle import route, static_file, default_app
    cachedir = os.path.abspath(cachedir)

    @route('/cache/:filename#.*#')
    def server_static(filename):
        response = static_file(filename, root=cachedir, mimetype="application/octet-stream")
        if filename.endswith(".rl"):
            response.headers["Content-Disposition"] = "inline; filename=collection.pdf"
        return response
    s = WSGIServer((address, port), default_app())
    s.start()
    return s


def make_cachedir(cachedir):
    if not os.path.isdir(cachedir):
        os.makedirs(cachedir)
    for i in range(0x100, 0x200):
        p = os.path.join(cachedir, hex(i)[3:])
        if not os.path.isdir(p):
            os.mkdir(p)


def main():
    global cachedir, cacheurl
    numgreenlets = 10
    http_address = '0.0.0.0'
    http_port = 8898
    serve_files = True
    from mwlib import argv
    opts, args = argv.parse(sys.argv[1:], "--no-serve-files --serve-files-port= --serve-files-address= --serve-files --cachedir= --url= --numprocs=")
    for o, a in opts:
        if o == "--cachedir":
            cachedir = a
        elif o == "--url":
            cacheurl = a
        elif o == "--numprocs":
            numgreenlets = int(a)
        elif o == "--no-serve-files":
            serve_files = False
        elif o == "--serve-files-port":
            http_port = int(a)
        elif o == "--serve-files-address":
            http_address = str(a)

    if cachedir is None:
        sys.exit("nslave: missing --cachedir argument")

    if serve_files:
        wsgi_server = start_serving_files(cachedir, http_address, http_port)
        port = wsgi_server.socket.getsockname()[1]
        if not cacheurl:
            cacheurl = "http://%s:%s/cache" % (find_ip(), port)
        print "serving files from %r at url %s" % (cachedir, cacheurl)

    if not cacheurl:
        sys.exit("--url option missing")

    make_cachedir(cachedir)
    from mwlib.async import slave
    slave.main(commands, numgreenlets=numgreenlets, argv=args)

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = nuwiki

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import os
import zipfile
import shutil
import tempfile
import urllib
import sqlite3dbm
from mwlib import myjson as json

from mwlib import nshandling, utils
from mwlib.log import Log

log = Log('nuwiki')


class page(object):
    expanded = 0
    def __init__(self, meta, rawtext):
        self.__dict__.update(meta)
        self.rawtext = rawtext

class DumbJsonDB(object):

    def __init__(self, fn, allow_pickle=False):
        self.fn = fn
        self.allow_pickle = allow_pickle
        self.read_db()

    def read_db(self):
        self.db = sqlite3dbm.open(self.fn)

    def __getitem__(self, key):
        v = self.db.get(key, '')
        if v:
            return json.loads(v)
        else:
            return None

    def get(self, key, default=None):
        res = self[key]
        if res == None:
            return default
        else:
            return res

    def items(self):
        return self.db.items()

    def __getstate__(self):
        # FIXME: pickling zip based containers not supported and currently not needed.
        # if desired the content of the db file need to be persisted...
        assert self.allow_pickle, 'ERROR: pickling not allowed for zip files. Use unzipped zip file instead'
        d = self.__dict__.copy()
        del d['db']
        return d

    def __setstate__(self, d):
        self.__dict__ = d
        self.read_db()



class nuwiki(object):
    def __init__(self, path, allow_pickle=False):
        self.path = os.path.abspath(path)
        d = os.path.join(self.path, "images", "safe")
        if not os.path.exists(d):
            try:
                os.makedirs(d)
            except OSError, exc:
                if exc.errno != 17: # file exists
                    raise
            
        self.excluded = set(x.get("title") for x in self._loadjson("excluded.json", []))            

        self.revisions = {}
        self._read_revisions()

        fn = os.path.join(self.path, 'authors.db')
        if not os.path.exists(fn):
            self.authors = None
            log.warn('no authors present. parsing revision info instead')
        else:
            self.authors = DumbJsonDB(fn, allow_pickle=allow_pickle)

        fn = os.path.join(self.path, 'html.db')
        if not os.path.exists(fn):
            self.html = self.extractHTML(self._loadjson("parsed_html.json", {}))
            log.warn('no html present. parsing revision info instead')
        else:
            self.html = DumbJsonDB(fn, allow_pickle=allow_pickle)

        fn = os.path.join(self.path, 'imageinfo.db')
        if not os.path.exists(fn):
            self.imageinfo = self._loadjson("imageinfo.json", {})
            log.warn('loading imageinfo from pickle')
        else:
            self.imageinfo = DumbJsonDB(fn, allow_pickle=allow_pickle)

        self.redirects = self._loadjson("redirects.json", {})
        self.siteinfo = self._loadjson("siteinfo.json", {})
        self.nshandler = nshandling.nshandler(self.siteinfo)        
        self.en_nshandler = nshandling.get_nshandler_for_lang('en') 
        self.nfo = self._loadjson("nfo.json", {})

        self.set_make_print_template()

    def __getstate__(self):
        d = self.__dict__.copy()
        del d['make_print_template']
        return d

    def __setstate__(self, d):
        self.__dict__ = d
        self.set_make_print_template()

    def set_make_print_template(self):
        self.make_print_template = None

    def _loadjson(self, path, default=None):
        path = self._pathjoin(path)
        if self._exists(path):
            return json.load(open(path, "rb"))
        return default
        
    def _read_revisions(self):
        count = 1
        while 1:
            fn = self._pathjoin("revisions-%s.txt" % count)
            if not os.path.exists(fn):
                break
            count += 1
            print "reading", fn
            d=unicode(open(self._pathjoin(fn), "rb").read(), "utf-8")
            pages = d.split("\n --page-- ")

            for p in pages[1:]:
                jmeta, rawtext = p.split("\n", 1)
                meta = json.loads(jmeta)
                pg = Page(meta, rawtext)
                if pg.title in self.excluded and pg.ns!=0:
                    pg.rawtext = unichr(0xebad)
                revid = meta.get("revid")
                if revid is None:
                    self.revisions[pg.title] = pg
                    continue

                self.revisions[meta["revid"]] = pg

                # else:
                #     print "excluding:", repr(pg.title)
                
        tmp = self.revisions.items()
        tmp.sort(reverse=True)
        for revid, p in tmp:
            title = p.title
            if title not in self.revisions:
                self.revisions[title] = p
                
    def _pathjoin(self, *p):
        return os.path.join(self.path, *p)
    
    def _exists(self, p):
        return os.path.exists(p)
    
    def get_siteinfo(self):
        return self.siteinfo
    
    def _get_page(self, name, revision=None):
        if revision is not None and name not in self.redirects:
            try:
                page = self.revisions.get(int(revision))
            except TypeError:
                print "Warning: non-integer revision %r" % revision
            else:
                if page and page.rawtext:
                    redirect = self.nshandler.redirect_matcher(page.rawtext)
                    if redirect:
                        return self.get_page(self.nshandler.get_fqname(redirect))
                return page

        oldname = name
        name = self.redirects.get(name, name)
        
        return self.revisions.get(name) or self.revisions.get(oldname)

    def get_page(self, name, revision=None):
        retval = self._get_page(name,revision=revision)
        # if retval is None:
        #     log.warning("missing page %r" % ((name,revision),))
        return retval
    
    def normalize_and_get_page(self, name, defaultns):
        fqname = self.nshandler.get_fqname(name, defaultns=defaultns)
        return self.get_page(fqname)

    def normalize_and_get_image_path(self, name):
        assert isinstance(name, basestring)
        name = unicode(name)
        ns, partial, fqname = self.nshandler.splitname(name, defaultns=6)
        if ns != 6:
            return

        if "/" in fqname:
            return None
        
        
        
        
        p = self._pathjoin("images", utils.fsescape(fqname))
        if not self._exists(p):
            fqname = 'File:' + partial # Fallback to default language english
            p = self._pathjoin("images", utils.fsescape(fqname))
            if not self._exists(p):
                return None

        if 1:
            from hashlib import md5
            
            hd = md5(fqname.encode("utf-8")).hexdigest()
            ext = os.path.splitext(p)[-1]
            ext = ext.replace(' ', '')
            # mediawiki gives us png's for these extensions. let's change them here.
            if ext.lower() in (".gif", ".svg", '.tif', '.tiff'):
                # print "change xt:", ext
                ext = ".png"
            hd += ext
                
            safe_path = self._pathjoin("images", "safe", hd)
            if not os.path.exists(safe_path):
                try:
                    os.symlink(os.path.join("..", utils.fsescape(fqname)), safe_path)
                except OSError, exc:
                    if exc.errno != 17: # File exists
                        raise
            return safe_path
        return p

    def get_data(self, name):
        return self._loadjson(name+".json")

    def articles(self):
        res = list(set([p.title for p in self.revisions.values() if p.ns==0]))
        res.sort()
        return res

    def select(self, start, end):
        res = set()
        for p in self.revisions.values():
            if start <= p.title <= end:
                res.add(p.title)
        res = list(res)
        res.sort()
        return res

    def extractHTML(self, parsed_html):
        html = {}
        for article in parsed_html:
            _id = article.get('page') or article.get('oldid')
            html[_id] = article
        return html

NuWiki = nuwiki
Page = page

def extract_member(zipfile, member, dstdir):
    """Copied and adjusted from Python 2.6 stdlib zipfile.py module.

       Extract the ZipInfo object 'member' to a physical
       file on the path targetpath.
    """

    assert dstdir.endswith(os.path.sep), "/ missing at end"
    
    fn = member.filename
    if isinstance(fn, str):
        fn = unicode(fn, 'utf-8')
    targetpath = os.path.normpath(os.path.join(dstdir, fn))
    
    if not targetpath.startswith(dstdir):
        raise RuntimeError("bad filename in zipfile %r" % (targetpath, ))
        
    # Create all upper directories if necessary.
    if member.filename.endswith("/"):
        upperdirs = targetpath
    else:
        upperdirs = os.path.dirname(targetpath)
        
    if not os.path.isdir(upperdirs):
        os.makedirs(upperdirs)

    if not member.filename.endswith("/"):
        open(targetpath, 'wb').write(zipfile.read(member.filename))

def extractall(zf, dst):
    dst = os.path.normpath(os.path.abspath(dst))+os.path.sep
    
    for zipinfo in zf.infolist():
        extract_member(zf, zipinfo, dst)
    
       
class adapt(object):
    edits = None
    interwikimap = None
    was_tmpdir = False
    
    def __init__(self, path_or_instance):
        if isinstance(path_or_instance, zipfile.ZipFile):
            zf = path_or_instance
            tmpdir = tempfile.mkdtemp()
            extractall(zf, tmpdir)
            path_or_instance = tmpdir
            self.was_tmpdir = True
            
        if isinstance(path_or_instance, basestring):
            self.nuwiki = NuWiki(path_or_instance, allow_pickle=not self.was_tmpdir)
        else:
            self.nuwiki = path_or_instance
        self.siteinfo = self.nuwiki.get_siteinfo()
        self.metabook = self.nuwiki.get_data("metabook")
        
    def __getattr__(self, name):
        try:
            return getattr(self.nuwiki, name)
        except AttributeError:
            raise AttributeError()

    def __getstate__(self):
        return self.__dict__

    def __setstate__(self, d):
        self.__dict__ = d

    def getURL(self, name, revision=None, defaultns=nshandling.NS_MAIN):
        base_url = self.nfo["base_url"]
        if not base_url.endswith("/"):
            base_url += "/"
        script_extension = self.nfo.get("script_extension") or ".php"


        p = '%sindex%s?' % (base_url, script_extension)
        if revision is not None:
            return p + 'oldid=%s' % revision
        else:
            fqtitle = self.nshandler.get_fqname(name, defaultns=defaultns)
            return p + 'title=%s' % urllib.quote(fqtitle.replace(' ', '_').encode('utf-8'), safe=':/@')
    
    def getDescriptionURL(self, name):
        return self.getURL(name, defaultns=nshandling.NS_FILE)

    def getAuthors(self, title, revision=None):
        fqname = self.nshandler.get_fqname(title)
        if fqname in self.redirects:
            res = self._getAuthors(title, self.redirects.get(fqname, fqname), revision=revision)
        else:
            res = None

        return res if res is not None else self._getAuthors(title, fqname, revision=revision)

    def _getAuthors(self, title, fqname, revision=None):
        if getattr(self.nuwiki, 'authors', None) is not None:
            authors = self.nuwiki.authors[fqname]
            return authors
        else:
            from mwlib.authors import get_authors
            if self.edits is None:
                edits = self.edits = {}
                for edit in self.nuwiki.get_data("edits") or []:
                    try:
                        edits[edit['title']] = edit.get("revisions")
                    except KeyError:
                        continue

            revisions = self.edits.get(fqname, [])
            authors = get_authors(revisions)

            return authors
    
    def getSource(self, title, revision=None):
        from mwlib.metabook import make_source

        g = self.siteinfo['general']
        return make_source(
            name='%s (%s)' % (g['sitename'], g['lang']),
            url=g['base'],
            language=g['lang'],
            base_url=self.nfo['base_url'],
            script_extension=self.nfo['script_extension'],
        )

    def getHTML(self, title, revision=None):
        if revision:
            return self.nuwiki.html.get(revision, {})
        else:
            return self.nuwiki.html.get(title, {})

    def getParsedArticle(self, title, revision=None):
        if revision:
            page = self.nuwiki.get_page(None, revision)
        else:
            page = self.normalize_and_get_page(title, 0)

        if page:
            raw = page.rawtext
            expandTemplates = not page.expanded
        else:
            raw = None
            expandTemplates = True

        if raw is None:
            return None

        from mwlib import uparser        

        return uparser.parseString(title=title, raw=raw, wikidb=self, lang=self.siteinfo["general"]["lang"], expandTemplates=expandTemplates)

    def getLicenses(self):
        from mwlib import metabook
        licenses = self.nuwiki.get_data('licenses') or []
        res = []
        for x in licenses:
            if isinstance(x, dict):
                res.append(metabook.license(title=x["title"], wikitext=x["wikitext"],  _wiki=self))
            elif isinstance(x, metabook.license):
                res.append(x)
                x._wiki = self
        return res
    
    def clear(self):
        if self.was_tmpdir and os.path.exists(self.nuwiki.path):
            print 'removing %r' % self.nuwiki.path
            shutil.rmtree(self.nuwiki.path, ignore_errors=True)
    
    def getDiskPath(self, name, size=None):
        return self.nuwiki.normalize_and_get_image_path(name)

    def get_image_description_page(self, name):
        ns, partial, fqname = self.nshandler.splitname(name, nshandling.NS_FILE)
        page = self.get_page(fqname)
        if page is not None:
            return page
        fqname = self.en_nshandler.get_fqname(partial, nshandling.NS_FILE)
        return self.get_page(fqname)

    def getImageTemplates(self, name, wikidb=None):
        from mwlib.expander import get_templates

        page = self.get_image_description_page(name)
        if page is not None:
            return get_templates(page.rawtext)
        print 'no such image: %r' % name
        return []

    def getImageTemplatesAndArgs(self, name, wikidb=None):
        from mwlib.expander import get_templates, get_template_args
        page = self.get_image_description_page(name)
        if page is not None:
            templates = get_templates(page.rawtext)
            from mwlib.expander import find_template
            from mwlib.templ.evaluate import Expander
            from mwlib.templ.parser import parse
            from mwlib.templ.misc import DictDB
            args = set()
            e=Expander('', wikidb=DictDB())
            # avoid parsing with every call to find_template
            parsed_raw=[parse(page.rawtext, replace_tags=e.replace_tags)]
            for t in templates:
                tmpl = find_template(None, t, parsed_raw[:])
                arg_list = tmpl[1]
                for arg in arg_list:
                    if isinstance(arg, basestring) and len(arg) > 3 and ' ' not in arg:
                        args.add(arg)
            templates.update(args)
            return templates
        return []


    def getImageWords(self, name, wikidb=None):
        import re
        page = self.get_image_description_page(name)
        if page is not None:
            words = re.split('\{|\}|\[|\]| |\,|\|', page.rawtext)
            return list(set([w.lower() for w in  words if w]))
        print 'no such image: %r' % name
        return []



    def getContributors(self, name, wikidb=None):
        page = self.get_image_description_page(name)
        if page is None:
            return []
        users = getContributorsFromInformationTemplate(page.rawtext, page.title, self)
        if users:
            return users
        return self.getAuthors(page.title)


def getContributorsFromInformationTemplate(raw, title, wikidb):
    from mwlib.expander import find_template, get_templates, get_template_args, Expander
    from mwlib import uparser, parser, advtree
    from mwlib.templ.parser import parse
    
    def getUserLinks(raw):
        def isUserLink(node):
            return isinstance(node, parser.NamespaceLink) and node.namespace == 2 # NS_USER

        result = list(set([
            u.target
            for u in uparser.parseString(title,
                raw=raw,
                wikidb=wikidb,
            ).filter(isUserLink)
        ]))
        result.sort()
        return result

    def get_authors_from_template_args(template):
        args = get_template_args(template, expander)

        author_arg = args.get('Author', None)
        if author_arg:
            # userlinks = getUserLinks(author_arg)
            # if userlinks:
            #     return userlinks
            node = uparser.parseString('', raw=args['Author'], wikidb=wikidb)
            advtree.extendClasses(node)
            txt = node.getAllDisplayText().strip()
            if txt:
                return [txt]

        if args.args:
            return getUserLinks('\n'.join([args.get(i, u'') for i in range(len(args.args))]))

        return []

    expander = Expander(u'', title, wikidb)
    parsed_raw = [parse(raw, replace_tags=expander.replace_tags)]
    template = find_template(None, 'Information', parsed_raw[:])
    if template is not None:
        authors = get_authors_from_template_args(template)
        if authors:
            return authors
    authors = []
    for template in get_templates(raw):
        t = find_template(None, template, parsed_raw[:])
        if t is not None:
            authors.extend(get_authors_from_template_args(t))
    if authors:
        return authors
    return getUserLinks(raw)

########NEW FILE########
__FILENAME__ = odfconf
#! /usr/bin/env python

# Copyright (c) 2008, PediaPress GmbH
# See README.rst for additional licensing information.

class OdfConf(object):
        paper = {
                        'PAPER_WIDTH' : 8.3,
                        'PAPER_HEIGHT' : 11.7,
                        'MAX_IMG_COVER_FACTOR' : 0.4, #[%] 0.4 = the image covers max 40% of the page
                        'PAGE_BORDER_TOP' : 0.8,      #[inch] spaces between paper-border and printable area
                        'PAGE_BORDER_BOTTOM' : 0.8,   #[inch] spaces between paper-border and printable area
                        'PAGE_BORDER_LEFT' : 0.8,     #[inch] spaces between paper-border and printable area
                        'PAGE_BORDER_RIGHT' : 0.8,    #[inch] spaces between paper-border and printable area
                        'IMG_DPI_STANDARD' : 96,      #[dpi] means a image with 120px, needs 120[px]/96[dpi] =  1,25[in] inces on DIN A4
                        'IMG_DPI_INLINE' : 96,       #[dpi] see above
                        }
        paper['IMG_MAX_WIDTH'] = paper['MAX_IMG_COVER_FACTOR'] * \
            (paper['PAPER_WIDTH'] - paper['PAGE_BORDER_LEFT'] - paper['PAGE_BORDER_RIGHT'])
        paper['IMG_MAX_HEIGHT'] = paper['MAX_IMG_COVER_FACTOR'] * \
            (paper['PAPER_HEIGHT'] - paper['PAGE_BORDER_TOP'] - paper['PAGE_BORDER_BOTTOM'])
        

# Config paper size  (in INCH)
#
# 1pt = 1/72in, see  http://en.wikipedia.org/wiki/Point_(typography)#Current_DTP_point_system
# Constants (all values in inch!)

#inline_img_dpi = 100 # scaling factor for inline images. 100 dpi should be the ideal size in relation to 10pt text size 
#????
#targetWidth = 800  # target image width 
#scale = 1./75

########NEW FILE########
__FILENAME__ = odfstyles
#! /usr/bin/env python
# -*- coding: utf-8 -*-
# See README.rst for additional licensing information.
#
#  This file defines the styles for odfwriter.py
#  This file mainly connects
#
# See odpy: 
# and the od* spec: http://docs.oasis-open.org/office/v1.1/OS/OpenDocument-v1.1-html/OpenDocument-v1.1.html
#



from odf import style
from odf import text

#CONTENT:
#
# Font styles: dejaVuSerif, dejaVuSerif, dejaVuSerif, dejaVuSansMono
#
# Section styles: sect
#          FIXME:  n/a
#
# Paragraph styles: standard, textbody, definitionterm, hr, center, blockquote
#                   indentedSingle, indentedDouble, indentedTriple, imgCaption, tableCaption
#            FIXME: footnote (liststyle does not work)
#
# Text styles: emphasis, italic, strong, bold, sub, sup, underline, 
#              strike, big, small, var, deleted, inserted, cite (?),
#       FIXME:  overline (there is no overline in ooo Writer), teletyped
#
# Table styles: 
#        FIXME: dumbcolumn
#
# List styles: numberedlist, unorderedlist, definitionlist
#       FIXME: <not reviewed yet>
#
# Header styles: ArticleHeader, h0-h5
#         FIXME: Rename h0-h5
#
# Graphic styles: frmOuter, frmInner, frmOuterRight, frmOuterLeft, frmOuterCenter,
#          FIXME: formula, imageMap
#
# General FIXME: add "style:display-name" for each element

#
# Font Styles
# see http://books.evc-cit.info/odbook/ch03.html#char-para-styling-section for a fast introdution
#
dejaVuSerif = style.FontFace(
        name="DejaVuSerif",fontfamily="'DejaVu Serif'", fontfamilygeneric="roman", fontpitch="variable")
dejaVuSans = style.FontFace(
        name="DejaVuSans", fontfamily="'DejaVu Sans'", fontfamilygeneric="swiss", fontpitch="variable")
dejaVuMono = style.FontFace(
        name="DejaVumono",fontfamily="'DejaVu mono'",fontfamilygeneric="modern", fontpitch="fixed")
dejaVuSansMono = style.FontFace(
        name="DejaVuSansMono", fontfamily="'DejaVu Sans Mono'", fontfamilygeneric="swiss", fontpitch="fixed")

#
# Section styles 
#
sect  = style.Style(name="Sect1", family="section")

sectTable = style.Style(name="SectTable", family="section")

#
# Paragraph styles
#


# textbody is the default for text
textbody = style.Style(name="TextBody", family="paragraph")
textbody.addElement(
    style.ParagraphProperties(
        marginbottom="0.05in", margintop="0.04in", textalign="left",
        punctuationwrap="hanging", linebreak="strict",
        orphans="3", keeptogether="always",

    )
)
textbody.addElement(
    style.TextProperties(
        fontsize="12pt", language="en", country="US",
        fontname="DejaVuSerif"
    )
)

#
# special paragraph styles:  paragraph text styles
#
preformatted = style.Style(name="Preformatted",family="paragraph")
preformatted.addElement(
    style.ParagraphProperties(
        marginleft=".25in", marginright=".25in", margintop=".25in", marginbottom=".25in", 
        backgroundcolor="#e6e6e6",
        orphans="3", keeptogether="always",
    )
)
preformatted.addElement(
    style.TextProperties(
                fontname="DejaVumono", 
                fontsize="10pt"
        )
)


definitionterm = style.Style(name="definitionterm", family="paragraph")
definitionterm.addElement(
    style.TextProperties(
        fontweight="bold",
        fontname="DejaVuSerif"
    )
)



hr = style.Style(name="HorizontalLine", family="paragraph")
hr.addElement(
    style.ParagraphProperties(
        margintop="0in", marginbottom="0.1965in",padding="0in",
        borderlinewidthbottom="0.0008in 0.0138in 0.0008in", 
        borderleft="none",borderright="none",bordertop="none",
        borderbottom="0.0154in double #808080"
    )
)

tableCaption = style.Style(name="TableCaption", family="paragraph")
tableCaption.addElement(
    style.ParagraphProperties(
        textalign="center",
        marginbottom="0.1in"
    )
)
tableCaption.addElement(
    style.TextProperties(
        fontweight="bold",
        fontname="DejaVuSerif"
    )
)


center = style.Style(name="Center", family="paragraph")
center.addElement(
    style.ParagraphProperties(
        textalign="center"
    )
)
center.addElement(
    style.TextProperties(
        fontname="DejaVuSerif"
    )
)

blockquote = style.Style(name="Blockquote", family="paragraph")
blockquote.addElement(
    style.ParagraphProperties(
        marginleft="0.12in",marginright="0.12in", margintop="0in",marginbottom="0.15in",
        orphans="3", keeptogether="always"
    )
)
blockquote.addElement(
    style.TextProperties(
        fontname="DejaVuSerif"
    )
)


indentedSingle = style.Style(name="IndentedSingle", family="paragraph")
indentedSingle.addElement(
    style.ParagraphProperties(
        marginleft="0.12in", marginright="0in", margintop="0.05in", marginbottom="0.04in",
        orphans="3", keeptogether="always"
    )
)
indentedSingle.addElement(
    style.TextProperties(
        fontname="DejaVuSerif"
    )
)

indentedDouble = style.Style(name="IndentedDouble", family="paragraph")
indentedDouble.addElement(
    style.ParagraphProperties(
        marginleft="0.24in", marginright="0in", margintop="0.05in", marginbottom="0.04in",
        orphans="3", keeptogether="always"
    )
)
indentedDouble.addElement(
    style.TextProperties(
        fontname="DejaVuSerif"
    )
)

indentedTriple = style.Style(name="IndentedTriple", family="paragraph")
indentedTriple.addElement(
    style.ParagraphProperties(
        marginleft="0.36in", marginright="0in", margintop="0.05in", marginbottom="0.04in",
        orphans="3", keeptogether="always"
    )
)
indentedTriple.addElement(
    style.TextProperties(
        fontname="DejaVuSerif"
    )
)


footnote = style.Style(name="Footnote", family="paragraph", liststylename="FootnoteList") #fixme: liststyle does not work
footnote.addElement(
    style.TextProperties(
        fontsize="10pt", fontname="DejaVuSerif"
    )
)


#
# Text styles (inline)
#
emphasis = style.Style(name="Emphasis",family="text")
emphasis.addElement(
    style.TextProperties(
        fontstyle="italic", fontname="DejaVuSerif"
    )
) # should be also bold, but is paresed differntly

italic = emphasis #alternative name

strong = style.Style(name="Bold",family="text")
strong.addElement(
    style.TextProperties(
        fontweight="bold", fontname="DejaVuSerif"
    )
)
bold = strong #alternative name

sup = style.Style(name="Sup", family="text")
sup.addElement(
    style.TextProperties(
        textposition="super", fontname="DejaVuSerif"
    )
)

sub = style.Style(name="Sub", family="text") 
sub.addElement(
    style.TextProperties(
        textposition="-30% 50%", fontname="DejaVuSerif"
    )
)

underline = style.Style(name="Underline", family="text")
underline.addElement(
    style.TextProperties(
        textunderlinestyle="solid", fontname="DejaVuSerif"
    )
)

strike =  style.Style(name="Strike", family="text")
strike.addElement(
    style.TextProperties(
        textlinethroughstyle="solid", fontname="DejaVuSerif"
    )
)

big = style.Style(name="Big", family="text")
big.addElement(
    style.TextProperties(
        fontsize="125%", fontname="DejaVuSerif"
    )
)


small = style.Style(name="Small", family="text")
small.addElement(
    style.TextProperties(
        fontsize="80%", fontname="DejaVuSerif"
    )
)

teletyped = style.Style(name="Teletyped", family="text")
teletyped.addElement(
    style.TextProperties(
        fontsize="80%", fontname="DejaVumono"
    )
)


# logical text tags:
var = emphasis 
cite = emphasis
deleted = strike
inserted = underline
code = teletyped
source = preformatted

overline = textbody # try to FIXME, but there is no overline in ooo Writer




#
# Table styles
#

dumbcolumn = style.Style(name="Dumbcolumn", family="table-column") # REALLY FIXME
dumbcolumn.addElement(style.TableColumnProperties(attributes={'columnwidth':"1.0in"}))

#
# List styles
#

##2do: testcases
#ordered list (until lvl2, then bullets)
numberedlist = text.ListStyle(name="numberedlist")
numberedlist.addElement(
    text.ListLevelStyleNumber(
        level="1", numprefix="  ", numsuffix=".  ", numformat="1"
    )
)
numberedlist.addElement(
    text.ListLevelStyleNumber(
        level="2", numprefix="  ", numsuffix=")  ", numformat="a"
    )
)
numberedlist.addElement(
    text.ListLevelStyleBullet(
        level="3", numprefix="  ", numsuffix="   ", bulletchar=u'•'
    )
)


# unordered list
unorderedlist = text.ListStyle(name="unorderedlist")
unorderedlist.addElement(
    text.ListLevelStyleBullet(
        level="1",numprefix="   ", bulletchar=u'•', numsuffix="   "
    )
)


definitionlist = text.ListStyle(name="definitionlist")
definitionlist.addElement(
    text.ListLevelStyleBullet(
        level="1", bulletchar=' ', numsuffix=""
    )
)
#footnote list ##fixme: does not work
footnoteLLSN = text.ListLevelStyleNumber(
        level="1", numformat="1"
)

footnoteLLSN.addElement(
    style.ListLevelProperties(
        spacebefore="0.02in", minlabelwidth="0.2in"
    )
)
footnotelist = text.ListStyle(name="FootnoteList")
footnotelist.addElement(footnoteLLSN)

#
# Header Syles
#

ArticleHeader = style.Style(name="HeadingArticle", family="paragraph", defaultoutlinelevel="1")
ArticleHeader.addElement(
    style.ParagraphProperties(
            margintop="0in",marginbottom="0.15in", keepwithnext="always"
    )
)
ArticleHeader.addElement(
    style.TextProperties(
        fontsize="24pt", fontname="DejaVuSans"
    )
)

h0 = ArticleHeader #alternative name


chapter = style.Style(name="Chapter", family="paragraph", defaultoutlinelevel="1")
chapter.addElement(
    style.ParagraphProperties(
        margintop="0.3in",marginbottom="0.15in", keepwithnext="always"
    )
)
chapter.addElement(
    style.TextProperties(
        fontsize="32pt", fontname="DejaVuSans"
    )
)



h1 = style.Style(name="Heading1", family="paragraph", defaultoutlinelevel="2")
h1.addElement(
    style.ParagraphProperties(
        margintop="0.3in",marginbottom="0.15in", keepwithnext="always"
    )
)
h1.addElement(
    style.TextProperties(
        fontsize="20pt", fontname="DejaVuSans"
    )
)


h2 = style.Style(name="Heading2", family="paragraph", defaultoutlinelevel="3")
h2.addElement(
    style.TextProperties(
        fontsize="18pt", fontname="DejaVuSans"
    )
)
h2.addElement(
    style.ParagraphProperties(
        margintop="0.3in",marginbottom="0.08in", keepwithnext="always"
    )
)


h3 = style.Style(name="Heading3", family="paragraph", defaultoutlinelevel="4")
h3.addElement(
    style.TextProperties(
        fontsize="16pt", fontname="DejaVuSans"
    )
)
h3.addElement(
    style.ParagraphProperties(
        margintop="0.3in",marginbottom="0.05in", keepwithnext="always"
    )
)


h4 = style.Style(name="Heading4", family="paragraph", defaultoutlinelevel="5")
h4.addElement(
    style.TextProperties(
        fontsize="14pt", fontname="DejaVuSans"
    )
)
h4.addElement(
    style.ParagraphProperties(
        margintop="0.3in",marginbottom="0.05in", keepwithnext="always"
    )
)


h5 = style.Style(name="Heading5", family="paragraph", defaultoutlinelevel="6")
h5.addElement(
    style.TextProperties(
        fontsize="10pt", fontname="DejaVuSans"
    )
)
h5.addElement(
    style.ParagraphProperties(
        margintop="0.3in",marginbottom="0.05in", keepwithnext="always"
    )
)


# the text under a image
imgCaption = style.Style(name="ImageCaption", family="paragraph", parentstylename=textbody)
imgCaption.addElement(
    style.ParagraphProperties(
        textalign="center", justifysingleword="false" 
    )
)
imgCaption.addElement(
    style.TextProperties(
        fontsize="10pt", fontname="DejaVuSerif",
    )
)

#
# Graphic styles:
#
##2do: where is grafic used?
##fixme: clean it
#graphic = style.Style(name="Graphic", family="graphic")
#graphic.addElement(style.GraphicProperties(wrap="dynamic", verticalrel="paragraph", horizontalrel="paragraph"))
#graphic.addElement(
#    style.GraphicProperties(padding="0.15in",borderleft="none",borderright="0.0154in double #FFFFFF",
#                            bordertop="0.0154in double #FFFFFF",borderbottom="0.0154in double #FFFFFF"))

#graphic.addElement(style.GraphicProperties(padding="0.15in", border="0.01in single #ff00ff"))
#graphic = style.Style(name="Graphic", family="graphic")
#graphic.addElement(style.GraphicProperties(runthrough="foreground", wrap="dynamic", numberwrappedparagraphs="no-limit",
#                                           verticalpos="top", verticalrel="page",horizontalpos="center", horizontalrel="page"))




# define a outer frame:
# frmOuter and frmInner are mainy used to format and align the images

frmOuter = style.Style(name="mwlibfrmOuter", family="graphic")
frmStyGraPropDef = style.GraphicProperties(
        marginleft="0.1in", marginright="0.1in", margintop="0.1in", marginbottom="0.1in",
        wrap="right", numberwrappedparagraphs="no-limit", 
        verticalpos="from-top", horizontalpos="from-left", 
        verticalrel="paragraph", horizontalrel="paragraph", 
        backgroundcolor="transparent", 
        padding="0.0402in", border="0.0138in solid #c0c0c0", 
        shadow="none"
)
frmOuter.addElement(frmStyGraPropDef)
frmOuter.internSpacing = 0.2 
# if frmOuter has marginleft/marginright set this float-value to the sum: internSpacing = marginleft + marginright

# frmOuterRight, frmOuterLeft and frmOuterCenter are like frmOuter, but need other alignment
frmOuterRight = style.Style(name="mwlibfrmOuterRight", family="graphic", parentstylename=frmOuter) # does not inherit GrapficPorpertys!
frmStyGraPropRight = style.GraphicProperties(
        marginleft="0.1in", marginright="0.1in", margintop="0.1in", marginbottom="0.1in",
        wrap="left", numberwrappedparagraphs="no-limit", 
        verticalpos="from-top", horizontalpos="right",
        verticalrel="paragraph", horizontalrel="paragraph", 
        backgroundcolor="transparent", 
        padding="0.0402in", border="0.0138in solid #c0c0c0", 
        shadow="none"
)
frmOuterRight.addElement(frmStyGraPropRight)

frmOuterLeft = style.Style(name="mwlibfrmOuterLeft", family="graphic", parentstylename=frmOuter) 
frmStyGraProbLeft = style.GraphicProperties(
        marginleft="0.1in", marginright="0.1in", margintop="0.1in", marginbottom="0.1in",
        wrap="right", numberwrappedparagraphs="no-limit", 
        verticalpos="from-top", horizontalpos="left",
        verticalrel="paragraph", horizontalrel="paragraph", 
        backgroundcolor="transparent", 
        padding="0.0402in", border="0.0138in solid #c0c0c0",
        shadow="none"
)
frmOuterLeft.addElement(frmStyGraProbLeft)

frmOuterCenter = style.Style(name="mwlibfrmOuterCenter", family="graphic", parentstylename=frmOuter) 
frmStyGraPropCenter = style.GraphicProperties(
        marginleft="0.1in", marginright="0.1in", margintop="0.1in", marginbottom="0.1in",
        wrap="paralell", numberwrappedparagraphs="no-limit", 
        verticalpos="from-top", horizontalpos="center", 
        verticalrel="paragraph", horizontalrel="paragraph", 
        backgroundcolor="transparent", 
        padding="0.0402in", border="0.0138in solid #c0c0c0",
        shadow="none"
)
frmOuterCenter.addElement(frmStyGraPropCenter)


# the inner frame
frmInner = style.Style(name="mwlib_frmInner", family="graphic")
frmInner.addElement(
    style.GraphicProperties(
        verticalpos="from-top", horizontalpos="center",
        verticalrel="paragraph", horizontalrel="paragraph",
        mirror="none", clip="rect(0in 0in 0in 0in)",
        luminance="0%", contrast="0%", red="0%", green="0%", blue="0%", 
        gamma="100%", colorinversion="false", imageopacity="100%", colormode="standard"
    )
)



##2do: imagemap is still in progress
imageMap = style.Style(name="frmImageMap", family="graphic")
"""imageMap.addElement(
    style.GraphicProperties(
        zindex="0"
    )
)
imageMap.addElement(
    style.TextProperties(
        anchortype="paragraph"
    )
)"""



##fixme: forumlars are still broken
# ----- math ----
"""
  <style:style style:name="Formula" style:family="graphic">
   <style:graphic-properties text:anchor-type="as-char" svg:y="0in" fo:margin-left="0.0791in" fo:margin-right="0.0791in" style:vertical-pos="middle" style:vertical-rel="text"/>
  </style:style>
and

 <office:automatic-styles>
  <style:style style:name="fr1" style:family="graphic" style:parent-style-name="Formula">
   <style:graphic-properties style:vertical-pos="middle" style:vertical-rel="text" draw:ole-draw-aspect="1"/>
  </style:style>
 </office:automatic-styles>

                (DRAWNS,u'auto-grow-height'),
                (DRAWNS,u'auto-grow-width'),


"""

formula = style.Style(name="Formula", family="graphic")
#formula.addElement(style.GraphicProperties(attributes={"anchortype":"as-char" ,"y":"0in","marginleft":"0.0791in","marginright":"0.0791in","verticalpos":"middle","verticalrel":"text", "oledrawaspect":"1"}))
#formula.addElement(style.GraphicProperties(verticalpos="middle",verticalrel="baseline",  minwidth="0.7902in", autogrowheight="1", autogrowwidth="1"))






def applyStylesToDoc(doc):

    for font in [dejaVuSerif,
                 dejaVuSans,
                 dejaVuMono,
                 dejaVuSansMono,
                 ]:
        doc.fontfacedecls.addElement(font)

    doc.automaticstyles.addElement(sect)

    for style in [dumbcolumn,
                  indentedSingle,
                  indentedDouble,
                  indentedTriple,
                  strike,
                  big,
                  small,
                  blockquote,
                  ArticleHeader,
                  cite,
                  underline,
                  sup,
                  sub,
                  center,
                  teletyped,
                  formula,
                  chapter,
                  h0,
                  h1,
                  h2,
                  h3,
                  h4,
                  h5,
                  sectTable,
                  textbody,
                  strong,
                  emphasis,
                  preformatted,
                  code,
                  source,
                  footnotelist,
                  footnote,
                  hr,
                  numberedlist,
                  unorderedlist,
                  definitionlist,
                  definitionterm,
                  frmOuterCenter,
                  frmOuterLeft,
                  frmOuterRight,
                  frmInner,
                  imgCaption,
                  tableCaption,
                  sectTable,]:
        doc.styles.addElement(style)


########NEW FILE########
__FILENAME__ = odfwriter
#! /usr/bin/env python

# Copyright (c) 2008-2009, PediaPress GmbH
# See README.rst for additional licensing information.
"""
TODO:
 * add license handling
 * implement missing methods: Imagemap, Hiero, Timeline, Gallery

More Info:
* http://books.evc-cit.info/odbook/book.html
* http://opendocumentfellowship.com/projects/odfpy
* http://testsuite.opendocumentfellowship.com/ sample documents
"""

from __future__ import division

import sys
import odf

from odf.opendocument import OpenDocumentText
from odf import text, dc, meta, table, draw, math, element
from mwlib.log import Log
from mwlib import advtree, writerbase, odfconf, parser
from mwlib import odfstyles as style
from mwlib.treecleaner import TreeCleaner

log = Log("odfwriter")

# check for ODF version
e = element.Element(qname = ("a","n"))
assert hasattr(e, "appendChild")
assert hasattr(e, "lastChild")
assert hasattr(e, "setAttribute")
del e

def showNode(obj):
    attrs = obj.__dict__.keys()
    log(obj.__class__.__name__ )
    stuff =  ["%s => %r" %(k,getattr(obj,k)) for k in attrs if
              (not k == "children") and getattr(obj,k)
              ]
    if stuff:
        log(repr(stuff))

class SkipChildren(object):
    "if returned by the writer no children are processed"
    def __init__(self, element=None):
        self.element = element


class ParagraphProxy(text.Element):
    """
    special handling since most problems occure arround paragraphs
    this is broken!
    """

    qname = (text.TEXTNS, 'p')
    def addElement(self, e):
        assert not hasattr(self, "writeto")
        assert e.parentNode is None
        assert e is not self

        if isinstance(e, ParagraphProxy):
            assert self.parentNode is not None
            #log("relinking paragraph %s" % e.type)
            self.parentNode.addElement(e) # add at the same level
            np = ParagraphProxy() # add copy at the same level
            np.attributes = self.attributes.copy()
            self.parentNode.addElement(np)
            self.writeto = np

        elif e.qname not in self.allowed_children:
            assert self.parentNode is not None
            #log("addElement", e.type, "not allowed in ", self.type)
            # find a parent that accepts this type
            p = self
            #print self, "looking for parent to accept", e
            while p.parentNode is not None and e.qname not in p.allowed_children:
                #print "p:", p
                assert p.parentNode is not p
                p = p.parentNode
            if e.qname not in p.allowed_children:
                assert p.parentNode is None
                log("ParagraphProxy:addElement() ", e.type, "not allowed in any parents, failed, should have been added to", self.type)
                return
            assert p is not self
            #log("addElement: moving", e.type, "to ", p.type)
            # add this type to the parent
            p.addElement(e)
            # add a new paragraph to this parent and link my addElement and addText to this
            np = ParagraphProxy()
            np.attributes = self.attributes
            p.addElement(np)
            self.writeto = np
        else:
            text.Element.addElement(self, e)

"""
we generate odf.text.Elements and
patch them with two specialities:
1) Element.writeto
2) ParagraphProxy(text.Element)
"""



class ODFWriter(object):
    ignoreUnknownNodes = True
    namedLinkCount = 1

    def __init__(self, env=None, status_callback=None, language="en", namespace="en.wikipedia.org", creator="", license="GFDL"):
        self.env = env
        self.status_callback = status_callback
        self.language = language
        self.namespace = namespace
        self.references = []
        self.doc =  OpenDocumentText()
        style.applyStylesToDoc(self.doc)
        self.text = self.doc.text
        self.namedLinkCount = 0
        self.conf = odfconf.OdfConf

        if creator:
            self.doc.meta.addElement(meta.InitialCreator(text=creator))
            self.doc.meta.addElement(dc.Creator(text=creator))
        if language is not None:
            self.doc.meta.addElement(dc.Language(text=language))
        if license is not None:
            self.doc.meta.addElement(meta.UserDefined(name="Rights", text=license))


    def writeTest(self, root):
        self.write(root, self.doc.text)

    def writeBook(self, book, output, removedArticlesFile=None, coverimage=None):
        """
        bookParseTree must be advtree and sent through preprocess()
        """

        if self.env and self.env.metabook:
            self.doc.meta.addElement(dc.Title(text=self.env.metabook.get("title", "")))
        #licenseArticle = self.env.metabook['source'].get('defaultarticlelicense','') # FIXME

        for e in book.children:
            self.write(e, self.doc.text)
        doc = self.getDoc()
        #doc.toXml("%s.odf.xml"%fn)
        doc.save(output, addsuffix=False)
        log( "writing to %r" % output )

    def getDoc(self, debuginfo=""):
        return self.doc

    def asstring(self, element = None):

        class Writer(object):
            def __init__(self):
                self.res = []
            def write(self, txt):
                if isinstance(txt, unicode):
                    self.res.append(str(txt))
                else:
                    self.res.append(txt)
            def getvalue(self):
                return "".join(self.res)

        s = Writer()
        if not element:
            element = self.doc.text
        element.toXml(0, s)

        return s.getvalue()

    def writeText(self, obj, parent):
        try:
            parent.addText(obj.caption)
        except odf.element.IllegalText:
            p = ParagraphProxy(stylename=style.textbody)
            try:
                parent.addElement(p)
                p.addText(obj.caption)
            except odf.element.IllegalChild:
                log("writeText:", obj, "not allowed in ", parent.type, "adding Paragraph failed")

    def write(self, obj, parent=None):
        assert parent is not None

        def saveAddChild(p,c):
            try:
                p.addElement(c)
                #print "save add child %r to %r" % (c, p)
                assert c.parentNode is not None # this check fails if the child could not be added
                return True
            except odf.element.IllegalChild:
                # fails if paragraph in span:  odfwriter >> write: u'text:p' 'not allowed in ' u'text:span' ', dumping'
                try: # maybe c has no attribute type
                    art = obj.getParentNodesByClass(advtree.Article)[0]
                    log("in article ", art.caption)
                    log("write:", c.type, "not allowed in ", p.type, ", dumping")
                except AttributeError:
                    log("missing .type attribute %r %r " %(c, p))
                return False


        while hasattr(parent, "writeto"):
            parent = parent.writeto # SPECIAL HANDLING

        # if its text, append to last node
        if isinstance(obj, parser.Text):
            self.writeText(obj, parent)
        else:
            # check for method
            m = "owrite" + obj.__class__.__name__
            m=getattr(self, m, None)

            if m: # find handler
                e = m(obj)

            elif self.ignoreUnknownNodes:
                log("Handler for node %s not found! SKIPPED" %obj.__class__.__name__)
                showNode(obj)
                e = None
            else:
                raise Exception("unknown node:%r" % obj)

            if isinstance(e, SkipChildren): # do not process children of this node
                if e.element is not None:
                    saveAddChild(parent, e.element)
                return # skip
            elif e is None:
                e = parent
            else:
                if not saveAddChild(parent, e):
                    return #

            for c in obj.children[:]:
                self.write(c,e)


    def writeChildren(self, obj, parent): # use this to avoid bugs!
        "writes only the children of a node"
        for c in obj:
            self.write(c, parent)


    def owriteArticle(self, a):
        self.references = [] # collect references
        title = a.caption
        log(u"processing article %s" % title)
        r = text.Section(stylename=style.sect, name=title) #, display="none")
        r.addElement(text.H(outlinelevel=1, stylename=style.ArticleHeader, text=title))
        return r

    def owriteChapter(self, obj):
        title = obj.caption
        item = text.Section(stylename=style.sect, name=title)
        item.addElement(text.H(outlinelevel=1, text=title, stylename=style.chapter))
        return item

    def owriteSection(self, obj):
        hXstyles = (style.h0,style.h1,style.h2,style.h3,style.h4,style.h5)

        # skip empty sections (as for eg References)
        hasDisplayContent = u"".join(x.getAllDisplayText().strip() for x in obj.children [1:]) \
            or obj.getChildNodesByClass(advtree.ImageLink) # FIXME, add AdvancedNode.hasContent property
        enabled = False
        if enabled and not hasDisplayContent:  # FIXME
            return SkipChildren()

        title = obj.children[0].getAllDisplayText()

        # = is level 0 as in article title =
        # == is level 1 as in mediawiki top level section ==
        # getSectionLevel() == 1 for most outer section level
        level = 1 + obj.getSectionLevel() # min: 1+0 = 1
        level = min(level, len(hXstyles))
        hX = hXstyles[level-1]

        r = text.Section(stylename=style.sect, name=title)
        r.addElement(text.H(outlinelevel=level, stylename=hX, text=title))
        obj.children = obj.children[1:]
        return r

    def owriteParagraph(self, obj):
        if obj.children:
            imgAsOnlyChild = bool(len(obj.children) == 1 and isinstance(obj.getFirstChild(), advtree.ImageLink))
            # handle special case nothing but an image in a paragraph
            if imgAsOnlyChild and isinstance(obj.next, advtree.Paragraph):
                img = obj.getFirstChild()
                img.moveto(obj.next.getFirstChild(), prefix=True)
                return SkipChildren()
            return ParagraphProxy(stylename=style.textbody)

    def owriteItem(self, item):
        li =text.ListItem()
        p = ParagraphProxy(stylename=style.textbody)
        li.addElement(p)
        li.writeto = p
        return li

    def owriteItemList(self, lst):
        if lst.numbered:
            return text.List(stylename=style.numberedlist)
        else:
            return text.List(stylename=style.unorderedlist)

    def owriteDefinitionList(self, obj):
        return text.List(stylename=style.definitionlist)

    def owriteDefinitionTerm(self, obj):
        li =text.ListItem()
        p = ParagraphProxy(stylename=style.definitionterm)
        li.addElement(p)
        li.writeto = p
        return li

    def owriteDefinitionDescription(self, obj):
        li = text.ListItem()
        p = ParagraphProxy(stylename=style.indentedSingle)
        li.addElement(p)
        li.writeto = p
        # FIXME, this should be handled in advtree!
        if not isinstance(obj.parent, advtree.DefinitionList):
            dl = text.List(stylename=style.definitionlist)
            dl.addElement(li)
            dl.writeto = p
            return dl

        return li


    def owriteBreakingReturn(self, obj):
        return text.LineBreak()

    def owriteCell(self, cell):
        t =  table.TableCell()
        p = ParagraphProxy(stylename=style.textbody)
        t.addElement(p)
        t.writeto = p
        # handle rowspan FIXME
        #
        #rs = cell.rowspan
        #if rs:
        #    t.setAttribute(":numberrowsspanned",str(rs))
        return t

    def owriteRow(self, row): # COLSPAN FIXME
        tr = table.TableRow()
        for c in row.children:
            cs =  c.colspan
            self.write(c,tr)
            if cs:
                tr.lastChild.setAttribute("numbercolumnsspanned",str(cs))
                for i in range(cs):
                    tr.addElement(table.CoveredTableCell())
        return SkipChildren(tr)

    def owriteCaption(self, obj):
        # are there caption not in tables ???? FIXME
        if isinstance(obj.parent, advtree.Table):
            return SkipChildren()
        pass # FIXME

    def owriteTable(self, obj): # FIXME ADD FORMATTING
        # http://books.evc-cit.info/odbook/ch04.html#text-table-section

        t = table.Table()
        tc = table.TableColumn(stylename=style.dumbcolumn,
                               numbercolumnsrepeated=str(obj.numcols)) # FIXME FIXME
        t.addElement(tc)

        captions = [c for c in obj.children if isinstance(c, advtree.Caption)]
        if not captions : # handle table w/o caption:
            return t
        else: # a section groups table-caption & table:
            if not len(captions) == 1:
                log("owriteTable: more than one Table Caption not handeled. Using only first Caption!")
            # group using a section
            sec = text.Section(stylename=style.sectTable, name="table section")
            p =  ParagraphProxy(stylename=style.tableCaption)
            sec.addElement(p)
            self.writeChildren(captions[0], p)# only one caption expected and allowed
            sec.addElement(t)
            sec.writeto=t
            return sec


# ---- inline formattings -------------------
# use span

    def owriteEmphasized(self, obj):
        return text.Span(stylename=style.emphasis)

    def owriteStrong(self, obj):
        return text.Span(stylename=style.strong)

    def owriteBold(self, obj):
        return text.Span(stylename=style.bold)

    def owriteItalic(self, obj):
        return text.Span(stylename=style.italic)

    def owriteSmall(self, obj):
        return text.Span(stylename=style.small)

    def owriteBig(self, obj):
        return text.Span(stylename=style.big)

    def owriteVar(self, obj):
        return text.Span(stylename=style.var)

    def owriteDeleted(self, obj):
        return text.Span(stylename=style.deleted)

    def owriteInserted(self, obj):
        return text.Span(stylename=style.inserted)

    def owriteRuby(self, obj):
        return text.Ruby()

    def owriteRubyText(self, obj):
        return text.RubyText()

    def owriteRubyBase(self, obj):
        return text.RubyBase()

    def owriteRubyParentheses(self, obj):
        pass # FIXME

    def owriteSub(self, obj):
        return text.Span(stylename=style.sub)

    def owriteSup(self, obj):
        return text.Span(stylename=style.sup)

    def owriteSpan(self, obj):
        return text.Span()

    def owriteOverline(self, s):
        return text.Span(stylename=style.overline)

    def owriteUnderline(self, s):
        return text.Span(stylename=style.underline)

    def owriteStrike(self, s):
        return text.Span(stylename=style.strike)

    def owriteTagNode(self, node):
        if getattr(node, 'caption', None) in ['hiero']:
            return SkipChildren()
        if getattr(node, 'caption', None) in ['abbr']:
            return self.owriteUnderline(node)


# ------- block formattings -------------------
# use paragraph

    def owriteCenter(self, s):
        return ParagraphProxy(stylename=style.center)

    def owriteCite(self, obj):
        return text.Span(stylename=style.cite)

    def owriteDiv(self, obj):
        return ParagraphProxy()

    def owriteTeletyped(self, obj):
        # (monospaced) or code, newlines ignored, spaces collapsed
        return text.Span(stylename=style.teletyped)

    def _replaceWhitespaces(self,obj, p):
        # replaces \n, \t and " " given from parser to ODF-valid tags
        # works on (styled) ParagraphProxy p
        rmap = {
            "\n":text.LineBreak,
            " ":text.S}
        col = []
        for c in obj.getAllDisplayText().replace("\t", " "*8).strip():
            if c in rmap:
                p.addText(u"".join(col))
                col = []
                p.addElement(rmap[c]())
            else:
                col.append(c)
        p.addText(u"".join(col)) # add remaining
        obj.children = []  # remove the children
        return p

    def owritePreFormatted(self, obj):
        p = ParagraphProxy(stylename=style.preformatted)
        return self._replaceWhitespaces(obj, p)

    def owriteSource(self, obj):
        p = ParagraphProxy(stylename=style.source)
        return self._replaceWhitespaces(obj, p)


    def owriteCode(self, obj):
        return text.Span(stylename=style.code)


    def owriteBlockquote(self, s):
        "margin to the left & right"
        indentlevel = len(s.caption)-1
        return ParagraphProxy(stylename=style.blockquote)


    def owriteIndented(self, s):
        "Writes a indented Paragraph. Margin to the left.\n Need a lenght of Indented.caption of 1,2 or 3."
        indentStyles = (style.indentedSingle, style.indentedDouble, style.indentedTriple)  # 0, 1, 2
        indentLevel = min(len(s.caption)-1, len(indentStyles)-1)
        return ParagraphProxy(stylename=indentStyles[indentLevel])


    def owriteMath(self, obj):
        """
        get a MATHML from Latex
        translate element tree to odf.Elements
        """
        #log("math")
        r = writerbase.renderMath(obj.caption, output_mode='mathml', render_engine='blahtexml')
        if r is None:
            log("writerbase.renderMath failed!")
            return
        #print mathml.ET.tostring(r)

        def _withETElement(e, parent):
            # translate to odf.Elements
            for c in e.getchildren():
                n = math.Element(qname=(math.MATHNS, str(c.tag)))
                parent.addElement(n)
                if c.text:
                    text = c.text
                    #if not isinstance(text, unicode):  text = text.decode("utf8")
                    n.appendChild(odf.element.Text(text)) # n.addText(c.text)
                    # rffixme: odfpy0.8 errors:"AttributeError: Element instance has no attribute 'elements'" -> this is a lie!
                _withETElement(c, n)

        mathframe = draw.Frame(stylename=style.formula, zindex=0, anchortype="as-char")
        mathobject = draw.Object()
        mathframe.addElement(mathobject)
        mroot = math.Math()
        mathobject.addElement(mroot)
        _withETElement(r, mroot)
        return mathframe

    def owriteLink(self, obj):
        a = text.A(href= obj.url or "#")
        if not obj.children:
            a.addText(obj.target)
        return a

    owriteArticleLink = owriteLink
    obwriteLangLink = owriteLink
    owriteNamespaceLink = owriteLink
    owriteInterwikiLink = owriteLink
    owriteSpecialLink = owriteLink



    def owriteURL(self, obj):
        a = text.A(href=obj.caption)
        if not obj.children:
            a.addText(obj.caption)
        return a


    def owriteNamedURL(self, obj):
        # FIXME handle references
        a = text.A(href=obj.caption)
        if not obj.children:
            name = "[%s]" % self.namedLinkCount
            self.namedLinkCount += 1
            a.addText(name)
        return a


    def owriteSpecialLink(self, obj): # whats that?
        a = text.A(href=obj.target)
        if not obj.children:
            a.addText(obj.target)
        return a

    def owriteCategoryLink(self, obj):
        if True: # FIXME, collect and add to the end of the page
            return SkipChildren()
        if not obj.colon and not obj.children:
            a = text.A(href=obj.target)
            a.addText(obj.target)
            return a

    def owriteLangLink(self, obj):
        return SkipChildren() # dont want them

    def owriteReference(self, t):
        self.references.append(t)
        n =  text.Note(noteclass="footnote")
        nc = text.NoteCitation()
        n.addElement(nc )
        nc.addText(str(len(self.references)))
        nb = text.NoteBody()
        n.addElement( nb )
        p = ParagraphProxy(stylename=style.footnote)
        nb.addElement(p)
        n.writeto = p
        return n

    def owriteReferenceList(self, t):
        # already in odf footnotes
        pass

    def owriteImageMap(self, obj):
        pass # write children # fixme

    def owriteImageLink(self,obj,isImageMap=False):
        # see http://books.evc-cit.info/odbook/ch04.html
        # see rl.writer for more advanced image integration, including inline, floating, etc.
        # http://code.pediapress.com/hg/mwlib.rl rlwriter.py

        from PIL import Image as PilImage

        def sizeImage(w,h):

            """ calculate the target image size in inch.
            @param: (w,h): w(idth), h(eight) of image in px
            @type int
            @return: (w,h): w(idth), h(eight) of target image in inch (!)
            @rtype float"""


            if obj.isInline:
                scale = 1 / self.conf.paper['IMG_DPI_STANDARD']
            else:
                scale = 1 / self.conf.paper['IMG_DPI_INLINE']

            wTarget = scale * w # wTarget is in inch now
            hTarget = scale * h # hTarget is in inch now

            ##2do: obey the value of thumpnail
            if wTarget > self.conf.paper['IMG_MAX_WIDTH'] or hTarget > self.conf.paper['IMG_MAX_HEIGHT']:
                # image still to large, re-resize to max possible:
                scale = min(self.conf.paper['IMG_MAX_WIDTH']/w, self.conf.paper['IMG_MAX_HEIGHT']/h)

                return (w*scale, h*scale, scale)
            else:
                return (wTarget, hTarget, scale)

        if obj.colon == True:
            return # writes children
            #fixme: handle isImageMap

        if not self.env or not self.env.images:
            return
            #fixme: handle isImageMap

        imgPath = self.env.images.getDiskPath(obj.target)#, size=targetWidth) ????
        if not imgPath:
            log.warning('invalid image url')
            return
        imgPath = imgPath.encode('utf-8')

        (wObj,hObj) = (obj.width or 0, obj.height or 0)
        # sometimes the parser delivers only one value, w or h, so set the other = 0

        try:
            img = PilImage.open(imgPath)
            if img.info.get('interlace',0) == 1:
                log.warning("got interlaced PNG which can't be handeled by PIL")
                return
        except IOError:
            log.warning('img can not be opened by PIL')
            return

        (wImg,hImg) = img.size

        if wImg == 0 or wImg == 0:
            return

        # sometimes the parser delivers only one value, w or h, so set the other "by hand"
        aspectRatio = wImg/hImg

        if wObj>0 and not hObj>0:
            hObj = wObj / aspectRatio
        elif hObj>0 and not wObj>0:
            wObj = aspectRatio / hObj
        elif wObj==0 and hObj==0:
            wObj, hObj = wImg, hImg
        #hint: wObj/hObj are the values of the Thumbnail
        #      wImg/hImg are the real values of the image

        (width, height, scale) = sizeImage( wObj, hObj)


        widthIn = "%.2fin" % (width)
        heightIn= "%.2fin" % (height)

        innerframe = draw.Frame(stylename=style.frmInner, width=widthIn, height=heightIn)

        if isImageMap:
            innerframe.wImg = wImg
            innerframe.hImg = hImg
            innerframe.rescaleFactor = scale # needed cuz image map coordinates needs the same rescaled
            log ("wObj ,wImg: %s,%s" %(wObj,wImg))

        href = self.doc.addPicture(imgPath)
        innerframe.addElement(draw.Image(href=href))

        if obj.isInline():
                return SkipChildren(innerframe) # FIXME something else formatting?
        else:
            innerframe.setAttribute( "anchortype", "paragraph")


        widthIn = "%.2fin" % (width + style.frmOuter.internSpacing)
        heightIn= "%.2fin" % (height)

        # set image alignment
        attrs = dict(width=widthIn, anchortype="paragraph")
        floats = dict(right  = style.frmOuterRight,
                      center = style.frmOuterCenter,
                      left   = style.frmOuterLeft)
        attrs["stylename"] = floats.get(obj.align, style.frmOuterLeft)
        stylename=style.frmOuterLeft,
        frame = draw.Frame(**attrs)

        tb = draw.TextBox()
        frame.addElement(tb)
        p = ParagraphProxy(stylename=style.imgCaption)
        tb.addElement(p)
        p.addElement(innerframe)
        frame.writeto = p
        if isImageMap:
            frame.writeImageMapTo = innerframe
        return frame

    def owriteFont(self, n):
        pass # simply write children

    def owriteNode(self, n):
        pass # simply write children

    def owriteGallery(self, obj):
        pass # simply write children FIXME

    def owriteHorizontalRule(self, obj):
        p = ParagraphProxy(stylename=style.hr)
        return p


# UNIMPLEMENTED  -----------------------------------------------

    def writeTimeline(self, obj):
        data = obj.caption
        pass # FIXME

    def writeHiero(self, obj): # FIXME parser support
        data = obj.caption
        pass # FIXME



# - func  ---------------------------------------------------


def writer(env, output, status_callback):
    if status_callback:
        buildbook_status = status_callback.getSubRange(0, 50)
    else:
        buildbook_status = None
    book = writerbase.build_book(env, status_callback=buildbook_status)
    scb = lambda status, progress :  status_callback is not None and status_callback(status=status, progress=progress)
    scb(status='preprocessing', progress=50)
    preprocess(book)
    scb(status='rendering', progress=60)
    w = ODFWriter(env, status_callback=scb)
    w.writeBook(book, output=output)

writer.description = 'OpenDocument Text'
writer.content_type = 'application/vnd.oasis.opendocument.text'
writer.file_extension = 'odt'


# - helper funcs   r ---------------------------------------------------

def preprocess(root):
    #advtree.buildAdvancedTree(root)
    #xmltreecleaner.removeChildlessNodes(root)
    #xmltreecleaner.fixLists(root)
    #xmltreecleaner.fixParagraphs(root)
    #xmltreecleaner.fixBlockElements(root)
    #print"*** parser raw "*5
    #parser.show(sys.stdout, root)
    #print"*** new TreeCleaner "*5
    advtree.buildAdvancedTree(root)
    tc = TreeCleaner(root)
    tc.cleanAll()
    #parser.show(sys.stdout, root)

# ==============================================================================

def main():
    for fn in sys.argv[1:]:

        from mwlib.dummydb import DummyDB
        from mwlib.uparser import parseString
        db = DummyDB()
        input = unicode(open(fn).read(), 'utf8')
        r = parseString(title=fn, raw=input, wikidb=db)
        #parser.show(sys.stdout, r)
        #advtree.buildAdvancedTree(r)
        #tc = TreeCleaner(r)
        #tc.cleanAll()


        preprocess(r)
        parser.show(sys.stdout, r)
        odf = ODFWriter()
        odf.writeTest(r)
        doc = odf.getDoc()
        #doc.toXml("%s.xml"%fn)
        doc.save(fn, True)


if __name__=="__main__":
    main()

########NEW FILE########
__FILENAME__ = old_uparser
#! /usr/bin/env python

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

from mwlib import parser

def simplify(node, **kwargs):
    "concatenates textnodes in order to reduce the number of objects"
    Text = parser.Text
    
    last = None
    toremove = []
    for i,c in enumerate(node.children):
        if c.__class__ == Text: # would isinstance be safe?
            if last:
                last.caption += c.caption
                toremove.append(i)
            else:
                last = c
        else:
            simplify(c)
            last = None

    for i,ii in enumerate(toremove):
        del node.children[ii-i]


def removeBoilerplate(node, **kwargs):
    i = 0
    while i < len(node.children):
        x = node.children[i]
        if isinstance(x, parser.TagNode) and x.caption=='div':
            try:
                klass = x.values.get('class', '')
            except AttributeError:
                klass = ''
                
            if 'boilerplate' in klass:
                del node.children[i]
                continue
            
        i += 1

    for x in node.children:
        removeBoilerplate(x)


postprocessors = [removeBoilerplate, simplify]

########NEW FILE########
__FILENAME__ = options
import sys
import optparse

from mwlib import myjson as json

from mwlib.utils import start_logging
from mwlib import wiki, metabook, log

log = log.Log('mwlib.options')


        
class OptionParser(optparse.OptionParser):
    def __init__(self, usage='%prog [OPTIONS] [ARTICLETITLE...]'):
        self.config_values = []
        
        optparse.OptionParser.__init__(self, usage=usage)
        
        self.metabook = None
        
        a = self.add_option
        
        a("-c", "--config", action="callback", nargs=1, type="string", callback=self._cb_config, 
          help="configuration file, ZIP file or base URL")
        
        a("-i", "--imagesize",
          default=1200,
          help="max. pixel size (width or height) for images (default: 1200)")
        
        a("-m", "--metabook",
          help="JSON encoded text file with article collection")
        
        a("--collectionpage", help="Title of a collection page")
        
        a("-x", "--noimages", action="store_true",
          help="exclude images")
        
        a("-l", "--logfile", help="log to logfile")

        a("--username", help="username for login")
        a("--password", help="password for login")
        a("--domain",  help="domain for login")

        a("--title",
          help="title for article collection")
        
        a("--subtitle",
          help="subtitle for article collection")
        
        a("--editor",
          help="editor for article collection")
        
        a("--script-extension",
          default=".php",
          help="script extension for PHP scripts (default: .php)")
        
        
    def _cb_config(self, option, opt, value, parser):
        """handle multiple --config arguments by resetting parser.values and storing
        the old value in parser.config_values""" 
        
        import copy
        
        config_values = parser.config_values
        
        if not config_values:
            parser.config_values.append(parser.values)


        config_values[-1] = copy.deepcopy(config_values[-1])

        if parser.values.config:
            del parser.largs[:]        
        
        parser.values.__dict__ = copy.deepcopy(config_values[0].__dict__)

        config_values.append(parser.values)


        parser.values.config = value
        parser.values.pages = parser.largs

        
    def parse_args(self):
        self.options, self.args = optparse.OptionParser.parse_args(self, args=[unicode(x, "utf-8") for x in sys.argv[1:]])
        for c in self.config_values:
            if not hasattr(c, "pages"):
                c.pages = []
            
        if self.options.logfile:
            start_logging(self.options.logfile)
        
        if self.options.metabook:
            self.metabook = json.loads(unicode(open(self.options.metabook, 'rb').read(), 'utf-8'))
        
        try:
            self.options.imagesize = int(self.options.imagesize)
            assert self.options.imagesize > 0
        except (ValueError, AssertionError):
            self.error('Argument for --imagesize must be an integer > 0.')
        
        for title in self.args:
            if self.metabook is None:
                self.metabook = metabook.collection()
            
            self.metabook.append_article(title)

        return self.options, self.args
    
    def makewiki(self):
        kw = self.options.__dict__.copy()
        kw["metabook"] = self.metabook
        
        env = wiki.makewiki(**kw)
        
        if not env.metabook:
            self.metabook = env.metabook = metabook.collection()
            env.init_metabook()
            
        if self.options.noimages:
            env.images = None

        def setmb(name):
            n = getattr(self.options, name)
            if n:
                env.metabook[name] = n

        setmb("title")
        setmb("subtitle")
        setmb("editor")

        # add default licenses
        cfg = self.options.config or ""
        
        if cfg.startswith(":") and not env.metabook.licenses:
            mw_license_url = wiki.wpwikis.get(cfg[1:])['mw_license_url']
            env.metabook.licenses.append(dict(mw_license_url=mw_license_url,
                                              type="license"))

        return env

########NEW FILE########
__FILENAME__ = nodes

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

from mwlib import utoken

class Node(utoken.token):
    """Base class for all nodes"""
    
    caption = ''

    def __init__(self, caption=''):
        self.children = []
        self.caption = caption

    def __iter__(self):
        for x in self.children:
            yield x

    def __repr__(self):
        try:
            return utoken.token.__repr__(self)
        except:
            return "%s %r: %s children" % (self.__class__.__name__, self.caption, len(self.children))

    def __eq__(self, other):
        return (isinstance(other, self.__class__)
                and self.caption == other.caption
                and self.children == other.children)

    def __ne__(self, other):
        return not(self==other)

    def allchildren(self): # name is broken, returns self, which is not a child
        yield self 
        for c in self.children:
            for x in c.allchildren():
                yield x        

    def find(self, tp):
        """find instances of type tp in self.allchildren()"""
        return [x for x in self.allchildren() if isinstance(x, tp)]


    def filter(self, fun):
        for x in self.allchildren():
            if fun(x):
                yield x

    def _asText(self, out):
        out.write(self.caption)
        for x in self.children:
            x._asText(out)
        
    def asText(self, ):
        from StringIO import StringIO
        out = StringIO()
        self._asText(out)
        return out.getvalue()
                    
class Math(Node): pass
class Ref(Node): pass
class Item(Node): pass
class ItemList(Node):
    numbered = False

class Style(Node):
    """Describes text styles like italics, bold etc. The type of the style is
    contained in the caption attribute. The styled text is contained in
    children.
    
    The attribute caption can have the following values:
     * "''" for italic text
     * ''' for bold text
     * ":", "::", ... for indented text (number of ":"'s == indentation level)
     * ";" for a definition description term
    """

class Book(Node):
    pass

class Chapter(Node):
    pass
    
class Article(Node):
    url = None
    
class Paragraph(Node):
    pass

class Section(Node):
    """A section heading
    
    The level attribute contains the level of the section heading, i.e.::
    
       = test =    level=1
       == test ==  level=2
       etc.
    
    The first element in children contains the caption of the section as a Node
    instance with 0 or more children. Subsequent children are elements following
    the section heading.
    """

class Timeline(Node):
    """A <timeline> tag"""

class TagNode(Node):
    """Some tag i.e. <TAGNAME>...</TAGNAME>
    
    The caption attribute contains the tag name, e.g. 'br', 'pre', 'h1' etc.
    
    The children attribute contains the elements contained inside the opening and
    closing tag.
    
    Wikitext::
    
        Some text<br/>
        Some more text
    """

class PreFormatted(TagNode):
    """Preformatted text, encapsuled in <pre>...</pre>
    
    Wikitext::
    
       <pre>
         Some preformatted text
       </pre>
    """

class URL(Node):
    """A (external) URL, which can be a http, https, ftp or mailto URL
    
    The caption attribution contains the URL
    
    Wikitext::
    
       http://example.com/
       mailto:test@example.com
       ftp://example.com/
    """
    
class NamedURL(Node): 
    """A (potentially) named URL
    
    The caption attribute contains the URL. The children attribute contains the
    nodes making up the name of the URL (children can be empty if no name is
    specified).
    
    Wikitext::
    
       [http://example.com/ This is the name of the URL]
       [http://example.com/]
    """

class Table(Node):
    pass

class Row(Node):
    pass

class Cell(Node):
    pass

class Caption(Node):
    pass

class Link(Node):
    """Base class for all "wiki links", i.e. *not* URLs.
    
    All links are further specialized to some subclass of Link depending on the
    link prefix (usually the part before the ":").
    
    The target attribute contains the link target with the prefix stripped off.
    The full_target attribute contains the full link target (but with a potential
    leading ":" stripped off).
    The colon attribute is set to True if the original link target is prefixed
    with a ":".
    The url attribute can contain a valid HTTP URL. If the resolving didn't work
    this attribute is None.
    The children attribute can contain the nodes making up the name of the link.
    The namespace attribute is either set to one of the constants NS_... defined
    in mwlib.namespace (int) or to the prefix of the link (unicode).
    
    Wikitext::
    
      [[Some article|An article]]
      
    This Link would be specialized to an ArticleLink instance. The target attribute
    and the full_target attribute would both be u'Some article', the namespace
    attribute would be NS_MAIN (0) and the children attribute would contain a Text
    node with caption=u'An article'.
    
    Wikitext::
    
      [[Image:Bla.jpg]]
      
    This Link would be specialized to an ImageLink instance. The target attribute
    would be u'Bla.jpg', the full_target attribute would be u'Image:Bla.jpg' and
    the children attribute would be empty.
    """
    
    target = None
    colon = False
    url = None

    

    capitalizeTarget = False # Wiki-dependent setting, e.g. Wikipedia => True


# Link forms:

class ArticleLink(Link):
    """A link to an article
    
    Wikitext::
    
       [[An article]]
       [[An article|Some other text]]
    """

class SpecialLink(Link):
    """Base class for NamespaceLink and InterwikiLink
    
    A link with a prefix, which is *not* a CategoryLink or ImageLink.
    """

class NamespaceLink(SpecialLink):
    """A SpecialLink which has not been recognized as an InterwikiLink
    
    The namespace attribute contains the namespace prefix.
    """

class InterwikiLink(SpecialLink):
    """An 'interwiki' link, i.e. a link into another MediaWiki
    
    The namespace attribute is set to the interwiki prefix.
    
    Wikitext::
    
       [[wikibooks:Some article in wikibook]]
    """

# Non-links with same syntax:

class LangLink(Link):
    """A language link. This is essentially an interwiki link to a MediaWiki
    installation in a different language.
    
    The namesapce attribute is set to the language prefix.
    
    Wikitext::
    
      [[de:Ein Artikel]]
    """

class CategoryLink(Link):
    """A category link, i.e. a link that assigns the containing article
    to the given category.
    
    Wikitext::
    
      [[Category:Bla]]
    
    Note that links of the form [[:Category:Bla]] are *not* CategoryLinks,
    but SpecialLinks with namespace=NS_CATEGORY (14)!
    """

class ImageLink(Link):
    """An image link.
    
    The children attributes potentially contains the nodes making up the image
    caption.
    
    The following attributes are parsed from the wikitext and set accordingly
    (if present, otherwise None):
     * width: width in pixels (int)
     * height: height in pixles (int)
     * align: one of the strings 'left', 'right', 'center', 'none'
     * thumb: set to True if given
    """
    
    target = None
    width = None
    height = None
    align = ''
    thumb = False
    printargs = None
    alt = None
    link = None
    frame = None
    border = None
    upright = None
    
    def isInline(self):
        return not bool(self.align or self.thumb or self.frame)
            
class Text(Node):
    """Plain text
    
    The caption attribute contains the text (unicode),
    the children attribute is always the empty list.
    """
    
    def __repr__(self):
        return repr(self.caption)
    
    def __init__(self, txt):
        self.caption = txt
        self.children = []
    
class Control(Text):
    pass

########NEW FILE########
__FILENAME__ = styleanalyzer

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

# '''bold'''
# ''italic''

class state(object):
    def __init__(self, **kw):
        self.__dict__.update(kw)

    def clone(self, **kw):
        s=state(**self.__dict__)
        s.__dict__.update(kw)
        return s
    
    def __repr__(self):
        res = ["<state "]
        res.append(" %s " % self.apocount)
        if self.is_bold:
            res.append("bold ")
        if self.is_italic:
            res.append("italic ")
        
        res.append(">")
        return "".join(res)
    
    def get_next(self, count, res=None, previous=None):
        if previous is None:
            previous = self
            
        if res is None:
            res=[]

        def nextstate(**kw):
            cl = self.clone(previous=previous, **kw)
            res.append(cl)

        assert count>=2, "internal error"
        
        if count==2:
            nextstate(is_italic=not self.is_italic)
            
        if count==3:
            nextstate(is_bold=not self.is_bold)

            s=self.clone(apocount=self.apocount+1, previous=previous)
            
            s.get_next(2, res, previous=previous)
            

        if count==4:
            s=self.clone(apocount=self.apocount+1)
            s.get_next(3, res, previous=previous)
            
        if count==5:
            for x in self.get_next(2):
                x.get_next(3, res, previous=previous)
            for x in self.get_next(3):
                x.get_next(2, res, previous=previous)

            
            s=self.clone(apocount=self.apocount)
            s.get_next(4, res, previous=previous)


        if count>5:
            s = self.clone(apocount=self.apocount+(count-5))
            s.get_next(5, res, previous=previous)
            
        return res

def sort_states(states):
    tmp = [((x.apocount+x.is_bold+x.is_italic), x) for x in states]
    tmp.sort()
    return [x[1] for x in tmp]
    
def compute_path(counts):
    states = [state(is_bold=False, is_italic=False, previous=None, apocount=0)]
    
    for count in counts:
        new_states = []
        for s in states:
            s.get_next(count, new_states)
        states = new_states
        states = sort_states(states)
        best = states[0]
        #print "STATES:", states
        if best.apocount==0 and not best.is_italic and not best.is_bold:
            #print "CHOOSING PERFECT STATE"
            states = [best]
        else:
            states = states[:32]
            
    tmp = states[0]

    res = []
    while tmp.previous is not None:
        res.append(tmp)
        tmp = tmp.previous

    res.reverse()
    assert len(res)==len(counts)
    return res

########NEW FILE########
__FILENAME__ = parse_collection_page
import os, re, binascii
from collections import defaultdict
from mwlib import metabook, expander

uniq = "--%s--" % binascii.hexlify(os.urandom(16))

def extract_metadata(raw, fields, template_name="saved_book"):
    fields = list(fields)
    fields.append("")

    templ = "".join(u"%s%s\n{{{%s|}}}\n" % (uniq, f, f) for f in fields)
    db = expander.DictDB({template_name:templ})

    te = expander.Expander(raw, pagename="", wikidb=db)
    res = te.expandTemplates()

    d = defaultdict(unicode)
    for x in res.split(uniq)[1:-1]:
        name, val = x.split("\n", 1)
        val = val.strip()
        d[name] = val

    return d



def _buildrex():
    title_rex = '^==(?P<title>[^=].*?[^=])==$'
    subtitle_rex = '^===(?P<subtitle>[^=].*?[^=])===$'
    chapter_rex = '^;(?P<chapter>.+?)$'
    article_rex = '^:\[\[:?(?P<article>.+?)(?:\|(?P<displaytitle>.*?))?\]\]$'
    oldarticle_rex = '^:\[\{\{fullurl:(?P<oldarticle>.+?)\|oldid=(?P<oldid>.*?)\}\}(?P<olddisplaytitle>.*?)\]$'
    template_rex = '^\{\{(?P<template>.*?)\}\}$'
    template_start_rex = '^(?P<template_start>\{\{)$'
    template_end_rex = '.*?(?P<template_end>\}\})$'
    summary_rex = '(?P<summary>.*)'
    alltogether_rex = re.compile("(%s)|(%s)|(%s)|(%s)|(%s)|(%s)|(%s)|(%s)|(%s)" % (
        title_rex, subtitle_rex, chapter_rex, article_rex, oldarticle_rex,
        template_rex, template_start_rex, template_end_rex, summary_rex,
    ))
    return alltogether_rex


alltogether_rex = _buildrex()

def parse_collection_page(wikitext):
    """Parse wikitext of a MediaWiki collection page created by the Collection
    extension for MediaWiki.
    
    @param wikitext: wikitext of a MediaWiki collection page
    @type mwcollection: unicode
    
    @returns: metabook.collection
    @rtype: metabook.collection
    """
    mb = metabook.collection()

    
    summary = False
    noTemplate = True
    for line in wikitext.splitlines():
        line = line.strip()
        if not line:
            continue
        res = alltogether_rex.search(line)
        if not res:
            continue
        
        #look for initial templates and summaries
        #multilinetemplates need different handling to those that fit into one line
        if res.group('template_end') or res.group('template'):
            summary = True
            noTemplate = False
        elif res.group('template_start'):
            noTemplate = False
        elif res.group('summary'):
            pass
        else:
            summary = False
            noTemplate = False

        if res.group('title'):
            mb.title = res.group('title').strip()
        elif res.group('subtitle'):
            mb.subtitle = res.group('subtitle').strip()
        elif res.group('chapter'):
            mb.items.append(metabook.chapter(title=res.group('chapter').strip()))
        elif res.group('article'):
            mb.append_article(res.group('article'), res.group('displaytitle'))
        elif res.group('oldarticle'):
            mb.append_article(title=res.group('oldarticle'), displaytitle=res.group('olddisplaytitle'), revision=res.group('oldid'))
        elif res.group('summary') and (noTemplate or summary):
            mb.summary += res.group('summary') + " "

    return mb

########NEW FILE########
__FILENAME__ = podclient
"""Client to a Print-on-Demand partner service (e.g. pediapress.com)"""

import os, time, urlparse, urllib, urllib2, httplib

try:
    import simplejson as json
except ImportError:
    import json

from mwlib.log import Log
from mwlib.utils import get_multipart
from mwlib import conf

log = Log("mwapidb")

class PODClient(object):
    def __init__(self, posturl, redirecturl=None):
        self.posturl = posturl.encode('utf-8')
        self.redirecturl = redirecturl
    
    def _post(self, data, content_type=None):
        if content_type is not None:
            headers = {'Content-Type': content_type}
        else:
            headers = {}
        return urllib2.urlopen(urllib2.Request(self.posturl, data, headers=headers)).read()
    
    def post_status(self, status=None, progress=None, article=None, error=None):
        post_data = {}

        def setv(name, val):
            if val is None:
                return
            if not isinstance(val, str):
                val = val.encode("utf-8")
            post_data[name] = val

        setv("status", status)
        setv("error", error)
        setv("article", article)

        if progress is not None:
            post_data['progress'] = '%d' % progress

        self._post(urllib.urlencode(post_data))

    def streaming_post_zipfile(self, filename, fh=None):
        if fh is None:
            fh = open(filename, "rb")
            
        boundary = "-"*20 + ("%f" % time.time()) + "-"*20
        
        items = []
        items.append("--" + boundary)
        items.append('Content-Disposition: form-data; name="collection"; filename="collection.zip"')
        items.append('Content-Type: application/octet-stream')
        items.append('')
        items.append('')

        before = "\r\n".join(items)

        items = []
        items.append('')
        items.append('--' + boundary + '--')
        items.append('')
        after = "\r\n".join(items)
        
        clen = len(before)+len(after)+os.path.getsize(filename)
        
        print "POSTING TO:", self.posturl
            
        pr = urlparse.urlparse(self.posturl)
        path = pr.path
        if pr.query:
            path += "?"+pr.query
            
        h = httplib.HTTP(pr.hostname, pr.port)
        h.putrequest("POST", path)
        h.putheader("Host", pr.netloc)
        h.putheader("Content-Length", str(clen))
        h.putheader("User-Agent", conf.user_agent)
        h.putheader("Content-Type", "multipart/form-data; boundary=%s" % boundary)
        h.endheaders()
        
        h.send(before)

        while 1:
            data = fh.read(4096)
            if not data:
                break
            h.send(data)
        
        h.send(after)
        
        errcode, errmsg, headers = h.getreply()
        # h.file.read()
        print "ERRCODE:", (errcode, errmsg, headers)
        
        if errcode!=200:
            raise RuntimeError("upload failed: %r" % (errmsg,))
        
        
    def post_zipfile(self, filename):
        f = open(filename, "rb")
        content_type, data = get_multipart('collection.zip', f.read(), 'collection')
        f.close()
        log.info('POSTing zipfile %r to %s (%d Bytes)' % (filename, self.posturl, len(data)))
        self._post(data, content_type=content_type)

def podclient_from_serviceurl(serviceurl):
    result = json.loads(unicode(urllib2.urlopen(serviceurl, data="any").read(), 'utf-8'))
    return PODClient(result["post_url"], redirecturl=result["redirect_url"])

########NEW FILE########
__FILENAME__ = postman
#! /usr/bin/env python

import gevent, gevent.monkey
gevent.monkey.patch_all()

import os, sys, getpass, socket, traceback, StringIO

from mwlib.podclient import PODClient
from mwlib.status import Status
from mwlib.utils import send_mail

cachedir = "cache"

def get_collection_dir(collection_id):
    return os.path.join(cachedir, collection_id[:2], collection_id)

    

def _get_args(writer_options=None,
              language=None,
              zip_only=False, 
              **kw):
    
    args = []

    if zip_only:
        return args
    
    if writer_options:
        args.extend(['--writer-options', writer_options])

    if language:
        args.extend(['--language', language])

    return args

def uploadfile(ipath, posturl, fh=None):
    if fh is None:
        fh = open(ipath, "rb")
    
    podclient = PODClient(posturl)

    status = Status(podclient=podclient)
    
    try:
        status(status='uploading', progress=0)
        podclient.streaming_post_zipfile(ipath, fh)
        status(status='finished', progress=100)
    except Exception, err:
        status(status='error')
        raise err


    
def report_upload_status(posturl, fh):
    podclient = PODClient(posturl)
    
    fh.seek(0, 2)
    size = fh.tell()
    fh.seek(0, 0)

    status = Status(podclient=podclient)
    numdots = 0
    
    last = None
    while 1:
        cur = fh.tell()
        if cur != last:
            if cur==size:
                break
            numdots = (numdots + 1) % 10
            status("uploading"+"."*numdots,  progress=100.0*cur/size)
            last = cur
                
        else:
            gevent.sleep(0.1)

def report_mwzip_status(posturl, jobid, host, port):
    podclient = PODClient(posturl)
    status = Status(podclient=podclient)
    
    from mwlib.async import rpcclient
    sp = rpcclient.serverproxy(host, port)

    last = {}
    while 1:
        res = sp.qinfo(jobid=jobid) or {}
        
        done = res.get("done", False)
        if done:
            break
        info = res.get("info", {})
        if info!=last:
            status(status=info.get("status", "fetching"),
                   progress=info.get("progress", 0.0))
            last = info
        else:
            gevent.sleep(0.5)
        
def report_exception(posturl, (tp, err, tb)):
    print "reporting error to", posturl, repr(str(err)[:50])

    podclient = PODClient(posturl)
    podclient.post_status(error=str(err))

mailfrom = "%s@%s" % (getpass.getuser(), socket.gethostname())

def report_exception_mail(subject, exc_info):
    mailto = os.environ.get("MAILTO")
    if not mailto:
        print "MAILTO not set. not sending email."
        return

    print "sending mail to", mailto

    f=StringIO.StringIO()
    traceback.print_exception(*exc_info, file=f)

    send_mail(mailfrom, [mailto], subject, f.getvalue())


class commands(object):
    def statusfile(self):
        host = self.proxy._rpcclient.host
        port = self.proxy._rpcclient.port
        return 'qserve://%s:%s/%s' % (host, port, self.jobid)
    
    def rpc_post(self, params):
        post_url = params["post_url"]

        def _doit(metabook_data=None, collection_id=None, base_url=None, post_url=None, **kw):
            dir = get_collection_dir(collection_id)
            def getpath(p):
                return os.path.join(dir, p)

            jobid = "%s:makezip" % (collection_id, )
            g=gevent.spawn_later(0.2, report_mwzip_status, post_url, jobid, self.proxy._rpcclient.host, self.proxy._rpcclient.port)

            try:
                self.qaddw(channel="makezip", payload=dict(params=params), jobid=jobid, timeout=20 * 60)
            finally:
                g.kill()
                del g
                
            ipath = getpath("collection.zip")
            fh = open(ipath, "rb")
            
            g=gevent.spawn(report_upload_status, post_url, fh)
            try:
                uploadfile(ipath, post_url, fh)
            finally:
                g.kill()
                del g

        def doit(**params):
            try:
                return _doit(**params)
            except Exception:
                exc_info = sys.exc_info()
                gevent.spawn(report_exception, post_url, exc_info)
                gevent.spawn(report_exception_mail, "zip upload failed", exc_info)
                del exc_info
                raise

        return doit(**params)

def main():
    global cachedir
    from mwlib import argv
    opts, args = argv.parse(sys.argv[1:], "--cachedir=")
    for o, a in opts:
        if o=="--cachedir":
            cachedir = a
        
    from mwlib.async import slave
    
    slave.main(commands, numgreenlets=32, argv=args)
    
if __name__=="__main__":        
    main()

########NEW FILE########
__FILENAME__ = compat

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

from mwlib.refine import core
from mwlib.parser import nodes as N
from mwlib.utoken import token as T
from mwlib import nshandling


tok2class = {
    T.t_complex_table : N.Table,
    T.t_complex_caption: N.Caption,
    T.t_complex_table_row: N.Row,
    T.t_complex_table_cell: N.Cell,
    T.t_complex_link: N.Link,
    T.t_complex_section: N.Section,
    T.t_complex_article: N.Article,
    T.t_complex_tag: N.TagNode,
    T.t_complex_named_url: N.NamedURL,
    T.t_complex_style: N.Style,
    T.t_complex_node: N.Node,
    T.t_complex_line: N.Node,
    T.t_http_url: N.URL,
    T.t_complex_preformatted: N.PreFormatted,
    }


        

def _change_classes(node):
    if isinstance(node, T):
        if node.type==T.t_complex_table and node.children:

            node.children = [x for x in node.children if x.type in (T.t_complex_table_row, T.t_complex_caption) or x.tagname == "caption"]
        elif node.type==T.t_complex_table_row and node.children:
            node.children = [x for x in node.children if x.type==T.t_complex_table_cell]

        if node.type==T.t_http_url:
            node.caption = node.text
            node.children=[]

        if node.type==T.t_complex_compat:
            node.__class__ = node.compatnode.__class__
            node.__dict__ = node.compatnode.__dict__
            return
        
        if node.type==T.t_magicword:
            node.caption = u""
            node.children = []
            node.__class__=N.Text
            return

        if node.type==T.t_html_tag_end:
            node.caption = u""
            node.children = []
            node.__class__=N.Text
            return
            
        klass = tok2class.get(node.type, N.Text)
        
            
        if klass==N.Text:
            node.caption=node.text or u""
            assert not node.children, "%r has children" % (node,)
            
        node.__class__=klass
        
        if node.type==T.t_hrule or (node.type in (T.t_html_tag, T.t_html_tag_end) and node.rawtagname=='hr'):
            node.__class__=N.TagNode
            node.caption = "hr"
            
        if node.rawtagname=='br':
            node.__class__=N.TagNode
            node.caption="br"

        if node.type==T.t_complex_style:
            node.__class__=N.Style
            
        if node.__class__==N.Text:
            node.caption=node.text or u""
            assert not node.children, "%r has children" % (node,)
            
            
        if node.children is None:
            node.children = []
            
        if node.vlist is None:
            node.vlist = {}
        if node.type==T.t_complex_tag:
            node.caption = node.tagname
            if node.tagname=='p':
                node.__class__=N.Paragraph
            elif node.tagname=='caption':
                node.__class__=N.Caption
            elif node.tagname=='ref':
                pass
                #node.__class__=N.Ref
            elif node.tagname=='ul':
                node.__class__=N.ItemList
            elif node.tagname=='ol':
                node.__class__=N.ItemList
                node.numbered=True
            elif node.tagname=='li':
                node.__class__=N.Item
            elif node.tagname=="timeline":
                node.__class__=N.Timeline
                node.caption = node.timeline
            elif node.tagname=="imagemap":
                if node.imagemap.imagelink:
                    _change_classes(node.imagemap.imagelink)
            elif node.tagname=="math":
                node.__class__=N.Math
                node.caption = node.math
            elif node.tagname=='b':
                node.__class__=N.Style
                node.caption = "'''"
            elif node.tagname=='pre':
                node.__class__=N.PreFormatted
            elif node.tagname=='blockquote':
                node.__class__=N.Style
                node.caption = "-"
            elif node.tagname=="strong":
                node.__class__=N.Style
                node.caption = "'''"
            elif node.tagname=="cite":
                node.__class__=N.Style
                node.caption="cite"
            elif node.tagname=="big":
                node.__class__=N.Style
                node.caption="big"
            elif node.tagname=="small":
                node.__class__=N.Style
                node.caption="small"
            elif node.tagname=="s":
                node.__class__=N.Style
                node.caption="s"                
            elif node.tagname=="var":
                node.__class__=N.Style
                node.caption="var"
            elif node.tagname=="i":
                node.__class__=N.Style
                node.caption="''"
            elif node.tagname=="em":
                node.__class__=N.Style
                node.caption="''"
            elif node.tagname=="sup":
                node.__class__=N.Style
                node.caption="sup"
            elif node.tagname=="sub":
                node.__class__=N.Style
                node.caption="sub"
            elif node.tagname=="u":
                node.__class__=N.Style
                node.caption=="u"
                
        if node.__class__==N.Link:
            ns = node.ns
            
            if node.colon:
                ns = nshandling.NS_SPECIAL
                
            if ns==nshandling.NS_IMAGE:
                node.__class__ = N.ImageLink
            elif ns==nshandling.NS_MAIN:
                node.__class__ = N.ArticleLink
            elif ns==nshandling.NS_CATEGORY:
                node.__class__ = N.CategoryLink
            elif ns is not None:
                node.__class__ = N.NamespaceLink
            elif node.langlink:
                node.__class__ = N.LangLink
                node.namespace = node.target.split(":", 1)[0]
            elif node.interwiki:
                node.__class__ = N.InterwikiLink
                node.namespace = node.interwiki

            ns, partial, full = node.nshandler.splitname(node.target)
            if node.namespace is None:
                node.namespace = node.ns
            
                
            
        node = node.children
        
    if node:
        for x in node:
            _change_classes(x)

    
    
def parse_txt(raw, **kwargs):
    sub = core.parse_txt(raw, **kwargs)
    article = T(type=T.t_complex_article, start=0, len=0, children=sub)
    _change_classes(article)
    return article

########NEW FILE########
__FILENAME__ = core
#! /usr/bin/env python
# -*- compile-command: "../../tests/test_refine.py" -*-

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

from mwlib.utoken import tokenize, show, token as T, walknode, walknodel
from mwlib.refine import util
from mwlib import tagext, uniq, nshandling

from mwlib.refine.parse_table import parse_tables, parse_table_cells, parse_table_rows, fix_tables, remove_table_garbage
from mwlib.refine.tagparser import tagparser

try:
    from mwlib.refine import _core
except ImportError:
    _core =  None

T.t_complex_table = "complex_table"
T.t_complex_caption = "complex_caption"
T.t_complex_table_row = "complex_table_row"
T.t_complex_table_cell = "complex_table_cell"
T.t_complex_tag = "complex_tag"
T.t_complex_link = "link"
T.t_complex_section = "section"
T.t_complex_article = "article"
T.t_complex_indent = "indent"
T.t_complex_line = "line"
T.t_complex_named_url = "named_url"
T.t_complex_style = "style"
T.t_complex_node = "node"
T.t_complex_preformatted = "preformatted"
T.t_complex_compat = "compat"

T.t_vlist = "vlist"

T.children = None

def get_token_walker(skip_tags=set()):
    def walk(tokens):
        res =  [tokens]
        todo = [tokens]
        
        while todo:
            for x in todo.pop():
                children = x.children
                if children:
                    todo.append(children)
                    if x.tagname not in skip_tags:
                        res.append(children)
        return res
    
    return walk

if _core is not None:
    get_token_walker = _core.token_walker
    

def get_recursive_tag_parser(tagname, blocknode=False):
    tp = tagparser()
    tp.add(tagname, 10, blocknode=blocknode)
    return tp
    
parse_div = get_recursive_tag_parser("div", blocknode=True)


def parse_inputbox(tokens, xopts):
    get_recursive_tag_parser("inputbox")(tokens, xopts)
    
    for t in tokens:
        if t.tagname=='inputbox':
            t.inputbox = T.join_as_text(t.children)
            del t.children[:]

def _parse_gallery_txt(txt, xopts):
    lines = [x.strip() for x in txt.split("\n")]
    sub = []
    for x in lines:
        if not x:
            continue

        assert xopts.expander is not None, "no expander in _parse_gallery_txt"
        xnew = xopts.expander.parseAndExpand(x, keep_uniq=True)

        linode = parse_txt(u'[['+xnew+']]', xopts)

        if linode:
            n = linode[0]
            if n.ns==nshandling.NS_IMAGE:
                sub.append(n)
                continue
        sub.append(T(type=T.t_text, text=xnew))
    return sub

class bunch(object):
    def __init__(self, **kw):
        self.__dict__.update(kw)
        
class parse_sections(object):
    def __init__(self, tokens, xopts):
        self.tokens = tokens
        self.run()
        
    def run(self):
        tokens = self.tokens
        i = 0

        sections = []
        current = bunch(start=None, end=None, endtitle=None)

        def create():
            if current.start is None or current.endtitle is None:
                return False
            
            l1 = tokens[current.start].text.count("=")
            l2 = tokens[current.endtitle].text.count("=")
            level = min (l1, l2)

            # FIXME: make this a caption
            caption = T(type=T.t_complex_node, children=tokens[current.start+1:current.endtitle]) 
            if l2>l1:
                caption.children.append(T(type=T.t_text, text=u"="*(l2-l1)))
            elif l1>l2:
                caption.children.insert(0, T(type=T.t_text, text=u"="*(l1-l2)))

            body = T(type=T.t_complex_node, children=tokens[current.endtitle+1:i])
              
            sect = T(type=T.t_complex_section, tagname="@section", children=[caption, body], level=level, blocknode=True)
            tokens[current.start:i] = [sect] 
            

            while sections and level<=sections[-1].level:
                sections.pop()
            if sections:
                sections[-1].children.append(tokens[current.start])
                del tokens[current.start]
                current.start -= 1

            sections.append(sect)
            return True


        
        while i<len(self.tokens):
            t = tokens[i]
            if t.type==T.t_section:
                if create():
                    i = current.start+1
                    current = bunch(start=None, end=None, endtitle=None)
                else:
                    current.start = i
                    i += 1
            elif t.type==T.t_section_end:
                current.endtitle = i
                i+= 1
            else:
                i+=1

        create()

class parse_urls(object):
    def __init__(self, tokens, xopts):
        self.tokens = tokens
        self.run()
        
    def run(self):
        tokens = self.tokens
        i=0
        start = None
        while i<len(tokens):
            t = tokens[i]
            
            if t.type==T.t_urllink and start is None:
                start = i
                i+=1
            elif t.type==T.t_special and t.text=="]" and start is not None:
                sub = self.tokens[start+1:i]
                self.tokens[start:i+1] = [T(type=T.t_complex_named_url, children=sub, caption=self.tokens[start].text[1:])]
                i = start
                start = None
            elif t.type==T.t_2box_close and start is not None:
                self.tokens[i].type = T.t_special
                self.tokens[i].text = "]"
                sub = self.tokens[start+1:i]
                self.tokens[start:i] = [T(type=T.t_complex_named_url, children=sub, caption=self.tokens[start].text[1:])]
                i = start
                start = None
            else:
                i+=1
                

class parse_singlequote(object):
    def __init__(self, tokens, xopts):
        self.tokens = tokens
        self.run()

    def run(self):
        def finish():
            assert len(counts)==len(styles)
            
            from mwlib.parser import styleanalyzer
            states = styleanalyzer.compute_path(counts)

            
            last_apocount = 0
            for i, s in enumerate(states):
                apos = "'"*(s.apocount-last_apocount)
                if apos:
                    styles[i].children.insert(0, T(type=T.t_text, text=apos))
                last_apocount = s.apocount

                if s.is_bold and s.is_italic:
                    styles[i].caption = "'''"
                    inner = T(type=T.t_complex_style, caption="''", children=styles[i].children)
                    styles[i].children = [inner]
                elif s.is_bold:
                    styles[i].caption = "'''"
                elif s.is_italic:
                    styles[i].caption = "''"
                else:
                    styles[i].type = T.t_complex_node
            
        
        tokens = self.tokens
        pos = 0
        start = None
        counts = []
        styles = []
        
        while pos<len(tokens):
            t = tokens[pos]
            if t.type==T.t_singlequote:
                if start is None:
                    counts.append(len(t.text))
                    start = pos
                    pos+=1
                else:
                    tokens[start:pos] = [T(type=T.t_complex_style, children=tokens[start+1:pos])]
                    styles.append(tokens[start])
                    pos = start+1
                    start = None
            elif t.type==T.t_newline:
                if start is not None:
                    tokens[start:pos] = [T(type=T.t_complex_style, children=tokens[start+1:pos])]
                    styles.append(tokens[start])
                    pos = start
                    start = None
                pos += 1
                
                if counts:
                    finish()
                    counts = []
                    styles = []
            else:
                pos += 1

        
        if start is not None:
            tokens[start:pos] = [T(type=T.t_complex_style, children=tokens[start+1:pos])]
            styles.append(tokens[start])
            
        if counts:
            finish()
                
                    
class parse_preformatted(object):
    need_walker = False
    def __init__(self, tokens, xopts):
        walker = get_token_walker(skip_tags=set(["table", "li", "tr", "@section"]))
        for t in walker(tokens):
            self.tokens = t
            self.run()

    def run(self):
        tokens = self.tokens
        i = 0
        start = None
        while i<len(tokens):
            t = tokens[i]
            if t.type==T.t_pre:
                assert start is None
                start = i
                i+=1
            elif t.type==T.t_newline and start is not None:
                sub = tokens[start+1:i+1]
                if start>0 and tokens[start-1].type==T.t_complex_preformatted:
                    del tokens[start:i+1]
                    tokens[start-1].children.extend(sub)
                    i = start
                else:    
                    tokens[start:i+1] = [T(type=T.t_complex_preformatted, children=sub, blocknode=True)]
                    i = start+1
                start = None
            elif t.blocknode or (t.type==T.t_complex_tag and t.tagname in ("blockquote", "table", "timeline", "div")):
                start = None
                i+=1
            else:
                i+=1
                
            
class parse_lines(object):
    def __init__(self, tokens, xopts):
        self.tokens = tokens
        self.run()

    def splitdl(self, item):
        for i, x in enumerate(item.children):
            if x.type==T.t_special and x.text==':':
                s=T(type=T.t_complex_style, caption=':', children=item.children[i+1:])
                del item.children[i:]
                return s
                 
    def analyze(self, lines):
        def getchar(node):
            assert node.type==T.t_complex_line
            if node.lineprefix:
                return node.lineprefix[0]
            return None
        
        
        lines.append(T(type=T.t_complex_line, lineprefix='<guard>')) # guard

        
        startpos = 0
        while startpos<len(lines)-1:
            prefix = getchar(lines[startpos])
            if prefix is None:
                if lines[startpos].tagname:
                    lines[startpos].type = T.t_complex_tag
                else:
                    lines[startpos].type = T.t_complex_node
                startpos+=1
                continue

            endtag = None
            if prefix==':':
                node = T(type=T.t_complex_style, caption=':')
                newitem = lambda: T(type=T.t_complex_node, blocknode=True)
            elif prefix=='*':
                node = T(type=T.t_complex_tag, tagname="ul")
                newitem = lambda: T(type=T.t_complex_tag, tagname="li", blocknode=True)
                endtag = "ul"
            elif prefix=="#":
                node = T(type=T.t_complex_tag, tagname="ol")
                newitem = lambda: T(type=T.t_complex_tag, tagname="li", blocknode=True)
                endtag = "ol"
            elif prefix==';':
                node = T(type=T.t_complex_style, caption=';')
                newitem = lambda: T(type=T.t_complex_node, blocknode=True)
            else:
                assert 0
                
            node.children = []
            dd = None

            def appendline():
                line = lines[startpos]
                if endtag:
                    for i, x in enumerate(line.children):
                        if x.rawtagname==endtag and x.type==T.t_html_tag_end:
                            after = line.children[i+1:]
                            del line.children[i:]
                            item.children.append(line)
                            lines[startpos] = T(type=T.t_complex_line, tagname="p", lineprefix=None, children=after)
                            return
                
                item.children.append(lines[startpos])
                del lines[startpos]
                
            while startpos<len(lines)-1 and getchar(lines[startpos])==prefix:
                # collect items
                item = newitem()
                item.children=[]
                appendline()
                
                while startpos<len(lines)-1 and prefix==getchar(lines[startpos]) and len(lines[startpos].lineprefix)>1:
                    appendline()

                for x in item.children:
                    x.lineprefix=x.lineprefix[1:]
                self.analyze(item.children)
                node.children.append(item)
                if prefix==';' and item.children and item.children[0].type==T.t_complex_node:
                    dd = self.splitdl(item.children[0])
                    if dd is not None:
                        break
                if prefix in ":;":
                    break
                
            lines.insert(startpos, node)
            startpos += 1
            if dd is not None:
                lines.insert(startpos, dd)
                startpos += 1
        del lines[-1] # remove guard
        
    def run(self):
        tokens = self.tokens
        i = 0
        lines = []
        startline = None
        firsttoken = None

        def getlineprefix():
            return (tokens[startline].text or "").strip()
        
        while i<len(self.tokens):
            t = tokens[i]
            if t.type in (T.t_item, T.t_colon):
                if firsttoken is None:
                    firsttoken = i
                startline = i
                i+=1
            elif t.type==T.t_newline and startline is not None:
                sub = self.tokens[startline+1:i+1]
                lines.append(T(type=T.t_complex_line, start=tokens[startline].start, len=0, children=sub, lineprefix=getlineprefix()))
                startline = None
                i+=1
            elif t.type==T.t_break:
                if startline is not None:
                    sub = self.tokens[startline+1:i]
                    lines.append(T(type=T.t_complex_line, start=tokens[startline].start, len=0, children=sub, lineprefix=getlineprefix()))
                    startline=None
                if lines:
                    self.analyze(lines)
                    self.tokens[firsttoken:i] = lines
                    i = firsttoken
                    firsttoken=None
                    lines=[]
                    continue
                    
                firsttoken = None
                
                lines = []
                i+=1
            else:
                if startline is None and lines:
                    self.analyze(lines)
                    self.tokens[firsttoken:i] = lines
                    i = firsttoken
                    lines=[]
                    firsttoken=None
                else:
                    i+=1

        if startline is not None:
            sub = self.tokens[startline+1:]
            lines.append(T(type=T.t_complex_line, start=tokens[startline].start, children=sub, lineprefix=getlineprefix()))

        if lines:
            self.analyze(lines)
            self.tokens[firsttoken:] = lines                

        
class parse_links(object):
    def __init__(self, tokens, xopts):
        self.xopts = xopts
        lang = xopts.lang
        imagemod = xopts.imagemod
        
        self.tokens = tokens
        self.lang = lang
        
        self.nshandler = xopts.nshandler
        assert self.nshandler is not None, 'nshandler not set'
        
            
        if imagemod is None:
            imagemod = util.ImageMod()
        self.imagemod = imagemod
        
        self.run()

    def handle_image_modifier(self, mod, node):
        mod_type, mod_match = self.imagemod.parse(mod)
        if mod_type is None:
            return False
        util.handle_imagemod(node, mod_type, mod_match)
        if node.thumb or node.align or node.frame=="frame":
            node.blocknode=True
            
        return True
    
    def extract_image_modifiers(self, marks, node):
        cap = None
        for i in range(1,len(marks)-1):
            tmp = self.tokens[marks[i]+1:marks[i+1]]
            if not self.handle_image_modifier(T.join_as_text(tmp), node):
                cap = tmp
        return cap

    
        
    def run(self):
        tokens = self.tokens
        i = 0
        marks = []

        stack = []
        
        
        while i<len(self.tokens):
            t = tokens[i]
            if t.type==T.t_2box_open:
                if len(marks)>1:
                    stack.append(marks)
                marks = [i]
                i+=1
            elif t.type == T.t_newline and len(marks) < 2:
                if stack:
                    marks = stack.pop()
                else:
                    marks = []
                i += 1
            elif t.type==T.t_special and t.text=="|":
                marks.append(i)
                i+=1
            elif t.type==T.t_2box_close and marks:
                marks.append(i)
                start = marks[0]
                
                target = T.join_as_text(tokens[start+1:marks[1]]).strip()
                target=target.strip(u"\u200e\u200f")
                if target.startswith(":"):
                    target = target[1:]
                    colon = True
                else:
                    colon = False

                ilink = self.nshandler.resolve_interwiki(target)
                if ilink:
                    url = ilink.url
                    ns = None
                    partial = ilink.partial
                    langlink = ilink.language
                    interwiki = ilink.prefix
                    full = None
                else:
                    if target.startswith('/') and self.xopts.title:
                        ns, partial, full = self.nshandler.splitname(self.xopts.title + target)
                        if full.endswith('/'):
                            full = full[:-1]
                            target = target[1:-1]
                    else:
                        ns, partial, full = self.nshandler.splitname(target)

                    if self.xopts.wikidb is not None:
                        url = self.xopts.wikidb.getURL(full)
                    else:
                        url = None
                    langlink = None
                    interwiki = None

                if not ilink and not partial:
                    i+=1
                    if stack:
                        marks=stack.pop()
                    else:
                        marks=[]
                    continue

                node = T(type=T.t_complex_link, children=[], ns=ns, colon=colon, lang=self.lang, nshandler=self.nshandler, url=url)
                if langlink:
                    node.langlink = langlink
                if interwiki:
                    node.interwiki = interwiki
                    
                sub = None
                if ns==nshandling.NS_IMAGE:
                    sub = self.extract_image_modifiers(marks, node)                        
                elif len(marks)>2:
                    sub = tokens[marks[1]+1:marks[-1]]

                if sub is None:
                    sub = [] 
                    
                node.children = sub
                tokens[start:i+1] = [node]
                node.target = target
                node.full_target = full
                if stack:
                    marks = stack.pop()
                else:
                    marks = []
                i = start+1
            else:
                i+=1




class parse_paragraphs(object):
    need_walker = False
    
    def __init__(self, tokens, xopts):
        walker = get_token_walker(skip_tags=set(["p", "ol", "ul", "table", "tr", "@section"]))
        for t in walker(tokens):
            self.tokens = t
            self.run()
        
    def run(self):
        tokens = self.tokens
        i = 0
        first = 0
        def create(delta=1):
            sub = tokens[first:i]
            if sub:
                tokens[first:i+delta] = [T(type=T.t_complex_tag, tagname='p', children=sub, blocknode=True)]

        
        while i<len(self.tokens):
            t = tokens[i]
            if t.type==T.t_break:
                create()
                first += 1
                i = first
            elif t.blocknode: # blocknode
                create(delta=0)
                first += 1
                i = first
            else:
                i+=1
                
        if first:
            create()

    

class combined_parser(object):
    def __init__(self, parsers):
        self.parsers = parsers

    def __call__(self, tokens, xopts):
        parsers = list(self.parsers)

        default_walker = get_token_walker(skip_tags=set(["table", "tr", "@section"]))
        
        while parsers:
            p = parsers.pop()

            need_walker = getattr(p, "need_walker", True)
            if need_walker:
                # print "using default token walker for", p
                walker = default_walker
                for x in walker(tokens):
                    p(x, xopts)
            else:
                p(tokens, xopts)
                

def mark_style_tags(tokens, xopts):
    tags = set("abbr tt strike ins del small sup sub b strong cite i u em big font s var kbd".split())

    todo = [(0, dict(), tokens)]

    
    def create():
        if not state or i<=start:
            return False

        children = tokens[start:i]
        for tag, tok in state.items():
            outer = T(type=T.t_complex_tag, tagname=tag, children=children, vlist=tok.vlist)
            children = [outer]
        tokens[start:i] = [outer]
        return True
            
            
    while todo:
        i, state, tokens = todo.pop()
        start = i
        while i<len(tokens):
            t = tokens[i]
            if t.type==T.t_html_tag and t.rawtagname in tags:
                del tokens[i]
                if t.tag_selfClosing:
                    continue

                if t.rawtagname in state:
                    if create():
                        start += 1
                        i = start
                    start = i
                    
                    del state[t.rawtagname]
                    continue
                    
                if create():
                    start += 1
                    i = start
                start = i
                state[t.rawtagname]=t
            elif t.type==T.t_html_tag_end and t.rawtagname in tags:
                del tokens[i]
                rawtagname = t.rawtagname
                
                if rawtagname not in state:
                    if rawtagname=="sup":
                        rawtagname="sub"
                    elif rawtagname=="sub":
                        rawtagname="sup"
                        
                if rawtagname in state:
                    if create():
                        start += 1
                        i = start
                    del state[rawtagname]
            elif t.children:
                if create():
                    start += 1
                    i = start
                assert tokens[i] is t
                if t.type in (T.t_complex_table, T.t_complex_table_row, T.t_complex_table_cell):
                    todo.append((i+1, state, tokens))
                    todo.append((0, dict(), t.children))                    
                else:    
                    todo.append((i+1, state, tokens))
                    todo.append((0, state, t.children))
                break
            else:
                i+=1
        create()
mark_style_tags.need_walker = False

class parse_uniq(object):
    def __init__(self, tokens, xopts):
        self.tagextensions=tagext.default_registry

        uniquifier = xopts.uniquifier
        if uniquifier is None:
            return

        i = 0
        while i<len(tokens):
            t = tokens[i]
            if t.type!=T.t_uniq:
                i+=1
                continue
            
            text = t.text
            try:
                match = uniquifier.uniq2repl[text]
            except KeyError:
                t.type==T.t_text
                i+=1
                continue

            vlist = match["vlist"]
            if vlist:
                vlist = util.parseParams(vlist)
            else:
                vlist = None

            inner = match["inner"]
            name = match["tagname"]
            
            try:
                m = getattr(self, "create_"+str(name))
            except AttributeError:
                m = self._create_generic
                
            tokens[i] = m(name, vlist, inner or u"", xopts)
            if tokens[i] is None:
                del tokens[i]
            else:
                i += 1
                
            
    def _create_generic(self, name, vlist, inner, xopts):
        if not vlist:
            vlist = {}
        if name in self.tagextensions:
            node = self.tagextensions[name](inner, vlist)
            if node is None:
                retval = None
            else:
                retval = T(type=T.t_complex_compat, compatnode=node)

            return retval
        
        children = [T(type=T.t_text, text=inner)]
        return T(type=T.t_complex_tag, tagname=name, vlist=vlist, children=children)

    def create_pre(self, name, vlist, inner, xopts):
        inner = util.replace_html_entities(util.remove_nowiki_tags(inner))
        return self._create_generic(name,  vlist, inner, xopts)
    
    def create_source(self, name, vlist, inner, xopts):
        children = [T(type=T.t_text, text=inner)]
        blocknode = True
        if vlist and vlist.get("enclose",  "")=="none":
            blocknode=False
            
        return T(type=T.t_complex_tag, tagname=name, vlist=vlist, children=children, blocknode=blocknode)
    
    def create_ref(self, name, vlist, inner, xopts):
        expander = xopts.expander
        if expander is not None and inner:
            inner = expander.parseAndExpand(inner, True)

        if inner:
            # <ref>* not an item</ref>
            children = parse_txt("<br />"+inner, xopts)
            if children[0].children: # paragraph had been created...
                del children[0].children[0]
            else:
                del children[0]
        else:
            children = []
            
        return T(type=T.t_complex_tag, tagname="ref", vlist=vlist, children=children)

    def create_timeline(self, name, vlist, inner, xopts):
        return T(type=T.t_complex_tag, tagname="timeline", vlist=vlist, timeline=inner, blocknode=True)

    def create_math(self, name, vlist, inner, xopts):
        return T(type=T.t_complex_tag, tagname="math", vlist=vlist, math=inner)
    
    def create_gallery(self, name, vlist, inner, xopts):
        sub = _parse_gallery_txt(inner, xopts)
        return T(type=T.t_complex_tag, tagname="gallery", vlist=vlist, children=sub, blocknode=True)

    def create_poem(self, name, vlist, inner, xopts):
        expander = xopts.expander
        if expander is not None and inner:
            inner = expander.parseAndExpand(inner, True)

        res = []
        res.append(u"\n")
        for line in inner.split("\n"):
            if line.strip():
                res.append(":")
            if line.startswith(" "):
                res.append(u"&nbsp;")
            res.append(line.strip())
            res.append(u"\n")
        res.append(u"\n")
        res = u"".join(res)
        children = parse_txt(res, xopts)
        return T(type=T.t_complex_tag, tagname="poem", vlist=vlist, children=children)
    
    def create_imagemap(self, name, vlist, inner, xopts):
        from mwlib import imgmap
        txt = inner
        t = T(type=T.t_complex_tag, tagname="imagemap", vlist=vlist)
        t.imagemap = imgmap.ImageMapFromString(txt)
        if t.imagemap.image:
            t.imagemap.imagelink = None
            s = u"[["+t.imagemap.image+u"]]"
            res = parse_txt(s, xopts)
            if res and res[0].type==T.t_complex_link and res[0].ns==6:
                t.imagemap.imagelink = res[0]

        return t

    def create_nowiki(self, name, vlist, inner, xopts):
        txt = inner
        txt = util.replace_html_entities(txt)
        return T(type=T.t_text, text=txt)

    def create_pages(self, name, vlist, inner, xopts):
        expander = xopts.expander

        if not vlist:
            vlist = {}
        s = vlist.get("from")
        e = vlist.get("to")
        children = []
        if s and e and expander:
            nshandler = expander.nshandler
            page_ns = nshandler._find_namespace("Page")[1]

            try:
                si = int(s)
                ei = int(e)
            except ValueError:
                s = nshandler.get_fqname(s, page_ns)
                e = nshandler.get_fqname(e, page_ns)
                pages = expander.db.select(s, e)
            else:
                base = vlist.get("index", "")
                base = nshandler.get_fqname(base, page_ns)
                pages = [u"%s/%s" % (base, i) for i in range(si, ei+1)]

            rawtext = u"".join(u"{{%s}}\n" % x for x in pages)
            te = expander.__class__(rawtext, pagename=expander.pagename, wikidb=expander.db)
            children = parse_txt(te.expandTemplates(True),
                                 xopts=XBunch(**xopts.__dict__),
                                 expander=te,
                                 uniquifier=te.uniquifier)


        return T(type=T.t_complex_tag, tagname=name, vlist=vlist, children=children)


class XBunch(object):
    def __init__(self, **kw):
        self.__dict__.update(kw)

    def __getattr__(self, name):
        return None

def fix_urllink_inside_link(tokens,  xopt):
    idx = 0
    last = None
    while idx<len(tokens)-1:
        t = tokens[idx]
        if t.type==T.t_2box_open:
            last = T.t_2box_open
        elif t.type==T.t_urllink:
            last = T.t_urllink
        elif t.type==T.t_2box_close:
            if tokens[idx+1].type==T.t_special and tokens[idx+1].text=="]" and last==T.t_urllink:
                tokens[idx], tokens[idx+1] = tokens[idx+1], tokens[idx]

        idx += 1
        

def fix_named_url_double_brackets(tokens, xopt):
    idx = 0
    while idx<len(tokens)-1:
        t = tokens[idx]
        if t.type==T.t_2box_open and tokens[idx+1].type==T.t_http_url:
            tokens[idx].text = "["
            tokens[idx].type = T.t_special
            tokens[idx+1].text = "["+tokens[idx+1].text
            tokens[idx+1].type = T.t_urllink
        idx += 1
    fix_urllink_inside_link(tokens, xopt)
    
    
def fix_break_between_pre(tokens, xopt):
    idx = 0
    while idx<len(tokens)-1:
        t = tokens[idx]
        if t.type==T.t_break and t.text.startswith(" ") and tokens[idx+1].type==T.t_pre:
            tokens[idx:idx+1] = [T(type=T.t_pre, text=" "), T(type=T.t_newline, text=u"\n")]
            idx += 2
        else:
            idx+=1

def fixlitags(tokens, xopts):
    root = T(type=T.t_complex_tag, tagname="div")
    todo = [(root, tokens)]
    while todo:
        parent, tokens = todo.pop()
        if parent.tagname not in ("ol", "ul"):
            idx = 0
            while idx<len(tokens):
                start = idx
                while idx<len(tokens) and tokens[idx].tagname=="li":
                    idx+=1

                if idx>start:
                    lst = T(type=T.t_complex_tag, tagname="ul", children=tokens[start:idx])
                    tokens[start:idx+1] = [lst]
                    idx = start+1
                else:
                    idx += 1
                    
        for t in tokens:
            if t.children:
                todo.append((t, t.children))
fixlitags.need_walker = False

def parse_txt(txt, xopts=None, **kwargs):
    if xopts is None:
        xopts = XBunch(**kwargs)
    else:
        xopts.__dict__.update(**kwargs)

    if xopts.expander is None:
        from mwlib.expander import Expander,  DictDB    
        xopts.expander = Expander("", "pagename", wikidb=DictDB())
            
    if xopts.nshandler is None:
        xopts.nshandler = nshandling.get_nshandler_for_lang(xopts.lang or 'en')
    
    xopts.imagemod = util.ImageMod(xopts.magicwords)

    uniquifier = xopts.uniquifier
    if uniquifier is None:
        uniquifier = uniq.Uniquifier()
        txt = uniquifier.replace_tags(txt)
        xopts.uniquifier = uniquifier

    tokens = tokenize(txt, uniquifier=uniquifier)
    
    td2 =  tagparser()
    a = td2.add
    
    a("code"       , 10)
    a("span"       , 20)
    
    a("li"         , 25, blocknode=True, nested=False)
    a("dl"         , 28, blocknode=True)
    a("dt"         , 26, blocknode=True, nested=False)
    a("dd"         , 26, blocknode=True, nested=True)


    td1 =  tagparser()
    a = td1.add
    a("blockquote" , 5)
    a("references" , 15)
    
    a("p"          , 30, blocknode=True, nested=False)
    a("ul"         , 35, blocknode=True)
    a("ol"         , 40, blocknode=True)
    a("center"     , 45, blocknode=True)

    td_parse_h = tagparser()
    for i in range(1, 7):
        td_parse_h.add("h%s" % i, i)
        
        
    parsers = [fixlitags,
               mark_style_tags,
               parse_singlequote,
               parse_preformatted,
               td2, 
               parse_paragraphs,
               td1, 
               parse_lines,
               parse_div,
               parse_links,
               parse_urls,
               parse_inputbox,
               td_parse_h, 
               parse_sections,
               remove_table_garbage, 
               fix_tables, 
               parse_tables,
               parse_uniq,
               fix_named_url_double_brackets, 
               fix_break_between_pre]
    
    combined_parser(parsers)(tokens, xopts)
    return tokens

########NEW FILE########
__FILENAME__ = parse_table
# -*- compile-command: "../../tests/test_refine.py" -*-

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

from mwlib.utoken import show, token as T
from mwlib.refine import util

class parse_table_cells(object):
    def __init__(self, tokens, xopts):
        self.tokens = tokens
        self.run()
        
    def is_table_cell_start(self, token):
        return token.type==T.t_column or (token.type==T.t_html_tag and token.rawtagname in ("td", "th"))

    def is_table_cell_end(self, token):
        return token.type==T.t_html_tag_end and token.rawtagname in ("td", "th")

    def find_modifier(self, cell):
        children = cell.children
        if not children:
            return
        for i,x in enumerate(children):
            t = children[i]
            if t.type==T.t_2box_open:
                break
            if t.type==T.t_special and t.text=="|":
                mod = T.join_as_text(children[:i])
                cell.vlist = util.parseParams(mod)
                
                del children[:i+1]
                return

    def replace_tablecaption(self, children):
        i = 0
        while i<len(children):
            if children[i].type == T.t_tablecaption:
                children[i].type = T.t_special
                children[i].text = u"|"
                children.insert(i+1, T(type=T.t_text, text="+"))
            i+=1
            
                
                
    def run(self):
        tokens = self.tokens
        i = 0
        start = None
        self.is_header = False
        
        def makecell(skip_end=0):
            st = tokens[start].text.strip()
            if st=="|":
                self.is_header = False
            elif st=="!":
                self.is_header = True
            is_header = self.is_header
            
            if tokens[start].rawtagname=="th":
                is_header = True
            elif tokens[start].rawtagname=="td":
                is_header = False

            if is_header:
                tagname = "th"
            else:
                tagname = "td"
                
                
            search_modifier = tokens[start].text.strip() in ("|", "!", "||", "!!")
            sub = tokens[start+1:i-skip_end]
            self.replace_tablecaption(sub)
            tokens[start:i] = [T(type=T.t_complex_table_cell, tagname=tagname,
                                 start=tokens[start].start, children=sub,
                                 vlist=tokens[start].vlist, is_header=is_header)]
            if search_modifier:
                self.find_modifier(tokens[start])
        
        while i < len(tokens):
            if self.is_table_cell_start(tokens[i]):
                if start is not None:
                    makecell()                        
                    start += 1
                    i = start+1
                else:
                    start = i
                    i+=1
                        
            elif self.is_table_cell_end(tokens[i]):
                if start is not None:
                    i+=1
                    makecell(skip_end=1)                    
                    i = start+1
                    start = None
                else:
                    i+= 1
            else:
                i += 1

        if start is not None:
            makecell()
            
                
class parse_table_rows(object):
    def __init__(self, tokens, xopts):
        self.tokens = tokens
        self.xopts = xopts
        self.run()
        
    def is_table_row_start(self, token):
        return token.type==T.t_row or (token.type==T.t_html_tag and token.rawtagname=='tr')

    def is_table_row_end(self, token):
        return token.type==T.t_html_tag_end and token.rawtagname=='tr'
    
    def find_modifier(self, row):
        children = row.children
        for i,x in enumerate(children):
            if x.type in (T.t_newline, T.t_break):
                mod = T.join_as_text(children[:i])
                #print "MODIFIER:", repr(mod)
                row.vlist = util.parseParams(mod)
                del children[:i]
                return
            
    def is_table_cell_start(self, token):
        return token.type==T.t_column or (token.type==T.t_html_tag and token.rawtagname in ("td", "th"))
    
    def run(self):
        tokens = self.tokens
        i = 0

        start = None
        remove_start = 1
        rowbegintoken = None
        def should_find_modifier():
            if rowbegintoken is None:
                return False
            if rowbegintoken.rawtagname:
                return False
            return True

        def args():
            if rowbegintoken is None:
                return {}
            return dict(vlist=rowbegintoken.vlist)
        
            
        while i < len(tokens):
            if start is None and self.is_table_cell_start(tokens[i]):
                rowbegintoken = None
                start = i
                remove_start = 0
                i+=1
            elif self.is_table_row_start(tokens[i]):
                if start is not None:
                    children = tokens[start+remove_start:i]
                    tokens[start:i] = [T(type=T.t_complex_table_row, tagname="tr", start=tokens[start].start, children=children, **args())]
                    if should_find_modifier():
                        self.find_modifier(tokens[start])
                    parse_table_cells(children, self.xopts)
                    start += 1  # we didn't remove the start symbol above
                    rowbegintoken= tokens[start]
                    remove_start = 1
                    i = start+1
                    
                else:
                    rowbegintoken = tokens[i]
                    remove_start = 1
                    start = i
                    i+=1
            elif self.is_table_row_end(tokens[i]):
                if start is not None:
                    sub = tokens[start+remove_start:i]
                    tokens[start:i+1] = [T(type=T.t_complex_table_row, tagname="tr", start=tokens[start].start, children=sub, **args())]
                    if should_find_modifier():
                        self.find_modifier(tokens[start])
                    parse_table_cells(sub, self.xopts)
                    i = start+1
                    start = None
                    rowbegintoken = None
                else:
                    i+= 1
            else:
                i += 1

        if start is not None:
            sub = tokens[start+remove_start:]
            tokens[start:] = [T(type=T.t_complex_table_row, tagname="tr", start=tokens[start].start, children=sub, **args())]
            if should_find_modifier():
                self.find_modifier(tokens[start])
            parse_table_cells(sub, self.xopts)
        
class parse_tables(object):
    def __init__(self, tokens, xopts):
        self.xopts = xopts
        self.tokens = tokens
        self.run()
        
    def is_table_start(self, token):
        return token.type==T.t_begintable or (token.type==T.t_html_tag and token.rawtagname=="table")

    def is_table_end(self, token):
        return token.type==T.t_endtable or (token.type==T.t_html_tag_end and token.rawtagname=="table")

    def handle_rows(self, sublist):
        parse_table_rows(sublist, self.xopts)

    def find_modifier(self, table):
        children = table.children
        def compute_mod():
            mod = T.join_as_text(children[:i])
            #print "MODIFIER:", repr(mod)
            table.vlist = util.parseParams(mod)
            del children[:i]

        i = 0    
        for i,x in enumerate(children):
            if x.type in (T.t_newline, T.t_break):
                break

        compute_mod()
        
    def find_caption(self, table):
        children = table.children
        start = None
        i = 0
        while i < len(children):
            t = children[i]
            if t.type==T.t_tablecaption:
                start = i
                i += 1
                break

            if t.text is None or t.text.strip():
                return
            i+=1

        modifier = None
        
        while i<len(children):
            t = children[i]
            if t.tagname not in ("ref",) and (t.text is None or t.text.startswith("\n")):
                if modifier:
                    mod = T.join_as_text(children[start:modifier])
                    vlist = util.parseParams(mod)
                    sub = children[modifier+1:i]
                else:
                    sub = children[start+1:i]
                    vlist = {}
                    
                caption = T(type=T.t_complex_caption, children=sub, vlist=vlist)
                children[start:i] = [caption]
                return
            elif t.text=="|" and modifier is None:
                modifier = i
            elif t.type == T.t_2box_open and modifier is None:
                modifier =  0
                
            i += 1
            
    def run(self):
        tokens = self.tokens
        i = 0
        stack = []

        def maketable():
            start = stack.pop()
            starttoken = tokens[start]
            sub = tokens[start+1:i]
            from mwlib.refine import core
            tp = core.tagparser()
            tp.add("caption", 5)
            tp(sub, self.xopts)
            tokens[start:i+1] = [T(type=T.t_complex_table,
                                   tagname="table", start=tokens[start].start, children=sub,
                                   vlist=starttoken.vlist, blocknode=True)]
            if starttoken.text.strip() == "{|":
                self.find_modifier(tokens[start])
            self.handle_rows(sub)
            self.find_caption(tokens[start])
            return start

            
        while i < len(tokens):
            if self.is_table_start(tokens[i]):
                stack.append(i)
                i+=1
            elif self.is_table_end(tokens[i]):
                if stack:
                    i = maketable()+1
                else:
                    i += 1
            else:
                i += 1

        while stack:
            maketable()
        
class fix_tables(object):
    def __init__(self, tokens, xopts):
        self.xopts = xopts
        self.tokens = tokens
        self.run()
        
    def run(self):
        tokens = self.tokens
        for x in tokens:
            if x.type != T.t_complex_table:
                continue

            rows = [c for c in x.children if c.type in (T.t_complex_table_row, T.t_complex_caption)]
            if not rows:
                x.type = T.t_complex_node
                x.tagname = None
                
def extract_garbage(tokens, is_allowed,  is_whitespace=None):
    if is_whitespace is None:
        is_whitespace = lambda t: t.type in (T.t_newline,  T.t_break)
        
    res = []
    i = 0
    start = None
    
    while i<len(tokens):
        if is_whitespace(tokens[i]):
            if start is None:
                start = i
            i+=1
        elif is_allowed(tokens[i]):
            start = None
            i+=1
        else:
            if start is None:
                start = i
            i+=1
            
            # find end of garbage
            
            while i<len(tokens):
                if is_allowed(tokens[i]):
                   break
                i+= 1
                
            garbage = tokens[start:i]
            del tokens[start:i]
            i = start
            res.append(T(type=T.t_complex_node, children=garbage))
            
    return res

class remove_table_garbage(object):
    need_walker = False
    
    def __init__(self, tokens, xopts):
        from mwlib.refine import core
        walker = core.get_token_walker()
        for t in walker(tokens):
            self.tokens = t
            self.run()
        
    def run(self):
        tokens = self.tokens
        tableidx = 0
        while tableidx<len(tokens):
            if tokens[tableidx].type==T.t_complex_table:
                # garbage = extract_garbage(tokens[tableidx].children,
                #                           is_allowed=lambda t: t.type in (T.t_complex_table_row, T.t_complex_caption))

                tmp = []
                for c in tokens[tableidx].children:
                    if c.type==T.t_complex_table_row:
                        rowgarbage = extract_garbage(c.children,
                                                     is_allowed=lambda t: t.type in (T.t_complex_table_cell, ))
                        tmp.extend(rowgarbage)
                        
                        
                tokens[tableidx+1:tableidx+1] = tmp
            tableidx+=1

########NEW FILE########
__FILENAME__ = tagparser

# Copyright (c) 2009 PediaPress GmbH
# See README.rst for additional licensing information.

"""parse tags in parallel"""

import sys
from mwlib.utoken import token as T

class taginfo(object):
    def __init__(self, tagname=None, prio=None, blocknode=False,  nested=True):
        assert None not in (tagname, prio, nested, blocknode)
        self.tagname = tagname
        self.prio = prio
        self.nested = nested
        self.blocknode = blocknode
                 
class tagparser(object):
    def __init__(self, tags=[]):
        self.name2tag = name2tag = {}
        for t in tags:
            name2tag[t.tagname] = t

        self.guard = (None, taginfo(tagname="", prio=sys.maxint, nested=True, blocknode=False))

    def add(self, tagname=None,  prio=None, blocknode=False, nested=True):
        t = taginfo(tagname=tagname, prio=prio, blocknode=blocknode, nested=nested)
        self.name2tag[t.tagname] = t
        
    def find_in_stack(self, tag):
        pos = len(self.stack)-1
        while pos>0:
            _, t = self.stack[pos]
            if t.tagname==tag.tagname:
                return pos
            
            if tag.prio > t.prio:
                pos -= 1
            else:
                break
            
        return 0

    def close_stack(self, spos, tokens, pos):
        close = self.stack[spos:]
        del self.stack[spos:]
        close.reverse()
        
        for i, t in close:
            vlist=tokens[i].vlist
            display = vlist.get("style", {}).get("display", "").lower()
            if display=="inline":
                blocknode = False
            elif display=="block":
                blocknode = True
            else:
                blocknode=t.blocknode
            
            sub = tokens[i+1:pos]
            tokens[i:pos] = [T(type=T.t_complex_tag,  children=sub,  tagname=t.tagname, blocknode=blocknode,  vlist=tokens[i].vlist)]
            pos = i+1
            
        return pos
    
    
    def __call__(self, tokens,  xopts):
        pos=0
        self.stack = stack = [self.guard]
        get = self.name2tag.get
        
        while pos<len(tokens):
            t = tokens[pos]
            # print t
            tag = get(t.rawtagname)
            if tag is None:
                pos += 1
                continue
            if t.type==T.t_html_tag:
                if t.tag_selfClosing:
                    tokens[pos].type = T.t_complex_tag
                    tokens[pos].tagname = tokens[pos].rawtagname
                    tokens[pos].rawtagname = None 
                    pos += 1
                else:
                    if stack[-1][1].prio==tag.prio and not tag.nested:
                        pos = self.close_stack(len(stack)-1, tokens, pos)
                        assert tokens[pos] is t
                        
                    stack.append((pos,  tag))
                    pos+=1
            else:
                assert t.type==T.t_html_tag_end
                # find a matching tag in the stack
                spos = self.find_in_stack(tag)
                if spos:
                    pos = self.close_stack(spos, tokens, pos)
                    assert tokens[pos] is t
                    del tokens[pos]
                else:
                    pos += 1
                    
        self.close_stack(1, tokens, pos)

########NEW FILE########
__FILENAME__ = uparser

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

from mwlib import expander, nshandling, metabook
from mwlib.log import Log
from mwlib.refine import core, compat

log = Log('refine.uparser')

def parseString(
    title=None,
    raw=None,
    wikidb=None,
    revision=None,
    lang=None,
    magicwords=None,
    expandTemplates=True):
    """parse article with title from raw mediawiki text"""

    uniquifier = None
    siteinfo = None
    assert title is not None, 'no title given'
    if raw is None:
        page = wikidb.normalize_and_get_page(title, 0)
        if page:
            raw = page.rawtext
        else:
            raw = None

        assert raw is not None, "cannot get article %r" % (title,)
    input = raw
    te = None
    if wikidb:
        if expandTemplates:
            te = expander.Expander(raw, pagename=title, wikidb=wikidb)
            input = te.expandTemplates(True)
            uniquifier = te.uniquifier
        if hasattr(wikidb, 'get_siteinfo'):
            siteinfo = wikidb.get_siteinfo()

        src = None
        if hasattr(wikidb, 'getSource'):
            src = wikidb.getSource(title, revision=revision)
            assert not isinstance(src, dict)

        if not src:
            src=metabook.source()

        if lang is None:
            lang = src.language
        if magicwords is None:
            if siteinfo is not None and 'magicwords' in siteinfo:
                magicwords = siteinfo['magicwords']
            else:
                magicwords = src.get('magicwords')

    if siteinfo is None:
        nshandler = nshandling.get_nshandler_for_lang(lang)
    else:
        nshandler = nshandling.nshandler(siteinfo)
    a = compat.parse_txt(input, title=title, wikidb=wikidb, nshandler=nshandler, lang=lang, magicwords=magicwords, uniquifier=uniquifier, expander=te)

    a.caption = title
    if te and te.magic_displaytitle:
        a.caption = te.magic_displaytitle

    from mwlib.old_uparser import postprocessors
    for x in postprocessors:
        x(a, title=title, revision=revision, wikidb=wikidb, lang=lang)

    return a

def simpleparse(raw,lang=None):    # !!! USE FOR DEBUGGING ONLY !!! does not use post processors
    a=compat.parse_txt(raw,lang=lang)
    core.show(a)
    return a

########NEW FILE########
__FILENAME__ = util

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import re
import htmlentitydefs

paramrx = re.compile(r"(?P<name>\w+)\s*=\s*(?P<value>(?:(?:\".*?\")|(?:\'.*?\')|(?:(?:\w|[%:#])+)))", re.DOTALL)
def parseParams(s):
    def style2dict(s):
        res = {}
        for x in s.split(';'):
            if ':' in x:
                var, value = x.split(':', 1)
                var = var.strip().lower()
                value = value.strip()
                res[var] = value

        return res
    
    def maybeInt(v):
        try:
            return int(v)
        except:
            return v
    
    r = {}
    for name, value in paramrx.findall(s):
        if value.startswith('"') or value.startswith("'"):
            value = value[1:-1]
            
        if name.lower() == 'style':
            value = style2dict(value)
            r['style'] = value
        else:
            r[name] = maybeInt(value)
    return r



class ImageMod(object):
    default_magicwords = [
        {u'aliases': [u'thumbnail', u'thumb'], u'case-sensitive': u'', u'name': u'img_thumbnail'},
        {u'aliases': [u'thumbnail=$1', u'thumb=$1'], u'case-sensitive': u'', u'name': u'img_manualthumb'},
        {u'aliases': [u'right'], u'case-sensitive': u'', u'name': u'img_right'},
        {u'aliases': [u'left'], u'case-sensitive': u'', u'name': u'img_left'},
        {u'aliases': [u'none'], u'case-sensitive': u'', u'name': u'img_none'},
        {u'aliases': [u'$1px'], u'case-sensitive': u'', u'name': u'img_width'},
        {u'aliases': [u'center', u'centre'], u'case-sensitive': u'', u'name': u'img_center'},
        {u'aliases': [u'framed', u'enframed', u'frame'], u'case-sensitive': u'', u'name': u'img_framed'},
        {u'aliases': [u'frameless'], u'case-sensitive': u'', u'name': u'img_frameless'},
        {u'aliases': [u'page=$1', u'page $1'], u'case-sensitive': u'', u'name': u'img_page'},
        {u'aliases': [u'upright', u'upright=$1', u'upright $1'], u'case-sensitive': u'', u'name': u'img_upright'},
        {u'aliases': [u'border'], u'case-sensitive': u'', u'name': u'img_border'},
        {u'aliases': [u'baseline'], u'case-sensitive': u'', u'name': u'img_baseline'},
        {u'aliases': [u'sub'], u'case-sensitive': u'', u'name': u'img_sub'},
        {u'aliases': [u'super', u'sup'], u'case-sensitive': u'', u'name': u'img_super'},
        {u'aliases': [u'top'], u'case-sensitive': u'', u'name': u'img_top'},
        {u'aliases': [u'text-top'], u'case-sensitive': u'', u'name': u'img_text_top'},
        {u'aliases': [u'middle'], u'case-sensitive': u'', u'name': u'img_middle'},
        {u'aliases': [u'bottom'], u'case-sensitive': u'', u'name': u'img_bottom'},
        {u'aliases': [u'text-bottom'], u'case-sensitive': u'', u'name': u'img_text_bottom'},
        {u'aliases': [u'link=$1'], u'case-sensitive': u'', u'name': u'img_link'},
        {u'aliases': [u'alt=$1'], u'case-sensitive': u'', u'name': u'img_alt'},
        ]

    def __init__(self, magicwords=None):        
        self.alias_map = {}
        self.initAliasMap(self.default_magicwords)
        if magicwords is not None:
            self.initAliasMap(magicwords)

    def initAliasMap(self, magicwords):
        for m in magicwords:            
            if not m['name'].startswith('img_'):
                continue
            name = m['name']
            aliases = m['aliases']
            aliases_regexp = '|'.join(['^(%s)$' % re.escape(a) for a in aliases])
            if name == 'img_upright':
                aliases_regexp = aliases_regexp.replace('\\$1', '\\s*([0-9.]+)\\s*')
            elif name == 'img_width':
                aliases_regexp = aliases_regexp.replace('\\$1', '\\s*([0-9x]+)\\s*')
            elif name in ['img_alt', 'img_link']:
                aliases_regexp = aliases_regexp.replace('\\$1', '(.*)')
            self.alias_map[name] = aliases_regexp

    def parse(self, mod):
        mod = mod.lower().strip()
        for mod_type, mod_reg in self.alias_map.items():
            rx = re.compile(mod_reg, re.IGNORECASE)
            mo = rx.match(mod)
            if mo:
                for match in  mo.groups()[::-1]:
                    if match:
                        return (mod_type, match)
        return (None, None)



def handle_imagemod(self, mod_type, match):
    if mod_type == 'img_alt':
        self.alt = match

    if mod_type == 'img_link':
        self.link = match

    if mod_type == 'img_thumbnail':
        self.thumb = True

    if mod_type == 'img_left':
        self.align = 'left'
    if mod_type == 'img_right':
        self.align = 'right'
    if mod_type == 'img_center':
        self.align = 'center'
    if mod_type == 'img_none':
        self.align = 'none'
    if mod_type == 'img_framed':
        self.frame = 'frame'
    if mod_type == 'img_frameless':
        self.frame = 'frameless'

    if mod_type == 'img_border':
        self.border = True

    if mod_type == 'img_upright':
        try:
            scale = float(match)
        except ValueError:
            scale = 0.75
        self.upright = scale

    if mod_type == 'img_width':                
        # x200px or 100x200px or 200px
        width, height = (match.split('x')+['0'])[:2]
        try:
            width = int(width)
        except ValueError:
            width = 0

        try:
            height = int(height)
        except ValueError:
            height = 0

        self.width = width
        self.height = height

        
def resolve_entity(e):
    if e[1]=='#':
        try:
            if e[2]=='x' or e[2]=='X':
                return unichr(int(e[3:-1], 16))
            else:
                return unichr(int(e[2:-1]))
        except ValueError:
            return e        
    else:
        try:
            return unichr(htmlentitydefs.name2codepoint[e[1:-1]])
        except KeyError:
            return e

def replace_html_entities(txt):
    return re.sub("&.*?;", lambda mo: resolve_entity(mo.group(0)), txt)

def remove_nowiki_tags(txt, _rx=re.compile("<nowiki>(.*?)</nowiki>",  re.IGNORECASE | re.DOTALL)):
    return _rx.sub(lambda mo: mo.group(1), txt)

########NEW FILE########
__FILENAME__ = sanitychecker
# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.
"""
class for defining DTD-Like Rules for the tree
"""
from advtree import Article

from mwlib.log import Log
log = Log("sanitychecker")

# -----------------------------------------------------------
# Constraints
# -----------------------------------------------------------

class ConstraintBase(object):
    def __init__(self, *klasses):
        self.klasses = klasses

    def test(self, nodes): 
        return True,None # passed

    def __repr__(self):
        return "%s(%s)" %(self.__class__.__name__, ", ".join(k.__name__ for k in self.klasses))


class Forbid(ConstraintBase):
    "forbid any of the classes"
    def test(self, nodes): 
        for n in nodes:
            if n.__class__ in self.klasses:
                return False, n
        return True, None
    

class Allow(ConstraintBase):
    "allow only these classes"
    def test(self, nodes): 
        for n in nodes:
            if not n.__class__ in self.klasses:
                return False, n
        return True, None


class Require(ConstraintBase):
    "require any of these classes"
    def test(self, nodes): 
        for n in nodes:
            if n.__class__ in self.klasses:
                return True, n
        return False, None
  
class Equal(ConstraintBase):
    "node classes and their order must be equal to these klasses"
    def test(self, nodes): 
        if len(nodes) != len(self.klasses):
            return False, None # FIXME what could we report?
        for i,n in enumerate(nodes):
            if n.__class__ !=  self.klasses[i]:
                return False, n
        return True, None
    

# -----------------------------------------------------------
# Rules regarding [Children, AllChildren, Parents, ...]
# -----------------------------------------------------------

class RuleBase:
    def __init__(self, klass, constraint):
        self.klass = klass
        self.constraint = constraint
    
    def _tocheck(self, node):
       return [] 

    def test(self, node):
        if node.__class__ == self.klass:
            return self.constraint.test( self._tocheck(node) )
        return True, None

    def __repr__(self):
        return "%s(%s, %r)" %(self.__class__.__name__, self.klass.__name__, self.constraint)

class ChildrenOf(RuleBase):
    def _tocheck(self, node):
        return node.children

class AllChildrenOf(RuleBase):
    def _tocheck(self, node):
        return node.getAllChildren()

class ParentsOf(RuleBase):
    def _tocheck(self, node):
        return node.parents

class ParentOf(RuleBase):
    def _tocheck(self, node):
        if node.parent:
            return [node.parent]
        return []

class SiblingsOf(RuleBase):
    def _tocheck(self, node):
        return node.siblings



# example custom rules

class RequireChild(RuleBase):

    def __init__(self, klass):
        self.klass = klass

    def __repr__(self):
        return "%s(%s)" %(self.__class__.__name__, self.klass.__name__)

    def test(self, node):
        if node.__class__ == self.klass:
            if not len(node.children):
                return False, node
        return True, None




# -----------------------------------------------------------
# Callbacks
# -----------------------------------------------------------
"""
callbacks get called if added to rules
callback return values should be:
 * True if it modified the tree and the sanity check needs to restart
 * False if the tree is left unmodified
"""
class SanityException(Exception):
    pass    

def exceptioncb(rule, node=None, parentnode=None):
    raise SanityException("%r  err:%r" %(rule, node or parentnode) )

def warncb(rule, node=None, parentnode=None):
    log.warn("%r node:%r parent:%r" %(rule, node, parentnode))
    return False

def removecb(rule, node=None, parentnode=None):
    assert node and node.parent
    node.parent.removeChild(node)
    return True



# -----------------------------------------------------------
# Container for sanity rules
# -----------------------------------------------------------

class SanityChecker(object):

    def __init__(self):
        self.rules = []

    def addRule(self, rule, actioncb=exceptioncb):
        self.rules.append((rule, actioncb))
    
    def check(self, tree):
        """ 
        check each node with each rule
        on failure call callback
        """
        modified = True
        while modified:
            modified = False
            for node in tree.allchildren():
                #if node.__class__ == Article:
                #    log.info("checking article:", node.caption.encode('utf-8'))
                for r,cb in self.rules:
                    passed, errnode = r.test(node)
                    if not passed and cb:
                        if cb(r, errnode or node):
                            modified = True
                            break
                if modified:
                    break

def demo(tree):
    "for documentation only, see tests for more demos"
    from mwlib.advtree import Table, Row, Cell, Text, ImageLink, PreFormatted

    sc = SanityChecker()
    rules = [ChildrenOf(Table, Allow(Row)),
             ChildrenOf(Row, Allow(Cell)),
             AllChildrenOf(Cell, Require(Text, ImageLink)),
             AllChildrenOf(Cell, Forbid(PreFormatted)),
             ChildrenOf(PreFormatted, Equal(Text)),
             ]
    
    def mycb(rule, node=None, parentnode=None):
        print "failed", rule, node or parentnode
        modifiedtree = False
        return modifiedtree

    for r in rules:
        sc.addRule( r, mycb)
    #sc.check(anytree)
    


########NEW FILE########
__FILENAME__ = serve
#! /usr/bin/env python

"""WSGI server interface to mw-render and mw-zip/mw-post"""

import sys, os, time, re, shutil, StringIO, errno
from hashlib import md5

from mwlib import myjson as json
from mwlib import log, _version
from mwlib.metabook import calc_checksum

log = log.Log('mwlib.serve')
collection_id_rex = re.compile(r'^[a-z0-9]{16}$')


def make_collection_id(data):
    sio = StringIO.StringIO()
    for key in (
        _version.version,
        'base_url',
        'script_extension',
        'login_credentials',
    ):
        sio.write(repr(data.get(key)))
    mb = data.get('metabook')
    if mb:
        if isinstance(mb, str):
            mb = unicode(mb, 'utf-8')
        mbobj = json.loads(mb)
        sio.write(calc_checksum(mbobj))
        num_articles = len(list(mbobj.articles()))
        sys.stdout.write("new-collection %s\t%r\t%r\n" % (num_articles, data.get("base_url"), data.get("writer")))

    return md5(sio.getvalue()).hexdigest()[:16]


def get_collection_dirs(cache_dir):
    """Generator yielding full paths of collection directories"""

    for dirpath, dirnames, filenames in os.walk(cache_dir):
        new_dirnames = []
        for d in dirnames:
            if collection_id_rex.match(d):
                yield os.path.join(dirpath, d)
            else:
                new_dirnames.append(d)
        dirnames[:] = new_dirnames


def _path_contains_entry_older_than(path, ts):
    for fn in os.listdir(path):
        if os.stat(os.path.join(path, fn)).st_mtime < ts:
            return True
    return False


def _find_collection_dirs_to_purge(collection_dirs, ts):
    for path in collection_dirs:
        try:
            if _path_contains_entry_older_than(path, ts):
                yield path
        except OSError, err:
            if err.errno != errno.ENOENT:
                log.ERROR("error while examining %r: %s" % (path, err))


def _rmtree(path):
    try:
        shutil.rmtree(path)
    except OSError, exc:
        if exc.errno != errno.ENOENT:
            log.ERROR('could not remove directory %r: %s' % (path, exc))


def purge_cache(max_age, cache_dir):
    """Remove all subdirectories of cache_dir whose mtime is before now-max_age

    @param max_age: max age of directories in seconds
    @type max_age: int

    @param cache_dir: cache directory
    @type cache_dir: basestring
    """

    for path in _find_collection_dirs_to_purge(get_collection_dirs(cache_dir), time.time() - max_age):
        _rmtree(path)

########NEW FILE########
__FILENAME__ = fetch_siteinfo
#! /usr/bin/env python

import sys
import urllib
try:
    import simplejson as json
except ImportError:
    import json

def fetch(lang):
    url = 'http://%s.wikipedia.org/w/api.php?action=query&meta=siteinfo&siprop=general|namespaces|namespacealiases|magicwords|interwikimap&format=json' % lang
    print 'fetching %r' % url
    data = urllib.urlopen(url).read()
    fn = 'siteinfo-%s.json' % lang
    print 'writing %r' % fn
    data = json.loads(data)['query']
    json.dump(data, open(fn, 'wb'), indent=4, sort_keys=True)

def main(argv):
    languages = argv[1:]
    if not languages:
        languages = "de en es fr it ja nl no pl pt simple sv".split()

    for lang in languages:
        fetch(lang.lower())

if __name__ == '__main__':
    main(sys.argv)

########NEW FILE########
__FILENAME__ = snippets
#! /usr/bin/env python

"""provide some mediawiki markup example snippets"""

import os

class snippet(object):
    def __init__(self, txt, id):
        self.txt = txt
        self.id = id
    def __repr__(self):
        return "<%s %r %r...>" % (self.__class__.__name__, self.id, self.txt[:10])
    
def get_all():
    fp = os.path.join(os.path.dirname(__file__), 'snippets.txt')
    
    examples = unicode(open(fp).read(), 'utf-8').split(unichr(12)+'\n')[1:]
    res=[]
    for i, x in enumerate(examples):
        res.append(snippet(x, i))
    return res
    
            

    

########NEW FILE########
__FILENAME__ = status

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import os
import sys
try:
    import simplejson as json
except ImportError:
    import json

from mwlib.log import Log

log = Log('mwlib.status')

class Status(object):
    qproxy = None
    stdout = sys.stdout
    
    def __init__(self,
        filename=None,
        podclient=None,
        progress_range=(0, 100),
        status=None,         
    ):
        self.filename = filename
        self.podclient = podclient
        if status is not None:
            self.status = status
        else:
            self.status = {}
        self.progress_range = progress_range

    
    def getSubRange(self, start, end):
        progress_range = (self.scaleProgress(start), self.scaleProgress(end))
        return Status(filename=self.filename, podclient=self.podclient, status=self.status, progress_range=progress_range)
    
    def scaleProgress(self, progress):
        return (
            self.progress_range[0]
            + progress*(self.progress_range[1] - self.progress_range[0])/100
            )


    def __call__(self, status=None, progress=None, article=None, auto_dump=True,
        **kwargs):
        if status is not None and status != self.status.get('status'):
            self.status['status'] = status
        
        if progress is not None:
            progress = min(max(0, progress), 100)
            progress = self.scaleProgress(progress)
            if progress > self.status.get('progress', -1):
                self.status['progress'] = progress
        
        if article is not None and article != self.status.get('article'):
            if 'article' in self.status and not article: # allow explicitly deleting the article from the status
                del self.status['article']
            else:
                self.status['article'] = article
            


        if self.podclient is not None:
            self.podclient.post_status(**self.status)

        msg = []
        msg.append("%s%%" % (self.status.get("progress", self.progress_range[0]),))
        msg.append(self.status.get("status", ""))
        msg.append(self.status.get("article", ""))
        msg = u" ".join(msg).encode("utf-8")

        if self.stdout:
            isatty = getattr(self.stdout, "isatty", None)
            if isatty and isatty():
                self.stdout.write("\x1b[K"+msg+"\r")
            else:
                self.stdout.write(msg)
            self.stdout.flush()
        
        self.status.update(kwargs)
        
        if auto_dump:
            self.dump()
    
    def dump(self):
        if not self.filename:
            return

        if not self.qproxy and self.filename.startswith("qserve://"):
            fn = self.filename[len("qserve://"):]
            host, jobid=fn.split("/")
            try:
                jobid = int(jobid)
            except ValueError:
                jobid = jobid.strip('"')
                
            if ":" in host:
                host, port = host.split(":")
                port = int(port)
            else:
                port = 14311

            from mwlib.async import rpcclient
            self.qproxy = rpcclient.serverproxy(host=host, port=port)
            self.jobid = jobid

        if self.qproxy:
            self.qproxy.qsetinfo(jobid=self.jobid, info=self.status)
            return
            
        try:    
            open(self.filename + '.tmp', 'wb').write(
                json.dumps(self.status).encode('utf-8')
            )
            os.rename(self.filename + '.tmp', self.filename)
        except Exception, exc:
            log.ERROR('Could not write status file %r: %s' % (
                self.filename, exc
            ))
    

########NEW FILE########
__FILENAME__ = strftime
#! /usr/bin/env python

# taken from http://code.activestate.com/recipes/306860/ by Andrew Dalke

# Format a datetime through its full proleptic Gregorian date range.
#
# >>> strftime(datetime.date(1850, 8, 2), "%Y/%M/%d was a %A")
# '1850/00/02 was a Friday'
# >>>

import re, datetime, time

# remove the unsupposed "%s" command.  But don't
# do it if there's an even number of %s before the s
# because those are all escaped.  Can't simply
# remove the s because the result of
#  %sY
# should be %Y if %s isn't supported, not the
# 4 digit year.
_illegal_s = re.compile(r"((^|[^%])(%%)*%s)")

def _findall(text, substr):
     # Also finds overlaps
     sites = []
     i = 0
     while 1:
         j = text.find(substr, i)
         if j == -1:
             break
         sites.append(j)
         i=j+1
     return sites

# Every 28 years the calendar repeats, except through century leap
# years where it's 6 years.  But only if you're using the Gregorian
# calendar.  ;)

def strftime(dt, fmt):
    if _illegal_s.search(fmt):
        raise TypeError("This strftime implementation does not handle %s")
    if dt.year > 1900:
        return dt.strftime(fmt)

    year = dt.year
    # For every non-leap year century, advance by
    # 6 years to get into the 28-year repeat cycle
    delta = 2000 - year
    off = 6*(delta // 100 + delta // 400)
    year = year + off

    # Move to around the year 2000
    year = year + ((2000 - year)//28)*28
    timetuple = dt.timetuple()
    s1 = time.strftime(fmt, (year,) + timetuple[1:])
    sites1 = _findall(s1, str(year))
    
    s2 = time.strftime(fmt, (year+28,) + timetuple[1:])
    sites2 = _findall(s2, str(year+28))

    sites = []
    for site in sites1:
        if site in sites2:
            sites.append(site)
            
    s = s1
    syear = "%4d" % (dt.year,)
    for site in sites:
        s = s[:site] + syear + s[site+4:]
    return s

# Make sure that the day names are in order
# from 1/1/1 until August 2000
def test():
    s = strftime(datetime.date(1800, 9, 23),
                 "%Y has the same days as 1980 and 2008")
    if s != "1800 has the same days as 1980 and 2008":
        raise AssertionError(s)

    print "Testing all day names from 0001/01/01 until 2000/08/01"
    # Get the weekdays.  Can't hard code them; they could be
    # localized.
    days = []
    for i in range(1, 10):
        days.append(datetime.date(2000, 1, i).strftime("%A"))
    nextday = {}
    for i in range(8):
        nextday[days[i]] = days[i+1]

    startdate = datetime.date(1, 1, 1)
    enddate = datetime.date(2000, 8, 1)
    prevday = strftime(startdate, "%A")
    one_day = datetime.timedelta(1)

    testdate = startdate + one_day
    while testdate < enddate:
        if (testdate.day == 1 and testdate.month == 1 and
            (testdate.year % 100 == 0)):
            print "Testing century", testdate.year
        day = strftime(testdate, "%A")
        if nextday[prevday] != day:
            raise AssertionError(str(testdate))
        prevday = day
        testdate = testdate + one_day

if __name__ == "__main__":
    test()

########NEW FILE########
__FILENAME__ = tagext
#! /usr/bin/env py.test
# -*- coding: utf-8 -*-

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.


"""
provide mechanism to support tag extensions, i.e. custom tags

List of Tag Extensions:
http://www.mediawiki.org/wiki/Category:Tag_extensions

Examples for Sites and their supported tags:
http://wikitravel.org/en/Special:Version
http://www.mediawiki.org/wiki/Special:Version
http://en.wikipedia.org/wiki/Special:Version
http://wiki.services.openoffice.org/wiki/Special:Version 
http://www.wikia.com/wiki/Special:Version
http://en.wikibooks.org/wiki/Special:Version


List of tags used by Wikia:
http://code.pediapress.com/wiki/wiki/ParserExtensionTags
"""

class ExtensionRegistry(object):
    def __init__(self):
        self.name2ext = {}

    def registerExtension(self, k):
        name = k.name
        assert name not in self.name2ext, 'tag extension for %r already registered' % (name, )
        self.name2ext[name] = k()
        return k
        
    def names(self):
        return self.name2ext.keys()

    def __getitem__(self, n):
        return self.name2ext[n]

    def __contains__(self, n):
        return n in self.name2ext
        
default_registry = ExtensionRegistry()
register = default_registry.registerExtension

def _parse(txt):
    """parse text....and try to return a 'better' (some inner) node"""
    from mwlib.refine.compat import parse_txt
    from mwlib import parser
    
    res = parse_txt(txt)
    

    # res is an parser.Article. 
    if len(res.children)!=1:
        res.__class__ = parser.Node
        return res

    res = res.children[0]
    
    if res.__class__==parser.Paragraph:
        res.__class__ = parser.Node

    return res

    # if len(res.children)!=1:
    #     return res
    # return res.children[0]

class TagExtension(object):
    name=None
    def __call__(self, source, attributes):
        return None
    def parse(self, txt):
        return _parse(txt)

class IgnoreTagBase(TagExtension):
    def __call__(self, source, attributes):
        return # simply skip for now


# ---
# --- what follows are some implementations of TagExtensions
# ---

# list of tags than can not easily be re implemented in PDFs 
tags_to_ignore = """chem chess choose dpl dynamicpagelist feyn forum
go googlemapkml graph greek ling plot ppch
randomimage schem staff teng tipa verbatim
aoaudio aovideo bloglist cgamer ggtube googlemap
gtrailer gvideo nicovideo tangler video videogallery
wegame youtube imagelink badge comments createform
fan featuredimage gallerypopulate linkfilter listpages
loggedin loggedout newusers pagequality pollembed randomfeatureduser
randomgameunit randomimagebycategory randomuserswithavatars
rss siteactivity userpoll videogallerypopulate vote
welcomeuser xsound pageby uml graphviz categorytree summary slippymap""".split()

for name in tags_to_ignore:
    def _f(name):
        class I(IgnoreTagBase):
            name = name
        register(I)
    _f(name)

class Rot13Extension(TagExtension):
    """
    example extension
    """
    name = 'rot13' # must equal the tag-name
    def __call__(self, source, attributes):
        """
        @source : unparse wikimarkup from the elements text
        @attributes: XML style attributes of this tag

        this functions builds wikimarkup and returns a parse tree for this
        """
        return self.parse("rot13(%s) is %s" % (source, source.encode('rot13')))
register(Rot13Extension)


class TimelineExtension(TagExtension):
    name = "timeline"
    def __call__(self, source, attributes):
        from mwlib.parser import Timeline
        return Timeline(source)
register(TimelineExtension)


class MathExtension(TagExtension):
    name = "math"
    def __call__(self, source, attributes):
        from mwlib.parser import Math
        return Math(source)
register(MathExtension)

class IDLExtension(TagExtension):
    # http://wiki.services.openoffice.org/wiki/Special:Version
    name = "idl"
    def __call__(self, source, attributes):
        return self.parse('<source lang="idl">%s</source>' % source)        
register(IDLExtension)

class Syntaxhighlight(TagExtension):
    '''http://www.mediawiki.org/wiki/Syntaxhighlight'''
    name = "syntaxhighlight"
    def __call__(self, source, attributes):
        return self.parse('<source%s>%s</source>' % (''.join(' %s=%s' % (k, v)
                                                               for k, v in attributes.items()),
                                                      source))
register(Syntaxhighlight)

class RDFExtension(TagExtension):
    # http://www.mediawiki.org/wiki/Extension:RDF
    # uses turtle notation :(
    name = "rdf"
    def __call__(self, source, attributes):
        #return self.parse("<!--\n%s\n -->" % source)
        return # simply skip for now, since comments are not parsed correctly

register(RDFExtension)

class HieroExtension(TagExtension):
    name = "hiero"
    def __call__(self, source, attributes):
        from mwlib import parser
        tn = parser.TagNode("hiero")
        tn.children.append(parser.Text(source))
        return tn   
register(HieroExtension)

class LabledSectionTransclusionExtensionHotFix(IgnoreTagBase):
    #http://www.mediawiki.org/wiki/Extension:Labeled_Section_Transclusion
    name = "section"
register(LabledSectionTransclusionExtensionHotFix)

# --- wiki travel extensions ----

class ListingExtension(TagExtension):
    " http://wikitravel.org/en/Wikitravel:Listings "
    name = "listing"
    attrs = [(u"name",u"'''%s'''"),
             ("alt",u"(''%s'')"),
             ("address",u", %s"),
             ("directions",u" (''%s'')"),
             ("phone", u", ☎ %s"),
             ("fax", u", fax: %s"),
             ("email", u", e-mail: %s"),
             ("url", u", %s"),
             ("hours", u", %s"),
             ("price", u", %s"),
             # ("lat", u", Latitude: %s"), # disable in print as it is disabled in WP as well
             # ("long", u", Longitude: %s"),
             ("tags", u", Tags: %s")]
    def __call__(self, source, attributes):
        t = u"".join(v%attributes[k] for k,v in self.attrs if attributes.get(k,None))
        if source:
            t += u", %s" % source
        return self.parse(t)

register(ListingExtension)

class SeeExtension(ListingExtension):
    name = "see"
register(SeeExtension)

class BuyExtension(ListingExtension):
    name = "buy"
register(BuyExtension)

class DoExtension(ListingExtension):
    name = "do"
register(DoExtension)

class EatExtension(ListingExtension):
    name = "eat"
register(EatExtension)

class DrinkExtension(ListingExtension):
    name = "drink"
register(DrinkExtension)

class SleepExtension(ListingExtension):
    name = "sleep"
register(SleepExtension)


#print default_registry.name2ext

########NEW FILE########
__FILENAME__ = evaluate

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

from mwlib.templ import magics, log, DEBUG, parser, mwlocals
from mwlib.uniq import Uniquifier
from mwlib import nshandling, siteinfo, metabook

class TemplateRecursion(Exception): pass

def flatten(node, expander, variables, res):
    t=type(node)
    if isinstance(node, (unicode, str)):
        res.append(node)
        return True

    if expander.recursion_count > expander.recursion_limit:
        raise TemplateRecursion()
    
    
    expander.recursion_count += 1
    try:
        before = variables.count
        oldlen = len(res)
        try:
            if t is list or t is tuple:
                for x in node:
                    flatten(x, expander, variables, res)
            else:
                node.flatten(expander, variables, res)
        except TemplateRecursion:
            if expander.recursion_count > 2:
                raise
            del res[oldlen:]
            log.warn("template recursion error ignored")
        after = variables.count
        return before==after
    finally:
        expander.recursion_count -= 1
        
        
class MemoryLimitError(Exception):
    pass


def equalsplit(node):
    if isinstance(node, basestring):
        return None, node

    try:
        idx = node.index(eqmark)
    except ValueError:
        return None, node

    return node[:idx], node[idx+1:]

    
def equalsplit_25(node):
    if isinstance(node, basestring):
        return None, node

    try:
        idx = list(node).index(eqmark)
    except ValueError:
        return None, node

    return node[:idx], node[idx+1:]
    
if not hasattr(tuple, 'index'):
    equalsplit = equalsplit_25

class ArgumentList(object):
    def __init__(self, args=tuple(), expander=None, variables=None):
        self.args = tuple(args)
        

        assert expander is not None
        #assert variables is not None
        
        self.expander = expander
        self.variables = variables
        self.varcount = 1
        self.varnum = 0
        
        self.namedargs = {}
        self.count = 0
            
    def __len__(self):
        self.count += 1
        return len(self.args)

    def __getitem__(self, n):
        self.count += 1
        if isinstance(n, slice):
            start = n.start or 0
            stop = n.stop or len(self)
            return [self.get(x,  None) or u"" for x in range(start, stop)]
        return self.get(n, None) or u''
        
    def get(self, n, default):
        self.count += 1
        if isinstance(n, (int, long)):
            try:
                a=self.args[n]
            except IndexError:
                return default
            if isinstance(a, unicode):
                return a.strip()
            tmp = []
            flatten(a, self.expander, self.variables, tmp)
            _insert_implicit_newlines(tmp)
            tmp = u"".join(tmp).strip()
            if len(tmp)>256*1024:
                raise MemoryLimitError("template argument too long: %s bytes" % len(tmp))
            # FIXME: cache value ???
            return tmp

        assert isinstance(n, basestring), "expected int or string"

        if n not in self.namedargs:
            while self.varnum < len(self.args):
                arg = self.args[self.varnum]
                self.varnum += 1

                name, val = equalsplit(arg)
                if name is not None:
                    tmp = []
                    flatten(name, self.expander, self.variables, tmp)
                    _insert_implicit_newlines(tmp)
                    name = u"".join(tmp).strip()
                    do_strip = True
                else:
                    name = str(self.varcount)
                    self.varcount+=1
                    do_strip = False

                if do_strip and isinstance(val, unicode):
                    val = val.strip()
                self.namedargs[name] = (do_strip, val)
                
                if n==name:
                    break

        try:
            do_strip, val = self.namedargs[n]
            if isinstance(val, unicode):
                return val
        except KeyError:
            return default

        tmp = []
        flatten(val, self.expander, self.variables, tmp)
        _insert_implicit_newlines(tmp)
        tmp=u"".join(tmp)
        if do_strip:
            tmp = tmp.strip()
            
        self.namedargs[n] = (do_strip, tmp)
        return tmp
    
def is_implicit_newline(raw):
    """should we add a newline to templates starting with *, #, :, ;, {|
    see: http://meta.wikimedia.org/wiki/Help:Newlines_and_spaces#Automatic_newline_at_the_start
    """
    sw = raw.startswith
    for x in ('*', '#', ':', ';', '{|'):
        if sw(x):
            return True
    return False 

from mwlib.templ.marks import mark, mark_start, mark_end, mark_maybe_newline, maybe_newline, dummy_mark, eqmark

def _insert_implicit_newlines(res, maybe_newline=maybe_newline):
    # do not pass the second argument
    res.append(dummy_mark)
    res.append(dummy_mark)

    for i, p in enumerate(res):
        if p is maybe_newline:
            s1 = res[i+1]
            s2 = res[i+2]
            if i and res[i-1].endswith("\n"):
                continue
            
            if isinstance(s1, mark):
                continue
            if len(s1)>=2:
                if is_implicit_newline(s1):
                    res[i] = '\n'
            else:
                if is_implicit_newline(''.join([s1, s2])):
                    res[i] = '\n'
    del res[-2:]

    
class Expander(object):
    magic_displaytitle = None   # set via {{DISPLAYTITLE:...}}
    def __init__(self, txt, pagename="", wikidb=None, recursion_limit=100):
        assert wikidb is not None, "must supply wikidb argument in Expander.__init__"
        self.pagename = pagename
        self.db = wikidb
        self.uniquifier = Uniquifier()

        si = None
        try:
            si = self.db.get_siteinfo()
        except Exception, err:
            print 'Caught: %s' % err

        if si is None:
            print "WARNING: failed to get siteinfo from %r" % (self.db,)
            si = siteinfo.get_siteinfo("de")
            
        self.nshandler = nshandler = nshandling.nshandler(si)
        self.siteinfo = si

        if self.db and hasattr(self.db, "getSource"):
            source = self.db.getSource(pagename) or metabook.source()
            local_values = source.locals or u""
            local_values = mwlocals.parse_locals(local_values)
        else:
            local_values = None
            source = {}

        # XXX we really should call Expander with a nuwiki.page object.
        revisionid = 0
        if self.db and hasattr(self.db, "nuwiki") and pagename:
            page = self.db.nuwiki.get_page(self.pagename)
            if page is not None:
                revisionid = getattr(page, 'revid', 0) or 0

        self.resolver = magics.MagicResolver(pagename=pagename, revisionid=revisionid)
        self.resolver.siteinfo = si
        self.resolver.nshandler = nshandler
        
        self.resolver.wikidb = wikidb
        self.resolver.local_values = local_values
        self.resolver.source = source
        
        self.recursion_limit = recursion_limit
        self.recursion_count = 0
        self.aliasmap = parser.aliasmap(self.siteinfo)

        self.parsed = parser.parse(txt, included=False, replace_tags=self.replace_tags, siteinfo=self.siteinfo)
        #show(self.parsed)
        self.parsedTemplateCache = {}

    def resolve_magic_alias(self, name):
        return self.aliasmap.resolve_magic_alias(name)

    def replace_tags(self, txt):
        return self.uniquifier.replace_tags(txt)
        
    def getParsedTemplate(self, name):
        if not name or name.startswith("[[") or "|" in name:
            return None


        if name.startswith("/"):
            name = self.pagename+name
            ns = 0
        else:
            ns = 10

        try:
            return self.parsedTemplateCache[name]
        except KeyError:
            pass

        page = self.db.normalize_and_get_page(name, ns)
        if page:
            raw = page.rawtext
        else:
            raw = None
            
        if raw is None:
            res = None
        else:
            res = self._parse_raw_template(name=name, raw=raw)
                
        self.parsedTemplateCache[name] = res
        return res

    def _parse_raw_template(self, name, raw):
        return parser.parse(raw, replace_tags=self.replace_tags)
    
    def _expand(self, parsed, keep_uniq=False):
        res = ["\n"] # guard, against implicit newlines at the beginning
        flatten(parsed, self, ArgumentList(expander=self), res)
        _insert_implicit_newlines(res)
        res[0] = u''
        res = u"".join(res)
        if not keep_uniq:
            res=self.uniquifier.replace_uniq(res)
        return res
        
    def parseAndExpand(self, txt, keep_uniq=False):
        parsed = parser.parse(txt, included=False, replace_tags=self.replace_tags)
        return self._expand(parsed, keep_uniq=keep_uniq)
    
    def expandTemplates(self, keep_uniq=False):
        return self._expand(self.parsed, keep_uniq=keep_uniq)

########NEW FILE########
__FILENAME__ = magics
#! /usr/bin/env python

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

"""expand magic variables/colon functions
http://meta.wikimedia.org/wiki/Help:Colon_function
http://meta.wikimedia.org/wiki/Help:Magic_words
http://meta.wikimedia.org/wiki/ParserFunctions
"""

import re, datetime, urllib, urlparse
from mwlib.log import Log
from mwlib import expr

iferror_rx = re.compile(r'<(div|span|p|strong)\s[^<>]*class="error"[^<>]*>', re.I)

log = Log("expander")


def singlearg(fun):
    def wrap(self, args):
        rl = args
        if not rl:
            a = u''
        else:
            a = rl[0]

        return fun(self, a)

    return wrap


def noarg(fun):
    def wrap(self, *args):
        return fun(self)
    return wrap


def as_numeric(x):
    try:
        return int(x)
    except ValueError:
        pass
    return float(x)


def maybe_numeric_compare(a, b):
    if a == b:
        return True
    try:
        try:
            a = int(a)
        except ValueError:
            a = float(a)
        try:
            b = int(b)
        except ValueError:
            b = float(b)
    except ValueError:
        return False

    return a == b


def urlquote(u):
    if isinstance(u, unicode):
        u = u.encode('utf-8')
    return urllib.quote(u)


class OtherMagic(object):
    def DEFAULTSORT(self, args):
        """see http://en.wikipedia.org/wiki/Template:DEFAULTSORT"""
        return u""


class TimeMagic(object):
    utcnow = datetime.datetime.utcnow()

    @noarg
    def CURRENTDAY(self):
        """Displays the current day in numeric form."""
        return "%s" % self.utcnow.day

    @noarg
    def CURRENTDAY2(self):
        """[MW1.5+] Ditto with leading zero 01 .. 31)."""
        return "%02d" % self.utcnow.day

    @noarg
    def CURRENTDAYNAME(self):
        """Displays the current day in named form."""
        return self.utcnow.strftime("%A")

    @noarg
    def CURRENTDOW(self):
        """current day as number (0=Sunday, 1=Monday...)."""
        return str((self.utcnow.weekday() + 1) % 7)

    @noarg
    def CURRENTMONTH(self):
        """The number 01 .. 12 of the current month."""
        return "%02d" % self.utcnow.month

    @noarg
    def CURRENTMONTHABBREV(self):
        """[MW1.5+] current month abbreviated Jan .. Dec."""
        return self.utcnow.strftime("%b")

    @noarg
    def CURRENTMONTHNAME(self):
        """current month in named form January .. December.   """
        return self.utcnow.strftime("%B")

    @noarg
    def CURRENTTIME(self):
        """The current time of day (00:00 .. 23:59)."""
        return self.utcnow.strftime("%H:%M")

    @noarg
    def CURRENTWEEK(self):
        """Number of the current week (1-53) according to ISO 8601 with no leading zero."""
        return str(self.utcnow.isocalendar()[1])

    @noarg
    def CURRENTYEAR(self):
        """Returns the current year."""
        return str(self.utcnow.year)

    @noarg
    def CURRENTTIMESTAMP(self):
        """[MW1.7+] Returns the current time stamp. e.g.: 20060528125203"""
        return self.utcnow.strftime("%Y%m%d%H%M%S")


class LocaltimeMagic(object):
    now = datetime.datetime.now()

    @noarg
    def LOCALDAY(self):
        """Displays the current day in numeric form."""
        return "%s" % self.now.day

    @noarg
    def LOCALDAY2(self):
        """[MW1.5+] Ditto with leading zero 01 .. 31)."""
        return "%02d" % self.now.day

    @noarg
    def LOCALDAYNAME(self):
        """Displays the current day in named form."""
        return self.now.strftime("%A")

    @noarg
    def LOCALDOW(self):
        """current day as number (0=Sunday, 1=Monday...)."""
        return str((self.now.weekday() + 1) % 7)

    @noarg
    def LOCALMONTH(self):
        """The number 01 .. 12 of the current month."""
        return "%02d" % self.now.month

    @noarg
    def LOCALMONTHABBREV(self):
        """[MW1.5+] current month abbreviated Jan .. Dec."""
        return self.now.strftime("%b")

    @noarg
    def LOCALMONTHNAME(self):
        """current month in named form January .. December.   """
        return self.now.strftime("%B")

    @noarg
    def LOCALTIME(self):
        """The current time of day (00:00 .. 23:59)."""
        return self.now.strftime("%H:%M")

    @noarg
    def LOCALWEEK(self):
        """Number of the current week (1-53) according to ISO 8601 with no leading zero."""
        return str(self.now.isocalendar()[1])

    @noarg
    def LOCALYEAR(self):
        """Returns the current year."""
        return str(self.now.year)

    @noarg
    def LOCALTIMESTAMP(self):
        """[MW1.7+] Returns the current time stamp. e.g.: 20060528125203"""
        return self.now.strftime("%Y%m%d%H%M%S")

from functools import wraps


class PageMagic(object):
    source = {}

    def __init__(self, pagename='', server="http://en.wikipedia.org", revisionid=0):
        self.pagename = pagename
        self.server = server
        self.revisionid = revisionid

        self.niceurl = urlparse.urljoin(self.server, 'wiki')

    def _wrap_pagename(f):
        @wraps(f)
        def wrapper(self, args):
            pagename = self.pagename
            if args.args:
                pagename = args[0]
            return f(self, pagename)
        return wrapper

    def _quoted(f):
        @wraps(f)
        def wrapper(*args, **kwargs):
            return urlquote(f(*args, **kwargs).replace(' ', '_'))
        return wrapper

    @_wrap_pagename
    def PAGENAME(self, pagename):
        """Returns the name of the current page, including all levels (Title/Subtitle/Sub-subtitle)"""
        return self.nshandler.splitname(pagename)[1]

    """same as PAGENAME but More URL-friendly percent encoded
    special characters (To use an articlename in an external link).
    """
    PAGENAMEE = _quoted(PAGENAME)

    @_wrap_pagename
    def FULLPAGENAME(self, pagename):
        return pagename

    FULLPAGENAMEE = _quoted(FULLPAGENAME)

    @_wrap_pagename
    def SUBPAGENAME(self, pagename):
        """[MW1.6+] Returns the name of the current page, excluding parent
        pages ('Title/Subtitle' becomes 'Subtitle').
        """
        return pagename.split('/')[-1]

    SUBPAGENAMEE = _quoted(SUBPAGENAME)

    @_wrap_pagename
    def BASEPAGENAME(self, pagename):
        """[MW1.7+] The basename of a subpage ('Title/Subtitle' becomes 'Title')
        """
        return self.nshandler.splitname(pagename)[1].rsplit('/', 1)[0]

    BASEPAGENAMEE = _quoted(BASEPAGENAME)

    @_wrap_pagename
    def NAMESPACE(self, pagename):
        """Returns the name of the namespace the current page resides in."""
        ns, partial, full = self.nshandler.splitname(pagename)
        return full[:-len(partial) - 1]

    NAMESPACEE = _quoted(NAMESPACE)

    @_wrap_pagename
    def TALKSPACE(self, pagename):
        ns, partial, fullname = self.nshandler.splitname(pagename)
        if not ns % 2:
            ns += 1
        return self.nshandler.get_nsname_by_number(ns)

    TALKSPACEE = _quoted(TALKSPACE)

    @_wrap_pagename
    def SUBJECTSPACE(self, pagename):
        ns, partial, fullname = self.nshandler.splitname(pagename)
        if ns % 2:
            ns -= 1
        return self.nshandler.get_nsname_by_number(ns)

    SUBJECTSPACEE = _quoted(SUBJECTSPACE)
    ARTICLESPACE = SUBJECTSPACE
    ARTICLESPACEE = SUBJECTSPACEE

    @_wrap_pagename
    def TALKPAGENAME(self, pagename):
        ns, partial, fullname = self.nshandler.splitname(pagename)
        if not ns % 2:
            ns += 1
        return self.nshandler.get_nsname_by_number(ns) + ':' + partial

    TALKPAGENAMEE = _quoted(TALKPAGENAME)

    @_wrap_pagename
    def SUBJECTPAGENAME(self, pagename):
        ns, partial, fullname = self.nshandler.splitname(pagename)
        if ns % 2:
            ns -= 1
        return self.nshandler.get_nsname_by_number(ns) + ':' + partial

    SUBJECTPAGENAMEE = _quoted(SUBJECTPAGENAME)
    ARTICLEPAGENAME = SUBJECTPAGENAME
    ARTICLEPAGENAMEE = SUBJECTPAGENAMEE

    def REVISIONID(self, args):
        """[MW1.5+] The unique identifying number of a page, see Help:Diff."""
        return str(self.revisionid)

    @noarg
    def SITENAME(self):
        """Value of $wgSitename."""
        return ""

    def NS(self, args):
        """Returns the name of a given namespace number."""
        try:
            namespaces = self.siteinfo["namespaces"]
        except (AttributeError, KeyError):
            from mwlib import siteinfo
            namespaces = siteinfo.get_siteinfo("en")["namespaces"]

        ns = args[0]
        try:
            retval = namespaces[ns]['*']
        except KeyError:
            retval = ''

        return retval

    def LOCALURL(self, args):
        """Returns the local URL of a given page. The page might not exist."""
        url = "/wiki/" + "".join(args.get(0, u""))
        return url

    def LOCALURLE(self, args):
        """Returns the local URL of a given page. The page might not exist."""
        return urlquote(self.LOCALURL(args))

    def URLENCODE(self, args):
        """[MW1.7+] To use a variable (parameter in a template) with spaces in an external link."""
        url = urllib.quote_plus(args[0].encode('utf-8'))
        return url

    @noarg
    def SERVER(self):
        """Value of $wgServer"""
        return self.server

    def FULLURL(self, args):
        a = args[0].capitalize().replace(' ', '_')
        a = urllib.quote_plus(a.encode('utf-8'))
        if len(args) >= 2:
            q = "?%s" % args[1]
        else:
            q = ""
        return '%s/%s%s' % (self.niceurl, a, q)

    @noarg
    def SERVERNAME(self):
        return self.server[len('http://'):]


class NumberMagic(object):
    def NUMBEROFARTICLES(self, args):
        """A variable which returns the total number of articles on the Wiki."""
        return "0"

    def NUMBEROFPAGES(self, args):
        """[MW1.7+] Returns the total number of pages. """
        return "0"

    def NUMBEROFFILES(self, args):
        """[MW1.5+] Returns the number of uploaded files (rows in the image table)."""
        return "0"

    def NUMBEROFUSERS(self, args):
        """[MW1.7+] Returns the number of registered users (rows in the user table)."""
        return "0"

    def CURRENTVERSION(self, args):
        """[MW1.7+] Returns the current version of MediaWiki being run. [5]"""
        return "1.7alpha"


class StringMagic(object):
    @singlearg
    def LC(self, a):
        return a.lower()

    @singlearg
    def UC(self, a):
        return a.upper()

    @singlearg
    def LCFIRST(self, a):
        return a[:1].lower() + a[1:]

    @singlearg
    def UCFIRST(self, a):
        return a[:1].upper() + a[1:]

    def PADLEFT(self, args):
        s = args[0]
        try:
            width = int(args[1])
        except ValueError:
            return s

        fillstr = args[2] or u'0'
        return ''.join([fillstr[i % len(fillstr)] for i in range(width - len(s))]) + s

    def PADRIGHT(self, args):
        s = args[0]
        try:
            width = int(args[1])
        except ValueError:
            return s

        fillstr = args[2] or u'0'
        return s + ''.join([fillstr[i % len(fillstr)] for i in range(width - len(s))])


class ParserFunctions(object):
    wikidb = None

    def _error(self, s):
        return '<strong class="error">%s</strong>' % (s,)

    def LANGUAGE(self, args):
        """implement http://meta.wikimedia.org/wiki/Help:Parser_function#.23language:"""

        return args[0]  # FIXME this is just a dummy implementation.

    def TAG(self, args):
        name = args[0].strip()
        r = u"<%s>%s</%s>" % (name, args[1], name)
        return r

    def IFEXIST(self, args):
        name = args[0]
        if not name or not self.wikidb:
            return args.get(args[2], "")

        nsnum, suffix, full = self.wikidb.nshandler.splitname(name)
        if nsnum == -2:
            exists = bool(self.wikidb.normalize_and_get_image_path(name.split(":")[1]))
        else:
            exists = bool(self.wikidb.normalize_and_get_page(name, 0))

        if exists:
            return args[1]
        else:
            return args[2]

    def EXPR(self, rl):
        import math
        if rl:
            try:
                ex = rl[0].strip()
                if not ex:
                    return u""
                val = expr.expr(ex)
                if int(val) == val and math.fabs(val) < 1e14:
                    return str(int(val))
                r = str(float(val))
            except Exception, err:
                # log("ERROR: error while evaluating #expr:%r\n" % (ex,))
                return self._error(err)

            if "e" in r:
                f, i = r.split("e")
                i = int(i)
                if i < 0:
                    sign = ''
                else:
                    sign = '+'
                fixed = str(float(f)) + "E" + sign + str(int(i))
                return fixed
            return r
        return u"0"

    def IFEXPR(self, rl):
        try:
            ex = rl[0].strip()
            if ex:
                r = expr.expr(rl[0])
            else:
                r = False
        except Exception, err:
            # log("ERROR: error while evaluating #ifexpr:%r\n" % (rl[0],))
            return self._error(err)

        if r:
            return rl[1]
        else:
            return rl[2]

    def TITLEPARTS(self, args):
        title = args[0]
        try:
            numseg = int(args[1])
        except ValueError:
            numseg = 0

        try:
            start = int(args[2])
        except ValueError:
            start = 1

        if start > 0:
            start -= 1

        parts = title.split("/")[start:]
        if numseg:
            parts = parts[:numseg]
        return "/".join(parts)

    def IFERROR(self, args):
        val = args[0]
        bad = args[1]
        good = args.get(2, None)
        if good is None:
            good = val

        if iferror_rx.search(val):
            return bad
        else:
            return good


for x in dir(ParserFunctions):
    if x.startswith("_"):
        continue
    setattr(ParserFunctions, "#" + x, getattr(ParserFunctions, x))
    delattr(ParserFunctions, x)


class DummyResolver(object):
    pass


class MagicResolver(TimeMagic, LocaltimeMagic, PageMagic, NumberMagic, StringMagic, ParserFunctions, OtherMagic, DummyResolver):
    local_values = None

    def __call__(self, name, args):
        try:
            name = str(name)
        except UnicodeEncodeError:
            return None

        upper = name.upper()

        if self.local_values:
            try:
                return self.local_values[upper]
            except KeyError:
                pass

        m = getattr(self, upper, None)
        if m is None:
            return None

        if isinstance(m, basestring):
            return m

        res = m(args) or ''  # FIXME: catch TypeErros
        assert isinstance(res, basestring), "MAGIC %r returned %r" % (name, res)
        return res

    def has_magic(self, name):
        try:
            name = str(name)
        except UnicodeEncodeError:
            return False

        m = getattr(self, name.upper(), None)
        return m is not None


magic_words = ['basepagename', 'basepagenamee', 'contentlanguage', 'currentday', 'currentday2', 'currentdayname', 'currentdow', 'currenthour', 'currentmonth', 'currentmonthabbrev', 'currentmonthname', 'currentmonthnamegen', 'currenttime', 'currenttimestamp', 'currentversion', 'currentweek', 'currentyear', 'defaultsort', 'directionmark', 'displaytitle', 'fullpagename', 'fullpagenamee', 'language', 'localday', 'localday2', 'localdayname', 'localdow', 'localhour', 'localmonth', 'localmonthabbrev', 'localmonthname', 'localmonthnamegen', 'localtime', 'localtimestamp', 'localweek', 'localyear', 'namespace', 'namespacee', 'newsectionlink', 'numberofadmins', 'numberofarticles', 'numberofedits', 'numberoffiles', 'numberofpages', 'numberofusers', 'pagename', 'pagenamee', 'pagesinnamespace', 'revisionday', 'revisionday2', 'revisionid', 'revisionmonth', 'revisiontimestamp', 'revisionyear', 'scriptpath', 'server', 'servername', 'sitename', 'subjectpagename', 'subjectpagenamee', 'subjectspace', 'subjectspacee', 'subpagename', 'subpagenamee', 'talkpagename', 'talkpagenamee', 'talkspace', 'talkspacee', 'urlencode']


def _populate_dummy():
    m = MagicResolver()

    def get_dummy(name):
        def resolve(*args):
            log.warn("using dummy resolver for %s" % (name,))
            return u""
        return resolve

    missing = set()
    for x in magic_words:
        if not m.has_magic(x):
            missing.add(x)
            setattr(DummyResolver, x.upper(), get_dummy(x))

    if missing:
        missing = list(missing)
        missing.sort()
        #log.info("installed dummy resolvers for %s" % (", ".join(missing),))

_populate_dummy()

########NEW FILE########
__FILENAME__ = magic_nodes
import locale
from xml.sax.saxutils import quoteattr
from mwlib.templ import nodes, evaluate


class Subst(nodes.Node):
    def flatten(self, expander, variables, res):
        name = []
        evaluate.flatten(self[0], expander, variables, name)
        name = u"".join(name).strip()

        res.append("{{subst:%s}}" % (name,))


class Safesubst(nodes.Template):
    def _get_args(self):
        return self[1:]


class Time(nodes.Node):
    def flatten(self, expander, variables, res):
        format = []
        evaluate.flatten(self[0], expander, variables, format)
        format = u"".join(format).strip()

        if len(self) > 1:
            d = []
            evaluate.flatten(self[1], expander, variables, d)
            d = u"".join(d).strip()
        else:
            d = None

        from mwlib.templ import magic_time
        res.append(magic_time.time(format, d))


class Anchorencode(nodes.Node):
    def flatten(self, expander, variables, res):
        arg = []
        evaluate.flatten(self[0], expander, variables, arg)
        arg = u"".join(arg)

        # Note: mediawiki has a bug. It tries not to touch colons by replacing '.3A' with
        # with the colon. However, if the original string contains the substring '.3A',
        # it will also replace it with a colon. We do *not* reproduce that bug here...
        import urllib
        e = urllib.quote_plus(arg.encode('utf-8'), ':').replace('%', '.').replace('+', '_')
        res.append(e)


def _rel2abs(rel, base):
    rel = rel.rstrip("/")
    if rel in (u"", "."):
        return base
    if not (rel.startswith("/") or rel.startswith("./") or rel.startswith("../")):
        base = u""

    import posixpath
    p = posixpath.normpath("/%s/%s/" % (base, rel)).strip("/")
    return p


class rel2abs(nodes.Node):
    def flatten(self, expander, variables, res):
        arg = []
        evaluate.flatten(self[0], expander, variables, arg)
        arg = u"".join(arg).strip()

        arg2 = []
        if len(self) > 1:
            evaluate.flatten(self[1], expander, variables, arg2)
        arg2 = u"".join(arg2).strip()
        if not arg2:
            arg2 = expander.pagename

        res.append(_rel2abs(arg, arg2))


class Tag(nodes.Node):
    def flatten(self, expander, variables, res):
        name = []
        evaluate.flatten(self[0], expander, variables, name)
        name = u"".join(name).strip()
        parameters = u''

        for parm in self[2:]:
            tmp = []
            evaluate.flatten(parm, expander, variables, tmp)
            evaluate._insert_implicit_newlines(tmp)
            tmp = u"".join(tmp)
            if "=" in tmp:
                key, value = tmp.split("=", 1)
                parameters += " %s=%s" % (key, quoteattr(value))

        tmpres = []
        tmpres.append("<%s%s>" % (name, parameters))

        if len(self) > 1:
            tmp = []
            evaluate.flatten(self[1], expander, variables, tmp)
            evaluate._insert_implicit_newlines(tmp)
            tmp = u"".join(tmp)
            tmpres.append(tmp)

        tmpres.append("</%s>" % (name,))
        tmpres = u"".join(tmpres)
        tmpres = expander.uniquifier.replace_tags(tmpres)
        res.append(tmpres)


class NoOutput(nodes.Node):
    def flatten(self, expander, variables, res):
        pass


class Defaultsort(NoOutput):
    pass


class Displaytitle(nodes.Node):
    def flatten(self, expander, variables, res):
        name = []
        evaluate.flatten(self[0], expander, variables, name)
        name = u"".join(name).strip()
        expander.magic_displaytitle = name


def reverse_formatnum(val):
    try:
        return str(locale.atoi(val))
    except ValueError:
        pass

    try:
        return str(locale.atof(val))
    except ValueError:
        pass

    return val


def _formatnum(val):
    try:
        val = long(val)
    except ValueError:
        pass
    else:
        return locale.format("%d", val, True)

    try:
        val = float(val)
    except ValueError:
        return val

    return locale.format("%g", val, True)


def formatnum(val):
    res = _formatnum(val)
    if isinstance(res, str):
        return unicode(res, "utf-8", "replace")
    else:
        return res

class Formatnum(nodes.Node):
    def flatten(self, expander, variables, res):
        arg0 = []
        evaluate.flatten(self[0], expander, variables, arg0)
        arg0 = u"".join(arg0)

        if len(self) > 1:
            arg1 = []
            evaluate.flatten(self[1], expander, variables, arg1)
            arg1 = u"".join(arg1)
        else:
            arg1 = u""

        if arg1.strip() in (u"r", u"R"):
            res.append(reverse_formatnum(arg0))
        else:
            res.append(formatnum(arg0))

        # print "FORMATNUM:", (arg0, arg1, res[-1])


def make_switchnode(args):
    return nodes.SwitchNode((args[0], args[1:]))

registry = {'#time': Time,
            'subst': Subst,
            'safesubst': Safesubst,
            'anchorencode': Anchorencode,
            '#tag': Tag,
            'displaytitle': Displaytitle,
            'defaultsort': Defaultsort,
            '#rel2abs': rel2abs,
            '#switch': make_switchnode,
            '#if': nodes.IfNode,
            '#ifeq': nodes.IfeqNode,
            'formatnum': Formatnum
            }

########NEW FILE########
__FILENAME__ = magic_time
import sys
import datetime
import re
import calendar
import roman
from timelib import strtodatetime as parsedate

from mwlib.strftime import strftime

def ampm(date):
    if date.hour < 12:
        return "am"
    else:
        return "pm"

rx = re.compile('"[^"]*"|xr|\\\\.|.')
codemap = dict(
    y = '%y',
    Y = '%Y',
    n = lambda d: str(d.month),
    m = '%m',
    M = '%b',
    F = '%B',
    W = lambda d: "%02d" % (d.isocalendar()[1],),
    j = lambda d: str(d.day),
    d = '%d',
    z = lambda d: str(d.timetuple().tm_yday-1),
    D = '%a',
    l = '%A',
    N = lambda d: str(d.isoweekday()),
    w = lambda d: str(d.isoweekday() % 7),
    a = ampm,
    A = lambda d: ampm(d).upper(),
    g = lambda d: str(((d.hour-1) % 12) + 1),
    h = "%I",
    G = lambda d: str(d.hour),
    H = lambda d: "%02d" % (d.hour,),
    i = '%M',
    s = '%S',
    U = lambda d: str(calendar.timegm(d.timetuple())),
    L = lambda d: str(int(calendar.isleap(d.year))),
    c = "%Y-%m-%dT%H:%M:%S+00:00",
    r = "%a, %d %b %Y %H:%M:%S +0000",
    t = lambda d: str(calendar.monthrange(d.year, d.month)[1]),
    xr = ("process_next", lambda n: roman.toRoman(int(n))),
    )


def formatdate(format, date):
    split = rx.findall(format)
    process_next = None

    tmp = []
    for x in split:
        f = codemap.get(x, None)
        if f is None:
            if len(x)==2 and x.startswith("\\"):
                tmp.append(x[1])
            elif len(x)>=2 and x.startswith('"'):
                tmp.append(x[1:-1])
            else:
                tmp.append(x)
        else:
            if isinstance(f, tuple):
                process_next = f[1]
                continue

            if isinstance(f, basestring):
                res = strftime(date, f)
            else:
                res = f(date)

            if process_next:
                try:
                    res = process_next(res)
                except ValueError:
                    pass
                process_next = None
            tmp.append(res)

    tmp = u"".join(tmp).strip()
    return tmp

def time(format, datestring=None):
    date = None
    if datestring:
        if re.match("\d\d\d\d$", datestring):
            try:
                date = datetime.datetime.now().replace(hour=int(datestring[:2]), minute=int(datestring[2:]), second=0)
            except ValueError:
                pass
        
        if date is None:
            try:
                date = parsedate(datestring)
            except ValueError:
                pass
            except Exception, err:
                sys.stderr.write("ERROR in parsedate: %r while parsing %r" % (err, datestring))
                pass

        if date is None:
            return  u'<strong class="error">Error: invalid time</strong>'
        
    if date is None:
        date = datetime.datetime.now()

    return formatdate(format, date)

########NEW FILE########
__FILENAME__ = marks

class mark(unicode):
    def __new__(klass, msg):
        r=unicode.__new__(klass)
        r.msg = msg
        return r
    
    def __repr__(self):
        return '<%s %r>' % (self.__class__.__name__, self.msg,)

class mark_start(mark): pass
class mark_end(mark): pass
class mark_maybe_newline(mark): pass

maybe_newline = mark_maybe_newline('maybe_newline')
dummy_mark = mark('dummy')

class _eqmark(unicode):
    def __eq__(self, other):
        return self is other

eqmark = _eqmark("=")

########NEW FILE########
__FILENAME__ = misc

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

from mwlib.siteinfo import get_siteinfo
from mwlib.templ import evaluate

class page(object):
    def __init__(self, rawtext):
        self.rawtext = rawtext
        
                 
class DictDB(object):
    """wikidb implementation used for testing"""
    def __init__(self, *args, **kw):
        if args:
            self.d, = args
        else:
            self.d = {}
        
        self.d.update(kw)

        normd = {}
        for k, v in self.d.items():
            normd[k.lower().replace(" ",  "_")] = v
        self.d = normd

        self.siteinfo = get_siteinfo('de')

    def normalize_and_get_page(self, title, defaultns=0):
        return page(self.d.get(title.lower().replace(" ", "_"), u""))
        
    def get_siteinfo(self):
        return self.siteinfo


    
def expandstr(s, expected=None, wikidb=None, pagename='thispage'):
    """debug function. expand templates in string s"""
    if wikidb:
        db = wikidb
    else:
        db = DictDB(dict(a=s))

    te = evaluate.Expander(s, pagename=pagename, wikidb=db)
    res = te.expandTemplates()
    print "EXPAND: %r -> %r" % (s, res)
    if expected is not None:
        assert res==expected, "expected %r, got %r" % (expected, res)
    return res

########NEW FILE########
__FILENAME__ = mwlocals
def get_locals_txt():
    names = """LOCALDAY LOCALDAY2 LOCALDAYNAME LOCALDOW LOCALMONTH
LOCALMONTHABBREV LOCALMONTHNAME LOCALTIME LOCALYEAR LOCALTIMESTAMP
NUMBEROFARTICLES NUMBEROFPAGES NUMBEROFFILES NUMBEROFUSERS CURRENTVERSION
"""
    names = [x for x in names.split() if x]

    return "\n----\n".join(["%s={{%s}}" % (x, x) for x in names]+["{{LOCALVARS}}\n"])

def parse_locals(localstr):
    if isinstance(localstr, str):
        localstr = unicode(localstr)
    res = {}
    for x in localstr.split("\n----\n"):
        try:
            name, val = x.split('=', 1)
        except ValueError:
            continue
        if name:
            res[name] = val
    return res

########NEW FILE########
__FILENAME__ = nodes

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

from mwlib.templ import magics


class Node(tuple):
    def __eq__(self, other):
        return type(self)==type(other) and tuple.__eq__(self, other)    
    def __ne__(self, other):
        return type(self)!=type(other) or tuple.__ne__(self, other)
    
    def __repr__(self):
        return "%s%s" % (self.__class__.__name__, tuple.__repr__(self))
    
    def show(self, out=None):
        show(self, out=out)
        
    def flatten(self, expander, variables, res):
        for x in self:
            if isinstance(x, basestring):
                res.append(x)
            else:
                flatten(x, expander, variables, res)
    
class IfNode(Node):
    def flatten(self, expander, variables, res):
        cond = []
        flatten(self[0], expander, variables, cond)
        cond = u"".join(cond).strip()

        # template blacklisting results in 0xebad
        # see http://code.pediapress.com/wiki/ticket/700#comment:1
        cond = cond.strip(unichr(0xebad))
        
        res.append(maybe_newline)
        tmp = []
        if cond:
            if len(self)>1:
                flatten(self[1], expander, variables, tmp)
        else:
            if len(self)>2:
                flatten(self[2], expander, variables, tmp)
        _insert_implicit_newlines(tmp)
        res.append(u"".join(tmp).strip())
        res.append(dummy_mark)

class IfeqNode(Node):
    def flatten(self, expander, variables, res):        
        v1 = []
        flatten(self[0], expander, variables, v1)
        v1 = u"".join(v1).strip()

        v2 = []
        if len(self)>1:
            flatten(self[1], expander, variables, v2)
        v2 = u"".join(v2).strip()




        from mwlib.templ.magics import maybe_numeric_compare

        res.append(maybe_newline)
        tmp = []
        


        if maybe_numeric_compare(v1,v2):
            if len(self)>2:
                flatten(self[2], expander, variables, tmp)
        else:
            if len(self)>3:
                flatten(self[3], expander, variables, tmp)
        
        _insert_implicit_newlines(tmp)
        res.append(u"".join(tmp).strip())
        res.append(dummy_mark)


def maybe_numeric(a):
    try:
        return int(a)
    except ValueError:
        pass
    
    try:
        return float(a)
    except ValueError:
        pass
    return None

        
class SwitchNode(Node):
    fast = None
    unresolved = None

    def _store_key(self, key, value, fast, unresolved):
        if isinstance(key, basestring):
            key = key.strip()
            if key in fast:
                return

            fast[key] = (len(unresolved), value)
            num_key = maybe_numeric(key)
            if num_key is not None and num_key not in fast:
                fast[num_key] = (len(unresolved), value)
        else:
            unresolved.append((key,value))
            
    def _init(self):
        args = [equalsplit(x) for x in self[1]]
        
        unresolved = []
        fast = {}

        nokey_seen = []
        
        for key, value in args:
            if key is not None:
                key = optimize(list(key))
            if type(value) is tuple:
                value = optimize(list(value))


            if key is None:
                nokey_seen.append(value)
                continue

            for k in nokey_seen:
                self._store_key(k, value, fast, unresolved)
            del nokey_seen[:]
            self._store_key(key, value, fast, unresolved)

        if nokey_seen:
            self._store_key(u'#default', nokey_seen[-1], fast, unresolved)
            
        self.unresolved = tuple(unresolved)
        self.fast = fast
        self.sentinel = (len(self.unresolved)+1, None)
        
    def flatten(self, expander, variables, res):
        if self.unresolved is None:
            self._init()

        res.append(maybe_newline)
        val = []
        flatten(self[0], expander, variables, val)
        val = u"".join(val).strip()

        num_val = maybe_numeric(val)
        
        t1 = self.fast.get(val, self.sentinel)
        t2 = self.fast.get(num_val, self.sentinel)

        pos, retval = min(t1, t2)
        
        if pos is None:
            pos = len(self.unresolved)+1
        
        for k, v in self.unresolved[:pos]:
            tmp = []
            flatten(k, expander, variables, tmp)
            tmp = u"".join(tmp).strip()
            if tmp==val:
                retval = v
                break
            if num_val is not None and maybe_numeric(tmp)==num_val:
                retval = v
                break
            
        if retval is None:
            for a in expander.aliasmap.get_aliases("default") or ["#default"]:
                retval = self.fast.get(a)
                if retval is not None:
                    retval = retval[1]
                    break
            retval = retval or u""

        tmp = []
        flatten(retval, expander, variables, tmp)
        _insert_implicit_newlines(tmp)
        tmp = u"".join(tmp).strip()
        res.append(tmp)
        res.append(dummy_mark)

class Variable(Node):
    def flatten(self, expander, variables, res):
        name = []
        flatten(self[0], expander, variables, name)
        name = u"".join(name).strip()
        if len(name)>256*1024:
            raise MemoryLimitError("template name too long: %s bytes" % (len(name),))

        v = variables.get(name, None)

        if v is None:
            if len(self)>1:
                flatten(self[1], expander, variables, res)
            else:
                # FIXME. breaks If ???
                res.append(u"{{{%s}}}" % (name,))
        else:
            res.append(v)
       
class Template(Node):
    def flatten(self, expander, variables, res):
        try:
            return self._flatten(expander, variables, res)
        except RuntimeError, err:
            # we expect a "RuntimeError: maximum recursion depth exceeded" here.
            # logging this error is rather hard...
            try:
                log.warn("error %s ignored" % (err,))
            except:
                pass
            
    def _get_args(self):
        return self[1]

    def _flatten(self, expander, variables, res):
        
        name = []
        flatten(self[0], expander, variables, name)
        name = u"".join(name).strip()
        if len(name)>256*1024:
            raise MemoryLimitError("template name too long: %s bytes" % (len(name),))

        args = self._get_args()

        remainder = None
        if ":" in name:
            try_name, try_remainder = name.split(':', 1)
            from mwlib.templ import magic_nodes
            try_name = expander.resolve_magic_alias(try_name) or try_name

            klass = magic_nodes.registry.get(try_name)
            if klass is not None:
                children = (try_remainder, )+args
                # print "MAGIC:", klass,  children
                klass(children).flatten(expander, variables, res)
                return
            
            if expander.resolver.has_magic(try_name):
                name=try_name
                remainder = try_remainder
                
            if name=='#ifeq':
                res.append(maybe_newline)
                tmp=[]
                if len(args)>=1:
                    flatten(args[0], expander, variables, tmp)
                other = u"".join(tmp).strip()
                remainder = remainder.strip()
                tmp = []
                if magics.maybe_numeric_compare(remainder, other):
                    if len(args)>=2:
                        flatten(args[1], expander, variables, tmp)
                        res.append(u"".join(tmp).strip())
                else:
                    if len(args)>=3:
                        flatten(args[2], expander, variables, tmp)
                        res.append(u"".join(tmp).strip())
                res.append(dummy_mark)
                return
        
        var = []
        if remainder is not None:
            var.append(remainder)
        
        for x in args:
            var.append(x)

        var = ArgumentList(args=var, expander=expander, variables=variables)
        
        rep = expander.resolver(name, var)

        if rep is not None:
            res.append(maybe_newline)
            res.append(rep)
            res.append(dummy_mark)
        else:            
            p = expander.getParsedTemplate(name)
            if p:
                if DEBUG:
                    msg = "EXPANDING %r %r  ===> " % (name, var)
                    oldidx = len(res)
                res.append(mark_start(repr(name)))
                res.append(maybe_newline)
                flatten(p, expander, var, res)
                res.append(mark_end(repr(name)))

                if DEBUG:
                    msg += repr("".join(res[oldidx:]))
                    print msg

def show(node, indent=0, out=None):
    import sys
    
    if out is None:
        out=sys.stdout

    out.write("%s\n" % (node,))
    

from mwlib.templ.evaluate import maybe_newline, mark_start, mark_end, dummy_mark, flatten, MemoryLimitError, ArgumentList, equalsplit, _insert_implicit_newlines
from mwlib.templ import log, DEBUG
from mwlib.templ.parser import optimize

########NEW FILE########
__FILENAME__ = parser

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import re
from mwlib.templ.nodes import Node, Variable, Template, IfNode, SwitchNode
from mwlib.templ.scanner import symbols, tokenize
from mwlib.templ.marks import eqmark

from hashlib import sha1 as digest

class aliasmap(object):
    def __init__(self, siteinfo):
        _map = {}
        _name2aliases = {}

        for d in siteinfo.get("magicwords", []):
            name = d["name"]
            aliases = d["aliases"]
            _name2aliases[name] = aliases
            hashname = "#" + name
            for a in aliases:
                _map[a] = name
                _map["#"+a] = hashname

        self._map = _map
        self._name2aliases = _name2aliases

    def resolve_magic_alias(self, name):
        if name.startswith("#"):
            t = self._map.get(name[1:])
            if t:
                return "#" + t
        else:
            return self._map.get(name)

    def get_aliases(self, name):
        return self._name2aliases.get(name) or []


def optimize(node):
    if type(node) is tuple:
        return tuple(optimize(x) for x in node)
    
    if isinstance(node, basestring):
        return node

    if len(node)==1 and type(node) in (list, Node):
        return optimize(node[0])

    if isinstance(node, Node): #(Variable, Template, IfNode)):
        return node.__class__(tuple(optimize(x) for x in node))
    else:
        # combine strings
        res = []
        tmp = []
        for x in (optimize(x) for x in node):
            if isinstance(x, basestring) and x is not eqmark:
                tmp.append(x)
            else:
                if tmp:
                    res.append(u''.join(tmp))
                    tmp = []
                res.append(x)
        if tmp:
            res.append(u''.join(tmp))

        node[:] = res
    
    
    if len(node)==1 and type(node) in (list, Node):
        return optimize(node[0])

    if type(node) is list:
        return tuple(node)
        
    return node

from mwlib import lrucache

class Parser(object):
    use_cache = False
    _cache = lrucache.mt_lrucache(2000)
    
    def __init__(self, txt, included=True, replace_tags=None, siteinfo=None):
        if isinstance(txt, str):
            txt = unicode(txt)
            
        self.txt = txt
        self.included = included
        self.replace_tags = replace_tags
        if siteinfo is None:
            from mwlib.siteinfo import get_siteinfo
            siteinfo = get_siteinfo("en")
        self.siteinfo = siteinfo
        self.name2rx = {"if": re.compile("^#if:"),
                        "switch": re.compile("^#switch:")}


        magicwords = self.siteinfo.get("magicwords", [])
        for d in magicwords:
            name = d["name"]
            if name in ("if", "switch"):
                aliases = [re.escape(x) for x in d["aliases"]]
                rx = "^#(%s):" % ("|".join(aliases),)
                self.name2rx[name] = re.compile(rx)
                # print name, rx

        self.aliasmap = aliasmap(self.siteinfo)

    def getToken(self):
        return self.tokens[self.pos]

    def setToken(self, tok):
        self.tokens[self.pos] = tok


    def variableFromChildren(self, children):
        v=[]

        try:
            idx = children.index(u"|")
        except ValueError:
            v.append(children)
        else:
            v.append(children[:idx])
            v.append(children[idx+1:])

        return Variable(v)
        
    def _eatBrace(self, num):
        ty, txt = self.getToken()
        assert ty == symbols.bra_close
        assert len(txt)>= num
        newlen = len(txt)-num
        if newlen==0:
            self.pos+=1
            return
        
        if newlen==1:
            ty = symbols.txt

        txt = txt[:newlen]
        self.setToken((ty, txt))

    def _strip_ws(self, cond):
        if isinstance(cond, unicode):
            return cond.strip()

        cond = list(cond)
        if cond and isinstance(cond[0], unicode):
            if not cond[0].strip():
                del cond[0]

        if cond and isinstance(cond[-1], unicode):
            if not cond[-1].strip():
                del cond[-1]
        cond = tuple(cond)
        return cond
    
    def switchnodeFromChildren(self, children):
        children[0] = children[0].split(":", 1)[1]
        args = self._parse_args(children)
        value = optimize(args[0])
        value = self._strip_ws(value)
        return SwitchNode((value, tuple(args[1:])))
        
    def ifnodeFromChildren(self, children):
        children[0] = children[0].split(":", 1)[1]
        args = self._parse_args(children)
        cond = optimize(args[0])
        cond = self._strip_ws(cond)
        
        args[0] = cond
        n = IfNode(tuple(args))
        return n

    def magicNodeFromChildren(self, children, klass):
        children[0] = children[0].split(":", 1)[1]
        args = self._parse_args(children)
        return klass(args)
    
    def _parse_args(self, children, append_arg=False):
        args = []
        arg = []

        linkcount = 0
        for c in children:
            if c==u'[[':
                linkcount += 1
            elif c==']]':
                if linkcount:
                    linkcount -= 1
            elif c==u'|' and linkcount==0:
                args.append(arg)
                arg = []
                append_arg = True
                continue
            elif c==u"=" and linkcount==0:
                arg.append(eqmark)
                continue
            arg.append(c)


        if append_arg or arg:
            args.append(arg)

        return [optimize(x) for x in args]

    def _is_good_name(self, node):

        # we stop here on the first colon. this is wrong but we don't have
        # the list of allowed magic functions here...
        done = False
        if isinstance(node, basestring):
            node = [node]
            
        for x in node:
            if not isinstance(x, basestring):
                continue
            if ":" in x:
                x = x.split(":")[0]
                done=True
                
            if "[" in x or "]" in x:
                return False
            if done:
                break
        return True
    
    def templateFromChildren(self, children):
        if children and isinstance(children[0], unicode):
            s = children[0].strip().lower()
            if self.name2rx["if"].match(s):
                return self.ifnodeFromChildren(children)
            if self.name2rx["switch"].match(s):
                return self.switchnodeFromChildren(children)
            
            if u':' in s:
                from mwlib.templ import magic_nodes
                name, first = s.split(':', 1)
                name = self.aliasmap.resolve_magic_alias(name) or name
                if name in magic_nodes.registry:
                    return self.magicNodeFromChildren(children, magic_nodes.registry[name])
                
                
            
        # find the name
        name = []
        append_arg = False
        idx = 0
        for idx, c in enumerate(children):
            if c==u'|':
                append_arg = True
                break
            name.append(c)

        name = optimize(name)
        if isinstance(name, unicode):
            name = name.strip()

        if not self._is_good_name(name):
            return Node([u"{{"] + children + [u"}}"])
        
        args = self._parse_args(children[idx+1:], append_arg=append_arg)
        
        return Template([name, tuple(args)])
        
    def parseOpenBrace(self):
        ty, txt = self.getToken()
        n = []

        numbraces = len(txt)
        self.pos += 1

        linkcount = 0
        
        while 1:
            ty, txt = self.getToken()

            if ty==symbols.bra_open:
                n.append(self.parseOpenBrace())
            elif ty is None:
                break
            elif ty==symbols.bra_close and linkcount==0:
                closelen = len(txt)
                if closelen==2 or numbraces==2:
                    t=self.templateFromChildren(n)
                    n=[]
                    n.append(t)
                    self._eatBrace(2)
                    numbraces-=2
                else:
                    v=self.variableFromChildren(n)
                    n=[]
                    n.append(v)
                    self._eatBrace(3)
                    numbraces -= 3

                if numbraces < 2:
                    break
            elif ty==symbols.noi:
                self.pos += 1 # ignore <noinclude>
            else: # link, txt
                if txt=="[[":
                    linkcount += 1
                elif txt=="]]" and linkcount>0:
                    linkcount -= 1

                n.append(txt)
                self.pos += 1            

        if numbraces:
            n.insert(0, "{"*numbraces)
            
        return n
        
    def parse(self):
        if self.use_cache:
            fp = digest(self.txt.encode('utf-8')).digest()     
            try:
                return self._cache[fp]
            except KeyError:
                pass
        
        
        self.tokens = tokenize(self.txt, included=self.included, replace_tags=self.replace_tags)
        self.pos = 0
        n = []
        
        while 1:
            ty, txt = self.getToken()
            if ty==symbols.bra_open:
                n.append(self.parseOpenBrace())
            elif ty is None:
                break
            elif ty==symbols.noi:
                self.pos += 1   # ignore <noinclude>
            else: # bra_close, link, txt                
                n.append(txt)
                self.pos += 1

        n=optimize(n)
        
        if self.use_cache:
            self._cache[fp] = n
        
        return n

def parse(txt, included=True, replace_tags=None, siteinfo=None):
    return Parser(txt, included=included, replace_tags=replace_tags, siteinfo=siteinfo).parse()

########NEW FILE########
__FILENAME__ = pp

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import re

rxc = lambda s: re.compile(s, re.DOTALL | re.IGNORECASE)

onlyincluderx   = rxc("<onlyinclude>(.*?)</onlyinclude>")
noincluderx     = rxc("<noinclude(?:\s[^<>]*)?>.*?(</noinclude>|$)")
includeonlyrx   = rxc("<includeonly(?:\s[^<>]*)?>.*?(?:</includeonly>|$)")


def get_remove_tags(tags):
    r = rxc("</?(%s)(?:\s[^<>]*)?>" % ("|".join(tags)))
    return lambda s: r.sub(u'', s)

remove_not_included = get_remove_tags(["onlyinclude", "noinclude"])


def preprocess(txt, included=True):
    if included:
        txt = noincluderx.sub(u'', txt)

        if "<onlyinclude>" in txt:
            # if onlyinclude tags are used, only use text between those tags. template 'legend' is a example
            txt = "".join(onlyincluderx.findall(txt))
    else:
        txt = includeonlyrx.sub(u'', txt)
        txt = remove_not_included(txt)

    return txt

########NEW FILE########
__FILENAME__ = scanner

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import re
from mwlib.templ import pp

splitpattern = """
({{+)                     # opening braces
|(}}+)                    # closing braces
|(\[\[|\]\])              # link
|((?:<noinclude>.*?</noinclude>)|(?:</?includeonly>))  # noinclude, comments: usually ignore
|(?P<text>(?:<nowiki>.*?</nowiki>)          # nowiki
|(?:<math>.*?</math>)
|(?:<imagemap[^<>]*>.*?</imagemap>)
|(?:<gallery[^<>]*>.*?</gallery>)
|(?:<ref[^<>]*/>)
|(?:<source[^<>]*>.*?</source>)
|(?:<pre.*?>.*?</pre>)
|(?:=)
|(?:[\[\]\|{}<])                                  # all special characters
|(?:[^=\[\]\|{}<]*))                               # all others
"""

splitrx = re.compile(splitpattern, re.VERBOSE | re.DOTALL | re.IGNORECASE)

class symbols:
    bra_open = 1
    bra_close = 2
    link = 3
    noi = 4
    txt = 5

def tokenize(txt, included=True, replace_tags=None):
    txt = pp.preprocess(txt, included=included)

    if replace_tags is not None:        
        txt = replace_tags(txt)
    
    tokens = []
    for (v1, v2, v3, v4, v5) in splitrx.findall(txt):
        if v5:
            tokens.append((5, v5))        
        elif v4:
            tokens.append((4, v4))
        elif v3:
            tokens.append((3, v3))
        elif v2:
            tokens.append((2, v2))
        elif v1:
            tokens.append((1, v1))

    tokens.append((None, ''))
    
    return tokens

########NEW FILE########
__FILENAME__ = timeline
#! /usr/bin/env python

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

"""implement http://meta.wikimedia.org/wiki/EasyTimeline
"""

import os
import tempfile
import subprocess
from hashlib import md5

font = None

def _setupenv():
    global font

    if "GDFONTPATH" in os.environ:
        font = "FreeSans"
        return

    paths = [os.path.expanduser("~/mwlibfonts/freefont"),
             "/usr/share/fonts/TTF",
             "/usr/share/fonts/truetype/freefont"]

    for p in paths:
        if os.path.exists(os.path.join(p, "FreeSans.ttf")):
            os.environ["GDFONTPATH"] = p
            font = "FreeSans"

_setupenv()

_basedir = None
def _get_global_basedir():
    global _basedir
    if not _basedir:
        _basedir = tempfile.mkdtemp(prefix='timeline-')
        import atexit
        import shutil
        atexit.register(shutil.rmtree, _basedir)
    return _basedir

def drawTimeline(script, basedir=None):
    if isinstance(script, unicode):
        script = script.encode('utf8')
    if basedir is None:
        basedir = _get_global_basedir()
        
    m=md5()
    m.update(script)
    ident = m.hexdigest()

    pngfile = os.path.join(basedir, ident+'.png')
    
    if os.path.exists(pngfile):
        return pngfile

    scriptfile = os.path.join(basedir, ident+'.txt')
    open(scriptfile, 'w').write(script)
    et = os.path.join(os.path.dirname(__file__), "EasyTimeline.pl")
    
    err = os.system("perl %s -P /usr/bin/ploticus -f %s -T %s -i %s" % (et, font or "ascii", basedir, scriptfile))
    if err != 0:
        return None

    if os.path.exists(pngfile):
        return pngfile

    return None

    

########NEW FILE########
__FILENAME__ = treecleaner
#! /usr/bin/env python
#! -*- coding:utf-8 -*-

# Copyright (c) 2007, PediaPress GmbH
# See README.rst for additional licensing information.

import sys
import unicodedata

from mwlib.advtree import removeNewlines
from mwlib.advtree import (Article, ArticleLink, Big, Blockquote, Book, BreakingReturn, Caption, CategoryLink, Cell, Center, Chapter,
                           Cite, Code,DefinitionDescription, DefinitionList, DefinitionTerm, Deleted, Div, Emphasized, Gallery,
                           HorizontalRule, ImageLink, ImageMap, Inserted, InterwikiLink, Italic, Item, ItemList, LangLink, Link,
                           Math, NamedURL, NamespaceLink, Overline, Paragraph, PreFormatted, Reference, ReferenceList,
                           Row, Section, Small, Source, Span, SpecialLink, Strike, Strong, Sub, Sup, Table, Teletyped, Text, Timeline,
                           Underline, URL, Var)

from mwlib.treecleanerhelper import getNodeHeight, splitRow
from mwlib import parser
from mwlib.writer import styleutils, miscutils

def show(n):
    parser.show(sys.stdout, n, verbose=True)

def tryRemoveNode(node):
    if node.parent is not None:
        node.parent.removeChild(node)
        return True


def _all(list):
    for item in list:
        if item == False:
            return False
    return True

def _any(list):
    for x in list:
        if x:
            return True
    return False


class TreeCleaner(object):

    """The TreeCleaner object cleans the parse tree to optimize writer ouput.

    All transformations should be lossless.

    """


    cleanerMethods = ['cleanVlist',
                      'markInfoboxes',
                      'removeEditLinks',
                      'removeEmptyTextNodes',
                      'removeInvisibleLinks', 
                      'cleanSectionCaptions',
                      'removeChildlessNodes',
                      'removeNoPrintNodes',
                      'removeListOnlyParagraphs',
                      'removeInvalidFiletypes',
                      'fixParagraphs',
                      'simplifyBlockNodes',
                      'removeAbsolutePositionedNode',
                      'removeScrollElements',
                      'galleryFix',
                      'fixRegionListTables',
                      'fixNesting',
                      'removeChildlessNodes',
                      'unNestEndingCellContent',
                      'removeCriticalTables',
                      'removeTextlessStyles', 
                      'removeBrokenChildren',
                      'fixTableColspans',
                      'removeEmptyTrailingTableRows',
                      'splitTableLists', 
                      'transformSingleColTables',
                      'splitTableToColumns', 
                      'linearizeWideNestedTables',
                      'removeBreakingReturns', 
                      'removeEmptyReferenceLists',
                      'swapNodes',
                      'removeBigSectionsFromCells',
                      'transformNestedTables',
                      'splitBigTableCells',
                      'limitImageCaptionsize',
                      'removeDuplicateLinksInReferences',
                      'fixItemLists',
                      'fixSubSup',
                      'removeLeadingParaInList',
                      'removeChildlessNodes', # methods above might leave empty nodes behind - clean up
                      'removeNewlines', # imported from advtree - clean up newlines that are not needed
                      'removeBreakingReturns',
                      'removeSeeAlso',
                      'buildDefinitionLists',
                      'restrictChildren',
                      'fixReferenceNodes',
                      'removeBrokenChildren',
                      'fixMathDir',
                      'fixNesting', # pull DefinitionLists out of Paragraphs
                      'fixPreFormatted',
                      'fixListNesting',
                      'handleOnlyInPrint',
                      'removeEmptyTextNodes',
                      'removeChildlessNodes', 
                      'removeBreakingReturns',
                      'removeEmptySections',
                      'markShortParagraph',
                      ]

    skipMethods = []


    def __init__(self, tree, save_reports=False, nesting_strictness='loose', status_cb=None, rtl=False):
        """Init with parsetree.

        The input tree needs to be an AdvancedTree, generated by advtree.buildAdvancedTree
        """
        
        self.tree = tree
        # list of actions by the treecleaner
        # each cleaner method has to report its actions
        # this helps debugging and testing the treecleaner
        self.reports = []

        # reports are only saved, if set to True
        self.save_reports = save_reports

        self.status_cb=status_cb
        self.rtl = rtl
        # list of nodes which do not require child nodes
        self.childlessOK = [ArticleLink, BreakingReturn, CategoryLink, Cell, Chapter, Code, 
                            HorizontalRule, ImageLink, ImageMap, InterwikiLink, LangLink, Link, Math,
                            NamedURL, NamespaceLink, ReferenceList, Reference, SpecialLink, Text, Timeline, URL]
        # exceptions to the above. if any of the list items is explicitly set as a css style the node is not removed
        common_attrs = [u'width', u'height', u'page-break-before', u'page-break-after']
        self.childless_exceptions = {Div: common_attrs,
                                     Span: common_attrs}

        # FIXME: not used currently. remove if this is not used soon. could be used as reference
        # list nodes that apply styles to their children
        # FIXME: Center node might be problematic. Center is a block node and not inline
        self.inlineStyleNodes = [Big, Center, Cite, Code, Deleted, Emphasized, Inserted, Italic,
                                 Overline, Small, Strike, Strong, Sub, Sup, Teletyped, Underline, Var]


        # USED IN fixNesting if nesting_strictness == 'loose'
        # keys are nodes, that are not allowed to be inside one of the nodes in the value-list
        # ex: pull image links out of preformatted nodes
        # fixme rename to ancestors
        self.forbidden_parents = {ImageLink:[PreFormatted],
                                  ItemList:[PreFormatted],
                                  Source:self.inlineStyleNodes,
                                  DefinitionList:[Paragraph],
                                  Blockquote:[PreFormatted],
                                  Center:[PreFormatted],
                                  Paragraph:[PreFormatted],
                                  Section:[PreFormatted],
                                  Gallery:[PreFormatted, DefinitionDescription, DefinitionList, DefinitionTerm],
                                  Table:[DefinitionList, DefinitionDescription],
                                  PreFormatted: [Code],
                                  }
        self.forbidden_parents[Source].append(PreFormatted)

        # when checking nesting, some Nodes prevent outside nodes to be visible to inner nodes
        # ex: Paragraphs can not be inside Paragraphs. but if the inner paragraph is inside a
        # table which itself is inside a paragraph this is not a problem
        self.outsideParentsInvisible = [Table, Section, Reference]
        self.nesting_strictness = nesting_strictness # loose | strict

        
        # ex: delete preformatted nodes which are inside reference nodes,
        # all children off the preformatted node are kept
        self.removeNodes = {PreFormatted: [Reference, PreFormatted],
                            Cite: [Item, Reference],
                            Code: [PreFormatted],
                            ImageLink: [Reference],
                            Div: [Reference, Item],
                            Center:[Reference],
                            Teletyped:[Reference],
                            ReferenceList: [Reference],
                            Teletyped: [Source],
                            Table:[ImageLink],
                            Reference:[Reference],
                            Paragraph:[Gallery],
                            }

        self.removeNodesAllChildren = {Table:[ImageLink], # used to indicate that children should be removed
                                       }

        
        # ex: some tags need to be swapped: center nodes have to be pulled out of underline nodes
        # e.g. but only if the center is a direct and only child
        self.swapNodesMap = { Center:[Underline, Emphasized]} # { ChildClass: [ParentClass, ParentClass2]}


        # list of css classes OR id's which trigger the removal of the node from the tree
        # the following list is wikipedia specific
        self.noDisplayClasses = ['hiddenStructure',
                                 'dablink',
                                 'rellink',
                                 'editlink',
                                 'metadata',
                                 'noprint',
                                 'portal',
                                 'sisterproject',
                                 'NavFrame',
                                 'geo-multi-punct',
                                 'geo-nondefault',
                                 'coordinates_3_ObenRechts',
                                 'microformat',
                                 'navbox',
                                 'navbox-vertical',
                                 'Vorlage_Gesundheitshinweis',
                                 ]

        # keys are nodes which can only have child nodes of types inside the valuelist.
        # children of different classes are deleted
        self.allowedChildren = {Gallery: [ImageLink],
                                }

        self.cell_splitter_params = {
            'maxCellHeight': (7*72) * 3/4 ,
            'lineHeight':  26,
            'charsPerLine': 40,
            'paragraphMargin': 2, # add 10 pt margin-safety after each node
            'imgHeight': 6, # approximate image height in units of lineHeights
            }


        self.style_nodes = [Italic, Emphasized, Strong, Overline, Underline, Sub, Sup, Small, Big, Var]

        # list of classes or IDs of table nodes which are split into their content. used by splitTableToColumns
        self.split_table_classIDs = ['mp-upper'] 

        # remove ImageLinks which end with the following file types
        self.forbidden_file_endings = ['ogg']

        # emtpy sections are removed by removeEmptySections
        # all node classes that have content but no text need to be listed here to prevent removal
        self.contentWithoutTextClasses = [Gallery, ImageLink]


    def clean(self, cleanerMethods):
        """Clean parse tree using cleaner methods in the methodList."""
        cleanerList = []
        for method in cleanerMethods:
            f = getattr(self, method, None)
            if f:
                cleanerList.append(f)
            else:
                raise 'TreeCleaner has no method: %r' % method

        # FIXME: performance could be improved, if individual articles would be cleaned
        # the algorithm below splits on the first level, if a book is found
        # --> if chapters are used, whole chapters are cleaned which slows things down

        if self.tree.__class__ == Book :
            children = self.tree.children
        else:
            children = [self.tree]

        total_children = len(children)
        for (i, child) in enumerate(children):
            for cleaner in cleanerList:
                try:
                    cleaner(child)
                except Exception, e:
                    self.report('ERROR:', e)
                    print 'TREECLEANER ERROR in %s: %r' % (getattr(child, 'caption', u'').encode('utf-8'),
                                                           repr(e))
                    import traceback
                    traceback.print_exc()
            if self.status_cb:
                self.status_cb(progress=100*i/total_children)

    def cleanAll(self, skipMethods=[]):
        """Clean parse tree using all available cleaner methods."""
        skipMethods = skipMethods or self.skipMethods
        self.clean([cm for cm in self.cleanerMethods if cm not in skipMethods])

    def report(self, *args):
        if not self.save_reports:
            return
        caller = sys._getframe(1).f_code.co_name
        msg = ''
        if args:
            msg = ' '.join([repr(arg) for arg in args])
        self.reports.append((caller, msg))

    def getReports(self):
        return self.reports

    def removeNewlines(self, node):
        removeNewlines(node)

    def removeEmptyTextNodes(self, node):
        """Removes Text nodes which contain no text at all.

        Text nodes which only contain whitespace are kept.
        """
        if node.__class__ == Text and node.parent:
            if (node.previous and node.previous.isblocknode and node.next and node.next.isblocknode and not node.caption.strip()) or not node.caption:
                self.report('removed empty text node')
                node.parent.removeChild(node)
                return
        for c in node.children:
            self.removeEmptyTextNodes(c)

    def removeListOnlyParagraphs(self, node):
        """Removes paragraph nodes which only have lists as the only childen - keep the lists."""
        if node.__class__ == Paragraph:
            list_only_children = _all([c.__class__ == ItemList for c in node.children])
            if list_only_children and node.parent:
                self.report('replaced children:', node, '-->', node.children, 'for node:', node.parent)
                node.parent.replaceChild(node, node.children)
                
        for c in node.children[:]:
            self.removeListOnlyParagraphs(c)

    def removeChildlessNodes(self, node):
        """Remove nodes that have no children except for nodes in childlessOk list."""   
        is_exception = False
        if node.__class__ in self.childless_exceptions.keys() and node.style:
            for style_type in self.childless_exceptions[node.__class__]:
                if style_type in node.style.keys():
                    is_exception = True

        if not node.children and node.__class__ not in self.childlessOK and not is_exception:
            if node.parent.__class__ == Section and not node.previous: 
                return # make sure that the first child of a section is not removed - this is the section caption
            removeNode = node
            while removeNode.parent and not removeNode.siblings and removeNode.parent.__class__ not in self.childlessOK:
                removeNode = removeNode.parent
            if removeNode.parent:
                self.report('removed:', removeNode)
                removeNode.parent.removeChild(removeNode)
        for c in node.children[:]:
            self.removeChildlessNodes(c)
            
    # FIXME: this method is obsolete as of now. 'navbox' is now a member of the noDisplayClasses and removed alltogether
    def removeCriticalTables(self, node):
        """Remove problematic table nodes - keep content.
               
        The content is preserved if possible and only the outmost 'container' table is removed.
        """

        if node.__class__ == Table and node.hasClassID(['navbox']):
            children = []
            for row in node.children:
                for cell in row:
                    for n in cell:
                        children.append(n)
            if node.parent:
                self.report('replaced child:', node, children)
                node.parent.replaceChild(node, children)
            return

        for c in node.children:
            self.removeCriticalTables(c)

    def fixTableColspans(self, node):
        """ Fix erronous colspanning information in table nodes.

        1. SINGLE CELL COLSPAN: if a row contains a single cell, the
           colspanning amount is limited to the maximum table width
        """
        # SINGLE CELL COLSPAN 
        if node.__class__ == Table:
            maxwidth = 0
            for row in node.children:
                numCells = len(row.children)
                rowwidth = 0
                for cell in row.children:
                    colspan = cell.attributes.get('colspan', 1)
                    if numCells > 1:
                        rowwidth += colspan
                    else:
                        rowwidth += 1
                maxwidth = max(maxwidth,  rowwidth)
            for row in node.children:
                numCells = len(row.children)
                if numCells == 1:
                    cell = row.children[0]
                    colspan = cell.attributes.get('colspan', 1)
                    if colspan and colspan > maxwidth:
                        self.report('fixed colspan from', cell.vlist.get('colspan', 'undefined'), 'to', maxwidth)
                        cell.vlist['colspan'] = maxwidth
        # /SINGLE CELL COLSPAN

        def emptyEndingCell(row):
            if not row.children:
                return False
            last_cell = row.children[-1]
            if not last_cell.children:
                return last_cell

        if node.__class__ == Table:
            # FIX for: http://de.wikipedia.org/w/index.php?title=Benutzer:Volker.haas/Test&oldid=73993014
            if len(node.children) == 1 and node.children[0].__class__ == Row:
                row = node.children[0]
                cell =  emptyEndingCell(row)
                while cell:
                    cell.parent.removeChild(cell)
                    cell = emptyEndingCell(row)
                    self.report('removed empty cell in single-row table')

        for c in node.children:
            self.fixTableColspans(c)

    def removeBrokenChildren(self, node):
        """Remove Nodes (while keeping their children) which can't be nested with their parents."""
        if node.__class__ in self.removeNodes.keys():
            if _any([parent.__class__ in self.removeNodes[node.__class__] for parent in node.parents]):
                if node.children and not _any([ parent.__class__ in self.removeNodesAllChildren.get(node.__class__, []) for parent in node.parents]):
                    children = node.children
                    self.report('replaced child', node, children)
                    node.parent.replaceChild(node, newchildren=children)
                else:
                    self.report('removed child', node)
                    node.parent.removeChild(node)
                #return

        for c in node.children:
            self.removeBrokenChildren(c)


    def transformSingleColTables(self, node):
        # "not 'box' in node.attr(class)" is a hack to detect infoboxes and thelike. they are not split into divs.
        # tables like this should be detected and marked in a separate module probably
        single_col = node.__class__ == Table and node.numcols == 1
        is_long = len(node.getAllDisplayText()) > 2500

        contains_gallery = len(node.getChildNodesByClass(Gallery)) > 0
        many_cells = False
        if single_col:
            all_images = True
            many_cells = len(node.getChildNodesByClass(Cell)) > 200
            for row in node.children:
                for cell in row.children:
                    for item in cell.children:
                        if item.__class__ != ImageLink:
                            all_images = False
        else:
            all_images = False
        if node.__class__ == Table:
            nested_tables = node.getChildNodesByClass(Table)
            nested_rows = 0
            for t in nested_tables:
                nested_rows = max(nested_rows, len(t.getChildNodesByClass(Row)))
            many_nested_rows = nested_rows > 35
        else:
            many_nested_rows = False
        if single_col and ( (not getattr(node, 'isInfobox', False) and (is_long or many_cells))
                            or ((is_long or many_nested_rows) and many_cells)
                            or all_images or contains_gallery
                            or len(node.getChildNodesByClass(Row)) == 1
                            ):
            if not node.parents:
                return
            divs = []
            items = []
            content_len = len(node.getAllDisplayText())
            if content_len > 4000 \
                   or all_images \
                   or len(node.getChildNodesByClass(Cell)) > 30 and content_len > 500 \
                   or (node.getChildNodesByClass(Section) and node.getChildNodesByClass(ImageLink) and content_len > 1000) \
                   or contains_gallery:
                div_wrapper = False
            else:
                div_wrapper = True
            for row in node:
                for cell in row:
                    if div_wrapper:
                        d = Div()
                        d.border = 1
                        d.vlist = node.vlist
                        for item in cell:
                            d.appendChild(item)
                        divs.append(d)
                    else:
                        for item in cell:
                            items.append(item)
            parent = node.parent
            if div_wrapper:
                parent.replaceChild(node, divs)
                self.report('replaced single col table with div. div children:',  parent.children)
            else:
                parent.replaceChild(node, items)
                self.report('replaced single col table with items:',  parent.children)
        for c in node.children:
            self.transformSingleColTables(c)

    def _getNext(self, node): #FIXME: name collides with advtree.getNext
        if not (node.next or node.parent):
            return
        next = node.next or node.parent.next
        if next and not next.isblocknode:
            if not next.getAllDisplayText().strip():
                return self._getNext(next)
        return next

    def _getPrev(self, node): #FIXME: name collides with advtree.getPrev(ious)
        if not (node.previous or node.parent):
            return
        prev = node.previous or node.parent 
        if prev and not prev.isblocknode:
            if not prev.getAllDisplayText().strip():
                return self._getPrev(prev)
        return prev

    def _nextAdjacentNode(self, node):
        if node and node.next:
            res = node.next.getFirstLeaf() or node.next
            return res
        if node.parent:
            return self._nextAdjacentNode(node.parent)
        return None


    def removeBreakingReturns(self, node): 
        """Remove BreakingReturns that occur around blocknodes or as the first/last element inside a blocknode."""
        if node.isblocknode:
            changed = True
            while changed:
                check_node = [node.getFirstLeaf(),
                             node.getLastLeaf(),
                             self._getNext(node),
                             self._getPrev(node)
                             ]
                changed = False
                for n in check_node:
                    if n.__class__ == BreakingReturn:
                        self.report('removing node', n)
                        tryRemoveNode(n)
                        changed = True

        if node.__class__ == BreakingReturn:
            next_node = self._nextAdjacentNode(node)
            if next_node.__class__ == BreakingReturn:
                node.parent.removeChild(node)


        for c in node.children:
            self.removeBreakingReturns(c)


    def _fixParagraphs(self, node):
        """Move paragraphs to the child list of the last section (if existent)"""

        if isinstance(node, Paragraph) and isinstance(node.previous, Section) \
                and node.previous is not node.parent:
            prev = node.previous
            parent = node.parent
            target = prev.getLastChild()
            self.report('moving node', node, 'to', target)
            node.moveto(target)
            return True # changed
        else:
            for c in node.children[:]:
                if self._fixParagraphs(c):
                    return True

    def fixParagraphs(self, node):
        while self._fixParagraphs(node):
            pass

    def _nestingBroken(self, node):
        # FIXME: the list below is used and not node.isblocknode. is there a reason for that?
        blocknodes = (Paragraph, PreFormatted, ItemList, Section, Table,
                      Blockquote, DefinitionList, HorizontalRule, Source)
        parents = node.getParents()
        clean_parents = []
        parents.reverse()
        for p in parents:
            if p.__class__ not in self.outsideParentsInvisible:
                clean_parents.append(p)
            else:
                break
        #clean_parents.reverse()
        parents = clean_parents

        if self.nesting_strictness == 'loose':
            for parent in parents:
                if parent.__class__ in self.forbidden_parents.get(node.__class__, []):
                    return parent
        elif self.nesting_strictness == 'strict':
            for parent in parents:
                if node.__class__ != Section and node.__class__ in blocknodes and parent.__class__ in blocknodes:
                    return parent
        return None
           

    def _markNodes(self, node, divide, problem_node=None):
        got_divide = False
        for c in node.children:
            if getattr(node, 'nesting_pos', None):
                c.nesting_pos = node.nesting_pos
                continue
            if c in divide:
                got_divide = True
                if c == problem_node:
                    c.nesting_pos = 'problem'
                continue
            if not got_divide:
                c.nesting_pos = 'top'
            else:
                c.nesting_pos = 'bottom'
        for c in node.children:
            self._markNodes(c, divide, problem_node=problem_node)

    def _cleanUpMarks(self, node):
        if hasattr(node, 'nesting_pos'):
            del node.nesting_pos
        for c in node.children:
            self._cleanUpMarks(c)
            
    def _filterTree(self, node, nesting_filter=[]):
        if getattr(node, 'nesting_pos', None) in nesting_filter:
            node.parent.removeChild(node)
            return
        for c in node.children[:]:
            self._filterTree(c, nesting_filter=nesting_filter)

    def _isException(self, node):
        try:
            has_direction = node.vlist['style']['direction']
        except (KeyError, AttributeError, TypeError):
            return False
        else:
            return True


    def _fixNesting(self, node):
        """Nesting of nodes is corrected.

        The strictness depends on nesting_strictness which can either be 'loose' or 'strict'.
        Depending on the strictness the _nestingBroken method uses different approaches to
        detect forbidden nesting.

        Example for 'strict' setting: (bn --> blocknode, nbn --> nonblocknode)
        bn_1
         nbn_2
         bn_3
         nbn_4

        becomes:
        bn_1.1
         nbn_2
        bn_3
        bn_1.2
         nbn_4
        """

        if self._isException(node):
            return

        bad_parent = self._nestingBroken(node)
        if not bad_parent:
            for c in node.children:
                if self._fixNesting(c):
                    return True
            return False

        divide = node.getParents()
        divide.append(node)
        self._markNodes(bad_parent, divide, problem_node=node)

        top_tree = bad_parent.copy()
        self._filterTree(top_tree, nesting_filter=['bottom', 'problem'])
        middle_tree = bad_parent.copy()
        self._filterTree(middle_tree, nesting_filter=['top', 'bottom'])
        middle_tree = middle_tree.children[0]
        bottom_tree = bad_parent.copy()
        self._filterTree(bottom_tree, nesting_filter=['top', 'problem'])
        new_tree = [part for part in [top_tree, middle_tree, bottom_tree] if part != None]

        self.report('moved', node, 'from', bad_parent)
        parent = bad_parent.parent
        parent.replaceChild(bad_parent, new_tree)
        self._cleanUpMarks(parent)
        return True
    
    def fixNesting(self, node):
        while self._fixNesting(node):
            pass
        
   
    # ex: some tags need to be swapped: center nodes have to be pulled out of underline nodes
    # e.g. but only if the center is a direct and only child
    def swapNodes(self, node):
        """Swaps two nodes if nesting is problematic.

        Some writers have problems with some node combinations
        ex. <u><center>Text</center></u> --> <center><u>Text</u></center>
        """
        def swap(a,b): 
            assert len(a.children) == 1 and a.children[0] is b and b.parent is a and a.parent is not None
            ap = a.parent
            ap.replaceChild(a, [b])
            a.children = [] # a.removeChild(b) wouldn't work since check for b.parent which already is ap fails
            for c in b.children:
                a.appendChild(c)
            b.children = []
            b.appendChild(a)

        if node.__class__ in self.swapNodesMap:
            p = node.parent
            if p and p.parent and p.__class__ in self.swapNodesMap[node.__class__] and len(p.children) == 1:
                self.report('swapping nodes:', node.parent, node)
                swap(node.parent, node)

        for c in node.children[:]:
            self.swapNodes(c)

    def removeBigSectionsFromCells(self, node):
        """Remove very big sections from tables. It can be assumed that they were not intentionally put inside the table"""
        if node.__class__ == Cell:   
            sections = [n for n in node.children if n.__class__ == Section]
            if len(node.getAllDisplayText()) > 2000 and sections:
                for section in sections:
                    if len(section.getAllDisplayText()) > 2000:
                        parentTable = node.getParentNodesByClass(Table)[-1]
                        self.report('move big section out of table')
                        section.moveto(parentTable)
                        

        for c in node.children:
            self.removeBigSectionsFromCells(c)

    def transformNestedTables(self, node):
        """ Remove Container tables that only contain large nested tables"""
        
        if node.__class__ == Table and node.parent and not node.getParentNodesByClass(Table):

            # remove tables which only contain a single table
            if len(node.children) == 1 and node.numcols == 1:
                first_cell_content = node.children[0].children[0].children
                if len(first_cell_content) == 1 and first_cell_content[0].__class__ == Table:
                    node.parent.replaceChild(node, first_cell_content)
                    return
                
            parent = node.parent
            rows = [ r for r in node.children if r.__class__ == Row]
            captions = [ c for c in node.children if c.__class__ == Caption]
            tables = []
            non_tables = []
            for row in rows:
                for cell in row.children:
                    for item in cell.children:
                        if item.__class__ != Table:
                            non_tables.append(item)
                        else:
                            tables.append(item)

            if non_tables:
                non_tables_text = ''.join([ n.getAllDisplayText() for n in non_tables]).strip()
            else:
                non_tables_text = None
            if tables:
                tables_text = ''.join([ n.getAllDisplayText() for n in tables]).strip()
            else:
                tables_text = None

            if tables and (len(tables_text) > 500 ) and not non_tables_text:
                if captions:
                    for c in captions[::-1]:
                        tables.insert(0, c)
                parent.replaceChild(node, tables)
                self.report('removed container table around large tables', node, tables)
                return

        for c in node.children:
            self.transformNestedTables(c)
    
            
            
    def splitBigTableCells(self, node):
        """Splits table cells if their height exceeds the output page height.

        This method is only needed for writers that output on a paginated medium.
        Often these writers can not handle tables where a single cell exceeds the page height.
        Using heuristics in the treecleanerhelper.getNodeHeight function the height of a cell
        is estimated and the cell is split if necessary.        
        """      

        if node.__class__ == Row:
            for cell in node.children:
                h = getNodeHeight(cell, self.cell_splitter_params)
                if h > self.cell_splitter_params['maxCellHeight'] and len(cell.children) > 1:
                    rows = splitRow(node, self.cell_splitter_params)
                    self.report('replacing child', node, rows)
                    node.parent.replaceChild(node, rows)
                    return

            return

        for c in node.children[:]:
            self.splitBigTableCells(c)


    def _getNamedRefs(self, node):
        named_refs= []
        for n in node.getChildNodesByClass(Reference) + [node]:
            if n.__class__ == Reference and n.attributes.get('name'):
                named_refs.append(n)
        return named_refs

    def _safeRemove(self, node, named_refs):
        if node in named_refs:
            node.no_display = True
            return
        for ref in named_refs:
            ref.no_display = True
            table_parents = node.getParentNodesByClass(Table)
            if table_parents:
                ref.moveto(table_parents[0], prefix=True)
            else:
                ref.moveto(node, prefix=True)
        node.parent.removeChild(node)
            
    def removeNoPrintNodes(self, node):
        if (node.hasClassID(self.noDisplayClasses) or not node.visible) and node.parent:
            named_refs = self._getNamedRefs(node)
            if named_refs:
                self.report('removing child - keeping named reference', node)
                self._safeRemove(node, named_refs)
            else:
                self.report('removing child', node)
                node.parent.removeChild(node)
            return

        for c in node.children[:]:
            self.removeNoPrintNodes(c)


    def cleanSectionCaptions(self, node):
        """Remove all block nodes from Section nodes, keep the content. If section title is empty replace section by br node"""

        if node.__class__ == Section and node.parents:
            if not node.children:
                self.report('section contained no children')
                return
            if not node.children[0].getAllDisplayText():
                children = [BreakingReturn()]
                if len(node.children) > 1: # at least one "content" node
                    children.extend(node.children)
                self.report('replaced section with empty title with br node')
                node.parent.replaceChild(node, children)
    
        if node.__class__ == Section:
            caption_node = node.children[0]
            children = caption_node.getAllChildren()
            for c in children:
                if c.isblocknode:
                    self.report('removed block node', c)
                    c.parent.replaceChild(c, c.children)

        for c in node.children[:]:
            self.cleanSectionCaptions(c)
            

    def buildDefinitionLists(self, node):
        if node.__class__ in [DefinitionTerm, DefinitionDescription]:
            if node.getChildNodesByClass(ItemList) or node.getParentNodesByClass(ItemList):
                return
            prev = node.getPrevious()
            parent = node.getParent()
            if prev.__class__ == DefinitionList: 
                node.moveto(prev.getLastChild())
                self.report('moved node to prev. definition list')
            else: 
                dl = DefinitionList()
                parent.replaceChild(node, [dl])
                dl.appendChild(node)
                self.report('created new definition list')

        for c in node.children[:]:
            self.buildDefinitionLists(c)


    def restrictChildren(self, node):

        if node.__class__ in self.allowedChildren.keys():
            for c in node.children[:]:
                if c.__class__ not in self.allowedChildren[node.__class__]:
                    node.removeChild(c)
                    self.report('removed restricted child %s from parent %s' % (c, node))
            return 

        for c in node.children:
            self.restrictChildren(c)


    def simplifyBlockNodes(self, node):
        """Remove paragraphs which have a single block node child - keep the child"""
        if node.__class__ == Paragraph:
            if len(node.children) == 1 and node.children[0].isblocknode:
                if node.parent:
                    node.parent.replaceChild(node, [node.children[0]])
                    self.report('remove superfluous wrapping paragraph from node:', node.children[0])

        for c in node.children:
            self.simplifyBlockNodes(c)

    def removeTextlessStyles(self, node):
        """Remove style nodes that have no children with text"""
        if node.__class__ in self.style_nodes:
            if not node.getAllDisplayText().strip() and node.parent:
                if node.children:
                    node.parent.replaceChild(node, newchildren=node.children)
                    self.report('remove style', node, 'with text-less children', node.children )
                else:
                    node.parent.removeChild(node)
                    self.report('removed style without children', node)
                return

        for c in node.children[:]:
            self.removeTextlessStyles(c)
        

    def removeInvisibleLinks(self, node):
        """Remove category links that are not displayed in the text, but only used to stick the article in a category"""

        if (node.__class__ == CategoryLink or node.__class__ == LangLink) and not node.colon and node.parent:
            node.parent.removeChild(node)
            self.report('remove invisible link', node)
            return

        for c in node.children[:]:
            self.removeInvisibleLinks(c)
          

    def fixPreFormatted(self, node):
        """Rearrange PreFormatted nodes. Text is broken down into individual lines which are separated by BreakingReturns """
        if node.__class__ == PreFormatted:
            if not node.getAllDisplayText().strip() and node.parent:
                node.parent.removeChild(node)
                self.report('removed empty preformatted', node)
            children = node.getAllChildren()
            for c in children:
                lines = c.caption.split('\n')
                if len(lines) > 1:
                    text_nodes = []
                    for line in lines:
                        t = Text(line)
                        text_nodes.append(t)
                        text_nodes.append(BreakingReturn())
                    text_nodes.pop()  # remove last BR
                    c.parent.replaceChild(c, text_nodes)
            return

        for c in node.children:
            self.fixPreFormatted(c)
            
    def fixListNesting(self, node):
        """workaround for #81"""
        if node.__class__ == ItemList and len(node.children) == 1:
            item = node.children[0]
            if len(item.children) == 1 and item.children[0].__class__ == ItemList:
                dd = DefinitionDescription()
                dd.appendChild(item.children[0])
                node.parent.replaceChild(node, [dd])
                self.report('transformed indented list item', node)

        for c in node.children:
            self.fixListNesting(c)


    def linearizeWideNestedTables(self, node):
        """Remove wide tables which are nesting inside another table """
        if node.__class__ == Table:
            if getattr(node, 'isInfobox', False):
                return
            parent_tables = node.getParentNodesByClass(Table)            
            if parent_tables and node.numcols > 15:
                while parent_tables:
                    parent_table = parent_tables.pop(0)
                    cell_items = []
                    for row in parent_table.children:
                        for cell in row.children:
                            for item in cell.children:
                                cell_items.append(item)
                    self.report('wide nested table linearized. wrapper:', node, ' replaced by items:', cell_items)
                    parent_table.parent.replaceChild(parent_table, cell_items)

        for c in node.children:
            self.linearizeWideNestedTables(c)


    def _isBigCell(self, cell):
        is_big = False
        content_len = len(cell.getAllDisplayText())
        num_images = 1 + len(cell.getChildNodesByClass(ImageLink))
        if content_len > 5000/num_images:
            return True

        tables = cell.getChildNodesByClass(Table)
        if tables:
            for table in tables:
                if table.numcols > 30:
                    return True
                if len(table.children) >= 25:
                    return True

        itemlists = cell.getChildNodesByClass(ItemList)
        for itemlist in itemlists:
            if len(itemlist.children) > 25:
                return True

        return is_big


            
    def splitTableToColumns(self, node):
        """Removes a table if contained cells are very large. Column content is linearized."""
        if node.__class__ == Table and not getattr(node, 'isInfobox', False):
            split_table = False
            for row in node.children:
                for cell in row.children:
                    if self._isBigCell(cell):
                        split_table = True

            if node.numcols == 2 and not split_table:
                num_border_tables = 0
                for t in node.getChildNodesByClass(Table):
                    if styleutils.tableBorder(t):
                        colspan = t.getParentNodesByClass(Cell)[0].colspan
                        if colspan != 2:
                            num_border_tables += 1
                if num_border_tables >= 3:
                    split_table = True
                        
            if node.hasClassID(self.split_table_classIDs):
                split_table = True

            if node.numcols >= 3 and len(node.getAllDisplayText())>2500:
                # table in "impact" section of http://en.wikipedia.org/wiki/Futurama
                headings = [False]*node.numcols
                lists = [False]*node.numcols
                for row in node.children:
                    for col_idx, cell in enumerate(row.children):
                        if cell.getChildNodesByClass(Section) or cell.getChildNodesByClass(Big):
                            headings[col_idx] = True
                        if cell.getChildNodesByClass(ItemList):
                            lists[col_idx] = True
                if any(headings) and all(lists):
                    split_table = True

            if split_table:
                cols = [[] for i in range(node.numcols)]

                for row in node.children:
                    for col_idx, cell in enumerate(row.children):
                        for item in cell.children:
                            cols[col_idx].append(item)

                lin_cols = []
                for col in cols:
                    for item in col:
                        lin_cols.append(item)
                self.report('removed table. outputting linearize columns')
                node.parent.replaceChild(node, lin_cols)

        for c in node.children[:]:
            self.splitTableToColumns(c)           

    def fixReferenceNodes(self, node):
        ref_nodes = node.getChildNodesByClass(Reference)

        for ref_node in ref_nodes:
            txt = ref_node.getAllDisplayText().strip()
            if len(txt) <= 1:
                ref_node.parent.removeChild(ref_node)
                self.report('removed empty ref node')

        name2children = {}
        for ref_node in ref_nodes:
            ref_name = ref_node.attributes.get('name')
            if ref_name and ref_name != ref_name.strip('"'):
                ref_name = ref_name.strip('"')
                ref_node.vlist['name'] = ref_name
            if ref_name and ref_node.children and not name2children.has_key(ref_name):
                name2children[ref_name] = ref_node.children

        ref_defined = {}
        for ref_node in ref_nodes:
            ref_name = ref_node.attributes.get('name')
            if not ref_name or not name2children.has_key(ref_name):
                continue
            if ref_node.children:
                if ref_defined.get(ref_name): # del children
                    ref_node.children = []
                else:
                    ref_defined[ref_name] = True
            else:
                if not ref_defined.get(ref_name): # move ref here
                    children = name2children[ref_name]
                    for child in children:
                        ref_node.appendChild(child)
                    ref_defined[ref_name] = True

    def removeEmptyReferenceLists(self, node):
        """
        empty ReferenceLists are removed. they typically stick in a section which only contains the ReferenceList. That section is also removed
        """
        if node.__class__ == ReferenceList:
            sections = node.getParentNodesByClass(Section)
            if sections:
                section = sections[0]
                display_text = []
                for c in section.children[1:]:
                    display_text.append(c.getAllDisplayText().strip())
                if not ''.join(display_text).strip() and section.parent:
                    section.parent.removeChild(section)
                    self.report('removed empty reference list')
                        
        for c in node.children:
            self.removeEmptyReferenceLists(c)



    def removeDuplicateLinksInReferences(self, node):
        if node.__class__ == Reference:
            seen_targets = {}
            links = node.getChildNodesByClass(NamedURL)
            links.extend(node.getChildNodesByClass(URL))
            links.extend(node.getChildNodesByClass(ArticleLink))
            if links:
                for link in links:
                    target = getattr(link, 'caption', None)
                    if target:
                        if seen_targets.get(target):
                            link.parent.removeChild(link)
                        else:
                            seen_targets[target] = True

        for c in node.children:
            self.removeDuplicateLinksInReferences(c)
                
    def removeInvalidFiletypes(self, node):
        """remove ImageLinks which end with the following file types"""
        if node.__class__ == ImageLink:
            for file_ending in self.forbidden_file_endings:
                if node.target.endswith(file_ending):
                    self.report("removed invalid 'image' type with target %r", node.target)
                    node.parent.removeChild(node)

        for c in node.children:
            self.removeInvalidFiletypes(c)

    def limitImageCaptionsize(self, node):

        if node.__class__ == ImageLink:
            txt = node.getAllDisplayText()
            if len(txt) > 500:
                brs = node.getChildNodesByClass(BreakingReturn)
                for br in brs:
                    br.parent.removeChild(br)
                if brs:
                    self.report('removed BreakingReturns from long image caption')

        for c in node.children:
            self.limitImageCaptionsize(c)

    def removeLeadingParaInList(self, node):

        if node.__class__ in [Item, Reference]:
            if node.children and node.children[0].__class__ == Paragraph:
                node.replaceChild(node.children[0], node.children[0].children)
                self.report('remove leading Paragraph in Item')
        for c in node.children:
            self.removeLeadingParaInList(c)


    def fixItemLists(self, node):
        if node.__class__ == ItemList:
            for child in node.children:
                if child.__class__ != Item:
                    i = Item()
                    node.replaceChild(child, [i])
                    i.appendChild(child)
                    self.report('ItemList contained %r. node wrapped in Item node' % child.__class__.__name__)
                    
        for c in node.children:
            self.fixItemLists(c)
    

    def _isEmptyRow(self, row):
        for cell in row.children:
            if cell.children:
                return False
        return True

    def removeEmptyTrailingTableRows(self, node):

        if node.__class__ == Table:
            while node.children and self._isEmptyRow(node.children[-1]):
                node.removeChild(node.children[-1])
                self.report('remove emtpy trailing table row')

        for c in node.children:
            self.removeEmptyTrailingTableRows(c)


    def removeEmptySections(self, node):
        """Remove section nodes which do not contain any text """
        if node.__class__ == Section and node.parent and not node.getParentNodesByClass(Table):
            if len(node.children) == 1:
                node.parent.removeChild(node)
                self.report('removed empty section')
                return
            has_txt = False
            for klass in self.contentWithoutTextClasses:
                if node.getChildNodesByClass(klass):
                    has_txt = True
                    break
            if not has_txt:
                for c in node.children[1:]:
                    if c.getAllDisplayText():
                        has_txt = True
                        break

            if not has_txt:
                self.report('removing empty section')
                node.parent.removeChild(node)
                return
        
        for c in node.children[:]:
            self.removeEmptySections(c)


    def _splitRow(self, node, max_items, all_items):
        cells = node.children
        node.children = []
        for row_index in range(max_items):
            nr = node.copy()
            for (col_index, cell) in enumerate(cells):
                try:
                    content = all_items[col_index][row_index]
                except IndexError:
                    content = None
                cell.children = []
                nc = cell.copy()
                nc.compact = True
                if content:
                    item_list = ItemList()
                    item_list.appendChild(content)
                    item_list.compact = True
                    nc.appendChild(item_list)
                nr.appendChild(nc)                        
            nr.moveto(node, prefix=True)
            if row_index < max_items-1:
                nr.suppress_bottom_border = True
        node.parent.removeChild(node)

    def splitTableLists(self, node):
        """a table row which contains only itemlists is split into muliple rows."""

        if node.__class__ == Row:
            only_lists = True
            max_items = 0
            all_items = []
            for cell in node.children:
                if cell.rowspan > 1:
                    only_lists = False
                    break
                items = [ item for item in cell.getChildNodesByClass(Item) if len(item.getParentNodesByClass(ItemList)) < 2]
                max_items = max(max_items, len(items))
                all_items.append(items)
                for item in cell:
                    if item.__class__ != ItemList:
                        only_lists = False
                        break
                if not only_lists:
                    break
            if only_lists and max_items > 5:
                self._splitRow(node, max_items, all_items)
                self.report('splitting list only table row')
                return
            
        for c in node.children:
            self.splitTableLists(c)
                


    def markShortParagraph(self, node):
        """Hint for writers that allows for special handling of short paragraphs """
        if node.__class__ == Paragraph \
               and len(node.getAllDisplayText()) < 80 \
               and not node.getParentNodesByClass(Table) \
               and not _any([c.isblocknode for c in node.children]):
            node.short_paragraph = True
            
        for c in node.children:
            self.markShortParagraph(c)

    def handleOnlyInPrint(self, node):
        '''Remove nodes with the css class "printonly" which contain URLs.

        printonly nodes are used in citations for example to explicitly print out URLs.
        Since we handle URLs differently, we can ignore printonly nodes
        '''
        if 'printonly' in node.attributes.get('class', ''):
            if _any([c.__class__ in [URL,
                                     NamedURL,
                                     ArticleLink,
                                     NamespaceLink,
                                     InterwikiLink,
                                     SpecialLink] for c in node.children]):
                self.report('removed "printonly" node:', node)
                node.parent.removeChild(node)
                return
        for c in node.children:
            self.handleOnlyInPrint(c)


    def markInfoboxes(self, node):
        if node.__class__ == Article:
            article_ns = getattr(node, 'ns', 0)
            tables = node.getChildNodesByClass(Table)
            found_infobox = False
            for table in tables:
                if miscutils.hasInfoboxAttrs(table) and article_ns != 100:
                    table.isInfobox = found_infobox = True
            if found_infobox or not tables:
                return
            if miscutils.articleStartsWithTable(node, max_text_until_infobox=200) and article_ns != 100:
                tables[0].isInfobox = True
            return

        for c in node.children:
            self.markInfoboxes(c)


    def removeAbsolutePositionedNode(self, node):
        def pos(n):
            return n.style.get('position', '').lower().strip()

        if pos(node) in ['absolute', 'relative']:
            for p in node.getParents():
                if pos(p) in ['absolute', 'relative']:
                    node.parent.removeChild(node)
                    self.report('removed absolute positioned node', node)
                    return

        for c in node.children[:]:
            self.removeAbsolutePositionedNode(c)


    def _unNestCond(self, node):
        tables = node.getChildNodesByClass(Table)
        if tables:
            for table in tables:
                if len(table.children) > 20:
                    return True
        return False


    def unNestEndingCellContent(self, node):
        '''http://de.wikipedia.org/w/index.php?title=Bahnstrecke_Berlin%E2%80%93Dresden&oldid=72891289'''
        if node.__class__ == Table and not node.getParentNodesByClass(Table):
            if not node.children:
                return
            last_row = node.children[-1]
            if not last_row or  len(last_row.children) != 1:
                return
            last_cell = last_row.children[0]
            if last_cell.__class__ != Cell or last_cell.colspan != node.numcols:
                return
            if self._unNestCond(last_cell):
                d = Div()
                d.border = 1
                d.vlist = last_cell.vlist
                for item in last_cell.children:
                    d.appendChild(item)
                last_cell.children = []
                d.moveto(node)
                self.report('moved content behind table', d)

        for c in node.children:
            self.unNestEndingCellContent(c)


    def removeScrollElements(self, node):
        '''overflow:auto
http://en.wikipedia.org/wiki/Pope_John_Paul_II
http://de.wikipedia.org/wiki/Portal:Maschinenbau/Themenliste_Maschinenbau
http://de.wikipedia.org/wiki/Portal:Ethnologie
'''
        if node.style and node.parent and node.style.get('overflow', '').lower() == 'auto':
            height = styleutils.scaleLength(node.style.get('height', ''))
            if height > 100:
                if node.getParentNodesByClass(Table) or node.__class__ == Table :
                    node.force_tablesplit = True
                    if node.getParentNodesByClass(Table):
                        table_node = node.getParentNodesByClass(Table)[0]
                    else:
                        table_node = node
                    content = []
                    for cell in table_node.getChildNodesByClass(Cell):
                        content.extend(cell.children)
                    table_node.parent.replaceChild(table_node, content)
                    self.report('removed overflow:auto table')
                    return
                else:
                    continue_node = node.parent
                    node.parent.replaceChild(node, node.children)
                    node = continue_node
                    self.report('removed overflow:auto element')

        for c in node.children:
            self.removeScrollElements(c)


    def galleryFix(self, node):
        '''move gallery nodes out of tables.'''
        galleries = node.getChildNodesByClass(Gallery)
        for g in galleries:
            tables = g.getParentNodesByClass(Table)
            if tables:
                g.moveto(tables[0])
                self.report('removed gallery from table')

    def fixSubSup(self, node):
        if node.__class__ in [Sup, Sub] and node.parent:
            if len(node.getAllDisplayText())>200:
                node.parent.replaceChild(node, node.children)
                self.report('removed long sup/sub')
        for c in node.children:
            self.fixSubSup(c)

    def removeEditLinks(self, node):

        if node.__class__ == NamedURL and node.caption.endswith('?action=edit'):
            self.report('removing edit link', node)
            node.parent.removeChild(node)

        for c in node:
            self.removeEditLinks(c)

    def removeSeeAlso(self, node):
        try:
            seealso_section =  _('See also')
        except NameError:
            seealso_section = 'See also'


        if node.__class__ == Section \
           and len(node.children):
            try:
                section_title = node.children[0].children[0].caption
            except IndexError:
                section_title = ''
            if isinstance(section_title, basestring) and section_title.strip() == seealso_section:
                self.report('removed see also section', node)
                node.parent.removeChild(node)

        for c in node:
            self.removeSeeAlso(c)



    def cleanVlist(self, node):
        if node.vlist:
            for attr, val in node.vlist.items():
                if attr != attr.lower():
                    node.vlist[attr.lower()] = val

        for c in node:
            self.cleanVlist(c)

    def _isLTR(self, node):
        if isinstance(node, Math):
            return True
        if isinstance(node, Text):
            for c in node.caption:
                if unicodedata.bidirectional(c) not in ['WS']:
                    return False
            return True
        return False

    def fixMathDir(self, node):
        if not self.rtl:
            return
        math_nodes = node.getChildNodesByClass(Math)
        for m in math_nodes:
            p = m.parent
            if all(self._isLTR(c) for c in p.children):
                p.vlist['dir'] = 'ltr'

    # wikivoyage tweaks
    def fixRegionListTables(self, node):
        if (node.vlist
            and node.vlist.get('id', '') == 'region_list'
            and isinstance(node, Div)):
            rows = node.getChildNodesByClass(Row)
            t = Table()
            for row in rows:
                t.appendChild(row)
            for c in node.children[:]:
                if isinstance(c, Table):
                    node.removeChild(c)
            node.appendChild(t)
        for c in node:
            self.fixRegionListTables(c)

########NEW FILE########
__FILENAME__ = treecleanerhelper
#! /usr/bin/env python
#! -*- coding:utf-8 -*-

# Copyright (c) 2007, PediaPress GmbH
# See README.rst for additional licensing information.

import math

from advtree import Cell, ImageLink, Link, Math, NamedURL, Reference, Text, URL

def getNodeHeight(node, params):
    lineHeight = params['lineHeight']
    charsPerLine = params['charsPerLine']
    paragraphMargin = params['paragraphMargin']
    imgHeight = params['imgHeight']
    
    height = 0
    nonFollowNodes = [Reference, NamedURL]
    amap = {Text:"caption", Link:"target", URL:"caption", Math:"caption", NamedURL:'caption'}
    access = amap.get(node.__class__, "")
    if access:
        if node.__class__ == Link and node.children:
            txt = ''
        else:
            txt = getattr(node, access)
        if txt:
            addHeight = math.ceil(len(txt)/charsPerLine) * lineHeight # 40 chars per line --> number of lines --> 20pt height per line
            if node.isblocknode:
                addHeight += paragraphMargin
            else:
                addHeight = addHeight / 2 # for inline nodes we reduce the height guess. below that is compensated for blocknode-heights w/o text
            height += addHeight            
    elif node.__class__ == ImageLink:
        if node.isInline(): #image heights are just wild guesses. in case of normal image, we assume 5 lines of text in height
            height += 0 #lineHeight
        else:
            height += lineHeight * imgHeight
    elif node.isblocknode: # compensation for e.g. listItems which contain text. 
        height += 0.5 * lineHeight

    for n in node.children[:]:
        if n.__class__  not in nonFollowNodes:
            height += getNodeHeight(n, params)
    return height

def splitRow(row, params):
    maxCellHeight = params['maxCellHeight']
    newrows = []
    cols = [ [] for i in range(len(row.children))]
    for (colindex, cell) in enumerate(row.children):
        cellHeight = 0
        items = []
        for item in cell.children:
            cellHeight += getNodeHeight(item, params)
            if not items or cellHeight < maxCellHeight:
                items.append(item)
            else:
                cols[colindex].append(items)
                items = [item]
                cellHeight = 0
        if items:
            cols[colindex].append(items)

    maxNewRows = max([len(col) for col in cols])
    
    for rowindex in range(maxNewRows):        
        newrow = row.copy()
        newrow.children = []
        for colindex in range(len(cols)):
            try:
                cellchildren = cols[colindex][rowindex]
            except IndexError:
                cellchildren = [] # fixme maybe some better empty child
            cell = Cell()
            try:
                cell.vlist = row.children[colindex].vlist
            except:
                pass
            for c in cellchildren:
                cell.appendChild(c)
            newrow.appendChild(cell)
            newrow.suppress_bottom_border = True
        newrows.append(newrow)   

    return newrows

########NEW FILE########
__FILENAME__ = trustedrevs
#! /usr/bin/env python

import mwclient
import urllib2
import time

class WikiTrustServerError(Exception):
    "For some reason the API often respons with an error"
    msg_identifier = 'EERROR detected.  Try again in a moment, or report an error on the WikiTrust bug tracker.'

class TrustedRevisions(object):
    min_trust = 0.80
    wikitrust_api = 'http://en.collaborativetrust.com/WikiTrust/RemoteAPI'
    
    def __init__(self, site_name = 'en.wikipedia.org'):
            self.site = mwclient.Site(site_name)
        
    def getWikiTrustScore(self, title, revid):
        # http://en.collaborativetrust.com/WikiTrust/RemoteAPI?method=quality&title=Buster_Keaton&pageid=43055&revid=364710354
        url = '%s?method=quality&title=%s&revid=%d' % \
                (self.wikitrust_api, urllib2.quote(title), revid)
        r = urllib2.urlopen(url).read()
        if r == WikiTrustServerError.msg_identifier:
            raise WikiTrustServerError
        return 1 - float(r) # r is the likelyhood of being spam 
        
            
    def getTrustedRevision(self, title, min_trust=None, max_age=None):
        min_trust = min_trust or self.min_trust
        best_rev = None
        now = time.time() 
        p= self.site.Pages[title]
        for rev in p.revisions():
            #add basic info
            rev['age'] = (now - time.mktime(rev['timestamp']))/(24*3600) # days
            rev['title'] = title
            
            # don't use bot revs
            if 'bot' in rev['user'].lower(): 
                continue
            # don't use reverted revs (but rather the original one)
            if 'revert' in rev.get('comment','').lower():
                continue
            
            try:
                rev['score'] = self.getWikiTrustScore(title, rev['revid'])
            except WikiTrustServerError:
                #print '%s\t%d\terror' %(title, rev['revid'])
                continue
            
            #print '%s\t%d\t%.2f' %(title, rev['revid'], rev['score'])            
            if not best_rev or rev['score'] > best_rev['score']:
                best_rev = rev
            if rev['score'] > min_trust: # break if we have a sufficient score    
                break
            if max_age and rev['age'] > max_age: # break if articles get too old
                break
            
        return best_rev
        
        
if __name__ == '__main__':
    import sys
    tr = TrustedRevisions()
    trev = tr.getTrustedRevision(sys.argv[1])
    print 'Found revision:', trev
    print 'title:%s revid:%d age:%.2f days' %(trev['title'], trev['revid'], trev['age'])

########NEW FILE########
__FILENAME__ = uniq

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import os
import re

class Uniquifier(object):
    random_string = None
    rx = None
    def __init__(self):
        self.uniq2repl = {}
        if self.random_string is None:
            import binascii
            r=os.urandom(8)
            self.__class__.random_string = binascii.hexlify(r)
       
    def get_uniq(self, repl, name):
        r = self.random_string
        count = len(self.uniq2repl)
        retval = "\x7fUNIQ-%s-%s-%s-QINU\x7f" % (name, count, r)
        self.uniq2repl[retval] = repl
        return retval
        
    def _repl_from_uniq(self, mo):
        u = mo.group(0)
        t = self.uniq2repl.get(u, None)
        if t is None:
            return u
        return t["complete"]

    def replace_uniq(self, txt):
        rx=re.compile("\x7fUNIQ-[a-z0-9]+-\\d+-[a-f0-9]+-QINU\x7f")
        txt = rx.sub(self._repl_from_uniq, txt)
        return txt
    
    def _repl_to_uniq(self, mo):
        tagname = mo.group("tagname")
        if tagname is None:
            # comment =  mo.group("comment")
            if self.txt[mo.start()]=='\n' and self.txt[mo.end()-1]=='\n':
                return '\n'
            return (mo.group(2) or "")+(mo.group(3) or "")

        else:
            tagname = tagname.lower()
        
        r = dict(
            tagname=tagname,
            inner = mo.group("inner") or u"",
            vlist = mo.group("vlist") or u"",
            complete = mo.group(0) )

        if tagname==u"nowiki":
            r["complete"] = r["inner"]

        return self.get_uniq(r, tagname)
    
    def replace_tags(self, txt):
        self.txt = txt
        rx = self.rx
        if rx is None:
            tags = set("nowiki math imagemap gallery source pre ref timeline poem pages".split())
            from mwlib import tagext
            tags.update(tagext.default_registry.names())

            rx = """
                (?P<comment> (\\n[ ]*)?<!--.*?-->([ ]*\\n)?) |
                (?:
                <(?P<tagname> NAMES)
                (?P<vlist> \\s[^<>]*)?
                (/>
                 |
                 (?<!/) >
                (?P<inner>.*?)
                </(?P=tagname)\\s*>))
            """

            rx = rx.replace("NAMES", "|".join(list(tags)))
            rx = re.compile(rx, re.VERBOSE | re.DOTALL | re.IGNORECASE)
            self.rx = rx 
        newtxt = rx.sub(self._repl_to_uniq, txt)
        return newtxt

########NEW FILE########
__FILENAME__ = uparser

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

from mwlib.refine.uparser import simpleparse, parseString

########NEW FILE########
__FILENAME__ = utils
from email.mime.text import MIMEText
from email.utils import make_msgid, formatdate

import errno
import os
import pprint
import re
import smtplib
import socket
import StringIO
import sys
import tempfile
import time
import traceback
import urllib
import urllib2
import urlparse
import UserDict

from mwlib.log import Log

from hashlib import md5

# provide all() for python 2.4
all = all
log = Log('mwlib.utils')


def fsescape(s):
    """Escape string to be safely used in path names

    @param s: some string
    @type s: basestring

    @returns: escaped string
    @rtype: str
    """

    res = []
    for x in s:
        c = ord(x)
        if c > 127 or c == 47 or c == 92:  # 47==slash, 92==backslash
            res.append("~%s~" % c)
        elif c == 126:  # ord("~")==126
            res.append("~~")
        else:
            res.append(x.encode('ascii'))
    return "".join(res)


def start_logging(path, stderr_only=False):
    """Redirect all output to sys.stdout or sys.stderr to be appended to a file,
    redirect sys.stdin to /dev/null.

    @param path: filename of logfile
    @type path: basestring

    @param stderr_only: if True, only redirect stderr, not stdout & stdin
    @type stderr_only: bool
    """

    if not stderr_only:
        sys.stdout.flush()
    sys.stderr.flush()

    f = open(path, "a")
    fd = f.fileno()
    if not stderr_only:
        os.dup2(fd, 1)
    os.dup2(fd, 2)

    if not stderr_only:
        null = os.open(os.path.devnull, os.O_RDWR)
        os.dup2(null, 0)
        os.close(null)


def get_multipart(filename, data, name):
    """Build data in format multipart/form-data to be used to POST binary data

    @param filename: filename to be used in multipart request
    @type filenaem: basestring

    @param data: binary data to include
    @type data: str

    @param name: name to be used in multipart request
    @type name: basestring

    @returns: tuple containing content-type and body for the request
    @rtype: (str, str)
    """

    if isinstance(filename, unicode):
        filename = filename.encode('utf-8', 'ignore')
    if isinstance(name, unicode):
        name = name.encode('utf-8', 'ignore')

    boundary = "-" * 20 + ("%f" % time.time()) + "-" * 20

    items = []
    items.append("--" + boundary)
    items.append('Content-Disposition: form-data; name="%(name)s"; filename="%(filename)s"'\
                 % {'name': name, 'filename': filename})
    items.append('Content-Type: application/octet-stream')
    items.append('')
    items.append(data)
    items.append('--' + boundary + '--')
    items.append('')

    body = "\r\n".join(items)
    content_type = 'multipart/form-data; boundary=%s' % boundary

    return content_type, body


def safe_unlink(filename):
    """Never failing os.unlink()"""

    try:
        os.unlink(filename)
    except Exception, exc:
        log.warn('Could not remove file %r: %s' % (filename, exc))


fetch_cache = {}


def fetch_url(url, ignore_errors=False, fetch_cache=fetch_cache,
    max_cacheable_size=1024, expected_content_type=None, opener=None,
    output_filename=None, post_data=None, timeout=10.0):
    """Fetch given URL via HTTP

    @param ignore_errors: if True, log but otherwise ignore errors, return None
    @type ignore_errors: bool

    @param fetch_cache: dictionary used as cache, with urls as keys and fetched
        data as values
    @type fetch_cache: dict

    @param max_cacheable_size: max. size for responses to be cached
    @type max_cacheable_size: int

    @param expected_content_type: if given, raise (or log) an error if the
        content-type of the reponse does not mathc
    @type expected_content_type: str

    @param opener: if give, use this opener instead of instantiating a new one
    @type opener: L{urllib2.URLOpenerDirector}

    @param output_filename: write response to given file
    @type output_filename: basestring

    @param post_data: if given use POST request
    @type post_data: dict

    @param timeout: timeout in seconds
    @type timeout: float

    @returns: fetched response or True if filename was given; None when
        ignore_errors is True, and the request failed
    @rtype: str
    """

    if not post_data and url in fetch_cache:
        return fetch_cache[url]
    log.info("fetching %r" % (url,))
    if not hasattr(socket, "_delegate_methods"):  # not using gevent?
        socket.setdefaulttimeout(timeout)
    if opener is None:
        opener = urllib2.build_opener()
        opener.addheaders = [('User-agent', 'mwlib')]
    try:
        if post_data:
            post_data = urllib.urlencode(post_data)
        result = opener.open(url, post_data)
        data = result.read()
        if expected_content_type:
            content_type = result.info().gettype()
            if content_type != expected_content_type:
                msg = 'Got content-type %r, expected %r' % (
                    content_type,
                    expected_content_type,
                )
                if ignore_errors:
                    log.warn(msg)
                else:
                    raise RuntimeError(msg)
                return None
    except urllib2.URLError, err:
        if ignore_errors:
            log.error("%s - while fetching %r" % (err, url))
            return None
        raise RuntimeError('Could not fetch %r: %s' % (url, err))
    #log.info("got %r (%dB in %.2fs)" % (url, len(data), time.time() - start_time))

    if hasattr(fetch_cache, 'max_cacheable_size'):
        max_cacheable_size = max(fetch_cache.max_cacheable_size, max_cacheable_size)
    if len(data) <= max_cacheable_size:
        fetch_cache[url] = data

    if output_filename:
        open(output_filename, 'wb').write(data)
        return True
    else:
        return data


def uid(max_length=10):
    """Generate a unique identifier of given maximum length

    @parma max_length: maximum length of identifier
    @type max_length: int

    @returns: unique identifier
    @rtype: str
    """

    some_bytes = os.urandom((max_length + 1) // 2)
    return "".join(hex(ord(x))[2:] for x in some_bytes)[:max_length]


def ensure_dir(d):
    """If directory d does not exist, create it

    @param d: name of an existing or not-yet-existing directory
    @type d: basestring

    @returns: d
    @rtype: basestring
    """

    if not os.path.isdir(d):
        os.makedirs(d)
    return d


def send_mail(from_email, to_emails, subject, body, headers=None, host='mail', port=25):
    """Send an email via SMTP

    @param from_email: email address for From: header
    @type from_email: str

    @param to_emails: sequence of email addresses for To: header
    @type to_email: [str]

    @param subject: text for Subject: header
    @type subject: unicode

    @param body: text for message body
    @type body: unicode

    @param host: mail server host
    @type host: str

    @param port: mail server port
    @type port: int
    """

    connection = smtplib.SMTP(host, port)
    msg = MIMEText(body.encode('utf-8'), 'plain', 'utf-8')
    msg['Subject'] = subject.encode('utf-8')
    msg['From'] = from_email
    msg['To'] = ', '.join(to_emails)
    msg['Date'] = formatdate()
    msg['Message-ID'] = make_msgid()
    if headers is not None:
        for k, v in headers.items():
            if not isinstance(v, str):
                v = str(v)
            msg[k] = v
    connection.sendmail(from_email, to_emails, msg.as_string())
    connection.close()


def asunicode(x):
    if not isinstance(x, basestring):
        x = repr(x)

    if isinstance(x, str):
        x = unicode(x, "utf-8", "replace")

    return x


def ppdict(dct):
    items = dct.items()
    items.sort()
    tmp = []
    write = tmp.append

    for k, v in items:
        write(u"*" + asunicode(k) + u"*")
        v = asunicode(v)
        lines = v.split("\n")
        for x in lines:
            write(" " * 4 + x)
        write("")

    return u"\n".join(tmp)


def report(system='', subject='', from_email=None, mail_recipients=None, mail_headers=None, **kw):
    log.report('system=%r subject=%r' % (system, subject))

    text = []
    text.append('SYSTEM: %r\n' % system)
    text.append('%s\n' % traceback.format_exc())
    try:
        fqdn = socket.getfqdn()
    except:
        fqdn = 'not available'
    text.append('CWD: %r\n' % os.getcwd())

    text.append(ppdict(kw).encode("utf-8"))
    # text.append('KEYWORDS:\n%s\n' % pprint.pformat(kw, indent=4))
    text.append('ENV:\n%s\n' % pprint.pformat(dict(os.environ), indent=4))

    text = '\n'.join(text)

    if not (from_email and mail_recipients):
        return text

    try:
        if not isinstance(subject, str):
            subject = repr(subject)
        send_mail(
            from_email,
            mail_recipients,
            'REPORT [%s]: %s' % (fqdn, subject),
            text,
            headers=mail_headers,
        )
        log.info('sent mail to %r' % mail_recipients)
    except Exception, e:
        log.ERROR('Could not send mail: %s' % e)
    return text


def get_safe_url(url):
    if not isinstance(url, str):
        url = url.encode('utf-8')

    nonwhitespace_rex = re.compile(r'^\S+$')
    try:
        result = urlparse.urlsplit(url)
        scheme, netloc, path, query, fragment = result
    except Exception, exc:
        log.warn('urlparse(%r) failed: %s' % (url, exc))
        return None

    if not (scheme and netloc):
        log.warn('Empty scheme or netloc: %r %r' % (scheme, netloc))
        return None

    if not (nonwhitespace_rex.match(scheme) and nonwhitespace_rex.match(netloc)):
        log.warn('Found whitespace in scheme or netloc: %r %r' % (scheme, netloc))
        return None

    try:
        # catches things like path='bla " target="_blank'
        path = urllib.quote(urllib.unquote(path))
    except Exception, exc:
        log.warn('quote(unquote(%r)) failed: %s' % (path, exc))
        return None
    try:
        return urlparse.urlunsplit((scheme, netloc, path, query, fragment))
    except Exception, exc:
        log.warn('urlunparse() failed: %s' % exc)


def get_nodeweight(obj):
    """
    utility function that returns a
    node class and it's weight
    can be used for statistics
    to get some stats when NO Advanced Nodes are available
    """
    k = obj.__class__.__name__
    if k in ('Text',):
        return k, len(obj.caption)
    elif k == 'ImageLink' and obj.isInline():
        return 'InlineImageLink', 1
    return k, 1


# -- extract text from pdf file. used for testing only.
def pdf2txt(path):
    """extract text from pdf file"""
    # based on http://code.activestate.com/recipes/511465/
    import pyPdf

    content = []
    pdf = pyPdf.PdfFileReader(file(path, "rb"))

    numpages = pdf.getNumPages()
    for i in range(0, numpages):
        # Extract text from page and add to content
        content.append(pdf.getPage(i).extractText())

    return "\n".join(content)


def garble_password(argv):
    argv = list(argv)
    idx = 0
    while True:
        try:
            idx = argv[idx:].index('--password') + 1
        except ValueError:
            break
        if idx >= len(argv):
            break
        argv[idx] = '{OMITTED}'
    return argv

########NEW FILE########
__FILENAME__ = utoken
#! /usr/bin/env python

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

# unified/universal token

import sys
import re
import _uscan as _mwscan
from mwlib.refine.util import resolve_entity, parseParams

def walknode(node, filt=lambda x: True):
    if not isinstance(node, token):
        for x in node:
            for k in walknode(x):
                if filt(k):
                    yield k
        return
    
    if filt(node):
        yield node
        
    if node.children:
        for x in node.children:
            for k in walknode(x):
                if filt(k):
                    yield k

def walknodel(node, filt=lambda x:True):
    return list(walknode(node, filt=filt))

def show(node, out=None, indent=0, verbose=False):
    if node is None:
        return
    
    if out is None:
        out = sys.stdout

    if not isinstance(node, token):
        for x in node:
            show(x, out=out, indent=indent, verbose=verbose)
        return

    out.write("%s%r\n" % ("    "*indent, node))
    
    children = node.children
    if children:
        for x in children:
            show(x, out=out, indent=indent+1, verbose=verbose)
            
class _show(object):
    def __get__(self, obj, type=None):
        if obj is None:
            return lambda node, out=None: show(node, out=out)
        else:
            return lambda out=None: show(obj, out=out)
            
class token(object):
    caption = ''
    vlist = None
    target = None
    level = None
    children = None

    rawtagname = None 
    tagname = None
    ns = None
    lineprefix = None
    interwiki = None
    langlink = None
    namespace = None
    blocknode = False

    # image attributes
    align = None
    thumb = False
    frame = None
    
    
    t_end = 0
    t_text = 1
    t_entity = 2
    t_special = 3
    t_magicword = 4
    t_comment = 5
    t_2box_open = 6
    t_2box_close = 7
    t_http_url = 8
    t_break = 9
    t_begintable = t_begin_table = 10
    t_endtable = t_end_table = 11
    t_html_tag = 12
    t_singlequote = 13
    t_pre = 14
    t_section = 15
    t_endsection = t_section_end = 16
    
    t_item = 17
    t_colon = 18
    t_semicolon = 19
    t_hrule = 20
    t_newline = 21
    t_column = 22
    t_row = 23
    t_tablecaption = 24
    t_urllink = 25
    t_uniq = 26
    
    t_html_tag_end = 100
    
    token2name = {}
    _text = None

    @staticmethod
    def join_as_text(tokens):
        return u"".join([x.text or u"" for x in tokens])
    
    def _get_text(self):
        if self._text is None and self.source is not None:
            self._text = self.source[self.start:self.start+self.len]
        return self._text
    
    def _set_text(self, t):
        self._text = t

    text = property(_get_text, _set_text)
    
    def __init__(self, type=None, start=None, len=None, source=None, text=None, **kw):
        self.type = type
        self.start = start
        self.len = len
        self.source = source
        if text is not None:
            self.text = text
            
        self.__dict__.update(kw)

    def __repr__(self):
        if type(self) is token:
            r = [self.token2name.get(self.type, self.type)]
        else:
            r =  [self.__class__.__name__]
        if self.text is not None:
            r.append(repr(self.text)[1:])
        if self.tagname:
            r.append(" tagname=")
            r.append(repr(self.tagname))
        if self.rawtagname:
            r.append(" rawtagname=")
            r.append(repr(self.rawtagname))
            
        if self.vlist:
            r.append(" vlist=")
            r.append(repr(self.vlist))
        if self.target:
            r.append(" target=")
            r.append(repr(self.target))
        if self.level:
            r.append(" level=")
            r.append(repr(self.level))
        if self.ns is not None:
            r.append(" ns=")
            r.append(repr(self.ns))
        if self.lineprefix is not None:
            r.append(" lineprefix=")
            r.append(self.lineprefix)
        if self.interwiki:
            r.append(" interwiki=")
            r.append(repr(self.interwiki))
        if self.langlink:
            r.append(" langlink=")
            r.append(repr(self.langlink))
        if self.type==self.t_complex_style:
            r.append(repr(self.caption))
        elif self.caption:
            r.append("->")
            r.append(repr(self.caption))
            
        return u"".join(r)
    
            
    show = _show()
    
token2name = token.token2name
for d in dir(token):
    if d.startswith("t_"):
        token2name[getattr(token, d)] = d
del d, token2name

def _split_tag(txt):
    m=re.match(" *(\w+)(.*)", txt, re.DOTALL)
    assert m is not None, "could not match tag name"
    name = m.group(1)
    values = m.group(2)
    return name, values
    
def _analyze_html_tag(t):
    text = t.text
    selfClosing = False
    if text.startswith(u"</"):
        name = text[2:-1]
        isEndToken = True
    elif text.endswith("/>"):
        name = text[1:-2]
        selfClosing = True
        isEndToken = False # ???
    else:
        name = text[1:-1]
        isEndToken = False

    name, values = _split_tag(name)
    t.vlist = parseParams(values)
    name = name.lower()

    if name=='br':
        isEndToken = False

    t.rawtagname = name
    t.tag_selfClosing = selfClosing
    t.tag_isEndToken = isEndToken
    if isEndToken:
        t.type = t.t_html_tag_end

def dump_tokens(text, tokens):
    for type, start, len in tokens:
        print type, repr(text[start:start+len])
           
def scan(text):
    text += u"\0"*32    
    return _mwscan.scan(text)
                         
class _compat_scanner(object):
    allowed_tags = None

    def _init_allowed_tags(self):
        self.allowed_tags = set("""
abbr b big blockquote br center cite code del div em endfeed font h1 h2 h3
h4 h5 h6 hr i index inputbox ins kbd li ol p pages references rss s small span
startfeed strike strong sub sup caption table td th tr tt u ul var dl dt dd
""".split())
        
        
    def __call__(self, text, uniquifier=None):
        if self.allowed_tags is None:
            self._init_allowed_tags()

        if isinstance(text, str):
            text = unicode(text)
            
        tokens = scan(text)

        res = []

        def g():
            return text[start:start+tlen]

        for type,  start, tlen in tokens:
            
            if type==token.t_begintable:
                txt = g()
                count = txt.count(":")
                if count:
                    res.append(token(type=token.t_colon, start=start, len=count, source=text))
                tlen -= count
                start += count
                    
                
                
            t = token(type=type, start=start, len=tlen, source=text)

            if type==token.t_entity:
                t.text = resolve_entity(g())
                t.type = token.t_text
                res.append(t)
            elif type==token.t_html_tag:
                s = g()
                if uniquifier:
                    s=uniquifier.replace_uniq(s)
                    t.text = s
                _analyze_html_tag(t)
                tagname = t.rawtagname
                
                if tagname in self.allowed_tags:
                    res.append(t)
                else:
                    res.append(token(type=token.t_text, start=start, len=tlen, source=text))
            else:
                res.append(t)

        return res
        
compat_scan = _compat_scanner()

def tokenize(input, name="unknown", uniquifier=None):
    assert input is not None, "must specify input argument in tokenize"
    return compat_scan(input, uniquifier=uniquifier)

########NEW FILE########
__FILENAME__ = wiki
#! /usr/bin/env python

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import os
from ConfigParser import ConfigParser
import StringIO

from mwlib.log import Log
from mwlib import myjson
from mwlib.metabook import wikiconf

log = Log('mwlib.utils')
    

def wiki_obsolete_cdb(path=None,  **kwargs):
    raise RuntimeError("cdb file format is not supported anymore.")


dispatch = dict(
    wiki = dict(cdb=wiki_obsolete_cdb, nucdb=wiki_obsolete_cdb)
)

_en_license_url = 'http://en.wikipedia.org/w/index.php?title=Help:Books/License&action=raw'
wpwikis = dict(
    de = dict(baseurl='http://de.wikipedia.org/w/', 
              mw_license_url='http://de.wikipedia.org/w/index.php?title=Hilfe:Buchfunktion/Lizenz&action=raw'),
    en = dict(baseurl='http://en.wikipedia.org/w/', mw_license_url=_en_license_url),
    fr = dict(baseurl='http://fr.wikipedia.org/w/', mw_license_url=None),
    es = dict(baseurl='http://es.wikipedia.org/w/', mw_license_url=None),
    pt = dict(baseurl='http://pt.wikipedia.org/w/', mw_license_url=None),
    enwb = dict(baseurl='http://en.wikibooks.org/w', mw_license_url=_en_license_url),
    commons = dict(baseurl='http://commons.wikimedia.org/w/', mw_license_url=_en_license_url)
    )


class Environment(object):
    wikiconf = None
    
    def __init__(self, metabook=None):
        self.metabook = metabook
        self.images = None
        self.wiki = None
        self.configparser = ConfigParser()
        defaults=StringIO.StringIO("""
[wiki]
name=
url=
""")
        self.configparser.readfp(defaults)
    
    def init_metabook(self):
        if self.metabook:
            self.metabook.set_environment(self)

    def getLicenses(self):
        return self.wiki.getLicenses()
    
class MultiEnvironment(Environment):
    wiki = None
    images = None
    
    def __init__(self, path):
        Environment.__init__(self)
        self.path = path
        self.metabook = myjson.load(open(os.path.join(self.path, "metabook.json")))
        self.id2env = {}
        
    def init_metabook(self):
        from mwlib import nuwiki
        if not self.metabook:
            return
        
        for x in self.metabook.articles():
            id = x.wikiident
            assert id, "article has no wikiident: %r" % (x,)
            assert "/" not in id
            assert ".." not in id
            
            if id not in self.id2env:
                env = Environment()
                env.images = env.wiki = nuwiki.adapt(os.path.join(self.path, id))
                self.id2env[id] = env
            else:
                env = self.id2env[id]
            x._env = env
            
    def getLicenses(self):
        res = list(self.metabook.licenses or [])
        for t in res:
            t._wiki = None
            
        for x in self.id2env.values():
            tmp = x.wiki.getLicenses()
            for t in tmp:
                t._env = x
            res += tmp
        
        return res

def ndict(**kw):
    for k, v in kw.items():
        if v is None:
            del kw[k]
    return kw

def _makewiki(conf, metabook=None, **kw):
    kw = ndict(**kw)
    res = Environment(metabook)
    
    url = None
    if conf.startswith(':'):
        if conf[1:] not in wpwikis:
            wpwikis[conf[1:]] =  dict(baseurl = "http://%s.wikipedia.org/w/" % conf[1:],
                                      mw_license_url =  None)
            

        url = wpwikis.get(conf[1:])['baseurl']

    if conf.startswith("http://") or conf.startswith("https://"):
        url = conf

    if url:
        res.wiki = None
        res.wikiconf = wikiconf(baseurl=url, **kw)
        res.image = None
        return res

    nfo_fn = os.path.join(conf, 'nfo.json')
    if os.path.exists(nfo_fn):
        from mwlib import nuwiki
        from mwlib import myjson as json

        try:
            format = json.load(open(nfo_fn, 'rb'))['format']
        except KeyError:
            pass
        else:
            if format == 'nuwiki':
                res.images = res.wiki = nuwiki.adapt(conf)
                res.metabook = res.wiki.metabook
                return res
            elif format == 'multi-nuwiki':
                return MultiEnvironment(conf)

    if os.path.exists(os.path.join(conf, "content.json")):
        raise RuntimeError("old zip wikis are not supported anymore")

    # yes, I really don't want to type this everytime
    wc = os.path.join(conf, "wikiconf.txt")
    if os.path.exists(wc):
        conf = wc 
        
    if conf.lower().endswith(".zip"):
        import zipfile
        from mwlib import myjson as json
        conf = os.path.abspath(conf)
        
        zf = zipfile.ZipFile(conf)
        try:
            format = json.loads(zf.read("nfo.json"))["format"]
        except KeyError:
            raise RuntimeError("old zip wikis are not supported anymore")

        if format=="nuwiki":
            from mwlib import nuwiki
            res.images = res.wiki = nuwiki.adapt(zf)
            if metabook is None:
                res.metabook = res.wiki.metabook
            return res
        elif format==u'multi-nuwiki':
            from mwlib import nuwiki
            import tempfile
            tmpdir = tempfile.mkdtemp()
            nuwiki.extractall(zf, tmpdir)
            res = MultiEnvironment(tmpdir)
            return res
        else:
            raise RuntimeError("unknown format %r" % (format,))
        
    

    cp = res.configparser
    
    if not cp.read(conf):
        raise RuntimeError("could not read config file %r" % (conf,))

        
    for s in ['images', 'wiki']:
        if not cp.has_section(s):
            continue
        
        args = dict(cp.items(s))
        if "type" not in args:
            raise RuntimeError("section %r does not have key 'type'" % s)
        t = args['type']
        del args['type']
        try:
            m = dispatch[s][t]
        except KeyError:
            raise RuntimeError("cannot handle type %r in section %r" % (t, s))

        setattr(res, s, m(**args))
    
    assert res.wiki is not None, '_makewiki should have set wiki attribute'
    return res

def makewiki(config, metabook=None, **kw):
    if not config:
        res = Environment(metabook)
    else:
        res = _makewiki(config, metabook=metabook, **kw)
        
    if res.wiki:
        res.wiki.env = res
    if res.images:
        res.images.env = res

    res.init_metabook()
    
    return res

########NEW FILE########
__FILENAME__ = fontswitcher
#! /usr/bin/env python
#! -*- coding:utf-8 -*-

# Copyright (c) 2007, PediaPress GmbH
# See README.rst for additional licensing information.

import re
import os

class Scripts(object):

    def __init__(self):
        scripts_filename = os.path.join(os.path.dirname(__file__), 'scripts.txt')
        self.script2code_block = {}
        self.code_block2scripts = []
        self.readScriptFile(scripts_filename)

    def readScriptFile(self, fn):
        try:
            f = open(fn)
        except IOError:
            raise Exception('scripts.txt file not found at: %r' % fn)
        for line in f.readlines():
            res = re.search('([A-Z0-9]+)\.\.([A-Z0-9]+); (.*)' , line.strip())
            if res:
                start_block, end_block, script = res.groups()
                self.script2code_block[script.lower()] = (int(start_block, 16), int(end_block, 16))
                self.code_block2scripts.append((int(start_block, 16), int(end_block, 16), script))

    def getCodePoints(self, script):
        code_block = self.script2code_block.get(script.lower())
        if not code_block:
            return (0, 0)
        else:
            return code_block

    def getScriptsForCodeBlock(self, code_block):
        scripts = set()
        for start_block, end_block, script in self.code_block2scripts:
            if start_block <= code_block[0] <= end_block:
                scripts.add(script)
            if start_block <= code_block[1] <= end_block:
                scripts.add(script)
                break
        return scripts

    def getScriptsForCodeBlocks(self, code_blocks):
        scripts = set()
        for code_block in code_blocks:
            scripts = scripts.union(self.getScriptsForCodeBlock(code_block))
        return scripts

    def getScripts(self, txt):
        scripts = set()
        idx = 0
        txt_len = len(txt)
        while idx < txt_len:
            for block_start, block_end, script in self.code_block2scripts:
                while idx < txt_len and block_start <= ord(txt[idx]) <= block_end:
                    if txt[idx] != ' ':
                        scripts.add(script)
                    idx += 1
        return list(scripts)

class FontSwitcher(object):

    def __init__(self, char_blacklist_file=None):
        self.scripts = Scripts()
        self.default_font = None
        self.code_points2font = []

        self.space_like_chars = [i for i in range(33) if not i in [9, 10, 13]] + [127]
        self.remove_chars = [173] # 173 = softhyphen
        self.ignore_chars = [8206, # left to right mark
                           8207, # right to left mark
                           ]
        self.no_switch_chars = self.space_like_chars + self.ignore_chars + self.remove_chars
        self.char_blacklist = self.readCharBlacklist(char_blacklist_file)

        self.cjk_fonts = [] # list of font names for cjk scripts
        self.space_cjk = False # when switching fonts, indicate that cjk text is present

    def readCharBlacklist(self, char_blacklist_file):
        if not char_blacklist_file:
            return {}
        char_blacklist = {}
        for char in open(char_blacklist_file).readlines():
            if char:
                char = int(char.strip())
                char_blacklist[char]=True
        return char_blacklist

    def unregisterFont(self, unreg_font_name):
        registered_entries = []
        i = 0
        for block_start, block_end, font_name in self.code_points2font:
            if unreg_font_name == font_name:
                registered_entries.append(i)
            i += 1
        registered_entries.reverse()
        for entry in registered_entries:
            self.code_points2font.pop(entry)


    def registerFont(self, font_name, code_points=[]):
        """ Register a font to certain scripts or a range of code point blocks"""
        if not code_points:
            return
        for block in code_points:
            if isinstance(block, basestring):
                block = self.scripts.getCodePoints(block)
            block_start, block_end = block
            self.code_points2font.insert(0, (block_start, block_end, font_name))

    def registerDefaultFont(self, font_name=None):
        self.default_font = font_name

    def getFont(self, ord_char):
        for block_start, block_end, font_name in self.code_points2font:
            if block_start <= ord_char <= block_end:
                return font_name
        return self.default_font

    def getFontList(self, txt, spaces_to_default=False):
        txt_list = []
        last_font = None
        last_txt = []
        for c in txt:
            ord_c = ord(c)
            blacklisted = self.char_blacklist.get(ord_c, False)
            if ord_c in self.no_switch_chars or blacklisted:
                if ord_c in self.remove_chars:
                    c = ''
                if ord_c in self.space_like_chars:
                    c = ' '
                if blacklisted:
                    c = unichr(9633) # U+25A1 WHITE SQUARE
                if last_font:
                    font = last_font
                else:
                    font = self.default_font
            else:
                font = self.getFont(ord_c)
            if font != last_font and last_txt:
                txt_list.append((''.join(last_txt), last_font))
                last_txt = []
            last_txt.append(c)
            last_font = font
        if last_txt:
            txt_list.append((''.join(last_txt), last_font))


        if spaces_to_default:
            new_txt_list = []
            for txt, font in txt_list:
                if font!= self.default_font:
                    if txt.startswith(' '):
                        new_txt_list.append((' ', self.default_font))
                        new_txt_list.append((txt[1:], font))
                    elif txt.endswith(' '):
                        new_txt_list.append((txt[:-1], font))
                        new_txt_list.append((' ', self.default_font))
                    else:
                        new_txt_list.append((txt, font))
                else:
                    new_txt_list.append((txt, font))
            txt_list = new_txt_list

        if self.space_cjk:
            for txt, font in txt_list:
                if font in self.cjk_fonts:
                    return (txt_list, True)
            return (txt_list, False)
        return txt_list


if __name__ == '__main__':
    _scripts = Scripts()

    blocks = [
        (9472, 9580),
        #(4352, 4607),
        #(12592, 12687),
        ]

    scripts = _scripts.getScriptsForCodeBlocks(blocks)
    print scripts
    for script in scripts:
        print _scripts.getCodePoints(script)

########NEW FILE########
__FILENAME__ = formatter
#! /usr/bin/env python
#! -*- coding:utf-8 -*-

# Copyright (c) 2007, PediaPress GmbH
# See README.rst for additional licensing information.

from __future__ import division

from mwlib.writer import styleutils
from mwlib import advtree

class Formatter(object):
    """store the current formatting state"""


    css_style_map = {'font-style': {'italic': [('emphasized_style', 'change')],
                                    'oblique': [('emphasized_style', 'change')],
                                    'normal': [('emphasized_style', 'reset')]
                                    },
                     'font-family': {'Courier':[('teletype_style', 'change')],
                                     },
                     'font-weight': {'bold': [('strong_style', 'change')],
                                     'bolder': [('strong_style', 'change')], # bolder and bold are treated the same
                                     'normal': [('strong_style', 'reset')],
                                     'lighter': [('strong_style', 'reset')], # treat lighter as normal
                                     },
                     'text-decoration': {'overline': [('overline_style', 'change'),
                                                      ('underline_style', 'reset'),
                                                      ('strike_style', 'reset'),
                                                      ],
                                         'underline': [('underline_style', 'change'),
                                                       ('overline_style', 'reset'),
                                                       ('strike_style', 'reset'),
                                                       ],
                                         'line-through': [('strike_style', 'change'),
                                                          ('underline_style', 'reset'),
                                                          ('overline_style', 'reset'),
                                                          ],
                                         },
                     'color': {'*':('color_style', styleutils.rgbColorFromNode)
                               },
                     }


    def __init__(self, font_switcher=None, output_encoding=None, word_split_len=20):

        self.font_switcher = font_switcher

        self.default_font = 'DejaVuSerif'
        self.default_mono_font = 'DejaVuSansMono'

        self.output_encoding = output_encoding

        self.render_styles = self.registerRenderStyles()
        self.node_styles = self.registerNodeStyles()
        
        for style, start_style, end_style, start_attr in self.render_styles:
            setattr(self, style, 0)
       
        self.source_mode = 0 
        self.pre_mode = 0
        self.index_mode = 0
        self.gallery_mode = 0
        self.footnote_mode = 0
        self.minimize_space_mode = 0 # used for tables if we try to safe space
        
        self.sectiontitle_mode = False
        self.attribution_mode = True
        self.last_font = None
        self.table_nesting = 0
        self.rel_font_size = 1

        self.grouping_chars = ('', '')
        self.word_split_len = word_split_len

    def registerRenderStyles(self):
        # example for render styles in html. should probably be overridden when subclassed
        return [
            ('emphasized_style', '<em>', '</em>'),
            ('strong_style', '<strong>', '</strong>'),
            ('small_style', '<small>', '</small>'),
            ('big_style', '<big>', '</big>'),
            ('sub_style', '<sub>', '</sub>'),
            ('sup_style', '<sup>','</sup>'),
            ('teletype_style', '<tt>', '</tt>'),
            ('strike_style', '<strike>', '</strike>'),
            ('underline_style', '<u>', '</u>'),
            ('overline_style', '', '')
            ]

    def registerNodeStyles(self):
        return {
            advtree.Emphasized:'emphasized_style',
            advtree.Strong: 'strong_style',
            advtree.Small: 'small_style',
            advtree.Big: 'big_style',
            advtree.Sub: 'sub_style',
            advtree.Sup: 'sup_style',
            advtree.Teletyped: 'teletype_style',
            advtree.Code: 'teletype_style',
            advtree.Var: 'teletype_style',
            advtree.Strike: 'strike_style',
            advtree.Underline: 'underline_style',
            advtree.Overline: 'overline_style',
            }

    def startStyle(self):
        start = []
        for style, style_start, style_end, start_arg in self.render_styles:
            if getattr(self, style, 0) > 0:
                if start_arg:
                    start.append(style_start % getattr(self, start_arg))
                else:
                    start.append(style_start)
        if start:
            start.insert(0, self.grouping_chars[0])
        return ''.join(start)

    def endStyle(self):
        end = []
        for style, style_start, style_end, start_arg in self.render_styles[::-1]: # reverse style list
            if getattr(self, style, 0) > 0:
                end.append(style_end)
        if end:
            end.append(self.grouping_chars[1])
        return ''.join(end)


    def setRelativeFontSize(self, rel_font_size):
        # ignore anything too large. see search engine optimized article
        # http://fr.wikipedia.org/wiki/Licensed_to_Ill (template "Infobox Musique (œuvre)")
        if rel_font_size > 10:
            return
        rel_font_size = min(rel_font_size, 5)
        self.fontsize_style += 1
        self.rel_font_size = rel_font_size
    
    def checkFontSize(self, node_style):
        font_style = node_style.get('font-size')
        if not font_style:
            return
                
        size, unit = styleutils.parseLength(font_style)
        if size and unit in ['%', 'pt', 'px', 'em']:
            if unit == '%':
                self.setRelativeFontSize(size/100)
            elif unit ==  'pt':
                self.setRelativeFontSize(size/10)
            elif unit ==  'px':
                self.setRelativeFontSize(size/12)
            elif unit ==  'em':
                self.setRelativeFontSize(size)
            return

        if font_style == 'xx-small':
            self.setRelativeFontSize(0.5)
        elif font_style == 'x-small':
            self.setRelativeFontSize(0.75)
        elif font_style == 'small':
            self.setRelativeFontSize(1.0)
        elif font_style == 'medium':
            self.setRelativeFontSize(1.25)
        elif font_style == 'large':
            self.setRelativeFontSize(1.5)
        elif font_style in ['x-large', 'xx-large']:
            self.setRelativeFontSize(1.75)
                           
    
    def changeCssStyle(self, node, mode=None):        
        css = self.css_style_map
        for node_style, style_value in node.style.items():
            if node_style in css:
                for render_style, action in css[node_style].get(style_value, []):
                    if action == 'change':
                        setattr(self, render_style, getattr(self, render_style) + 1)
                    elif action == 'reset':
                        setattr(self, render_style, 0)
                if css[node_style].keys() == ['*']:
                    attr_name, method = css[node_style]['*']
                    val = method(node)
                    if val:
                        setattr(self, attr_name, val)

        self.checkFontSize(node.style)

    def changeNodeStyle(self, node):
        style = self.node_styles.get(node.__class__)
        if style:
            setattr(self, style, getattr(self, style) + 1)

    def getCurrentStyles(self):
        styles = []
        for style, start, end, start_attr in self.render_styles:
            styles.append((style, getattr(self, style)))
        styles.append(('rel_font_size', self.rel_font_size))
        return styles
    
    def setStyle(self, node):
        current_styles = self.getCurrentStyles()
        self.changeNodeStyle(node)
        self.changeCssStyle(node, mode='set')
        return current_styles
            
    def resetStyle(self, styles):
        for attr, val in styles:
            setattr(self, attr, val)

    def clearStyles(self, styles):
        for attr, val in styles:
            if attr == 'rel_font_size':
                setattr(self, attr, 1)
            else:
                setattr(self, attr, 0)

    def cleanText(self, txt, break_long=False, escape=True):
        if not txt:
            return ''

        if self.pre_mode:
            txt = self.escapeText(txt)
            txt = self.pre_mode_hook(txt)
            txt = self.font_switcher.fontifyText(txt)
        elif self.source_mode:
            pass
        else:
            if escape:
                if self.minimize_space_mode > 0 or (break_long and max(len(w) for w in txt.split(' ')) > self.word_split_len):
                    txt = self.escapeAndHyphenateText(txt)
                else:
                    txt = self.escapeText(txt)
            txt = self.font_switcher.fontifyText(txt, break_long=break_long)

        if self.sectiontitle_mode:
            txt = txt.lstrip()
            self.sectiontitle_mode = False
            
        if self.table_nesting > 0 and not self.source_mode and not self.pre_mode:
            txt = self.table_mode_hook(txt)        

        if self.output_encoding:
            txt = txt.encode(self.output_encoding)
        return txt

    def styleText(self, txt, break_long=False):
        if not txt.strip():
            if self.output_encoding:
                txt = txt.encode(self.output_encoding)
            return txt
        styled = []
        styled.append(self.startStyle())
        styled.append(self.cleanText(txt, break_long=break_long))
        styled.append(self.endStyle())
        return ''.join(styled)


    def switchFont(self, font):
        self.last_font = self.default_font
        self.default_font = font

    def restoreFont(self):
        self.default_font = self.last_font


    ### the methods below are the ones that should probably be overriden when subclassing the formatter

    def pre_mode_hook(self, txt):
        return txt

    def table_mode_hook(self, txt):
        return txt

    def escapeText(self, txt): 
        return txt

    def escapeAndHyphenateText(self, txt):
        return txt

########NEW FILE########
__FILENAME__ = imageutils
#! /usr/bin/env python
#! -*- coding:utf-8 -*-

# Copyright (c) 2007, PediaPress GmbH
# See README.rst for additional licensing information.

from __future__ import division


from PIL import Image

class ImageUtils(object):

    def __init__(self, print_width,
                 print_height,
                 default_thumb_width,
                 img_min_res,
                 img_max_thumb_width,
                 img_max_thumb_height,
                 img_inline_scale_factor,
                 print_width_px
                 ):
        
        self.print_width = print_width
        self.print_height = print_height        
        self.default_thumb_width = default_thumb_width
        self.img_min_res = img_min_res
        self.img_max_thumb_width = img_max_thumb_width
        self.img_max_thumb_height = img_max_thumb_height
        self.img_inline_scale_factor = img_inline_scale_factor
        self.print_width_px = print_width_px
        
    def getImageSize(self, img_node, img_path=None,
                     max_print_width=None, max_print_height=None,
                     fullsize_thumbs=False, img_size=None):
        max_w = getattr(img_node, 'width', None)
        max_h = getattr(img_node, 'height', None)
        if img_path:
            try:
                img = Image.open(img_path)
            except IOError:  # img either missing or corrupt
                return (0, 0)
            px_w, px_h = img.size
        else:
            px_w,  px_h = img_size
            
        ar = px_w/px_h
        if max_h and max_w:
            if max_h*ar > max_w:
                max_h = max_w/ar
            elif max_w/ar > max_h:
                max_w = max_h*ar
        if max_h and not max_w:
            max_w = max_h*ar

        # check if thumb, then assign default width
        if getattr(img_node, 'thumb', None) or getattr(img_node, 'frame', None) or getattr(img_node, 'frameless', None) or getattr(img_node, 'align', None) in ['right', 'left']:
            max_w = max_w or self.default_thumb_width
            if fullsize_thumbs:
                max_w = self.print_width_px
            if getattr(img_node, 'align', None) not in ['center', 'none']:
                img_node.floating = True
            if getattr(img_node, 'upright', 1):
                max_w *= getattr(img_node, 'upright', 1)
        if not max_w:
            max_w = min(self.print_width_px, px_w)
        max_w = min(self.print_width_px, max_w)
        scale = max_w / self.print_width_px
        img_print_width = self.print_width*scale

        if max_print_width and img_print_width > max_print_width:
            img_print_width = max_print_width

        if max_print_height:
            img_print_width = min(img_print_width, max_print_height*ar)

        # check min resolution
        resulting_dpi = px_w / img_print_width * 72
        if resulting_dpi < self.img_min_res:
            img_print_width = (resulting_dpi/self.img_min_res)*img_print_width

        # check size limits for floating images
        if getattr(img_node, 'floating', False):
            img_print_width = min(img_print_width, self.print_width*self.img_max_thumb_width, self.print_height*self.img_max_thumb_height*ar)

        if img_node.isInline():
            if img_print_width < self.print_width/2: # scale "small" inline images
                img_print_width *= self.img_inline_scale_factor
            else: # FIXME: full width images are 12pt too wide - we need to check why
                img_print_width -= 12

        img_print_width = min(img_print_width, self.print_width, self.print_height*ar*0.9)
        img_print_height = img_print_width/ar
        return (img_print_width, img_print_height)

########NEW FILE########
__FILENAME__ = licensechecker
#! /usr/bin/env python
#! -*- coding:utf-8 -*-

# Copyright (c) 2007, PediaPress GmbH
# See README.rst for additional licensing information.

from __future__ import division

import os
import csv
import sys
import tempfile
try:
    import simplejson as json
except ImportError:
    import json


class License(object):

    def __init__(self, name='', display_name='', license_type=None, description=''):
        self.name = name
        self.display_name = display_name
        self.license_type = license_type # free|nonfree|unrelated|unknown
        self.description = description

    def __str__(self):
        if self.display_name:
            display_name = ' - text: %s' % self.display_name
        else:
            display_name = ''
        return "<License:%(name)r - type:%(type)r%(displayname)r>" % { 'name': self.name,
                                                                       'type': self.license_type,
                                                                       'displayname': display_name,
                                                                       }
    __repr__ = __str__

    def __cmp__(self, other):
        if self.display_name < other.display_name:
            return -1
        elif self.display_name == other.display_name:
            return 0
        else:
            return 1

class LicenseChecker(object):

    def __init__(self, image_db=None, filter_type=None):
        self.image_db = image_db
        self.filter_type = self._checkFilterType(filter_type, default_filter='blacklist')
        self.licenses = {}
        self.initStats()
        self.display_cache = {}

    def readLicensesCSV(self, fn=None):
        if not fn:
            fn = os.path.join(os.path.dirname(__file__), 'wplicenses.csv')
        for data in csv.reader(open(fn)):
            try:
                (name, display_name, license_type, dummy, license_description) = data
            except ValueError:
                continue
            if not name:
                continue
            name = unicode(name, 'utf-8').lower()
            lic = License(name=name)
            lic.display_name = unicode(display_name, 'utf-8')
            if license_description.startswith('-'):
                license_description = license_description[1:]
            lic.description = unicode(license_description.strip(), 'utf-8') 
            if license_type in ['free-display', 'nonfree-display']:
                lic.license_type = 'free'
            elif license_type in ['nonfree']:
                lic.license_type = 'nonfree'                
            else:
                lic.license_type = 'unrelated'
            self.licenses[name] = lic

    def initStats(self):
        self.unknown_licenses = {}
        self.rejected_images = set()
        self.accepted_images = set()
        self.license_display_name = {}

    def _checkFilterType(self, filter_type=None, default_filter='blacklist'):
        if filter_type in ['blacklist', 'whitelist', 'nofilter']:
            return filter_type
        else:
            return default_filter

    def _getLicenses(self, templates, imgname):
        licenses = []
        for template in templates:
            assert isinstance(template, unicode)
            lic = self.licenses.get(template, None)
            if not lic:
                lic = License(name=template)
                lic.license_type = 'unknown'
            licenses.append(lic)
        return licenses


    def _checkLicenses(self, licenses, imgname, stats=True):
        assert self.image_db, 'No image_db passed when initializing LicenseChecker'
        for lic in licenses:
            if lic.license_type == 'free':
                self.license_display_name[imgname] = lic.display_name
                return True
            elif lic.license_type == 'nonfree':
                self.license_display_name[imgname] = lic.display_name
                return True if self.filter_type == 'nofilter' else False
        for lic in licenses:
            if lic.license_type == 'unknown' and stats:
                urls = self.unknown_licenses.get(lic.name, set())
                urls.add(self.image_db.getDescriptionURL(imgname) or self.image_db.getURL(imgname) or imgname)
                self.unknown_licenses[lic.name] = urls

        self.license_display_name[imgname] = ''
        if self.filter_type == 'whitelist':
            return False
        elif self.filter_type in ['blacklist', 'nofilter']:
            return True


    def displayImage(self, imgname):
        if imgname in self.display_cache:
            return self.display_cache[imgname]
        if self.image_db is None:
            return False
        templates = [t.lower() for t in self.image_db.getImageTemplatesAndArgs(imgname)]
        licenses = self._getLicenses(templates, imgname)
        display_img = self._checkLicenses(licenses, imgname)
        url = self.image_db.getDescriptionURL(imgname) or self.image_db.getURL(imgname) or imgname
        if display_img:
            self.accepted_images.add(url)
        else:
            self.rejected_images.add(url)
        self.display_cache[imgname] = display_img
        return display_img


    def getLicenseDisplayName(self, imgname):
        text = self.license_display_name.get(imgname, None)
        if not text == None:
            return text
        else:
            self.displayImage(imgname)
            return self.license_display_name.get(imgname, '')

    @property
    def free_img_ratio(self):
        r = len(self.rejected_images)
        a = len(self.accepted_images)
        if a + r > 0:
            ratio = a/(a+r)
        else:
            ratio = 1
        return ratio

    def dumpStats(self):
        stats = []
        stats.append('IMAGE LICENSE STATS - accepted: %d - rejected: %d --> accept ratio: %.2f' % (len(self.accepted_images), len(self.rejected_images), self.free_img_ratio))

        images = set()
        for urls in self.unknown_licenses.values():
            for url in urls:
                images.add(repr(url))        
        stats.append('Images without license information: %s' % (' '.join(list(images))))
        stats.append('##############################')
        stats.append('Rejected Images: %s' % ' '.join(list(self.rejected_images)))
        return '\n'.join(stats)

    def dumpUnknownLicenses(self, _dir):
        if not self.unknown_licenses:
            return
        fn = tempfile.mktemp(dir=_dir, prefix='licensestats_',  suffix='.json')
        f = open(fn, 'w')
        unknown_licenses = {}
        for (license, urls) in self.unknown_licenses.items():
            unknown_licenses[license] = list(urls)
        f.write(json.dumps(unknown_licenses))
        f.close()


    def analyseUnknownLicenses(self, _dir):
        files = os.listdir(_dir)
        unknown_licenses = {}
        for fn in files:
            fn = os.path.join(_dir, fn)
            if not fn.endswith('json'):
                continue
            content = unicode(open(fn).read(), 'utf-8')
            try:
                licenses = json.loads(content)
            except ValueError:
                print 'no json object found in file', fn
                continue
            for (license, urls) in licenses.items():
                if not self.licenses.get(license, False):
                    seen_urls = unknown_licenses.get(license, set())
                    seen_urls.update(set(urls))
                    unknown_licenses[license] = seen_urls
        sorted_licenses = [ (len(urls), license, urls) for license, urls in unknown_licenses.items()]
        sorted_licenses.sort(reverse=True)
        for num_urls, license, urls in sorted_licenses:
            args = { 'template': license.encode('utf-8'),
                     'num_images': num_urls,
                     'img_str': '\n'.join([i.encode('utf-8') for i in list(urls)[:5]])
                     }
            print "\nTEMPLATE: %(template)s (num rejected images: %(num_images)d)\nIMAGES:\n%(img_str)s\n" % args


    def dumpLicenseInfoContent(self):

        licenses = [v for v in self.licenses.values()]
        licenses.sort()

        tmpl_txt = """
{{/ImageLicenseItem
|template_name=%(lic_name)s
|license=%(display_name)s
|display_allowed=%(allowed)s
|license_text_url=
|full_text_required=
|description=%(description)s
}}"""        
        
        #for templ, lic in self.licenses.items():
        for lic in licenses:
            if lic.license_type in ['free', 'nonfree']:
                #print "{{/ImageLicenseItem|template_name=%(lic_name)s|license=%(display_name)s|display_allowed=%(allowed)s|description=%(description)s}}" % {
                if lic.license_type=='free':
                    allowedstr = "yes"
                else:
                    allowedstr = "no"
                    
                print tmpl_txt % {
                    'lic_name': lic.name.encode('utf-8'),
                    'display_name': lic.display_name.encode('utf-8'),
                    'allowed': allowedstr,
                    'description': lic.description.encode('utf-8'), 
                    }
                #print lic.name, lic.display_name, lic.license_type
        

                    
if __name__ == '__main__':

    lc = LicenseChecker()
    lc.readLicensesCSV()

    # lc.dumpLicenseInfoContent()
    # sys.exit(1)


    if len(sys.argv) > 1:
        stats_dir = sys.argv[1]
    else:
        stats_dir = os.environ.get('HIQ_STATSDIR')
    if not stats_dir:
        print 'specify stats_dir as first arg, or set environment var HIQ_STATSIDR'
        sys.exit(1)

    
    lc.analyseUnknownLicenses(stats_dir)

########NEW FILE########
__FILENAME__ = miscutils
#! /usr/bin/env python
#! -*- coding:utf-8 -*-

# Copyright (c) 2007, 2008, 2009 PediaPress GmbH
# See README.rst for additional licensing information.

from __future__ import division

from mwlib import advtree

def hasInfoboxAttrs(node):
    infobox_classIDs = ['infobox', 'taxobox']
    if node.hasClassID(infobox_classIDs):
        return True
    if node.attributes.get('summary', '').lower() in infobox_classIDs:
        return True    
    return False

def textInNode(node):
    amap = {advtree.Text:"caption", advtree.Link:"target", advtree.URL:"caption", advtree.Math:"caption", advtree.ImageLink:"caption" }
    access = amap.get(node.__class__, "")
    if access:
        txt = getattr(node, access)
        if txt:
            return len(txt)
        else:
            return 0
    else:
        return 0

def textBeforeInfoBox(node, infobox, txt_list=[]):
    txt_list.append((textInNode(node), node==infobox))        
    for c in node:
        textBeforeInfoBox(c, infobox, txt_list)
    sum_txt = 0    
    for len_txt, is_infobox in txt_list:
        sum_txt += len_txt
        if is_infobox:
            return sum_txt
    return sum_txt

def articleStartsWithInfobox(article_node, max_text_until_infobox=0):
    assert article_node.__class__ == advtree.Article, 'articleStartsWithInfobox needs to be called with Article node'
    infobox = None
    for table in article_node.getChildNodesByClass(advtree.Table):
        if hasInfoboxAttrs(table):
            infobox = table
    if not infobox:
        return False
    return textBeforeInfoBox(article_node, infobox, []) <= max_text_until_infobox


def articleStartsWithTable(article_node, max_text_until_infobox=0):
    assert article_node.__class__ == advtree.Article, 'articleStartsWithInfobox needs to be called with Article node'
    tables = article_node.getChildNodesByClass(advtree.Table)
    if not tables:
        return False
    return textBeforeInfoBox(article_node, tables[0], []) <= max_text_until_infobox

    

########NEW FILE########
__FILENAME__ = styleutils
#! /usr/bin/env python
#! -*- coding:utf-8 -*-

# Copyright (c) 2007, PediaPress GmbH
# See README.rst for additional licensing information.

from __future__ import division
import re

from mwlib import advtree
from mwlib.htmlcolornames import colorname2rgb_map

def _colorFromStr(colorStr):

    def hex2rgb(r, g, b):
        try:
            conv = lambda c: max(0, min(1.0, int(c, 16) / 255))
            return (conv(r), conv(g), conv(b))
        except ValueError:
            return None
    def hexshort2rgb(r, g, b):
        try:
            conv = lambda c: max(0, min(1.0, int(2*c, 16) / 255))
            return (conv(r), conv(g), conv(b))
        except:
            return None
    def rgb2rgb(r, g, b):
        try:
            conv = lambda c: max(0, min(1.0, int(c) / 255))
            return (conv(r), conv(g), conv(b))
        except ValueError:
            return None
    def colorname2rgb(colorStr):
        rgb = colorname2rgb_map.get(colorStr.lower(), None)
        if rgb:
            return tuple(max(0, min(1, channel/255)) for channel in rgb)
        else:
            return None
                   
    try:
        colorStr = str(colorStr)
    except:
        return None
    rgbval = re.search('rgb\( *(\d{1,}) *, *(\d{1,3}) *, *(\d{1,3}) *\)', colorStr)          
    hexval = re.search('#?([0-9a-f]{2})([0-9a-f]{2})([0-9a-f]{2})', colorStr)
    hexvalshort = re.search('#([0-9a-f])([0-9a-f])([0-9a-f])', colorStr)
    if rgbval:
        return rgb2rgb(rgbval.group(1), rgbval.group(2), rgbval.group(3))
    elif hexval:
        return hex2rgb(hexval.group(1), hexval.group(2), hexval.group(3))
    elif hexvalshort:
        return hexshort2rgb(hexvalshort.group(1), hexvalshort.group(2), hexvalshort.group(3))
    else:
        return colorname2rgb(colorStr)
    return None


def _rgb2GreyScale(rgb_triple, darknessLimit=1):
    grey = min(1, max(darknessLimit, 0.3*rgb_triple[0] + 0.59*rgb_triple[1] + 0.11*rgb_triple[2] ))
    return (grey, grey, grey)

def rgbBgColorFromNode(node, greyScale=False, darknessLimit=0, follow=True):
    """Extract background color from node attributes/style. Result is a rgb triple w/ individual values between [0...1]

    The darknessLimit parameter is only used when greyScale is requested. This is for b/w output formats that do not
    switch text-color.
    """

    colorStr = node.attributes.get('bgcolor', None) or \
               node.style.get('background') or \
               node.style.get('background-color')
            
    color = None
    if colorStr:
        color = _colorFromStr(colorStr.lower())
        if greyScale and color:
            return _rgb2GreyScale(color, darknessLimit)
    elif node.parent and follow:
        return rgbBgColorFromNode(node.parent, greyScale=greyScale, darknessLimit=darknessLimit)
    return color

def rgbColorFromNode(node, greyScale=False, darknessLimit=0):
    """Extract text color from node attributes/style. Result is a rgb triple w/ individual values between [0...1]"""

    colorStr = node.style.get('color', None) or \
               node.attributes.get('color', None)
    color = None
    if colorStr:
        color = _colorFromStr(colorStr.lower())
        if greyScale and color:
            return _rgb2GreyScale(color, darknessLimit)
    elif node.parent:
        return rgbColorFromNode(node.parent, greyScale=greyScale, darknessLimit=darknessLimit)
    return color


def getBaseAlign(node):
    if node.__class__ == advtree.Cell and getattr(node, 'is_header', False):
        return 'center'
    return 'none'


def _getTextAlign(node):
    align = node.style.get('text-align', 'none').lower()
    if align == 'none' and node.__class__ in  [advtree.Div, advtree.Cell, advtree.Row]:
        align = node.attributes.get('align', 'none').lower()
    if align not in ['left', 'center', 'right', 'justify', 'none']:
        return 'left'
    if node.__class__ == advtree.Center:
        align = 'center'
    if align == "none" and node.parent:
        return _getTextAlign(node.parent)
    return align


def getTextAlign(node):
    """ return the text alignment of a node. possible return values are left|right|center|none"""
    nodes = node.getParents()
    nodes.append(node)
    align = getBaseAlign(node)
    for n in nodes:
        parent_align = _getTextAlign(n)
        if parent_align != 'none':
            align = parent_align
    return align

def getVerticalAlign(node):
    align = None
    for n in node.parents + [node]:
        _align = n.style.get('vertical-align', None) or n.vlist.get('valign', None)
        if _align in ['top', 'middle', 'bottom']:
            align = _align
    return align or 'top'

def tableBorder(node):
    borderBoxes = ['prettytable',
                   'metadata',
                   'wikitable',
                   'infobox',
                   'ujinfobox', # used in hu.wikipedia
                   'infobox_v2', # used in fr.wikipedia
                   'infoboks',
                   'toccolours',
                   'navbox',
                   'float-right',
                   'taxobox',
                   'info',
                   'collapsibleTable0',
                   'palaeobox',
                   ]
    noBorderBoxes = ['cquote',
                     ]
    attributes = node.attributes
    style = attributes.get('style', {})

    classes = set([ c.strip() for c in attributes.get('class','').split()])
    if set(borderBoxes).intersection(classes):
        return True
    if set(noBorderBoxes).intersection(classes):
        return False
    if style.get('border-style', None) == 'none':
        return False
    if attributes.get('border', "0") != "0" or \
           style.get('border', "0") != "0" or \
           style.get('border-style', "none") != 'none' or \
           style.get('border-width', "0") != "0":
        return True

    bgColor = attributes.get('background-color') or style.get('background-color')
    if bgColor and bgColor!= 'transparent':
        return True # FIXME this is probably not very accurate

    bs = attributes.get('border-spacing',None)
    if bs:
        bs_val = re.match('(?P<bs>\d)',bs)
        if bs_val and int(bs_val.groups('bs')[0]) > 0:
            return True

    return False

def parseLength(txt):
    length_res = re.search(r'(?P<val>.*?)(?P<unit>(pt|px|em|%))', txt)
    length = unit = None
    if length_res:
        unit = length_res.group('unit')
        try:
            length = float(length_res.group('val'))
        except ValueError:
            length = None
    return (length, unit)

mw_px2pt = 12/16
mw_em2pt = 9.6
            
def scaleLength(length_str, reference=None):
    length, unit = parseLength(length_str)
    if not length:
        return 0
    if unit == 'pt':
        return length
    elif unit == 'px':
        return length * mw_px2pt
    elif unit == 'em':
        return length * mw_em2pt
    elif unit == '%' and reference:
        return length/100 * reference
    return 0

########NEW FILE########
__FILENAME__ = writerbase
#! /usr/bin/env python

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import urllib

from mwlib import parser, log, metabook, wiki

# import functions needed by most writers that should be accessible through writerbase
from mwlib.mathutils import renderMath

log = log.Log('mwlib.writerbase')

class WriterError(RuntimeError):
    pass

def build_book(env, status_callback=None):
    book = parser.Book()
    progress = 0
    if status_callback is None:
        status_callback = lambda **kwargs: None
        
    num_articles = float(len(env.metabook.articles()))
    if num_articles > 0:
        progress_step = 100/num_articles
        
    lastChapter = None
    for item in env.metabook.walk():
        if item.type == 'chapter':
            chapter = parser.Chapter(item.title.strip())
            book.appendChild(chapter)
            lastChapter = chapter
        elif item.type == 'article':
            status_callback(status='parsing', progress=progress, article=item.title)
            progress += progress_step

            if item._env:
                wiki = item._env.wiki
            else:
                wiki = env.wiki
            
            a = wiki.getParsedArticle(title=item.title, revision=item.revision)
            
            if a is not None:
                if item.displaytitle is not None:
                    a.caption = item.displaytitle
                url = wiki.getURL(item.title, item.revision)                
                if url:
                    a.url = url
                else:
                    a.url = None
                source = wiki.getSource(item.title, item.revision)
                if source:
                    a.wikiurl = source.url
                else:
                    a.wikiurl = None
                    
                a.authors = wiki.getAuthors(item.title, revision=item.revision)
                if lastChapter:
                    lastChapter.appendChild(a)
                else:
                    book.appendChild(a)
            else:
                log.warn('No such article: %r' % item.title)

    status_callback(status='parsing', progress=progress, article='')
    return book

########NEW FILE########
__FILENAME__ = _conf
import types, os, py


def as_bool(val):
    val = val.lower()
    if val in ("yes", "true", "on", "1"):
        return True
    if val in ("no", "false", "off", "0"):
        return False

    return False


class confbase(object):
    def __init__(self):
        self._inicfg = None

    def readrc(self, path=None):
        if path is None:
            path = os.path.expanduser("~/.mwlibrc")
            if not os.path.exists(path):
                return
        self._inicfg = py.iniconfig.IniConfig(path, None)

    def get(self, section, name, default=None, convert=str):
        if section == "mwlib":
            varname = "MWLIB_%s" % name.upper()
        else:
            varname = "MWLIB_%s_%s" % (section.upper(), name.upper())

        if varname in os.environ:
            return convert(os.environ[varname])
        if self._inicfg is not None:
            return self._inicfg.get(section, name, default=default, convert=convert)
        return default

    @property
    def noedits(self):
        return self.get("fetch", "noedits", False, as_bool)

    @property
    def user_agent(self):
        from mwlib._version import version
        return self.get("mwlib", "user_agent", "") or ("mwlib %s" % version)


class confmod(confbase, types.ModuleType):
    def __init__(self, *args, **kwargs):
        confbase.__init__(self)
        types.ModuleType.__init__(self, *args, **kwargs)
        self.__file__ = __file__

########NEW FILE########
__FILENAME__ = _locale
import locale

_supported = ['aa_DJ', 'aa_DJ.UTF-8', 'aa_ER', 'aa_ER@saaho', 'aa_ET',
'af_ZA', 'af_ZA.UTF-8', 'am_ET', 'an_ES', 'an_ES.UTF-8', 'ar_AE',
'ar_AE.UTF-8', 'ar_BH', 'ar_BH.UTF-8', 'ar_DZ', 'ar_DZ.UTF-8',
'ar_EG', 'ar_EG.UTF-8', 'ar_IN', 'ar_IQ', 'ar_IQ.UTF-8', 'ar_JO',
'ar_JO.UTF-8', 'ar_KW', 'ar_KW.UTF-8', 'ar_LB', 'ar_LB.UTF-8',
'ar_LY', 'ar_LY.UTF-8', 'ar_MA', 'ar_MA.UTF-8', 'ar_OM',
'ar_OM.UTF-8', 'ar_QA', 'ar_QA.UTF-8', 'ar_SA', 'ar_SA.UTF-8',
'ar_SD', 'ar_SD.UTF-8', 'ar_SY', 'ar_SY.UTF-8', 'ar_TN',
'ar_TN.UTF-8', 'ar_YE', 'ar_YE.UTF-8', 'as_IN', 'ast_ES',
'ast_ES.UTF-8', 'az_AZ', 'be_BY', 'be_BY.UTF-8', 'be_BY@latin',
'bem_ZM', 'ber_DZ', 'ber_MA', 'bg_BG', 'bg_BG.UTF-8', 'bho_IN',
'bn_BD', 'bn_IN', 'bo_CN', 'bo_IN', 'br_FR', 'br_FR.UTF-8',
'br_FR@euro', 'brx_IN', 'bs_BA', 'bs_BA.UTF-8', 'byn_ER', 'ca_AD',
'ca_AD.UTF-8', 'ca_ES', 'ca_ES.UTF-8', 'ca_ES@euro', 'ca_FR',
'ca_FR.UTF-8', 'ca_IT', 'ca_IT.UTF-8', 'crh_UA', 'cs_CZ',
'cs_CZ.UTF-8', 'csb_PL', 'cv_RU', 'cy_GB', 'cy_GB.UTF-8', 'da_DK',
'da_DK.UTF-8', 'de_AT', 'de_AT.UTF-8', 'de_AT@euro', 'de_BE',
'de_BE.UTF-8', 'de_BE@euro', 'de_CH', 'de_CH.UTF-8', 'de_DE',
'de_DE.UTF-8', 'de_DE@euro', 'de_LU', 'de_LU.UTF-8', 'de_LU@euro',
'dv_MV', 'dz_BT', 'el_CY', 'el_CY.UTF-8', 'el_GR', 'el_GR.UTF-8',
'en_AG', 'en_AU', 'en_AU.UTF-8', 'en_BW', 'en_BW.UTF-8', 'en_CA',
'en_CA.UTF-8', 'en_DK', 'en_DK.UTF-8', 'en_GB', 'en_GB.UTF-8',
'en_HK', 'en_HK.UTF-8', 'en_IE', 'en_IE.UTF-8', 'en_IE@euro', 'en_IN',
'en_NG', 'en_NZ', 'en_NZ.UTF-8', 'en_PH', 'en_PH.UTF-8', 'en_SG',
'en_SG.UTF-8', 'en_US', 'en_US.UTF-8', 'en_ZA', 'en_ZA.UTF-8',
'en_ZM', 'en_ZW', 'en_ZW.UTF-8', 'es_AR', 'es_AR.UTF-8', 'es_BO',
'es_BO.UTF-8', 'es_CL', 'es_CL.UTF-8', 'es_CO', 'es_CO.UTF-8',
'es_CR', 'es_CR.UTF-8', 'es_CU', 'es_DO', 'es_DO.UTF-8', 'es_EC',
'es_EC.UTF-8', 'es_ES', 'es_ES.UTF-8', 'es_ES@euro', 'es_GT',
'es_GT.UTF-8', 'es_HN', 'es_HN.UTF-8', 'es_MX', 'es_MX.UTF-8',
'es_NI', 'es_NI.UTF-8', 'es_PA', 'es_PA.UTF-8', 'es_PE',
'es_PE.UTF-8', 'es_PR', 'es_PR.UTF-8', 'es_PY', 'es_PY.UTF-8',
'es_SV', 'es_SV.UTF-8', 'es_US', 'es_US.UTF-8', 'es_UY',
'es_UY.UTF-8', 'es_VE', 'es_VE.UTF-8', 'et_EE', 'et_EE.ISO-8859-15',
'et_EE.UTF-8', 'eu_ES', 'eu_ES.UTF-8', 'eu_ES@euro', 'fa_IR', 'ff_SN',
'fi_FI', 'fi_FI.UTF-8', 'fi_FI@euro', 'fil_PH', 'fo_FO',
'fo_FO.UTF-8', 'fr_BE', 'fr_BE.UTF-8', 'fr_BE@euro', 'fr_CA',
'fr_CA.UTF-8', 'fr_CH', 'fr_CH.UTF-8', 'fr_FR', 'fr_FR.UTF-8',
'fr_FR@euro', 'fr_LU', 'fr_LU.UTF-8', 'fr_LU@euro', 'fur_IT', 'fy_DE',
'fy_NL', 'ga_IE', 'ga_IE.UTF-8', 'ga_IE@euro', 'gd_GB', 'gd_GB.UTF-8',
'gez_ER', 'gez_ER@abegede', 'gez_ET', 'gez_ET@abegede', 'gl_ES',
'gl_ES.UTF-8', 'gl_ES@euro', 'gu_IN', 'gv_GB', 'gv_GB.UTF-8', 'ha_NG',
'he_IL', 'he_IL.UTF-8', 'hi_IN', 'hne_IN', 'hr_HR', 'hr_HR.UTF-8',
'hsb_DE', 'hsb_DE.UTF-8', 'ht_HT', 'hu_HU', 'hu_HU.UTF-8', 'hy_AM',
'hy_AM.ARMSCII-8', 'id_ID', 'id_ID.UTF-8', 'ig_NG', 'ik_CA', 'is_IS',
'is_IS.UTF-8', 'it_CH', 'it_CH.UTF-8', 'it_IT', 'it_IT.UTF-8',
'it_IT@euro', 'iu_CA', 'iw_IL', 'iw_IL.UTF-8', 'ja_JP.EUC-JP',
'ja_JP.UTF-8', 'ka_GE', 'ka_GE.UTF-8', 'kk_KZ', 'kk_KZ.UTF-8',
'kl_GL', 'kl_GL.UTF-8', 'km_KH', 'kn_IN', 'ko_KR.EUC-KR',
'ko_KR.UTF-8', 'kok_IN', 'ks_IN', 'ks_IN@devanagari', 'ku_TR',
'ku_TR.UTF-8', 'kw_GB', 'kw_GB.UTF-8', 'ky_KG', 'lb_LU', 'lg_UG',
'lg_UG.UTF-8', 'li_BE', 'li_NL', 'lij_IT', 'lo_LA', 'lt_LT',
'lt_LT.UTF-8', 'lv_LV', 'lv_LV.UTF-8', 'mai_IN', 'mg_MG',
'mg_MG.UTF-8', 'mhr_RU', 'mi_NZ', 'mi_NZ.UTF-8', 'mk_MK',
'mk_MK.UTF-8', 'ml_IN', 'mn_MN', 'mr_IN', 'ms_MY', 'ms_MY.UTF-8',
'mt_MT', 'mt_MT.UTF-8', 'my_MM', 'nan_TW@latin', 'nb_NO',
'nb_NO.UTF-8', 'nds_DE', 'nds_NL', 'ne_NP', 'nl_AW', 'nl_BE',
'nl_BE.UTF-8', 'nl_BE@euro', 'nl_NL', 'nl_NL.UTF-8', 'nl_NL@euro',
'nn_NO', 'nn_NO.UTF-8', 'nr_ZA', 'nso_ZA', 'oc_FR', 'oc_FR.UTF-8',
'om_ET', 'om_KE', 'om_KE.UTF-8', 'or_IN', 'os_RU', 'pa_IN', 'pa_PK',
'pap_AN', 'pl_PL', 'pl_PL.UTF-8', 'ps_AF', 'pt_BR', 'pt_BR.UTF-8',
'pt_PT', 'pt_PT.UTF-8', 'pt_PT@euro', 'ro_RO', 'ro_RO.UTF-8', 'ru_RU',
'ru_RU.KOI8-R', 'ru_RU.UTF-8', 'ru_UA', 'ru_UA.UTF-8', 'rw_RW',
'sa_IN', 'sc_IT', 'sd_IN', 'sd_IN@devanagari', 'se_NO', 'shs_CA',
'si_LK', 'sid_ET', 'sk_SK', 'sk_SK.UTF-8', 'sl_SI', 'sl_SI.UTF-8',
'so_DJ', 'so_DJ.UTF-8', 'so_ET', 'so_KE', 'so_KE.UTF-8', 'so_SO',
'so_SO.UTF-8', 'sq_AL', 'sq_AL.UTF-8', 'sq_MK', 'sr_ME', 'sr_RS',
'sr_RS@latin', 'ss_ZA', 'st_ZA', 'st_ZA.UTF-8', 'sv_FI',
'sv_FI.UTF-8', 'sv_FI@euro', 'sv_SE', 'sv_SE.UTF-8', 'sw_KE', 'sw_TZ',
'ta_IN', 'ta_LK', 'te_IN', 'tg_TJ', 'tg_TJ.UTF-8', 'th_TH',
'th_TH.UTF-8', 'ti_ER', 'ti_ET', 'tig_ER', 'tk_TM', 'tl_PH',
'tl_PH.UTF-8', 'tn_ZA', 'tr_CY', 'tr_CY.UTF-8', 'tr_TR',
'tr_TR.UTF-8', 'ts_ZA', 'tt_RU', 'tt_RU@iqtelif', 'ug_CN', 'uk_UA',
'uk_UA.UTF-8', 'unm_US', 'ur_IN', 'ur_PK', 'uz_UZ', 'uz_UZ@cyrillic',
've_ZA', 'vi_VN', 'vi_VN.TCVN', 'wa_BE', 'wa_BE.UTF-8', 'wa_BE@euro',
'wae_CH', 'wal_ET', 'wo_SN', 'xh_ZA', 'xh_ZA.UTF-8', 'yi_US',
'yi_US.UTF-8', 'yo_NG', 'yue_HK', 'zh_CN', 'zh_CN.GB18030',
'zh_CN.GBK', 'zh_CN.UTF-8', 'zh_HK', 'zh_HK.UTF-8', 'zh_SG',
'zh_SG.GBK', 'zh_SG.UTF-8', 'zh_TW', 'zh_TW.EUC-TW', 'zh_TW.UTF-8',
'zu_ZA', 'zu_ZA.UTF-8']

lang2locale = {
    "de": ("de_DE.UTF-8", "de_DE"),
    "en": ("en_US.UTF-8",)}

current_lang = None


def set_locale_from_lang(lang):
    global current_lang

    if lang == current_lang:
        return

    prefix = lang + u"_"
    canonical = "%s_%s" % (lang, lang.upper())
    candidates = sorted(set([x for x in [canonical, canonical+".UTF-8"] + _supported if x.startswith(prefix)]),
                        key=lambda x: (x.endswith("UTF-8"), x.startswith(canonical)),
                        reverse=True)

    for x in candidates:
        try:
            locale.setlocale(locale.LC_NUMERIC, x)
            current_lang = lang
            print "set locale to %r based on the language %r" % (x, current_lang)
            return
        except locale.Error:
            pass

    print "failed to set locale for language %r, tried %r" % (lang, candidates)

########NEW FILE########
__FILENAME__ = _version
__version_info__ = (0, 15, 15)
display_version = version = __version__ = "0.15.15"
gitid = gitversion = ""

def main():
    import pkg_resources
    for r in ("mwlib", "mwlib.rl", "mwlib.ext", "mwlib.hiq"):
        try:
            v = pkg_resources.require(r)[0].version
            print r, v
        except:
            continue

########NEW FILE########
__FILENAME__ = anywikisampler
#!/usr/bin/env python

# Copyright (c) 2008, PediaPress GmbH
# See README.rst for additional licensing information.

import urllib
import sys
import cgi
import subprocess
import BaseHTTPServer
import SimpleHTTPServer
import urllib2
from mwlib.utils import daemonize
try:
    import simplejson as json
except ImportError:
    import json

# init host and port fu**ed up style # FIXME
host, port = "localhost", 8000
if len(sys.argv) > 1:
    host = sys.argv[1]
if len(sys.argv) > 2:
    port = int(sys.argv[2])


mwzip_cmd = 'mw-zip' # (Path to) mw-zip executable.
default_baseurl = "en.wikipedia.org/w"
serviceurl = "http://pediapress.com/api/collections/"
thisservice = "http://%s:%d/"% (host, port)

class State(object):
    articles = []
    baseurl = default_baseurl
    bookmarklet ="javascript:location.href='%s?addarticle='+location.href" % thisservice


class MyHandler(SimpleHTTPServer.SimpleHTTPRequestHandler):

    # all shared between all requests
    state = State()

    def renderpage(self):
        response = """<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
        <form action="">
        baseurl: <input type="text" name="baseurl" value="%s">
        <br/>
        <input type="submit" value="submit"/>
        <input type="submit" name="reset" value="reset"/>
        <input type="submit" name="order" value="order"/>
        <a href="%s">refresh if pages were added</a>
        <br/>
        Articles:        <br/>
        <textarea cols="80" rows="24" name="articles">%s</textarea>
        </form>
        <h5>usage</h5>
        drag and drop this bookmarklet to <a href="%s">add article</a>s from any supported mediawiki
        <br/>
        bookmark this page to get back to this page :)
        <br/>
        Note, only articles from one wiki allowed, baseurl should be set so, that api.php can be found
        <br/>
        Click order to send your collection to pediapress
        </body>
        </html>
        """ %(self.state.baseurl, 
              thisservice,
              ("\n".join(self.state.articles)),
              self.state.bookmarklet,
              )
        return response
    
    def do_GET(self):
        path, query = urllib.splitquery(self.path)
        path = [urllib.unquote_plus(x) for x in path.split("/")]
        query = cgi.parse_qs(query or "")
        print path, query
        app = path[1]
        args = path[2:]
        print args, query
        redir = False
        
        print self.state.articles

        if "articles" in query:
            self.state.articles = [x.strip() for x in query["articles"][0].split("\n") if x.strip()]
            redir = True

        if "baseurl" in query:
            self.state.baseurl = query["baseurl"][0].strip() 
            redir = True

        if "addarticle" in query:
            url = query["addarticle"][0]
            self.state.articles.append( url )
            self.send_response(301)
            self.send_header("Location", url)
            self.end_headers()
            return 

        if "order" in query and self.state.articles and self.state.baseurl:
            return self.do_zip_post()

        if "reset" in query:
            self.state.articles = []
            self.state.baseurl = default_baseurl
            redir = True

        if redir:
            self.send_response(301)
            self.send_header("Location", thisservice)
            self.end_headers()
            return 

        print self.state.articles
        
        response = self.renderpage()
        #self.send_header("Content-type", "text/html")
        #self.send_header("Content-length", str(len(response)))
        self.end_headers()
        self.wfile.write(response)
        
    def do_zip_post(self):

        # automatically acquires a post url 
        u = urllib2.urlopen(serviceurl, data="any")
        h = json.loads(u.read())
        posturl = h["post_url"]
        redirect_url = h["redirect_url"]
        print "acquired post url", posturl
        print "redirected browser to", redirect_url

        
        args = [
            mwzip_cmd,
            '--daemonize',
            '--conf', "http://" +self.state.baseurl,
            '--posturl', posturl,
        ]
        
        for a in self.state.articles:
            a = a.split("/")[-1]
            if a:
                args.append('%s' % str(a))

        print "executing", mwzip_cmd, args
        daemonize()
        rc = subprocess.call(executable=mwzip_cmd, args=args)
        if rc != 0:
            self.send_response(500)
            self.end_headers()
            self.wfile.write("post failed")
        else:
            self.send_response(301)
            self.send_header("Location", redirect_url)
            self.end_headers()



def run():
    server_address = (host, port)
    httpd = BaseHTTPServer.HTTPServer(server_address, MyHandler)
    print "usage: %s [host=localhost] [port=8000]" % sys.argv[0]
    print "listening as http://%s:%d" % (host, port)
    httpd.serve_forever()


if __name__ == "__main__":
    run()

########NEW FILE########
__FILENAME__ = bigswitch
#! /usr/bin/env python
# -*- coding: utf-8 -*-


einwohnerzahlen = u"""
{{#switch: {{{1}}}
| 64001 = 190 | 64001 (Jahr) = 1999 | 64001 (Name) = Aast
| 64002 = 144 | 64002 (Jahr) = 2006 | 64002 (Name) = Abère
| 64003 = 200 | 64003 (Jahr) = 1999 | 64003 (Name) = Abidos
| 64004 = 107 | 64004 (Jahr) = 1999 | 64004 (Name) = Abitain
| 64005 = 467 | 64005 (Jahr) = 2006 | 64005 (Name) = Abos
| 64006 = 434 | 64006 (Jahr) = 1999 | 64006 (Name) = Accous
| 64007 = 834 | 64007 (Jahr) = 2007 | 64007 (Name) = Agnos
| 64008 = 300 | 64008 (Jahr) = 2005 | 64008 (Name) = Ahaxe-Alciette-Bascassan
| 64009 = 1452 | 64009 (Jahr) = 2005 | 64009 (Name) = Ahetze
| 64010 = 642 | 64010 (Jahr) = 2006 | 64010 (Name) = Aïcirits-Camou-Suhast
| 64011 = 118 | 64011 (Jahr) = 2004 | 64011 (Name) = Aincille
| 64012 = 148 | 64012 (Jahr) = 2006 | 64012 (Name) = Ainharp
| 64013 = 164 | 64013 (Jahr) = 2005 | 64013 (Name) = Ainhice-Mongelos
| 64014 = 651 | 64014 (Jahr) = 2004 | 64014 (Name) = Ainhoa
| 64015 = 233 | 64015 (Jahr) = 2005 | 64015 (Name) = Alçay-Alçabéhéty-Sunharette
| 64016 = 395 | 64016 (Jahr) = 1999 | 64016 (Name) = Aldudes
| 64017 = 269 | 64017 (Jahr) = 2007 | 64017 (Name) = Alos-Sibas-Abense
| 64018 = 367 | 64018 (Jahr) = 1999 | 64018 (Name) = Amendeuix-Oneix
| 64019 = 204 | 64019 (Jahr) = 1999 | 64019 (Name) = Amorots-Succos
| 64020 = 215 | 64020 (Jahr) = 1999 | 64020 (Name) = Ance
| 64021 = 603 | 64021 (Jahr) = 2005 | 64021 (Name) = Andoins
| 64022 = 123 | 64022 (Jahr) = 2005 | 64022 (Name) = Andrein
| 64023 = 739 | 64023 (Jahr) = 1999 | 64023 (Name) = Angaïs
| 64024 = 37500 | 64024 (Jahr) = 2005 | 64024 (Name) = Anglet
| 64025 = 103 | 64025 (Jahr) = 2005 | 64025 (Name) = Angous
| 64026 = 247 | 64026 (Jahr) = 1999 | 64026 (Name) = Anhaux
| 64027 = 185 | 64027 (Jahr) = 2007 | 64027 (Name) = Anos
| 64028 = 142 | 64028 (Jahr) = 1999 | 64028 (Name) = Anoye
| 64029 = 653 | 64029 (Jahr) = 1999 | 64029 (Name) = Aramits
| 64031 = 93 | 64031 (Jahr) = 2004 | 64031 (Name) = Arancou
| 64032 = 179 | 64032 (Jahr) = 2007 | 64032 (Name) = Araujuzon
| 64033 = 118 | 64033 (Jahr) = 2004 | 64033 (Name) = Araux
| 64034 = 302 | 64034 (Jahr) = 2005 | 64034 (Name) = Arbérats-Sillègue
| 64035 = 1460 | 64035 (Jahr) = 2004 | 64035 (Name) = Arbonne
| 64036 = 242 | 64036 (Jahr) = 2005 | 64036 (Name) = Arbouet-Sussaute
| 64037 = 1082 | 64037 (Jahr) = 2007 | 64037 (Name) = Arbus
| 64038 = 2985 | 64038 (Jahr) = 2006 | 64038 (Name) = Arcangues
| 64039 = 190 | 64039 (Jahr) = 2006 | 64039 (Name) = Aren
| 64040 = 1094 | 64040 (Jahr) = 1999 | 64040 (Name) = Arette
| 64041 = 550 | 64041 (Jahr) = 2004 | 64041 (Name) = Aressy
| 64042 = 707 | 64042 (Jahr) = 2006 | 64042 (Name) = Argagnon
| 64043 = 253 | 64043 (Jahr) = 2007 | 64043 (Name) = Argelos
| 64044 = 89 | 64044 (Jahr) = 2004 | 64044 (Name) = Arget
| 64045 = 72 | 64045 (Jahr) = 2006 | 64045 (Name) = Arhansus
| 64046 = 357 | 64046 (Jahr) = 1999 | 64046 (Name) = Armendarits
| 64047 = 279 | 64047 (Jahr) = 2006 | 64047 (Name) = Arnéguy
| 64048 = 68 | 64048 (Jahr) = 1999 | 64048 (Name) = Arnos
| 64049 = 246 | 64049 (Jahr) = 2004 | 64049 (Name) = Aroue-Ithorots-Olhaïby
| 64050 = 103 | 64050 (Jahr) = 2004 | 64050 (Name) = Arrast-Larrebieu
| 64051 = 322 | 64051 (Jahr) = 2005 | 64051 (Name) = Arraute-Charritte
| 64052 = 106 | 64052 (Jahr) = 2006 | 64052 (Name) = Arricau-Bordes
| 64053 = 119 | 64053 (Jahr) = 2005 | 64053 (Name) = Arrien
| 64054 = 728 | 64054 (Jahr) = 2006 | 64054 (Name) = Arros-de-Nay
| 64056 = 149 | 64056 (Jahr) = 2006 | 64056 (Name) = Arrosès
| 64057 = 1594 | 64057 (Jahr) = 2004 | 64057 (Name) = Arthez-de-Béarn
| 64058 = 508 | 64058 (Jahr) = 1999 | 64058 (Name) = Arthez-d'Asson
| 64059 = 811 | 64059 (Jahr) = 2005 | 64059 (Name) = Artigueloutan
| 64060 = 1468 | 64060 (Jahr) = 2007 | 64060 (Name) = Artiguelouve
| 64061 = 3153 | 64061 (Jahr) = 2006 | 64061 (Name) = Artix
| 64062 = 2248 | 64062 (Jahr) = 2007 | 64062 (Name) = Arudy
| 64063 = 978 | 64063 (Jahr) = 2004 | 64063 (Name) = Arzacq-Arraziguet
| 64064 = 520 | 64064 (Jahr) = 2007 | 64064 (Name) = Asasp-Arros
| 64065 = 3669 | 64065 (Jahr) = 2007 | 64065 (Name) = Ascain
| 64066 = 280 | 64066 (Jahr) = 2006 | 64066 (Name) = Ascarat
| 64067 = 1479 | 64067 (Jahr) = 1999 | 64067 (Name) = Assat
| 64068 = 1802 | 64068 (Jahr) = 2004 | 64068 (Name) = Asson
| 64069 = 246 | 64069 (Jahr) = 2004 | 64069 (Name) = Aste-Béon
| 64070 = 292 | 64070 (Jahr) = 2006 | 64070 (Name) = Astis
| 64071 = 189 | 64071 (Jahr) = 2006 | 64071 (Name) = Athos-Aspis
| 64072 = 629 | 64072 (Jahr) = 2004 | 64072 (Name) = Aubertin
| 64073 = 214 | 64073 (Jahr) = 2004 | 64073 (Name) = Aubin
| 64074 = 53 | 64074 (Jahr) = 1999 | 64074 (Name) = Aubous
| 64075 = 220 | 64075 (Jahr) = 1999 | 64075 (Name) = Audaux
| 64077 = 140 | 64077 (Jahr) = 2004 | 64077 (Name) = Auga
| 64078 = 209 | 64078 (Jahr) = 1999 | 64078 (Name) = Auriac
| 64079 = 113 | 64079 (Jahr) = 2007 | 64079 (Name) = Aurions-Idernes
| 64080 = 479 | 64080 (Jahr) = 1999 | 64080 (Name) = Aussevielle
| 64081 = 254 | 64081 (Jahr) = 2006 | 64081 (Name) = Aussurucq
| 64082 = 128 | 64082 (Jahr) = 2006 | 64082 (Name) = Auterrive
| 64083 = 144 | 64083 (Jahr) = 2007 | 64083 (Name) = Autevielle-Saint-Martin-Bideren
| 64084 = 149 | 64084 (Jahr) = 2007 | 64084 (Name) = Aydie
| 64085 = 100 | 64085 (Jahr) = 2005 | 64085 (Name) = Aydius
| 64086 = 955 | 64086 (Jahr) = 2005 | 64086 (Name) = Ayherre
| 64087 = 773 | 64087 (Jahr) = 2004 | 64087 (Name) = Baigts-de-Béarn
| 64088 = 223 | 64088 (Jahr) = 2005 | 64088 (Name) = Balansun
| 64089 = 126 | 64089 (Jahr) = 2006 | 64089 (Name) = Baleix
| 64090 = 118 | 64090 (Jahr) = 2005 | 64090 (Name) = Baliracq-Maumusson
| 64091 = 378 | 64091 (Jahr) = 2005 | 64091 (Name) = Baliros
| 64092 = 348 | 64092 (Jahr) = 2007 | 64092 (Name) = Banca
| 64093 = 774 | 64093 (Jahr) = 1999 | 64093 (Name) = Barcus
| 64094 = 1421 | 64094 (Jahr) = 2004 | 64094 (Name) = Bardos
| 64095 = 489 | 64095 (Jahr) = 1999 | 64095 (Name) = Barinque
| 64096 = 155 | 64096 (Jahr) = 2006 | 64096 (Name) = Barraute-Camu
| 64097 = 569 | 64097 (Jahr) = 2005 | 64097 (Name) = Barzun
| 64098 = 72 | 64098 (Jahr) = 1999 | 64098 (Name) = Bassillon-Vauzé
| 64099 = 99 | 64099 (Jahr) = 2004 | 64099 (Name) = Bastanès
| 64100 = 2316 | 64100 (Jahr) = 2004 | 64100 (Name) = Bassussarry
| 64101 = 553 | 64101 (Jahr) = 2005 | 64101 (Name) = Baudreix
| 64102 = 44200 | 64102 (Jahr) = 2005 | 64102 (Name) = Bayonne
| 64103 = 199 | 64103 (Jahr) = 2004 | 64103 (Name) = Bédeille
| 64104 = 534 | 64104 (Jahr) = 2006 | 64104 (Name) = Bedous
| 64105 = 313 | 64105 (Jahr) = 1999 | 64105 (Name) = Béguios
| 64106 = 435 | 64106 (Jahr) = 2006 | 64106 (Name) = Béhasque-Lapiste
| 64107 = 72 | 64107 (Jahr) = 1999 | 64107 (Name) = Béhorléguy
| 64108 = 788 | 64108 (Jahr) = 2004 | 64108 (Name) = Bellocq
| 64109 = 1603 | 64109 (Jahr) = 1999 | 64109 (Name) = Bénéjacq
| 64110 = 212 | 64110 (Jahr) = 2007 | 64110 (Name) = Béost
| 64111 = 110 | 64111 (Jahr) = 2006 | 64111 (Name) = Bentayou-Sérée
| 64112 = 475 | 64112 (Jahr) = 1999 | 64112 (Name) = Bérenx
| 64113 = 117 | 64113 (Jahr) = 2005 | 64113 (Name) = Bergouey-Viellenave
| 64114 = 505 | 64114 (Jahr) = 2006 | 64114 (Name) = Bernadets
| 64115 = 136 | 64115 (Jahr) = 1999 | 64115 (Name) = Berrogain-Laruns
| 64116 = 254 | 64116 (Jahr) = 2004 | 64116 (Name) = Bescat
| 64117 = 126 | 64117 (Jahr) = 2007 | 64117 (Name) = Bésingrand
| 64118 = 51 | 64118 (Jahr) = 2004 | 64118 (Name) = Bétracq
| 64119 = 540 | 64119 (Jahr) = 2006 | 64119 (Name) = Beuste
| 64120 = 505 | 64120 (Jahr) = 2005 | 64120 (Name) = Beyrie-sur-Joyeuse
| 64121 = 171 | 64121 (Jahr) = 2007 | 64121 (Name) = Beyrie-en-Béarn
| 64122 = 30047 | 64122 (Jahr) = 1999 | 64122 (Name) = Biarritz
| 64123 = 1167 | 64123 (Jahr) = 2005 | 64123 (Name) = Bidache
| 64124 = 645 | 64124 (Jahr) = 1999 | 64124 (Name) = Bidarray
| 64125 = 5614 | 64125 (Jahr) = 2006 | 64125 (Name) = Bidart
| 64126 = 1183 | 64126 (Jahr) = 2006 | 64126 (Name) = Bidos
| 64127 = 459 | 64127 (Jahr) = 2005 | 64127 (Name) = Bielle
| 64128 = 151 | 64128 (Jahr) = 2005 | 64128 (Name) = Bilhères
| 64129 = 13393 | 64129 (Jahr) = 1999 | 64129 (Name) = Billère
| 64130 = 831 | 64130 (Jahr) = 1999 | 64130 (Name) = Biriatou
| 64131 = 552 | 64131 (Jahr) = 2005 | 64131 (Name) = Biron
| 64132 = 4613 | 64132 (Jahr) = 2007 | 64132 (Name) = Bizanos
| 64133 = 1147 | 64133 (Jahr) = 2006 | 64133 (Name) = Boeil-Bezing
| 64134 = 284 | 64134 (Jahr) = 1999 | 64134 (Name) = Bonloc
| 64135 = 668 | 64135 (Jahr) = 1999 | 64135 (Name) = Bonnut
| 64136 = 172 | 64136 (Jahr) = 2006 | 64136 (Name) = Borce
| 64137 = 666 | 64137 (Jahr) = 2004 | 64137 (Name) = Bordères
| 64138 = 2176 | 64138 (Jahr) = 2005 | 64138 (Name) = Bordes
| 64139 = 998 | 64139 (Jahr) = 2005 | 64139 (Name) = Bosdarros
| 64140 = 7325 | 64140 (Jahr) = 2005 | 64140 (Name) = Boucau
| 64141 = 336 | 64141 (Jahr) = 2004 | 64141 (Name) = Boueilh-Boueilho-Lasque
| 64142 = 650 | 64142 (Jahr) = 1999 | 64142 (Name) = Bougarber
| 64143 = 117 | 64143 (Jahr) = 2005 | 64143 (Name) = Bouillon
| 64144 = 129 | 64144 (Jahr) = 2007 | 64144 (Name) = Boumourt
| 64145 = 348 | 64145 (Jahr) = 2007 | 64145 (Name) = Bourdettes
| 64146 = 277 | 64146 (Jahr) = 2004 | 64146 (Name) = Bournos
| 64147 = 1988 | 64147 (Jahr) = 1999 | 64147 (Name) = Briscous
| 64148 = 874 | 64148 (Jahr) = 2005 | 64148 (Name) = Bruges-Capbis-Mifaget
| 64149 = 236 | 64149 (Jahr) = 2007 | 64149 (Name) = Bugnein
| 64150 = 143 | 64150 (Jahr) = 2005 | 64150 (Name) = Bunus
| 64151 = 96 | 64151 (Jahr) = 2005 | 64151 (Name) = Burgaronne
| 64152 = 1707 | 64152 (Jahr) = 2006 | 64152 (Name) = Buros
| 64153 = 62 | 64153 (Jahr) = 2006 | 64153 (Name) = Burosse-Mendousse
| 64154 = 162 | 64154 (Jahr) = 2006 | 64154 (Name) = Bussunarits-Sarrasquette
| 64155 = 94 | 64155 (Jahr) = 1999 | 64155 (Name) = Bustince-Iriberry
| 64156 = 424 | 64156 (Jahr) = 2007 | 64156 (Name) = Buziet
| 64157 = 867 | 64157 (Jahr) = 2004 | 64157 (Name) = Buzy
| 64158 = 133 | 64158 (Jahr) = 1999 | 64158 (Name) = Cabidos
| 64159 = 101 | 64159 (Jahr) = 2007 | 64159 (Name) = Cadillon
| 64160 = 5849 | 64160 (Jahr) = 2007 | 64160 (Name) = Cambo-les-Bains
| 64161 = 789 | 64161 (Jahr) = 2007 | 64161 (Name) = Came
| 64162 = 106 | 64162 (Jahr) = 2007 | 64162 (Name) = Camou-Cihigue
| 64165 = 254 | 64165 (Jahr) = 2006 | 64165 (Name) = Cardesse
| 64166 = 184 | 64166 (Jahr) = 2005 | 64166 (Name) = Caro
| 64167 = 168 | 64167 (Jahr) = 2006 | 64167 (Name) = Carrère
| 64168 = 498 | 64168 (Jahr) = 2005 | 64168 (Name) = Carresse-Cassaber
| 64170 = 199 | 64170 (Jahr) = 2004 | 64170 (Name) = Castagnède
| 64171 = 221 | 64171 (Jahr) = 2007 | 64171 (Name) = Casteide-Cami
| 64172 = 204 | 64172 (Jahr) = 2007 | 64172 (Name) = Casteide-Candau
| 64173 = 131 | 64173 (Jahr) = 1999 | 64173 (Name) = Casteide-Doat
| 64174 = 57 | 64174 (Jahr) = 2005 | 64174 (Name) = Castéra-Loubix
| 64175 = 159 | 64175 (Jahr) = 2007 | 64175 (Name) = Castet
| 64176 = 184 | 64176 (Jahr) = 2005 | 64176 (Name) = Castetbon
| 64177 = 657 | 64177 (Jahr) = 1999 | 64177 (Name) = Castétis
| 64178 = 386 | 64178 (Jahr) = 2004 | 64178 (Name) = Castetnau-Camblong
| 64179 = 147 | 64179 (Jahr) = 1999 | 64179 (Name) = Castetner
| 64180 = 171 | 64180 (Jahr) = 2005 | 64180 (Name) = Castetpugon
| 64181 = 244 | 64181 (Jahr) = 1999 | 64181 (Name) = Castillon(Canton d'Arthez-de-Béarn)
| 64182 = 71 | 64182 (Jahr) = 2004 | 64182 (Name) = Castillon(Canton de Lembeye)
| 64183 = 449 | 64183 (Jahr) = 2004 | 64183 (Name) = Caubios-Loos
| 64184 = 477 | 64184 (Jahr) = 2005 | 64184 (Name) = Cescau
| 64185 = 86 | 64185 (Jahr) = 2004 | 64185 (Name) = Cette-Eygun
| 64186 = 214 | 64186 (Jahr) = 2005 | 64186 (Name) = Charre
| 64187 = 242 | 64187 (Jahr) = 2004 | 64187 (Name) = Charritte-de-Bas
| 64188 = 1163 | 64188 (Jahr) = 2006 | 64188 (Name) = Chéraute
| 64189 = 6282 | 64189 (Jahr) = 2006 | 64189 (Name) = Ciboure
| 64190 = 216 | 64190 (Jahr) = 2005 | 64190 (Name) = Claracq
| 64191 = 2109 | 64191 (Jahr) = 2007 | 64191 (Name) = Coarraze
| 64192 = 110 | 64192 (Jahr) = 2004 | 64192 (Name) = Conchez-de-Béarn
| 64193 = 82 | 64193 (Jahr) = 2004 | 64193 (Name) = Corbère-Abères
| 64194 = 331 | 64194 (Jahr) = 2005 | 64194 (Name) = Coslédaà-Lube-Boast
| 64195 = 107 | 64195 (Jahr) = 2004 | 64195 (Name) = Coublucq
| 64196 = 152 | 64196 (Jahr) = 2006 | 64196 (Name) = Crouseilles
| 64197 = 225 | 64197 (Jahr) = 1999 | 64197 (Name) = Cuqueron
| 64198 = 1603 | 64198 (Jahr) = 2005 | 64198 (Name) = Denguin
| 64199 = 179 | 64199 (Jahr) = 1999 | 64199 (Name) = Diusse
| 64200 = 176 | 64200 (Jahr) = 2007 | 64200 (Name) = Doazon
| 64201 = 217 | 64201 (Jahr) = 2004 | 64201 (Name) = Dognen
| 64202 = 467 | 64202 (Jahr) = 2004 | 64202 (Name) = Domezain-Berraute
| 64203 = 234 | 64203 (Jahr) = 2004 | 64203 (Name) = Doumy
| 64204 = 435 | 64204 (Jahr) = 1999 | 64204 (Name) = Eaux-Bonnes
| 64205 = 228 | 64205 (Jahr) = 2005 | 64205 (Name) = Escos
| 64206 = 105 | 64206 (Jahr) = 1999 | 64206 (Name) = Escot
| 64207 = 385 | 64207 (Jahr) = 2005 | 64207 (Name) = Escou
| 64208 = 231 | 64208 (Jahr) = 1999 | 64208 (Name) = Escoubès
| 64209 = 392 | 64209 (Jahr) = 2006 | 64209 (Name) = Escout
| 64210 = 158 | 64210 (Jahr) = 2007 | 64210 (Name) = Escurès
| 64211 = 193 | 64211 (Jahr) = 1999 | 64211 (Name) = Eslourenties-Daban
| 64212 = 153 | 64212 (Jahr) = 2007 | 64212 (Name) = Espéchède
| 64213 = 1879 | 64213 (Jahr) = 1999 | 64213 (Name) = Espelette
| 64214 = 483 | 64214 (Jahr) = 2007 | 64214 (Name) = Espès-Undurein
| 64215 = 113 | 64215 (Jahr) = 2007 | 64215 (Name) = Espiute
| 64216 = 894 | 64216 (Jahr) = 2004 | 64216 (Name) = Espoey
| 64217 = 541 | 64217 (Jahr) = 2004 | 64217 (Name) = Esquiule
| 64218 = 382 | 64218 (Jahr) = 2004 | 64218 (Name) = Estérençuby
| 64219 = 238 | 64219 (Jahr) = 2004 | 64219 (Name) = Estialescq
| 64220 = 441 | 64220 (Jahr) = 2005 | 64220 (Name) = Estos
| 64221 = 122 | 64221 (Jahr) = 1999 | 64221 (Name) = Etcharry
| 64222 = 62 | 64222 (Jahr) = 2004 | 64222 (Name) = Etchebar
| 64223 = 105 | 64223 (Jahr) = 1999 | 64223 (Name) = Etsaut
| 64224 = 617 | 64224 (Jahr) = 2004 | 64224 (Name) = Eysus
| 64225 = 386 | 64225 (Jahr) = 2004 | 64225 (Name) = Féas
| 64226 = 152 | 64226 (Jahr) = 2006 | 64226 (Name) = Fichous-Riumayou
| 64227 = 592 | 64227 (Jahr) = 2004 | 64227 (Name) = Gabaston
| 64228 = 215 | 64228 (Jahr) = 1999 | 64228 (Name) = Gabat
| 64229 = 114 | 64229 (Jahr) = 2007 | 64229 (Name) = Gamarthe
| 64230 = 5225 | 64230 (Jahr) = 2007 | 64230 (Name) = Gan
| 64231 = 523 | 64231 (Jahr) = 2006 | 64231 (Name) = Garindein
| 64232 = 188 | 64232 (Jahr) = 2004 | 64232 (Name) = Garlède-Mondebat
| 64233 = 1242 | 64233 (Jahr) = 2004 | 64233 (Name) = Garlin
| 64234 = 212 | 64234 (Jahr) = 2007 | 64234 (Name) = Garos
| 64235 = 282 | 64235 (Jahr) = 2006 | 64235 (Name) = Garris
| 64236 = 92 | 64236 (Jahr) = 1999 | 64236 (Name) = Gayon
| 64237 = 3714 | 64237 (Jahr) = 2004 | 64237 (Name) = Gelos
| 64238 = 2004 | 64238 (Jahr) = 2005 | 64238 (Name) = Ger
| 64239 = 124 | 64239 (Jahr) = 2007 | 64239 (Name) = Gerderest
| 64240 = 158 | 64240 (Jahr) = 2007 | 64240 (Name) = Gère-Bélesten
| 64241 = 408 | 64241 (Jahr) = 2004 | 64241 (Name) = Géronce
| 64242 = 70 | 64242 (Jahr) = 1999 | 64242 (Name) = Gestas
| 64243 = 123 | 64243 (Jahr) = 1999 | 64243 (Name) = Géus-d'Arzacq
| 64244 = 199 | 64244 (Jahr) = 1999 | 64244 (Name) = Geüs-d'Oloron
| 64245 = 554 | 64245 (Jahr) = 2005 | 64245 (Name) = Goès
| 64246 = 175 | 64246 (Jahr) = 2004 | 64246 (Name) = Gomer
| 64247 = 479 | 64247 (Jahr) = 2005 | 64247 (Name) = Gotein-Libarrenx
| 64249 = 1284 | 64249 (Jahr) = 1999 | 64249 (Name) = Guéthary
| 64250 = 864 | 64250 (Jahr) = 2007 | 64250 (Name) = Guiche
| 64251 = 232 | 64251 (Jahr) = 2007 | 64251 (Name) = Guinarthe-Parenties
| 64252 = 827 | 64252 (Jahr) = 2004 | 64252 (Name) = Gurmençon
| 64253 = 374 | 64253 (Jahr) = 2007 | 64253 (Name) = Gurs
| 64254 = 441 | 64254 (Jahr) = 1999 | 64254 (Name) = Hagetaubin
| 64255 = 483 | 64255 (Jahr) = 2007 | 64255 (Name) = Halsou
| 64256 = 5742 | 64256 (Jahr) = 2006 | 64256 (Name) = Hasparren
| 64257 = 257 | 64257 (Jahr) = 2006 | 64257 (Name) = Haut-de-Bosdarros
| 64258 = 103 | 64258 (Jahr) = 2004 | 64258 (Name) = Haux
| 64259 = 680 | 64259 (Jahr) = 2005 | 64259 (Name) = Hélette
| 64260 = 13000 | 64260 (Jahr) = 2005 | 64260 (Name) = Hendaye
| 64261 = 365 | 64261 (Jahr) = 2006 | 64261 (Name) = Herrère
| 64262 = 264 | 64262 (Jahr) = 2005 | 64262 (Name) = Higuères-Souye
| 64263 = 164 | 64263 (Jahr) = 2004 | 64263 (Name) = L'Hôpital-d'Orion
| 64264 = 76 | 64264 (Jahr) = 2004 | 64264 (Name) = L'Hôpital-Saint-Blaise
| 64265 = 79 | 64265 (Jahr) = 2005 | 64265 (Name) = Hosta
| 64266 = 188 | 64266 (Jahr) = 2005 | 64266 (Name) = Hours
| 64267 = 88 | 64267 (Jahr) = 1999 | 64267 (Name) = Ibarrolle
| 64268 = 262 | 64268 (Jahr) = 1999 | 64268 (Name) = Idaux-Mendy
| 64269 = 5154 | 64269 (Jahr) = 1999 | 64269 (Name) = Idron-Ousse-Sendets
| 64270 = 819 | 64270 (Jahr) = 1999 | 64270 (Name) = Igon
| 64271 = 450 | 64271 (Jahr) = 2006 | 64271 (Name) = Iholdy
| 64272 = 131 | 64272 (Jahr) = 2004 | 64272 (Name) = Ilharre
| 64273 = 826 | 64273 (Jahr) = 2007 | 64273 (Name) = Irissarry
| 64274 = 290 | 64274 (Jahr) = 1999 | 64274 (Name) = Irouléguy
| 64275 = 618 | 64275 (Jahr) = 1999 | 64275 (Name) = Ispoure
| 64276 = 262 | 64276 (Jahr) = 1999 | 64276 (Name) = Issor
| 64277 = 377 | 64277 (Jahr) = 2004 | 64277 (Name) = Isturits
| 64279 = 177 | 64279 (Jahr) = 1999 | 64279 (Name) = Itxassou
| 64280 = 442 | 64280 (Jahr) = 2004 | 64280 (Name) = Izeste
| 64281 = 131 | 64281 (Jahr) = 2004 | 64281 (Name) = Jasses
| 64282 = 906 | 64282 (Jahr) = 2004 | 64282 (Name) = Jatxou
| 64283 = 142 | 64283 (Jahr) = 2004 | 64283 (Name) = Jaxu
| 64284 = 7087 | 64284 (Jahr) = 2004 | 64284 (Name) = Jurançon
| 64285 = 224 | 64285 (Jahr) = 2006 | 64285 (Name) = Juxue
| 64286 = 387 | 64286 (Jahr) = 2007 | 64286 (Name) = Laà-Mondrans
| 64287 = 110 | 64287 (Jahr) = 2004 | 64287 (Name) = Laàs
| 64288 = 529 | 64288 (Jahr) = 2005 | 64288 (Name) = Labastide-Cézéracq
| 64289 = 990 | 64289 (Jahr) = 2006 | 64289 (Name) = La Bastide-Clairence
| 64290 = 459 | 64290 (Jahr) = 2007 | 64290 (Name) = Labastide-Monréjeau
| 64291 = 292 | 64291 (Jahr) = 1999 | 64291 (Name) = Labastide-Villefranche
| 64292 = 250 | 64292 (Jahr) = 2007 | 64292 (Name) = Labatmale
| 64293 = 151 | 64293 (Jahr) = 2004 | 64293 (Name) = Labatut
| 64294 = 158 | 64294 (Jahr) = 2006 | 64294 (Name) = Labets-Biscay
| 64295 = 92 | 64295 (Jahr) = 2005 | 64295 (Name) = Labeyrie
| 64296 = 105 | 64296 (Jahr) = 1999 | 64296 (Name) = Lacadée
| 64297 = 127 | 64297 (Jahr) = 2004 | 64297 (Name) = Lacarre
| 64298 = 128 | 64298 (Jahr) = 2007 | 64298 (Name) = Lacarry-Arhan-Charritte-de-Haut
| 64299 = 229 | 64299 (Jahr) = 2005 | 64299 (Name) = Lacommande
| 64300 = 695 | 64300 (Jahr) = 2006 | 64300 (Name) = Lacq
| 64301 = 1228 | 64301 (Jahr) = 2005 | 64301 (Name) = Lagor
| 64302 = 501 | 64302 (Jahr) = 1999 | 64302 (Name) = Lagos
| 64303 = 168 | 64303 (Jahr) = 2007 | 64303 (Name) = Laguinge-Restoue
| 64304 = 1973 | 64304 (Jahr) = 2005 | 64304 (Name) = Lahonce
| 64305 = 457 | 64305 (Jahr) = 2007 | 64305 (Name) = Lahontan
| 64306 = 700 | 64306 (Jahr) = 2004 | 64306 (Name) = Lahourcade
| 64307 = 213 | 64307 (Jahr) = 2004 | 64307 (Name) = Lalongue
| 64308 = 265 | 64308 (Jahr) = 2004 | 64308 (Name) = Lalonquette
| 64309 = 214 | 64309 (Jahr) = 1999 | 64309 (Name) = Lamayou
| 64310 = 513 | 64310 (Jahr) = 2004 | 64310 (Name) = Lanne-en-Barétous
| 64311 = 167 | 64311 (Jahr) = 2006 | 64311 (Name) = Lannecaube
| 64312 = 275 | 64312 (Jahr) = 2005 | 64312 (Name) = Lanneplaà
| 64313 = 284 | 64313 (Jahr) = 1999 | 64313 (Name) = Lantabat
| 64314 = 407 | 64314 (Jahr) = 2004 | 64314 (Name) = Larceveau-Arros-Cibits
| 64315 = 901 | 64315 (Jahr) = 2006 | 64315 (Name) = Laroin
| 64316 = 214 | 64316 (Jahr) = 1999 | 64316 (Name) = Larrau
| 64317 = 1428 | 64317 (Jahr) = 2006 | 64317 (Name) = Larressore
| 64318 = 159 | 64318 (Jahr) = 2006 | 64318 (Name) = Larreule
| 64319 = 193 | 64319 (Jahr) = 2007 | 64319 (Name) = Larribar-Sorhapuru
| 64320 = 1365 | 64320 (Jahr) = 2007 | 64320 (Name) = Laruns
| 64321 = 244 | 64321 (Jahr) = 2006 | 64321 (Name) = Lasclaveries
| 64322 = 291 | 64322 (Jahr) = 2007 | 64322 (Name) = Lasse
| 64323 = 90 | 64323 (Jahr) = 2005 | 64323 (Name) = Lasserre
| 64324 = 1600 | 64324 (Jahr) = 2006 | 64324 (Name) = Lasseube
| 64325 = 175 | 64325 (Jahr) = 2007 | 64325 (Name) = Lasseubetat
| 64326 = 117 | 64326 (Jahr) = 2007 | 64326 (Name) = Lay-Lamidou
| 64327 = 182 | 64327 (Jahr) = 1999 | 64327 (Name) = Lecumberry
| 64328 = 1091 | 64328 (Jahr) = 1999 | 64328 (Name) = Ledeuix
| 64329 = 1142 | 64329 (Jahr) = 2007 | 64329 (Name) = Lée
| 64330 = 292 | 64330 (Jahr) = 2005 | 64330 (Name) = Lées-Athas
| 64331 = 695 | 64331 (Jahr) = 1999 | 64331 (Name) = Lembeye
| 64332 = 138 | 64332 (Jahr) = 2005 | 64332 (Name) = Lème
| 64334 = 225 | 64334 (Jahr) = 2007 | 64334 (Name) = Léren
| 64335 = 9439 | 64335 (Jahr) = 2004 | 64335 (Name) = Lescar
| 64336 = 203 | 64336 (Jahr) = 1999 | 64336 (Name) = Lescun
| 64337 = 149 | 64337 (Jahr) = 2004 | 64337 (Name) = Lespielle
| 64338 = 114 | 64338 (Jahr) = 1999 | 64338 (Name) = Lespourcy
| 64339 = 801 | 64339 (Jahr) = 2007 | 64339 (Name) = Lestelle-Bétharram
| 64340 = 76 | 64340 (Jahr) = 2005 | 64340 (Name) = Lichans-Sunhar
| 64341 = 129 | 64341 (Jahr) = 1999 | 64341 (Name) = Lichos
| 64342 = 231 | 64342 (Jahr) = 2005 | 64342 (Name) = Licq-Athérey
| 64343 = 422 | 64343 (Jahr) = 2007 | 64343 (Name) = Limendous
| 64344 = 364 | 64344 (Jahr) = 2004 | 64344 (Name) = Livron
| 64345 = 204 | 64345 (Jahr) = 2006 | 64345 (Name) = Lohitzun-Oyhercq
| 64346 = 182 | 64346 (Jahr) = 2004 | 64346 (Name) = Lombia
| 64347 = 122 | 64347 (Jahr) = 2004 | 64347 (Name) = Lonçon
| 64348 = 12000 | 64348 (Jahr) = 2005 | 64348 (Name) = Lons
| 64349 = 445 | 64349 (Jahr) = 2005 | 64349 (Name) = Loubieng
| 64350 = 580 | 64350 (Jahr) = 1999 | 64350 (Name) = Louhossoa
| 64351 = 146 | 64351 (Jahr) = 2007 | 64351 (Name) = Lourdios-Ichère
| 64352 = 325 | 64352 (Jahr) = 2005 | 64352 (Name) = Lourenties
| 64353 = 1100 | 64353 (Jahr) = 2006 | 64353 (Name) = Louvie-Juzon
| 64354 = 112 | 64354 (Jahr) = 2007 | 64354 (Name) = Louvie-Soubiron
| 64355 = 116 | 64355 (Jahr) = 2004 | 64355 (Name) = Louvigny
| 64356 = 106 | 64356 (Jahr) = 2007 | 64356 (Name) = Luc-Armau
| 64357 = 48 | 64357 (Jahr) = 2004 | 64357 (Name) = Lucarré
| 64358 = 290 | 64358 (Jahr) = 2007 | 64358 (Name) = Lucgarier
| 64359 = 966 | 64359 (Jahr) = 2006 | 64359 (Name) = Lucq-de-Béarn
| 64360 = 225 | 64360 (Jahr) = 2006 | 64360 (Name) = Lurbe-Saint-Christau
| 64361 = 157 | 64361 (Jahr) = 2005 | 64361 (Name) = Lussagnet-Lusson
| 64362 = 256 | 64362 (Jahr) = 1999 | 64362 (Name) = Luxe-Sumberraute
| 64363 = 348 | 64363 (Jahr) = 1999 | 64363 (Name) = Lys
| 64364 = 530 | 64364 (Jahr) = 2005 | 64364 (Name) = Macaye
| 64365 = 393 | 64365 (Jahr) = 2006 | 64365 (Name) = Malaussanne
| 64366 = 123 | 64366 (Jahr) = 2006 | 64366 (Name) = Mascaraàs-Haron
| 64367 = 747 | 64367 (Jahr) = 2007 | 64367 (Name) = Maslacq
| 64368 = 240 | 64368 (Jahr) = 2004 | 64368 (Name) = Masparraute
| 64369 = 248 | 64369 (Jahr) = 2005 | 64369 (Name) = Maspie-Lalonquère-Juillacq
| 64370 = 499 | 64370 (Jahr) = 2007 | 64370 (Name) = Maucor
| 64371 = 3315 | 64371 (Jahr) = 2004 | 64371 (Name) = Mauléon-Licharre
| 64372 = 109 | 64372 (Jahr) = 2004 | 64372 (Name) = Maure
| 64373 = 2143 | 64373 (Jahr) = 1999 | 64373 (Name) = Mazères-Lezons
| 64374 = 891 | 64374 (Jahr) = 2006 | 64374 (Name) = Mazerolles
| 64375 = 259 | 64375 (Jahr) = 2007 | 64375 (Name) = Méharin
| 64376 = 743 | 64376 (Jahr) = 2005 | 64376 (Name) = Meillon
| 64377 = 707 | 64377 (Jahr) = 1999 | 64377 (Name) = Mendionde
| 64378 = 213 | 64378 (Jahr) = 2007 | 64378 (Name) = Menditte
| 64379 = 182 | 64379 (Jahr) = 1999 | 64379 (Name) = Mendive
| 64380 = 205 | 64380 (Jahr) = 2007 | 64380 (Name) = Méracq
| 64381 = 271 | 64381 (Jahr) = 2004 | 64381 (Name) = Méritein
| 64382 = 330 | 64382 (Jahr) = 2005 | 64382 (Name) = Mesplède
| 64383 = 95 | 64383 (Jahr) = 2005 | 64383 (Name) = Mialos
| 64385 = 186 | 64385 (Jahr) = 2006 | 64385 (Name) = Miossens-Lanusse
| 64386 = 1084 | 64386 (Jahr) = 2005 | 64386 (Name) = Mirepeix
| 64387 = 513 | 64387 (Jahr) = 2007 | 64387 (Name) = Momas
| 64388 = 120 | 64388 (Jahr) = 2004 | 64388 (Name) = Momy
| 64389 = 292 | 64389 (Jahr) = 2005 | 64389 (Name) = Monassut-Audiracq
| 64390 = 151 | 64390 (Jahr) = 2004 | 64390 (Name) = Moncaup
| 64391 = 345 | 64391 (Jahr) = 2006 | 64391 (Name) = Moncayolle-Larrory-Mendibieu
| 64392 = 101 | 64392 (Jahr) = 1999 | 64392 (Name) = Moncla
| 64393 = 4393 | 64393 (Jahr) = 2007 | 64393 (Name) = Monein
| 64394 = 81 | 64394 (Jahr) = 1999 | 64394 (Name) = Monpezat
| 64395 = 126 | 64395 (Jahr) = 2006 | 64395 (Name) = Monségur
| 64396 = 845 | 64396 (Jahr) = 1999 | 64396 (Name) = Mont
| 64397 = 109 | 64397 (Jahr) = 2005 | 64397 (Name) = Montagut
| 64398 = 459 | 64398 (Jahr) = 2007 | 64398 (Name) = Montaner
| 64399 = 2126 | 64399 (Jahr) = 2005 | 64399 (Name) = Montardon
| 64400 = 1011 | 64400 (Jahr) = 2006 | 64400 (Name) = Montaut
| 64401 = 76 | 64401 (Jahr) = 2005 | 64401 (Name) = Mont-Disse
| 64403 = 166 | 64403 (Jahr) = 2007 | 64403 (Name) = Montfort
| 64404 = 349 | 64404 (Jahr) = 1999 | 64404 (Name) = Montory
| 64405 = 4121 | 64405 (Jahr) = 2006 | 64405 (Name) = Morlaàs
| 64406 = 454 | 64406 (Jahr) = 2004 | 64406 (Name) = Morlanne
| 64407 = 4280 | 64407 (Jahr) = 2004 | 64407 (Name) = Mouguerre
| 64408 = 38 | 64408 (Jahr) = 1999 | 64408 (Name) = Mouhous
| 64409 = 769 | 64409 (Jahr) = 2007 | 64409 (Name) = Moumour
| 64410 = 7618 | 64410 (Jahr) = 2005 | 64410 (Name) = Mourenx
| 64411 = 249 | 64411 (Jahr) = 2007 | 64411 (Name) = Musculdy
| 64412 = 111 | 64412 (Jahr) = 2005 | 64412 (Name) = Nabas
| 64413 = 577 | 64413 (Jahr) = 2007 | 64413 (Name) = Narcastet
| 64414 = 116 | 64414 (Jahr) = 2006 | 64414 (Name) = Narp
| 64415 = 1304 | 64415 (Jahr) = 2005 | 64415 (Name) = Navailles-Angos
| 64416 = 1138 | 64416 (Jahr) = 2005 | 64416 (Name) = Navarrenx
| 64417 = 3288 | 64417 (Jahr) = 2006 | 64417 (Name) = Nay
| 64418 = 145 | 64418 (Jahr) = 1999 | 64418 (Name) = Noguères
| 64419 = 967 | 64419 (Jahr) = 2004 | 64419 (Name) = Nousty
| 64420 = 237 | 64420 (Jahr) = 2007 | 64420 (Name) = Ogenne-Camptort
| 64421 = 1106 | 64421 (Jahr) = 2004 | 64421 (Name) = Ogeu-les-Bains
| 64422 = 10992 | 64422 (Jahr) = 1999 | 64422 (Name) = Oloron-Sainte-Marie
| 64423 = 175 | 64423 (Jahr) = 2004 | 64423 (Name) = Oraàs
| 64424 = 554 | 64424 (Jahr) = 2004 | 64424 (Name) = Ordiarp
| 64425 = 479 | 64425 (Jahr) = 2007 | 64425 (Name) = Orègue
| 64426 = 207 | 64426 (Jahr) = 1999 | 64426 (Name) = Orin
| 64427 = 155 | 64427 (Jahr) = 2005 | 64427 (Name) = Orion
| 64428 = 125 | 64428 (Jahr) = 2005 | 64428 (Name) = Orriule
| 64429 = 96 | 64429 (Jahr) = 2006 | 64429 (Name) = Orsanco
| 64430 = 10200 | 64430 (Jahr) = 2005 | 64430 (Name) = Orthez
| 64431 = 461 | 64431 (Jahr) = 2004 | 64431 (Name) = Os-Marsillon
| 64432 = 83 | 64432 (Jahr) = 2007 | 64432 (Name) = Ossas-Suhare
| 64433 = 330 | 64433 (Jahr) = 2005 | 64433 (Name) = Osse-en-Aspe
| 64434 = 44 | 64434 (Jahr) = 2005 | 64434 (Name) = Ossenx
| 64435 = 230 | 64435 (Jahr) = 2007 | 64435 (Name) = Osserain-Rivareyte
| 64436 = 785 | 64436 (Jahr) = 2006 | 64436 (Name) = Ossès
| 64437 = 197 | 64437 (Jahr) = 2006 | 64437 (Name) = Ostabat-Asme
| 64438 = 391 | 64438 (Jahr) = 2004 | 64438 (Name) = Ouillon
| 64440 = 351 | 64440 (Jahr) = 2007 | 64440 (Name) = Ozenx-Montestrucq
| 64441 = 263 | 64441 (Jahr) = 2007 | 64441 (Name) = Pagolle
| 64442 = 241 | 64442 (Jahr) = 2006 | 64442 (Name) = Parbayse
| 64443 = 967 | 64443 (Jahr) = 2004 | 64443 (Name) = Pardies
| 64444 = 440 | 64444 (Jahr) = 2007 | 64444 (Name) = Pardies-Piétat
| 64445 = 83000 | 64445 (Jahr) = 2005 | 64445 (Name) = Pau
| 64446 = 134 | 64446 (Jahr) = 2007 | 64446 (Name) = Peyrelongue-Abos
| 64447 = 124 | 64447 (Jahr) = 2004 | 64447 (Name) = Piets-Plasence-Moustrou
| 64448 = 1575 | 64448 (Jahr) = 2004 | 64448 (Name) = Poey-de-Lescar
| 64449 = 182 | 64449 (Jahr) = 2005 | 64449 (Name) = Poey-d'Oloron
| 64450 = 179 | 64450 (Jahr) = 1999 | 64450 (Name) = Pomps
| 64451 = 88 | 64451 (Jahr) = 2004 | 64451 (Name) = Ponson-Debat-Pouts
| 64452 = 246 | 64452 (Jahr) = 2004 | 64452 (Name) = Ponson-Dessus
| 64453 = 2743 | 64453 (Jahr) = 2005 | 64453 (Name) = Pontacq
| 64454 = 118 | 64454 (Jahr) = 2007 | 64454 (Name) = Pontiacq-Viellepinte
| 64455 = 168 | 64455 (Jahr) = 1999 | 64455 (Name) = Portet
| 64456 = 36 | 64456 (Jahr) = 1999 | 64456 (Name) = Pouliacq
| 64457 = 200 | 64457 (Jahr) = 2005 | 64457 (Name) = Poursiugues-Boucoue
| 64458 = 255 | 64458 (Jahr) = 1999 | 64458 (Name) = Préchacq-Josbaig
| 64459 = 152 | 64459 (Jahr) = 1999 | 64459 (Name) = Préchacq-Navarrenx
| 64460 = 361 | 64460 (Jahr) = 2005 | 64460 (Name) = Précilhon
| 64461 = 1107 | 64461 (Jahr) = 2006 | 64461 (Name) = Puyoô
| 64462 = 411 | 64462 (Jahr) = 2004 | 64462 (Name) = Ramous
| 64463 = 659 | 64463 (Jahr) = 2005 | 64463 (Name) = Rébénacq
| 64464 = 82 | 64464 (Jahr) = 2004 | 64464 (Name) = Ribarrouy
| 64465 = 162 | 64465 (Jahr) = 2006 | 64465 (Name) = Riupeyrous
| 64466 = 237 | 64466 (Jahr) = 1999 | 64466 (Name) = Rivehaute
| 64467 = 685 | 64467 (Jahr) = 1999 | 64467 (Name) = Rontignon
| 64468 = 124 | 64468 (Jahr) = 2007 | 64468 (Name) = Roquiague
| 64469 = 356 | 64469 (Jahr) = 2007 | 64469 (Name) = Saint-Abit
| 64470 = 566 | 64470 (Jahr) = 2006 | 64470 (Name) = Saint-Armou
| 64471 = 369 | 64471 (Jahr) = 2007 | 64471 (Name) = Saint-Boès
| 64472 = 774 | 64472 (Jahr) = 2005 | 64472 (Name) = Saint-Castin
| 64473 = 334 | 64473 (Jahr) = 2007 | 64473 (Name) = Sainte-Colome
| 64474 = 125 | 64474 (Jahr) = 2007 | 64474 (Name) = Saint-Dos
| 64475 = 212 | 64475 (Jahr) = 2006 | 64475 (Name) = Sainte-Engrâce
| 64476 = 390 | 64476 (Jahr) = 2006 | 64476 (Name) = Saint-Esteben
| 64477 = 1602 | 64477 (Jahr) = 2006 | 64477 (Name) = Saint-Étienne-de-Baïgorry
| 64478 = 754 | 64478 (Jahr) = 2007 | 64478 (Name) = Saint-Faust
| 64479 = 155 | 64479 (Jahr) = 2007 | 64479 (Name) = Saint-Girons
| 64480 = 187 | 64480 (Jahr) = 2004 | 64480 (Name) = Saint-Gladie-Arrive-Munein
| 64481 = 188 | 64481 (Jahr) = 1999 | 64481 (Name) = Saint-Goin
| 64482 = 636 | 64482 (Jahr) = 2007 | 64482 (Name) = Saint-Jammes
| 64483 = 13300 | 64483 (Jahr) = 2005 | 64483 (Name) = Saint-Jean-de-Luz
| 64484 = 855 | 64484 (Jahr) = 2007 | 64484 (Name) = Saint-Jean-le-Vieux
| 64485 = 1511 | 64485 (Jahr) = 2005 | 64485 (Name) = Saint-Jean-Pied-de-Port
| 64486 = 79 | 64486 (Jahr) = 2007 | 64486 (Name) = Saint-Jean-Poudge
| 64487 = 235 | 64487 (Jahr) = 2004 | 64487 (Name) = Saint-Just-Ibarre
| 64488 = 413 | 64488 (Jahr) = 2006 | 64488 (Name) = Saint-Laurent-Bretagne
| 64489 = 293 | 64489 (Jahr) = 2005 | 64489 (Name) = Saint-Martin-d'Arberoue
| 64490 = 442 | 64490 (Jahr) = 1999 | 64490 (Name) = Saint-Martin-d'Arrossa
| 64491 = 185 | 64491 (Jahr) = 2004 | 64491 (Name) = Saint-Médard
| 64492 = 243 | 64492 (Jahr) = 2005 | 64492 (Name) = Saint-Michel
| 64493 = 1874 | 64493 (Jahr) = 2006 | 64493 (Name) = Saint-Palais
| 64494 = 205 | 64494 (Jahr) = 1999 | 64494 (Name) = Saint-Pé-de-Léren
| 64495 = 5106 | 64495 (Jahr) = 2006 | 64495 (Name) = Saint-Pée-sur-Nivelle
| 64496 = 4380 | 64496 (Jahr) = 2005 | 64496 (Name) = Saint-Pierre-d'Irube
| 64498 = 369 | 64498 (Jahr) = 1999 | 64498 (Name) = Saint-Vincent
| 64499 = 4759 | 64499 (Jahr) = 1999 | 64499 (Name) = Salies-de-Béarn
| 64500 = 282 | 64500 (Jahr) = 2004 | 64500 (Name) = Salles-Mongiscard
| 64501 = 585 | 64501 (Jahr) = 2007 | 64501 (Name) = Sallespisse
| 64502 = 555 | 64502 (Jahr) = 2006 | 64502 (Name) = Sames
| 64503 = 71 | 64503 (Jahr) = 2006 | 64503 (Name) = Samsons-Lion
| 64504 = 2262 | 64504 (Jahr) = 2005 | 64504 (Name) = Sare
| 64505 = 258 | 64505 (Jahr) = 2005 | 64505 (Name) = Sarpourenx
| 64506 = 218 | 64506 (Jahr) = 2005 | 64506 (Name) = Sarrance
| 64507 = 112 | 64507 (Jahr) = 2007 | 64507 (Name) = Saubole
| 64508 = 118 | 64508 (Jahr) = 2004 | 64508 (Name) = Saucède
| 64509 = 181 | 64509 (Jahr) = 2004 | 64509 (Name) = Sauguis-Saint-Etienne
| 64510 = 836 | 64510 (Jahr) = 2004 | 64510 (Name) = Sault-de-Navailles
| 64511 = 2641 | 64511 (Jahr) = 2005 | 64511 (Name) = Sauvagnon
| 64512 = 207 | 64512 (Jahr) = 2004 | 64512 (Name) = Sauvelade
| 64513 = 1304 | 64513 (Jahr) = 1999 | 64513 (Name) = Sauveterre-de-Béarn
| 64514 = 176 | 64514 (Jahr) = 2006 | 64514 (Name) = Séby
| 64515 = 190 | 64515 (Jahr) = 2005 | 64515 (Name) = Sedze-Maubecq
| 64516 = 339 | 64516 (Jahr) = 1999 | 64516 (Name) = Sedzère
| 64517 = 174 | 64517 (Jahr) = 2004 | 64517 (Name) = Séméacq-Blachon
| 64519 = 3439 | 64519 (Jahr) = 2005 | 64519 (Name) = Serres-Castet
| 64520 = 685 | 64520 (Jahr) = 2007 | 64520 (Name) = Serres-Morlaàs
| 64521 = 488 | 64521 (Jahr) = 2004 | 64521 (Name) = Serres-Sainte-Marie
| 64522 = 486 | 64522 (Jahr) = 2006 | 64522 (Name) = Sévignacq-Meyracq
| 64523 = 667 | 64523 (Jahr) = 2007 | 64523 (Name) = Sévignacq
| 64524 = 342 | 64524 (Jahr) = 1999 | 64524 (Name) = Simacourbe
| 64525 = 675 | 64525 (Jahr) = 2006 | 64525 (Name) = Siros
| 64526 = 1123 | 64526 (Jahr) = 2005 | 64526 (Name) = Soumoulou
| 64527 = 1177 | 64527 (Jahr) = 2007 | 64527 (Name) = Souraïde
| 64528 = 205 | 64528 (Jahr) = 1999 | 64528 (Name) = Suhescun
| 64529 = 329 | 64529 (Jahr) = 2004 | 64529 (Name) = Sus
| 64530 = 251 | 64530 (Jahr) = 1999 | 64530 (Name) = Susmiou
| 64531 = 53 | 64531 (Jahr) = 2006 | 64531 (Name) = Tabaille-Usquain
| 64532 = 69 | 64532 (Jahr) = 2005 | 64532 (Name) = Tadousse-Ussau
| 64533 = 628 | 64533 (Jahr) = 2005 | 64533 (Name) = Tardets-Sorholus
| 64534 = 190 | 64534 (Jahr) = 2006 | 64534 (Name) = Taron-Sadirac-Viellenave
| 64535 = 496 | 64535 (Jahr) = 2007 | 64535 (Name) = Tarsacq
| 64536 = 697 | 64536 (Jahr) = 1999 | 64536 (Name) = Thèze
| 64537 = 132 | 64537 (Jahr) = 2007 | 64537 (Name) = Trois-Villes
| 64538 = 613 | 64538 (Jahr) = 2004 | 64538 (Name) = Uhart-Cize
| 64539 = 214 | 64539 (Jahr) = 2006 | 64539 (Name) = Uhart-Mixe
| 64540 = 1996 | 64540 (Jahr) = 2005 | 64540 (Name) = Urcuit
| 64541 = 266 | 64541 (Jahr) = 2007 | 64541 (Name) = Urdès
| 64542 = 67 | 64542 (Jahr) = 2005 | 64542 (Name) = Urdos
| 64543 = 365 | 64543 (Jahr) = 1999 | 64543 (Name) = Urepel
| 64544 = 60 | 64544 (Jahr) = 1999 | 64544 (Name) = Urost
| 64545 = 7759 | 64545 (Jahr) = 2007 | 64545 (Name) = Urrugne
| 64546 = 2028 | 64546 (Jahr) = 2007 | 64546 (Name) = Urt
| 64547 = 4971 | 64547 (Jahr) = 1999 | 64547 (Name) = Ustaritz
| 64548 = 151 | 64548 (Jahr) = 2005 | 64548 (Name) = Uzan
| 64549 = 1148 | 64549 (Jahr) = 2006 | 64549 (Name) = Uzein
| 64550 = 709 | 64550 (Jahr) = 1999 | 64550 (Name) = Uzos
| 64551 = 282 | 64551 (Jahr) = 2004 | 64551 (Name) = Verdets
| 64552 = 194 | 64552 (Jahr) = 2006 | 64552 (Name) = Vialer
| 64554 = 164 | 64554 (Jahr) = 2007 | 64554 (Name) = Viellenave-d'Arthez
| 64555 = 152 | 64555 (Jahr) = 2005 | 64555 (Name) = Viellenave-de-Navarrenx
| 64556 = 404 | 64556 (Jahr) = 2007 | 64556 (Name) = Vielleségure
| 64557 = 341 | 64557 (Jahr) = 1999 | 64557 (Name) = Vignes
| 64558 = 2039 | 64558 (Jahr) = 2004 | 64558 (Name) = Villefranque
| 64559 = 736 | 64559 (Jahr) = 2005 | 64559 (Name) = Viodos-Abense-de-Bas
| 64560 = 162 | 64560 (Jahr) = 2004 | 64560 (Name) = Viven
| 65001 = 244 | 65001 (Jahr) = 2006 | 65001 (Name) = Adast
| 65002 = 711 | 65002 (Jahr) = 2004 | 65002 (Name) = Adé
| 65003 = 80 | 65003 (Jahr) = 1999 | 65003 (Name) = Adervielle-Pouchergues
| 65004 = 290 | 65004 (Jahr) = 1999 | 65004 (Name) = Agos-Vidalos
| 65005 = 337 | 65005 (Jahr) = 2004 | 65005 (Name) = Allier
| 65006 = 325 | 65006 (Jahr) = 2007 | 65006 (Name) = Ancizan
| 65007 = 1303 | 65007 (Jahr) = 2005 | 65007 (Name) = Andrest
| 65009 = 181 | 65009 (Jahr) = 2007 | 65009 (Name) = Anères
| 65010 = 231 | 65010 (Jahr) = 2007 | 65010 (Name) = Angos
| 65011 = 141 | 65011 (Jahr) = 1999 | 65011 (Name) = Les Angles
| 65012 = 48 | 65012 (Jahr) = 1999 | 65012 (Name) = Anla
| 65013 = 62 | 65013 (Jahr) = 1999 | 65013 (Name) = Ansost
| 65014 = 29 | 65014 (Jahr) = 2004 | 65014 (Name) = Antichan
| 65015 = 111 | 65015 (Jahr) = 2006 | 65015 (Name) = Antin
| 65016 = 132 | 65016 (Jahr) = 2007 | 65016 (Name) = Antist
| 65017 = 260 | 65017 (Jahr) = 1999 | 65017 (Name) = Aragnouet
| 65018 = 116 | 65018 (Jahr) = 2004 | 65018 (Name) = Arbéost
| 65019 = 512 | 65019 (Jahr) = 2005 | 65019 (Name) = Arcizac-Adour
| 65020 = 191 | 65020 (Jahr) = 1999 | 65020 (Name) = Arcizac-ez-Angles
| 65021 = 352 | 65021 (Jahr) = 2006 | 65021 (Name) = Arcizans-Avant
| 65022 = 97 | 65022 (Jahr) = 2005 | 65022 (Name) = Arcizans-Dessus
| 65023 = 16 | 65023 (Jahr) = 2007 | 65023 (Name) = Ardengost
| 65024 = 135 | 65024 (Jahr) = 1999 | 65024 (Name) = Argelès
| 65025 = 3255 | 65025 (Jahr) = 2007 | 65025 (Name) = Argelès-Gazost
| 65026 = 66 | 65026 (Jahr) = 2007 | 65026 (Name) = Aries-Espénan
| 65027 = 56 | 65027 (Jahr) = 2005 | 65027 (Name) = Armenteule
| 65028 = 194 | 65028 (Jahr) = 2006 | 65028 (Name) = Arné
| 65029 = 506 | 65029 (Jahr) = 2004 | 65029 (Name) = Arras-en-Lavedan
| 65031 = 838 | 65031 (Jahr) = 2006 | 65031 (Name) = Arreau
| 65032 = 748 | 65032 (Jahr) = 2005 | 65032 (Name) = Arrens-Marsous
| 65033 = 113 | 65033 (Jahr) = 2005 | 65033 (Name) = Arrodets-ez-Angles
| 65034 = 25 | 65034 (Jahr) = 2006 | 65034 (Name) = Arrodets
| 65035 = 472 | 65035 (Jahr) = 2006 | 65035 (Name) = Artagnan
| 65036 = 110 | 65036 (Jahr) = 1999 | 65036 (Name) = Artalens-Souin
| 65037 = 75 | 65037 (Jahr) = 2004 | 65037 (Name) = Artiguemy
| 65038 = 24 | 65038 (Jahr) = 2004 | 65038 (Name) = Artigues
| 65039 = 42 | 65039 (Jahr) = 1999 | 65039 (Name) = Aspin-Aure
| 65040 = 235 | 65040 (Jahr) = 2007 | 65040 (Name) = Aspin-en-Lavedan
| 65041 = 107 | 65041 (Jahr) = 1999 | 65041 (Name) = Asque
| 65042 = 525 | 65042 (Jahr) = 2007 | 65042 (Name) = Asté
| 65043 = 301 | 65043 (Jahr) = 2005 | 65043 (Name) = Astugue
| 65044 = 240 | 65044 (Jahr) = 2007 | 65044 (Name) = Aubarède
| 65045 = 256 | 65045 (Jahr) = 2007 | 65045 (Name) = Aucun
| 65046 = 76 | 65046 (Jahr) = 2005 | 65046 (Name) = Aulon
| 65047 = 7469 | 65047 (Jahr) = 2006 | 65047 (Name) = Aureilhan
| 65048 = 721 | 65048 (Jahr) = 2006 | 65048 (Name) = Aurensan
| 65049 = 292 | 65049 (Jahr) = 2004 | 65049 (Name) = Auriébat
| 65050 = 76 | 65050 (Jahr) = 2007 | 65050 (Name) = Avajan
| 65051 = 180 | 65051 (Jahr) = 1999 | 65051 (Name) = Aventignan
| 65052 = 68 | 65052 (Jahr) = 2006 | 65052 (Name) = Averan
| 65053 = 53 | 65053 (Jahr) = 2005 | 65053 (Name) = Aveux
| 65054 = 548 | 65054 (Jahr) = 2006 | 65054 (Name) = Avezac-Prat-Lahitte
| 65055 = 298 | 65055 (Jahr) = 2005 | 65055 (Name) = Ayros-Arbouix
| 65056 = 388 | 65056 (Jahr) = 1999 | 65056 (Name) = Ayzac-Ost
| 65057 = 955 | 65057 (Jahr) = 2006 | 65057 (Name) = Azereix
| 65058 = 146 | 65058 (Jahr) = 1999 | 65058 (Name) = Azet
| 65059 = 8048 | 65059 (Jahr) = 1999 | 65059 (Name) = Bagnères-de-Bigorre
| 65060 = 50 | 65060 (Jahr) = 2005 | 65060 (Name) = Banios
| 65061 = 49 | 65061 (Jahr) = 2006 | 65061 (Name) = Barbachen
| 65062 = 3414 | 65062 (Jahr) = 2004 | 65062 (Name) = Barbazan-Debat
| 65063 = 136 | 65063 (Jahr) = 1999 | 65063 (Name) = Barbazan-Dessus
| 65064 = 64 | 65064 (Jahr) = 2006 | 65064 (Name) = Bareilles
| 65065 = 251 | 65065 (Jahr) = 2007 | 65065 (Name) = Barlest
| 65066 = 31 | 65066 (Jahr) = 2007 | 65066 (Name) = Barrancoueu
| 65067 = 128 | 65067 (Jahr) = 2005 | 65067 (Name) = Barry
| 65068 = 18 | 65068 (Jahr) = 2004 | 65068 (Name) = Barthe
| 65069 = 1153 | 65069 (Jahr) = 2004 | 65069 (Name) = La Barthe-de-Neste
| 65070 = 462 | 65070 (Jahr) = 2007 | 65070 (Name) = Bartrès
| 65071 = 38 | 65071 (Jahr) = 1999 | 65071 (Name) = Batsère
| 65072 = 1325 | 65072 (Jahr) = 2004 | 65072 (Name) = Bazet
| 65073 = 342 | 65073 (Jahr) = 2005 | 65073 (Name) = Bazillac
| 65074 = 134 | 65074 (Jahr) = 2005 | 65074 (Name) = Bazordan
| 65075 = 130 | 65075 (Jahr) = 2006 | 65075 (Name) = Bazus-Aure
| 65076 = 41 | 65076 (Jahr) = 2007 | 65076 (Name) = Bazus-Neste
| 65077 = 416 | 65077 (Jahr) = 2005 | 65077 (Name) = Beaucens
| 65078 = 378 | 65078 (Jahr) = 1999 | 65078 (Name) = Beaudéan
| 65079 = 170 | 65079 (Jahr) = 2007 | 65079 (Name) = Bégole
| 65080 = 485 | 65080 (Jahr) = 2006 | 65080 (Name) = Bénac
| 65081 = 53 | 65081 (Jahr) = 2004 | 65081 (Name) = Benqué
| 65082 = 51 | 65082 (Jahr) = 2006 | 65082 (Name) = Berbérust-Lias
| 65083 = 593 | 65083 (Jahr) = 2004 | 65083 (Name) = Bernac-Debat
| 65084 = 311 | 65084 (Jahr) = 2007 | 65084 (Name) = Bernac-Dessus
| 65085 = 106 | 65085 (Jahr) = 2006 | 65085 (Name) = Bernadets-Debat
| 65086 = 140 | 65086 (Jahr) = 2005 | 65086 (Name) = Bernadets-Dessus
| 65087 = 209 | 65087 (Jahr) = 2006 | 65087 (Name) = Bertren
| 65088 = 41 | 65088 (Jahr) = 1999 | 65088 (Name) = Betbèze
| 65089 = 112 | 65089 (Jahr) = 2006 | 65089 (Name) = Betpouey
| 65090 = 73 | 65090 (Jahr) = 2006 | 65090 (Name) = Betpouy
| 65091 = 73 | 65091 (Jahr) = 2005 | 65091 (Name) = Bettes
| 65092 = 200 | 65092 (Jahr) = 2004 | 65092 (Name) = Beyrède-Jumet
| 65093 = 199 | 65093 (Jahr) = 2004 | 65093 (Name) = Bize
| 65094 = 112 | 65094 (Jahr) = 2005 | 65094 (Name) = Bizous
| 65095 = 313 | 65095 (Jahr) = 1999 | 65095 (Name) = Bonnefont
| 65096 = 74 | 65096 (Jahr) = 2004 | 65096 (Name) = Bonnemazon
| 65097 = 182 | 65097 (Jahr) = 2005 | 65097 (Name) = Bonrepos
| 65098 = 274 | 65098 (Jahr) = 2007 | 65098 (Name) = Boô-Silhen
| 65099 = 148 | 65099 (Jahr) = 1999 | 65099 (Name) = Bordères-Louron
| 65100 = 3879 | 65100 (Jahr) = 2006 | 65100 (Name) = Bordères-sur-l'Échez
| 65101 = 600 | 65101 (Jahr) = 2005 | 65101 (Name) = Bordes
| 65102 = 17 | 65102 (Jahr) = 2007 | 65102 (Name) = Bouilh-Devant
| 65103 = 89 | 65103 (Jahr) = 2004 | 65103 (Name) = Bouilh-Péreuilh
| 65104 = 293 | 65104 (Jahr) = 2005 | 65104 (Name) = Boulin
| 65105 = 207 | 65105 (Jahr) = 2006 | 65105 (Name) = Bourg-de-Bigorre
| 65106 = 145 | 65106 (Jahr) = 2006 | 65106 (Name) = Bourisp
| 65107 = 91 | 65107 (Jahr) = 2005 | 65107 (Name) = Bourréac
| 65108 = 722 | 65108 (Jahr) = 2004 | 65108 (Name) = Bours
| 65109 = 23 | 65109 (Jahr) = 1999 | 65109 (Name) = Bramevaque
| 65110 = 87 | 65110 (Jahr) = 2004 | 65110 (Name) = Bugard
| 65111 = 70 | 65111 (Jahr) = 2006 | 65111 (Name) = Bulan
| 65112 = 123 | 65112 (Jahr) = 2004 | 65112 (Name) = Bun
| 65113 = 267 | 65113 (Jahr) = 1999 | 65113 (Name) = Burg
| 65114 = 78 | 65114 (Jahr) = 2006 | 65114 (Name) = Buzon
| 65115 = 238 | 65115 (Jahr) = 2006 | 65115 (Name) = Cabanac
| 65116 = 221 | 65116 (Jahr) = 1999 | 65116 (Name) = Cadéac
| 65117 = 52 | 65117 (Jahr) = 2004 | 65117 (Name) = Cadeilhan-Trachère
| 65118 = 28 | 65118 (Jahr) = 2006 | 65118 (Name) = Caharet
| 65119 = 383 | 65119 (Jahr) = 2005 | 65119 (Name) = Caixon
| 65120 = 207 | 65120 (Jahr) = 2005 | 65120 (Name) = Calavanté
| 65121 = 384 | 65121 (Jahr) = 2005 | 65121 (Name) = Camalès
| 65122 = 24 | 65122 (Jahr) = 2007 | 65122 (Name) = Camous
| 65123 = 1483 | 65123 (Jahr) = 1999 | 65123 (Name) = Campan
| 65124 = 58 | 65124 (Jahr) = 2006 | 65124 (Name) = Camparan
| 65125 = 315 | 65125 (Jahr) = 2004 | 65125 (Name) = Campistrous
| 65126 = 160 | 65126 (Jahr) = 1999 | 65126 (Name) = Campuzan
| 65127 = 1074 | 65127 (Jahr) = 1999 | 65127 (Name) = Capvern
| 65128 = 130 | 65128 (Jahr) = 1999 | 65128 (Name) = Castelbajac
| 65129 = 763 | 65129 (Jahr) = 2007 | 65129 (Name) = Castelnau-Magnoac
| 65130 = 695 | 65130 (Jahr) = 2006 | 65130 (Name) = Castelnau-Rivière-Basse
| 65131 = 192 | 65131 (Jahr) = 1999 | 65131 (Name) = Castelvieilh
| 65132 = 33 | 65132 (Jahr) = 2007 | 65132 (Name) = Castéra-Lanusse
| 65133 = 181 | 65133 (Jahr) = 2007 | 65133 (Name) = Castéra-Lou
| 65134 = 15 | 65134 (Jahr) = 2006 | 65134 (Name) = Casterets
| 65135 = 89 | 65135 (Jahr) = 2007 | 65135 (Name) = Castillon
| 65136 = 36 | 65136 (Jahr) = 2004 | 65136 (Name) = Caubous
| 65137 = 85 | 65137 (Jahr) = 2007 | 65137 (Name) = Caussade-Rivière
| 65138 = 1107 | 65138 (Jahr) = 2004 | 65138 (Name) = Cauterets
| 65139 = 32 | 65139 (Jahr) = 1999 | 65139 (Name) = Cazarilh
| 65140 = 20 | 65140 (Jahr) = 2005 | 65140 (Name) = Cazaux-Debat
| 65141 = 41 | 65141 (Jahr) = 2005 | 65141 (Name) = Cazaux-Fréchet-Anéran-Camors
| 65142 = 206 | 65142 (Jahr) = 1999 | 65142 (Name) = Chelle-Debat
| 65143 = 89 | 65143 (Jahr) = 2005 | 65143 (Name) = Chelle-Spou
| 65144 = 87 | 65144 (Jahr) = 2005 | 65144 (Name) = Cheust
| 65145 = 45 | 65145 (Jahr) = 2007 | 65145 (Name) = Chèze
| 65146 = 287 | 65146 (Jahr) = 2007 | 65146 (Name) = Chis
| 65147 = 563 | 65147 (Jahr) = 2007 | 65147 (Name) = Cieutat
| 65148 = 110 | 65148 (Jahr) = 2004 | 65148 (Name) = Cizos
| 65149 = 166 | 65149 (Jahr) = 2005 | 65149 (Name) = Clarac
| 65150 = 394 | 65150 (Jahr) = 1999 | 65150 (Name) = Clarens
| 65151 = 139 | 65151 (Jahr) = 2006 | 65151 (Name) = Collongues
| 65153 = 119 | 65153 (Jahr) = 2007 | 65153 (Name) = Coussan
| 65154 = 37 | 65154 (Jahr) = 2006 | 65154 (Name) = Créchets
| 65155 = 51 | 65155 (Jahr) = 1999 | 65155 (Name) = Devèze
| 65156 = 183 | 65156 (Jahr) = 1999 | 65156 (Name) = Dours
| 65157 = 25 | 65157 (Jahr) = 2007 | 65157 (Name) = Ens
| 65158 = 90 | 65158 (Jahr) = 2004 | 65158 (Name) = Esbareich
| 65159 = 376 | 65159 (Jahr) = 2005 | 65159 (Name) = Escala
| 65160 = 99 | 65160 (Jahr) = 2004 | 65160 (Name) = Escaunets
| 65161 = 243 | 65161 (Jahr) = 2007 | 65161 (Name) = Escondeaux
| 65162 = 28 | 65162 (Jahr) = 1999 | 65162 (Name) = Esconnets
| 65163 = 31 | 65163 (Jahr) = 2004 | 65163 (Name) = Escots
| 65164 = 104 | 65164 (Jahr) = 2006 | 65164 (Name) = Escoubès-Pouts
| 65165 = 177 | 65165 (Jahr) = 2006 | 65165 (Name) = Esparros
| 65166 = 53 | 65166 (Jahr) = 2006 | 65166 (Name) = Espèche
| 65167 = 45 | 65167 (Jahr) = 1999 | 65167 (Name) = Espieilh
| 65168 = 438 | 65168 (Jahr) = 2004 | 65168 (Name) = Esquièze-Sère
| 65169 = 67 | 65169 (Jahr) = 1999 | 65169 (Name) = Estaing
| 65170 = 82 | 65170 (Jahr) = 2007 | 65170 (Name) = Estampures
| 65171 = 29 | 65171 (Jahr) = 2007 | 65171 (Name) = Estarvielle
| 65172 = 39 | 65172 (Jahr) = 2007 | 65172 (Name) = Estensan
| 65173 = 207 | 65173 (Jahr) = 2007 | 65173 (Name) = Esterre
| 65174 = 101 | 65174 (Jahr) = 1999 | 65174 (Name) = Estirac
| 65175 = 51 | 65175 (Jahr) = 2005 | 65175 (Name) = Ferrère
| 65176 = 110 | 65176 (Jahr) = 1999 | 65176 (Name) = Ferrières
| 65177 = 151 | 65177 (Jahr) = 2007 | 65177 (Name) = Fontrailles
| 65178 = 35 | 65178 (Jahr) = 2007 | 65178 (Name) = Fréchède
| 65179 = 21 | 65179 (Jahr) = 1999 | 65179 (Name) = Fréchendets
| 65180 = 13 | 65180 (Jahr) = 2005 | 65180 (Name) = Fréchet-Aure
| 65181 = 111 | 65181 (Jahr) = 2004 | 65181 (Name) = Fréchou-Fréchet
| 65182 = 81 | 65182 (Jahr) = 1999 | 65182 (Name) = Gaillagos
| 65183 = 804 | 65183 (Jahr) = 1999 | 65183 (Name) = Galan
| 65184 = 145 | 65184 (Jahr) = 1999 | 65184 (Name) = Galez
| 65185 = 408 | 65185 (Jahr) = 2006 | 65185 (Name) = Gardères
| 65186 = 45 | 65186 (Jahr) = 1999 | 65186 (Name) = Gaudent
| 65187 = 119 | 65187 (Jahr) = 2004 | 65187 (Name) = Gaussan
| 65188 = 154 | 65188 (Jahr) = 2004 | 65188 (Name) = Gavarnie
| 65189 = 268 | 65189 (Jahr) = 2006 | 65189 (Name) = Gayan
| 65190 = 68 | 65190 (Jahr) = 2004 | 65190 (Name) = Gazave
| 65191 = 138 | 65191 (Jahr) = 2007 | 65191 (Name) = Gazost
| 65192 = 263 | 65192 (Jahr) = 2007 | 65192 (Name) = Gèdre
| 65193 = 66 | 65193 (Jahr) = 2005 | 65193 (Name) = Gembrie
| 65194 = 96 | 65194 (Jahr) = 2005 | 65194 (Name) = Générest
| 65195 = 151 | 65195 (Jahr) = 2007 | 65195 (Name) = Génos
| 65196 = 68 | 65196 (Jahr) = 1999 | 65196 (Name) = Gensac
| 65197 = 170 | 65197 (Jahr) = 2007 | 65197 (Name) = Ger
| 65198 = 1140 | 65198 (Jahr) = 2006 | 65198 (Name) = Gerde
| 65199 = 36 | 65199 (Jahr) = 2006 | 65199 (Name) = Germ
| 65200 = 129 | 65200 (Jahr) = 2005 | 65200 (Name) = Germs-sur-l'Oussouet
| 65201 = 153 | 65201 (Jahr) = 2004 | 65201 (Name) = Geu
| 65202 = 268 | 65202 (Jahr) = 2004 | 65202 (Name) = Gez
| 65203 = 21 | 65203 (Jahr) = 2007 | 65203 (Name) = Gez-ez-Angles
| 65204 = 26 | 65204 (Jahr) = 2004 | 65204 (Name) = Gonez
| 65205 = 68 | 65205 (Jahr) = 2006 | 65205 (Name) = Gouaux
| 65206 = 228 | 65206 (Jahr) = 2004 | 65206 (Name) = Goudon
| 65207 = 44 | 65207 (Jahr) = 2007 | 65207 (Name) = Gourgue
| 65208 = 15 | 65208 (Jahr) = 2006 | 65208 (Name) = Grailhen
| 65209 = 78 | 65209 (Jahr) = 1999 | 65209 (Name) = Grézian
| 65210 = 48 | 65210 (Jahr) = 2007 | 65210 (Name) = Grust
| 65211 = 151 | 65211 (Jahr) = 2004 | 65211 (Name) = Guchan
| 65212 = 368 | 65212 (Jahr) = 1999 | 65212 (Name) = Guchen
| 65213 = 133 | 65213 (Jahr) = 1999 | 65213 (Name) = Guizerix
| 65214 = 43 | 65214 (Jahr) = 2007 | 65214 (Name) = Hachan
| 65215 = 46 | 65215 (Jahr) = 2004 | 65215 (Name) = Hagedet
| 65216 = 84 | 65216 (Jahr) = 2005 | 65216 (Name) = Hauban
| 65217 = 43 | 65217 (Jahr) = 2006 | 65217 (Name) = Hautaget
| 65218 = 620 | 65218 (Jahr) = 2007 | 65218 (Name) = Hèches
| 65219 = 107 | 65219 (Jahr) = 2006 | 65219 (Name) = Hères
| 65220 = 220 | 65220 (Jahr) = 2006 | 65220 (Name) = Hibarette
| 65221 = 216 | 65221 (Jahr) = 2007 | 65221 (Name) = Hiis
| 65222 = 132 | 65222 (Jahr) = 2006 | 65222 (Name) = Hitte
| 65223 = 1073 | 65223 (Jahr) = 2005 | 65223 (Name) = Horgues
| 65224 = 183 | 65224 (Jahr) = 1999 | 65224 (Name) = Houeydets
| 65225 = 108 | 65225 (Jahr) = 2004 | 65225 (Name) = Hourc
| 65226 = 2663 | 65226 (Jahr) = 2004 | 65226 (Name) = Ibos
| 65228 = 131 | 65228 (Jahr) = 2005 | 65228 (Name) = Ilhet
| 65229 = 41 | 65229 (Jahr) = 2005 | 65229 (Name) = Ilheu
| 65230 = 233 | 65230 (Jahr) = 2006 | 65230 (Name) = Izaourt
| 65231 = 184 | 65231 (Jahr) = 2005 | 65231 (Name) = Izaux
| 65232 = 57 | 65232 (Jahr) = 2005 | 65232 (Name) = Jacque
| 65233 = 300 | 65233 (Jahr) = 2007 | 65233 (Name) = Jarret
| 65234 = 114 | 65234 (Jahr) = 2006 | 65234 (Name) = Jézeau
| 65235 = 3716 | 65235 (Jahr) = 2004 | 65235 (Name) = Juillan
| 65236 = 241 | 65236 (Jahr) = 1999 | 65236 (Name) = Julos
| 65237 = 193 | 65237 (Jahr) = 1999 | 65237 (Name) = Juncalas
| 65238 = 235 | 65238 (Jahr) = 1999 | 65238 (Name) = Labassère
| 65239 = 161 | 65239 (Jahr) = 1999 | 65239 (Name) = Labastide
| 65240 = 360 | 65240 (Jahr) = 1999 | 65240 (Name) = Labatut-Rivière
| 65241 = 100 | 65241 (Jahr) = 2005 | 65241 (Name) = Laborde
| 65242 = 194 | 65242 (Jahr) = 2007 | 65242 (Name) = Lacassagne
| 65243 = 431 | 65243 (Jahr) = 2005 | 65243 (Name) = Lafitole
| 65244 = 492 | 65244 (Jahr) = 2007 | 65244 (Name) = Lagarde
| 65245 = 217 | 65245 (Jahr) = 2006 | 65245 (Name) = Lagrange
| 65247 = 95 | 65247 (Jahr) = 2004 | 65247 (Name) = Arrayou-Lahitte
| 65248 = 240 | 65248 (Jahr) = 2005 | 65248 (Name) = Lahitte-Toupière
| 65249 = 73 | 65249 (Jahr) = 2007 | 65249 (Name) = Lalanne
| 65250 = 109 | 65250 (Jahr) = 1999 | 65250 (Name) = Lalanne-Trie
| 65251 = 1645 | 65251 (Jahr) = 2005 | 65251 (Name) = Laloubère
| 65252 = 645 | 65252 (Jahr) = 1999 | 65252 (Name) = Lamarque-Pontacq
| 65253 = 58 | 65253 (Jahr) = 2006 | 65253 (Name) = Lamarque-Rustaing
| 65254 = 133 | 65254 (Jahr) = 1999 | 65254 (Name) = Laméac
| 65255 = 28 | 65255 (Jahr) = 2004 | 65255 (Name) = Lançon
| 65256 = 155 | 65256 (Jahr) = 2007 | 65256 (Name) = Lanespède
| 65257 = 561 | 65257 (Jahr) = 2004 | 65257 (Name) = Lanne
| 65258 = 5762 | 65258 (Jahr) = 2007 | 65258 (Name) = Lannemezan
| 65259 = 149 | 65259 (Jahr) = 2006 | 65259 (Name) = Lansac
| 65260 = 79 | 65260 (Jahr) = 2005 | 65260 (Name) = Lapeyre
| 65261 = 41 | 65261 (Jahr) = 2006 | 65261 (Name) = Laran
| 65262 = 390 | 65262 (Jahr) = 2005 | 65262 (Name) = Larreule
| 65263 = 102 | 65263 (Jahr) = 1999 | 65263 (Name) = Larroque
| 65264 = 248 | 65264 (Jahr) = 1999 | 65264 (Name) = Lascazères
| 65265 = 301 | 65265 (Jahr) = 1999 | 65265 (Name) = Laslades
| 65266 = 37 | 65266 (Jahr) = 1999 | 65266 (Name) = Lassales
| 65267 = 490 | 65267 (Jahr) = 2007 | 65267 (Name) = Lau-Balagnas
| 65268 = 161 | 65268 (Jahr) = 2005 | 65268 (Name) = Layrisse
| 65269 = 166 | 65269 (Jahr) = 2004 | 65269 (Name) = Lescurry
| 65270 = 195 | 65270 (Jahr) = 2007 | 65270 (Name) = Lespouey
| 65271 = 379 | 65271 (Jahr) = 2005 | 65271 (Name) = Lézignan
| 65272 = 75 | 65272 (Jahr) = 2006 | 65272 (Name) = Lhez
| 65273 = 174 | 65273 (Jahr) = 2005 | 65273 (Name) = Liac
| 65274 = 158 | 65274 (Jahr) = 1999 | 65274 (Name) = Libaros
| 65275 = 76 | 65275 (Jahr) = 2007 | 65275 (Name) = Lies
| 65276 = 96 | 65276 (Jahr) = 2007 | 65276 (Name) = Lizos
| 65277 = 82 | 65277 (Jahr) = 2004 | 65277 (Name) = Lombrès
| 65278 = 41 | 65278 (Jahr) = 2007 | 65278 (Name) = Lomné
| 65279 = 199 | 65279 (Jahr) = 2006 | 65279 (Name) = Lortet
| 65280 = 373 | 65280 (Jahr) = 2005 | 65280 (Name) = Loubajac
| 65281 = 202 | 65281 (Jahr) = 1999 | 65281 (Name) = Loucrup
| 65282 = 282 | 65282 (Jahr) = 2007 | 65282 (Name) = Loudenvielle
| 65283 = 47 | 65283 (Jahr) = 2005 | 65283 (Name) = Loudervielle
| 65284 = 912 | 65284 (Jahr) = 1999 | 65284 (Name) = Louey
| 65285 = 149 | 65285 (Jahr) = 2004 | 65285 (Name) = Louit
| 65286 = 15100 | 65286 (Jahr) = 2005 | 65286 (Name) = Lourdes
| 65287 = 733 | 65287 (Jahr) = 1999 | 65287 (Name) = Loures-Barousse
| 65288 = 75 | 65288 (Jahr) = 2006 | 65288 (Name) = Lubret-Saint-Luc
| 65289 = 107 | 65289 (Jahr) = 2006 | 65289 (Name) = Luby-Betmont
| 65290 = 162 | 65290 (Jahr) = 2005 | 65290 (Name) = Luc
| 65291 = 165 | 65291 (Jahr) = 2006 | 65291 (Name) = Lugagnan
| 65292 = 337 | 65292 (Jahr) = 2007 | 65292 (Name) = Luquet
| 65293 = 106 | 65293 (Jahr) = 2005 | 65293 (Name) = Lustar
| 65294 = 182 | 65294 (Jahr) = 1999 | 65294 (Name) = Lutilhous
| 65295 = 1077 | 65295 (Jahr) = 2004 | 65295 (Name) = Luz-Saint-Sauveur
| 65296 = 476 | 65296 (Jahr) = 2005 | 65296 (Name) = Madiran
| 65297 = 54 | 65297 (Jahr) = 1999 | 65297 (Name) = Mansan
| 65298 = 67 | 65298 (Jahr) = 2004 | 65298 (Name) = Marquerie
| 65299 = 221 | 65299 (Jahr) = 2007 | 65299 (Name) = Marsac
| 65300 = 58 | 65300 (Jahr) = 2006 | 65300 (Name) = Marsas
| 65301 = 187 | 65301 (Jahr) = 2007 | 65301 (Name) = Marseillan
| 65303 = 303 | 65303 (Jahr) = 1999 | 65303 (Name) = Mascaras
| 65304 = 2478 | 65304 (Jahr) = 2004 | 65304 (Name) = Maubourguet
| 65305 = 121 | 65305 (Jahr) = 2006 | 65305 (Name) = Mauléon-Barousse
| 65306 = 225 | 65306 (Jahr) = 2007 | 65306 (Name) = Mauvezin
| 65307 = 288 | 65307 (Jahr) = 2004 | 65307 (Name) = Mazères-de-Neste
| 65308 = 121 | 65308 (Jahr) = 2007 | 65308 (Name) = Mazerolles
| 65309 = 15 | 65309 (Jahr) = 2006 | 65309 (Name) = Mazouau
| 65310 = 246 | 65310 (Jahr) = 2005 | 65310 (Name) = Mérilheu
| 65311 = 69 | 65311 (Jahr) = 1999 | 65311 (Name) = Mingot
| 65312 = 30 | 65312 (Jahr) = 1999 | 65312 (Name) = Molère
| 65313 = 605 | 65313 (Jahr) = 2005 | 65313 (Name) = Momères
| 65314 = 211 | 65314 (Jahr) = 2006 | 65314 (Name) = Monfaucon
| 65315 = 462 | 65315 (Jahr) = 2005 | 65315 (Name) = Monléon-Magnoac
| 65316 = 104 | 65316 (Jahr) = 2006 | 65316 (Name) = Monlong
| 65317 = 36 | 65317 (Jahr) = 1999 | 65317 (Name) = Mont
| 65318 = 269 | 65318 (Jahr) = 2006 | 65318 (Name) = Montastruc
| 65319 = 129 | 65319 (Jahr) = 2006 | 65319 (Name) = Montégut
| 65320 = 723 | 65320 (Jahr) = 2006 | 65320 (Name) = Montgaillard
| 65321 = 102 | 65321 (Jahr) = 2006 | 65321 (Name) = Montignac
| 65322 = 224 | 65322 (Jahr) = 2004 | 65322 (Name) = Montoussé
| 65323 = 47 | 65323 (Jahr) = 2007 | 65323 (Name) = Montsérié
| 65324 = 144 | 65324 (Jahr) = 1999 | 65324 (Name) = Moulédous
| 65325 = 57 | 65325 (Jahr) = 1999 | 65325 (Name) = Moumoulous
| 65326 = 101 | 65326 (Jahr) = 2005 | 65326 (Name) = Mun
| 65327 = 171 | 65327 (Jahr) = 2006 | 65327 (Name) = Nestier
| 65328 = 101 | 65328 (Jahr) = 2007 | 65328 (Name) = Neuilh
| 65329 = 228 | 65329 (Jahr) = 2005 | 65329 (Name) = Nistos
| 65330 = 178 | 65330 (Jahr) = 2005 | 65330 (Name) = Nouilhan
| 65331 = 3285 | 65331 (Jahr) = 1999 | 65331 (Name) = Odos
| 65332 = 99 | 65332 (Jahr) = 2007 | 65332 (Name) = Oléac-Debat
| 65333 = 89 | 65333 (Jahr) = 1999 | 65333 (Name) = Oléac-Dessus
| 65334 = 204 | 65334 (Jahr) = 1999 | 65334 (Name) = Omex
| 65335 = 412 | 65335 (Jahr) = 2004 | 65335 (Name) = Ordizan
| 65336 = 40 | 65336 (Jahr) = 2005 | 65336 (Name) = Organ
| 65337 = 110 | 65337 (Jahr) = 2004 | 65337 (Name) = Orieux
| 65338 = 248 | 65338 (Jahr) = 1999 | 65338 (Name) = Orignac
| 65339 = 300 | 65339 (Jahr) = 2006 | 65339 (Name) = Orincles
| 65340 = 1671 | 65340 (Jahr) = 2005 | 65340 (Name) = Orleix
| 65341 = 121 | 65341 (Jahr) = 2005 | 65341 (Name) = Oroix
| 65342 = 79 | 65342 (Jahr) = 2006 | 65342 (Name) = Osmets
| 65343 = 178 | 65343 (Jahr) = 2006 | 65343 (Name) = Ossen
| 65344 = 2171 | 65344 (Jahr) = 1999 | 65344 (Name) = Ossun
| 65345 = 32 | 65345 (Jahr) = 1999 | 65345 (Name) = Ossun-ez-Angles
| 65346 = 128 | 65346 (Jahr) = 2005 | 65346 (Name) = Oueilloux
| 65347 = 30 | 65347 (Jahr) = 2004 | 65347 (Name) = Ourde
| 65348 = 53 | 65348 (Jahr) = 1999 | 65348 (Name) = Ourdis-Cotdoussan
| 65349 = 6 | 65349 (Jahr) = 2004 | 65349 (Name) = Ourdon
| 65350 = 1202 | 65350 (Jahr) = 2007 | 65350 (Name) = Oursbelille
| 65351 = 46 | 65351 (Jahr) = 2005 | 65351 (Name) = Ousté
| 65352 = 194 | 65352 (Jahr) = 2007 | 65352 (Name) = Ouzous
| 65353 = 268 | 65353 (Jahr) = 2005 | 65353 (Name) = Ozon
| 65354 = 71 | 65354 (Jahr) = 2006 | 65354 (Name) = Pailhac
| 65355 = 67 | 65355 (Jahr) = 2004 | 65355 (Name) = Paréac
| 65356 = 64 | 65356 (Jahr) = 2004 | 65356 (Name) = Péré
| 65357 = 121 | 65357 (Jahr) = 2007 | 65357 (Name) = Peyraube
| 65358 = 58 | 65358 (Jahr) = 2007 | 65358 (Name) = Peyret-Saint-André
| 65359 = 24 | 65359 (Jahr) = 2006 | 65359 (Name) = Peyriguère
| 65360 = 233 | 65360 (Jahr) = 1999 | 65360 (Name) = Peyrouse
| 65361 = 81 | 65361 (Jahr) = 2005 | 65361 (Name) = Peyrun
| 65362 = 1286 | 65362 (Jahr) = 2007 | 65362 (Name) = Pierrefitte-Nestalas
| 65363 = 454 | 65363 (Jahr) = 2007 | 65363 (Name) = Pinas
| 65364 = 35 | 65364 (Jahr) = 2006 | 65364 (Name) = Pintac
| 65366 = 797 | 65366 (Jahr) = 2006 | 65366 (Name) = Poueyferré
| 65367 = 119 | 65367 (Jahr) = 2007 | 65367 (Name) = Poumarous
| 65368 = 38 | 65368 (Jahr) = 2005 | 65368 (Name) = Pouy
| 65369 = 538 | 65369 (Jahr) = 1999 | 65369 (Name) = Pouyastruc
| 65370 = 1114 | 65370 (Jahr) = 2004 | 65370 (Name) = Pouzac
| 65371 = 193 | 65371 (Jahr) = 2004 | 65371 (Name) = Préchac
| 65372 = 596 | 65372 (Jahr) = 2005 | 65372 (Name) = Pujo
| 65373 = 206 | 65373 (Jahr) = 2004 | 65373 (Name) = Puntous
| 65374 = 244 | 65374 (Jahr) = 2004 | 65374 (Name) = Puydarrieux
| 65375 = 1336 | 65375 (Jahr) = 1999 | 65375 (Name) = Rabastens-de-Bigorre
| 65376 = 188 | 65376 (Jahr) = 2006 | 65376 (Name) = Recurt
| 65377 = 164 | 65377 (Jahr) = 2005 | 65377 (Name) = Réjaumont
| 65378 = 70 | 65378 (Jahr) = 2004 | 65378 (Name) = Ricaud
| 65379 = 9 | 65379 (Jahr) = 2006 | 65379 (Name) = Ris
| 65380 = 106 | 65380 (Jahr) = 1999 | 65380 (Name) = Sabalos
| 65381 = 35 | 65381 (Jahr) = 2006 | 65381 (Name) = Sabarros
| 65382 = 61 | 65382 (Jahr) = 2005 | 65382 (Name) = Sacoué
| 65383 = 183 | 65383 (Jahr) = 2004 | 65383 (Name) = Sadournin
| 65384 = 132 | 65384 (Jahr) = 2006 | 65384 (Name) = Sailhan
| 65385 = 100 | 65385 (Jahr) = 2005 | 65385 (Name) = Saint-Arroman
| 65386 = 115 | 65386 (Jahr) = 1999 | 65386 (Name) = Saint-Créac
| 65387 = 134 | 65387 (Jahr) = 2005 | 65387 (Name) = Saint-Lanne
| 65388 = 1084 | 65388 (Jahr) = 2005 | 65388 (Name) = Saint-Lary-Soulan
| 65389 = 886 | 65389 (Jahr) = 2005 | 65389 (Name) = Saint-Laurent-de-Neste
| 65390 = 386 | 65390 (Jahr) = 2006 | 65390 (Name) = Saint-Lézer
| 65391 = 37 | 65391 (Jahr) = 2004 | 65391 (Name) = Sainte-Marie
| 65392 = 354 | 65392 (Jahr) = 2005 | 65392 (Name) = Saint-Martin
| 65393 = 93 | 65393 (Jahr) = 2004 | 65393 (Name) = Saint-Pastous
| 65394 = 260 | 65394 (Jahr) = 2007 | 65394 (Name) = Saint-Paul
| 65395 = 1261 | 65395 (Jahr) = 2005 | 65395 (Name) = Saint-Pé-de-Bigorre
| 65396 = 381 | 65396 (Jahr) = 2004 | 65396 (Name) = Saint-Savin
| 65397 = 150 | 65397 (Jahr) = 2005 | 65397 (Name) = Saint-Sever-de-Rustan
| 65398 = 216 | 65398 (Jahr) = 2005 | 65398 (Name) = Saléchan
| 65399 = 85 | 65399 (Jahr) = 2006 | 65399 (Name) = Saligos
| 65400 = 192 | 65400 (Jahr) = 2007 | 65400 (Name) = Salles
| 65401 = 403 | 65401 (Jahr) = 2005 | 65401 (Name) = Salles-Adour
| 65402 = 8 | 65402 (Jahr) = 1999 | 65402 (Name) = Samuran
| 65403 = 75 | 65403 (Jahr) = 1999 | 65403 (Name) = Sanous
| 65404 = 154 | 65404 (Jahr) = 2007 | 65404 (Name) = Sariac-Magnoac
| 65405 = 73 | 65405 (Jahr) = 2005 | 65405 (Name) = Sarlabous
| 65406 = 244 | 65406 (Jahr) = 2007 | 65406 (Name) = Sarniguet
| 65407 = 104 | 65407 (Jahr) = 1999 | 65407 (Name) = Sarp
| 65408 = 660 | 65408 (Jahr) = 2005 | 65408 (Name) = Sarrancolin
| 65409 = 290 | 65409 (Jahr) = 2006 | 65409 (Name) = Sarriac-Bigorre
| 65410 = 568 | 65410 (Jahr) = 2006 | 65410 (Name) = Sarrouilles
| 65411 = 60 | 65411 (Jahr) = 1999 | 65411 (Name) = Sassis
| 65412 = 165 | 65412 (Jahr) = 2006 | 65412 (Name) = Sauveterre
| 65413 = 128 | 65413 (Jahr) = 1999 | 65413 (Name) = Sazos
| 65414 = 88 | 65414 (Jahr) = 2004 | 65414 (Name) = Ségalas
| 65415 = 246 | 65415 (Jahr) = 2007 | 65415 (Name) = Ségus
| 65416 = 73 | 65416 (Jahr) = 2004 | 65416 (Name) = Seich
| 65417 = 5028 | 65417 (Jahr) = 2005 | 65417 (Name) = Séméac
| 65418 = 215 | 65418 (Jahr) = 1999 | 65418 (Name) = Sénac
| 65419 = 81 | 65419 (Jahr) = 2007 | 65419 (Name) = Sentous
| 65420 = 73 | 65420 (Jahr) = 2007 | 65420 (Name) = Sère-en-Lavedan
| 65421 = 53 | 65421 (Jahr) = 2007 | 65421 (Name) = Sère-Lanso
| 65422 = 270 | 65422 (Jahr) = 2007 | 65422 (Name) = Séron
| 65423 = 124 | 65423 (Jahr) = 2005 | 65423 (Name) = Sère-Rustaing
| 65424 = 91 | 65424 (Jahr) = 2007 | 65424 (Name) = Sers
| 65425 = 422 | 65425 (Jahr) = 2004 | 65425 (Name) = Siarrouy
| 65426 = 124 | 65426 (Jahr) = 1999 | 65426 (Name) = Sinzos
| 65427 = 311 | 65427 (Jahr) = 1999 | 65427 (Name) = Siradan
| 65428 = 58 | 65428 (Jahr) = 1999 | 65428 (Name) = Sireix
| 65429 = 213 | 65429 (Jahr) = 2007 | 65429 (Name) = Sombrun
| 65430 = 34 | 65430 (Jahr) = 1999 | 65430 (Name) = Soréac
| 65431 = 100 | 65431 (Jahr) = 2004 | 65431 (Name) = Sost
| 65432 = 171 | 65432 (Jahr) = 2005 | 65432 (Name) = Soublecause
| 65433 = 3023 | 65433 (Jahr) = 2006 | 65433 (Name) = Soues
| 65435 = 259 | 65435 (Jahr) = 2005 | 65435 (Name) = Soulom
| 65436 = 265 | 65436 (Jahr) = 2006 | 65436 (Name) = Souyeaux
| 65437 = 137 | 65437 (Jahr) = 2006 | 65437 (Name) = Tajan
| 65438 = 71 | 65438 (Jahr) = 2006 | 65438 (Name) = Talazac
| 65439 = 229 | 65439 (Jahr) = 1999 | 65439 (Name) = Tarasteix
| 65440 = 45800 | 65440 (Jahr) = 2005 | 65440 (Name) = Tarbes
| 65441 = 74 | 65441 (Jahr) = 2007 | 65441 (Name) = Thèbe
| 65442 = 183 | 65442 (Jahr) = 2007 | 65442 (Name) = Thermes-Magnoac
| 65443 = 15 | 65443 (Jahr) = 2005 | 65443 (Name) = Thuy
| 65444 = 247 | 65444 (Jahr) = 2006 | 65444 (Name) = Tibiran-Jaunac
| 65445 = 212 | 65445 (Jahr) = 2005 | 65445 (Name) = Tilhouse
| 65446 = 428 | 65446 (Jahr) = 2006 | 65446 (Name) = Tostat
| 65447 = 1252 | 65447 (Jahr) = 2007 | 65447 (Name) = Tournay
| 65448 = 80 | 65448 (Jahr) = 2006 | 65448 (Name) = Tournous-Darré
| 65449 = 111 | 65449 (Jahr) = 2004 | 65449 (Name) = Tournous-Devant
| 65450 = 30 | 65450 (Jahr) = 1999 | 65450 (Name) = Tramezaïgues
| 65451 = 653 | 65451 (Jahr) = 2005 | 65451 (Name) = Trébons
| 65452 = 1065 | 65452 (Jahr) = 2004 | 65452 (Name) = Trie-sur-Baïse
| 65453 = 53 | 65453 (Jahr) = 2005 | 65453 (Name) = Troubat
| 65454 = 75 | 65454 (Jahr) = 2004 | 65454 (Name) = Trouley-Labarthe
| 65455 = 437 | 65455 (Jahr) = 2004 | 65455 (Name) = Tuzaguet
| 65456 = 271 | 65456 (Jahr) = 2006 | 65456 (Name) = Uglas
| 65457 = 73 | 65457 (Jahr) = 1999 | 65457 (Name) = Ugnouas
| 65458 = 33 | 65458 (Jahr) = 2006 | 65458 (Name) = Uz
| 65459 = 109 | 65459 (Jahr) = 2006 | 65459 (Name) = Uzer
| 65460 = 4787 | 65460 (Jahr) = 1999 | 65460 (Name) = Vic-en-Bigorre
| 65461 = 89 | 65461 (Jahr) = 2006 | 65461 (Name) = Vidou
| 65462 = 259 | 65462 (Jahr) = 2006 | 65462 (Name) = Vidouze
| 65463 = 82 | 65463 (Jahr) = 2005 | 65463 (Name) = Viella
| 65464 = 478 | 65464 (Jahr) = 2006 | 65464 (Name) = Vielle-Adour
| 65465 = 358 | 65465 (Jahr) = 2004 | 65465 (Name) = Vielle-Aure
| 65466 = 73 | 65466 (Jahr) = 2005 | 65466 (Name) = Vielle-Louron
| 65467 = 80 | 65467 (Jahr) = 2004 | 65467 (Name) = Vier-Bordes
| 65468 = 50 | 65468 (Jahr) = 2006 | 65468 (Name) = Vieuzos
| 65469 = 32 | 65469 (Jahr) = 2005 | 65469 (Name) = Viey
| 65470 = 117 | 65470 (Jahr) = 2005 | 65470 (Name) = Viger
| 65471 = 189 | 65471 (Jahr) = 1999 | 65471 (Name) = Vignec
| 65472 = 80 | 65472 (Jahr) = 2005 | 65472 (Name) = Villefranque
| 65473 = 320 | 65473 (Jahr) = 2006 | 65473 (Name) = Villelongue
| 65474 = 94 | 65474 (Jahr) = 2007 | 65474 (Name) = Villembits
| 65475 = 58 | 65475 (Jahr) = 1999 | 65475 (Name) = Villemur
| 65476 = 52 | 65476 (Jahr) = 1999 | 65476 (Name) = Villenave-près-Béarn
| 65477 = 43 | 65477 (Jahr) = 1999 | 65477 (Name) = Villenave-près-Marsac
| 65478 = 44 | 65478 (Jahr) = 2006 | 65478 (Name) = Viscos
| 65479 = 331 | 65479 (Jahr) = 2004 | 65479 (Name) = Visker
| 65480 = 27 | 65480 (Jahr) = 2004 | 65480 (Name) = Vizos
| 65481 = 233 | 65481 (Jahr) = 2007 | 65481 (Name) = Barèges
| 65482 = 433 | 65482 (Jahr) = 2004 | 65482 (Name) = Cantaous
| 66001 = 67 | 66001 (Jahr) = 2004 | 66001 (Name) = L'Albère
| 66002 = 2318 | 66002 (Jahr) = 1999 | 66002 (Name) = Alénya
| 66003 = 3644 | 66003 (Jahr) = 2005 | 66003 (Name) = Amélie-les-Bains-Palalda
| 66004 = 590 | 66004 (Jahr) = 1999 | 66004 (Name) = Les Angles
| 66005 = 547 | 66005 (Jahr) = 1999 | 66005 (Name) = Angoustrine-Villeneuve-des-Escaldes
| 66006 = 181 | 66006 (Jahr) = 2006 | 66006 (Name) = Ansignan
| 66007 = 101 | 66007 (Jahr) = 2005 | 66007 (Name) = Arboussols
| 66008 = 9869 | 66008 (Jahr) = 2004 | 66008 (Name) = Argelès-sur-Mer
| 66009 = 2719 | 66009 (Jahr) = 2007 | 66009 (Name) = Arles-sur-Tech
| 66010 = 45 | 66010 (Jahr) = 2007 | 66010 (Name) = Ayguatébia-Talau
| 66011 = 3328 | 66011 (Jahr) = 1999 | 66011 (Name) = Bages
| 66012 = 2874 | 66012 (Jahr) = 2006 | 66012 (Name) = Baho
| 66013 = 58 | 66013 (Jahr) = 1999 | 66013 (Name) = Baillestavy
| 66014 = 2396 | 66014 (Jahr) = 2006 | 66014 (Name) = Baixas
| 66015 = 1007 | 66015 (Jahr) = 1999 | 66015 (Name) = Banyuls-dels-Aspres
| 66016 = 4644 | 66016 (Jahr) = 2007 | 66016 (Name) = Banyuls-sur-Mer
| 66017 = 4033 | 66017 (Jahr) = 2006 | 66017 (Name) = Le Barcarès
| 66018 = 95 | 66018 (Jahr) = 2005 | 66018 (Name) = La Bastide
| 66019 = 215 | 66019 (Jahr) = 1999 | 66019 (Name) = Bélesta
| 66020 = 779 | 66020 (Jahr) = 2004 | 66020 (Name) = Bolquère
| 66021 = 6944 | 66021 (Jahr) = 1999 | 66021 (Name) = Bompas
| 66022 = 56 | 66022 (Jahr) = 2006 | 66022 (Name) = Boule-d'Amont
| 66023 = 777 | 66023 (Jahr) = 2007 | 66023 (Name) = Bouleternère
| 66024 = 4858 | 66024 (Jahr) = 2004 | 66024 (Name) = Le Boulou
| 66025 = 1198 | 66025 (Jahr) = 2004 | 66025 (Name) = Bourg-Madame
| 66026 = 918 | 66026 (Jahr) = 2005 | 66026 (Name) = Brouilla
| 66027 = 756 | 66027 (Jahr) = 2004 | 66027 (Name) = La Cabanasse
| 66028 = 8230 | 66028 (Jahr) = 2005 | 66028 (Name) = Cabestany
| 66029 = 101 | 66029 (Jahr) = 2004 | 66029 (Name) = Caixas
| 66030 = 214 | 66030 (Jahr) = 2006 | 66030 (Name) = Calce
| 66032 = 57 | 66032 (Jahr) = 2006 | 66032 (Name) = Calmeilles
| 66033 = 437 | 66033 (Jahr) = 2005 | 66033 (Name) = Camélas
| 66034 = 106 | 66034 (Jahr) = 2007 | 66034 (Name) = Campôme
| 66035 = 41 | 66035 (Jahr) = 2005 | 66035 (Name) = Campoussy
| 66036 = 49 | 66036 (Jahr) = 2007 | 66036 (Name) = Canaveilles
| 66037 = 10182 | 66037 (Jahr) = 1999 | 66037 (Name) = Canet-en-Roussillon
| 66038 = 4831 | 66038 (Jahr) = 2007 | 66038 (Name) = Canohès
| 66039 = 143 | 66039 (Jahr) = 2005 | 66039 (Name) = Caramany
| 66040 = 34 | 66040 (Jahr) = 1999 | 66040 (Name) = Casefabre
| 66041 = 576 | 66041 (Jahr) = 2004 | 66041 (Name) = Cases-de-Pène
| 66042 = 228 | 66042 (Jahr) = 2006 | 66042 (Name) = Cassagnes
| 66043 = 130 | 66043 (Jahr) = 1999 | 66043 (Name) = Casteil
| 66044 = 373 | 66044 (Jahr) = 2004 | 66044 (Name) = Castelnou
| 66045 = 687 | 66045 (Jahr) = 2006 | 66045 (Name) = Catllar
| 66046 = 573 | 66046 (Jahr) = 2006 | 66046 (Name) = Caudiès-de-Fenouillèdes
| 66047 = 6 | 66047 (Jahr) = 1999 | 66047 (Name) = Caudiès-de-Conflent
| 66048 = 1551 | 66048 (Jahr) = 2006 | 66048 (Name) = Cerbère
| 66049 = 7568 | 66049 (Jahr) = 2006 | 66049 (Name) = Céret
| 66050 = 3469 | 66050 (Jahr) = 2007 | 66050 (Name) = Claira
| 66051 = 226 | 66051 (Jahr) = 2005 | 66051 (Name) = Clara
| 66052 = 387 | 66052 (Jahr) = 2005 | 66052 (Name) = Codalet
| 66053 = 2944 | 66053 (Jahr) = 2007 | 66053 (Name) = Collioure
| 66054 = 52 | 66054 (Jahr) = 2007 | 66054 (Name) = Conat
| 66055 = 591 | 66055 (Jahr) = 2005 | 66055 (Name) = Corbère
| 66056 = 966 | 66056 (Jahr) = 2007 | 66056 (Name) = Corbère-les-Cabanes
| 66057 = 451 | 66057 (Jahr) = 2004 | 66057 (Name) = Corneilla-de-Conflent
| 66058 = 1407 | 66058 (Jahr) = 1999 | 66058 (Name) = Corneilla-la-Rivière
| 66059 = 1938 | 66059 (Jahr) = 2006 | 66059 (Name) = Corneilla-del-Vercol
| 66060 = 198 | 66060 (Jahr) = 2006 | 66060 (Name) = Corsavy
| 66061 = 134 | 66061 (Jahr) = 1999 | 66061 (Name) = Coustouges
| 66062 = 179 | 66062 (Jahr) = 2007 | 66062 (Name) = Dorres
| 66063 = 219 | 66063 (Jahr) = 1999 | 66063 (Name) = Les Cluses
| 66064 = 475 | 66064 (Jahr) = 2004 | 66064 (Name) = Egat
| 66065 = 641 | 66065 (Jahr) = 1999 | 66065 (Name) = Elne
| 66066 = 623 | 66066 (Jahr) = 2007 | 66066 (Name) = Enveitg
| 66067 = 613 | 66067 (Jahr) = 2006 | 66067 (Name) = Err
| 66068 = 87 | 66068 (Jahr) = 2006 | 66068 (Name) = Escaro
| 66069 = 2850 | 66069 (Jahr) = 2005 | 66069 (Name) = Espira-de-l'Agly
| 66070 = 175 | 66070 (Jahr) = 2006 | 66070 (Name) = Espira-de-Conflent
| 66071 = 1902 | 66071 (Jahr) = 2007 | 66071 (Name) = Estagel
| 66072 = 498 | 66072 (Jahr) = 2005 | 66072 (Name) = Estavar
| 66073 = 154 | 66073 (Jahr) = 2007 | 66073 (Name) = Estoher
| 66074 = 396 | 66074 (Jahr) = 2005 | 66074 (Name) = Eus
| 66075 = 161 | 66075 (Jahr) = 2004 | 66075 (Name) = Eyne
| 66076 = 60 | 66076 (Jahr) = 2005 | 66076 (Name) = Felluns
| 66077 = 84 | 66077 (Jahr) = 2007 | 66077 (Name) = Fenouillet
| 66078 = 150 | 66078 (Jahr) = 2006 | 66078 (Name) = Fillols
| 66079 = 143 | 66079 (Jahr) = 2005 | 66079 (Name) = Finestret
| 66080 = 120 | 66080 (Jahr) = 2004 | 66080 (Name) = Fontpédrouse
| 66081 = 109 | 66081 (Jahr) = 2007 | 66081 (Name) = Fontrabiouse
| 66082 = 435 | 66082 (Jahr) = 2006 | 66082 (Name) = Formiguères
| 66083 = 39 | 66083 (Jahr) = 1999 | 66083 (Name) = Fosse
| 66084 = 916 | 66084 (Jahr) = 2004 | 66084 (Name) = Fourques
| 66085 = 329 | 66085 (Jahr) = 1999 | 66085 (Name) = Fuilla
| 66086 = 21 | 66086 (Jahr) = 2006 | 66086 (Name) = Glorianes
| 66088 = 4993 | 66088 (Jahr) = 1999 | 66088 (Name) = Ille-sur-Têt
| 66089 = 207 | 66089 (Jahr) = 2005 | 66089 (Name) = Joch
| 66090 = 56 | 66090 (Jahr) = 2004 | 66090 (Name) = Jujols
| 66091 = 60 | 66091 (Jahr) = 2004 | 66091 (Name) = Lamanère
| 66092 = 97 | 66092 (Jahr) = 2006 | 66092 (Name) = Lansac
| 66093 = 1941 | 66093 (Jahr) = 2006 | 66093 (Name) = Laroque-des-Albères
| 66094 = 1926 | 66094 (Jahr) = 2004 | 66094 (Name) = Latour-Bas-Elne
| 66095 = 386 | 66095 (Jahr) = 2006 | 66095 (Name) = Latour-de-Carol
| 66096 = 1046 | 66096 (Jahr) = 2005 | 66096 (Name) = Latour-de-France
| 66097 = 138 | 66097 (Jahr) = 2004 | 66097 (Name) = Lesquerde
| 66098 = 285 | 66098 (Jahr) = 2004 | 66098 (Name) = La Llagonne
| 66099 = 322 | 66099 (Jahr) = 2007 | 66099 (Name) = Llauro
| 66100 = 148 | 66100 (Jahr) = 2006 | 66100 (Name) = Llo
| 66101 = 1797 | 66101 (Jahr) = 2005 | 66101 (Name) = Llupia
| 66102 = 21 | 66102 (Jahr) = 2006 | 66102 (Name) = Mantet
| 66103 = 513 | 66103 (Jahr) = 2004 | 66103 (Name) = Marquixanes
| 66104 = 641 | 66104 (Jahr) = 2006 | 66104 (Name) = Los Masos
| 66105 = 290 | 66105 (Jahr) = 2006 | 66105 (Name) = Matemale
| 66106 = 2546 | 66106 (Jahr) = 2006 | 66106 (Name) = Maureillas-las-Illas
| 66107 = 901 | 66107 (Jahr) = 2006 | 66107 (Name) = Maury
| 66108 = 3731 | 66108 (Jahr) = 2005 | 66108 (Name) = Millas
| 66109 = 213 | 66109 (Jahr) = 2005 | 66109 (Name) = Molitg-les-Bains
| 66111 = 120 | 66111 (Jahr) = 1999 | 66111 (Name) = Montalba-le-Château
| 66112 = 202 | 66112 (Jahr) = 2006 | 66112 (Name) = Montauriol
| 66113 = 195 | 66113 (Jahr) = 2004 | 66113 (Name) = Montbolo
| 66114 = 1551 | 66114 (Jahr) = 2004 | 66114 (Name) = Montescot
| 66115 = 1045 | 66115 (Jahr) = 2004 | 66115 (Name) = Montesquieu-des-Albères
| 66116 = 202 | 66116 (Jahr) = 2006 | 66116 (Name) = Montferrer
| 66117 = 284 | 66117 (Jahr) = 2007 | 66117 (Name) = Mont-Louis
| 66118 = 290 | 66118 (Jahr) = 2005 | 66118 (Name) = Montner
| 66119 = 307 | 66119 (Jahr) = 2004 | 66119 (Name) = Mosset
| 66120 = 73 | 66120 (Jahr) = 2005 | 66120 (Name) = Nahuja
| 66121 = 943 | 66121 (Jahr) = 2006 | 66121 (Name) = Néfiach
| 66122 = 71 | 66122 (Jahr) = 2007 | 66122 (Name) = Nohèdes
| 66123 = 182 | 66123 (Jahr) = 2005 | 66123 (Name) = Nyer
| 66124 = 2007 | 66124 (Jahr) = 2005 | 66124 (Name) = Font-Romeu-Odeillo-Via
| 66125 = 402 | 66125 (Jahr) = 2004 | 66125 (Name) = Olette
| 66126 = 275 | 66126 (Jahr) = 2007 | 66126 (Name) = Oms
| 66127 = 726 | 66127 (Jahr) = 2005 | 66127 (Name) = Opoul-Périllos
| 66128 = 15 | 66128 (Jahr) = 2007 | 66128 (Name) = Oreilla
| 66129 = 1093 | 66129 (Jahr) = 1999 | 66129 (Name) = Ortaffa
| 66130 = 1253 | 66130 (Jahr) = 1999 | 66130 (Name) = Osséja
| 66132 = 496 | 66132 (Jahr) = 2004 | 66132 (Name) = Palau-de-Cerdagne
| 66133 = 2117 | 66133 (Jahr) = 1999 | 66133 (Name) = Palau-del-Vidre
| 66134 = 569 | 66134 (Jahr) = 1999 | 66134 (Name) = Passa
| 66136 = 115000 | 66136 (Jahr) = 2005 | 66136 (Name) = Perpignan
| 66137 = 578 | 66137 (Jahr) = 2004 | 66137 (Name) = Le Perthus
| 66138 = 143 | 66138 (Jahr) = 1999 | 66138 (Name) = Peyrestortes
| 66139 = 47 | 66139 (Jahr) = 2004 | 66139 (Name) = Pézilla-de-Conflent
| 66140 = 2957 | 66140 (Jahr) = 2004 | 66140 (Name) = Pézilla-la-Rivière
| 66141 = 7134 | 66141 (Jahr) = 2007 | 66141 (Name) = Pia
| 66142 = 27 | 66142 (Jahr) = 1999 | 66142 (Name) = Planès
| 66143 = 96 | 66143 (Jahr) = 2007 | 66143 (Name) = Planèzes
| 66144 = 3884 | 66144 (Jahr) = 2006 | 66144 (Name) = Pollestres
| 66145 = 2642 | 66145 (Jahr) = 2007 | 66145 (Name) = Ponteilla
| 66146 = 98 | 66146 (Jahr) = 1999 | 66146 (Name) = Porta
| 66147 = 128 | 66147 (Jahr) = 2005 | 66147 (Name) = Porté-Puymorens
| 66148 = 4579 | 66148 (Jahr) = 2004 | 66148 (Name) = Port-Vendres
| 66149 = 6221 | 66149 (Jahr) = 2006 | 66149 (Name) = Prades
| 66150 = 1141 | 66150 (Jahr) = 2006 | 66150 (Name) = Prats-de-Mollo-la-Preste
| 66151 = 70 | 66151 (Jahr) = 2005 | 66151 (Name) = Prats-de-Sournia
| 66152 = 69 | 66152 (Jahr) = 1999 | 66152 (Name) = Prugnanes
| 66153 = 68 | 66153 (Jahr) = 1999 | 66153 (Name) = Prunet-et-Belpuig
| 66154 = 93 | 66154 (Jahr) = 2004 | 66154 (Name) = Puyvalador
| 66155 = 108 | 66155 (Jahr) = 2004 | 66155 (Name) = Py
| 66156 = 85 | 66156 (Jahr) = 1999 | 66156 (Name) = Rabouillet
| 66157 = 12 | 66157 (Jahr) = 2005 | 66157 (Name) = Railleu
| 66158 = 139 | 66158 (Jahr) = 2006 | 66158 (Name) = Rasiguères
| 66159 = 42 | 66159 (Jahr) = 2007 | 66159 (Name) = Réal
| 66160 = 1258 | 66160 (Jahr) = 2004 | 66160 (Name) = Reynès
| 66161 = 1175 | 66161 (Jahr) = 2007 | 66161 (Name) = Ria-Sirach
| 66162 = 219 | 66162 (Jahr) = 1999 | 66162 (Name) = Rigarda
| 66164 = 8496 | 66164 (Jahr) = 2005 | 66164 (Name) = Rivesaltes
| 66165 = 597 | 66165 (Jahr) = 2007 | 66165 (Name) = Rodès
| 66166 = 347 | 66166 (Jahr) = 1999 | 66166 (Name) = Sahorre
| 66167 = 971 | 66167 (Jahr) = 2005 | 66167 (Name) = Saillagouse
| 66168 = 2674 | 66168 (Jahr) = 2005 | 66168 (Name) = Saint-André
| 66169 = 120 | 66169 (Jahr) = 2007 | 66169 (Name) = Saint-Arnac
| 66170 = 101 | 66170 (Jahr) = 1999 | 66170 (Name) = Sainte-Colombe-de-la-Commanderie
| 66171 = 8573 | 66171 (Jahr) = 1999 | 66171 (Name) = Saint-Cyprien
| 66172 = 11120 | 66172 (Jahr) = 2007 | 66172 (Name) = Saint-Estève
| 66173 = 654 | 66173 (Jahr) = 1999 | 66173 (Name) = Saint-Féliu-d'Amont
| 66174 = 216 | 66174 (Jahr) = 1999 | 66174 (Name) = Saint-Féliu-d'Avall
| 66175 = 2783 | 66175 (Jahr) = 2007 | 66175 (Name) = Saint-Génis-des-Fontaines
| 66176 = 1849 | 66176 (Jahr) = 1999 | 66176 (Name) = Saint-Hippolyte
| 66177 = 595 | 66177 (Jahr) = 2004 | 66177 (Name) = Saint-Jean-Lasseille
| 66178 = 1775 | 66178 (Jahr) = 1999 | 66178 (Name) = Saint-Jean-Pla-de-Corts
| 66179 = 1267 | 66179 (Jahr) = 2004 | 66179 (Name) = Saint-Laurent-de-Cerdans
| 66180 = 8224 | 66180 (Jahr) = 2004 | 66180 (Name) = Saint-Laurent-de-la-Salanque
| 66181 = 140 | 66181 (Jahr) = 1999 | 66181 (Name) = Sainte-Léocadie
| 66182 = 3842 | 66182 (Jahr) = 2004 | 66182 (Name) = Sainte-Marie
| 66183 = 103 | 66183 (Jahr) = 2007 | 66183 (Name) = Saint-Marsal
| 66184 = 47 | 66184 (Jahr) = 1999 | 66184 (Name) = Saint-Martin
| 66185 = 304 | 66185 (Jahr) = 2007 | 66185 (Name) = Saint-Michel-de-Llotes
| 66186 = 2319 | 66186 (Jahr) = 2006 | 66186 (Name) = Saint-Nazaire
| 66187 = 1938 | 66187 (Jahr) = 2005 | 66187 (Name) = Saint-Paul-de-Fenouillet
| 66188 = 231 | 66188 (Jahr) = 2005 | 66188 (Name) = Saint-Pierre-dels-Forcats
| 66189 = 4320 | 66189 (Jahr) = 2006 | 66189 (Name) = Saleilles
| 66190 = 2750 | 66190 (Jahr) = 2006 | 66190 (Name) = Salses-le-Château
| 66191 = 17 | 66191 (Jahr) = 2006 | 66191 (Name) = Sansa
| 66192 = 81 | 66192 (Jahr) = 2006 | 66192 (Name) = Sauto
| 66193 = 228 | 66193 (Jahr) = 2005 | 66193 (Name) = Serdinya
| 66194 = 262 | 66194 (Jahr) = 2006 | 66194 (Name) = Serralongue
| 66195 = 5825 | 66195 (Jahr) = 1999 | 66195 (Name) = Le Soler
| 66196 = 2926 | 66196 (Jahr) = 2006 | 66196 (Name) = Sorède
| 66197 = 36 | 66197 (Jahr) = 2007 | 66197 (Name) = Souanyas
| 66198 = 414 | 66198 (Jahr) = 2006 | 66198 (Name) = Sournia
| 66199 = 81 | 66199 (Jahr) = 2006 | 66199 (Name) = Taillet
| 66201 = 57 | 66201 (Jahr) = 2007 | 66201 (Name) = Tarerach
| 66202 = 194 | 66202 (Jahr) = 2004 | 66202 (Name) = Targassonne
| 66203 = 51 | 66203 (Jahr) = 2007 | 66203 (Name) = Taulis
| 66204 = 318 | 66204 (Jahr) = 2007 | 66204 (Name) = Taurinya
| 66205 = 903 | 66205 (Jahr) = 2007 | 66205 (Name) = Tautavel
| 66206 = 84 | 66206 (Jahr) = 1999 | 66206 (Name) = Le Tech
| 66207 = 575 | 66207 (Jahr) = 2005 | 66207 (Name) = Terrats
| 66208 = 1360 | 66208 (Jahr) = 2004 | 66208 (Name) = Théza
| 66209 = 36 | 66209 (Jahr) = 2006 | 66209 (Name) = Thuès-Entre-Valls
| 66210 = 7427 | 66210 (Jahr) = 2005 | 66210 (Name) = Thuir
| 66211 = 145 | 66211 (Jahr) = 1999 | 66211 (Name) = Tordères
| 66212 = 2956 | 66212 (Jahr) = 2005 | 66212 (Name) = Torreilles
| 66213 = 5743 | 66213 (Jahr) = 2004 | 66213 (Name) = Toulouges
| 66214 = 686 | 66214 (Jahr) = 2004 | 66214 (Name) = Tresserre
| 66215 = 111 | 66215 (Jahr) = 2007 | 66215 (Name) = Trévillach
| 66216 = 60 | 66216 (Jahr) = 2006 | 66216 (Name) = Trilla
| 66217 = 1554 | 66217 (Jahr) = 2007 | 66217 (Name) = Trouillas
| 66218 = 308 | 66218 (Jahr) = 1999 | 66218 (Name) = Ur
| 66219 = 56 | 66219 (Jahr) = 2005 | 66219 (Name) = Urbanya
| 66220 = 41 | 66220 (Jahr) = 2005 | 66220 (Name) = Valcebollère
| 66221 = 26 | 66221 (Jahr) = 2005 | 66221 (Name) = Valmanya
| 66222 = 1483 | 66222 (Jahr) = 2004 | 66222 (Name) = Vernet-les-Bains
| 66223 = 238 | 66223 (Jahr) = 2004 | 66223 (Name) = Villefranche-de-Conflent
| 66224 = 2912 | 66224 (Jahr) = 2007 | 66224 (Name) = Villelongue-de-la-Salanque
| 66225 = 1230 | 66225 (Jahr) = 2004 | 66225 (Name) = Villelongue-dels-Monts
| 66226 = 1124 | 66226 (Jahr) = 2006 | 66226 (Name) = Villemolaque
| 66227 = 3767 | 66227 (Jahr) = 2006 | 66227 (Name) = Villeneuve-de-la-Raho
| 66228 = 1292 | 66228 (Jahr) = 2006 | 66228 (Name) = Villeneuve-la-Rivière
| 66230 = 1666 | 66230 (Jahr) = 1999 | 66230 (Name) = Vinça
| 66231 = 530 | 66231 (Jahr) = 2005 | 66231 (Name) = Vingrau
| 66232 = 33 | 66232 (Jahr) = 2007 | 66232 (Name) = Vira
| 66233 = 128 | 66233 (Jahr) = 1999 | 66233 (Name) = Vivès
| 66234 = 88 | 66234 (Jahr) = 2004 | 66234 (Name) = Le Vivier
| 67001 = 2230 | 67001 (Jahr) = 2007 | 67001 (Name) = Achenheim
| 67002 = 398 | 67002 (Jahr) = 2005 | 67002 (Name) = Adamswiller
| 67003 = 473 | 67003 (Jahr) = 2006 | 67003 (Name) = Albé
| 67004 = 453 | 67004 (Jahr) = 1999 | 67004 (Name) = Allenwiller
| 67005 = 746 | 67005 (Jahr) = 1999 | 67005 (Name) = Alteckendorf
| 67006 = 215 | 67006 (Jahr) = 1999 | 67006 (Name) = Altenheim
| 67008 = 1156 | 67008 (Jahr) = 2005 | 67008 (Name) = Altorf
| 67009 = 399 | 67009 (Jahr) = 1999 | 67009 (Name) = Altwiller
| 67010 = 1788 | 67010 (Jahr) = 2004 | 67010 (Name) = Andlau
| 67011 = 848 | 67011 (Jahr) = 2005 | 67011 (Name) = Artolsheim
| 67012 = 655 | 67012 (Jahr) = 2004 | 67012 (Name) = Aschbach
| 67013 = 269 | 67013 (Jahr) = 2007 | 67013 (Name) = Asswiller
| 67014 = 731 | 67014 (Jahr) = 1999 | 67014 (Name) = Auenheim
| 67016 = 698 | 67016 (Jahr) = 2005 | 67016 (Name) = Avolsheim
| 67017 = 299 | 67017 (Jahr) = 2007 | 67017 (Name) = Baerendorf
| 67018 = 652 | 67018 (Jahr) = 2004 | 67018 (Name) = Balbronn
| 67019 = 924 | 67019 (Jahr) = 1999 | 67019 (Name) = Baldenheim
| 67020 = 891 | 67020 (Jahr) = 2004 | 67020 (Name) = Barembach
| 67021 = 6417 | 67021 (Jahr) = 2005 | 67021 (Name) = Barr
| 67022 = 260 | 67022 (Jahr) = 2005 | 67022 (Name) = Bassemberg
| 67023 = 904 | 67023 (Jahr) = 2004 | 67023 (Name) = Batzendorf
| 67025 = 1881 | 67025 (Jahr) = 2006 | 67025 (Name) = Beinheim
| 67026 = 131 | 67026 (Jahr) = 2005 | 67026 (Name) = Bellefosse
| 67027 = 156 | 67027 (Jahr) = 2004 | 67027 (Name) = Belmont
| 67028 = 5315 | 67028 (Jahr) = 2007 | 67028 (Name) = Benfeld
| 67029 = 394 | 67029 (Jahr) = 2004 | 67029 (Name) = Berg
| 67030 = 517 | 67030 (Jahr) = 1999 | 67030 (Name) = Bergbieten
| 67031 = 1411 | 67031 (Jahr) = 2007 | 67031 (Name) = Bernardswiller
| 67032 = 202 | 67032 (Jahr) = 2005 | 67032 (Name) = Bernardvillé
| 67033 = 521 | 67033 (Jahr) = 2004 | 67033 (Name) = Bernolsheim
| 67034 = 1981 | 67034 (Jahr) = 2004 | 67034 (Name) = Berstett
| 67035 = 365 | 67035 (Jahr) = 2006 | 67035 (Name) = Berstheim
| 67036 = 332 | 67036 (Jahr) = 1999 | 67036 (Name) = Bettwiller
| 67037 = 363 | 67037 (Jahr) = 2006 | 67037 (Name) = Biblisheim
| 67038 = 272 | 67038 (Jahr) = 2004 | 67038 (Name) = Bietlenheim
| 67039 = 425 | 67039 (Jahr) = 2005 | 67039 (Name) = Bilwisheim
| 67040 = 750 | 67040 (Jahr) = 1999 | 67040 (Name) = Bindernheim
| 67041 = 253 | 67041 (Jahr) = 1999 | 67041 (Name) = Birkenwald
| 67043 = 17700 | 67043 (Jahr) = 2005 | 67043 (Name) = Bischheim
| 67044 = 266 | 67044 (Jahr) = 2004 | 67044 (Name) = Bischholtz
| 67045 = 3064 | 67045 (Jahr) = 2004 | 67045 (Name) = Bischoffsheim
| 67046 = 12800 | 67046 (Jahr) = 2005 | 67046 (Name) = Bischwiller
| 67047 = 144 | 67047 (Jahr) = 2007 | 67047 (Name) = Bissert
| 67048 = 459 | 67048 (Jahr) = 2007 | 67048 (Name) = Bitschhoffen
| 67049 = 1369 | 67049 (Jahr) = 1999 | 67049 (Name) = Blaesheim
| 67050 = 29 | 67050 (Jahr) = 2005 | 67050 (Name) = Blancherupt
| 67051 = 286 | 67051 (Jahr) = 2004 | 67051 (Name) = Blienschwiller
| 67052 = 2107 | 67052 (Jahr) = 1999 | 67052 (Name) = Boersch
| 67053 = 299 | 67053 (Jahr) = 2007 | 67053 (Name) = Boesenbiesen
| 67054 = 426 | 67054 (Jahr) = 2005 | 67054 (Name) = Bolsenheim
| 67055 = 1182 | 67055 (Jahr) = 2005 | 67055 (Name) = Boofzheim
| 67056 = 563 | 67056 (Jahr) = 2005 | 67056 (Name) = Bootzheim
| 67057 = 179 | 67057 (Jahr) = 1999 | 67057 (Name) = Bosselshausen
| 67058 = 364 | 67058 (Jahr) = 2005 | 67058 (Name) = Bossendorf
| 67059 = 440 | 67059 (Jahr) = 2005 | 67059 (Name) = Bourg-Bruche
| 67060 = 482 | 67060 (Jahr) = 2007 | 67060 (Name) = Bourgheim
| 67061 = 3935 | 67061 (Jahr) = 2004 | 67061 (Name) = Bouxwiller
| 67062 = 270 | 67062 (Jahr) = 2004 | 67062 (Name) = Breitenau
| 67063 = 705 | 67063 (Jahr) = 2007 | 67063 (Name) = Breitenbach
| 67065 = 1194 | 67065 (Jahr) = 2005 | 67065 (Name) = Breuschwickersheim
| 67066 = 2725 | 67066 (Jahr) = 2004 | 67066 (Name) = La Broque
| 67067 = 9825 | 67067 (Jahr) = 2007 | 67067 (Name) = Brumath
| 67068 = 200 | 67068 (Jahr) = 2004 | 67068 (Name) = Buswiller
| 67069 = 512 | 67069 (Jahr) = 2005 | 67069 (Name) = Buhl
| 67070 = 309 | 67070 (Jahr) = 1999 | 67070 (Name) = Burbach
| 67071 = 415 | 67071 (Jahr) = 1999 | 67071 (Name) = Bust
| 67072 = 678 | 67072 (Jahr) = 1999 | 67072 (Name) = Butten
| 67073 = 3876 | 67073 (Jahr) = 2007 | 67073 (Name) = Châtenois
| 67074 = 668 | 67074 (Jahr) = 2005 | 67074 (Name) = Cleebourg
| 67075 = 518 | 67075 (Jahr) = 2006 | 67075 (Name) = Climbach
| 67076 = 455 | 67076 (Jahr) = 1999 | 67076 (Name) = Colroy-la-Roche
| 67077 = 513 | 67077 (Jahr) = 1999 | 67077 (Name) = Cosswiller
| 67078 = 187 | 67078 (Jahr) = 2004 | 67078 (Name) = Crastatt
| 67079 = 166 | 67079 (Jahr) = 2004 | 67079 (Name) = Croettwiller
| 67080 = 1439 | 67080 (Jahr) = 2006 | 67080 (Name) = Dachstein
| 67081 = 619 | 67081 (Jahr) = 2004 | 67081 (Name) = Dahlenheim
| 67082 = 974 | 67082 (Jahr) = 2007 | 67082 (Name) = Dalhunden
| 67083 = 756 | 67083 (Jahr) = 2005 | 67083 (Name) = Dambach
| 67084 = 1924 | 67084 (Jahr) = 2007 | 67084 (Name) = Dambach-la-Ville
| 67085 = 627 | 67085 (Jahr) = 2007 | 67085 (Name) = Dangolsheim
| 67086 = 370 | 67086 (Jahr) = 2007 | 67086 (Name) = Daubensand
| 67087 = 1424 | 67087 (Jahr) = 1999 | 67087 (Name) = Dauendorf
| 67088 = 386 | 67088 (Jahr) = 2007 | 67088 (Name) = Dehlingen
| 67089 = 2637 | 67089 (Jahr) = 2006 | 67089 (Name) = Dettwiller
| 67090 = 520 | 67090 (Jahr) = 2004 | 67090 (Name) = Diebolsheim
| 67091 = 347 | 67091 (Jahr) = 1999 | 67091 (Name) = Diedendorf
| 67092 = 630 | 67092 (Jahr) = 2005 | 67092 (Name) = Dieffenbach-au-Val
| 67093 = 361 | 67093 (Jahr) = 1999 | 67093 (Name) = Dieffenbach-lès-Woerth
| 67094 = 234 | 67094 (Jahr) = 2004 | 67094 (Name) = Dieffenthal
| 67095 = 1640 | 67095 (Jahr) = 2007 | 67095 (Name) = Diemeringen
| 67096 = 314 | 67096 (Jahr) = 2005 | 67096 (Name) = Dimbsthal
| 67097 = 1315 | 67097 (Jahr) = 2007 | 67097 (Name) = Dingsheim
| 67098 = 1313 | 67098 (Jahr) = 2007 | 67098 (Name) = Dinsheim-sur-Bruche
| 67099 = 300 | 67099 (Jahr) = 2006 | 67099 (Name) = Domfessel
| 67100 = 254 | 67100 (Jahr) = 2004 | 67100 (Name) = Donnenheim
| 67101 = 2408 | 67101 (Jahr) = 2007 | 67101 (Name) = Dorlisheim
| 67102 = 210 | 67102 (Jahr) = 2006 | 67102 (Name) = Dossenheim-Kochersberg
| 67103 = 1069 | 67103 (Jahr) = 1999 | 67103 (Name) = Dossenheim-sur-Zinsel
| 67104 = 955 | 67104 (Jahr) = 2004 | 67104 (Name) = Drachenbronn-Birlenbach
| 67105 = 1468 | 67105 (Jahr) = 1999 | 67105 (Name) = Drulingen
| 67106 = 4921 | 67106 (Jahr) = 2004 | 67106 (Name) = Drusenheim
| 67107 = 587 | 67107 (Jahr) = 2006 | 67107 (Name) = Duntzenheim
| 67108 = 1494 | 67108 (Jahr) = 1999 | 67108 (Name) = Duppigheim
| 67109 = 598 | 67109 (Jahr) = 2006 | 67109 (Name) = Durningen
| 67110 = 1001 | 67110 (Jahr) = 1999 | 67110 (Name) = Durrenbach
| 67111 = 394 | 67111 (Jahr) = 2004 | 67111 (Name) = Durstel
| 67112 = 2818 | 67112 (Jahr) = 2007 | 67112 (Name) = Duttlenheim
| 67113 = 387 | 67113 (Jahr) = 2006 | 67113 (Name) = Eberbach-Seltz
| 67115 = 1952 | 67115 (Jahr) = 2006 | 67115 (Name) = Ebersheim
| 67116 = 470 | 67116 (Jahr) = 2006 | 67116 (Name) = Ebersmunster
| 67117 = 475 | 67117 (Jahr) = 2006 | 67117 (Name) = Eckartswiller
| 67118 = 5937 | 67118 (Jahr) = 1999 | 67118 (Name) = Eckbolsheim
| 67119 = 1425 | 67119 (Jahr) = 2006 | 67119 (Name) = Eckwersheim
| 67120 = 511 | 67120 (Jahr) = 2007 | 67120 (Name) = Eichhoffen
| 67121 = 796 | 67121 (Jahr) = 2007 | 67121 (Name) = Elsenheim
| 67122 = 1361 | 67122 (Jahr) = 2006 | 67122 (Name) = Wangenbourg-Engenthal
| 67123 = 450 | 67123 (Jahr) = 2006 | 67123 (Name) = Engwiller
| 67124 = 1837 | 67124 (Jahr) = 2005 | 67124 (Name) = Entzheim
| 67125 = 1996 | 67125 (Jahr) = 2004 | 67125 (Name) = Epfig
| 67126 = 238 | 67126 (Jahr) = 2005 | 67126 (Name) = Erckartswiller
| 67127 = 1035 | 67127 (Jahr) = 2007 | 67127 (Name) = Ergersheim
| 67128 = 1564 | 67128 (Jahr) = 2007 | 67128 (Name) = Ernolsheim-Bruche
| 67129 = 597 | 67129 (Jahr) = 2005 | 67129 (Name) = Ernolsheim-lès-Saverne
| 67130 = 9632 | 67130 (Jahr) = 2004 | 67130 (Name) = Erstein
| 67131 = 4758 | 67131 (Jahr) = 2006 | 67131 (Name) = Eschau
| 67132 = 916 | 67132 (Jahr) = 2007 | 67132 (Name) = Eschbach
| 67133 = 484 | 67133 (Jahr) = 2006 | 67133 (Name) = Eschbourg
| 67134 = 165 | 67134 (Jahr) = 2006 | 67134 (Name) = Eschwiller
| 67135 = 811 | 67135 (Jahr) = 2007 | 67135 (Name) = Ettendorf
| 67136 = 267 | 67136 (Jahr) = 2004 | 67136 (Name) = Eywiller
| 67137 = 4846 | 67137 (Jahr) = 2004 | 67137 (Name) = Fegersheim
| 67138 = 432 | 67138 (Jahr) = 2005 | 67138 (Name) = Fessenheim-le-Bas
| 67139 = 453 | 67139 (Jahr) = 2005 | 67139 (Name) = Flexbourg
| 67140 = 667 | 67140 (Jahr) = 2004 | 67140 (Name) = Forstfeld
| 67141 = 540 | 67141 (Jahr) = 2004 | 67141 (Name) = Forstheim
| 67142 = 279 | 67142 (Jahr) = 2004 | 67142 (Name) = Fort-Louis
| 67143 = 605 | 67143 (Jahr) = 2005 | 67143 (Name) = Fouchy
| 67144 = 303 | 67144 (Jahr) = 1999 | 67144 (Name) = Fouday
| 67145 = 231 | 67145 (Jahr) = 2005 | 67145 (Name) = Friedolsheim
| 67146 = 628 | 67146 (Jahr) = 2005 | 67146 (Name) = Friesenheim
| 67147 = 525 | 67147 (Jahr) = 2004 | 67147 (Name) = Froeschwiller
| 67148 = 191 | 67148 (Jahr) = 2005 | 67148 (Name) = Frohmuhl
| 67149 = 378 | 67149 (Jahr) = 2007 | 67149 (Name) = Furchhausen
| 67150 = 1006 | 67150 (Jahr) = 1999 | 67150 (Name) = Furdenheim
| 67151 = 4377 | 67151 (Jahr) = 2007 | 67151 (Name) = Gambsheim
| 67152 = 7031 | 67152 (Jahr) = 1999 | 67152 (Name) = Geispolsheim
| 67153 = 200 | 67153 (Jahr) = 2005 | 67153 (Name) = Geiswiller
| 67154 = 2802 | 67154 (Jahr) = 2005 | 67154 (Name) = Gerstheim
| 67155 = 897 | 67155 (Jahr) = 2006 | 67155 (Name) = Gertwiller
| 67156 = 2273 | 67156 (Jahr) = 2006 | 67156 (Name) = Geudertheim
| 67158 = 318 | 67158 (Jahr) = 2007 | 67158 (Name) = Gingsheim
| 67159 = 206 | 67159 (Jahr) = 1999 | 67159 (Name) = Goerlingen
| 67160 = 1076 | 67160 (Jahr) = 2004 | 67160 (Name) = Goersdorf
| 67161 = 373 | 67161 (Jahr) = 2006 | 67161 (Name) = Gottenhouse
| 67162 = 301 | 67162 (Jahr) = 1999 | 67162 (Name) = Gottesheim
| 67163 = 407 | 67163 (Jahr) = 2004 | 67163 (Name) = Gougenheim
| 67164 = 785 | 67164 (Jahr) = 2004 | 67164 (Name) = Goxwiller
| 67165 = 385 | 67165 (Jahr) = 2007 | 67165 (Name) = Grandfontaine
| 67166 = 182 | 67166 (Jahr) = 2006 | 67166 (Name) = Grassendorf
| 67167 = 1211 | 67167 (Jahr) = 2005 | 67167 (Name) = Grendelbruch
| 67168 = 1408 | 67168 (Jahr) = 2005 | 67168 (Name) = Gresswiller
| 67169 = 2763 | 67169 (Jahr) = 2005 | 67169 (Name) = Gries
| 67172 = 1904 | 67172 (Jahr) = 2006 | 67172 (Name) = Griesheim-près-Molsheim
| 67173 = 1142 | 67173 (Jahr) = 2005 | 67173 (Name) = Griesheim-sur-Souffel
| 67174 = 1227 | 67174 (Jahr) = 2006 | 67174 (Name) = Gumbrechtshoffen
| 67176 = 349 | 67176 (Jahr) = 1999 | 67176 (Name) = Gundershoffen
| 67177 = 700 | 67177 (Jahr) = 2004 | 67177 (Name) = Gunstett
| 67178 = 267 | 67178 (Jahr) = 2004 | 67178 (Name) = Gungwiller
| 67179 = 650 | 67179 (Jahr) = 2004 | 67179 (Name) = Haegen
| 67180 = 35100 | 67180 (Jahr) = 2005 | 67180 (Name) = Haguenau
| 67181 = 300 | 67181 (Jahr) = 2005 | 67181 (Name) = Handschuheim
| 67182 = 1440 | 67182 (Jahr) = 2004 | 67182 (Name) = Hangenbieten
| 67183 = 842 | 67183 (Jahr) = 2006 | 67183 (Name) = Harskirchen
| 67184 = 1893 | 67184 (Jahr) = 2005 | 67184 (Name) = Hatten
| 67185 = 632 | 67185 (Jahr) = 2006 | 67185 (Name) = Hattmatt
| 67186 = 341 | 67186 (Jahr) = 2004 | 67186 (Name) = Hegeney
| 67187 = 443 | 67187 (Jahr) = 2004 | 67187 (Name) = Heidolsheim
| 67188 = 562 | 67188 (Jahr) = 1999 | 67188 (Name) = Heiligenberg
| 67189 = 974 | 67189 (Jahr) = 2006 | 67189 (Name) = Heiligenstein
| 67190 = 174 | 67190 (Jahr) = 2005 | 67190 (Name) = Hengwiller
| 67191 = 1899 | 67191 (Jahr) = 2005 | 67191 (Name) = Herbitzheim
| 67192 = 793 | 67192 (Jahr) = 2006 | 67192 (Name) = Herbsheim
| 67194 = 4438 | 67194 (Jahr) = 2005 | 67194 (Name) = Herrlisheim
| 67195 = 510 | 67195 (Jahr) = 2004 | 67195 (Name) = Hessenheim
| 67196 = 2362 | 67196 (Jahr) = 2006 | 67196 (Name) = Hilsenheim
| 67197 = 1362 | 67197 (Jahr) = 2007 | 67197 (Name) = Hindisheim
| 67198 = 106 | 67198 (Jahr) = 1999 | 67198 (Name) = Hinsbourg
| 67199 = 84 | 67199 (Jahr) = 2004 | 67199 (Name) = Hinsingen
| 67200 = 768 | 67200 (Jahr) = 2004 | 67200 (Name) = Hipsheim
| 67201 = 331 | 67201 (Jahr) = 2004 | 67201 (Name) = Hirschland
| 67202 = 3036 | 67202 (Jahr) = 2006 | 67202 (Name) = Hochfelden
| 67203 = 326 | 67203 (Jahr) = 2006 | 67203 (Name) = Hochstett
| 67204 = 10700 | 67204 (Jahr) = 2005 | 67204 (Name) = Hoenheim
| 67205 = 4337 | 67205 (Jahr) = 2005 | 67205 (Name) = Hoerdt
| 67206 = 1214 | 67206 (Jahr) = 2004 | 67206 (Name) = Hoffen
| 67207 = 212 | 67207 (Jahr) = 2007 | 67207 (Name) = Hohatzenheim
| 67208 = 480 | 67208 (Jahr) = 1999 | 67208 (Name) = Hohengoeft
| 67209 = 271 | 67209 (Jahr) = 1999 | 67209 (Name) = Hohfrankenheim
| 67210 = 386 | 67210 (Jahr) = 1999 | 67210 (Name) = Le Hohwald
| 67212 = 275 | 67212 (Jahr) = 1999 | 67212 (Name) = Holtzheim
| 67213 = 671 | 67213 (Jahr) = 1999 | 67213 (Name) = Hunspach
| 67214 = 450 | 67214 (Jahr) = 1999 | 67214 (Name) = Hurtigheim
| 67215 = 421 | 67215 (Jahr) = 2004 | 67215 (Name) = Huttendorf
| 67216 = 2220 | 67216 (Jahr) = 2004 | 67216 (Name) = Huttenheim
| 67217 = 293 | 67217 (Jahr) = 2007 | 67217 (Name) = Ichtratzheim
| 67218 = 25600 | 67218 (Jahr) = 2005 | 67218 (Name) = Illkirch-Graffenstaden
| 67220 = 360 | 67220 (Jahr) = 2006 | 67220 (Name) = Ingenheim
| 67221 = 290 | 67221 (Jahr) = 2004 | 67221 (Name) = Ingolsheim
| 67222 = 3988 | 67222 (Jahr) = 2006 | 67222 (Name) = Ingwiller
| 67223 = 1027 | 67223 (Jahr) = 2006 | 67223 (Name) = Innenheim
| 67225 = 95 | 67225 (Jahr) = 2007 | 67225 (Name) = Issenhausen
| 67226 = 2063 | 67226 (Jahr) = 2004 | 67226 (Name) = Ittenheim
| 67227 = 275 | 67227 (Jahr) = 2006 | 67227 (Name) = Itterswiller
| 67228 = 698 | 67228 (Jahr) = 2007 | 67228 (Name) = Neugartheim-Ittlenheim
| 67229 = 170 | 67229 (Jahr) = 1999 | 67229 (Name) = Jetterswiller
| 67230 = 1897 | 67230 (Jahr) = 1999 | 67230 (Name) = Kaltenhouse
| 67231 = 209 | 67231 (Jahr) = 2007 | 67231 (Name) = Kauffenheim
| 67232 = 214 | 67232 (Jahr) = 2004 | 67232 (Name) = Keffenach
| 67233 = 1121 | 67233 (Jahr) = 1999 | 67233 (Name) = Kertzfeld
| 67234 = 1470 | 67234 (Jahr) = 2005 | 67234 (Name) = Keskastel
| 67235 = 402 | 67235 (Jahr) = 2005 | 67235 (Name) = Kesseldorf
| 67236 = 576 | 67236 (Jahr) = 2007 | 67236 (Name) = Kienheim
| 67237 = 1923 | 67237 (Jahr) = 1999 | 67237 (Name) = Kilstett
| 67238 = 560 | 67238 (Jahr) = 2006 | 67238 (Name) = Kindwiller
| 67239 = 1509 | 67239 (Jahr) = 2006 | 67239 (Name) = Kintzheim
| 67240 = 543 | 67240 (Jahr) = 2006 | 67240 (Name) = Kirchheim
| 67241 = 151 | 67241 (Jahr) = 1999 | 67241 (Name) = Kirrberg
| 67242 = 514 | 67242 (Jahr) = 1999 | 67242 (Name) = Kirrwiller
| 67244 = 131 | 67244 (Jahr) = 2004 | 67244 (Name) = Kleingoeft
| 67245 = 195 | 67245 (Jahr) = 2005 | 67245 (Name) = Knoersheim
| 67246 = 1079 | 67246 (Jahr) = 2007 | 67246 (Name) = Kogenheim
| 67247 = 827 | 67247 (Jahr) = 2005 | 67247 (Name) = Kolbsheim
| 67248 = 1584 | 67248 (Jahr) = 2004 | 67248 (Name) = Krautergersheim
| 67249 = 191 | 67249 (Jahr) = 2005 | 67249 (Name) = Krautwiller
| 67250 = 706 | 67250 (Jahr) = 2006 | 67250 (Name) = Kriegsheim
| 67252 = 922 | 67252 (Jahr) = 2007 | 67252 (Name) = Kurtzenhouse
| 67253 = 649 | 67253 (Jahr) = 2005 | 67253 (Name) = Kuttolsheim
| 67254 = 837 | 67254 (Jahr) = 2007 | 67254 (Name) = Kutzenhausen
| 67255 = 439 | 67255 (Jahr) = 2006 | 67255 (Name) = Lalaye
| 67256 = 3089 | 67256 (Jahr) = 2005 | 67256 (Name) = Lampertheim
| 67257 = 680 | 67257 (Jahr) = 2004 | 67257 (Name) = Lampertsloch
| 67258 = 131 | 67258 (Jahr) = 1999 | 67258 (Name) = Landersheim
| 67259 = 960 | 67259 (Jahr) = 2005 | 67259 (Name) = Langensoultzbach
| 67260 = 275 | 67260 (Jahr) = 1999 | 67260 (Name) = Laubach
| 67261 = 2247 | 67261 (Jahr) = 2004 | 67261 (Name) = Lauterbourg
| 67263 = 1728 | 67263 (Jahr) = 2007 | 67263 (Name) = Lembach
| 67264 = 845 | 67264 (Jahr) = 2007 | 67264 (Name) = Leutenheim
| 67265 = 550 | 67265 (Jahr) = 2005 | 67265 (Name) = Lichtenberg
| 67266 = 584 | 67266 (Jahr) = 2007 | 67266 (Name) = Limersheim
| 67267 = 17100 | 67267 (Jahr) = 2005 | 67267 (Name) = Lingolsheim
| 67268 = 2527 | 67268 (Jahr) = 2004 | 67268 (Name) = Lipsheim
| 67269 = 252 | 67269 (Jahr) = 2005 | 67269 (Name) = Littenheim
| 67270 = 323 | 67270 (Jahr) = 2006 | 67270 (Name) = Lixhausen
| 67271 = 586 | 67271 (Jahr) = 2006 | 67271 (Name) = Lobsann
| 67272 = 353 | 67272 (Jahr) = 2004 | 67272 (Name) = Lochwiller
| 67273 = 510 | 67273 (Jahr) = 2007 | 67273 (Name) = Lohr
| 67274 = 211 | 67274 (Jahr) = 2006 | 67274 (Name) = Lorentzen
| 67275 = 775 | 67275 (Jahr) = 2005 | 67275 (Name) = Lupstein
| 67276 = 1813 | 67276 (Jahr) = 2006 | 67276 (Name) = Lutzelhouse
| 67277 = 680 | 67277 (Jahr) = 2005 | 67277 (Name) = Mackenheim
| 67278 = 619 | 67278 (Jahr) = 2004 | 67278 (Name) = Mackwiller
| 67279 = 172 | 67279 (Jahr) = 2005 | 67279 (Name) = Maennolsheim
| 67280 = 832 | 67280 (Jahr) = 2007 | 67280 (Name) = Maisonsgoutte
| 67281 = 4130 | 67281 (Jahr) = 2006 | 67281 (Name) = Marckolsheim
| 67282 = 3365 | 67282 (Jahr) = 1999 | 67282 (Name) = Marlenheim
| 67283 = 2688 | 67283 (Jahr) = 2007 | 67283 (Name) = Marmoutier
| 67285 = 1377 | 67285 (Jahr) = 2006 | 67285 (Name) = Matzenheim
| 67286 = 1302 | 67286 (Jahr) = 1999 | 67286 (Name) = Meistratzheim
| 67287 = 581 | 67287 (Jahr) = 2006 | 67287 (Name) = Melsheim
| 67288 = 323 | 67288 (Jahr) = 2007 | 67288 (Name) = Memmelshoffen
| 67289 = 511 | 67289 (Jahr) = 2007 | 67289 (Name) = Menchhoffen
| 67290 = 859 | 67290 (Jahr) = 2005 | 67290 (Name) = Merkwiller-Pechelbronn
| 67291 = 3507 | 67291 (Jahr) = 1999 | 67291 (Name) = Mertzwiller
| 67292 = 554 | 67292 (Jahr) = 1999 | 67292 (Name) = Mietesheim
| 67293 = 510 | 67293 (Jahr) = 1999 | 67293 (Name) = Minversheim
| 67295 = 653 | 67295 (Jahr) = 2005 | 67295 (Name) = Mittelbergheim
| 67296 = 1738 | 67296 (Jahr) = 2004 | 67296 (Name) = Mittelhausbergen
| 67297 = 546 | 67297 (Jahr) = 2005 | 67297 (Name) = Mittelhausen
| 67298 = 472 | 67298 (Jahr) = 2005 | 67298 (Name) = Mittelschaeffolsheim
| 67299 = 920 | 67299 (Jahr) = 2005 | 67299 (Name) = Mollkirch
| 67300 = 9452 | 67300 (Jahr) = 2005 | 67300 (Name) = Molsheim
| 67301 = 1813 | 67301 (Jahr) = 2005 | 67301 (Name) = Mommenheim
| 67302 = 1799 | 67302 (Jahr) = 1999 | 67302 (Name) = Monswiller
| 67303 = 568 | 67303 (Jahr) = 2005 | 67303 (Name) = Morsbronn-les-Bains
| 67304 = 551 | 67304 (Jahr) = 2006 | 67304 (Name) = Morschwiller
| 67305 = 1933 | 67305 (Jahr) = 1999 | 67305 (Name) = Mothern
| 67306 = 636 | 67306 (Jahr) = 2007 | 67306 (Name) = Muhlbach-sur-Bruche
| 67307 = 410 | 67307 (Jahr) = 2005 | 67307 (Name) = Mulhausen
| 67308 = 692 | 67308 (Jahr) = 1999 | 67308 (Name) = Munchhausen
| 67309 = 5050 | 67309 (Jahr) = 2006 | 67309 (Name) = Mundolsheim
| 67310 = 1088 | 67310 (Jahr) = 2006 | 67310 (Name) = Mussig
| 67311 = 1870 | 67311 (Jahr) = 2005 | 67311 (Name) = Muttersholtz
| 67312 = 419 | 67312 (Jahr) = 2006 | 67312 (Name) = Mutzenhouse
| 67313 = 5976 | 67313 (Jahr) = 2005 | 67313 (Name) = Mutzig
| 67314 = 599 | 67314 (Jahr) = 2006 | 67314 (Name) = Natzwiller
| 67315 = 661 | 67315 (Jahr) = 2007 | 67315 (Name) = Neewiller-près-Lauterbourg
| 67317 = 652 | 67317 (Jahr) = 2005 | 67317 (Name) = Neubois
| 67319 = 326 | 67319 (Jahr) = 2004 | 67319 (Name) = Neuhaeusel
| 67320 = 620 | 67320 (Jahr) = 1999 | 67320 (Name) = Neuve-Eglise
| 67321 = 401 | 67321 (Jahr) = 2007 | 67321 (Name) = Neuviller-la-Roche
| 67322 = 1134 | 67322 (Jahr) = 2004 | 67322 (Name) = Neuwiller-lès-Saverne
| 67324 = 4329 | 67324 (Jahr) = 2005 | 67324 (Name) = Niederbronn-les-Bains
| 67325 = 1391 | 67325 (Jahr) = 2005 | 67325 (Name) = Niederhaslach
| 67326 = 138 | 67326 (Jahr) = 1999 | 67326 (Name) = Niederhausbergen
| 67327 = 919 | 67327 (Jahr) = 2004 | 67327 (Name) = Niederlauterbach
| 67328 = 724 | 67328 (Jahr) = 2004 | 67328 (Name) = Niedermodern
| 67329 = 1244 | 67329 (Jahr) = 2007 | 67329 (Name) = Niedernai
| 67330 = 871 | 67330 (Jahr) = 2006 | 67330 (Name) = Niederroedern
| 67331 = 1248 | 67331 (Jahr) = 2006 | 67331 (Name) = Niederschaeffolsheim
| 67333 = 272 | 67333 (Jahr) = 1999 | 67333 (Name) = Niedersoultzbach
| 67334 = 139 | 67334 (Jahr) = 2004 | 67334 (Name) = Niedersteinbach
| 67335 = 785 | 67335 (Jahr) = 2006 | 67335 (Name) = Nordheim
| 67336 = 1518 | 67336 (Jahr) = 2004 | 67336 (Name) = Nordhouse
| 67337 = 457 | 67337 (Jahr) = 2007 | 67337 (Name) = Nothalten
| 67338 = 1214 | 67338 (Jahr) = 1999 | 67338 (Name) = Obenheim
| 67339 = 3727 | 67339 (Jahr) = 1999 | 67339 (Name) = Betschdorf
| 67340 = 1506 | 67340 (Jahr) = 2006 | 67340 (Name) = Oberbronn
| 67341 = 350 | 67341 (Jahr) = 2005 | 67341 (Name) = Oberdorf-Spachbach
| 67342 = 1773 | 67342 (Jahr) = 2007 | 67342 (Name) = Oberhaslach
| 67343 = 4397 | 67343 (Jahr) = 2006 | 67343 (Name) = Oberhausbergen
| 67344 = 279 | 67344 (Jahr) = 1999 | 67344 (Name) = Oberhoffen-lès-Wissembourg
| 67345 = 3123 | 67345 (Jahr) = 2007 | 67345 (Name) = Oberhoffen-sur-Moder
| 67346 = 520 | 67346 (Jahr) = 2007 | 67346 (Name) = Oberlauterbach
| 67347 = 1526 | 67347 (Jahr) = 2006 | 67347 (Name) = Obermodern-Zutzendorf
| 67348 = 10800 | 67348 (Jahr) = 2005 | 67348 (Name) = Obernai
| 67349 = 480 | 67349 (Jahr) = 2006 | 67349 (Name) = Oberroedern
| 67350 = 2052 | 67350 (Jahr) = 2004 | 67350 (Name) = Oberschaeffolsheim
| 67351 = 1752 | 67351 (Jahr) = 2006 | 67351 (Name) = Seebach
| 67352 = 389 | 67352 (Jahr) = 2005 | 67352 (Name) = Obersoultzbach
| 67353 = 225 | 67353 (Jahr) = 2006 | 67353 (Name) = Obersteinbach
| 67354 = 445 | 67354 (Jahr) = 2005 | 67354 (Name) = Odratzheim
| 67355 = 1220 | 67355 (Jahr) = 2005 | 67355 (Name) = Oermingen
| 67356 = 2024 | 67356 (Jahr) = 2006 | 67356 (Name) = Offendorf
| 67358 = 826 | 67358 (Jahr) = 2005 | 67358 (Name) = Offwiller
| 67359 = 1263 | 67359 (Jahr) = 1999 | 67359 (Name) = Ohlungen
| 67360 = 721 | 67360 (Jahr) = 1999 | 67360 (Name) = Ohnenheim
| 67361 = 487 | 67361 (Jahr) = 2007 | 67361 (Name) = Olwisheim
| 67362 = 564 | 67362 (Jahr) = 2006 | 67362 (Name) = Orschwiller
| 67363 = 683 | 67363 (Jahr) = 1999 | 67363 (Name) = Osthoffen
| 67364 = 946 | 67364 (Jahr) = 1999 | 67364 (Name) = Osthouse
| 67365 = 10500 | 67365 (Jahr) = 2005 | 67365 (Name) = Ostwald
| 67366 = 750 | 67366 (Jahr) = 2004 | 67366 (Name) = Ottersthal
| 67367 = 1177 | 67367 (Jahr) = 1999 | 67367 (Name) = Otterswiller
| 67368 = 1513 | 67368 (Jahr) = 1999 | 67368 (Name) = Ottrott
| 67369 = 217 | 67369 (Jahr) = 2006 | 67369 (Name) = Ottwiller
| 67370 = 688 | 67370 (Jahr) = 2004 | 67370 (Name) = Petersbach
| 67371 = 604 | 67371 (Jahr) = 2007 | 67371 (Name) = La Petite-Pierre
| 67372 = 2677 | 67372 (Jahr) = 2005 | 67372 (Name) = Pfaffenhoffen
| 67373 = 296 | 67373 (Jahr) = 2004 | 67373 (Name) = Pfalzweyer
| 67374 = 752 | 67374 (Jahr) = 1999 | 67374 (Name) = Pfettisheim
| 67375 = 1281 | 67375 (Jahr) = 2007 | 67375 (Name) = Pfulgriesheim
| 67377 = 795 | 67377 (Jahr) = 1999 | 67377 (Name) = Plaine
| 67378 = 3651 | 67378 (Jahr) = 2006 | 67378 (Name) = Plobsheim
| 67379 = 926 | 67379 (Jahr) = 2005 | 67379 (Name) = Preuschdorf
| 67380 = 211 | 67380 (Jahr) = 1999 | 67380 (Name) = Printzheim
| 67381 = 346 | 67381 (Jahr) = 1999 | 67381 (Name) = Puberg
| 67382 = 717 | 67382 (Jahr) = 2004 | 67382 (Name) = Quatzenheim
| 67383 = 151 | 67383 (Jahr) = 2004 | 67383 (Name) = Rangen
| 67384 = 293 | 67384 (Jahr) = 1999 | 67384 (Name) = Ranrupt
| 67385 = 259 | 67385 (Jahr) = 2005 | 67385 (Name) = Ratzwiller
| 67386 = 212 | 67386 (Jahr) = 2007 | 67386 (Name) = Rauwiller
| 67387 = 308 | 67387 (Jahr) = 2007 | 67387 (Name) = Reichsfeld
| 67388 = 5470 | 67388 (Jahr) = 2004 | 67388 (Name) = Reichshoffen
| 67389 = 4882 | 67389 (Jahr) = 1999 | 67389 (Name) = Reichstett
| 67391 = 450 | 67391 (Jahr) = 2007 | 67391 (Name) = Reinhardsmunster
| 67392 = 961 | 67392 (Jahr) = 2005 | 67392 (Name) = Reipertswiller
| 67394 = 240 | 67394 (Jahr) = 2005 | 67394 (Name) = Retschwiller
| 67395 = 322 | 67395 (Jahr) = 2004 | 67395 (Name) = Reutenbourg
| 67396 = 183 | 67396 (Jahr) = 2004 | 67396 (Name) = Rexingen
| 67397 = 2613 | 67397 (Jahr) = 2007 | 67397 (Name) = Rhinau
| 67398 = 342 | 67398 (Jahr) = 2004 | 67398 (Name) = Richtolsheim
| 67400 = 1120 | 67400 (Jahr) = 2005 | 67400 (Name) = Riedseltz
| 67401 = 262 | 67401 (Jahr) = 2007 | 67401 (Name) = Rimsdorf
| 67402 = 84 | 67402 (Jahr) = 2007 | 67402 (Name) = Ringeldorf
| 67403 = 359 | 67403 (Jahr) = 1999 | 67403 (Name) = Ringendorf
| 67404 = 917 | 67404 (Jahr) = 2004 | 67404 (Name) = Rittershoffen
| 67405 = 1905 | 67405 (Jahr) = 1999 | 67405 (Name) = Roeschwoog
| 67406 = 269 | 67406 (Jahr) = 1999 | 67406 (Name) = Rohr
| 67407 = 1611 | 67407 (Jahr) = 2007 | 67407 (Name) = Rohrwiller
| 67408 = 1318 | 67408 (Jahr) = 2007 | 67408 (Name) = Romanswiller
| 67409 = 956 | 67409 (Jahr) = 2005 | 67409 (Name) = Roppenheim
| 67410 = 681 | 67410 (Jahr) = 2004 | 67410 (Name) = Rosenwiller
| 67411 = 4721 | 67411 (Jahr) = 2007 | 67411 (Name) = Rosheim
| 67412 = 772 | 67412 (Jahr) = 2004 | 67412 (Name) = Rossfeld
| 67413 = 569 | 67413 (Jahr) = 2006 | 67413 (Name) = Rosteig
| 67414 = 1581 | 67414 (Jahr) = 2005 | 67414 (Name) = Rothau
| 67415 = 484 | 67415 (Jahr) = 2005 | 67415 (Name) = Rothbach
| 67416 = 446 | 67416 (Jahr) = 2004 | 67416 (Name) = Rott
| 67417 = 292 | 67417 (Jahr) = 1999 | 67417 (Name) = Rottelsheim
| 67418 = 1013 | 67418 (Jahr) = 2006 | 67418 (Name) = Rountzenheim
| 67420 = 1182 | 67420 (Jahr) = 1999 | 67420 (Name) = Russ
| 67421 = 932 | 67421 (Jahr) = 2005 | 67421 (Name) = Saales
| 67422 = 579 | 67422 (Jahr) = 2007 | 67422 (Name) = Saasenheim
| 67423 = 490 | 67423 (Jahr) = 2007 | 67423 (Name) = Saessolsheim
| 67424 = 228 | 67424 (Jahr) = 2007 | 67424 (Name) = Saint-Blaise-la-Roche
| 67425 = 611 | 67425 (Jahr) = 2004 | 67425 (Name) = Saint-Jean-Saverne
| 67426 = 341 | 67426 (Jahr) = 2007 | 67426 (Name) = Saint-Martin
| 67427 = 332 | 67427 (Jahr) = 1999 | 67427 (Name) = Saint-Maurice
| 67428 = 460 | 67428 (Jahr) = 2006 | 67428 (Name) = Saint-Nabor
| 67429 = 599 | 67429 (Jahr) = 2005 | 67429 (Name) = Saint-Pierre
| 67430 = 606 | 67430 (Jahr) = 1999 | 67430 (Name) = Saint-Pierre-Bois
| 67431 = 214 | 67431 (Jahr) = 2007 | 67431 (Name) = Salenthal
| 67432 = 579 | 67432 (Jahr) = 2006 | 67432 (Name) = Salmbach
| 67433 = 1073 | 67433 (Jahr) = 1999 | 67433 (Name) = Sand
| 67434 = 3161 | 67434 (Jahr) = 2007 | 67434 (Name) = Sarre-Union
| 67435 = 956 | 67435 (Jahr) = 2005 | 67435 (Name) = Sarrewerden
| 67436 = 457 | 67436 (Jahr) = 1999 | 67436 (Name) = Saulxures
| 67437 = 11300 | 67437 (Jahr) = 2005 | 67437 (Name) = Saverne
| 67438 = 706 | 67438 (Jahr) = 1999 | 67438 (Name) = Schaeffersheim
| 67439 = 349 | 67439 (Jahr) = 1999 | 67439 (Name) = Schaffhouse-sur-Zorn
| 67440 = 509 | 67440 (Jahr) = 2004 | 67440 (Name) = Schaffhouse-près-Seltz
| 67441 = 282 | 67441 (Jahr) = 1999 | 67441 (Name) = Schalkendorf
| 67442 = 1099 | 67442 (Jahr) = 2007 | 67442 (Name) = Scharrachbergheim-Irmstett
| 67443 = 829 | 67443 (Jahr) = 2006 | 67443 (Name) = Scheibenhard
| 67444 = 114 | 67444 (Jahr) = 2005 | 67444 (Name) = Scherlenheim
| 67445 = 2958 | 67445 (Jahr) = 2006 | 67445 (Name) = Scherwiller
| 67446 = 442 | 67446 (Jahr) = 2007 | 67446 (Name) = Schillersdorf
| 67447 = 31400 | 67447 (Jahr) = 2005 | 67447 (Name) = Schiltigheim
| 67448 = 2453 | 67448 (Jahr) = 2007 | 67448 (Name) = Schirmeck
| 67449 = 2027 | 67449 (Jahr) = 1999 | 67449 (Name) = Schirrhein
| 67450 = 630 | 67450 (Jahr) = 1999 | 67450 (Name) = Schirrhoffen
| 67451 = 1395 | 67451 (Jahr) = 1999 | 67451 (Name) = Schleithal
| 67452 = 1001 | 67452 (Jahr) = 1999 | 67452 (Name) = Schnersheim
| 67453 = 473 | 67453 (Jahr) = 1999 | 67453 (Name) = Schoenau
| 67454 = 448 | 67454 (Jahr) = 2005 | 67454 (Name) = Schoenbourg
| 67455 = 672 | 67455 (Jahr) = 2007 | 67455 (Name) = Schoenenbourg
| 67456 = 391 | 67456 (Jahr) = 2006 | 67456 (Name) = Schopperten
| 67458 = 4558 | 67458 (Jahr) = 2007 | 67458 (Name) = Schweighouse-sur-Moder
| 67459 = 684 | 67459 (Jahr) = 2004 | 67459 (Name) = Schwenheim
| 67460 = 1611 | 67460 (Jahr) = 2007 | 67460 (Name) = Schwindratzheim
| 67461 = 242 | 67461 (Jahr) = 2005 | 67461 (Name) = Schwobsheim
| 67462 = 19200 | 67462 (Jahr) = 2005 | 67462 (Name) = Sélestat
| 67463 = 2985 | 67463 (Jahr) = 1999 | 67463 (Name) = Seltz
| 67464 = 801 | 67464 (Jahr) = 2004 | 67464 (Name) = Sermersheim
| 67465 = 2023 | 67465 (Jahr) = 2006 | 67465 (Name) = Sessenheim
| 67466 = 519 | 67466 (Jahr) = 2004 | 67466 (Name) = Siegen
| 67467 = 411 | 67467 (Jahr) = 2005 | 67467 (Name) = Siewiller
| 67468 = 582 | 67468 (Jahr) = 1999 | 67468 (Name) = Siltzheim
| 67469 = 334 | 67469 (Jahr) = 2004 | 67469 (Name) = Singrist
| 67470 = 107 | 67470 (Jahr) = 2007 | 67470 (Name) = Solbach
| 67471 = 6219 | 67471 (Jahr) = 2006 | 67471 (Name) = Souffelweyersheim
| 67472 = 4570 | 67472 (Jahr) = 2005 | 67472 (Name) = Soufflenheim
| 67473 = 791 | 67473 (Jahr) = 2004 | 67473 (Name) = Soultz-les-Bains
| 67474 = 2645 | 67474 (Jahr) = 2006 | 67474 (Name) = Soultz-sous-Forêts
| 67475 = 248 | 67475 (Jahr) = 2006 | 67475 (Name) = Sparsbach
| 67476 = 646 | 67476 (Jahr) = 2004 | 67476 (Name) = Stattmatten
| 67477 = 543 | 67477 (Jahr) = 2006 | 67477 (Name) = Steige
| 67478 = 1932 | 67478 (Jahr) = 2004 | 67478 (Name) = Steinbourg
| 67479 = 634 | 67479 (Jahr) = 2006 | 67479 (Name) = Steinseltz
| 67480 = 1514 | 67480 (Jahr) = 1999 | 67480 (Name) = Still
| 67481 = 960 | 67481 (Jahr) = 1999 | 67481 (Name) = Stotzheim
| 67482 = 272500 | 67482 (Jahr) = 2005 | 67482 (Name) = Strasbourg
| 67483 = 221 | 67483 (Jahr) = 2005 | 67483 (Name) = Struth
| 67484 = 410 | 67484 (Jahr) = 2007 | 67484 (Name) = Stundwiller
| 67485 = 1501 | 67485 (Jahr) = 2005 | 67485 (Name) = Stutzheim-Offenheim
| 67486 = 1356 | 67486 (Jahr) = 2006 | 67486 (Name) = Sundhouse
| 67487 = 1581 | 67487 (Jahr) = 2006 | 67487 (Name) = Surbourg
| 67488 = 167 | 67488 (Jahr) = 2004 | 67488 (Name) = Thal-Drulingen
| 67489 = 717 | 67489 (Jahr) = 2005 | 67489 (Name) = Thal-Marmoutier
| 67490 = 544 | 67490 (Jahr) = 2006 | 67490 (Name) = Thanvillé
| 67491 = 285 | 67491 (Jahr) = 2007 | 67491 (Name) = Tieffenbach
| 67492 = 592 | 67492 (Jahr) = 2004 | 67492 (Name) = Traenheim
| 67493 = 460 | 67493 (Jahr) = 2004 | 67493 (Name) = Triembach-au-Val
| 67494 = 461 | 67494 (Jahr) = 2004 | 67494 (Name) = Trimbach
| 67495 = 2719 | 67495 (Jahr) = 2005 | 67495 (Name) = Truchtersheim
| 67496 = 1091 | 67496 (Jahr) = 1999 | 67496 (Name) = Uberach
| 67497 = 623 | 67497 (Jahr) = 1999 | 67497 (Name) = Uhlwiller
| 67498 = 687 | 67498 (Jahr) = 2007 | 67498 (Name) = Uhrwiller
| 67499 = 290 | 67499 (Jahr) = 2004 | 67499 (Name) = Urbeis
| 67500 = 1357 | 67500 (Jahr) = 1999 | 67500 (Name) = Urmatt
| 67501 = 578 | 67501 (Jahr) = 2007 | 67501 (Name) = Uttenheim
| 67502 = 171 | 67502 (Jahr) = 2004 | 67502 (Name) = Uttenhoffen
| 67503 = 177 | 67503 (Jahr) = 1999 | 67503 (Name) = Uttwiller
| 67504 = 1342 | 67504 (Jahr) = 2006 | 67504 (Name) = Valff
| 67505 = 410 | 67505 (Jahr) = 2005 | 67505 (Name) = La Vancelle
| 67506 = 5720 | 67506 (Jahr) = 2004 | 67506 (Name) = Vendenheim
| 67507 = 1691 | 67507 (Jahr) = 2004 | 67507 (Name) = Villé
| 67508 = 429 | 67508 (Jahr) = 2007 | 67508 (Name) = Voellerdingen
| 67509 = 351 | 67509 (Jahr) = 2005 | 67509 (Name) = Volksberg
| 67510 = 377 | 67510 (Jahr) = 2006 | 67510 (Name) = Wahlenheim
| 67511 = 792 | 67511 (Jahr) = 2006 | 67511 (Name) = Walbourg
| 67512 = 1130 | 67512 (Jahr) = 2005 | 67512 (Name) = La Walck
| 67513 = 137 | 67513 (Jahr) = 2006 | 67513 (Name) = Waldersbach
| 67514 = 666 | 67514 (Jahr) = 2007 | 67514 (Name) = Waldhambach
| 67515 = 530 | 67515 (Jahr) = 1999 | 67515 (Name) = Waldolwisheim
| 67516 = 720 | 67516 (Jahr) = 2007 | 67516 (Name) = Waltenheim-sur-Zorn
| 67517 = 705 | 67517 (Jahr) = 2004 | 67517 (Name) = Wangen
| 67519 = 5859 | 67519 (Jahr) = 2007 | 67519 (Name) = La Wantzenau
| 67520 = 5571 | 67520 (Jahr) = 2007 | 67520 (Name) = Wasselonne
| 67521 = 458 | 67521 (Jahr) = 2005 | 67521 (Name) = Weinbourg
| 67522 = 559 | 67522 (Jahr) = 2004 | 67522 (Name) = Weislingen
| 67523 = 2582 | 67523 (Jahr) = 2004 | 67523 (Name) = Weitbruch
| 67524 = 572 | 67524 (Jahr) = 2005 | 67524 (Name) = Weiterswiller
| 67525 = 1636 | 67525 (Jahr) = 2004 | 67525 (Name) = Westhoffen
| 67526 = 1382 | 67526 (Jahr) = 2005 | 67526 (Name) = Westhouse
| 67527 = 254 | 67527 (Jahr) = 2005 | 67527 (Name) = Westhouse-Marmoutier
| 67528 = 545 | 67528 (Jahr) = 2006 | 67528 (Name) = Weyer
| 67529 = 3073 | 67529 (Jahr) = 2004 | 67529 (Name) = Weyersheim
| 67530 = 433 | 67530 (Jahr) = 2006 | 67530 (Name) = Wickersheim-Wilshausen
| 67531 = 318 | 67531 (Jahr) = 2007 | 67531 (Name) = Wildersbach
| 67532 = 1072 | 67532 (Jahr) = 2006 | 67532 (Name) = Willgottheim
| 67534 = 662 | 67534 (Jahr) = 2005 | 67534 (Name) = Wilwisheim
| 67535 = 105 | 67535 (Jahr) = 1999 | 67535 (Name) = Wimmenau
| 67536 = 164 | 67536 (Jahr) = 2006 | 67536 (Name) = Windstein
| 67537 = 465 | 67537 (Jahr) = 2004 | 67537 (Name) = Wingen
| 67538 = 1617 | 67538 (Jahr) = 2004 | 67538 (Name) = Wingen-sur-Moder
| 67539 = 1105 | 67539 (Jahr) = 2004 | 67539 (Name) = Wingersheim
| 67540 = 613 | 67540 (Jahr) = 1999 | 67540 (Name) = Wintershouse
| 67541 = 577 | 67541 (Jahr) = 2004 | 67541 (Name) = Wintzenbach
| 67542 = 266 | 67542 (Jahr) = 2004 | 67542 (Name) = Wintzenheim-Kochersberg
| 67543 = 2017 | 67543 (Jahr) = 1999 | 67543 (Name) = Wisches
| 67544 = 7978 | 67544 (Jahr) = 2007 | 67544 (Name) = Wissembourg
| 67545 = 497 | 67545 (Jahr) = 2005 | 67545 (Name) = Witternheim
| 67546 = 582 | 67546 (Jahr) = 2007 | 67546 (Name) = Wittersheim
| 67547 = 1923 | 67547 (Jahr) = 2006 | 67547 (Name) = Wittisheim
| 67548 = 502 | 67548 (Jahr) = 1999 | 67548 (Name) = Wiwersheim
| 67550 = 1814 | 67550 (Jahr) = 2007 | 67550 (Name) = Woerth
| 67551 = 3832 | 67551 (Jahr) = 1999 | 67551 (Name) = Wolfisheim
| 67552 = 352 | 67552 (Jahr) = 2007 | 67552 (Name) = Wolfskirchen
| 67553 = 296 | 67553 (Jahr) = 1999 | 67553 (Name) = Wolschheim
| 67554 = 865 | 67554 (Jahr) = 2007 | 67554 (Name) = Wolxheim
| 67555 = 216 | 67555 (Jahr) = 2004 | 67555 (Name) = Zehnacker
| 67556 = 186 | 67556 (Jahr) = 2007 | 67556 (Name) = Zeinheim
| 67557 = 725 | 67557 (Jahr) = 2007 | 67557 (Name) = Zellwiller
| 67558 = 808 | 67558 (Jahr) = 2005 | 67558 (Name) = Zinswiller
| 67559 = 255 | 67559 (Jahr) = 2007 | 67559 (Name) = Zittersheim
| 67560 = 183 | 67560 (Jahr) = 2007 | 67560 (Name) = Zoebersdorf
| 68001 = 1065 | 68001 (Jahr) = 2004 | 68001 (Name) = Algolsheim
| 68002 = 387 | 68002 (Jahr) = 2007 | 68002 (Name) = Altenach
| 68004 = 5526 | 68004 (Jahr) = 2004 | 68004 (Name) = Altkirch
| 68005 = 1875 | 68005 (Jahr) = 2006 | 68005 (Name) = Ammerschwihr
| 68006 = 308 | 68006 (Jahr) = 2004 | 68006 (Name) = Ammerzwiller
| 68007 = 2227 | 68007 (Jahr) = 2005 | 68007 (Name) = Andolsheim
| 68008 = 446 | 68008 (Jahr) = 2006 | 68008 (Name) = Appenwihr
| 68009 = 775 | 68009 (Jahr) = 2007 | 68009 (Name) = Artzenheim
| 68010 = 1132 | 68010 (Jahr) = 2006 | 68010 (Name) = Aspach
| 68011 = 1250 | 68011 (Jahr) = 2006 | 68011 (Name) = Aspach-le-Bas
| 68012 = 1420 | 68012 (Jahr) = 2007 | 68012 (Name) = Aspach-le-Haut
| 68013 = 836 | 68013 (Jahr) = 1999 | 68013 (Name) = Attenschwiller
| 68014 = 407 | 68014 (Jahr) = 2007 | 68014 (Name) = Aubure
| 68015 = 2514 | 68015 (Jahr) = 2006 | 68015 (Name) = Baldersheim
| 68016 = 818 | 68016 (Jahr) = 2006 | 68016 (Name) = Balgau
| 68017 = 829 | 68017 (Jahr) = 2005 | 68017 (Name) = Ballersdorf
| 68018 = 762 | 68018 (Jahr) = 1999 | 68018 (Name) = Balschwiller
| 68019 = 566 | 68019 (Jahr) = 2005 | 68019 (Name) = Baltzenheim
| 68020 = 1572 | 68020 (Jahr) = 1999 | 68020 (Name) = Bantzenheim
| 68021 = 3452 | 68021 (Jahr) = 2005 | 68021 (Name) = Bartenheim
| 68022 = 1281 | 68022 (Jahr) = 2006 | 68022 (Name) = Battenheim
| 68023 = 954 | 68023 (Jahr) = 2007 | 68023 (Name) = Beblenheim
| 68024 = 187 | 68024 (Jahr) = 2005 | 68024 (Name) = Bellemagny
| 68025 = 225 | 68025 (Jahr) = 2004 | 68025 (Name) = Bendorf
| 68026 = 1274 | 68026 (Jahr) = 2006 | 68026 (Name) = Bennwihr
| 68027 = 321 | 68027 (Jahr) = 2006 | 68027 (Name) = Berentzwiller
| 68028 = 1850 | 68028 (Jahr) = 2004 | 68028 (Name) = Bergheim
| 68029 = 1008 | 68029 (Jahr) = 1999 | 68029 (Name) = Bergholtz
| 68030 = 376 | 68030 (Jahr) = 2006 | 68030 (Name) = Bergholtzzell
| 68031 = 612 | 68031 (Jahr) = 2004 | 68031 (Name) = Bernwiller
| 68032 = 1058 | 68032 (Jahr) = 1999 | 68032 (Name) = Berrwiller
| 68033 = 465 | 68033 (Jahr) = 2006 | 68033 (Name) = Bettendorf
| 68034 = 288 | 68034 (Jahr) = 1999 | 68034 (Name) = Bettlach
| 68035 = 348 | 68035 (Jahr) = 2006 | 68035 (Name) = Biederthal
| 68036 = 2329 | 68036 (Jahr) = 2005 | 68036 (Name) = Biesheim
| 68037 = 359 | 68037 (Jahr) = 2005 | 68037 (Name) = Biltzheim
| 68038 = 900 | 68038 (Jahr) = 2004 | 68038 (Name) = Bischwihr
| 68039 = 588 | 68039 (Jahr) = 2004 | 68039 (Name) = Bisel
| 68040 = 2131 | 68040 (Jahr) = 2005 | 68040 (Name) = Bitschwiller-lès-Thann
| 68041 = 1618 | 68041 (Jahr) = 2006 | 68041 (Name) = Blodelsheim
| 68042 = 3608 | 68042 (Jahr) = 2004 | 68042 (Name) = Blotzheim
| 68043 = 3580 | 68043 (Jahr) = 2004 | 68043 (Name) = Bollwiller
| 68044 = 836 | 68044 (Jahr) = 2006 | 68044 (Name) = Le Bonhomme
| 68045 = 618 | 68045 (Jahr) = 2005 | 68045 (Name) = Bourbach-le-Bas
| 68046 = 408 | 68046 (Jahr) = 2006 | 68046 (Name) = Bourbach-le-Haut
| 68049 = 470 | 68049 (Jahr) = 2005 | 68049 (Name) = Bouxwiller
| 68050 = 394 | 68050 (Jahr) = 2006 | 68050 (Name) = Bréchaumont
| 68051 = 884 | 68051 (Jahr) = 2004 | 68051 (Name) = Breitenbach-Haut-Rhin
| 68052 = 131 | 68052 (Jahr) = 2005 | 68052 (Name) = Bretten
| 68054 = 346 | 68054 (Jahr) = 2007 | 68054 (Name) = Brinckheim
| 68055 = 1027 | 68055 (Jahr) = 2005 | 68055 (Name) = Bruebach
| 68056 = 6180 | 68056 (Jahr) = 2006 | 68056 (Name) = Brunstatt
| 68057 = 273 | 68057 (Jahr) = 2006 | 68057 (Name) = Buethwiller
| 68058 = 3190 | 68058 (Jahr) = 2006 | 68058 (Name) = Buhl
| 68059 = 1277 | 68059 (Jahr) = 2004 | 68059 (Name) = Burnhaupt-le-Bas
| 68060 = 1550 | 68060 (Jahr) = 2004 | 68060 (Name) = Burnhaupt-le-Haut
| 68061 = 950 | 68061 (Jahr) = 2007 | 68061 (Name) = Buschwiller
| 68062 = 1786 | 68062 (Jahr) = 2006 | 68062 (Name) = Carspach
| 68063 = 11000 | 68063 (Jahr) = 2005 | 68063 (Name) = Cernay
| 68064 = 888 | 68064 (Jahr) = 2006 | 68064 (Name) = Chalampé
| 68065 = 518 | 68065 (Jahr) = 2006 | 68065 (Name) = Chavannes-sur-l'Étang
| 68066 = 65300 | 68066 (Jahr) = 2005 | 68066 (Name) = Colmar
| 68067 = 357 | 68067 (Jahr) = 2004 | 68067 (Name) = Courtavon
| 68068 = 2259 | 68068 (Jahr) = 2005 | 68068 (Name) = Dannemarie
| 68069 = 1143 | 68069 (Jahr) = 2006 | 68069 (Name) = Dessenheim
| 68070 = 1675 | 68070 (Jahr) = 2005 | 68070 (Name) = Didenheim
| 68071 = 251 | 68071 (Jahr) = 1999 | 68071 (Name) = Diefmatten
| 68072 = 1310 | 68072 (Jahr) = 2005 | 68072 (Name) = Dietwiller
| 68073 = 397 | 68073 (Jahr) = 1999 | 68073 (Name) = Dolleren
| 68074 = 477 | 68074 (Jahr) = 2007 | 68074 (Name) = Durlinsdorf
| 68075 = 883 | 68075 (Jahr) = 1999 | 68075 (Name) = Durmenach
| 68076 = 824 | 68076 (Jahr) = 2004 | 68076 (Name) = Durrenentzen
| 68077 = 324 | 68077 (Jahr) = 2007 | 68077 (Name) = Eglingen
| 68078 = 1541 | 68078 (Jahr) = 2004 | 68078 (Name) = Eguisheim
| 68079 = 269 | 68079 (Jahr) = 2007 | 68079 (Name) = Elbach
| 68080 = 241 | 68080 (Jahr) = 2004 | 68080 (Name) = Emlingen
| 68081 = 462 | 68081 (Jahr) = 2006 | 68081 (Name) = Saint-Bernard
| 68082 = 6967 | 68082 (Jahr) = 2007 | 68082 (Name) = Ensisheim
| 68083 = 386 | 68083 (Jahr) = 2006 | 68083 (Name) = Eschbach-au-Val
| 68084 = 1394 | 68084 (Jahr) = 2007 | 68084 (Name) = Eschentzwiller
| 68085 = 306 | 68085 (Jahr) = 2007 | 68085 (Name) = Eteimbes
| 68086 = 201 | 68086 (Jahr) = 2006 | 68086 (Name) = Falkwiller
| 68087 = 433 | 68087 (Jahr) = 2004 | 68087 (Name) = Feldbach
| 68088 = 923 | 68088 (Jahr) = 2006 | 68088 (Name) = Feldkirch
| 68089 = 1649 | 68089 (Jahr) = 2005 | 68089 (Name) = Fellering
| 68090 = 1041 | 68090 (Jahr) = 2004 | 68090 (Name) = Ferrette
| 68091 = 2195 | 68091 (Jahr) = 2004 | 68091 (Name) = Fessenheim
| 68092 = 388 | 68092 (Jahr) = 2004 | 68092 (Name) = Fislis
| 68093 = 1448 | 68093 (Jahr) = 2006 | 68093 (Name) = Flaxlanden
| 68094 = 727 | 68094 (Jahr) = 2006 | 68094 (Name) = Folgensbourg
| 68095 = 967 | 68095 (Jahr) = 1999 | 68095 (Name) = Fortschwihr
| 68096 = 288 | 68096 (Jahr) = 1999 | 68096 (Name) = Franken
| 68097 = 1292 | 68097 (Jahr) = 1999 | 68097 (Name) = Fréland
| 68098 = 550 | 68098 (Jahr) = 2004 | 68098 (Name) = Friesen
| 68099 = 606 | 68099 (Jahr) = 1999 | 68099 (Name) = Froeningen
| 68100 = 340 | 68100 (Jahr) = 2004 | 68100 (Name) = Fulleren
| 68101 = 767 | 68101 (Jahr) = 2006 | 68101 (Name) = Galfingue
| 68102 = 489 | 68102 (Jahr) = 2005 | 68102 (Name) = Geishouse
| 68103 = 412 | 68103 (Jahr) = 2006 | 68103 (Name) = Geispitzen
| 68104 = 257 | 68104 (Jahr) = 1999 | 68104 (Name) = Geiswasser
| 68105 = 284 | 68105 (Jahr) = 2004 | 68105 (Name) = Gildwiller
| 68106 = 292 | 68106 (Jahr) = 2004 | 68106 (Name) = Goldbach-Altenbach
| 68107 = 375 | 68107 (Jahr) = 1999 | 68107 (Name) = Gommersdorf
| 68108 = 559 | 68108 (Jahr) = 2005 | 68108 (Name) = Grentzingen
| 68109 = 756 | 68109 (Jahr) = 2007 | 68109 (Name) = Griesbach-au-Val
| 68110 = 823 | 68110 (Jahr) = 2005 | 68110 (Name) = Grussenheim
| 68111 = 836 | 68111 (Jahr) = 2007 | 68111 (Name) = Gueberschwihr
| 68112 = 11500 | 68112 (Jahr) = 2005 | 68112 (Name) = Guebwiller
| 68113 = 1314 | 68113 (Jahr) = 1999 | 68113 (Name) = Guémar
| 68114 = 144 | 68114 (Jahr) = 1999 | 68114 (Name) = Guevenatten
| 68115 = 1205 | 68115 (Jahr) = 2006 | 68115 (Name) = Guewenheim
| 68116 = 708 | 68116 (Jahr) = 2005 | 68116 (Name) = Gundolsheim
| 68117 = 916 | 68117 (Jahr) = 2007 | 68117 (Name) = Gunsbach
| 68118 = 4685 | 68118 (Jahr) = 2005 | 68118 (Name) = Habsheim
| 68119 = 687 | 68119 (Jahr) = 2007 | 68119 (Name) = Hagenbach
| 68120 = 1001 | 68120 (Jahr) = 1999 | 68120 (Name) = Hagenthal-le-Bas
| 68121 = 561 | 68121 (Jahr) = 2006 | 68121 (Name) = Hagenthal-le-Haut
| 68122 = 646 | 68122 (Jahr) = 2004 | 68122 (Name) = Hartmannswiller
| 68123 = 840 | 68123 (Jahr) = 2006 | 68123 (Name) = Hattstatt
| 68124 = 441 | 68124 (Jahr) = 2005 | 68124 (Name) = Hausgauen
| 68125 = 448 | 68125 (Jahr) = 2005 | 68125 (Name) = Hecken
| 68126 = 2836 | 68126 (Jahr) = 2004 | 68126 (Name) = Hégenheim
| 68127 = 601 | 68127 (Jahr) = 1999 | 68127 (Name) = Heidwiller
| 68128 = 645 | 68128 (Jahr) = 2006 | 68128 (Name) = Heimersdorf
| 68129 = 1218 | 68129 (Jahr) = 1999 | 68129 (Name) = Heimsbrunn
| 68130 = 785 | 68130 (Jahr) = 1999 | 68130 (Name) = Heiteren
| 68131 = 181 | 68131 (Jahr) = 1999 | 68131 (Name) = Heiwiller
| 68132 = 794 | 68132 (Jahr) = 2005 | 68132 (Name) = Helfrantzkirch
| 68133 = 186 | 68133 (Jahr) = 2005 | 68133 (Name) = Henflingen
| 68134 = 1615 | 68134 (Jahr) = 2004 | 68134 (Name) = Herrlisheim-près-Colmar
| 68135 = 2265 | 68135 (Jahr) = 2004 | 68135 (Name) = Hésingue
| 68136 = 287 | 68136 (Jahr) = 1999 | 68136 (Name) = Hettenschlag
| 68137 = 432 | 68137 (Jahr) = 1999 | 68137 (Name) = Hindlingen
| 68138 = 2127 | 68138 (Jahr) = 2005 | 68138 (Name) = Hirsingue
| 68139 = 1259 | 68139 (Jahr) = 2007 | 68139 (Name) = Hirtzbach
| 68140 = 981 | 68140 (Jahr) = 1999 | 68140 (Name) = Hirtzfelden
| 68141 = 2078 | 68141 (Jahr) = 2007 | 68141 (Name) = Hochstatt
| 68142 = 343 | 68142 (Jahr) = 2007 | 68142 (Name) = Hohrod
| 68143 = 1270 | 68143 (Jahr) = 2005 | 68143 (Name) = Holtzwihr
| 68144 = 888 | 68144 (Jahr) = 2004 | 68144 (Name) = Hombourg
| 68145 = 506 | 68145 (Jahr) = 1999 | 68145 (Name) = Horbourg-Wihr
| 68146 = 1652 | 68146 (Jahr) = 2007 | 68146 (Name) = Houssen
| 68147 = 591 | 68147 (Jahr) = 2006 | 68147 (Name) = Hunawihr
| 68148 = 271 | 68148 (Jahr) = 2006 | 68148 (Name) = Hundsbach
| 68149 = 6324 | 68149 (Jahr) = 2005 | 68149 (Name) = Huningue
| 68150 = 488 | 68150 (Jahr) = 2006 | 68150 (Name) = Husseren-les-Châteaux
| 68151 = 1017 | 68151 (Jahr) = 2007 | 68151 (Name) = Husseren-Wesserling
| 68152 = 2235 | 68152 (Jahr) = 2006 | 68152 (Name) = Illfurth
| 68153 = 711 | 68153 (Jahr) = 2006 | 68153 (Name) = Illhaeusern
| 68154 = 15100 | 68154 (Jahr) = 2005 | 68154 (Name) = Illzach
| 68155 = 4583 | 68155 (Jahr) = 2007 | 68155 (Name) = Ingersheim
| 68156 = 3415 | 68156 (Jahr) = 2006 | 68156 (Name) = Issenheim
| 68157 = 1052 | 68157 (Jahr) = 2005 | 68157 (Name) = Jebsheim
| 68158 = 502 | 68158 (Jahr) = 1999 | 68158 (Name) = Jettingen
| 68159 = 889 | 68159 (Jahr) = 2005 | 68159 (Name) = Jungholtz
| 68160 = 517 | 68160 (Jahr) = 2005 | 68160 (Name) = Kappelen
| 68161 = 544 | 68161 (Jahr) = 2007 | 68161 (Name) = Katzenthal
| 68162 = 2676 | 68162 (Jahr) = 1999 | 68162 (Name) = Kaysersberg
| 68163 = 4139 | 68163 (Jahr) = 2005 | 68163 (Name) = Kembs
| 68164 = 779 | 68164 (Jahr) = 2005 | 68164 (Name) = Kientzheim
| 68165 = 285 | 68165 (Jahr) = 2005 | 68165 (Name) = Kiffis
| 68166 = 13400 | 68166 (Jahr) = 2005 | 68166 (Name) = Kingersheim
| 68167 = 843 | 68167 (Jahr) = 1999 | 68167 (Name) = Kirchberg
| 68168 = 303 | 68168 (Jahr) = 2005 | 68168 (Name) = Knoeringue
| 68169 = 497 | 68169 (Jahr) = 2006 | 68169 (Name) = Koestlach
| 68170 = 553 | 68170 (Jahr) = 2007 | 68170 (Name) = Koetzingue
| 68171 = 1019 | 68171 (Jahr) = 2007 | 68171 (Name) = Kruth
| 68172 = 1691 | 68172 (Jahr) = 2004 | 68172 (Name) = Kunheim
| 68173 = 1985 | 68173 (Jahr) = 1999 | 68173 (Name) = Labaroche
| 68174 = 1592 | 68174 (Jahr) = 2006 | 68174 (Name) = Landser
| 68175 = 2049 | 68175 (Jahr) = 2007 | 68175 (Name) = Lapoutroie
| 68176 = 290 | 68176 (Jahr) = 2006 | 68176 (Name) = Largitzen
| 68177 = 1572 | 68177 (Jahr) = 1999 | 68177 (Name) = Lautenbach
| 68178 = 1002 | 68178 (Jahr) = 2004 | 68178 (Name) = Lautenbachzell
| 68179 = 985 | 68179 (Jahr) = 2007 | 68179 (Name) = Lauw
| 68180 = 813 | 68180 (Jahr) = 2004 | 68180 (Name) = Leimbach
| 68181 = 247 | 68181 (Jahr) = 2007 | 68181 (Name) = Levoncourt
| 68182 = 1120 | 68182 (Jahr) = 2007 | 68182 (Name) = Leymen
| 68183 = 184 | 68183 (Jahr) = 1999 | 68183 (Name) = Liebenswiller
| 68184 = 325 | 68184 (Jahr) = 2006 | 68184 (Name) = Liebsdorf
| 68185 = 1733 | 68185 (Jahr) = 2005 | 68185 (Name) = Lièpvre
| 68186 = 334 | 68186 (Jahr) = 2006 | 68186 (Name) = Ligsdorf
| 68187 = 311 | 68187 (Jahr) = 2007 | 68187 (Name) = Linsdorf
| 68188 = 607 | 68188 (Jahr) = 2006 | 68188 (Name) = Linthal
| 68189 = 762 | 68189 (Jahr) = 2005 | 68189 (Name) = Logelheim
| 68190 = 47 | 68190 (Jahr) = 1999 | 68190 (Name) = Lucelle
| 68191 = 709 | 68191 (Jahr) = 2005 | 68191 (Name) = Luemschwiller
| 68192 = 358 | 68192 (Jahr) = 2005 | 68192 (Name) = Valdieu-Lutran
| 68193 = 837 | 68193 (Jahr) = 2004 | 68193 (Name) = Luttenbach-près-Munster
| 68194 = 302 | 68194 (Jahr) = 2004 | 68194 (Name) = Lutter
| 68195 = 6070 | 68195 (Jahr) = 2004 | 68195 (Name) = Lutterbach
| 68196 = 250 | 68196 (Jahr) = 2006 | 68196 (Name) = Magny
| 68197 = 465 | 68197 (Jahr) = 2006 | 68197 (Name) = Magstatt-le-Bas
| 68198 = 276 | 68198 (Jahr) = 2007 | 68198 (Name) = Magstatt-le-Haut
| 68199 = 524 | 68199 (Jahr) = 2007 | 68199 (Name) = Malmerspach
| 68200 = 525 | 68200 (Jahr) = 2004 | 68200 (Name) = Manspach
| 68201 = 3238 | 68201 (Jahr) = 2005 | 68201 (Name) = Masevaux
| 68202 = 189 | 68202 (Jahr) = 1999 | 68202 (Name) = Mertzen
| 68203 = 1147 | 68203 (Jahr) = 1999 | 68203 (Name) = Merxheim
| 68204 = 1065 | 68204 (Jahr) = 1999 | 68204 (Name) = Metzeral
| 68205 = 1171 | 68205 (Jahr) = 2005 | 68205 (Name) = Meyenheim
| 68206 = 274 | 68206 (Jahr) = 2004 | 68206 (Name) = Michelbach
| 68207 = 719 | 68207 (Jahr) = 2006 | 68207 (Name) = Michelbach-le-Bas
| 68208 = 474 | 68208 (Jahr) = 2007 | 68208 (Name) = Michelbach-le-Haut
| 68209 = 780 | 68209 (Jahr) = 2007 | 68209 (Name) = Mittelwihr
| 68210 = 311 | 68210 (Jahr) = 2006 | 68210 (Name) = Mittlach
| 68211 = 416 | 68211 (Jahr) = 2007 | 68211 (Name) = Mitzach
| 68212 = 542 | 68212 (Jahr) = 2007 | 68212 (Name) = Moernach
| 68213 = 419 | 68213 (Jahr) = 1999 | 68213 (Name) = Mollau
| 68214 = 320 | 68214 (Jahr) = 2006 | 68214 (Name) = Montreux-Jeune
| 68215 = 837 | 68215 (Jahr) = 2004 | 68215 (Name) = Montreux-Vieux
| 68216 = 393 | 68216 (Jahr) = 2007 | 68216 (Name) = Mooslargue
| 68217 = 1818 | 68217 (Jahr) = 2005 | 68217 (Name) = Moosch
| 68218 = 2804 | 68218 (Jahr) = 2005 | 68218 (Name) = Morschwiller-le-Bas
| 68219 = 252 | 68219 (Jahr) = 1999 | 68219 (Name) = Mortzwiller
| 68221 = 860 | 68221 (Jahr) = 2006 | 68221 (Name) = Muespach
| 68222 = 951 | 68222 (Jahr) = 2007 | 68222 (Name) = Muespach-le-Haut
| 68223 = 760 | 68223 (Jahr) = 2006 | 68223 (Name) = Muhlbach-sur-Munster
| 68224 = 110900 | 68224 (Jahr) = 2005 | 68224 (Name) = Mulhouse
| 68225 = 1569 | 68225 (Jahr) = 2007 | 68225 (Name) = Munchhouse
| 68226 = 5108 | 68226 (Jahr) = 2004 | 68226 (Name) = Munster
| 68227 = 1013 | 68227 (Jahr) = 2006 | 68227 (Name) = Muntzenheim
| 68228 = 487 | 68228 (Jahr) = 2005 | 68228 (Name) = Munwiller
| 68229 = 136 | 68229 (Jahr) = 2006 | 68229 (Name) = Murbach
| 68230 = 406 | 68230 (Jahr) = 1999 | 68230 (Name) = Nambsheim
| 68231 = 2237 | 68231 (Jahr) = 2004 | 68231 (Name) = Neuf-Brisach
| 68232 = 526 | 68232 (Jahr) = 2005 | 68232 (Name) = Neuwiller
| 68233 = 428 | 68233 (Jahr) = 2004 | 68233 (Name) = Niederbruck
| 68234 = 322 | 68234 (Jahr) = 1999 | 68234 (Name) = Niederentzen
| 68235 = 965 | 68235 (Jahr) = 2004 | 68235 (Name) = Niederhergheim
| 68237 = 568 | 68237 (Jahr) = 2004 | 68237 (Name) = Niedermorschwihr
| 68238 = 657 | 68238 (Jahr) = 1999 | 68238 (Name) = Niffer
| 68239 = 454 | 68239 (Jahr) = 2004 | 68239 (Name) = Oberbruck
| 68240 = 563 | 68240 (Jahr) = 2005 | 68240 (Name) = Oberdorf
| 68241 = 525 | 68241 (Jahr) = 2007 | 68241 (Name) = Oberentzen
| 68242 = 1151 | 68242 (Jahr) = 2005 | 68242 (Name) = Oberhergheim
| 68243 = 150 | 68243 (Jahr) = 2004 | 68243 (Name) = Oberlarg
| 68244 = 379 | 68244 (Jahr) = 2007 | 68244 (Name) = Obermorschwihr
| 68245 = 439 | 68245 (Jahr) = 2007 | 68245 (Name) = Obermorschwiller
| 68246 = 973 | 68246 (Jahr) = 2006 | 68246 (Name) = Obersaasheim
| 68247 = 1320 | 68247 (Jahr) = 2007 | 68247 (Name) = Oderen
| 68248 = 765 | 68248 (Jahr) = 1999 | 68248 (Name) = Oltingue
| 68249 = 3608 | 68249 (Jahr) = 2006 | 68249 (Name) = Orbey
| 68250 = 979 | 68250 (Jahr) = 2007 | 68250 (Name) = Orschwihr
| 68251 = 883 | 68251 (Jahr) = 2005 | 68251 (Name) = Osenbach
| 68252 = 1564 | 68252 (Jahr) = 2007 | 68252 (Name) = Ostheim
| 68253 = 1926 | 68253 (Jahr) = 1999 | 68253 (Name) = Ottmarsheim
| 68254 = 665 | 68254 (Jahr) = 2005 | 68254 (Name) = Petit-Landau
| 68255 = 1307 | 68255 (Jahr) = 2005 | 68255 (Name) = Pfaffenheim
| 68256 = 7946 | 68256 (Jahr) = 1999 | 68256 (Name) = Pfastatt
| 68257 = 972 | 68257 (Jahr) = 1999 | 68257 (Name) = Pfetterhouse
| 68258 = 2819 | 68258 (Jahr) = 2007 | 68258 (Name) = Pulversheim
| 68259 = 505 | 68259 (Jahr) = 2006 | 68259 (Name) = Raedersdorf
| 68260 = 1093 | 68260 (Jahr) = 2004 | 68260 (Name) = Raedersheim
| 68261 = 180 | 68261 (Jahr) = 1999 | 68261 (Name) = Rammersmatt
| 68262 = 893 | 68262 (Jahr) = 1999 | 68262 (Name) = Ranspach
| 68263 = 660 | 68263 (Jahr) = 2005 | 68263 (Name) = Ranspach-le-Bas
| 68264 = 528 | 68264 (Jahr) = 2006 | 68264 (Name) = Ranspach-le-Haut
| 68265 = 794 | 68265 (Jahr) = 2007 | 68265 (Name) = Rantzwiller
| 68266 = 1749 | 68266 (Jahr) = 2004 | 68266 (Name) = Réguisheim
| 68267 = 1705 | 68267 (Jahr) = 2007 | 68267 (Name) = Reiningue
| 68268 = 637 | 68268 (Jahr) = 2005 | 68268 (Name) = Retzwiller
| 68269 = 4948 | 68269 (Jahr) = 2004 | 68269 (Name) = Ribeauvillé
| 68270 = 3351 | 68270 (Jahr) = 2004 | 68270 (Name) = Richwiller
| 68271 = 11900 | 68271 (Jahr) = 2005 | 68271 (Name) = Riedisheim
| 68272 = 382 | 68272 (Jahr) = 1999 | 68272 (Name) = Riedwihr
| 68273 = 700 | 68273 (Jahr) = 2007 | 68273 (Name) = Riespach
| 68274 = 243 | 68274 (Jahr) = 2006 | 68274 (Name) = Rimbach-près-Guebwiller
| 68275 = 506 | 68275 (Jahr) = 2005 | 68275 (Name) = Rimbach-près-Masevaux
| 68276 = 220 | 68276 (Jahr) = 2007 | 68276 (Name) = Rimbachzell
| 68277 = 1273 | 68277 (Jahr) = 2006 | 68277 (Name) = Riquewihr
| 68278 = 12900 | 68278 (Jahr) = 2005 | 68278 (Name) = Rixheim
| 68279 = 860 | 68279 (Jahr) = 2005 | 68279 (Name) = Roderen
| 68280 = 313 | 68280 (Jahr) = 1999 | 68280 (Name) = Rodern
| 68281 = 453 | 68281 (Jahr) = 2005 | 68281 (Name) = Roggenhouse
| 68282 = 200 | 68282 (Jahr) = 2007 | 68282 (Name) = Romagny
| 68283 = 892 | 68283 (Jahr) = 2006 | 68283 (Name) = Rombach-le-Franc
| 68284 = 755 | 68284 (Jahr) = 2006 | 68284 (Name) = Roppentzwiller
| 68285 = 366 | 68285 (Jahr) = 1999 | 68285 (Name) = Rorschwihr
| 68286 = 1988 | 68286 (Jahr) = 2007 | 68286 (Name) = Rosenau
| 68287 = 4491 | 68287 (Jahr) = 2005 | 68287 (Name) = Rouffach
| 68288 = 328 | 68288 (Jahr) = 2007 | 68288 (Name) = Ruederbach
| 68289 = 2657 | 68289 (Jahr) = 1999 | 68289 (Name) = Ruelisheim
| 68290 = 778 | 68290 (Jahr) = 2004 | 68290 (Name) = Rustenhart
| 68291 = 1031 | 68291 (Jahr) = 2004 | 68291 (Name) = Rumersheim-le-Haut
| 68292 = 2486 | 68292 (Jahr) = 2006 | 68292 (Name) = Saint-Amarin
| 68293 = 84 | 68293 (Jahr) = 2004 | 68293 (Name) = Saint-Cosme
| 68294 = 2049 | 68294 (Jahr) = 2006 | 68294 (Name) = Sainte-Croix-aux-Mines
| 68295 = 2362 | 68295 (Jahr) = 2004 | 68295 (Name) = Sainte-Croix-en-Plaine
| 68296 = 1049 | 68296 (Jahr) = 2007 | 68296 (Name) = Saint-Hippolyte
| 68297 = 19951 | 68297 (Jahr) = 1999 | 68297 (Name) = Saint-Louis
| 68298 = 5815 | 68298 (Jahr) = 1999 | 68298 (Name) = Sainte-Marie-aux-Mines
| 68299 = 279 | 68299 (Jahr) = 1999 | 68299 (Name) = Saint-Ulrich
| 68300 = 547 | 68300 (Jahr) = 1999 | 68300 (Name) = Sausheim
| 68301 = 1070 | 68301 (Jahr) = 2007 | 68301 (Name) = Schlierbach
| 68302 = 720 | 68302 (Jahr) = 2005 | 68302 (Name) = Schweighouse-Thann
| 68303 = 269 | 68303 (Jahr) = 2004 | 68303 (Name) = Schwoben
| 68304 = 1477 | 68304 (Jahr) = 2005 | 68304 (Name) = Sentheim
| 68305 = 1038 | 68305 (Jahr) = 2007 | 68305 (Name) = Seppois-le-Bas
| 68306 = 519 | 68306 (Jahr) = 2007 | 68306 (Name) = Seppois-le-Haut
| 68307 = 531 | 68307 (Jahr) = 2005 | 68307 (Name) = Sewen
| 68308 = 328 | 68308 (Jahr) = 2006 | 68308 (Name) = Sickert
| 68309 = 2647 | 68309 (Jahr) = 2006 | 68309 (Name) = Sierentz
| 68310 = 1097 | 68310 (Jahr) = 2005 | 68310 (Name) = Sigolsheim
| 68311 = 651 | 68311 (Jahr) = 2007 | 68311 (Name) = Sondernach
| 68312 = 344 | 68312 (Jahr) = 2004 | 68312 (Name) = Sondersdorf
| 68313 = 684 | 68313 (Jahr) = 2004 | 68313 (Name) = Soppe-le-Bas
| 68314 = 560 | 68314 (Jahr) = 2004 | 68314 (Name) = Soppe-le-Haut
| 68315 = 7131 | 68315 (Jahr) = 2007 | 68315 (Name) = Soultz-Haut-Rhin
| 68316 = 602 | 68316 (Jahr) = 1999 | 68316 (Name) = Soultzbach-les-Bains
| 68317 = 1165 | 68317 (Jahr) = 2007 | 68317 (Name) = Soultzeren
| 68318 = 2172 | 68318 (Jahr) = 2004 | 68318 (Name) = Soultzmatt
| 68319 = 745 | 68319 (Jahr) = 2004 | 68319 (Name) = Spechbach-le-Bas
| 68320 = 621 | 68320 (Jahr) = 2005 | 68320 (Name) = Spechbach-le-Haut
| 68321 = 3579 | 68321 (Jahr) = 2006 | 68321 (Name) = Staffelfelden
| 68322 = 1290 | 68322 (Jahr) = 2004 | 68322 (Name) = Steinbach
| 68323 = 645 | 68323 (Jahr) = 1999 | 68323 (Name) = Steinbrunn-le-Bas
| 68324 = 590 | 68324 (Jahr) = 2006 | 68324 (Name) = Steinbrunn-le-Haut
| 68325 = 625 | 68325 (Jahr) = 1999 | 68325 (Name) = Steinsoultz
| 68326 = 148 | 68326 (Jahr) = 2005 | 68326 (Name) = Sternenberg
| 68327 = 299 | 68327 (Jahr) = 2006 | 68327 (Name) = Stetten
| 68328 = 248 | 68328 (Jahr) = 2006 | 68328 (Name) = Storckensohn
| 68329 = 1305 | 68329 (Jahr) = 1999 | 68329 (Name) = Stosswihr
| 68330 = 321 | 68330 (Jahr) = 2007 | 68330 (Name) = Strueth
| 68331 = 1911 | 68331 (Jahr) = 1999 | 68331 (Name) = Sundhoffen
| 68332 = 637 | 68332 (Jahr) = 2004 | 68332 (Name) = Tagolsheim
| 68333 = 325 | 68333 (Jahr) = 2007 | 68333 (Name) = Tagsdorf
| 68334 = 8033 | 68334 (Jahr) = 1999 | 68334 (Name) = Thann
| 68335 = 501 | 68335 (Jahr) = 2007 | 68335 (Name) = Thannenkirch
| 68336 = 393 | 68336 (Jahr) = 1999 | 68336 (Name) = Traubach-le-Bas
| 68337 = 448 | 68337 (Jahr) = 1999 | 68337 (Name) = Traubach-le-Haut
| 68338 = 3731 | 68338 (Jahr) = 2007 | 68338 (Name) = Turckheim
| 68340 = 348 | 68340 (Jahr) = 2007 | 68340 (Name) = Ueberstrass
| 68341 = 859 | 68341 (Jahr) = 2007 | 68341 (Name) = Uffheim
| 68342 = 1493 | 68342 (Jahr) = 2005 | 68342 (Name) = Uffholtz
| 68343 = 1991 | 68343 (Jahr) = 2007 | 68343 (Name) = Ungersheim
| 68344 = 473 | 68344 (Jahr) = 2004 | 68344 (Name) = Urbès
| 68345 = 639 | 68345 (Jahr) = 1999 | 68345 (Name) = Urschenheim
| 68347 = 564 | 68347 (Jahr) = 2006 | 68347 (Name) = Vieux-Ferrette
| 68348 = 2873 | 68348 (Jahr) = 2006 | 68348 (Name) = Vieux-Thann
| 68349 = 3444 | 68349 (Jahr) = 2005 | 68349 (Name) = Village-Neuf
| 68350 = 539 | 68350 (Jahr) = 2007 | 68350 (Name) = Voegtlinshoffen
| 68351 = 519 | 68351 (Jahr) = 1999 | 68351 (Name) = Vogelgrun
| 68352 = 2301 | 68352 (Jahr) = 2006 | 68352 (Name) = Volgelsheim
| 68353 = 323 | 68353 (Jahr) = 1999 | 68353 (Name) = Wahlbach
| 68354 = 930 | 68354 (Jahr) = 1999 | 68354 (Name) = Walbach
| 68355 = 1178 | 68355 (Jahr) = 1999 | 68355 (Name) = Waldighofen
| 68356 = 878 | 68356 (Jahr) = 2006 | 68356 (Name) = Walheim
| 68357 = 555 | 68357 (Jahr) = 2006 | 68357 (Name) = Waltenheim
| 68358 = 492 | 68358 (Jahr) = 2005 | 68358 (Name) = Wasserbourg
| 68359 = 1705 | 68359 (Jahr) = 2005 | 68359 (Name) = Wattwiller
| 68360 = 596 | 68360 (Jahr) = 2007 | 68360 (Name) = Weckolsheim
| 68361 = 331 | 68361 (Jahr) = 2006 | 68361 (Name) = Wegscheid
| 68362 = 554 | 68362 (Jahr) = 1999 | 68362 (Name) = Wentzwiller
| 68363 = 573 | 68363 (Jahr) = 2005 | 68363 (Name) = Werentzhouse
| 68364 = 904 | 68364 (Jahr) = 2006 | 68364 (Name) = Westhalten
| 68365 = 1685 | 68365 (Jahr) = 2004 | 68365 (Name) = Wettolsheim
| 68366 = 610 | 68366 (Jahr) = 1999 | 68366 (Name) = Wickerschwihr
| 68367 = 1238 | 68367 (Jahr) = 2007 | 68367 (Name) = Widensolen
| 68368 = 1177 | 68368 (Jahr) = 2005 | 68368 (Name) = Wihr-au-Val
| 68370 = 197 | 68370 (Jahr) = 2005 | 68370 (Name) = Wildenstein
| 68371 = 328 | 68371 (Jahr) = 2007 | 68371 (Name) = Willer
| 68372 = 1918 | 68372 (Jahr) = 2004 | 68372 (Name) = Willer-sur-Thur
| 68373 = 366 | 68373 (Jahr) = 2004 | 68373 (Name) = Winkel
| 68374 = 7524 | 68374 (Jahr) = 2006 | 68374 (Name) = Wintzenheim
| 68375 = 10900 | 68375 (Jahr) = 2005 | 68375 (Name) = Wittelsheim
| 68376 = 14600 | 68376 (Jahr) = 2005 | 68376 (Name) = Wittenheim
| 68377 = 808 | 68377 (Jahr) = 2007 | 68377 (Name) = Wittersdorf
| 68378 = 380 | 68378 (Jahr) = 2006 | 68378 (Name) = Wolfersdorf
| 68379 = 972 | 68379 (Jahr) = 1999 | 68379 (Name) = Wolfgantzen
| 68380 = 430 | 68380 (Jahr) = 1999 | 68380 (Name) = Wolschwiller
| 68381 = 815 | 68381 (Jahr) = 2006 | 68381 (Name) = Wuenheim
| 68382 = 301 | 68382 (Jahr) = 2007 | 68382 (Name) = Zaessingue
| 68383 = 397 | 68383 (Jahr) = 2005 | 68383 (Name) = Zellenberg
| 68384 = 2503 | 68384 (Jahr) = 2004 | 68384 (Name) = Zillisheim
| 68385 = 880 | 68385 (Jahr) = 2006 | 68385 (Name) = Zimmerbach
| 68386 = 996 | 68386 (Jahr) = 2005 | 68386 (Name) = Zimmersheim
| #default = {{PAGENAME}}
}}

<noinclude>Dies ist eine programmierte Vorlage. Sie beruht auf der [[Vorlage:Einwohnerzahlen Islands]]. Eine Erklärung findet sich auf [[Benutzer Diskussion:PatDi/Bevölkerungszahlen 1]]. Für Hinweise, Anregungen und Fehlermeldungen bitte direkt an [[Benutzer Diskussion:PatDi|Patrick]] wenden.

[[Kategorie:Vorlage:Bevölkerungszahlen (Frankreich)]]
"""

import time
from mwlib import expander
db = expander.DictDB(einwohnerzahlen=einwohnerzahlen)
stime=time.time()
for i in range(5):
    expander.expandstr("{{einwohnerzahlen|68384}}", wikidb=db)
print (time.time()-stime)/5


########NEW FILE########
__FILENAME__ = citeweb
#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""
measure template expansion performance
"""

citeweb=u"""
<includeonly>{{
#if: {{#if: {{{url|}}} | {{#if: {{{title|}}} |1}}}}
  ||You must specify  '''''title = ''''' and '''''url = ''''' when using {{[[Template:cite web|cite web]]}}.
{{#if: {{NAMESPACE}}|| [[Category:Articles with broken citations]]}}
}}{{
#if: {{{archiveurl|}}}{{{archivedate|}}} 
  | {{#if: {{#if: {{{archiveurl|}}}| {{#if: {{{archivedate|}}} |1}}}}
    ||You must specify '''''archiveurl = ''''' and '''''archivedate = ''''' when using {{[[Template:cite web|cite web]]}}.
{{#if: {{NAMESPACE}}|| [[Category:Articles with broken citations]]}}
}}
}}{{#if: {{{author|}}}{{{last|}}}
  | {{#if: {{{authorlink|}}}
    | [[{{{authorlink}}}|{{#if: {{{last|}}}
      | {{{last}}}{{#if: {{{first|}}} | , {{{first}}} }}
      | {{{author}}}
    }}]]
    | {{#if: {{{last|}}}
      | {{{last}}}{{#if: {{{first|}}} | , {{{first}}} }}
      | {{{author}}}
    }}
  }}
}}{{#if: {{{author|}}}{{{last|}}}
  | {{#if: {{{coauthors|}}}| <nowiki>;</nowiki>&#32;{{{coauthors}}} }}
}}{{#if: {{{author|}}}{{{last|}}}|
    {{#if: {{{date|}}}
    | &#32;({{#ifeq:{{#time:Y-m-d|{{{date}}}}}|{{{date}}}|[[{{{date}}}]]|{{{date}}}}})
    | {{#if: {{{year|}}}
      | {{#if: {{{month|}}}
        | &#32;({{{month}}} {{{year}}})
        | &#32;({{{year}}})
      }}
    }}
  |}}
}}{{#if: {{{last|}}}{{{author|}}}
  | .&#32;}}{{
  #if: {{{editor|}}}
  | &#32;{{{editor}}}: 
}}{{#if: {{{archiveurl|}}}
    | {{#if: {{{archiveurl|}}} | {{#if: {{{title|}}} | [{{{archiveurl}}} {{{title}}}] }}}}
    | {{#if: {{{url|}}} | {{#if: {{{title|}}} | [{{{url}}} {{{title}}}] }}}}
}}{{#if: {{{format|}}} | &#32;({{{format|}}})
}}{{#if: {{{language|}}} | &#32;<span style="color:#555;">({{{language}}})</span> 
}}{{#if: {{{work|}}}
  | .&#32;''{{{work}}}''
}}{{#if: {{{pages|}}}
  | &#32;{{{pages}}}
}}{{#if: {{{publisher|}}}
  | .&#32;{{{publisher}}}{{#if: {{{author|}}}{{{last|}}}
    | 
    | {{#if: {{{date|}}}{{{year|}}}{{{month|}}} || }}
  }}
}}{{#if: {{{author|}}}{{{last|}}}
  ||{{#if: {{{date|}}}
    | &#32;({{#ifeq:{{#time:Y-m-d|{{{date}}}}}|{{{date}}}|[[{{{date}}}]]|{{#ifeq:{{#time:Y-m-d|{{{date}}}}}|1970-01-01|[[{{{date}}}]]|{{{date}}}}}}})
    | {{#if: {{{year|}}}
      | {{#if: {{{month|}}}
        | &#32;({{{month}}} {{{year}}})
        | &#32;({{{year}}})
      }}
    }}
  }}
}}.{{#if: {{{archivedate|}}}
  | &#32;Archived from [{{{url}}} the original] on [[{{{archivedate}}}]].
}}{{#if: {{{doi|}}} 
  | &#32;[[Digital object identifier|DOI]]:[http://dx.doi.org/{{{doi|{{{doilabel|}}}}}} {{{doi}}}].
}}{{#if: {{{accessdate|}}}
  | &#32;Retrieved on [[{{{accessdate}}}]]{{#if: {{{accessyear|}}} | , [[{{{accessyear}}}]] }}.
}}{{#if: {{{accessmonthday|}}}
  | &#32;Retrieved on {{{accessmonthday}}}{{#if: {{{accessyear|}}} | , {{{accessyear}}} }}.
}}{{#if: {{{accessdaymonth|}}}
  | &#32;Retrieved on {{{accessdaymonth}}}{{#if: {{{accessyear|}}} | &#32;{{{accessyear}}} }}.
}}{{#if: {{{quote|}}} 
  | &nbsp;“{{{quote}}}”
}}</includeonly><noinclude>

{{pp-template|small=yes}}
{{Documentation}}
<!-- PLEASE ADD CATEGORIES AND INTERWIKIS TO THE /doc SUBPAGE, THANKS -->
</noinclude>
"""


import time
from mwlib import expander

snippet = """
{{citeweb|url=http://www.webbyawards.com/webbys/winners-2004.php|title=Webby Awards 2004|publisher=The International Academy of Digital Arts and Sciences|date=2004|accessdate=2007-06-19}}
"""

db=expander.DictDB(citeweb=citeweb)
e=expander.Expander(snippet*1000, pagename='test', wikidb=db)
stime=time.time()
e.expandTemplates()
print time.time()-stime

########NEW FILE########
__FILENAME__ = multicoll
#! /usr/bin/env python

from mwlib import metabook

c = metabook.collection()
c.append_article(title="Mainz", wikiident="de")
c.append_article(title="Mainz", wikiident="en")
c.wikis.append(metabook.wikiconf(ident="de", baseurl="http://de.wikipedia.org/w/"))
c.wikis.append(metabook.wikiconf(ident="en", baseurl="http://en.wikipedia.org/w/"))

print c.dumps()

                                 

########NEW FILE########
__FILENAME__ = mw-serve-stresser
#!/usr/bin/env python
# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

from optparse import OptionParser
import os
import urllib
import urllib2
import tempfile
import time
import random
import subprocess
import sys
try:
    import simplejson as json
except ImportError:
    import json

from mwlib import mwapidb, utils, log, bookshelf
import mwlib.metabook


RENDER_TIMEOUT_DEFAULT = 60*60 # 60 minutes
LICENSE_URL = 'http://en.wikipedia.org/w/index.php?title=Wikipedia:Text_of_the_GNU_Free_Documentation_License&action=raw'

system = 'mw-serve-stresser'
log = log.Log('mw-serve-stresser')

# disable fetch cache
utils.fetch_url_orig = utils.fetch_url 
def fetch_url(*args, **kargs):
    kargs["fetch_cache"] = {} 
    return utils.fetch_url_orig(*args, **kargs) 
utils.fetch_url = fetch_url

writer_options = {
    'rl': 'strict',
}

def getRandomArticles(api, min=1, max=100):
    #"http://en.wikipedia.org/w/api.php?action=query&list=random&rnnamespace=0&rnlimit=10"
    num = random.randint(min, max)
    articles = set()
    steps = (1+num/10)
    print num, steps
    for i in range(steps):
        res = api.query(list="random", rnnamespace=0, rnlimit=10)
        if res is None or 'query' not in res:
            print res
            time.sleep(5)
            continue
        res = res["query"]["random"]
        for x in res:
            articles.add(x["title"])
    return list(articles)[:max]


def getMetabook(articles):
    metabook = mwlib.metabook.collection()
    metabook.title = u"title test"
    metabook.subtitle = u"sub title test"
    for a in articles:
        article = mwlib.metabook.article(title=a)
        metabook.items.append(article)
    addLicense(metabook)
    return metabook


def getRandomMetabook(api, min=1, max=100):
    b = bookshelf.Bookshelf(api)
    booknames = b.booknames()
    num_articles = -1
    mbook = None
    tries = 100
    while tries and num_articles > max or num_articles < min:
        tries -=1
        if tries == 0:
            return None
        bn = random.choice(booknames)
        log.info("getRandomMetabook trying %r" % bn)
        c = api.content_query(bn)
        if not c:
            continue
        mbook = mwlib.metabook.parse_collection_page(c)
        num_articles = len(mbook.articles())
        log.info("getRandomMetabook num arts min:%d this:%d max:%d" % (min, num_articles, max))
    mbook['book_page'] = bn
    addLicense(mbook)
    return mbook

def addLicense(mbook):
    license_text = utils.fetch_url(
        LICENSE_URL,
        ignore_errors=False,
        expected_content_type='text/x-wiki',
        )
    license_text = unicode(license_text, 'utf-8')
    license = { 'mw_rights_text': license_text,
                'name': 'GNU Free Documentation License',
                }
    mbook['licenses'] = [license]

def postRenderCommand(metabook, baseurl, serviceurl, writer):
    log.info('POSTing render command %s %s' % (baseurl, writer))
    data = {
        "metabook": json.dumps(metabook),
        "writer": writer,
        "writer_options": writer_options.get(writer, ''),
        "base_url": baseurl.encode('utf-8'),
        "command":"render",
    }
    data = urllib.urlencode(data)
    res = urllib2.urlopen(urllib2.Request(serviceurl.encode("utf8"), data)).read()
    return json.loads(unicode(res, 'utf-8'))

def postRenderKillCommand(collection_id, serviceurl, writer):
    log.info('POSTing render_kill command %r' % collection_id)
    data = {
        "collection_id": collection_id,
        "writer": writer,
        "command":"render_kill",
    }
    data = urllib.urlencode(data)
    res = urllib2.urlopen(urllib2.Request(serviceurl.encode("utf8"), data)).read()
    return json.loads(unicode(res, 'utf-8'))

def getRenderStatus(colid, serviceurl, writer):
    #log.info('get render status')
    data = urllib.urlencode({"command": "render_status", "collection_id": colid, 'writer': writer})
    res = urllib2.urlopen(urllib2.Request(serviceurl.encode("utf8"), data)).read()
    return json.loads(unicode(res, 'utf-8'))

def download(colid, serviceurl, writer):
    log.info('download')
    data = urllib.urlencode({"command": "download", "collection_id": colid, 'writer': writer})
    return urllib2.urlopen(urllib2.Request(serviceurl.encode("utf8"), data)) # fh

def reportError(command, metabook, res, baseurl, writer,
    from_email=None,
    mail_recipients=None,
):
    utils.report(
        system=system,
        subject='Error %r with command %r' % (res.get('reason', '?'), command),
        error=res.get('error'),
        res=res,
        metabook=metabook,
        baseurl=baseurl,
        writer=writer,
        from_email=from_email,
        mail_recipients=mail_recipients,
    )
    sys.exc_clear()

def checkDoc(data, writer):
    log.info('checkDoc %s' % writer)
    assert len(data) > 0
    if writer == 'rl':
        fd, filename = tempfile.mkstemp(suffix='.pdf')
        os.write(fd, data)
        os.close(fd)
        try:
            popen = subprocess.Popen(args=['pdfinfo', filename], stdout=subprocess.PIPE)
            rc = popen.wait()
            assert rc == 0, 'pdfinfo rc = %d' % rc
            for line in popen.stdout:
                line = line.strip()
                if not line.startswith('Pages:'):
                    continue
                num_pages = int(line.split()[-1])
                assert num_pages > 0, 'PDF is empty'
                break
            else:
                raise RuntimeError('invalid PDF')
        finally:
            os.unlink(filename)

def checkservice(api, serviceurl, baseurl, writer, maxarticles,
                 from_email=None,
                 mail_recipients=None,
                 render_timeout = RENDER_TIMEOUT_DEFAULT # seconds or None
                 ):
#    arts = getRandomArticles(api, min=1, max=maxarticles)
#    log.info('random articles: %r' % arts)
#    metabook = getMetabook(arts)
    metabook = getRandomMetabook(api, min=5, max=maxarticles)
    if not metabook:
        reportError('render', metabook, dict(reason="getRandomMetabook Failed"), baseurl, writer,
                    from_email=from_email,
                    mail_recipients=mail_recipients)
        time.sleep(60)
                    
    res = postRenderCommand(metabook, baseurl, serviceurl, writer)
    collection_id = res['collection_id']
    st = time.time()
    while True:
        time.sleep(1)
        res = getRenderStatus(res["collection_id"], serviceurl, writer)
        if res["state"] != "progress":
            break
        if render_timeout and (time.time()-st) > render_timeout:
            log.timeout('Killing render proc for collection ID %r' % collection_id)
            r = postRenderKillCommand(collection_id, serviceurl, writer)
            if r['killed']:
                log.info('Killed.')
            else:
                log.warn('Nothing to kill!?')
            res["state"] = "failed"
            res["reason"] = "render_timeout (%ds)" % render_timeout
            break
    if res["state"] == "finished":
        d = download(res["collection_id"], serviceurl, writer).read()
        log.info("received %s document with %d bytes" % (writer, len(d)))        
        checkDoc(d, writer)
        return True
    else:
        reportError('render', metabook, res, baseurl, writer,
            from_email=from_email,
            mail_recipients=mail_recipients,
        )
    return False

    

def main():
    parser = OptionParser(usage="%prog [OPTIONS]")
    parser.add_option("-b", "--baseurl", help="baseurl of wiki")
    parser.add_option("-w", "--writer", help="writer to use")
    parser.add_option('-l', '--logfile', help='log output to LOGFILE')
    parser.add_option('-f', '--from-email',
        help='From: email address for error mails',
    )
    parser.add_option('-r', '--mail-recipients',
        help='To: email addresses ("," separated) for error mails',
    )
    parser.add_option('-m', '--max-narticles',
        help='maximum number of articles for random collections (min is 1)',
        default=10,
    )
    parser.add_option('-s', '--serviceurl',
        help="location of the mw-serve server to test",
        default='http://tools.pediapress.com/mw-serve/',
        #default='http://localhost:8899/mw-serve/',
    )
    use_help = 'Use --help for usage information.'
    options, args = parser.parse_args()   
    
    assert options.from_email

    if options.logfile:
        utils.start_logging(options.logfile)

    baseurl2api = {}
    baseurls = options.baseurl.split()
    for baseurl in baseurls:
        baseurl2api[baseurl] = mwapidb.APIHelper(baseurl)
    
    maxarts = int(options.max_narticles)
    mail_recipients = None
    if options.mail_recipients:
        mail_recipients = options.mail_recipients.split(',')
    ok_count = 0
    fail_count = 0
    while True:
        baseurl = random.choice(baseurls)
        try:
            ok = checkservice(baseurl2api[baseurl],
                options.serviceurl,
                baseurl,
                options.writer,
                maxarts,
                from_email=options.from_email,
                mail_recipients=mail_recipients,
            )
            if ok:
                ok_count += 1
                log.check('OK')
            else:
                fail_count += 1
                log.check('FAIL!')
        except KeyboardInterrupt:
            break
        except:
            fail_count += 1
            log.check('EPIC FAIL!!!')
            utils.report(
                system=system,
                subject='checkservice() failed, waiting 60seconds',
                from_email=options.from_email,
                mail_recipients=mail_recipients,
            )
            sys.exc_clear()
            time.sleep(60)
        log.info('%s, %s\tok: %d, failed: %d' % (
            baseurl, options.writer, ok_count, fail_count,
        ))


if __name__ == '__main__':
    #print getRandomMetabook(mwapidb.APIHelper("http://en.wikipedia.org/w"), min=10, max=20)
    main()
    

########NEW FILE########
__FILENAME__ = time-expr
#! /usr/bin/env python

import sys
import time
from mwlib import expr

e = []
for x in sys.stdin:
    e.append(eval(x))

print "have %s expressions" % len(e)
stime=time.time()
for x in e:
    expr.expr(u"1+2")
print "needed", time.time()-stime

########NEW FILE########
__FILENAME__ = time-parse
#! /usr/bin/env python
# -*- coding: utf-8 -*-
import sys
import time

s = unicode(open(sys.argv[1], "rb").read(), "utf-8")

from mwlib import uparser, advtree, treecleaner
from mwlib.refine import compat

stime = time.time()
r = compat.parse_txt(s)
print "parse:", time.time()-stime

stime = time.time()
advtree.buildAdvancedTree(r)
print "tree", time.time()-stime

stime = time.time()
tc = treecleaner.TreeCleaner(r)
tc.cleanAll()
print "clean:", time.time()-stime

########NEW FILE########
__FILENAME__ = memeater
#! /usr/bin/env python
# -*- coding: utf-8 -*-

from mwlib.expander import DictDB, expandstr

d4 = """
{{#if:{{{inline|}}} | dfklghsldfkghslkdfghslkdfjhglskjdfghlskjdfg }}

{{d4|
Image: {{{1}}}
{{#if:{{{Masse|}}}{{{4|}}}|{{{Masse|{{{4}}}}}}}}
{{{Alt|{{{Titel|{{{3|{{{Ziel|{{{2|&nbsp;}}}}}}}}}}}}}}}
{{#if:{{{Ziel|}}}{{{2|}}}
 |
default [[{{{Ziel|{{{2}}}}}}|{{{Titel|{{{Ziel|{{{2}}}}}}}}}]]
 |{{#if:{{{Formen|}}}
  |
  |default [[Bild:{{{Bild|{{{1}}}}}}]]
}}}}
}}
"""


def main():

    db = DictDB(d4=d4)
    expandstr(d4, wikidb=db)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = test_advtree
#! /usr/bin/env py.test
# -*- coding: utf-8 -*-
# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import sys, pytest

from mwlib.advtree import (
    PreFormatted, Text,  buildAdvancedTree, Section, BreakingReturn,  _idIndex,
    Indented, DefinitionList, DefinitionTerm, DefinitionDescription, Item, Cell, Span, Row, ImageLink
)
from mwlib.dummydb import DummyDB
from mwlib.uparser import parseString
from mwlib import parser


def _treesanity(r):
    "check that parents match their children"
    for c in r.allchildren():
        if c.parent:
            assert c in c.parent.children
            assert _idIndex(c.parent.children, c) >= 0
        for cc in c:
            assert cc.parent
            assert cc.parent is c


def test_copy():
    raw = """
===[[Leuchtturm|Leuchttürme]] auf Fehmarn===
*[[Leuchtturm Flügge]] super da
*[[Leuchtturm Marienleuchte]] da auch
*[[Leuchtturm Strukkamphuk]] spitze
*[[Leuchtturm Staberhuk]] supi
*[[Leuchtturm Westermarkelsdorf]]
""".decode("utf8")

    db = DummyDB()
    r = parseString(title="X33", raw=raw, wikidb=db)
    buildAdvancedTree(r)
    c = r.copy()
    _treesanity(c)

    def _check(n1, n2):
        assert n1.caption == n2.caption
        assert n1.__class__ == n2.__class__
        assert len(n1.children) == len(n2.children)
        for i, c1 in enumerate(n1):
            _check(c1, n2.children[i])

    _check(r, c)


def test_removeNewlines():

    # test no action within preformattet
    t = PreFormatted()
    text = u"\t \n\t\n\n  \n\n"
    tn = Text(text)
    t.children.append(tn)
    buildAdvancedTree(t)
    _treesanity(t)
    assert tn.caption == text

    # tests remove node w/ whitespace only if at border
    t = Section()
    tn = Text(text)
    t.children.append(tn)
    buildAdvancedTree(t)
    _treesanity(t)
    #assert tn.caption == u""
    assert not t.children

    # test remove newlines
    text = u"\t \n\t\n\n KEEP  \n\n"
    t = Section()
    tn = Text(text)
    t.children.append(tn)
    buildAdvancedTree(t)
    _treesanity(t)
    assert tn.caption.count("\n") == 0
    assert len(tn.caption) == len(text)
    assert t.children


def test_identity():
    raw = """
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
<br/>
""".decode("utf8")

    db = DummyDB()
    r = parseString(title="X33", raw=raw, wikidb=db)
    buildAdvancedTree(r)
    _treesanity(r)

    brs = r.getChildNodesByClass(BreakingReturn)
    for i, br in enumerate(brs):
        assert br in br.siblings
        assert i == _idIndex(br.parent.children, br)
        assert len([x for x in br.parent.children if x is not br]) == len(brs) - 1
        for bbr in brs:
            if br is bbr:
                continue
            assert br == bbr
            assert br is not bbr


# FIXME isNavBox removed from advtree. could be implemented in general treecleaner - move test there
## def test_isnavbox():
##     raw = """
## == test ==
## <div class="noprint">
## some text
## </div>
## """.decode("utf8")
##     db = DummyDB()
##     r = parseString(title="X33", raw=raw, wikidb=db)
##     buildAdvancedTree(r)
##     assert 1 == len([c for c in r.getAllChildren() if c.isNavBox()])
def test_definitiondescription():
    raw = u"""
== test ==

:One
::Two
:::Three
::::Four

"""
    db = DummyDB()
    r = parseString(title="t", raw=raw, wikidb=db)
    parser.show(sys.stdout, r)

    buildAdvancedTree(r)
    dd = r.getChildNodesByClass(DefinitionDescription)
    print "DD:", dd
    for c in dd:
        assert c.indentlevel == 1
    assert len(dd) == 4


@pytest.mark.xfail
def test_defintion_list():
    """http://code.pediapress.com/wiki/ticket/221"""
    raw = u''';termA
:descr1
'''

    for i in range(2):
        r = parseString(title='t', raw=raw)
        buildAdvancedTree(r)
        dls = r.getChildNodesByClass(DefinitionList)
        assert len(dls) == 1
        assert dls[0].getChildNodesByClass(DefinitionTerm)
        assert dls[0].getChildNodesByClass(DefinitionDescription)
        raw = raw.replace('\n', '')


def test_ulist():
    """http://code.pediapress.com/wiki/ticket/222"""
    raw = u"""
* A item
*: B Previous item continues.
"""
    r = parseString(title='t', raw=raw)
    buildAdvancedTree(r)
#    parser.show(sys.stdout, r)
    assert len(r.getChildNodesByClass(Item)) == 1


def test_colspan():
    raw = '''<table><tr><td colspan="bogus">no colspan </td></tr></table>'''
    r = parseString(title='t', raw=raw)
    buildAdvancedTree(r)
    assert r.getChildNodesByClass(Cell)[0].colspan is 1

    raw = '''<table><tr><td colspan="-1">no colspan </td></tr></table>'''
    r = parseString(title='t', raw=raw)
    buildAdvancedTree(r)
    assert r.getChildNodesByClass(Cell)[0].colspan is 1

    raw = '''<table><tr><td colspan="2">colspan1</td></tr></table>'''
    r = parseString(title='t', raw=raw)
    buildAdvancedTree(r)
    assert r.getChildNodesByClass(Cell)[0].colspan is 2


def test_attributes():
    t1 = '''
{|
|- STYLE="BACKGROUND:#FFDEAD;"
|stuff
|}
'''
    r = parseString(title='t', raw=t1)
    buildAdvancedTree(r)
    n = r.getChildNodesByClass(Row)[0]
    print n.attributes, n.style
    assert isinstance(n.style, dict)
    assert isinstance(n.attributes, dict)
    assert n.style["background"] == "#FFDEAD"


def getAdvTree(raw):
    tree = parseString(title='test', raw=raw)
    buildAdvancedTree(tree)
    return tree


def test_img_no_caption():
    raw = u'''[[Image:Chicken.jpg|image caption]]

[[Image:Chicken.jpg|none|image caption: align none]]

[[Image:Chicken.jpg|none|200px|image caption: align none, 200px]]

[[Image:Chicken.jpg|frameless|image caption frameless]]

[[Image:Chicken.jpg|none|200px|align none, 200px]]

<gallery perrow="2">
Image:Luna-16.jpg
Image:Lunokhod 1.jpg
Image:Voyager.jpg
Image:Cassini Saturn Orbit Insertion.jpg|
</gallery>
'''

    tree = getAdvTree(raw)
    images = tree.getChildNodesByClass(ImageLink)
    assert len(images) == 9
    for image in images:
        assert image.render_caption == False


def test_img_has_caption():
    raw = u'''[[Image:Chicken.jpg|thumb|image caption thumb]]

[[Image:Chicken.jpg|framed|image caption framed]]

[[Image:Chicken.jpg|none|thumb|align none, thumb]]

<gallery perrow="2">
Image:Luna-16.jpg|''[[Luna 16]]''<br>First unmanned lunar sample return
Image:Lunokhod 1.jpg|''[[Lunokhod 1]]''<br>First lunar rover
Image:Voyager.jpg|''[[Voyager 2]]''<br>First Uranus flyby<br>First Neptune flyby
Image:Cassini Saturn Orbit Insertion.jpg|''[[Cassini–Huygens]]''<br>First Saturn orbiter
</gallery>

[[Image:Horton Erythromelalgia 1939.png|center|600px|<div align="center">"Erythromelalgia of the head", Horton&nbsp;1939<ref name="BTH39"/></div>|frame]]

[[Image:Anatomie1.jpg|thumb|none|500px|Bild der anatomischen Verhältnisse. In Rot die Hauptschlagader (Carotis).]]
'''

    tree = getAdvTree(raw)
    images = tree.getChildNodesByClass(ImageLink)
    assert len(images) == 9
    for image in images:
        assert image.render_caption == True

########NEW FILE########
__FILENAME__ = test_async_import
#! /usr/bin/env py.test


def test_async_import():
    from mwlib.async import jobs
    jobs.workq

########NEW FILE########
__FILENAME__ = test_authors
#! /usr/bin/env py.test

from mwlib import authors

example_1 = [
    {u'comment': u'Dated {{Dead link}}. (Build p607)',
     u'minor': u'',
     u'parentid': 417308957,
     u'revid': 417310285,
     u'size': 925,
     u'user': u'SmackBot'},
    {u'comment': u'{{dead link}}',
     u'parentid': 397347170,
     u'revid': 417308957,
     u'size': 909,
     u'user': u'Denelson83'},
    {u'comment': u'',
     u'parentid': 304555476,
     u'revid': 397347170,
     u'size': 896,
     u'user': u'Palaeozoic99'},
    {u'comment': u'narrower cat',
     u'parentid': 228403955,
     u'revid': 304555476,
     u'size': 880,
     u'user': u'The Tom'},
    {u'comment': u'remove line',
     u'minor': u'',
     u'parentid': 222909353,
     u'revid': 228403955,
     u'size': 878,
     u'user': u'Kwanesum'},
    {u'comment': u'',
     u'parentid': 222907785,
     u'revid': 222909353,
     u'size': 884,
     u'user': u'Skookum1'},
    {u'comment': u'stub sort',
     u'parentid': 192472026,
     u'revid': 222907785,
     u'size': 884,
     u'user': u'Skookum1'},
    {u'comment': u'add category',
     u'revid': 192472026,
     u'size': 850,
     u'user': u'CosmicPenguin'},
    {u'comment': u'migrate coor to {{[[Template:Coord|coord]]}}, add name parameter  using [[Project:AutoWikiBrowser|AWB]]',
     u'minor': u'',
     u'revid': 189340607,
     u'size': 807,
     u'user': u'Qyd'},
    {u'comment': u'removing extra ]',
     u'minor': u'',
     u'parentid': 100254749,
     u'revid': 100254810,
     u'size': 783,
     u'user': u'Nationalparks'},
    {u'comment': u'expanded with link',
     u'parentid': 75720259,
     u'revid': 100254749,
     u'size': 784,
     u'user': u'Nationalparks'},
    {u'comment': u'coor, links',
     u'parentid': 47218945,
     u'revid': 75720259,
     u'size': 583,
     u'user': u'Qyd'},
    {u'comment': u'info, cat',
     u'minor': u'',
     u'parentid': 47218755,
     u'revid': 47218945,
     u'size': 457,
     u'user': u'Nationalparks'},
    {u'comment': u'new stub',
     u'parentid': 0,
     u'revid': 47218755,
     u'size': 362,
     u'user': u'Nationalparks'}]


def test_empty():
    assert authors.get_authors([]) == []


def test_example_1():
    a = authors.get_authors(example_1)
    print a
    assert a == [u'CosmicPenguin', u'Denelson83', u'Kwanesum', u'Nationalparks', u'Palaeozoic99', u'Qyd', u'Skookum1', u'The Tom', 'ANONIPEDITS:0']

########NEW FILE########
__FILENAME__ = test_convert_templates
#! /usr/bin/env py.test
# -*- coding: utf-8 -*-

"""tests for the convert macros, I can't get them to work inside our
mediawiki installation.
"""


from mwlib import expander
from mwlib.expander import expandstr, DictDB

db = {
"convert":
"""<includeonly>{{convert/{{{2}}}|{{{1}}}|{{#ifeq:{{#expr:{{{3|0}}}*0}}|0|0}}|{{{3|}}}|{{{4|}}}|{{{5|}}}|{{{6|}}}|r={{#ifeq:{{{sp}}}|us|er|re}}|d=L{{{lk|off}}}A{{{abbr|off}}}D{{{disp|b}}}S{{{adj|{{{sing|off}}}}}}|s={{{sigfig|}}}}}</includeonly><noinclude>{{pp-template|small=yes}}{{esoteric}}
{{template doc}}</noinclude>""",

"convert/ft":
"""{{convert/{{#ifeq:{{{4}}}|in|and/in|{{{d}}}}}|{{{1}}}|{{{2|}}}|{{{3|}}}|{{{4|}}}|{{{5|}}}|{{{6|}}}|s={{{s|}}}|r={{{r}}}|d={{{d}}}
|u=ft
|n=foot
|l=feet
|t=Foot (length)
|o=m
|b=0.3048
|j=-0.515985037-{{{j|0}}}}}<noinclude>{{pp-template|small=yes}}
[[Category:Subtemplates of Template Convert]]
</noinclude>
""",

"Convert/LoffAoffDbSoff":
"""{{formatnum:{{{1}}}}}&nbsp;{{#ifeq:{{{1}}}|1|{{{n}}}|{{{l|{{{n}}}s}}}}} ({{convert/{{#if:{{{2|}}}|{{{o}}}|{{{3}}}}}|{{{1}}}|{{{1}}}*{{{b}}}|{{#if:{{{2|}}}|{{{3|}}}|{{{4|}}}}}|{{{s|}}}|r={{{r}}}|j={{{j}}}|d=LoffAonSoff}})<noinclude>
[[Category:Subtemplates of Template Convert]]{{pp-template|small=yes}}
</noinclude>""",
"convert/m":
"""{{convert/{{{d}}}|{{{1}}}|{{{2|}}}|{{{3|}}}|{{{4|}}}|s={{{s|}}}|r={{{r}}}
|u=m
|n=met{{{r}}}
|t=metre
|o=ft
|b=1
|j=0-{{{j|0}}}}}<noinclude>{{pp-template|small=yes}}
[[Category:Subtemplates of Template Convert]]
</noinclude>""",

"Convert/LoffAonSoff":
"""{{convert/{{#if:{{{4|}}}|s}}{{#if:{{{3|}}}|p}}round|{{{1}}}|{{{2}}}/{{{b}}}|{{{3}}}|{{{4}}}|{{{j}}}}}&nbsp;{{{u}}}<noinclude>
[[Category:Subtemplates of Template Convert]]{{pp-template|small=yes}}
</noinclude>""",

"Convert/round":
"""{{#ifexpr:{{{2}}}=0|0|{{formatnum:{{rnd|{{{2}}}|({{max/2|{{precision/+|1{{{1}}}}}+({{{5}}}-0.1989700043)round0|1-{{ordomag|{{{2}}}}}}})}}}}}}<noinclude>
[[Category:Subtemplates of Template Convert]]{{pp-template|small=yes}}
</noinclude>""",

"Rnd":
"""<includeonly>{{rnd/+|{{{1}}}|{{{2}}}|{{rnd/0{{#expr:{{{2}}}>0}}|{{{1}}}|{{{2}}}}}}}</includeonly><noinclude>{{pp-template}}
{{template doc}}
</noinclude>""",

"Rnd/+":
"""<includeonly>{{#ifeq:{{#expr:{{{3}}}*0}}|0|{{{3}}}|{{#expr:{{{1}}}round{{{2}}}}}}}</includeonly><noinclude>{{pp-template|small=yes}}</noinclude>""",

"Max/2":
"""<includeonly>{{#ifexpr:{{{1}}}<{{{2}}}|{{{2}}}|{{{1}}}}}</includeonly><noinclude>
{{pp-template|small=yes}}2-parameter version</noinclude>""",

"Precision/+":
"""<includeonly>{{#expr:{{precision/{{#expr:3*{{{1}}}>{{{1}}}0}}|{{{1}}}}}}}</includeonly><noinclude>
{{pp-template|small=yes}}</noinclude>""",

"Precision/0":
"""<includeonly>{{precision/0{{#expr:{{{1}}}={{{1}}}round-6}}|1{{{1}}}}}</includeonly><noinclude>
{{pp-template|small=yes}}</noinclude>""",

"Precision/00":
"""<includeonly>-({{{1}}}={{{1}}}round-5)-({{{1}}}={{{1}}}round-4)-({{{1}}}={{{1}}}round-3)-({{{1}}}={{{1}}}round-2)-({{{1}}}={{{1}}}round-1)</includeonly><noinclude>
{{pp-template|small=yes}}</noinclude>""",
"Ordomag":
"""{{Ordomag/+|{{#ifexpr:{{{1}}}<0|-}}{{{1}}}}}<noinclude>{{pp-template|small=yes}}{{documentation}}</noinclude>""",

"Ordomag/+":
"""<includeonly>{{#expr:{{Ordomag/{{#expr:({{{1}}}>=1000000)-(1>{{{1}}})}}|{{{1}}}}}}}</includeonly><noinclude>
{{pp-template|small=yes}}</noinclude>""",

"Ordomag/-1":
"""{{ordomag/{{#expr:0-2*({{{1}}}<0.000001)}}|{{{1}}}*1000000}}-6""",

"Ordomag/0":
"""<includeonly>5-({{{1}}}<100000)-({{{1}}}<10000)-({{{1}}}<1000)-({{{1}}}<100)-({{{1}}}<10)</includeonly><noinclude>
{{pp-template|small=yes}}</noinclude>""",

"Rnd/01":
"""{{rnd/-|{{#expr:{{{1}}}round{{{2}}}}}|{{{2}}}}}<noinclude>{{pp-template}}</noinclude>""",

"Rnd/-":
"""<includeonly>{{#expr:{{{1}}}}}<!--
-->{{#ifexpr: {{{2}}}>0  and {{{1}}}={{{1}}}round0 |.0}}<!--
-->{{#ifexpr: {{{2}}}>1  and {{{1}}}={{{1}}}round1  |0}}<!--
-->{{#ifexpr: {{{2}}}>2  and {{{1}}}={{{1}}}round2  |0}}<!--
-->{{#ifexpr: {{{2}}}>3  and {{{1}}}={{{1}}}round3  |0}}<!--
-->{{#ifexpr: {{{2}}}>4  and {{{1}}}={{{1}}}round4  |0}}<!--
-->{{#ifexpr: {{{2}}}>5  and {{{1}}}={{{1}}}round5  |0}}<!--
-->{{#ifexpr: {{{2}}}>6  and {{{1}}}={{{1}}}round6  |0}}<!--
-->{{#ifexpr: {{{2}}}>7  and {{{1}}}={{{1}}}round7  |0}}<!--
-->{{#ifexpr: {{{2}}}>8  and {{{1}}}={{{1}}}round8  |0}}<!--
-->{{#ifexpr: {{{2}}}>9  and {{{1}}}={{{1}}}round9  |0}}<!--
-->{{#ifexpr: {{{2}}}>10 and {{{1}}}={{{1}}}round10 |0}}<!--
-->{{#ifexpr: {{{2}}}>11 and {{{1}}}={{{1}}}round11 |0}}<!--
-->{{#ifexpr: {{{2}}}>12 and {{{1}}}={{{1}}}round12 |0}}<!--
--></includeonly><noinclude>{{pp-template}}Adds trailing zeros:

*{{xpd|rnd/-|2|3}}

Used by {{tiw|rnd}}:
*{{xpd|#expr:2.0004 round 3}}
*{{xpd|rnd|2.0004|3}}
</noinclude>""",

"precision/1":
    """{{precision/-1{{#expr:{{{1}}}5={{{1}}}5round7}}|{{{1}}}5}}""",

"precision/-11":
    """6-({{{1}}}={{{1}}}round2)-({{{1}}}={{{1}}}round3)-({{{1}}}={{{1}}}round4)-({{{1}}}={{{1}}}round5)-({{{1}}}={{{1}}}round6)""",
"precision/-10":
    """{{precision/-2{{#expr:{{{1}}}={{{1}}}round13}}|{{{1}}}}}""",
"args":
    """1={{{1}}}
2={{{2}}}
3={{{3}}}
""",
}


def getdb():
    return DictDB(**db)


def test_round():
    expandstr("{{rnd|2.0004|3}}", "2.000", wikidb=getdb())
    expandstr("{{rnd|0.000020004|8}}", "2.0E-5000", wikidb=getdb())
    expandstr("{{rnd|0|8}}", "0.00000000", wikidb=getdb())


def test_max_2():
    expandstr("{{max/2|-1|1}}", "1", wikidb=getdb())
    expandstr("{{max/2|1|-1}}", "1", wikidb=getdb())


def test_round_plus_1():
    expandstr("{{rnd/+|1.056|2|5-1}}", "1.06", wikidb=getdb())


def test_round_plus_2():
    expandstr("{{rnd/+|1.056|2|5}}", "5", wikidb=getdb())


def test_round_plus_3():
    expandstr("{{rnd/+|1.056|2|abc}}", "1.06", wikidb=getdb())


def test_precision_plus_1():
    expandstr("{{precision/+|0.77}}", "2", wikidb=getdb())


def test_convert_ft_in_m_float():
    expandstr("{{convert|2.5|ft|m}}", "2.5&nbsp;feet (0.76&nbsp;m)\n", wikidb=getdb())


def test_convert_ft_in_m_int():
    expandstr("{{convert|12|ft|m}}", "12&nbsp;feet (3.7&nbsp;m)\n", wikidb=getdb())


def test_round_minus():
    expandstr("{{rnd/-|0.00002|8}}", "2.0E-5000", wikidb=getdb())

########NEW FILE########
__FILENAME__ = test_ebad
#! /usr/bin/env py.test

from mwlib.refine import core
from mwlib.utoken import token as T


def test_ebad_in_text():
    txt = T.join_as_text(core.parse_txt(u"foo\uebadbar"))
    assert txt == "foobar", "\uebad should be stripped"

########NEW FILE########
__FILENAME__ = test_expander
#! /usr/bin/env py.test
# -*- coding: utf-8 -*-

from mwlib import expander
from mwlib.expander import expandstr, DictDB
from mwlib.dummydb import DummyDB


def parse_and_show(s):
    res = expander.parse(s)
    print "PARSE:", repr(s)
    expander.show(res)
    return res


def test_noexpansion_inside_pre():
    res = expandstr("<pre>A{{Pipe}}B</pre>", "<pre>A{{Pipe}}B</pre>", wikidb=DictDB(Pipe="C"))
    print res


def test_undefined_variable():
    db = DictDB(Art="{{Pipe}}",
                Pipe="{{{undefined_variable}}}")

    te = expander.Expander(db.normalize_and_get_page("Art", 0).rawtext, pagename="thispage", wikidb=db)
    res = te.expandTemplates()
    print "EXPANDED:", repr(res)
    assert u"{{{undefined_variable}}}" in res, "wrong expansion for undefined variable"


def test_birth_date_and_age():
    db = DictDB({
            "birth date and age": '[[ {{{3|{{{day|{{{3}}}}}}}}}]] [[{{{1|{{{year|{{{1}}}}}}}}}]]<font class="noprint"> (age&nbsp;{{age | {{{1|{{{year|{{{1}}}}}}}}} | {{{2|{{{month|{{{2}}}}}}}}} | {{{3|{{{day|{{{3}}}}}}}}} }})</font>',

            "age": '<includeonly>{{#expr:({{{4|{{CURRENTYEAR}}}}})-({{{1}}})-(({{{5|{{CURRENTMONTH}}}}})<({{{2}}})or({{{5|{{CURRENTMONTH}}}}})=({{{2}}})and({{{6|{{CURRENTDAY}}}}})<({{{3}}}))}}</includeonly>',
            })
    res = expandstr('{{birth date and age|1960|02|8}}', wikidb=db)

    print "EXPANDED:", repr(res)
    import datetime
    now = datetime.datetime.now()
    b = datetime.datetime(1960, 2, 8)
    age = now.year - b.year
    if now.month * 32 + now.day < b.month * 32 + b.day:
        age -= 1

    expected = u"age&nbsp;%s" % age
    assert expected in res


def test_five():
    txt = "text of the tnext template"
    db = DictDB(
        t1="{{{{{1}}}}}",
        tnext=txt)
    expandstr("{{t1|tnext}}", expected=txt, wikidb=db)


def test_five_parser():
    n = parse_and_show("{{{{{1}}}}}")
    assert isinstance(n, expander.Template)


def test_five_two_three():
    n = parse_and_show("{{{{{1}} }}}")
    assert isinstance(n, expander.Variable)


def test_five_three_two():
    n = parse_and_show("{{{{{1}}} }}")
    assert isinstance(n, expander.Template)


def test_alfred():
    """I start to hate that Alfred_Gusenbauer"""
    db = DictDB(
        a="{{ibox2|birth_date=1960}}",
        ibox2="{{{birth{{#if:{{{birthdate|}}}||_}}date}}}"
        )
    te = expander.Expander(db.normalize_and_get_page("a", 0).rawtext, pagename="thispage", wikidb=db)
    res = te.expandTemplates()
    print "EXPANDED:", repr(res)
    assert "1960" in res


def test_switch_empty_fallback():
    expandstr("{{#switch:||foo=good}}", "good")


def test_switch_numeric_comparison():
    expandstr("{{ #switch: +07 | 7 = Yes | 007 = Bond | No }}", "Yes")


def test_switch_case_sensitive1():
    expandstr("{{ #switch: A | a=lower | A=UPPER }}", "UPPER")


def test_switch_case_sensitive2():
    expandstr("{{ #switch: A | a=lower | UPPER }}", "UPPER")


def test_switch_case_sensitive3():
    expandstr("{{ #switch: a | a=lower | UPPER }}", "lower")


def test_switch_fall_through():
    expandstr("{{#switch: a| a | b |c=first|default}}", "first")


def test_switch_fall_through_computed():
    expandstr("{{#switch:aaa|{{#if:1|aaa}}|b=fine}}", "fine")


def test_names_insensitive():
    expandstr("{{ #SWItch: A | a=lower | UPPER }}", "UPPER")


def test_ifeq_numeric_comparison():
    expandstr("{{ #ifeq: +07 | 007 | 1 | 0 }}", "1")


def test_ifeq_numeric_comparison2():
    expandstr('{{ #ifeq: "+07" | "007" | 1 | 0 }}', '0')


def test_ifeq_case_sensitive():
    expandstr("{{ #ifeq: A | a | 1 | 0 }}", "0")


def test_ifeq_strip():
    """http://code.pediapress.com/wiki/ticket/260"""
    expandstr("{{#ifeq: bla |    bla  |yes|no}}", "yes")


def test_ifexpr():
    expandstr("{{ #ifexpr: 10 > 9 | yes | no }}", "yes")


def test_expr_round():
    """round creates integers if it can"""
    expandstr("{{#expr: 10.0443 round -1}}", "10")


def test_expr_round2():
    expandstr("{{#expr: 10.0443 round 2}}", "10.04")


def test_too_many_args():
    expandstr("{{LC:AB|CD}}", "ab")


def test_lc_named_arg():
    expandstr("{{LC:a=AB|CD}}", "a=ab")


def test_named_variable_whitespace():
    """http://code.pediapress.com/wiki/ticket/23"""

    expandstr("{{doit|notable roles=these are the notable roles}}",
              "these are the notable roles",
              wikidb=DictDB(doit="{{{notable roles}}}"))


def test_pipe_inside_imagemap():
    """pipes inside image maps should not separate template arguments
    well, they do not separate arguments with the version running on en.wikipedia.org.
    they do separate arguments with the version running on pediapress.com:8080.
    (which is hopefully a newer version)
    """

    db = DictDB(
        sp="""{{#ifeq: {{{1}}} | 1
| <imagemap>
 Image:Padlock-silver-medium.svg|20px
 rect 0 0 1000 1000 [[Wikipedia:Protection policy|This page has been semi-protected from editing]]
 desc none
 </imagemap>
|bla
}}
""")
    result = expandstr("{{sp|1}}", wikidb=db)
    assert "</imagemap>" in result


def test_expand_comment():
    s = "foo\n     <!-- comment --->     \nbar"
    e = "foo\nbar"
    expandstr(s, e)


def test_tokenize_gallery():
    gall = """
<gallery caption="Sample gallery" widths="100px" heights="100px" perrow="6">
Image:Drenthe-Position.png|[[w:Drenthe|Drenthe]], the least crowded province
Image:Friesland-Position.png|[[w:Friesland|Friesland]] has many lakes
</gallery>
"""
    tokens = expander.tokenize(gall)
    g = tokens[1][1]
    assert g.startswith("<gallery")
    assert g.endswith("</gallery>")


def test_template_name_colon():
    """http://code.pediapress.com/wiki/ticket/36
    """
    p = parse_and_show("{{Template:foobar}}")
    assert isinstance(p, expander.Template), 'expected a template'
    assert p[0] == u'Template:foobar'


def test_expand_parser_func_name():
    expandstr("{{ {{NZ}}expr: 1+1}}", "2",
              wikidb=DictDB(NZ="#"))


def test_expand_name_with_colon():
    wikidb = DictDB()
    wikidb.d['bla:blubb'] = 'foo'
    expandstr("{{bla:blubb}}", "foo", wikidb=wikidb)


def test_parser_func_from_template():
    expandstr("{{ {{bla}} 1 + 1}}", "2", wikidb=DictDB(bla="#expr:"))


def test_bad_expr_name():
    s = expandstr("{{expr:1+1}}")  # '#' missing
    assert s != '2', "bad result"


def test_parmpart():
    parmpart = """{{#ifeq:/{{{2|}}}
|{{#titleparts:/{{{2|}}}|1|{{#expr:1+{{{1|1}}}}}}}
|
|{{#titleparts:/{{{2|}}}|1|{{#expr:1+{{{1|1}}}}}}}
}}"""
    expandstr("{{ParmPart|0|a/b}}", "", wikidb=DictDB(ParmPart=parmpart))
    expandstr("{{ParmPart|1|a/b}}", "a", wikidb=DictDB(ParmPart=parmpart))
    expandstr("{{ParmPart|2|a/b}}", "b", wikidb=DictDB(ParmPart=parmpart))
    expandstr("{{ParmPart|3|a/b}}", "", wikidb=DictDB(ParmPart=parmpart))


def test_titleparts():
    expandstr("{{#titleparts:Help:Link/a/b|0|}}", "Help:Link/a/b")
    expandstr("{{#titleparts:Help:Link/a/b|1|}}", "Help:Link")
    expandstr("{{#titleparts:Help:Link/a/b|2|}}", "Help:Link/a")
    expandstr("{{#titleparts:Help:Link/a/b|3|}}", "Help:Link/a/b")
    expandstr("{{#titleparts:Help:Link/a/b|4|}}", "Help:Link/a/b")


def test_titleparts_2params():
    expandstr("{{#titleparts:Help:Link/a/b|2|2}}", "a/b")
    expandstr("{{#titleparts:Help:Link/a/b|1|2}}", "a")
    expandstr("{{#titleparts:Help:Link/a/b|1|3}}", "b")


def test_titleparts_negative():
    expandstr("{{#titleparts:Help:Link/a/b|-1|}}", "Help:Link/a")
    expandstr("{{#titleparts:Help:Link/a/b|1|-1|}}", "b")


def test_titleparts_nonint():
    expandstr("{{#titleparts:Help:Link/a/b|bla}}", "Help:Link/a/b")


def test_iferror():
    yield expandstr, "{{#iferror:{{#expr:1+1}}|bad input|valid expression}}", "valid expression"
    yield expandstr, "{{#iferror:{{#expr:1+Z}}|bad input|valid expression}}", "bad input"
    yield expandstr, "{{#iferror:{{#expr:1+1}}|bad input}}", "2"
    yield expandstr, "{{#iferror:{{#expr:1+Z}}|bad input}}", "bad input"
    yield expandstr, "{{#iferror:{{#expr:1+1}}}}", "2"
    yield expandstr, "{{#iferror:{{#expr:1+Z}}}}", ""
    yield expandstr, "{{#iferror:good|bad input|}}", ""


def test_no_implicit_newline():
    yield expandstr, "foo\n{{#if: 1|#bar}}", "foo\n#bar"
    yield expandstr, "{{#if: 1|#bar}}", "#bar"


def test_implicit_newline_noinclude():
    expandstr("foo {{tt}}", "foo \n{|", wikidb=DictDB(tt="<noinclude></noinclude>{|"))


def test_implicit_newline_includeonly():
    expandstr("foo {{tt}}", "foo \n{|", wikidb=DictDB(tt="<includeonly>{|</includeonly>"))


def test_implicit_newline_begintable():
    expandstr("foo {{tt}}", "foo \n{|", wikidb=DictDB(tt="{|"))


def test_implicit_newline_colon():
    expandstr("foo {{tt}}", "foo \n:", wikidb=DictDB(tt=":"))


def test_implicit_newline_semicolon():
    expandstr("foo {{tt}}", "foo \n;", wikidb=DictDB(tt=";"))


def test_implicit_newline_ifeq():
    expandstr("{{#ifeq: 1 | 1 | foo {{#if: 1 | {{{!}} }}}}", "foo \n{|", wikidb=DictDB({"!": "|"}))


def test_empty_template():
    """test for http://code.pediapress.com/wiki/ticket/126"""
    expandstr("{{}}", "")


def test_implicit_newline_magic():
    expandstr("foo {{#if: 1 | :xxx }} bar", "foo \n:xxx bar")
    expandstr("foo {{#ifexpr: 1 | #xxx }} bar", "foo \n#xxx bar")


def test_implicit_newline_switch():
    """http://code.pediapress.com/wiki/ticket/386"""
    expandstr("* foo{{#switch:bar|bar=* bar}}", "* foo\n* bar")


def test_implicit_newline_inner():
    yield expandstr, "ab {{#if: 1| cd {{#if:||#f9f9f9}}}} ef", "ab cd \n#f9f9f9 ef"
    yield expandstr, "ab {{#switch: 1| 1=cd {{#if:||#f9f9f9}}}} ef", "ab cd \n#f9f9f9 ef"
    yield expandstr, "ab{{#tag:imagemap|{{#if:1|#abc}} }}ef", "ab<imagemap>\n#abc </imagemap>ef"


def test_implicit_newline_param():
    """http://code.pediapress.com/wiki/ticket/877"""
    wikidb = DictDB(dict(foo="foo{{{1}}}", bar="{|", baz="|"))

    def doit(a, b):
        expandstr(a, b, wikidb=wikidb)

    yield doit, "{{foo|{{bar}}}}", "foo\n{|"
    yield doit, "{{foo|1={{bar}}}}", "foo{|"
    yield doit, "{{foo|{{{baz}} }}", "foo{| "
    yield doit, "{{foo|\nbar\n}}baz", "foo\nbar\nbaz"
    yield doit, "{{foo| bar }}baz", "foo bar baz"


def test_expand_after_named():
    db = DictDB(
        show="{{{1}}}",
        a="a=bc")
    expandstr("{{show|{{a}}}}", "a=bc",  wikidb=db)


def test_padleft():
    yield expandstr, "{{padleft:7|3|0}}", "007"
    yield expandstr, "{{padleft:0|3|0}}", "000"
    yield expandstr, "{{padleft:bcd|6|a}}", "aaabcd"
    yield expandstr, "{{padleft:|3|abcde}}", "abc"
    yield expandstr, "{{padleft:bla|5|xyz}}", "xybla"
    yield expandstr, "{{padleft:longstring|3|abcde}}", "longstring"
    # {{padleft:cafe|8|-}} = ----cafe


def test_padright():
    yield expandstr, "{{padright:bcd|6|a}}", "bcdaaa"
    yield expandstr, "{{padright:0|6|a}}", "0aaaaa"
    yield expandstr, "{{padright:|3|abcde}}", "abc"
    yield expandstr, "{{padright:bla|5|xyz}}", "blaxy"
    yield expandstr, "{{padright:longstring|3|abcde}}", "longstring"


def test_urlencode():
    expandstr('{{urlencode:x y @}}', 'x+y+%40')


def test_urlencode_non_ascii():
    expandstr(u'{{urlencode:L\xe9onie}}', 'L%C3%A9onie')


def test_anchorencode():
    """http://code.pediapress.com/wiki/ticket/213"""
    expandstr('{{anchorencode:x #y @}}', 'x_.23y_.40')


def test_anchorencode_non_ascii():
    expandstr(u"{{anchorencode:\u0107}}", ".C4.87")


def test_fullurl():
    expandstr('{{fullurl:x y @}}', 'http://en.wikipedia.org/wiki/X_y_%40')


def test_fullurl_nonascii():
    expandstr(u'{{fullurl:L\xe9onie}}', 'http://en.wikipedia.org/wiki/L%C3%A9onie')


def test_server():
    expandstr('{{server}}', 'http://en.wikipedia.org')


def test_servername():
    expandstr('{{servername}}', 'en.wikipedia.org')


def test_1x_newline_and_spaces():
    # see http://en.wikipedia.org/wiki/Help:Newlines_and_spaces#Spaces_and.2For_newlines_as_value_of_an_unnamed_parameter
    wikidb = DictDB()
    wikidb.d['1x'] = '{{{1}}}'

    def e(a, b):
        return expandstr(a, b, wikidb=wikidb)

    yield e, 'a{{#if:1|\n}}b', 'ab'
    yield e, 'a{{#if:1|b\n}}c', 'abc'
    yield e, 'a{{#if:1|\nb}}c', 'abc'

    yield e, 'a{{1x|\n}}b', 'a\nb'
    yield e, 'a{{1x|b\n}}c', 'ab\nc'
    yield e, 'a{{1x|\nb}}c', 'a\nbc'

    yield e, 'a{{1x|1=\n}}b', 'ab'

    yield e, 'a{{1x|1=b\n}}c', 'abc'
    yield e, 'a{{1x|1=\nb}}c', 'abc'


def test_variable_alternative():
    wikidb = DictDB(t1='{{{var|undefined}}}')
    expandstr('{{t1|var=}}', '', wikidb=wikidb)


def test_implicit_newline_after_expand():
    wikidb = DictDB(tone='{{{1}}}{{{2}}}')
    expandstr('foo {{tone||:}} bar', 'foo \n: bar', wikidb=wikidb)


def test_pagename_non_ascii():
    def e(a, b):
        return expandstr(a, b, pagename=u'L\xe9onie s')
    yield e, '{{PAGENAME}}', u'L\xe9onie s'
    yield e, '{{PAGENAMEE}}', 'L%C3%A9onie_s'

    yield e, '{{BASEPAGENAME}}', u'L\xe9onie s'
    yield e, '{{BASEPAGENAMEE}}', 'L%C3%A9onie_s'

    yield e, '{{FULLPAGENAME}}', u'L\xe9onie s'
    yield e, '{{FULLPAGENAMEE}}', 'L%C3%A9onie_s'

    yield e, '{{SUBPAGENAME}}', u'L\xe9onie s'
    yield e, '{{SUBPAGENAMEE}}', 'L%C3%A9onie_s'


def test_get_templates():
    def doit(source, expected):
        r = expander.get_templates(source, u'')
        assert r == expected, "expected %r, got %r" % (expected, r)

    yield doit, "{{foo| {{ bar }} }}", set("foo bar".split())
    yield doit, "{{foo{{{1}}} }}", set()
    yield doit, "{{{ {{foo}} }}}", set(['foo'])
    yield doit, "{{ #if: {{{1}}} |yes|no}}", set()


def test_noinclude_end():
    expandstr("{{foo}}", "foo", wikidb=DictDB(foo="foo<noinclude>bar should not be in expansion"))


def test_monthnumber():
    wikidb = DictDB(MONTHNUMBER="{{#if:{{{1|}}}|{{#switch:{{lc:{{{1}}}}}|january|jan=1|february|feb=2|march|mar=3|apr|april=4|may=5|june|jun=6|july|jul=7|august|aug=8|september|sep=9|october|oct=10|november|nov=11|december|dec=12|{{#ifexpr:{{{1}}}<0|{{#ifexpr:(({{{1}}})round 0)!=({{{1}}})|{{#expr:12-(((0.5-({{{1}}}))round 0)mod 12)}}|{{#expr:12-(((11.5-({{{1}}}))round 0)mod 12)}}}}|{{#expr:(((10.5+{{{1}}})round 0)mod 12)+1}}}}}}|Missing required parameter 1=''month''!}}")

    expandstr("{{MONTHNUMBER|12}}", "12", wikidb=wikidb)


def test_switch_default_template():
    expandstr("{{#switch:1|{{#if:1|5|12}}}}", "5")


def test_preserve_space_in_tag():
    expandstr("{{#tag:imagemap|cd }}", "<imagemap>cd </imagemap>")


def test_localurle_umlaut():
    """http://code.pediapress.com/wiki/ticket/473"""
    r = expandstr(u"{{LOCALURLE:F\xfcbar}}")
    assert r.endswith('/F%C3%BCbar')


def test_equal_inside_link():
    db = DictDB(t1="{{{1}}}")
    expandstr("{{t1|[[abc|foo=5]]}}", "[[abc|foo=5]]", wikidb=db)


def test_tag_parametrs():
    yield expandstr, '{{#tag:test|contents|a=b|c=d}}', '<test a="b" c="d">contents</test>'
    yield expandstr, "{{#tag:div|contents|a}}"


def test_rel2abs():
    yield expandstr, "{{#rel2abs: ./quok | Help:Foo/bar/baz }}", "Help:Foo/bar/baz/quok"
    yield expandstr, "{{#rel2abs: ../quok | Help:Foo/bar/baz }}", "Help:Foo/bar/quok"
    yield expandstr, "{{#rel2abs: ../. | Help:Foo/bar/baz }}", "Help:Foo/bar"

    yield expandstr, "{{#rel2abs: ../quok/. | Help:Foo/bar/baz }}", "Help:Foo/bar/quok"
    yield expandstr, "{{#rel2abs: ../../quok | Help:Foo/bar/baz }}", "Help:Foo/quok"
    yield expandstr, "{{#rel2abs: ../../../quok | Help:Foo/bar/baz }}", "quok"
    yield expandstr, "{{#rel2abs: abc | foo}}", "abc"
    yield expandstr, "{{#rel2abs: /abc | foo}}", "foo/abc"


def test_namespace():
    yield expandstr, "{{NAMESPACE}}", "Benutzer", None, "User:Schmir"
    yield expandstr, "{{NAMESPACE}}", ""
    yield expandstr, "{{NAMESPACE:Mainz}}", ""
    yield expandstr, "{{NAMESPACE:User_talk:Schmir}}", "Benutzer Diskussion"
    yield expandstr, "{{NAMESPACE:User talk:Schmir}}", "Benutzer Diskussion"
    yield expandstr, "{{NAMESPACE:  benutzer diskussion:Schmir}}", "Benutzer Diskussion"


def test_pagename():
    yield expandstr, "{{PAGENAME}}", "Thispage"
    yield expandstr, "{{PAGENAME|Mainz}}", "Mainz"
    yield expandstr, "{{PAGENAME:User:Schmir}}", "Schmir"
    yield expandstr, "{{PAGENAME:acdc}}", "Acdc"


def test_namespace_as_template_type_error():
    """https://github.com/pediapress/mwlib/issues/3"""
    yield expandstr, "{{NAMESPACE|}}"
    yield expandstr, "{{NAMESPACE|foo}}"
    yield expandstr, "{{NAMESPACE|foo|bla}}"
    yield expandstr, "{{NAMESPACE||bla}}"


def test_preprocess_uniq_after_comment():
    s = u"""
<!--
these <ref> tags should be ignored: <ref>
-->

foo was missing<ref>bar</ref> <!-- some comment--> baz


<references />
"""
    e = expander.Expander(s,  pagename="test",  wikidb=DictDB())
    raw = e.expandTemplates()
    print repr(raw)
    assert u"foo was missing" in raw, "text is missing"


def test_dynamic_parserfun():
    yield expandstr, "{{{{#if: 1|}}#time: Y-m-d | 2009-1-2}}", "2009-01-02"

    yield expandstr, "{{{{#if: 1|}}#switch: A | a=lower | A=UPPER }}", "UPPER"

    yield expandstr, "{{{{#if: 1|}}#if: 1 | yes}}", "yes"


def test_iferror_switch_default():
    """http://code.pediapress.com/wiki/ticket/648"""
    yield expandstr, "{{#iferror: [[foo {{bar}}]] | yes|no}}", "no"
    yield expandstr, u"""{{#switch: bla
| #default = {{#iferror: [[foo {{bar}}]] | yes|no}}
}}""", "no"


def test_variable_subst():
    yield expandstr, "{{{{{subst|}}}#if: 1| yes| no}}", "yes"
    yield expandstr, "{{{{{subst|}}}#expr: 1+1}}", "2"
    yield expandstr, "{{{{{susbst|}}}#ifexpr: 1+1|yes|no}}", "yes"


def test_link_vs_expander():
    """http://code.pediapress.com/wiki/ticket/752"""
    yield expandstr, "{{#if: 1|  [[foo|bar}}123", "{{#if: 1|  [[foo|bar}}123"


def test_pagemagic():
    def expand_page(tpl, expected):
        return expandstr('{{%s}}' % tpl, expected,
                pagename='Benutzer:Anonymous user!/sandbox/my page')

    def expand_talk(tpl, expected):
        return expandstr('{{%s}}' % tpl, expected,
                pagename='Benutzer Diskussion:Anonymous user!/sandbox/my page')

    yield expand_page, 'PAGENAME', 'Anonymous user!/sandbox/my page'
    yield expand_page, 'PAGENAMEE', 'Anonymous_user%21/sandbox/my_page'
    yield expand_talk, 'PAGENAME', 'Anonymous user!/sandbox/my page'
    yield expand_talk, 'PAGENAMEE', 'Anonymous_user%21/sandbox/my_page'
    yield expand_page, 'BASEPAGENAME', 'Anonymous user!/sandbox'
    yield expand_page, 'BASEPAGENAMEE', 'Anonymous_user%21/sandbox'
    yield expand_talk, 'BASEPAGENAME', 'Anonymous user!/sandbox'
    yield expand_talk, 'BASEPAGENAMEE', 'Anonymous_user%21/sandbox'
    yield expand_page, 'SUBPAGENAME', 'my page'
    yield expand_page, 'SUBPAGENAMEE', 'my_page'
    yield expand_talk, 'SUBPAGENAME', 'my page'
    yield expand_talk, 'SUBPAGENAMEE', 'my_page'
    yield expand_page, 'NAMESPACE', 'Benutzer'
    yield expand_page, 'NAMESPACEE', 'Benutzer'
    yield expand_talk, 'NAMESPACE', 'Benutzer Diskussion'
    yield expand_talk, 'NAMESPACEE', 'Benutzer_Diskussion'
    yield expand_page, 'FULLPAGENAME', 'Benutzer:Anonymous user!/sandbox/my page'
    yield expand_page, 'FULLPAGENAMEE', 'Benutzer%3AAnonymous_user%21/sandbox/my_page'
    yield expand_talk, 'FULLPAGENAME', 'Benutzer Diskussion:Anonymous user!/sandbox/my page'
    yield expand_talk, 'FULLPAGENAMEE', 'Benutzer_Diskussion%3AAnonymous_user%21/sandbox/my_page'
    yield expand_page, 'TALKSPACE', 'Benutzer Diskussion'
    yield expand_page, 'TALKSPACEE', 'Benutzer_Diskussion'
    yield expand_talk, 'TALKSPACE', 'Benutzer Diskussion'
    yield expand_talk, 'TALKSPACEE', 'Benutzer_Diskussion'
    yield expand_page, 'SUBJECTSPACE', 'Benutzer'
    yield expand_page, 'SUBJECTSPACEE', 'Benutzer'
    yield expand_talk, 'SUBJECTSPACE', 'Benutzer'
    yield expand_talk, 'SUBJECTSPACEE', 'Benutzer'
    yield expand_page, 'ARTICLESPACE', 'Benutzer'
    yield expand_page, 'ARTICLESPACEE', 'Benutzer'
    yield expand_talk, 'ARTICLESPACE', 'Benutzer'
    yield expand_talk, 'ARTICLESPACEE', 'Benutzer'
    yield expand_page, 'TALKPAGENAME', 'Benutzer Diskussion:Anonymous user!/sandbox/my page'
    yield expand_page, 'TALKPAGENAMEE', 'Benutzer_Diskussion%3AAnonymous_user%21/sandbox/my_page'
    yield expand_talk, 'TALKPAGENAME', 'Benutzer Diskussion:Anonymous user!/sandbox/my page'
    yield expand_talk, 'TALKPAGENAMEE', 'Benutzer_Diskussion%3AAnonymous_user%21/sandbox/my_page'
    yield expand_page, 'SUBJECTPAGENAME', 'Benutzer:Anonymous user!/sandbox/my page'
    yield expand_page, 'SUBJECTPAGENAMEE', 'Benutzer%3AAnonymous_user%21/sandbox/my_page'
    yield expand_talk, 'SUBJECTPAGENAME', 'Benutzer:Anonymous user!/sandbox/my page'
    yield expand_talk, 'SUBJECTPAGENAMEE', 'Benutzer%3AAnonymous_user%21/sandbox/my_page'
    yield expand_page, 'ARTICLEPAGENAME', 'Benutzer:Anonymous user!/sandbox/my page'
    yield expand_page, 'ARTICLEPAGENAMEE', 'Benutzer%3AAnonymous_user%21/sandbox/my_page'
    yield expand_talk, 'ARTICLEPAGENAME', 'Benutzer:Anonymous user!/sandbox/my page'
    yield expand_talk, 'ARTICLEPAGENAMEE', 'Benutzer%3AAnonymous_user%21/sandbox/my_page'


def test_pagemagic_with_arg():
    def expand_page(tpl, expected):
        return expandstr('{{%s:%s}}' % (tpl, 'Benutzer:Anonymous user!/sandbox/my page'),
                expected, pagename='Help:Irrelevant')

    def expand_talk(tpl, expected):
        return expandstr('{{%s:%s}}' % (tpl, 'Benutzer Diskussion:Anonymous user!/sandbox/my page'),
                expected, pagename='Help:Irrelevant')

    yield expand_page, 'PAGENAME', 'Anonymous user!/sandbox/my page'
    yield expand_page, 'PAGENAMEE', 'Anonymous_user%21/sandbox/my_page'
    yield expand_talk, 'PAGENAME', 'Anonymous user!/sandbox/my page'
    yield expand_talk, 'PAGENAMEE', 'Anonymous_user%21/sandbox/my_page'
    yield expand_page, 'BASEPAGENAME', 'Anonymous user!/sandbox'
    yield expand_page, 'BASEPAGENAMEE', 'Anonymous_user%21/sandbox'
    yield expand_talk, 'BASEPAGENAME', 'Anonymous user!/sandbox'
    yield expand_talk, 'BASEPAGENAMEE', 'Anonymous_user%21/sandbox'
    yield expand_page, 'SUBPAGENAME', 'my page'
    yield expand_page, 'SUBPAGENAMEE', 'my_page'
    yield expand_talk, 'SUBPAGENAME', 'my page'
    yield expand_talk, 'SUBPAGENAMEE', 'my_page'
    yield expand_page, 'NAMESPACE', 'Benutzer'
    yield expand_page, 'NAMESPACEE', 'Benutzer'
    yield expand_talk, 'NAMESPACE', 'Benutzer Diskussion'
    yield expand_talk, 'NAMESPACEE', 'Benutzer_Diskussion'
    yield expand_page, 'FULLPAGENAME', 'Benutzer:Anonymous user!/sandbox/my page'
    yield expand_page, 'FULLPAGENAMEE', 'Benutzer%3AAnonymous_user%21/sandbox/my_page'
    yield expand_talk, 'FULLPAGENAME', 'Benutzer Diskussion:Anonymous user!/sandbox/my page'
    yield expand_talk, 'FULLPAGENAMEE', 'Benutzer_Diskussion%3AAnonymous_user%21/sandbox/my_page'
    yield expand_page, 'TALKSPACE', 'Benutzer Diskussion'
    yield expand_page, 'TALKSPACEE', 'Benutzer_Diskussion'
    yield expand_talk, 'TALKSPACE', 'Benutzer Diskussion'
    yield expand_talk, 'TALKSPACEE', 'Benutzer_Diskussion'
    yield expand_page, 'SUBJECTSPACE', 'Benutzer'
    yield expand_page, 'SUBJECTSPACEE', 'Benutzer'
    yield expand_talk, 'SUBJECTSPACE', 'Benutzer'
    yield expand_talk, 'SUBJECTSPACEE', 'Benutzer'
    yield expand_page, 'ARTICLESPACE', 'Benutzer'
    yield expand_page, 'ARTICLESPACEE', 'Benutzer'
    yield expand_talk, 'ARTICLESPACE', 'Benutzer'
    yield expand_talk, 'ARTICLESPACEE', 'Benutzer'
    yield expand_page, 'TALKPAGENAME', 'Benutzer Diskussion:Anonymous user!/sandbox/my page'
    yield expand_page, 'TALKPAGENAMEE', 'Benutzer_Diskussion%3AAnonymous_user%21/sandbox/my_page'
    yield expand_talk, 'TALKPAGENAME', 'Benutzer Diskussion:Anonymous user!/sandbox/my page'
    yield expand_talk, 'TALKPAGENAMEE', 'Benutzer_Diskussion%3AAnonymous_user%21/sandbox/my_page'
    yield expand_page, 'SUBJECTPAGENAME', 'Benutzer:Anonymous user!/sandbox/my page'
    yield expand_page, 'SUBJECTPAGENAMEE', 'Benutzer%3AAnonymous_user%21/sandbox/my_page'
    yield expand_talk, 'SUBJECTPAGENAME', 'Benutzer:Anonymous user!/sandbox/my page'
    yield expand_talk, 'SUBJECTPAGENAMEE', 'Benutzer%3AAnonymous_user%21/sandbox/my_page'
    yield expand_page, 'ARTICLEPAGENAME', 'Benutzer:Anonymous user!/sandbox/my page'
    yield expand_page, 'ARTICLEPAGENAMEE', 'Benutzer%3AAnonymous_user%21/sandbox/my_page'
    yield expand_talk, 'ARTICLEPAGENAME', 'Benutzer:Anonymous user!/sandbox/my page'
    yield expand_talk, 'ARTICLEPAGENAMEE', 'Benutzer%3AAnonymous_user%21/sandbox/my_page'


def test_ns():
    """http://code.pediapress.com/wiki/ticket/902"""
    yield expandstr, "{{NS:2}}", "Benutzer"


def test_localized_expander():
    db = DummyDB("nl")
    e = expander.Expander(u"{{#als: 1 | yes | no}}", wikidb=db)
    res = e.expandTemplates()
    assert res == "yes"


def test_localized_switch_default():
    db = DummyDB("nl")
    e = expander.Expander(u"{{#switch: 1 | #standaard=foobar}}", wikidb=db)
    res = e.expandTemplates()
    assert res == "foobar"


def test_localized_expr():
    db = DummyDB("nl")
    e = expander.Expander(u"{{#expressie: 1+2*3}}", wikidb=db)
    res = e.expandTemplates()
    assert res == "7"


def test_resolve_magic_alias():
    db = DummyDB("nl")
    e = expander.Expander(u"{{#als: 1 | yes | no}}", wikidb=db)
    assert e.resolve_magic_alias(u"#als") == u"#if"
    assert e.resolve_magic_alias(u"#foobar") is None


def test_safesubst():
    yield expandstr, "{{safesubst:#expr:1+2}}", "3"
    yield expandstr, "{{{{{|safesubst:}}}#expr:1+3}}", "4"
    yield expandstr, "{{safesubst:#if: 1| yes | no}}", "yes"

########NEW FILE########
__FILENAME__ = test_expander_parser
#! /usr/bin/env py.test

from mwlib.expander import parse
from mwlib.siteinfo import get_siteinfo
from mwlib.templ import nodes, magic_nodes

nl_siteinfo = get_siteinfo("nl")


def test_no_arguments():
    t = parse(u"{{bla}}")
    assert t[1] == (), "expected an empty tuple"


def test_one_empty_arguments():
    t = parse(u"{{bla|}}")
    assert len(t[1]) == 1, "expected exactly one argument"


def test_parse_if():
    t = parse(u"{{#if: 1 | yes | no}}")
    print t
    assert isinstance(t, nodes.IfNode)

    t = parse(u"{{#if: 1 | yes | no}}", siteinfo=nl_siteinfo)
    print t
    assert isinstance(t, nodes.IfNode)


def test_parse_if_localized():
    t = parse(u"{{#als: 1 | yes | no}}", siteinfo=nl_siteinfo)
    print t
    assert isinstance(t, nodes.IfNode)


def test_parse_switch():
    t = parse(u"{{#switch: A | a=lower | UPPER}}")
    print t
    assert isinstance(t, nodes.SwitchNode)

    t = parse(u"{{#switch: A | a=lower | UPPER}}", siteinfo=nl_siteinfo)
    print t
    assert isinstance(t, nodes.SwitchNode)


def test_parse_switch_localized():
    t = parse(u"{{#schakelen: A | a=lower | UPPER}}", siteinfo=nl_siteinfo)
    print t
    assert isinstance(t, nodes.SwitchNode)


def test_parse_time():
    t = parse(u"{{#time:Y-m-d|2006-09-28}}")
    print t
    assert isinstance(t, magic_nodes.Time)


def test_parse_time_localized():
    t = parse(u"{{#tijd:Y-m-d|2006-09-28}}", siteinfo=nl_siteinfo)
    print t
    assert isinstance(t, magic_nodes.Time)

########NEW FILE########
__FILENAME__ = test_expander_time
#! /usr/bin/env py.test

from mwlib.expander import expandstr


def test_codes():
    def e(s, expected, date="09 Feb 2008 10:55:17"):
        expandstr(u'{{#time:%s|%s}}' % (s, date), expected)

    yield e, "Y-m-d", "2008-02-09"

    yield e, "Y", "2008"
    yield e, "y", "08"

    yield e, "n", "2"
    yield e, "m", "02"
    yield e, "M", "Feb"
    yield e, "F", "February"
    yield e, "W", "06"
    yield e, "j", "9"
    yield e, "d", "09"
    yield e, "z", "39"
    yield e, "D", "Sat"

    yield e, "l", "Saturday"

    yield e, "N", "7", "2008-11-9"  # sunday
    yield e, "N", "1", "2008-11-10"

    yield e, "w", "0", "2008-11-9"  # sunday
    yield e, "w", "1", "2008-11-10"

    yield e, "a", "am"
    yield e, "a", "pm", "09 Feb 2008 12:00:00"

    yield e, "A", "AM"
    yield e, "A", "PM", "09 Feb 2008 12:00:00"

    yield e, "g", "10"
    yield e, "g", "12", "09 Feb 2008 12:00:00"
    yield e, "g", "1", "09 Feb 2008 13:00:00"
    yield e, "g", "12", "09 Feb 2008 00:00:00"

    yield e, "h", "10"
    yield e, "h", "12", "09 Feb 2008 12:00:00"
    yield e, "h", "01", "09 Feb 2008 13:00:00"
    yield e, "g", "12", "09 Feb 2008 00:00:00"

    yield e, "G", "1", "09 Feb 2008 01:00:00"
    yield e, "G", "23", "09 Feb 2008 23:00:00"

    yield e, "H", "01", "09 Feb 2008 01:00:00"
    yield e, "H", "23", "09 Feb 2008 23:00:00"

    yield e, "i", "55"
    yield e, "s", "17"

    yield e, "U", "1202554517"

    yield e, "L", "1"
    yield e, "L", "1", "09 Feb 2000"
    yield e, "L", "0", "09 Feb 2009"

    yield e, "t", "29"

    yield e, "c", "2008-02-09T10:55:17+00:00"

    yield e, "r", "Sat, 09 Feb 2008 10:55:17 +0000"

    yield e, "xrY", "MMVIII"
    yield e, "xrU", "XVI", "1970-1-1 + 16 second"
    yield e, 'xr"foobar"', "foobar"


def test_examples():
    yield expandstr, '{{ #time: l [[F j|"Fourth of" F]] [[Y]] | 4 March 2007 }}', 'Sunday [[March 4|Fourth of March]] [[2007]]'


def test_backslash_quote():
    yield expandstr, '{{#time: \\Y|4 March 2007}}', 'Y'
    yield expandstr, '{{#time: \\\\Y|4 March 2007}}', '\\2007'


def test_time_vs_year():
    """http://code.pediapress.com/wiki/ticket/350"""
    expandstr('{{#time:G:i|2008}}', '20:08')


def test_time_vs_year_illegal_time():
    expandstr('{{#time:Y|1970}}', "1970")


def test_before_1900():
    expandstr("{{#time:c|1883-1-1}}", "1883-01-01T00:00:00+00:00")


def test_dateutil_raises_typeerror():
    yield expandstr, "{{#time:c|2007-09-27PM EDT}}"
    yield expandstr, "{{#iferror:{{#time:c|2007-09-27PM EDT}}|yes|no}}", "yes"


def test_time_minus_days():
    yield expandstr, "{{#time:Y-m-d| 20070827000000 -12 day}}", "2007-08-15"

########NEW FILE########
__FILENAME__ = test_expr
#! /usr/bin/env py.test
# -*- coding: utf-8 -*-

import math
from mwlib.expander import expandstr
from mwlib import expr


def ee(s, expected=None):
    s = expandstr("{{#expr:%s}}" % (s,))
    if isinstance(expected, (float, int, long)):
        assert math.fabs(float(s) - expected) < 1e-5
    elif expected is not None:
        assert s == expected, "expected %r, got %r" % (expected, s)

    return s


def test_pi():
    ee('pI', math.pi)
    ee('PI', math.pi)
    ee('pi*2', math.pi * 2)
    ee('pi+1', math.pi + 1)


def test_e():
    ee('e', math.e)
    ee('E', math.e)
    ee("e+1", math.e + 1)
    ee("2*e", math.e * 2)


def test_pow():
    ee("2^5", 32)
    ee("-2^4", 16)


def test_ln():
    ee("ln 2.7182818284590451", 1)


def test_exp():
    ee("exp(1)", 2.7182818284590451)
    ee("exp(0)", 1)


def test_abs_int():
    ee("abs(-5)", "5")


def test_abs():
    ee("abs(-3.0)", 3.0)
    ee("abs(3.0)", 3.0)
    ee("abs 1-3", -2)


def test_sin():
    ee("sin 3.1415926535897931", 0)


def test_cos():
    ee("cos 3.1415926535897931", -1)


def test_tan():
    ee("tan 3.1415926535897931", 0)
    ee("tan 0.785398163397", 1)


def test_asin():
    ee("asin 0.5", 0.5235987755983)
    ee("asin 0", 0)


def test_acos():
    ee("acos 0", 1.5707963267949)
    ee("acos 0.5", 1.0471975512)


def test_atan():
    ee("atan 0", 0)
    ee("atan 1", 0.785398163397)


def test_floor():
    ee("floor 5.1", "5")
    ee("floor -5.1", "-6")


def test_trunc():
    ee("trunc 5.1", "5")
    ee("trunc -5.1", "-5")


def test_ceil():
    ee("ceil 5.1", "6")
    ee("ceil -5.1", "-5")


def test_scientific():
    ee("1e25", "1.0E+25")
    ee("1E25", "1.0E+25")
    ee("1e-10", "1.0E-10")
    ee("1E-10", "1.0E-10")


def test_unary_double_plus():
    ee("++5", 5)


def test_unary_double_minus():
    ee("--5", 5)


def test_unary_plus():
    ee("0-+5", -5)


def test_mod_unary():
    ee("--1.253702 mod 360", 1)


def test_mod():
    ee("1.253702 mod 360", 1)


def test_unary_paren():
    ee("10+(--100)", 110)


def test_unary_pow_minus():
    ee("2^-10", 0.0009765625)


def test_unary_pow_plus():
    ee("2^+10", 1024)


def test_expr_repr():
    yield expandstr, "{{#expr:99999999999999}}", "99999999999999"
    yield expandstr, "{{#expr:99999999999999+1}}", "1.0E+14"
    yield expandstr, "{{#expr:0.1+0.9}}", "1"


def test_unary_minus_sin():
    """http://code.pediapress.com/wiki/ticket/450"""
    val = expr.expr("-sin(1.5707963267948966)")
    print val
    assert math.fabs(-1 - val) < 0.0001

    val = expr.expr("-sin ((90--82)*3.14159265358979/180)*(90+-80.0833333)*1.55*1.30522+49.3")
    print val
    assert math.fabs(46.507864831337 - val) < 0.0001


def test_empty_expr():
    yield expandstr, "{{#expr:   }}", ""
    yield expandstr, "{{#ifexpr:    |yes|no}}", "no"


def test_braindamaged_scientific():
    yield ee, "{{#expr: (-1)e(-0.5)}}", -0.31622776601684
    yield expandstr, "{{#expr:1e2e3}}", "100000"
    yield ee, "{{#expr:2*e}}", 2 * math.e
    yield ee, "{{#expr: e E E}}", 1420.9418661882

########NEW FILE########
__FILENAME__ = test_iferror_rx
#! /usr/bin/env py.test

from mwlib.templ import magics


def test_long_running_match(alarm):
    s = '<div ' + ' c' * 2000 + 'class="error">'
    alarm(0.01)
    assert magics.iferror_rx.match(s)


def test_long_running_no_match(alarm):
    s = '<div ' + ' c' * 2000 + 'class="erro">'
    alarm(0.01)
    assert not magics.iferror_rx.match(s)


def test_long_running_match_newline(alarm):
    s = '<div ' + ' c' * 1000 + '\n' + ' c' * 1000 + 'class="error">'
    alarm(0.01)
    assert magics.iferror_rx.match(s)

########NEW FILE########
__FILENAME__ = test_imgmap
#! /usr/bin/env py.test

from mwlib import imgmap


def test_whitespace_line():
    s = """image:large.png|Map
# XX
poly 642 127 615 162 635 205 [[Drenthe]]
"""
    map1 = imgmap.ImageMapFromString(s)
    print "MAP1:", map1
    map2 = imgmap.ImageMapFromString(s.replace("# XX", "  "))
    print "MAP2:", map2

    assert map2.image, "missing image"
    assert map2.entries, "missing entries"

########NEW FILE########
__FILENAME__ = test_math_utils
#! /usr/bin/env py.test
# -*- coding: utf-8 -*-

import os, shutil, tempfile, pytest
from mwlib.mathutils import renderMath


class TestMathUtils(object):
    def setup_method(self, method):
        self.tmpdir = tempfile.mkdtemp()

    def teardown_method(self, method):
        shutil.rmtree(self.tmpdir, ignore_errors=True)

    def blahtexml_present(self):
        ret = os.system('blahtexml')
        print "BLAHTEX:", ret
        if ret != 0:
            return False
        else:
            return True

    def texvc_present(self):
        ret = os.system('texvc')
        if ret != 0:
            return False
        else:
            return True

    def test_math(self):
        latexlist = [r"\sqrt{4}=2",
                     r"a^2 + b^2 = c^2\,",
                     r"E = m c^2",
                     r"\begin{matrix}e^{\mathrm{i}\,\pi}\end{matrix}+1=0\;",
                     r"1\,\mathrm{\frac{km}{h}} = 0{,}2\overline{7}\,\mathrm{\frac{m}{s}}",
                     r'\text{björn}',
                     ]
        for latex in latexlist:
            latex = unicode(latex, 'utf-8')
            if self.blahtexml_present():
                res = renderMath(latex, self.tmpdir, output_mode='png', render_engine='blahtexml')
                assert res
                res = renderMath(latex, self.tmpdir, output_mode='mathml', render_engine='blahtexml')
                assert res
            if self.texvc_present():
                res = renderMath(latex, self.tmpdir, output_mode='png', render_engine='texvc')
                assert res

    @pytest.mark.xfail
    def test_math_complex(self):

        latex = r"""\begin{array}{ccc}
    F^2\sim W&\Leftrightarrow&\frac{F_1^2}{F_2^2}=\frac{W_1}{W_2}\\
    \ln\frac{F_1}{F_2}\,\mathrm{Np}&=&
    \frac{1}{2}\ln\frac{W_1}{W_2}\,\mathrm{Np}\\
    20\,\lg\frac{F_1}{F_2}\,\mathrm{dB}&=&
    10\,\lg\frac{W_1}{W_2}\,\mathrm{dB}
    \end{array}"""
        latex = unicode(latex)
        if self.blahtexml_present():
            res = renderMath(latex, self.tmpdir, output_mode='mathml', render_engine='blahtexml')
            assert res
        else:
            assert False

    def test_single_quote_bug(self):
        """http://code.pediapress.com/wiki/ticket/241"""

        if self.texvc_present():
            res = renderMath(u"f'(x) = x", self.tmpdir, output_mode='png', render_engine='texvc')
            assert res

########NEW FILE########
__FILENAME__ = test_metabook
#! /usr/bin/env py.test

from mwlib import metabook,  myjson as json

test_wikitext1 = '''== Title ==
=== Subtitle ===
{{Template}}

Summary line 1
Summary line 2

;Chapter 1
:[[Article 1]]
:[[:Article 2]]

;Chapter 2
:[[Article 3|Display Title 1]]
:[{{fullurl:Article 4|oldid=4}}Display Title 2]

'''

test_wikitext2 = '''== Title ==
=== Subtitle ===
{{
Template
}}

Summary line 1
Summary line 2

;Chapter 1
:[[Article 1]]
:[[:Article 2]]

;Chapter 2
:[[Article 3|Display Title 1]]
:[{{fullurl:Article 4|oldid=4}}Display Title 2]

'''

test_metabook = {
    'type': 'collection',
    'version': 1,
    'title': u'bla',
    'items': [
        {
            'type': 'chapter',
            'title': 'Chapter 1',
            'items': [
                {
                    'type': 'article',
                    'title': 'Article 1',
                    'content_type': 'text/x-wiki',
                },
                {
                    'type': 'article',
                    'title': 'Article 2',
                    'content_type': 'text/x-wiki',
                },
            ],
        },
        {
            'type': 'chapter',
            'title': 'Chapter 2',
            'items': [
                {
                    'type': 'article',
                    'title': 'Article 3',
                    'displaytitle': 'Display Title',
                    'content_type': 'text/x-wiki',
                },
            ],
        },
    ],
}

test_metabook = json.loads(json.dumps(test_metabook))


def test_parse_collection_page():
    #first parsestring
    mb = metabook.parse_collection_page(test_wikitext1)
    print mb

    assert mb['type'] == 'collection'
    assert mb['version'] == 1
    assert mb['title'] == 'Title'
    assert mb['subtitle'] == 'Subtitle'
    assert mb['summary'] == 'Summary line 1 Summary line 2 '
    items = mb['items']
    assert len(items) == 2
    assert items[0]['type'] == 'chapter'
    assert items[0]['title'] == 'Chapter 1'
    arts = items[0]['items']
    assert len(arts) == 2
    assert arts[0]['type'] == 'article'
    assert arts[0]['title'] == 'Article 1'
    assert arts[1]['type'] == 'article'
    assert arts[1]['title'] == 'Article 2'
    assert items[1]['type'] == 'chapter'
    assert items[1]['title'] == 'Chapter 2'
    arts = items[1]['items']
    assert len(arts) == 2
    assert arts[0]['type'] == 'article'
    assert arts[0]['title'] == 'Article 3'
    assert arts[0]['displaytitle'] == 'Display Title 1'
    assert arts[1]['title'] == 'Article 4'
    assert arts[1]['revision'] == '4'
    assert arts[1]['displaytitle'] == 'Display Title 2'

    #second parsestring
    mb = metabook.parse_collection_page(test_wikitext2)
    assert mb['type'] == 'collection'
    assert mb['version'] == metabook.collection.version
    assert mb['title'] == 'Title'
    assert mb['subtitle'] == 'Subtitle'
    assert mb['summary'] == 'Summary line 1 Summary line 2 '
    items = mb['items']
    assert len(items) == 2
    assert items[0]['type'] == 'chapter'
    assert items[0]['title'] == 'Chapter 1'
    arts = items[0]['items']
    assert len(arts) == 2
    assert arts[0]['type'] == 'article'
    assert arts[0]['title'] == 'Article 1'
    assert arts[1]['type'] == 'article'
    assert arts[1]['title'] == 'Article 2'
    assert items[1]['type'] == 'chapter'
    assert items[1]['title'] == 'Chapter 2'
    arts = items[1]['items']
    assert len(arts) == 2
    assert arts[0]['type'] == 'article'
    assert arts[0]['title'] == 'Article 3'
    assert arts[0]['displaytitle'] == 'Display Title 1'
    assert arts[1]['title'] == 'Article 4'
    assert arts[1]['revision'] == '4'
    assert arts[1]['displaytitle'] == 'Display Title 2'


def test_get_item_list():
    expected = [
        {
            'type': 'chapter',
            'title': 'Chapter 1',
        },
        {
            'type': 'article',
            'title': 'Article 1',
        },
        {
            'type': 'article',
            'title': 'Article 2',
        },
        {
            'type': 'chapter',
            'title': 'Chapter 2',
        },
        {
            'type': 'article',
            'title': 'Article 3',
        },
    ]

    result = test_metabook.walk()
    assert len(result) == len(expected)
    for e, r in zip(expected, result):
        assert e['type'] == r['type']
        assert e['title'] == r['title']

    expected = [
        {
            'type': 'article',
            'title': 'Article 1',
        },
        {
            'type': 'article',
            'title': 'Article 2',
        },
        {
            'type': 'article',
            'title': 'Article 3',
        },
    ]
    result = test_metabook.walk(filter_type='article')
    assert len(result) == len(expected)
    for e, r in zip(expected, result):
        assert e['type'] == r['type']
        assert e['title'] == r['title']


def test_checksum():
    cs1 = metabook.calc_checksum(test_metabook)
    print cs1
    assert cs1
    assert isinstance(cs1, str)
    import copy
    tm2 = copy.deepcopy(test_metabook)

    tm2['title'] = tm2['title'] + '123'
    assert metabook.calc_checksum(tm2) != cs1

########NEW FILE########
__FILENAME__ = test_miscutils
#! /usr/bin/env py.test
# -*- coding: utf-8 -*-
# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import sys
from mwlib.dummydb import DummyDB
from mwlib.uparser import parseString
from mwlib import parser
from mwlib.writer import miscutils


def getTreeFromMarkup(raw):
    return parseString(title="Test", raw=raw, wikidb=DummyDB())


def show(tree):
    parser.show(sys.stdout, tree)


def test_articleStartsWithInfobox1():

    raw = '''
Some text in a paragraph

Some text in a paragraph [[http://ysfine.com]]

{| class="infobox"
|-
| bla || bla
|}

Some more text
'''
    tree = getTreeFromMarkup(raw)
    #show(tree)
    assert miscutils.articleStartsWithInfobox(tree, max_text_until_infobox=100) == True
    assert miscutils.articleStartsWithInfobox(tree, max_text_until_infobox=10) == False

########NEW FILE########
__FILENAME__ = test_mwscan
#! /usr/bin/env py.test

from mwlib import utoken as mwscan


def test_resolve_symbolic_entity():
    assert mwscan.resolve_entity(u"&amp;") == u"&", "bad result"


def test_resolve_numeric_entity():
    assert mwscan.resolve_entity(u"&#32;") == u' ', "expected space"


def test_resolve_hex_entity():
    assert mwscan.resolve_entity(u"&#x20;") == u' ', "expected space"


def test_resolve_entity_out_of_range():
    s = "&#x1000000;"
    assert mwscan.resolve_entity(s) == s, "should expand to same string"


def test_url():
    s = mwscan.scan("http://tools.wikimedia.de/~magnus/geo/geohack.php?language=de&params=50_0_0_N_8_16_16_E_type:city(190934)_region:DE-RP")
    print s
    assert len(s) == 1, "expected one url"


def _check_table_markup(s):
    toks = [t[0] for t in mwscan.scan(s)]
    print "TOKENS:", toks
    assert mwscan.token.t_begin_table not in toks, "should not contain table markup"
    assert mwscan.token.t_end_table not in toks, "should not contain table markup"


def test_table_bol_begin_code():
    _check_table_markup("<code>{|</code>")


def test_table_bol_begin():
    _check_table_markup("foo {| bar")


def test_table_bol_end_code():
    _check_table_markup("<code>|}</code>")


def test_table_bol_end():
    _check_table_markup("foo |} bar")

########NEW FILE########
__FILENAME__ = test_nserve
#! /usr/bin/env py.test
# -*- coding: utf-8 -*-

import pytest, gevent, urllib, urllib2, bottle
from mwlib import nserve

import wsgi_intercept.urllib_intercept

try:
    import simplejson as json
except ImportError:
    import json


def post(**kw):
    try:
        r = urllib2.urlopen('http://app.de/', urllib.urlencode(kw))
        data = r.read()
        data = json.loads(data)
        return (r.code, data)
    except urllib2.HTTPError, e:
        return (e.code, e.read())


def get_exception_raiser(msg, exception_class=RuntimeError):
    def raise_exc(*args, **kwargs):
        raise exception_class(msg)
    return raise_exc

raise_greenletexit = get_exception_raiser("killed", gevent.GreenletExit)


def pytest_funcarg__app(request):
    wsgi_intercept.urllib_intercept.install_opener()
    request.addfinalizer(wsgi_intercept.urllib_intercept.uninstall_opener)
    wsgi_intercept.add_wsgi_intercept('app.de', 80, bottle.default_app)
    request.addfinalizer(lambda: wsgi_intercept.remove_wsgi_intercept("app.de", 80))
    return None


def pytest_funcarg__busy(request):
    busy = {}
    monkeypatch = request.getfuncargvalue("monkeypatch")
    monkeypatch.setattr(nserve, "busy", busy)
    return busy


def pytest_funcarg__wq(request):
    busy = request.getfuncargvalue("busy")
    wq = nserve.watch_qserve(("localhost", 8888), busy)
    wq.getstats_timeout = 0.01
    wq.sleep_time = 0.01
    busy[wq.ident] = True
    return wq


# -- tests

def test_make_collection_id_version(monkeypatch):
    data = {}
    id1 = nserve.make_collection_id(data)
    from mwlib import _version
    monkeypatch.setattr(_version, "version", (0, 1, 0))
    id2 = nserve.make_collection_id(data)
    assert id1 != id2


def test_check_collection_id():
    cc = nserve.Application().check_collection_id
    assert not cc("a" * 15)
    assert not cc("a" * 17)
    assert cc("a" * 16)
    assert not cc("A" * 16)
    assert cc("0123456789abcdef")
    assert not cc("g" * 16)


def test_choose_idle_qserve(monkeypatch, busy):
    assert nserve.choose_idle_qserve() is None
    busy["host1"] = True
    assert nserve.choose_idle_qserve() is None
    busy["host2"] = False
    assert nserve.choose_idle_qserve() == "host2"
    busy["host1"] = False
    assert set(nserve.choose_idle_qserve() for i in range(20)) == set(["host1", "host2"])


def test_watch_qserve_iterate_overloaded(busy, wq):
    wq._getstats = lambda *args: dict(busy=dict(render=11))
    wq._iterate()
    assert busy[wq.ident] == "system overloaded"


def test_watch_qserve_iterate_down(busy, wq):
    wq._getstats = get_exception_raiser("getstats failed")
    wq._iterate()
    assert busy[wq.ident] == "system down"


def test_watch_qserve_iterate_killable(busy, wq):
    wq._getstats = raise_greenletexit
    pytest.raises(gevent.GreenletExit, wq._iterate)


def test_watch_qserve_call_killable(wq):
    wq._getstats = raise_greenletexit
    wq._sleep = get_exception_raiser("do not call sleep")
    pytest.raises(gevent.GreenletExit, wq)


def test_app_no_command(app):
    code, data = post()
    assert (code, data) == (400, "no command given")


def test_app_unknown_command(app):
    code, data = post(command="gohome")
    assert code == 400
    assert "no such command" in data


def test_app_do_render_overloaded(app):
    code, data = post(command="render")
    print code, data

    assert code == 200
    assert "overloaded" in data["error"]


def test_app_do_render_missing_metabook(app, busy):
    busy[("host1", 8000)] = False

    code, data = post(command="render", writer="odf")
    print code, data

    assert code == 200
    assert "metabook or collection_id required" in data["error"]


def test_app_dispatch_bad_collid(app, busy):
    code, data = post(command="render", collection_id="a" * 15)
    print code, data
    assert code == 404


@pytest.mark.parametrize(("filename", "ext", "expected"), [
        (u"Motörhead", "pdf", "inline; filename=Motorhead.pdf;filename*=UTF-8''Mot%C3%B6rhead.pdf"),
        (None, "pdf", "inline; filename=collection.pdf"),
        ("  ;;;", "pdf", "inline; filename=collection.pdf;filename*=UTF-8''%3B%3B%3B.pdf"),
        ("Peter Hartz", "pdf", "inline; filename=Peter-Hartz.pdf;filename*=UTF-8''Peter%20Hartz.pdf"),
        ("foo", "pdf", "inline; filename=foo.pdf")])
def test_get_content_disposition(filename, ext, expected):
    res = nserve.get_content_disposition(filename, ext)
    assert res == expected


def test_content_disposition_comma():
    a, u = nserve.get_content_disposition_values("foo,bar", "pdf")
    assert a == "foo-bar"

def test_content_disposition_merge():
    a, u = nserve.get_content_disposition_values("foo ,\"bar", "pdf")
    assert a == "foo-bar"


########NEW FILE########
__FILENAME__ = test_nshandling
#! /usr/bin/env py.test

from mwlib import nshandling, siteinfo
siteinfo_de = siteinfo.get_siteinfo("de")
assert siteinfo_de, "cannot find german siteinfo"


def test_fqname():
    def get_fqname(name, expected):
        fqname = nshandler.get_fqname(name)
        print "%r -> %r" % (name, fqname)
        assert fqname == expected

    nshandler = nshandling.nshandler(siteinfo_de)

    d = get_fqname
    e = "Benutzer:Schmir"

    yield d, "User:Schmir", e
    yield d, "user:Schmir", e
    yield d, "benutzer:schmir", e
    yield d, " user: schmir ", e
    yield d, "___user___:___schmir  __", e
    yield d, "User:SchmiR", "Benutzer:SchmiR"


def test_fqname_defaultns():
    def get_fqname(name, expected):
        fqname = nshandler.get_fqname(name, 10)  # Vorlage
        print "%r -> %r" % (name, fqname)
        assert fqname == expected

    nshandler = nshandling.nshandler(siteinfo_de)
    d = get_fqname

    yield d, "user:schmir", "Benutzer:Schmir"
    yield d, "schmir", "Vorlage:Schmir"
    yield d, ":schmir", "Schmir"


def test_redirect_matcher():
    m = nshandling.get_nshandler_for_lang("en").redirect_matcher
    assert m("#REDIRECT [[Data structure#Active data structures]]") == "Data structure",  "bad redirect"


def test_localized_redirect_matcher():
    m = nshandling.get_nshandler_for_lang("de").redirect_matcher
    assert m("#REDIRECT [[Data structure]]") == "Data structure",  "bad redirect"
    assert m("#WEITERLEITUNG [[Data structure]]") == "Data structure",  "bad redirect"

########NEW FILE########
__FILENAME__ = test_nuwiki
#! /usr/bin/env py.test
# -*- coding: utf-8 -*-

import os
import shutil
import subprocess
import tempfile
import zipfile

from mwlib.nuwiki import adapt


class Test_nuwiki_xnet(object):
    def setup_class(cls):
        cls.tmpdir = tempfile.mkdtemp()
        cls.zipfn = os.path.join(cls.tmpdir, 'test.zip')
        err = subprocess.call(['mw-zip',
            '-o', cls.zipfn,
            '-c', ':de',
            'Monty Python',
        ])
        assert os.path.isfile(cls.zipfn)
        assert err == 0,  "command failed"

    def teardown_class(cls):
        if os.path.exists(cls.tmpdir):
            shutil.rmtree(cls.tmpdir)

    def setup_method(self, method):
        self.nuwiki = adapt(zipfile.ZipFile(self.zipfn, 'r')).nuwiki

    def test_init(self):
        assert 'Monty Python' in self.nuwiki.revisions
        assert self.nuwiki.siteinfo['general']['base'] == 'http://de.wikipedia.org/wiki/Wikipedia:Hauptseite'
        assert self.nuwiki.siteinfo['general']['lang'] == 'de'
        assert self.nuwiki.nshandler is not None
        assert self.nuwiki.nfo['base_url'] == 'http://de.wikipedia.org/w/'

########NEW FILE########
__FILENAME__ = test_odfwriter
#! /usr/bin/env py.test
# -*- coding: utf-8 -*-
# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import os, sys, re, tempfile
from StringIO import StringIO
import py

from mwlib.dummydb import DummyDB
from mwlib.uparser import parseString
from mwlib import advtree
import mwlib.parser
from mwlib.odfwriter import ODFWriter, preprocess

ODFWriter.ignoreUnknownNodes = False

# hook for reuse of generated files


def removefile(fn):
    os.remove(fn)
odtfile_cb = removefile

# read the odflint script as module.
# calling this in process speeds up the tests considerably.


def _get_odflint_module():
    exe = py.path.local.sysfind("odflint")
    assert exe is not None, "odflint not found"

    argv = sys.argv[:]
    stderr = sys.stderr
    odflint = sys.__class__("odflint")

    try:
        sys.stderr = StringIO()
        del sys.argv[1:]
        try:
            execfile(exe.strpath, odflint.__dict__)
        except SystemExit:
            pass
        return odflint
    finally:
        sys.argv[:] = argv
        sys.stderr = stderr

odflint = _get_odflint_module()


def lintfile(path):
    stdout, stderr = sys.stdout, sys.stderr
    try:
        sys.stdout = sys.stderr = StringIO()
        odflint.lint(path)
        return sys.stdout.getvalue()
    finally:
        sys.stdout = stdout
        sys.stderr = stderr


class ValidationError(Exception):
    def __init__(self, value):
        self.value = value

    def __str__(self):
        return repr(self.value)


def validate(odfw):
    "THIS USES odflint AND WILL FAIL IF NOT INSTALLED"
    fh, tfn = tempfile.mkstemp()
    odfw.getDoc().save(tfn, True)
    tfn += ".odt"
    r = lintfile(tfn)
    if len(r):
        raise ValidationError(r)
    odtfile_cb(tfn)


def getXML(wikitext):
    db = DummyDB()
    r = parseString(title="test", raw=wikitext, wikidb=db)
    advtree.buildAdvancedTree(r)
    preprocess(r)
    mwlib.parser.show(sys.stdout, r)
    odfw = ODFWriter()
    odfw.writeTest(r)
    validate(odfw)
    xml = odfw.asstring()
    #print xml # usefull to inspect generateded xml
    return xml


def test_pass():
    raw = """
== Hello World ==
kthxybye
""".decode("utf8")
    xml = getXML(raw)


def test_fixparagraphs():
    raw = """
<p>
<ul><li>a</li></ul>
</p>
""".decode("utf8")
    xml = getXML(raw)


def test_gallery():
    raw = """
<gallery>
Image:Wikipedesketch1.png|The Wikipede
Image:Wikipedesketch1.png|A Wikipede
Image:Wikipedesketch1.png|Wikipede working
Image:Wikipedesketch1.png|Wikipede's Habitat
Image:Wikipedesketch1.png|A mascot for Wikipedia
Image:Wikipedesketch1.png|One logo for Wikipedia
Image:Wikipedesketch1.png|Wikipedia has bugs
Image:Wikipedesketch1.png|The mascot of Wikipedia
</gallery>""".decode("utf8")
    xml = getXML(raw)


def test_validatetags():
    """
    this test checks only basic XHTML validation
    """
    raw = r'''<b class="test">bold</b>
<big>big</big>
<blockquote>blockquote</blockquote>
break after <br/> and before this
<table class="testi vlist"><caption>caption for the table</caption><thead><th>heading</th></thead><tbody><tr><td>cell</td></tr></tbody></table>
<center>center</center>
<cite>cite</cite>
<code>code</code>
<source class="test_class" id="test_id">source</source>
<dl><dt>dt</dt><dd>dd</dd></dl>
<del>deleted</del>
<div>division</div>
<em>em</em>
<font>font</font>
<h1>h1</h1>
<h6>h6</h6>
<hr/>
<i>i</i>
<ins>ins</ins>
<ol><li>li 1</li><li>li 2</li></ol>
<ul><li>li 1</li><li>li 2</li></ul>
<p>paragraph</p>
<pre>preformatted</pre>
<ruby><rb>A</rb><rp>(</rp><rt>aaa</rt><rp>)</rp></ruby>
<s>s</s>
<small>small</small>
<span>span</span>
<strike>strke</strike>
<strong>strong</strong>
<sub>sub</sub>
<sup>sup</sup>
<tt>teletyped</tt>
<u>u</u>
<var>var</var>
th<!-- this is comment -->is includes a comment'''.decode("utf8")

    for x in raw.split("\n"):
        yield getXML, x


def test_sections():
    raw = '''
== Section 1 ==

text with newline above

more text with newline, this will result in paragrahps

=== This should be a sub section ===
currently the parser ends sections at paragraphs.
unless this bug is fixed subsections are not working

==== subsub section ====
this test will validate, but sections will be broken.

'''.decode("utf8")
    xml = getXML(raw)

    reg = re.compile(r'text:outline-level="(\d)"', re.MULTILINE)
    res = list(reg.findall(xml))
    goal = [u'1', u'2', u'3', u'4']
    print res, "should be", goal
    if not res == goal:
        print xml
        assert res == goal


def test_invalid_level_sections():
    raw = '''

= 1 =
== 2 ==
=== 3 ===
==== 4 ====
===== 5 =====
====== 6 ======
======= 7 =======
======== 8 ========
text
'''.decode("utf8")
    xml = getXML(raw)

    reg = re.compile(r'text:outline-level="(\d)"', re.MULTILINE)
    res = list(reg.findall(xml))
    goal = ['1', '2', '3', '4', '5', '6', '6', '6', '6']  # article title is on the first level, therefore we have "6"*4
    print res, "should be", goal
    if not res == goal:
        print xml
        assert res == goal


def disabled_test_empty_sections():
    # decision to show empty sections
    raw = '''=  =
= correct =
== with title no children =='''.decode("utf8")
    xml = getXML(raw)
    reg = re.compile(r'text:name="(.*?)"', re.MULTILINE)
    res = list(reg.findall(xml))
    goal = [u'test', 'correct ']  # article title is on the first level,
    print res, "should be", goal
    if not res == goal:
        print xml
        assert res == goal


def test_newlines():
    raw = '''== Rest of the page ==

A single
newline
has no
effect on the
layout.

<h1>heading 1</h1>

But an empty line
starts a new paragraph.

You can break lines<br />
without starting a new paragraph.
'''.decode("utf8")
    xml = getXML(raw)


def test_bold():
    raw = """
is this '''bold'''

another '''bold''

""".decode("utf8")
    xml = getXML(raw)


def test_ulists():
    raw = '''== Rest of the page ==

* Unordered Lists are easy to do:
** start every line with a star,
*** more stars means deeper levels.
* A newline
* in a list
marks the end of the list.
* Of course,
* you can
* start again.
'''.decode("utf8")
    xml = getXML(raw)


def test_olists():
    raw = '''== Rest of the page ==


# Numbered lists are also good
## very organized
## easy to follow
# A newline
# in a list
marks the end of the list.
# New numbering starts
# with 1.

'''.decode("utf8")
    xml = getXML(raw)


def test_mixedlists():
    raw = '''== Rest of the page ==

* You can even do mixed lists
*# and nest them
*#* or break lines<br />in lists

'''.decode("utf8")
    xml = getXML(raw)


def test_definitionlists():
    raw = '''== Rest of the page ==
; word : definition of the word
; longer phrase
: phrase defined


'''.decode("utf8")
    xml = getXML(raw)


def test_preprocess():
    raw = '''== Rest of the page ==

A single
newline
has no
effect on the
layout.

<h1>heading 1</h1>

But an empty line
starts a new paragraph.

You can break lines<br />
without starting a new paragraph.

* Unordered Lists are easy to do:
** start every line with a star,
*** more stars means deeper levels.
* A newline
* in a list
marks the end of the list.
* Of course,
* you can
* start again.


# Numbered lists are also good
## very organized
## easy to follow
# A newline
# in a list
marks the end of the list.
# New numbering starts
# with 1.

* You can even do mixed lists
*# and nest them
*#* or break lines<br />in lists

'''.decode("utf8")
    xml = getXML(raw)


def test_paragraphsinsections():
    raw = '''== section 1 ==
s1 paragraph 1

s1 paragraph 2

=== subsection ===
sub1 paragraph 1

sub1 paragraph 1

== section 2 ==
s2 paragraph 1

s2 paragraph 2

'''.decode("utf8")
    xml = getXML(raw)


def test_math():
    raw = r'''
<math> Q = \begin{bmatrix} 1 & 0 & 0 \\ 0 & \frac{\sqrt{3}}{2} & \frac12 \\ 0 & -\frac12 & \frac{\sqrt{3}}{2} \end{bmatrix} </math>
'''.decode("utf8")
    xml = getXML(raw)


def test_math2():
    raw = r'''<math>\exp(-\gamma x)</math>'''
    xml = getXML(raw)


def test_snippets():
    from mwlib import snippets
    for s in snippets.get_all():
        print "testing", repr(s.txt)
        xml = getXML(s.txt)


def test_horizontalrule():
    raw = r'''before_hr<hr/>after_hr'''
    xml = getXML(raw)


def test_tables():
    raw = r'''
{| border="1" cellspacing="0" cellpadding="5" align="center"
! This
! is
|-
| a
| cheese
|-
|}
'''
    xml = getXML(raw)
    assert "cheese" in xml


def test_colspan():
    raw = r'''
<table>
<tr><td>a</td><td>b</td></tr>
<tr><td colspan="2">ab</td></tr>
</table>
'''
    xml = getXML(raw)


def test_definitiondescription():
    # works with a hack
    raw = r'''
: a
:* b
'''
    xml = getXML(raw)


def test_italic():
    # DOES NOT WORK FOR ME in OpenOffice
    raw = r'''
=== a===
B (''Molothrus ater'') are


'''
    xml = getXML(raw)
    print xml
    assert "Molothrus" in xml

########NEW FILE########
__FILENAME__ = test_parser
#! /usr/bin/env py.test
# -*- coding: utf-8 -*-

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import pytest
from mwlib import parser, expander, uparser
from mwlib.expander import DictDB
from mwlib.dummydb import DummyDB
from mwlib.refine import util, core

parse = uparser.simpleparse


def test_headings():
    r = parse(u"""
= 1 =
== 2 ==
= 3 =
""")

    sections = [x.children[0].asText().strip() for x in r.children if isinstance(x, parser.Section)]
    assert sections == [u"1", u"3"]


def check_style(s, counts):
    print "PARSING:", repr(s)
    art = parse(s)
    styles = art.find(parser.Style)
    assert len(styles) == len(counts), "wrong number of styles"
    for i, x in enumerate(styles):
        assert len(x.caption) == counts[i]


def test_style():
    yield check_style, u"''frankfurt''", (2,)
    yield check_style, u"'''mainz'''", (3,)
    yield check_style, u"'''''hamburg'''''", (3, 2)
    yield check_style, u"'''''foo'' bla'''", (3, 2, 3)
    yield check_style, u"'''''mainz''' bla''", (3, 2, 2)
    yield check_style, u"'''''''''''''''''''pp'''''", (3, 2)
    yield check_style, u"'''test''bla", (2,)


def test_style_fails():
    """http://code.pediapress.com/wiki/ticket/375"""

    check_style(u"'''strong only ''also emphasized'' strong only'''", (3, 3, 2, 3))


def test_single_quote_after_style():
    """http://code.pediapress.com/wiki/ticket/20"""

    def check(s, outer, inner=None):
        art = parse(s)
        styles = art.find(parser.Style)
        style = styles[0]
        assert style.caption == outer
        if inner is None:
            assert len(styles) == 1
        else:
            assert len(styles) == 2
            assert styles[1].caption == inner

    check(u"''pp'''s", "''")
    check(u"'''pp''''s", "'''")
    check(u"''pp'''", "''")
    check(u"'''pp''''", "'''")
    check(u"'''''pp''''''", "'''", "''")
    check(u"'''''''''''''''''''pp''''''", "'''", "''")


def test_links_in_style():
    s = parse(u"'''[[mainz]]'''").find(parser.Style)[0]
    assert isinstance(s.children[0], parser.Link)


def test_parse_image_inline():
    #r=parse("[[Bild:flag of Italy.svg|30px]]")
    #img = [x for x in r.allchildren() if isinstance(x, parser.ImageLink)][0]
    #print "IMAGE:", img, img.isInline()

    r = parse(u'{| cellspacing="2" border="0" cellpadding="3" bgcolor="#EFEFFF" width="100%"\n|-\n| width="12%" bgcolor="#EEEEEE"| 9. Juli 2006\n| width="13%" bgcolor="#EEEEEE"| Berlin\n| width="20%" bgcolor="#EEEEEE"| [[Bild:flag of Italy.svg|30px]] \'\'\'Italien\'\'\'\n| width="3%" bgcolor="#EEEEEE"| \u2013\n| width="20%" bgcolor="#EEEEEE"| [[Bild:flag of France.svg|30px]] Frankreich\n| width="3%" bgcolor="#EEEEEE"|\n| width="25%" bgcolor="#EEEEEE"| [[Fu\xdfball-Weltmeisterschaft 2006/Finalrunde#Finale: Italien .E2.80.93 Frankreich 6:4 n. E..2C 1:1 n. V. .281:1.2C 1:1.29|6:4 n. E., (1:1, 1:1, 1:1)]]\n|}\n', lang='de')
    images = r.find(parser.ImageLink)

    assert len(images) == 2

    for i in images:
        print "-->Image:", i, i.isInline()


def test_parse_image_6():
    """http://code.pediapress.com/wiki/ticket/6"""
    r = parse("[[Bild:img.jpg|thumb|+some text]] [[Bild:img.jpg|thumb|some text]]", lang='de')

    images = r.find(parser.ImageLink)
    assert len(images) == 2
    print images
    assert images[0].isInline() == images[1].isInline()


def test_self_closing_nowiki():
    parse(u"<nowiki/>")
    parse(u"<nowiki  />")
    parse(u"<nowiki       />")
    parse(u"<NOWIKI>[. . .]</NOWIKI>")


def test_switch_default():
    db = DictDB(
        Bonn="""{{Infobox
|Bundesland         = Nordrhein-Westfalen
}}
""",
        Infobox="""{{#switch: {{{Bundesland}}}
        | Bremen = [[Bremen (Land)|Bremen]]
        | #default = [[{{{Bundesland|Bayern}}}]]
}}
""")

    te = expander.Expander(db.normalize_and_get_page("Bonn", 0).rawtext, pagename="thispage", wikidb=db)
    res = te.expandTemplates()

    print "EXPANDED:", repr(res)
    assert "Nordrhein-Westfalen" in res


def test_tag_expand_vs_uniq():
    db = DictDB(
        Foo="""{{#tag:pre|inside pre}}"""
        )
    r = uparser.parseString(title="Foo", wikidb=db)
    core.show(r)
    pre = r.find(parser.PreFormatted)
    assert len(pre) == 1, "expected a preformatted node"


def test_pipe_table():

    db = DictDB(Foo="""
bla
{{{ {{Pipe}}}
blubb
""",
                   Pipe="|")

    te = expander.Expander(db.normalize_and_get_page("Foo", 0).rawtext, pagename="thispage", wikidb=db)
    res = te.expandTemplates()

    print "EXPANDED:", repr(res)
    assert "bla" in res
    assert "blubb" in res


def test_pipe_begin_table():

    db = DictDB(Foo="""
bla
{{{Pipe}} |}
blubb
""",
              Pipe="|")

    te = expander.Expander(db.normalize_and_get_page("Foo", 0).rawtext, pagename="thispage", wikidb=db)
    res = te.expandTemplates()

    print "EXPANDED:", repr(res)
    assert "bla" in res
    assert "blubb" in res
    assert "{|" in res


def test_cell_parse_bug():
    """http://code.pediapress.com/wiki/ticket/17"""
    # mediawiki actually pulls out that ImageLink from the table...

    r = parse("""{|
|-
[[Image:bla.png|bla]]
|}""")
    print r

    images = r.find(parser.ImageLink)
    assert images


def test_table_not_eating():
    """internal parser error.
    http://code.pediapress.com/wiki/ticket/32
    http://code.pediapress.com/wiki/ticket/29
"""
    uparser.simpleparse("""{|)
|10<sup>10<sup>100</sup></sup>||gsdfgsdfg
|}""")


def test_table_not_eating2():
    """internal parser error.
    http://code.pediapress.com/wiki/ticket/32
    http://code.pediapress.com/wiki/ticket/29
"""
    uparser.simpleparse("""{|
<tr><td>'''Birth&nbsp;name'''</td><td colspan="2">Alanis Nadine Morissette</td></tr><tr><td>'''Born'''</td>
|}
""")


def test_parse_comment():
    ex = """foo
<!-- comment --->
bar"""
    expanded = expander.expandstr(ex)
    print "EXPANDED:", expanded
    assert "\n\n" not in expanded


def test_nowiki_entities():
    """http://code.pediapress.com/wiki/ticket/40"""
    node = parse("<nowiki>&amp;</nowiki>")
    txt = node.find(parser.Text)[0]
    assert txt.caption == u'&', "expected an ampersand"


def test_blockquote_with_newline():
    """http://code.pediapress.com/wiki/ticket/41"""
    node = parse("<blockquote>\nblockquoted</blockquote>").find(parser.Style)[0]
    print "STYLE:", node
    assert "blockquoted" in node.asText(), "expected 'blockquoted'"


def test_blockquote_with_two_paras():
    """http://code.pediapress.com/wiki/ticket/41"""

    node = parse("<blockquote>\nblockquoted\n\nmust be inside</blockquote>")
    print 'BLOCKQUOTE:', node.children
    assert len(node.children) == 1, "expected exactly one child node"


def test_newlines_in_bold_tag():
    """http://code.pediapress.com/wiki/ticket/41"""
    node = parse('<b>test\n\nfoo</b>')
    styles = node.find(parser.Style)
    txt = ''.join([x.asText() for x in styles])
    print "TXT:", txt

    assert "foo" in txt, "foo should be bold"


def test_percent_table_style():
    """http://code.pediapress.com/wiki/ticket/39. thanks xyb."""

    def check(s):
        r = parse(s)
        t = r.find(parser.Table)[0]
        print t
        assert t.vlist['width'] == u'80%', "got wrong value %r" % (t.vlist['width'],)

    check('''{| class="toccolours" width="80%"
|-
| foo
|}''')

    check('''{| class="toccolours" width=80%
|-
| foo
|}''')


def test_parseParams():
    def check(s, expected):
        res = util.parseParams(s)
        print repr(s), "-->", res, "expected:", expected

        assert res == expected, "bad result"

    check("width=80pt", dict(width="80pt"))
    check("width=80ex", dict(width="80ex"))


def test_ol_ul():
    """http://code.pediapress.com/wiki/ticket/33"""

    r = parse("#num\n*bul\n")
    lists = r.find(parser.ItemList)
    assert len(lists) == 2, "expected two ItemList children"

    r = parse("*num\n#bul\n")
    lists = r.find(parser.ItemList)
    assert len(lists) == 2, "expected two ItemList children"


def test_nested_lists():
    """http://code.pediapress.com/wiki/ticket/33"""
    r = parse("""
# lvl 1
#* lvl 2
#* lvl 2
# lvl 1
""")
    lists = r.find(parser.ItemList)
    assert len(lists) == 2, "expected two lists"

    outer = lists[0]
    inner = lists[1]
    assert len(outer.children) == 2, "outer list must have 2 children"
    assert len(inner.children) == 2, "inner list must have 2 children"


def test_nested_list_listitem():
    r = parse("** wurst\n")
    outer = r.find(parser.ItemList)[0]
    assert isinstance(outer.children[0], parser.Item), "expected an Item inside ItemList"


def checktag(tagname):
    source = "<%s>foobar</%s>" % (tagname, tagname)
    r = parse(source)
    print "R:", r
    nodes = r.find(parser.TagNode)
    print "NODES:", nodes
    assert len(nodes) == 1, "expected a TagNode"
    n = nodes[0]
    assert n.caption == tagname, "expected another node"


def test_strike_tag():
    checktag("strike")


def test_del_tag():
    checktag("del")


def test_ins_tag():
    checktag("ins")


def test_tt_tag():
    checktag("tt")


def test_code_tag():
    checktag("code")


def test_center_tag():
    checktag("center")


def test_headings_nonclosed():
    r = parse("= nohead\nbla")
    print "R:", r
    sections = r.find(parser.Section)
    assert sections == [], "expected no sections"


def test_headings_unbalanced_1():
    r = parse("==head=")  # section caption should '=head'
    print "R:", r
    section = r.find(parser.Section)[0]
    print "SECTION:", section
    print "ASTEXT:", section.asText()

    assert section.level == 1, 'expected level 1 section'
    assert section.asText() == '=head'


def test_headings_unbalanced_2():
    r = parse("=head==")  # section caption should 'head='
    print "R:", r
    section = r.find(parser.Section)[0]
    print "SECTION:", section
    print "ASTEXT:", section.asText()

    assert section.level == 1, 'expected level 1 section'
    assert section.asText() == 'head='


def test_headings_tab_end():
    r = parse("=heading=\t")
    print "R:", r
    assert isinstance(r.children[0], parser.Section), "expected first child to be a Section"


def test_table_extra_cells_and_rows():
    """http://code.pediapress.com/wiki/ticket/19"""
    s = """
<table>
  <tr>
    <td>1</td>
  </tr>
</table>"""
    r = parse(s)
    cells = r.find(parser.Cell)
    assert len(cells) == 1, "expected exactly one cell"

    rows = r.find(parser.Row)
    assert len(rows) == 1, "expected exactly one row"


def test_table_rowspan():
    """http://code.pediapress.com/wiki/ticket/19"""
    s = """
<table align="left">
  <tr align="right">
    <td rowspan=3 colspan=18>1</td>
  </tr>
</table>"""
    r = parse(s)
    cells = r.find(parser.Cell)
    assert len(cells) == 1, "expected exactly one cell"
    cell = cells[0]
    print "VLIST:", cell.vlist
    assert cell.vlist == dict(rowspan=3, colspan=18), "bad vlist in cell"

    row = r.find(parser.Row)[0]
    print "ROW:", row
    assert row.vlist == dict(align="right"), "bad vlist in row"

    table = r.find(parser.Table)[0]
    print "TABLE.VLIST:", table.vlist
    assert table.vlist == dict(align="left"), "bad vlist in table"


def test_extra_cell_stray_tag():
    """http://code.pediapress.com/wiki/ticket/18"""
    cells = parse("""
{|
| bla bla </sub> dfg sdfg
|}""").find(parser.Cell)
    assert len(cells) == 1, "expected exactly one cell"


def test_gallery_complex():
    gall = """<gallery caption="Sample gallery" widths="100px" heights="100px" perrow="6">
Image:Drenthe-Position.png|[[w:Drenthe|Drenthe]], the least crowded province
Image:Flevoland-Position.png
Image:Friesland-Position.png|[[w:Friesland|Friesland]] has many lakes
Image:Gelderland-Position.png
Image:Groningen-Position.png
Image:Limburg-Position.png
Image:Noord_Brabant-Position.png
Image:Noord_Holland-Position.png
Image:Overijssel-Position.png
Image:Zuid_Holland-Position.png|[[w:South Holland|South Holland]], the most crowded province
lakes
Image:Zeeland-Position.png
</gallery>
"""
    res = parse(gall).find(parser.TagNode)[0]
    print "VLIST:", res.vlist
    print "RES:", res

    assert res.vlist == {'caption': 'Sample gallery', 'heights': '100px', 'perrow': 6, 'widths': '100px'}
    assert len(res.children) == 12, 'expected 12 children'
    assert isinstance(res.children[10], parser.Text), "expected text for the 'lakes' line"


def test_colon_nobr():
    tagnodes = parse(";foo\n:bar\n").find(parser.TagNode)
    assert not tagnodes, "should have no br tagnodes"


def test_nonascii_in_tags():
    r = parse(u"<dfg\u0147>")


def test_mailto_named():
    r = parse("[mailto:ralf@brainbot.com me]")
    assert r.find(parser.NamedURL), "expected a NamedLink"


def test_irc_named():
    r = parse("[irc://freenode.net/pediapress #pediapress]")
    assert r.find(parser.NamedURL), "expected a NamedLink"


def test_irc():
    r = parse("irc://freenode.net/pediapress #pediapress")
    assert r.find(parser.URL), "expected a URL"


def test_news_named():
    r = parse("[news:alt.foo.bar #pediapress]")
    assert r.find(parser.NamedURL), "expected a NamedLink"


def test_news():
    r = parse("news:alt.foo.bar #pediapress")
    assert r.find(parser.URL), "expected a URL"


def test_namedurl_inside_link():
    r = parse("[http://foo.com baz]]]")
    assert r.find(parser.NamedURL), "expected a NamedURL"


def test_namedurl_with_style():
    """http://code.pediapress.com/wiki/ticket/461"""
    r = parse(u"[http://thetangent.org Internetpräsenz von ''The Tangent'']")
    named = r.find(parser.NamedURL)
    assert len(named) == 1, "expected a NamedURL instance"
    styles = named[0].find(parser.Style)
    assert len(styles) == 1, "expected a style"


def test_mailto():
    r = parse("mailto:ralf@brainbot.com")
    assert r.find(parser.URL), "expected a URL node"


def _check_text_in_pretag(txt):
    r = parse("<pre>%s</pre>" % txt)
    p = r.find(parser.PreFormatted)[0]
    assert len(p.children) == 1, "expected exactly one child node inside PreFormatted"
    t = p.children[0]
    assert t == parser.Text(txt), "child node contains wrong text"


def test_pre_tag_newlines():
    """http://code.pediapress.com/wiki/ticket/79"""
    _check_text_in_pretag("\ntext1\ntext2\n\ntext3")


def test_pre_tag_list():
    """http://code.pediapress.com/wiki/ticket/82"""
    _check_text_in_pretag("\n* item1\n* item2")


def test_pre_tag_link():
    """http://code.pediapress.com/wiki/ticket/78"""
    _check_text_in_pretag("\ntext [[link]] text\n")


def test_parse_preformatted_pipe():
    """http://code.pediapress.com/wiki/ticket/92"""
    r = parse(" |foobar\n")
    assert r.find(parser.PreFormatted), "expected a preformatted node"


def test_parse_preformatted_math():
    r = parse(' <math>1+2=3</math>\n')
    assert r.find(parser.PreFormatted), 'expected a preformatted node'


def test_parse_preformatted_blockquote():
    r = parse(' <blockquote>blub</blockquote>')
    stylenode = r.find(parser.Style)
    assert not r.find(parser.PreFormatted) and stylenode and stylenode[0].caption == '-', 'expected blockquote w/o preformatted node'


def test_no_preformatted_with_source():
    """http://code.pediapress.com/wiki/ticket/174"""

    s = """  <source>

  </source>
* foo
"""
    r = parse(s)
    p = r.find(parser.PreFormatted)
    print p
    assert not p, "should not contain a preformatted node"


def test_parse_eolstyle_inside_blockquote():
    r = parse("<blockquote>\n:foo</blockquote>")
    stylenode = r.find(parser.Style)[-1]
    assert stylenode.caption == ':', "bad stylenode"


def _parse_url(u):
    url = parse("url: %s " % u).find(parser.URL)[0]
    assert url.caption == u, "url has wrong caption"


def test_url_parsing_plus():
    _parse_url("http://mw/foo+bar")


def test_url_parsing_comma():
    _parse_url("http://mw/foo,bar")


def test_url_parsing_umlauts():
    "http://code.pediapress.com/wiki/ticket/77"

    _parse_url(u"http://aÄfoo.de")
    _parse_url(u"http://aäfoo.de")

    _parse_url(u"http://aüfoo.de")
    _parse_url(u"http://aÜfoo.de")

    _parse_url(u"http://aöfoo.de")
    _parse_url(u"http://aÖfoo.de")


def test_table_markup_in_link_pipe_plus():
    """http://code.pediapress.com/wiki/ticket/54"""
    r = parse("[[bla|+blubb]]").find(parser.Link)[0]
    assert r.target == 'bla', "wrong target"


def test_table_markup_in_link_pipe_pipe():
    """http://code.pediapress.com/wiki/ticket/54"""
    r = parse("[[bla||blubb]]").find(parser.Link)[0]
    assert r.target == 'bla', "wrong target"


def test_table_markup_in_link_table_pipe_plus():
    """http://code.pediapress.com/wiki/ticket/11"""
    r = parse("{|\n|+\n|[[bla|+blubb]]\n|}").find(parser.Link)[0]
    assert r.target == 'bla', "wrong target"


def test_table_markup_in_link_table_pipe_pipe():
    """http://code.pediapress.com/wiki/ticket/11"""

    r = parse("{|\n|+\n|[[bla||blubb]]\n|}").find(parser.Link)
    assert not r, "table should not contain a link"

#     r=parse("{|\n|+\n|[[bla||blubb]]\n|}").find(parser.Link)[0]
#     assert r.target=='bla', "wrong target"


def test_source_tag():
    source = "\nwhile(1){ {{#expr:1+1}}\n  i++;\n}\n\nreturn 0;\n"
    s = '<source lang="c">%s</source>' % source

    r = parse(s).find(parser.TagNode)[0]
    print r
    assert r.vlist["lang"] == "c", "wrong lang attribute"
    assert r.children == [parser.Text(source)], "bad children"


def test_self_closing_style():
    "http://code.pediapress.com/wiki/ticket/93"
    styles = parse("<b />bla").find(parser.Style)
    if not styles:
        return
    s = styles[0]
    assert s.children == [], 'expected empty or no style node'


def test_timeline():
    """http://code.pediapress.com/wiki/ticket/86 """
    source = "\nthis is the timeline script!\n"
    r = parse("<timeline>%s</timeline>" % source).find(parser.Timeline)[0]
    print r
    assert r.children == [], "expected no children"
    assert r.caption == source, "bad script"


def test_nowiki_self_closing():
    """http://code.pediapress.com/wiki/ticket/102"""
    links = parse("<nowiki />[[foobar]]").find(parser.Link)
    assert links, "expected a link"


def test_nowiki_closing():
    """http://code.pediapress.com/wiki/ticket/102"""
    links = parse("</nowiki>[[foobar]]").find(parser.Link)
    assert links, "expected a link"


def test_math_stray():
    """http://code.pediapress.com/wiki/ticket/102"""
    links = parse("</math>[[foobar]]").find(parser.Link)
    assert links, "expected a link"

    links = parse("<math />[[foobar]]").find(parser.Link)
    assert links, "expected a link"


def test_math_basic():
    m = parse("<math>foobar</math>").find(parser.Math)[0]
    assert m.caption == "foobar"


def test_timeline_stray():
    """http://code.pediapress.com/wiki/ticket/102"""
    links = parse("</timeline>[[foobar]]").find(parser.Link)
    assert links, "expected a link"

    links = parse("<timeline />[[foobar]]").find(parser.Link)
    assert links, "expected a link"


def test_ftp_url():
    """http://code.pediapress.com/wiki/ticket/98"""

    def checkurl(url):
        urls = parse("foo %s bar" % url).find(parser.URL)
        assert urls, "expected a url"
        assert urls[0].caption == url, "bad url"

        urls = parse("[%s bar]" % url).find(parser.NamedURL)
        assert urls, "expected a named url"
        assert urls[0].caption == url, "bad url"

    yield checkurl, "ftp://bla.com:8888/asdfasdf+ad'fdsf$fasd{}/~ralf?=blubb/@#&*(),blubb"
    yield checkurl, "ftp://bla.com:8888/$blubb/"


def test_http_url():
    def checkurl(url):
        caption = parse("foo [%s]" % (url,)).find(parser.NamedURL)[0].caption
        assert caption == url, "bad named url"

        caption = parse(url).find(parser.URL)[0].caption
        assert caption == url, "bad url"

    yield checkurl, "http://pediapress.com/'bla"
    yield checkurl, "http://pediapress.com/bla/$/blubb"
    yield checkurl, "http://pediapress.com/foo*bar"
    yield checkurl, "http://pediapress.com/foo|bar"
    yield checkurl, "http://pediapress.com/{curly_braces}"

def test_schemeless_url():

    url = '[//toolserver.org/~geohack/geohack.php?pagename=Benutzer%3AVolker.haas/Test&language=de&params=51.4213888889_N_9.64805555556_E_dim:100_region:DE-NI_type:waterbody&title=Namentlicher+Beginn+Weser]'

    caption = parse(url).find(parser.NamedURL)[0].caption
    assert caption == url[1:-1]

def test_source_vlist():
    r = parse("<source lang=c>int main()</source>").find(parser.TagNode)[0]
    assert r.vlist == dict(lang='c'), "bad value: %r" % (r.vlist,)


def test_not_pull_in_alpha_image():
    link = parse("[[Image:link.jpg|ab]]cd").find(parser.Link)[0]
    assert "cd" not in link.asText(), "'cd' not in linkstext"


@pytest.mark.xfail
def test_pull_in_alpha():
    """http://code.pediapress.com/wiki/ticket/130"""
    link = parse("[[link|ab]]cd").find(parser.Link)[0]
    assert "cd" in link.asText(), "'cd' not in linkstext"


def test_not_pull_in_numeric():
    link = parse("[[link|ab]]12").find(parser.Link)[0]

    assert "12" not in link.asText(), "'12' in linkstext"


def test_section_consume_break():
    r = parse('= about us =\n\nfoobar').find(parser.Section)[0]
    txt = r.asText()
    assert 'foobar' in txt, 'foobar should be inside the section'


def test_text_caption_none_bug():
    lst = parse("[[]]").find(parser.Text)
    print lst
    for x in lst:
        assert x.caption is not None


def test_link_inside_gallery():
    links = parse("<gallery>Bild:Guanosinmonophosphat protoniert.svg|[[Guanosinmonophosphat]] <br /> (GMP)</gallery>", lang='de').find(parser.Link)
    print links
    assert len(links) == 2, "expected 2 links"


def test_indented_table():
    r = parse(':::{| class="prettytable" \n|-\n| bla || blub\n|}')
    style = r.find(parser.Style)[-1]
    tables = style.find(parser.Table)
    assert len(tables) == 1, "expected a table as child"


def test_double_exclamation_mark_in_table():
    r = parse('{|\n|-\n| bang!!\n| cell2\n|}\n')
    cells = r.find(parser.Cell)
    print "CELLS:", cells
    assert len(cells) == 2, 'expected two cells'
    txt = cells[0].asText()
    print "TXT:", txt
    assert "!!" in txt, 'expected "!!" in cell'


def test_table_row_exclamation_mark():
    r = parse('''{|
|-
! bgcolor="#ffccaa" | foo || bar
|}''')
    cells = r.find(parser.Cell)
    print "CELLS:", cells
    assert len(cells) == 2, 'expected exactly two cells'


def test_unknown_tag():
    """http://code.pediapress.com/wiki/ticket/212"""
    r = parse("<nosuchtag>foobar</nosuchtag>")
    txt = r.asText()
    print "TXT:", repr(txt)
    assert u'<nosuchtag>' in txt, 'opening tag missing in asText()'
    assert u'</nosuchtag>' in txt, 'closing tag missing in asText()'

# Test varieties of link


def test_plain_link():
    r = parse("[[bla]]").find(parser.ArticleLink)[0]
    assert r.target == 'bla'


def test_piped_link():
    r = parse("[[bla|blubb]]").find(parser.ArticleLink)[0]
    assert r.target == 'bla'
    assert r.children[0].caption == 'blubb'


def test_category_link():
    r = parse("[[category:bla]]").find(parser.CategoryLink)[0]
    assert r.target == 'category:bla', "wrong target"
    assert r.namespace == 14, "wrong namespace"


def test_category_colon_link():
    r = parse("[[:category:bla]]").find(parser.SpecialLink)[0]
    assert r.target == 'category:bla', "wrong target"
    assert r.namespace == 14, "wrong namespace"
    assert not isinstance(r, parser.CategoryLink)


def test_image_link():
    t = uparser.parseString('', u'[[画像:Tajima mihonoura03s3200.jpg]]', lang='ja')
    r = t.find(parser.ImageLink)[0]
    assert r.target == u'画像:Tajima mihonoura03s3200.jpg'
    assert r.namespace == 6, "wrong namespace"


def test_image_colon_link():
    r = parse("[[:image:bla.jpg]]").find(parser.SpecialLink)[0]
    assert r.target == 'image:bla.jpg'
    assert r.namespace == 6, "wrong namespace"
    assert not isinstance(r, parser.ImageLink), "should not be an image link"


def test_interwiki_link():
    r = parse("[[wikt:bla]]").find(parser.InterwikiLink)[0]
    assert r.target == 'wikt:bla', "wrong target"
    assert r.namespace == 'wikt', "wrong namespace"
    r = parse("[[mw:bla]]").find(parser.InterwikiLink)[0]
    assert r.target == 'mw:bla', "wrong target"
    assert r.namespace == 'mw', "wrong namespace"


def test_language_link():
    r = parse("[[es:bla]]").find(parser.LangLink)[0]
    assert r.target == 'es:bla', "wrong target"
    assert r.namespace == 'es', "wrong namespace"


def test_long_language_link():
    r = parse("[[csb:bla]]").find(parser.LangLink)[0]
    assert r.target == 'csb:bla', "wrong target"
    assert r.namespace == 'csb', "wrong namespace"


def test_subpage_link():
    r = parse('[[/SomeSubPage]]').find(parser.ArticleLink)[0]
    assert r.target == '/SomeSubPage', "wrong target"


@pytest.mark.xfail
def test_normalize():
    r = parse("[[MediaWiki:__bla_ _ ]]").find(parser.NamespaceLink)[0]
    assert r.target == 'MediaWiki:__bla_ _ '
    assert r.namespace == 8


def test_quotes_in_tags():
    """http://code.pediapress.com/wiki/ticket/199"""
    vlist = parse("""<source attr="value"/>""").find(parser.TagNode)[0].vlist
    print "VLIST:", vlist
    assert vlist == dict(attr="value"), "bad vlist"

    vlist = parse("""<source attr='value'/>""").find(parser.TagNode)[0].vlist
    print "VLIST:", vlist
    assert vlist == dict(attr="value"), "bad vlist"


def test_imagemap():
    r = parse('''<imagemap>Image:Ppwiki.png| exmaple map|100px|thumb|left
    # this is a comment
    # rectangle half picture (left)
    rect 0 0 50 100 [[na| link description]]

    #circle top right
    circle 75 25 24 [http://external.link | just a other link desc]

    #poly bottom right
    poly 51 51 100 51 75 100 [[bleh| blubb]]
    </imagemap>
''')


def test_bad_imagemap():
    """http://code.pediapress.com/wiki/ticket/572"""

    r = parse("""<imagemap>
foo bar baz
</imagemap>""")


def test_link_with_quotes():
    """http://code.pediapress.com/wiki/ticket/303"""
    r = parse("[[David O'Leary]]")
    link = r.find(parser.ArticleLink)[0]
    assert link.target == "David O'Leary", 'Link target not fully detected'


def test_no_tab_removal():
    d = DummyDB()
    r = uparser.parseString(title='', raw='\ttext', wikidb=d)
    assert not r.find(parser.PreFormatted), 'unexpected PreFormatted node'


def test_nowiki_inside_tags():
    """http://code.pediapress.com/wiki/ticket/366"""

    s = """<span style="color:<nowiki>#</nowiki>DF6108;">foo</span>"""
    r = parse(s)
    tags = r.find(parser.TagNode)
    print "tags:", tags
    assert tags, "no tag node found"
    tag = tags[0]
    print "vlist:", tag.vlist
    assert tag.vlist == {'style': {u'color': u'#DF6108'}}, "bad vlist"


def test_misformed_tag():
    s = '<div"barbaz">bold</div>'
    r = parse(s).find(parser.TagNode)
    assert len(r) == 1, "expected a TagNode"


def test_p_tag():
    s = u"<p>para1</p><p>para2</p>"
    r = parse(s).find(parser.Paragraph)
    print "PARAGRAPHS:", r
    assert len(r) == 2, "expected 2 paragraphs"


def test_table_style_parsing_1():
    """http://code.pediapress.com/wiki/ticket/172"""
    s = '{| class="prettytable"\n|-\n|blub\n|align="center"|+bla\n|}\n'
    r = parse(s)
    cells = r.find(parser.Cell)
    print "VLIST:", cells[1].vlist
    assert cells[1].vlist == dict(align="center"), "bad vlist"


def test_table_style_parsing_jtalbot():
    """mentioned in http://code.pediapress.com/wiki/ticket/172,
    but real issue is:
    http://code.pediapress.com/wiki/ticket/366
    """
    s = '{| \n|-\n| cell 1 <ref>this|| not cell2</ref>\n|}\n'
    r = parse(s)
    cells = r.find(parser.Cell)
    assert len(cells) == 1, "expected exactly one cell"


def test_force_close_1():
    s = """{|
|-
| <ul><li>bla</li>
| bla 2
|-
| bla 3
| bla 4
|}
"""
    r = parse(s)
    cells = r.find(parser.Cell)
    print "CELLS:", cells
    assert len(cells) == 4, "expected 4 cells"


@pytest.mark.xfail
def test_force_close_code():
    s = "before <code>inside<code> after"
    r = parse(s)

    tagnodes = r.find(parser.TagNode)
    assert len(tagnodes) == 1, "expected exactly one tagnode"
    txt = tagnodes.asText()
    print "TXT:", txt
    assert "after" not in txt


def test_force_close_section_in_li():
    """example from http://code.pediapress.com/wiki/ticket/63"""
    s = """<ul><li>item
== section 1 ==
baz
"""
    r = parse(s)
    items = r.find(parser.Item)
    assert len(items) == 1, "epxected one item"
    sections = items[0].find(parser.Section)
    assert len(sections) == 1, "expected exactly one section inside li"


def test_namedurl_inside_list():
    r = parse(u"* [http://pediapress.com pediapress]")
    urls = r.find(parser.NamedURL)
    assert len(urls) == 1, "expected exactly one NamedURL"


def test_link_in_sectiontitle():
    r = parse("== [[mainz]] ==")
    links = r.find(parser.Link)
    assert len(links) == 1, "expected exactly one link"


def test_table_whitespace_before_cell():
    r = parse("""
{|
|-
 | bgcolor="#aacccc" | cell1
| cell2
|}
""")
    cells = r.find(parser.Cell)
    print "CELLS:", cells
    assert len(cells) == 2, "expected exactly 3 cells"
    print "VLIST:", cells[0].vlist
    assert cells[0].vlist == dict(bgcolor="#aacccc")


def test_table_whitespace_before_row():
    r = parse('''
{|
  |- bgcolor="#aacccc"
| | cell1
| cell2
|}
''')
    rows = r.find(parser.Row)
    print "ROWS:", rows
    assert len(rows) == 1, "expected exactly one row"
    print "VLIST:", rows[0].vlist
    assert rows[0].vlist == dict(bgcolor="#aacccc")


def test_table_whitespace_before_begintable():
    def check(s):
        r = parse(s)
        tables = r.find(parser.Table)
        assert len(tables) == 1, "expected exactly one table"
        assert tables[0].vlist == dict(bgcolor="#aacccc")

    yield check, '''
 {| bgcolor="#aacccc"
|-
| cell1
| cell2
|}
'''

    yield check, '''
:::{| bgcolor="#aacccc"
|-
| cell1
| cell2
|}
'''


def test_closing_td_in_cell():
    r = parse("""
{|
<td>foo</td>
|}""")
    cell = r.find(parser.Cell)[0]
    assert "td" not in cell.asText()


def test_i_tag():
    r = parse("<i>i</i>")
    s = r.find(parser.Style)[0]
    assert s.caption == "''"


def test_em_tag():
    r = parse("<em>i</em>")
    s = r.find(parser.Style)[0]
    assert s.caption == "''"


def test_big_tag():
    r = parse("<big>i</big>")
    s = r.find(parser.Style)[0]
    assert s.caption == "big"


def test_small_tag():
    r = parse("<small>i</small>")
    s = r.find(parser.Style)[0]
    assert s.caption == "small"


def test_sup_tag():
    r = parse("<sup>i</sup>")
    s = r.find(parser.Style)[0]
    assert s.caption == "sup"


def test_sub_tag():
    r = parse("<sub>i</sub>")
    s = r.find(parser.Style)[0]
    assert s.caption == "sub"


def test_cite_tag():
    r = parse("<cite>i</cite>")
    s = r.find(parser.Style)[0]
    assert s.caption == "cite"


def test_u_tag():
    r = parse("<u>i</u>")
    s = r.find(parser.Style)[0]
    assert s.caption == "u"


def test_strong_tag():
    r = parse("<strong>i</strong>")
    s = r.find(parser.Style)[0]
    assert s.caption == "'''"


def test_hr_tag():
    r = parse("<hr>")
    s = r.find(parser.TagNode)[0]
    assert s.caption == "hr"


def test_hr_line():
    r = parse("------------")
    s = r.find(parser.TagNode)[0]
    assert s.caption == "hr"


def test_broken_link():
    r = parse("[[Image:|bla]]")
    links = r.find(parser.Link)
    assert not links, "expected no links"


def test_broken_link_whitespace():
    r = parse("[[Image:   |bla]]")
    links = r.find(parser.Link)
    assert not links, "expected no links"


def test_comment_inside_nowiki():
    comment = 'this is a comment'
    s = expander.expandstr('<pre><!-- this is a comment --></pre>')
    assert comment in s
    r = parse(s)
    txt = r.asText()
    assert 'this is a comment' in txt


def test_imagemod_space_px():
    """http://code.pediapress.com/wiki/ticket/475"""
    r = parse("[[Image:Thales foo.jpg|400 px|right]]")
    img = r.find(parser.ImageLink)[0]
    txt = img.asText()
    assert 'px' not in txt, 'should contain no children'


def test_imagemod_upright():
    """http://code.pediapress.com/wiki/ticket/459"""

    def doit(s, expected):
        up = parse(s, lang='de').find(parser.ImageLink)[0].upright
        assert up == expected, 'expected %s got %s' % (expected, up)

    yield doit, "[[Datei:bla.jpg|upright|thumb|foobar]]", 0.75
    yield doit, "[[Datei:bla.jpg|upright=0.5|thumb|foobar]]", 0.5


def test_imagemod_localised_magicwords():
    magicwords = [
        {u'aliases': [u'center', u'foobar'], u'case-sensitive': u'', u'name': u'img_center'},
        ]

    def parsei(s, magicwords):
        res = uparser.parseString(title='test',  raw=s, magicwords=magicwords)
        img = res.find(parser.ImageLink)[0]
        return img

    r = parsei(u'[[Image:bla.jpg|foobar]]', magicwords)
    assert r.align == u'center'


def test_paragraph_vs_italic():
    """http://code.pediapress.com/wiki/ticket/514"""
    r = parse("<i>first italic\n\nstill italic</i>")
    styles = r.find(parser.Style)
    txt = " ".join([x.asText() for x in styles])
    assert "first italic" in txt
    assert "still italic" in txt
    paras = r.find(parser.Paragraph)
    assert len(paras) == 2


def test_pull_in_styletags_1():
    s = '<b> one\n\n== two ==\n\nthree\n</b>\nfour\n'
    r = uparser.simpleparse(s)
    styles = r.find(parser.Style)
    txt = " ".join(x.asText() for x in styles)
    assert "one" in txt
    assert "two" in txt
    assert "three" in txt
    assert "four" not in txt


def test_magicwords():
    txt = parse("__NOTOC__").asText()
    print txt
    assert "NOTOC" not in txt

    txt = parse("__NOTOC____NOEDITSECTION__").asText()
    print txt
    assert "NOTOC" not in txt

    txt = parse('__NOINDEX__').asText()
    print txt
    assert 'NOINDEX' not in txt

    txt = parse('__NOGLOSSARY__').asText()
    print txt
    assert 'NOGLOSSARY' not in txt




def test_vlist_newline():
    def check(txt):
        tag = parse(txt).find(parser.TagNode)[0]
        assert tag.vlist, "vlist should not be empty"

    yield check, '<ref\n\nname="bla">bla</ref>'
    yield check, '<ref\n\nname\n\n=\n\n"bla">bla</ref>'
    yield check, '<ref\n\nname\n\n=\n\n"bla\n">bla</ref>'


def test_span_vs_ref():
    s = """<ref><span>bla</ref> after"""
    r = parse(s)
    spans = [x for x in r.find(parser.TagNode) if x.tagname == "span"]
    print "SPAN:", spans
    span = spans[0]
    txt = span.asText()
    assert "after" not in txt


def test_double_source():
    """http://code.pediapress.com/wiki/ticket/536"""
    s = """
<source lang=c>int main</source>
between
<source lang=c>int main2</source>
"""
    r = parse(s)
    nodes = [x for x in r.find(parser.TagNode) if x.tagname == "source"]
    print nodes
    assert len(nodes) == 2


def test_style_tags_vlist():
    s = """
<font color="#00C000">green</font>
"""
    ftag = parse(s).find(parser.TagNode)[0]
    print ftag
    assert ftag.vlist


def test_stray_tag():
    s = "abc</div>def"
    txt = parse(s).asText()
    print txt
    assert "div" not in txt, "stray tag in output"

########NEW FILE########
__FILENAME__ = test_parse_params
#! /usr/bin/env py.test
# -*- coding: utf-8 -*-

from mwlib.refine.util import parseParams


def test_display_none():
    r = parseParams("class=geo style=display:none")
    assert r == {'class': 'geo', 'style': {'display': 'none'}}


def test_bgcolor_hashmark_no_doublequote():
    """http://code.pediapress.com/wiki/ticket/654"""
    r = parseParams("bgcolor=#ffffff")
    assert r == dict(bgcolor="#ffffff")

########NEW FILE########
__FILENAME__ = test_purge_cache
#! /usr/bin/env py.test

import os, shutil, errno, time, pytest
from mwlib.serve import \
    get_collection_dirs, _rmtree, _find_collection_dirs_to_purge, purge_cache


@pytest.fixture
def tree(tmpdir):
    tmpdir.join("a"*16).ensure(dir=1)
    tmpdir.join("b"*16, "collection.zip").ensure()
    return sorted(get_collection_dirs(tmpdir.strpath))


def test_nested_collection_dirs(tmpdir, monkeypatch):
    tmpdir.join("a" * 16, "b" * 16).ensure(dir=1)
    res = list(get_collection_dirs(tmpdir.strpath))
    print "found", res
    assert res == [tmpdir.join("a" * 16).strpath]


def test_rmtree_nonexistent(tmpdir):
    _rmtree(tmpdir.join("foobar").strpath)


def test_rmtree_other_error(tmpdir, monkeypatch):
    def permdenied(_):
        raise OSError(errno.EPERM, "permission denied")
    monkeypatch.setattr(shutil, "rmtree", permdenied)
    _rmtree(tmpdir.strpath)


def test_rmtree(tmpdir):
    dirpath = tmpdir.join("foo")
    dirpath.ensure(dir=1)
    _rmtree(dirpath.strpath)
    assert not dirpath.check()


def test_find_collection_dirs_to_purge(tmpdir, tree):
    now = time.time()

    res1 = list(_find_collection_dirs_to_purge(tree, now - 100))
    assert res1 == []

    res2 = list(_find_collection_dirs_to_purge(tree, now + 1))
    assert res2 == [tree[1]]


def test_purge_cache(tmpdir, tree, monkeypatch):
    now = time.time()
    monkeypatch.setattr(time, "time", lambda: now + 3600)
    purge_cache(7200, tmpdir.strpath)
    for x in tree:
        assert os.path.isdir(x), "directory has been deleted"

    purge_cache(1800, tmpdir.strpath)
    assert os.path.isdir(tree[0]), "empty directory has been deleted"
    assert not os.path.exists(tree[1]), "directory still there"

########NEW FILE########
__FILENAME__ = test_redirect
#! /usr/bin/env py.test

import os

from mwlib import utils

here = os.path.dirname(__file__)


def mwzip(metabook):
    os.environ["S"] = metabook
    dst = os.environ["D"] = metabook.replace(".mb.json", ".zip")
    err = os.system("mw-zip -x -m $S -o $D")
    assert err == 0, "mw-zip failed"
    return dst


def render_get_text(metabook):
    z = mwzip(metabook)
    os.environ["S"] = z
    dst = os.environ["D"] = z.replace(".zip",  ".pdf")
    err = os.system("mw-render -c $S -o $D -w rl")
    assert err == 0, "mw-render failed"
    txt = utils.pdf2txt(dst)
    return txt


def no_redirect(mb):
    mb = os.path.join(here, mb)
    txt = render_get_text(mb)
    print "txt:",  repr(txt)
    assert "redirect" not in txt.lower(), "redirect not resolved"


def test_redirect_canthus_xnet():
    no_redirect("canthus.mb.json")


def test_redirect_kolumne_xnet():
    no_redirect("kolumne.mb.json")

########NEW FILE########
__FILENAME__ = test_refine
#! /usr/bin/env py.test

import pytest
from mwlib.refine import core
from mwlib import nshandling

tokenize = core.tokenize
show = core.show
T = core.T


def parse_txt(*args, **kwargs):
    p = core.parse_txt(*args, **kwargs)
    core.show(p)
    return p


def empty():
    empty = core.XBunch()
    empty.nshandler = nshandling.get_nshandler_for_lang('de')
    return empty


def test_parse_row_missing_beginrow():
    tokens = tokenize("<td>implicit row starts here</td><td>cell2</td>")
    core.parse_table_rows(tokens, empty())
    show(tokens)
    assert len(tokens) == 1
    assert tokens[0].type == T.t_complex_table_row
    assert [x.type for x in tokens[0].children] == [T.t_complex_table_cell] * 2


def test_parse_table_cells_missing_close():
    tokens = core.tokenize("<td>bla")
    core.parse_table_cells(tokens, empty())
    show(tokens)
    assert tokens[0].type == T.t_complex_table_cell, "expected a complex table cell"


def test_parse_table_cells_closed_by_next_cell():
    tokens = core.tokenize("<td>foo<td>bar")
    core.parse_table_cells(tokens, empty())
    show(tokens)
    assert tokens[0].type == T.t_complex_table_cell
    assert tokens[1].type == T.t_complex_table_cell

    assert len(tokens[0].children) == 1
    assert tokens[0].children[0]


def test_parse_table_cells_pipe():
    tokens = tokenize("{|\n|cell0||cell1||cell2\n|}")[2:-2]
    print "BEFORE:"
    show(tokens)
    core.parse_table_cells(tokens, empty())
    print "AFTER"
    show(tokens)
    assert len(tokens) == 3

    for i, x in enumerate(tokens):
        print "cell", i
        assert x.type == T.t_complex_table_cell, "cell %s bad" % (i,)
        assert len(x.children) == 1, "cell %s has wrong number of children" % (i,)
        assert x.children[0].type == T.t_text
        assert x.children[0].text == ('cell%s' % i)


def test_parse_cell_modifier():
    tokens = tokenize("""{|
|align="right"|cell0|still_cell0
|}""")[2:-2]

    print "BEFORE:"
    show(tokens)
    core.parse_table_cells(tokens, empty())
    print "AFTER"
    show(tokens)
    assert tokens[0].type == T.t_complex_table_cell
    assert tokens[0].vlist == dict(align="right")
    assert T.join_as_text(tokens[0].children) == "cell0|still_cell0"


def test_parse_table_modifier():
    tokens = tokenize("""{| border="1"
|}
""")

    print "BEFORE:"
    show(tokens)
    core.parse_tables(tokens, empty())

    print "AFTER:"
    show(tokens)

    assert tokens[0].type == T.t_complex_table
    assert tokens[0].vlist == dict(border=1)


def test_parse_table_row_modifier():
    tokens = tokenize("""{|
|- style="background:red; color:white"
| cell
|}
""")[2:-2]

    print "BEFORE:"
    show(tokens)
    core.parse_table_rows(tokens, empty())

    print "AFTER:"
    show(tokens)

    assert tokens[0].vlist


def test_parse_link():
    tokens = tokenize("[[link0]][[link2]]")
    core.parse_links(tokens, empty())
    show(tokens)
    assert len(tokens) == 2
    assert tokens[0].type == T.t_complex_link
    assert tokens[1].type == T.t_complex_link


def test_no_row_modifier():
    s = "{|\n|foo||bar\n|}"
    r = core.parse_txt(s)
    core.show(r)
    cells = list(core.walknode(r, lambda x: x.type == core.T.t_complex_table_cell))
    print "CELLS:", cells
    assert len(cells) == 2, "expected 2 cells"


def test_parse_para_vs_preformatted():
    s = ' foo\n\nbar\n'
    r = core.parse_txt(s)
    core.show(r)
    pre = list(core.walknode(r, lambda x: x.type == core.T.t_complex_preformatted))[0]
    core.show(pre)
    textnodes = list(core.walknode(pre, lambda x: x.type == core.T.t_text))
    txt = ''.join([x.text for x in textnodes])
    assert u'bar' not in txt


def test_duplicate_nesting():
    s = u"""<b>
[[foo|bar]] between
</b>"""
    r = core.parse_txt(s)
    bolds = list(core.walknode(r, lambda x: x.tagname == "b"))
    core.show(bolds)

    for x in bolds:
        for y in x.children or []:
            assert y.tagname != "b"


def test_ref_no_newline():
    s = u"""<ref>* no item</ref>"""
    r = core.parse_txt(s)
    core.show(r)
    linodes = list(core.walknode(r, lambda x: x.tagname == "li"))
    assert not linodes


def test_tab_table():
    s = """
\t{|
|-
\t| cell1
| cell2
\t|}after
"""
    r = core.parse_txt(s)
    core.show(r)
    tables = []

    def allowed(node):
        retval = bool(tables)
        if node.type == T.t_complex_table:
            tables.append(node)
        return retval
    nodes = [x for x in r if allowed(x)]
    assert nodes, "bad  or no table"

    cells = core.walknodel(r, lambda x: x.type == T.t_complex_table_cell)
    assert len(cells) == 2, "expected two cells"


def test_parse_ul_not_preformatted():
    """http://code.pediapress.com/wiki/ticket/554"""
    s = """
<ul>
   <li>bla blub
   <li>bla bla
 </ul>
"""
    r = parse_txt(s)
    core.show(r)
    pre = core.walknodel(r, lambda x: x.type == T.t_complex_preformatted)
    assert not pre, "should contain no preformatted nodes"


def test_link_vs_center():
    """http://code.pediapress.com/wiki/ticket/559"""
    s = """[[foo|bar <center> not closed]]"""
    r = parse_txt(s)
    core.show(r)
    assert r[0].type == T.t_complex_link, "expected a link"


def test_no_combine_dd_dt():
    """http://code.pediapress.com/wiki/ticket/549"""
    def doit(s):
        r = parse_txt(s)
        core.show(r)
        styles = core.walknodel(r, lambda x: x.type == T.t_complex_style)
        print styles
        assert len(styles) == 2

    yield doit, ":first\n:second\n"
    yield doit, ";first\n;second\n"


def test_combine_preformatted():
    """http://code.pediapress.com/wiki/ticket/569"""
    s = " preformatted\n and more preformatted\n"
    r = parse_txt(s)
    core.show(r)
    pre = core.walknodel(r, lambda x: x.type == T.t_complex_preformatted)
    assert len(pre) == 1, "expected exactly one preformatted node"


def test_bad_section():
    """http://code.pediapress.com/wiki/ticket/588"""
    s = """<div>
div ends here
== this is </div> a section title ==
some text
"""
    r = parse_txt(s)
    show(r)


def test_mark_style_595():
    """http://code.pediapress.com/wiki/ticket/595"""
    r = parse_txt('<b><i>[[Article link|Display text]]</i></b> after')
    b = core.walknodel(r, lambda x: x.tagname == "b")
    print b
    assert len(b) == 1, "expected exactly one bold node"


def test_unexpected_end():
    """http://code.pediapress.com/wiki/ticket/607"""
    parse_txt("{|")


def test_link_in_table_caption():
    """http://code.pediapress.com/wiki/ticket/578"""
    s = """{|
|+ id="CFNP" [[bla | blubb]]
|-
| a || b
|}
"""
    r = parse_txt(s)
    with_vlist = core.walknodel(r, lambda x: bool(x.vlist))
    print with_vlist

    assert not with_vlist,  "no node should contain a vlist"


def test_html_entity_in_pre():
    r = parse_txt("<pre>&gt;</pre>")
    txt = r[0].children[0].text
    print txt
    assert txt == ">",  "wrong text"


def test_nowiki_in_pre():
    """http://code.pediapress.com/wiki/ticket/617"""
    r = parse_txt("<pre><nowiki>foo</nowiki></pre>")
    txt = r[0].children[0].text
    print txt
    assert txt == "foo",  "wrong text"


def test_s_tag():
    r = parse_txt("<s>strike</s>")
    s = core.walknodel(r, lambda x: x.tagname == "s")
    assert len(s) == 1


def test_var_tag():
    r = parse_txt("<var>strike</var>")
    s = core.walknodel(r, lambda x: x.tagname == "var")
    assert len(s) == 1


def test_empty_link():
    """http://code.pediapress.com/wiki/ticket/621"""
    r = parse_txt("[[de:]]")
    print r
    assert r[0].type == T.t_complex_link


def test_source():
    r = parse_txt("foo <source>bar</source> baz")
    show(r)
    assert r[0].tagname == "p"
    assert r[1].tagname == "source"
    assert r[1].blocknode == True
    assert r[2].tagname == "p"


def test_source_enclose():
    r = parse_txt('foo <source enclose="none">bar</source> baz')
    show(r)
    assert r[0].type == T.t_text
    assert r[1].tagname == "source"
    assert r[1].blocknode == False
    assert r[2].type == T.t_text


def test_urllink_in_link():
    """http://code.pediapress.com/wiki/ticket/602"""
    r = parse_txt("[[foo|[http://baz.com baz]]]")
    li = core.walknodel(r, lambda x: x.type == T.t_complex_link)
    assert len(li) == 1,  "expected one link"
    nu = core.walknodel(r, lambda x: x.type == T.t_complex_named_url)
    show(r)
    assert len(nu) == 1, "expected exactly one named url"


def test_urllink_in_brackets():
    """http://code.pediapress.com/wiki/ticket/556"""
    r = parse_txt("[[http://example.com bla]]")
    show(r)
    nu = core.walknodel(r, lambda x: x.type == T.t_complex_named_url)
    print nu
    assert len(nu) == 1,  "expected exactly one named url"


def test_lines_with_table_space():
    parse_txt("""* foo
 :{|
 |-
 | bar
 | baz
 |}
""")


def test_sub_close_sup():
    """http://code.pediapress.com/wiki/ticket/634"""
    r = parse_txt("<sup>foo</sub>bar")
    show(r)
    assert "bar" not in T.join_as_text(r[0].children), "bar should not be inside sup tag"


def test_sup_close_sub():
    """http://code.pediapress.com/wiki/ticket/634"""
    r = parse_txt("<sub>foo</sup>bar")
    show(r)
    assert "bar" not in T.join_as_text(r[0].children), "bar should not be inside sub tag"


def test_dd_dt_tags_inside_table():
    r = parse_txt("""{|
|-
| blubb <dl> bla <dt>foobazbar</dt>
|}
<dl> bla <dt>foobazbar</dt>
""")
    show(r)
    #assert 0 # FIXME


def test_left_to_right_mark():
    def doit(s):
        r = parse_txt(s)
        show(r)
        target = r[0].target
        assert target == "Image:foo.jpg", "wrong target"

    for mark in (u"\u200e", u"\u200f"):
        s = u"[[Image:foo.jpg" + mark + "|thumb|foobar]]"
        yield doit, s


def test_image_blocknode():

    def blocknode(s):
        r = parse_txt(s)[0]
        assert r.blocknode

    def noblocknode(s):
        r = parse_txt(s)[0]
        assert not r.blocknode

    yield noblocknode, "[[Image:foo.png]]"
    yield noblocknode, "[[Image:foo.png|150px]]"
    yield noblocknode, "[[Image:foo.png|frameless]]"

    yield blocknode, "[[Image:foo.png|left]]"
    yield blocknode, "[[Image:foo.png|thumb]]"
    yield blocknode, "[[Image:foo.png|frame]]"


def test_no_preformatted_inside_li():
    """stupid: http://code.pediapress.com/wiki/ticket/676"""
    r = parse_txt("""<ol><li>in li:
  foo
  bar
</li></ol>
""")
    core.show(r)
    pre = core.walknodel(r, lambda x: x.type == T.t_complex_preformatted)
    assert not pre,  "should not contain preformatted"


def test_preformatted_empty_line():
    r = parse_txt("foo\n  pre1\n  \n  pre2\nbar\n")
    core.show(r)
    pre = core.walknodel(r, lambda x: x.type == T.t_complex_preformatted)
    assert len(pre) == 1, "expected exactly one preformatted node"


def test_inputbox():
    s = "</inputbox>"

    r = core.parse_txt(s)
    core.show(r)


def test_ref_inside_caption():
    s = """
{|
|+ table capshun <ref>references fun</ref>
| hey || ho
|}"""
    r = core.parse_txt(s)
    core.show(r)
    cap = core.walknodel(r, lambda x: x.type == T.t_complex_caption)[0]
    print "caption:"
    core.show(cap)
    refs = core.walknodel(cap, lambda x: x.tagname == "ref")
    assert refs


def test_tr_inside_caption():
    """http://code.pediapress.com/wiki/ticket/709"""
    s = """
{|
|+ table capshun <tr><td>bla</td></tr>
|}"""
    r = core.parse_txt(s)
    core.show(r)
    cap = core.walknodel(r, lambda x: x.type == T.t_complex_caption)[0]
    print "caption:"
    core.show(cap)

    rows = core.walknodel(r, lambda x: x.type == T.t_complex_table_row)
    print "ROWS:",  rows
    assert len(rows) == 1,  "no rows found"

    rows = core.walknodel(cap, lambda x: x.type == T.t_complex_table_row)
    print "ROWS:",  rows
    assert len(rows) == 0, "row in table caption found"


def test_ul_inside_star():
    """http://code.pediapress.com/wiki/ticket/735"""
    r = core.parse_txt("""
* foo
* bar </ul> baz
""")
    core.show(r)
    ul = core.walknodel(r, lambda x: x.tagname == "ul")

    def baz(x):
        if x.text and "baz" in x.text:
            return True

    b1 = core.walknodel(ul, baz)
    b2 = core.walknodel(r, baz)

    assert not b1, "baz should not be inside ul"
    assert b2,  "baz missing"


def test_div_vs_link():
    r = core.parse_txt("""[[File:ACDC_logo.gif|thumb| <div style="background-color:#fee8ab"> foo ]]""")
    core.show(r)
    assert r[0].type == T.t_complex_link,  "expected an image link"


def test_link_vs_section():
    r = core.parse_txt("[[acdc\n== foo ]] ==\n")
    core.show(r)
    assert r[0].type != T.t_complex_link, "should not parse a link here"


def test_div_vs_section():
    r = core.parse_txt("""== foo <div style="background-color:#ff0000"> bar ==
baz
""")
    core.show(r)
    assert r[0].level == 2,  "expected a section"


def test_comment_in_gallery():
    """http://code.pediapress.com/wiki/ticket/741"""
    r = core.parse_txt("""<gallery>
Image:ACDC_logo.gif|capshun<!--comment-->
</gallery>
""")
    core.show(r)
    txt = T.join_as_text(core.walknodel(r[0].children,  lambda x: True))
    print "TXT:",  repr(txt)
    assert "capshun" in txt, "bad text??"
    assert "comment" not in txt,  "comment not stripped"


def test_parserfun_in_gallery():
    r = core.parse_txt("""<gallery>
Image:ACDC_logo.gif| capshun {{#if: 1|yes}}

</gallery>
""")
    core.show(r)
    txt = T.join_as_text(core.walknodel(r[0].children,  lambda x: True))
    print "TXT:",  repr(txt)
    assert "capshun" in txt, "bad text??"
    assert "capshun yes" in txt,  "#if failed to expand"


def test_span_vs_lines():
    r = core.parse_txt("""* foo <span> bar
* baz
""")
    core.show(r)

    ul = core.walknodel(r, lambda x: x.tagname == "ul")
    assert len(ul) == 1,  "expected one list"


def test_named_url_in_double_brackets():
    """http://code.pediapress.com/wiki/ticket/556"""
    r = core.parse_txt("[[http://foo.com baz]]")
    core.show(r)
    named = core.walknodel(r, lambda x: x.type == T.t_complex_named_url)
    assert len(named) == 1, "expected a named url"
    txt = T.join_as_text(r)
    print "TXT:",  repr(txt)
    assert "[" in txt, "missing ["
    assert "]" in txt, "missing ]"
    assert "[[" not in txt, "bad text"
    assert "]]" not in txt, "bad text"


def test_link_vs_namedurl():
    r = core.parse_txt("[[acdc [http://web.de bla]]")
    core.show(r)
    txt = T.join_as_text(r)
    print "TXT:",  repr(txt)

    assert "[[acdc " in txt, "wrong text"
    assert txt.endswith("]"), "wrong text"

    assert r[0].type != T.t_complex_link, "should not be an article link"

    urls = core.walknodel(r, lambda x: x.type == T.t_complex_named_url)
    assert len(urls) == 1, "no named url found"


def test_span_vs_paragraph():
    """http://code.pediapress.com/wiki/ticket/751"""
    r = core.parse_txt("foo<span>\n\nbar</span>\n\n")
    core.show(r)
    p = [x for x in r if x.tagname == "p"]
    print "PARAS:",  p
    assert len(p) == 2,  "expected two paragraphs"

    txts = [T.join_as_text(x.children) for x in p]
    print txts
    assert "foo" in txts[0]
    assert "bar" in txts[1]


def test_last_unitialized():
    """last variable was not initialized in fix_urllink_inside_link"""
    core.parse_txt("]]]")


def test_style_tag_closes_same():
    r = core.parse_txt("foo<u>bar<u>baz")
    core.show(r)
    utags = core.walknodel(r, lambda x: x.tagname == "u")

    print "utags:", utags
    txt = "".join([T.join_as_text(x.children) for x in utags])
    print "txt:", txt
    assert txt == u"bar"


def test_hashmark_link():
    r = core.parse_txt("[[#foo]]")
    core.show(r)
    assert r[0].type == T.t_complex_link, "not a link"


def test_ref_drop_text_newlines():
    """http://code.pediapress.com/wiki/ticket/812"""
    r = core.parse_txt("<ref>bar\n\n</ref>")
    core.show(r)
    txt = T.join_as_text(core.walknodel(r, lambda x: 1))
    assert "bar" in txt, "text dropped"


def test_sections_same_depth():
    s = """=Level 1=
foo
==Level 2A==
bar
==Level 2B==
baz"""
    r = parse_txt(s)
    assert len(r) == 1, "section not nested"


def test_references_with_paragraphs():
    s = "<references>\n\n<ref>bla</ref>\n\n</references>"
    r = core.parse_txt(s)
    core.show(r)
    references = core.walknodel(r, lambda x: x.tagname == "references")
    assert len(references) == 1, "expected exactly one references node, got %s" % len(references)
    refs = core.walknodel(references, lambda x: x.tagname == "ref")
    assert len(refs) == 1, "expected exactly one ref node inside the references node, got %s" % len(refs)


def test_newline_in_link_target():
    """http://code.pediapress.com/wiki/ticket/906"""
    s = "[[Albert\nEinstein]]"
    r = core.parse_txt(s)
    core.show(r)
    links = core.walknodel(r, lambda x: x.type == T.t_complex_link)
    assert not links, "links found"


def test_newline_in_link_text():
    """http://code.pediapress.com/wiki/ticket/906"""
    s = "[[Albert Einstein | Albert\nEinstein]]"
    r = core.parse_txt(s)
    core.show(r)
    links = core.walknodel(r, lambda x: x.type == T.t_complex_link)
    assert links, "no links found"

########NEW FILE########
__FILENAME__ = test_render
#! /usr/bin/env py.test

import os, glob, pytest


def zipfiles():
    return glob.glob(os.path.abspath(os.path.join(os.path.dirname(__file__), "*.zip")))


def writer_names():
    import pkg_resources
    for x in pkg_resources.iter_entry_points("mwlib.writers"):
        try:
            x.load()
            yield x.name
        except Exception:
            pass


@pytest.fixture(params=writer_names())
def writer(request):
    return request.param


@pytest.fixture(params=zipfiles())
def zipfile(request):
    return request.param


def test_render(writer, zipfile, tmpdir):
    cmd = "mw-render -w %s -c %s -o %s" % (writer, zipfile, tmpdir.join("output"))
    print "running", cmd
    err = os.system(cmd)
    assert err == 0

########NEW FILE########
__FILENAME__ = test_sanitychecker
#! /usr/bin/env py.test
# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.
from mwlib.advtree import buildAdvancedTree, PreFormatted, Text, Section, ImageLink, Row, Cell, Table, Article, Strong
from mwlib.sanitychecker import Forbid, Allow, Require, Equal, ChildrenOf, AllChildrenOf, SanityException
from mwlib.sanitychecker import ParentOf, ParentsOf, SiblingsOf, SanityChecker, removecb, exceptioncb, RequireChild


def setup():

    # WARNING, ALTERING THIS'll PROPABLY BREAK ALL TESTS! EDIT WITH CARE
    t = [Article(),
         [Section(), [PreFormatted(), [Text("bla blub"), ImageLink()]]],
         [Table(), [Row(), [Cell(), PreFormatted(), [Text("jo")]], ImageLink()]],
         [Section(), [Section(), [Strong()]]],
         [Text("bla")],
         ]
    # WARNING, ALTERING THE ABOVE PROPABLY BREAK ALL TESTS! EDIT WITH CARE

    def rec(elements, parent):
        last = None
        for c in elements:
            if type(c) == type([]):
                assert last
                rec(c, last)
            else:
                if parent:
                    parent.children.append(c)
                last = c

    rec(t, None)
    t = t[0]
    buildAdvancedTree(t)

    #import mwlib.parser, sys;  mwlib.parser.show(sys.stderr, t, 0)

    return t


def checkpass(*rules):
    tree = setup()
    sc = SanityChecker()
    for r in rules:
        sc.addRule(r)
    sc.check(tree)  # should pass


def checkfail(*rules):
    tree = setup()
    sc = SanityChecker()
    for r in rules:
        sc.addRule(r)
    failed = False
    try:
        sc.check(tree)
    except SanityException:
        failed = True
    assert failed


def test_Allow():
    checkfail(ChildrenOf(Table, Allow(Row)))
    checkpass(ChildrenOf(Article, Allow(Section, Text, Table)))


def test_Require():
    checkfail(ChildrenOf(PreFormatted, Require(Section)))
    checkpass(ChildrenOf(PreFormatted, Require(Text)))


def test_Forbid():
    checkfail(ChildrenOf(Section, Forbid(Section)))
    checkpass(ChildrenOf(Table, Forbid(Section)))


def test_Equal():
    checkfail(ChildrenOf(Table, Equal(Row, Row)))
    checkpass(ChildrenOf(Article, Equal(Section, Table, Section, Text)))


def test_removeCB():
    checkfail(RequireChild(Strong))
    tree = setup()
    sc = SanityChecker()
    sc.addRule(RequireChild(Strong), removecb)  # this removes the
    sc.check(tree)
    # now traverse this tree and assert there is no strong
    for c in tree.allchildren():
        assert not isinstance(c, Strong)

########NEW FILE########
__FILENAME__ = test_serve
#! /usr/bin/env py.test

import time
from mwlib import myjson as json
from mwlib import serve


def mkcolldir(tmpdir, name):
    cid = serve.make_collection_id({'metabook': json.dumps({'title': name,  "type": "collection"})})
    d = tmpdir.join(cid[0], cid[:2], cid).ensure(dir=1)
    d.join("output.rl").write("bla")
    return d


def test_purge_cache(tmpdir):
    d1 = mkcolldir(tmpdir, 'c1')
    d2 = mkcolldir(tmpdir, 'c2')
    d2.join("output.rl").setmtime(time.time() - 2)
    serve.purge_cache(1, tmpdir.strpath)
    assert d1.check()
    assert not d2.check()

########NEW FILE########
__FILENAME__ = test_styleanalyzer
#! /usr/bin/env py.test

import signal
from mwlib.parser import styleanalyzer


def test_many_styles():
    signal.alarm(2)  # , signal.siginterrupt)
    signal.signal(signal.SIGALRM, signal.default_int_handler)
    counts = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2]
    try:
        states = styleanalyzer.compute_path(counts)
    except KeyboardInterrupt:
        states = None
    finally:
        signal.alarm(0)

    if states is None:
        raise RuntimeError("styleanaluzer.compute_path took more then 2 seconds to finish")

########NEW FILE########
__FILENAME__ = test_styleutils
#! /usr/bin/env py.test
# -*- coding: utf-8 -*-
# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.


import sys
from mwlib.dummydb import DummyDB
from mwlib.uparser import parseString
from mwlib import parser
from mwlib import advtree
from mwlib.writer import styleutils


def getTreeFromMarkup(raw):
    tree = parseString(title="Test", raw=raw, wikidb=DummyDB())
    advtree.buildAdvancedTree(tree)
    return tree


def show(tree):
    parser.show(sys.stdout, tree)


def test_getTextAlign():
    raw = '''
{|
|-
! center
! style="text-align:right;"|right
|- style="text-align:left;"
! left
! style="text-align:right;"|right
|}
    '''
    tree = getTreeFromMarkup(raw)
    for cell in tree.getChildNodesByClass(advtree.Cell):
        txt = cell.getAllDisplayText().strip()
        align = styleutils.getTextAlign(cell)
        assert txt == align, 'alignment not correctly parsed'


def test_getTextAlign2():
    raw = '''
left

<div style="text-align:right;">
right

<div style="text-align:left;">
left

{| class="prettytable"
|-
| left
| style="text-align:right;" | right
|}

{| class="prettytable" style="text-align:right;"
|-
| right
| style="text-align:center;" | center
|}
</div>
</div>'''

    tree = getTreeFromMarkup(raw)
    for cell in tree.getChildNodesByClass(advtree.Cell):
        txt = cell.getAllDisplayText().strip()
        align = styleutils.getTextAlign(cell)

        if txt != align:
            show(cell)

        assert txt == align, 'alignment not correctly parsed. expected:|%s|, got |%s|' % (txt, align)

########NEW FILE########
__FILENAME__ = test_table
#! /usr/bin/env py.test

from mwlib import dummydb, parser, expander, uparser
from mwlib.expander import DictDB

parse = uparser.simpleparse


def test_simple_table():
    r = parse("""{|
|-
| A || B
|-
| C || D
|}""")
    table = r.find(parser.Table)[0]
    print "TABLE:", table

    assert len(table.children) == 2, "expected two rows"
    for x in table.children:
        assert len(x.children) == 2, "expected 2 cells"


def test_parse_caption():
    s = """{|
|+ caption
|}
"""
    n = parse(s)
    t = n.find(parser.Table)[0]
    assert isinstance(t.children[0], parser.Caption), "expected a caption node"


def test_table_header():
    s = """
{|
|-
! header1 !! header2
|-
| cell1 || cell2
|}
"""

    r = parse(s)
    cells = r.find(parser.Cell)
    assert len(cells) == 4


def test_table_header_2():
    s = """
{|
|-
! header 1 || header 2
| header 3
|-
| cell 1 || cell 2
! cell 3
|}
"""
    r = parse(s)
    cells = r.find(parser.Cell)
    is_header = [x.is_header for x in cells]
    assert is_header == [True, True, False, False, False, True]


def test_caption_modifier():
    s = """
{|
|+style="font-size: 1.25em;" | caption
|-
| cell1
| cell2
|}
"""
    r = parse(s)
    c = r.find(parser.Caption)[0]
    assert c.vlist


def _get_styled_txt(s):
    r = parse(s)
    styles = r.find(parser.Style)
    txt = " ".join(x.asText() for x in styles)
    return txt


def test_table_vs_style_tags_cell_barrier():
    s = """
{|
|-
| cell1<b>bold
| cell2<b>
|}
after
"""
    txt = _get_styled_txt(s)

    assert "cell2" not in txt
    assert "after" not in txt


def test_table_vs_style_tags_continue_after():
    s = """
<b>
{|
|-
| cell1
| cell2
|}
after
"""

    txt = _get_styled_txt(s)
    assert "cell1" not in txt
    assert "after" in txt

########NEW FILE########
__FILENAME__ = test_tagext
#! /usr/bin/env py.test
# -*- coding: utf-8 -*-

# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

from mwlib import uparser
from mwlib import parser
parse = uparser.simpleparse


def test_rot13():
    r = parse(u"""<rot13>test</rot13>""")  # grfg
    txt = [x.caption for x in r.find(parser.Text)]
    assert txt == [u'rot13(test) is grfg']


def test_idl():
    stuff = u"\n\t\ta:=b '''c''' v"
    r = parse(u"""<idl>%s</idl>""" % stuff)
    tn = r.find(parser.TagNode)[0]
    assert isinstance(tn, parser.TagNode)
    assert tn.caption == "source"
    assert tn.vlist["lang"] == "idl"
    for c in tn.children[0].children:
        assert isinstance(c, parser.Text)
    assert stuff == u"".join(c.caption for c in tn.children)

def test_syntaxhightlight():
    raw = '''<syntaxhighlight lang="php">
<?php
    $v = "string";    // sample initialization
?>
html text
<?
    echo $v;         // end of php code
?>
</syntaxhighlight>

'''
    p = parse(raw)
    src = p.find(parser.TagNode)[0]
    assert src and src.tagname == 'source', 'Syntaxhighight node not treated as source'

def test_listing():
    raw = u'''
* <listing name="Attraction name" alt="local or alternative name" address="Address" directions="directions" phone="+91-22-2222-1234" email="fakeemail@fakehost.com" fax="+91-22-2222-1235" url="http://www.example.com" hours="9 pm -5:30 pm" price="Rs. 50 for entrance" lat="latitude" long="longitude" tags="comma,separated,tag_labels">Stuff about the attraction.</listing>'''
    r = parse(raw)


def test_rdf():
    raw = u'''<rdf>
    <> dc:source <http://www.example.com/some/upstream/document.txt>, Wikipedia:AnotherArticle .

    <http://www.example.com/some/upstream/document.txt>
      a cc:Work;
      dc:creator "Anne Example-Person", "Anne Uther-Person";
      dc:contributor "Yadda Nudda Person";
      dc:dateCopyrighted "14 Mar 2005";
      cc:License cc:by-sa-1.0.
 </rdf>'''

    r = parse(raw)
    assert not r.children


def test_poem():
    raw = '''
<poem>
1bla bla
2bla bla
 3bla bla
4bla bla
</poem>
'''
    r = parse(raw)


def test_section():
    raw = '''
a <section begin=chapter1/> 1bla bla <section end=chapter1/> bla
'''
    r = parse(raw)

########NEW FILE########
__FILENAME__ = test_templ_parser
#! /usr/bin/env py.test

import copy
from mwlib.templ import nodes, parser
from mwlib.siteinfo import get_siteinfo

si = copy.deepcopy(get_siteinfo("en"))
del si["magicwords"]


def test_aliasmap_no_magicwords():
    parser.aliasmap(si)


def test_parser_no_magicwords():
    p = parser.Parser(u"some text", siteinfo=si)
    p.parse()

########NEW FILE########
__FILENAME__ = test_templ_pp
#! /usr/bin/env py.test

from mwlib.templ import pp


def preprocess(s, expected, included=True):
    res = pp.preprocess(s, included=included)
    print "preprocess(%r) -> %r" % (s, res)
    if expected is not None:
        assert res == expected, "bad preprocess result"


def test_includeonly_included():
    d = lambda s, e: preprocess(s, e, included=False)
    yield d, "foo<includeonly>bar baz\n\n\nbla</includeonly>123<includeonly>foo bar</includeonly>456", "foo123456"
    yield d, "foo<includeonly>bar baz\n\n\nbla", "foo"
    yield d, "foo<ONLYINCLUDE>123</onlyinclude>456", "foo123456"
    yield d, "foo<NOINCLUDE>123</noinclude>456", "foo123456"


def test_noinclude_ws():
    yield preprocess, "<noinclude>foo</noinclude>", "", True
    yield preprocess, "<noinclude >foo</noinclude>", "", True
    yield preprocess, "<noinclude sadf asdf asd>foo</noinclude>", "", True
    yield preprocess, "<noinclude\n\t>foo</noinclude>", "", True

    yield preprocess, "<noincludetypo>foo</noinclude>", "<noincludetypo>foo</noinclude>", True


def test_includeonly_ws():
    yield preprocess, "<includeonly>foo</includeonly>", "", False
    yield preprocess, "<includeonly >foo</includeonly>", "", False
    yield preprocess, "<includeonly\n>foo</includeonly>", "", False


def test_remove_not_included_ws():
    yield preprocess, "<noinclude>foo", "foo", False
    yield preprocess, "<noinclude >foo", "foo", False
    yield preprocess, "<noinclude\n>foo", "foo", False
    yield preprocess, "<noinclude\nid=5 >foo", "foo", False

    yield preprocess, "<onlyinclude\nid=5 >foo", "foo", False

########NEW FILE########
__FILENAME__ = test_timeline
#! /usr/bin/env py.test
# -*- coding: utf-8 -*-

import pytest, os
if not os.path.isfile('/usr/bin/ploticus'):
    pytest.skip("no ploticus")

from mwlib import timeline

example_script = """ImageSize  = width:800 height:100
PlotArea   = left:65 right:15 bottom:20 top:5
AlignBars  = justify

Colors =
  id:neogene   value:rgb(0.99215,0.8,0.54)
  id:paleogene value:rgb(1,0.7019,0)
  id:cretaceous   value:rgb(0.5,0.764,0.1098)
  id:jurassic      value:rgb(0.302,0.706,0.5)
  id:triassic    value:rgb(0.403,0.765,0.716)
  id:permian   value:rgb(0.404,0.776,0.867)
  id:carboniferous     value:rgb(0.6,0.741,0.855)
  id:devonian  value:rgb(0.6,0.6,0.788)
  id:silurian  value:rgb(0.694,0.447,0.714)
  id:ordovician      value:rgb(0.976,0.506,0.651)
  id:cambrian  value:rgb(0.984,0.5,0.373)
  id:neoproterozoic    value:rgb(0.792,0.647,0.583)
  id:mesoproterozoic    value:rgb(0.867,0.761,0.533)
  id:paleoproterozoic    value:rgb(0.702,0.698,0.369)
  id:eoarchean    value:rgb(0.5,0.565,0.565)
  id:paleoarchean    value:rgb(0.6,0.592,0.569)
  id:mesoarchean    value:rgb(0.698,0.65,0.6)
  id:neoarchean    value:rgb(0.796,0.804,0.784)
  id:ediacaran     value:rgb(0.918,0.847,0.737)
  id:cryogenian    value:rgb(0.863,0.671,0.667)
  id:tonian        value:rgb(0.796,0.643,0.424)
  id:stratherian   value:rgb(1,1,0.8)   # light yellow
  id:calymmian     value:rgb(1,1,0.8)   # light yellow
  id:orosirian     value:rgb(1,1,0.8)   # light yellow
  id:rhyacian      value:rgb(1,1,0.8)   # light yellow
  id:siderian     value:rgb(1,1,0.8)   # light yellow
  id:ectasian      value:rgb(1,1,0.8)   # light yellow
  id:stenian      value:rgb(1,1,0.8)   # light yellow
  id:cenozoic   value:rgb(1,1,0)
  id:mesozoic   value:rgb(0.5,0.6784,0.3176)
  id:paleozoic  value:rgb(0.5,0.7098,0.835)
  id:phanerozoic value:rgb(0.7019,0.886,0.819)
  id:proterozoic value:rgb(0.8,0.85,0.568)
  id:archean   value:rgb(0.6,0.6784,0.6745)
  id:hadean value:rgb(0.4,0.4,0.4)
  id:black  value:black
  id:white  value:white

Period      = from:-4567.17 till:0
TimeAxis    = orientation:horizontal
ScaleMajor  = unit:year increment:500 start:-4500
ScaleMinor  = unit:year increment:100 start:-4500

Define $markred = text:"*" textcolor:red shift:(0,3) fontsize:10

PlotData=
  align:center textcolor:black fontsize:8 mark:(line,black) width:25 shift:(0,-5)

  bar:eon

  at:      0   align:right  $markred
  at:   -542   align:left   $markred shift:(2,3)
  from: -542   till:    0   text:[[Phanerozoic]]  color:phanerozoic
  from:-2500   till: -542   text:[[Proterozoic]]  color:proterozoic
  from:-3800   till: -2500  text:[[Archean]]      color:archean
  from: start  till: -3800  text:[[Hadean]]       color:hadean


  bar:era

  from:  -65.5 till:    0   text:[[Cenozoic|C~z]] shift:(0,1.5)        color:cenozoic
  from: -251   till:  -65.5 text:[[Mesozoic|Meso~zoic]] shift:(0,1.5)  color:mesozoic
  from: -542   till: -251 text:[[Paleozoic|Paleo~zoic]] shift:(0,1.5)  color:paleozoic
  from: -1000  till:  -542  text:[[Neoproterozoic|Neoprote-~rozoic]] shift:(0,1.8) color:neoproterozoic
  from:-1600   till:  -1000  text:[[Mesoproterozoic]] color:mesoproterozoic
  from:-2500   till: -1600  text:[[Paleoproterozoic]] color:paleoproterozoic
  from:-2800   till: -2500  text:[[Neoarchean|Neo-~archean]] shift:(0,1.5)     color:neoarchean
  from:-3200   till: -2800  text:[[Mesoarchean|Meso-~archean]] shift:(0,1.5)   color:mesoarchean
  from:-3600   till: -3200  text:[[Paleoarchean|Paleo-~archean]] shift:(0,1.5) color:paleoarchean
  from:-3800   till: -3600  text:[[Eoarchean|Eoar-~chean]] shift:(0,0.5) color:eoarchean fontsize:6
  from:start   till: -3800  color:white

  bar:period

  fontsize:6
  from:   -23.03 till:    0    color:neogene
  from:  -65.5 till:   -23.03  color:paleogene
  from: -145.5   till:  -65.5  color:cretaceous
  from: -199.6   till: -145.5  color:jurassic
  from: -251   till: -199.6    color:triassic
  from: -299   till: -251      color:permian
  from: -359.2   till: -299    color:carboniferous
  from: -416 till: -359.2      color:devonian
  from: -443.7 till: -416      color:silurian
  from: -488.3   till: -443.7  color:ordovician
  from: -542   till: -488.3    color:cambrian

  from: -630   till:  -542  text:[[Ediacaran|Ed.]] color:ediacaran
  from: -850   till:  -630  text:[[Cryogenian|Cryo-~genian]] color:cryogenian shift:(0,0.5)
  from: -1000  till:  -850  text:[[Tonian|Ton-~ian]] color:tonian shift:(0,0.5)
  from: -1200  till:  -1000 text:[[Stenian|Ste-~nian]] color:mesoproterozoic shift:(0,0.5)
  from: -1400  till:  -1200 text:[[Ectasian|Ect-~asian]] color:mesoproterozoic shift:(0,0.5)
  from: -1600  till:  -1400 text:[[Calymmian|Calym-~mian]] color:mesoproterozoic shift:(0,0.5)
  from: -1800  till:  -1600 text:[[Statherian|Stath-~erian]] color:paleoproterozoic shift:(0,0.5)
  from: -2050  till:  -1800 text:[[Orosirian|Oro-~sirian]] color:paleoproterozoic shift:(0,0.5)
  from: -2300  till:  -2050 text:[[Rhyacian|Rhy-~acian]] color:paleoproterozoic shift:(0,0.5)
  from: -2500  till:  -2300 text:[[Siderian|Sid-~erian]] color:paleoproterozoic shift:(0,0.5)
  from: start  till:  -2500 color:white
"""


def test_draw_timeline():
    fp = timeline.drawTimeline(example_script)
    print "result in", fp
    assert fp, "no image file created"

########NEW FILE########
__FILENAME__ = test_treecleaner
#! /usr/bin/env py.test
# -*- coding: utf-8 -*-
# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import sys, pytest
from mwlib.advtree import buildAdvancedTree
from mwlib import parser
from mwlib.treecleaner import TreeCleaner, _all, _any
from mwlib.advtree import (Article, ArticleLink, Blockquote, BreakingReturn, CategoryLink, Cell, Center, Chapter,
                     Cite, Code, DefinitionList, DefinitionDescription, Div, Emphasized, Gallery, HorizontalRule, ImageLink, InterwikiLink, Item,
                     ItemList, LangLink, Link, Math, NamedURL, NamespaceLink, Paragraph, PreFormatted,
                     Reference, ReferenceList, Row, Section, Source, SpecialLink, Strong, Table, Text, Underline,
                     URL)



def _treesanity(r):
    "check that parents match their children"
    for c in r.allchildren():
        if c.parent:
            assert c in c.parent.children
            assert len([x for x in c.parent.children if x is c]) == 1
        for cc in c:
            assert cc.parent
            assert cc.parent is c


def getTreeFromMarkup(raw):
    from mwlib.dummydb import DummyDB
    from mwlib.uparser import parseString
    return parseString(title="Test", raw=raw, wikidb=DummyDB())


def cleanMarkup(raw):
    print "Parsing %r" % (raw,)

    tree = getTreeFromMarkup(raw)

    print "before treecleaner: >>>"
    showTree(tree)
    print "<<<"

    print '=' * 20
    buildAdvancedTree(tree)
    tc = TreeCleaner(tree, save_reports=True)
    tc.cleanAll(skipMethods=[])
    reports = tc.getReports()
    print "after treecleaner: >>>"
    showTree(tree)
    print "<<<"
    return (tree, reports)


def cleanMarkupSingle(raw, cleanerMethod):
    tree = getTreeFromMarkup(raw)
    buildAdvancedTree(tree)
    tc = TreeCleaner(tree, save_reports=True)
    tc.clean([cleanerMethod])
    reports = tc.getReports()
    return (tree, reports)


def showTree(tree):
    parser.show(sys.stdout, tree, 0)


def test_fixLists():
    raw = r"""
para

* list item 1
* list item 2
** list item 2.1
* list item 3

* list 2 item 1
* list 2 item 2

para

* list 3
"""
    tree, reports = cleanMarkup(raw)
    lists = tree.getChildNodesByClass(ItemList)
    for li in lists:
        print li, li.getParents()
        assert _all([p.__class__ != Paragraph for p in li.getParents()])
    _treesanity(tree)


def test_fixLists2():
    raw = r"""
* list item 1
* list item 2
some text in the same paragraph

another paragraph
    """
    # cleaner should do nothing
    tree, reports = cleanMarkup(raw)
    lists = tree.getChildNodesByClass(ItemList)
    li = lists[0]
    assert li.parent.__class__ == Paragraph
    txt = ''.join([x.asText() for x in li.siblings])
    assert u'some text in the same paragraph' in txt
    assert u'another' not in txt


def test_fixLists3():
    raw = r"""
* ul1
* ul2
# ol1
# ol2
"""
    tree, reports = cleanMarkup(raw)
    assert len(tree.children) == 2  # 2 itemlists as only children of article
    assert _all([c.__class__ == ItemList for c in tree.children])


def test_childlessNodes():
    raw = r"""
blub

<source></source>

*

<div></div>

<u></u>
    """
    tree, reports = cleanMarkup(raw)
    assert len(tree.children) == 1  # assert only the 'blub' paragraph is left and the rest removed
    assert tree.children[0].__class__ == Paragraph


def test_removeLangLinks():
    raw = r"""
bla
[[de:Blub]]
[[en:Blub]]
[[es:Blub]]
blub
"""
    tree, reports = cleanMarkup(raw)
    showTree(tree)
    langlinks = tree.find(LangLink)
    assert not langlinks, 'expected no LangLink instances'


def test_removeCriticalTables():
    raw = r'''
{| class="navbox"
|-
| bla
| blub
|}

blub
'''
    tree, reports = cleanMarkup(raw)
    assert len(tree.getChildNodesByClass(Table)) == 0


def test_fixTableColspans():
    raw = r'''
{|
|-
| colspan="5" | bla
|-
| bla
| blub
|}
    '''
    tree, reports = cleanMarkup(raw)
    t = tree.getChildNodesByClass(Table)[0]
    cell = t.children[0].children[0]
    assert cell.colspan == 2


def test_fixTableColspans2():
    '''http://es.wikipedia.org/w/index.php?title=Rep%C3%BAblica_Dominicana&oldid=36394218'''
    raw = r'''
{|
|-
| colspan="2" | bla1
|-
| colspan="3" | bla2

|}
    '''
    tree, reports = cleanMarkup(raw)
    t = tree.getChildNodesByClass(Table)[0]
    cell = t.children[0].children[0]
    showTree(t)
    assert cell.colspan == 1

# test changed after 3258e2f7978fc4592567bb64977ba5404ee949da
def test_fixTableColspans3():
    '''http://es.wikipedia.org/w/index.php?title=Rep%C3%BAblica_Dominicana&oldid=36394218'''
    raw = ur'''
{| cellpadding="0" cellspacing="0" border="0" style="margin:0px; padding:0px; border:0px; background-color:transparent; vertical-align:middle;"
|-
| style="vertical-align:bottom; padding:0px; margin:0px; border:0px;" | Ägyptische Hieroglyphen können die Funktion von [[Schriftzeichen|Phonogrammen, Ideogrammen]] oder [[Determinativ]]en übernehmen. Die meisten Hieroglyphen können eine oder maximal zwei dieser Funktionen übernehmen, einzelne auch alle drei. Welche Funktion ein Zeichen hat, zeigt der Kontext, in vielen Fällen lassen sich die Verwendungen kaum abgrenzen. So ist das Zeichen <hiero>ra</hiero> Ideogramm in <hiero>ra:Z1</hiero> ''r<span class="Unicode">ˁ(w)</span>'' (Sonnengott) „Re“, in der vollständigeren Schreibung des gleichen Wortes als <hiero>r:a-ra:Z1</hiero> dient es nur als Determinativ; das Zeichen <hiero>pr</hiero> wird im Wort <hiero>pr:r-D54</hiero> ''pr(j)'' „herausgehen“ als Phonogramm ''pr'' aufgefasst, während es in <hiero>pr:Z1</hiero> ''pr(w)'' „Haus“ als Logogramm fungiert. Aufschluss darüber, ob und wie ein Zeichen gelesen werden kann, gibt im Allgemeinen die Zeichenliste der ''Egyptian Grammar'' von [[Alan Gardiner]],<ref>Gardiner 1927</ref> die jedoch nicht vollständig und in Einzelfällen überholt ist.
| style="vertical-align:bottom; padding:0px; margin:0px; border:0px;" |
| style="vertical-align:bottom; padding:0px; margin:0px; border:0px;" |
| style="vertical-align:bottom; padding:0px; margin:0px; border:0px;" |
| style="vertical-align:bottom; padding:0px; margin:0px; border:0px;" |
| style="vertical-align:bottom; padding:0px; margin:0px; border:0px;" |
| style="vertical-align:bottom; padding:0px; margin:0px; border:0px;" |
| style="vertical-align:bottom; padding:0px; margin:0px; border:0px;" |
| style="vertical-align:bottom; padding:0px; margin:0px; border:0px;" |
| style="vertical-align:bottom; padding:0px; margin:0px; border:0px;" |
| style="vertical-align:bottom; padding:0px; margin:0px; border:0px;" |
| style="vertical-align:bottom; padding:0px; margin:0px; border:0px;" |
| style="vertical-align:bottom; padding:0px; margin:0px; border:0px;" |
| style="vertical-align:bottom; padding:0px; margin:0px; border:0px;" |
| style="vertical-align:bottom; padding:0px; margin:0px; border:0px;" |
| style="vertical-align:bottom; padding:0px; margin:0px; border:0px;" |
| style="vertical-align:bottom; padding:0px; margin:0px; border:0px;" |
| style="vertical-align:bottom; padding:0px; margin:0px; border:0px;" |
| style="vertical-align:bottom; padding:0px; margin:0px; border:0px;" |
| style="vertical-align:bottom; padding:0px; margin:0px; border:0px;" |
|}
    '''
    tree, reports = cleanMarkup(raw)

    assert len(tree.getChildNodesByClass(Table)
               ) == 0, 'single row/col Table not transformed to div'
    assert len(tree.getChildNodesByClass(Div)
               ) == 1, 'single row/col Table not transformed to div'


def test_removeBrokenChildren():
    raw = r'''
<ref>
 preformatted text
</ref>
    '''

    tree, reports = cleanMarkup(raw)
    assert len(tree.getChildNodesByClass(PreFormatted)) == 0


def test_fixNesting1():
    raw = '''
:{|
|-
||bla||blub
|}
    '''
    tree, reports = cleanMarkup(raw)
    table = tree.getChildNodesByClass(Table)[0]
    assert not table.getParentNodesByClass(DefinitionDescription)

# changed with f94aa985f50d2db9749c57a7a5e22c0c8d487af4
@pytest.mark.xfail
def test_fixNesting2():
    raw = r'''
<div><div>
* bla
* blub
</div></div>
    '''
    tree, reports = cleanMarkup(raw)
    list_node = tree.getChildNodesByClass(ItemList)[0]
    assert not _any([p.__class__ == Div for p in list_node.getParents()])

# the two tests below only make sense if paragraph nesting is forbidden - this is not the case anymore
# but maybe they are interesting in the future - therefore I did not delete them

## def test_fixNesting3():
##     raw = r'''
## <strike>
## para 1

## para 2
## </strike>
##     '''

##     tree, reports = cleanMarkup(raw)
##     paras = tree.getChildNodesByClass(Paragraph)
##     for para in paras:
##         assert not para.getChildNodesByClass(Paragraph)

## def test_fixNesting4():
##     raw = """
## <strike>

## <div>
##  indented para 1

## regular para

##  indented para 2

## </div>

## </strike>
## """

##     tree = getTreeFromMarkup(raw)
##     tree, reports = cleanMarkup(raw)
##     paras = tree.getChildNodesByClass(Paragraph)
##     for para in paras:
##         assert not para.getChildNodesByClass(Paragraph)


def test_fixNesting5():
    raw = """
<strike>
<div>

<div>

<div>
para 1
</div>

para 2
</div>

<div>
para 2
</div>

</div>
</strike>
    """

    tree, reports = cleanMarkup(raw)
    paras = tree.getChildNodesByClass(Paragraph)
    for para in paras:
        assert not para.getChildNodesByClass(Paragraph)


def test_fixNesting6():
    raw = u"""''„Drei Affen, zehn Minuten.“'' <ref>Dilbert writes a poem and presents it to Dogbert:<poem style>
''DOGBERT: I once read that given infinite time, a thousand monkeys with typewriters would eventually write the complete works of Shakespeare.''
''DILBERT: But what about my poem?''
''DOGBERT: Three monkeys, ten minutes.“''</poem></ref>

<references/>
    """

    tree, reports = cleanMarkup(raw)
    showTree(tree)
    from pprint import pprint
    pprint(reports)

    assert len(tree.getChildNodesByClass(Reference)) == 1


def test_swapNodes():
    raw = r'''
<u><center>Text</center></u>
    '''
    tree, reports = cleanMarkup(raw)
    center_node = tree.getChildNodesByClass(Center)[0]
    assert not _any([p.__class__ == Underline for p in center_node.getParents()])


@pytest.mark.xfail
def test_splitBigTableCells():
    '''
    Splitting big table cells can not properly be tested here.
    Testing needs to be done in the writers, since this test is writer
    specific and the output has to be verfied
    '''
    assert False


@pytest.mark.xfail
def test_fixParagraphs():
    raw = r'''  '''  # FIXME: which markup results in paragraphs which are not properly nested with preceeding sections?
    tree, reports = cleanMarkup(raw)
    assert False


def test_cleanSectionCaptions():
    raw = r'''
==<center>centered heading</center>==
bla
    '''

    tree, reports = cleanMarkup(raw)
    section_node = tree.getChildNodesByClass(Section)[0]
    assert _all([p.__class__ != Center for p in section_node.children[0].getAllChildren()])


def test_cleanSectionCaptions2():
    raw = """=== ===
    bla
    """

    tree, reports = cleanMarkup(raw)
    assert len(tree.getChildNodesByClass(Section)) == 0


def numBR(tree):
    return len(tree.getChildNodesByClass(BreakingReturn))


def test_removebreakingreturnsInside():
    # remove BRs at the inside 'borders' of block nodes
    raw = '''
{|
|-
|<br/>blub<br/>
|text
|-
|<source></source><br/>text
| text
|-
|<br/><source></source><br/><br/>text
| text
|}
'''
    tree, reports = cleanMarkup(raw)  # 1 & 2
    assert numBR(tree) == 0


def test_removebreakingreturnsOutside():
    # remove BRs at the outside 'borders' of block nodes
    raw = '''
<br/>

== section heading ==

<br/>

text

<br/>

<br/>

== section heading 2 ==

<br/><br/>

== section heading 3 ==
<br/>bla</br/>
'''

    tree, reports = cleanMarkup(raw)
    showTree(tree)
    assert numBR(tree) == 0


def test_removebreakingreturnsMultiple():
    # remove BRs at the outside 'borders' of block nodes
    raw = '''
paragraph

<br/>

<br/>

paragraph
'''

    tree, reports = cleanMarkup(raw)
    assert numBR(tree) == 0

# mwlib.refine creates a whitespace only paragraph containing the first
# br tag. in the old parser this first paragraph also contained the source node.


@pytest.mark.xfail
def test_removebreakingreturnsNoremove():
    raw = """
<br/>
<source>
int main()
</source>

<br/>
 <br/> bla <br/> blub

ordinary paragraph. inside <br/> tags should not be removed
"""

    tree, reports = cleanMarkup(raw)
    # the only br tags that should remain after cleaning are the ones inside the preformatted node
    assert numBR(tree) == 3


def test_preserveEmptyTextNodes():
    raw = """[[blub]] ''bla''"""
    tree, reports = cleanMarkup(raw)
    p = [x for x in tree.find(Text) if x.caption == u' ']
    assert len(p) == 1, 'expected one space node'


def test_gallery():
    raw = """<gallery>
Image:There_Screenshot02.jpg|Activities include hoverboarding, with the ability to perform stunts such as dropping down from space
Image:Scenery.jpg|A wide pan over a seaside thatched-roof village
|Members can join and create interest groups
Image:Landmark02.jpg|There contains many landmarks, including a replica of New Orleans
Image:Emotes01.jpg|Avatars can display over 100 emotes
<!-- Deleted image removed: Image:Popoutemotes01.jpg|Avatars can display a wide variety of pop-out emotes -->
Image:Zona.jpg|Zona Island, a place where new members first log in.
Image:Hoverboat01.jpg|A member made vehicle. As an avatar users can paint and build a variety of items.
Image:|Zona Island, a place where new members first log in
<!-- Deleted image removed: Image:OldWaterinHole.jpg|The Old Waterin' Hole: a place where users can sit and chat while in a social club/bar-like environment. -->
</gallery>"""

    tree, reports = cleanMarkup(raw)
    gallery = tree.find(Gallery)[0]
    assert len(gallery.children) == 6


def test_removeTextlessStyles():

    raw = "'''bold text'''"
    tree, reports = cleanMarkup(raw)
    showTree(tree)
    assert tree.find(Strong)

    raw = "text <em><br/></em> text"
    tree, reports = cleanMarkup(raw)
    showTree(tree)
    assert tree.find(BreakingReturn) and not tree.find(Emphasized)


def test_splitTableLists1():
    raw = '''
{|
|-
|
* item 1
* item 2
* item 3
* item 4
* item 5
* item 6

|
* item 7
* item 8
|}
    '''
    tree, reports = cleanMarkup(raw)
    numrows = len(tree.getChildNodesByClass(Row))
    assert numrows == 6, 'ItemList should have been splitted to 6 rows, numrows was: %d' % numrows


def test_splitTableLists2():
    raw = '''
{|
|-
|
* item 1
** item 1.1
** item 1.2
** item 1.3
** item 1.4
** item 1.5
** item 1.6
* item 2
* item 3
* item 4
* item 5
* item 6

|
* item 7
* item 8
|}
    '''
    tree, reports = cleanMarkup(raw)
    numrows = len(tree.getChildNodesByClass(Row))
    assert numrows == 6, 'ItemList should have been splitted to 6 rows, numrows was: %d' % numrows


def test_removeEmptySection():
    raw = '''
== section 1 ==

== section 2 ==

'''
    tree, reports = cleanMarkup(raw)
    assert len(tree.getChildNodesByClass(Section)) == 0, 'section not removed'


def test_noRemoveEmptySection():
    raw = '''
== section 1 ==
[[Image:bla.png]]

== section 2 ==

[[Image:bla.png]]

== section 3 ==

<gallery>
Image:bla.png
</gallery>

== section 4 ==
<div>
[[Image:bla.png]]
</div>
'''

    tree, reports = cleanMarkup(raw)
    assert len(tree.getChildNodesByClass(Section)) == 4, 'section falsly removed'

########NEW FILE########
__FILENAME__ = test_uniq
#! /usr/bin/env py.test

from mwlib import utoken, uniq


def test_self_closing():
    u = uniq.Uniquifier()
    s = u.replace_tags("""
<ref />
----
<ref>
</ref>
""")
    assert s.count("UNIQ") == 2


def test_empty_nowiki():
    u = uniq.Uniquifier()
    s = u.replace_tags("abc<nowiki></nowiki>def")
    assert 'UNIQ' in s
    r = u.replace_uniq(s)
    assert r == "abcdef"


def test_space_in_closing_tag():
    u = uniq.Uniquifier()
    s = u.replace_tags("foo<ref>bar</ref >baz")
    assert "UNIQ" in s, "ref tag not recognized"


def test_comment():
    u = uniq.Uniquifier()

    def repl(txt, expected):
        res = u.replace_tags(txt)
        print repr(txt),  "->",  repr(res)
        assert res == expected

    yield repl,  "foo<!-- bla -->bar",  "foobar"
    yield repl,  "foo\n<!-- bla -->\nbar",  "foo\nbar"
    yield repl,  "foo\n<!-- bla -->bar",  "foo\nbar"
    yield repl,  "foo<!-- bla -->\nbar",  "foo\nbar"

########NEW FILE########
__FILENAME__ = test_utils
#! /usr/bin/env py.test
# -*- coding: utf-8 -*-

"""Unittests for mwlib.utils"""

import os

from mwlib import utils


def test_fsescape():
    test_set = (
        (u'abc', 'abc'),
        ('abc', 'abc'),
        (u'ä', '~228~'),
        ('ä', '~195~~164~'),
        (u'~', '~~'),
        ('~', '~~'),
    )
    for s_in, s_out in test_set:
        assert utils.fsescape(s_in) == s_out
        assert type(utils.fsescape(s_in)) is str


def test_uid():
    uids = set()
    for i in range(100):
        uid = utils.uid()
        assert uid not in uids
        uids.add(uid)
    for max_length in range(1, 20):
        assert len(utils.uid(max_length)) <= max_length


def test_report():
    data = utils.report(system='system123', subject='subject123', foo='foo123')
    assert 'foo' in data
    assert 'foo123' in data


def test_get_safe_url():
    g = utils.get_safe_url
    assert g('http://bla" target="_blank') is None
    assert g('http') is None
    assert g('http://bla/foo/bar" target="_blank') == 'http://bla/foo/bar%22%20target%3D%22_blank'
    assert g(u'http://xyz/wiki/%D0%91%D0%94%D0%A1%D0%9C') == 'http://xyz/wiki/%D0%91%D0%94%D0%A1%D0%9C'


def test_garble_password():
    x = utils.garble_password(['foo', '--password', 'secret'])
    assert 'secret' not in x
    utils.garble_password(['foo', '--password'])
    utils.garble_password(['foo'])

########NEW FILE########
__FILENAME__ = test_writerutils
#! /usr/bin/env py.test
# -*- coding: utf-8 -*-
# Copyright (c) 2007-2009 PediaPress GmbH
# See README.rst for additional licensing information.

import sys

from mwlib import parser
from mwlib import advtree
from mwlib.treecleaner import TreeCleaner
from mwlib.writer import styleutils


def showTree(tree):
    parser.show(sys.stdout, tree, 0)


def getTreeFromMarkup(raw):
    from mwlib.dummydb import DummyDB
    from mwlib.uparser import parseString
    return parseString(title="Test", raw=raw, wikidb=DummyDB())


def buildAdvTree(raw):
    tree = getTreeFromMarkup(raw)
    advtree.buildAdvancedTree(tree)
    tc = TreeCleaner(tree, save_reports=True)
    tc.cleanAll(skipMethods=[])
    return tree


def test_textalign1():
    raw = '''
{|
|-
| style="text-align:right;" | right aligned
| style="text-align:left;" | left aligned
|-
| style="text-align:center;" | centered
| style="text-align:bogus;" | bogus align --> left
|}
'''
    tree = buildAdvTree(raw)
    cells = tree.getChildNodesByClass(advtree.Cell)
    correct_align = ['right', 'left', 'center', 'left']
    for (i, cell) in enumerate(cells):
        align = styleutils.getTextAlign(cell)
        assert align == correct_align[i], 'styleutils.getCelTextAlign return false alignment'


def test_textalign2():
    raw = '''
{| style="text-align:right;" class="prettytable"
|- style="text-align:center;"
| style="text-align:left;" | left aligned
| centered
|-
| right aligned
| style="text-align:center;" | centered
|-
| align="center" | centered
| align="left" | left
|}


<center>
some centered text

<div style="text-align:left;">
left aligned div in the middle
</div>

and more centering
</center>
'''
    tree = buildAdvTree(raw)
    tree.show()
    cells = tree.getChildNodesByClass(advtree.Cell)
    correct_align = ['left', 'center', 'right', 'center', 'center', 'left']
    for (i, cell) in enumerate(cells):
        align = styleutils.getTextAlign(cell)
        assert align == correct_align[i], 'styleutils.getCelTextAlign return false alignment'

    center = tree.getChildNodesByClass(advtree.Center)
    texts = center[0].getChildNodesByClass(advtree.Text)
    correct_align = ['center', 'left', 'center']
    for (i, txt) in enumerate(texts):
        assert styleutils.getTextAlign(txt) == correct_align[i], 'styleutils.getCelTextAlign return false alignment'


def test_textalign3():
    raw = '''
{| style="text-align:right;width:100%;" class="prettytable"
|-
| right aligned text that gives us some space
| more text, text, text
|- align="center"
| style="text-align:left;" | left aligned
| centered
|}
'''
    tree = buildAdvTree(raw)
    cells = tree.getChildNodesByClass(advtree.Cell)
    correct_align = ['right', 'right', 'left', 'center']
    for (i, cell) in enumerate(cells):
        align = styleutils.getTextAlign(cell)
        assert align == correct_align[i], 'styleutils.getCelTextAlign return false alignment'

########NEW FILE########
__FILENAME__ = test_zipwiki
#! /usr/bin/env py.test

import os
import tempfile

from mwlib import parser, wiki


class Test_xnet_zipwiki(object):
    def setup_class(cls):
        fd, cls.zip_filename = tempfile.mkstemp(suffix=".zip")
        os.close(fd)
        print 'generating ZIP file'
        rc = os.system('mw-zip -c :en -o %s "The Living Sea"' % cls.zip_filename)
        print 'ZIP file generation finished'
        assert rc == 0, 'Could not create ZIP file. Is mw-zip in PATH?'

    def teardown_class(cls):
        if os.path.exists(cls.zip_filename):
            os.unlink(cls.zip_filename)

    def setup_method(self, method):
        print "reading",  self.zip_filename
        self.env = wiki.makewiki(self.zip_filename)
        self.wikidb = self.env.wiki
        self.imagedb = self.env.images

    def teardown_method(self, method):
        # self.imagedb.clean()
        pass

    def test_getArticle(self):
        a = self.wikidb.normalize_and_get_page(u'The Living Sea', 0)
        assert isinstance(a.rawtext, unicode)
        assert a.rawtext

    def test_getParsedArticle(self):
        p = self.wikidb.getParsedArticle(u'The Living Sea')
        assert isinstance(p, parser.Article)

    def test_getURL(self):
        url = self.wikidb.getURL(u'The Living Sea')
        assert url == 'http://en.wikipedia.org/w/index.php?title=The_Living_Sea'

    def test_ImageDB(self):
        p = self.imagedb.getDiskPath(u'Thelivingseaimax.jpg')
        assert isinstance(p, basestring)
        assert os.path.isfile(p)
        assert os.stat(p).st_size > 0
        assert p == self.imagedb.getDiskPath(u'Thelivingseaimax.jpg', 123)

        url = self.imagedb.getDescriptionURL(u'Thelivingseaimax.jpg')
        assert url == 'http://en.wikipedia.org/w/index.php?title=File:Thelivingseaimax.jpg'

        templates = self.imagedb.getImageTemplates(u"Thelivingseaimax.jpg")
        print templates
        assert templates

        contribs = self.imagedb.getContributors(u"Thelivingseaimax.jpg")
        print contribs
        assert contribs

    def test_getSource(self):
        src = self.wikidb.getSource(u'The Living Sea')
        print src

########NEW FILE########
__FILENAME__ = set-interwiki
#! /usr/bin/env python

"""usage: set-interwiki SITEINFO.JSON | mysql WIKIDB
    create entries in interwiki table from existing siteinfo
    (see http://www.mediawiki.org/wiki/Manual:Interwiki)
"""

import sys
try:
    import simplejson as json
except ImportError:
    import json

def main():
    if len(sys.argv)!=2:
        import __main__
        print __main__.__doc__
        sys.exit(10)
        
    siteinfo = json.load(open(sys.argv[1]))
    for e in siteinfo["interwikimap"]:
        msg = u"INSERT INTO interwiki SET iw_prefix='%s', iw_url='%s', iw_local=1, iw_trans=0 ;" % (e["prefix"], e["url"])
        print msg.encode("utf-8")

if __name__=="__main__":
    main()
    

########NEW FILE########
