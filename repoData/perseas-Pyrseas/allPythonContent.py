__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Pyrseas documentation build configuration file, created by
# sphinx-quickstart on Fri Dec 17 22:06:15 2010.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.append(os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'Pyrseas'
copyright = u'2013, Joe Abbate'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
from pyrseas import __version__ as vers
# The short X.Y version.
version = '.'.join(vers.split('.')[:2])
# The full version, including alpha/beta/rc tags.
release = vers

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of documents that shouldn't be included in the build.
#unused_docs = []

# List of directories, relative to source directory, that shouldn't be searched
# for source files.
exclude_trees = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  Major themes that come with
# Sphinx are currently 'default' and 'sphinxdoc'.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_use_modindex = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'Pyrseasdoc'


# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'Pyrseas.tex', u'Pyrseas Documentation',
   u'Joe Abbate', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_use_modindex = True

########NEW FILE########
__FILENAME__ = audit
# -*- coding: utf-8 -*-
"""
    pyrseas.augment.audit
    ~~~~~~~~~~~~~~~~~~~~~

    This module defines two classes: CfgAuditColumn derived from
    DbAugment and CfgAuditColumnDict derived from DbAugmentDict.
"""
from pyrseas.augment import DbAugment, DbAugmentDict
from pyrseas.dbobject import split_schema_obj


class CfgAuditColumn(DbAugment):
    """An augmentation that adds automatically maintained audit columns"""

    keylist = ['name']

    def apply(self, table, augdb):
        """Apply audit columns to argument table.

        :param table: table to which columns/triggers will be added
        :param augdb: augment dictionaries
        """
        currdb = augdb.current
        sch = table.schema
        for col in self.columns:
            augdb.columns[col].apply(table)
        if hasattr(self, 'triggers'):
            for trg in self.triggers:
                augdb.triggers[trg].apply(table)
                for newtrg in table.triggers:
                    fncsig = table.triggers[newtrg].procedure
                    fnc = fncsig[:fncsig.find('(')]
                    (sch, fnc) = split_schema_obj(fnc)
                    if (sch, fncsig) not in currdb.functions:
                        newfunc = augdb.functions[fnc].apply(
                            sch, augdb.columns.col_trans_tbl, augdb)
                        # add new function to the current db
                        augdb.add_func(sch, newfunc)
                        augdb.add_lang(newfunc.language)


class CfgAuditColumnDict(DbAugmentDict):
    "The collection of audit column augmentations"

    cls = CfgAuditColumn

    def __init__(self, config):
        for aud in config:
            self[aud] = CfgAuditColumn(name=aud, **config[aud])

    def from_map(self, inaudcols):
        """Initalize the dictionary of functions by converting the input map

        :param inaudcols: YAML map defining the audit column configuration
        """
        for aud in inaudcols:
            audcol = CfgAuditColumn(name=aud)
            for attr in inaudcols[aud]:
                if attr == 'columns':
                    audcol.columns = [col for col in inaudcols[aud][attr]]
                elif attr == 'triggers':
                    audcol.triggers = [col for col in inaudcols[aud][attr]]
            self[audcol.name] = audcol

########NEW FILE########
__FILENAME__ = column
# -*- coding: utf-8 -*-
"""
    pyrseas.augment.column
    ~~~~~~~~~~~~~~~~~~~~~~

    This module defines two classes: CfgColumn derived from
    DbAugment and CfgColumnDict derived from DbAugmentDict.
"""
from pyrseas.augment import DbAugmentDict, DbAugment
from pyrseas.dbobject.column import Column


class CfgColumn(DbAugment):
    "A configuration column definition"

    keylist = ['name']

    def apply(self, table):
        """Add columns to the table passed in.

        :param table: table to which the columns will be added
        """
        if self.name in table.column_names():
            for col in table.columns:
                if col.name == self.name:
                    col.type = self.type
                    if hasattr(self, 'not_null'):
                        col.not_null = self.not_null
                    if hasattr(self, 'default'):
                        col.default = self.default
        else:
            newcol = Column(schema=table.schema, table=table.name,
                            **self.__dict__)
            newcol.number = 0
            newcol._table = table
            table.columns.append(newcol)


class CfgColumnDict(DbAugmentDict):
    "The collection of configuration columns"

    cls = CfgColumn

    def __init__(self, config):
        self.col_trans_tbl = []
        for col in config:
            if not 'name' in config[col]:
                config[col]['name'] = col
            self[col] = CfgColumn(**config[col])
            self.col_trans_tbl.append(('{{%s}}' % col, self[col].name))

    def from_map(self, incols):
        """Initialize the dictionary of columns by converting the input dict

        :param incols: YAML dictionary defining the columns
        """
        renames = False
        for col in incols:
            if col in self:
                ccol = self[col]
            else:
                self[col] = ccol = CfgColumn(name=col)
            for attr, val in list(incols[col].items()):
                setattr(ccol, attr, val)
                if attr == 'name':
                    renames = True
        if renames:
            self.col_trans_tbl = [('{{%s}}' % col, self[col].name)
                                  for col in self]

########NEW FILE########
__FILENAME__ = function
# -*- coding: utf-8 -*-
"""
    pyrseas.augment.function
    ~~~~~~~~~~~~~~~~~~~~~~~~

    This module defines two classes: CfgFunction derived from
    DbAugment and CfgFunctionDict derived from DbAugmentDict.
"""
from pyrseas.augment import DbAugmentDict, DbAugment
from pyrseas.dbobject.function import Function


class CfgFunctionSource(DbAugment):
    "A configuration function source or part thereof"
    pass


class CfgFunctionTemplate(CfgFunctionSource):
    "A configuration function source template"

    pass


class CfgFunctionSourceDict(DbAugmentDict):

    cls = CfgFunctionSource

    def __init__(self, cfg_templates):
        for templ in cfg_templates:
            src = cfg_templates[templ]
            dct = {'source': src}
            self[templ] = CfgFunctionTemplate(name=templ, **dct)

    def from_map(self, intempls):
        """Initialize the dict of templates by converting the input list

        :param intempls: YAML list defining the function templates
        """
        for templ in intempls:
            self[templ] = CfgFunctionTemplate(
                name=templ, source=intempls[templ])


class CfgFunction(DbAugment):
    "A configuration function definition"

    keylist = ['name', 'arguments']

    def apply(self, schema, trans_tbl, augdb):
        """Add a function to a given schema.

        :param schema: name of the schema in which to create the function
        :param trans_tbl: translation table
        :param augdb: augmenter dictionaries
        """
        newfunc = Function(schema=schema, **self.__dict__)
        newfunc.volatility = 'v'
        src = newfunc.source
        if '{{' in src and '}}' in src:
            pref = src.find('{{')
            prefix = src[:pref]
            suf = src.find('}}')
            suffix = src[suf + 2:]
            tmplkey = src[pref + 2:suf]
            if tmplkey not in augdb.funcsrcs:
                if '{{'+tmplkey+'}}' not in [pat for (pat, repl) in trans_tbl]:
                    raise KeyError("Function template '%s' not found" %
                                   tmplkey)
            else:
                newfunc.source = prefix + augdb.funcsrcs[tmplkey].source + \
                    suffix

        for (pat, repl) in trans_tbl:
            if '{{' in newfunc.source:
                newfunc.source = newfunc.source.replace(pat, repl)
            if '{{' in newfunc.name:
                newfunc.name = newfunc.name.replace(pat, repl)
            if '{{' in newfunc.description:
                newfunc.description = newfunc.description.replace(pat, repl)
        return newfunc


class CfgFunctionDict(DbAugmentDict):
    "The collection of configuration functions"

    cls = CfgFunction

    def __init__(self, config):
        for func in config:
            fncdict = config[func]
            paren = func.find('(')
            (fnc, args) = (func[:paren], func[paren + 1:-1])
            fncname = fnc
            dct = fncdict.copy()
            if 'name' in dct:
                fncname = dct['name']
                del dct['name']
            self[fnc] = CfgFunction(name=fncname, arguments=args, **dct)

    def from_map(self, infuncs):
        """Initialize the dictionary of functions by converting the input list

        :param infuncs: YAML list defining the functions
        """
        for func in infuncs:
            paren = func.find('(')
            (fnc, args) = (func[:paren], func[paren + 1:-1])
            if fnc in self:
                cfnc = self[fnc]
            else:
                self[fnc] = cfnc = CfgFunction(name=fnc, arguments=args)
            for attr, val in list(infuncs[func].items()):
                setattr(cfnc, attr, val)

########NEW FILE########
__FILENAME__ = schema
# -*- coding: utf-8 -*-
"""
    pyrseas.augment.schema
    ~~~~~~~~~~~~~~~~~~~~~~

    This defines two classes, AugSchema and AugSchemaDict, derived from
    DbAugment and DbAugmentDict, respectively.
"""
from pyrseas.augment import DbAugmentDict, DbAugment
from pyrseas.augment.table import AugTable


class AugSchema(DbAugment):
    """A database schema definition, i.e., a named collection of tables,
    views, triggers and other schema objects."""

    keylist = ['name']

    def apply(self, augdb):
        """Augment objects in a schema.

        :param augdb: the augmenter dictionaries
        """
        for tbl in self.tables:
            self.tables[tbl].apply(augdb)

    def add_func(self, func):
        """Add a function to the schema if not already present

        :param func: the possibly new function
        """
        sch = self.current
        if not hasattr(sch, 'functions'):
            sch.functions = {}
        if func.name not in sch.functions:
            sch.functions.update({func.name: func})


class AugSchemaDict(DbAugmentDict):
    "The collection of schemas in a database"

    cls = AugSchema

    def from_map(self, augmap, augdb):
        """Initialize the dictionary of schemas by converting the augmenter map

        :param augmap: the input YAML map defining the augmentations
        :param augdb: collection of dictionaries defining the augmentations

        Starts the recursive analysis of the input map and
        construction of the internal collection of dictionaries
        describing the database objects.
        """
        for key in augmap:
            (objtype, spc, sch) = key.partition(' ')
            if spc != ' ' or objtype != 'schema':
                raise KeyError("Unrecognized object type: %s" % key)
            schema = self[sch] = AugSchema(name=sch)
            inschema = augmap[key]
            augtables = {}
            augfuncs = {}
            for key in inschema:
                if key.startswith('table '):
                    augtables.update({key: inschema[key]})
                elif key.startswith('function '):
                    augfuncs.update({key: inschema[key]})
                else:
                    raise KeyError("Expected typed object, found '%s'" % key)
            augdb.tables.from_map(schema, augtables, augdb)

    def link_current(self, schemas):
        """Connect schemas to be augmented to actual database schemas

        :param schemas: schemas in current database
        """
        for sch in self:
            if not sch in schemas:
                raise KeyError("Schema %s not in current database" % sch)
            if not hasattr(self[sch], 'current'):
                self[sch].current = schemas[sch]

    def link_refs(self, dbtables):
        """Connect tables and functions to their respective schemas

        :param dbtables: dictionary of tables

        Fills in the `tables` dictionary for each schema by
        traversing the `dbtables` dictionary.
        """
        for (sch, tbl) in dbtables:
            table = dbtables[(sch, tbl)]
            assert self[sch]
            schema = self[sch]
            if isinstance(table, AugTable):
                if not hasattr(schema, 'tables'):
                    schema.tables = {}
                schema.tables.update({tbl: table})

########NEW FILE########
__FILENAME__ = table
# -*- coding: utf-8 -*-
"""
    pyrseas.augment.table
    ~~~~~~~~~~~~~~~~~~~~~

    This module defines three classes: AugDbClass derived from
    DbAugment, AugTable derived from AugDbClass, and AugClassDict
    derived from DbAugmentDict.
"""
from pyrseas.augment import DbAugmentDict, DbAugment


class AugDbClass(DbAugment):
    """A table, sequence or view"""

    keylist = ['schema', 'name']


class AugTable(AugDbClass):
    """A database table definition"""

    def apply(self, augdb):
        """Augment tables in a schema.

        :param augdb: the augmenter dictionaries
        """
        currtbl = augdb.current.tables[self.current.key()]
        if hasattr(self, 'audit_columns'):
            if self.audit_columns not in augdb.auditcols:
                raise KeyError("Specification %s not in current configuration"
                               % self.audit_columns)
            augdb.auditcols[self.audit_columns].apply(currtbl, augdb)


class AugClassDict(DbAugmentDict):
    "The collection of tables and similar objects in a database"

    cls = AugDbClass

    def from_map(self, schema, inobjs, augdb):
        """Initalize the dictionary of tables by converting the input map

        :param schema: schema owning the tables
        :param inobjs: YAML map defining the schema objects
        :param augdb: collection of dictionaries defining the augmentations
        """
        for k in inobjs:
            (objtype, spc, key) = k.partition(' ')
            if spc != ' ' or objtype not in ['table']:
                raise KeyError("Unrecognized object type: %s" % k)
            if objtype == 'table':
                self[(schema.name, key)] = table = AugTable(
                    schema=schema.name, name=key)
                intable = inobjs[k]
                if not intable:
                    raise ValueError("Table '%s' has no specification" % k)
                for attr in intable:
                    if attr == 'audit_columns':
                        setattr(table, attr, intable[attr])
                    else:
                        raise KeyError("Unrecognized attribute '%s' for %s"
                                       % (attr, k))
            else:
                raise KeyError("Unrecognized object type: %s" % k)

    def link_current(self, tables):
        """Connect tables to be augmented to actual database tables

        :param tables: tables in current schema
        """
        for (sch, tbl) in self:
            if not (sch, tbl) in tables:
                raise KeyError("Table %s.%s not in current database" % (
                    sch, tbl))
            if not hasattr(self[(sch, tbl)], 'current'):
                self[(sch, tbl)].current = tables[(sch, tbl)]

########NEW FILE########
__FILENAME__ = trigger
# -*- coding: utf-8 -*-
"""
    pyrseas.augment.trigger
    ~~~~~~~~~~~~~~~~~~~~~~~

    This module defines two classes: CfgTrigger derived from
    DbAugment and CfgTriggerDict derived from DbAugmentDict.
"""
from pyrseas.augment import DbAugmentDict, DbAugment
from pyrseas.dbobject import split_schema_obj
from pyrseas.dbobject.trigger import Trigger


class CfgTrigger(DbAugment):
    "A configuration trigger definition"

    keylist = ['name']

    def apply(self, table):
        """Create a trigger for the table passed in.

        :param table: table on which the trigger will be created
        """
        newtrg = Trigger(schema=table.schema, table=table.name,
                         **self.__dict__)
        if newtrg.name.startswith('{{table_name}}'):
            newtrg.name = newtrg.name.replace(newtrg.name[:14], table.name)
        newtrg._table = table
        if not hasattr(table, 'triggers'):
            table.triggers = {}
        if hasattr(newtrg, 'procedure'):
            if newtrg.procedure.startswith('{{table_name}}'):
                newtrg.procedure = newtrg.procedure.replace(
                    newtrg.procedure[:14], table.name)
            (sch, fnc) = split_schema_obj(newtrg.procedure)
            if sch != table.schema:
                newtrg.procedure = "%s.%s" % (table.schema, fnc)
        table.triggers.update({newtrg.name: newtrg})


class CfgTriggerDict(DbAugmentDict):
    "The collection of configuration triggers"

    cls = CfgTrigger

    def __init__(self, config):
        """Initialize internal configuration triggers"""
        for trg in config:
            self[trg] = CfgTrigger(**config[trg])

    def from_map(self, intrigs):
        """Initialize the dictionary of triggers by converting the input dict

        :param intrigs: YAML dictionary defining the triggers
        """
        for trg in intrigs:
            if trg in self:
                ctrg = self[trg]
            else:
                self[trg] = ctrg = CfgTrigger(name=trg)
            for attr, val in list(intrigs[trg].items()):
                setattr(ctrg, attr, val)

########NEW FILE########
__FILENAME__ = augmentdb
# -*- coding: utf-8 -*-
"""
    pyrseas.augmentdb
    ~~~~~~~~~~~~~~~~~

    An `AugmentDatabase` is initialized with a DbConnection object.
    It consists of two "dictionary" container objects, each holding
    various dictionary objects.  The `db` Dicts object (inherited from
    its parent class), defines the database schemas, including their
    tables and other objects, by querying the system catalogs.  The
    `adb` AugDicts object defines the augmentation schemas and the
    configuration objects based on the aug_map supplied to the `apply`
    method.
"""
from pyrseas.database import Database
from pyrseas.dbobject.language import Language
from pyrseas.augment.schema import AugSchemaDict
from pyrseas.augment.table import AugClassDict
from pyrseas.augment.column import CfgColumnDict
from pyrseas.augment.function import CfgFunctionDict, CfgFunctionSourceDict
from pyrseas.augment.trigger import CfgTriggerDict
from pyrseas.augment.audit import CfgAuditColumnDict


def cfg_section(config, section):
    "Return the configuration section if present, else an empty dict"
    return config[section] if section in config else {}


class AugmentDatabase(Database):
    """A database that is to be augmented"""

    class AugDicts(object):
        """A holder for dictionaries (maps) describing augmentations"""

        def __init__(self, config):
            """Initialize the various DbAugmentDict-derived dictionaries

            :param config: configuration dictionary
            """
            self.schemas = AugSchemaDict()
            self.tables = AugClassDict()
            self.columns = CfgColumnDict(cfg_section(config, 'columns'))
            self.funcsrcs = CfgFunctionSourceDict(
                cfg_section(config, 'function_templates'))
            self.functions = CfgFunctionDict(cfg_section(config, 'functions'))
            self.triggers = CfgTriggerDict(cfg_section(config, 'triggers'))
            self.auditcols = CfgAuditColumnDict(
                cfg_section(config, 'audit_columns'))

        def _link_refs(self):
            """Link related objects"""
            self.schemas.link_refs(self.tables)

        def _link_current(self, db):
            """Link augment objects to current catalog objects"""
            self.current = db
            self.schemas.link_current(db.schemas)
            self.tables.link_current(db.tables)

        def add_func(self, schema, function):
            """Add a function to a schema if not already present

            :param schema: schema name
            :param function: the possibly new function
            """
            if schema in self.schemas:
                self.schemas[schema].add_func(function)
            elif schema in self.current.schemas:
                sch = self.current.schemas[schema]
                if not hasattr(sch, 'functions'):
                    sch.functions = {}
                if function.name not in sch.functions:
                    sch.functions.update({function.name: function})

        def add_lang(self, lang):
            """Add a language if not already present

            :param lang: the possibly new language
            """
            if lang not in self.current.languages:
                self.current.languages[lang] = Language(name=lang)

    def from_augmap(self, aug_map):
        """Populate the augment objects from the input augment map

        :param aug_map: a YAML map defining the desired augmentations

        The `adb` holder is populated by various DbAugmentDict-derived
        classes by traversing the YAML augmentation map. The objects
        in the dictionary are then linked to related objects, e.g.,
        tables are linked to the schemas they belong.
        """
        self.adb = self.AugDicts(cfg_section(self.config, 'augmenter'))
        aug_schemas = {}
        for key in aug_map:
            if key == 'augmenter':
                self._from_cfgmap(aug_map[key])
            elif key.startswith('schema '):
                aug_schemas.update({key: aug_map[key]})
            else:
                raise KeyError("Expected typed object, found '%s'" % key)
        self.adb.schemas.from_map(aug_schemas, self.adb)
        self.adb._link_refs()
        self.adb._link_current(self.db)

    def _from_cfgmap(self, cfg_map):
        """Populate configuration objects from the input configuration map

        :param cfg_map: a YAML map defining augmentation configuration

        The augmentations dictionary is populated by various
        DbAugmentDict-derived classes by traversing the YAML
        configuration map.
        """
        for key in cfg_map:
            if key == 'columns':
                self.adb.columns.from_map(cfg_map[key])
            elif key in ['function_templates', 'function_segments']:
                self.adb.funcsrcs.from_map(cfg_map[key])
            elif key == 'functions':
                self.adb.functions.from_map(cfg_map[key])
            elif key == 'triggers':
                self.adb.triggers.from_map(cfg_map[key])
            elif key == 'audit_columns':
                self.adb.auditcols.from_map(cfg_map[key])
            else:
                raise KeyError("Expected typed object, found '%s'" % key)

    def apply(self, aug_map):
        """Apply augmentations to an existing database

        :param aug_map: a YAML map defining the desired augmentations

        Merges an existing database definition, as fetched from the
        catalogs, with an input YAML defining augmentations on various
        objects and an optional configuration map or the predefined
        configuration.
        """
        if not self.db:
            self.from_catalog()
        self.from_augmap(aug_map)
        for sch in self.adb.schemas:
            self.adb.schemas[sch].apply(self.adb)
        return self.to_map()

########NEW FILE########
__FILENAME__ = cmdargs
# -*- coding: utf-8 -*-
"""Utility module for command line argument parsing"""

import os
from argparse import ArgumentParser, FileType
import getpass

import yaml

from pyrseas.config import Config

_cfg = None

HELP_TEXT = {
    'host': "database server host or socket directory",
    'port': "database server port number",
    'username': "database user name"
}


def _help_dflt(arg, config):
    kwdargs = {'help': HELP_TEXT[arg]}
    if arg in config:
        kwdargs['help'] += " (default %(default)s)"
        kwdargs['default'] = config[arg]
    return kwdargs


def _repo_path(cfg, key=None):
    """Return path to root directory of repository or subdirectory

    :return: path
    """
    repo = cfg['repository']
    if 'path' in repo:
        path = repo['path']
    else:
        path = os.getcwd()
    subdir = '' if key is None else repo[key]
    return os.path.normpath(os.path.join(path, subdir))


def cmd_parser(description, version):
    """Create command line argument parser with common PostgreSQL options

    :param description: text to display before the argument help
    :param version: version of the caller
    :return: the created parser
    """
    global _cfg

    parent = ArgumentParser(add_help=False)
    parent.add_argument('dbname', help='database name')
    group = parent.add_argument_group('Connection options')
    if _cfg is None:
        _cfg = Config()
    dbcfg = _cfg['database'] if 'database' in _cfg else {}
    group.add_argument('-H', '--host', **_help_dflt('host', dbcfg))
    group.add_argument('-p', '--port', type=int, **_help_dflt('port', dbcfg))
    group.add_argument('-U', '--username', **_help_dflt('username', dbcfg))
    group.add_argument('-W', '--password', action="store_true",
                       help="force password prompt")
    parent.add_argument('-c', '--config', type=FileType('r'),
                        help="configuration file path")
    parent.add_argument('-r', '--repository', default=_repo_path(_cfg),
                        help="root of repository (default %(default)s)")
    parent.add_argument('-o', '--output', type=FileType('w'),
                        help="output file name (default stdout)")
    parser = ArgumentParser(parents=[parent], description=description)
    parser.add_argument('--version', action='version',
                        version='%(prog)s ' + '%s' % version)
    return parser


def parse_args(parser):
    """Parse command line arguments and return configuration object

    :param parser: ArgumentParser created by cmd_parser
    :return: a Configuration object
    """
    arg_opts = parser.parse_args()
    args = vars(arg_opts)
    for key in ['database', 'files']:
        if key not in _cfg:
            _cfg[key] = {}

    def tfr(prim, key, val):
        _cfg[prim][key] = val
        del args[key]

    for key in ['dbname', 'host', 'port', 'username']:
        tfr('database', key, args[key])
    tfr('database', 'password',
        (getpass.getpass() if args['password'] else None))

    for key in ['output', 'config']:
        tfr('files', key, args[key])

    if 'config' in _cfg['files'] and _cfg['files']['config']:
        _cfg.merge(yaml.safe_load(_cfg['files']['config']))
    if 'repository' in args:
        if args['repository'] != os.getcwd():
            _cfg['repository']['path'] = args['repository']
        del args['repository']

    _cfg['files']['metadata_path'] = _repo_path(_cfg, 'metadata')
    _cfg['files']['data_path'] = _repo_path(_cfg, 'data')

    _cfg['options'] = arg_opts
    return _cfg

########NEW FILE########
__FILENAME__ = config
# -*- coding: utf-8 -*-
"""Utility module for configuration file parsing"""

import os
import sys

import yaml


CFG_FILE = os.environ.get("PYRSEAS_CONFIG_FILE", "config.yaml")


def _home_dir():
    if sys.platform == 'win32':
        dir = os.getenv('APPDATA', '')
    else:
        dir = os.path.join(os.environ['HOME'], '.config')
    return os.path.abspath(dir)


def _load_cfg(cfgdir):
    cfgpath = ''
    cfg = {}
    if cfgdir is not None:
        if os.path.isdir(cfgdir):
            cfgpath = os.path.join(cfgdir, CFG_FILE)
        elif os.path.isfile(cfgdir):
            cfgpath = cfgdir
        if os.path.exists(cfgpath):
            with open(cfgpath) as f:
                cfg = yaml.safe_load(f)
    return cfg


class Config(dict):
    "A configuration dictionary"

    def __init__(self, sys_only=False):
        self.update(_load_cfg(
            os.environ.get("PYRSEAS_SYS_CONFIG", os.path.abspath(os.path.join(
                           os.path.dirname(__file__))))))
        if sys_only:
            return
        self.merge(_load_cfg(os.environ.get("PYRSEAS_USER_CONFIG",
                             os.path.join(_home_dir(), 'pyrseas'))))
        if 'repository' in self and 'path' in self['repository']:
            cfgpath = self['repository']['path']
        else:
            cfgpath = os.getcwd()
        self.merge(_load_cfg(cfgpath))

    def merge(self, cfg):
        """Merge extra configuration

        :param cfg: extra configuration (dict)
        """
        for key, val in list(cfg.items()):
            if key in self:
                self[key].update(val)
            else:
                self[key] = val

########NEW FILE########
__FILENAME__ = database
# -*- coding: utf-8 -*-
"""
    pyrseas.database
    ~~~~~~~~~~~~~~~~

    A `Database` is initialized with a DbConnection object.  It
    consists of one or two `Dicts` objects, each holding various
    dictionary objects.  The `db` Dicts object defines the database
    schemas, including their tables and other objects, by querying the
    system catalogs.  The `ndb` Dicts object defines the schemas based
    on the `input_map` supplied to the `from_map` method.
"""
import os
import sys

import yaml

from pyrseas.yamlutil import yamldump
from pyrseas.lib.dbconn import DbConnection
from pyrseas.dbobject import fetch_reserved_words
from pyrseas.dbobject.language import LanguageDict
from pyrseas.dbobject.cast import CastDict
from pyrseas.dbobject.schema import SchemaDict
from pyrseas.dbobject.dbtype import TypeDict
from pyrseas.dbobject.table import ClassDict
from pyrseas.dbobject.column import ColumnDict
from pyrseas.dbobject.constraint import ConstraintDict
from pyrseas.dbobject.index import IndexDict
from pyrseas.dbobject.function import ProcDict
from pyrseas.dbobject.operator import OperatorDict
from pyrseas.dbobject.operclass import OperatorClassDict
from pyrseas.dbobject.operfamily import OperatorFamilyDict
from pyrseas.dbobject.rule import RuleDict
from pyrseas.dbobject.trigger import TriggerDict
from pyrseas.dbobject.conversion import ConversionDict
from pyrseas.dbobject.textsearch import TSConfigurationDict, TSDictionaryDict
from pyrseas.dbobject.textsearch import TSParserDict, TSTemplateDict
from pyrseas.dbobject.foreign import ForeignDataWrapperDict
from pyrseas.dbobject.foreign import ForeignServerDict, UserMappingDict
from pyrseas.dbobject.foreign import ForeignTableDict
from pyrseas.dbobject.extension import ExtensionDict
from pyrseas.dbobject.collation import CollationDict
from pyrseas.dbobject.eventtrig import EventTriggerDict


def flatten(lst):
    "Flatten a list possibly containing lists to a single list"
    for elem in lst:
        if isinstance(elem, list) and not isinstance(elem, str):
            for subelem in flatten(elem):
                yield subelem
        else:
            yield elem


class CatDbConnection(DbConnection):
    """A database connection, specialized for querying catalogs"""

    def connect(self):
        """Connect to the database"""
        super(CatDbConnection, self).connect()
        try:
            self.execute("set search_path to public, pg_catalog")
        except:
            self.rollback()
            self.execute("set search_path to pg_catalog")
        self.commit()
        self._version = self.conn.server_version

    @property
    def version(self):
        "The server's version number"
        return self._version


class Database(object):
    """A database definition, from its catalogs and/or a YAML spec."""

    class Dicts(object):
        """A holder for dictionaries (maps) describing a database"""

        def __init__(self, dbconn=None):
            """Initialize the various DbObjectDict-derived dictionaries

            :param dbconn: a DbConnection object
            """
            self.schemas = SchemaDict(dbconn)
            self.extensions = ExtensionDict(dbconn)
            self.languages = LanguageDict(dbconn)
            self.casts = CastDict(dbconn)
            self.types = TypeDict(dbconn)
            self.tables = ClassDict(dbconn)
            self.columns = ColumnDict(dbconn)
            self.constraints = ConstraintDict(dbconn)
            self.indexes = IndexDict(dbconn)
            self.functions = ProcDict(dbconn)
            self.operators = OperatorDict(dbconn)
            self.operclasses = OperatorClassDict(dbconn)
            self.operfams = OperatorFamilyDict(dbconn)
            self.rules = RuleDict(dbconn)
            self.triggers = TriggerDict(dbconn)
            self.conversions = ConversionDict(dbconn)
            self.tstempls = TSTemplateDict(dbconn)
            self.tsdicts = TSDictionaryDict(dbconn)
            self.tsparsers = TSParserDict(dbconn)
            self.tsconfigs = TSConfigurationDict(dbconn)
            self.fdwrappers = ForeignDataWrapperDict(dbconn)
            self.servers = ForeignServerDict(dbconn)
            self.usermaps = UserMappingDict(dbconn)
            self.ftables = ForeignTableDict(dbconn)
            self.collations = CollationDict(dbconn)
            self.eventtrigs = EventTriggerDict(dbconn)

    def __init__(self, config):
        """Initialize the database

        :param config: configuration dictionary
        """
        db = config['database']
        self.dbconn = CatDbConnection(db['dbname'], db['username'],
                                      db['password'], db['host'], db['port'])
        self.db = None
        self.config = config

    def _link_refs(self, db):
        """Link related objects"""
        db.languages.link_refs(db.functions)
        copycfg = {}
        if 'datacopy' in self.config:
            copycfg = self.config['datacopy']
        db.schemas.link_refs(db, copycfg)
        db.tables.link_refs(db.columns, db.constraints, db.indexes, db.rules,
                            db.triggers)
        db.functions.link_refs(db.eventtrigs)
        db.fdwrappers.link_refs(db.servers)
        db.servers.link_refs(db.usermaps)
        db.ftables.link_refs(db.columns)
        db.types.link_refs(db.columns, db.constraints, db.functions)

    def _trim_objects(self, schemas):
        """Remove unwanted schema objects

        :param schemas: list of schemas to keep
        """
        for objtype in ['types', 'tables', 'constraints', 'indexes',
                        'functions', 'operators', 'operclasses', 'operfams',
                        'rules', 'triggers', 'conversions', 'tstempls',
                        'tsdicts', 'tsparsers', 'tsconfigs', 'extensions',
                        'collations', 'eventtrigs']:
            objdict = getattr(self.db, objtype)
            for obj in list(objdict.keys()):
                # obj[0] is the schema name in all these dicts
                if obj[0] not in schemas:
                    del objdict[obj]
        for sch in list(self.db.schemas.keys()):
            if sch not in schemas:
                del self.db.schemas[sch]
        # exclude database-wide objects
        self.db.languages = LanguageDict()
        self.db.casts = CastDict()

    def from_catalog(self):
        """Populate the database objects by querying the catalogs

        The `db` holder is populated by various DbObjectDict-derived
        classes by querying the catalogs. The objects in the
        dictionary are then linked to related objects, e.g., columns
        are linked to the tables they belong.
        """
        self.db = self.Dicts(self.dbconn)
        if self.dbconn.conn:
            self.dbconn.conn.close()
        self._link_refs(self.db)

    def from_map(self, input_map, langs=None):
        """Populate the new database objects from the input map

        :param input_map: a YAML map defining the new database
        :param langs: list of language templates

        The `ndb` holder is populated by various DbObjectDict-derived
        classes by traversing the YAML input map. The objects in the
        dictionary are then linked to related objects, e.g., columns
        are linked to the tables they belong.
        """
        self.ndb = self.Dicts()
        input_schemas = {}
        input_extens = {}
        input_langs = {}
        input_casts = {}
        input_fdws = {}
        input_ums = {}
        input_evttrigs = {}
        for key in input_map:
            if key.startswith('schema '):
                input_schemas.update({key: input_map[key]})
            elif key.startswith('extension '):
                input_extens.update({key: input_map[key]})
            elif key.startswith('language '):
                input_langs.update({key: input_map[key]})
            elif key.startswith('cast '):
                input_casts.update({key: input_map[key]})
            elif key.startswith('foreign data wrapper '):
                input_fdws.update({key: input_map[key]})
            elif key.startswith('user mapping for '):
                input_ums.update({key: input_map[key]})
            elif key.startswith('event trigger '):
                input_evttrigs.update({key: input_map[key]})
            else:
                raise KeyError("Expected typed object, found '%s'" % key)
        self.ndb.extensions.from_map(input_extens, langs, self.ndb)
        self.ndb.languages.from_map(input_langs)
        self.ndb.schemas.from_map(input_schemas, self.ndb)
        self.ndb.casts.from_map(input_casts, self.ndb)
        self.ndb.fdwrappers.from_map(input_fdws, self.ndb)
        self.ndb.eventtrigs.from_map(input_evttrigs, self.ndb)
        self._link_refs(self.ndb)

    def map_from_dir(self):
        """Read the database maps starting from metadata directory

        :return: dictionary
        """
        metadata_dir = self.config['files']['metadata_path']
        if not os.path.isdir(metadata_dir):
            sys.exit("Metadata directory '%s' doesn't exist" % metadata_dir)

        def load(subdir, obj):
            with open(os.path.join(subdir, obj), 'r') as f:
                objmap = yaml.safe_load(f)
            return objmap if isinstance(objmap, dict) else {}

        inmap = {}
        for entry in os.listdir(metadata_dir):
            if entry.endswith('.yaml'):
                if entry.startswith('database.'):
                    continue
                if not entry.startswith('schema.'):
                    inmap.update(load(metadata_dir, entry))
            else:
                # skip over unknown files/dirs
                if not entry.startswith('schema.'):
                    continue
                # read schema.xxx.yaml first
                schmap = load(metadata_dir, entry + '.yaml')
                assert(len(schmap) == 1)
                key = list(schmap.keys())[0]
                inmap.update({key: {}})
                subdir = os.path.join(metadata_dir, entry)
                if os.path.isdir(subdir):
                    for schobj in os.listdir(subdir):
                        schmap[key].update(load(subdir, schobj))
                inmap.update(schmap)

        return inmap

    def to_map(self):
        """Convert the db maps to a single hierarchy suitable for YAML

        :return: a YAML-suitable dictionary (without Python objects)
        """
        if not self.db:
            self.from_catalog()

        opts = self.config['options']

        def mkdir_parents(dir):
            head, tail = os.path.split(dir)
            if head and not os.path.isdir(head):
                mkdir_parents(head)
            if tail:
                os.mkdir(dir)

        if opts.multiple_files:
            opts.metadata_dir = self.config['files']['metadata_path']
            if not os.path.exists(opts.metadata_dir):
                mkdir_parents(opts.metadata_dir)
            dbfilepath = os.path.join(opts.metadata_dir, 'database.%s.yaml' %
                                      self.dbconn.dbname)
            if os.path.exists(dbfilepath):
                with open(dbfilepath, 'r') as f:
                    objmap = yaml.safe_load(f)
                for obj, val in objmap.items():
                    if isinstance(val, dict):
                        dirpath = ''
                        for schobj, fpath in val.items():
                            filepath = os.path.join(opts.metadata_dir, fpath)
                            if os.path.exists(filepath):
                                os.remove(filepath)
                                if schobj == 'schema':
                                    (dirpath, ext) = os.path.splitext(filepath)
                        if os.path.exists(dirpath):
                            os.rmdir(dirpath)
                    else:
                        filepath = os.path.join(opts.metadata_dir, val)
                        if (os.path.exists(filepath)):
                            os.remove(filepath)

        dbmap = self.db.extensions.to_map(opts)
        dbmap.update(self.db.languages.to_map(opts))
        dbmap.update(self.db.casts.to_map(opts))
        dbmap.update(self.db.fdwrappers.to_map(opts))
        dbmap.update(self.db.eventtrigs.to_map(opts))
        if 'datacopy' in self.config:
            opts.data_dir = self.config['files']['data_path']
            if not os.path.exists(opts.data_dir):
                mkdir_parents(opts.data_dir)
        dbmap.update(self.db.schemas.to_map(opts))

        if opts.multiple_files:
            with open(dbfilepath, 'w') as f:
                f.write(yamldump(dbmap))

        return dbmap

    def diff_map(self, input_map):
        """Generate SQL to transform an existing database

        :param input_map: a YAML map defining the new database
        :return: list of SQL statements

        Compares the existing database definition, as fetched from the
        catalogs, to the input YAML map and generates SQL statements
        to transform the database into the one represented by the
        input.
        """
        if not self.db:
            self.from_catalog()
        opts = self.config['options']
        if opts.schemas:
            schlist = ['schema ' + sch for sch in opts.schemas]
            for sch in input_map:
                if sch not in schlist and sch.startswith('schema '):
                    del input_map[sch]
            self._trim_objects(opts.schemas)

        if opts.quote_reserved:
            fetch_reserved_words(self.dbconn)

        langs = None
        if self.dbconn.version >= 90100:
            langs = [lang[0] for lang in self.dbconn.fetchall(
                "SELECT tmplname FROM pg_pltemplate")]
        self.from_map(input_map, langs)
        if opts.revert:
            (self.db, self.ndb) = (self.ndb, self.db)
        stmts = self.db.schemas.diff_map(self.ndb.schemas)
        stmts.append(self.db.extensions.diff_map(self.ndb.extensions))
        stmts.append(self.db.languages.diff_map(self.ndb.languages))
        stmts.append(self.db.types.diff_map(self.ndb.types))
        stmts.append(self.db.functions.diff_map(self.ndb.functions))
        stmts.append(self.db.operators.diff_map(self.ndb.operators))
        stmts.append(self.db.operfams.diff_map(self.ndb.operfams))
        stmts.append(self.db.operclasses.diff_map(self.ndb.operclasses))
        stmts.append(self.db.eventtrigs.diff_map(self.ndb.eventtrigs))
        stmts.append(self.db.tables.diff_map(self.ndb.tables))
        stmts.append(self.db.constraints.diff_map(self.ndb.constraints))
        stmts.append(self.db.indexes.diff_map(self.ndb.indexes))
        stmts.append(self.db.columns.diff_map(self.ndb.columns))
        stmts.append(self.db.triggers.diff_map(self.ndb.triggers))
        stmts.append(self.db.rules.diff_map(self.ndb.rules))
        stmts.append(self.db.conversions.diff_map(self.ndb.conversions))
        stmts.append(self.db.tsdicts.diff_map(self.ndb.tsdicts))
        stmts.append(self.db.tstempls.diff_map(self.ndb.tstempls))
        stmts.append(self.db.tsparsers.diff_map(self.ndb.tsparsers))
        stmts.append(self.db.tsconfigs.diff_map(self.ndb.tsconfigs))
        stmts.append(self.db.casts.diff_map(self.ndb.casts))
        stmts.append(self.db.collations.diff_map(self.ndb.collations))
        stmts.append(self.db.fdwrappers.diff_map(self.ndb.fdwrappers))
        stmts.append(self.db.servers.diff_map(self.ndb.servers))
        stmts.append(self.db.usermaps.diff_map(self.ndb.usermaps))
        stmts.append(self.db.ftables.diff_map(self.ndb.ftables))
        stmts.append(self.db.operators._drop())
        stmts.append(self.db.operclasses._drop())
        stmts.append(self.db.operfams._drop())
        stmts.append(self.db.functions._drop())
        stmts.append(self.db.types._drop())
        stmts.append(self.db.extensions._drop())
        stmts.append(self.db.schemas._drop())
        stmts.append(self.db.servers._drop())
        stmts.append(self.db.fdwrappers._drop())
        stmts.append(self.db.languages._drop())
        if 'datacopy' in self.config:
            opts.data_dir = self.config['files']['data_path']
            stmts.append(self.ndb.schemas.data_import(opts))
        return [s for s in flatten(stmts)]

########NEW FILE########
__FILENAME__ = dbaugment
#!/usr/bin/python
# -*- coding: utf-8 -*-
"""dbaugment - Augment a PostgreSQL database"""

from __future__ import print_function
import sys
from argparse import FileType

import yaml

from pyrseas import __version__
from pyrseas.yamlutil import yamldump
from pyrseas.augmentdb import AugmentDatabase
from pyrseas.cmdargs import cmd_parser, parse_args


def main():
    """Augment database specifications"""
    parser = cmd_parser("Generate a modified schema for a PostgreSQL "
                        "database, in YAML format, augmented with specified "
                        "attributes and procedures", __version__)
    # TODO: processing of multiple files, owner and privileges
    parser.add_argument('-m', '--multiple-files', action='store_true',
                        help='multiple files (metadata directory)')
    parser.add_argument('-O', '--no-owner', action='store_true',
                        help='exclude object ownership information')
    parser.add_argument('-x', '--no-privileges', action='store_true',
                        dest='no_privs',
                        help='exclude privilege (GRANT/REVOKE) information')
    parser.add_argument('spec', nargs='?', type=FileType('r'),
                        default=sys.stdin, help='YAML augmenter specification')
    cfg = parse_args(parser)
    output = cfg['files']['output']
    options = cfg['options']
    augdb = AugmentDatabase(cfg)
    augmap = yaml.safe_load(options.spec)
    try:
        outmap = augdb.apply(augmap)
    except BaseException as exc:
        if type(exc) != KeyError:
            raise
        sys.exit("ERROR: %s" % str(exc))
    print(yamldump(outmap), file=output or sys.stdout)
    if output:
        output.close()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = cast
# -*- coding: utf-8 -*-
"""
    pyrseas.dbobject.cast
    ~~~~~~~~~~~~~~~~~~~~~

    This module defines two classes: Cast derived from DbObject and
    CastDict derived from DbObjectDict.
"""
from pyrseas.dbobject import DbObject, DbObjectDict, commentable


CONTEXTS = {'a': 'assignment', 'e': 'explicit', 'i': 'implicit'}
METHODS = {'f': 'function', 'i': 'inout', 'b': 'binary coercible'}


class Cast(DbObject):
    """A cast"""

    keylist = ['source', 'target']
    objtype = "CAST"
    single_extern_file = True

    def extern_key(self):
        """Return the key to be used in external maps for this cast

        :return: string
        """
        return '%s (%s as %s)' % (self.objtype.lower(), self.source,
                                  self.target)

    def identifier(self):
        """Return a full identifier for a cast object

        :return: string
        """
        return "(%s AS %s)" % (self.source, self.target)

    def to_map(self, no_owner=False, no_privs=False):
        """Convert a cast to a YAML-suitable format

        :return: dictionary
        """
        dct = self._base_map()
        dct['context'] = CONTEXTS[self.context]
        dct['method'] = METHODS[self.method]
        return dct

    @commentable
    def create(self):
        """Return SQL statements to CREATE the cast

        :return: SQL statements
        """
        with_clause = "\n    WITH"
        if hasattr(self, 'function'):
            with_clause += " FUNCTION %s" % self.function
        elif self.method == 'i':
            with_clause += " INOUT"
        else:
            with_clause += "OUT FUNCTION"
        as_clause = ''
        if self.context == 'a':
            as_clause = "\n    AS ASSIGNMENT"
        elif self.context == 'i':
            as_clause = "\n    AS IMPLICIT"
        return ["CREATE CAST (%s AS %s)%s%s" % (
                self.source, self.target, with_clause, as_clause)]


class CastDict(DbObjectDict):
    "The collection of casts in a database"

    cls = Cast
    query = \
        """SELECT castsource::regtype AS source,
                  casttarget::regtype AS target,
                  CASE WHEN castmethod = 'f' THEN castfunc::regprocedure
                       ELSE NULL::regprocedure END AS function,
                  castcontext AS context, castmethod AS method,
                  obj_description(c.oid, 'pg_cast') AS description
           FROM pg_cast c
                JOIN pg_type s ON (castsource = s.oid)
                     JOIN pg_namespace sn ON (s.typnamespace = sn.oid)
                JOIN pg_type t ON (casttarget = t.oid)
                     JOIN pg_namespace tn ON (t.typnamespace = tn.oid)
                LEFT JOIN pg_proc p ON (castfunc = p.oid)
                     LEFT JOIN pg_namespace pn ON (p.pronamespace = pn.oid)
           WHERE substring(sn.nspname for 3) != 'pg_'
              OR substring(tn.nspname for 3) != 'pg_'
              OR (castfunc != 0 AND substring(pn.nspname for 3) != 'pg_')
           ORDER BY castsource, casttarget"""

    def from_map(self, incasts, newdb):
        """Initalize the dictionary of casts by converting the input map

        :param incasts: YAML map defining the casts
        :param newdb: collection of dictionaries defining the database
        """
        for key in incasts:
            if not key.startswith('cast (') or ' AS ' not in key.upper() \
                    or key[-1:] != ')':
                raise KeyError("Unrecognized object type: %s" % key)
            asloc = key.upper().find(' AS ')
            src = key[6:asloc]
            trg = key[asloc + 4:-1]
            incast = incasts[key]
            self[(src, trg)] = cast = Cast(source=src, target=trg)
            if not incast:
                raise ValueError("Cast '%s' has no specification" % key[5:])
            for attr, val in list(incast.items()):
                setattr(cast, attr, val)
            if not hasattr(cast, 'context'):
                raise ValueError("Cast '%s' missing context" % key[5:])
            if not hasattr(cast, 'context'):
                raise ValueError("Cast '%s' missing method" % key[5:])
            cast.context = cast.context[:1].lower()
            cast.method = cast.method[:1].lower()
            if 'description' in incast:
                cast.description = incast['description']

    def diff_map(self, incasts):
        """Generate SQL to transform existing casts

        :param incasts: a YAML map defining the new casts
        :return: list of SQL statements

        Compares the existing cast definitions, as fetched from the
        catalogs, to the input map and generates SQL statements to
        transform the casts accordingly.
        """
        stmts = []
        # check input casts
        for (src, trg) in incasts:
            incast = incasts[(src, trg)]
            # does it exist in the database?
            if (src, trg) not in self:
                # create new cast
                stmts.append(incast.create())
            else:
                # check cast objects
                stmts.append(self[(src, trg)].diff_map(incast))

        # check existing casts
        for (src, trg) in self:
            cast = self[(src, trg)]
            # if missing, mark it for dropping
            if (src, trg) not in incasts:
                stmts.append(cast.drop())

        return stmts

########NEW FILE########
__FILENAME__ = collation
# -*- coding: utf-8 -*-
"""
    pyrseas.dbobject.collation
    ~~~~~~~~~~~~~~~~~~~~~~~~~~

    This defines two classes, Collation and CollationDict, derived from
    DbSchemaObject and DbObjectDict, respectively.
"""
from pyrseas.dbobject import DbObjectDict, DbSchemaObject
from pyrseas.dbobject import commentable, ownable


class Collation(DbSchemaObject):
    """A collation definition"""

    keylist = ['schema', 'name']
    objtype = "COLLATION"
    single_extern_file = True

    @commentable
    @ownable
    def create(self):
        """Return SQL statements to CREATE the collation

        :return: SQL statements
        """
        return ["CREATE COLLATION %s (\n    LC_COLLATE = '%s',"
                "\n    LC_CTYPE = '%s')" % (
                self.qualname(), self.lc_collate, self.lc_ctype)]


class CollationDict(DbObjectDict):
    "The collection of collations in a database."

    cls = Collation
    query = \
        """SELECT nspname AS schema, collname AS name, rolname AS owner,
                  collcollate AS lc_collate, collctype AS lc_ctype,
                  obj_description(c.oid, 'pg_collation') AS description
           FROM pg_collation c
                JOIN pg_roles r ON (r.oid = collowner)
                JOIN pg_namespace n ON (collnamespace = n.oid)
           WHERE (nspname != 'pg_catalog' AND nspname != 'information_schema')
           ORDER BY nspname, collname"""

    def from_map(self, schema, inmap):
        """Initialize the dictionary of collations by examining the input map

        :param schema: the schema owing the collations
        :param inmap: the input YAML map defining the collations
        """
        for key in inmap:
            if not key.startswith('collation '):
                raise KeyError("Unrecognized object type: %s" % key)
            cll = key[10:]
            incoll = inmap[key]
            coll = Collation(schema=schema.name, name=cll, **incoll)
            if incoll:
                if 'oldname' in incoll:
                    coll.oldname = incoll['oldname']
                    del incoll['oldname']
                if 'description' in incoll:
                    coll.description = incoll['description']
            self[(schema.name, cll)] = coll

    def _from_catalog(self):
        """Initialize the dictionary of collations by querying the catalogs"""
        if self.dbconn.version < 90100:
            return
        for coll in self.fetch():
            self[coll.key()] = coll

    def diff_map(self, incolls):
        """Generate SQL to transform existing collations

        :param incolls: a YAML map defining the new collations
        :return: list of SQL statements

        Compares the existing collation definitions, as fetched from
        the catalogs, to the input map and generates SQL statements to
        create, drop or change the collations accordingly.
        """
        stmts = []
        # check input collations
        for cll in incolls:
            incoll = incolls[cll]
            # does it exist in the database?
            if cll in self:
                stmts.append(self[cll].diff_map(incoll))
            else:
                # check for possible RENAME
                if hasattr(incoll, 'oldname'):
                    oldname = incoll.oldname
                    try:
                        stmts.append(self[oldname].rename(incoll.name))
                        del self[oldname]
                    except KeyError as exc:
                        exc.args = ("Previous name '%s' for collation '%s' "
                                    "not found" % (oldname, incoll.name), )
                        raise
                else:
                    # create new collation
                    stmts.append(incoll.create())
        # check database collations
        for (sch, cll) in self:
            # if missing, drop it
            if (sch, cll) not in incolls:
                stmts.append(self[(sch, cll)].drop())

        return stmts

########NEW FILE########
__FILENAME__ = column
# -*- coding: utf-8 -*-
"""
    pyrseas.dbobject.column
    ~~~~~~~~~~~~~~~~~~~~~~~

    This module defines two classes: Column derived from
    DbSchemaObject and ColumnDict derived from DbObjectDict.
"""
from pyrseas.dbobject import DbObjectDict, DbSchemaObject, quote_id
from pyrseas.dbobject.privileges import privileges_from_map, add_grant
from pyrseas.dbobject.privileges import diff_privs


class Column(DbSchemaObject):
    "A table column definition"

    keylist = ['schema', 'table']
    allprivs = 'arwx'

    def to_map(self, no_privs):
        """Convert a column to a YAML-suitable format

        :param no_privs: exclude privilege information
        :return: dictionary
        """
        if hasattr(self, 'dropped'):
            return None
        dct = self._base_map(False, no_privs)
        del dct['number'], dct['name']
        if '_table' in dct:
            del dct['_table']
        if '_type' in dct:
            del dct['_type']
        if 'collation' in dct and dct['collation'] == 'default':
            del dct['collation']
        if hasattr(self, 'inherited'):
            dct['inherited'] = (self.inherited != 0)
        if hasattr(self, 'statistics') and self.statistics == -1:
            del dct['statistics']
        return {self.name: dct}

    def add(self):
        """Return a string to specify the column in a CREATE or ALTER TABLE

        :return: partial SQL statement
        """
        stmt = "%s %s" % (quote_id(self.name), self.type)
        if hasattr(self, 'not_null'):
            stmt += ' NOT NULL'
        if hasattr(self, 'default'):
            if not self.default.startswith('nextval'):
                stmt += ' DEFAULT ' + self.default
        if hasattr(self, 'collation') and self.collation != 'default':
            stmt += ' COLLATE "%s"' % self.collation
        return (stmt, '' if not hasattr(self, 'description')
                else self.comment())

    def add_privs(self):
        """Generate SQL statements to grant privileges on new column

        :return: list of SQL statements
        """
        stmts = []
        if hasattr(self, 'privileges'):
            for priv in self.privileges:
                stmts.append(add_grant(self._table, priv, self.name))
        return stmts

    def diff_privileges(self, incol):
        """Generate SQL statements to grant or revoke privileges

        :param incol: a YAML map defining the input column
        :return: list of SQL statements
        """
        stmts = []
        currprivs = self.privileges if hasattr(self, 'privileges') else {}
        newprivs = incol.privileges if hasattr(incol, 'privileges') else {}
        stmts.append(diff_privs(self._table, currprivs, incol._table, newprivs,
                                self.name))
        return stmts

    def comment(self):
        """Return a SQL COMMENT statement for the column

        :return: SQL statement
        """
        return "COMMENT ON COLUMN %s.%s IS %s" % (
            self._table.qualname(), self.name, self._comment_text())

    def drop(self):
        """Return string to drop the column via ALTER TABLE

        :return: SQL statement
        """
        if hasattr(self, 'dropped'):
            return ""
        if hasattr(self, '_table'):
            (comptype, objtype) = (self._table.objtype, 'COLUMN')
            compname = self._table.qualname()
        elif hasattr(self, '_type'):
            (comptype, objtype) = ('TYPE', 'ATTRIBUTE')
            compname = self._type.qualname()
        else:
            raise TypeError("Cannot determine type of %s", self.name)
        return "ALTER %s %s DROP %s %s" % (comptype, compname, objtype,
                                           self.name)

    def rename(self, newname):
        """Return SQL statement to RENAME the column

        :param newname: the new name of the object
        :return: SQL statement
        """
        if hasattr(self, '_table'):
            (comptype, objtype) = (self._table.objtype, 'COLUMN')
            compname = self._table.qualname()
        elif hasattr(self, '_type'):
            (comptype, objtype) = ('TYPE', 'ATTRIBUTE')
            compname = self._type.qualname()
        else:
            raise TypeError("Cannot determine type of %s", self.name)
        stmt = "ALTER %s %s RENAME %s %s TO %s" % (
            comptype, compname, objtype, self.name, newname)
        self.name = newname
        return stmt

    def set_sequence_default(self):
        """Return SQL statements to set a nextval() DEFAULT

        :return: list of SQL statements
        """
        stmts = []
        stmts.append("ALTER TABLE %s ALTER COLUMN %s SET DEFAULT %s" % (
            self.qualname(self.table), quote_id(self.name), self.default))
        return stmts

    def diff_map(self, incol):
        """Generate SQL to transform an existing column

        :param insequence: a YAML map defining the new column
        :return: list of partial SQL statements

        Compares the column to an input column and generates partial
        SQL statements to transform it into the one represented by the
        input.
        """
        stmts = []
        base = "ALTER COLUMN %s " % quote_id(self.name)
        # check NOT NULL
        if not hasattr(self, 'not_null') and hasattr(incol, 'not_null'):
            stmts.append(base + "SET NOT NULL")
        if hasattr(self, 'not_null') and not hasattr(incol, 'not_null'):
            stmts.append(base + "DROP NOT NULL")
        # check data types
        if not hasattr(self, 'type'):
            raise ValueError("Column '%s' missing datatype" % self.name)
        if not hasattr(incol, 'type'):
            raise ValueError("Input column '%s' missing datatype" % incol.name)
        if self.type != incol.type:
            # validate type conversion?
            stmts.append(base + "TYPE %s" % incol.type)
        # check DEFAULTs
        if not hasattr(self, 'default') and hasattr(incol, 'default'):
            stmts.append(base + "SET DEFAULT %s" % incol.default)
        if hasattr(self, 'default') and not hasattr(incol, 'default'):
            stmts.append(base + "DROP DEFAULT")
        # check STATISTICS
        if hasattr(self, 'statistics'):
            if self.statistics == -1 and (
                hasattr(incol, 'statistics') and incol.statistics != -1):
                stmts.append(base + "SET STATISTICS %d" % incol.statistics)
            if self.statistics != -1 and (
                not hasattr(incol, 'statistics') or incol.statistics == -1):
                stmts.append(base + "SET STATISTICS -1")

        return (", ".join(stmts), self.diff_description(incol))


QUERY_PRE91 = \
    """SELECT nspname AS schema, relname AS table, attname AS name,
              attnum AS number, format_type(atttypid, atttypmod) AS type,
              attnotnull AS not_null, attinhcount AS inherited,
              pg_get_expr(adbin, adrelid) AS default,
              attstattarget AS statistics, attisdropped AS dropped,
              array_to_string(attacl, ',') AS privileges,
              col_description(c.oid, attnum) AS description
       FROM pg_attribute JOIN pg_class c ON (attrelid = c.oid)
            JOIN pg_namespace ON (relnamespace = pg_namespace.oid)
            LEFT JOIN pg_attrdef ON (attrelid = pg_attrdef.adrelid
                 AND attnum = pg_attrdef.adnum)
       WHERE relkind in ('c', 'r', 'f')
             AND (nspname != 'pg_catalog'
                  AND nspname != 'information_schema')
             AND attnum > 0
       ORDER BY nspname, relname, attnum"""


class ColumnDict(DbObjectDict):
    "The collection of columns in tables in a database"

    cls = Column
    query = \
        """SELECT nspname AS schema, relname AS table, attname AS name,
                  attnum AS number, format_type(atttypid, atttypmod) AS type,
                  attnotnull AS not_null, attinhcount AS inherited,
                  pg_get_expr(adbin, adrelid) AS default,
                  attstattarget AS statistics,
                  collname AS collation, attisdropped AS dropped,
                  array_to_string(attacl, ',') AS privileges,
                  col_description(c.oid, attnum) AS description
           FROM pg_attribute JOIN pg_class c ON (attrelid = c.oid)
                JOIN pg_namespace ON (relnamespace = pg_namespace.oid)
                LEFT JOIN pg_attrdef ON (attrelid = pg_attrdef.adrelid
                     AND attnum = pg_attrdef.adnum)
                LEFT JOIN pg_collation l ON (attcollation = l.oid)
           WHERE relkind in ('c', 'r', 'f')
                 AND (nspname != 'pg_catalog'
                      AND nspname != 'information_schema')
                 AND attnum > 0
           ORDER BY nspname, relname, attnum"""

    def _from_catalog(self):
        """Initialize the dictionary of columns by querying the catalogs"""
        if self.dbconn.version < 90100:
            self.query = QUERY_PRE91
        for col in self.fetch():
            if hasattr(col, 'privileges'):
                col.privileges = col.privileges.split(',')
            sch, tbl = col.key()
            if (sch, tbl) not in self:
                self[(sch, tbl)] = []
            self[(sch, tbl)].append(col)

    def from_map(self, table, incols):
        """Initialize the dictionary of columns by converting the input list

        :param table: table or type owning the columns/attributes
        :param incols: YAML list defining the columns
        """
        if not incols:
            raise ValueError("Table '%s' has no columns" % table.name)
        cols = self[(table.schema, table.name)] = []

        for incol in incols:
            for key in incol:
                if isinstance(incol[key], dict):
                    arg = incol[key]
                else:
                    arg = {'type': incol[key]}
                col = Column(schema=table.schema, table=table.name, name=key,
                             **arg)
                if hasattr(col, 'privileges'):
                    if not hasattr(table, 'owner'):
                        raise ValueError("Column '%s.%s' has privileges but "
                                         "no owner information" % (
                                         table.name, key))
                    col.privileges = privileges_from_map(
                        col.privileges, col.allprivs, table.owner)
                cols.append(col)

    def diff_map(self, incols):
        """Generate SQL to transform existing columns

        :param incols: a YAML map defining the new columns
        :return: list of SQL statements

        Compares the existing column definitions, as fetched from the
        catalogs, to the input map and generates SQL statements to
        transform the columns accordingly.

        This takes care of dropping columns that are not present in
        the input map.  It's separate so that it can be done last,
        after other table, constraint and index changes.
        """
        stmts = []
        if not incols or not self:
            return stmts

        for (sch, tbl) in incols:
            if (sch, tbl) in self:
                for col in self[(sch, tbl)]:
                    if col.name not in [c.name for c in incols[(sch, tbl)]] \
                            and not hasattr(col, 'dropped') \
                            and not hasattr(col, 'inherited'):
                        stmts.append(col.drop())

        return stmts

########NEW FILE########
__FILENAME__ = constraint
# -*- coding: utf-8 -*-
"""
    pyrseas.constraint
    ~~~~~~~~~~~~~~~~~~

    This module defines six classes: Constraint derived from
    DbSchemaObject, CheckConstraint, PrimaryKey, ForeignKey and
    UniqueConstraint derived from Constraint, and ConstraintDict
    derived from DbObjectDict.
"""
from pyrseas.dbobject import DbObjectDict, DbSchemaObject
from pyrseas.dbobject import quote_id, split_schema_obj, commentable


ACTIONS = {'r': 'restrict', 'c': 'cascade', 'n': 'set null',
           'd': 'set default'}


class Constraint(DbSchemaObject):
    """A constraint definition, such as a primary key, foreign key or
       unique constraint"""

    keylist = ['schema', 'table', 'name']

    def key_columns(self):
        """Return comma-separated list of key column names

        :return: string
        """
        return ", ".join([quote_id(col) for col in self.keycols])

    @commentable
    def add(self):
        """Return string to add the constraint via ALTER TABLE

        :return: SQL statement

        Works as is for primary keys and unique constraints but has
        to be overridden for check constraints and foreign keys.
        """
        stmts = []
        tblspc = ''
        if hasattr(self, 'tablespace'):
            tblspc = " USING INDEX TABLESPACE %s" % self.tablespace
        stmts.append("ALTER TABLE %s ADD CONSTRAINT %s %s (%s)%s" % (
                     self._table.qualname(), quote_id(self.name),
                     self.objtype, self.key_columns(), tblspc))
        if hasattr(self, 'cluster') and self.cluster:
            stmts.append("CLUSTER %s USING %s" % (
                quote_id(self.table), quote_id(self.name)))
        return stmts

    def drop(self):
        """Return string to drop the constraint via ALTER TABLE

        :return: SQL statement
        """
        if not hasattr(self, 'dropped') or not self.dropped:
            self.dropped = True
            return "ALTER TABLE %s DROP CONSTRAINT %s" % (
                self._table.qualname(), quote_id(self.name))
        return []

    def comment(self):
        """Return SQL statement to create COMMENT on constraint

        :return: SQL statement
        """
        return "COMMENT ON CONSTRAINT %s ON %s IS %s" % (
            quote_id(self.name), self._table.qualname(), self._comment_text())


class CheckConstraint(Constraint):
    "A check constraint definition"

    objtype = "CHECK"

    def to_map(self, dbcols):
        """Convert a check constraint definition to a YAML-suitable format

        :param dbcols: dictionary of dbobject columns
        :return: dictionary
        """
        dct = self._base_map()
        if '_table' in dct:
            del dct['_table']
        if 'target' in dct:
            del dct['target']
        if dbcols:
            dct['columns'] = [dbcols[k - 1] for k in self.keycols]
            del dct['keycols']
        return {self.name: dct}

    @commentable
    def add(self):
        """Return string to add the CHECK constraint via ALTER TABLE

        :return: SQL statement
        """
        return ["ALTER TABLE %s ADD CONSTRAINT %s %s (%s)" % (
                self._table.qualname(), quote_id(self.name), self.objtype,
                self.expression)]

    def diff_map(self, inchk):
        """Generate SQL to transform an existing CHECK constraint

        :param inchk: a YAML map defining the new CHECK constraint
        :return: list of SQL statements

        Compares the CHECK constraint to an input constraint and generates
        SQL statements to transform it into the one represented by the
        input.
        """
        stmts = []
        # TODO: to be implemented
        stmts.append(self.diff_description(inchk))
        return stmts


class PrimaryKey(Constraint):
    "A primary key constraint definition"

    objtype = "PRIMARY KEY"

    def to_map(self, dbcols):
        """Convert a primary key definition to a YAML-suitable format

        :param dbcols: dictionary of dbobject columns
        :return: dictionary
        """
        dct = self._base_map()
        if dct['access_method'] == 'btree':
            del dct['access_method']
        del dct['_table']
        dct['columns'] = [dbcols[k - 1] for k in self.keycols]
        del dct['keycols']
        return {self.name: dct}

    def diff_map(self, inpk):
        """Generate SQL to transform an existing primary key

        :param inpk: a YAML map defining the new primary key
        :return: list of SQL statements

        Compares the primary key to an input primary key and generates
        SQL statements to transform it into the one represented by the
        input.
        """
        stmts = []
        # TODO: to be implemented (via ALTER DROP and ALTER ADD)
        if hasattr(inpk, 'cluster'):
            if not hasattr(self, 'cluster'):
                stmts.append("CLUSTER %s USING %s" % (
                    quote_id(self.table), quote_id(self.name)))
        elif hasattr(self, 'cluster'):
            stmts.append("ALTER TABLE %s\n    SET WITHOUT CLUSTER" %
                         quote_id(self.table))
        stmts.append(self.diff_description(inpk))
        return stmts


class ForeignKey(Constraint):
    "A foreign key constraint definition"

    objtype = "FOREIGN KEY"

    def ref_columns(self):
        """Return comma-separated list of reference column names

        :return: string
        """
        return ", ".join(self.ref_cols)

    def to_map(self, dbcols, refcols):
        """Convert a foreign key definition to a YAML-suitable format

        :param dbcols: dictionary of dbobject columns
        :return: dictionary
        """
        dct = self._base_map()
        del dct['_table']
        dct['columns'] = [dbcols[k - 1] for k in self.keycols]
        del dct['keycols']
        refsch = hasattr(self, 'ref_schema') and self.ref_schema or self.schema
        ref_cols = [refcols[k - 1] for k in self.ref_cols]
        dct['references'] = {'table': dct['ref_table'], 'columns': ref_cols}
        if 'ref_schema' in dct:
            dct['references'].update(schema=refsch)
            del dct['ref_schema']
        del dct['ref_table'], dct['ref_cols']
        return {self.name: dct}

    @commentable
    def add(self):
        """Return string to add the foreign key via ALTER TABLE

        :return: SQL statement
        """
        match = ''
        if hasattr(self, 'match'):
            match = " MATCH %s" % self.match.upper()
        actions = ''
        if hasattr(self, 'on_update'):
            actions = " ON UPDATE %s" % self.on_update.upper()
        if hasattr(self, 'on_delete'):
            actions += " ON DELETE %s" % self.on_delete.upper()
        if getattr(self, 'deferrable', False):
            actions += " DEFERRABLE"
        if getattr(self, 'deferred', False):
            actions += " INITIALLY DEFERRED"

        return "ALTER TABLE %s ADD CONSTRAINT %s FOREIGN KEY (%s) " \
            "REFERENCES %s (%s)%s%s" % (
            self._table.qualname(), quote_id(self.name), self.key_columns(),
            self.references.qualname(), self.ref_columns(), match, actions)

    def diff_map(self, infk):
        """Generate SQL to transform an existing foreign key

        :param infk: a YAML map defining the new foreign key
        :return: list of SQL statements

        Compares the foreign key to an input foreign key and generates
        SQL statements to transform it into the one represented by the
        input.
        """
        stmts = []
        # TODO: to be implemented (via ALTER DROP and ALTER ADD)
        stmts.append(self.diff_description(infk))
        return stmts


class UniqueConstraint(Constraint):
    "A unique constraint definition"

    objtype = "UNIQUE"

    def to_map(self, dbcols):
        """Convert a unique constraint definition to a YAML-suitable format

        :param dbcols: dictionary of dbobject columns
        :return: dictionary
        """
        dct = self._base_map()
        if dct['access_method'] == 'btree':
            del dct['access_method']
        del dct['_table']
        dct['columns'] = []
        dct['columns'] = [dbcols[k - 1] for k in self.keycols]
        del dct['keycols']
        return {self.name: dct}

    def diff_map(self, inuc):
        """Generate SQL to transform an existing unique constraint

        :param inuc: a YAML map defining the new unique constraint
        :return: list of SQL statements

        Compares the unique constraint to an input unique constraint
        and generates SQL statements to transform it into the one
        represented by the input.
        """
        stmts = []
        # TODO: to be implemented (via ALTER DROP and ALTER ADD)
        if hasattr(inuc, 'cluster'):
            if not hasattr(self, 'cluster'):
                stmts.append("CLUSTER %s USING %s" % (
                    quote_id(self.table), quote_id(self.name)))
        elif hasattr(self, 'cluster'):
            stmts.append("ALTER TABLE %s\n    SET WITHOUT CLUSTER" %
                         quote_id(self.table))
        stmts.append(self.diff_description(inuc))
        return stmts

MATCHTYPES_PRE93 = {'f': 'full', 'p': 'partial', 'u': 'simple'}
COMMON_ATTRS = ['access_method', 'tablespace', 'description', 'cluster']


class ConstraintDict(DbObjectDict):
    "The collection of table or column constraints in a database"

    cls = Constraint
    query = \
        """SELECT nspname AS schema,
                  CASE WHEN contypid = 0 THEN conrelid::regclass::name
                       ELSE contypid::regtype::name END AS table,
                  conname AS name,
                  CASE WHEN contypid != 0 THEN 'd' ELSE '' END AS target,
                  contype AS type, conkey AS keycols,
                  condeferrable AS deferrable, condeferred AS deferred,
                  confrelid::regclass AS ref_table, confkey AS ref_cols,
                  consrc AS expression, confupdtype AS on_update,
                  confdeltype AS on_delete, confmatchtype AS match,
                  amname AS access_method, spcname AS tablespace,
                  indisclustered AS cluster,
                  obj_description(c.oid, 'pg_constraint') AS description
           FROM pg_constraint c
                JOIN pg_namespace ON (connamespace = pg_namespace.oid)
                LEFT JOIN pg_class cl on (conname = relname)
                LEFT JOIN pg_index i ON (i.indexrelid = cl.oid)
                LEFT JOIN pg_tablespace t ON (cl.reltablespace = t.oid)
                LEFT JOIN pg_am on (relam = pg_am.oid)
           WHERE (nspname != 'pg_catalog' AND nspname != 'information_schema')
                 AND conislocal
           ORDER BY schema, 2, name"""
    match_types = {'f': 'full', 'p': 'partial', 's': 'simple'}

    def _from_catalog(self):
        """Initialize the dictionary of constraints by querying the catalogs"""
        if self.dbconn.version < 90300:
            self.match_types = MATCHTYPES_PRE93
        for constr in self.fetch():
            constr.unqualify()
            sch, tbl, cns = constr.key()
            sch, tbl = split_schema_obj('%s.%s' % (sch, tbl))
            constr_type = constr.type
            del constr.type
            if constr_type != 'f':
                del constr.ref_table
                del constr.on_update
                del constr.on_delete
                del constr.match
            if constr_type == 'c':
                self[(sch, tbl, cns)] = CheckConstraint(**constr.__dict__)
            elif constr_type == 'p':
                self[(sch, tbl, cns)] = PrimaryKey(**constr.__dict__)
            elif constr_type == 'f':
                # normalize reference schema/table:
                # if reftbl is qualified, split the schema out,
                # otherwise it's in the 'public' schema (set as default
                # when connecting)
                if constr.on_update == 'a':
                    del constr.on_update
                else:
                    constr.on_update = ACTIONS[constr.on_update]
                if constr.on_delete == 'a':
                    del constr.on_delete
                else:
                    constr.on_delete = ACTIONS[constr.on_delete]
                if self.match_types[constr.match] == 'simple':
                    del constr.match
                else:
                    constr.match = self.match_types[constr.match]
                reftbl = constr.ref_table
                (constr.ref_schema, constr.ref_table) = split_schema_obj(
                    reftbl)
                self[(sch, tbl, cns)] = ForeignKey(**constr.__dict__)
            elif constr_type == 'u':
                self[(sch, tbl, cns)] = UniqueConstraint(**constr.__dict__)

    def from_map(self, table, inconstrs, target=''):
        """Initialize the dictionary of constraints by converting the input map

        :param table: table affected by the constraints
        :param inconstrs: YAML map defining the constraints
        """
        if 'check_constraints' in inconstrs:
            chks = inconstrs['check_constraints']
            for cns in chks:
                check = CheckConstraint(table=table.name, schema=table.schema,
                                        name=cns)
                val = chks[cns]
                try:
                    check.expression = val['expression']
                except KeyError as exc:
                    exc.args = ("Constraint '%s' is missing expression"
                                % cns, )
                    raise
                if check.expression[0] == '(' and check.expression[-1] == ')':
                    check.expression = check.expression[1:-1]
                if 'columns' in val:
                    check.keycols = val['columns']
                if target:
                    check.target = target
                if 'description' in val:
                    check.description = val['description']
                self[(table.schema, table.name, cns)] = check
        if 'primary_key' in inconstrs:
            cns = list(inconstrs['primary_key'].keys())[0]
            pkey = PrimaryKey(table=table.name, schema=table.schema,
                              name=cns)
            val = inconstrs['primary_key'][cns]
            try:
                pkey.keycols = val['columns']
            except KeyError as exc:
                exc.args = ("Constraint '%s' is missing columns" % cns, )
                raise
            for attr, value in list(val.items()):
                if attr in COMMON_ATTRS:
                    setattr(pkey, attr, value)
            self[(table.schema, table.name, cns)] = pkey
        if 'foreign_keys' in inconstrs:
            fkeys = inconstrs['foreign_keys']
            for cns in fkeys:
                fkey = ForeignKey(table=table.name, schema=table.schema,
                                  name=cns)
                val = fkeys[cns]
                if 'on_update' in val:
                    act = val['on_update']
                    if act.lower() not in list(ACTIONS.values()):
                        raise ValueError("Invalid action '%s' for constraint "
                                         "'%s'" % (act, cns))
                    fkey.on_update = act
                if 'on_delete' in val:
                    act = val['on_delete']
                    if act.lower() not in list(ACTIONS.values()):
                        raise ValueError("Invalid action '%s' for constraint "
                                         "'%s'" % (act, cns))
                    fkey.on_delete = act
                if 'deferrable' in val:
                    fkey.deferrable = True
                if 'deferred' in val:
                    fkey.deferred = True
                if 'match' in val:
                    mat = val['match']
                    if mat.lower() not in list(self.match_types.values()):
                        raise ValueError("Invalid match type '%s' for "
                                         "constraint '%s'" % (mat, cns))
                    fkey.match = mat
                try:
                    fkey.keycols = val['columns']
                except KeyError as exc:
                    exc.args = ("Constraint '%s' is missing columns" % cns, )
                    raise
                try:
                    refs = val['references']
                except KeyError as exc:
                    exc.args = ("Constraint '%s' missing references" % cns, )
                    raise
                try:
                    fkey.ref_table = refs['table']
                except KeyError as exc:
                    exc.args = ("Constraint '%s' missing table reference"
                                % cns, )
                    raise
                try:
                    fkey.ref_cols = refs['columns']
                except KeyError as exc:
                    exc.args = ("Constraint '%s' missing reference columns"
                                % cns, )
                    raise
                sch = table.schema
                if 'schema' in refs:
                    sch = refs['schema']
                fkey.ref_schema = sch
                if 'description' in val:
                    fkey.description = val['description']
                self[(table.schema, table.name, cns)] = fkey
        if 'unique_constraints' in inconstrs:
            uconstrs = inconstrs['unique_constraints']
            for cns in uconstrs:
                unq = UniqueConstraint(table=table.name, schema=table.schema,
                                       name=cns)
                val = uconstrs[cns]
                try:
                    unq.keycols = val['columns']
                except KeyError as exc:
                    exc.args = ("Constraint '%s' is missing columns" % cns, )
                    raise
                for attr, value in list(val.items()):
                    if attr in COMMON_ATTRS:
                        setattr(unq, attr, value)
                self[(table.schema, table.name, cns)] = unq

    def diff_map(self, inconstrs):
        """Generate SQL to transform existing constraints

        :param inconstrs: a YAML map defining the new constraints
        :return: list of SQL statements

        Compares the existing constraint definitions, as fetched from
        the catalogs, to the input map and generates SQL statements to
        transform the constraints accordingly.
        """
        stmts = []
        # foreign keys are processed in a second pass
        # constraints cannot be renamed
        for turn in (1, 2):
            # check database constraints
            for (sch, tbl, cns) in self:
                constr = self[(sch, tbl, cns)]
                if isinstance(constr, ForeignKey):
                    if turn == 1:
                        continue
                elif turn == 2:
                    continue
                # if missing, drop it
                if (sch, tbl, cns) not in inconstrs \
                        and not hasattr(constr, 'target'):
                    stmts.append(constr.drop())
            # check input constraints
            for (sch, tbl, cns) in inconstrs:
                inconstr = inconstrs[(sch, tbl, cns)]
                # skip DOMAIN constraints
                if hasattr(inconstr, 'target'):
                    continue
                if isinstance(inconstr, ForeignKey):
                    if turn == 1:
                        continue
                elif turn == 2:
                    continue
                # does it exist in the database?
                if (sch, tbl, cns) not in self:
                    # add the new constraint
                    stmts.append(inconstr.add())
                else:
                    # check constraint objects
                    stmts.append(self[(sch, tbl, cns)].diff_map(inconstr))

        return stmts

########NEW FILE########
__FILENAME__ = conversion
# -*- coding: utf-8 -*-
"""
    pyrseas.dbobject.conversion
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~

    This defines two classes, Conversion and ConversionDict, derived from
    DbSchemaObject and DbObjectDict, respectively.
"""
from pyrseas.dbobject import DbObjectDict, DbSchemaObject
from pyrseas.dbobject import commentable, ownable


class Conversion(DbSchemaObject):
    """A conversion definition"""

    keylist = ['schema', 'name']
    objtype = "CONVERSION"
    single_extern_file = True

    @commentable
    @ownable
    def create(self):
        """Return SQL statements to CREATE the conversion

        :return: SQL statements
        """
        dflt = ''
        if hasattr(self, 'default') and self.default:
            dflt = 'DEFAULT '
        return ["CREATE %sCONVERSION %s\n    FOR '%s' TO '%s' FROM %s" % (
                dflt, self.qualname(), self.source_encoding,
                self.dest_encoding, self.function)]


class ConversionDict(DbObjectDict):
    "The collection of conversions in a database."

    cls = Conversion
    query = \
        """SELECT nspname AS schema, conname AS name,
                  pg_encoding_to_char(c.conforencoding) AS source_encoding,
                  pg_encoding_to_char(c.contoencoding) AS dest_encoding,
                  conproc AS function, condefault AS default,
                  obj_description(c.oid, 'pg_conversion') AS description
           FROM pg_conversion c
                JOIN pg_namespace n ON (connamespace = n.oid)
           WHERE (nspname != 'pg_catalog' AND nspname != 'information_schema')
           ORDER BY nspname, conname"""

    def from_map(self, schema, inmap):
        """Initialize the dictionary of conversions by examining the input map

        :param schema: the schema owing the conversions
        :param inmap: the input YAML map defining the conversions
        """
        for key in inmap:
            if not key.startswith('conversion '):
                raise KeyError("Unrecognized object type: %s" % key)
            cnv = key[11:]
            inconv = inmap[key]
            conv = Conversion(schema=schema.name, name=cnv, **inconv)
            if inconv:
                if 'oldname' in inconv:
                    conv.oldname = inconv['oldname']
                    del inconv['oldname']
                if 'description' in inconv:
                    conv.description = inconv['description']
            self[(schema.name, cnv)] = conv

    def diff_map(self, inconvs):
        """Generate SQL to transform existing conversions

        :param inconvs: a YAML map defining the new conversions
        :return: list of SQL statements

        Compares the existing conversion definitions, as fetched from
        the catalogs, to the input map and generates SQL statements to
        create, drop or change the conversions accordingly.
        """
        stmts = []
        # check input conversions
        for cnv in inconvs:
            inconv = inconvs[cnv]
            # does it exist in the database?
            if cnv in self:
                stmts.append(self[cnv].diff_map(inconv))
            else:
                # check for possible RENAME
                if hasattr(inconv, 'oldname'):
                    oldname = inconv.oldname
                    try:
                        stmts.append(self[oldname].rename(inconv.name))
                        del self[oldname]
                    except KeyError as exc:
                        exc.args = ("Previous name '%s' for conversion '%s' "
                                    "not found" % (oldname, inconv.name), )
                        raise
                else:
                    # create new conversion
                    stmts.append(inconv.create())
        # check database conversions
        for (sch, cnv) in self:
            # if missing, drop it
            if (sch, cnv) not in inconvs:
                stmts.append(self[(sch, cnv)].drop())

        return stmts

########NEW FILE########
__FILENAME__ = dbtype
# -*- coding: utf-8 -*-
"""
    pyrseas.dbobject.dbtype
    ~~~~~~~~~~~~~~~~~~~~~~~

    This module defines six classes: DbType derived from
    DbSchemaObject, BaseType, Composite, Domain and Enum derived from
    DbType, and DbTypeDict derived from DbObjectDict.
"""
from pyrseas.dbobject import DbObjectDict, DbSchemaObject
from pyrseas.dbobject import split_schema_obj, commentable, ownable
from pyrseas.dbobject.constraint import CheckConstraint


ALIGNMENT_TYPES = {'c': 'char', 's': 'int2', 'i': 'int4', 'd': 'double'}
STORAGE_TYPES = {'p': 'plain', 'e': 'external', 'm': 'main', 'x': 'extended'}
OPT_FUNCS = ('receive', 'send', 'typmod_in', 'typmod_out', 'analyze')


class DbType(DbSchemaObject):
    """A composite, domain or enum type"""

    keylist = ['schema', 'name']
    objtype = "TYPE"


class BaseType(DbType):
    """A composite type"""

    def to_map(self, no_owner):
        """Convert a type to a YAML-suitable format

        :param no_owner: exclude type owner information
        :return: dictionary
        """
        dct = self._base_map(no_owner)
        del dct['dep_funcs']
        if self.internallength < 0:
            dct['internallength'] = 'variable'
        dct['alignment'] = ALIGNMENT_TYPES[self.alignment]
        dct['storage'] = STORAGE_TYPES[self.storage]
        if self.delimiter == ',':
            del dct['delimiter']
        return dct

    @commentable
    @ownable
    def create(self):
        """Return SQL statements to CREATE the base type

        :return: SQL statements
        """
        stmts = []
        stmts.append("CREATE TYPE %s" % self.qualname())
        stmts.append(self.dep_funcs['input'].create(basetype=True))
        stmts.append(self.dep_funcs['output'].create(basetype=True))
        opt_clauses = []
        for fnc in OPT_FUNCS:
            if fnc in self.dep_funcs:
                stmts.append(self.dep_funcs[fnc].create(basetype=True))
                opt_clauses.append("%s = %s" % (
                    fnc.upper(), self.dep_funcs[fnc].qualname()))
        if hasattr(self, 'internallength'):
            opt_clauses.append("INTERNALLENGTH = %s" % self.internallength)
        if hasattr(self, 'alignment'):
            opt_clauses.append("ALIGNMENT = %s" % self.alignment)
        if hasattr(self, 'storage'):
            opt_clauses.append("STORAGE = %s" % self.storage)
        if hasattr(self, 'delimiter'):
            opt_clauses.append("DELIMITER = '%s'" % self.delimiter)
        if hasattr(self, 'category'):
            opt_clauses.append("CATEGORY = '%s'" % self.category)
        if hasattr(self, 'preferred'):
            opt_clauses.append("PREFERRED = TRUE")
        stmts.append("CREATE TYPE %s (\n    INPUT = %s,"
                     "\n    OUTPUT = %s%s%s)" % (
                     self.qualname(), self.input, self.output, opt_clauses and
                     ',\n    ' or '', ',\n    '.join(opt_clauses)))
        return stmts

    def drop(self):
        """Return SQL statement to DROP the base type

        :return: SQL statement

        We have to override the super method and add CASCADE to drop
        dependent functions.
        """
        return ["DROP TYPE %s CASCADE" % self.qualname()]


class Composite(DbType):
    """A composite type"""

    def to_map(self, no_owner):
        """Convert a type to a YAML-suitable format

        :param no_owner: exclude type owner information
        :return: dictionary
        """
        if not hasattr(self, 'attributes'):
            return
        attrs = []
        for attr in self.attributes:
            att = attr.to_map(False)
            if att:
                attrs.append(att)
        dct = {'attributes': attrs}
        if not no_owner and hasattr(self, 'owner'):
            dct.update(owner=self.owner)
        if hasattr(self, 'description'):
            dct.update(description=self.description)
        return dct

    @commentable
    @ownable
    def create(self):
        """Return SQL statements to CREATE the composite type

        :return: SQL statements
        """
        attrs = []
        for att in self.attributes:
            attrs.append("    " + att.add()[0])
        return ["CREATE TYPE %s AS (%s)" % (
                self.qualname(), ",\n".join(attrs))]

    def diff_map(self, intype):
        """Generate SQL to transform an existing composite type

        :param intype: the new composite type
        :return: list of SQL statements

        Compares the type to an input type and generates SQL
        statements to transform it into the one represented by the
        input.
        """
        stmts = []
        if not hasattr(intype, 'attributes'):
            raise KeyError("Composite '%s' has no attributes" % intype.name)
        attrnames = [attr.name for attr in self.attributes
                     if not hasattr(attr, 'dropped')]
        dbattrs = len(attrnames)

        base = "ALTER TYPE %s\n    " % (self.qualname())
        # check input attributes
        for (num, inattr) in enumerate(intype.attributes):
            if hasattr(inattr, 'oldname'):
                assert(self.attributes[num].name == inattr.oldname)
                stmts.append(self.attributes[num].rename(inattr.name))
            # check existing attributes
            if num < dbattrs and self.attributes[num].name == inattr.name:
                (stmt, descr) = self.attributes[num].diff_map(inattr)
                if stmt:
                    stmts.append(base + stmt)
                if descr:
                    stmts.append(descr)
            # add new attributes
            elif inattr.name not in attrnames:
                (stmt, descr) = inattr.add()
                stmts.append(base + "ADD ATTRIBUTE %s" % stmt)
                if descr:
                    stmts.append(descr)

        if hasattr(intype, 'owner'):
            if intype.owner != self.owner:
                stmts.append(self.alter_owner(intype.owner))
        stmts.append(self.diff_description(intype))

        return stmts


class Enum(DbType):
    "An enumerated type definition"

    @commentable
    @ownable
    def create(self):
        """Return SQL statements to CREATE the enum

        :return: SQL statements
        """
        lbls = ["'%s'" % lbl for lbl in self.labels]
        return ["CREATE TYPE %s AS ENUM (%s)" % (
                self.qualname(), ",\n    ".join(lbls))]


class Domain(DbType):
    "A domain definition"

    objtype = "DOMAIN"

    def to_map(self, no_owner):
        """Convert a domain to a YAML-suitable format

        :param no_owner: exclude domain owner information
        :return: dictionary
        """
        dct = self._base_map(no_owner)
        if hasattr(self, 'check_constraints'):
            if not 'check_constraints' in dct:
                dct.update(check_constraints={})
            for cns in list(self.check_constraints.values()):
                dct['check_constraints'].update(
                    self.check_constraints[cns.name].to_map(None))

        return dct

    @commentable
    @ownable
    def create(self):
        """Return SQL statements to CREATE the domain

        :return: SQL statements
        """
        create = "CREATE DOMAIN %s AS %s" % (self.qualname(), self.type)
        if hasattr(self, 'not_null'):
            create += ' NOT NULL'
        if hasattr(self, 'default'):
            create += ' DEFAULT ' + str(self.default)
        if hasattr(self, 'check_constraints'):
            cnslist = []
            for cns in list(self.check_constraints.values()):
                cnslist.append(" CONSTRAINT %s CHECK (%s)" % (
                    cns.name, cns.expression))
            create += ", ".join(cnslist)
        return [create]


class TypeDict(DbObjectDict):
    "The collection of domains and enums in a database"

    cls = DbType
    query = \
        """SELECT nspname AS schema, typname AS name, typtype AS kind,
                  format_type(typbasetype, typtypmod) AS type,
                  typnotnull AS not_null, typdefault AS default,
                  ARRAY(SELECT enumlabel FROM pg_enum e WHERE t.oid = enumtypid
                  ORDER BY e.oid) AS labels, rolname AS owner,
                  typinput::regproc AS input, typoutput::regproc AS output,
                  typreceive::regproc AS receive, typsend::regproc AS send,
                  typmodin::regproc AS typmod_in,
                  typmodout::regproc AS typmod_out,
                  typanalyze::regproc AS analyze,
                  typlen AS internallength, typalign AS alignment,
                  typstorage AS storage, typdelim AS delimiter,
                  typcategory AS category, typispreferred AS preferred,
                  obj_description(t.oid, 'pg_type') AS description
           FROM pg_type t
                JOIN pg_roles r ON (r.oid = typowner)
                JOIN pg_namespace n ON (typnamespace = n.oid)
                LEFT JOIN pg_class c ON (typrelid = c.oid)
           WHERE typisdefined AND (typtype in ('d', 'e')
                 OR (typtype = 'c' AND relkind = 'c')
                 OR (typtype = 'b' AND typarray != 0))
             AND nspname NOT IN ('pg_catalog', 'pg_toast',
                                 'information_schema')
             AND t.oid NOT IN (
                 SELECT objid FROM pg_depend WHERE deptype = 'e'
                              AND classid = 'pg_type'::regclass)
           ORDER BY nspname, typname"""

    def _from_catalog(self):
        """Initialize the dictionary of types by querying the catalogs"""
        for dbtype in self.fetch():
            sch, typ = dbtype.key()
            kind = dbtype.kind
            del dbtype.kind
            if kind != 'b':
                del dbtype.input, dbtype.output
                del dbtype.receive, dbtype.send
                del dbtype.typmod_in, dbtype.typmod_out, dbtype.analyze
                del dbtype.internallength, dbtype.alignment, dbtype.storage
                del dbtype.delimiter, dbtype.category
            if kind == 'd':
                self[(sch, typ)] = Domain(**dbtype.__dict__)
            elif kind == 'e':
                del dbtype.type
                self[(sch, typ)] = Enum(**dbtype.__dict__)
                if not hasattr(self[(sch, typ)], 'labels'):
                    self[(sch, typ)].labels = {}
            elif kind == 'c':
                del dbtype.type
                self[(sch, typ)] = Composite(**dbtype.__dict__)
            elif kind == 'b':
                del dbtype.type
                for attr in OPT_FUNCS:
                    if getattr(dbtype, attr) == '-':
                        delattr(dbtype, attr)
                self[(sch, typ)] = BaseType(**dbtype.__dict__)

    def from_map(self, schema, inobjs, newdb):
        """Initalize the dictionary of types by converting the input map

        :param schema: schema owning the types
        :param inobjs: YAML map defining the schema objects
        :param newdb: collection of dictionaries defining the database
        """
        for k in inobjs:
            (objtype, spc, key) = k.partition(' ')
            if spc != ' ' or not objtype in ['domain', 'type']:
                raise KeyError("Unrecognized object type: %s" % k)
            if objtype == 'domain':
                self[(schema.name, key)] = domain = Domain(
                    schema=schema.name, name=key)
                indomain = inobjs[k]
                if not indomain:
                    raise ValueError("Domain '%s' has no specification" % k)
                for attr, val in list(indomain.items()):
                    setattr(domain, attr, val)
                if 'oldname' in indomain:
                    domain.oldname = indomain['oldname']
                newdb.constraints.from_map(domain, indomain, 'd')
                if 'description' in indomain:
                    domain.description = indomain['description']
            elif objtype == 'type':
                intype = inobjs[k]
                if 'labels' in intype:
                    self[(schema.name, key)] = dtype = Enum(
                        schema=schema.name, name=key)
                    dtype.labels = intype['labels']
                elif 'attributes' in intype:
                    self[(schema.name, key)] = dtype = Composite(
                        schema=schema.name, name=key)
                    try:
                        newdb.columns.from_map(dtype, intype['attributes'])
                    except KeyError as exc:
                        exc.args = ("Type '%s' has no attributes" % key, )
                        raise
                elif 'input' in intype:
                    self[(schema.name, key)] = dtype = BaseType(
                        schema=schema.name, name=key)
                for attr, val in list(intype.items()):
                    setattr(dtype, attr, val)
                if 'oldname' in intype:
                    dtype.oldname = intype['oldname']
                if 'description' in intype:
                    dtype.description = intype['description']
            else:
                raise KeyError("Unrecognized object type: %s" % k)

    def link_refs(self, dbcolumns, dbconstrs, dbfuncs):
        """Connect various objects to their corresponding types or domains

        :param dbcolumns: dictionary of columns
        :param dbconstrs: dictionary of constraints
        :param dbfuncs: dictionary of functions

        Fills the `check_constraints` dictionaries for each domain by
        traversing the `dbconstrs` dictionary. Fills the attributes
        list for composite types. Fills the dependent functions
        dictionary for base types.
        """
        for (sch, typ) in dbcolumns:
            if (sch, typ) in self:
                assert isinstance(self[(sch, typ)], Composite)
                self[(sch, typ)].attributes = dbcolumns[(sch, typ)]
                for attr in dbcolumns[(sch, typ)]:
                    attr._type = self[(sch, typ)]
        for (sch, typ, cns) in dbconstrs:
            constr = dbconstrs[(sch, typ, cns)]
            if not hasattr(constr, 'target') or constr.target != 'd':
                continue
            assert self[(sch, typ)]
            dbtype = self[(sch, typ)]
            if isinstance(constr, CheckConstraint):
                if not hasattr(dbtype, 'check_constraints'):
                    dbtype.check_constraints = {}
                dbtype.check_constraints.update({cns: constr})
        for (sch, typ) in self:
            dbtype = self[(sch, typ)]
            if isinstance(dbtype, BaseType):
                if not hasattr(dbtype, 'dep_funcs'):
                    dbtype.dep_funcs = {}
                (sch, infnc) = split_schema_obj(dbtype.input, sch)
                args = 'cstring'
                if not (sch, infnc, args) in dbfuncs:
                    args = 'cstring, oid, integer'
                func = dbfuncs[(sch, infnc, args)]
                dbtype.dep_funcs.update({'input': func})
                func._dep_type = dbtype
                (sch, outfnc) = split_schema_obj(dbtype.output, sch)
                func = dbfuncs[(sch, outfnc, dbtype.qualname())]
                dbtype.dep_funcs.update({'output': func})
                func._dep_type = dbtype
                for attr in OPT_FUNCS:
                    if hasattr(dbtype, attr):
                        (sch, fnc) = split_schema_obj(
                            getattr(dbtype, attr), sch)
                        if attr == 'receive':
                            arg = 'internal'
                        elif attr == 'send':
                            arg = dbtype.qualname()
                        elif attr == 'typmod_in':
                            arg = 'cstring[]'
                        elif attr == 'typmod_out':
                            arg = 'integer'
                        elif attr == 'analyze':
                            arg = 'internal'
                        func = dbfuncs[(sch, fnc, arg)]
                        dbtype.dep_funcs.update({attr: func})
                        func._dep_type = dbtype

    def diff_map(self, intypes):
        """Generate SQL to transform existing domains and types

        :param intypes: a YAML map defining the new domains/types
        :return: list of SQL statements

        Compares the existing domain/type definitions, as fetched from
        the catalogs, to the input map and generates SQL statements to
        transform the domains/types accordingly.
        """
        stmts = []
        # check input types
        for (sch, typ) in intypes:
            intype = intypes[(sch, typ)]
            # does it exist in the database?
            if (sch, typ) not in self:
                if not hasattr(intype, 'oldname'):
                    # create new type
                    stmts.append(intype.create())
                else:
                    stmts.append(self[(sch, intype.oldname)].rename(typ))
                    del self[(sch, intype.oldname)]

        # check existing types
        for (sch, typ) in self:
            dbtype = self[(sch, typ)]
            # if missing, mark it for dropping
            if (sch, typ) not in intypes:
                dbtype.dropped = False
            else:
                # check type objects
                stmts.append(dbtype.diff_map(intypes[(sch, typ)]))

        return stmts

    def _drop(self):
        """Actually drop the types

        :return: SQL statements
        """
        stmts = []
        for (sch, typ) in self:
            dbtype = self[(sch, typ)]
            if hasattr(dbtype, 'dropped'):
                stmts.append(dbtype.drop())
                if isinstance(dbtype, BaseType):
                    for func in dbtype.dep_funcs:
                        if func in ['typmod_in', 'typmod_out', 'analyze']:
                            stmts.append(dbtype.dep_funcs[func].drop())
        return stmts

########NEW FILE########
__FILENAME__ = eventtrig
# -*- coding: utf-8 -*-
"""
    pyrseas.dbobject.eventtrig
    ~~~~~~~~~~~~~~~~~~~~~~~~~~

    This module defines two classes: EventTrigger derived from
    DbObject, and EventTriggerDict derived from DbObjectDict.
"""
from pyrseas.dbobject import DbObjectDict, DbObject
from pyrseas.dbobject import quote_id, commentable

EXEC_PROC = 'EXECUTE PROCEDURE '


class EventTrigger(DbObject):
    """An event trigger"""

    keylist = ['name']
    objtype = "EVENT TRIGGER"

    @commentable
    def create(self):
        """Return SQL statements to CREATE the event trigger

        :return: SQL statements
        """
        filter = ''
        if hasattr(self, 'tags'):
            filter = "\n    WHEN tag IN (%s)" % ", ".join(
                ["'%s'" % tag for tag in self.tags])
        return ["CREATE %s %s\n    ON %s%s\n    EXECUTE PROCEDURE %s" % (
                self.objtype, quote_id(self.name), self.event, filter,
                self.procedure)]


class EventTriggerDict(DbObjectDict):
    "The collection of event triggers in a database"

    cls = EventTrigger
    query = \
        """SELECT evtname AS name, evtevent AS event, rolname AS owner,
                  evtenabled AS enabled, evtfoid::regprocedure AS procedure,
                  evttags AS tags,
                  obj_description(t.oid, 'pg_event_trigger') AS description
           FROM pg_event_trigger t
                JOIN pg_roles ON (evtowner = pg_roles.oid)
           ORDER BY 1"""
    enable_modes = {'O': True, 'D': False, 'R': 'replica',
                    'A': 'always'}

    def _from_catalog(self):
        """Initialize the dictionary of triggers by querying the catalogs"""
        if self.dbconn.version < 90300:
            return
        for trig in self.fetch():
            trig.enabled = self.enable_modes[trig.enabled]
            self[trig.key()] = trig

    def from_map(self, intriggers, newdb):
        """Initalize the dictionary of triggers by converting the input map

        :param intriggers: YAML map defining the event triggers
        :param newdb: dictionary of input database
        """
        for key in intriggers:
            if not key.startswith('event trigger '):
                raise KeyError("Unrecognized object type: %s" % key)
            trg = key[14:]
            intrig = intriggers[key]
            if not intrig:
                raise ValueError("Event trigger '%s' has no specification" %
                                 trg)
            self[trg] = trig = EventTrigger(name=trg)
            for attr, val in list(intrig.items()):
                setattr(trig, attr, val)
            if 'oldname' in intrig:
                trig.oldname = intrig['oldname']
            if 'description' in intrig:
                trig.description = intrig['description']

    def diff_map(self, intriggers):
        """Generate SQL to transform existing event triggers

        :param intriggers: a YAML map defining the new event triggers
        :return: list of SQL statements

        Compares the existing event trigger definitions, as fetched
        from the catalogs, to the input map and generates SQL
        statements to transform the event triggers accordingly.
        """
        stmts = []
        # check input triggers
        for trg in intriggers:
            intrig = intriggers[trg]
            # does it exist in the database?
            if trg not in self:
                if not hasattr(intrig, 'oldname'):
                    # create new trigger
                    stmts.append(intrig.create())
                else:
                    stmts.append(self[trg].rename(intrig))
            else:
                # check trigger objects
                stmts.append(self[trg].diff_map(intrig))

        # check existing triggers
        for trg in self:
            trig = self[trg]
            # if missing, drop them
            if trg not in intriggers:
                    stmts.append(trig.drop())

        return stmts

########NEW FILE########
__FILENAME__ = extension
# -*- coding: utf-8 -*-
"""
    pyrseas.dbobject.extension
    ~~~~~~~~~~~~~~~~~~~~~~~~~~

    This module defines two classes: Extension derived from DbObject,
    and ExtensionDict derived from DbObjectDict.
"""
from pyrseas.dbobject import DbObjectDict, DbObject
from pyrseas.dbobject import quote_id, commentable


class Extension(DbObject):
    """An extension"""

    keylist = ['name']
    objtype = "EXTENSION"
    single_extern_file = True

    @commentable
    def create(self):
        """Return SQL statements to CREATE the extension

        :return: SQL statements
        """
        opt_clauses = []
        if hasattr(self, 'schema') and self.schema != 'public':
            opt_clauses.append("SCHEMA %s" % quote_id(self.schema))
        if hasattr(self, 'version'):
            opt_clauses.append("VERSION '%s'" % self.version)
        return ["CREATE EXTENSION %s%s" % (
                quote_id(self.name), ('\n    ' + '\n    '.join(opt_clauses))
                if opt_clauses else '')]


class ExtensionDict(DbObjectDict):
    "The collection of extensions in a database"

    cls = Extension
    query = \
        """SELECT extname AS name, nspname AS schema, extversion AS version,
                  rolname AS owner,
                  obj_description(e.oid, 'pg_extension') AS description
           FROM pg_extension e
                JOIN pg_roles r ON (r.oid = extowner)
                JOIN pg_namespace n ON (extnamespace = n.oid)
           WHERE nspname != 'information_schema'
           ORDER BY extname"""

    def _from_catalog(self):
        """Initialize the dictionary of extensions by querying the catalogs"""
        if self.dbconn.version < 90100:
            return
        for ext in self.fetch():
            self[ext.key()] = ext

    def from_map(self, inexts, langtempls, newdb):
        """Initalize the dictionary of extensions by converting the input map

        :param inexts: YAML map defining the extensions
        :param langtempls: list of language templates
        :param newdb: dictionary of input database
        """
        for key in inexts:
            if not key.startswith('extension '):
                raise KeyError("Unrecognized object type: %s" % key)
            ext = key[10:]
            inexten = inexts[key]
            self[ext] = exten = Extension(name=ext)
            for attr, val in list(inexten.items()):
                setattr(exten, attr, val)
            if exten.name in langtempls:
                lang = {'language %s' % exten.name: {'_ext': 'e'}}
                newdb.languages.from_map(lang)

    def diff_map(self, inexts):
        """Generate SQL to transform existing extensions

        :param inexts: a YAML map defining the new extensions
        :return: list of SQL statements

        Compares the existing extension definitions, as fetched from
        the catalogs, to the input map and generates SQL statements to
        transform the extensions accordingly.
        """
        stmts = []
        # check input extensions
        for ext in inexts:
            inexten = inexts[ext]
            # does it exist in the database?
            if ext not in self:
                if not hasattr(inexten, 'oldname'):
                    # create new extension
                    stmts.append(inexten.create())
                else:
                    stmts.append(self[ext].rename(inexten))
            # check extension objects
            else:
                # extension owner cannot be altered, set no_owner to True
                stmts.append(self[ext].diff_map(inexten, no_owner=True))

        # check existing extensions
        for ext in self:
            exten = self[ext]
            # if missing, drop them
            if ext not in inexts:
                    stmts.append(exten.drop())

        return stmts

    def _drop(self):
        """Actually drop the extension

        :return: SQL statements
        """
        stmts = []
        for ext in self:
            if hasattr(self[ext], 'dropped'):
                stmts.append(self[ext].drop())
        return stmts

########NEW FILE########
__FILENAME__ = foreign
# -*- coding: utf-8 -*-
"""
    pyrseas.dbobject.foreign
    ~~~~~~~~~~~~~~~~~~~~~~~~

    This defines nine classes: DbObjectWithOptions derived from
    DbObject, ForeignDataWrapper, ForeignServer and UserMapping
    derived from DbObjectWithOptions, ForeignDataWrapperDict,
    ForeignServerDict and UserMappingDict derived from DbObjectDict,
    ForeignTable derived from DbObjectWithOptions and Table, and
    ForeignTableDict derived from ClassDict.
"""
from pyrseas.dbobject import DbObjectDict, DbObject
from pyrseas.dbobject import quote_id, commentable, ownable, grantable
from pyrseas.dbobject.table import ClassDict, Table
from pyrseas.dbobject.privileges import privileges_from_map


class DbObjectWithOptions(DbObject):
    """Helper class for database objects with OPTIONS clauses"""

    def options_clause(self):
        """Create the OPTIONS clause

        :param optdict: the dictionary of options
        :return: SQL OPTIONS clause
        """
        opts = []
        for opt in self.options:
            (nm, val) = opt.split('=', 1)
            opts.append("%s '%s'" % (nm, val))
        return "OPTIONS (%s)" % ', '.join(opts)

    def diff_options(self, newopts):
        """Compare options lists and generate SQL OPTIONS clause

        :newopts: list of new options
        :return: SQL OPTIONS clause

        Generate ([ADD|SET|DROP key 'value') clauses from two lists in the
        form of 'key=value' strings.
        """
        def to_dict(optlist):
            return dict(opt.split('=', 1) for opt in optlist)

        oldopts = {}
        if hasattr(self, 'options'):
            oldopts = to_dict(self.options)
        newopts = to_dict(newopts)
        clauses = []
        for key, val in list(newopts.items()):
            if key not in oldopts:
                clauses.append("%s '%s'" % (key, val))
            elif val != oldopts[key]:
                clauses.append("SET %s '%s'" % (key, val))
        for key, val in list(oldopts.items()):
            if key not in newopts:
                clauses.append("DROP %s" % key)
        return clauses and "OPTIONS (%s)" % ', '.join(clauses) or ''

    def diff_map(self, inobj):
        """Generate SQL to transform an existing object

        :param inobj: a YAML map defining the new object
        :return: list of SQL statements
        """
        stmts = super(DbObjectWithOptions, self).diff_map(inobj)
        newopts = []
        if hasattr(inobj, 'options'):
            newopts = inobj.options
        diff_opts = self.diff_options(newopts)
        if diff_opts:
            stmts.append("ALTER %s %s %s" % (
                self.objtype, self.identifier(), diff_opts))
        return stmts


class ForeignDataWrapper(DbObjectWithOptions):
    """A foreign data wrapper definition"""

    objtype = "FOREIGN DATA WRAPPER"
    single_extern_file = True

    @property
    def allprivs(self):
        return 'U'

    def to_map(self, no_owner, no_privs):
        """Convert wrappers and subsidiary objects to a YAML-suitable format

        :param no_owner: exclude object owner information
        :param no_privs: exclude privilege information
        :return: dictionary
        """
        wrapper = self._base_map(no_owner, no_privs)
        if hasattr(self, 'servers'):
            srvs = {}
            for srv in self.servers:
                srvs.update(self.servers[srv].to_map(no_owner, no_privs))
            wrapper.update(srvs)
            del wrapper['servers']
        return wrapper

    @commentable
    @grantable
    @ownable
    def create(self):
        """Return SQL statements to CREATE the data wrapper

        :return: SQL statements
        """
        clauses = []
        for fnc in ['validator', 'handler']:
            if hasattr(self, fnc):
                clauses.append("%s %s" % (fnc.upper(), getattr(self, fnc)))
        if hasattr(self, 'options'):
            clauses.append(self.options_clause())
        return ["CREATE FOREIGN DATA WRAPPER %s%s" % (
                quote_id(self.name),
                clauses and '\n    ' + ',\n    '.join(clauses) or '')]

    def diff_map(self, inwrapper):
        """Generate SQL to transform an existing wrapper

        :param inwrapper: a YAML map defining the new wrapper
        :return: list of SQL statements
        """
        stmts = super(ForeignDataWrapper, self).diff_map(inwrapper)
        if hasattr(inwrapper, 'owner'):
            if inwrapper.owner != self.owner:
                stmts.append(self.alter_owner(inwrapper.owner))
        stmts.append(self.diff_description(inwrapper))
        return stmts


QUERY_PRE91 = \
    """SELECT fdwname AS name, CASE WHEN fdwvalidator = 0 THEN NULL
                ELSE fdwvalidator::regproc END AS validator,
                fdwoptions AS options, rolname AS owner,
              array_to_string(fdwacl, ',') AS privileges,
              obj_description(w.oid, 'pg_foreign_data_wrapper') AS
                  description
       FROM pg_foreign_data_wrapper w
            JOIN pg_roles r ON (r.oid = fdwowner)
       ORDER BY fdwname"""


class ForeignDataWrapperDict(DbObjectDict):
    "The collection of foreign data wrappers in a database"

    cls = ForeignDataWrapper
    query = \
        """SELECT fdwname AS name, CASE WHEN fdwhandler = 0 THEN NULL
                      ELSE fdwhandler::regproc END AS handler,
                  CASE WHEN fdwvalidator = 0 THEN NULL
                      ELSE fdwvalidator::regproc END AS validator,
                  fdwoptions AS options, rolname AS owner,
                  array_to_string(fdwacl, ',') AS privileges,
                  obj_description(w.oid, 'pg_foreign_data_wrapper') AS
                      description
           FROM pg_foreign_data_wrapper w
                JOIN pg_roles r ON (r.oid = fdwowner)
           ORDER BY fdwname"""

    def _from_catalog(self):
        """Initialize the dictionary of wrappers by querying the catalogs"""
        if self.dbconn.version < 90100:
            self.query = QUERY_PRE91
        super(ForeignDataWrapperDict, self)._from_catalog()

    def from_map(self, inwrappers, newdb):
        """Initialize the dictionary of wrappers by examining the input map

        :param inwrappers: input YAML map defining the data wrappers
        :param newdb: collection of dictionaries defining the database
        """
        for key in inwrappers:
            if not key.startswith('foreign data wrapper '):
                raise KeyError("Unrecognized object type: %s" % key)
            fdw = key[21:]
            self[fdw] = wrapper = ForeignDataWrapper(name=fdw)
            inwrapper = inwrappers[key]
            inservs = {}
            for key in inwrapper:
                if key.startswith('server '):
                    inservs.update({key: inwrapper[key]})
                elif key in ['handler', 'validator', 'options', 'owner',
                             'oldname', 'description']:
                    setattr(wrapper, key, inwrapper[key])
                elif key == 'privileges':
                    wrapper.privileges = privileges_from_map(
                        inwrapper[key], wrapper.allprivs, inwrapper['owner'])
                else:
                    raise KeyError("Expected typed object, found '%s'" % key)
            newdb.servers.from_map(wrapper, inservs, newdb)

    def link_refs(self, dbservers):
        """Connect servers to their respective foreign data wrappers

        :param dbservers: dictionary of foreign servers
        """
        for (fdw, srv) in dbservers:
            dbserver = dbservers[(fdw, srv)]
            assert self[fdw]
            wrapper = self[fdw]
            if not hasattr(wrapper, 'servers'):
                wrapper.servers = {}
            wrapper.servers.update({srv: dbserver})

    def diff_map(self, inwrappers):
        """Generate SQL to transform existing data wrappers

        :param input_map: a YAML map defining the new data wrappers
        :return: list of SQL statements

        Compares the existing data wrapper definitions, as fetched from the
        catalogs, to the input map and generates SQL statements to
        transform the data wrappers accordingly.
        """
        stmts = []
        # check input data wrappers
        for fdw in inwrappers:
            infdw = inwrappers[fdw]
            # does it exist in the database?
            if fdw in self:
                stmts.append(self[fdw].diff_map(infdw))
            else:
                # check for possible RENAME
                if hasattr(infdw, 'oldname'):
                    oldname = infdw.oldname
                    try:
                        stmts.append(self[oldname].rename(infdw.name))
                        del self[oldname]
                    except KeyError as exc:
                        exc.args = ("Previous name '%s' for data wrapper "
                                    "'%s' not found" % (oldname, infdw.name), )
                        raise
                else:
                    # create new data wrapper
                    stmts.append(infdw.create())
        # check database data wrappers
        for fdw in self:
            # if missing, drop it
            if fdw not in inwrappers:
                self[fdw].dropped = True
        return stmts

    def _drop(self):
        """Actually drop the wrappers

        :return: SQL statements
        """
        stmts = []
        for fdw in self:
            if hasattr(self[fdw], 'dropped'):
                stmts.append(self[fdw].drop())
        return stmts


class ForeignServer(DbObjectWithOptions):
    """A foreign server definition"""

    objtype = "SERVER"
    privobjtype = "FOREIGN SERVER"
    keylist = ['wrapper', 'name']

    @property
    def allprivs(self):
        return 'U'

    def identifier(self):
        """Returns a full identifier for the foreign server

        :return: string
        """
        return quote_id(self.name)

    def to_map(self, no_owner, no_privs):
        """Convert servers and subsidiary objects to a YAML-suitable format

        :param no_owner: exclude server owner information
        :param no_privs: exclude privilege information
        :return: dictionary
        """
        key = self.extern_key()
        server = {key: self._base_map(no_owner, no_privs)}
        if hasattr(self, 'usermaps'):
            umaps = {}
            for umap in self.usermaps:
                umaps.update({umap: self.usermaps[umap].to_map()})
            server[key]['user mappings'] = umaps
            del server[key]['usermaps']
        return server

    @commentable
    @grantable
    @ownable
    def create(self):
        """Return SQL statements to CREATE the server

        :return: SQL statements
        """
        clauses = []
        options = []
        for opt in ['type', 'version']:
            if hasattr(self, opt):
                clauses.append("%s '%s'" % (opt.upper(), getattr(self, opt)))
        if hasattr(self, 'options'):
            options.append(self.options_clause())
        return ["CREATE SERVER %s%s\n    FOREIGN DATA WRAPPER %s%s" % (
                quote_id(self.name),
                clauses and ' ' + ' '.join(clauses) or '',
                quote_id(self.wrapper),
                options and '\n    ' + ',\n    '.join(options) or '')]

    def diff_map(self, inserver):
        """Generate SQL to transform an existing server

        :param inserver: a YAML map defining the new server
        :return: list of SQL statements
        """
        stmts = super(ForeignServer, self).diff_map(inserver)
        if hasattr(inserver, 'owner'):
            if inserver.owner != self.owner:
                stmts.append(self.alter_owner(inserver.owner))
        stmts.append(self.diff_description(inserver))
        return stmts


class ForeignServerDict(DbObjectDict):
    "The collection of foreign servers in a database"

    cls = ForeignServer
    query = \
        """SELECT fdwname AS wrapper, srvname AS name, srvtype AS type,
                  srvversion AS version, srvoptions AS options,
                  rolname AS owner, array_to_string(srvacl, ',') AS privileges,
                  obj_description(s.oid, 'pg_foreign_server') AS description
           FROM pg_foreign_server s
                JOIN pg_roles r ON (r.oid = srvowner)
                JOIN pg_foreign_data_wrapper w ON (srvfdw = w.oid)
           ORDER BY fdwname, srvname"""

    def from_map(self, wrapper, inservers, newdb):
        """Initialize the dictionary of servers by examining the input map

        :param wrapper: associated foreign data wrapper
        :param inservers: input YAML map defining the foreign servers
        :param newdb: collection of dictionaries defining the database
        """
        for key in inservers:
            if not key.startswith('server '):
                raise KeyError("Unrecognized object type: %s" % key)
            srv = key[7:]
            self[(wrapper.name, srv)] = serv = ForeignServer(
                wrapper=wrapper.name, name=srv)
            inserv = inservers[key]
            if inserv:
                for attr, val in list(inserv.items()):
                    setattr(serv, attr, val)
                if 'user mappings' in inserv:
                    newdb.usermaps.from_map(serv, inserv['user mappings'])
                if 'oldname' in inserv:
                    del inserv['oldname']
                if 'privileges' in inserv:
                    serv.privileges = privileges_from_map(
                        inserv['privileges'], serv.allprivs, serv.owner)

    def to_map(self, no_owner, no_privs):
        """Convert the server dictionary to a regular dictionary

        :param no_owner: exclude server owner information
        :param no_privs: exclude privilege information
        :return: dictionary

        Invokes the `to_map` method of each server to construct a
        dictionary of foreign servers.
        """
        servers = {}
        for srv in self:
            servers.update(self[srv].to_map(no_owner, no_privs))
        return servers

    def link_refs(self, dbusermaps):
        """Connect user mappings to their respective servers

        :param dbusermaps: dictionary of user mappings
        """
        for (fdw, srv, usr) in dbusermaps:
            dbusermap = dbusermaps[(fdw, srv, usr)]
            assert self[(fdw, srv)]
            server = self[(fdw, srv)]
            if not hasattr(server, 'usermaps'):
                server.usermaps = {}
            server.usermaps.update({usr: dbusermap})

    def diff_map(self, inservers):
        """Generate SQL to transform existing foreign servers

        :param inservers: a YAML map defining the new foreign servers
        :return: list of SQL statements

        Compares the existing server definitions, as fetched from the
        catalogs, to the input map and generates SQL statements to
        transform the foreign servers accordingly.
        """
        stmts = []
        # check input foreign servers
        for (fdw, srv) in inservers:
            insrv = inservers[(fdw, srv)]
            # does it exist in the database?
            if (fdw, srv) in self:
                stmts.append(self[(fdw, srv)].diff_map(insrv))
            else:
                # check for possible RENAME
                if hasattr(insrv, 'oldname'):
                    oldname = insrv.oldname
                    try:
                        stmts.append(self[(fdw, oldname)].rename(insrv.name))
                        del self[oldname]
                    except KeyError as exc:
                        exc.args = ("Previous name '%s' for dictionary '%s' "
                                    "not found" % (oldname, insrv.name), )
                        raise
                else:
                    # create new dictionary
                    stmts.append(insrv.create())
        # check database foreign servers
        for srv in self:
            # if missing, drop it
            if srv not in inservers:
                self[srv].dropped = True
        return stmts

    def _drop(self):
        """Actually drop the servers

        :return: SQL statements
        """
        stmts = []
        for srv in self:
            if hasattr(self[srv], 'dropped'):
                stmts.append(self[srv].drop())
        return stmts


class UserMapping(DbObjectWithOptions):
    """A user mapping definition"""

    objtype = "USER MAPPING"

    keylist = ['wrapper', 'server', 'username']

    def extern_key(self):
        """Return the key to be used in external maps for this user mapping

        :return: string
        """
        return self.username

    def identifier(self):
        """Return a full identifier for a user mapping object

        :return: string
        """
        return "FOR %s SERVER %s" % (
            self.username == 'PUBLIC' and 'PUBLIC' or quote_id(self.username),
            quote_id(self.server))

    def create(self):
        """Return SQL statements to CREATE the user mapping

        :return: SQL statements
        """
        options = []
        if hasattr(self, 'options'):
            options.append(self.options_clause())
        return ["CREATE USER MAPPING FOR %s\n    SERVER %s%s" % (
                self.username == 'PUBLIC' and 'PUBLIC' or
                quote_id(self.username), quote_id(self.server),
                options and '\n    ' + ',\n    '.join(options) or '')]


class UserMappingDict(DbObjectDict):
    "The collection of user mappings in a database"

    cls = UserMapping
    query = \
        """SELECT fdwname AS wrapper, s.srvname AS server,
                  CASE umuser WHEN 0 THEN 'PUBLIC' ELSE
                  usename END AS username, umoptions AS options
           FROM pg_user_mappings u
                JOIN pg_foreign_server s ON (u.srvid = s.oid)
                JOIN pg_foreign_data_wrapper w ON (srvfdw = w.oid)
           ORDER BY fdwname, s.srvname, 3"""

    def from_map(self, server, inusermaps):
        """Initialize the dictionary of mappings by examining the input map

        :param server: foreign server associated with mappings
        :param inusermaps: input YAML map defining the user mappings
        """
        for key in inusermaps:
            usermap = UserMapping(wrapper=server.wrapper, server=server.name,
                                  username=key)
            inusermap = inusermaps[key]
            if inusermap:
                for attr, val in list(inusermap.items()):
                    setattr(usermap, attr, val)
                if 'oldname' in inusermap:
                    del inusermap['oldname']
            self[(server.wrapper, server.name, key)] = usermap

    def to_map(self):
        """Convert the user mapping dictionary to a regular dictionary

        :return: dictionary

        Invokes the `to_map` method of each mapping to construct a
        dictionary of user mappings.
        """
        usermaps = {}
        for um in self:
            usermaps.update(self[um].to_map())
        return usermaps

    def diff_map(self, inusermaps):
        """Generate SQL to transform existing user mappings

        :param input_map: a YAML map defining the new user mappings
        :return: list of SQL statements

        Compares the existing user mapping definitions, as fetched from the
        catalogs, to the input map and generates SQL statements to
        transform the user mappings accordingly.
        """
        stmts = []
        # check input user mappings
        for (fdw, srv, usr) in inusermaps:
            inump = inusermaps[(fdw, srv, usr)]
            # does it exist in the database?
            if (fdw, srv, usr) in self:
                stmts.append(self[(fdw, srv, usr)].diff_map(inump))
            else:
                # check for possible RENAME
                if hasattr(inump, 'oldname'):
                    oldname = inump.oldname
                    try:
                        stmts.append(self[(fdw, srv, oldname)].rename(
                            inump.name))
                        del self[(fdw, srv, oldname)]
                    except KeyError as exc:
                        exc.args = ("Previous name '%s' for user mapping '%s' "
                                    "not found" % (oldname, inump.name), )
                        raise
                else:
                    # create new user mapping
                    stmts.append(inump.create())
        # check database user mappings
        for (fdw, srv, usr) in self:
            # if missing, drop it
            if (fdw, srv, usr) not in inusermaps:
                stmts.append(self[(fdw, srv, usr)].drop())
        return stmts


class ForeignTable(DbObjectWithOptions, Table):
    """A foreign table definition"""

    objtype = "FOREIGN TABLE"
    privobjtype = "TABLE"

    def to_map(self, opts):
        """Convert a foreign table to a YAML-suitable format

        :param opts: options to include/exclude tables, etc.
        :return: dictionary
        """
        if hasattr(opts, 'excl_tables') and opts.excl_tables \
                and self.name in opts.excl_tables:
            return {}
        if not hasattr(self, 'columns'):
            return {}
        cols = []
        for i in range(len(self.columns)):
            col = self.columns[i].to_map(opts.no_privs)
            if col:
                cols.append(col)
        tbl = {'columns': cols, 'server': self.server}
        attrlist = ['options', 'description']
        if not opts.no_owner:
            attrlist.append('owner')
        for attr in attrlist:
            if hasattr(self, attr):
                tbl.update({attr: getattr(self, attr)})
        if not opts.no_privs and hasattr(self, 'privileges'):
            tbl.update({'privileges': self.map_privs()})

        return tbl

    @grantable
    def create(self):
        """Return SQL statements to CREATE the foreign table

        :return: SQL statements
        """
        stmts = []
        cols = []
        options = []
        for col in self.columns:
            cols.append("    " + col.add()[0])
        if hasattr(self, 'options'):
            options.append(self.options_clause())
        stmts.append("CREATE FOREIGN TABLE %s (\n%s)\n    SERVER %s%s" % (
            self.qualname(), ",\n".join(cols), self.server,
            options and '\n    ' + ',\n    '.join(options) or ''))
        if hasattr(self, 'owner'):
            stmts.append(self.alter_owner())
        if hasattr(self, 'description'):
            stmts.append(self.comment())
        for col in self.columns:
            if hasattr(col, 'description'):
                stmts.append(col.comment())
        return stmts

    def drop(self):
        """Return a SQL DROP statement for the foreign table

        :return: SQL statement
        """
        return "DROP %s %s" % (self.objtype, self.identifier())

    def diff_map(self, intable):
        """Generate SQL to transform an existing table

        :param intable: a YAML map defining the new table
        :return: list of SQL statements
        """
        stmts = super(ForeignTable, self).diff_map(intable)
        if hasattr(intable, 'owner'):
            if intable.owner != self.owner:
                stmts.append(self.alter_owner(intable.owner))
        stmts.append(self.diff_description(intable))
        return stmts


class ForeignTableDict(ClassDict):
    "The collection of foreign tables in a database"

    cls = ForeignTable
    query = \
        """SELECT nspname AS schema, relname AS name, srvname AS server,
                  ftoptions AS options, rolname AS owner,
                  array_to_string(relacl, ',') AS privileges,
                  obj_description(c.oid, 'pg_class') AS description
           FROM pg_class c JOIN pg_foreign_table f ON (ftrelid = c.oid)
                JOIN pg_roles r ON (r.oid = relowner)
                JOIN pg_foreign_server s ON (ftserver = s.oid)
                JOIN pg_namespace ON (relnamespace = pg_namespace.oid)
           WHERE relkind = 'f'
                 AND (nspname != 'pg_catalog'
                      AND nspname != 'information_schema')
           ORDER BY nspname, relname"""

    def _from_catalog(self):
        """Initialize the dictionary of tables by querying the catalogs"""
        if self.dbconn.version < 90100:
            return
        for tbl in self.fetch():
            if hasattr(tbl, 'privileges'):
                tbl.privileges = tbl.privileges.split(',')
            self[tbl.key()] = tbl

    def from_map(self, schema, inobjs, newdb):
        """Initalize the dictionary of tables by converting the input map

        :param schema: schema owning the tables
        :param inobjs: YAML map defining the schema objects
        :param newdb: collection of dictionaries defining the database
        """
        for key in inobjs:
            if not key.startswith('foreign table '):
                raise KeyError("Unrecognized object type: %s" % key)
            ftb = key[14:]
            self[(schema.name, ftb)] = ftable = ForeignTable(
                schema=schema.name, name=ftb)
            inftable = inobjs[key]
            if not inftable:
                raise ValueError("Foreign table '%s' has no specification" %
                                 ftb)
            try:
                newdb.columns.from_map(ftable, inftable['columns'])
            except KeyError as exc:
                exc.args = ("Foreign table '%s' has no columns" % ftb, )
                raise
            for attr in ['server', 'options', 'owner', 'description']:
                if attr in inftable:
                    setattr(ftable, attr, inftable[attr])
            if 'privileges' in inftable:
                ftable.privileges = privileges_from_map(
                    inftable['privileges'], ftable.allprivs, ftable.owner)

    def link_refs(self, dbcolumns):
        """Connect columns to their respective foreign tables

        :param dbcolumns: dictionary of columns
        """
        for (sch, tbl) in dbcolumns:
            if (sch, tbl) in self:
                assert isinstance(self[(sch, tbl)], ForeignTable)
                self[(sch, tbl)].columns = dbcolumns[(sch, tbl)]
                for col in dbcolumns[(sch, tbl)]:
                    col._table = self[(sch, tbl)]

    def diff_map(self, intables):
        """Generate SQL to transform existing foreign tables

        :param intables: a YAML map defining the new foreign tables
        :return: list of SQL statements

        Compares the existing foreign table definitions, as fetched
        from the catalogs, to the input map and generates SQL
        statements to transform the foreign tables accordingly.
        """
        stmts = []
        # check input tables
        for (sch, tbl) in intables:
            intbl = intables[(sch, tbl)]
            # does it exist in the database?
            if (sch, tbl) not in self:
                # check for possible RENAME
                if hasattr(intbl, 'oldname'):
                    oldname = intbl.oldname
                    try:
                        stmts.append(self[(sch, oldname)].rename(intbl.name))
                        del self[(sch, oldname)]
                    except KeyError as exc:
                        exc.args = ("Previous name '%s' for foreign table "
                                    "'%s' not found" % (oldname, intbl.name), )
                        raise
                else:
                    # create new table
                    stmts.append(intbl.create())

        # check database tables
        for (sch, tbl) in self:
            table = self[(sch, tbl)]
            # if missing, drop it
            if (sch, tbl) not in intables:
                stmts.append(table.drop())
            else:
                # compare table objects
                stmts.append(table.diff_map(intables[(sch, tbl)]))

        return stmts

########NEW FILE########
__FILENAME__ = function
# -*- coding: utf-8 -*-
"""
    pyrseas.dbobject.function
    ~~~~~~~~~~~~~~~~~~~~~~~~~

    This module defines four classes: Proc derived from
    DbSchemaObject, Function and Aggregate derived from Proc, and
    FunctionDict derived from DbObjectDict.
"""
from pyrseas.lib.pycompat import strtypes
from pyrseas.dbobject import DbObjectDict, DbSchemaObject
from pyrseas.dbobject import commentable, ownable, grantable, split_schema_obj
from pyrseas.dbobject.privileges import privileges_from_map

VOLATILITY_TYPES = {'i': 'immutable', 's': 'stable', 'v': 'volatile'}


class Proc(DbSchemaObject):
    """A procedure such as a FUNCTION or an AGGREGATE"""

    keylist = ['schema', 'name', 'arguments']

    @property
    def allprivs(self):
        return 'X'

    def extern_key(self):
        """Return the key to be used in external maps for this function

        :return: string
        """
        return '%s %s(%s)' % (self.objtype.lower(), self.name, self.arguments)

    def identifier(self):
        """Return a full identifier for a function object

        :return: string
        """
        return "%s(%s)" % (self.qualname(), self.arguments)


class Function(Proc):
    """A procedural language function"""

    objtype = "FUNCTION"

    def to_map(self, no_owner, no_privs):
        """Convert a function to a YAML-suitable format

        :param no_owner: exclude function owner information
        :param no_privs: exclude privilege information
        :return: dictionary
        """
        dct = self._base_map(no_owner)
        if self.volatility == 'v':
            del dct['volatility']
        else:
            dct['volatility'] = VOLATILITY_TYPES[self.volatility]
        if hasattr(self, 'dependent_table'):
            del dct['dependent_table']
        if hasattr(self, 'obj_file'):
            dct['link_symbol'] = self.source
            del dct['source']
        if hasattr(self, '_dep_type'):
            del dct['_dep_type']
        if hasattr(self, 'cost') and self.cost != 0:
            if self.language in ['c', 'internal']:
                if self.cost == 1:
                    del dct['cost']
            else:
                if self.cost == 100:
                    del dct['cost']
        if hasattr(self, 'rows') and self.rows != 0:
            if self.rows == 1000:
                del dct['rows']
        if hasattr(self, 'privileges'):
            if no_privs:
                del dct['privileges']
            else:
                dct['privileges'] = self.map_privs()

        return dct

    @commentable
    @grantable
    @ownable
    def create(self, newsrc=None, basetype=False):
        """Return SQL statements to CREATE or REPLACE the function

        :param newsrc: new source for a changed function
        :return: SQL statements
        """
        stmts = []
        if hasattr(self, '_dep_type') and not basetype:
            return stmts
        if hasattr(self, 'dependent_table'):
            stmts.append(self.dependent_table.create())
        if hasattr(self, 'obj_file'):
            src = "'%s', '%s'" % (self.obj_file,
                                  hasattr(self, 'link_symbol')
                                  and self.link_symbol or self.name)
        elif self.language == 'internal':
            src = "$$%s$$" % (newsrc or self.source)
        else:
            src = "$_$%s$_$" % (newsrc or self.source)
        volat = leakproof = strict = secdef = cost = rows = config = ''
        if hasattr(self, 'volatility'):
            volat = ' ' + VOLATILITY_TYPES[self.volatility].upper()
        if hasattr(self, 'leakproof') and self.leakproof is True:
            leakproof = ' LEAKPROOF'
        if hasattr(self, 'strict') and self.strict:
            strict = ' STRICT'
        if hasattr(self, 'security_definer') and self.security_definer:
            secdef = ' SECURITY DEFINER'
        if hasattr(self, 'configuration'):
            config = ' SET %s' % self.configuration[0]
        if hasattr(self, 'cost') and self.cost != 0:
            if self.language in ['c', 'internal']:
                if self.cost != 1:
                    cost = " COST %s" % self.cost
            else:
                if self.cost != 100:
                    cost = " COST %s" % self.cost
        if hasattr(self, 'rows') and self.rows != 0:
            if self.rows != 1000:
                rows = " ROWS %s" % self.rows

        args = self.allargs if hasattr(self, 'allargs') else self.arguments
        stmts.append("CREATE%s FUNCTION %s(%s) RETURNS %s\n    LANGUAGE %s"
                     "%s%s%s%s%s%s%s\n    AS %s" % (
                     newsrc and " OR REPLACE" or '', self.qualname(),
                     args, self.returns, self.language, volat, leakproof,
                     strict, secdef, cost, rows, config, src))
        return stmts

    def diff_map(self, infunction):
        """Generate SQL to transform an existing function

        :param infunction: a YAML map defining the new function
        :return: list of SQL statements

        Compares the function to an input function and generates SQL
        statements to transform it into the one represented by the
        input.
        """
        stmts = []
        if hasattr(self, 'source') and hasattr(infunction, 'source'):
            if self.source != infunction.source:
                stmts.append(self.create(infunction.source))
        if hasattr(infunction, 'owner'):
            if infunction.owner != self.owner:
                stmts.append(self.alter_owner(infunction.owner))
        if hasattr(self, 'leakproof') and self.leakproof is True:
            if hasattr(infunction, 'leakproof') and \
                    infunction.leakproof is True:
                stmts.append("ALTER FUNCTION %s LEAKPROOF" % self.identifier())
            else:
                stmts.append("ALTER FUNCTION %s NOT LEAKPROOF"
                             % self.identifier())
        elif hasattr(infunction, 'leakproof') and infunction.leakproof is True:
            stmts.append("ALTER FUNCTION %s LEAKPROOF" % self.qualname())
        stmts.append(self.diff_privileges(infunction))
        stmts.append(self.diff_description(infunction))
        return stmts


class Aggregate(Proc):
    """An aggregate function"""

    objtype = "AGGREGATE"

    def to_map(self, no_owner, no_privs):
        """Convert an agggregate to a YAML-suitable format

        :param no_owner: exclude aggregate owner information
        :param no_privs: exclude privilege information
        :return: dictionary
        """
        dct = self._base_map(no_owner, no_privs)
        del dct['language']
        return dct

    @commentable
    @grantable
    @ownable
    def create(self):
        """Return SQL statements to CREATE the aggregate

        :return: SQL statements
        """
        opt_clauses = []
        if hasattr(self, 'finalfunc'):
            opt_clauses.append("FINALFUNC = %s" % self.finalfunc)
        if hasattr(self, 'initcond'):
            opt_clauses.append("INITCOND = '%s'" % self.initcond)
        if hasattr(self, 'sortop'):
            opt_clauses.append("SORTOP = %s" % self.sortop)
        return ["CREATE AGGREGATE %s(%s) (\n    SFUNC = %s,"
                "\n    STYPE = %s%s%s)" % (
                self.qualname(),
                self.arguments, self.sfunc, self.stype,
                opt_clauses and ',\n    ' or '', ',\n    '.join(opt_clauses))]

QUERY_PRE92 = \
    """SELECT nspname AS schema, proname AS name,
              pg_get_function_identity_arguments(p.oid) AS arguments,
              pg_get_function_arguments(p.oid) AS allargs,
              pg_get_function_result(p.oid) AS returns,
              rolname AS owner, array_to_string(proacl, ',') AS privileges,
              l.lanname AS language, provolatile AS volatility,
              proisstrict AS strict, proisagg, prosrc AS source,
              probin::text AS obj_file, proconfig AS configuration,
              prosecdef AS security_definer, procost AS cost,
              aggtransfn::regproc AS sfunc, aggtranstype::regtype AS stype,
              aggfinalfn::regproc AS finalfunc,
              agginitval AS initcond, aggsortop::regoper AS sortop,
              obj_description(p.oid, 'pg_proc') AS description,
              prorows::integer AS rows
       FROM pg_proc p
            JOIN pg_roles r ON (r.oid = proowner)
            JOIN pg_namespace n ON (pronamespace = n.oid)
            JOIN pg_language l ON (prolang = l.oid)
            LEFT JOIN pg_aggregate a ON (p.oid = aggfnoid)
       WHERE (nspname != 'pg_catalog' AND nspname != 'information_schema')
         AND p.oid NOT IN (
             SELECT objid FROM pg_depend WHERE deptype = 'e'
                          AND classid = 'pg_proc'::regclass)
       ORDER BY nspname, proname"""


class ProcDict(DbObjectDict):
    "The collection of regular and aggregate functions in a database"

    cls = Proc
    query = \
        """SELECT nspname AS schema, proname AS name,
                  pg_get_function_identity_arguments(p.oid) AS arguments,
                  pg_get_function_arguments(p.oid) AS allargs,
                  pg_get_function_result(p.oid) AS returns,
                  rolname AS owner, array_to_string(proacl, ',') AS privileges,
                  l.lanname AS language, provolatile AS volatility,
                  proisstrict AS strict, proisagg, prosrc AS source,
                  probin::text AS obj_file, proconfig AS configuration,
                  prosecdef AS security_definer, procost AS cost,
                  proleakproof AS leakproof,
                  aggtransfn::regproc AS sfunc, aggtranstype::regtype AS stype,
                  aggfinalfn::regproc AS finalfunc,
                  agginitval AS initcond, aggsortop::regoper AS sortop,
                  obj_description(p.oid, 'pg_proc') AS description,
                  prorows::integer AS rows
           FROM pg_proc p
                JOIN pg_roles r ON (r.oid = proowner)
                JOIN pg_namespace n ON (pronamespace = n.oid)
                JOIN pg_language l ON (prolang = l.oid)
                LEFT JOIN pg_aggregate a ON (p.oid = aggfnoid)
           WHERE (nspname != 'pg_catalog' AND nspname != 'information_schema')
             AND p.oid NOT IN (
                 SELECT objid FROM pg_depend WHERE deptype = 'e'
                              AND classid = 'pg_proc'::regclass)
           ORDER BY nspname, proname"""

    def _from_catalog(self):
        """Initialize the dictionary of procedures by querying the catalogs"""
        if self.dbconn.version < 90200:
            self.query = QUERY_PRE92
        for proc in self.fetch():
            if hasattr(proc, 'privileges'):
                proc.privileges = proc.privileges.split(',')
            sch, prc, arg = proc.key()
            if hasattr(proc, 'allargs') and proc.allargs == proc.arguments:
                del proc.allargs
            if hasattr(proc, 'proisagg'):
                del proc.proisagg
                del proc.source
                del proc.volatility
                del proc.returns
                del proc.cost
                if proc.finalfunc == '-':
                    del proc.finalfunc
                if proc.sortop == '0':
                    del proc.sortop
                self[(sch, prc, arg)] = Aggregate(**proc.__dict__)
            else:
                self[(sch, prc, arg)] = Function(**proc.__dict__)

    def from_map(self, schema, infuncs):
        """Initalize the dictionary of functions by converting the input map

        :param schema: schema owning the functions
        :param infuncs: YAML map defining the functions
        """
        for key in infuncs:
            (objtype, spc, fnc) = key.partition(' ')
            if spc != ' ' or objtype not in ['function', 'aggregate']:
                raise KeyError("Unrecognized object type: %s" % key)
            paren = fnc.find('(')
            if paren == -1 or fnc[-1:] != ')':
                raise KeyError("Invalid function signature: %s" % fnc)
            arguments = fnc[paren + 1:-1]
            infunc = infuncs[key]
            fnc = fnc[:paren]
            if objtype == 'function':
                self[(schema.name, fnc, arguments)] = func = Function(
                    schema=schema.name, name=fnc, arguments=arguments)
            else:
                self[(schema.name, fnc, arguments)] = func = Aggregate(
                    schema=schema.name, name=fnc, arguments=arguments)
                func.language = 'internal'
            if not infunc:
                raise ValueError("Function '%s' has no specification" % fnc)
            for attr in infunc:
                setattr(func, attr, infunc[attr])
            if hasattr(func, 'volatility'):
                func.volatility = func.volatility[:1].lower()
            if isinstance(func, Function):
                src = hasattr(func, 'source')
                obj = hasattr(func, 'obj_file')
                if (src and obj) or not (src or obj):
                    raise ValueError("Function '%s': either source or "
                                     "obj_file must be specified" % fnc)
            if 'privileges' in infunc:
                func.privileges = privileges_from_map(
                    infunc['privileges'], func.allprivs, func.owner)

    def link_refs(self, dbeventtrigs):
        """Connect event triggers to the functions executed

        :param dbeventtrigs: dictionary of event triggers

        Fills in the `event_triggers` list for each function by
        traversing the `dbeventtrigs` dictionary.
        """
        for key in dbeventtrigs:
            evttrg = dbeventtrigs[key]
            (sch, fnc) = split_schema_obj(evttrg.procedure)
            func = self[(sch, fnc[:-2], '')]
            if not hasattr(func, 'event_triggers'):
                func.event_triggers = []
            func.event_triggers.append(evttrg.name)

    def diff_map(self, infuncs):
        """Generate SQL to transform existing functions

        :param infuncs: a YAML map defining the new functions
        :return: list of SQL statements

        Compares the existing function definitions, as fetched from
        the catalogs, to the input map and generates SQL statements to
        transform the functions accordingly.
        """
        stmts = []
        created = False
        # check input functions
        for (sch, fnc, arg) in infuncs:
            infunc = infuncs[(sch, fnc, arg)]
            if isinstance(infunc, Aggregate):
                continue
            # does it exist in the database?
            if (sch, fnc, arg) not in self:
                if not hasattr(infunc, 'oldname'):
                    # create new function
                    stmts.append(infunc.create())
                    created = True
                else:
                    stmts.append(self[(sch, fnc, arg)].rename(infunc))
            else:
                # check function objects
                diff_stmts = self[(sch, fnc, arg)].diff_map(infunc)
                for stmt in diff_stmts:
                    if isinstance(stmt, list) and stmt:
                        stmt = stmt[0]
                    if isinstance(stmt, strtypes) and \
                            stmt.startswith("CREATE "):
                        created = True
                        break
                stmts.append(diff_stmts)

        # check input aggregates
        for (sch, fnc, arg) in infuncs:
            infunc = infuncs[(sch, fnc, arg)]
            if not isinstance(infunc, Aggregate):
                continue
            # does it exist in the database?
            if (sch, fnc, arg) not in self:
                if not hasattr(infunc, 'oldname'):
                    # create new function
                    stmts.append(infunc.create())
                else:
                    stmts.append(self[(sch, fnc, arg)].rename(infunc))
            else:
                # check function objects
                stmts.append(self[(sch, fnc, arg)].diff_map(infunc))

        # check existing functions
        for (sch, fnc, arg) in self:
            func = self[(sch, fnc, arg)]
            # if missing, mark it for dropping
            if (sch, fnc, arg) not in infuncs:
                func.dropped = False

        if created:
            stmts.insert(0, "SET check_function_bodies = false")
        return stmts

    def _drop(self):
        """Actually drop the functions

        :return: SQL statements
        """
        stmts = []
        for (sch, fnc, arg) in self:
            func = self[(sch, fnc, arg)]
            if isinstance(func, Aggregate) and hasattr(func, 'dropped'):
                stmts.append(func.drop())

        for (sch, fnc, arg) in self:
            func = self[(sch, fnc, arg)]
            if hasattr(func, 'dropped') and not hasattr(func, '_dep_type'):
                stmts.append(func.drop())

        return stmts

########NEW FILE########
__FILENAME__ = index
# -*- coding: utf-8 -*-
"""
    pyrseas.dbobject.index
    ~~~~~~~~~~~~~~~~~~~~~~

    This defines two classes, Index and IndexDict, derived
    from DbSchemaObject and DbObjectDict, respectively.
"""
from pyrseas.dbobject import DbObjectDict, DbSchemaObject
from pyrseas.dbobject import quote_id, split_schema_obj, commentable


def split_exprs(idx_exprs):
    "Helper function to split index expressions from pg_get_expr()"
    keyexprs = []
    nopen = nclose = beg = curr = 0
    for c in idx_exprs:
        curr += 1
        if c == '(':
            nopen += 1
        elif c == ')':
            nclose += 1
            if nopen > 0 and nopen == nclose:
                if idx_exprs[beg] == ',':
                    beg += 1
                if idx_exprs[beg] == ' ':
                    beg += 1
                keyexprs.append(idx_exprs[beg:curr])
                beg = curr
                nopen = nclose = 0
    return keyexprs


class Index(DbSchemaObject):
    """A physical index definition, other than a primary key or unique
    constraint index.
    """

    keylist = ['schema', 'table', 'name']
    objtype = "INDEX"

    def key_expressions(self):
        """Return comma-separated list of key column names and qualifiers

        :return: string
        """
        colspec = []
        for col in self.keys:
            if isinstance(col, str):
                colspec.append(col)
            else:
                clause = list(col.keys())[0]
                vals = list(col.values())[0]
                if 'collation' in vals:
                    clause += ' COLLATE ' + vals['collation']
                if 'opclass' in vals:
                    clause += ' ' + vals['opclass']
                if 'order' in vals:
                    clause += ' ' + vals['order'].upper()
                if 'nulls' in vals:
                    clause += ' NULLS ' + vals['nulls'].upper()
                colspec.append(clause)
        return ", ".join(colspec)

    def to_map(self):
        """Convert an index definition to a YAML-suitable format

        :return: dictionary
        """
        dct = self._base_map()
        if dct['access_method'] == 'btree':
            del dct['access_method']
        return {self.name: dct}

    @commentable
    def create(self):
        """Return a SQL statement to CREATE the index

        :return: SQL statements
        """
        stmts = []
        unq = hasattr(self, 'unique') and self.unique
        acc = ''
        if hasattr(self, 'access_method') and self.access_method != 'btree':
            acc = 'USING %s ' % self.access_method
        tblspc = ''
        if hasattr(self, 'tablespace'):
            tblspc = '\n    TABLESPACE %s' % self.tablespace
        pred = ''
        if hasattr(self, 'predicate'):
            pred = '\n    WHERE %s' % self.predicate
        stmts.append("CREATE %sINDEX %s ON %s %s(%s)%s%s" % (
            'UNIQUE ' if unq else '', quote_id(self.name),
            self.qualname(self.table), acc, self.key_expressions(), tblspc,
            pred))
        if hasattr(self, 'cluster') and self.cluster:
            stmts.append("CLUSTER %s USING %s" % (
                quote_id(self.table), quote_id(self.name)))
        return stmts

    def diff_map(self, inindex):
        """Generate SQL to transform an existing index

        :param inindex: a YAML map defining the new index
        :return: list of SQL statements

        Compares the index to an input index and generates SQL
        statements to transform it into the one represented by the
        input.
        """
        stmts = []
        if not hasattr(self, 'unique'):
            self.unique = False
        if self.access_method != inindex.access_method \
                or self.unique != inindex.unique:
            stmts.append("DROP INDEX %s" % self.qualname())
            self.access_method = inindex.access_method
            self.unique = inindex.unique
            stmts.append(self.create())
        # TODO: need to deal with changes in keycols

        base = "ALTER INDEX %s\n    " % self.qualname()
        if hasattr(inindex, 'tablespace'):
            if not hasattr(self, 'tablespace') \
                    or self.tablespace != inindex.tablespace:
                stmts.append(base + "SET TABLESPACE %s"
                             % quote_id(inindex.tablespace))
        elif hasattr(self, 'tablespace'):
            stmts.append(base + "SET TABLESPACE pg_default")
        if hasattr(inindex, 'cluster'):
            if not hasattr(self, 'cluster'):
                stmts.append("CLUSTER %s USING %s" % (
                    quote_id(self.table), quote_id(self.name)))
        elif hasattr(self, 'cluster'):
            stmts.append("ALTER TABLE %s\n    SET WITHOUT CLUSTER" %
                         quote_id(self.table))
        stmts.append(self.diff_description(inindex))
        return stmts


class IndexDict(DbObjectDict):
    "The collection of indexes on tables in a database"

    cls = Index
    query = \
        """SELECT nspname AS schema, indrelid::regclass AS table,
                  c.relname AS name, amname AS access_method,
                  indisunique AS unique, indkey AS keycols,
                  pg_get_expr(indexprs, indrelid) AS keyexprs,
                  pg_get_expr(indpred, indrelid) AS predicate,
                  pg_get_indexdef(indexrelid) AS defn,
                  spcname AS tablespace, indisclustered AS cluster,
                  obj_description (c.oid, 'pg_class') AS description
           FROM pg_index JOIN pg_class c ON (indexrelid = c.oid)
                JOIN pg_namespace ON (relnamespace = pg_namespace.oid)
                JOIN pg_am ON (relam = pg_am.oid)
                LEFT JOIN pg_tablespace t ON (c.reltablespace = t.oid)
           WHERE NOT indisprimary
                 AND (nspname != 'pg_catalog'
                      AND nspname != 'information_schema')
                 AND c.relname NOT IN (
                     SELECT conname FROM pg_constraint
                     WHERE contype = 'u')
           ORDER BY schema, 2, name"""

    def _from_catalog(self):
        """Initialize the dictionary of indexes by querying the catalogs"""
        for index in self.fetch():
            index.unqualify()
            sch, tbl, idx = index.key()
            sch, tbl = split_schema_obj('%s.%s' % (sch, tbl))
            keydefs, _, _ = index.defn.partition(' WHERE ')
            _, _, keydefs = keydefs.partition(' USING ')
            keydefs = keydefs[keydefs.find(' (') + 2:-1]
            # split expressions (result of pg_get_expr)
            if hasattr(index, 'keyexprs'):
                keyexprs = split_exprs(index.keyexprs)
                del index.keyexprs
            # parse the keys
            i = 0
            rest = keydefs
            index.keys = []
            for col in index.keycols.split():
                keyopts = []
                extra = {}
                if col == '0':
                    expr = keyexprs[i]
                    if rest and rest[0] == '(':
                        expr = '(' + expr + ')'
                    assert(rest.startswith(expr))
                    key = expr
                    extra = {'type': 'expression'}
                    explen = len(expr)
                    loc = rest[explen:].find(',')
                    if loc == 0:
                        keyopts = []
                        rest = rest[explen + 1:].lstrip()
                    elif loc == -1:
                        keyopts = rest[explen:].split()
                        rest = ''
                    else:
                        keyopts = rest[explen:explen + loc].split()
                        rest = rest[explen + loc + 1:].lstrip()
                    i += 1
                else:
                    loc = rest.find(',')
                    key = rest[:loc] if loc != -1 else rest.lstrip()
                    keyopts = key.split()[1:]
                    key = key.split()[0]
                    rest = rest[loc + 1:]
                rest = rest.lstrip()
                skipnext = False
                for j, opt in enumerate(keyopts):
                    if skipnext:
                        skipnext = False
                        continue
                    if opt.upper() not in ['COLLATE', 'ASC', 'DESC', 'NULLS',
                                           'FIRST', 'LAST']:
                        extra.update(opclass=opt)
                        continue
                    elif opt == 'COLLATE':
                        extra.update(collation=keyopts[j + 1])
                        skipnext = True
                    elif opt == 'NULLS':
                        extra.update(nulls=keyopts[j + 1].lower())
                        skipnext = True
                    elif opt == 'DESC':
                        extra.update(order='desc')
                if extra:
                    key = {key: extra}
                index.keys.append(key)
            del index.defn, index.keycols
            self[(sch, tbl, idx)] = index

    def from_map(self, table, inindexes):
        """Initialize the dictionary of indexes by converting the input map

        :param table: table owning the indexes
        :param inindexes: YAML map defining the indexes
        """
        for i in inindexes:
            idx = Index(schema=table.schema, table=table.name, name=i)
            val = inindexes[i]
            if 'keys' in val:
                idx.keys = val['keys']
            elif 'columns' in val:
                idx.keys = val['columns']
            else:
                raise KeyError("Index '%s' is missing keys specification" % i)
            for attr in ['access_method', 'unique', 'tablespace', 'predicate',
                         'cluster']:
                if attr in val:
                    setattr(idx, attr, val[attr])
            if not hasattr(idx, 'access_method'):
                idx.access_method = 'btree'
            if not hasattr(idx, 'unique'):
                idx.unique = False
            if 'description' in val:
                idx.description = val['description']
            if 'oldname' in val:
                idx.oldname = val['oldname']
            self[(table.schema, table.name, i)] = idx

    def diff_map(self, inindexes):
        """Generate SQL to transform existing indexes

        :param inindexes: a YAML map defining the new indexes
        :return: list of SQL statements

        Compares the existing index definitions, as fetched from the
        catalogs, to the input map and generates SQL statements to
        transform the indexes accordingly.
        """
        stmts = []
        # check input indexes
        for (sch, tbl, idx) in inindexes:
            inidx = inindexes[(sch, tbl, idx)]
            # does it exist in the database?
            if (sch, tbl, idx) not in self:
                # check for possible RENAME
                if hasattr(inidx, 'oldname'):
                    oldname = inidx.oldname
                    try:
                        stmts.append(self[(sch, tbl, oldname)].rename(
                            inidx.name))
                        del self[(sch, tbl, oldname)]
                    except KeyError as exc:
                        exc.args = ("Previous name '%s' for index '%s' "
                                    "not found" % (oldname, inidx.name), )
                        raise
                else:
                    # create new index
                    stmts.append(inidx.create())

        # check database indexes
        for (sch, tbl, idx) in self:
            index = self[(sch, tbl, idx)]
            # if missing, drop it
            if (sch, tbl, idx) not in inindexes:
                stmts.append(index.drop())
            else:
                # compare index objects
                stmts.append(index.diff_map(inindexes[(sch, tbl, idx)]))

        return stmts

########NEW FILE########
__FILENAME__ = language
# -*- coding: utf-8 -*-
"""
    pyrseas.dbobject.language
    ~~~~~~~~~~~~~~~~~~~~~~~~~

    This defines two classes, Language and LanguageDict, derived from
    DbObject and DbObjectDict, respectively.
"""
from pyrseas.dbobject import DbObjectDict, DbObject, quote_id
from pyrseas.dbobject.function import Function


class Language(DbObject):
    """A procedural language definition"""

    keylist = ['name']
    objtype = "LANGUAGE"
    single_extern_file = True

    def to_map(self, no_owner, no_privs):
        """Convert language to a YAML-suitable format

        :param no_owner: exclude language owner information
        :return: dictionary
        """
        if hasattr(self, '_ext'):
            return None
        dct = self._base_map(no_owner, no_privs)
        if 'functions' in dct:
            del dct['functions']
        return dct

    def create(self):
        """Return SQL statements to CREATE the language

        :return: SQL statements
        """
        stmts = []
        if not hasattr(self, '_ext'):
            stmts.append("CREATE LANGUAGE %s" % quote_id(self.name))
            if hasattr(self, 'owner'):
                stmts.append(self.alter_owner())
            if hasattr(self, 'description'):
                stmts.append(self.comment())
        return stmts


QUERY_PRE91 = \
        """SELECT lanname AS name, lanpltrusted AS trusted,
                  rolname AS owner, array_to_string(lanacl, ',') AS privileges,
                  obj_description(l.oid, 'pg_language') AS description
           FROM pg_language l
                JOIN pg_roles r ON (r.oid = lanowner)
           WHERE lanispl
           ORDER BY lanname"""


class LanguageDict(DbObjectDict):
    "The collection of procedural languages in a database."

    cls = Language
    query = \
        """SELECT lanname AS name, lanpltrusted AS trusted,
                  rolname AS owner, array_to_string(lanacl, ',') AS privileges,
                  obj_description(l.oid, 'pg_language') AS description
           FROM pg_language l
                JOIN pg_roles r ON (r.oid = lanowner)
           WHERE lanispl
             AND lanname NOT IN (SELECT tmplname FROM pg_pltemplate)
           ORDER BY lanname"""

    def _from_catalog(self):
        """Initialize the dictionary of languages by querying the catalogs"""
        if self.dbconn.version < 90100:
            self.query = QUERY_PRE91
        super(LanguageDict, self)._from_catalog()

    def from_map(self, inmap):
        """Initialize the dictionary of languages by examining the input map

        :param inmap: the input YAML map defining the languages
        """
        for key in inmap:
            (objtype, spc, lng) = key.partition(' ')
            if spc != ' ' or objtype != 'language':
                raise KeyError("Unrecognized object type: %s" % key)
            language = self[lng] = Language(name=lng)
            inlanguage = inmap[key]
            if inlanguage:
                for attr, val in list(inlanguage.items()):
                    setattr(language, attr, val)
                if 'oldname' in inlanguage:
                    del inlanguage['oldname']

    def link_refs(self, dbfunctions):
        """Connect functions to their respective languages

        :param dbfunctions: dictionary of functions

        Fills in the `functions` dictionary for each language by
        traversing the `dbfunctions` dictionary, which is keyed by
        schema and function name.
        """
        for (sch, fnc, arg) in dbfunctions:
            func = dbfunctions[(sch, fnc, arg)]
            if func.language in ['sql', 'c', 'internal']:
                continue
            try:
                language = self[(func.language)]
            except KeyError as exc:
                if func.language == 'plpgsql':
                    continue
                raise exc
            if isinstance(func, Function):
                if not hasattr(language, 'functions'):
                    language.functions = {}
                language.functions.update({fnc: func})

    def diff_map(self, inlanguages):
        """Generate SQL to transform existing languages

        :param input_map: a YAML map defining the new languages
        :return: list of SQL statements

        Compares the existing language definitions, as fetched from the
        catalogs, to the input map and generates SQL statements to
        transform the languages accordingly.
        """
        stmts = []
        # check input languages
        for lng in inlanguages:
            inlng = inlanguages[lng]
            # does it exist in the database?
            if lng in self:
                if not hasattr(inlng, '_ext'):
                    stmts.append(self[lng].diff_map(inlng))
            else:
                # check for possible RENAME
                if hasattr(inlng, 'oldname'):
                    oldname = inlng.oldname
                    try:
                        stmts.append(self[oldname].rename(inlng.name))
                        del self[oldname]
                    except KeyError as exc:
                        exc.args = ("Previous name '%s' for language '%s' "
                                    "not found" % (oldname, inlng.name), )
                        raise
                else:
                    # create new language
                    stmts.append(inlng.create())
        # check database languages
        for lng in self:
            # if missing, drop it
            if lng not in inlanguages:
                # special case: plpgsql is installed in 9.0
                if self.dbconn.version >= 90000 \
                        and self[lng].name == 'plpgsql':
                    continue
                self[lng].dropped = True
        return stmts

    def _drop(self):
        """Actually drop the languages

        :return: SQL statements
        """
        stmts = []
        for lng in self:
            if hasattr(self[lng], 'dropped'):
                stmts.append(self[lng].drop())
        return stmts

########NEW FILE########
__FILENAME__ = operator
# -*- coding: utf-8 -*-
"""
    pyrseas.dbobject.operator
    ~~~~~~~~~~~~~~~~~~~~~~~~~

    This module defines two classes: Operator derived from
    DbSchemaObject and OperatorDict derived from DbObjectDict.
"""
from pyrseas.dbobject import DbObjectDict, DbSchemaObject
from pyrseas.dbobject import quote_id, commentable, ownable


class Operator(DbSchemaObject):
    """An operator"""

    keylist = ['schema', 'name', 'leftarg', 'rightarg']
    objtype = "OPERATOR"
    single_extern_file = True

    def extern_key(self):
        """Return the key to be used in external maps for this operator

        :return: string
        """
        return '%s %s(%s, %s)' % (self.objtype.lower(), self.name,
                                  self.leftarg, self.rightarg)

    def qualname(self):
        """Return the schema-qualified name of the operator

        :return: string

        No qualification is used if the schema is 'public'.
        """
        return self.schema == 'public' and self.name \
            or "%s.%s" % (quote_id(self.schema), self.name)

    def identifier(self):
        """Return a full identifier for an operator object

        :return: string
        """
        return "%s(%s, %s)" % (self.qualname(), self.leftarg, self.rightarg)

    @commentable
    @ownable
    def create(self):
        """Return SQL statements to CREATE or REPLACE the operator

        :return: SQL statements
        """
        opt_clauses = []
        if self.leftarg != 'NONE':
            opt_clauses.append("LEFTARG = %s" % self.leftarg)
        if self.rightarg != 'NONE':
            opt_clauses.append("RIGHTARG = %s" % self.rightarg)
        if hasattr(self, 'commutator'):
            opt_clauses.append("COMMUTATOR = OPERATOR(%s)" % self.commutator)
        if hasattr(self, 'negator'):
            opt_clauses.append("NEGATOR = OPERATOR(%s)" % self.negator)
        if hasattr(self, 'restrict'):
            opt_clauses.append("RESTRICT = %s" % self.restrict)
        if hasattr(self, 'join'):
            opt_clauses.append("JOIN = %s" % self.join)
        if hasattr(self, 'hashes') and self.hashes:
            opt_clauses.append("HASHES")
        if hasattr(self, 'merges') and self.merges:
            opt_clauses.append("MERGES")
        return ["CREATE OPERATOR %s (\n    PROCEDURE = %s%s%s)" % (
                self.qualname(), self.procedure,
                ',\n    ' if opt_clauses else '', ',\n    '.join(opt_clauses))]


class OperatorDict(DbObjectDict):
    "The collection of operators in a database"

    cls = Operator
    query = \
        """SELECT nspname AS schema, oprname AS name, rolname AS owner,
                  oprleft::regtype AS leftarg, oprright::regtype AS rightarg,
                  oprcode AS procedure, oprcom::regoper AS commutator,
                  oprnegate::regoper AS negator, oprrest AS restrict,
                  oprjoin AS join, oprcanhash AS hashes,
                  oprcanmerge AS merges,
                  obj_description(o.oid, 'pg_operator') AS description
           FROM pg_operator o
                JOIN pg_roles r ON (r.oid = oprowner)
                JOIN pg_namespace n ON (oprnamespace = n.oid)
           WHERE (nspname != 'pg_catalog' AND nspname != 'information_schema')
             AND o.oid NOT IN (
                 SELECT objid FROM pg_depend WHERE deptype = 'e'
                              AND classid = 'pg_operator'::regclass)
           ORDER BY nspname, oprname"""

    def _from_catalog(self):
        """Initialize the dictionary of operators by querying the catalogs"""
        for oper in self.fetch():
            sch, opr, lft, rgt = oper.key()
            if lft == '-':
                lft = oper.leftarg = 'NONE'
            if rgt == '-':
                rgt = oper.rightarg = 'NONE'
            if oper.commutator == '0':
                del oper.commutator
            if oper.negator == '0':
                del oper.negator
            if oper.restrict == '-':
                del oper.restrict
            if oper.join == '-':
                del oper.join
            self[(sch, opr, lft, rgt)] = Operator(**oper.__dict__)

    def from_map(self, schema, inopers):
        """Initalize the dictionary of operators by converting the input map

        :param schema: schema owning the operators
        :param inopers: YAML map defining the operators
        """
        for key in inopers:
            (objtype, spc, opr) = key.partition(' ')
            if spc != ' ' or objtype != 'operator':
                raise KeyError("Unrecognized object type: %s" % key)
            paren = opr.find('(')
            if paren == -1 or opr[-1:] != ')':
                raise KeyError("Invalid operator signature: %s" % opr)
            (leftarg, rightarg) = opr[paren + 1:-1].split(',')
            rightarg = rightarg.lstrip()
            inoper = inopers[key]
            opr = opr[:paren]
            self[(schema.name, opr, leftarg, rightarg)] = oper = Operator(
                schema=schema.name, name=opr, leftarg=leftarg,
                rightarg=rightarg)
            if not inoper:
                raise ValueError("Operator '%s' has no specification" % opr)
            for attr, val in list(inoper.items()):
                setattr(oper, attr, val)
            if 'oldname' in inoper:
                oper.oldname = inoper['oldname']
            if 'description' in inoper:
                oper.description = inoper['description']

    def diff_map(self, inopers):
        """Generate SQL to transform existing operators

        :param inopers: a YAML map defining the new operators
        :return: list of SQL statements

        Compares the existing operator definitions, as fetched from
        the catalogs, to the input map and generates SQL statements to
        transform the operators accordingly.
        """
        stmts = []
        # check input operators
        for (sch, opr, lft, rgt) in inopers:
            inoper = inopers[(sch, opr, lft, rgt)]
            # does it exist in the database?
            if (sch, opr, lft, rgt) not in self:
                if not hasattr(inoper, 'oldname'):
                    # create new operator
                    stmts.append(inoper.create())
                else:
                    stmts.append(self[(sch, opr, lft, rgt)].rename(inoper))
            else:
                # check operator objects
                stmts.append(self[(sch, opr, lft, rgt)].diff_map(inoper))

        # check existing operators
        for (sch, opr, lft, rgt) in self:
            oper = self[(sch, opr, lft, rgt)]
            # if missing, mark it for dropping
            if (sch, opr, lft, rgt) not in inopers:
                oper.dropped = False

        return stmts

    def _drop(self):
        """Actually drop the operators

        :return: SQL statements
        """
        stmts = []
        for (sch, opr, lft, rgt) in self:
            oper = self[(sch, opr, lft, rgt)]
            if hasattr(oper, 'dropped'):
                stmts.append(oper.drop())
        return stmts

########NEW FILE########
__FILENAME__ = operclass
# -*- coding: utf-8 -*-
"""
    pyrseas.dbobject.operclass
    ~~~~~~~~~~~~~~~~~~~~~~~~~~

    This module defines two classes: OperatorClass derived from
    DbSchemaObject and OperatorClassDict derived from DbObjectDict.
"""
from pyrseas.dbobject import DbObjectDict, DbSchemaObject
from pyrseas.dbobject import commentable, ownable


class OperatorClass(DbSchemaObject):
    """An operator class"""

    keylist = ['schema', 'name', 'index_method']
    objtype = "OPERATOR CLASS"
    single_extern_file = True

    def extern_key(self):
        """Return the key to be used in external maps for this operator

        :return: string
        """
        return '%s %s using %s' % (self.objtype.lower(), self.name,
                                   self.index_method)

    def identifier(self):
        """Return a full identifier for an operator class

        :return: string
        """
        return "%s USING %s" % (self.qualname(), self.index_method)

    def to_map(self, no_owner):
        """Convert operator class to a YAML-suitable format

        :return: dictionary
        """
        dct = self._base_map(no_owner)
        if self.name == self.family:
            del dct['family']
        return dct

    @commentable
    @ownable
    def create(self):
        """Return SQL statements to CREATE the operator class

        :return: SQL statements
        """
        dflt = ''
        if hasattr(self, 'default') and self.default:
            dflt = "DEFAULT "
        clauses = []
        for (strat, oper) in list(self.operators.items()):
            clauses.append("OPERATOR %d %s" % (strat, oper))
        for (supp, func) in list(self.functions.items()):
            clauses.append("FUNCTION %d %s" % (supp, func))
        if hasattr(self, 'storage'):
            clauses.append("STORAGE %s" % self.storage)
        return ["CREATE OPERATOR CLASS %s\n    %sFOR TYPE %s USING %s "
                "AS\n    %s" % (
                self.qualname(), dflt, self.type, self.index_method,
                ',\n    ' .join(clauses))]


class OperatorClassDict(DbObjectDict):
    "The collection of operator classes in a database"

    cls = OperatorClass
    query = \
        """SELECT nspname AS schema, opcname AS name, rolname AS owner,
                  amname AS index_method, opfname AS family,
                  opcintype::regtype AS type, opcdefault AS default,
                  opckeytype::regtype AS storage,
                  obj_description(o.oid, 'pg_opclass') AS description
           FROM pg_opclass o JOIN pg_am a ON (opcmethod = a.oid)
                JOIN pg_roles r ON (r.oid = opcowner)
                JOIN pg_opfamily f ON (opcfamily = f.oid)
                JOIN pg_namespace n ON (opcnamespace = n.oid)
           WHERE (nspname != 'pg_catalog' AND nspname != 'information_schema')
             AND o.oid NOT IN (
                 SELECT objid FROM pg_depend WHERE deptype = 'e'
                              AND classid = 'pg_opclass'::regclass)
           ORDER BY nspname, opcname, amname"""

    opquery = \
        """SELECT nspname AS schema, opcname AS name, amname AS index_method,
                  amopstrategy AS strategy, amopopr::regoperator AS operator
           FROM pg_opclass o JOIN pg_am a ON (opcmethod = a.oid)
                JOIN pg_namespace n ON (opcnamespace = n.oid), pg_amop ao,
                pg_depend
           WHERE refclassid = 'pg_opclass'::regclass
             AND classid = 'pg_amop'::regclass AND objid = ao.oid
             AND refobjid = o.oid
             AND (nspname != 'pg_catalog' AND nspname != 'information_schema')
             AND o.oid NOT IN (
                 SELECT objid FROM pg_depend WHERE deptype = 'e'
                              AND classid = 'pg_opclass'::regclass)
           ORDER BY nspname, opcname, amname, amopstrategy"""

    prquery = \
        """SELECT nspname AS schema, opcname AS name, amname AS index_method,
                  amprocnum AS support, amproc::regprocedure AS function
           FROM pg_opclass o JOIN pg_am a ON (opcmethod = a.oid)
                JOIN pg_namespace n ON (opcnamespace = n.oid), pg_amproc ap,
                pg_depend
           WHERE refclassid = 'pg_opclass'::regclass
             AND classid = 'pg_amproc'::regclass AND objid = ap.oid
             AND refobjid = o.oid
             AND (nspname != 'pg_catalog' AND nspname != 'information_schema')
             AND o.oid NOT IN (
                 SELECT objid FROM pg_depend WHERE deptype = 'e'
                              AND classid = 'pg_opclass'::regclass)
           ORDER BY nspname, opcname, amname, amprocnum"""

    def _from_catalog(self):
        """Initialize the dictionary of operator classes from the catalogs"""
        for opclass in self.fetch():
            if opclass.storage == '-':
                del opclass.storage
            self[opclass.key()] = OperatorClass(**opclass.__dict__)
        opers = self.dbconn.fetchall(self.opquery)
        self.dbconn.rollback()
        for (sch, opc, idx, strat, oper) in opers:
            opcls = self[(sch, opc, idx)]
            if not hasattr(opcls, 'operators'):
                opcls.operators = {}
            opcls.operators.update({strat: oper})
        funcs = self.dbconn.fetchall(self.prquery)
        self.dbconn.rollback()
        for (sch, opc, idx, supp, func) in funcs:
            opcls = self[(sch, opc, idx)]
            if not hasattr(opcls, 'functions'):
                opcls.functions = {}
            opcls.functions.update({supp: func})

    def from_map(self, schema, inopcls):
        """Initalize the dictionary of operator classes from the input map

        :param schema: schema owning the operator classes
        :param inopcls: YAML map defining the operator classes
        """
        for key in inopcls:
            if not key.startswith('operator class ') or not ' using ' in key:
                raise KeyError("Unrecognized object type: %s" % key)
            pos = key.rfind(' using ')
            opc = key[15:pos]  # 15 = len('operator class ')
            idx = key[pos + 7:]  # 7 = len(' using ')
            inopcl = inopcls[key]
            self[(schema.name, opc, idx)] = opclass = OperatorClass(
                schema=schema.name, name=opc, index_method=idx)
            if not inopcl:
                raise ValueError("Operator '%s' has no specification" % opc)
            for attr, val in list(inopcl.items()):
                setattr(opclass, attr, val)
            if 'oldname' in inopcl:
                opclass.oldname = inopcl['oldname']
            if 'description' in inopcl:
                opclass.description = inopcl['description']

    def diff_map(self, inopcls):
        """Generate SQL to transform existing operator classes

        :param inopcls: a YAML map defining the new operator classes
        :return: list of SQL statements

        Compares the existing operator class definitions, as fetched
        from the catalogs, to the input map and generates SQL
        statements to transform the operator classes accordingly.
        """
        stmts = []
        # check input operator classes
        for (sch, opc, idx) in inopcls:
            inoper = inopcls[(sch, opc, idx)]
            # does it exist in the database?
            if (sch, opc, idx) not in self:
                if not hasattr(inoper, 'oldname'):
                    # create new operator
                    stmts.append(inoper.create())
                else:
                    stmts.append(self[(sch, opc, idx)].rename(inoper))
            else:
                # check operator objects
                stmts.append(self[(sch, opc, idx)].diff_map(inoper))

        # check existing operators
        for (sch, opc, idx) in self:
            oper = self[(sch, opc, idx)]
            # if missing, mark it for dropping
            if (sch, opc, idx) not in inopcls:
                oper.dropped = False

        return stmts

    def _drop(self):
        """Actually drop the operator classes

        :return: SQL statements
        """
        stmts = []
        for (sch, opc, idx) in self:
            oper = self[(sch, opc, idx)]
            if hasattr(oper, 'dropped'):
                stmts.append(oper.drop())
        return stmts

########NEW FILE########
__FILENAME__ = operfamily
# -*- coding: utf-8 -*-
"""
    pyrseas.dbobject.operfamily
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~

    This module defines two classes: OperatorFamily derived from
    DbSchemaObject and OperatorFamilyDict derived from DbObjectDict.
"""
from pyrseas.dbobject import DbObjectDict, DbSchemaObject
from pyrseas.dbobject import commentable, ownable


class OperatorFamily(DbSchemaObject):
    """An operator family"""

    keylist = ['schema', 'name', 'index_method']
    objtype = "OPERATOR FAMILY"
    single_extern_file = True

    def extern_key(self):
        """Return the key to be used in external maps for the operator family

        :return: string
        """
        return '%s %s using %s' % (self.objtype.lower(), self.name,
                                   self.index_method)

    def identifier(self):
        """Return a full identifier for an operator family object

        :return: string
        """
        return "%s USING %s" % (self.qualname(), self.index_method)

    @commentable
    @ownable
    def create(self):
        """Return SQL statements to CREATE the operator family

        :return: SQL statements
        """
        return ["CREATE OPERATOR FAMILY %s USING %s" % (
                self.qualname(), self.index_method)]


class OperatorFamilyDict(DbObjectDict):
    "The collection of operator families in a database"

    cls = OperatorFamily
    query = \
        """SELECT nspname AS schema, opfname AS name, rolname AS owner,
                  amname AS index_method,
                  obj_description(o.oid, 'pg_opfamily') AS description
           FROM pg_opfamily o
                JOIN pg_roles r ON (r.oid = opfowner)
                JOIN pg_am a ON (opfmethod = a.oid)
                JOIN pg_namespace n ON (opfnamespace = n.oid)
           WHERE (nspname != 'pg_catalog' AND nspname != 'information_schema')
             AND o.oid NOT IN (
                 SELECT objid FROM pg_depend WHERE deptype = 'e'
                              AND classid = 'pg_opfamily'::regclass)
           ORDER BY opfnamespace, opfname, amname"""

    def from_map(self, schema, inopfams):
        """Initalize the dict of operator families by converting the input map

        :param schema: schema owning the operators
        :param inopfams: YAML map defining the operator families
        """
        for key in inopfams:
            if not key.startswith('operator family ') or not ' using ' in key:
                raise KeyError("Unrecognized object type: %s" % key)
            pos = key.rfind(' using ')
            opf = key[16:pos]  # 16 = len('operator family ')
            idx = key[pos + 7:]  # 7 = len(' using ')
            inopfam = inopfams[key]
            self[(schema.name, opf, idx)] = opfam = OperatorFamily(
                schema=schema.name, name=opf, index_method=idx)
            for attr, val in list(inopfam.items()):
                setattr(opfam, attr, val)
            if 'oldname' in inopfam:
                opfam.oldname = inopfam['oldname']
            if 'description' in inopfam:
                opfam.description = inopfam['description']

    def diff_map(self, inopfams):
        """Generate SQL to transform existing operator families

        :param inopfams: a YAML map defining the new operator families
        :return: list of SQL statements

        Compares the existing operator family definitions, as fetched
        from the catalogs, to the input map and generates SQL
        statements to transform the operator families accordingly.
        """
        stmts = []
        # check input operator families
        for (sch, opf, idx) in inopfams:
            inopfam = inopfams[(sch, opf, idx)]
            # does it exist in the database?
            if (sch, opf, idx) not in self:
                if not hasattr(inopfam, 'oldname'):
                    # create new operator family
                    stmts.append(inopfam.create())
                else:
                    stmts.append(self[(sch, opf, idx)].rename(inopfam))
            else:
                # check operator family objects
                stmts.append(self[(sch, opf, idx)].diff_map(inopfam))

        # check existing operator families
        for (sch, opf, idx) in self:
            oper = self[(sch, opf, idx)]
            # if missing, mark it for dropping
            if (sch, opf, idx) not in inopfams:
                oper.dropped = False

        return stmts

    def _drop(self):
        """Actually drop the operator families

        :return: SQL statements
        """
        stmts = []
        for (sch, opf, idx) in self:
            oper = self[(sch, opf, idx)]
            if hasattr(oper, 'dropped'):
                stmts.append(oper.drop())
        return stmts

########NEW FILE########
__FILENAME__ = privileges
# -*- coding: utf-8 -*-
"""
    pyrseas.dbobject.privileges
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~

    This defines functions for dealing with access privileges.
"""

PRIVCODES = {'a': 'insert', 'r': 'select', 'w': 'update', 'd': 'delete',
             'D': 'truncate', 'x': 'references', 't': 'trigger',
             'X': 'execute', 'U': 'usage', 'C': 'create'}
PRIVILEGES = dict((v, k) for k, v in list(PRIVCODES.items()))


def _split_privs(privspec):
    """Split the aclitem into three parts

    :param privspec: privilege specification (aclitem)
    :return: tuple with grantee, privilege codes and granto
    """
    (usr, prvgrant) = privspec.split('=')
    if usr == '':
        usr = 'PUBLIC'
    (privcodes, grantor) = prvgrant.split('/')
    return (usr, privcodes, grantor)


def _expand_priv_lists(obj, privcodes, subobj):
    """Convert privilege code strings to expanded lists

    :param obj: the object on which the privilege is granted
    :param privcodes: string of privilege codes
    :param subobj: sub-object name (e.g., column name)
    :return: tuple of lists with decoded privileges
    """
    privs = []
    wgo = []
    if privcodes == obj.allprivs and len(obj.allprivs) > 1:
        privs = ['ALL']
    else:
        if subobj:
            subobj = ' (%s)' % subobj
        for code in sorted(PRIVCODES.keys()):
            if code in privcodes:
                priv = PRIVCODES[code].upper() + subobj
                if code + '*' in privcodes:
                    wgo.append(priv)
                else:
                    privs.append(priv)
    return (privs, wgo)


def privileges_to_map(privspec, allprivs, owner):
    """Map a set of privileges in PostgreSQL format to YAML-suitable format

    :param privspec: privilege specification
    :param allprivs: privilege list equal to ALL
    :param owner: object owner
    :return: dictionary

    Access privileges are specified as aclitem's as follows:
    <grantee>=<privlist>/<grantor>.  The grantee and grantor are user
    names.  The privlist is a set of single letter codes, each letter
    optionally followed by an asterisk to indicate WITH GRANT OPTION.
    """
    (usr, privcodes, grantor) = _split_privs(privspec)
    privs = []
    if privcodes == allprivs and len(allprivs) > 1:
        privs = ['all']
    else:
        for code in sorted(PRIVCODES.keys()):
            if code in privcodes:
                priv = PRIVCODES[code]
                if code + '*' in privcodes:
                    priv = {priv: {'grantable': True}}
                privs.append(priv)
    if owner and grantor != owner:
        privs = {'privs': privs, 'grantor': grantor}
    return {usr: privs}


def privileges_from_map(privlist, allprivs, owner):
    """Map privileges from YAML-suitable format to an internal list

    :param privspec: privilege specification
    :param allprivs: privilege list equal to ALL
    :param owner: object owner
    :return: list
    """
    retlist = []
    for priv in privlist:
        usr = list(priv.keys())[0]
        privs = priv[usr]
        grantor = owner
        if 'grantor' in privs:
            grantor = privs['grantor']
            privs = privs['privs']
        if usr == 'PUBLIC':
            usr = ''
        prvcodes = ''
        if privs == ['all']:
            prvcodes = allprivs
        else:
            for prv in privs:
                if isinstance(prv, dict):
                    key = list(prv.keys())[0]
                else:
                    key = prv
                if key in PRIVILEGES:
                    prvcodes += PRIVILEGES[key]
                if isinstance(prv, dict) and isinstance(prv[key], dict) and \
                        'grantable' in prv[key] and prv[key]['grantable']:
                    prvcodes += '*'
        retlist.append("%s=%s/%s" % (usr, prvcodes, grantor))
    return retlist


def add_grant(obj, privspec, subobj=''):
    """Return GRANT statements on the object based on the privilege spec

    :param obj: the object on which the privilege is granted
    :param privspec: the privilege specification (aclitem)
    :param subobj: sub-object name (e.g., column name)
    :return: list of GRANT statements
    """
    (usr, privcodes, grantor) = _split_privs(privspec)
    (privs, wgo) = _expand_priv_lists(obj, privcodes, subobj)
    objtype = obj.objtype
    if hasattr(obj, 'privobjtype'):
        objtype = obj.privobjtype
    stmts = []
    if privs:
        stmts.append("GRANT %s ON %s %s TO %s" % (
            ', '.join(privs), objtype, obj.identifier(), usr))
    if wgo:
        stmts.append("GRANT %s ON %s %s TO %s WITH GRANT OPTION" % (
            ', '.join(wgo), objtype, obj.identifier(), usr))
    return stmts


def add_revoke(obj, privspec, subobj=''):
    """Return REVOKE statements on the object based on the privilege spec

    :param obj: the object on which the privilege is to be revoked
    :param privspec: the privilege specification (aclitem)
    :param subobj: sub-object name (e.g., column name)
    :return: list of REVOKE statements
    """
    (usr, privcodes, grantor) = _split_privs(privspec)
    (privs, wgo) = _expand_priv_lists(obj, privcodes, subobj)
    objtype = obj.objtype
    if hasattr(obj, 'privobjtype'):
        objtype = obj.privobjtype
    stmts = []
    if wgo:
        stmts.append("REVOKE %s ON %s %s FROM %s" % (
            ', '.join(wgo), objtype, obj.identifier(), usr))
    if privs:
        stmts.append("REVOKE %s ON %s %s FROM %s" % (
            ', '.join(privs), objtype, obj.identifier(), usr))
    return stmts


def diff_privs(currobj, currlist, newobj, newlist, subobj=''):
    """Return GRANT or REVOKE statements to adjust object privileges

    :param currobj: current object
    :param currlist: list of current privileges
    :param newobj: new object
    :param newlist: list of new privileges
    :param subobj: sub-object (e.g., column name)
    :return: list of GRANT and REVOKE statements
    """
    def rejoin(privdict, usr, grantor):
        return '%s=%s/%s' % ('' if usr == 'PUBLIC' else usr,
                             privdict[(usr, grantor)], grantor)

    stmts = []
    currprivs = {}
    newprivs = {}
    for privspec in currlist:
        (usr, privcodes, grantor) = _split_privs(privspec)
        currprivs[(usr, grantor)] = privcodes
    for privspec in newlist:
        (usr, privcodes, grantor) = _split_privs(privspec)
        newprivs[(usr, grantor)] = privcodes
    for (usr, gtor) in currprivs:
        if (usr, gtor) not in newprivs:
            stmts.append(add_revoke(currobj, rejoin(currprivs, usr, gtor),
                                    subobj))
    for (usr, gtor) in newprivs:
        if (usr, gtor) not in currprivs:
            stmts.append(add_grant(newobj, rejoin(newprivs, usr, gtor),
                                   subobj))
        else:
            if sorted(currprivs[(usr, gtor)]) != sorted(newprivs[(usr, gtor)]):
                stmts.append(add_revoke(currobj, rejoin(currprivs, usr, gtor),
                                        subobj))
                stmts.append(add_grant(newobj, rejoin(newprivs, usr, gtor),
                                       subobj))
    return stmts

########NEW FILE########
__FILENAME__ = rule
# -*- coding: utf-8 -*-
"""
    pyrseas.dbobject.rule
    ~~~~~~~~~~~~~~~~~~~~~

    This defines two classes, Rule and RuleDict, derived from
    DbSchemaObject and DbObjectDict, respectively.
"""
from pyrseas.dbobject import DbObjectDict, DbSchemaObject
from pyrseas.dbobject import quote_id, commentable


class Rule(DbSchemaObject):
    """A rewrite rule definition"""

    keylist = ['schema', 'table', 'name']
    objtype = "RULE"

    def identifier(self):
        """Return a full identifier for a rule object

        :return: string
        """
        return "%s ON %s" % (quote_id(self.name), self._table.qualname())

    def to_map(self):
        """Convert rule to a YAML-suitable format

        :return: dictionary
        """
        dct = self._base_map()
        del dct['_table']
        return {self.name: dct}

    @commentable
    def create(self):
        """Return SQL statements to CREATE the rule

        :return: SQL statements
        """
        where = instead = ''
        if hasattr(self, 'condition'):
            where = ' WHERE %s' % self.condition
        if hasattr(self, 'instead'):
            instead = 'INSTEAD '
        return ["CREATE RULE %s AS ON %s\n    TO %s%s\n    DO %s%s" % (
                quote_id(self.name), self.event.upper(),
                self._table.qualname(), where, instead, self.actions)]


class RuleDict(DbObjectDict):
    "The collection of rewrite rules in a database."

    cls = Rule
    query = \
        """SELECT nspname AS schema, relname AS table, rulename AS name,
                  split_part('select,update,insert,delete', ',',
                      ev_type::int - 48) AS event, is_instead AS instead,
                  pg_get_ruledef(r.oid) AS definition,
                  obj_description(r.oid, 'pg_rewrite') AS description
           FROM pg_rewrite r JOIN pg_class c ON (ev_class = c.oid)
                JOIN pg_namespace n ON (relnamespace = n.oid)
           WHERE relkind = 'r'
             AND (nspname != 'pg_catalog' AND nspname != 'information_schema')
           ORDER BY nspname, relname, rulename"""

    def _from_catalog(self):
        """Initialize the dictionary of rules by querying the catalogs"""
        for rule in self.fetch():
            do_loc = rule.definition.index(' DO ')
            if 'WHERE' in rule.definition:
                rule.condition = rule.definition[rule.definition.index(
                    ' WHERE ') + 7:do_loc]
            if hasattr(rule, 'instead') and rule.instead:
                do_loc += 8
            rule.actions = rule.definition[do_loc + 4:-1]
            del rule.definition
            self[rule.key()] = rule

    def from_map(self, table, inmap):
        """Initialize the dictionary of rules by examining the input map

        :param inmap: the input YAML map defining the rules
        """
        for rul in inmap:
            inrule = inmap[rul]
            rule = Rule(table=table.name, schema=table.schema, name=rul,
                        **inrule)
            if inrule:
                if 'oldname' in inrule:
                    rule.oldname = inrule['oldname']
                    del inrule['oldname']
                if 'description' in inrule:
                    rule.description = inrule['description']
            self[(table.schema, table.name, rul)] = rule

    def diff_map(self, inrules):
        """Generate SQL to transform existing rules

        :param input_map: a YAML map defining the new rules
        :return: list of SQL statements

        Compares the existing rule definitions, as fetched from the
        catalogs, to the input map and generates SQL statements to
        transform the rules accordingly.
        """
        stmts = []
        # check input rules
        for rul in inrules:
            inrul = inrules[rul]
            # does it exist in the database?
            if rul in self:
                stmts.append(self[rul].diff_map(inrul))
            else:
                # check for possible RENAME
                if hasattr(inrul, 'oldname'):
                    oldname = inrul.oldname
                    try:
                        stmts.append(self[oldname].rename(inrul.name))
                        del self[oldname]
                    except KeyError as exc:
                        exc.args = ("Previous name '%s' for rule '%s' "
                                    "not found" % (oldname, inrul.name), )
                        raise
                else:
                    # create new rule
                    stmts.append(inrul.create())
        # check database rules
        for (sch, tbl, rul) in self:
            # if missing, drop it
            if (sch, tbl, rul) not in inrules:
                stmts.append(self[(sch, tbl, rul)].drop())

        return stmts

########NEW FILE########
__FILENAME__ = schema
# -*- coding: utf-8 -*-
"""
    pyrseas.dbobject.schema
    ~~~~~~~~~~~~~~~~~~~~~~~

    This defines two classes, Schema and SchemaDict, derived from
    DbObject and DbObjectDict, respectively.
"""
import os

from pyrseas.yamlutil import yamldump
from pyrseas.dbobject import DbObjectDict, DbObject
from pyrseas.dbobject import quote_id, split_schema_obj
from pyrseas.dbobject import commentable, ownable, grantable
from pyrseas.dbobject.dbtype import BaseType, Composite, Domain, Enum
from pyrseas.dbobject.table import Table, Sequence, View, MaterializedView
from pyrseas.dbobject.privileges import privileges_from_map


class Schema(DbObject):
    """A database schema definition, i.e., a named collection of tables,
    views, triggers and other schema objects."""

    keylist = ['name']
    objtype = 'SCHEMA'

    @property
    def allprivs(self):
        return 'UC'

    def extern_dir(self, root='.'):
        """Return the path to a directory to hold the schema objects.

        :return: directory path
        """
        (dir, ext) = os.path.splitext(os.path.join(root,
                                                   self.extern_filename()))
        return dir

    def to_map(self, dbschemas, opts):
        """Convert tables, etc., dictionaries to a YAML-suitable format

        :param dbschemas: dictionary of schemas
        :param opts: options to include/exclude schemas/tables, etc.
        :return: dictionary
        """
        if self.name == 'pyrseas':
            return {}
        no_owner = opts.no_owner
        no_privs = opts.no_privs
        schbase = {} if no_owner else {'owner': self.owner}
        if not no_privs and hasattr(self, 'privileges'):
            schbase.update({'privileges': self.map_privs()})
        if hasattr(self, 'description'):
            schbase.update(description=self.description)

        schobjs = []
        seltbls = getattr(opts, 'tables', [])
        if hasattr(self, 'tables'):
            for objkey in self.tables:
                if not seltbls or objkey in seltbls:
                    obj = self.tables[objkey]
                    schobjs.append((obj, obj.to_map(dbschemas, opts)))

        def mapper(objtypes):
            if hasattr(self, objtypes):
                schemadict = getattr(self, objtypes)
                for objkey in schemadict:
                    if objtypes == 'sequences' or (
                            not seltbls or objkey in seltbls):
                        obj = schemadict[objkey]
                        schobjs.append((obj, obj.to_map(opts)))

        for objtypes in ['ftables', 'sequences', 'views', 'matviews']:
            mapper(objtypes)

        def mapper2(objtypes):
            if hasattr(self, objtypes):
                schemadict = getattr(self, objtypes)
                for objkey in schemadict:
                    obj = schemadict[objkey]
                    schobjs.append((obj, obj.to_map(no_owner)))

        if hasattr(opts, 'tables') and not opts.tables or \
                not hasattr(opts, 'tables'):
            for objtypes in ['conversions', 'domains',
                             'operators', 'operclasses', 'operfams',
                             'tsconfigs', 'tsdicts', 'tsparsers', 'tstempls',
                             'types', 'collations']:
                mapper2(objtypes)
            if hasattr(self, 'functions'):
                for objkey in self.functions:
                    obj = self.functions[objkey]
                    schobjs.append((obj, obj.to_map(no_owner, no_privs)))

        # special case for pg_catalog schema
        if self.name == 'pg_catalog' and not schobjs:
            return {}

        if hasattr(self, 'datacopy') and self.datacopy:
            dir = self.extern_dir(opts.data_dir)
            if not os.path.exists(dir):
                os.mkdir(dir)
            for tbl in self.datacopy:
                self.tables[tbl].data_export(dbschemas.dbconn, dir)

        if opts.multiple_files:
            dir = self.extern_dir(opts.metadata_dir)
            if not os.path.exists(dir):
                os.mkdir(dir)
            filemap = {}
            for obj, objmap in schobjs:
                if objmap is not None:
                    extkey = obj.extern_key()
                    filepath = os.path.join(dir, obj.extern_filename())
                    with open(filepath, 'a') as f:
                        f.write(yamldump({extkey: objmap}))
                    outobj = {extkey:
                              os.path.relpath(filepath, opts.metadata_dir)}
                filemap.update(outobj)
            # always write the schema YAML file
            filepath = self.extern_filename()
            extkey = self.extern_key()
            with open(os.path.join(opts.metadata_dir, filepath), 'a') as f:
                f.write(yamldump({extkey: schbase}))
            filemap.update(schema=filepath)
            return {extkey: filemap}

        schmap = {obj.extern_key(): objmap for obj, objmap in schobjs
                  if objmap is not None}
        schmap.update(schbase)
        return {self.extern_key(): schmap}

    @commentable
    @grantable
    @ownable
    def create(self):
        """Return SQL statements to CREATE the schema

        :return: SQL statements
        """
        return ["CREATE SCHEMA %s" % quote_id(self.name)]

    def data_import(self, opts):
        """Generate SQL to import data from the tables in this schema

        :param opts: options to include/exclude schemas/tables, etc.
        :return: list of SQL statements
        """
        stmts = []
        if hasattr(self, 'datacopy') and self.datacopy:
            dir = self.extern_dir(opts.data_dir)
            for tbl in self.datacopy:
                stmts.append(self.tables[tbl].data_import(dir))
        return stmts


PREFIXES = {'domain ': 'types', 'type': 'types', 'table ': 'tables',
            'view ': 'tables', 'sequence ': 'tables',
            'materialized view ': 'tables',
            'function ': 'functions', 'aggregate ': 'functions',
            'operator family ': 'operfams', 'operator class ': 'operclasses',
            'conversion ': 'conversions', 'text search dictionary ': 'tsdicts',
            'text search template ': 'tstempls',
            'text search parser ': 'tsparsers',
            'text search configuration ': 'tsconfigs',
            'foreign table ': 'ftables', 'collation ': 'collations'}
SCHOBJS1 = ['types', 'tables', 'ftables']
SCHOBJS2 = ['collations', 'conversions', 'functions', 'operators',
            'operclasses', 'operfams', 'tsconfigs', 'tsdicts', 'tsparsers',
            'tstempls']


class SchemaDict(DbObjectDict):
    "The collection of schemas in a database.  Minimally, the 'public' schema."

    cls = Schema
    query = \
        """SELECT nspname AS name, rolname AS owner,
                  array_to_string(nspacl, ',') AS privileges,
                  obj_description(n.oid, 'pg_namespace') AS description
           FROM pg_namespace n
                JOIN pg_roles r ON (r.oid = nspowner)
           WHERE nspname NOT IN ('information_schema', 'pg_toast')
                 AND nspname NOT LIKE 'pg_temp\_%'
                 AND nspname NOT LIKE 'pg_toast_temp\_%'
           ORDER BY nspname"""

    def from_map(self, inmap, newdb):
        """Initialize the dictionary of schemas by converting the input map

        :param inmap: the input YAML map defining the schemas
        :param newdb: collection of dictionaries defining the database

        Starts the recursive analysis of the input map and
        construction of the internal collection of dictionaries
        describing the database objects.
        """
        for key in inmap:
            (objtype, spc, sch) = key.partition(' ')
            if spc != ' ' or objtype != 'schema':
                raise KeyError("Unrecognized object type: %s" % key)
            schema = self[sch] = Schema(name=sch)
            inschema = inmap[key]
            objdict = {}
            for key in sorted(inschema.keys()):
                mapped = False
                for prefix in PREFIXES:
                    if key.startswith(prefix):
                        otype = PREFIXES[prefix]
                        if otype not in objdict:
                            objdict[otype] = {}
                        objdict[otype].update({key: inschema[key]})
                        mapped = True
                        break
                # Needs separate processing because it overlaps
                # operator classes and operator families
                if not mapped and key.startswith('operator '):
                    otype = 'operators'
                    if otype not in objdict:
                        objdict[otype] = {}
                    objdict[otype].update({key: inschema[key]})
                    mapped = True
                elif key in ['oldname', 'owner', 'description']:
                    setattr(schema, key, inschema[key])
                    mapped = True
                elif key == 'privileges':
                    schema.privileges = privileges_from_map(
                        inschema[key], schema.allprivs, schema.owner)
                    mapped = True
                if not mapped and key != 'schema':
                    raise KeyError("Expected typed object, found '%s'" % key)

            for objtype in SCHOBJS1:
                if objtype in objdict:
                    subobjs = getattr(newdb, objtype)
                    subobjs.from_map(schema, objdict[objtype], newdb)
            for objtype in SCHOBJS2:
                if objtype in objdict:
                    subobjs = getattr(newdb, objtype)
                    subobjs.from_map(schema, objdict[objtype])

    def link_refs(self, db, datacopy):
        """Connect various schema objects to their respective schemas

        :param db: dictionary of dictionaries of all objects
        :param datacopy: dictionary of data copying info
        """
        def link_one(targdict, objtype, objkeys, subtype=None):
            schema = self[objkeys[0]]
            if subtype is None:
                subtype = objtype
            if not hasattr(schema, subtype):
                setattr(schema, subtype, {})
            objdict = getattr(schema, subtype)
            key = objkeys[1] if len(objkeys) == 2 else objkeys[1:]
            objdict.update({key: targdict[objkeys]})

        targ = db.types
        for keys in targ:
            dbtype = targ[keys]
            if isinstance(dbtype, Domain):
                link_one(targ, 'types', keys, 'domains')
            elif isinstance(dbtype, Enum) or isinstance(dbtype, Composite) \
                    or isinstance(dbtype, BaseType):
                link_one(targ, 'types', keys)
        targ = db.tables
        for keys in targ:
            table = targ[keys]
            type_ = 'tables'
            if isinstance(table, Table):
                link_one(targ, type_, keys)
            elif isinstance(table, Sequence):
                link_one(targ, type_, keys, 'sequences')
            elif isinstance(table, MaterializedView):
                link_one(targ, type_, keys, 'matviews')
            elif isinstance(table, View):
                link_one(targ, type_, keys, 'views')
        targ = db.functions
        for keys in targ:
            func = targ[keys]
            link_one(targ, 'functions', keys)
            if hasattr(func, 'returns'):
                rettype = func.returns
                if rettype.upper().startswith("SETOF "):
                    rettype = rettype[6:]
                (retsch, rettyp) = split_schema_obj(rettype, keys[0])
                if (retsch, rettyp) in db.tables:
                    deptbl = db.tables[(retsch, rettyp)]
                    if not hasattr(func, 'dependent_table'):
                        func.dependent_table = deptbl
                    if not hasattr(deptbl, 'dependent_funcs'):
                        deptbl.dependent_funcs = []
                    deptbl.dependent_funcs.append(func)
        for objtype in ['operators', 'operclasses', 'operfams', 'conversions',
                        'tsconfigs', 'tsdicts', 'tsparsers', 'tstempls',
                        'ftables', 'collations']:
            targ = getattr(db, objtype)
            for keys in targ:
                link_one(targ, objtype, keys)
        for key in datacopy:
            if not key.startswith('schema '):
                raise KeyError("Unrecognized object type: %s" % key)
            schema = self[key[7:]]
            if not hasattr(schema, 'datacopy'):
                schema.datacopy = []
            for tbl in datacopy[key]:
                if hasattr(schema, 'tables') and tbl in schema.tables:
                    schema.datacopy.append(tbl)

    def to_map(self, opts):
        """Convert the schema dictionary to a regular dictionary

        :param opts: options to include/exclude schemas/tables, etc.
        :return: dictionary

        Invokes the `to_map` method of each schema to construct a
        dictionary of schemas.
        """
        schemas = {}
        selschs = getattr(opts, 'schemas', [])
        for sch in self:
            if not selschs or sch in selschs:
                if hasattr(opts, 'excl_schemas') and opts.excl_schemas \
                        and sch in opts.excl_schemas:
                    continue
                schemas.update(self[sch].to_map(self, opts))

        return schemas

    def diff_map(self, inschemas):
        """Generate SQL to transform existing schemas

        :param input_map: a YAML map defining the new schemas
        :return: list of SQL statements

        Compares the existing schema definitions, as fetched from the
        catalogs, to the input map and generates SQL statements to
        transform the schemas accordingly.
        """
        stmts = []
        # check input schemas
        for sch in inschemas:
            insch = inschemas[sch]
            # does it exist in the database?
            if sch in self:
                stmts.append(self[sch].diff_map(insch))
            else:
                # check for possible RENAME
                if hasattr(insch, 'oldname'):
                    oldname = insch.oldname
                    try:
                        stmts.append(self[oldname].rename(insch.name))
                        del self[oldname]
                    except KeyError as exc:
                        exc.args = ("Previous name '%s' for schema '%s' "
                                    "not found" % (oldname, insch.name), )
                        raise
                else:
                    # create new schema
                    if insch.name not in ['pg_catalog']:
                        stmts.append(insch.create())
        # check database schemas
        for sch in self:
            # if missing and not 'public', drop it
            if sch not in ['public', 'pg_catalog'] and sch not in inschemas:
                self[sch].dropped = True
        return stmts

    def _drop(self):
        """Actually drop the schemas

        :return: SQL statements
        """
        stmts = []
        for sch in self:
            if sch != 'public' and hasattr(self[sch], 'dropped'):
                stmts.append(self[sch].drop())
        return stmts

    def data_import(self, opts):
        """Iterate over schemas with tables to be imported

        :param opts: options to include/exclude schemas/tables, etc.
        :return: list of SQL statements
        """
        return [self[sch].data_import(opts) for sch in self]

########NEW FILE########
__FILENAME__ = table
# -*- coding: utf-8 -*-
"""
    pyrseas.dbobject.table
    ~~~~~~~~~~~~~~~~~~~~~~

    This module defines six classes: DbClass derived from
    DbSchemaObject, Sequence, Table and View derived from DbClass,
    MaterializedView derived from View, and ClassDict derived from
    DbObjectDict.
"""
import os
import sys

from pyrseas.lib.pycompat import PY2
from pyrseas.dbobject import DbObjectDict, DbSchemaObject
from pyrseas.dbobject import quote_id, split_schema_obj
from pyrseas.dbobject import commentable, ownable, grantable
from pyrseas.dbobject.constraint import CheckConstraint, PrimaryKey
from pyrseas.dbobject.constraint import ForeignKey, UniqueConstraint
from pyrseas.dbobject.privileges import privileges_from_map, add_grant

MAX_BIGINT = 9223372036854775807


def seq_max_value(seq):
    if seq.max_value is None or seq.max_value == MAX_BIGINT:
        return " NO MAXVALUE"
    return " MAXVALUE %d" % seq.max_value


def seq_min_value(seq):
    if seq.min_value is None or seq.min_value == 1:
        return " NO MINVALUE"
    return " MINVALUE %d" % seq.min_value


class DbClass(DbSchemaObject):
    """A table, sequence or view"""

    keylist = ['schema', 'name']


class Sequence(DbClass):
    "A sequence generator definition"

    objtype = "SEQUENCE"

    @property
    def allprivs(self):
        return 'rwU'

    def get_attrs(self, dbconn):
        """Get the attributes for the sequence

        :param dbconn: a DbConnection object
        """
        data = dbconn.fetchone(
            """SELECT start_value, increment_by, max_value, min_value,
                      cache_value
               FROM %s.%s""" % (quote_id(self.schema), quote_id(self.name)))
        for key, val in list(data.items()):
            setattr(self, key, val)

    def get_dependent_table(self, dbconn):
        """Get the table and column name that uses or owns the sequence

        :param dbconn: a DbConnection object
        """
        data = dbconn.fetchone(
            """SELECT refobjid::regclass, refobjsubid
               FROM pg_depend
               WHERE objid = '%s'::regclass
                 AND refclassid = 'pg_class'::regclass""" % self.qualname())
        if data:
            (sch, self.owner_table) = split_schema_obj(data[0], self.schema)
            self.owner_column = data[1]
            return
        data = dbconn.fetchone(
            """SELECT adrelid::regclass
               FROM pg_attrdef a JOIN pg_depend ON (a.oid = objid)
               WHERE refobjid = '%s'::regclass
               AND classid = 'pg_attrdef'::regclass""" % self.qualname())
        if data:
            (sch, self.dependent_table) = split_schema_obj(
                data[0], self.schema)

    def to_map(self, opts):
        """Convert a sequence definition to a YAML-suitable format

        :param opts: options to include/exclude tables, etc.
        :return: dictionary
        """
        if hasattr(opts, 'tables') and opts.tables and \
                (self.name not in opts.tables and
                 not hasattr(self, 'owner_table') or
                 self.owner_table not in opts.tables) or (
                     hasattr(opts, 'excl_tables') and opts.excl_tables
                     and self.name in opts.excl_tables):
            return None
        seq = {}
        for key, val in list(self.__dict__.items()):
            if key in self.keylist or key == 'dependent_table' or (
                    key == 'owner' and opts.no_owner) or (
                    key == 'privileges' and opts.no_privs):
                continue
            if key == 'privileges':
                seq[key] = self.map_privs()
            elif key == 'max_value' and val == MAX_BIGINT:
                seq[key] = None
            elif key == 'min_value' and val == 1:
                seq[key] = None
            else:
                if PY2:
                    if isinstance(val, (int, long)) and val <= sys.maxsize:
                        seq[key] = int(val)
                    else:
                        seq[key] = str(val)
                else:
                    if isinstance(val, int):
                        seq[key] = int(val)
                    else:
                        seq[key] = str(val)

        return seq

    @commentable
    @grantable
    @ownable
    def create(self):
        """Return a SQL statement to CREATE the sequence

        :return: SQL statements
        """
        return ["""CREATE SEQUENCE %s
    START WITH %d
    INCREMENT BY %d
   %s
   %s
    CACHE %d""" % (self.qualname(), self.start_value, self.increment_by,
                   seq_max_value(self), seq_min_value(self), self.cache_value)]

    def add_owner(self):
        """Return statement to ALTER the sequence to indicate its owner table

        :return: SQL statement
        """
        stmts = []
        stmts.append("ALTER SEQUENCE %s OWNED BY %s.%s" % (
            self.qualname(), self.qualname(self.owner_table),
            quote_id(self.owner_column)))
        return stmts

    def diff_map(self, inseq):
        """Generate SQL to transform an existing sequence

        :param inseq: a YAML map defining the new sequence
        :return: list of SQL statements

        Compares the sequence to an input sequence and generates SQL
        statements to transform it into the one represented by the
        input.
        """
        stmts = []
        stmt = ""
        if self.start_value != inseq.start_value:
            stmt += " START WITH %d" % inseq.start_value
        if self.increment_by != inseq.increment_by:
            stmt += " INCREMENT BY %d" % inseq.increment_by
        maxval = self.max_value
        if maxval == MAX_BIGINT:
            maxval = None
        if maxval != inseq.max_value:
            stmt += seq_max_value(inseq)
        minval = self.min_value
        if minval == 1:
            minval = None
        if minval != inseq.min_value:
            stmt += seq_min_value(inseq)
        if self.cache_value != inseq.cache_value:
            stmt += " CACHE %d" % inseq.cache_value
        if stmt:
            stmts.append("ALTER SEQUENCE %s" % self.qualname() + stmt)

        if hasattr(inseq, 'owner'):
            if hasattr(self, 'owner') and inseq.owner != self.owner:
                stmts.append(self.alter_owner(inseq.owner))
        stmts.append(self.diff_privileges(inseq))
        stmts.append(self.diff_description(inseq))
        return stmts


class Table(DbClass):
    """A database table definition

    A table is identified by its schema name and table name.  It should
    have a list of columns.  It may have a primary_key, zero or more
    foreign_keys, zero or more unique_constraints, and zero or more
    indexes.
    """

    objtype = "TABLE"

    @property
    def allprivs(self):
        return 'arwdDxt'

    def column_names(self):
        """Return a list of column names in the table

        :return: list
        """
        return [c.name for c in self.columns]

    def to_map(self, dbschemas, opts):
        """Convert a table to a YAML-suitable format

        :param dbschemas: database dictionary of schemas
        :param opts: options to include/exclude tables, etc.
        :return: dictionary
        """
        if hasattr(opts, 'excl_tables') and opts.excl_tables \
                and self.name in opts.excl_tables or \
                not hasattr(self, 'columns'):
            return None
        cols = []
        for column in self.columns:
            col = column.to_map(opts.no_privs)
            if col:
                cols.append(col)
        tbl = {'columns': cols}
        attrlist = ['description', 'options', 'tablespace', 'unlogged']
        if not opts.no_owner:
            attrlist.append('owner')
        for attr in attrlist:
            if hasattr(self, attr):
                tbl.update({attr: getattr(self, attr)})
        if hasattr(self, 'check_constraints'):
            if not 'check_constraints' in tbl:
                tbl.update(check_constraints={})
            for k in list(self.check_constraints.values()):
                tbl['check_constraints'].update(
                    self.check_constraints[k.name].to_map(self.column_names()))
        if hasattr(self, 'primary_key'):
            tbl.update(primary_key=self.primary_key.to_map(
                self.column_names()))
        if hasattr(self, 'foreign_keys'):
            if not 'foreign_keys' in tbl:
                tbl['foreign_keys'] = {}
            for k in list(self.foreign_keys.values()):
                tbls = dbschemas[k.ref_schema].tables
                tbl['foreign_keys'].update(self.foreign_keys[k.name].to_map(
                    self.column_names(),
                    tbls[self.foreign_keys[k.name].ref_table]. column_names()))
        if hasattr(self, 'unique_constraints'):
            if not 'unique_constraints' in tbl:
                tbl.update(unique_constraints={})
            for k in list(self.unique_constraints.values()):
                tbl['unique_constraints'].update(
                    self.unique_constraints[k.name].to_map(
                        self.column_names()))
        if hasattr(self, 'indexes'):
            if not 'indexes' in tbl:
                tbl['indexes'] = {}
            for k in list(self.indexes.values()):
                tbl['indexes'].update(self.indexes[k.name].to_map())
        if hasattr(self, 'inherits'):
            if not 'inherits' in tbl:
                tbl['inherits'] = self.inherits
        if hasattr(self, 'rules'):
            if not 'rules' in tbl:
                tbl['rules'] = {}
            for k in list(self.rules.values()):
                tbl['rules'].update(self.rules[k.name].to_map())
        if hasattr(self, 'triggers'):
            if not 'triggers' in tbl:
                tbl['triggers'] = {}
            for k in list(self.triggers.values()):
                tbl['triggers'].update(self.triggers[k.name].to_map())

        if not opts.no_privs and hasattr(self, 'privileges'):
            tbl.update({'privileges': self.map_privs()})

        return tbl

    def create(self):
        """Return SQL statements to CREATE the table

        :return: SQL statements
        """
        stmts = []
        if hasattr(self, 'created'):
            return stmts
        cols = []
        colprivs = []
        for col in self.columns:
            if not (hasattr(col, 'inherited') and col.inherited):
                cols.append("    " + col.add()[0])
            colprivs.append(col.add_privs())
        unlogged = ''
        if hasattr(self, 'unlogged') and self.unlogged:
            unlogged = 'UNLOGGED '
        inhclause = ''
        if hasattr(self, 'inherits'):
            inhclause = " INHERITS (%s)" % ", ".join(t for t in self.inherits)
        opts = ''
        if hasattr(self, 'options'):
            opts = " WITH (%s)" % ', '.join(self.options)
        tblspc = ''
        if hasattr(self, 'tablespace'):
            tblspc = " TABLESPACE %s" % self.tablespace
        stmts.append("CREATE %sTABLE %s (\n%s)%s%s%s" % (
            unlogged, self.qualname(), ",\n".join(cols), inhclause, opts,
            tblspc))
        if hasattr(self, 'owner'):
            stmts.append(self.alter_owner())
        if hasattr(self, 'privileges'):
            for priv in self.privileges:
                stmts.append(add_grant(self, priv))
        if colprivs:
            stmts.append(colprivs)
        if hasattr(self, 'description'):
            stmts.append(self.comment())
        for col in self.columns:
            if hasattr(col, 'description'):
                stmts.append(col.comment())
        self.created = True
        return stmts

    def drop(self):
        """Return a SQL DROP statement for the table

        :return: SQL statement
        """
        stmts = []
        if not hasattr(self, 'dropped') or not self.dropped:
            if hasattr(self, 'dependent_funcs'):
                for fnc in self.dependent_funcs:
                    stmts.append(fnc.drop())
            self.dropped = True
            stmts.append("DROP TABLE %s" % self.identifier())
        return stmts

    def diff_options(self, newopts):
        """Compare options lists and generate SQL SET or RESET clause

        :newopts: list of new options
        :return: SQL SET / RESET clauses

        Generate ([SET|RESET storage_parameter=value) clauses from two
        lists in the form of 'key=value' strings.
        """
        def to_dict(optlist):
            return dict(opt.split('=', 1) for opt in optlist)

        oldopts = {}
        if hasattr(self, 'options'):
            oldopts = to_dict(self.options)
        newopts = to_dict(newopts)
        setclauses = []
        for key, val in list(newopts.items()):
            if key not in oldopts:
                setclauses.append("%s=%s" % (key, val))
            elif val != oldopts[key]:
                setclauses.append("%s=%s" % (key, val))
        resetclauses = []
        for key, val in list(oldopts.items()):
            if key not in newopts:
                resetclauses.append("%s" % key)
        clauses = ''
        if setclauses:
            clauses = "SET (%s)" % ', '.join(setclauses)
            if resetclauses:
                clauses += ', '
        if resetclauses:
            clauses += "RESET (%s)" % ', '.join(resetclauses)
        return clauses

    def diff_map(self, intable):
        """Generate SQL to transform an existing table

        :param intable: a YAML map defining the new table
        :return: list of SQL statements

        Compares the table to an input table and generates SQL
        statements to transform it into the one represented by the
        input.
        """
        stmts = []
        if not hasattr(intable, 'columns'):
            raise KeyError("Table '%s' has no columns" % intable.name)
        colnames = [col.name for col in self.columns
                    if not hasattr(col, 'dropped')]
        dbcols = len(colnames)

        colprivs = []
        base = "ALTER %s %s\n    " % (self.objtype, self.qualname())
        # check input columns
        for (num, incol) in enumerate(intable.columns):
            if hasattr(incol, 'oldname'):
                assert(self.columns[num].name == incol.oldname)
                stmts.append(self.columns[num].rename(incol.name))
            # check existing columns
            if num < dbcols and self.columns[num].name == incol.name:
                (stmt, descr) = self.columns[num].diff_map(incol)
                if stmt:
                    stmts.append(base + stmt)
                colprivs.append(self.columns[num].diff_privileges(incol))
                if descr:
                    stmts.append(descr)
            # add new columns
            elif incol.name not in colnames and \
                    not hasattr(incol, 'inherited'):
                (stmt, descr) = incol.add()
                stmts.append(base + "ADD COLUMN %s" % stmt)
                colprivs.append(incol.add_privs())
                if descr:
                    stmts.append(descr)

        newopts = []
        if hasattr(intable, 'options'):
            newopts = intable.options
        diff_opts = self.diff_options(newopts)
        if diff_opts:
            stmts.append("ALTER %s %s %s" % (self.objtype, self.identifier(),
                                             diff_opts))
        if hasattr(intable, 'owner'):
            if intable.owner != self.owner:
                stmts.append(self.alter_owner(intable.owner))
        stmts.append(self.diff_privileges(intable))
        if colprivs:
            stmts.append(colprivs)
        if hasattr(intable, 'tablespace'):
            if not hasattr(self, 'tablespace') \
                    or self.tablespace != intable.tablespace:
                stmts.append(base + "SET TABLESPACE %s"
                             % quote_id(intable.tablespace))
        elif hasattr(self, 'tablespace'):
            stmts.append(base + "SET TABLESPACE pg_default")

        stmts.append(self.diff_description(intable))

        return stmts

    def data_export(self, dbconn, dirpath):
        """Copy table data out to a file

        :param dbconn: database connection to use
        :param dirpath: full path to the directory for the file to be created
        """
        filepath = os.path.join(dirpath, self.extern_filename('data'))
        if hasattr(self, 'primary_key'):
            order_by = [self.columns[col - 1].name
                        for col in self.primary_key.keycols]
        else:
            order_by = ['%d' % (n + 1) for n in range(len(self.columns))]
        dbconn.sql_copy_to(
            "COPY (SELECT * FROM %s ORDER BY %s) TO STDOUT WITH CSV" % (
            self.qualname(), ', '.join(order_by)), filepath)

    def data_import(self, dirpath):
        """Generate SQL to import data into a table

        :param dirpath: full path for the directory for the file
        :return: list of SQL statements
        """
        filepath = os.path.join(dirpath, self.extern_filename('data'))
        stmts = []
        if hasattr(self, 'referred_by'):
            stmts.append("ALTER TABLE %s DROP CONSTRAINT %s" % (
                self.referred_by._table.qualname(), self.referred_by.name))
        stmts.append("TRUNCATE ONLY %s" % self.qualname())
        stmts.append(("\\copy ", self.qualname(), " from '", filepath,
                      "' csv"))
        if hasattr(self, 'referred_by'):
            stmts.append(self.referred_by.add())
        return stmts


class View(DbClass):
    """A database view definition

    A view is identified by its schema name and view name.
    """

    objtype = "VIEW"
    privobjtype = "TABLE"

    @property
    def allprivs(self):
        return 'arwdDxt'

    def to_map(self, opts):
        """Convert a view to a YAML-suitable format

        :param opts: options to include/exclude tables, etc.
        :return: dictionary
        """
        if hasattr(opts, 'excl_tables') and opts.excl_tables \
                and self.name in opts.excl_tables:
            return None
        view = self._base_map(opts.no_owner, opts.no_privs)
        if hasattr(self, 'triggers'):
            for key in list(self.triggers.values()):
                view['triggers'].update(self.triggers[key.name].to_map())
        return view

    @commentable
    @grantable
    @ownable
    def create(self, newdefn=None):
        """Return SQL statements to CREATE the table

        :return: SQL statements
        """
        defn = newdefn or self.definition
        if defn[-1:] == ';':
            defn = defn[:-1]
        return ["CREATE%s VIEW %s AS\n   %s" % (
                newdefn and " OR REPLACE" or '', self.qualname(), defn)]

    def diff_map(self, inview):
        """Generate SQL to transform an existing view

        :param inview: a YAML map defining the new view
        :return: list of SQL statements

        Compares the view to an input view and generates SQL
        statements to transform it into the one represented by the
        input.
        """
        stmts = []
        if self.definition != inview.definition:
            stmts.append(self.create(inview.definition))
        if hasattr(inview, 'owner'):
            if inview.owner != self.owner:
                stmts.append(self.alter_owner(inview.owner))
        stmts.append(self.diff_privileges(inview))
        stmts.append(self.diff_description(inview))
        return stmts


class MaterializedView(View):
    """A materialized view definition

    A materialized view is identified by its schema name and view name.
    """

    objtype = "MATERIALIZED VIEW"

    def to_map(self, opts):
        """Convert a materialized view to a YAML-suitable format

        :param opts: options to include/exclude tables, etc.
        :return: dictionary
        """
        if hasattr(opts, 'excl_tables') and opts.excl_tables \
                and self.name in opts.excl_tables:
            return None
        mvw = self._base_map(opts.no_owner, opts.no_privs)
        if hasattr(self, 'indexes'):
            if not 'indexes' in mvw:
                mvw['indexes'] = {}
            for k in list(self.indexes.values()):
                mvw['indexes'].update(self.indexes[k.name].to_map())
        return mvw

    @commentable
    @grantable
    @ownable
    def create(self, newdefn=None):
        """Return SQL statements to CREATE the materialized view

        :return: SQL statements
        """
        defn = newdefn or self.definition
        if defn[-1:] == ';':
            defn = defn[:-1]
        return ["CREATE %s %s AS\n   %s" % (
                self.objtype, self.qualname(), defn)]

    def diff_map(self, inview):
        """Generate SQL to transform an existing materialized view

        :param inview: a YAML map defining the new view
        :return: list of SQL statements

        Compares the view to an input view and generates SQL
        statements to transform it into the one represented by the
        input.
        """
        stmts = []
        if self.definition != inview.definition:
            stmts.append(self.create(inview.definition))
        if hasattr(inview, 'owner'):
            if inview.owner != self.owner:
                stmts.append(self.alter_owner(inview.owner))
        stmts.append(self.diff_privileges(inview))
        stmts.append(self.diff_description(inview))
        return stmts


QUERY_PRE91 = \
    """SELECT nspname AS schema, relname AS name, relkind AS kind,
              reloptions AS options, spcname AS tablespace,
              rolname AS owner, array_to_string(relacl, ',') AS privileges,
              CASE WHEN relkind = 'v' THEN pg_get_viewdef(c.oid, TRUE)
                   ELSE '' END AS definition,
              obj_description(c.oid, 'pg_class') AS description
       FROM pg_class c
            JOIN pg_roles r ON (r.oid = relowner)
            JOIN pg_namespace ON (relnamespace = pg_namespace.oid)
            LEFT JOIN pg_tablespace t ON (reltablespace = t.oid)
       WHERE relkind in ('r', 'S', 'v')
             AND (nspname != 'pg_catalog'
                  AND nspname != 'information_schema')
       ORDER BY nspname, relname"""

QUERY_PRE93 = \
    """SELECT nspname AS schema, relname AS name, relkind AS kind,
              reloptions AS options, relpersistence AS persistence,
              spcname AS tablespace, rolname AS owner,
              array_to_string(relacl, ',') AS privileges,
              CASE WHEN relkind = 'v' THEN pg_get_viewdef(c.oid, TRUE)
                   ELSE '' END AS definition,
              obj_description(c.oid, 'pg_class') AS description
       FROM pg_class c
            JOIN pg_roles r ON (r.oid = relowner)
            JOIN pg_namespace ON (relnamespace = pg_namespace.oid)
            LEFT JOIN pg_tablespace t ON (reltablespace = t.oid)
       WHERE relkind in ('r', 'S', 'v')
             AND (nspname != 'pg_catalog'
                  AND nspname != 'information_schema')
       ORDER BY nspname, relname"""

OBJTYPES = ['table', 'sequence', 'view', 'materialized view']


class ClassDict(DbObjectDict):
    "The collection of tables and similar objects in a database"

    cls = DbClass
    query = \
        """SELECT nspname AS schema, relname AS name, relkind AS kind,
                  reloptions AS options, relpersistence AS persistence,
                  spcname AS tablespace, rolname AS owner,
                  array_to_string(relacl, ',') AS privileges,
                  CASE WHEN relkind ~ '[vm]' THEN pg_get_viewdef(c.oid, TRUE)
                       ELSE '' END AS definition,
                  CASE WHEN relkind = 'm' THEN relispopulated
                       ELSE FALSE END AS with_data,
                  obj_description(c.oid, 'pg_class') AS description
           FROM pg_class c
                JOIN pg_roles r ON (r.oid = relowner)
                JOIN pg_namespace ON (relnamespace = pg_namespace.oid)
                LEFT JOIN pg_tablespace t ON (reltablespace = t.oid)
           WHERE relkind in ('r', 'S', 'v', 'm')
                 AND (nspname != 'pg_catalog'
                      AND nspname != 'information_schema')
           ORDER BY nspname, relname"""

    inhquery = \
        """SELECT inhrelid::regclass AS sub, inhparent::regclass AS parent,
                  inhseqno
           FROM pg_inherits
           ORDER BY 1, 3"""

    def _from_catalog(self):
        """Initialize the dictionary of tables by querying the catalogs"""
        if self.dbconn.version < 90100:
            self.query = QUERY_PRE91
        elif self.dbconn.version < 90300:
            self.query = QUERY_PRE93
        for table in self.fetch():
            sch, tbl = table.key()
            if hasattr(table, 'privileges'):
                table.privileges = table.privileges.split(',')
            if hasattr(table, 'persistence'):
                if table.persistence == 'u':
                    table.unlogged = True
                del table.persistence
            kind = table.kind
            del table.kind
            if kind == 'r':
                self[(sch, tbl)] = Table(**table.__dict__)
            elif kind == 'S':
                self[(sch, tbl)] = inst = Sequence(**table.__dict__)
                inst.get_attrs(self.dbconn)
                inst.get_dependent_table(self.dbconn)
            elif kind == 'v':
                self[(sch, tbl)] = View(**table.__dict__)
            elif kind == 'm':
                self[(sch, tbl)] = MaterializedView(**table.__dict__)
        inhtbls = self.dbconn.fetchall(self.inhquery)
        self.dbconn.rollback()
        for (tbl, partbl, num) in inhtbls:
            (sch, tbl) = split_schema_obj(tbl)
            table = self[(sch, tbl)]
            if not hasattr(table, 'inherits'):
                table.inherits = []
            table.inherits.append(partbl)

    def from_map(self, schema, inobjs, newdb):
        """Initalize the dictionary of tables by converting the input map

        :param schema: schema owning the tables
        :param inobjs: YAML map defining the schema objects
        :param newdb: collection of dictionaries defining the database
        """
        for k in inobjs:
            inobj = inobjs[k]
            objtype = None
            for typ in OBJTYPES:
                if k.startswith(typ):
                    objtype = typ
                    key = k[len(typ) + 1:]
            if objtype is None:
                raise KeyError("Unrecognized object type: %s" % k)
            if objtype == 'table':
                self[(schema.name, key)] = table = Table(
                    schema=schema.name, name=key)
                intable = inobj
                if not intable:
                    raise ValueError("Table '%s' has no specification" % k)
                for attr in ['inherits', 'owner', 'tablespace', 'oldname',
                             'description', 'options', 'unlogged']:
                    if attr in intable:
                        setattr(table, attr, intable[attr])
                try:
                    newdb.columns.from_map(table, intable['columns'])
                except KeyError as exc:
                    exc.args = ("Table '%s' has no columns" % key, )
                    raise
                newdb.constraints.from_map(table, intable)
                if 'indexes' in intable:
                    newdb.indexes.from_map(table, intable['indexes'])
                if 'rules' in intable:
                    newdb.rules.from_map(table, intable['rules'])
                if 'triggers' in intable:
                    newdb.triggers.from_map(table, intable['triggers'])
            elif objtype == 'sequence':
                self[(schema.name, key)] = seq = Sequence(
                    schema=schema.name, name=key)
                inseq = inobj
                if not inseq:
                    raise ValueError("Sequence '%s' has no specification" % k)
                for attr, val in list(inseq.items()):
                    setattr(seq, attr, val)
            elif objtype == 'view':
                self[(schema.name, key)] = view = View(
                    schema=schema.name, name=key)
                inview = inobj
                if not inview:
                    raise ValueError("View '%s' has no specification" % k)
                for attr, val in list(inview.items()):
                    setattr(view, attr, val)
                if 'triggers' in inview:
                    newdb.triggers.from_map(view, inview['triggers'])
            elif objtype == 'materialized view':
                self[(schema.name, key)] = mview = MaterializedView(
                    schema=schema.name, name=key)
                inmview = inobj
                if not inmview:
                    raise ValueError("View '%s' has no specification" % k)
                for attr, val in list(inmview.items()):
                    setattr(mview, attr, val)
            else:
                raise KeyError("Unrecognized object type: %s" % k)
            obj = self[(schema.name, key)]
            if 'privileges' in inobj:
                    if not hasattr(obj, 'owner'):
                        raise ValueError("%s '%s' has privileges but no "
                                         "owner information" %
                                         obj.objtype.capital(), table.name)
                    obj.privileges = privileges_from_map(
                        inobj['privileges'], obj.allprivs, obj.owner)

    def link_refs(self, dbcolumns, dbconstrs, dbindexes, dbrules, dbtriggers):
        """Connect columns, constraints, etc. to their respective tables

        :param dbcolumns: dictionary of columns
        :param dbconstrs: dictionary of constraints
        :param dbindexes: dictionary of indexes
        :param dbrules: dictionary of rules
        :param dbtriggers: dictionary of triggers

        Links each list of table columns in `dbcolumns` to the
        corresponding table. Fills the `foreign_keys`,
        `unique_constraints`, `indexes` and `triggers` dictionaries
        for each table by traversing the `dbconstrs`, `dbindexes` and
        `dbtriggers` dictionaries, which are keyed by schema, table
        and constraint, index or trigger name.
        """
        for (sch, tbl) in dbcolumns:
            if (sch, tbl) in self:
                assert isinstance(self[(sch, tbl)], Table)
                self[(sch, tbl)].columns = dbcolumns[(sch, tbl)]
                for col in dbcolumns[(sch, tbl)]:
                    col._table = self[(sch, tbl)]
        for (sch, tbl) in self:
            table = self[(sch, tbl)]
            if isinstance(table, Sequence) and hasattr(table, 'owner_table'):
                if isinstance(table.owner_column, int):
                    table.owner_column = self[(sch, table.owner_table)]. \
                        column_names()[table.owner_column - 1]
            elif isinstance(table, Table) and hasattr(table, 'inherits'):
                for partbl in table.inherits:
                    (parsch, partbl) = split_schema_obj(partbl)
                    assert self[(parsch, partbl)]
                    parent = self[(parsch, partbl)]
                    if not hasattr(parent, 'descendants'):
                        parent.descendants = []
                    parent.descendants.append(table)
        for (sch, tbl, cns) in dbconstrs:
            constr = dbconstrs[(sch, tbl, cns)]
            if hasattr(constr, 'target'):
                continue
            assert self[(sch, tbl)]
            constr._table = table = self[(sch, tbl)]
            if isinstance(constr, CheckConstraint):
                if not hasattr(table, 'check_constraints'):
                    table.check_constraints = {}
                table.check_constraints.update({cns: constr})
            elif isinstance(constr, PrimaryKey):
                table.primary_key = constr
            elif isinstance(constr, ForeignKey):
                if not hasattr(table, 'foreign_keys'):
                    table.foreign_keys = {}
                # link referenced and referrer
                constr.references = self[(constr.ref_schema, constr.ref_table)]
                # TODO: there can be more than one
                self[(constr.ref_schema, constr.ref_table)].referred_by = \
                    constr
                table.foreign_keys.update({cns: constr})
            elif isinstance(constr, UniqueConstraint):
                if not hasattr(table, 'unique_constraints'):
                    table.unique_constraints = {}
                table.unique_constraints.update({cns: constr})

        def link_one(targdict, schema, tbl, objkey, objtype):
            table = self[(schema, tbl)]
            if not hasattr(table, objtype):
                setattr(table, objtype, {})
            objdict = getattr(table, objtype)
            objdict.update({objkey: targdict[(schema, tbl, objkey)]})

        for (sch, tbl, idx) in dbindexes:
            link_one(dbindexes, sch, tbl, idx, 'indexes')
        for (sch, tbl, rul) in dbrules:
            link_one(dbrules, sch, tbl, rul, 'rules')
            dbrules[(sch, tbl, rul)]._table = self[(sch, tbl)]
        for (sch, tbl, trg) in dbtriggers:
            link_one(dbtriggers, sch, tbl, trg, 'triggers')
            dbtriggers[(sch, tbl, trg)]._table = self[(sch, tbl)]

    def _rename(self, obj, objtype):
        """Process a RENAME"""
        stmt = ''
        oldname = obj.oldname
        try:
            stmt = self[(obj.schema, oldname)].rename(obj.name)
            self[(obj.schema, obj.name)] = self[(obj.schema, oldname)]
            del self[(obj.schema, oldname)]
        except KeyError as exc:
            exc.args = ("Previous name '%s' for %s '%s' not found" % (
                oldname, objtype, obj.name), )
            raise
        return stmt

    def diff_map(self, intables):
        """Generate SQL to transform existing tables and sequences

        :param intables: a YAML map defining the new tables/sequences
        :return: list of SQL statements

        Compares the existing table/sequence definitions, as fetched
        from the catalogs, to the input map and generates SQL
        statements to transform the tables/sequences accordingly.
        """
        stmts = []
        # first pass: sequences owned by a table
        for (sch, seq) in intables:
            inseq = intables[(sch, seq)]
            if not isinstance(inseq, Sequence) or \
                    not hasattr(inseq, 'owner_table'):
                continue
            if (sch, seq) not in self:
                if hasattr(inseq, 'oldname'):
                    stmts.append(self._rename(inseq, "sequence"))
                else:
                    # create new sequence
                    stmts.append(inseq.create())

        # check input tables
        inhstack = []
        for (sch, tbl) in intables:
            intable = intables[(sch, tbl)]
            if not isinstance(intable, Table):
                continue
            # does it exist in the database?
            if (sch, tbl) not in self:
                if not hasattr(intable, 'oldname'):
                    # create new table
                    if hasattr(intable, 'inherits'):
                        inhstack.append(intable)
                    else:
                        stmts.append(intable.create())
                else:
                    stmts.append(self._rename(intable, "table"))
        while len(inhstack):
            intable = inhstack.pop()
            createit = True
            for partbl in intable.inherits:
                if intables[split_schema_obj(partbl)] in inhstack:
                    createit = False
            if createit:
                stmts.append(intable.create())
            else:
                inhstack.insert(0, intable)

        # check input views
        for (sch, tbl) in intables:
            intable = intables[(sch, tbl)]
            if not isinstance(intable, View):
                continue
            # does it exist in the database?
            if (sch, tbl) not in self:
                if hasattr(intable, 'oldname'):
                    stmts.append(self._rename(intable, "view"))
                else:
                    # create new view
                    stmts.append(intable.create())

        # second pass: input sequences not owned by tables
        for (sch, seq) in intables:
            inseq = intables[(sch, seq)]
            if not isinstance(inseq, Sequence):
                continue
            # does it exist in the database?
            if (sch, seq) not in self:
                if hasattr(inseq, 'oldname'):
                    stmts.append(self._rename(inseq, "sequence"))
                elif hasattr(inseq, 'owner_table'):
                    stmts.append(inseq.add_owner())
                else:
                    # create new sequence
                    stmts.append(inseq.create())

        # check database tables, sequences and views
        for (sch, tbl) in self:
            table = self[(sch, tbl)]
            # if missing, mark it for dropping
            if (sch, tbl) not in intables:
                table.dropped = False
            else:
                # check table/sequence/view objects
                stmts.append(table.diff_map(intables[(sch, tbl)]))

        # now drop the marked tables
        for (sch, tbl) in self:
            table = self[(sch, tbl)]
            if isinstance(table, Sequence) and hasattr(table, 'owner_table'):
                continue
            if hasattr(table, 'dropped') and not table.dropped:
                # first, drop all foreign keys
                if hasattr(table, 'foreign_keys'):
                    for fgn in table.foreign_keys:
                        stmts.append(table.foreign_keys[fgn].drop())
                # and drop the triggers
                if hasattr(table, 'triggers'):
                    for trg in table.triggers:
                        stmts.append(table.triggers[trg].drop())
                if hasattr(table, 'rules'):
                    for rul in table.rules:
                        stmts.append(table.rules[rul].drop())
                # drop views
                if isinstance(table, View):
                    stmts.append(table.drop())

        inhstack = []
        for (sch, tbl) in self:
            table = self[(sch, tbl)]
            if (isinstance(table, Sequence)
                    and (hasattr(table, 'owner_table')
                         or hasattr(table, 'dependent_table'))) \
                    or isinstance(table, View):
                continue
            if hasattr(table, 'dropped') and not table.dropped:
                # next, drop other subordinate objects
                if hasattr(table, 'check_constraints'):
                    for chk in table.check_constraints:
                        stmts.append(table.check_constraints[chk].drop())
                if hasattr(table, 'unique_constraints'):
                    for unq in table.unique_constraints:
                        stmts.append(table.unique_constraints[unq].drop())
                if hasattr(table, 'indexes'):
                    for idx in table.indexes:
                        stmts.append(table.indexes[idx].drop())
                if hasattr(table, 'rules'):
                    for rul in table.rules:
                        stmts.append(table.rules[rul].drop())
                if hasattr(table, 'primary_key'):
                    # TODO there can be more than one referred_by
                    if hasattr(table, 'referred_by'):
                        stmts.append(table.referred_by.drop())
                    stmts.append(table.primary_key.drop())
                # finally, drop the table itself
                if hasattr(table, 'descendants'):
                    inhstack.append(table)
                else:
                    stmts.append(table.drop())
        while len(inhstack):
            table = inhstack.pop()
            dropit = True
            for childtbl in table.descendants:
                if self[(childtbl.schema, childtbl.name)] in inhstack:
                    dropit = False
            if dropit:
                stmts.append(table.drop())
            else:
                inhstack.insert(0, table)
        for (sch, tbl) in self:
            table = self[(sch, tbl)]
            if isinstance(table, Sequence) \
                    and hasattr(table, 'dependent_table') \
                    and hasattr(table, 'dropped') and not table.dropped:
                stmts.append(table.drop())

        # last pass to deal with nextval DEFAULTs
        for (sch, tbl) in intables:
            intable = intables[(sch, tbl)]
            if not isinstance(intable, Table):
                continue
            if (sch, tbl) not in self:
                for col in intable.columns:
                    if hasattr(col, 'default') \
                            and col.default.startswith('nextval'):
                        stmts.append(col.set_sequence_default())

        return stmts

########NEW FILE########
__FILENAME__ = textsearch
# -*- coding: utf-8 -*-
"""
    pyrseas.dbobject.textsearch
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~

    This defines eight classes: TSConfiguration, TSDictionary,
    TSParser and TSTemplate derived from DbSchemaObject, and
    TSConfigurationDict, TSDictionaryDict, TSParserDict and
    TSTemplateDict derived from DbObjectDict.
"""
from pyrseas.dbobject import DbObjectDict, DbSchemaObject
from pyrseas.dbobject import commentable, ownable


class TSConfiguration(DbSchemaObject):
    """A text search configuration definition"""

    keylist = ['schema', 'name']
    objtype = "TEXT SEARCH CONFIGURATION"
    single_extern_file = True

    def to_map(self, no_owner):
        """Convert a text search configuration to a YAML-suitable format

        :return: dictionary
        """
        dct = self._base_map(no_owner)
        if '.' in self.parser:
            (sch, pars) = self.parser.split('.')
            if sch == self.schema:
                dct['parser'] = pars
        return dct

    @commentable
    @ownable
    def create(self):
        """Return SQL statements to CREATE the configuration

        :return: SQL statements
        """
        clauses = []
        clauses.append("PARSER = %s" % self.parser)
        return ["CREATE TEXT SEARCH CONFIGURATION %s (\n    %s)" % (
                self.qualname(), ',\n    '.join(clauses))]


class TSConfigurationDict(DbObjectDict):
    "The collection of text search configurations in a database"

    cls = TSConfiguration
    query = \
        """SELECT nc.nspname AS schema, cfgname AS name, rolname AS owner,
                  np.nspname || '.' || prsname AS parser,
                  obj_description(c.oid, 'pg_ts_config') AS description
           FROM pg_ts_config c
                JOIN pg_roles r ON (r.oid = cfgowner)
                JOIN pg_ts_parser p ON (cfgparser = p.oid)
                JOIN pg_namespace nc ON (cfgnamespace = nc.oid)
                JOIN pg_namespace np ON (prsnamespace = np.oid)
           WHERE (nc.nspname != 'pg_catalog'
                  AND nc.nspname != 'information_schema')
           ORDER BY nc.nspname, cfgname"""

    def from_map(self, schema, inconfigs):
        """Initialize the dictionary of configs by examining the input map

        :param schema: schema owning the configurations
        :param inconfigs: input YAML map defining the configurations
        """
        for key in inconfigs:
            if not key.startswith('text search configuration '):
                raise KeyError("Unrecognized object type: %s" % key)
            tsc = key[26:]
            self[(schema.name, tsc)] = config = TSConfiguration(
                schema=schema.name, name=tsc)
            inconfig = inconfigs[key]
            if inconfig:
                for attr, val in list(inconfig.items()):
                    setattr(config, attr, val)
                if 'oldname' in inconfig:
                    config.oldname = inconfig['oldname']
                    del inconfig['oldname']
                if 'description' in inconfig:
                    config.description = inconfig['description']

    def diff_map(self, inconfigs):
        """Generate SQL to transform existing configurations

        :param input_map: a YAML map defining the new configurations
        :return: list of SQL statements

        Compares the existing configuration definitions, as fetched from the
        catalogs, to the input map and generates SQL statements to
        transform the configurations accordingly.
        """
        stmts = []
        # check input configurations
        for (sch, tsc) in inconfigs:
            intsc = inconfigs[(sch, tsc)]
            # does it exist in the database?
            if (sch, tsc) in self:
                stmts.append(self[(sch, tsc)].diff_map(intsc))
            else:
                # check for possible RENAME
                if hasattr(intsc, 'oldname'):
                    oldname = intsc.oldname
                    try:
                        stmts.append(self[oldname].rename(intsc.name))
                        del self[oldname]
                    except KeyError as exc:
                        exc.args = ("Previous name '%s' for configuration "
                                    "'%s' not found" % (oldname, intsc.name), )
                        raise
                else:
                    # create new configuration
                    stmts.append(intsc.create())
        # check database configurations
        for (sch, tsc) in self:
            # if missing, drop it
            if (sch, tsc) not in inconfigs:
                stmts.append(self[(sch, tsc)].drop())
        return stmts


class TSDictionary(DbSchemaObject):
    """A text search dictionary definition"""

    keylist = ['schema', 'name']
    objtype = "TEXT SEARCH DICTIONARY"
    single_extern_file = True

    @commentable
    @ownable
    def create(self):
        """Return SQL statements to CREATE the dictionary

        :return: SQL statements
        """
        clauses = []
        clauses.append("TEMPLATE = %s" % self.template)
        if hasattr(self, 'options'):
            clauses.append(self.options)
        return ["CREATE TEXT SEARCH DICTIONARY %s (\n    %s)" % (
                self.qualname(), ',\n    '.join(clauses))]


class TSDictionaryDict(DbObjectDict):
    "The collection of text search dictionaries in a database"

    cls = TSDictionary
    query = \
        """SELECT nspname AS schema, dictname AS name, rolname AS owner,
                  tmplname AS template, dictinitoption AS options,
                  obj_description(d.oid, 'pg_ts_dict') AS description
           FROM pg_ts_dict d JOIN pg_ts_template t ON (dicttemplate = t.oid)
                JOIN pg_roles r ON (r.oid = dictowner)
                JOIN pg_namespace n ON (dictnamespace = n.oid)
           WHERE (nspname != 'pg_catalog' AND nspname != 'information_schema')
           ORDER BY nspname, dictname"""

    def from_map(self, schema, indicts):
        """Initialize the dictionary of dictionaries by examining the input map

        :param schema: schema owning the dictionaries
        :param indicts: input YAML map defining the dictionaries
        """
        for key in indicts:
            if not key.startswith('text search dictionary '):
                raise KeyError("Unrecognized object type: %s" % key)
            tsd = key[23:]
            self[(schema.name, tsd)] = tsdict = TSDictionary(
                schema=schema.name, name=tsd)
            indict = indicts[key]
            if indict:
                for attr, val in list(indict.items()):
                    setattr(tsdict, attr, val)
                if 'oldname' in indict:
                    tsdict.oldname = indict['oldname']
                    del indict['oldname']
                if 'description' in indict:
                    tsdict.description = indict['description']

    def diff_map(self, indicts):
        """Generate SQL to transform existing dictionaries

        :param input_map: a YAML map defining the new dictionaries
        :return: list of SQL statements

        Compares the existing dictionary definitions, as fetched from the
        catalogs, to the input map and generates SQL statements to
        transform the dictionaries accordingly.
        """
        stmts = []
        # check input dictionaries
        for (sch, tsd) in indicts:
            intsd = indicts[(sch, tsd)]
            # does it exist in the database?
            if (sch, tsd) in self:
                stmts.append(self[(sch, tsd)].diff_map(intsd))
            else:
                # check for possible RENAME
                if hasattr(intsd, 'oldname'):
                    oldname = intsd.oldname
                    try:
                        stmts.append(self[oldname].rename(intsd.name))
                        del self[oldname]
                    except KeyError as exc:
                        exc.args = ("Previous name '%s' for dictionary '%s' "
                                    "not found" % (oldname, intsd.name), )
                        raise
                else:
                    # create new dictionary
                    stmts.append(intsd.create())
        # check database dictionaries
        for (sch, tsd) in self:
            # if missing, drop it
            if (sch, tsd) not in indicts:
                stmts.append(self[(sch, tsd)].drop())
        return stmts


class TSParser(DbSchemaObject):
    """A text search parser definition"""

    keylist = ['schema', 'name']
    objtype = "TEXT SEARCH PARSER"
    single_extern_file = True

    @commentable
    @ownable
    def create(self):
        """Return SQL statements to CREATE the parser

        :return: SQL statements
        """
        clauses = []
        for attr in ['start', 'gettoken', 'end', 'lextypes']:
            clauses.append("%s = %s" % (attr.upper(), getattr(self, attr)))
        if hasattr(self, 'headline'):
            clauses.append("HEADLINE = %s" % self.headline)
        return ["CREATE TEXT SEARCH PARSER %s (\n    %s)" % (
                self.qualname(), ',\n    '.join(clauses))]


class TSParserDict(DbObjectDict):
    "The collection of text search parsers in a database"

    cls = TSParser
    query = \
        """SELECT nspname AS schema, prsname AS name,
                  prsstart::regproc AS start, prstoken::regproc AS gettoken,
                  prsend::regproc AS end, prslextype::regproc AS lextypes,
                  prsheadline::regproc AS headline,
                  obj_description(p.oid, 'pg_ts_parser') AS description
           FROM pg_ts_parser p
                JOIN pg_namespace n ON (prsnamespace = n.oid)
           WHERE (nspname != 'pg_catalog' AND nspname != 'information_schema')
           ORDER BY nspname, prsname"""

    def from_map(self, schema, inparsers):
        """Initialize the dictionary of parsers by examining the input map

        :param schema: schema owning the parsers
        :param inparsers: input YAML map defining the parsers
        """
        for key in inparsers:
            if not key.startswith('text search parser '):
                raise KeyError("Unrecognized object type: %s" % key)
            tsp = key[19:]
            self[(schema.name, tsp)] = parser = TSParser(
                schema=schema.name, name=tsp)
            inparser = inparsers[key]
            if inparser:
                for attr, val in list(inparser.items()):
                    setattr(parser, attr, val)
                if 'oldname' in inparser:
                    parser.oldname = inparser['oldname']
                    del inparser['oldname']
                if 'description' in inparser:
                    parser.description = inparser['description']

    def diff_map(self, inparsers):
        """Generate SQL to transform existing parsers

        :param input_map: a YAML map defining the new parsers
        :return: list of SQL statements

        Compares the existing parser definitions, as fetched from the
        catalogs, to the input map and generates SQL statements to
        transform the parsers accordingly.
        """
        stmts = []
        # check input parsers
        for (sch, tsp) in inparsers:
            intsp = inparsers[(sch, tsp)]
            # does it exist in the database?
            if (sch, tsp) in self:
                stmts.append(self[(sch, tsp)].diff_map(intsp))
            else:
                # check for possible RENAME
                if hasattr(intsp, 'oldname'):
                    oldname = intsp.oldname
                    try:
                        stmts.append(self[oldname].rename(intsp.name))
                        del self[oldname]
                    except KeyError as exc:
                        exc.args = ("Previous name '%s' for parser '%s' "
                                    "not found" % (oldname, intsp.name), )
                        raise
                else:
                    # create new parser
                    stmts.append(intsp.create())
        # check database parsers
        for (sch, tsp) in self:
            # if missing, drop it
            if (sch, tsp) not in inparsers:
                stmts.append(self[(sch, tsp)].drop())
        return stmts


class TSTemplate(DbSchemaObject):
    """A text search template definition"""

    keylist = ['schema', 'name']
    objtype = "TEXT SEARCH TEMPLATE"
    single_extern_file = True

    @commentable
    def create(self):
        """Return SQL statements to CREATE the template

        :return: SQL statements
        """
        clauses = []
        if hasattr(self, 'init'):
            clauses.append("INIT = %s" % self.init)
        clauses.append("LEXIZE = %s" % self.lexize)
        return ["CREATE TEXT SEARCH TEMPLATE %s (\n    %s)" % (
                self.qualname(), ',\n    '.join(clauses))]


class TSTemplateDict(DbObjectDict):
    "The collection of text search templates in a database"

    cls = TSTemplate
    query = \
        """SELECT nspname AS schema, tmplname AS name,
                  tmplinit::regproc AS init, tmpllexize::regproc AS lexize,
                  obj_description(p.oid, 'pg_ts_template') AS description
           FROM pg_ts_template p
                JOIN pg_namespace n ON (tmplnamespace = n.oid)
           WHERE (nspname != 'pg_catalog' AND nspname != 'information_schema')
           ORDER BY nspname, tmplname"""

    def from_map(self, schema, intemplates):
        """Initialize the dictionary of templates by examining the input map

        :param schema: schema owning the templates
        :param intemplates: input YAML map defining the templates
        """
        for key in intemplates:
            if not key.startswith('text search template '):
                raise KeyError("Unrecognized object type: %s" % key)
            tst = key[21:]
            self[(schema.name, tst)] = template = TSTemplate(
                schema=schema.name, name=tst)
            intemplate = intemplates[key]
            if intemplate:
                for attr, val in list(intemplate.items()):
                    setattr(template, attr, val)
                if 'oldname' in intemplate:
                    template.oldname = intemplate['oldname']
                    del intemplate['oldname']
                if 'description' in intemplate:
                    template.description = intemplate['description']

    def diff_map(self, intemplates):
        """Generate SQL to transform existing templates

        :param input_map: a YAML map defining the new templates
        :return: list of SQL statements

        Compares the existing template definitions, as fetched from the
        catalogs, to the input map and generates SQL statements to
        transform the templates accordingly.
        """
        stmts = []
        # check input templates
        for (sch, tst) in intemplates:
            intst = intemplates[(sch, tst)]
            # does it exist in the database?
            if (sch, tst) in self:
                stmts.append(self[(sch, tst)].diff_map(intst))
            else:
                # check for possible RENAME
                if hasattr(intst, 'oldname'):
                    oldname = intst.oldname
                    try:
                        stmts.append(self[oldname].rename(intst.name))
                        del self[oldname]
                    except KeyError as exc:
                        exc.args = ("Previous name '%s' for template '%s' "
                                    "not found" % (oldname, intst.name), )
                        raise
                else:
                    # create new template
                    stmts.append(intst.create())
        # check database templates
        for (sch, tst) in self:
            # if missing, drop it
            if (sch, tst) not in intemplates:
                stmts.append(self[(sch, tst)].drop())
        return stmts

########NEW FILE########
__FILENAME__ = trigger
# -*- coding: utf-8 -*-
"""
    pyrseas.dbobject.trigger
    ~~~~~~~~~~~~~~~~~~~~~~~~

    This module defines two classes: Trigger derived from
    DbSchemaObject, and TriggerDict derived from DbObjectDict.
"""
from pyrseas.dbobject import DbObjectDict, DbSchemaObject
from pyrseas.dbobject import quote_id, commentable

EXEC_PROC = 'EXECUTE PROCEDURE '
EVENT_TYPES = ['INSERT', 'UPDATE', 'DELETE', 'TRUNCATE']


class Trigger(DbSchemaObject):
    """A procedural language trigger"""

    keylist = ['schema', 'table', 'name']
    objtype = "TRIGGER"

    def identifier(self):
        """Returns a full identifier for the trigger

        :return: string
        """
        return "%s ON %s" % (quote_id(self.name), self._table.qualname())

    def to_map(self):
        """Convert a trigger to a YAML-suitable format

        :return: dictionary
        """
        dct = self._base_map()
        del dct['_table']
        if hasattr(self, 'columns'):
            dct['columns'] = [self._table.column_names()[int(k) - 1]
                              for k in self.columns.split()]
        return {self.name: dct}

    @commentable
    def create(self):
        """Return SQL statements to CREATE the trigger

        :return: SQL statements
        """
        constr = defer = ''
        if hasattr(self, 'constraint') and self.constraint:
            constr = "CONSTRAINT "
            if hasattr(self, 'deferrable') and self.deferrable:
                defer = "DEFERRABLE "
            if hasattr(self, 'initially_deferred') and self.initially_deferred:
                defer += "INITIALLY DEFERRED"
            if defer:
                defer = '\n    ' + defer
        evts = " OR ".join(self.events).upper()
        if hasattr(self, 'columns') and 'update' in self.events:
            evts = evts.replace("UPDATE", "UPDATE OF %s" % (
                ", ".join(self.columns)))
        cond = ''
        if hasattr(self, 'condition'):
            cond = "\n    WHEN (%s)" % self.condition
        return ["CREATE %sTRIGGER %s\n    %s %s ON %s%s\n    FOR EACH %s"
                "%s\n    EXECUTE PROCEDURE %s" % (
                constr, quote_id(self.name), self.timing.upper(), evts,
                self._table.qualname(), defer,
                self.level.upper(), cond, self.procedure)]


QUERY_PRE90 = \
    """SELECT nspname AS schema, relname AS table,
              tgname AS name, tgisconstraint AS constraint,
              tgdeferrable AS deferrable,
              tginitdeferred AS initially_deferred,
              pg_get_triggerdef(t.oid) AS definition,
              NULL AS columns,
              obj_description(t.oid, 'pg_trigger') AS description
       FROM pg_trigger t
            JOIN pg_class c ON (t.tgrelid = c.oid)
            JOIN pg_namespace n ON (c.relnamespace = n.oid)
            JOIN pg_roles ON (n.nspowner = pg_roles.oid)
            LEFT JOIN pg_constraint cn ON (tgconstraint = cn.oid)
       WHERE contype != 'f' OR contype IS NULL
         AND (nspname != 'pg_catalog' AND nspname != 'information_schema')
       ORDER BY 1, 2, 3"""


class TriggerDict(DbObjectDict):
    "The collection of triggers in a database"

    cls = Trigger
    query = \
        """SELECT nspname AS schema, relname AS table,
                  tgname AS name, pg_get_triggerdef(t.oid) AS definition,
                  CASE WHEN contype = 't' THEN true ELSE false END AS
                       constraint,
                  tgdeferrable AS deferrable,
                  tginitdeferred AS initially_deferred,
                  tgattr AS columns,
                  obj_description(t.oid, 'pg_trigger') AS description
           FROM pg_trigger t
                JOIN pg_class c ON (t.tgrelid = c.oid)
                JOIN pg_namespace n ON (c.relnamespace = n.oid)
                JOIN pg_roles ON (n.nspowner = pg_roles.oid)
                LEFT JOIN pg_constraint cn ON (tgconstraint = cn.oid)
           WHERE NOT tgisinternal
             AND (nspname != 'pg_catalog' AND nspname != 'information_schema')
           ORDER BY 1, 2, 3"""

    def _from_catalog(self):
        """Initialize the dictionary of triggers by querying the catalogs"""
        if self.dbconn.version < 90000:
            self.query = QUERY_PRE90
        for trig in self.fetch():
            for timing in ['BEFORE', 'AFTER', 'INSTEAD OF']:
                timspc = timing + ' '
                if timspc in trig.definition:
                    trig.timing = timing.lower()
                    evtstart = trig.definition.index(timspc) + len(timspc)
            evtend = trig.definition.index(' ON ', evtstart)
            events = trig.definition[evtstart:evtend]
            trig.events = []
            for evt in EVENT_TYPES:
                if evt in events:
                    trig.events.append(evt.lower())
            trig.level = ('FOR EACH ROW' in trig.definition and 'row' or
                          'statement')
            if 'WHEN (' in trig.definition:
                trig.condition = trig.definition[
                    trig.definition.index('WHEN (') + 6:
                    trig.definition.index(') EXECUTE PROCEDURE')]
            trig.procedure = trig.definition[trig.definition.index(EXEC_PROC)
                                             + len(EXEC_PROC):]
            del trig.definition
            self[trig.key()] = trig

    def from_map(self, table, intriggers):
        """Initalize the dictionary of triggers by converting the input map

        :param table: table owning the triggers
        :param intriggers: YAML map defining the triggers
        """
        for trg in intriggers:
            intrig = intriggers[trg]
            if not intrig:
                raise ValueError("Trigger '%s' has no specification" % trg)
            self[(table.schema, table.name, trg)] = trig = Trigger(
                schema=table.schema, table=table.name, name=trg)
            for attr, val in list(intrig.items()):
                setattr(trig, attr, val)
            if not hasattr(trig, 'level'):
                trig.level = 'statement'
            if 'oldname' in intrig:
                trig.oldname = intrig['oldname']
            if 'description' in intrig:
                trig.description = intrig['description']

    def diff_map(self, intriggers):
        """Generate SQL to transform existing triggers

        :param intriggers: a YAML map defining the new triggers
        :return: list of SQL statements

        Compares the existing trigger definitions, as fetched from
        the catalogs, to the input map and generates SQL statements to
        transform the triggers accordingly.
        """
        stmts = []
        # check input triggers
        for (sch, tbl, trg) in intriggers:
            intrig = intriggers[(sch, tbl, trg)]
            # does it exist in the database?
            if (sch, tbl, trg) not in self:
                if not hasattr(intrig, 'oldname'):
                    # create new trigger
                    stmts.append(intrig.create())
                else:
                    stmts.append(self[(sch, tbl, trg)].rename(intrig))
            else:
                # check trigger objects
                stmts.append(self[(sch, tbl, trg)].diff_map(intrig))

        # check existing triggers
        for (sch, tbl, trg) in self:
            trig = self[(sch, tbl, trg)]
            # if missing, drop them
            if (sch, tbl, trg) not in intriggers:
                    stmts.append(trig.drop())

        return stmts

########NEW FILE########
__FILENAME__ = dbtoyaml
#!/usr/bin/python
# -*- coding: utf-8 -*-
"""dbtoyaml - extract the schema of a PostgreSQL database in YAML format"""

from __future__ import print_function
import sys

from pyrseas import __version__
from pyrseas.yamlutil import yamldump
from pyrseas.database import Database
from pyrseas.cmdargs import cmd_parser, parse_args


def main(schema=None):
    """Convert database table specifications to YAML."""
    parser = cmd_parser("Extract the schema of a PostgreSQL database in "
                        "YAML format", __version__)
    parser.add_argument('-m', '--multiple-files', action='store_true',
                        help='output to multiple files (metadata directory)')
    parser.add_argument('-O', '--no-owner', action='store_true',
                        help='exclude object ownership information')
    parser.add_argument('-x', '--no-privileges', action='store_true',
                        dest='no_privs',
                        help='exclude privilege (GRANT/REVOKE) information')
    group = parser.add_argument_group("Object inclusion/exclusion options",
                                      "(each can be given multiple times)")
    group.add_argument('-n', '--schema', metavar='SCHEMA', dest='schemas',
                       action='append', default=[],
                       help="extract the named schema(s) (default all)")
    group.add_argument('-N', '--exclude-schema', metavar='SCHEMA',
                       dest='excl_schemas', action='append', default=[],
                       help="do NOT extract the named schema(s) "
                       "(default none)")
    group.add_argument('-t', '--table', metavar='TABLE', dest='tables',
                       action='append', default=[],
                       help="extract the named table(s) (default all)")
    group.add_argument('-T', '--exclude-table', metavar='TABLE',
                       dest='excl_tables', action='append', default=[],
                       help="do NOT extract the named table(s) "
                       "(default none)")
    parser.set_defaults(schema=schema)
    cfg = parse_args(parser)
    output = cfg['files']['output']
    options = cfg['options']
    if options.multiple_files and output:
        parser.error("Cannot specify both --multiple-files and --output")

    db = Database(cfg)
    dbmap = db.to_map()

    if not options.multiple_files:
        print(yamldump(dbmap), file=output or sys.stdout)
        if output:
            output.close()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = dbconn
# -*- coding: utf-8 -*-
"""
    pyrseas.lib.dbconn
    ~~~~~~~~~~~~~~~~~~

    A `DbConnection` is a helper class representing a connection to a
    PostgreSQL database.
"""
import sys

from psycopg2 import connect
from psycopg2.extras import DictConnection

from .pycompat import PY2

if PY2:
    from psycopg2.extensions import register_type, UNICODE
    register_type(UNICODE)


class DbConnection(object):
    """A database connection, possibly disconnected"""

    def __init__(self, dbname, user=None, pswd=None, host=None, port=None):
        """Initialize the connection information

        :param dbname: database name
        :param user: user name
        :param pswd: user password
        :param host: host name
        :param port: host port number
        """
        self.dbname = dbname
        self.user = '' if user is None else " user=%s" % user
        self.pswd = '' if pswd is None else " password=%s" % pswd
        self.host = '' if host is None else "host=%s " % host
        self.port = '' if port is None else "port=%d " % port
        self.conn = None

    def connect(self):
        """Connect to the database"""
        try:
            self.conn = connect("%s%sdbname=%s%s%s" % (
                self.host, self.port, self.dbname, self.user, self.pswd),
                connection_factory=DictConnection)
        except Exception as exc:
            if str(exc)[:6] == 'FATAL:':
                sys.exit("Database connection error: %s" % str(exc)[8:])
            else:
                raise exc

    def close(self):
        """Close the database connection"""
        if self.conn and not self.conn.closed:
            self.conn.close()
        self.conn = None

    def commit(self):
        """Commit currently open transaction"""
        self.conn.commit()

    def rollback(self):
        """Roll back currently open transaction"""
        self.conn.rollback()

    def execute(self, query, args=None):
        """Create a cursor, execute a query and return the cursor

        :param query: text of the statement to execute
        :param args: arguments to query
        :return: cursor
        """
        if self.conn is None or self.conn.closed:
            self.connect()
        curs = self.conn.cursor()
        try:
            curs.execute(query, args)
        except Exception as exc:
            self.conn.rollback()
            curs.close()
            raise exc
        return curs

    def fetchone(self, query, args=None):
        """Execute a single row SELECT query and return row

        :param query: a SELECT query to be executed
        :param args: arguments to query
        :return: a psycopg2 DictRow

        The cursor is closed.
        """
        curs = self.execute(query, args)
        row = curs.fetchone()
        curs.close()
        return row

    def fetchall(self, query, args=None):
        """Execute a SELECT query and return rows

        :param query: a SELECT query to be executed
        :param args: arguments to query
        :return: a list of psycopg2 DictRow's

        The cursor is closed.
        """
        curs = self.execute(query, args)
        rows = curs.fetchall()
        curs.close()
        return rows

    def copy_to(self, path, table, sep=','):
        """Execute a COPY command to a file

        :param path: file name/path to copy into
        :param table: possibly schema qualified table name
        :param sep: separator between columns
        """
        if self.conn is None or self.conn.closed:
            self.connect()
        with open(path, 'w') as f:
            curs = self.conn.cursor()
            try:
                curs.copy_to(f, table, sep)
            except:
                curs.close()
                raise

    def sql_copy_to(self, sql, path):
        """Execute an SQL COPY command to a file

        :param sql: SQL copy command
        :param path: file name/path to copy into
        """
        if self.conn is None or self.conn.closed:
            self.connect()
        with open(path, 'w') as f:
            curs = self.conn.cursor()
            try:
                curs.copy_expert(sql, f)
            except:
                curs.close()
                raise

    def copy_from(self, path, table, sep=','):
        """Execute a COPY command from a file

        :param path: file name/path to copy from
        :param table: possibly schema qualified table name
        :param sep: separator between columns
        """
        if self.conn is None or self.conn.closed:
            self.connect()
        with open(path, 'r') as f:
            curs = self.conn.cursor()
            try:
                curs.copy_from(f, table, sep)
            except:
                curs.close()
                raise

########NEW FILE########
__FILENAME__ = dbutils
# -*- coding: utf-8 -*-
"""Database utility functions and classes

These are primarily to assist in testing Pyrseas, i.e., without having
to depend on the application-level DbConnection.
"""
import os

from psycopg2 import connect
from psycopg2.extras import DictConnection
from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT


def pgconnect(dbname, user=None, host=None, port=None):
    "Connect to a Postgres database using psycopg2"
    user = '' if user is None else " user=%s" % user
    host = '' if host is None else "host=%s " % host
    port = '' if port is None else "port=%d " % port
    return connect("%s%sdbname=%s%s" % (host, port, dbname, user),
                   connection_factory=DictConnection)


def pgexecute(dbconn, oper, args=None):
    "Execute an operation using a cursor"
    curs = dbconn.cursor()
    try:
        curs.execute(oper, args)
    except:
        curs.close()
        dbconn.rollback()
        raise
    return curs


def pgexecute_auto(dbconn, oper):
    "Execute an operation using a cursor with autocommit enabled"
    isolation_level = dbconn.isolation_level
    dbconn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)
    curs = pgexecute(dbconn, oper)
    dbconn.set_isolation_level(isolation_level)
    return curs


ADMIN_DB = os.environ.get("PG_ADMIN_DB", 'postgres')
CREATE_DDL = "CREATE DATABASE %s TEMPLATE = template0"


class PostgresDb(object):
    """A PostgreSQL database connection

    This is separate from the one used by DbConnection, because tests
    need to create and drop databases and other objects,
    independently.
    """
    def __init__(self, name, user, host, port):
        self.name = name
        self.conn = None
        self.user = user
        self.host = host
        self.port = port and int(port)
        self._version = 0

    def connect(self):
        """Connect to the database

        If we're not already connected we first connect to the admin
        database and see if the given database exists.  If it doesn't,
        we create and then connect to it.
        """
        if not self.conn:
            conn = pgconnect(ADMIN_DB, self.user, self.host, self.port)
            curs = pgexecute(conn,
                             "SELECT 1 FROM pg_database WHERE datname = '%s'" %
                             self.name)
            row = curs.fetchone()
            if not row:
                curs.close()
                curs = pgexecute_auto(conn, CREATE_DDL % self.name)
                curs.close()
            conn.close()
            self.conn = pgconnect(self.name, self.user, self.host, self.port)
            curs = pgexecute(self.conn, "SHOW server_version_num")
            self._version = int(curs.fetchone()[0])

    def close(self):
        "Close the connection if still open"
        if not self.conn:
            return ValueError
        self.conn.close()

    @property
    def version(self):
        return self._version

    def create(self):
        "Drop the database if it exists and re-create it"
        conn = pgconnect(ADMIN_DB, self.user, self.host, self.port)
        curs = pgexecute_auto(conn, "DROP DATABASE IF EXISTS %s" % self.name)
        curs = pgexecute_auto(conn, CREATE_DDL % self.name)
        curs.close()
        conn.close()

    def drop(self):
        "Drop the database"
        conn = pgconnect(ADMIN_DB, self.user, self.host, self.port)
        curs = pgexecute_auto(conn, "DROP DATABASE %s" % self.name)
        curs.close()
        conn.close()

    def execute(self, stmt, args=None):
        "Execute a DDL statement"
        curs = pgexecute(self.conn, stmt, args)
        curs.close()

    def execute_commit(self, stmt, args=None):
        "Execute a DDL statement and commit"
        self.execute(stmt, args)
        self.conn.commit()

    def fetchone(self, query, args=None):
        "Execute a query and return one row"
        try:
            curs = pgexecute(self.conn, query, args)
        except Exception as exc:
            raise exc
        row = curs.fetchone()
        curs.close()
        return row

########NEW FILE########
__FILENAME__ = pycompat
# -*- coding: utf-8 -*-
"""
    pyrseas.lib.pycompat
    ~~~~~~~~~~~~~~~~~~~~

    Helpers for compatibility between Python 2.x and 3.x.
"""
import sys

PY2 = sys.version_info[0] == 2

if not PY2:
    strtypes = (str, )
else:
    strtypes = (str, unicode)

########NEW FILE########
__FILENAME__ = attribute
# -*- coding: utf-8 -*-
"""
    pyrseas.relation.attribute
"""
from pyrseas.lib.pycompat import PY2


class Attribute(object):
    "A relational attribute triple: attribute-type-value"

    def __init__(self, name, type_=str, value=None, nullable=False,
                 sysdefault=False):
        """Initialize an attribute

        :param name: attribute name
        :param type_: type
        :param value: value
        :param nullable: indicates whether attribute accepts NULLs
        :param sysdefault: indicates whether attribute has system default
        """
        self.name = name
        self.type = type_
        if value is not None:
            if (isinstance(value, int) and type_ == float and
                    float(value) == value):
                value = float(value)
            if not isinstance(value, type_):
                if not (PY2 and type_ == str and isinstance(value, unicode)):
                    raise ValueError("Value (%s) of %r is not of type '%s'" %
                                     (value, self, type_.__name__))
        self.value = value
        if not nullable:
            if value is None:
                if type_ == str:
                    self.value = ''
                elif type_ == int:
                    self.value = 0
                elif type_ == float:
                    self.value = 0.0
                elif type_ == bool:
                    self.value = False
                elif not sysdefault:
                    raise ValueError("No value provided for %r" % self)
        else:  # nullable
            if (type_ == int and value == 0) or (
                    type_ == bool and value is False) or (
                    type_ == str and value == '') or (
                    type_ == float and value == 0):
                self.value = None
        self.nullable = nullable
        self.sysdefault = sysdefault

    def __repr__(self):
        return "Attribute(%s %s)" % (self.name, self.type.__name__)

########NEW FILE########
__FILENAME__ = join
# -*- coding: utf-8 -*-
"""
    pyrseas.relation.join
"""
from pyrseas.relation.attribute import Attribute
from pyrseas.relation.tuple import Tuple


class ProjAttribute(Attribute):

    def __init__(self, name, type_=str, value=None, nullable=False,
                 sysdefault=False, basename=None, projection=None):
        super(ProjAttribute, self).__init__(name, type_, value, nullable,
                                            sysdefault)
        self.basename = basename or self.name
        self.projection = projection


class Projection(object):
    "A relational projection, from a single relvar"

    def __init__(self, rvname, attribs, rangevar=None):
        """Initialize a relational projection

        :param rvname: relvar name
        :param attribs: list of Attributes
        :param rangevar: range variable for relvar
        """
        self.rvname = rvname
        self.attributes = tuple([(attr.name, attr) for attr in attribs])
        self.rangevar = rangevar or rvname[0]


class JoinRelation(object):
    "A join of one or more projected relations"

    def __init__(self, projlist, join=None, extname=None):
        attribs = []
        rangevars = []
        for proj in projlist:
            if proj.rangevar in rangevars:
                raise ValueError("Duplicate rangevar '%s'" % proj.rangevar)
            rangevars.append(proj.rangevar)
            for (name, attr) in proj.attributes:
                if name not in attribs:
                    attr.projection = proj
                    attribs.append(attr)
        self.attributes = tuple([(attr.name, attr) for attr in attribs])
        self._required_attribs = [name for name, attr in self.attributes
                                  if (not attr.sysdefault and
                                      not attr.nullable)]
        name = projlist[0].rvname
        if extname is None:
            self.extname = name
        else:
            self.extname = extname
        self.from_clause = "%s %s" % (name, projlist[0].rangevar)
        if len(projlist) > 1:
            assert join is not None, "Must provide 'join' clause"
        if join:
            self.from_clause += " %s" % (join)

    def connect(self, dbconn):
        """Specify the database where the relations are present

        :param dbconn: DbConnection object
        """
        self.db = dbconn

    def tuple(self, *args, **kwargs):
        """Return a Tuple based on relation and passed-in arguments

        :param args: positional arguments corresponding to attributes
        :param kwargs: keyword arguments corresponding to attributes
        :return: Tuple
        """
        kwargs.update(dict(list(zip([name for name, attr in self.attributes],
                                    args))))
        attribs = []
        namelist = []
        attrs = dict(self.attributes)
        for (argname, argval) in list(kwargs.items()):
            attr = attrs[argname]
            attribs.append(Attribute(
                argname, attr.type, argval, nullable=attr.nullable,
                sysdefault=attr.sysdefault))
            namelist.append(argname)
        for name, attr in self.attributes:
            if name not in namelist and name in self._required_attribs:
                attribs.append(Attribute(name, attr.type,
                                         nullable=attr.nullable,
                                         sysdefault=attr.sysdefault))
        return Tuple(attribs)

    def where_clause(self, qry_args=None):
        if not qry_args:
            return ('', {})
        attrs = {}
        for name, attr in self.attributes:
            if attr.name != attr.basename:
                expr = "%s.%s" % (attr.projection.rangevar, attr.basename)
            else:
                expr = "%s.%s" % (attr.projection.rangevar, attr.name)
            attrs.update({attr.name: (expr, attr.type)})
        subclauses = []
        params = {}
        for name in qry_args:
            if name not in attrs:
                raise KeyError("Attribute '%s' not allowed in query string" %
                               name)
            (expr, type_) = attrs[name]
            if type_ == str:
                subclauses.append("%s ILIKE %%(%s)s" % (expr, name))
                params.update({name: '%%%s%%' % qry_args[name]})
            else:
                arg = qry_args[name].strip()
                oper = '='
                if arg[:2] in ['>=', '!=', '<=']:
                    oper = arg[:2]
                    arg = arg[2:].strip()
                elif arg[:1] in ['>', '<']:
                    oper = arg[:1]
                    arg = arg[1:].strip()
                subclauses.append("%s %s %%(%s)s" % (expr, oper, name))
                if type_ in (int, float):
                    arg = type_(arg)
                params.update({name: arg})

        return (" WHERE %s" % " AND ".join(subclauses), params)

    def count(self, qry_args=None):
        """Execute a COUNT() possibly based on a WHERE clause

        :param qry_args: query arguments to form WHERE clause
        :return: integer result from COUNT()
        """
        (where, params) = self.where_clause(qry_args)
        row = self.db.fetchone(
            "SELECT COUNT(*) FROM " + self.from_clause + where, params)
        self.db.rollback()
        return row[0]

    def subset(self, limit='ALL', offset=0, qry_args='', order=[]):
        """Execute a multiple-tuple retrieval and return the tuple data

        :param limit: literal 'ALL' or integer, max tuples to return
        :param offset: integer, offset into subset
        :param qry_args: dictionary of query arguments
        :param order: list of attributes to sort on, possibly including DESC
        :return: list of tuples
        """
        (where, params) = self.where_clause(qry_args)

        def getsubset_qry():
            if not hasattr(self, 'getsubset_qry'):
                exprs = []
                for name, attr in self.attributes:
                    if attr.name != attr.basename:
                        exprs.append("%s.%s AS %s" % (
                            attr.projection.rangevar, attr.basename,
                            attr.name))
                    else:
                        exprs.append("%s.%s" % (attr.projection.rangevar,
                                                attr.name))
                self.getsubset_qry = "SELECT %s FROM %s" % (
                    ", ".join(exprs), self.from_clause)
            return self.getsubset_qry

        slice_ = " LIMIT %s OFFSET %d" % (limit, offset)
        orderby = " ORDER BY 1"
        if order:
            attrnames = [name for (name, attr) in self.attributes]
            for name in order:
                nm = name.rstrip()
                if nm[-5:].upper() == ' DESC':
                    nm = nm[:-5]
                elif nm[-4:].upper() == ' ASC':
                    nm = nm[:-4]
                if nm not in attrnames:
                    raise AttributeError("JoinRelation %s has no attribute "
                                         "'%s'" % (self.extname, nm))
            orderby = " ORDER BY %s" % ", ".join(order)
        query = getsubset_qry() + where + orderby + slice_
        rows = self.db.fetchall(query, params)
        self.db.rollback()
        return [self.tuple(**row) for row in rows]

########NEW FILE########
__FILENAME__ = relvar
# -*- coding: utf-8 -*-
"""
    pyrseas.relation.relvar
"""
from psycopg2 import DatabaseError

from pyrseas.relation import Attribute, Tuple
from pyrseas.relation.tuple import tuple_values_dict


class RelVar(object):
    "A relation variable, commonly known as database table"

    def __init__(self, name, attribs, key=[], extname=None):
        """Initialize a relation variable

        :param name: relvar name
        :param attribs: list of Attributes
        :param key: list of attribute names forming the key
        :param extname: external name for relvar
        """
        self.name = name
        self.attributes = tuple([(attr.name, attr) for attr in attribs])
        self._required_attribs = [
            attrname for attrname, attr in self.attributes if (
                not attr.sysdefault and not attr.nullable)]
        self.key = key
        self.extname = extname or name

    def connect(self, dbconn):
        """Specify the database where the relvar is present

        :param dbconn: DbConnection object
        """
        self.db = dbconn

    def tuple(self, *args, **kwargs):
        """Return a Tuple based on relvar and passed-in arguments

        :param args: positional arguments corresponding to attributes
        :param kwargs: keyword arguments corresponding to attributes
        :return: Tuple
        """
        kwargs.update(dict(list(zip([name for name, attr in self.attributes],
                                    args))))
        attribs = []
        namelist = []
        attrs = dict(self.attributes)
        for (argname, argval) in list(kwargs.items()):
            attr = attrs[argname]
            attribs.append(Attribute(
                argname, attr.type, argval, nullable=attr.nullable,
                sysdefault=attr.sysdefault))
            namelist.append(argname)
        for name in self._required_attribs:
            if name not in namelist:
                raise ValueError("Missing required attribute: %s" % name)

        return Tuple(attribs)

    def key_tuple(self, *args, **kwargs):
        """Return a Tuple of key attributes, with given values

        :param args: positional arguments corresponding to key attributes
        :param kwargs: keyword arguments corresponding to key attributes
        :return: Tuple
        """
        kwargs.update(dict(list(zip(self.key, args))))
        return Tuple([Attribute(name, type(val), val) for name, val in
                     list(kwargs.items())])

    def default_tuple(self):
        """Return a Tuple of key and required attributes, with default values

        :return: Tuple
        """
        attrs = self.key[:]
        attrs.extend([name for name in self._required_attribs
                      if not name in attrs])
        return Tuple([Attribute(name, attr.type)
                      for name, attr in self.attributes if name in attrs])

    def insert_one(self, newtuple, retkey=False):
        """Execute a single-tuple INSERT command

        :param newtuple: the tuple to be inserted
        :param retkey: indicates assigned key values should be returned
        """
        attrnames = [name for name, typ in newtuple._heading]
        targets = '(%s)' % ", ".join(attrnames)
        values_list = 'VALUES (%s)' % ", ".join(
            ['%%(%s)s' % name for name in attrnames])
        cmd = "INSERT INTO %s %s %s" % (self.name, targets, values_list)
        if retkey:
            cmd += " RETURNING %s" % ", ".join(self.key)
        curs = self.db.execute(cmd, tuple_values_dict(newtuple))
        if curs.rowcount != 1:
            self.db.rollback()
            raise DatabaseError("Failed to add %s %r" % (self.extname, self))
        if retkey:
            attrdict = dict(self.attributes)
            rettuple = Tuple([Attribute(name, attrdict[name].type)
                              for name in self.key])
            row = curs.fetchone()
            for attr, type_ in rettuple._heading:
                setattr(rettuple, attr, row[attr])
        curs.close()
        if retkey:
            return rettuple

    def where_clause(self, tuple_version=False):
        """Return WHERE clause for use by get, update and delete methods

        :param tuple_version: indicates whether tuple_version should be added
        :return: string
        """
        clause = "WHERE %s" % " AND ".join(
            ['%s = %%(_kv_%s)s' % (attr, attr) for attr in self.key])
        if tuple_version:
            clause += " AND xmin = %(xmin)s"
        return clause

    def key_values(self, tup):
        """Return dictionary of key values for use by get_one method

        :param tup: Tuple object to get key values from
        :return: dictionary
        """
        return {'_kv_%s' % attr: getattr(tup, attr) for attr in self.key}

    def key_values_update(self, keytuple, currtuple=None):
        """Return dictionary of key values for use by update and delete

        :param keytuple: Tuple object to get key values from
        :param currtuple: previous version of tuple
        :return: dictionary

        This includes {'xmin': tuple_version}.
        """
        keyvals = self.key_values(keytuple)
        verstuple = keytuple if currtuple is None else currtuple
        keyvals.update(xmin=verstuple._tuple_version)
        return keyvals

    def update_one(self, newtuple, keytuple, currtuple=None):
        """Execute a single-tuple UPDATE command using the primary key

        :param newtuple: Tuple with new values
        :param keytuple: Tuple with key values
        :param currtuple: previous version of newtuple
        """
        def update_one_cmd():
            if changed_values or not hasattr(self, 'update_one_cmd'):
                setlist = "SET %s" % ", ".join(
                    ['%s = %%(%s)s' % (c, c) for c in
                     list(changed_values.keys())])
                self.update_one_cmd = "UPDATE %s %s %s" % (
                    self.name, setlist,
                    self.where_clause(currtuple is not None))
            return self.update_one_cmd

        if currtuple:
            changed_values = tuple_values_dict(currtuple, newtuple)
            if not changed_values:
                return
        else:
            changed_values = tuple_values_dict(newtuple)
        values = self.key_values_update(keytuple, currtuple)
        values.update(changed_values)
        curs = self.db.execute(update_one_cmd(), values)
        if curs.rowcount != 1:
            self.db.rollback()
            raise DatabaseError("Failed to update %s %r" % (
                self.extname, self))
        curs.close()

    def delete_one(self, keytuple, currtuple=None):
        """Execute a single-tuple DELETE command using the primary key

        :param keytuple: Tuple with key values
        :param currtuple: tuple from previous get
        """
        def delete_one_cmd():
            if not hasattr(self, 'delete_one_cmd'):
                self.delete_one_cmd = "DELETE FROM %s %s" % (
                    self.name, self.where_clause(currtuple is not None))
            return self.delete_one_cmd

        values = self.key_values_update(keytuple, currtuple)
        curs = self.db.execute(delete_one_cmd(), values)
        if curs.rowcount != 1:
            self.db.rollback()
            raise DatabaseError("Failed to delete %s %r" % (
                self.extname, self))
        curs.close()

    def get_one(self, keytuple):
        """Execute a single-tuple retrieval and return the tuple data

        :param keytuple: Tuple with key values
        :return: Tuple or None
        """
        attrdict = dict(self.attributes)
        attrnames = list(attrdict.keys())

        def get_one_qry():
            if not hasattr(self, 'get_one_qry'):
                from_clause = self.name
                self.get_one_qry = "SELECT %s.xmin, %s FROM %s %s" % (
                    self.name, ", ".join(attrnames), from_clause,
                    self.where_clause())
            return self.get_one_qry

        key = self.key_values(keytuple)
        row = self.db.fetchone(get_one_qry(), key)
        self.db.rollback()
        if not row:
            return None
        tup = Tuple([Attribute(name, attrdict[name].type, row[name],
                               nullable=attrdict[name].nullable,
                               sysdefault=attrdict[name].sysdefault)
                     for name in attrnames])
        tup._tuple_version = row['xmin']
        return tup

########NEW FILE########
__FILENAME__ = tuple
# -*- coding: utf-8 -*-
"""
    pyrseas.relation.tuple
"""
from datetime import datetime, time
from pyrseas.relation.attribute import Attribute

RESERVED_ATTRIBUTE_NAMES = (
    '_heading', '_nullable_attribs', '_tuple_version', '_sysdefault_attribs')


class Tuple(object):
    "A relational n-tuple: a set of attributes"

    def __init__(self, attribs):
        """Initialize a relational tuple

        :param attribs: list of Attributes (or single Attribute)
        """
        if not isinstance(attribs, list):
            attribs = [attribs]
        self._sysdefault_attribs = []
        self._nullable_attribs = []
        self._tuple_version = None
        heading = []
        for attr in attribs:
            assert isinstance(attr, Attribute)
            assert attr.name not in RESERVED_ATTRIBUTE_NAMES, \
                "Cannot use '%s' as attribute name" % attr.name
            setattr(self, attr.name, attr.value)
            heading.append((attr.name, attr.type))
            if attr.sysdefault:
                self._sysdefault_attribs.append(attr.name)
            if attr.nullable:
                self._nullable_attribs.append(attr.name)
        # This has to be last for __setattr__
        self._heading = tuple(heading)

    def __setattr__(self, name, value):
        if not hasattr(self, '_heading') or name == '_tuple_version':
            object.__setattr__(self, name, value)
            return
        assert name not in RESERVED_ATTRIBUTE_NAMES, \
            "Attribute '%s' cannot be set" % name
        if not name in self.__dict__:
            raise AttributeError("%r has no attribute '%s'" % (self, name))
        attrdict = dict(self._heading)
        attr = Attribute(name, attrdict[name], value,
                         nullable=name in self._nullable_attribs)
        object.__setattr__(self, name, attr.value)

    def __repr__(self):
        return "Tuple(%s)" % ", ".join("%s %s" % (name, type_.__name__)
                                       for name, type_ in self._heading)


def tuple_values_dict(currtuple, newtuple=None):
    """Return dictionary of attributes with their values

    :param currtuple: current Tuple
    :param newtuple: optional Tuple with new values
    :return: dictionary of attributes and values
    """
    valdict = {}
    for attr in currtuple.__dict__:
        if attr in RESERVED_ATTRIBUTE_NAMES:
            continue
        currval = getattr(currtuple, attr)
        if newtuple is None:
            valdict.update({attr: currval})
        elif hasattr(newtuple, attr):
            newval = getattr(newtuple, attr)
            try:
                diff = currval != newval
            except TypeError:
                # TODO: more needed for naive vs. aware datetime/time
                if isinstance(newval, datetime) or isinstance(newval, time):
                    if newval.utcoffset() is None:
                        diff = True
                else:
                    raise
            if diff:
                valdict.update({attr: newval})
    if newtuple is not None:
        for newattr in newtuple.__dict__:
            if newattr in RESERVED_ATTRIBUTE_NAMES:
                continue
            if newattr not in currtuple.__dict__:
                valdict.update({newattr: getattr(newtuple, newattr)})
    return valdict

########NEW FILE########
__FILENAME__ = testutils
# -*- coding: utf-8 -*-
"""Utility functions and classes for testing Pyrseas"""

import sys
import os
import getpass
import tempfile
from unittest import TestCase

import yaml

from pyrseas.config import Config
from pyrseas.database import Database
from pyrseas.augmentdb import AugmentDatabase
from pyrseas.lib.dbconn import DbConnection
from pyrseas.lib.dbutils import pgexecute, PostgresDb


def fix_indent(stmt):
    "Fix specifications which are in a new line with indentation"
    return stmt.replace('   ', ' ').replace('  ', ' ').replace('\n ', ' '). \
        replace('( ', '(')


def remove_temp_files(tmpdir, prefix=''):
    "Remove files in a temporary directory"
    for tfile in glob.glob(os.path.join(tmpdir, prefix + '*')):
        if os.path.isdir(tfile):
            for entry in os.listdir(tfile):
                entry = os.path.join(tmpdir, tfile, entry)
                if os.path.isdir(entry):
                    for file in os.listdir(entry):
                        os.remove(os.path.join(entry, file))
                    os.rmdir(entry)
                else:
                    os.remove(entry)
            os.rmdir(tfile)
        else:
            os.remove(tfile)


TEST_DBNAME = os.environ.get("PYRSEAS_TEST_DB", 'pyrseas_testdb')
TEST_USER = os.environ.get("PYRSEAS_TEST_USER", getpass.getuser())
TEST_HOST = os.environ.get("PYRSEAS_TEST_HOST", None)
TEST_PORT = int(os.environ.get("PYRSEAS_TEST_PORT", 5432))
PG_OWNER = 'postgres'
TEST_DIR = os.path.join(tempfile.gettempdir(),
                        os.environ.get("PYRSEAS_TEST_DIR", 'pyrseas_test'))
TRAVIS = (os.environ.get("TRAVIS", 'false') == 'true')


class PgTestDb(PostgresDb):
    """A PostgreSQL database connection for testing."""

    def clear(self):
        "Drop tables and other objects"
        STD_DROP = 'DROP %s IF EXISTS "%s" CASCADE'
        # Schemas other than 'public'
        curs = pgexecute(
            self.conn,
            """SELECT nspname FROM pg_namespace
               WHERE nspname NOT IN ('public', 'information_schema')
                     AND substring(nspname for 3) != 'pg_'
               ORDER BY nspname""")
        objs = curs.fetchall()
        curs.close()
        self.conn.rollback()
        for obj in objs:
                self.execute(STD_DROP % ('SCHEMA', obj[0]))
        self.conn.commit()

        # Extensions
        if self.version >= 90100:
            curs = pgexecute(
                self.conn,
                """SELECT extname FROM pg_extension
                          JOIN pg_namespace n ON (extnamespace = n.oid)
                   WHERE nspname NOT IN ('information_schema')
                     AND extname != 'plpgsql'""")
            exts = curs.fetchall()
            curs.close()
            self.conn.rollback()
            for ext in exts:
                self.execute(STD_DROP % ('EXTENSION', ext[0]))
            self.conn.commit()

        # Tables, sequences and views
        curs = pgexecute(
            self.conn,
            """SELECT relname, relkind FROM pg_class
                      JOIN pg_namespace ON (relnamespace = pg_namespace.oid)
               WHERE relkind in ('r', 'S', 'v', 'f', 'm')
                     AND nspname NOT IN ('pg_catalog', 'information_schema')
               ORDER BY relkind DESC""")
        objs = curs.fetchall()
        curs.close()
        self.conn.rollback()
        for obj in objs:
            if obj['relkind'] == 'r':
                objtype = 'TABLE'
            elif obj['relkind'] == 'S':
                objtype = 'SEQUENCE'
            elif obj['relkind'] == 'v':
                objtype = 'VIEW'
            elif obj['relkind'] == 'f':
                objtype = 'FOREIGN TABLE'
            elif obj['relkind'] == 'm':
                objtype = 'MATERIALIZED VIEW'
            self.execute(STD_DROP % (objtype, obj[0]))
        self.conn.commit()

        # Types (base, composite and enums) and domains
        #
        # TYPEs have to be done before FUNCTIONs because base types depend
        # on functions, and we're using CASCADE. Also, exclude base array
        # types because they depend on the scalar types.
        curs = pgexecute(
            self.conn,
            """SELECT typname, typtype FROM pg_type t
                      JOIN pg_namespace n ON (typnamespace = n.oid)
               WHERE typtype IN ('b', 'c', 'd', 'e')
                 AND NOT (typtype = 'b' AND typarray = 0)
                 AND nspname NOT IN ('pg_catalog', 'pg_toast',
                     'information_schema')""")
        types = curs.fetchall()
        curs.close()
        self.conn.rollback()
        for typ in types:
            self.execute(STD_DROP % ('DOMAIN' if typ['typtype'] == 'd'
                                     else 'TYPE', typ[0]))
        self.conn.commit()

        # Functions
        curs = pgexecute(
            self.conn,
            """SELECT proisagg, p.oid::regprocedure AS proc
               FROM pg_proc p JOIN pg_namespace n ON (pronamespace = n.oid)
               WHERE nspname NOT IN ('pg_catalog', 'information_schema')
               ORDER BY 1, 2""")
        funcs = curs.fetchall()
        curs.close()
        self.conn.rollback()
        for func in funcs:
            self.execute('DROP %s IF EXISTS %s CASCADE' % (
                'AGGREGATE' if func['proisagg'] else 'FUNCTION', func['proc']))
        self.conn.commit()

        # Languages
        if self.version < 90000:
            if self.is_plpgsql_installed():
                self.execute_commit("DROP LANGUAGE plpgsql")

        # Operators
        curs = pgexecute(
            self.conn,
            """SELECT o.oid::regoperator
               FROM pg_operator o JOIN pg_namespace n ON (oprnamespace = n.oid)
               WHERE nspname NOT IN ('pg_catalog', 'information_schema')""")
        opers = curs.fetchall()
        curs.close()
        self.conn.rollback()
        for oper in opers:
            self.execute("DROP OPERATOR IF EXISTS %s CASCADE" % (oper[0]))
        self.conn.commit()

        # Operator families
        curs = pgexecute(
            self.conn,
            """SELECT opfname, amname
               FROM pg_opfamily o JOIN pg_am a ON (opfmethod = a.oid)
                    JOIN pg_namespace n ON (opfnamespace = n.oid)
               WHERE nspname NOT IN ('pg_catalog', 'information_schema')""")
        opfams = curs.fetchall()
        curs.close()
        self.conn.rollback()
        for opfam in opfams:
            self.execute(
                'DROP OPERATOR FAMILY IF EXISTS "%s" USING "%s" CASCADE' % (
                    opfam[0], opfam[1]))
        self.conn.commit()

        # Operator classes
        curs = pgexecute(
            self.conn,
            """SELECT opcname, amname
               FROM pg_opclass o JOIN pg_am a ON (opcmethod = a.oid)
                    JOIN pg_namespace n ON (opcnamespace = n.oid)
               WHERE nspname NOT IN ('pg_catalog', 'information_schema')""")
        opcls = curs.fetchall()
        curs.close()
        self.conn.rollback()
        for opcl in opcls:
            self.execute(
                'DROP OPERATOR CLASS IF EXISTS "%s" USING "%s" CASCADE' % (
                    opcl[0], opcl[1]))
        self.conn.commit()

        # Conversions
        curs = pgexecute(
            self.conn,
            """SELECT conname FROM pg_conversion c
                      JOIN pg_namespace n ON (connamespace = n.oid)
               WHERE nspname NOT IN ('pg_catalog', 'information_schema')""")
        convs = curs.fetchall()
        curs.close()
        self.conn.rollback()
        for cnv in convs:
            self.execute(STD_DROP % ('CONVERSION', cnv[0]))
        self.conn.commit()

        # Collations
        if self.version >= 90100:
            curs = pgexecute(
                self.conn,
                """SELECT collname FROM pg_collation c
                          JOIN pg_namespace n ON (collnamespace = n.oid)
                   WHERE nspname NOT IN (
                         'pg_catalog', 'information_schema')""")
            colls = curs.fetchall()
            curs.close()
            self.conn.rollback()
            for coll in colls:
                self.execute(STD_DROP % ('COLLATION', coll[0]))
            self.conn.commit()

        # User mappings
        curs = pgexecute(
            self.conn,
            """SELECT CASE umuser WHEN 0 THEN 'PUBLIC' ELSE
                  pg_get_userbyid(umuser) END AS username, s.srvname
               FROM pg_user_mappings u
                  JOIN pg_foreign_server s ON (srvid = s.oid)""")
        umaps = curs.fetchall()
        curs.close()
        self.conn.rollback()
        for ump in umaps:
            self.execute('DROP USER MAPPING IF EXISTS FOR "%s" SERVER "%s"' % (
                ump[0], ump[1]))
        self.conn.commit()

        # Servers
        curs = pgexecute(self.conn, "SELECT srvname FROM pg_foreign_server")
        servs = curs.fetchall()
        curs.close()
        self.conn.rollback()
        for srv in servs:
            self.execute(STD_DROP % ('SERVER', srv[0]))
        self.conn.commit()

        # Foreign data wrappers
        curs = pgexecute(self.conn,
                         "SELECT fdwname FROM pg_foreign_data_wrapper")
        fdws = curs.fetchall()
        curs.close()
        self.conn.rollback()
        for fdw in fdws:
            self.execute(STD_DROP % ('FOREIGN DATA WRAPPER', fdw[0]))
        self.conn.commit()

    def is_plpgsql_installed(self):
        "Is PL/pgSQL installed?"
        curs = pgexecute(self.conn,
                         "SELECT 1 FROM pg_language WHERE lanname = 'plpgsql'")
        row = curs.fetchone()
        curs.close()
        return row and True

    def is_superuser(self):
        "Is current user a superuser?"
        curs = pgexecute(self.conn, "SELECT 1 FROM pg_roles WHERE rolsuper "
                         "AND rolname = CURRENT_USER ")
        row = curs.fetchone()
        curs.close()
        return row and True


def _connect_clear(dbname):
    db = PgTestDb(dbname, TEST_USER, TEST_HOST, TEST_PORT)
    db.connect()
    db.clear()
    return db


class PyrseasTestCase(TestCase):
    """Base class for most test cases"""

    def setUp(self):
        self.db = _connect_clear(TEST_DBNAME)
        self.cfg = Config(sys_only=True)
        if not 'database' in self.cfg:
            self.cfg.update(database={})
        dbc = self.cfg['database']
        dbc['dbname'] = self.db.name
        dbc['username'] = self.db.user
        dbc['password'] = None
        dbc['host'] = self.db.host
        dbc['port'] = self.db.port

    def tearDown(self):
        self.db.close()

    def database(self):
        """The Pyrseas Database instance"""
        return Database(self.cfg)

    def config_options(self, **kwargs):
        class Opts():
            def __init__(self, **kwargs):
                [setattr(self, opt, val) for opt, val in list(kwargs.items())]
        self.cfg['options'] = Opts(**kwargs)


class DatabaseToMapTestCase(PyrseasTestCase):
    """Base class for "database to map" test cases"""

    superuser = False

    def to_map(self, stmts, config={}, schemas=[], tables=[], no_owner=True,
               no_privs=True, superuser=False, multiple_files=False):
        """Execute statements and return a database map.

        :param stmts: list of SQL statements to execute
        :param config: dictionary of configuration information
        :param schemas: list of schemas to map
        :param tables: list of tables to map
        :param no_owner: exclude object owner information
        :param no_privs: exclude privilege information
        :param superuser: must be superuser to run
        :param multiple_files: emulate --multiple_files option
        :return: possibly trimmed map of database
        """
        if (self.superuser or superuser) and not self.db.is_superuser():
            self.skipTest("Must be a superuser to run this test")
        for stmt in stmts:
            self.db.execute(stmt)
        self.db.conn.commit()
        if multiple_files:
            self.cfg.merge({'files': {'metadata_path': os.path.join(
                            TEST_DIR, self.cfg['repository']['metadata'])}})
        if 'datacopy' in config:
            self.cfg.merge({'files': {'data_path': os.path.join(
                            TEST_DIR, self.cfg['repository']['data'])}})
        self.config_options(schemas=schemas, tables=tables, no_owner=no_owner,
                            no_privs=no_privs, multiple_files=multiple_files)
        self.cfg.merge(config)
        return self.database().to_map()

    def yaml_load(self, filename, subdir=None):
        """Read a file in the metadata_path and process it with YAML load

        :param filename: name of the file
        :param subdir: name of a subdirectory where the file is located
        :return: YAML dictionary
        """
        with open(os.path.join(self.cfg['files']['metadata_path'],
                               subdir or '', filename), 'r') as f:
            inmap = f.read()
        return yaml.safe_load(inmap)

    def remove_tempfiles(self):
        remove_temp_files(TEST_DIR)


class InputMapToSqlTestCase(PyrseasTestCase):
    """Base class for "input map to SQL" test cases"""

    superuser = False

    def to_sql(self, inmap, stmts=None, config={}, superuser=False, schemas=[],
               revert=False, quote_reserved=False):
        """Execute statements and compare database to input map.

        :param inmap: dictionary defining target database
        :param stmts: list of SQL database setup statements
        :param config: dictionary of configuration information
        :param superuser: indicates test requires superuser privilege
        :param schemas: list of schemas to diff
        :param revert: generate statements to back out changes
        :param quote_reserved: fetch reserved words
        :return: list of SQL statements
        """
        if (self.superuser or superuser) and not self.db.is_superuser():
            self.skipTest("Must be a superuser to run this test")
        if stmts:
            for stmt in stmts:
                self.db.execute(stmt)
            self.db.conn.commit()

        if 'datacopy' in config:
            self.cfg.merge({'files': {'data_path': os.path.join(
                            TEST_DIR, self.cfg['repository']['data'])}})
        self.config_options(schemas=schemas, revert=revert,
                            quote_reserved=quote_reserved)
        self.cfg.merge(config)
        return self.database().diff_map(inmap)

    def std_map(self, plpgsql_installed=False):
        "Return a standard schema map for the default database"
        base = {'schema public': {
                'owner': PG_OWNER,
                'privileges': [{PG_OWNER: ['all']}, {'PUBLIC': ['all']}],
                'description': 'standard public schema'}}
        if (self.db._version >= 90000 or plpgsql_installed) \
                and self.db._version < 90100:
            base.update({'language plpgsql': {'trusted': True}})
        if self.db._version >= 90100:
            base.update({'extension plpgsql': {
                        'schema': 'pg_catalog',
                        'description': "PL/pgSQL procedural language"}})
        return base


import glob
import subprocess

TEST_DBNAME_SRC = os.environ.get("PYRSEAS_TEST_DB_SRC", 'pyrseas_testdb_src')


class DbMigrateTestCase(TestCase):

    @classmethod
    def setUpClass(cls):
        cls.srcdb = _connect_clear(TEST_DBNAME_SRC)
        cls.db = _connect_clear(TEST_DBNAME)
        progdir = os.path.abspath(os.path.dirname(__file__))
        cls.dbtoyaml = os.path.join(progdir, 'dbtoyaml.py')
        cls.yamltodb = os.path.join(progdir, 'yamltodb.py')
        cls.tmpdir = TEST_DIR
        if not os.path.exists(cls.tmpdir):
            os.mkdir(cls.tmpdir)

    @classmethod
    def remove_tempfiles(cls, prefix):
        remove_temp_files(cls.tmpdir, prefix)

    def execute_script(self, path, scriptname):
        scriptfile = os.path.join(os.path.abspath(os.path.dirname(path)),
                                  scriptname)
        lines = []
        with open(scriptfile, 'r') as fd:
            lines = [line.strip() for line in fd if line != '\n' and
                     not line.startswith('--')]
        self.srcdb.execute_commit(' '.join(lines))

    def tempfile_path(self, filename):
        return os.path.join(self.tmpdir, filename)

    def _db_params(self):
        args = []
        if self.db.host is not None:
            args.append("--host=%s" % self.db.host)
        if self.db.port is not None:
            args.append("--port=%d " % self.db.port)
        if self.db.user is not None:
            args.append("--username=%s" % self.db.user)
        return args

    def lines(self, the_file):
        with open(the_file) as f:
            lines = f.readlines()
        return lines

    def run_pg_dump(self, dumpfile, srcdb=False, incldata=False):
        """Run pg_dump using special scripts or directly (on Travis-CI)

        :param dumpfile: path to the pg_dump output file
        :param srcdb: run against source database
        """
        if TRAVIS:
            pg_dumpver = 'pg_dump'
        else:
            v = self.srcdb._version
            pg_dumpver = "pg_dump%d%d" % (v // 10000,
                                          (v - v // 10000 * 10000) // 100)
            if sys.platform == 'win32':
                pg_dumpver += '.bat'
        dbname = self.srcdb.name if srcdb else self.db.name
        args = [pg_dumpver]
        args.extend(self._db_params())
        if not incldata:
            args.extend(['-s'])
        args.extend(['-f', dumpfile, dbname])
        subprocess.check_call(args)

    def invoke(self, args):
        args.insert(0, sys.executable)
        path = [os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))]
        path.append(os.path.abspath(os.path.join(os.path.dirname(
                    yaml.__file__), '..')))
        env = os.environ.copy()
        env.update({'PYTHONPATH': os.pathsep.join(path)})
        subprocess.check_call(args, env=env)

    def create_yaml(self, yamlfile='', srcdb=False):
        dbname = self.srcdb.name if srcdb else self.db.name
        args = [self.dbtoyaml]
        args.extend(self._db_params())
        if yamlfile:
            args.extend(['-o', yamlfile, dbname])
        else:
            args.extend(['-r', TEST_DIR, '-m', dbname])
        self.invoke(args)

    def migrate_target(self, yamlfile, outfile):
        args = [self.yamltodb]
        args.extend(self._db_params())
        if yamlfile:
            args.extend(['-u', '-o', outfile, self.db.name, yamlfile])
        else:
            args.extend(['-u', '-o', outfile, '-r', TEST_DIR, '-m',
                         self.db.name])
        self.invoke(args)


class AugmentToMapTestCase(PyrseasTestCase):

    def to_map(self, stmts, augmap):
        """Apply an augment map and return a map of the updated database.

        :param stmts: list of SQL statements to execute
        :param augmap: dictionary describing the augmentations
        :return: dictionary of the updated database
        """
        for stmt in stmts:
            self.db.execute(stmt)
        self.db.conn.commit()
        self.config_options(schemas=[], tables=[], no_owner=True,
                            no_privs=True, multiple_files=False)
        db = AugmentDatabase(self.cfg)
        return db.apply(augmap)


class RelationTestCase(object):

    @classmethod
    def setup_class(cls):
        cls.pgdb = PostgresDb(TEST_DBNAME, TEST_USER, TEST_HOST, TEST_PORT)
        cls.pgdb.connect()
        cls.db = DbConnection(TEST_DBNAME, TEST_USER, None, TEST_HOST,
                              TEST_PORT)

    @classmethod
    def teardown_class(cls):
        cls.pgdb.close()

########NEW FILE########
__FILENAME__ = yamltodb
#!/usr/bin/python
# -*- coding: utf-8 -*-
"""yamltodb - generate SQL statements to update a PostgreSQL database
to match the schema specified in a YAML file"""

from __future__ import print_function
import sys
from argparse import FileType

import yaml

from pyrseas import __version__
from pyrseas.database import Database
from pyrseas.cmdargs import cmd_parser, parse_args
from pyrseas.lib.pycompat import PY2


def main():
    """Convert YAML specifications to database DDL."""
    parser = cmd_parser("Generate SQL statements to update a PostgreSQL "
                        "database to match the schema specified in a "
                        "YAML-formatted file(s)", __version__)
    parser.add_argument('-m', '--multiple-files', action='store_true',
                        help='input from multiple files (metadata directory)')
    parser.add_argument('spec', nargs='?', type=FileType('r'),
                        default=sys.stdin, help='YAML specification')
    parser.add_argument('-1', '--single-transaction', action='store_true',
                        dest='onetrans', help="wrap commands in BEGIN/COMMIT")
    parser.add_argument('-u', '--update', action='store_true',
                        help="apply changes to database (implies -1)")
    parser.add_argument('--revert', action='store_true',
                        help="generate SQL to revert changes")
    parser.add_argument('--quote-reserved', action='store_true',
                        help="quote SQL reserved words")
    parser.add_argument('-n', '--schema', metavar='SCHEMA', dest='schemas',
                        action='append', default=[],
                        help="process only named schema(s) (default all)")
    cfg = parse_args(parser)
    output = cfg['files']['output']
    options = cfg['options']
    db = Database(cfg)
    if options.multiple_files:
        inmap = db.map_from_dir()
    else:
        inmap = yaml.safe_load(options.spec)

    stmts = db.diff_map(inmap)
    if stmts:
        fd = output or sys.stdout
        if options.onetrans or options.update:
            print("BEGIN;", file=fd)
        for stmt in stmts:
            if isinstance(stmt, tuple):
                outstmt = "".join(stmt) + '\n'
            else:
                outstmt = "%s;\n" % stmt
            if PY2:
                outstmt = outstmt.encode('utf-8')
            print(outstmt, file=fd)
        if options.onetrans or options.update:
            print("COMMIT;", file=fd)
        if options.update:
            try:
                for stmt in stmts:
                    if isinstance(stmt, tuple):
                        # expected format: (\copy, table, from, path, csv)
                        db.dbconn.copy_from(stmt[3], stmt[1])
                    else:
                        db.dbconn.execute(stmt)
            except:
                db.dbconn.rollback()
                raise
            else:
                db.dbconn.commit()
                print("Changes applied", file=sys.stderr)
        if output:
            output.close()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = yamlutil
# -*- coding: utf-8 -*-
"""Pyrseas YAML utilities"""

from yaml import add_representer, dump

from pyrseas.lib.pycompat import PY2


if PY2:
    from yaml import safe_dump as dump

    class MultiLineStr(unicode):
        """ Marker for multiline strings"""
else:
    class MultiLineStr(str):
        """ Marker for multiline strings"""


def MultiLineStr_presenter(dumper, data):
    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')
add_representer(MultiLineStr, MultiLineStr_presenter)


def yamldump(objmap):
    """Dump an object map using yaml.dump with certain defaults

    :param objmap: dictionary
    :return: dumped object map
    """
    return dump(objmap, default_flow_style=False, allow_unicode=True)

########NEW FILE########
__FILENAME__ = test_audit
# -*- coding: utf-8 -*-
"""Test audit columns"""

import pytest

from pyrseas.testutils import AugmentToMapTestCase

CREATE_STMT = "CREATE TABLE t1 (c1 integer, c2 text)"
FUNC_SRC1 = """
BEGIN
  NEW.modified_by_user = SESSION_USER;
  NEW.modified_timestamp = CURRENT_TIMESTAMP;
  RETURN NEW;
END"""

FUNC_SRC2 = """
BEGIN
  NEW.updated = CURRENT_TIMESTAMP;
  RETURN NEW;
END"""


class AuditColumnsTestCase(AugmentToMapTestCase):
    """Test mapping of audit column augmentations"""

    def test_predef_column(self):
        "Add predefined audit column"
        augmap = {'schema public': {'table t1': {
            'audit_columns': 'created_date_only'}}}
        dbmap = self.to_map([CREATE_STMT], augmap)
        expmap = {'columns': [
            {'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
            {'created_date': {'type': 'date', 'not_null': True,
                              'default': "('now'::text)::date"}}]}
        assert expmap == dbmap['schema public']['table t1']

    def test_unknown_table(self):
        "Error on non-existent table"
        augmap = {'schema public': {'table t2': {
            'audit_columns': 'created_date_only'}}}
        with pytest.raises(KeyError):
            self.to_map([CREATE_STMT], augmap)

    def test_bad_audit_spec(self):
        "Error on bad audit column specification"
        augmap = {'schema public': {'table t1': {
            'audit_column': 'created_date_only'}}}
        with pytest.raises(KeyError):
            self.to_map([CREATE_STMT], augmap)

    def test_unknown_audit_spec(self):
        "Error on non-existent audit column specification"
        augmap = {'schema public': {'table t1': {
            'audit_columns': 'created_date'}}}
        with pytest.raises(KeyError):
            self.to_map([CREATE_STMT], augmap)

    def test_new_column(self):
        "Add new (non-predefined) audit column"
        augmap = {'augmenter': {'columns': {
            'modified_date': {'type': 'date', 'not_null': True,
                              'default': "('now'::text)::date"}},
            'audit_columns': {'modified_date_only': {
                'columns': ['modified_date']}}},
            'schema public': {'table t1': {
                'audit_columns': 'modified_date_only'}}}
        dbmap = self.to_map([CREATE_STMT], augmap)
        expmap = {'columns': [
            {'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
            {'modified_date': {'type': 'date', 'not_null': True,
                               'default': "('now'::text)::date"}}]}
        assert expmap == dbmap['schema public']['table t1']

    def test_rename_column(self):
        "Add predefined audit column but with new name"
        augmap = {'augmenter': {'columns': {
            'modified_timestamp': {'name': 'updated'}}},
            'schema public': {'table t1': {
                'audit_columns': 'modified_only'}}}
        dbmap = self.to_map([CREATE_STMT], augmap)
        colmap = {'columns': [
            {'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
            {'updated': {'type': 'timestamp with time zone',
                         'not_null': True}}],
            'triggers': {'t1_20_audit_modified_only': {
                'events': ['insert', 'update'], 'level': 'row',
                'procedure': 'audit_modified()', 'timing': 'before'}}}
        funcmap = {'language': 'plpgsql', 'returns': 'trigger',
                   'security_definer': True, 'description':
                   'Provides modified_timestamp values for audit columns.',
                   'source': FUNC_SRC2}
        assert dbmap['schema public']['table t1'] == colmap
        assert dbmap['schema public']['function audit_modified()'] == funcmap

    def test_change_column_type(self):
        "Add predefined audit column but with changed datatype"
        augmap = {'augmenter': {'columns': {'created_date': {'type': 'text'}}},
                  'schema public': {'table t1': {
                  'audit_columns': 'created_date_only'}}}
        dbmap = self.to_map([CREATE_STMT], augmap)
        expmap = {'columns': [
            {'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
            {'created_date': {'type': 'text', 'not_null': True,
                              'default': "('now'::text)::date"}}]}
        assert expmap == dbmap['schema public']['table t1']

    def test_columns_with_trigger(self):
        "Add predefined audit columns with trigger"
        augmap = {'schema public': {'table t1': {'audit_columns': 'default'}}}
        dbmap = self.to_map([CREATE_STMT], augmap)
        expmap = {'columns': [
            {'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
            {'modified_by_user': {'type': 'character varying(63)',
                                  'not_null': True}},
            {'modified_timestamp': {'type': 'timestamp with time zone',
                                    'not_null': True}}],
            'triggers': {'t1_20_audit_default': {
                'events': ['update'], 'level': 'row',
                'procedure': 'audit_default()', 'timing': 'before'}}}
        assert expmap == dbmap['schema public']['table t1']
        assert dbmap['schema public']['function audit_default()'][
            'returns'] == 'trigger'
        assert dbmap['schema public']['function audit_default()'][
            'source'] == FUNC_SRC1

    def test_nonpublic_schema_with_trigger(self):
        "Add predefined audit columns with trigger in a non-public schema"
        stmts = ["CREATE SCHEMA s1",
                 "CREATE TABLE s1.t1 (c1 integer, c2 text)"]
        augmap = {'schema s1': {'table t1': {'audit_columns': 'default'}}}
        dbmap = self.to_map(stmts, augmap)
        expmap = {'columns': [
            {'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
            {'modified_by_user': {'type': 'character varying(63)',
                                  'not_null': True}},
            {'modified_timestamp': {'type': 'timestamp with time zone',
                                    'not_null': True}}],
            'triggers': {'t1_20_audit_default': {
                'events': ['update'], 'level': 'row',
                'procedure': 's1.audit_default()', 'timing': 'before'}}}
        assert expmap == dbmap['schema s1']['table t1']
        assert dbmap['schema s1']['function audit_default()']['returns'] == \
            'trigger'
        assert dbmap['schema s1']['function audit_default()'][
            'source'] == FUNC_SRC1

    def test_skip_existing_columns(self):
        "Do not add already existing audit columns"
        stmts = [CREATE_STMT,
                 "ALTER TABLE t1 ADD modified_by_user varchar(63) NOT NULL",
                 "ALTER TABLE t1 ADD modified_timestamp "
                 "timestamp with time zone NOT NULL"]
        augmap = {'schema public': {'table t1': {
            'audit_columns': 'default'}}}
        dbmap = self.to_map(stmts, augmap)
        expmap = [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                  {'modified_by_user': {'type': 'character varying(63)',
                                        'not_null': True}},
                  {'modified_timestamp': {'type': 'timestamp with time zone',
                                          'not_null': True}}]
        assert expmap == dbmap['schema public']['table t1']['columns']

    def test_change_existing_columns(self):
        "Change already existing audit columns"
        stmts = [CREATE_STMT, "ALTER TABLE t1 ADD modified_by_user text ",
                 "ALTER TABLE t1 ADD modified_timestamp "
                 "timestamp with time zone NOT NULL"]
        augmap = {'schema public': {'table t1': {'audit_columns': 'default'}}}
        dbmap = self.to_map(stmts, augmap)
        expmap = [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                  {'modified_by_user': {'type': 'character varying(63)',
                                        'not_null': True}},
                  {'modified_timestamp': {'type': 'timestamp with time zone',
                                          'not_null': True}}]
        assert expmap == dbmap['schema public']['table t1']['columns']

    def test_custom_function_template(self):
        "Add new (non-predefined) audit trigger using a function template"
        template = """
        BEGIN
          NEW.{{modified_by_user}} = SESSION_USER;
          NEW.{{modified_timestamp}} = CURRENT_TIMESTAMP::timestamp(0);
          RETURN NEW;
        END"""
        source = """
        BEGIN
          NEW.modified_by_user = SESSION_USER;
          NEW.modified_timestamp = CURRENT_TIMESTAMP::timestamp(0);
          RETURN NEW;
        END"""
        augmap = {
            'augmenter': {
                'audit_columns': {'custom': {
                    'columns': ['modified_by_user', 'modified_timestamp'],
                    'triggers': ['custom_audit']}},
                'function_templates': {'custom_template': template},
                'functions': {'custom_audit()': {
                    'description': 'Maintain custom audit columns',
                    'language': 'plpgsql',
                    'returns': 'trigger',
                    'security_definer': True,
                    'source': '{{custom_template}}'}},
                'triggers': {'custom_audit': {
                    'events': ['insert', 'update'],
                    'level': 'row',
                    'name': '{{table_name}}_20_custom_audit',
                    'procedure': 'custom_audit()',
                    'timing': 'before'}}},
            'schema public': {'table t1': {
                'audit_columns': 'custom'}}}
        dbmap = self.to_map([CREATE_STMT], augmap)
        expmap = {'columns': [
            {'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
            {'modified_by_user': {'type': 'character varying(63)',
                                  'not_null': True}},
            {'modified_timestamp': {'type': 'timestamp with time zone',
                                    'not_null': True}}],
            'triggers': {'t1_20_custom_audit': {
                'events': ['insert', 'update'], 'level': 'row',
                'procedure': 'custom_audit()', 'timing': 'before'}}}
        assert expmap == dbmap['schema public']['table t1']
        assert dbmap['schema public']['function custom_audit()'][
            'returns'] == 'trigger'
        assert dbmap['schema public']['function custom_audit()'][
            'source'] == source

    def test_custom_function_inline_with_column_substitution(self):
        "Add new (non-predefined) audit trigger using an inline definition"
        template = """
        BEGIN
          NEW.{{modified_by_user}} = SESSION_USER;
          NEW.{{modified_timestamp}} = CURRENT_TIMESTAMP::timestamp(0);
          RETURN NEW;
        END"""
        source = """
        BEGIN
          NEW.modified_by_user = SESSION_USER;
          NEW.modified_timestamp = CURRENT_TIMESTAMP::timestamp(0);
          RETURN NEW;
        END"""
        augmap = {
            'augmenter': {
                'audit_columns': {'custom': {
                    'columns': ['modified_by_user', 'modified_timestamp'],
                    'triggers': ['custom_audit']}},
                'functions': {'custom_audit()': {
                    'description': 'Maintain custom audit columns',
                    'language': 'plpgsql',
                    'returns': 'trigger',
                    'security_definer': True,
                    'source': template}},
                'triggers': {'custom_audit': {
                    'events': ['insert', 'update'],
                    'level': 'row',
                    'name': '{{table_name}}_20_custom_audit',
                    'procedure': 'custom_audit()',
                    'timing': 'before'}}},
            'schema public': {'table t1': {
                'audit_columns': 'custom'}}}
        dbmap = self.to_map([CREATE_STMT], augmap)
        expmap = {'columns': [
            {'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
            {'modified_by_user': {'type': 'character varying(63)',
                                  'not_null': True}},
            {'modified_timestamp': {'type': 'timestamp with time zone',
                                    'not_null': True}}],
            'triggers': {'t1_20_custom_audit': {
                'events': ['insert', 'update'], 'level': 'row',
                'procedure': 'custom_audit()', 'timing': 'before'}}}
        assert expmap == dbmap['schema public']['table t1']
        assert dbmap['schema public']['function custom_audit()'][
            'returns'] == 'trigger'
        assert dbmap['schema public']['function custom_audit()'][
            'source'] == source

########NEW FILE########
__FILENAME__ = test_cast
# -*- coding: utf-8 -*-
"""Test casts"""

import pytest

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

SOURCE = "SELECT CAST($1::int AS boolean)"
CREATE_FUNC = "CREATE FUNCTION int2_bool(smallint) RETURNS boolean " \
    "LANGUAGE sql IMMUTABLE AS $_$%s$_$" % SOURCE
CREATE_DOMAIN = "CREATE DOMAIN d1 AS integer"
CREATE_STMT1 = "CREATE CAST (smallint AS boolean) WITH FUNCTION " \
    "int2_bool(smallint)"
CREATE_STMT3 = "CREATE CAST (d1 AS integer) WITH INOUT AS IMPLICIT"
DROP_STMT = "DROP CAST IF EXISTS (smallint AS boolean)"
COMMENT_STMT = "COMMENT ON CAST (smallint AS boolean) IS 'Test cast 1'"


class CastToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing casts"""

    def test_map_cast_function(self):
        "Map a cast with a function"
        dbmap = self.to_map([CREATE_FUNC, CREATE_STMT1], superuser=True)
        expmap = {'function': 'int2_bool(smallint)', 'context': 'explicit',
                  'method': 'function'}
        assert dbmap['cast (smallint as boolean)'] == expmap

    def test_map_cast_inout(self):
        "Map a cast with INOUT"
        dbmap = self.to_map([CREATE_DOMAIN, CREATE_STMT3])
        expmap = {'context': 'implicit', 'method': 'inout'}
        assert dbmap['cast (d1 as integer)'] == expmap

    def test_map_cast_comment(self):
        "Map a cast comment"
        dbmap = self.to_map([CREATE_FUNC, CREATE_STMT1, COMMENT_STMT],
                            superuser=True)
        assert dbmap['cast (smallint as boolean)']['description'] == \
            'Test cast 1'


class CastToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input casts"""

    def test_create_cast_function(self):
        "Create a cast with a function"
        stmts = [DROP_STMT, CREATE_FUNC]
        inmap = self.std_map()
        inmap.update({'cast (smallint as boolean)': {
            'function': 'int2_bool(smallint)', 'context': 'explicit',
            'method': 'function'}})
        sql = self.to_sql(inmap, stmts)
        assert fix_indent(sql[0]) == CREATE_STMT1

    def test_create_cast_inout(self):
        "Create a cast with INOUT"
        stmts = [CREATE_DOMAIN, "DROP CAST IF EXISTS (d1 AS integer)"]
        inmap = self.std_map()
        inmap.update({'cast (d1 as integer)': {
            'context': 'implicit', 'method': 'inout'}})
        inmap['schema public'].update({'domain d1': {'type': 'integer'}})
        sql = self.to_sql(inmap, stmts)
        assert fix_indent(sql[0]) == CREATE_STMT3

    def test_create_cast_schema(self):
        "Create a cast using a type/domain in a non-public schema"
        stmts = ["CREATE SCHEMA s1", "CREATE DOMAIN s1.d1 AS integer",
                 "DROP CAST IF EXISTS (integer AS s1.d1)"]
        inmap = self.std_map()
        inmap.update({'cast (integer as s1.d1)': {
            'context': 'assignment', 'method': 'binary coercible'}})
        inmap.update({'schema s1': {'domain d1': {'type': 'integer'}}})
        sql = self.to_sql(inmap, stmts)
        assert fix_indent(sql[0]) == "CREATE CAST (integer AS s1.d1) " \
            "WITHOUT FUNCTION AS ASSIGNMENT"

    def test_bad_cast_map(self):
        "Error creating a cast with a bad map"
        inmap = self.std_map()
        inmap.update({'(smallint as boolean)': {
            'function': 'int2_bool(smallint)', 'context': 'explicit',
            'method': 'function'}})
        with pytest.raises(KeyError):
            self.to_sql(inmap)

    def test_drop_cast(self):
        "Drop an existing cast"
        stmts = [DROP_STMT, CREATE_FUNC, CREATE_STMT1]
        sql = self.to_sql(self.std_map(), stmts, superuser=True)
        assert sql[0] == "DROP CAST (smallint AS boolean)"

    def test_cast_with_comment(self):
        "Create a cast with a comment"
        inmap = self.std_map()
        inmap.update({'cast (smallint as boolean)': {
            'description': 'Test cast 1', 'function': 'int2_bool(smallint)',
            'context': 'explicit', 'method': 'function'}})
        inmap['schema public'].update({'function int2_bool(smallint)': {
            'returns': 'boolean', 'language': 'sql', 'immutable': True,
            'source': SOURCE}})
        sql = self.to_sql(inmap, [DROP_STMT])
        # sql[0] -> SET, sql[1] -> CREATE FUNCTION
        assert fix_indent(sql[2]) == CREATE_STMT1
        assert sql[3] == COMMENT_STMT

    def test_comment_on_cast(self):
        "Create a comment for an existing cast"
        stmts = [DROP_STMT, CREATE_FUNC, CREATE_STMT1]
        inmap = self.std_map()
        inmap.update({'cast (smallint as boolean)': {
            'description': 'Test cast 1', 'function': 'int2_bool(smallint)',
            'context': 'explicit', 'method': 'function'}})
        inmap['schema public'].update({'function int2_bool(smallint)': {
            'returns': 'boolean', 'language': 'sql', 'immutable': True,
            'source': SOURCE}})
        sql = self.to_sql(inmap, stmts, superuser=True)
        assert sql == [COMMENT_STMT]

    def test_drop_cast_comment(self):
        "Drop a comment on an existing cast"
        stmts = [DROP_STMT, CREATE_FUNC, CREATE_STMT1, COMMENT_STMT]
        inmap = self.std_map()
        inmap.update({'cast (smallint as boolean)': {
            'function': 'int2_bool(smallint)', 'context': 'explicit',
            'method': 'function'}})
        inmap['schema public'].update({'function int2_bool(smallint)': {
            'returns': 'boolean', 'language': 'sql', 'immutable': True,
            'source': SOURCE}})
        assert self.to_sql(inmap, stmts, superuser=True) == \
            ["COMMENT ON CAST (smallint AS boolean) IS NULL"]

    def test_change_cast_comment(self):
        "Change existing comment on a cast"
        stmts = [DROP_STMT, CREATE_FUNC, CREATE_STMT1, COMMENT_STMT]
        inmap = self.std_map()
        inmap.update({'cast (smallint as boolean)': {
            'description': 'Changed cast 1', 'function': 'int2_bool(smallint)',
            'context': 'explicit', 'method': 'function'}})
        inmap['schema public'].update({'function int2_bool(smallint)': {
            'returns': 'boolean', 'language': 'sql', 'immutable': True,
            'source': SOURCE}})
        assert self.to_sql(inmap, stmts, superuser=True) == \
            ["COMMENT ON CAST (smallint AS boolean) IS 'Changed cast 1'"]

########NEW FILE########
__FILENAME__ = test_collation
# -*- coding: utf-8 -*-
"""Test collations

These tests require that the locale fr_FR.utf8 (or equivalent) be installed.
"""
import sys

import pytest

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

if sys.platform == 'win32':
    COLL = 'French_France.1252'
else:
    COLL = 'fr_FR.UTF-8'

CREATE_STMT = "CREATE COLLATION c1 (LC_COLLATE = '%s', LC_CTYPE = '%s')" % (
    COLL, COLL)
COMMENT_STMT = "COMMENT ON COLLATION c1 IS 'Test collation c1'"


class CollationToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing collations"""

    def test_map_collation(self):
        "Map a collation"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        dbmap = self.to_map([CREATE_STMT])
        expmap = {'lc_collate': COLL, 'lc_ctype': COLL}
        assert dbmap['schema public']['collation c1'] == expmap

    def test_map_collation_comment(self):
        "Map a collation comment"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        dbmap = self.to_map([CREATE_STMT, COMMENT_STMT])
        assert dbmap['schema public']['collation c1']['description'] == \
            'Test collation c1'

    def test_map_column_collation(self):
        "Map a table with a column collation"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        dbmap = self.to_map(
            [CREATE_STMT, "CREATE TABLE t1 (c1 integer, c2 text COLLATE c1)"])
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'text', 'collation': 'c1'}}]}
        assert dbmap['schema public']['table t1'] == expmap

    def test_index_collation(self):
        "Map an index with column collation"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        stmts = [CREATE_STMT, "CREATE TABLE t1 (c1 integer, c2 text)",
                 "CREATE INDEX t1_idx ON t1 (c2 COLLATE c1)"]
        dbmap = self.to_map(stmts)
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'text'}}],
                  'indexes': {'t1_idx': {
                  'keys': [{'c2': {'collation': 'c1'}}]}}}
        assert dbmap['schema public']['table t1'] == expmap


class CollationToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input collations"""

    def test_create_collation(self):
        "Create a collation"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        inmap = self.std_map()
        inmap['schema public'].update({'collation c1': {
            'lc_collate': COLL, 'lc_ctype': COLL}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_STMT

    def test_create_collation_schema(self):
        "Create a collation in a non-public schema"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        inmap = self.std_map()
        inmap.update({'schema s1': {'collation c1': {
            'lc_collate': COLL, 'lc_ctype': COLL}}})
        sql = self.to_sql(inmap, ["CREATE SCHEMA s1"])
        assert fix_indent(sql[0]) == "CREATE COLLATION s1.c1 (" \
            "LC_COLLATE = '%s', LC_CTYPE = '%s')" % (COLL, COLL)

    def test_bad_collation_map(self):
        "Error creating a collation with a bad map"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        inmap = self.std_map()
        inmap['schema public'].update({'c1': {
            'lc_collate': COLL, 'lc_ctype': COLL}})
        with pytest.raises(KeyError):
            self.to_sql(inmap)

    def test_drop_collation(self):
        "Drop an existing collation"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        sql = self.to_sql(self.std_map(), [CREATE_STMT])
        assert sql[0] == "DROP COLLATION c1"

    def test_collation_with_comment(self):
        "Create a collation with a comment"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        inmap = self.std_map()
        inmap['schema public'].update({'collation c1': {
            'description': 'Test collation c1',
            'lc_collate': COLL, 'lc_ctype': COLL}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_STMT
        assert sql[1] == COMMENT_STMT

    def test_create_table_column_collation(self):
        "Create a table with a column with non-default collation"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer', 'not_null': True}},
                        {'c2': {'type': 'text', 'collation': 'c1'}}]}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == "CREATE TABLE t1 (c1 integer NOT NULL, " \
            'c2 text COLLATE "c1")'

    def test_create_index_collation(self):
        "Create an index with column collation"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        stmts = [CREATE_STMT, "CREATE TABLE t1 (c1 integer, c2 text)"]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'indexes': {'t1_idx': {'keys': [{'c2': {'collation': 'c1'}}]}}}})
        sql = self.to_sql(inmap, stmts)
        assert fix_indent(sql[0]) == \
            "CREATE INDEX t1_idx ON t1 (c2 COLLATE c1)"

    def test_create_type_attribute_collation(self):
        "Create a composite type with an attribute with non-default collation"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        inmap = self.std_map()
        inmap['schema public'].update({'type t1': {
            'attributes': [{'x': {'type': 'integer'}},
                           {'y': {'type': 'text', 'collation': 'c1'}}]}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == "CREATE TYPE t1 AS (x integer, " \
            'y text COLLATE "c1")'

########NEW FILE########
__FILENAME__ = test_column
# -*- coding: utf-8 -*-
"""Test columns"""

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

TYPELIST = [
    ('SMALLINT', 'smallint'),
    ('INTEGER', 'integer'),
    ('BIGINT', 'bigint'),
    ('int', 'integer'),
    ('int2', 'smallint'),
    ('int4', 'integer'),
    ('int8', 'bigint'),
    ('NUMERIC', 'numeric'),
    ('NUMERIC(1)', 'numeric(1,0)'),
    ('NUMERIC(12)', 'numeric(12,0)'),
    ('NUMERIC(1000)', 'numeric(1000,0)'),
    ('NUMERIC(12,2)', 'numeric(12,2)'),
    ('NUMERIC(1000,500)', 'numeric(1000,500)'),
    ('DECIMAL', 'numeric'),
    ('dec(9,5)', 'numeric(9,5)'),
    ('REAL', 'real'),
    ('DOUBLE PRECISION', 'double precision'),
    ('FLOAT', 'double precision'),
    ('FLOAT(1)', 'real'),
    ('FLOAT(24)', 'real'),
    ('FLOAT(25)', 'double precision'),
    ('FLOAT(53)', 'double precision'),
    # SERIAL and BIGSERIAL have side effects
    ('MONEY', 'money'),
    ('CHARACTER(1)', 'character(1)'),
    ('CHARACTER VARYING(200000)', 'character varying(200000)'),
    ('CHAR(16)', 'character(16)'),
    ('VARCHAR(256)', 'character varying(256)'),
    ('TEXT', 'text'),
    ('CHAR', 'character(1)'),
    ('CHARACTER VARYING', 'character varying'),
    ('"char"', '"char"'),
    ('name', 'name'),
    ('bytea', 'bytea'),
    ('DATE', 'date'),
    ('TIME', 'time without time zone'),
    ('TIME WITHOUT TIME ZONE', 'time without time zone'),
    ('TIME WITH TIME ZONE', 'time with time zone'),
    ('TIMESTAMP', 'timestamp without time zone'),
    ('TIMESTAMP WITHOUT TIME ZONE', 'timestamp without time zone'),
    ('TIMESTAMP WITH TIME ZONE', 'timestamp with time zone'),
    ('TIME(0)', 'time(0) without time zone'),
    ('TIME(1) WITHOUT TIME ZONE', 'time(1) without time zone'),
    ('TIME(2) WITH TIME ZONE', 'time(2) with time zone'),
    ('TIMESTAMP(3)', 'timestamp(3) without time zone'),
    ('TIMESTAMP(4) WITHOUT TIME ZONE', 'timestamp(4) without time zone'),
    ('TIMESTAMP(5) WITH TIME ZONE', 'timestamp(5) with time zone'),
    ('INTERVAL', 'interval'),
    ('INTERVAL(6)', 'interval(6)'),
    ('INTERVAL YEAR', 'interval year'),
    ('INTERVAL MONTH', 'interval month'),
    ('INTERVAL DAY', 'interval day'),
    ('INTERVAL HOUR', 'interval hour'),
    ('INTERVAL MINUTE', 'interval minute'),
    ('INTERVAL SECOND', 'interval second'),
    ('INTERVAL YEAR TO MONTH', 'interval year to month'),
    ('INTERVAL DAY TO HOUR', 'interval day to hour'),
    ('INTERVAL DAY TO MINUTE', 'interval day to minute'),
    ('INTERVAL DAY TO SECOND', 'interval day to second'),
    ('INTERVAL HOUR TO MINUTE', 'interval hour to minute'),
    ('INTERVAL HOUR TO SECOND', 'interval hour to second'),
    ('INTERVAL MINUTE TO SECOND', 'interval minute to second'),
    ('INTERVAL SECOND(3)', 'interval second(3)'),
    ('INTERVAL HOUR TO SECOND(5)', 'interval hour to second(5)'),
    ('BOOLEAN', 'boolean'),
    ('POINT', 'point'),
    ('LINE', 'line'),
    ('LSEG', 'lseg'),
    ('BOX', 'box'),
    ('PATH', 'path'),
    ('POLYGON', 'polygon'),
    ('CIRCLE', 'circle'),
    ('cidr', 'cidr'),
    ('inet', 'inet'),
    ('macaddr', 'macaddr'),
    ('BIT(2)', 'bit(2)'),
    ('BIT VARYING(100)', 'bit varying(100)'),
    ('BIT', 'bit(1)'),
    ('BIT VARYING', 'bit varying'),
    ('tsvector', 'tsvector'),
    ('tsquery', 'tsquery'),
    ('UUID', 'uuid'),
    ('XML', 'xml')]

CREATE_STMT1 = "CREATE TABLE t1 (c1 integer, c2 text)"
CREATE_STMT2 = "CREATE TABLE t1 (c1 integer, c2 text, c3 date)"
CREATE_STMT3 = "CREATE TABLE t1 (c1 integer, c2 text, c3 date, c4 text)"
DROP_COL_STMT = "ALTER TABLE t1 DROP COLUMN c3"


class ColumnToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of column-related elements in created tables"""

    def test_data_types(self):
        "Map a table with columns for (almost) each native PostgreSQL type"
        colstab = []
        colsmap = []
        for colnum, (coltype, maptype) in enumerate(TYPELIST):
            col = "c%d" % (colnum + 1)
            colstab.append("%s %s" % (col, coltype))
            colsmap.append({col: {'type': maptype}})
        dbmap = self.to_map(["CREATE TABLE t1 (%s)" % ", ".join(colstab)])
        expmap = {'columns': colsmap}
        assert dbmap['schema public']['table t1'] == expmap

    def test_not_null(self):
        "Map a table with a NOT NULL column"
        stmts = ["CREATE TABLE t1 (c1 INTEGER, c2 INTEGER NULL, "
                 "c3 INTEGER NOT NULL)"]
        dbmap = self.to_map(stmts)
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'integer'}},
                              {'c3': {'type': 'integer', 'not_null': True}}]}

        assert dbmap['schema public']['table t1'] == expmap

    def test_column_defaults(self):
        "Map a table with various types and each with a DEFAULT clause"
        stmts = ["CREATE TABLE t1 (c1 INTEGER DEFAULT 12345, "
                 "c2 NUMERIC DEFAULT 98.76, c3 REAL DEFAULT 15e-2, "
                 "c4 TEXT DEFAULT 'Abc def', c5 DATE DEFAULT CURRENT_DATE, "
                 "c6 TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP, "
                 "c7 BOOLEAN DEFAULT FALSE)"]
        dbmap = self.to_map(stmts)
        expmap = {'columns': [
            {'c1': {'type': 'integer', 'default': '12345'}},
            {'c2': {'type': 'numeric', 'default': '98.76'}},
            {'c3': {'type': 'real', 'default': '0.15'}},
            {'c4': {'type': 'text', 'default': "'Abc def'::text"}},
            {'c5': {'type': 'date', 'default': "('now'::text)::date"}},
            {'c6': {'type': 'timestamp with time zone', 'default': 'now()'}},
            {'c7': {'type': 'boolean', 'default': 'false'}}]}
        assert dbmap['schema public']['table t1'] == expmap

    def test_statistics(self):
        "Map a table with column statistics"
        stmts = [CREATE_STMT1, "ALTER TABLE t1 ALTER c1 SET STATISTICS 100"]
        dbmap = self.to_map(stmts)
        expmap = {'columns': [{'c1': {'type': 'integer', 'statistics': 100}},
                              {'c2': {'type': 'text'}}]}
        assert dbmap['schema public']['table t1'] == expmap


class ColumnToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation of column-related statements from input schemas"""

    def test_create_table_with_defaults(self):
        "Create a table with two column DEFAULTs, one referring to a SEQUENCE"
        inmap = self.std_map()
        inmap.update({'schema s1': {'table t1': {
            'columns': [{'c1': {'type': 'integer', 'not_null': True,
                                'default':
                                    "nextval('s1.t1_c1_seq'::regclass)"}},
                        {'c2': {'type': 'text', 'not_null': True,
                                'collation': 'en_US.utf8'}},
                        {'c3': {'type': 'date', 'not_null': True,
                                'default': "('now'::text)::date"}}]},
            'sequence t1_c1_seq': {
                'cache_value': 1, 'increment_by': 1, 'max_value': None,
                'min_value': None, 'start_value': 1,
                'owner_table': 't1', 'owner_column': 'c1'}}})
        sql = self.to_sql(inmap, ["CREATE SCHEMA s1"])
        assert fix_indent(sql[0]) == "CREATE SEQUENCE s1.t1_c1_seq " \
            "START WITH 1 INCREMENT BY 1 NO MAXVALUE NO MINVALUE CACHE 1"
        assert fix_indent(sql[1]) == "CREATE TABLE s1.t1 (" \
            'c1 integer NOT NULL, c2 text NOT NULL COLLATE "en_US.utf8", ' \
            "c3 date NOT NULL DEFAULT ('now'::text)::date)"
        assert sql[2] == "ALTER SEQUENCE s1.t1_c1_seq OWNED BY s1.t1.c1"
        assert sql[3] == "ALTER TABLE s1.t1 ALTER COLUMN c1 " \
            "SET DEFAULT nextval('s1.t1_c1_seq'::regclass)"

    def test_set_column_not_null(self):
        "Change a nullable column to NOT NULL"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer', 'not_null': True}},
                        {'c2': {'type': 'text'}}]}})
        sql = self.to_sql(inmap, [CREATE_STMT1])
        assert fix_indent(sql[0]) == \
            "ALTER TABLE t1 ALTER COLUMN c1 SET NOT NULL"

    def test_change_column_types(self):
        "Change the datatypes of two columns"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'bigint'}},
                        {'c2': {'type': 'varchar(25)'}}]}})
        sql = self.to_sql(inmap, [CREATE_STMT1])
        assert fix_indent(sql[0]) == \
            "ALTER TABLE t1 ALTER COLUMN c1 TYPE bigint"
        assert fix_indent(sql[1]) == \
            "ALTER TABLE t1 ALTER COLUMN c2 TYPE varchar(25)"

    def test_add_column1(self):
        "Add new column to a table"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c3': {'type': 'date'}}, {'c4': {'type': 'text'}}]}})
        sql = self.to_sql(inmap, [CREATE_STMT2])
        assert fix_indent(sql[0]) == "ALTER TABLE t1 ADD COLUMN c4 text"

    def test_add_column2(self):
        "Add column to a table that has a dropped column"
        stmts = [CREATE_STMT2, "ALTER TABLE t1 DROP COLUMN c2"]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c3': {'type': 'date'}}]}})
        sql = self.to_sql(inmap, stmts)
        assert len(sql) == 1
        assert fix_indent(sql[0]) == "ALTER TABLE t1 ADD COLUMN c2 text"

    def test_add_column3(self):
        "No change on a table that has a dropped column"
        stmts = [CREATE_STMT3, DROP_COL_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c4': {'type': 'text'}}]}})
        sql = self.to_sql(inmap, stmts)
        assert len(sql) == 0

    def test_add_column4(self):
        "Add two columns to a table that has a dropped column"
        stmts = [CREATE_STMT2, DROP_COL_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c3': {'type': 'date'}}, {'c4': {'type': 'text'}}]}})
        sql = self.to_sql(inmap, stmts)
        assert fix_indent(sql[0]) == "ALTER TABLE t1 ADD COLUMN c3 date"
        assert fix_indent(sql[1]) == "ALTER TABLE t1 ADD COLUMN c4 text"

    def test_drop_column1(self):
        "Drop a column from the end of a table"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c3': {'type': 'date'}}]}})
        sql = self.to_sql(inmap, [CREATE_STMT3])
        assert fix_indent(sql[0]) == "ALTER TABLE t1 DROP COLUMN c4"

    def test_drop_column2(self):
        "Drop a column from the middle of a table"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c4': {'type': 'text'}}]}})
        sql = self.to_sql(inmap, [CREATE_STMT3])
        assert fix_indent(sql[0]) == "ALTER TABLE t1 DROP COLUMN c3"

    def test_drop_column3(self):
        "Drop a column from the beginning of a table"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c2': {'type': 'text'}}, {'c3': {'type': 'date'}},
                        {'c4': {'type': 'text'}}]}})
        sql = self.to_sql(inmap, [CREATE_STMT3])
        assert fix_indent(sql[0]) == "ALTER TABLE t1 DROP COLUMN c1"

    def test_rename_column(self):
        "Rename a table column"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}},
                        {'c3': {'type': 'text', 'oldname': 'c2'}}]}})
        sql = self.to_sql(inmap, [CREATE_STMT1])
        assert sql[0] == "ALTER TABLE t1 RENAME COLUMN c2 TO c3"

    def test_drop_add_column1(self):
        "Drop and re-add table column from the end, almost like a RENAME"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c4': {'type': 'date'}}]}})
        sql = self.to_sql(inmap, [CREATE_STMT2])
        assert fix_indent(sql[0]) == "ALTER TABLE t1 ADD COLUMN c4 date"
        assert sql[1] == "ALTER TABLE t1 DROP COLUMN c3"

    def test_drop_add_column2(self):
        "Drop and re-add table column from the beginning"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c2': {'type': 'text'}}, {'c3': {'type': 'date'}},
                        {'c4': {'type': 'text'}}]}})
        sql = self.to_sql(inmap, [CREATE_STMT2])
        assert fix_indent(sql[0]) == "ALTER TABLE t1 ADD COLUMN c4 text"
        assert sql[1] == "ALTER TABLE t1 DROP COLUMN c1"

    def test_drop_add_column3(self):
        "Drop and re-add table columns from table with dropped column"
        stmts = [CREATE_STMT2, DROP_COL_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c2': {'type': 'text'}}, {'c3': {'type': 'date'}},
                        {'c4': {'type': 'text'}}]}})
        sql = self.to_sql(inmap, stmts)
        assert fix_indent(sql[0]) == "ALTER TABLE t1 ADD COLUMN c3 date"
        assert fix_indent(sql[1]) == "ALTER TABLE t1 ADD COLUMN c4 text"
        assert sql[2] == "ALTER TABLE t1 DROP COLUMN c1"

    def test_drop_column_in_schema(self):
        "Drop a column from a table in a non-public schema"
        stmts = ["CREATE SCHEMA s1",
                 "CREATE TABLE s1.t1 (c1 integer, c2 text, c3 date)"]
        inmap = self.std_map()
        inmap.update({'schema s1': {'table t1': {
            'columns': [{'c1': {'type': 'integer'}},
                        {'c2': {'type': 'text'}}]}}})
        sql = self.to_sql(inmap, stmts)
        assert fix_indent(sql[0]) == "ALTER TABLE s1.t1 DROP COLUMN c3"

    def test_inherit_add_parent_column(self):
        "Add a column to parent table, child should not add as well"
        stmts = [CREATE_STMT1, "CREATE TABLE t2 (c3 date) INHERITS (t1)"]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c4': {'type': 'text'}}]}})
        inmap['schema public'].update({'table t2': {
            'columns': [{'c1': {'type': 'integer', 'inherited': True}},
                        {'c2': {'type': 'text', 'inherited': True}},
                        {'c3': {'type': 'date'}},
                        {'c4': {'type': 'text', 'inherited': True}}],
            'inherits': ['t1']}})
        sql = self.to_sql(inmap, stmts)
        assert len(sql) == 1
        assert fix_indent(sql[0]) == "ALTER TABLE t1 ADD COLUMN c4 text"

    def test_inherit_drop_parent_column(self):
        "Drop a column from a parent table, child should not drop as well"
        stmts = [CREATE_STMT1, "CREATE TABLE t2 (c3 date) INHERITS (t1)"]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}]}})
        inmap['schema public'].update({'table t2': {
            'columns': [{'c1': {'type': 'integer', 'inherited': True}},
                        {'c3': {'type': 'date'}}], 'inherits': ['t1']}})
        sql = self.to_sql(inmap, stmts)
        assert len(sql) == 1
        assert fix_indent(sql[0]) == "ALTER TABLE t1 DROP COLUMN c2"

    def test_alter_statistics(self):
        "Alter a table to add column statistics"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer', 'statistics': 100}},
                        {'c2': {'type': 'text'}}]}})
        sql = self.to_sql(inmap, [CREATE_STMT1, "ALTER TABLE t1 ALTER c2 "
                                  "SET STATISTICS 1000"])
        assert fix_indent(sql[0]) == \
            "ALTER TABLE t1 ALTER COLUMN c1 SET STATISTICS 100"
        assert fix_indent(sql[1]) == \
            "ALTER TABLE t1 ALTER COLUMN c2 SET STATISTICS -1"

########NEW FILE########
__FILENAME__ = test_constraint
# -*- coding: utf-8 -*-
"""Test constraints"""

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

COMMENT_STMT = "COMMENT ON CONSTRAINT cns1 ON t1 IS 'Test constraint cns1'"


class CheckConstraintToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of created CHECK constraints"""

    def test_check_constraint_1(self):
        "Map a table with a CHECK constraint"
        stmts = ["CREATE TABLE t1 (c1 INTEGER, c2 SMALLINT CHECK (c2 < 1000))"]
        dbmap = self.to_map(stmts)
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'smallint'}}],
                  'check_constraints': {'t1_c2_check': {
                  'columns': ['c2'], 'expression': '(c2 < 1000)'}}}
        assert dbmap['schema public']['table t1'] == expmap

    def test_check_constraint_2(self):
        "Map a table with a two-column, named CHECK constraint"
        stmts = ["CREATE TABLE t1 (c1 INTEGER, c2 INTEGER, "
                 "CONSTRAINT t1_check_ratio CHECK (c2 * 100 / c1 <= 50))"]
        dbmap = self.to_map(stmts)
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'integer'}}],
                  'check_constraints': {'t1_check_ratio': {
                  'columns': ['c2', 'c1'],
                  'expression': '(((c2 * 100) / c1) <= 50)'}}}
        assert dbmap['schema public']['table t1'] == expmap


class CheckConstraintToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input CHECK constraints"""

    def test_create_w_check_constraint(self):
        "Create new table with a single column CHECK constraint"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}},
                        {'c2': {'type': 'text'}}],
            'check_constraints': {
                't1_c1_check': {'columns': ['c1'],
                                'expression': 'c1 > 0 and c1 < 1000000'}}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == "CREATE TABLE t1 (c1 integer, c2 text)"
        assert fix_indent(sql[1]) == "ALTER TABLE t1 ADD CONSTRAINT " \
            "t1_c1_check CHECK (c1 > 0 and c1 < 1000000)"

    def test_add_check_constraint(self):
        "Add a two-column CHECK constraint to an existing table"
        stmts = ["CREATE TABLE t1 (c1 INTEGER NOT NULL, c2 INTEGER NOT NULL, "
                 "c3 TEXT)"]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer', 'not_null': True}},
                        {'c2': {'type': 'integer', 'not_null': True}},
                        {'c3': {'type': 'text'}}],
            'check_constraints': {
                't1_check_2_1': {'columns': ['c2', 'c1'],
                                 'expression': 'c2 != c1'}}}})
        sql = self.to_sql(inmap, stmts)
        assert fix_indent(sql[0]) == "ALTER TABLE t1 ADD CONSTRAINT " \
            "t1_check_2_1 CHECK (c2 != c1)"


class PrimaryKeyToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of created PRIMARY KEYs"""

    map_pkey1 = {'columns': [{'c1': {'type': 'integer', 'not_null': True}},
                             {'c2': {'type': 'text'}}],
                 'primary_key': {'t1_pkey': {'columns': ['c1']}}}
    map_pkey2 = {'columns': [
        {'c1': {'type': 'integer', 'not_null': True}},
        {'c2': {'type': 'character(5)', 'not_null': True}},
        {'c3': {'type': 'text'}}],
        'primary_key': {'t1_pkey': {'columns': ['c2', 'c1']}}}

    map_pkey3 = {'columns': [{'c1': {'type': 'integer', 'not_null': True}},
                             {'c2': {'type': 'text'}}],
                 'primary_key': {'t1_prim_key': {'columns': ['c1']}}}

    def test_primary_key_1(self):
        "Map a table with a single-column primary key"
        stmts = ["CREATE TABLE t1 (c1 INTEGER PRIMARY KEY, c2 TEXT)"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1'] == self.map_pkey1

    def test_primary_key_2(self):
        "Map a table with a single-column primary key, table-level constraint"
        stmts = ["CREATE TABLE t1 (c1 INTEGER, c2 TEXT, PRIMARY KEY (c1))"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1'] == self.map_pkey1

    def test_primary_key_3(self):
        "Map a table with two-column primary key, atypical order"
        stmts = ["CREATE TABLE t1 (c1 INTEGER, c2 CHAR(5), c3 TEXT, "
                 "PRIMARY KEY (c2, c1))"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1'] == self.map_pkey2

    def test_primary_key_4(self):
        "Map a table with a named primary key constraint"
        stmts = ["CREATE TABLE t1 (c1 INTEGER, c2 TEXT, "
                 "CONSTRAINT t1_prim_key PRIMARY KEY (c1))"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1'] == self.map_pkey3

    def test_primary_key_5(self):
        "Map a table with a named primary key, column level constraint"
        stmts = ["CREATE TABLE t1 ("
                 "c1 INTEGER CONSTRAINT t1_prim_key PRIMARY KEY, c2 TEXT)"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1'] == self.map_pkey3

    def test_primary_key_cluster(self):
        "Map a table with a primary key and CLUSTER on it"
        stmts = ["CREATE TABLE t1 (c1 integer PRIMARY KEY, c2 text)",
                 "CLUSTER t1 USING t1_pkey"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1'] == {
            'columns': [{'c1': {'type': 'integer', 'not_null': True}},
                        {'c2': {'type': 'text'}}],
            'primary_key': {'t1_pkey': {'columns': ['c1'], 'cluster': True}}}

    def test_map_pk_comment(self):
        "Map a primary key with a comment"
        stmts = ["CREATE TABLE t1 (c1 integer CONSTRAINT cns1 PRIMARY KEY, "
                 "c2 text)", COMMENT_STMT]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1']['primary_key']['cns1'][
            'description'] == 'Test constraint cns1'


class PrimaryKeyToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input PRIMARY KEYs"""

    def test_create_with_primary_key(self):
        "Create new table with single column primary key"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'text'}}, {'c2': {'type': 'integer'}}],
            'primary_key': {'t1_pkey': {'columns': ['c2']}}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == "CREATE TABLE t1 (c1 text, c2 integer)"
        assert fix_indent(sql[1]) == "ALTER TABLE t1 ADD CONSTRAINT t1_pkey " \
            "PRIMARY KEY (c2)"

    def test_add_primary_key(self):
        "Add a two-column primary key to an existing table"
        stmts = ["CREATE TABLE t1 (c1 INTEGER NOT NULL, c2 INTEGER NOT NULL, "
                 "c3 TEXT)"]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer', 'not_null': True}},
                        {'c2': {'type': 'integer', 'not_null': True}},
                        {'c3': {'type': 'text'}}],
            'primary_key': {'t1_pkey': {'columns': ['c1', 'c2']}}}})
        sql = self.to_sql(inmap, stmts)
        assert fix_indent(sql[0]) == "ALTER TABLE t1 ADD CONSTRAINT t1_pkey " \
            "PRIMARY KEY (c1, c2)"

    def test_drop_primary_key(self):
        "Drop a primary key on an existing table"
        stmts = ["CREATE TABLE t1 (c1 INTEGER NOT NULL PRIMARY KEY, c2 TEXT)"]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer', 'not_null': True},
                         'c2': {'type': 'text'}}]}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["ALTER TABLE t1 DROP CONSTRAINT t1_pkey"]

    def test_primary_key_clustered(self):
        "Create new table clustered on the primary key"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer', 'not_null': True}},
                        {'c2': {'type': 'text'}}],
            'primary_key': {'t1_pkey': {'columns': ['c1'], 'cluster': True}}}})
        sql = self.to_sql(inmap)
        assert sql[2] == "CLUSTER t1 USING t1_pkey"

    def test_primary_key_uncluster(self):
        "Remove cluster from table clustered on the primary key"
        stmts = ["CREATE TABLE t1 (c1 integer PRIMARY KEY, c2 text)",
                 "CLUSTER t1 USING t1_pkey"]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer', 'not_null': True}},
                        {'c2': {'type': 'text'}}],
            'primary_key': {'t1_pkey': {'columns': ['c1']}}}})
        sql = self.to_sql(inmap, stmts)
        assert fix_indent(sql[0]) == "ALTER TABLE t1 SET WITHOUT CLUSTER"


class ForeignKeyToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of created FOREIGN KEYs"""

    map_fkey1 = {'columns': [{'c1': {'type': 'integer'}},
                             {'c2': {'type': 'integer'}},
                             {'c3': {'type': 'text'}}],
                 'foreign_keys': {'t1_c2_fkey': {
                     'columns': ['c2'], 'references': {
                         'schema': 'public', 'table': 't2',
                         'columns': ['pc1']}}}}

    map_fkey2 = {'columns': [{'c1': {'type': 'integer'}},
                             {'c2': {'type': 'character(5)'}},
                             {'c3': {'type': 'integer'}},
                             {'c4': {'type': 'date'}},
                             {'c5': {'type': 'text'}}],
                 'foreign_keys': {'t1_c2_fkey': {
                     'columns': ['c2', 'c3', 'c4'], 'references': {
                         'schema': 'public', 'table': 't2',
                         'columns': ['pc2', 'pc1', 'pc3']}}}}

    map_fkey3 = {'columns': [{'c1': {'type': 'integer'}},
                             {'c2': {'type': 'character(5)'}},
                             {'c3': {'type': 'integer'}},
                             {'c4': {'type': 'date'}},
                             {'c5': {'type': 'text'}}],
                 'foreign_keys': {'t1_fgn_key': {
                     'columns': ['c2', 'c3', 'c4'], 'references': {
                         'schema': 'public', 'table': 't2',
                         'columns': ['pc2', 'pc1', 'pc3']}}}}

    map_fkey4 = {'columns': [{'c1': {'type': 'integer', 'not_null': True}},
                             {'c2': {'type': 'character(5)',
                                     'not_null': True}},
                             {'c3': {'type': 'integer'}},
                             {'c4': {'type': 'date'}},
                             {'c5': {'type': 'text'}}],
                 'primary_key': {'t1_prim_key': {'columns': ['c1', 'c2']}},
                 'foreign_keys': {
                     't1_fgn_key1': {
                         'columns': ['c2', 'c3', 'c4'],
                         'references': {'schema': 'public', 'table': 't2',
                                        'columns': ['pc2', 'pc1', 'pc3']}},
                     't1_fgn_key2': {'columns': ['c2'],
                         'references': {'schema': 'public', 'table': 't3',
                                        'columns': ['qc1']}}}}

    def test_foreign_key_1(self):
        "Map a table with a single-column foreign key on another table"
        stmts = ["CREATE TABLE t2 (pc1 INTEGER PRIMARY KEY, pc2 TEXT)",
                 "CREATE TABLE t1 (c1 INTEGER, "
                 "c2 INTEGER REFERENCES t2 (pc1), c3 TEXT)"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1'] == self.map_fkey1

    def test_foreign_key_2(self):
        "Map a table with a single-column foreign key, table level constraint"
        stmts = ["CREATE TABLE t2 (pc1 INTEGER PRIMARY KEY, pc2 TEXT)",
                 "CREATE TABLE t1 (c1 INTEGER, c2 INTEGER, c3 TEXT, "
                 "FOREIGN KEY (c2) REFERENCES t2 (pc1))"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1'] == self.map_fkey1

    def test_foreign_key_3(self):
        "Map a table with a three-column foreign key"
        stmts = ["CREATE TABLE t2 (pc1 INTEGER, pc2 CHAR(5), pc3 DATE, "
                 "pc4 TEXT, PRIMARY KEY (pc2, pc1, pc3))",
                 "CREATE TABLE t1 (c1 INTEGER, c2 CHAR(5), c3 INTEGER, "
                 "c4 DATE, c5 TEXT, "
                 "FOREIGN KEY (c2, c3, c4) REFERENCES t2 (pc2, pc1, pc3))"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1'] == self.map_fkey2

    def test_foreign_key_4(self):
        "Map a table with a named, three-column foreign key"
        stmts = ["CREATE TABLE t2 (pc1 INTEGER, pc2 CHAR(5), pc3 DATE, "
                 "pc4 TEXT, PRIMARY KEY (pc2, pc1, pc3))",
                 "CREATE TABLE t1 (c1 INTEGER, c2 CHAR(5), c3 INTEGER, "
                 "c4 DATE, c5 TEXT, "
                 "CONSTRAINT t1_fgn_key FOREIGN KEY (c2, c3, c4) "
                 "REFERENCES t2 (pc2, pc1, pc3))"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1'] == self.map_fkey3

    def test_foreign_key_5(self):
        "Map a table with a primary key and two foreign keys"
        stmts = ["CREATE TABLE t2 (pc1 INTEGER, pc2 CHAR(5), pc3 DATE, "
                 "pc4 TEXT, PRIMARY KEY (pc2, pc1, pc3))",
                 "CREATE TABLE t3 (qc1 CHAR(5) PRIMARY KEY, qc2 text)",
                 "CREATE TABLE t1 (c1 INTEGER, c2 CHAR(5), c3 INTEGER, "
                 "c4 DATE, c5 TEXT, "
                 "CONSTRAINT t1_prim_key PRIMARY KEY (c1, c2), "
                 "CONSTRAINT t1_fgn_key1 FOREIGN KEY (c2, c3, c4) "
                 "REFERENCES t2 (pc2, pc1, pc3), "
                 "CONSTRAINT t1_fgn_key2 FOREIGN KEY (c2) "
                 "REFERENCES t3 (qc1))"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1'] == self.map_fkey4

    def test_foreign_key_actions(self):
        "Map a table with foreign key ON UPDATE/ON DELETE actions"
        stmts = ["CREATE TABLE t2 (pc1 INTEGER PRIMARY KEY, pc2 TEXT)",
                 "CREATE TABLE t1 (c1 INTEGER, c2 INTEGER, c3 TEXT, "
                 "FOREIGN KEY (c2) REFERENCES t2 (pc1) "
                 "ON UPDATE RESTRICT ON DELETE SET NULL)"]
        dbmap = self.to_map(stmts)
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'integer'}},
                              {'c3': {'type': 'text'}}],
                  'foreign_keys': {'t1_c2_fkey': {
                      'columns': ['c2'], 'on_update': 'restrict',
                      'on_delete': 'set null',
                      'references': {'schema': 'public', 'table': 't2',
                                     'columns': ['pc1']}}}}
        assert dbmap['schema public']['table t1'] == expmap

    def test_cross_schema_foreign_key(self):
        "Map a table with a foreign key on a table in another schema"
        stmts = ["CREATE SCHEMA s1",
                 "CREATE TABLE t2 (pc1 INTEGER PRIMARY KEY, pc2 TEXT)",
                 "CREATE TABLE s1.t1 (c1 INTEGER PRIMARY KEY, "
                 "c2 INTEGER REFERENCES t2 (pc1), c3 TEXT)"]
        dbmap = self.to_map(stmts)
        t2map = {'columns': [{'pc1': {'type': 'integer', 'not_null': True}},
                             {'pc2': {'type': 'text'}}],
                 'primary_key': {'t2_pkey': {'columns': ['pc1']}}}
        t1map = {'table t1': {
            'columns': [{'c1': {'type': 'integer', 'not_null': True}},
                        {'c2': {'type': 'integer'}}, {'c3': {'type': 'text'}}],
            'primary_key': {'t1_pkey': {'columns': ['c1']}},
            'foreign_keys': {'t1_c2_fkey': {
                'columns': ['c2'],
                'references': {'schema': 'public', 'table': 't2',
                               'columns': ['pc1']}}}}}
        assert dbmap['schema public']['table t2'] == t2map
        assert dbmap['schema s1'] == t1map

    def test_multiple_foreign_key(self):
        "Map a table with its primary key referenced by two others"
        stmts = ["CREATE TABLE t1 (pc1 integer PRIMARY KEY, pc2 text)",
                 "CREATE TABLE t2 (c1 integer, "
                 "c2 integer REFERENCES t1 (pc1), c3 text, "
                 "c4 integer REFERENCES t1 (pc1))"]
        dbmap = self.to_map(stmts)
        t1map = {'columns': [{'pc1': {'type': 'integer', 'not_null': True}},
                             {'pc2': {'type': 'text'}}],
                 'primary_key': {'t1_pkey': {'columns': ['pc1']}}}
        t2map = {'columns': [{'c1': {'type': 'integer'}},
                             {'c2': {'type': 'integer'}},
                             {'c3': {'type': 'text'}},
                             {'c4': {'type': 'integer'}}],
                 'foreign_keys': {
                     't2_c2_fkey': {
                         'columns': ['c2'],
                         'references': {'schema': 'public', 'table': 't1',
                                        'columns': ['pc1']}},
                     't2_c4_fkey': {
                         'columns': ['c4'],
                         'references': {'schema': 'public', 'table': 't1',
                                        'columns': ['pc1']}}}}
        assert dbmap['schema public']['table t1'] == t1map
        assert dbmap['schema public']['table t2'] == t2map

    def test_foreign_key_dropped_column(self):
        "Map a table with a foreign key after a column has been dropped"
        stmts = ["CREATE TABLE t1 (pc1 integer PRIMARY KEY, pc2 text)",
                 "CREATE TABLE t2 (c1 integer, c2 text, c3 smallint, "
                 "c4 integer REFERENCES t1 (pc1))",
                 "ALTER TABLE t2 DROP COLUMN c3"]
        dbmap = self.to_map(stmts)
        t2map = {'columns': [{'c1': {'type': 'integer'}},
                             {'c2': {'type': 'text'}},
                             {'c4': {'type': 'integer'}}],
                 'foreign_keys': {'t2_c4_fkey': {
                     'columns': ['c4'],
                     'references': {'schema': 'public', 'table': 't1',
                                    'columns': ['pc1']}}}}
        assert dbmap['schema public']['table t2'] == t2map

    def test_foreign_key_deferred(self):
        "Check constraints deferred status"
        stmts = ["CREATE TABLE t2 (pc1 INTEGER PRIMARY KEY, pc2 TEXT)",
                 "CREATE TABLE t1 (c1 INTEGER, "
                 "c2 INTEGER REFERENCES t2 (pc1), "
                 "c3 INTEGER REFERENCES t2 (pc1) DEFERRABLE, "
                 "c4 INTEGER REFERENCES t2 (pc1) DEFERRABLE "
                 "INITIALLY DEFERRED)"]
        dbmap = self.to_map(stmts)
        fks = dbmap['schema public']['table t1']['foreign_keys']
        assert not fks['t1_c2_fkey'].get('deferrable')
        assert not fks['t1_c2_fkey'].get('deferred')
        assert fks['t1_c3_fkey'].get('deferrable')
        assert not fks['t1_c3_fkey'].get('deferred')
        assert fks['t1_c4_fkey'].get('deferrable')
        assert fks['t1_c4_fkey'].get('deferred')

    def test_foreign_key_match(self):
        "Map a foreign key constraint with a MATCH specification"
        stmts = ["CREATE TABLE t2 (pc1 INTEGER PRIMARY KEY, pc2 TEXT)",
                 "CREATE TABLE t1 (c1 INTEGER, "
                 "c2 INTEGER REFERENCES t2 (pc1) MATCH FULL, c3 TEXT)"]
        dbmap = self.to_map(stmts)
        t1map = {'columns': [{'c1': {'type': 'integer'}},
                             {'c2': {'type': 'integer'}},
                             {'c3': {'type': 'text'}}],
                 'foreign_keys': {'t1_c2_fkey': {
                     'columns': ['c2'], 'match': 'full',
                     'references': {'schema': 'public', 'table': 't2',
                                    'columns': ['pc1']}}}}
        assert dbmap['schema public']['table t1'] == t1map

    def test_map_fk_comment(self):
        "Map a foreign key with a comment"
        stmts = ["CREATE TABLE t2 (pc1 INTEGER PRIMARY KEY, pc2 TEXT)",
                 "CREATE TABLE t1 (c1 INTEGER, c2 INTEGER "
                 "CONSTRAINT cns1 REFERENCES t2 (pc1), c3 TEXT)", COMMENT_STMT]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1']['foreign_keys']['cns1'][
            'description'] == 'Test constraint cns1'


class ForeignKeyToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input FOREIGN KEYs"""

    def test_create_with_foreign_key(self):
        "Create a table with a foreign key constraint"
        inmap = self.std_map()
        inmap['schema public'].update(
            {'table t1': {'columns': [{'c11': {'type': 'integer'}},
                                      {'c12': {'type': 'text'}}]},
             'table t2': {'columns': [{'c21': {'type': 'integer'}},
                                      {'c22': {'type': 'text'}},
                                      {'c23': {'type': 'integer'}}],
                          'foreign_keys': {'t2_c23_fkey': {
                              'columns': ['c23'],
                              'references': {'columns': ['c11'],
                                             'table': 't1'}}}}})
        sql = self.to_sql(inmap)
        # can't control which table will be created first
        crt1 = 0
        crt2 = 1
        if 't1' in sql[1]:
            crt1 = 1
            crt2 = 0
        assert fix_indent(sql[crt1]) == \
            "CREATE TABLE t1 (c11 integer, c12 text)"
        assert fix_indent(sql[crt2]) == \
            "CREATE TABLE t2 (c21 integer, c22 text, c23 integer)"
        assert fix_indent(sql[2]) == "ALTER TABLE t2 ADD CONSTRAINT " \
            "t2_c23_fkey FOREIGN KEY (c23) REFERENCES t1 (c11)"

    def test_create_foreign_key_deferred(self):
        "Create a table with various foreign key deferring constraint"
        inmap = self.std_map()
        inmap['schema public'].update(
            {'table t1': {'columns': [{'c11': {'type': 'integer'}},
                                      {'c12': {'type': 'text'}}]},
             'table t2': {'columns': [{'c21': {'type': 'integer'}},
                                      {'c22': {'type': 'text'}},
                                      {'c23': {'type': 'integer'}},
                                      {'c24': {'type': 'integer'}},
                                      {'c25': {'type': 'integer'}}],
                          'foreign_keys': {'t2_c23_fkey': {
                              'columns': ['c23'],
                              'references': {'columns': ['c11'],
                                             'table': 't1'}},
                          't2_c24_fkey': {
                              'columns': ['c24'],
                              'references': {'columns': ['c11'],
                                             'table': 't1'},
                              'deferrable': True},
                          't2_c25_fkey': {
                              'columns': ['c25'],
                              'references': {'columns': ['c11'],
                                             'table': 't1'},
                              'deferrable': True, 'deferred': True}}}})
        sql = self.to_sql(inmap)
        # can't control which table/constraint will be created first
        sql[0:2] = list(sorted(sql[0:2]))
        sql[2:5] = list(sorted(sql[2:5]))

        assert fix_indent(sql[0]) == "CREATE TABLE t1 (c11 integer, c12 text)"
        assert fix_indent(sql[1]) == "CREATE TABLE t2 (c21 integer, " \
            "c22 text, c23 integer, c24 integer, c25 integer)"
        assert fix_indent(sql[2]) == "ALTER TABLE t2 ADD CONSTRAINT " \
            "t2_c23_fkey FOREIGN KEY (c23) REFERENCES t1 (c11)"
        assert fix_indent(sql[3]) == "ALTER TABLE t2 ADD CONSTRAINT " \
            "t2_c24_fkey FOREIGN KEY (c24) REFERENCES t1 (c11) DEFERRABLE"
        assert fix_indent(sql[4]) == "ALTER TABLE t2 ADD CONSTRAINT " \
            "t2_c25_fkey FOREIGN KEY (c25) REFERENCES t1 (c11) " \
            "DEFERRABLE INITIALLY DEFERRED"

    def test_add_foreign_key(self):
        "Add a two-column foreign key to an existing table"
        stmts = ["CREATE TABLE t1 (c11 INTEGER NOT NULL, "
                 "c12 INTEGER NOT NULL, c13 TEXT, PRIMARY KEY (c11, c12))",
                 "CREATE TABLE t2 (c21 INTEGER NOT NULL, "
                 "c22 TEXT, c23 INTEGER, c24 INTEGER, PRIMARY KEY (c21))"]
        inmap = self.std_map()
        inmap['schema public'].update({
            'table t1': {'columns': [
                        {'c11': {'type': 'integer', 'not_null': True}},
                        {'c12': {'type': 'integer', 'not_null': True}},
                        {'c13': {'type': 'text'}}],
                'primary_key': {'t1_pkey': {'columns': ['c11', 'c12']}}},
            'table t2': {'columns': [
                        {'c21': {'type': 'integer', 'not_null': True}},
                        {'c22': {'type': 'text'}},
                        {'c23': {'type': 'integer'}},
                        {'c24': {'type': 'integer'}}],
                'primary_key': {'t2_pkey': {'columns': ['c21']}},
                'foreign_keys': {'t2_c23_fkey': {
                    'columns': ['c23', 'c24'],
                    'references': {'columns': ['c11', 'c12'],
                                   'table': 't1'}}}}})
        sql = self.to_sql(inmap, stmts)
        assert fix_indent(sql[0]) == "ALTER TABLE t2 ADD CONSTRAINT " \
            "t2_c23_fkey FOREIGN KEY (c23, c24) REFERENCES t1 (c11, c12)"

    def test_drop_foreign_key(self):
        "Drop a foreign key on an existing table"
        stmts = ["CREATE TABLE t1 (c11 INTEGER NOT NULL, c12 TEXT, "
                 "PRIMARY KEY (c11))",
                 "CREATE TABLE t2 (c21 INTEGER NOT NULL PRIMARY KEY, "
                 "c22 INTEGER NOT NULL REFERENCES t1 (c11), c23 TEXT)"]
        inmap = self.std_map()
        inmap['schema public'].update({
            'table t1': {
                'columns': [{'c11': {'type': 'integer', 'not_null': True}},
                            {'c12': {'type': 'text'}}],
                'primary_key': {'t1_pkey': {'columns': ['c11']}}},
            'table t2': {
                'columns': [{'c21': {'type': 'integer', 'not_null': True}},
                            {'c22': {'type': 'integer', 'not_null': True}},
                            {'c23': {'type': 'text'}}],
                'primary_key': {'t2_pkey': {'columns': ['c21']}}}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["ALTER TABLE t2 DROP CONSTRAINT t2_c22_fkey"]

    def test_create_foreign_key_actions(self):
        "Create a table with foreign key ON UPDATE/ON DELETE actions"
        inmap = self.std_map()
        inmap['schema public'].update({
            'table t1': {'columns': [{'c11': {'type': 'integer'}},
                                     {'c12': {'type': 'text'}}]},
            'table t2': {'columns': [{'c21': {'type': 'integer'}},
                                     {'c22': {'type': 'text'}},
                                     {'c23': {'type': 'integer'}}],
                         'foreign_keys': {'t2_c23_fkey': {
                             'columns': ['c23'], 'on_update': 'cascade',
                             'on_delete': 'set default',
                             'references': {'columns': ['c11'],
                                            'table': 't1'}}}}})
        sql = self.to_sql(inmap)
        # won't check CREATE TABLEs explicitly here (see first test instead)
        assert fix_indent(sql[2]) == "ALTER TABLE t2 ADD CONSTRAINT " \
            "t2_c23_fkey FOREIGN KEY (c23) REFERENCES t1 (c11) " \
            "ON UPDATE CASCADE ON DELETE SET DEFAULT"

    def test_foreign_key_match(self):
        "Create a foreign key constraint with a MATCH specification"
        stmts = ["CREATE TABLE t2 (pc1 INTEGER PRIMARY KEY, pc2 TEXT)"]
        inmap = self.std_map()
        inmap['schema public'].update({
            'table t1': {'columns': [{'c1': {'type': 'integer'}},
                                     {'c2': {'type': 'integer'}},
                                     {'c3': {'type': 'text'}}],
                         'foreign_keys': {'t1_c2_fkey': {
                             'columns': ['c2'], 'match': 'full',
                             'references': {'schema': 'public', 'table': 't2',
                                            'columns': ['pc1']}}}},
            'table t2': {'columns': [{'pc1': {'type': 'integer',
                                              'not_null': True}},
                                     {'pc2': {'type': 'text'}}],
                         'primary_key': {'t2_pkey': {'columns': ['pc1']}}}})
        sql = self.to_sql(inmap, stmts)
        assert fix_indent(sql[1]) == "ALTER TABLE t1 ADD CONSTRAINT " \
            "t1_c2_fkey FOREIGN KEY (c2) REFERENCES t2 (pc1) MATCH FULL"


class UniqueConstraintToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of created UNIQUE constraints"""

    map_unique1 = {'columns': [{'c1': {'type': 'integer'}},
                               {'c2': {'type': 'text'}}],
                   'unique_constraints': {'t1_c1_key': {'columns': ['c1']}}}

    map_unique2 = {'columns': [{'c1': {'type': 'integer'}},
                               {'c2': {'type': 'character(5)'}},
                               {'c3': {'type': 'text'}}],
                   'unique_constraints': {
                       't1_c1_c2_key': {'columns': ['c1', 'c2']}}}

    map_unique3 = {'columns': [{'c1': {'type': 'integer'}},
                               {'c2': {'type': 'text'}}],
                   'unique_constraints': {
                       't1_unique_key': {'columns': ['c1']}}}

    def test_unique_1(self):
        "Map a table with a single-column unique constraint"
        dbmap = self.to_map(["CREATE TABLE t1 (c1 INTEGER UNIQUE, c2 TEXT)"])
        assert dbmap['schema public']['table t1'] == self.map_unique1

    def test_unique_2(self):
        "Map a table with a single-column unique constraint, table level"
        stmts = ["CREATE TABLE t1 (c1 INTEGER, c2 TEXT, UNIQUE (c1))"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1'] == self.map_unique1

    def test_unique_3(self):
        "Map a table with a two-column unique constraint"
        stmts = ["CREATE TABLE t1 (c1 INTEGER, c2 CHAR(5), c3 TEXT, "
                 "UNIQUE (c1, c2))"]
        dbmap = self.to_map(stmts)
        if self.db.version < 90000:
            self.map_unique2.update({'unique_constraints': {
                't1_c1_key': {'columns': ['c1', 'c2']}}})
        assert dbmap['schema public']['table t1'] == self.map_unique2

    def test_unique_4(self):
        "Map a table with a named unique constraint"
        stmts = ["CREATE TABLE t1 (c1 INTEGER, c2 TEXT, "
                 "CONSTRAINT t1_unique_key UNIQUE (c1))"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1'] == self.map_unique3

    def test_unique_5(self):
        "Map a table with a named unique constraint, column level"
        stmts = ["CREATE TABLE t1 ( "
                 "c1 INTEGER CONSTRAINT t1_unique_key UNIQUE, c2 TEXT)"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1'] == self.map_unique3

    def test_map_unique_cluster(self):
        "Map a table with a unique constraint and CLUSTER on it"
        stmts = ["CREATE TABLE t1 (c1 integer, c2 text UNIQUE)",
                 "CLUSTER t1 USING t1_c2_key"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1'] == {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'unique_constraints': {'t1_c2_key': {
                'columns': ['c2'], 'cluster': True}}}


class UniqueConstraintToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input UNIQUE constraints"""

    def test_create_w_unique_constraint(self):
        "Create new table with a single column unique constraint"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'unique_constraints': {'t1_c1_key': {'columns': ['c1']}}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == "CREATE TABLE t1 (c1 integer, c2 text)"
        assert fix_indent(sql[1]) == \
            "ALTER TABLE t1 ADD CONSTRAINT t1_c1_key UNIQUE (c1)"

    def test_add_unique_constraint(self):
        "Add a two-column unique constraint to an existing table"
        stmts = ["CREATE TABLE t1 (c1 INTEGER NOT NULL, "
                 "c2 INTEGER NOT NULL, c3 TEXT)"]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer', 'not_null': True}},
                        {'c2': {'type': 'integer', 'not_null': True}},
                        {'c3': {'type': 'text'}}],
            'unique_constraints': {'t1_c2_key': {'columns': ['c2', 'c1'],
                                                 'unique': True}}}})
        sql = self.to_sql(inmap, stmts)
        assert fix_indent(sql[0]) == \
            "ALTER TABLE t1 ADD CONSTRAINT t1_c2_key UNIQUE (c2, c1)"

    def test_drop_unique_constraint(self):
        "Drop a unique constraint on an existing table"
        stmts = ["CREATE TABLE t1 (c1 INTEGER NOT NULL UNIQUE, c2 TEXT)"]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer', 'not_null': True},
                         'c2': {'type': 'text'}}]}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["ALTER TABLE t1 DROP CONSTRAINT t1_c1_key"]

    def test_create_unique_clustered(self):
        "Create new table clustered on the unique constraint index"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'unique_constraints': {'t1_c1_key': {'columns': ['c1'],
                                                 'cluster': True}}}})
        sql = self.to_sql(inmap)
        assert sql[2] == "CLUSTER t1 USING t1_c1_key"

    def test_unique_cluster(self):
        "Cluster a table on the unique constraint index"
        stmts = ["CREATE TABLE t1 (c1 integer UNIQUE, c2 text)"]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'unique_constraints': {'t1_c1_key': {'columns': ['c1'],
                                                 'cluster': True}}}})
        sql = self.to_sql(inmap, stmts)
        assert sql[0] == "CLUSTER t1 USING t1_c1_key"


class ConstraintCommentTestCase(InputMapToSqlTestCase):
    """Test creation of comments on constraints"""

    def test_check_constraint_with_comment(self):
        "Create a CHECK constraint with a comment"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'check_constraints': {'cns1': {
                'columns': ['c1'], 'expression': 'c1 > 50',
                'description': 'Test constraint cns1'}}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == "CREATE TABLE t1 (c1 integer, c2 text)"
        assert fix_indent(sql[1]) == \
            "ALTER TABLE t1 ADD CONSTRAINT cns1 CHECK (c1 > 50)"
        assert sql[2] == COMMENT_STMT

    def test_comment_on_primary_key(self):
        "Create a comment for an existing primary key"
        stmts = ["CREATE TABLE t1 (c1 text CONSTRAINT cns1 PRIMARY KEY, "
                 "c2 integer)"]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'text', 'not_null': True}},
                        {'c2': {'type': 'integer'}}],
            'primary_key': {'cns1': {'columns': ['c2'],
                                     'description': 'Test constraint cns1'}}}})
        sql = self.to_sql(inmap, stmts)
        assert sql == [COMMENT_STMT]

    def test_drop_foreign_key_comment(self):
        "Drop the comment on an existing foreign key"
        stmts = ["CREATE TABLE t2 (c21 integer PRIMARY KEY, c22 text)",
                 "CREATE TABLE t1 (c11 integer, c12 text, "
                 "c13 integer CONSTRAINT cns1 REFERENCES t2 (c21))",
                 COMMENT_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({
            'table t2': {
                'columns': [{'c21': {'type': 'integer', 'not_null': True}},
                            {'c22': {'type': 'text'}}],
                'primary_key': {'t2_pkey': {'columns': ['c21']}}},
            'table t1': {
                'columns': [{'c11': {'type': 'integer'}},
                            {'c12': {'type': 'text'}},
                            {'c13': {'type': 'integer'}}],
                'foreign_keys': {'cns1': {
                    'columns': ['c13'],
                    'references': {'columns': ['c21'], 'table': 't2'}}}}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["COMMENT ON CONSTRAINT cns1 ON t1 IS NULL"]

    def test_change_unique_constraint_comment(self):
        "Change existing comment on a unique constraint"
        stmts = ["CREATE TABLE t1 (c1 integer CONSTRAINT cns1 UNIQUE, "
                 "c2 text)", COMMENT_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'unique_constraints': {'cns1': {
                'columns': ['c1'],
                'description': "Changed constraint cns1"}}}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["COMMENT ON CONSTRAINT cns1 ON t1 IS "
                       "'Changed constraint cns1'"]

    def test_constraint_comment_schema(self):
        "Add comment on a constraint for a table in another schema"
        stmts = ["CREATE SCHEMA s1", "CREATE TABLE s1.t1 (c1 integer "
                 "CONSTRAINT cns1 CHECK (c1 > 50), c2 text)"]
        inmap = self.std_map()
        inmap.update({'schema s1': {'table t1': {
            'columns': [{'c1': {'type': 'integer'}},
                        {'c2': {'type': 'text'}}],
            'check_constraints': {'cns1': {
                'columns': ['c1'], 'expression': 'c1 > 50',
                'description': 'Test constraint cns1'}}}}})
        sql = self.to_sql(inmap, stmts)
        assert sql[0] == "COMMENT ON CONSTRAINT cns1 ON s1.t1 IS " \
            "'Test constraint cns1'"

########NEW FILE########
__FILENAME__ = test_conversion
# -*- coding: utf-8 -*-
"""Test conversions"""

import pytest

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

CREATE_STMT = "CREATE CONVERSION c1 FOR 'LATIN1' TO 'UTF8' " \
    "FROM iso8859_1_to_utf8"
DROP_STMT = "DROP CONVERSION IF EXISTS c1"
COMMENT_STMT = "COMMENT ON CONVERSION c1 IS 'Test conversion c1'"


class ConversionToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing conversions"""

    def test_map_conversion(self):
        "Map a conversion"
        dbmap = self.to_map([CREATE_STMT])
        expmap = {'source_encoding': 'LATIN1', 'dest_encoding': 'UTF8',
                  'function': 'iso8859_1_to_utf8'}
        assert dbmap['schema public']['conversion c1'] == expmap

    def test_map_conversion_comment(self):
        "Map a conversion comment"
        dbmap = self.to_map([CREATE_STMT, COMMENT_STMT])
        assert dbmap['schema public']['conversion c1']['description'] == \
            'Test conversion c1'


class ConversionToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input conversions"""

    def test_create_conversion(self):
        "Create a conversion"
        inmap = self.std_map()
        inmap['schema public'].update({'conversion c1': {
            'source_encoding': 'LATIN1', 'dest_encoding': 'UTF8',
            'function': 'iso8859_1_to_utf8'}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_STMT

    def test_create_conversion_schema(self):
        "Create a conversion in a non-public schema"
        inmap = self.std_map()
        inmap.update({'schema s1': {'conversion c1': {
            'source_encoding': 'LATIN1', 'dest_encoding': 'UTF8',
            'function': 'iso8859_1_to_utf8', 'default': True}}})
        sql = self.to_sql(inmap, ["CREATE SCHEMA s1"])
        assert fix_indent(sql[0]) == "CREATE DEFAULT CONVERSION s1.c1 " \
            "FOR 'LATIN1' TO 'UTF8' FROM iso8859_1_to_utf8"

    def test_bad_conversion_map(self):
        "Error creating a conversion with a bad map"
        inmap = self.std_map()
        inmap['schema public'].update({'c1': {
            'source_encoding': 'LATIN1', 'dest_encoding': 'UTF8',
            'function': 'iso8859_1_to_utf8'}})
        with pytest.raises(KeyError):
            self.to_sql(inmap)

    def test_drop_conversion(self):
        "Drop an existing conversion"
        sql = self.to_sql(self.std_map(), [CREATE_STMT])
        assert sql[0] == "DROP CONVERSION c1"

    def test_conversion_with_comment(self):
        "Create a conversion with a comment"
        inmap = self.std_map()
        inmap['schema public'].update({'conversion c1': {
            'description': 'Test conversion c1',
            'source_encoding': 'LATIN1', 'dest_encoding': 'UTF8',
            'function': 'iso8859_1_to_utf8'}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_STMT
        assert sql[1] == COMMENT_STMT

    def test_comment_on_conversion(self):
        "Create a comment for an existing conversion"
        inmap = self.std_map()
        inmap['schema public'].update({'conversion c1': {
            'description': 'Test conversion c1',
            'source_encoding': 'LATIN1', 'dest_encoding': 'UTF8',
            'function': 'iso8859_1_to_utf8'}})
        sql = self.to_sql(inmap, [CREATE_STMT])
        assert sql == [COMMENT_STMT]

    def test_drop_conversion_comment(self):
        "Drop a comment on an existing conversion"
        stmts = [CREATE_STMT, COMMENT_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'conversion c1': {
            'source_encoding': 'LATIN1', 'dest_encoding': 'UTF8',
            'function': 'iso8859_1_to_utf8'}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["COMMENT ON CONVERSION c1 IS NULL"]

    def test_change_conversion_comment(self):
        "Change existing comment on a conversion"
        stmts = [CREATE_STMT, COMMENT_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'conversion c1': {
            'description': 'Changed conversion c1',
            'source_encoding': 'LATIN1', 'dest_encoding': 'UTF8',
            'function': 'iso8859_1_to_utf8'}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["COMMENT ON CONVERSION c1 IS 'Changed conversion c1'"]

########NEW FILE########
__FILENAME__ = test_domain
# -*- coding: utf-8 -*-
"""Test domains"""

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

CREATE_STMT = "CREATE DOMAIN d1 AS integer"
DROP_STMT = "DROP DOMAIN IF EXISTS d1"
COMMENT_STMT = "COMMENT ON DOMAIN d1 IS 'Test domain d1'"


class DomainToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of created domains"""

    def test_domain(self):
        "Map a simple domain"
        dbmap = self.to_map([CREATE_STMT])
        assert dbmap['schema public']['domain d1'] == {'type': 'integer'}

    def test_domain_not_null(self):
        "Map a domain with a NOT NULL constraint"
        dbmap = self.to_map([CREATE_STMT + " NOT NULL"])
        expmap = {'type': 'integer', 'not_null': True}
        assert dbmap['schema public']['domain d1'] == expmap

    def test_domain_default(self):
        "Map a domain with a DEFAULT"
        dbmap = self.to_map(["CREATE DOMAIN d1 AS date DEFAULT CURRENT_DATE"])
        expmap = {'type': 'date', 'default': "('now'::text)::date"}
        assert dbmap['schema public']['domain d1'] == expmap

    def test_domain_check(self):
        "Map a domain with a CHECK constraint"
        dbmap = self.to_map([CREATE_STMT + " CHECK (VALUE >= 1888)"])
        expmap = {'type': 'integer', 'check_constraints': {
            'd1_check': {'expression': '(VALUE >= 1888)'}}}
        assert dbmap['schema public']['domain d1'] == expmap


class DomainToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input domains"""

    def test_create_domain(self):
        "Create a simple domain"
        inmap = self.std_map()
        inmap['schema public'].update({'domain d1': {'type': 'integer'}})
        sql = self.to_sql(inmap)
        assert sql == [CREATE_STMT]

    def test_create_domain_default(self):
        "Create a domain with a DEFAULT and NOT NULL"
        inmap = self.std_map()
        inmap['schema public'].update({'domain d1': {
            'type': 'integer', 'not_null': True, 'default': 0}})
        sql = self.to_sql(inmap)
        assert sql == [CREATE_STMT + " NOT NULL DEFAULT 0"]

    def test_create_domain_check(self):
        "Create a domain with a CHECK constraint"
        inmap = self.std_map()
        inmap['schema public'].update({'domain d1': {
            'type': 'integer', 'check_constraints': {'d1_check': {
            'expression': '(VALUE >= 1888)'}}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_STMT + \
            " CONSTRAINT d1_check CHECK (VALUE >= 1888)"

    def test_drop_domain(self):
        "Drop an existing domain"
        sql = self.to_sql(self.std_map(), [CREATE_STMT])
        assert sql == ["DROP DOMAIN d1"]

    def test_rename_domain(self):
        "Rename an existing domain"
        inmap = self.std_map()
        inmap['schema public'].update({'domain d2': {
            'oldname': 'd1', 'type': 'integer'}})
        sql = self.to_sql(inmap, [CREATE_STMT])
        assert sql == ["ALTER DOMAIN d1 RENAME TO d2"]

########NEW FILE########
__FILENAME__ = test_eventtrig
# -*- coding: utf-8 -*-
"""Test event triggers"""

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

FUNC_SRC = "BEGIN RAISE NOTICE 'Command % executed', tg_tag; END"
CREATE_FUNC_STMT = "CREATE FUNCTION f1() RETURNS event_trigger " \
    "LANGUAGE plpgsql AS $_$%s$_$" % FUNC_SRC
CREATE_STMT = "CREATE EVENT TRIGGER et1 ON ddl_command_end %s" \
    "EXECUTE PROCEDURE f1()"
DROP_TABLE_STMT = "DROP TABLE IF EXISTS t1"
DROP_FUNC_STMT = "DROP FUNCTION IF EXISTS f1()"
COMMENT_STMT = "COMMENT ON EVENT TRIGGER et1 IS 'Test event trigger et1'"


class EventTriggerToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing event triggers"""

    def setUp(self):
        super(self.__class__, self).setUp()
        if self.db.version < 90000:
            if not self.db.is_plpgsql_installed():
                self.db.execute_commit("CREATE LANGUAGE plpgsql")

    def test_map_event_trigger_simple(self):
        "Map a simple event trigger"
        if self.db.version < 90300:
            self.skipTest('Only available on PG 9.3')
        stmts = [CREATE_FUNC_STMT, CREATE_STMT % '']
        dbmap = self.to_map(stmts)
        assert dbmap['event trigger et1'] == {
            'enabled': True, 'event': 'ddl_command_end', 'procedure': 'f1()'}

    def test_map_event_trigger_filter(self):
        "Map a trigger with tag filter variables"
        if self.db.version < 90300:
            self.skipTest('Only available on PG 9.3')
        stmts = [CREATE_FUNC_STMT, CREATE_STMT % (
            "WHEN tag IN ('CREATE TABLE', 'CREATE VIEW') ")]
        dbmap = self.to_map(stmts)
        assert dbmap['event trigger et1'] == {
            'enabled': True, 'event': 'ddl_command_end',
            'tags': ['CREATE TABLE', 'CREATE VIEW'], 'procedure': 'f1()'}

    def test_map_event_trigger_comment(self):
        "Map a trigger comment"
        if self.db.version < 90300:
            self.skipTest('Only available on PG 9.3')
        stmts = [CREATE_FUNC_STMT, CREATE_STMT % '', COMMENT_STMT]
        dbmap = self.to_map(stmts)
        assert dbmap['event trigger et1']['description'] == \
            'Test event trigger et1'


class EventTriggerToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input triggers"""

    def setUp(self):
        super(self.__class__, self).setUp()
        if self.db.version < 90000:
            if not self.db.is_plpgsql_installed():
                self.db.execute_commit("CREATE LANGUAGE plpgsql")

    def test_create_event_trigger_simple(self):
        "Create a simple event trigger"
        if self.db.version < 90300:
            self.skipTest('Only available on PG 9.3')
        inmap = self.std_map(plpgsql_installed=True)
        inmap['schema public'].update({'function f1()': {
            'language': 'plpgsql', 'returns': 'event_trigger',
            'source': FUNC_SRC}})
        inmap.update({'event trigger et1': {
            'enabled': True, 'event': 'ddl_command_end', 'procedure': 'f1()'}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[1]) == CREATE_FUNC_STMT
        assert fix_indent(sql[2]) == CREATE_STMT % ''

    def test_create_event_trigger_filter(self):
        "Create an event trigger with tag filter variables"
        if self.db.version < 90300:
            self.skipTest('Only available on PG 9.3')
        inmap = self.std_map(plpgsql_installed=True)
        inmap['schema public'].update({'function f1()': {
            'language': 'plpgsql', 'returns': 'event_trigger',
            'source': FUNC_SRC}})
        inmap.update({'event trigger et1': {
            'enabled': True, 'event': 'ddl_command_end', 'procedure': 'f1()',
            'tags': ['CREATE TABLE', 'CREATE VIEW']}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[1]) == CREATE_FUNC_STMT
        assert fix_indent(sql[2]) == CREATE_STMT % (
            "WHEN tag IN ('CREATE TABLE', 'CREATE VIEW') ")

    def test_create_event_trigger_func_schema(self):
        "Create an event trigger with function in a non-public schema"
        if self.db.version < 90300:
            self.skipTest('Only available on PG 9.3')
        inmap = self.std_map(plpgsql_installed=True)
        inmap.update({'schema s1': {'function f1()': {
            'language': 'plpgsql', 'returns': 'event_trigger',
            'source': FUNC_SRC}}})
        inmap.update({'event trigger et1': {
            'enabled': True, 'event': 'ddl_command_end',
            'procedure': 's1.f1()'}})
        sql = self.to_sql(inmap, ["CREATE SCHEMA s1"])
        assert fix_indent(sql[1]) == "CREATE FUNCTION s1.f1() " \
            "RETURNS event_trigger LANGUAGE plpgsql AS $_$%s$_$" % FUNC_SRC
        assert fix_indent(sql[2]) == "CREATE EVENT TRIGGER et1 " \
            "ON ddl_command_end EXECUTE PROCEDURE s1.f1()"

    def test_drop_event_trigger(self):
        "Drop an existing event trigger"
        if self.db.version < 90300:
            self.skipTest('Only available on PG 9.3')
        stmts = [CREATE_FUNC_STMT, CREATE_STMT % '']
        inmap = self.std_map(plpgsql_installed=True)
        inmap['schema public'].update({'function f1()': {
            'language': 'plpgsql', 'returns': 'event_trigger',
            'source': FUNC_SRC}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["DROP EVENT TRIGGER et1"]

    def test_drop_event_trigger_function(self):
        "Drop an existing event trigger and the related function"
        if self.db.version < 90300:
            self.skipTest('Only available on PG 9.3')
        stmts = [CREATE_FUNC_STMT, CREATE_STMT % '']
        inmap = self.std_map(plpgsql_installed=True)
        sql = self.to_sql(inmap, stmts)
        assert sql[0] == "DROP EVENT TRIGGER et1"
        assert sql[1] == "DROP FUNCTION f1()"

    def test_create_event_trigger_with_comment(self):
        "Create an event trigger with a comment"
        if self.db.version < 90300:
            self.skipTest('Only available on PG 9.3')
        inmap = self.std_map(plpgsql_installed=True)
        inmap['schema public'].update({'function f1()': {
            'language': 'plpgsql', 'returns': 'event_trigger',
            'source': FUNC_SRC}})
        inmap.update({'event trigger et1': {
            'enabled': True, 'event': 'ddl_command_end', 'procedure': 'f1()',
            'description': 'Test event trigger et1'}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[1]) == CREATE_FUNC_STMT
        assert fix_indent(sql[2]) == CREATE_STMT % ''
        assert sql[3] == COMMENT_STMT

########NEW FILE########
__FILENAME__ = test_extension
# -*- coding: utf-8 -*-
"""Test extensions"""

import pytest

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

CREATE_STMT = "CREATE EXTENSION pg_trgm"
TRGM_COMMENT = "text similarity measurement and index searching based on " \
    "trigrams"


class ExtensionToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing extensions"""

    superuser = True

    def test_map_extension(self):
        "Map an existing extension"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        VERS = '1.0' if self.db.version < 90300 else '1.1'
        dbmap = self.to_map([CREATE_STMT])
        assert dbmap['extension pg_trgm'] == {
            'schema': 'public', 'version': VERS, 'description': TRGM_COMMENT}

    def test_map_no_depends(self):
        "Ensure no dependencies are included when mapping an extension"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        dbmap = self.to_map([CREATE_STMT])
        assert 'type gtrgm' not in dbmap['schema public']
        assert not 'operator %(text, text)' in dbmap['schema public']
        assert not 'function show_trgm(text)' in dbmap['schema public']

    def test_map_lang_extension(self):
        "Map a procedural language as an extension"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        dbmap = self.to_map(["CREATE EXTENSION plperl"])
        assert dbmap['extension plperl'] == {
            'schema': 'pg_catalog', 'version': '1.0',
            'description': "PL/Perl procedural language"}
        assert not 'language plperl' in dbmap

    def test_map_extension_schema(self):
        "Map an existing extension"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        VERS = '1.0' if self.db.version < 90300 else '1.1'
        dbmap = self.to_map(["CREATE SCHEMA s1", CREATE_STMT + " SCHEMA s1"])
        assert dbmap['extension pg_trgm'] == {
            'schema': 's1', 'version': VERS, 'description': TRGM_COMMENT}


class ExtensionToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation for input extensions"""

    def test_create_extension(self):
        "Create a extension that didn't exist"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        inmap = self.std_map()
        inmap.update({'extension pg_trgm': {'schema': 'public'}})
        sql = self.to_sql(inmap)
        assert sql == [CREATE_STMT]

    def test_bad_extension_map(self):
        "Error creating a extension with a bad map"
        inmap = self.std_map()
        inmap.update({'pg_trgm': {'schema': 'public'}})
        with pytest.raises(KeyError):
            self.to_sql(inmap)

    def test_drop_extension(self):
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        "Drop an existing extension"
        sql = self.to_sql(self.std_map(), [CREATE_STMT], superuser=True)
        assert sql == ["DROP EXTENSION pg_trgm"]

    def test_create_extension_schema(self):
        "Create a extension in a given schema"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        inmap = self.std_map()
        inmap.update({'schema s1': {},
                      'extension pg_trgm': {'schema': 's1', 'version': '1.0'}})
        sql = self.to_sql(inmap)
        assert sql[0] == 'CREATE SCHEMA s1'
        assert fix_indent(sql[1]) == \
            "CREATE EXTENSION pg_trgm SCHEMA s1 VERSION '1.0'"

    def test_create_lang_extension(self):
        "Create a language extension and a function in that language"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        inmap = self.std_map()
        inmap.update({'extension plperl': {
            'description': "PL/Perl procedural language"}})
        inmap['schema public'].update({'function f1()': {
            'language': 'plperl', 'returns': 'text',
            'source': "return \"dummy\";"}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == "CREATE EXTENSION plperl"
        # skip over COMMENT and SET statements
        assert fix_indent(sql[3]) == "CREATE FUNCTION f1() RETURNS text " \
            "LANGUAGE plperl AS $_$return \"dummy\";$_$"

    def test_comment_extension(self):
        "Change the comment for an existing extension"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        inmap = self.std_map()
        inmap.update({'extension pg_trgm': {
            'schema': 'public', 'description': "Trigram extension"}})
        sql = self.to_sql(inmap, [CREATE_STMT], superuser=True)
        assert sql == ["COMMENT ON EXTENSION pg_trgm IS 'Trigram extension'"]

    def test_no_alter_owner_extension(self):
        """Do not alter the owner of an existing extension.

        ALTER EXTENSION extension_name OWNER is not a valid form.
        """
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        # create a new owner that is different from self.db.user
        new_owner = 'new_%s' % self.db.user
        inmap = self.std_map()
        inmap.update({'extension pg_trgm': {'schema': 'public', 'owner': new_owner}})
        sql = self.to_sql(inmap, [CREATE_STMT], superuser=True)
        assert 'ALTER EXTENSION pg_trgm OWNER TO %s' % new_owner not in sql

########NEW FILE########
__FILENAME__ = test_extern_file
# -*- coding: utf-8 -*-
"""Test external files used in --multiple-files option"""
import sys

import pytest

from pyrseas.testutils import PyrseasTestCase
from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.dbobject.schema import Schema
from pyrseas.dbobject.function import Function
from pyrseas.dbobject.table import Sequence, Table, View

if sys.platform == 'win32':
    COLL = 'French_France.1252'
else:
    COLL = 'fr_FR.UTF-8'

CREATE_FDW = "CREATE FOREIGN DATA WRAPPER "
SOURCE1 = "SELECT 'dummy'::text"
SOURCE2 = "SELECT $1::text"
DROP_LANG = "DROP LANGUAGE IF EXISTS plperl CASCADE"
DROP_TSC = "DROP TEXT SEARCH CONFIGURATION IF EXISTS tsc1, tsc2"
DROP_TSP = "DROP TEXT SEARCH PARSER IF EXISTS tsp1 CASCADE"


class ExternalFilenameMapTestCase(DatabaseToMapTestCase):

    def setUp(self):
        super(ExternalFilenameMapTestCase, self).setUp()
        self.remove_tempfiles()

    def test_map_casts(self):
        "Map casts"
        self.to_map(["CREATE FUNCTION int2_bool(smallint) RETURNS boolean "
                     "LANGUAGE sql IMMUTABLE AS "
                     "$_$SELECT CAST($1::int AS boolean)$_$",
                     "CREATE DOMAIN d1 AS integer",
                     "CREATE CAST (smallint AS boolean) WITH FUNCTION "
                     "int2_bool(smallint)",
                     "CREATE CAST (d1 AS integer) WITH INOUT AS IMPLICIT"],
                    superuser=True, multiple_files=True)
        expmap = {'cast (smallint as boolean)': {
            'function': 'int2_bool(smallint)', 'context': 'explicit',
            'method': 'function'}, 'cast (d1 as integer)':
            {'context': 'implicit', 'method': 'inout'}}
        assert self.yaml_load('cast.yaml') == expmap

    def test_map_extension(self):
        "Map extensions"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        TRGM_VERS = '1.0' if self.db.version < 90300 else '1.1'
        self.to_map(["CREATE EXTENSION pg_trgm"], superuser=True,
                    multiple_files=True)
        expmap = {'extension plpgsql': {
            'schema': 'pg_catalog', 'version': '1.0',
            'description': 'PL/pgSQL procedural language'},
            'extension pg_trgm': {'schema': 'public', 'version': TRGM_VERS,
            'description': "text similarity measurement and index searching "
            "based on trigrams"}}
        assert self.yaml_load('extension.yaml') == expmap

    def test_map_fd_wrappers(self):
        "Map foreign data wrappers"
        self.to_map([CREATE_FDW + "fdw1", CREATE_FDW + "fdw2 "
                     "OPTIONS (debug 'true')"], superuser=True,
                    multiple_files=True)
        expmap = {'foreign data wrapper fdw1': {},
                  'foreign data wrapper fdw2': {'options': ['debug=true']}}
        assert self.yaml_load('foreign_data_wrapper.yaml') == expmap

    def test_map_language(self):
        "Map languages"
        if self.db.version >= 90100:
            self.skipTest('Only available before PG 9.1')
        self.to_map([DROP_LANG, "CREATE LANGUAGE plperl"], multiple_files=True)
        assert self.yaml_load('language.yaml')['language plperl'] == {
            'trusted': True}
        self.db.execute_commit(DROP_LANG)

    def test_collations(self):
        "Map collations"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        self.to_map(["CREATE COLLATION coll1 (LC_COLLATE = '%s', "
                     "LC_CTYPE = '%s')" % (COLL, COLL),
                     "COMMENT ON COLLATION coll1 IS 'A test collation'",
                     "CREATE COLLATION coll2 (LC_COLLATE = '%s', "
                     "LC_CTYPE = '%s')" % (COLL, COLL)],
                    multiple_files=True)
        assert self.yaml_load('collation.yaml', 'schema.public') == {
            'collation coll1': {'lc_collate': COLL, 'lc_ctype': COLL,
                                'description': "A test collation"},
            'collation coll2': {'lc_collate': COLL, 'lc_ctype': COLL}}

    def test_map_conversion(self):
        "Map conversions"
        self.to_map(["CREATE CONVERSION conv1 FOR 'LATIN1' TO 'UTF8' "
                     "FROM iso8859_1_to_utf8",
                     "COMMENT ON CONVERSION conv1 IS 'A test conversion'"],
                    multiple_files=True)
        assert self.yaml_load('conversion.yaml', 'schema.public') == {
            'conversion conv1': {'source_encoding': 'LATIN1',
            'dest_encoding': 'UTF8', 'function': 'iso8859_1_to_utf8',
            'description': 'A test conversion'}}

    def test_map_functions(self):
        "Map functions"
        self.to_map(["CREATE FUNCTION f1() RETURNS text LANGUAGE sql "
                     "IMMUTABLE AS $_$%s$_$" % SOURCE1,
                     "CREATE FUNCTION f2() RETURNS text LANGUAGE sql "
                     "IMMUTABLE AS $_$%s$_$" % SOURCE1], multiple_files=True)
        expmap = {'language': 'sql', 'returns': 'text',
                  'source': SOURCE1, 'volatility': 'immutable'}
        assert self.yaml_load('function.f1.yaml', 'schema.public') == {
            'function f1()': expmap}
        assert self.yaml_load('function.f2.yaml', 'schema.public') == {
            'function f2()': expmap}

    def test_map_functions_merged(self):
        "Map functions into a merged file"
        self.to_map(["CREATE FUNCTION f3(integer) RETURNS text LANGUAGE sql "
                     "IMMUTABLE AS $_$%s$_$" % SOURCE2,
                     "CREATE FUNCTION f3(real) RETURNS text LANGUAGE sql "
                     "IMMUTABLE AS $_$%s$_$" % SOURCE2], multiple_files=True)
        expmap = {'language': 'sql', 'returns': 'text',
                  'source': SOURCE2, 'volatility': 'immutable'}
        assert self.yaml_load('function.f3.yaml', 'schema.public') == {
            'function f3(integer)': expmap, 'function f3(real)': expmap}

    def test_map_operator(self):
        "Map operators and an operator class"
        self.to_map(["CREATE OPERATOR < (PROCEDURE = int4lt, LEFTARG = int, "
                     "RIGHTARG = int)", "CREATE OPERATOR = (PROCEDURE = "
                     "int4eq, LEFTARG = int, RIGHTARG = int)", "CREATE "
                     "OPERATOR > (PROCEDURE = int4gt, LEFTARG = int, "
                     "RIGHTARG = int)",
                     "CREATE OPERATOR CLASS oc1 FOR TYPE integer USING btree "
                     "AS OPERATOR 1 public.<, OPERATOR 3 public.=, OPERATOR "
                     "5 public.>, FUNCTION 1 btint4cmp(integer,integer)"],
                    superuser=True, multiple_files=True)
        oprmap = {'operator <(integer, integer)': {'procedure': 'int4lt'},
                  'operator =(integer, integer)': {'procedure': 'int4eq'},
                  'operator >(integer, integer)': {'procedure': 'int4gt'}}
        opcmap = {'operator class oc1 using btree': {
            'type': 'integer', 'operators': {1: '<(integer,integer)',
            3: '=(integer,integer)', 5: '>(integer,integer)'},
            'functions': {1: 'btint4cmp(integer,integer)'}}}
        assert self.yaml_load('operator.yaml', 'schema.public') == oprmap
        assert self.yaml_load('operator_class.yaml', 'schema.public') == opcmap

    def test_map_operator_family(self):
        "Map operator families"
        self.to_map(["CREATE SCHEMA s1",
                     "CREATE OPERATOR FAMILY s1.of1 USING btree",
                     "CREATE OPERATOR FAMILY s1.of2 USING btree"],
                    superuser=True, multiple_files=True)
        assert self.yaml_load('operator_family.yaml', 'schema.s1') == {
            'operator family of1 using btree': {},
            'operator family of2 using btree': {}}

    def test_map_tables(self):
        "Map tables"
        self.to_map(["CREATE TABLE t1 (c1 integer PRIMARY KEY, c2 text)",
                     "CREATE TABLE t2 (c1 integer, c2 integer, c3 text, "
                     "FOREIGN KEY (c2) REFERENCES t1 (c1))"],
                    multiple_files=True)
        expmap1 = {'table t1': {'columns': [
            {'c1': {'type': 'integer', 'not_null': True}},
            {'c2': {'type': 'text'}}],
            'primary_key': {'t1_pkey': {'columns': ['c1']}}}}
        expmap2 = {'table t2': {'columns': [
            {'c1': {'type': 'integer'}}, {'c2': {'type': 'integer'}},
            {'c3': {'type': 'text'}}],
            'foreign_keys': {'t2_c2_fkey': {'columns': ['c2'], 'references': {
            'schema': 'public', 'table': 't1', 'columns': ['c1']}}}}}
        assert self.yaml_load('table.t1.yaml', 'schema.public') == expmap1
        assert self.yaml_load('table.t2.yaml', 'schema.public') == expmap2

    def test_map_tables_merged(self):
        "Map tables into a merged file"
        self.to_map(["CREATE TABLE account_transfers_with_extra_padding_1 "
                     "(c1 integer, c2 text)",
                     "CREATE TABLE account_transfers_with_extra_padding_2 "
                     "(c1 integer, c2 text)"],
                    multiple_files=True)
        expmap = {'table account_transfers_with_extra_padding_1': {'columns': [
            {'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}]},
            'table account_transfers_with_extra_padding_2': {'columns': [
            {'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}]}}
        assert self.yaml_load('table.account_transfers_with_extra_pad.yaml',
                              'schema.public') == expmap

    def test_map_textsearch(self):
        "Map text search components"
        self.to_map([DROP_TSC, DROP_TSP,
                     "CREATE TEXT SEARCH PARSER tsp1 (START = prsd_start, "
                     "GETTOKEN = prsd_nexttoken, END = prsd_end, "
                     "LEXTYPES = prsd_lextype, HEADLINE = prsd_headline)",
                     "CREATE TEXT SEARCH CONFIGURATION tsc1 (PARSER = tsp1)",
                     "CREATE TEXT SEARCH CONFIGURATION tsc2 (PARSER = tsp1)"],
                    superuser=True, multiple_files=True)
        assert self.yaml_load('text_search_configuration.yaml',
                              'schema.public') == \
            {'text search configuration tsc1': {'parser': 'tsp1'},
             'text search configuration tsc2': {'parser': 'tsp1'}}
        assert self.yaml_load('text_search_parser.yaml', 'schema.public') == {
            'text search parser tsp1': {'start': 'prsd_start',
            'gettoken': 'prsd_nexttoken', 'end': 'prsd_end',
            'lextypes': 'prsd_lextype', 'headline': 'prsd_headline'}}
        self.db.execute(DROP_TSC)
        self.db.execute_commit(DROP_TSP)

    def test_map_drop_table(self):
        "Map three tables, drop one and map the remaining two"
        self.to_map(["CREATE TABLE t1 (c1 integer, c2 text)",
                     "CREATE TABLE t2 (c1 integer, c2 text)",
                     "CREATE TABLE t3 (c1 integer, c2 text)"],
                    multiple_files=True)
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'text'}}]}
        for tbl in ['t1', 't2', 't3']:
            assert self.yaml_load('table.%s.yaml' % tbl, 'schema.public')[
                'table %s' % tbl] == expmap
        self.to_map(["DROP TABLE t2"], multiple_files=True)
        with pytest.raises(IOError) as exc:
            self.yaml_load('table.t2.yaml', 'schema.public')
        assert 'No such file' in str(exc.value)
        for tbl in ['t1', 't3']:
            assert self.yaml_load('table.%s.yaml' % tbl, 'schema.public')[
                'table %s' % tbl] == expmap


class ExternalFilenameTestCase(PyrseasTestCase):

    def test_function(self):
        "Map a function"
        obj = Function(name="Weird/Or-what?")
        assert obj.extern_filename() == 'function.weird_or_what_.yaml'

    def test_schema(self):
        "Map a schema"
        obj = Schema(name="A/C Schema")
        assert obj.extern_filename() == 'schema.a_c_schema.yaml'

    def test_long_name_schema(self):
        "Map a schema with a long name"
        nm = 'a_schema_with_a_very_but_very_very_long_long_long_loooonng_name'
        obj = Schema(name=nm)
        assert obj.extern_filename() == 'schema.%s.yaml' % nm

    def test_table(self):
        "Map a table"
        obj = Table(name="Weird/Or-what?HOW.WeiRD")
        assert obj.extern_filename() == 'table.weird_or_what_how_weird.yaml'

    def test_table_unicode(self):
        "Map a table with Unicode characters"
        obj = Table(name="Fundação\\Größe таблица")
        assert obj.extern_filename() == 'table.fundação_größe_таблица.yaml'

    def test_sequence(self):
        "Map a sequence"
        obj = Sequence(name="Weird/Or-what?_seq")
        assert obj.extern_filename() == 'sequence.weird_or_what__seq.yaml'

    def test_view(self):
        "Map a view"
        obj = View(name="Weirder/Don't You Think?")
        assert obj.extern_filename() == 'view.weirder_don_t_you_think_.yaml'

########NEW FILE########
__FILENAME__ = test_foreign
# -*- coding: utf-8 -*-
"""Test text search objects"""

import pytest

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

CREATE_FDW_STMT = "CREATE FOREIGN DATA WRAPPER fdw1"
CREATE_FS_STMT = "CREATE SERVER fs1 FOREIGN DATA WRAPPER fdw1"
CREATE_UM_STMT = "CREATE USER MAPPING FOR PUBLIC SERVER fs1"
CREATE_FT_STMT = "CREATE FOREIGN TABLE ft1 (c1 integer, c2 text) SERVER fs1"
DROP_FDW_STMT = "DROP FOREIGN DATA WRAPPER IF EXISTS fdw1"
DROP_FS_STMT = "DROP SERVER IF EXISTS fs1"
DROP_UM_STMT = "DROP USER MAPPING IF EXISTS FOR PUBLIC SERVER fs1"
COMMENT_FDW_STMT = "COMMENT ON FOREIGN DATA WRAPPER fdw1 IS " \
    "'Test foreign data wrapper fdw1'"
COMMENT_FS_STMT = "COMMENT ON SERVER fs1 IS 'Test server fs1'"


class ForeignDataWrapperToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing foreign data wrappers"""

    superuser = True

    def test_map_fd_wrapper(self):
        "Map an existing foreign data wrapper"
        dbmap = self.to_map([CREATE_FDW_STMT])
        assert dbmap['foreign data wrapper fdw1'] == {}

    def test_map_wrapper_validator(self):
        "Map a foreign data wrapper with a validator function"
        dbmap = self.to_map(["CREATE FOREIGN DATA WRAPPER fdw1 "
                             "VALIDATOR postgresql_fdw_validator"])
        assert dbmap['foreign data wrapper fdw1'] == {
            'validator': 'postgresql_fdw_validator'}

    def test_map_wrapper_options(self):
        "Map a foreign data wrapper with options"
        dbmap = self.to_map(["CREATE FOREIGN DATA WRAPPER fdw1 "
                             "OPTIONS (debug 'true')"])
        assert dbmap['foreign data wrapper fdw1'] == {
            'options': ['debug=true']}

    def test_map_fd_wrapper_comment(self):
        "Map a foreign data wrapper with a comment"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        dbmap = self.to_map([CREATE_FDW_STMT, COMMENT_FDW_STMT])
        assert dbmap['foreign data wrapper fdw1']['description'] == \
            'Test foreign data wrapper fdw1'


class ForeignDataWrapperToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation for input foreign data wrappers"""

    def test_create_fd_wrapper(self):
        "Create a foreign data wrapper that didn't exist"
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_FDW_STMT

    def test_create_wrapper_validator(self):
        "Create a foreign data wrapper with a validator function"
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {
            'validator': 'postgresql_fdw_validator'}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == "CREATE FOREIGN DATA WRAPPER fdw1 " \
            "VALIDATOR postgresql_fdw_validator"

    def test_create_wrapper_options(self):
        "Create a foreign data wrapper with options"
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {
            'options': ['debug=true']}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == "CREATE FOREIGN DATA WRAPPER fdw1 " \
            "OPTIONS (debug 'true')"

    def test_bad_map_fd_wrapper(self):
        "Error creating a foreign data wrapper with a bad map"
        inmap = self.std_map()
        inmap.update({'fdw1': {}})
        with pytest.raises(KeyError):
            self.to_sql(inmap)

    def test_drop_fd_wrapper(self):
        "Drop an existing foreign data wrapper"
        sql = self.to_sql(self.std_map(), [CREATE_FDW_STMT], superuser=True)
        assert sql[0] == "DROP FOREIGN DATA WRAPPER fdw1"

    def test_alter_wrapper_options(self):
        "Change foreign data wrapper options"
        stmts = [CREATE_FDW_STMT + " OPTIONS (opt1 'valA', opt2 'valB')"]
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {
            'options': ['opt1=valX', 'opt3=valY']}})
        sql = self.to_sql(inmap, stmts, superuser=True)
        sql = fix_indent(sql[0]).split('OPTIONS ')
        assert sql[0] == "ALTER FOREIGN DATA WRAPPER fdw1 "
        assert sorted(sql[1][1:-1].split(', ')) == [
            "DROP opt2", "SET opt1 'valX'", "opt3 'valY'"]

    def test_comment_on_fd_wrapper(self):
        "Create a comment for an existing foreign data wrapper"
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {
            'description': "Test foreign data wrapper fdw1"}})
        sql = self.to_sql(inmap, [CREATE_FDW_STMT], superuser=True)
        assert sql[0] == COMMENT_FDW_STMT


class ForeignServerToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing foreign servers"""

    superuser = True

    def test_map_server(self):
        "Map an existing foreign server"
        dbmap = self.to_map([CREATE_FDW_STMT, CREATE_FS_STMT])
        assert dbmap['foreign data wrapper fdw1'] == {'server fs1': {}}

    def test_map_server_type_version(self):
        "Map a foreign server with type and version"
        stmts = [CREATE_FDW_STMT,
                 "CREATE SERVER fs1 TYPE 'test' VERSION '1.0' "
                 "FOREIGN DATA WRAPPER fdw1"]
        dbmap = self.to_map(stmts)
        assert dbmap['foreign data wrapper fdw1'] == {'server fs1': {
            'type': 'test', 'version': '1.0'}}

    def test_map_server_options(self):
        "Map a foreign server with options"
        stmts = [CREATE_FDW_STMT,
                 "CREATE SERVER fs1 FOREIGN DATA WRAPPER fdw1 "
                 "OPTIONS (dbname 'test')"]
        dbmap = self.to_map(stmts)
        assert dbmap['foreign data wrapper fdw1'] == {'server fs1': {
            'options': ['dbname=test']}}

    def test_map_server_comment(self):
        "Map a foreign server with a comment"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        stmts = [CREATE_FDW_STMT, CREATE_FS_STMT, COMMENT_FS_STMT]
        dbmap = self.to_map(stmts)
        assert dbmap['foreign data wrapper fdw1'] == {'server fs1': {
            'description': 'Test server fs1'}}


class ForeignServerToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation for input foreign servers"""

    def test_create_server(self):
        "Create a foreign server that didn't exist"
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'server fs1': {}}})
        sql = self.to_sql(inmap, [CREATE_FDW_STMT], superuser=True)
        assert fix_indent(sql[0]) == CREATE_FS_STMT

    def test_create_wrapper_server(self):
        "Create a foreign data wrapper and its server"
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'server fs1': {}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_FDW_STMT
        assert fix_indent(sql[1]) == CREATE_FS_STMT

    def test_create_server_type_version(self):
        "Create a foreign server with type and version"
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'server fs1': {
            'type': 'test', 'version': '1.0'}}})
        sql = self.to_sql(inmap, [CREATE_FDW_STMT], superuser=True)
        assert fix_indent(sql[0]) == "CREATE SERVER fs1 TYPE 'test' " \
            "VERSION '1.0' FOREIGN DATA WRAPPER fdw1"

    def test_create_server_options(self):
        "Create a foreign server with options"
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'server fs1': {
            'options': ['dbname=test']}}})
        sql = self.to_sql(inmap, [CREATE_FDW_STMT], superuser=True)
        assert fix_indent(sql[0]) == "CREATE SERVER fs1 " \
            "FOREIGN DATA WRAPPER fdw1 OPTIONS (dbname 'test')"

    def test_bad_map_server(self):
        "Error creating a foreign server with a bad map"
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'fs1': {}}})
        with pytest.raises(KeyError):
            self.to_sql(inmap)

    def test_drop_server(self):
        "Drop an existing foreign server"
        stmts = [CREATE_FDW_STMT, CREATE_FS_STMT]
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {}})
        sql = self.to_sql(inmap, stmts, superuser=True)
        assert sql[0] == "DROP SERVER fs1"

    def test_add_server_options(self):
        "Add options to a foreign server"
        stmts = [CREATE_FDW_STMT, CREATE_FS_STMT]
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'server fs1': {
            'options': ['opt1=valA', 'opt2=valB']}}})
        sql = self.to_sql(inmap, stmts, superuser=True)
        sql = fix_indent(sql[0]).split('OPTIONS ')
        assert sql[0] == "ALTER SERVER fs1 "
        assert sorted(sql[1][1:-1].split(', ')) == [
            "opt1 'valA'", "opt2 'valB'"]

    def test_drop_server_wrapper(self):
        "Drop an existing foreign data wrapper and its server"
        sql = self.to_sql(self.std_map(), [CREATE_FDW_STMT, CREATE_FS_STMT],
                          superuser=True)
        assert sql[0] == "DROP SERVER fs1"
        assert sql[1] == "DROP FOREIGN DATA WRAPPER fdw1"

    def test_comment_on_server(self):
        "Create a comment for an existing foreign server"
        stmts = [CREATE_FDW_STMT, CREATE_FS_STMT]
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'server fs1': {
            'description': "Test server fs1"}}})
        sql = self.to_sql(inmap, stmts, superuser=True)
        assert sql == [COMMENT_FS_STMT]


class UserMappingToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing user mappings"""

    superuser = True

    def test_map_user_mapping(self):
        "Map an existing user mapping"
        stmts = [CREATE_FDW_STMT, CREATE_FS_STMT, CREATE_UM_STMT]
        dbmap = self.to_map(stmts)
        assert dbmap['foreign data wrapper fdw1']['server fs1'][
            'user mappings'] == {'PUBLIC': {}}

    def test_map_user_mapping_options(self):
        "Map a user mapping with options"
        stmts = [CREATE_FDW_STMT, CREATE_FS_STMT, CREATE_UM_STMT +
                 " OPTIONS (user 'john', password 'doe')"]
        dbmap = self.to_map(stmts)
        assert dbmap['foreign data wrapper fdw1']['server fs1'] == {
            'user mappings': {'PUBLIC': {'options': [
                'user=john', 'password=doe']}}}


class UserMappingToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation for input user mappings"""

    def test_create_user_mapping(self):
        "Create a user mapping that didn't exist"
        stmts = [CREATE_FDW_STMT, CREATE_FS_STMT]
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'server fs1': {
            'user mappings': {'PUBLIC': {}}}}})
        sql = self.to_sql(inmap, stmts, superuser=True)
        assert fix_indent(sql[0]) == CREATE_UM_STMT

    def test_create_wrapper_server_mapping(self):
        "Create a FDW, server and user mapping"
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'server fs1': {
            'user mappings': {'PUBLIC': {}}}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_FDW_STMT
        assert fix_indent(sql[1]) == CREATE_FS_STMT
        assert fix_indent(sql[2]) == CREATE_UM_STMT

    def test_create_user_mapping_options(self):
        "Create a user mapping with options"
        stmts = [CREATE_FDW_STMT, CREATE_FS_STMT]
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'server fs1': {
            'user mappings': {'PUBLIC': {'options': [
                'user=john', 'password=doe']}}}}})
        sql = self.to_sql(inmap, stmts, superuser=True)
        assert fix_indent(sql[0]) == CREATE_UM_STMT + \
            " OPTIONS (user 'john', password 'doe')"

    def test_drop_user_mapping(self):
        "Drop an existing user mapping"
        stmts = [CREATE_FDW_STMT, CREATE_FS_STMT, CREATE_UM_STMT]
        sql = self.to_sql(self.std_map(), stmts, superuser=True)
        assert sql[0] == "DROP USER MAPPING FOR PUBLIC SERVER fs1"

    def test_drop_user_mapping_options(self):
        "Drop options from a user mapping"
        stmts = [CREATE_FDW_STMT, CREATE_FS_STMT,
                 CREATE_UM_STMT + " OPTIONS (opt1 'valA', opt2 'valB')"]
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'server fs1': {
            'user mappings': {'PUBLIC': {}}}}})
        sql = self.to_sql(inmap, stmts, superuser=True)
        sql = fix_indent(sql[0]).split('OPTIONS ')
        assert sql[0] == "ALTER USER MAPPING FOR PUBLIC SERVER fs1 "
        assert sorted(sql[1][1:-1].split(', ')) == ["DROP opt1", "DROP opt2"]


class ForeignTableToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing foreign tables"""

    superuser = True

    def test_map_foreign_table(self):
        "Map an existing foreign table"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        stmts = [CREATE_FDW_STMT, CREATE_FS_STMT, CREATE_FT_STMT]
        dbmap = self.to_map(stmts)
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'text'}}], 'server': 'fs1'}
        assert dbmap['schema public']['foreign table ft1'] == expmap

    def test_map_foreign_table_options(self):
        "Map a foreign table with options"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        stmts = [CREATE_FDW_STMT, CREATE_FS_STMT, CREATE_FT_STMT +
                 " OPTIONS (user 'jack')"]
        dbmap = self.to_map(stmts)
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'text'}}], 'server': 'fs1',
                  'options': ['user=jack']}
        assert dbmap['schema public']['foreign table ft1'] == expmap


class ForeignTableToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation for input foreign tables"""

    superuser = True

    def test_create_foreign_table(self):
        "Create a foreign table that didn't exist"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'server fs1': {}}})
        inmap['schema public'].update({'foreign table ft1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'server': 'fs1'}})
        sql = self.to_sql(inmap, [CREATE_FDW_STMT, CREATE_FS_STMT])
        assert fix_indent(sql[0]) == CREATE_FT_STMT

    def test_create_foreign_table_options(self):
        "Create a foreign table with options"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'server fs1': {}}})
        inmap['schema public'].update({'foreign table ft1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'server': 'fs1', 'options': ['user=jack']}})
        sql = self.to_sql(inmap, [CREATE_FDW_STMT, CREATE_FS_STMT])
        assert fix_indent(sql[0]), CREATE_FT_STMT + " OPTIONS (user 'jack')"

    def test_bad_map_foreign_table(self):
        "Error creating a foreign table with a bad map"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'server fs1': {}}})
        inmap['schema public'].update({'ft1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'server': 'fs1'}})
        with pytest.raises(KeyError):
            self.to_sql(inmap)

    def test_drop_foreign_table(self):
        "Drop an existing foreign table"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        stmts = [CREATE_FDW_STMT, CREATE_FS_STMT, CREATE_FT_STMT]
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'server fs1': {}}})
        sql = self.to_sql(inmap, stmts)
        assert sql[0] == "DROP FOREIGN TABLE ft1"

    def test_drop_foreign_table_server(self):
        "Drop a foreign table and associated server"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        stmts = [CREATE_FDW_STMT, CREATE_FS_STMT, CREATE_FT_STMT]
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {}})
        sql = self.to_sql(inmap, stmts)
        assert sql[0] == "DROP FOREIGN TABLE ft1"
        assert sql[1] == "DROP SERVER fs1"

    def test_alter_foreign_table_options(self):
        "Alter options for a foreign table"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        stmts = [CREATE_FDW_STMT, CREATE_FS_STMT, CREATE_FT_STMT +
                 " OPTIONS (opt1 'valA', opt2 'valB')"]
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'server fs1': {}}})
        inmap['schema public'].update({'foreign table ft1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'server': 'fs1', 'options': ['opt1=valX', 'opt2=valY']}})
        sql = self.to_sql(inmap, stmts)
        assert fix_indent(sql[0]), "ALTER FOREIGN TABLE ft1 " \
            "OPTIONS (SET opt1 'valX', SET opt2 'valY')"

    def test_add_column(self):
        "Add new column to a foreign table"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        stmts = [CREATE_FDW_STMT, CREATE_FS_STMT, CREATE_FT_STMT]
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'server fs1': {}}})
        inmap['schema public'].update({'foreign table ft1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c3': {'type': 'date'}}], 'server': 'fs1'}})
        sql = self.to_sql(inmap, stmts)
        assert fix_indent(sql[0]) == \
            "ALTER FOREIGN TABLE ft1 ADD COLUMN c3 date"

########NEW FILE########
__FILENAME__ = test_function
# -*- coding: utf-8 -*-
"""Test functions"""

import pytest

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

SOURCE1 = "SELECT 'dummy'::text"
CREATE_STMT1 = "CREATE FUNCTION f1() RETURNS text LANGUAGE sql IMMUTABLE AS " \
    "$_$%s$_$" % SOURCE1
DROP_STMT1 = "DROP FUNCTION IF EXISTS f1()"

SOURCE2 = "SELECT GREATEST($1, $2)"
CREATE_STMT2 = "CREATE FUNCTION f1(integer, integer) RETURNS integer " \
    "LANGUAGE sql IMMUTABLE AS $_$%s$_$" % SOURCE2
DROP_STMT2 = "DROP FUNCTION IF EXISTS f1(integer, integer)"
COMMENT_STMT = "COMMENT ON FUNCTION f1(integer, integer) IS 'Test function f1'"

SOURCE3 = "SELECT * FROM generate_series($1, $2)"
CREATE_STMT3 = "CREATE FUNCTION f2(integer, integer) RETURNS SETOF integer " \
    "ROWS 20 LANGUAGE sql IMMUTABLE AS $_$%s$_$" % SOURCE3

SOURCE4 = "SELECT $1 + $2"
CREATE_STMT4 = "CREATE FUNCTION f1(integer, integer) RETURNS integer " \
    "LANGUAGE sql IMMUTABLE LEAKPROOF AS $_$%s$_$" % SOURCE4


class FunctionToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing functions"""

    def test_map_function(self):
        "Map a very simple function with no arguments"
        dbmap = self.to_map([CREATE_STMT1])
        expmap = {'language': 'sql', 'returns': 'text',
                  'source': SOURCE1, 'volatility': 'immutable'}
        assert dbmap['schema public']['function f1()'] == expmap

    def test_map_function_with_args(self):
        "Map a function with two arguments"
        stmts = ["CREATE FUNCTION f1(integer, integer) RETURNS integer "
                 "LANGUAGE sql AS $_$%s$_$" % SOURCE2]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['function f1(integer, integer)'] == \
            {'language': 'sql', 'returns': 'integer', 'source': SOURCE2}

    def test_map_function_default_args(self):
        "Map a function with default arguments"
        stmts = ["CREATE FUNCTION f1(integer, integer) RETURNS integer "
                 "LANGUAGE sql AS $_$%s$_$" % SOURCE2]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['function f1(integer, integer)'] == \
            {'language': 'sql', 'returns': 'integer', 'source': SOURCE2}

    def test_map_void_function(self):
        "Map a function returning void"
        stmts = ["CREATE TABLE t1 (c1 integer, c2 text)",
                 "CREATE FUNCTION f1() RETURNS void LANGUAGE sql AS "
                 "$_$INSERT INTO t1 VALUES (1, 'dummy')$_$"]
        dbmap = self.to_map(stmts)
        expmap = {'language': 'sql', 'returns': 'void',
                  'source': "INSERT INTO t1 VALUES (1, 'dummy')"}
        assert dbmap['schema public']['function f1()'] == expmap

    def test_map_setof_row_function(self):
        "Map a function returning a set of rows"
        stmts = ["CREATE TABLE t1 (c1 integer, c2 text)",
                 "CREATE FUNCTION f1() RETURNS SETOF t1 LANGUAGE sql AS "
                 "$_$SELECT * FROM t1$_$"]
        dbmap = self.to_map(stmts)
        expmap = {'language': 'sql', 'returns': 'SETOF t1',
                  'source': "SELECT * FROM t1"}
        assert dbmap['schema public']['function f1()'] == expmap

    def test_map_security_definer_function(self):
        "Map a function that is SECURITY DEFINER"
        stmts = ["CREATE FUNCTION f1() RETURNS text LANGUAGE sql "
                 "SECURITY DEFINER AS $_$%s$_$" % SOURCE1]
        dbmap = self.to_map(stmts)
        expmap = {'language': 'sql', 'returns': 'text',
                  'source': SOURCE1, 'security_definer': True}
        assert dbmap['schema public']['function f1()'] == expmap

    def test_map_c_lang_function(self):
        "Map a dynamically loaded C language function"
        # NOTE 1: Needs contrib/spi module to be available
        # NOTE 2: Needs superuser privilege
        stmts = ["CREATE FUNCTION autoinc() RETURNS trigger "
                 "AS '$libdir/autoinc' LANGUAGE c"]
        dbmap = self.to_map(stmts, superuser=True)
        expmap = {'language': 'c', 'obj_file': '$libdir/autoinc',
                  'link_symbol': 'autoinc', 'returns': 'trigger'}
        assert dbmap['schema public']['function autoinc()'] == expmap

    def test_map_function_config(self):
        "Map a function with a configuration parameter"
        stmts = ["CREATE FUNCTION f1() RETURNS date LANGUAGE sql SET "
                 "datestyle to postgres, dmy AS $_$SELECT CURRENT_DATE$_$"]
        dbmap = self.to_map(stmts)
        expmap = {'language': 'sql', 'returns': 'date',
                  'configuration': ['DateStyle=postgres, dmy'],
                  'source': "SELECT CURRENT_DATE"}
        assert dbmap['schema public']['function f1()'] == expmap

    def test_map_function_comment(self):
        "Map a function comment"
        dbmap = self.to_map([CREATE_STMT2, COMMENT_STMT])
        assert dbmap['schema public']['function f1(integer, integer)'][
            'description'] == 'Test function f1'

    def test_map_function_rows(self):
        "Map a function rows"
        dbmap = self.to_map([CREATE_STMT3])
        assert dbmap['schema public']['function f2(integer, integer)'][
            'rows'] == 20

    def test_map_function_leakproof(self):
        "Map a function with LEAKPROOF qualifier"
        if self.db.version < 90200:
            self.skipTest('Only available on PG 9.2 or later')
        dbmap = self.to_map([CREATE_STMT4], superuser=True)
        expmap = {'language': 'sql', 'returns': 'integer', 'leakproof': True,
                  'source': SOURCE4, 'volatility': 'immutable'}
        assert dbmap['schema public']['function f1(integer, integer)'] == \
            expmap


class FunctionToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input functions"""

    def test_create_function(self):
        "Create a very simple function with no arguments"
        inmap = self.std_map()
        inmap['schema public'].update({'function f1()': {
            'language': 'sql', 'returns': 'text', 'source': SOURCE1,
            'volatility': 'immutable'}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[1]) == CREATE_STMT1

    def test_create_function_with_args(self):
        "Create a function with two arguments"
        inmap = self.std_map()
        inmap['schema public'].update({
            'function f1(integer, integer)': {
                'language': 'sql', 'returns': 'integer', 'source': SOURCE2}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[1]) == "CREATE FUNCTION f1(integer, integer) " \
            "RETURNS integer LANGUAGE sql AS $_$%s$_$" % SOURCE2

    def test_create_setof_row_function(self):
        "Create a function returning a set of rows"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}},
                        {'c2': {'type': 'text'}}]}})
        inmap['schema public'].update({
            'function f1()': {'language': 'sql', 'returns': 'SETOF t1',
                              'source': "SELECT * FROM t1"}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[2]) == "CREATE FUNCTION f1() RETURNS " \
            "SETOF t1 LANGUAGE sql AS $_$SELECT * FROM t1$_$"

    def test_create_setof_row_function_rows(self):
        "Create a function returning a set of rows with suggested number"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}},
                        {'c2': {'type': 'text'}}]}})
        inmap['schema public'].update({
            'function f1()': {'language': 'sql', 'returns': 'SETOF t1',
                              'source': "SELECT * FROM t1", 'rows': 50}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[2]) == "CREATE FUNCTION f1() RETURNS SETOF t1 " \
            "LANGUAGE sql ROWS 50 AS $_$SELECT * FROM t1$_$"

    def test_create_security_definer_function(self):
        "Create a SECURITY DEFINER function"
        inmap = self.std_map()
        inmap['schema public'].update({'function f1()': {
            'language': 'sql', 'returns': 'text', 'source': SOURCE1,
            'security_definer': True}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[1]) == "CREATE FUNCTION f1() RETURNS text " \
            "LANGUAGE sql SECURITY DEFINER AS $_$%s$_$" % SOURCE1

    def test_create_c_lang_function(self):
        "Create a dynamically loaded C language function"
        # NOTE 1: Needs contrib/spi module to be available
        # NOTE 2: Needs superuser privilege
        inmap = self.std_map()
        inmap['schema public'].update({'function autoinc()': {
            'language': 'c', 'returns': 'trigger',
            'obj_file': '$libdir/autoinc'}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[1]) == "CREATE FUNCTION autoinc() " \
            "RETURNS trigger LANGUAGE c AS '$libdir/autoinc', 'autoinc'"

    def test_create_function_config(self):
        "Create a function with a configuration parameter"
        inmap = self.std_map()
        inmap['schema public'].update({'function f1()': {
            'language': 'sql', 'returns': 'date',
            'configuration': ['DateStyle=postgres, dmy'],
            'source': "SELECT CURRENT_DATE"}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[1]) == "CREATE FUNCTION f1() RETURNS date " \
            "LANGUAGE sql SET DateStyle=postgres, dmy AS " \
            "$_$SELECT CURRENT_DATE$_$"

    def test_create_function_in_schema(self):
        "Create a function within a non-public schema"
        inmap = self.std_map()
        inmap.update({'schema s1': {'function f1()': {
            'language': 'sql', 'returns': 'text', 'source': SOURCE1,
            'volatility': 'immutable'}}})
        sql = self.to_sql(inmap, ["CREATE SCHEMA s1"])
        assert fix_indent(sql[1]) == "CREATE FUNCTION s1.f1() RETURNS text " \
            "LANGUAGE sql IMMUTABLE AS $_$%s$_$" % SOURCE1

    def test_bad_function_map(self):
        "Error creating a function with a bad map"
        inmap = self.std_map()
        inmap['schema public'].update({'f1()': {
            'language': 'sql', 'returns': 'text', 'source': SOURCE1}})
        with pytest.raises(KeyError):
            self.to_sql(inmap)

    def test_drop_function(self):
        "Drop an existing function with no arguments"
        sql = self.to_sql(self.std_map(), [CREATE_STMT1])
        assert sql == ["DROP FUNCTION f1()"]

    def test_drop_function_with_args(self):
        "Drop an existing function which has arguments"
        sql = self.to_sql(self.std_map(), [CREATE_STMT2])
        assert sql[0] == "DROP FUNCTION f1(integer, integer)"

    def test_change_function_defn(self):
        "Change function definition"
        inmap = self.std_map()
        inmap['schema public'].update({'function f1()': {
            'language': 'sql', 'returns': 'text',
            'source': "SELECT 'example'::text", 'volatility': 'immutable'}})
        sql = self.to_sql(inmap, [CREATE_STMT1])
        assert fix_indent(sql[1]) == "CREATE OR REPLACE FUNCTION f1() " \
            "RETURNS text LANGUAGE sql IMMUTABLE AS " \
            "$_$SELECT 'example'::text$_$"

    def test_function_with_comment(self):
        "Create a function with a comment"
        inmap = self.std_map()
        inmap['schema public'].update({
            'function f1(integer, integer)': {
                'description': 'Test function f1', 'language': 'sql',
                'returns': 'integer', 'source': SOURCE2}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[1]) == "CREATE FUNCTION f1(integer, integer) " \
            "RETURNS integer LANGUAGE sql AS $_$%s$_$" % SOURCE2
        assert sql[2] == COMMENT_STMT

    def test_comment_on_function(self):
        "Create a comment for an existing function"
        inmap = self.std_map()
        inmap['schema public'].update({
            'function f1(integer, integer)': {
                'description': 'Test function f1', 'language': 'sql',
                'returns': 'integer', 'source': SOURCE2}})
        sql = self.to_sql(inmap, [CREATE_STMT2])
        assert sql == [COMMENT_STMT]

    def test_drop_function_comment(self):
        "Drop a comment on an existing function"
        stmts = [CREATE_STMT2, COMMENT_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({
            'function f1(integer, integer)': {
                'language': 'sql', 'returns': 'integer', 'source': SOURCE2}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["COMMENT ON FUNCTION f1(integer, integer) IS NULL"]

    def test_change_function_comment(self):
        "Change existing comment on a function"
        stmts = [CREATE_STMT2, COMMENT_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({
            'function f1(integer, integer)': {
                'description': 'Changed function f1', 'language': 'sql',
                'returns': 'integer', 'source': SOURCE2}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["COMMENT ON FUNCTION f1(integer, integer) IS "
                       "'Changed function f1'"]

    def test_function_leakproof(self):
        "Create a function with LEAKPROOF qualifier"
        if self.db.version < 90200:
            self.skipTest('Only available on PG 9.2 or later')
        inmap = self.std_map()
        inmap['schema public'].update({
            'function f1(integer, integer)': {
                'language': 'sql', 'returns': 'integer', 'leakproof': True,
                'source': SOURCE4, 'volatility': 'immutable'}})
        sql = self.to_sql(inmap, superuser=True)
        assert fix_indent(sql[1]) == "CREATE FUNCTION f1(integer, integer) " \
            "RETURNS integer LANGUAGE sql IMMUTABLE LEAKPROOF AS " \
            "$_$%s$_$" % SOURCE4

    def test_alter_function_leakproof(self):
        "Change a function with LEAKPROOF qualifier"
        if self.db.version < 90200:
            self.skipTest('Only available on PG 9.2 or later')
        inmap = self.std_map()
        inmap['schema public'].update({
            'function f1(integer, integer)': {
                'language': 'sql', 'returns': 'integer',
                'source': SOURCE4, 'volatility': 'immutable'}})
        sql = self.to_sql(inmap, [CREATE_STMT4], superuser=True)
        assert fix_indent(sql[0]) == \
            "ALTER FUNCTION f1(integer, integer) NOT LEAKPROOF"


class AggregateToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing aggregates"""

    def test_map_aggregate(self):
        "Map a simple aggregate"
        stmts = [CREATE_STMT2, "CREATE AGGREGATE a1 (integer) ("
                 "SFUNC = f1, STYPE = integer)"]
        dbmap = self.to_map(stmts)
        expmap = {'sfunc': 'f1', 'stype': 'integer'}
        assert dbmap['schema public']['function f1(integer, integer)'] == \
            {'language': 'sql', 'returns': 'integer', 'source': SOURCE2,
             'volatility': 'immutable'}
        assert dbmap['schema public']['aggregate a1(integer)'] == expmap

    def test_map_aggregate_init_final(self):
        "Map an aggregate with an INITCOND and a FINALFUNC"
        stmts = [CREATE_STMT2,
                 "CREATE FUNCTION f2(integer) RETURNS float "
                 "LANGUAGE sql AS $_$SELECT $1::float$_$ IMMUTABLE",
                 "CREATE AGGREGATE a1 (integer) (SFUNC = f1, STYPE = integer, "
                 "FINALFUNC = f2, INITCOND = '-1')"]
        dbmap = self.to_map(stmts)
        expmap = {'sfunc': 'f1', 'stype': 'integer',
                  'initcond': '-1', 'finalfunc': 'f2'}
        assert dbmap['schema public']['function f1(integer, integer)'] == \
            {'language': 'sql', 'returns': 'integer', 'source': SOURCE2,
             'volatility': 'immutable'}
        assert dbmap['schema public']['function f2(integer)'] == \
            {'language': 'sql', 'returns': 'double precision',
             'source': "SELECT $1::float", 'volatility': 'immutable'}
        assert dbmap['schema public']['aggregate a1(integer)'] == expmap

    def test_map_aggregate_sortop(self):
        "Map an aggregate with a SORTOP"
        stmts = [CREATE_STMT2, "CREATE AGGREGATE a1 (integer) ("
                 "SFUNC = f1, STYPE = integer, SORTOP = >)"]
        dbmap = self.to_map(stmts)
        expmap = {'sfunc': 'f1', 'stype': 'integer',
                  'sortop': 'pg_catalog.>'}
        assert dbmap['schema public']['aggregate a1(integer)'] == expmap


class AggregateToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input aggregates"""

    def test_create_aggregate(self):
        "Create a simple aggregate"
        inmap = self.std_map()
        inmap['schema public'].update({'function f1(integer, integer)': {
            'language': 'sql', 'returns': 'integer', 'source': SOURCE2,
            'volatility': 'immutable'}})
        inmap['schema public'].update({'aggregate a1(integer)': {
            'sfunc': 'f1', 'stype': 'integer'}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[1]) == CREATE_STMT2
        assert fix_indent(sql[2]) == "CREATE AGGREGATE a1(integer) " \
            "(SFUNC = f1, STYPE = integer)"

    def test_create_aggregate_init_final(self):
        "Create an aggregate with an INITCOND and a FINALFUNC"
        inmap = self.std_map()
        inmap['schema public'].update({'function f1(integer, integer)': {
            'language': 'sql', 'returns': 'integer', 'source': SOURCE2,
            'volatility': 'immutable'}})
        inmap['schema public'].update({'function f2(integer)': {
            'language': 'sql', 'returns': 'double precision',
            'source': "SELECT $1::float", 'volatility': 'immutable'}})
        inmap['schema public'].update({'aggregate a1(integer)': {
            'sfunc': 'f1', 'stype': 'integer', 'initcond': '-1',
            'finalfunc': 'f2'}})
        sql = self.to_sql(inmap)
        funcs = sorted(sql[1:3])
        assert fix_indent(funcs[0]) == CREATE_STMT2
        assert fix_indent(funcs[1]) == "CREATE FUNCTION f2(integer) " \
            "RETURNS double precision LANGUAGE sql IMMUTABLE " \
            "AS $_$SELECT $1::float$_$"
        assert fix_indent(sql[3]) == "CREATE AGGREGATE a1(integer) " \
            "(SFUNC = f1, STYPE = integer, FINALFUNC = f2, INITCOND = '-1')"

    def test_drop_aggregate(self):
        "Drop an existing aggregate"
        stmts = [CREATE_STMT2, "CREATE AGGREGATE agg1 (integer) "
                 "(SFUNC = f1, STYPE = integer)"]
        sql = self.to_sql(self.std_map(), stmts)
        assert sql[0] == "DROP AGGREGATE agg1(integer)"
        assert sql[1], "DROP FUNCTION f1(integer == integer)"

########NEW FILE########
__FILENAME__ = test_index
# -*- coding: utf-8 -*-
"""Test indexes"""

import pytest

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

CREATE_TABLE_STMT = "CREATE TABLE t1 (c1 integer, c2 text)"
CREATE_STMT = "CREATE INDEX t1_idx ON t1 (c1)"
COMMENT_STMT = "COMMENT ON INDEX t1_idx IS 'Test index t1_idx'"


class IndexToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of created indexes"""

    def test_index_1(self):
        "Map a single-column index"
        dbmap = self.to_map([CREATE_TABLE_STMT, CREATE_STMT])
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'text'}}],
                  'indexes': {'t1_idx': {'keys': ['c1']}}}
        assert dbmap['schema public']['table t1'] == expmap

    def test_index_2(self):
        "Map a two-column index"
        stmts = ["CREATE TABLE t1 (c1 INTEGER, c2 CHAR(5), c3 TEXT)",
                 "CREATE UNIQUE INDEX t1_idx ON t1 (c1, c2)"]
        dbmap = self.to_map(stmts)
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'character(5)'}},
                              {'c3': {'type': 'text'}}],
                  'indexes': {'t1_idx': {'keys': ['c1', 'c2'],
                                         'unique': True}}}
        assert dbmap['schema public']['table t1'] == expmap

    def test_index_3(self):
        "Map a table with a unique index and a non-unique GIN index"
        stmts = ["CREATE TABLE t1 (c1 INTEGER, c2 CHAR(5), c3 tsvector)",
                 "CREATE UNIQUE INDEX t1_idx_1 ON t1 (c1, c2)",
                 "CREATE INDEX t1_idx_2 ON t1 USING gin (c3)"]
        dbmap = self.to_map(stmts)
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'character(5)'}},
                              {'c3': {'type': 'tsvector'}}],
                  'indexes': {'t1_idx_1': {'keys': ['c1', 'c2'],
                                           'unique': True},
                              't1_idx_2': {'keys': ['c3'],
                                           'access_method': 'gin'}}}
        assert dbmap['schema public']['table t1'] == expmap

    def test_map_index_partial(self):
        "Map a table with a partial index"
        dbmap = self.to_map([CREATE_TABLE_STMT,
                             "CREATE INDEX t1_idx ON t1 (c2) WHERE c1 > 42"])
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'text'}}],
                  'indexes': {'t1_idx': {'keys': ['c2'],
                                         'predicate': '(c1 > 42)'}}}
        assert dbmap['schema public']['table t1'] == expmap

    def test_index_function(self):
        "Map an index using a function"
        stmts = [CREATE_TABLE_STMT, "CREATE INDEX t1_idx ON t1 ((lower(c2)))"]
        dbmap = self.to_map(stmts)
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'text'}}],
                  'indexes': {'t1_idx': {'keys': [
                      {'lower(c2)': {'type': 'expression'}}]}}}
        assert dbmap['schema public']['table t1'] == expmap

    def test_index_function_complex(self):
        "Map indexes using nested functions and complex arguments"
        stmts = ["CREATE TABLE t1 (c1 integer, c2 text, c3 date)",
                 "CREATE INDEX t1_idx1 ON t1 (substring(c2 from position("
                 "'_begin' in c2)), substring(c2 from position('_end' in "
                 "c2)))",
                 "CREATE INDEX t1_idx2 ON t1 (extract(month from c3), "
                 "extract(day from c3))"]
        dbmap = self.to_map(stmts)
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'text'}},
                              {'c3': {'type': 'date'}}],
                  'indexes': {'t1_idx1': {
                      'keys': [{'"substring"(c2, "position"(c2, \'_begin\''
                                '::text))': {'type': 'expression'}},
                               {'"substring"(c2, "position"(c2, \'_end\''
                                '::text))': {'type': 'expression'}}]},
                              't1_idx2': {
                      'keys': [{"date_part('month'::text, c3)": {
                                'type': 'expression'}},
                               {"date_part('day'::text, c3)": {
                                'type': 'expression'}}]}}}
        assert dbmap['schema public']['table t1'] == expmap

    def test_index_col_opts(self):
        "Map an index with various column options"
        stmts = ["CREATE TABLE t1 (c1 cidr, c2 text)",
                 "CREATE INDEX t1_idx ON t1 (c1 cidr_ops NULLS FIRST, "
                 "c2 DESC)"]
        dbmap = self.to_map(stmts)
        expmap = {'columns': [{'c1': {'type': 'cidr'}},
                              {'c2': {'type': 'text'}}],
                  'indexes': {'t1_idx': {'keys': [
                      {'c1': {'opclass': 'cidr_ops', 'nulls': 'first'}},
                      {'c2': {'order': 'desc'}}]}}}
        assert dbmap['schema public']['table t1'] == expmap

    def test_index_mixed(self):
        "Map indexes using functions, a regular column and expressions"
        stmts = ["CREATE TABLE t1 (c1 integer, c2 text, c3 text)",
                 "CREATE INDEX t1_idx ON t1 (btrim(c3, 'x') NULLS FIRST, c1, "
                 "lower(c2) DESC)",
                 "CREATE INDEX t1_idx2 ON t1 ((c2 || ', ' || c3), "
                 "(c3 || ' ' || c2))"]
        dbmap = self.to_map(stmts)
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'text'}},
                              {'c3': {'type': 'text'}}],
                  'indexes': {'t1_idx': {
                      'keys': [{"btrim(c3, 'x'::text)": {
                                'type': 'expression', 'nulls': 'first'}},
                               'c1', {'lower(c2)': {
                                   'type': 'expression', 'order': 'desc'}}]},
                              't1_idx2': {
                      'keys': [{"(((c2 || ', '::text) || c3))": {
                                'type': 'expression'}},
                               {"(((c3 || ' '::text) || c2))": {
                                'type': 'expression'}}]}}}
        assert dbmap['schema public']['table t1'] == expmap

    def test_map_index_cluster(self):
        "Map a table with an index and cluster on it"
        dbmap = self.to_map([CREATE_TABLE_STMT, CREATE_STMT,
                             "CLUSTER t1 USING t1_idx"])
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'text'}}],
                  'indexes': {'t1_idx': {'keys': ['c1'], 'cluster': True}}}
        assert dbmap['schema public']['table t1'] == expmap

    def test_map_index_comment(self):
        "Map an index comment"
        dbmap = self.to_map([CREATE_TABLE_STMT, CREATE_STMT, COMMENT_STMT])
        assert dbmap['schema public']['table t1']['indexes']['t1_idx'][
            'description'] == 'Test index t1_idx'


class IndexToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input indexes"""

    def test_create_table_with_index(self):
        "Create new table with a single column index"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}},
                        {'c2': {'type': 'text'}}],
            'indexes': {'t1_idx': {'keys': ['c1']}}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == "CREATE TABLE t1 (c1 integer, c2 text)"
        assert sql[1] == "CREATE INDEX t1_idx ON t1 (c1)"

    def test_add_index(self):
        "Add a two-column unique index to an existing table"
        stmts = ["CREATE TABLE t1 (c1 INTEGER NOT NULL, "
                 "c2 INTEGER NOT NULL, c3 TEXT)"]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer', 'not_null': True}},
                        {'c2': {'type': 'integer', 'not_null': True}},
                        {'c3': {'type': 'text'}}],
            'indexes': {'t1_idx': {'keys': ['c2', 'c1'], 'unique': True}}}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["CREATE UNIQUE INDEX t1_idx ON t1 (c2, c1)"]

    def test_add_index_schema(self):
        "Add an index to an existing table in a non-public schema"
        stmts = ["CREATE SCHEMA s1",
                 "CREATE TABLE s1.t1 (c1 INTEGER NOT NULL, "
                 "c2 INTEGER NOT NULL, c3 TEXT)"]
        inmap = self.std_map()
        inmap.update({'schema s1': {'table t1': {
            'columns': [{'c1': {'type': 'integer', 'not_null': True}},
                        {'c2': {'type': 'integer', 'not_null': True}},
                        {'c3': {'type': 'text'}}],
            'indexes': {'t1_idx': {'keys': ['c2', 'c1'], 'unique': True}}}}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["CREATE UNIQUE INDEX t1_idx ON s1.t1 (c2, c1)"]

    def test_add_index_back_compat(self):
        "Add a index to an existing table accepting back-compatible spec"
        stmts = ["CREATE TABLE t1 (c1 INTEGER NOT NULL, "
                 "c2 INTEGER NOT NULL, c3 TEXT)"]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer', 'not_null': True}},
                        {'c2': {'type': 'integer', 'not_null': True}},
                        {'c3': {'type': 'text'}}],
            'indexes': {'t1_idx': {'columns': ['c2', 'c1'], 'unique': True,
                                   'access_method': 'btree'}}}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["CREATE UNIQUE INDEX t1_idx ON t1 (c2, c1)"]

    def test_bad_index(self):
        "Fail on creating an index without columns or expression"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'indexes': {'t1_idx': {'access_method': 'btree'}}}})
        with pytest.raises(KeyError):
            self.to_sql(inmap)

    def test_create_partial(self):
        "Create a partial index"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}},
                        {'c2': {'type': 'text'}}],
            'indexes': {'t1_idx': {'keys': ['c2'],
                                   'predicate': '(c1 > 42)'}}}})
        sql = self.to_sql(inmap, [CREATE_TABLE_STMT])
        assert fix_indent(sql[0]) == \
            "CREATE INDEX t1_idx ON t1 (c2) WHERE (c1 > 42)"

    def test_create_index_function(self):
        "Create an index which uses a function"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'indexes': {'t1_idx': {'keys': [{
                'lower(c2)': {'type': 'expression'}}]}}}})
        sql = self.to_sql(inmap)
        assert sql[1] == "CREATE INDEX t1_idx ON t1 (lower(c2))"

    def test_create_index_col_opts(self):
        "Create table and an index with column options"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'cidr'}}, {'c2': {'type': 'text'}}],
            'indexes': {'t1_idx': {'keys': [{'c1': {
                'opclass': 'cidr_ops', 'nulls': 'first'}}, 'c2']}}}})
        sql = self.to_sql(inmap)
        assert sql[1] == "CREATE INDEX t1_idx ON t1 " \
            "(c1 cidr_ops NULLS FIRST, c2)"

    def test_index_mixed(self):
        "Create indexes using functions, a regular column and expressions"
        stmts = ["CREATE TABLE t1 (c1 integer, c2 text, c3 text)"]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c3': {'type': 'text'}}],
            'indexes': {'t1_idx': {'keys': [
                {"btrim(c3, 'x'::text)": {'type': 'expression',
                                          'nulls': 'first'}}, 'c1',
                {'lower(c2)': {'type': 'expression', 'order': 'desc'}}]},
                't1_idx2': {'keys': [
                    {"(((c2 || ', '::text) || c3))": {'type': 'expression'}},
                    {"(((c3 || ' '::text) || c2))": {
                        'type': 'expression'}}]}}}})
        sql = sorted(self.to_sql(inmap, stmts))
        assert sql[0] == "CREATE INDEX t1_idx ON t1 (" \
            "btrim(c3, 'x'::text) NULLS FIRST, c1, lower(c2) DESC)"
        assert sql[1] == "CREATE INDEX t1_idx2 ON t1 (" \
            "(((c2 || ', '::text) || c3)), (((c3 || ' '::text) || c2)))"

    def test_create_table_with_index_clustered(self):
        "Create new table clustered on a single column index"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}},
                        {'c2': {'type': 'text'}}],
            'indexes': {'t1_idx': {'keys': ['c1'], 'cluster': True}}}})
        sql = self.to_sql(inmap)
        assert sql[2] == "CLUSTER t1 USING t1_idx"

    def test_cluster_table_with_index(self):
        "Change a table with an index to cluster on it"
        stmts = [CREATE_TABLE_STMT, CREATE_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'indexes': {'t1_idx': {'keys': ['c1'], 'cluster': True}}}})
        sql = self.to_sql(inmap, stmts)
        assert sql[0] == "CLUSTER t1 USING t1_idx"

    def test_uncluster_table_with_index(self):
        "Change a table clustered on an index to remove cluster"
        stmts = [CREATE_TABLE_STMT, CREATE_STMT, "CLUSTER t1 USING t1_idx"]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'indexes': {'t1_idx': {'keys': ['c1']}}}})
        sql = self.to_sql(inmap, stmts)
        assert fix_indent(sql[0]) == "ALTER TABLE t1 SET WITHOUT CLUSTER"

    def test_comment_on_index(self):
        "Create a comment for an existing index"
        stmts = [CREATE_TABLE_STMT, CREATE_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'indexes': {'t1_idx': {'keys': ['c1'],
                                   'description': 'Test index t1_idx'}}}})
        sql = self.to_sql(inmap, stmts)
        assert sql == [COMMENT_STMT]

    def test_drop_index_comment(self):
        "Drop the comment on an existing index"
        stmts = [CREATE_TABLE_STMT, CREATE_STMT, COMMENT_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'indexes': {'t1_idx': {'keys': ['c1']}}}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["COMMENT ON INDEX t1_idx IS NULL"]

    def test_change_index_comment(self):
        "Change existing comment on an index"
        stmts = [CREATE_TABLE_STMT, CREATE_STMT, COMMENT_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'indexes': {'t1_idx': {'keys': ['c1'],
                                   'description': 'Changed index t1_idx'}}}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["COMMENT ON INDEX t1_idx IS 'Changed index t1_idx'"]

########NEW FILE########
__FILENAME__ = test_language
# -*- coding: utf-8 -*-
"""Test languages"""

import pytest

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase

CREATE_STMT = "CREATE LANGUAGE plperl"
DROP_STMT = "DROP LANGUAGE IF EXISTS plperl CASCADE"
COMMENT_STMT = "COMMENT ON LANGUAGE plperl IS 'Test language PL/Perl'"


class LanguageToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing languages"""

    def test_map_language(self):
        "Map an existing language"
        if self.db.version >= 90100:
            self.skipTest('Only available before PG 9.1')
        dbmap = self.to_map([DROP_STMT, CREATE_STMT])
        assert dbmap['language plperl'] == {'trusted': True}

    def test_map_language_comment(self):
        "Map a language with a comment"
        if self.db.version >= 90100:
            self.skipTest('Only available before PG 9.1')
        dbmap = self.to_map([DROP_STMT, CREATE_STMT, COMMENT_STMT],
                            superuser=True)
        assert dbmap['language plperl']['description'] == \
            'Test language PL/Perl'


class LanguageToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation for input languages"""

    def tearDown(self):
        self.db.execute_commit(DROP_STMT)
        self.db.close()

    def test_create_language(self):
        "Create a language that didn't exist"
        if self.db.version >= 90100:
            self.skipTest('Only available before PG 9.1')
        sql = self.to_sql({'language plperl': {}})
        assert sql == [CREATE_STMT]

    def test_bad_language_map(self):
        "Error creating a language with a bad map"
        with pytest.raises(KeyError):
            self.to_sql({'plperl': {}})

    def test_drop_language(self):
        "Drop an existing language"
        if self.db.version >= 90100:
            self.skipTest('Only available before PG 9.1')
        sql = self.to_sql({}, [CREATE_STMT])
        assert sql == ["DROP LANGUAGE plperl"]

    def test_drop_language_function(self):
        "Drop an existing function and the language it uses"
        if self.db.version >= 90100:
            self.skipTest('Only available before PG 9.1')
        stmts = [CREATE_STMT, "CREATE FUNCTION f1() RETURNS text "
                 "LANGUAGE plperl AS $_$return \"dummy\";$_$"]
        sql = self.to_sql({}, stmts)
        assert sql == ["DROP FUNCTION f1()", "DROP LANGUAGE plperl"]

    def test_comment_on_language(self):
        "Create a comment for an existing language"
        if self.db.version >= 90100:
            self.skipTest('Only available before PG 9.1')
        inmap = self.std_map()
        inmap.update({'language plperl': {
            'description': "Test language PL/Perl"}})
        sql = self.to_sql(inmap, [CREATE_STMT])
        assert sql == [COMMENT_STMT]

########NEW FILE########
__FILENAME__ = test_matview
# -*- coding: utf-8 -*-
"""Test materialized views"""

import pytest

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

CREATE_TABLE = "CREATE TABLE t1 (c1 INTEGER, c2 TEXT, c3 INTEGER)"
VIEW_STMT = "SELECT c1, c3 * 2 AS mc3 FROM t1"
CREATE_STMT = "CREATE MATERIALIZED VIEW mv1 AS " + VIEW_STMT
COMMENT_STMT = "COMMENT ON MATERIALIZED VIEW mv1 IS 'Test matview mv1'"
VIEW_DEFN = " SELECT t1.c1,\n    t1.c3 * 2 AS mc3\n   FROM t1;"


class MatViewToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of created materialized views"""

    def test_map_view(self):
        "Map a created materialized view"
        if self.db.version < 90300:
            self.skipTest('Only available on PG 9.3')
        stmts = [CREATE_TABLE, CREATE_STMT]
        dbmap = self.to_map(stmts)
        expmap = {'definition': VIEW_DEFN, 'with_data': True}
        assert dbmap['schema public']['materialized view mv1'] == expmap

    def test_map_view_comment(self):
        "Map a materialized view with a comment"
        if self.db.version < 90300:
            self.skipTest('Only available on PG 9.3')
        dbmap = self.to_map([CREATE_TABLE, CREATE_STMT, COMMENT_STMT])
        assert dbmap['schema public']['materialized view mv1'][
            'description'] == 'Test matview mv1'

    def test_map_view_index(self):
        "Map a materialized view with an index"
        if self.db.version < 90300:
            self.skipTest('Only available on PG 9.3')
        stmts = [CREATE_TABLE, CREATE_STMT,
                 "CREATE INDEX idx1 ON mv1 (mc3)"]
        dbmap = self.to_map(stmts)
        expmap = {'definition': VIEW_DEFN, 'with_data': True,
                  'indexes': {'idx1': {'keys': ['mc3']}}}
        assert dbmap['schema public']['materialized view mv1'] == expmap


class MatViewToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input materialized views"""

    def test_create_view(self):
        "Create a materialized view"
        if self.db.version < 90300:
            self.skipTest('Only available on PG 9.3')
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c3': {'type': 'integer'}}]}})
        inmap['schema public'].update({'materialized view mv1': {
            'definition': "SELECT c1, c3 * 2 AS mc3 FROM t1"}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == "CREATE TABLE t1 (c1 integer, " \
            "c2 text, c3 integer)"
        assert fix_indent(sql[1]) == \
            "CREATE MATERIALIZED VIEW mv1 AS SELECT c1, c3 * 2 AS mc3 FROM t1"

    def test_bad_view_map(self):
        "Error creating a materialized view with a bad map"
        if self.db.version < 90300:
            self.skipTest('Only available on PG 9.3')
        inmap = self.std_map()
        inmap['schema public'].update({'mv1': {'definition': VIEW_DEFN}})
        with pytest.raises(KeyError):
            self.to_sql(inmap)

    def test_drop_view(self):
        "Drop an existing materialized view with table dependencies"
        if self.db.version < 90300:
            self.skipTest('Only available on PG 9.3')
        stmts = ["CREATE TABLE t1 (c1 INTEGER, c2 TEXT)",
                 "CREATE TABLE t2 (c1 INTEGER, c3 TEXT)",
                 "CREATE MATERIALIZED VIEW mv1 AS SELECT t1.c1, c2, c3 "
                 "FROM t1 JOIN t2 ON (t1.c1 = t2.c1)"]
        sql = self.to_sql(self.std_map(), stmts)
        assert sql[0] == "DROP MATERIALIZED VIEW mv1"
        # can't control which table will be dropped first
        drt1 = 1
        drt2 = 2
        if 't1' in sql[2]:
            drt1 = 2
            drt2 = 1
        assert sql[drt1] == "DROP TABLE t1"
        assert sql[drt2] == "DROP TABLE t2"

    def test_view_with_comment(self):
        "Create a materialized view with a comment"
        if self.db.version < 90300:
            self.skipTest('Only available on PG 9.3')
        inmap = self.std_map()
        inmap['schema public'].update({'materialized view mv1': {
            'definition': VIEW_STMT, 'description': "Test matview mv1"}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_STMT
        assert sql[1] == COMMENT_STMT

########NEW FILE########
__FILENAME__ = test_operator
# -*- coding: utf-8 -*-
"""Test operators"""

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

CREATE_STMT = "CREATE OPERATOR + (PROCEDURE = textcat, LEFTARG = text, " \
    "RIGHTARG = text)"
DROP_STMT = "DROP OPERATOR IF EXISTS +(text, text)"
COMMENT_STMT = "COMMENT ON OPERATOR +(text, text) IS 'Test operator +'"


class OperatorToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing operators"""

    def test_map_operator(self):
        "Map a simple operator"
        dbmap = self.to_map([CREATE_STMT])
        expmap = {'procedure': 'textcat'}
        assert dbmap['schema public']['operator +(text, text)'] == expmap

    def test_map_operator_rightarg(self):
        "Map a unitary operator with a right argument"
        stmts = ["CREATE OPERATOR + (PROCEDURE = upper, RIGHTARG = text)"]
        dbmap = self.to_map(stmts)
        if self.db.version < 90200:
            expmap = {'procedure': 'upper'}
        else:
            expmap = {'procedure': 'pg_catalog.upper'}
        assert dbmap['schema public']['operator +(NONE, text)'] == expmap

    def test_map_operator_commutator(self):
        "Map an operator with a commutator"
        stmts = ["CREATE OPERATOR && (PROCEDURE = int4pl, LEFTARG = integer, "
                 "RIGHTARG = integer, COMMUTATOR = OPERATOR(public.&&))"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['operator &&(integer, integer)'] == \
            {'procedure': 'int4pl', 'commutator': 'public.&&'}

    def test_map_operator_hash(self):
        "Map an operator with HASHES clause"
        stmts = ["CREATE OPERATOR + (PROCEDURE = texteq, LEFTARG = text, "
                 "RIGHTARG = text, HASHES)"]
        dbmap = self.to_map(stmts)
        expmap = {'procedure': 'texteq', 'hashes': True}
        assert dbmap['schema public']['operator +(text, text)'] == expmap

    def test_map_operator_comment(self):
        "Map a operator comment"
        dbmap = self.to_map([CREATE_STMT, COMMENT_STMT])
        assert dbmap['schema public']['operator +(text, text)'][
            'description'] == 'Test operator +'


class OperatorToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input operators"""

    def test_create_operator(self):
        "Create a simple operator"
        inmap = self.std_map()
        inmap['schema public'].update({'operator +(text, text)': {
            'procedure': 'textcat'}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_STMT

    def test_create_operator_rightarg(self):
        "Create a unitary operator with a right argument"
        inmap = self.std_map()
        inmap['schema public'].update({'operator +(NONE, text)': {
            'procedure': 'upper'}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == "CREATE OPERATOR + (" \
            "PROCEDURE = upper, RIGHTARG = text)"

    def test_create_operator_commutator(self):
        "Create an operator with a commutator"
        inmap = self.std_map()
        inmap['schema public'].update({'operator &&(integer, integer)': {
            'procedure': 'int4pl', 'commutator': 'public.&&'}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == "CREATE OPERATOR && (" \
            "PROCEDURE = int4pl, LEFTARG = integer, RIGHTARG = integer, " \
            "COMMUTATOR = OPERATOR(public.&&))"

    def test_create_operator_in_schema(self):
        "Create a operator within a non-public schema"
        inmap = self.std_map()
        inmap.update({'schema s1': {'operator +(text, text)': {
            'procedure': 'textcat'}}})
        sql = self.to_sql(inmap, ["CREATE SCHEMA s1"])
        assert fix_indent(sql[0]) == "CREATE OPERATOR s1.+ " \
            "(PROCEDURE = textcat, LEFTARG = text, RIGHTARG = text)"

    def test_drop_operator(self):
        "Drop an existing operator"
        sql = self.to_sql(self.std_map(), [CREATE_STMT])
        assert sql == ["DROP OPERATOR +(text, text)"]

    def test_operator_with_comment(self):
        "Create a operator with a comment"
        inmap = self.std_map()
        inmap['schema public'].update({
            'operator +(text, text)': {'description': 'Test operator +',
                                       'procedure': 'textcat'}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_STMT
        assert sql[1] == COMMENT_STMT

    def test_comment_on_operator(self):
        "Create a comment for an existing operator"
        inmap = self.std_map()
        inmap['schema public'].update({
            'operator +(text, text)': {'description': 'Test operator +',
                                       'procedure': 'textcat'}})
        sql = self.to_sql(inmap, [CREATE_STMT])
        assert sql == [COMMENT_STMT]

    def test_drop_operator_comment(self):
        "Drop a comment on an existing operator"
        stmts = [CREATE_STMT, COMMENT_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({
            'operator +(text, text)': {'returns': 'integer',
                                       'procedure': 'textcat'}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["COMMENT ON OPERATOR +(text, text) IS NULL"]

    def test_change_operator_comment(self):
        "Change existing comment on a operator"
        stmts = [CREATE_STMT, COMMENT_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({
            'operator +(text, text)': {'description': 'Changed operator +',
                                       'procedure': 'textcat'}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["COMMENT ON OPERATOR +(text, text) IS "
                       "'Changed operator +'"]

########NEW FILE########
__FILENAME__ = test_operclass
# -*- coding: utf-8 -*-
"""Test operator classes"""

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

CREATE_TYPE_STMT = """
CREATE TYPE myint;
CREATE FUNCTION myintin(cstring) RETURNS myint AS 'int4in' LANGUAGE internal;
CREATE FUNCTION myintout(myint) RETURNS cstring AS 'int4out' LANGUAGE internal;
CREATE TYPE myint (INPUT = myintin, OUTPUT = myintout);
CREATE FUNCTION myinteq(myint,myint) RETURNS boolean AS 'int4eq'
       LANGUAGE internal;
CREATE FUNCTION myintlt(myint,myint) RETURNS boolean AS 'int4lt'
       LANGUAGE internal;
CREATE OPERATOR < (PROCEDURE = myintlt, LEFTARG = myint, RIGHTARG = myint);
CREATE OPERATOR = (PROCEDURE = myinteq, LEFTARG = myint, RIGHTARG = myint);
CREATE FUNCTION btmyintcmp(myint,myint) RETURNS integer AS 'btint4cmp'
       LANGUAGE internal;
"""

CREATE_STMT = "CREATE OPERATOR CLASS oc1 FOR TYPE integer USING btree " \
    "AS OPERATOR 1 <, OPERATOR 3 =, FUNCTION 1 btint4cmp(integer,integer)"
CREATE_STMT_LONG = "CREATE OPERATOR CLASS oc1 FOR TYPE integer USING btree " \
    "AS OPERATOR 1 <(integer,integer), OPERATOR 3 =(integer,integer), " \
    "FUNCTION 1 btint4cmp(integer,integer)"
DROP_STMT = "DROP OPERATOR CLASS IF EXISTS oc1 USING btree"
COMMENT_STMT = "COMMENT ON OPERATOR CLASS oc1 USING btree IS " \
    "'Test operator class oc1'"


class OperatorClassToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing operator classes"""

    superuser = True

    def test_map_operclass(self):
        "Map an operator class"
        dbmap = self.to_map([CREATE_STMT])
        assert dbmap['schema public']['operator class oc1 using btree'] == \
            {'type': 'integer',
             'operators': {1: '<(integer,integer)', 3: '=(integer,integer)'},
             'functions': {1: 'btint4cmp(integer,integer)'}}

    def test_map_operclass_default(self):
        "Map a default operator class"
        stmts = [CREATE_TYPE_STMT,
                 CREATE_STMT.replace('integer', 'myint')
                 .replace('int4', 'myint')
                 .replace(' FOR ', ' DEFAULT FOR ')]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['operator class oc1 using btree'] == \
            {'type': 'myint', 'default': True,
             'operators': {1: '<(myint,myint)', 3: '=(myint,myint)'},
             'functions': {1: 'btmyintcmp(myint,myint)'}}

    def test_map_operclass_comment(self):
        "Map an operator class comment"
        dbmap = self.to_map([CREATE_STMT, COMMENT_STMT])
        assert dbmap['schema public']['operator class oc1 using btree'][
            'description'] == 'Test operator class oc1'


class OperatorClassToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input operators"""

    def test_create_operclass(self):
        "Create an operator class"
        inmap = self.std_map()
        inmap['schema public'].update({'operator class oc1 using btree': {
            'type': 'integer',
            'operators': {1: '<(integer,integer)', 3: '=(integer,integer)'},
            'functions': {1: 'btint4cmp(integer,integer)'}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_STMT_LONG

    def test_create_operclass_default(self):
        "Create a default operator class"
        inmap = self.std_map()
        inmap['schema public'].update({'operator class oc1 using btree': {
            'default': True, 'type': 'myint',
            'operators': {1: '<(myint,myint)', 3: '=(myint,myint)'},
            'functions': {1: 'btmyintcmp(myint,myint)'}}})
        sql = self.to_sql(inmap, [CREATE_TYPE_STMT], superuser=True)
        assert fix_indent(sql[0]) == CREATE_STMT_LONG.replace(
            ' FOR ', ' DEFAULT FOR ').replace('integer', 'myint').replace(
                'int4', 'myint')

    def test_create_operclass_in_schema(self):
        "Create a operator within a non-public schema"
        inmap = self.std_map()
        inmap.update({'schema s1': {'operator class oc1 using btree': {
            'type': 'integer',
            'operators': {1: '<(integer,integer)', 3: '=(integer,integer)'},
            'functions': {1: 'btint4cmp(integer,integer)'}}}})
        sql = self.to_sql(inmap, ["CREATE SCHEMA s1"])
        assert fix_indent(sql[0]) == "CREATE OPERATOR CLASS s1.oc1 FOR " \
            "TYPE integer USING btree AS OPERATOR 1 <(integer,integer), " \
            "OPERATOR 3 =(integer,integer), " \
            "FUNCTION 1 btint4cmp(integer,integer)"

    def test_drop_operclass(self):
        "Drop an existing operator"
        sql = self.to_sql(self.std_map(), [CREATE_STMT], superuser=True)
        assert sql == ["DROP OPERATOR CLASS oc1 USING btree",
                       "DROP OPERATOR FAMILY oc1 USING btree"]

    def test_operclass_with_comment(self):
        "Create an operator class with a comment"
        inmap = self.std_map()
        inmap['schema public'].update({'operator class oc1 using btree': {
            'description': 'Test operator class oc1', 'type': 'integer',
            'operators': {1: '<(integer,integer)', 3: '=(integer,integer)'},
            'functions': {1: 'btint4cmp(integer,integer)'}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_STMT_LONG
        assert sql[1] == COMMENT_STMT

    def test_comment_on_operclass(self):
        "Create a comment for an existing operator class"
        inmap = self.std_map()
        inmap['schema public'].update({'operator class oc1 using btree': {
            'description': 'Test operator class oc1', 'type': 'integer',
            'operators': {1: '<(integer,integer)', 3: '=(integer,integer)'},
            'functions': {1: 'btint4cmp(integer,integer)'}},
            'operator family oc1 using btree': {}})
        sql = self.to_sql(inmap, [CREATE_STMT], superuser=True)
        assert sql == [COMMENT_STMT]

    def test_drop_operclass_comment(self):
        "Drop the existing comment on an operator class"
        stmts = [CREATE_STMT, COMMENT_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'operator class oc1 using btree': {
            'type': 'integer',
            'operators': {1: '<(integer,integer)', 3: '=(integer,integer)'},
            'functions': {1: 'btint4cmp(integer,integer)'}},
            'operator family oc1 using btree': {}})
        sql = self.to_sql(inmap, stmts, superuser=True)
        assert sql == ["COMMENT ON OPERATOR CLASS oc1 USING btree IS NULL"]

    def test_change_operclass_comment(self):
        "Change existing comment on an operator class"
        stmts = [CREATE_STMT, COMMENT_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'operator class oc1 using btree': {
            'description': 'Changed operator class oc1', 'type': 'integer',
            'operators': {1: '<(integer,integer)', 3: '=(integer,integer)'},
            'functions': {1: 'btint4cmp(integer,integer)'}},
            'operator family oc1 using btree': {}})
        sql = self.to_sql(inmap, stmts, superuser=True)
        assert sql == ["COMMENT ON OPERATOR CLASS oc1 USING btree IS "
                       "'Changed operator class oc1'"]

########NEW FILE########
__FILENAME__ = test_operfamily
# -*- coding: utf-8 -*-
"""Test operator families"""

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

CREATE_STMT = "CREATE OPERATOR FAMILY of1 USING btree"
DROP_STMT = "DROP OPERATOR FAMILY IF EXISTS of1 USING btree"
COMMENT_STMT = "COMMENT ON OPERATOR FAMILY of1 USING btree IS " \
    "'Test operator family of1'"


class OperatorFamilyToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing operators"""

    superuser = True

    def test_map_operfam(self):
        "Map an operator family"
        dbmap = self.to_map([CREATE_STMT])
        assert dbmap['schema public']['operator family of1 using btree'] == {}

    def test_map_operfam_comment(self):
        "Map an operator family comment"
        dbmap = self.to_map([CREATE_STMT, COMMENT_STMT])
        assert dbmap['schema public']['operator family of1 using btree'][
            'description'] == 'Test operator family of1'


class OperatorFamilyToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input operators"""

    def test_create_operfam(self):
        "Create an operator family"
        inmap = self.std_map()
        inmap['schema public'].update({'operator family of1 using btree': {}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_STMT

    def test_create_operfam_in_schema(self):
        "Create an operator family within a non-public schema"
        inmap = self.std_map()
        inmap.update({'schema s1': {'operator family of1 using btree': {}}})
        sql = self.to_sql(inmap, ["CREATE SCHEMA s1"])
        assert fix_indent(sql[0]) == \
            "CREATE OPERATOR FAMILY s1.of1 USING btree"

    def test_drop_operfam(self):
        "Drop an existing operator family"
        sql = self.to_sql(self.std_map(), [CREATE_STMT], superuser=True)
        assert sql == ["DROP OPERATOR FAMILY of1 USING btree"]

    def test_operfam_with_comment(self):
        "Create an operator family with a comment"
        inmap = self.std_map()
        inmap['schema public'].update({'operator family of1 using btree': {
            'description': 'Test operator family of1'}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_STMT
        assert sql[1] == COMMENT_STMT

    def test_comment_on_operfam(self):
        "Create a comment for an existing operator family"
        inmap = self.std_map()
        inmap['schema public'].update({'operator family of1 using btree': {
            'description': 'Test operator family of1'}})
        sql = self.to_sql(inmap, [CREATE_STMT], superuser=True)
        assert sql == [COMMENT_STMT]

    def test_drop_operfam_comment(self):
        "Drop a comment on an existing operator family"
        stmts = [CREATE_STMT, COMMENT_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'operator family of1 using btree': {}})
        sql = self.to_sql(inmap, stmts, superuser=True)
        assert sql == ["COMMENT ON OPERATOR FAMILY of1 USING btree IS NULL"]

    def test_change_operfam_comment(self):
        "Change existing comment on a operator"
        stmts = [CREATE_STMT, COMMENT_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'operator family of1 using btree': {
            'description': 'Changed operator family of1'}})
        sql = self.to_sql(inmap, stmts, superuser=True)
        assert sql == ["COMMENT ON OPERATOR FAMILY of1 USING btree IS "
                       "'Changed operator family of1'"]

########NEW FILE########
__FILENAME__ = test_owner
# -*- coding: utf-8 -*-
"""Test object ownership

The majority of other tests exclude owner information.  These
explicitly request it.
"""

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

CREATE_TABLE = "CREATE TABLE t1 (c1 integer, c2 text)"
SOURCE1 = "SELECT 'dummy'::text"
SOURCE2 = "SELECT $1 * $2"
CREATE_FUNC = "CREATE FUNCTION f1() RETURNS text LANGUAGE sql IMMUTABLE AS " \
    "$_$%s$_$" % SOURCE1
CREATE_TYPE = "CREATE TYPE t1 AS (x integer, y integer)"


class OwnerToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of object owner information"""

    def test_map_type(self):
        "Map a composite type"
        dbmap = self.to_map([CREATE_TYPE], no_owner=False)
        expmap = {'attributes': [{'x': {'type': 'integer'}},
                                 {'y': {'type': 'integer'}}],
                  'owner': self.db.user}
        assert dbmap['schema public']['type t1'] == expmap

    def test_map_table(self):
        "Map a table"
        dbmap = self.to_map([CREATE_TABLE], no_owner=False)
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'text'}}],
                  'owner': self.db.user}
        assert dbmap['schema public']['table t1'] == expmap

    def test_map_function(self):
        "Map a function"
        dbmap = self.to_map([CREATE_FUNC], no_owner=False)
        expmap = {'language': 'sql', 'returns': 'text', 'owner': self.db.user,
                  'source': SOURCE1, 'volatility': 'immutable'}
        assert dbmap['schema public']['function f1()'] == expmap


class OwnerToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation of owner object information"""

    def test_create_type(self):
        "Create a composite type"
        inmap = self.std_map()
        inmap['schema public'].update({'type t1': {
            'attributes': [{'x': {'type': 'integer'}},
                           {'y': {'type': 'integer'}}],
            'owner': self.db.user}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_TYPE
        assert sql[1] == "ALTER TYPE t1 OWNER TO %s" % self.db.user

    def test_create_table(self):
        "Create a table"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'owner': self.db.user}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_TABLE
        assert sql[1] == "ALTER TABLE t1 OWNER TO %s" % self.db.user

    def test_create_function(self):
        "Create a function in a schema and a comment"
        inmap = self.std_map()
        inmap.update({'schema s1': {'function f1()': {
            'language': 'sql', 'returns': 'text', 'source': SOURCE1,
            'volatility': 'immutable', 'owner': self.db.user,
            'description': 'Test function'}}})
        sql = self.to_sql(inmap, ["CREATE SCHEMA s1"])
        # skip first two statements as those are tested elsewhere
        assert sql[2] == "ALTER FUNCTION s1.f1() OWNER TO %s" % self.db.user
        # to verify correct order of invocation of ownable and commentable
        assert sql[3] == "COMMENT ON FUNCTION s1.f1() IS 'Test function'"

    def test_create_function_default_args(self):
        "Create a function with default arguments"
        inmap = self.std_map()
        inmap['schema public'].update({
            'function f1(integer, INOUT integer)': {
                'allargs': 'integer, INOUT integer DEFAULT 1',
                'language': 'sql', 'returns': 'integer', 'source': SOURCE2,
                'owner': self.db.user}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[1]) == \
            "CREATE FUNCTION f1(integer, INOUT integer DEFAULT 1) " \
            "RETURNS integer LANGUAGE sql AS $_$%s$_$" % SOURCE2
        assert sql[2] == "ALTER FUNCTION f1(integer, INOUT integer) " \
            "OWNER TO %s" % self.db.user

    def test_change_table_owner(self):
        "Change the owner of a table"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'owner': 'someuser'}})
        sql = self.to_sql(inmap, [CREATE_TABLE])
        assert sql[0] == "ALTER TABLE t1 OWNER TO someuser"

########NEW FILE########
__FILENAME__ = test_privs
# -*- coding: utf-8 -*-
"""Test object privileges

The majority of other tests exclude access privileges.  These
explicitly request it.  In addition, the roles 'user1' and 'user2'
are created if they don't exist.
"""

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase

CREATE_TABLE = "CREATE TABLE t1 (c1 integer, c2 text)"
SOURCE1 = "SELECT 'dummy'::text"
CREATE_FUNC = "CREATE FUNCTION f1() RETURNS text LANGUAGE sql IMMUTABLE AS " \
    "$_$%s$_$" % SOURCE1
CREATE_FDW = "CREATE FOREIGN DATA WRAPPER fdw1"
CREATE_FS = "CREATE SERVER fs1 FOREIGN DATA WRAPPER fdw1"
GRANT_SELECT = "GRANT SELECT ON TABLE t1 TO %s"
GRANT_INSUPD = "GRANT INSERT, UPDATE ON TABLE t1 TO %s"


def check_extra_users(db):
    "Check existence of extra test users"
    for user in ['user1', 'user2']:
        row = db.fetchone("SELECT 1 FROM pg_roles WHERE rolname = %s", (user,))
        if row is None:
            db.execute_commit("CREATE ROLE %s" % user)


class PrivilegeToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of object privilege information"""

    def setUp(self):
        super(DatabaseToMapTestCase, self).setUp()
        check_extra_users(self.db)

    def test_map_schema(self):
        "Map a schema with some GRANTs"
        stmts = ["CREATE SCHEMA s1", "GRANT USAGE ON SCHEMA s1 TO PUBLIC",
                 "GRANT CREATE, USAGE ON SCHEMA s1 TO user1"]
        dbmap = self.to_map(stmts, no_privs=False)
        expmap = {'privileges': [{self.db.user: ['all']},
                                 {'PUBLIC': ['usage']}, {'user1': ['all']}]}
        assert dbmap['schema s1'] == expmap

    def test_map_table(self):
        "Map a table with various GRANTs"
        stmts = [CREATE_TABLE, GRANT_SELECT % 'PUBLIC', GRANT_INSUPD % 'user1',
                 "GRANT REFERENCES, TRIGGER ON t1 TO user2 WITH GRANT OPTION"]
        dbmap = self.to_map(stmts, no_privs=False)
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'text'}}],
                  'privileges': [{self.db.user: ['all']},
                                 {'PUBLIC': ['select']},
                                 {'user1': ['insert', 'update']},
                                 {'user2': [{'trigger': {'grantable': True}},
                                            {'references': {
                                                'grantable': True}}]}]}
        assert dbmap['schema public']['table t1'] == expmap

    def test_map_column(self):
        "Map a table with GRANTs on column"
        self.maxDiff = None
        stmts = [CREATE_TABLE, GRANT_SELECT % 'PUBLIC',
                 "GRANT INSERT (c1, c2) ON t1 TO user1",
                 "GRANT INSERT (c2), UPDATE (c2) ON t1 TO user2"]
        dbmap = self.to_map(stmts, no_privs=False)
        expmap = {'columns': [
            {'c1': {'type': 'integer', 'privileges': [{'user1': ['insert']}]}},
            {'c2': {'type': 'text', 'privileges': [
            {'user1': ['insert']}, {'user2': ['insert', 'update']}]}}],
            'privileges': [{self.db.user: ['all']}, {'PUBLIC': ['select']}]}
        assert dbmap['schema public']['table t1'] == expmap

    def test_map_sequence(self):
        "Map a sequence with various GRANTs"
        stmts = ["CREATE SEQUENCE seq1",
                 "GRANT SELECT ON SEQUENCE seq1 TO PUBLIC",
                 "GRANT USAGE, UPDATE ON SEQUENCE seq1 TO user1"]
        dbmap = self.to_map(stmts, no_privs=False)
        expmap = {'start_value': 1, 'increment_by': 1, 'max_value': None,
                  'min_value': None, 'cache_value': 1,
                  'privileges': [{self.db.user: ['all']},
                                 {'PUBLIC': ['select']},
                                 {'user1': ['usage', 'update']}]}
        assert dbmap['schema public']['sequence seq1'] == expmap

    def test_map_view(self):
        "Map a view with various GRANTs"
        stmts = ["CREATE VIEW v1 AS SELECT now()::date AS today",
                 "GRANT SELECT ON v1 TO PUBLIC",
                 "GRANT REFERENCES ON v1 TO user1"]
        dbmap = self.to_map(stmts, no_privs=False)
        expmap = {'definition': " SELECT now()::date AS today;",
                  'privileges': [{self.db.user: ['all']},
                                 {'PUBLIC': ['select']},
                                 {'user1': ['references']}]}
        assert dbmap['schema public']['view v1'] == expmap

    def test_map_function(self):
        "Map a function with a GRANT and REVOKE from PUBLIC"
        stmts = [CREATE_FUNC, "REVOKE ALL ON FUNCTION f1() FROM PUBLIC",
                 "GRANT EXECUTE ON FUNCTION f1() TO user1"]
        dbmap = self.to_map(stmts, no_privs=False)
        expmap = {'language': 'sql', 'returns': 'text',
                  'source': SOURCE1, 'volatility': 'immutable',
                  'privileges': [{self.db.user: ['execute']},
                                 {'user1': ['execute']}]}
        assert dbmap['schema public']['function f1()'] == expmap

    def test_map_language(self):
        "Map a  language but REVOKE default privilege"
        if self.db.version >= 90100:
            self.skipTest('Only available before PG 9.1')
        stmts = ["DROP LANGUAGE IF EXISTS plperl CASCADE",
                 "CREATE LANGUAGE plperl",
                 "REVOKE USAGE ON LANGUAGE plperl FROM PUBLIC"]
        dbmap = self.to_map(stmts, no_privs=False)
        self.db.execute_commit("DROP LANGUAGE plperl")
        expmap = {'trusted': True, 'privileges': [{self.db.user: ['usage']}]}
        assert dbmap['language plperl'] == expmap

    def test_map_fd_wrapper(self):
        "Map a foreign data wrapper with a GRANT"
        stmts = [CREATE_FDW,
                 "GRANT USAGE ON FOREIGN DATA WRAPPER fdw1 TO PUBLIC"]
        dbmap = self.to_map(stmts, no_privs=False, superuser=True)
        expmap = {'privileges': [{self.db.user: ['usage']},
                                 {'PUBLIC': ['usage']}]}
        assert dbmap['foreign data wrapper fdw1'] == expmap

    def test_map_server(self):
        "Map a foreign server with a GRANT"
        stmts = [CREATE_FDW, CREATE_FS,
                 "GRANT USAGE ON FOREIGN SERVER fs1 TO user1"]
        dbmap = self.to_map(stmts, no_privs=False, superuser=True)
        expmap = {'privileges': [{self.db.user: ['usage']},
                                 {'user1': ['usage']}]}
        assert dbmap['foreign data wrapper fdw1']['server fs1'] == expmap

    def test_map_foreign_table(self):
        "Map a foreign table with various GRANTs"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        stmts = [CREATE_FDW, CREATE_FS,
                 "CREATE FOREIGN TABLE ft1 (c1 integer, c2 text) SERVER fs1",
                 "GRANT SELECT ON ft1 TO PUBLIC",
                 "GRANT INSERT, UPDATE ON ft1 TO user1"]
        dbmap = self.to_map(stmts, no_privs=False, superuser=True)
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'text'}}], 'server': 'fs1',
                  'privileges': [{self.db.user: ['all']},
                                 {'PUBLIC': ['select']},
                                 {'user1': ['insert', 'update']}]}
        assert dbmap['schema public']['foreign table ft1'] == expmap


class PrivilegeToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation of privilege information (GRANTs)"""

    def setUp(self):
        super(InputMapToSqlTestCase, self).setUp()
        check_extra_users(self.db)

    def test_create_schema(self):
        "Create a schema with various privileges"
        inmap = self.std_map()
        inmap.update({'schema s1': {
            'owner': self.db.user, 'privileges': [{
            self.db.user: ['all']}, {'PUBLIC': ['usage', 'create']}]}})
        sql = self.to_sql(inmap)
        # sql[0] = CREATE SCHEMA
        # sql[1] = ALTER SCHEMA OWNER
        assert sql[2] == "GRANT ALL ON SCHEMA s1 TO %s" % self.db.user
        assert sql[3] == "GRANT ALL ON SCHEMA s1 TO PUBLIC"

    def test_schema_new_grant(self):
        "Grant privileges on an existing schema"
        inmap = self.std_map()
        inmap.update({'schema s1': {
            'owner': self.db.user, 'privileges': [{self.db.user: ['all']},
                                                  {'PUBLIC': ['create']}]}})
        sql = sorted(self.to_sql(inmap, ["CREATE SCHEMA s1"]))
        assert len(sql) == 2
        assert sql[0] == "GRANT ALL ON SCHEMA s1 TO %s" % self.db.user
        assert sql[1] == "GRANT CREATE ON SCHEMA s1 TO PUBLIC"

    def test_create_table(self):
        "Create a table with various privileges"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'owner': self.db.user,
            'privileges': [{self.db.user: ['all']}, {'PUBLIC': ['select']},
                           {'user1': ['insert', 'update']},
                           {'user2': [{'trigger': {'grantable': True}},
                                      {'references': {'grantable': True}}]}]}})
        sql = self.to_sql(inmap)
        # sql[0] = CREATE TABLE
        # sql[1] = ALTER TABLE OWNER
        assert sql[2] == "GRANT ALL ON TABLE t1 TO %s" % self.db.user
        assert sql[3] == GRANT_SELECT % 'PUBLIC'
        assert sql[4] == GRANT_INSUPD % 'user1'
        assert sql[5] == "GRANT TRIGGER, REFERENCES ON TABLE t1 " \
            "TO user2 WITH GRANT OPTION"

    def test_create_column_grants(self):
        "Create a table with colum-level privileges"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer', 'privileges': [{'user1': [
            'insert']}]}}, {'c2': {'type': 'text', 'privileges': [{'user1': [
                'insert']}, {'user2': ['insert', 'update']}]}}],
            'owner': self.db.user, 'privileges': [{self.db.user: ['all']},
                                                  {'PUBLIC': ['select']}]}})
        sql = self.to_sql(inmap)
        assert len(sql) == 7
        # sql[0] = CREATE TABLE
        # sql[1] = ALTER TABLE OWNER
        assert sql[2] == "GRANT ALL ON TABLE t1 TO %s" % self.db.user
        assert sql[3] == GRANT_SELECT % 'PUBLIC'
        assert sql[4] == "GRANT INSERT (c1) ON TABLE t1 TO user1"
        assert sql[5] == "GRANT INSERT (c2) ON TABLE t1 TO user1"
        assert sql[6] == "GRANT INSERT (c2), UPDATE (c2) ON TABLE t1 TO user2"

    def test_table_new_grant(self):
        "Grant select privileges on an existing table"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'owner': self.db.user,
            'privileges': [{self.db.user: ['all']}, {'user1': ['select']}]}})
        sql = self.to_sql(inmap, [CREATE_TABLE])
        assert len(sql) == 2
        sql = sorted(sql)
        assert sql[0] == "GRANT ALL ON TABLE t1 TO %s" % self.db.user
        assert sql[1] == GRANT_SELECT % 'user1'

    def test_table_change_grant(self):
        "Grant select privileges on an existing table"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'owner': self.db.user,
            'privileges': [{self.db.user: ['all']}, {'PUBLIC': ['select']},
                           {'user1': ['insert', 'update']}]}})
        sql = self.to_sql(inmap, [CREATE_TABLE, GRANT_SELECT % 'user1'])
        assert len(sql) == 3
        assert sorted(sql) == [GRANT_INSUPD % 'user1', GRANT_SELECT % 'PUBLIC',
                               "REVOKE SELECT ON TABLE t1 FROM user1"]

    def test_column_change_grants(self):
        "Change existing colum-level privileges"
        inmap = self.std_map()
        inmap['schema public'].update(
            {'table t1': {'columns': [{'c1': {
                'type': 'integer', 'privileges': [{
                'user1': ['insert']}, {'user2': ['insert', 'update']}]}},
            {'c2': {'type': 'text', 'privileges': [{'user1': ['insert']}]}}],
                'owner': self.db.user, 'privileges': [{self.db.user: ['all']},
                {'PUBLIC': ['select']}]}})
        stmts = [CREATE_TABLE, GRANT_SELECT % 'PUBLIC',
                 "GRANT INSERT (c1, c2) ON t1 TO user1",
                 "GRANT INSERT (c2), UPDATE (c2) ON t1 TO user2"]
        sql = self.to_sql(inmap, stmts)
        assert len(sql) == 2
        assert sql[0] == "GRANT INSERT (c1), UPDATE (c1) ON TABLE t1 TO user2"
        assert sql[1] == "REVOKE INSERT (c2), UPDATE (c2) ON TABLE t1 " \
            "FROM user2"

    def test_table_revoke_all(self):
        "Revoke all privileges on an existing table"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'owner': self.db.user}})
        stmts = [CREATE_TABLE, GRANT_SELECT % 'PUBLIC', GRANT_INSUPD % 'user1']
        sql = sorted(self.to_sql(inmap, stmts))
        assert len(sql) == 3
        assert sql[0] == "REVOKE ALL ON TABLE t1 FROM %s" % self.db.user
        assert sql[1] == "REVOKE INSERT, UPDATE ON TABLE t1 FROM user1"
        assert sql[2] == "REVOKE SELECT ON TABLE t1 FROM PUBLIC"

    def test_create_sequence(self):
        "Create a sequence with some privileges"
        inmap = self.std_map()
        inmap['schema public'].update({'sequence seq1': {
            'start_value': 1, 'increment_by': 1, 'max_value': None,
            'min_value': None, 'cache_value': 1, 'owner': self.db.user,
            'privileges': [{self.db.user: ['all']}, {'PUBLIC': ['select']}]}})
        sql = self.to_sql(inmap)
        # sql[0] = CREATE SEQUENCE
        # sql[1] = ALTER SEQUENCE OWNER
        assert sql[2] == "GRANT ALL ON SEQUENCE seq1 TO %s" % self.db.user
        assert sql[3] == "GRANT SELECT ON SEQUENCE seq1 TO PUBLIC"

    def test_sequence_new_grant(self):
        "Grant privileges on an existing sequence"
        inmap = self.std_map()
        inmap['schema public'].update({'sequence seq1': {
            'start_value': 1, 'increment_by': 1, 'max_value': None,
            'min_value': None, 'cache_value': 1, 'owner': self.db.user,
            'privileges': [{self.db.user: ['all']}, {'PUBLIC': ['select']}]}})
        sql = sorted(self.to_sql(inmap, ["CREATE SEQUENCE seq1"]))
        assert len(sql) == 2
        assert sql[0] == "GRANT ALL ON SEQUENCE seq1 TO %s" % self.db.user
        assert sql[1] == "GRANT SELECT ON SEQUENCE seq1 TO PUBLIC"

    def test_create_view(self):
        "Create a view with some privileges"
        inmap = self.std_map()
        inmap['schema public'].update({'view v1': {
            'definition': " SELECT now()::date AS today;",
            'owner': self.db.user,
            'privileges': [{self.db.user: ['all']}, {'user1': ['select']}]}})
        sql = self.to_sql(inmap)
        # sql[0] = CREATE VIEW
        # sql[1] = ALTER VIEW OWNER
        assert sql[2] == "GRANT ALL ON TABLE v1 TO %s" % self.db.user
        assert sql[3] == "GRANT SELECT ON TABLE v1 TO user1"

    def test_view_new_grant(self):
        "Grant privileges on an existing view"
        inmap = self.std_map()
        inmap['schema public'].update({'view v1': {
            'definition': " SELECT now()::date AS today;",
            'owner': self.db.user,
            'privileges': [{self.db.user: ['all']}, {'user1': ['select']}]}})
        sql = sorted(self.to_sql(inmap, ["CREATE VIEW v1 AS "
                                         "SELECT now()::date AS today"]))
        assert len(sql) == 2
        assert sql[0] == "GRANT ALL ON TABLE v1 TO %s" % self.db.user
        assert sql[1] == "GRANT SELECT ON TABLE v1 TO user1"

    def test_create_function(self):
        "Create a function with some privileges"
        inmap = self.std_map()
        inmap['schema public'].update({'function f1()': {
            'language': 'sql', 'returns': 'text', 'source': SOURCE1,
            'volatility': 'immutable', 'owner': self.db.user,
            'privileges': [{self.db.user: ['all']}, {'PUBLIC': ['execute']}]}})
        sql = self.to_sql(inmap)
        # sql[0] = SET check_function_bodies
        # sql[1] = CREATE FUNCTION
        # sql[2] = ALTER FUNCTION OWNER
        assert sql[3] == "GRANT EXECUTE ON FUNCTION f1() TO %s" % self.db.user
        assert sql[4] == "GRANT EXECUTE ON FUNCTION f1() TO PUBLIC"

    def test_function_new_grant(self):
        "Grant privileges on an existing function"
        inmap = self.std_map()
        inmap['schema public'].update({'function f1()': {
            'language': 'sql', 'returns': 'text', 'source': SOURCE1,
            'volatility': 'immutable', 'owner': self.db.user,
            'privileges': [{self.db.user: ['all']}, {'PUBLIC': ['execute']}]}})
        sql = self.to_sql(inmap, [CREATE_FUNC])
        assert len(sql) == 2
        sql = sorted(sql)
        # assumes self.db.user > PUBLIC
        assert sql[0] == "GRANT EXECUTE ON FUNCTION f1() TO PUBLIC"
        assert sql[1] == "GRANT EXECUTE ON FUNCTION f1() TO %s" % self.db.user

    def test_create_fd_wrapper(self):
        "Create a foreign data wrapper with some privileges"
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {
            'owner': self.db.user,
            'privileges': [{self.db.user: ['all']}, {'PUBLIC': ['usage']}]}})
        sql = self.to_sql(inmap)
        # sql[0] = CREATE FDW
        # sql[1] = ALTER FDW OWNER
        assert sql[2] == "GRANT USAGE ON FOREIGN DATA WRAPPER fdw1 " \
            "TO %s" % self.db.user
        assert sql[3] == "GRANT USAGE ON FOREIGN DATA WRAPPER fdw1 TO PUBLIC"

    def test_fd_wrapper_new_grant(self):
        "Grant privileges on an existing foreign data wrapper"
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {
            'owner': self.db.user,
            'privileges': [{self.db.user: ['all']}, {'PUBLIC': ['usage']}]}})
        sql = sorted(self.to_sql(inmap, [CREATE_FDW], superuser=True))
        assert len(sql) == 2
        # assumes self.db.user > PUBLIC
        assert sql[0] == "GRANT USAGE ON FOREIGN DATA WRAPPER fdw1 TO PUBLIC"
        assert sql[1] == "GRANT USAGE ON FOREIGN DATA WRAPPER fdw1 " \
            "TO %s" % self.db.user

    def test_create_server(self):
        "Create a foreign server with some privileges"
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'server fs1': {
            'owner': self.db.user,
            'privileges': [{self.db.user: ['all']}, {'user2': ['usage']}]}}})
        sql = self.to_sql(inmap, [CREATE_FDW], superuser=True)
        # sql[0] = CREATE SERVER
        # sql[1] = ALTER SERVER OWNER
        assert sql[2] == "GRANT USAGE ON FOREIGN SERVER fs1 TO %s" % \
            self.db.user
        assert sql[3] == "GRANT USAGE ON FOREIGN SERVER fs1 TO user2"

    def test_server_new_grant(self):
        "Grant privileges on an existing foreign server"
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'server fs1': {
            'owner': self.db.user,
            'privileges': [{self.db.user: ['all']}, {'user2': ['usage']}]}}})
        sql = sorted(self.to_sql(inmap, [CREATE_FDW, CREATE_FS],
                                 superuser=True))
        assert len(sql) == 2
        assert sql[0] == "GRANT USAGE ON FOREIGN SERVER fs1 TO %s" % \
            self.db.user
        assert sql[1] == "GRANT USAGE ON FOREIGN SERVER fs1 TO user2"

    def test_create_foreign_table(self):
        "Create a foreign table with some privileges"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'server fs1': {}}})
        inmap['schema public'].update({'foreign table ft1': {
            'columns': [{'c1': {'type': 'integer'}},
                        {'c2': {'type': 'text'}}], 'server': 'fs1',
            'owner': self.db.user,
            'privileges': [{self.db.user: ['all']}, {'PUBLIC': ['select']},
                           {'user1': ['insert', 'update']}]}})
        sql = self.to_sql(inmap, [CREATE_FDW, CREATE_FS], superuser=True)
        # sql[0] = CREATE TABLE
        # sql[1] = ALTER TABLE OWNER
        assert sql[2] == "GRANT ALL ON TABLE ft1 TO %s" % self.db.user
        assert sql[3] == "GRANT SELECT ON TABLE ft1 TO PUBLIC"
        assert sql[4] == "GRANT INSERT, UPDATE ON TABLE ft1 TO user1"

    def test_foreign_table_new_grant(self):
        "Grant privileges on an existing foreign table"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        inmap = self.std_map()
        inmap.update({'foreign data wrapper fdw1': {'server fs1': {}}})
        inmap['schema public'].update({'foreign table ft1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'server': 'fs1', 'owner': self.db.user,
            'privileges': [{self.db.user: ['all']}, {'PUBLIC': ['select']},
                           {'user1': ['insert', 'update']}]}})
        sql = sorted(self.to_sql(inmap, [
            CREATE_FDW, CREATE_FS,
            "CREATE FOREIGN TABLE ft1 (c1 integer, c2 text) SERVER fs1"],
            superuser=True))
        assert len(sql) == 3
        assert sql[0] == "GRANT ALL ON TABLE ft1 TO %s" % self.db.user
        assert sql[1] == "GRANT INSERT, UPDATE ON TABLE ft1 TO user1"
        assert sql[2] == "GRANT SELECT ON TABLE ft1 TO PUBLIC"

########NEW FILE########
__FILENAME__ = test_rule
# -*- coding: utf-8 -*-
"""Test rules"""

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

CREATE_TABLE_STMT = "CREATE TABLE t1 (c1 integer, c2 text)"
CREATE_STMT = "CREATE RULE r1 AS ON %s TO t1 DO %s"
COMMENT_STMT = "COMMENT ON RULE r1 ON t1 IS 'Test rule r1'"


class RuleToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing rules"""

    def test_map_rule_nothing(self):
        "Map a rule to do nothing"
        stmts = [CREATE_TABLE_STMT, CREATE_STMT % ('INSERT', 'NOTHING')]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1']['rules'] == \
            {'r1': {'event': 'insert', 'actions': 'NOTHING'}}

    def test_map_rule_instead(self):
        "Map rule with an INSTEAD action"
        stmts = [CREATE_TABLE_STMT,
                 CREATE_STMT % ('UPDATE', 'INSTEAD NOTHING')]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1']['rules'] == \
            {'r1': {'event': 'update', 'instead': True, 'actions': 'NOTHING'}}

    def test_map_rule_conditional(self):
        "Map rule with a qualification"
        stmts = [CREATE_TABLE_STMT,
                 "CREATE RULE r1 AS ON DELETE TO t1 WHERE OLD.c1 < 1000 "
                 "DO INSERT INTO t1 VALUES (OLD.c1 + 1000, OLD.c2)"]
        dbmap = self.to_map(stmts)
        fmt = "%s%s" if (self.db.version < 90300) else " %s\n %s"
        action = fmt % ("INSERT INTO t1 (c1, c2)",
                        " VALUES ((old.c1 + 1000), old.c2)")
        expmap = {'r1': {'event': 'delete', 'condition': "(old.c1 < 1000)",
                         'actions': action}}
        assert dbmap['schema public']['table t1']['rules'] == expmap

    def test_map_rule_multi_actions(self):
        "Map rule with multiple actions"
        stmts = [CREATE_TABLE_STMT, CREATE_STMT % (
            'UPDATE', "(INSERT INTO t1 (c1) VALUES (old.c1 + 100); "
            "INSERT INTO t1 (c1) VALUES (old.c1 + 200))")]
        dbmap = self.to_map(stmts)
        ins = "INSERT INTO t1 (c1)"
        if (self.db.version < 90300):
            fmt = "(%s%s %s%s )"
        else:
            fmt = "( %s\n %s\n %s\n %s\n)"
        actions = fmt % (ins, " VALUES ((old.c1 + 100));",
                         ins, " VALUES ((old.c1 + 200));")
        expmap = {'r1': {'event': 'update', 'actions': actions}}
        assert dbmap['schema public']['table t1']['rules'] == expmap

    def test_map_rule_comment(self):
        "Map a rule comment"
        stmts = [CREATE_TABLE_STMT, CREATE_STMT % ('INSERT', 'NOTHING'),
                 COMMENT_STMT]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1']['rules']['r1'][
            'description'] == 'Test rule r1'


class RuleToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input rules"""

    def test_create_rule_nothing(self):
        "Create a rule"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'rules': {'r1': {'event': 'insert', 'actions': 'NOTHING'}}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_TABLE_STMT
        assert fix_indent(sql[1]) == CREATE_STMT % ('INSERT', 'NOTHING')

    def test_create_rule_instead(self):
        "Create a rule with an INSTEAD action"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'rules': {'r1': {'event': 'update', 'instead': True,
                             'actions': 'NOTHING'}}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_TABLE_STMT
        assert fix_indent(sql[1]) == "CREATE RULE r1 AS ON UPDATE TO t1 " \
            "DO INSTEAD NOTHING"

    def test_create_rule_conditional(self):
        "Create a rule with qualification"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'rules': {'r1': {'event': 'delete',
                             'condition': "old.c1 < 1000",
                             'actions': "INSERT INTO t1 VALUES ("
                             "old.c1 + 1000, old.c2)"}}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_TABLE_STMT
        assert fix_indent(sql[1]) == "CREATE RULE r1 AS ON DELETE TO t1 " \
            "WHERE old.c1 < 1000 DO INSERT INTO t1 VALUES " \
            "(old.c1 + 1000, old.c2)"

    def test_create_rule_multi_actions(self):
        "Create a rule with multiple actions"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'rules': {'r1': {'event': 'update', 'actions':
                             "(INSERT INTO t1 VALUES (old.c1 + 100); "
                             "INSERT INTO t1 VALUES (old.c1 + 200));)"}}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_TABLE_STMT
        assert fix_indent(sql[1]) == "CREATE RULE r1 AS ON UPDATE " \
            "TO t1 DO (INSERT INTO t1 VALUES (old.c1 + 100); " \
            "INSERT INTO t1 VALUES (old.c1 + 200));)"

    def test_create_rule_in_schema(self):
        "Create a rule within a non-public schema"
        inmap = self.std_map()
        inmap.update({'schema s1': {
            'table t1': {'columns': [{'c1': {'type': 'integer'}},
                                     {'c2': {'type': 'text'}}],
                         'rules': {'r1': {'event': 'insert',
                                          'actions': 'NOTHING'}}}}})
        sql = self.to_sql(inmap, ["CREATE SCHEMA s1"])
        assert fix_indent(sql[1]) == "CREATE RULE r1 AS ON INSERT TO s1.t1 " \
            "DO NOTHING"

    def test_drop_rule(self):
        "Drop an existing rule"
        stmts = [CREATE_TABLE_STMT, CREATE_STMT % ('INSERT', 'NOTHING')]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}},
                        {'c2': {'type': 'text'}}]}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["DROP RULE r1 ON t1"]

    def test_drop_rule_table(self):
        "Drop an existing rule and the related table"
        stmts = [CREATE_TABLE_STMT, CREATE_STMT % ('INSERT', 'NOTHING')]
        sql = self.to_sql(self.std_map(), stmts)
        assert sql[0] == "DROP RULE r1 ON t1"
        assert sql[1] == "DROP TABLE t1"

    def test_rule_with_comment(self):
        "Create a rule with a comment"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'rules': {'r1': {'event': 'insert', 'description': 'Test rule r1',
                             'actions': 'NOTHING'}}}})
        sql = self.to_sql(inmap)
        assert sql[2] == COMMENT_STMT

    def test_comment_on_rule(self):
        "Create a comment on an existing rule"
        stmts = [CREATE_TABLE_STMT, CREATE_STMT % ('INSERT', 'NOTHING')]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'rules': {'r1': {'event': 'insert', 'description': 'Test rule r1',
                             'actions': 'NOTHING'}}}})
        sql = self.to_sql(inmap, stmts)
        assert sql == [COMMENT_STMT]

    def test_drop_rule_comment(self):
        "Drop the comment on an existing rule"
        stmts = [CREATE_TABLE_STMT, CREATE_STMT % ('INSERT', 'NOTHING'),
                 COMMENT_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'rules': {'r1': {'event': 'insert', 'actions': 'NOTHING'}}}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["COMMENT ON RULE r1 ON t1 IS NULL"]

    def test_change_rule_comment(self):
        "Change existing comment on a rule"
        stmts = [CREATE_TABLE_STMT, CREATE_STMT % ('INSERT', 'NOTHING'),
                 COMMENT_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'rules': {'r1': {'event': 'insert',
                             'description': 'Changed rule r1',
                             'actions': 'NOTHING'}}}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["COMMENT ON RULE r1 ON t1 IS 'Changed rule r1'"]

########NEW FILE########
__FILENAME__ = test_schema
# -*- coding: utf-8 -*-
"""Test schemas"""

import pytest

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase

CREATE_STMT = "CREATE SCHEMA s1"
COMMENT_STMT = "COMMENT ON SCHEMA s1 IS 'Test schema s1'"


class SchemaToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of created schemas"""

    def test_map_schema(self):
        "Map a created schema"
        dbmap = self.to_map([CREATE_STMT])
        assert dbmap['schema s1'] == {}

    def test_map_schema_comment(self):
        "Map a schema comment"
        dbmap = self.to_map([CREATE_STMT, COMMENT_STMT])
        assert dbmap['schema s1'] == {'description': 'Test schema s1'}

    def test_map_select_schema(self):
        "Map a single schema when three schemas exist"
        stmts = [CREATE_STMT, "CREATE SCHEMA s2", "CREATE SCHEMA s3"]
        dbmap = self.to_map(stmts, schemas=['s2'])
        assert 'schema s1' not in dbmap
        assert dbmap['schema s2'] == {}
        assert 'schema s3' not in dbmap


class SchemaToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input schemas"""

    _schmap = {'schema s1': {'description': 'Test schema s1'}}

    def test_create_schema(self):
        "Create a schema that didn't exist"
        inmap = self.std_map()
        inmap.update({'schema s1': {}})
        sql = self.to_sql(inmap)
        assert sql == [CREATE_STMT]

    def test_bad_schema_map(self):
        "Error creating a schema with a bad map"
        with pytest.raises(KeyError):
            self.to_sql({'s1': {}})

    def test_drop_schema(self):
        "Drop an existing schema"
        sql = self.to_sql(self.std_map(), [CREATE_STMT])
        assert sql == ["DROP SCHEMA s1"]

    def test_rename_schema(self):
        "Rename an existing schema"
        inmap = self.std_map()
        inmap.update({'schema s2': {'oldname': 's1'}})
        sql = self.to_sql(inmap, [CREATE_STMT])
        assert sql == ["ALTER SCHEMA s1 RENAME TO s2"]

    def test_bad_rename_schema(self):
        "Error renaming a non-existing schema"
        inmap = self.std_map()
        inmap.update({'schema s2': {'oldname': 's3'}})
        with pytest.raises(KeyError):
            self.to_sql(inmap, [CREATE_STMT])

    def test_schema_with_comment(self):
        "Create a schema with a comment"
        inmap = self.std_map()
        inmap.update(self._schmap)
        sql = self.to_sql(inmap)
        assert sql == [CREATE_STMT, COMMENT_STMT]

    def test_comment_on_schema(self):
        "Create a comment for an existing schema"
        inmap = self.std_map()
        inmap.update(self._schmap)
        sql = self.to_sql(inmap, [CREATE_STMT])
        assert sql == [COMMENT_STMT]

    def test_drop_schema_comment(self):
        "Drop a comment on an existing schema"
        inmap = self.std_map()
        inmap.update({'schema s1': {}})
        stmts = [CREATE_STMT, COMMENT_STMT]
        sql = self.to_sql(inmap, stmts)
        assert sql == ["COMMENT ON SCHEMA s1 IS NULL"]

    def test_change_schema_comment(self):
        "Change existing comment on a schema"
        inmap = self.std_map()
        inmap.update({'schema s1': {'description': 'Changed schema s1'}})
        stmts = [CREATE_STMT, COMMENT_STMT]
        sql = self.to_sql(inmap, stmts)
        assert sql == ["COMMENT ON SCHEMA s1 IS 'Changed schema s1'"]

class SchemaUndoSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation to revert schemas"""

    _schmap = {'schema s1': {'description': 'Test schema s1'}}

    def test_undo_create_schema(self):
        "Revert a schema creation"
        inmap = self.std_map()
        inmap.update({'schema s1': {}})
        sql = self.to_sql(inmap, revert=True)
        assert sql == ["DROP SCHEMA s1"]

    def test_undo_drop_schema(self):
        "Revert dropping a schema"
        sql = self.to_sql(self.std_map(), [CREATE_STMT], revert=True)
        assert sql[0] == CREATE_STMT

    def test_undo_comment_on_schema(self):
        "Revert creating comment on a schema"
        inmap = self.std_map()
        inmap.update(self._schmap)
        sql = self.to_sql(inmap, [CREATE_STMT], revert=True)
        assert sql == ["COMMENT ON SCHEMA s1 IS NULL"]

    def test_undo_drop_schema_comment(self):
        "Revert dropping comment on a schema"
        inmap = self.std_map()
        inmap.update({'schema s1': {}})
        stmts = [CREATE_STMT, COMMENT_STMT]
        sql = self.to_sql(inmap, stmts, revert=True)
        assert sql == [COMMENT_STMT]

########NEW FILE########
__FILENAME__ = test_sequence
# -*- coding: utf-8 -*-
"""Test sequences"""

import pytest

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

CREATE_STMT = "CREATE SEQUENCE seq1"
CREATE_STMT_FULL = "CREATE SEQUENCE seq1 START WITH 1 INCREMENT BY 1 " \
    "NO MAXVALUE NO MINVALUE CACHE 1"
COMMENT_STMT = "COMMENT ON SEQUENCE seq1 IS 'Test sequence seq1'"


class SequenceToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of created sequences"""

    def test_map_sequence(self):
        "Map a created sequence"
        dbmap = self.to_map([CREATE_STMT])
        expmap = {'start_value': 1, 'increment_by': 1, 'max_value': None,
                  'min_value': None, 'cache_value': 1}
        assert dbmap['schema public']['sequence seq1'] == expmap

    def test_map_sequence_comment(self):
        "Map a sequence with a comment"
        dbmap = self.to_map([CREATE_STMT, COMMENT_STMT])
        assert dbmap['schema public']['sequence seq1']['description'] == \
            'Test sequence seq1'


class SequenceToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input sequences"""

    def test_create_sequence(self):
        "Create a sequence"
        inmap = self.std_map()
        inmap['schema public'].update({'sequence seq1': {
            'start_value': 1, 'increment_by': 1, 'max_value': None,
            'min_value': None, 'cache_value': 1}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_STMT_FULL

    def test_create_sequence_in_schema(self):
        "Create a sequence within a non-public schema"
        inmap = self.std_map()
        inmap.update({'schema s1': {'sequence seq1': {
            'start_value': 1, 'increment_by': 1, 'max_value': None,
            'min_value': None, 'cache_value': 1}}})
        sql = self.to_sql(inmap, ["CREATE SCHEMA s1"])
        assert fix_indent(sql[0]) == "CREATE SEQUENCE s1.seq1 START WITH 1 " \
            "INCREMENT BY 1 NO MAXVALUE NO MINVALUE CACHE 1"

    def test_bad_sequence_map(self):
        "Error creating a sequence with a bad map"
        inmap = self.std_map()
        inmap['schema public'].update({'seq1': {
            'start_value': 1, 'increment_by': 1, 'max_value': None,
            'min_value': None, 'cache_value': 1}})
        with pytest.raises(KeyError):
            self.to_sql(inmap)

    def test_drop_sequence(self):
        "Drop an existing sequence"
        sql = self.to_sql(self.std_map(), [CREATE_STMT])
        assert sql == ["DROP SEQUENCE seq1"]

    def test_rename_sequence(self):
        "Rename an existing sequence"
        inmap = self.std_map()
        inmap['schema public'].update({'sequence seq2': {
            'oldname': 'seq1', 'start_value': 1, 'increment_by': 1,
            'max_value': None, 'min_value': None, 'cache_value': 1}})
        sql = self.to_sql(inmap, [CREATE_STMT])
        assert sql == ["ALTER SEQUENCE seq1 RENAME TO seq2"]

    def test_bad_rename_sequence(self):
        "Error renaming a non-existing sequence"
        inmap = self.std_map()
        inmap['schema public'].update({'sequence seq2': {
            'oldname': 'seq3', 'start_value': 1, 'increment_by': 1,
            'max_value': None, 'min_value': None, 'cache_value': 1}})
        with pytest.raises(KeyError):
            self.to_sql(inmap, [CREATE_STMT])

    def test_change_sequence(self):
        "Change sequence attributes"
        inmap = self.std_map()
        inmap['schema public'].update({'sequence seq1': {
            'start_value': 5, 'increment_by': 10, 'max_value': None,
            'min_value': None, 'cache_value': 30}})
        sql = self.to_sql(inmap, [CREATE_STMT])
        assert fix_indent(sql[0]) == "ALTER SEQUENCE seq1 START WITH 5 " \
            "INCREMENT BY 10 CACHE 30"

    def test_sequence_with_comment(self):
        "Create a sequence with a comment"
        inmap = self.std_map()
        inmap['schema public'].update({'sequence seq1': {
            'start_value': 1, 'increment_by': 1, 'max_value': None,
            'min_value': None, 'cache_value': 1,
            'description': "Test sequence seq1"}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_STMT_FULL
        assert sql[1] == COMMENT_STMT

    def test_comment_on_sequence(self):
        "Create a comment for an existing sequence"
        inmap = self.std_map()
        inmap['schema public'].update({'sequence seq1': {
            'start_value': 1, 'increment_by': 1, 'max_value': None,
            'min_value': None, 'cache_value': 1,
            'description': "Test sequence seq1"}})
        sql = self.to_sql(inmap, [CREATE_STMT])
        assert sql == [COMMENT_STMT]

    def test_drop_sequence_comment(self):
        "Drop the comment on an existing sequence"
        stmts = [CREATE_STMT, COMMENT_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'sequence seq1': {
            'start_value': 1, 'increment_by': 1, 'max_value': None,
            'min_value': None, 'cache_value': 1}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["COMMENT ON SEQUENCE seq1 IS NULL"]

    def test_change_sequence_comment(self):
        "Change existing comment on a sequence"
        stmts = [CREATE_STMT, COMMENT_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'sequence seq1': {
            'start_value': 1, 'increment_by': 1, 'max_value': None,
            'min_value': None, 'cache_value': 1,
            'description': "Changed sequence seq1"}})
        sql = self.to_sql(inmap, stmts)
        assert sql, ["COMMENT ON SEQUENCE seq1 IS 'Changed sequence seq1'"]


class SequenceUndoSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation to revert sequences"""

    def test_undo_create_sequence(self):
        "Revert a sequence creation"
        inmap = self.std_map()
        inmap['schema public'].update({'sequence seq1': {
            'start_value': 1, 'increment_by': 1, 'max_value': None,
            'min_value': None, 'cache_value': 1}})
        sql = self.to_sql(inmap, revert=True)
        assert sql == ["DROP SEQUENCE seq1"]

    def test_undo_create_sequence_in_schema(self):
        "Revert creating a sequence in a non-public schema"
        inmap = self.std_map()
        inmap.update({'schema s1': {'sequence seq1': {
            'start_value': 1, 'increment_by': 1, 'max_value': None,
            'min_value': None, 'cache_value': 1}}})
        sql = self.to_sql(inmap, ["CREATE SCHEMA s1"], revert=True)
        assert sql == ["DROP SEQUENCE s1.seq1"]

    def test_undo_drop_sequence(self):
        "Revert dropping a sequence"
        sql = self.to_sql(self.std_map(), [CREATE_STMT], revert=True)
        assert fix_indent(sql[0]) == CREATE_STMT_FULL

    def test_undo_change_sequence(self):
        "Revert changing sequence attributes"
        inmap = self.std_map()
        inmap['schema public'].update({'sequence seq1': {
            'start_value': 5, 'increment_by': 10, 'max_value': None,
            'min_value': None, 'cache_value': 30}})
        sql = self.to_sql(inmap, [CREATE_STMT], revert=True)
        assert fix_indent(sql[0]) == "ALTER SEQUENCE seq1 START WITH 1 " \
            "INCREMENT BY 1 NO MAXVALUE NO MINVALUE CACHE 1"

########NEW FILE########
__FILENAME__ = test_static
# -*- coding: utf-8 -*-
"""Test loading of data from and into static tables"""
import os

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase

CREATE_STMT = "CREATE TABLE t1 (c1 integer, c2 text)"
FILE_PATH = 'table.t1.data'
TABLE_DATA = [(1, 'abc'), (2, 'def'), (3, 'ghi')]
TABLE_DATA2 = [(1, 'abc', 'row 1'), (3, 'ghi', 'row 2'), (2, 'def', 'row 3'),
               (3, 'def', 'row 4')]


class StaticTableToMapTestCase(DatabaseToMapTestCase):
    """Test mapping and copying out of created tables"""

    def tearDown(self):
        self.remove_tempfiles()

    def test_copy_static_table(self):
        "Copy a two-column table to a file"
        self.db.execute(CREATE_STMT)
        for row in TABLE_DATA:
            self.db.execute("INSERT INTO t1 VALUES (%s, %s)", row)
        cfg = {'datacopy': {'schema public': ['t1']}}
        dbmap = self.to_map([], config=cfg)
        assert dbmap['schema public']['table t1'] == {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}]}
        recs = []
        with open(os.path.join(self.cfg['files']['data_path'],
                               "schema.public", FILE_PATH)) as f:
            for line in f:
                (c1, c2) = line.split(',')
                recs.append((int(c1), c2.rstrip()))
        assert recs == TABLE_DATA

    def test_copy_static_table_pk(self):
        "Copy a table that has a primary key"
        self.db.execute("CREATE TABLE t1 (c1 integer, c2 char(3), c3 text,"
                        "PRIMARY KEY (c2, c1))")
        for row in TABLE_DATA2:
            self.db.execute("INSERT INTO t1 VALUES (%s, %s, %s)", row)
        cfg = {'datacopy': {'schema public': ['t1']}}
        dbmap = self.to_map([], config=cfg)
        assert dbmap['schema public']['table t1'] == {
            'columns': [{'c1': {'type': 'integer', 'not_null': True}},
                        {'c2': {'type': 'character(3)', 'not_null': True}},
                        {'c3': {'type': 'text'}}],
            'primary_key': {'t1_pkey': {'columns': ['c2', 'c1']}}}
        recs = []
        with open(os.path.join(self.cfg['files']['data_path'],
                               "schema.public", FILE_PATH)) as f:
            for line in f:
                (c1, c2, c3) = line.split(',')
                recs.append((int(c1), c2, c3.rstrip()))
        assert recs == sorted(TABLE_DATA2)


class StaticTableToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation of table load statements"""

    def test_load_static_table(self):
        "Truncate and load a two-column table"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}},
                        {'c2': {'type': 'text'}}]}})
        cfg = {'datacopy': {'schema public': ['t1']}}
        sql = self.to_sql(inmap, [CREATE_STMT], config=cfg)
        copy_stmt = ("\\copy ", 't1', " from '",
                     os.path.join(self.cfg['files']['data_path'],
                                  "schema.public", FILE_PATH), "' csv")
        assert sql[0] == "TRUNCATE ONLY t1"
        assert sql[1] == copy_stmt

    def test_load_static_table_fk(self):
        "Truncate and load a table which has a foreign key dependency"
        stmts = ["CREATE TABLE t1 (pc1 integer PRIMARY KEY, pc2 text)",
                 "CREATE TABLE t2 (c1 integer, c2 integer REFERENCES t1, "
                 "c3 text)"]
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'pc1': {'type': 'integer', 'not_null': True}},
                        {'pc2': {'type': 'text'}}],
            'primary_key': {'t1_pkey': {'columns': ['pc1']}}}, 'table t2': {
                'columns': [{'c1': {'type': 'integer'}},
                            {'c2': {'type': 'integer'}},
                            {'c3': {'type': 'text'}}],
                'foreign_keys': {'t2_c2_fkey': {'columns': ['c2'],
                'references': {'schema': 'public', 'table': 't1',
                               'columns': ['pc1']}}}}})
        cfg = {'datacopy': {'schema public': ['t1']}}
        sql = self.to_sql(inmap, stmts, config=cfg)
        copy_stmt = ("\\copy ", 't1', " from '",
                     os.path.join(self.cfg['files']['data_path'],
                                  "schema.public", FILE_PATH), "' csv")
        assert sql[0] == "ALTER TABLE t2 DROP CONSTRAINT t2_c2_fkey"
        assert sql[1] == "TRUNCATE ONLY t1"
        assert sql[2] == copy_stmt
        assert sql[3] == "ALTER TABLE t2 ADD CONSTRAINT t2_c2_fkey " \
            "FOREIGN KEY (c2) REFERENCES t1 (pc1)"

########NEW FILE########
__FILENAME__ = test_table
# -*- coding: utf-8 -*-
"""Test tables"""

import pytest

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

CREATE_STMT = "CREATE TABLE t1 (c1 integer, c2 text)"
COMMENT_STMT = "COMMENT ON TABLE t1 IS 'Test table t1'"
CREATE_STOR_PARAMS = CREATE_STMT + \
    " WITH (fillfactor=90, autovacuum_enabled=false)"


class TableToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of created tables"""

    def test_create_table(self):
        "Map a table with two columns"
        dbmap = self.to_map([CREATE_STMT])
        assert dbmap['schema public']['table t1'] == {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}]}

    def test_map_table_comment(self):
        "Map a table comment"
        dbmap = self.to_map([CREATE_STMT, COMMENT_STMT])
        assert dbmap['schema public']['table t1'] == {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'description': 'Test table t1'}

    def test_map_table_comment_quotes(self):
        "Map a table comment with quotes"
        stmts = [CREATE_STMT, "COMMENT ON TABLE t1 IS "
                 "'A \"special\" person''s table t1'"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1'] == {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'description': "A \"special\" person's table t1"}

    def test_map_column_comments(self):
        "Map two column comments"
        stmts = [CREATE_STMT,
                 "COMMENT ON COLUMN t1.c1 IS 'Test column c1 of t1'",
                 "COMMENT ON COLUMN t1.c2 IS 'Test column c2 of t1'"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1'] == {
            'columns': [{'c1': {'type': 'integer',
                                'description': 'Test column c1 of t1'}},
                        {'c2': {'type': 'text',
                                'description': 'Test column c2 of t1'}}]}

    def test_map_table_options(self):
        "Map a table with options"
        dbmap = self.to_map([CREATE_STOR_PARAMS])
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'text'}}],
                  'options': ["fillfactor=90", 'autovacuum_enabled=false']}
        assert dbmap['schema public']['table t1'] == expmap

    def test_map_inherit(self):
        "Map a table that inherits from two other tables"
        stmts = [CREATE_STMT, "CREATE TABLE t2 (c3 integer)",
                 "CREATE TABLE t3 (c4 text) INHERITS (t1, t2)"]
        dbmap = self.to_map(stmts)
        expmap = {'columns': [{'c1': {'type': 'integer', 'inherited': True}},
                              {'c2': {'type': 'text', 'inherited': True}},
                              {'c3': {'type': 'integer', 'inherited': True}},
                              {'c4': {'type': 'text'}}],
                  'inherits': ['t1', 't2']}
        assert dbmap['schema public']['table t3'] == expmap

    def test_unlogged_table(self):
        "Map an unlogged table"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        dbmap = self.to_map(["CREATE UNLOGGED TABLE t1 (c1 integer, c2 text)"])
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'text'}}],
                  'unlogged': True}
        assert dbmap['schema public']['table t1'] == expmap

    def test_map_table_within_schema(self):
        "Map a schema and a table within it"
        stmts = ["CREATE SCHEMA s1",
                 "CREATE TABLE s1.t1 (c1 INTEGER, c2 TEXT)"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema s1'] == {
            'table t1': {'columns': [{'c1': {'type': 'integer'}},
                                     {'c2': {'type': 'text'}}]}}

    def test_map_table_quoted(self):
        "Map a schema and a table both of which need to be quoted"
        stmts = ['CREATE SCHEMA "a schema"',
                 'CREATE TABLE "a schema"."The.Table" ("column" SERIAL, '
                 'c2 TEXT)']
        dbmap = self.to_map(stmts)
        assert dbmap['schema a schema']['table The.Table'] == {
            'columns': [{'column': {'type': 'integer', 'not_null': True,
            'default':
            'nextval(\'"a schema"."The.Table_column_seq"\'::regclass)'}},
            {'c2': {'type': 'text'}}]}

    def test_map_select_tables(self):
        "Map two tables out of three present"
        stmts = [CREATE_STMT, "CREATE TABLE t2 (c1 integer, c2 text)",
                 "CREATE TABLE t3 (c1 integer, c2 text)"]
        dbmap = self.to_map(stmts, tables=['t2', 't1'])
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'text'}}]}
        assert dbmap['schema public']['table t1'] == expmap
        assert dbmap['schema public']['table t2'] == expmap
        assert 'table t3' not in dbmap['schema public']

    def test_map_table_sequence(self):
        "Map sequence if owned by a table"
        stmts = [CREATE_STMT, "CREATE TABLE t2 (c1 integer, c2 text)",
                 "CREATE SEQUENCE seq1", "ALTER SEQUENCE seq1 OWNED BY t2.c1",
                 "CREATE SEQUENCE seq2"]
        dbmap = self.to_map(stmts, tables=['t2'])
        self.db.execute_commit("DROP SEQUENCE seq1")
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'text'}}]}
        assert 'table t1' not in dbmap['schema public']
        assert dbmap['schema public']['table t2'] == expmap
        assert 'sequence seq1' in dbmap['schema public']
        assert not 'sequence seq2' in dbmap['schema public']


class TableToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation of table statements from input schemas"""

    def test_create_table(self):
        "Create a two-column table"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}},
                        {'c2': {'type': 'text'}}]}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_STMT

    def test_create_table_quoted_idents(self):
        "Create a table needing quoted identifiers"
        inmap = self.std_map()
        inmap['schema public'].update({'table order': {
            'columns': [{'primary': {'type': 'integer'}},
                        {'two words': {'type': 'text'}}]}})
        sql = self.to_sql(inmap, quote_reserved=True)
        assert fix_indent(sql[0]) == 'CREATE TABLE "order" (' \
            '"primary" integer, "two words" text)'

    def test_bad_table_map(self):
        "Error creating a table with a bad map"
        inmap = self.std_map()
        inmap['schema public'].update({'t1': {
            'columns': [{'c1': {'type': 'integer'}},
                        {'c2': {'type': 'text'}}]}})
        with pytest.raises(KeyError):
            self.to_sql(inmap)

    def test_missing_columns(self):
        "Error creating a table with no columns"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {'columns': []}})
        with pytest.raises(ValueError):
            self.to_sql(inmap)

    def test_drop_table(self):
        "Drop an existing table"
        sql = self.to_sql(self.std_map(), [CREATE_STMT])
        assert sql == ["DROP TABLE t1"]

    def test_rename_table(self):
        "Rename an existing table"
        inmap = self.std_map()
        inmap['schema public'].update({'table t2': {
            'oldname': 't1',
            'columns': [{'c1': {'type': 'integer'}},
                        {'c2': {'type': 'text'}}]}})
        sql = self.to_sql(inmap, [CREATE_STMT])
        assert sql == ["ALTER TABLE t1 RENAME TO t2"]

    def test_create_table_options(self):
        "Create a table with options"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}},
                        {'c2': {'type': 'text'}}],
            'options': ["fillfactor=90", "autovacuum_enabled=false"]}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_STOR_PARAMS

    def test_change_table_options(self):
        "Change a table's storage parameters"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}},
                        {'c2': {'type': 'text'}}],
            'options': ["fillfactor=70"]}})
        sql = self.to_sql(inmap, [CREATE_STOR_PARAMS])
        assert fix_indent(sql[0]) == "ALTER TABLE t1 SET (fillfactor=70), " \
            "RESET (autovacuum_enabled)"

    def test_create_table_within_schema(self):
        "Create a new schema and a table within it"
        inmap = self.std_map()
        inmap.update({'schema s1': {'table t1': {
            'columns': [{'c1': {'type': 'integer'}},
                        {'c2': {'type': 'text'}}]}}})
        sql = self.to_sql(inmap)
        expsql = ["CREATE SCHEMA s1",
                  "CREATE TABLE s1.t1 (c1 integer, c2 text)"]
        for i in range(len(expsql)):
            assert fix_indent(sql[i]) == expsql[i]

    def test_unlogged_table(self):
        "Create an unlogged table"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}},
                        {'c2': {'type': 'text'}}], 'unlogged': True}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == \
            "CREATE UNLOGGED TABLE t1 (c1 integer, c2 text)"


class TableCommentToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation of table and column COMMENT statements"""

    def _tblmap(self):
        "Return a table input map with a comment"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'description': 'Test table t1',
            'columns': [{'c1': {'type': 'integer'}},
                        {'c2': {'type': 'text'}}]}})
        return inmap

    def test_table_with_comment(self):
        "Create a table with a comment"
        sql = self.to_sql(self._tblmap())
        assert fix_indent(sql[0]) == CREATE_STMT
        assert sql[1] == COMMENT_STMT

    def test_comment_on_table(self):
        "Create a comment for an existing table"
        sql = self.to_sql(self._tblmap(), [CREATE_STMT])
        assert sql == [COMMENT_STMT]

    def test_table_comment_quotes(self):
        "Create a table comment with quotes"
        inmap = self._tblmap()
        inmap['schema public']['table t1']['description'] = \
            "A \"special\" person's table t1"
        sql = self.to_sql(inmap, [CREATE_STMT])
        assert sql == ["COMMENT ON TABLE t1 IS "
                       "'A \"special\" person''s table t1'"]

    def test_drop_table_comment(self):
        "Drop a comment on an existing table"
        inmap = self._tblmap()
        del inmap['schema public']['table t1']['description']
        sql = self.to_sql(inmap, [CREATE_STMT, COMMENT_STMT])
        assert sql == ["COMMENT ON TABLE t1 IS NULL"]

    def test_change_table_comment(self):
        "Change existing comment on a table"
        inmap = self._tblmap()
        inmap['schema public']['table t1'].update(
            {'description': 'Changed table t1'})
        sql = self.to_sql(inmap, [CREATE_STMT, COMMENT_STMT])
        assert sql == ["COMMENT ON TABLE t1 IS 'Changed table t1'"]

    def test_create_column_comments(self):
        "Create a table with column comments"
        inmap = self._tblmap()
        inmap['schema public']['table t1']['columns'][0]['c1'].update(
            description='Test column c1')
        inmap['schema public']['table t1']['columns'][1]['c2'].update(
            description='Test column c2')
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_STMT
        assert sql[1] == COMMENT_STMT
        assert sql[2] == "COMMENT ON COLUMN t1.c1 IS 'Test column c1'"
        assert sql[3] == "COMMENT ON COLUMN t1.c2 IS 'Test column c2'"

    def test_add_column_comment(self):
        "Add a column comment to an existing table"
        inmap = self._tblmap()
        inmap['schema public']['table t1']['columns'][0]['c1'].update(
            description='Test column c1')
        sql = self.to_sql(inmap, [CREATE_STMT, COMMENT_STMT])
        assert sql[0] == "COMMENT ON COLUMN t1.c1 IS 'Test column c1'"

    def test_add_column_with_comment(self):
        "Add a commented column to an existing table"
        inmap = self._tblmap()
        inmap['schema public']['table t1']['columns'].append({'c3': {
            'description': 'Test column c3', 'type': 'integer'}})
        sql = self.to_sql(inmap, [CREATE_STMT, COMMENT_STMT])
        assert fix_indent(sql[0]) == "ALTER TABLE t1 ADD COLUMN c3 integer"
        assert sql[1] == "COMMENT ON COLUMN t1.c3 IS 'Test column c3'"

    def test_drop_column_comment(self):
        "Drop a column comment on an existing table"
        stmts = [CREATE_STMT, COMMENT_STMT,
                 "COMMENT ON COLUMN t1.c1 IS 'Test column c1'"]
        sql = self.to_sql(self._tblmap(), stmts)
        assert sql[0] == "COMMENT ON COLUMN t1.c1 IS NULL"

    def test_change_column_comment(self):
        "Add a column comment to an existing table"
        inmap = self._tblmap()
        inmap['schema public']['table t1']['columns'][0]['c1'].update(
            description='Changed column c1')
        sql = self.to_sql(inmap, [CREATE_STMT, COMMENT_STMT])
        assert sql[0] == "COMMENT ON COLUMN t1.c1 IS 'Changed column c1'"


class TableInheritToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation of table inheritance statements"""

    def test_table_inheritance(self):
        "Create a table that inherits from another"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}},
                        {'c2': {'type': 'text'}}]}})
        inmap['schema public'].update({'table t2': {
            'columns': [{'c1': {'type': 'integer', 'inherited': True}},
                        {'c2': {'type': 'text', 'inherited': True}},
                        {'c3': {'type': 'numeric'}}], 'inherits': ['t1']}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_STMT
        assert fix_indent(sql[1]) == "CREATE TABLE t2 (c3 numeric) " \
            "INHERITS (t1)"

    def test_drop_inherited(self):
        "Drop tables that inherit from others"
        stmts = [CREATE_STMT, "CREATE TABLE t2 (c3 numeric) INHERITS (t1)",
                 "CREATE TABLE t3 (c4 date) INHERITS (t2)"]
        sql = self.to_sql(self.std_map(), stmts)
        assert sql == ["DROP TABLE t3", "DROP TABLE t2", "DROP TABLE t1"]

########NEW FILE########
__FILENAME__ = test_tablespace
# -*- coding: utf-8 -*-
"""Test tablespaces

These tests require the existence of tablespaces ts1 and ts2.
They should be owned by the user running the tests or the user should
have been granted CREATE (or ALL) privileges on the tablespaces.
"""

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent


CREATE_TABLE = "CREATE TABLE t1 (c1 integer, c2 text) TABLESPACE ts1"
CREATE_PRIM_KEY = "CREATE TABLE t1 (c1 integer PRIMARY KEY " \
    "USING INDEX TABLESPACE ts1, c2 text)"


class ToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of created tables"""

    def test_map_table(self):
        "Map a table using a tablespace"
        dbmap = self.to_map([CREATE_TABLE])
        expmap = {'columns': [{'c1': {'type': 'integer'}},
                              {'c2': {'type': 'text'}}],
                  'tablespace': 'ts1'}
        assert dbmap['schema public']['table t1'] == expmap

    def test_map_primary_key(self):
        "Map a table with a PRIMARY KEY using a tablespace"
        dbmap = self.to_map([CREATE_PRIM_KEY])
        expmap = {'columns': [{'c1': {'type': 'integer', 'not_null': True}},
                              {'c2': {'type': 'text'}}],
                  'primary_key': {'t1_pkey': {'columns': ['c1'],
                                              'tablespace': 'ts1'}}}
        assert dbmap['schema public']['table t1'] == expmap

    def test_map_index(self):
        "Map an index using a tablespace"
        dbmap = self.to_map(["CREATE TABLE t1 (c1 integer, c2 text)",
                             "CREATE UNIQUE INDEX t1_idx ON t1 (c1) "
                             "TABLESPACE ts1"])
        expmap = {'t1_idx': {'keys': ['c1'], 'tablespace': 'ts1',
                             'unique': True}}
        assert dbmap['schema public']['table t1']['indexes'] == expmap


class ToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation of table statements from input schemas"""

    def test_create_table(self):
        "Create a table in a tablespace"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'tablespace': 'ts1'}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_TABLE

    def test_move_table(self):
        "Move a table from one tablespace to another"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'tablespace': 'ts2'}})
        sql = self.to_sql(inmap, [CREATE_TABLE])
        assert fix_indent(sql[0]) == "ALTER TABLE t1 SET TABLESPACE ts2"

    def test_create_primary_key(self):
        "Create a table with a PRIMARY KEY in a different tablespace"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer', 'not_null': True}},
                        {'c2': {'type': 'text'}}],
            'primary_key': {'t1_pkey': {'columns': ['c1'],
                                        'tablespace': 'ts2'}},
            'tablespace': 'ts1'}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == "CREATE TABLE t1 " \
            "(c1 integer NOT NULL, c2 text) TABLESPACE ts1"
        assert fix_indent(sql[1]) == "ALTER TABLE t1 ADD CONSTRAINT " \
            "t1_pkey PRIMARY KEY (c1) USING INDEX TABLESPACE ts2"

    def test_create_index(self):
        "Create an index using a tablespace"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'indexes': {'t1_idx': {'keys': ['c1'], 'tablespace': 'ts2',
                                   'unique': True}},
            'tablespace': 'ts1'}})
        sql = self.to_sql(inmap, [CREATE_TABLE])
        assert fix_indent(sql[0]) == "CREATE UNIQUE INDEX t1_idx " \
            "ON t1 (c1) TABLESPACE ts2"

    def test_move_index(self):
        "Move a index from one tablespace to another"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}}],
            'indexes': {'t1_idx': {'keys': ['c1'], 'tablespace': 'ts2'}}}})
        stmts = ["CREATE TABLE t1 (c1 integer, c2 text)",
                 "CREATE INDEX t1_idx ON t1 (c1) TABLESPACE ts1"]
        sql = self.to_sql(inmap, stmts)
        assert fix_indent(sql[0]) == "ALTER INDEX t1_idx SET TABLESPACE ts2"

########NEW FILE########
__FILENAME__ = test_textsearch
# -*- coding: utf-8 -*-
"""Test text search objects"""

import pytest

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

CREATE_TSC_STMT = "CREATE TEXT SEARCH CONFIGURATION tsc1 (PARSER = tsp1)"
CREATE_TSD_STMT = "CREATE TEXT SEARCH DICTIONARY tsd1 (TEMPLATE = simple, " \
    "stopwords = 'english')"
CREATE_TSP_STMT = "CREATE TEXT SEARCH PARSER tsp1 (START = prsd_start, " \
    "GETTOKEN = prsd_nexttoken, END = prsd_end, LEXTYPES = prsd_lextype, " \
    "HEADLINE = prsd_headline)"
CREATE_TST_STMT = "CREATE TEXT SEARCH TEMPLATE tst1 (INIT = dsimple_init, " \
    "LEXIZE = dsimple_lexize)"
DROP_TSC_STMT = "DROP TEXT SEARCH CONFIGURATION IF EXISTS tsc1"
DROP_TSD_STMT = "DROP TEXT SEARCH DICTIONARY IF EXISTS tsd1"
DROP_TSP_STMT = "DROP TEXT SEARCH PARSER IF EXISTS tsp1 CASCADE"
DROP_TST_STMT = "DROP TEXT SEARCH TEMPLATE IF EXISTS tst1"
COMMENT_TSC_STMT = "COMMENT ON TEXT SEARCH CONFIGURATION tsc1 IS " \
    "'Test configuration tsc1'"
COMMENT_TSD_STMT = "COMMENT ON TEXT SEARCH DICTIONARY tsd1 IS " \
    "'Test dictionary tsd1'"
COMMENT_TSP_STMT = "COMMENT ON TEXT SEARCH PARSER tsp1 IS 'Test parser tsp1'"
COMMENT_TST_STMT = "COMMENT ON TEXT SEARCH TEMPLATE tst1 IS " \
    "'Test template tst1'"


class TextSearchConfigToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing text search configurations"""

    superuser = True

    def tearDown(self):
        if self.db.is_superuser():
            self.db.execute(DROP_TSC_STMT)
            self.db.execute_commit(DROP_TSP_STMT)
        else:
            self.db.execute_commit(DROP_TSC_STMT)
        self.db.close()

    def test_map_ts_config(self):
        "Map an existing text search configuration"
        stmts = [DROP_TSC_STMT, DROP_TSP_STMT, CREATE_TSP_STMT,
                 CREATE_TSC_STMT]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['text search configuration tsc1'] == {
            'parser': 'tsp1'}

    def test_map_cross_schema_ts_config(self):
        "Map a text search config with parser in different schema"
        stmts = ["CREATE SCHEMA s1",
                 "CREATE TEXT SEARCH PARSER s1.tsp1 "
                 "(START = prsd_start, GETTOKEN = prsd_nexttoken, "
                 "END = prsd_end, LEXTYPES = prsd_lextype)", DROP_TSC_STMT,
                 "CREATE TEXT SEARCH CONFIGURATION tsc1 (PARSER = s1.tsp1)"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['text search configuration tsc1'] == {
            'parser': 's1.tsp1'}

    def test_map_ts_config_comment(self):
        "Map a text search configuration with a comment"
        stmts = [DROP_TSC_STMT, DROP_TSP_STMT, CREATE_TSP_STMT,
                 CREATE_TSC_STMT, COMMENT_TSC_STMT]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['text search configuration tsc1'][
            'description'] == 'Test configuration tsc1'


class TextSearchConfigToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation for input text search configurations"""

    def tearDown(self):
        if self.db.is_superuser():
            self.db.execute(DROP_TSC_STMT)
            self.db.execute_commit(DROP_TSP_STMT)
        else:
            self.db.execute_commit(DROP_TSC_STMT)
        self.db.close()

    def test_create_ts_config(self):
        "Create a text search configuration that didn't exist"
        inmap = self.std_map()
        inmap['schema public'].update({'text search configuration tsc1': {
            'parser': 'tsp1'}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_TSC_STMT

    def test_create_ts_config_in_schema(self):
        "Create a text search config with parser in non-public schema"
        inmap = self.std_map()
        inmap.update({'schema s1': {'text search parser tsp1': {
            'start': 'prsd_start', 'gettoken': 'prsd_nexttoken',
            'end': 'prsd_end', 'lextypes': 'prsd_lextype'}}})
        inmap['schema public'].update({'text search configuration tsc1': {
            'parser': 's1.tsp1'}})
        sql = self.to_sql(inmap, ["CREATE SCHEMA s1"])
        assert fix_indent(sql[0]) == "CREATE TEXT SEARCH PARSER s1.tsp1 " \
            "(START = prsd_start, GETTOKEN = prsd_nexttoken, " \
            "END = prsd_end, LEXTYPES = prsd_lextype)"
        assert fix_indent(sql[1]) == \
            "CREATE TEXT SEARCH CONFIGURATION tsc1 (PARSER = s1.tsp1)"

    def test_bad_map_ts_config_(self):
        "Error creating a text search configuration with a bad map"
        inmap = self.std_map()
        inmap['schema public'].update({'tsc1': {'parser': 'tsp1'}})
        with pytest.raises(KeyError):
            self.to_sql(inmap)

    def test_drop_ts_config(self):
        "Drop an existing text search configuration"
        stmts = [CREATE_TSP_STMT, CREATE_TSC_STMT]
        sql = self.to_sql(self.std_map(), stmts, superuser=True)
        assert sql[0] == "DROP TEXT SEARCH PARSER tsp1"
        assert sql[1] == "DROP TEXT SEARCH CONFIGURATION tsc1"

    def test_comment_on_ts_config(self):
        "Create a comment for an existing text search configuration"
        stmts = [CREATE_TSP_STMT, CREATE_TSC_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'text search configuration tsc1': {
            'parser': 'tsp1', 'description': "Test configuration tsc1"},
            'text search parser tsp1': {
                'start': 'prsd_start', 'gettoken': 'prsd_nexttoken',
                'end': 'prsd_end', 'lextypes': 'prsd_lextype',
                'headline': 'prsd_headline'}})
        sql = self.to_sql(inmap, stmts, superuser=True)
        assert sql == [COMMENT_TSC_STMT]


class TextSearchDictToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing text search dictionaries"""

    def tearDown(self):
        self.db.execute_commit(DROP_TSD_STMT)
        self.db.close()

    def test_map_ts_dict(self):
        "Map an existing text search dictionary"
        dbmap = self.to_map([DROP_TSD_STMT, CREATE_TSD_STMT])
        assert dbmap['schema public']['text search dictionary tsd1'] == {
            'template': 'simple', 'options': "stopwords = 'english'"}

    def test_map_ts_dict_comment(self):
        "Map a text search dictionary with a comment"
        stmts = [DROP_TSD_STMT, CREATE_TSD_STMT, COMMENT_TSD_STMT]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['text search dictionary tsd1'][
            'description'], 'Test dictionary tsd1'


class TextSearchDictToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation for input text search dictionaries"""

    def tearDown(self):
        self.db.execute_commit(DROP_TSD_STMT)
        self.db.close()

    def test_create_ts_dict(self):
        "Create a text search dictionary that didn't exist"
        inmap = self.std_map()
        inmap['schema public'].update({'text search dictionary tsd1': {
            'template': 'simple', 'options': "stopwords = 'english'"}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_TSD_STMT

    def test_bad_map_ts_dict(self):
        "Error creating a text search dictionary with a bad map"
        inmap = self.std_map()
        inmap['schema public'].update({'tsd1': {
            'template': 'simple', 'options': "stopwords = 'english'"}})
        with pytest.raises(KeyError):
            self.to_sql(inmap)

    def test_drop_ts_dict(self):
        "Drop an existing text search dictionary"
        sql = self.to_sql(self.std_map(), [CREATE_TSD_STMT])
        assert sql == ["DROP TEXT SEARCH DICTIONARY tsd1"]

    def test_comment_on_ts_dict(self):
        "Create a comment for an existing text search dictionary"
        inmap = self.std_map()
        inmap['schema public'].update({'text search dictionary tsd1': {
            'template': 'simple', 'options': "stopwords = 'english'",
            'description': "Test dictionary tsd1"}})
        sql = self.to_sql(inmap, [CREATE_TSD_STMT])
        assert sql == [COMMENT_TSD_STMT]


class TextSearchParserToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing text search parsers"""

    superuser = True

    def tearDown(self):
        if self.db.is_superuser():
            self.db.execute_commit(DROP_TSP_STMT)
            self.db.close()

    def test_map_ts_parser(self):
        "Map an existing text search parser"
        stmts = [DROP_TSP_STMT, CREATE_TSP_STMT]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['text search parser tsp1'] == {
            'start': 'prsd_start', 'gettoken': 'prsd_nexttoken',
            'end': 'prsd_end', 'lextypes': 'prsd_lextype',
            'headline': 'prsd_headline'}

    def test_map_ts_parser_comment(self):
        "Map a text search parser with a comment"
        stmts = [DROP_TSP_STMT, CREATE_TSP_STMT, COMMENT_TSP_STMT]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['text search parser tsp1'][
            'description'] == 'Test parser tsp1'


class TextSearchParserToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation for input text search parsers"""

    def tearDown(self):
        if self.db.is_superuser():
            self.db.execute_commit(DROP_TSP_STMT)
            self.db.close()

    def test_create_ts_parser(self):
        "Create a text search parser that didn't exist"
        inmap = self.std_map()
        inmap['schema public'].update({'text search parser tsp1': {
            'start': 'prsd_start', 'gettoken': 'prsd_nexttoken',
            'end': 'prsd_end', 'lextypes': 'prsd_lextype',
            'headline': 'prsd_headline'}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_TSP_STMT

    def test_bad_map_ts_parser(self):
        "Error creating a text search parser with a bad map"
        inmap = self.std_map()
        inmap['schema public'].update({'tsp1': {
            'start': 'prsd_start', 'gettoken': 'prsd_nexttoken',
            'end': 'prsd_end', 'lextypes': 'prsd_lextype'}})
        with pytest.raises(KeyError):
            self.to_sql(inmap)

    def test_drop_ts_parser(self):
        "Drop an existing text search parser"
        sql = self.to_sql(self.std_map(), [CREATE_TSP_STMT], superuser=True)
        assert sql == ["DROP TEXT SEARCH PARSER tsp1"]

    def test_comment_on_ts_parser(self):
        "Create a comment for an existing text search parser"
        inmap = self.std_map()
        inmap['schema public'].update({'text search parser tsp1': {
            'start': 'prsd_start', 'gettoken': 'prsd_nexttoken',
            'end': 'prsd_end', 'lextypes': 'prsd_lextype',
            'headline': 'prsd_headline', 'description': "Test parser tsp1"}})
        sql = self.to_sql(inmap, [CREATE_TSP_STMT], superuser=True)
        assert sql == [COMMENT_TSP_STMT]


class TextSearchTemplateToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing text search templates"""

    superuser = True

    def test_map_ts_template(self):
        "Map an existing text search template"
        dbmap = self.to_map([DROP_TST_STMT, CREATE_TST_STMT])
        assert dbmap['schema public']['text search template tst1'] == {
            'init': 'dsimple_init', 'lexize': 'dsimple_lexize'}

    def test_map_ts_template_comment(self):
        "Map a text search template with a comment"
        stmts = [DROP_TST_STMT, CREATE_TST_STMT, COMMENT_TST_STMT]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['text search template tst1'][
            'description'], 'Test template tst1'


class TextSearchTemplateToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation for input text search templates"""

    def tearDown(self):
        if self.db.is_superuser():
            self.db.execute_commit(DROP_TST_STMT)
            self.db.close()

    def test_create_ts_template(self):
        "Create a text search template that didn't exist"
        inmap = self.std_map()
        inmap['schema public'].update({'text search template tst1': {
            'init': 'dsimple_init', 'lexize': 'dsimple_lexize'}})
        sql = self.to_sql(inmap, [DROP_TST_STMT], superuser=True)
        assert fix_indent(sql[0]) == CREATE_TST_STMT

    def test_bad_map_ts_template(self):
        "Error creating a text search template with a bad map"
        inmap = self.std_map()
        inmap['schema public'].update({'tst1': {
            'init': 'dsimple_init', 'lexize': 'dsimple_lexize'}})
        with pytest.raises(KeyError):
            self.to_sql(inmap)

    def test_drop_ts_template(self):
        "Drop an existing text search template"
        sql = self.to_sql(self.std_map(), [CREATE_TST_STMT], superuser=True)
        assert sql == ["DROP TEXT SEARCH TEMPLATE tst1"]

    def test_comment_on_ts_template(self):
        "Create a comment for an existing text search template"
        inmap = self.std_map()
        inmap['schema public'].update({'text search template tst1': {
            'init': 'dsimple_init', 'lexize': 'dsimple_lexize',
            'description': "Test template tst1"}})
        sql = self.to_sql(inmap, [CREATE_TST_STMT], superuser=True)
        assert sql == [COMMENT_TST_STMT]

########NEW FILE########
__FILENAME__ = test_trigger
# -*- coding: utf-8 -*-
"""Test triggers"""

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

FUNC_SRC = "BEGIN NEW.c3 := CURRENT_TIMESTAMP; RETURN NEW; END"
FUNC_INSTEAD_SRC = "BEGIN INSERT INTO t1 VALUES (NEW.c1, NEW.c2, now()); " \
    "RETURN NULL; END"
CREATE_TABLE_STMT = "CREATE TABLE t1 (c1 integer, c2 text, " \
    "c3 timestamp with time zone)"
CREATE_FUNC_STMT = "CREATE FUNCTION f1() RETURNS trigger LANGUAGE plpgsql " \
    "AS $_$%s$_$" % FUNC_SRC
CREATE_STMT = "CREATE TRIGGER tr1 BEFORE INSERT OR UPDATE ON t1 " \
    "FOR EACH ROW EXECUTE PROCEDURE f1()"
DROP_TABLE_STMT = "DROP TABLE IF EXISTS t1"
DROP_FUNC_STMT = "DROP FUNCTION IF EXISTS f1()"
COMMENT_STMT = "COMMENT ON TRIGGER tr1 ON t1 IS 'Test trigger tr1'"


class TriggerToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing triggers"""

    def setUp(self):
        super(self.__class__, self).setUp()
        if self.db.version < 90000:
            if not self.db.is_plpgsql_installed():
                self.db.execute_commit("CREATE LANGUAGE plpgsql")

    def test_map_trigger(self):
        "Map a simple trigger"
        stmts = [CREATE_TABLE_STMT, CREATE_FUNC_STMT, CREATE_STMT]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1']['triggers'] == {
            'tr1': {'timing': 'before', 'events': ['insert', 'update'],
                    'level': 'row', 'procedure': 'f1()'}}

    def test_map_trigger2(self):
        "Map another simple trigger with different attributes"
        stmts = [CREATE_TABLE_STMT, CREATE_FUNC_STMT,
                 "CREATE TRIGGER tr1 AFTER DELETE OR TRUNCATE ON t1 "
                 "EXECUTE PROCEDURE f1()"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1']['triggers'] == {
            'tr1': {'timing': 'after', 'events': ['delete', 'truncate'],
                    'level': 'statement', 'procedure': 'f1()'}}

    def test_map_trigger_update_cols(self):
        "Map trigger with UPDATE OF columns"
        if self.db.version < 90000:
            self.skipTest('Only available on PG 9.0')
        stmts = [CREATE_TABLE_STMT, CREATE_FUNC_STMT,
                 "CREATE TRIGGER tr1 AFTER INSERT OR UPDATE OF c1, c2 ON t1 "
                 "FOR EACH ROW EXECUTE PROCEDURE f1()"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1']['triggers'] == {
            'tr1': {'timing': 'after', 'events': ['insert', 'update'],
                    'columns': ['c1', 'c2'], 'level': 'row',
                    'procedure': 'f1()'}}

    def test_map_trigger_conditional(self):
        "Map trigger with a WHEN qualification"
        if self.db.version < 90000:
            self.skipTest('Only available on PG 9.0')
        stmts = [CREATE_TABLE_STMT, CREATE_FUNC_STMT,
                 "CREATE TRIGGER tr1 AFTER UPDATE ON t1 FOR EACH ROW "
                 "WHEN (OLD.c2 IS DISTINCT FROM NEW.c2) "
                 "EXECUTE PROCEDURE f1()"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1']['triggers'] == {
            'tr1': {'timing': 'after', 'events': ['update'],
                    'level': 'row', 'procedure': 'f1()',
                    'condition': '(old.c2 IS DISTINCT FROM new.c2)'}}

    def test_map_trigger_instead(self):
        "Map an INSTEAD OF trigger"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        stmts = [CREATE_TABLE_STMT, "CREATE VIEW v1 AS SELECT c1, c2 FROM t1",
                 "CREATE FUNCTION f1() RETURNS trigger LANGUAGE plpgsql AS "
                 "$_$%s$_$" % FUNC_INSTEAD_SRC,
                 "CREATE TRIGGER tr1 INSTEAD OF INSERT ON v1 "
                 "FOR EACH ROW EXECUTE PROCEDURE f1()"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['view v1']['triggers'] == {
            'tr1': {'timing': 'instead of', 'events': ['insert'],
                    'level': 'row', 'procedure': 'f1()'}}

    def test_map_trigger_comment(self):
        "Map a trigger comment"
        stmts = [CREATE_TABLE_STMT, CREATE_FUNC_STMT, CREATE_STMT,
                 COMMENT_STMT]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1']['triggers']['tr1'][
            'description'] == 'Test trigger tr1'


class ConstraintTriggerToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of existing constraint triggers"""

    def setUp(self):
        super(self.__class__, self).setUp()
        if self.db.version < 90000:
            if not self.db.is_plpgsql_installed():
                self.db.execute_commit("CREATE LANGUAGE plpgsql")

    def test_map_trigger(self):
        "Map a simple constraint trigger"
        stmts = [CREATE_TABLE_STMT, CREATE_FUNC_STMT,
                 "CREATE CONSTRAINT TRIGGER tr1 AFTER INSERT OR UPDATE ON t1 "
                 "FOR EACH ROW EXECUTE PROCEDURE f1()"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1']['triggers'] == {
            'tr1': {'constraint': True, 'timing': 'after',
                    'events': ['insert', 'update'], 'level': 'row',
                    'procedure': 'f1()'}}

    def test_map_trigger_deferrable(self):
        "Map a deferrable, initially deferred constraint trigger"
        stmts = [CREATE_TABLE_STMT, CREATE_FUNC_STMT,
                 "CREATE CONSTRAINT TRIGGER tr1 AFTER INSERT OR UPDATE ON t1 "
                 "DEFERRABLE INITIALLY DEFERRED "
                 "FOR EACH ROW EXECUTE PROCEDURE f1()"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['table t1']['triggers'] == {
            'tr1': {'constraint': True, 'deferrable': True,
                    'initially_deferred': True, 'timing': 'after',
                    'events': ['insert', 'update'], 'level': 'row',
                    'procedure': 'f1()'}}


class TriggerToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input triggers"""

    def setUp(self):
        super(self.__class__, self).setUp()
        if self.db.version < 90000:
            if not self.db.is_plpgsql_installed():
                self.db.execute_commit("CREATE LANGUAGE plpgsql")

    def test_create_trigger(self):
        "Create a simple trigger"
        inmap = self.std_map(plpgsql_installed=True)
        inmap['schema public'].update({'function f1()': {
            'language': 'plpgsql', 'returns': 'trigger', 'source': FUNC_SRC}})
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c3': {'type': 'timestamp with time zone'}}],
            'triggers': {'tr1': {
                'timing': 'before', 'events': ['insert', 'update'],
                'level': 'row', 'procedure': 'f1()'}}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[1]) == CREATE_FUNC_STMT
        assert fix_indent(sql[2]) == CREATE_TABLE_STMT
        assert fix_indent(sql[3]) == CREATE_STMT

    def test_create_trigger2(self):
        "Create another simple trigger with"
        inmap = self.std_map(plpgsql_installed=True)
        inmap['schema public'].update({'function f1()': {
            'language': 'plpgsql', 'returns': 'trigger', 'source': FUNC_SRC}})
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c3': {'type': 'timestamp with time zone'}}],
            'triggers': {'tr1': {'timing': 'after',
                                 'events': ['delete', 'truncate'],
                                 'procedure': 'f1()'}}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[1]) == CREATE_FUNC_STMT
        assert fix_indent(sql[2]) == CREATE_TABLE_STMT
        assert fix_indent(sql[3]) == "CREATE TRIGGER tr1 AFTER DELETE OR " \
            "TRUNCATE ON t1 FOR EACH STATEMENT EXECUTE PROCEDURE f1()"

    def test_create_trigger_update_cols(self):
        "Create a trigger with UPDATE OF columns"
        if self.db.version < 90000:
            self.skipTest('Only available on PG 9.0')
        inmap = self.std_map()
        inmap['schema public'].update({'function f1()': {
            'language': 'plpgsql', 'returns': 'trigger', 'source': FUNC_SRC}})
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c3': {'type': 'timestamp with time zone'}}],
            'triggers': {'tr1': {'timing': 'before', 'events': [
                'insert', 'update'], 'columns': ['c1', 'c2'], 'level': 'row',
                'procedure': 'f1()'}}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[1]) == CREATE_FUNC_STMT
        assert fix_indent(sql[2]) == CREATE_TABLE_STMT
        assert fix_indent(sql[3]) == "CREATE TRIGGER tr1 BEFORE INSERT OR " \
            "UPDATE OF c1, c2 ON t1 FOR EACH ROW EXECUTE PROCEDURE f1()"

    def test_create_trigger_conditional(self):
        "Create a trigger with a WHEN qualification"
        if self.db.version < 90000:
            self.skipTest('Only available on PG 9.0')
        inmap = self.std_map()
        inmap['schema public'].update({'function f1()': {
            'language': 'plpgsql', 'returns': 'trigger', 'source': FUNC_SRC}})
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c3': {'type': 'timestamp with time zone'}}],
            'triggers': {'tr1': {'timing': 'before', 'events': [
                'update'], 'level': 'row', 'procedure': 'f1()',
                'condition': '(old.c2 IS DISTINCT FROM new.c2)'}}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[1]) == CREATE_FUNC_STMT
        assert fix_indent(sql[2]) == CREATE_TABLE_STMT
        assert fix_indent(sql[3]) == "CREATE TRIGGER tr1 BEFORE UPDATE " \
            "ON t1 FOR EACH ROW WHEN ((old.c2 IS DISTINCT FROM new.c2)) " \
            "EXECUTE PROCEDURE f1()"

    def test_create_trigger_instead(self):
        "Create an INSTEAD OF trigger"
        inmap = self.std_map(plpgsql_installed=True)
        inmap['schema public'].update({'function f1()': {
            'language': 'plpgsql', 'returns': 'trigger',
            'source': FUNC_INSTEAD_SRC}})
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c3': {'type': 'timestamp with time zone'}}]},
            'view v1': {'definition': "SELECT c1, c2 FROM t1",
                        'triggers': {'tr1': {'timing': 'instead of',
                                             'events': ['insert'],
                                             'level': 'row',
                                             'procedure': 'f1()'}}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[1]) == "CREATE FUNCTION f1() RETURNS trigger " \
            "LANGUAGE plpgsql AS $_$%s$_$" % FUNC_INSTEAD_SRC
        assert fix_indent(sql[2]) == CREATE_TABLE_STMT
        assert fix_indent(sql[3]) == "CREATE VIEW v1 AS SELECT c1, c2 FROM t1"
        assert fix_indent(sql[4]) == "CREATE TRIGGER tr1 INSTEAD OF INSERT " \
            "ON v1 FOR EACH ROW EXECUTE PROCEDURE f1()"

    def test_create_trigger_in_schema(self):
        "Create a trigger within a non-public schema"
        inmap = self.std_map(plpgsql_installed=True)
        inmap.update({'schema s1': {'function f1()': {
            'language': 'plpgsql', 'returns': 'trigger', 'source': FUNC_SRC},
            'table t1': {
                'columns': [{'c1': {'type': 'integer'}},
                            {'c2': {'type': 'text'}},
                            {'c3': {'type': 'timestamp with time zone'}}],
                'triggers': {'tr1': {
                    'timing': 'before', 'events': ['insert', 'update'],
                    'level': 'row', 'procedure': 'f1()'}}}}})
        sql = self.to_sql(inmap, ["CREATE SCHEMA s1"])
        assert fix_indent(sql[3]) == "CREATE TRIGGER tr1 BEFORE INSERT OR " \
            "UPDATE ON s1.t1 FOR EACH ROW EXECUTE PROCEDURE f1()"

    def test_drop_trigger(self):
        "Drop an existing trigger"
        stmts = [CREATE_TABLE_STMT, CREATE_FUNC_STMT, CREATE_STMT]
        inmap = self.std_map(plpgsql_installed=True)
        inmap['schema public'].update({'function f1()': {
            'language': 'plpgsql', 'returns': 'trigger', 'source': FUNC_SRC}})
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c3': {'type': 'timestamp with time zone'}}]}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["DROP TRIGGER tr1 ON t1"]

    def test_drop_trigger_table(self):
        "Drop an existing trigger and the related table"
        stmts = [CREATE_TABLE_STMT, CREATE_FUNC_STMT, CREATE_STMT]
        inmap = self.std_map(plpgsql_installed=True)
        inmap['schema public'].update({'function f1()': {
            'language': 'plpgsql', 'returns': 'trigger', 'source': FUNC_SRC}})
        sql = self.to_sql(inmap, stmts)
        assert sql[0] == "DROP TRIGGER tr1 ON t1"
        assert sql[1] == "DROP TABLE t1"

    def test_trigger_with_comment(self):
        "Create a trigger with a comment"
        inmap = self.std_map(plpgsql_installed=True)
        inmap['schema public'].update({'function f1()': {
            'language': 'plpgsql', 'returns': 'trigger', 'source': FUNC_SRC}})
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c3': {'type': 'timestamp with time zone'}}],
            'triggers': {'tr1': {
                'description': 'Test trigger tr1',
                'timing': 'before', 'events': ['insert', 'update'],
                'level': 'row', 'procedure': 'f1()'}}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[1]) == CREATE_FUNC_STMT
        assert fix_indent(sql[2]) == CREATE_TABLE_STMT
        assert fix_indent(sql[3]) == CREATE_STMT
        assert sql[4] == COMMENT_STMT

    def test_comment_on_trigger(self):
        "Create a comment on an existing trigger"
        stmts = [CREATE_TABLE_STMT, CREATE_FUNC_STMT, CREATE_STMT]
        inmap = self.std_map(plpgsql_installed=True)
        inmap['schema public'].update({'function f1()': {
            'language': 'plpgsql', 'returns': 'trigger', 'source': FUNC_SRC}})
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c3': {'type': 'timestamp with time zone'}}],
            'triggers': {'tr1': {
                'description': 'Test trigger tr1',
                'timing': 'before', 'events': ['insert', 'update'],
                'level': 'row', 'procedure': 'f1()'}}}})
        sql = self.to_sql(inmap, stmts)
        assert sql == [COMMENT_STMT]

    def test_drop_trigger_comment(self):
        "Drop a comment on an existing trigger"
        stmts = [CREATE_TABLE_STMT, CREATE_FUNC_STMT, CREATE_STMT,
                 COMMENT_STMT]
        inmap = self.std_map(plpgsql_installed=True)
        inmap['schema public'].update({'function f1()': {
            'language': 'plpgsql', 'returns': 'trigger', 'source': FUNC_SRC}})
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c3': {'type': 'timestamp with time zone'}}],
            'triggers': {'tr1': {
                'timing': 'before', 'events': ['insert', 'update'],
                'level': 'row', 'procedure': 'f1()'}}}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["COMMENT ON TRIGGER tr1 ON t1 IS NULL"]

    def test_change_trigger_comment(self):
        "Change existing comment on a trigger"
        stmts = [CREATE_TABLE_STMT, CREATE_FUNC_STMT, CREATE_STMT,
                 COMMENT_STMT]
        inmap = self.std_map(plpgsql_installed=True)
        inmap['schema public'].update({'function f1()': {
            'language': 'plpgsql', 'returns': 'trigger', 'source': FUNC_SRC}})
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c3': {'type': 'timestamp with time zone'}}],
            'triggers': {'tr1': {
                'description': 'Changed trigger tr1',
                'timing': 'before', 'events': ['insert', 'update'],
                'level': 'row', 'procedure': 'f1()'}}}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["COMMENT ON TRIGGER tr1 ON t1 IS 'Changed trigger tr1'"]


class ConstraintTriggerToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input triggers"""

    def setUp(self):
        super(self.__class__, self).setUp()
        if self.db.version < 90000:
            if not self.db.is_plpgsql_installed():
                self.db.execute_commit("CREATE LANGUAGE plpgsql")

    def test_create_trigger(self):
        "Create a constraint trigger"
        inmap = self.std_map(plpgsql_installed=True)
        inmap['schema public'].update({'function f1()': {
            'language': 'plpgsql', 'returns': 'trigger', 'source': FUNC_SRC}})
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c3': {'type': 'timestamp with time zone'}}],
            'triggers': {'tr1': {
                'constraint': True, 'timing': 'after',
                'events': ['insert', 'update'], 'level': 'row',
                'procedure': 'f1()'}}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[1]) == CREATE_FUNC_STMT
        assert fix_indent(sql[2]) == CREATE_TABLE_STMT
        assert fix_indent(sql[3]) == "CREATE CONSTRAINT TRIGGER tr1 AFTER " \
            "INSERT OR UPDATE ON t1 FOR EACH ROW EXECUTE PROCEDURE f1()"

    def test_create_trigger_deferrable(self):
        "Create a deferrable constraint trigger"
        inmap = self.std_map(plpgsql_installed=True)
        inmap['schema public'].update({'function f1()': {
            'language': 'plpgsql', 'returns': 'trigger', 'source': FUNC_SRC}})
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c3': {'type': 'timestamp with time zone'}}],
            'triggers': {'tr1': {
                'constraint': True, 'deferrable': True,
                'initially_deferred': True, 'timing': 'after',
                'events': ['insert', 'update'], 'level': 'row',
                'procedure': 'f1()'}}}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[1]) == CREATE_FUNC_STMT
        assert fix_indent(sql[2]) == CREATE_TABLE_STMT
        assert fix_indent(sql[3]) == "CREATE CONSTRAINT TRIGGER tr1 " \
            "AFTER INSERT OR UPDATE ON t1 DEFERRABLE INITIALLY " \
            "DEFERRED FOR EACH ROW EXECUTE PROCEDURE f1()"

########NEW FILE########
__FILENAME__ = test_type
# -*- coding: utf-8 -*-
"""Test enums and other types"""

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

CREATE_COMPOSITE_STMT = "CREATE TYPE t1 AS (x integer, y integer, z integer)"
CREATE_ENUM_STMT = "CREATE TYPE t1 AS ENUM ('red', 'green', 'blue')"
CREATE_SHELL_STMT = "CREATE TYPE t1"
CREATE_FUNC_IN = "CREATE FUNCTION t1textin(cstring) RETURNS t1 " \
    "LANGUAGE internal IMMUTABLE STRICT AS $$textin$$"
CREATE_FUNC_OUT = "CREATE FUNCTION t1textout(t1) RETURNS cstring " \
    "LANGUAGE internal IMMUTABLE STRICT AS $$textout$$"
CREATE_TYPE_STMT = "CREATE TYPE t1 (INPUT = t1textin, OUTPUT = t1textout)"
DROP_STMT = "DROP TYPE IF EXISTS t1 CASCADE"
COMMENT_STMT = "COMMENT ON TYPE t1 IS 'Test type t1'"


class CompositeToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of created composite types"""

    def test_composite(self):
        "Map a composite type"
        dbmap = self.to_map([CREATE_COMPOSITE_STMT])
        assert dbmap['schema public']['type t1'] == {
            'attributes': [{'x': {'type': 'integer'}},
                           {'y': {'type': 'integer'}},
                           {'z': {'type': 'integer'}}]}

    def test_dropped_attribute(self):
        "Map a composite type which has a dropped attribute"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        stmts = [CREATE_COMPOSITE_STMT, "ALTER TYPE t1 DROP ATTRIBUTE y"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['type t1'] == {
            'attributes': [{'x': {'type': 'integer'}},
                           {'z': {'type': 'integer'}}]}


class CompositeToSqlTestCase(InputMapToSqlTestCase):
    """Test creation and modification of composite types"""

    def test_create_composite(self):
        "Create a composite type"
        inmap = self.std_map()
        inmap['schema public'].update({'type t1': {
            'attributes': [{'x': {'type': 'integer'}},
                           {'y': {'type': 'integer'}},
                           {'z': {'type': 'integer'}}]}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_COMPOSITE_STMT

    def test_drop_composite(self):
        "Drop an existing composite"
        sql = self.to_sql(self.std_map(), [CREATE_COMPOSITE_STMT])
        assert sql == ["DROP TYPE t1"]

    def test_rename_composite(self):
        "Rename an existing composite"
        inmap = self.std_map()
        inmap['schema public'].update({'type t2': {
            'oldname': 't1',
            'attributes': [{'x': {'type': 'integer'}},
                           {'y': {'type': 'integer'}},
                           {'z': {'type': 'integer'}}]}})
        sql = self.to_sql(inmap, [CREATE_COMPOSITE_STMT])
        assert sql == ["ALTER TYPE t1 RENAME TO t2"]

    def test_add_attribute(self):
        "Add an attribute to a composite type"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        inmap = self.std_map()
        inmap['schema public'].update({'type t1': {
            'attributes': [{'x': {'type': 'integer'}},
                           {'y': {'type': 'integer'}},
                           {'z': {'type': 'integer'}}]}})
        sql = self.to_sql(inmap, ["CREATE TYPE t1 AS (x integer, y integer)"])
        assert fix_indent(sql[0]) == "ALTER TYPE t1 ADD ATTRIBUTE z integer"

    def test_drop_attribute(self):
        "Drop an attribute from a composite type"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        inmap = self.std_map()
        inmap['schema public'].update({'type t1': {
            'attributes': [{'x': {'type': 'integer'}},
                           {'z': {'type': 'integer'}}]}})
        sql = self.to_sql(inmap, [CREATE_COMPOSITE_STMT])
        assert fix_indent(sql[0]) == "ALTER TYPE t1 DROP ATTRIBUTE y"

    def test_drop_attribute_schema(self):
        "Drop an attribute from a composite type within a non-public schema"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        inmap = self.std_map()
        inmap.update({'schema s1': {'type t1': {
            'attributes': [{'x': {'type': 'integer'}},
                           {'z': {'type': 'integer'}}]}}})
        sql = self.to_sql(inmap, [
            "CREATE SCHEMA s1",
            "CREATE TYPE s1.t1 AS (x integer, y integer, z integer)"])
        assert fix_indent(sql[0]) == "ALTER TYPE s1.t1 DROP ATTRIBUTE y"

    def test_rename_attribute(self):
        "Rename an attribute of a composite type"
        if self.db.version < 90100:
            self.skipTest('Only available on PG 9.1')
        inmap = self.std_map()
        inmap['schema public'].update({'type t1': {
            'attributes': [{'x': {'type': 'integer'}},
                           {'y1': {'type': 'integer', 'oldname': 'y'}},
                           {'z': {'type': 'integer'}}]}})
        sql = self.to_sql(inmap, [CREATE_COMPOSITE_STMT])
        assert fix_indent(sql[0]) == "ALTER TYPE t1 RENAME ATTRIBUTE y TO y1"


class EnumToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of created enum types"""

    def test_enum(self):
        "Map an enum"
        dbmap = self.to_map([CREATE_ENUM_STMT])
        assert dbmap['schema public']['type t1'] == {
            'labels': ['red', 'green', 'blue']}


class EnumToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input enums"""

    def test_create_enum(self):
        "Create an enum"
        inmap = self.std_map()
        inmap['schema public'].update({'type t1': {
            'labels': ['red', 'green', 'blue']}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_ENUM_STMT

    def test_drop_enum(self):
        "Drop an existing enum"
        sql = self.to_sql(self.std_map(), [CREATE_ENUM_STMT])
        assert sql == ["DROP TYPE t1"]

    def test_rename_enum(self):
        "Rename an existing enum"
        inmap = self.std_map()
        inmap['schema public'].update({'type t2': {
            'oldname': 't1', 'labels': ['red', 'green', 'blue']}})
        sql = self.to_sql(inmap, [CREATE_ENUM_STMT])
        assert sql == ["ALTER TYPE t1 RENAME TO t2"]


class BaseTypeToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of created base type types"""

    superuser = True

    def test_base_type(self):
        "Map a base type"
        stmts = [CREATE_SHELL_STMT, CREATE_FUNC_IN, CREATE_FUNC_OUT,
                 CREATE_TYPE_STMT]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['type t1'] == {
            'input': 't1textin', 'output': 't1textout',
            'internallength': 'variable', 'alignment': 'int4',
            'storage': 'plain', 'category': 'U'}

    def test_base_type_category(self):
        "Map a base type"
        stmts = [CREATE_SHELL_STMT, CREATE_FUNC_IN, CREATE_FUNC_OUT,
                 "CREATE TYPE t1 (INPUT = t1textin, OUTPUT = t1textout, "
                 "CATEGORY = 'S')"]
        dbmap = self.to_map(stmts)
        assert dbmap['schema public']['type t1'] == {
            'input': 't1textin', 'output': 't1textout',
            'internallength': 'variable', 'alignment': 'int4',
            'storage': 'plain', 'category': 'S'}


class BaseTypeToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input base types"""

    def test_create_base_type(self):
        "Create a base type"
        inmap = self.std_map()
        inmap['schema public'].update({'type t1': {
            'input': 't1textin', 'output': 't1textout',
            'internallength': 'variable', 'alignment': 'int4',
            'storage': 'plain'}, 'function t1textin(cstring)': {
                'language': 'internal', 'returns': 't1', 'strict': True,
                'volatility': 'immutable', 'source': 'textin'},
            'function t1textout(t1)': {
                'language': 'internal', 'returns': 'cstring',
                'strict': True, 'volatility': 'immutable',
                'source': 'textout'}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_SHELL_STMT
        assert fix_indent(sql[1]) == CREATE_FUNC_IN
        assert fix_indent(sql[2]) == CREATE_FUNC_OUT
        assert fix_indent(sql[3]) == "CREATE TYPE t1 (INPUT = t1textin, " \
            "OUTPUT = t1textout, INTERNALLENGTH = variable, " \
            "ALIGNMENT = int4, STORAGE = plain)"

    def test_drop_type(self):
        "Drop an existing base type"
        stmts = [CREATE_SHELL_STMT, CREATE_FUNC_IN, CREATE_FUNC_OUT,
                 CREATE_TYPE_STMT]
        sql = self.to_sql(self.std_map(), stmts, superuser=True)
        assert sql == ["DROP TYPE t1 CASCADE"]

########NEW FILE########
__FILENAME__ = test_view
# -*- coding: utf-8 -*-
"""Test views"""

import pytest

from pyrseas.testutils import DatabaseToMapTestCase
from pyrseas.testutils import InputMapToSqlTestCase, fix_indent

CREATE_STMT = "CREATE VIEW v1 AS SELECT now()::date AS today"
COMMENT_STMT = "COMMENT ON VIEW v1 IS 'Test view v1'"
VIEW_DEFN = " SELECT now()::date AS today;"


class ViewToMapTestCase(DatabaseToMapTestCase):
    """Test mapping of created views"""

    def test_map_view_no_table(self):
        "Map a created view without a table dependency"
        dbmap = self.to_map([CREATE_STMT])
        expmap = {'definition': VIEW_DEFN}
        assert dbmap['schema public']['view v1'] == expmap

    def test_map_view(self):
        "Map a created view with a table dependency"
        stmts = ["CREATE TABLE t1 (c1 INTEGER, c2 TEXT, c3 INTEGER)",
                 "CREATE VIEW v1 AS SELECT c1, c3 * 2 FROM t1"]
        dbmap = self.to_map(stmts)
        fmt = "%s%s" if (self.db.version < 90300) else "%s\n   %s"
        expmap = {'definition': fmt % (" SELECT t1.c1,",
                                       " t1.c3 * 2\n   FROM t1;")}
        assert dbmap['schema public']['view v1'] == expmap

    def test_map_view_comment(self):
        "Map a view with a comment"
        dbmap = self.to_map([CREATE_STMT, COMMENT_STMT])
        assert dbmap['schema public']['view v1']['description'] == \
            'Test view v1'


class ViewToSqlTestCase(InputMapToSqlTestCase):
    """Test SQL generation from input views"""

    def test_create_view_no_table(self):
        "Create a view with no table dependency"
        inmap = self.std_map()
        inmap['schema public'].update({'view v1': {'definition': VIEW_DEFN}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_STMT

    def test_create_view(self):
        "Create a view"
        inmap = self.std_map()
        inmap['schema public'].update({'table t1': {
            'columns': [{'c1': {'type': 'integer'}}, {'c2': {'type': 'text'}},
                        {'c3': {'type': 'integer'}}]}})
        inmap['schema public'].update({'view v1': {
            'definition': "SELECT c1, c3 * 2 FROM t1"}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == "CREATE TABLE t1 (c1 integer, " \
            "c2 text, c3 integer)"
        assert fix_indent(sql[1]) == \
            "CREATE VIEW v1 AS SELECT c1, c3 * 2 FROM t1"

    def test_create_view_in_schema(self):
        "Create a view within a non-public schema"
        inmap = self.std_map()
        inmap.update({'schema s1': {'view v1': {'definition': VIEW_DEFN}}})
        sql = self.to_sql(inmap, ["CREATE SCHEMA s1"])
        assert fix_indent(sql[0]) == \
            "CREATE VIEW s1.v1 AS SELECT now()::date AS today"

    def test_bad_view_map(self):
        "Error creating a view with a bad map"
        inmap = self.std_map()
        inmap['schema public'].update({'v1': {'definition': VIEW_DEFN}})
        with pytest.raises(KeyError):
            self.to_sql(inmap)

    def test_drop_view_no_table(self):
        "Drop an existing view without a table dependency"
        sql = self.to_sql(self.std_map(), [CREATE_STMT])
        assert sql == ["DROP VIEW v1"]

    def test_drop_view(self):
        "Drop an existing view with table dependencies"
        stmts = ["CREATE TABLE t1 (c1 INTEGER, c2 TEXT)",
                 "CREATE TABLE t2 (c1 INTEGER, c3 TEXT)",
                 "CREATE VIEW v1 AS SELECT t1.c1, c2, c3 "
                 "FROM t1 JOIN t2 ON (t1.c1 = t2.c1)"]
        sql = self.to_sql(self.std_map(), stmts)
        assert sql[0] == "DROP VIEW v1"
        # can't control which table will be dropped first
        drt1 = 1
        drt2 = 2
        if 't1' in sql[2]:
            drt1 = 2
            drt2 = 1
        assert sql[drt1] == "DROP TABLE t1"
        assert sql[drt2] == "DROP TABLE t2"

    def test_rename_view(self):
        "Rename an existing view"
        inmap = self.std_map()
        inmap['schema public'].update({'view v2': {
            'oldname': 'v1', 'definition': VIEW_DEFN}})
        sql = self.to_sql(inmap, [CREATE_STMT])
        assert sql == ["ALTER VIEW v1 RENAME TO v2"]

    def test_bad_rename_view(self):
        "Error renaming a non-existing view"
        inmap = self.std_map()
        inmap['schema public'].update({'view v2': {
            'oldname': 'v3', 'definition': VIEW_DEFN}})
        with pytest.raises(KeyError):
            self.to_sql(inmap, [CREATE_STMT])

    def test_change_view_defn(self):
        "Change view definition"
        inmap = self.std_map()
        inmap['schema public'].update({'view v1': {
            'definition': " SELECT now()::date AS todays_date;"}})
        sql = self.to_sql(inmap, [CREATE_STMT])
        assert fix_indent(sql[0]) == "CREATE OR REPLACE VIEW v1 AS " \
            "SELECT now()::date AS todays_date"

    def test_view_with_comment(self):
        "Create a view with a comment"
        inmap = self.std_map()
        inmap['schema public'].update({'view v1': {
            'definition': VIEW_DEFN, 'description': "Test view v1"}})
        sql = self.to_sql(inmap)
        assert fix_indent(sql[0]) == CREATE_STMT
        assert sql[1] == COMMENT_STMT

    def test_comment_on_view(self):
        "Create a comment for an existing view"
        inmap = self.std_map()
        inmap['schema public'].update({'view v1': {
            'definition': VIEW_DEFN, 'description': "Test view v1"}})
        sql = self.to_sql(inmap, [CREATE_STMT])
        assert sql == [COMMENT_STMT]

    def test_drop_view_comment(self):
        "Drop the comment on an existing view"
        stmts = [CREATE_STMT, COMMENT_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'view v1': {'definition': VIEW_DEFN}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["COMMENT ON VIEW v1 IS NULL"]

    def test_change_view_comment(self):
        "Change existing comment on a view"
        stmts = [CREATE_STMT, COMMENT_STMT]
        inmap = self.std_map()
        inmap['schema public'].update({'view v1': {
            'definition': VIEW_DEFN, 'description': "Changed view v1"}})
        sql = self.to_sql(inmap, stmts)
        assert sql == ["COMMENT ON VIEW v1 IS 'Changed view v1'"]

########NEW FILE########
__FILENAME__ = test_autodoc
# -*- coding: utf-8 -*-
"""Test dbtoyaml and yamltodb using autodoc schema

See http://cvs.pgfoundry.org/cgi-bin/cvsweb.cgi/~checkout~/autodoc/autodoc/
regressdatabase.sql?rev=1.2
"""
from pyrseas.testutils import DbMigrateTestCase


class AutodocTestCase(DbMigrateTestCase):

    @classmethod
    def tearDownClass(cls):
        cls.remove_tempfiles('autodoc')
        cls.remove_tempfiles('empty')

    def test_autodoc(self):
        # Create the source schema
        self.execute_script(__file__, 'autodoc-schema.sql')

        # Run pg_dump against source database
        srcdump = self.tempfile_path('autodoc-src.dump')
        self.run_pg_dump(srcdump, True)

        # Create source YAML file
        srcyaml = self.tempfile_path('autodoc-src.yaml')
        self.create_yaml(srcyaml, True)

        # Run pg_dump/dbtoyaml against empty target database
        emptydump = self.tempfile_path('empty.dump')
        self.run_pg_dump(emptydump)
        emptyyaml = self.tempfile_path('empty.yaml')
        self.create_yaml(emptyyaml)

        # Migrate the target database
        targsql = self.tempfile_path('autodoc.sql')
        self.migrate_target(srcyaml, targsql)

        # Run pg_dump against target database
        targdump = self.tempfile_path('autodoc.dump')
        self.run_pg_dump(targdump)

        # Create target YAML file
        targyaml = self.tempfile_path('autodoc.yaml')
        self.create_yaml(targyaml)

        # diff autodoc-src.dump against autodoc.dump
        assert self.lines(srcdump) == self.lines(targdump)
        # diff autodoc-src.yaml against autodoc.yaml
        assert self.lines(srcyaml) == self.lines(targyaml)

        # Undo the changes
        self.migrate_target(emptyyaml, targsql)

        # Run pg_dump against target database
        self.run_pg_dump(targdump)

        # Create target YAML file
        self.create_yaml(targyaml)

        # diff empty.dump against autodoc.dump
        assert self.lines(emptydump) == self.lines(targdump)
        # diff empty.yaml against autodoc.yaml
        assert self.lines(emptyyaml) == self.lines(targyaml)

########NEW FILE########
__FILENAME__ = test_autodoc_dir
# -*- coding: utf-8 -*-
"""Test dbtoyaml and yamltodb using autodoc schema but I/O to/from a directory

Same as test_autodoc.py but with directory tree instead of a single YAML file.
See http://cvs.pgfoundry.org/cgi-bin/cvsweb.cgi/~checkout~/autodoc/autodoc/
regressdatabase.sql?rev=1.2
"""
from pyrseas.testutils import DbMigrateTestCase


class AutodocTestCase(DbMigrateTestCase):

    @classmethod
    def tearDownClass(cls):
        cls.remove_tempfiles('autodoc')
        cls.remove_tempfiles('metadata')

    def test_autodoc(self):
        # Create the source schema
        self.execute_script(__file__, 'autodoc-schema.sql')

        # Run pg_dump against source database
        srcdump = self.tempfile_path('autodoc-src.dump')
        self.run_pg_dump(srcdump, True)

        # Create source YAML file and directory tree
        # Note: the single YAML file is for verification against the target,
        #       the YAML directory tree is for processing
        srcyaml = self.tempfile_path('autodoc-src.yaml')
        self.create_yaml(srcyaml, True)
        self.create_yaml(None, True)

        # Migrate the target database
        targsql = self.tempfile_path('autodoc.sql')
        self.migrate_target(None, targsql)

        # Run pg_dump against target database
        targdump = self.tempfile_path('autodoc.dump')
        self.run_pg_dump(targdump)

        # Create target YAML file
        targyaml = self.tempfile_path('autodoc.yaml')
        self.create_yaml(targyaml)

        # diff autodoc-src.dump against autodoc.dump
        assert self.lines(srcdump) == self.lines(targdump)
        # diff autodoc-src.yaml against autodoc.yaml
        assert self.lines(srcyaml) == self.lines(targyaml)

        # Undo the changes
        self.srcdb.execute_commit(
            "DROP SCHEMA inherit, product, store, warehouse CASCADE")
        # Create source YAML file and directory tree
        srcyaml = self.tempfile_path('autodoc-src-empty.yaml')
        self.create_yaml(srcyaml, True)
        self.create_yaml(None, True)
        targsql = self.tempfile_path('autodoc-empty.sql')
        self.migrate_target(None, targsql)

        # Run pg_dump against source database
        srcdump = self.tempfile_path('autodoc-src-empty.dump')
        self.run_pg_dump(srcdump, True)

        # Run pg_dump against target database
        targdump = self.tempfile_path('autodoc-empty.dump')
        self.run_pg_dump(targdump)

        # Create target YAML file
        targyaml = self.tempfile_path('autodoc-empty.yaml')
        self.create_yaml(targyaml)

        # diff empty.dump against autodoc.dump
        assert self.lines(srcdump) == self.lines(targdump)
        # diff empty.yaml against autodoc.yaml
        assert self.lines(srcyaml) == self.lines(targyaml)

########NEW FILE########
__FILENAME__ = test_filmversions
# -*- coding: utf-8 -*-
"""Test dbtoyaml and yamltodb using film versions schema

See https://pyrseas.wordpress.com/2011/02/07/
version-control-part-2-sql-databases/
"""
import os
from pyrseas.testutils import DbMigrateTestCase
from pyrseas.yamlutil import yamldump


class FilmTestCase(DbMigrateTestCase):

    @classmethod
    def tearDownClass(cls):
        cls.remove_tempfiles('film-0.')

    def test_film_version_01(self):
        "Create schema version 0.1"
        self.execute_script(__file__, 'film-schema-0.1.sql')

        # Run pg_dump against source database
        srcdump = self.tempfile_path('film-0.1-src.dump')
        self.run_pg_dump(srcdump, True)

        # Create source YAML file
        srcyaml = self.tempfile_path('film-0.1-src.yaml')
        self.create_yaml(srcyaml, True)

        # Run pg_dump/dbtoyaml against empty target database
        self.run_pg_dump(self.tempfile_path('film-0.0.dump'))
        self.create_yaml(self.tempfile_path('film-0.0.yaml'))

        # Migrate the target database
        self.migrate_target(srcyaml, self.tempfile_path('film-0.1.sql'))

        # Run pg_dump against target database
        targdump = self.tempfile_path('film-0.1.dump')
        self.run_pg_dump(targdump)

        # Create target YAML file
        targyaml = self.tempfile_path('film-0.1.yaml')
        self.create_yaml(targyaml)

        # diff film-0.1-src.dump against film-0.1.dump
        assert self.lines(srcdump) == self.lines(targdump)
        # diff film-0.1-src.yaml against film-0.1.yaml
        assert self.lines(srcyaml) == self.lines(targyaml)

    def test_film_version_02(self):
        "Update schema to version 0.2"
        self.execute_script(__file__, 'film-schema-0.2.sql')

        # Run pg_dump against source database
        srcdump = self.tempfile_path('film-0.2-src.dump')
        self.run_pg_dump(srcdump, True)

        # Create source YAML file
        srcyaml = self.tempfile_path('film-0.2-src.yaml')
        self.create_yaml(srcyaml, True)

        # Migrate the target database
        self.migrate_target(srcyaml, self.tempfile_path('film-0.2.sql'))

        # Run pg_dump against target database
        targdump = self.tempfile_path('film-0.2.dump')
        self.run_pg_dump(targdump)

        # Create target YAML file
        targyaml = self.tempfile_path('film-0.2.yaml')
        self.create_yaml(targyaml)

        # diff film-0.2-src.dump against film-0.2.dump
        assert self.lines(srcdump) == self.lines(targdump)
        # diff film-0.2-src.yaml against film-0.2.yaml
        assert self.lines(srcyaml) == self.lines(targyaml)

    def test_film_version_03(self):
        "Update schema to version 0.3"
        self.execute_script(__file__, 'film-schema-0.3a.sql')
        self.execute_script(__file__, 'film-schema-0.3b.sql')

        # Run pg_dump against source database
        srcdump = self.tempfile_path('film-0.3-src.dump')
        self.run_pg_dump(srcdump, True, True)

        # Create source YAML file
        usercfg = self.tempfile_path("usercfg.yaml")
        with open(usercfg, 'w') as f:
            f.write(yamldump({'repository': {'path': self.tempfile_path('')}}))
        os.environ["PYRSEAS_USER_CONFIG"] = usercfg
        with open(self.tempfile_path("config.yaml"), 'w') as f:
            f.write(yamldump({'datacopy': {'schema public': ['genre']}}))
        srcyaml = self.tempfile_path('film-0.3-src.yaml')
        self.create_yaml(srcyaml, True)

        # Migrate the target database
        self.migrate_target(srcyaml, self.tempfile_path('film-0.3.sql'))

        # Run pg_dump against target database
        targdump = self.tempfile_path('film-0.3.dump')
        self.run_pg_dump(targdump, False, True)

        # Create target YAML file
        targyaml = self.tempfile_path('film-0.3.yaml')
        self.create_yaml(targyaml)

        # diff film-0.3-src.dump against film-0.3.dump
        assert self.lines(srcdump) == self.lines(targdump)
        # diff film-0.3-src.yaml against film-0.3.yaml
        assert self.lines(srcyaml) == self.lines(targyaml)

    def test_film_version_04(self):
        "Revert to schema version 0.2"
        srcyaml = self.tempfile_path('film-0.2.yaml')
        self.migrate_target(srcyaml, self.tempfile_path('film-0.3-undo.sql'))

        # Run pg_dump against target database
        targdump = self.tempfile_path('film-0.3-undo.dump')
        self.run_pg_dump(targdump)

        # Create target YAML file
        targyaml = self.tempfile_path('film-0.3-undo.yaml')
        self.create_yaml(targyaml)

        # diff film-0.2.dump against film-0.3-undo.dump
        srcdump = self.tempfile_path('film-0.2.dump')
        assert self.lines(srcdump) == self.lines(targdump)
        # diff film-0.2.yaml against film-0.3-undo.yaml
        assert self.lines(srcyaml) == self.lines(targyaml)

    def test_film_version_05(self):
        "Revert to schema version 0.1"
        srcyaml = self.tempfile_path('film-0.1.yaml')
        self.migrate_target(srcyaml, self.tempfile_path('film-0.2-undo.sql'))

        # Run pg_dump against target database
        targdump = self.tempfile_path('film-0.2-undo.dump')
        self.run_pg_dump(targdump)

        # Create target YAML file
        targyaml = self.tempfile_path('film-0.2-undo.yaml')
        self.create_yaml(targyaml)

        # diff film-0.1.dump against film-0.2-undo.dump
        srcdump = self.tempfile_path('film-0.1.dump')
        assert self.lines(srcdump) == self.lines(targdump)
        # diff film-0.1.yaml against film-0.2-undo.yaml
        assert self.lines(srcyaml) == self.lines(targyaml)

    def test_film_version_06(self):
        "Revert to empty schema"
        srcyaml = self.tempfile_path('film-0.0.yaml')
        self.migrate_target(srcyaml, self.tempfile_path('film-0.1-undo.sql'))

        # Run pg_dump against target database
        targdump = self.tempfile_path('film-0.1-undo.dump')
        self.run_pg_dump(targdump)

        # Create target YAML file
        targyaml = self.tempfile_path('film-0.1-undo.yaml')
        self.create_yaml(targyaml)

        # diff film-0.0.dump against film-0.1-undo.dump
        srcdump = self.tempfile_path('film-0.0.dump')
        assert self.lines(srcdump) == self.lines(targdump)
        # diff film-0.0.yaml against film-0.1-undo.yaml
        assert self.lines(srcyaml) == self.lines(targyaml)

########NEW FILE########
__FILENAME__ = test_pagila
# -*- coding: utf-8 -*-
"""Test dbtoyaml and yamltodb using pagila schema

See http://cvs.pgfoundry.org/cgi-bin/cvsweb.cgi/dbsamples/pagila/
pagila-schema.sql?rev=1.8
"""
from difflib import unified_diff

from pyrseas.testutils import DbMigrateTestCase


class PagilaTestCase(DbMigrateTestCase):

    @classmethod
    def tearDown(cls):
        cls.remove_tempfiles('pagila')
        cls.remove_tempfiles('empty')

    def test_pagila(self):
        # Create the source schema
        if self.srcdb.version < 90000:
            self.srcdb.execute("CREATE PROCEDURAL LANGUAGE plpgsql")
            self.srcdb.execute_commit("ALTER PROCEDURAL LANGUAGE plpgsql "
                                      "OWNER TO postgres")
        self.execute_script(__file__, 'pagila-schema.sql')

        # Run pg_dump against source database
        srcdump = self.tempfile_path('pagila-src.dump')
        self.run_pg_dump(srcdump, True)

        # Create source YAML file
        srcyaml = self.tempfile_path('pagila-src.yaml')
        self.create_yaml(srcyaml, True)

        # Run pg_dump/dbtoyaml against empty target database
        emptydump = self.tempfile_path('empty.dump')
        self.run_pg_dump(emptydump)
        emptyyaml = self.tempfile_path('empty.yaml')
        self.create_yaml(emptyyaml)

        # Migrate the target database
        targsql = self.tempfile_path('pagila.sql')
        self.migrate_target(srcyaml, targsql)

        # Run pg_dump against target database
        targdump = self.tempfile_path('pagila.dump')
        self.run_pg_dump(targdump)

        # Create target YAML file
        targyaml = self.tempfile_path('pagila.yaml')
        self.create_yaml(targyaml)

        # diff pagila-src.dump against pagila.dump
        # order of triggers requires special handling
        adds = []
        subs = []
        for line in unified_diff(self.lines(srcdump), self.lines(targdump)):
            if line == '--- \n' or line == '+++ \n' or line.startswith('@@'):
                continue
            if line[:1] == '+':
                adds.append(line[1:-1])
            elif line[:1] == '-':
                subs.append(line[1:-1])
        subs = sorted(subs)
        for i, line in enumerate(sorted(adds)):
            assert line == subs[i]
        # diff pagila-src.yaml against pagila.yaml
        assert self.lines(srcyaml) == self.lines(targyaml)

        # Undo the changes
        self.migrate_target(emptyyaml, targsql)

        # Run pg_dump against target database
        self.run_pg_dump(targdump)

        # Create target YAML file
        self.create_yaml(targyaml)

        # diff empty.dump against pagila.dump
        assert self.lines(emptydump) == self.lines(targdump)
        # diff empty.yaml against pagila.yaml
        assert self.lines(emptyyaml) == self.lines(targyaml)

########NEW FILE########
__FILENAME__ = test_attribute
# -*- coding: utf-8 -*-
"""Test Attributes"""
from datetime import date

import pytest

from pyrseas.relation import Attribute


def test_attrib_defaults():
    "Create an attribute with default arguments"
    attr = Attribute('attr1')
    assert attr.name == 'attr1'
    assert attr.type == str
    assert attr.value == ''
    assert attr.nullable is False
    assert attr.sysdefault is False


def test_attrib_value():
    "Create an attribute with a given value"
    attr = Attribute('attr1', int, 123)
    assert attr.type == int
    assert attr.value == 123


def test_attrib_args():
    "Create an attribute with various arguments"
    attr = Attribute('attr1', float, 45.67, sysdefault=True)
    assert attr.name == 'attr1'
    assert attr.type == float
    assert attr.value == 45.67
    assert attr.nullable is False
    assert attr.sysdefault is True
    assert repr(attr) == "Attribute(attr1 float)"


def test_attrib_nullables():
    "Create nullable attributes that should get None as value"
    attr = Attribute('attr1', int, nullable=True)
    assert attr.value is None
    attr = Attribute('attr2', int, 0, nullable=True)
    assert attr.value is None
    attr = Attribute('attr3', str, '', nullable=True)
    assert attr.value is None
    attr = Attribute('attr4', date, nullable=True)
    assert attr.value is None


def test_attrib_value_error():
    "Validate declared type against value type"
    with pytest.raises(ValueError):
        Attribute('attr1', int, 12.34)


def test_attrib_nullable_value_error():
    "Validate declared type against value type for a nullable attribute"
    with pytest.raises(ValueError):
        Attribute('attr1', str, 123, nullable=True)


def test_attrib_not_nullable_no_value():
    "Ensure non-nullable, non-defaultable attribute has a value"
    with pytest.raises(ValueError):
        Attribute('attr1', date)


def test_attrib_not_nullable_sysdefault():
    "Allow non-nullable, system-defaultable attribute to be None"
    attr = Attribute('attr1', date, sysdefault=True)
    assert attr.value is None


def test_attrib_int_is_float():
    "Accept an int value for float type"
    attr = Attribute('attr1', float, 0)
    assert attr.type == float
    assert attr.value == 0.0


@pytest.mark.skipif("sys.version_info >= (3,0)")
def test_attrib_unicode_is_str():
    "Accept str as type synonym for unicode under Python 2"
    val = unicode('A string')
    attr = Attribute('attr1', value=val)
    assert attr.type == str
    assert attr.value == val

########NEW FILE########
__FILENAME__ = test_join
# -*- coding: utf-8 -*-
"""Test JoinRelations"""

import pytest

from pyrseas.relation import ProjAttribute, Projection, JoinRelation
from pyrseas.testutils import RelationTestCase

pr1 = Projection('arv', [ProjAttribute('a_id', int),
                         ProjAttribute('title'),
                         ProjAttribute('descr', nullable=True),
                         ProjAttribute('code', int)])

pr2 = Projection('arv', [ProjAttribute('title')])

pr3 = Projection('brv', [ProjAttribute('num', int),
                         ProjAttribute('name'),
                         ProjAttribute('a_id', int)])

jr1 = JoinRelation([pr1])
jr2 = JoinRelation([pr3, pr2], join="NATURAL JOIN arv a", extname='b_and_a')
jr3 = JoinRelation(
    [Projection('crv',
                [ProjAttribute('parent_id', int, basename='id1'),
                 ProjAttribute('code', int),
                 ProjAttribute('child_id', int, basename='id2')],
                rangevar='r'),
     Projection('arv',
                [ProjAttribute('parent_name', basename='name')],
                rangevar='p'),
     Projection('arv',
                [ProjAttribute('child_name', basename='name')],
                rangevar='c')],
    join="JOIN arv p ON (id1 = p.id) JOIN arv c ON (id2 = c.id)")


@pytest.fixture
def proj1(request):
    return pr1


@pytest.fixture
def joinrel1(request):
    return jr1


@pytest.fixture
def joinrel2(request):
    return jr2


@pytest.fixture
def joinrel3(request):
    return jr3


def test_projection(proj1):
    "Create a projection of a single relvar"
    assert proj1.rvname == 'arv'
    attr0 = proj1.attributes[0]
    assert attr0[0] == 'a_id'
    assert attr0[1].name == 'a_id'
    assert attr0[1].type == int
    assert attr0[1].nullable is False
    assert attr0[1].sysdefault is False
    assert attr0[1].basename == 'a_id'
    assert attr0[1].projection == proj1
    attr1 = proj1.attributes[1]
    assert attr1[0] == 'title'
    assert attr1[1].name == 'title'
    assert attr1[1].type == str
    attr2 = proj1.attributes[2]
    assert attr2[0] == 'descr'
    assert attr2[1].name == 'descr'
    assert attr2[1].type == str
    attr3 = proj1.attributes[3]
    assert attr3[0] == 'code'
    assert attr3[1].name == 'code'
    assert attr3[1].type == int
    assert proj1.rangevar == 'a'


def test_joinrel_single(joinrel1):
    "Create a join relation from a single projection"
    assert joinrel1.extname == 'arv'
    attr0 = joinrel1.attributes[0]
    assert attr0[1].name == 'a_id'
    assert attr0[1].type == int
    attr1 = joinrel1.attributes[1]
    assert attr1[0] == 'title'
    assert attr1[1].name == 'title'
    assert attr1[1].type == str
    assert joinrel1.from_clause == "arv a"


def test_joinrel_single_tuple_values(joinrel1):
    "Create a tuple for a single projection join from passed-in arguments"
    tup = joinrel1.tuple(1, 'abc', code=987)
    assert tup.a_id == 1
    assert tup.title == 'abc'
    assert tup.code == 987


def test_joinrel_invalid_attribute(joinrel1):
    "Create a tuple based on join relation but with incorrect type"
    with pytest.raises(ValueError):
        joinrel1.tuple(code=12.34)


def test_joinrel_unknown_attribute(joinrel1):
    "Create a tuple with an unknown attribute"
    with pytest.raises(KeyError):
        joinrel1.tuple(name='abc')


def test_joinrel_double(joinrel2):
    "Create a join relation from two projections"
    assert joinrel2.extname == 'b_and_a'
    attr0 = joinrel2.attributes[0]
    assert attr0[0] == 'num'
    assert attr0[1].name == 'num'
    assert attr0[1].type == int
    attr1 = joinrel2.attributes[1]
    assert attr1[0] == 'name'
    assert attr1[1].name == 'name'
    assert attr1[1].type == str
    attr2 = joinrel2.attributes[2]
    assert attr2[0] == 'a_id'
    assert attr2[1].name == 'a_id'
    assert attr2[1].type == int
    assert joinrel2.from_clause == "brv b NATURAL JOIN arv a"


def test_joinrel_three_way(joinrel3):
    "Create a join relation from three projections, two on the same relvar"
    assert joinrel3.extname == 'crv'
    attr0 = joinrel3.attributes[0]
    assert attr0[0] == 'parent_id'
    assert attr0[1].type == int
    assert attr0[1].basename == 'id1'
    attr1 = joinrel3.attributes[1]
    assert attr1[0] == 'code'
    assert attr1[1].type == int
    assert attr1[1].basename == 'code'
    attr2 = joinrel3.attributes[2]
    assert attr2[0] == 'child_id'
    assert attr2[1].type == int
    assert attr2[1].basename == 'id2'
    attr3 = joinrel3.attributes[3]
    assert attr3[0] == 'parent_name'
    assert attr3[1].type == str
    assert attr3[1].basename == 'name'
    attr4 = joinrel3.attributes[4]
    assert attr4[0] == 'child_name'
    assert attr4[1].type == str
    assert attr4[1].basename == 'name'
    assert joinrel3.from_clause == "crv r JOIN arv p ON (id1 = p.id) " \
        "JOIN arv c ON (id2 = c.id)"


def test_joinrel_dupe_rangevar(proj1):
    "Create a join relation with a duplicate rangevar"
    with pytest.raises(ValueError):
        JoinRelation([proj1, Projection('rv', [ProjAttribute('name')],
                                        rangevar='a')])


class TestJoinRel1(RelationTestCase):

    @pytest.fixture(autouse=True)
    def setup(self):
        self.relation = jr1
        self.relation.connect(self.db)
        self.pgdb.execute("DROP TABLE IF EXISTS arv CASCADE")
        self.pgdb.execute_commit(
            "CREATE TABLE arv (a_id integer PRIMARY KEY, "
            "title text NOT NULL, descr text, code integer NOT NULL)")

    def insert_multiple(self, count):
        self.pgdb.execute_commit(
            "INSERT INTO arv SELECT i, 'Title ' || i, 'Description ' || i, "
            "(i %% 3) + 1 FROM generate_series(1, %d) i" % count)

    def test_joinrel_get_several(self):
        "Get several tuples"
        self.insert_multiple(3)
        tuples = self.relation.subset()
        assert len(tuples) == 3
        assert tuples[0].a_id == 1
        assert tuples[0].title == 'Title 1'
        assert tuples[2].descr == 'Description 3'
        assert tuples[2].code == 1

    def test_joinrel_get_none(self):
        "Get several tuples but find none"
        assert len(self.relation.subset()) == 0

    def test_joinrel_get_slice(self):
        "Get a slice of tuples"
        self.insert_multiple(100)
        assert self.relation.count() == 100
        tuples = self.relation.subset(10, 30)
        assert len(tuples) == 10
        assert tuples[0].title == 'Title 31'
        assert tuples[9].code == 2

    def test_joinrel_search(self):
        "Get a subset of tuples by searching by title"
        self.insert_multiple(100)
        tuples = self.relation.subset(qry_args={'title': '7'})
        assert len(tuples) == 19
        assert tuples[2].title == 'Title 27'


class TestJoinRel2(RelationTestCase):

    @pytest.fixture(autouse=True)
    def setup(self):
        self.relation = jr2
        self.relation.connect(self.db)
        self.pgdb.execute("DROP TABLE IF EXISTS arv, brv CASCADE")
        self.pgdb.execute(
            "CREATE TABLE arv (a_id integer PRIMARY KEY, title text NOT NULL)")
        self.pgdb.execute(
            "CREATE TABLE brv (num integer PRIMARY KEY, name text NOT NULL, "
            "a_id integer NOT NULL REFERENCES arv (a_id))")
        self.pgdb.execute("INSERT INTO arv VALUES (1, 'John Doe'), "
                          "(2, 'Bob Smith'), (3, 'Peter Jones')")
        self.pgdb.execute_commit(
            "INSERT INTO brv SELECT i, 'Name ' || i, (i % 3) + 1 "
            "FROM generate_series(1, 100) i")

    def test_get_join_slice(self):
        "Get a slice of tuples from a join"
        assert self.relation.count() == 100
        tuples = self.relation.subset(10, 30)
        assert len(tuples) == 10
        assert tuples[0].name == 'Name 31'
        assert tuples[9].title == 'Bob Smith'

    def test_get_join_slice_order(self):
        "Get a slice of tuples from a join ordered by different attribute"
        tuples = self.relation.subset(10, 30, order=['title'])
        assert tuples[0].title == 'Bob Smith'
        assert tuples[9].title == 'John Doe'

    def test_get_join_slice_order_desc(self):
        "Get a slice of tuples from a join ordered DESCending"
        tuples = self.relation.subset(10, 0, order=['name DESC'])
        assert tuples[0].name == 'Name 99'
        assert tuples[9].title == 'John Doe'

    def test_get_join_order_by_unknown(self):
        "Get a slice of tuples ordered by unknown attribute"
        with pytest.raises(AttributeError):
            self.relation.subset(10, 30, order=['unknown'])

    def test_search_second_proj(self):
        "Get a subset of tuples by attribute of second projection"
        tuples = self.relation.subset(qry_args={'title': 'peter'})
        assert len(tuples) == 33

    def test_search_numeric_greater(self):
        "Get a subset of tuples by using greater than on integer attribute"
        tuples = self.relation.subset(qry_args={'num': '> 50'})
        assert len(tuples) == 50

    def test_search_two_args(self):
        "Get a subset of tuples by using two search attributes"
        tuples = self.relation.subset(qry_args={'title': 'peter',
                                                'num': '> 50'})
        assert len(tuples) == 16
        assert tuples[0].num == 53
        assert tuples[0].title == 'Peter Jones'
        assert tuples[15].num == 98


class TestJoinRel3(RelationTestCase):

    @pytest.fixture(autouse=True)
    def setup(self):
        self.relation = jr3
        self.relation.connect(self.db)
        self.pgdb.execute("DROP TABLE IF EXISTS arv, crv CASCADE")
        self.pgdb.execute(
            "CREATE TABLE arv (id integer PRIMARY KEY, name text NOT NULL)")
        self.pgdb.execute(
            "CREATE TABLE crv (id1 integer NOT NULL REFERENCES arv (id), "
            "id2 integer NOT NULL REFERENCES arv (id), "
            "code integer NOT NULL, PRIMARY KEY (id1, code, id2))")
        self.pgdb.execute(
            "INSERT INTO arv VALUES (1, 'John Doe'), (2, 'Bob Smith')")
        self.pgdb.execute(
            "INSERT INTO crv SELECT 1, 2, i FROM generate_series(11, 60) i")
        self.pgdb.execute_commit(
            "INSERT INTO crv SELECT 2, 1, i FROM generate_series(31, 80) i")

    def test_search_three_way(self):
        "Get a subset of tuple by parent_name"
        tuples = self.relation.subset(qry_args={'parent_name': 'Bob'},
                                      order=['parent_name', 'code'])
        assert len(tuples) == 50
        assert tuples[0].parent_name == 'Bob Smith'
        assert tuples[0].child_name == 'John Doe'
        assert tuples[0].code == 31
        assert tuples[49].code == 80

########NEW FILE########
__FILENAME__ = test_relvar
# -*- coding: utf-8 -*-
"""Test RelVars"""
from __future__ import unicode_literals

from copy import copy
from datetime import date, datetime, timedelta

import pytest
from psycopg2 import DatabaseError, IntegrityError

from pyrseas.relation import RelVar, Attribute
from pyrseas.testutils import RelationTestCase

TEST_DATA1 = {'title': "John Doe"}
TEST_DATA1x = {'id': 2, 'title': "Bob Smith"}
TEST_DATA2 = {'num': 123, 'name': "Name 1", 'id': 1}
TEST_DATA3 = {'id1': 1, 'id2': 2, 'code': 'ES', 'descr': 'Una descripción'}


rv1 = RelVar('rv1', [Attribute('id', int, sysdefault=True),
                     Attribute('title'),
                     Attribute('descr', nullable=True),
                     Attribute('updated', datetime, sysdefault=True)],
             key=['id'])

rv2 = RelVar('rv2', [Attribute('num', int), Attribute('name'),
                     Attribute('id', int)], key=['num'])

rv3 = RelVar('rv3', [Attribute('id1', int), Attribute('id2', int),
                     Attribute('code'), Attribute('descr'),
                     Attribute('created', date, sysdefault=True)],
             key=['id1', 'code', 'id2'])


@pytest.fixture
def relvar1(request):
    return rv1


def test_relvar_default_tuple(relvar1):
    "Create a tuple with default (blank) values"
    tup = relvar1.default_tuple()
    assert tup.id == 0
    assert tup.title == ''


def test_relvar_tuple_values(relvar1):
    "Create a tuple based on relvar and passed-in arguments"
    tup = relvar1.tuple(**TEST_DATA1)
    assert tup.title == TEST_DATA1['title']
    assert tup._heading == (('title', str), )


def test_relvar_invalid_attribute(relvar1):
    "Create a tuple based on relvar but with incorrect type"
    with pytest.raises(ValueError):
        relvar1.tuple(title=12.34)


def test_relvar_tuple_missing_required(relvar1):
    "Create a tuple without a required attribute"
    with pytest.raises(ValueError):
        relvar1.tuple(1)


def test_relvar_tuple_unknown_attribute(relvar1):
    "Create a tuple with an unknown attribute"
    with pytest.raises(KeyError):
        relvar1.tuple(code='abc')


class TestRelvar1(RelationTestCase):

    @pytest.fixture(autouse=True)
    def setup(self):
        self.relvar = rv1
        self.relvar.connect(self.db)
        self.pgdb.execute("DROP TABLE IF EXISTS rv1 CASCADE")
        self.pgdb.execute_commit(
            "CREATE TABLE rv1 (id serial PRIMARY KEY, "
            "title text NOT NULL UNIQUE, descr text, "
            "updated timestamp with time zone DEFAULT CURRENT_TIMESTAMP)")

    def insert_one(self):
        self.pgdb.execute_commit("INSERT INTO rv1 (title) VALUES (%(title)s)",
                                 (TEST_DATA1))

    def delete_one(self, id):
        self.pgdb.execute_commit("DELETE FROM rv1 WHERE id = %s", (id,))

    def get_one(self, id):
        return self.pgdb.fetchone("SELECT xmin, * FROM rv1 WHERE id = %s",
                                  (id,))

    def test_relvar_insert_one_serial(self):
        "Insert a tuple into a relvar with a sequenced primary key"
        newtuple = self.relvar.tuple(**TEST_DATA1)
        self.relvar.insert_one(newtuple)
        now = datetime.now()
        self.db.commit()
        row = self.get_one(1)
        assert row['title'] == newtuple.title
        assert (now - row['updated'].replace(tzinfo=None)) < timedelta(0, 1)

    def test_relvar_insert_one_override_pk(self):
        "Insert a tuple but override normal sequenced primary key value"
        data = TEST_DATA1.copy()
        data['id'] = 123
        newtuple = self.relvar.tuple(**data)
        self.relvar.insert_one(newtuple)
        self.db.commit()
        row = self.get_one(123)
        assert row['title'] == newtuple.title

    def test_relvar_insert_one_serial_return_pk(self):
        "Insert a tuple and return the generated primary key value"
        self.insert_one()
        newtuple = self.relvar.tuple(title=TEST_DATA1x['title'])
        retval = self.relvar.insert_one(newtuple, True)
        self.db.commit()
        row = self.get_one(retval.id)
        assert row['title'] == newtuple.title

    def test_relvar_insert_one_nullables(self):
        "Insert a tuple with nullable attributes as blanks"
        data = TEST_DATA1.copy()
        data['descr'] = ''
        newtuple = self.relvar.tuple(**data)
        self.relvar.insert_one(newtuple)
        self.db.commit()
        row = self.get_one(1)
        assert row['descr'] is None

    def test_relvar_dup_insert_pk(self):
        "Insert a duplicate by overriding normal sequenced primary key value"
        self.insert_one()
        data = TEST_DATA1.copy()
        data['id'] = 1
        newtuple = self.relvar.tuple(**data)
        with pytest.raises(IntegrityError):
            self.relvar.insert_one(newtuple)

    def test_relvar_dup_insert_alt_key(self):
        "Insert a duplicate on a unique attribute"
        self.insert_one()
        newtuple = self.relvar.tuple(**TEST_DATA1)
        with pytest.raises(IntegrityError):
            self.relvar.insert_one(newtuple)

    def test_relvar_get_one(self):
        "Retrieve a single tuple from a relvar"
        self.insert_one()
        now = datetime.now()
        currtuple = self.relvar.get_one(self.relvar.key_tuple(1))
        assert currtuple.id == 1
        assert currtuple.title == TEST_DATA1['title']
        assert (now - currtuple.updated.replace(tzinfo=None)) < timedelta(0, 1)

    def test_relvar_get_one_fail(self):
        "Fail to retrieve a single tuple from a relvar"
        assert self.relvar.get_one(self.relvar.key_tuple(1)) is None

    def test_relvar_update_one(self):
        "Update a single tuple in a relvar"
        self.insert_one()
        keytuple = self.relvar.key_tuple(1)
        currtuple = self.relvar.get_one(keytuple)
        currtuple.title = "Jane Doe"
        currtuple.updated = datetime.now()
        self.relvar.update_one(currtuple, keytuple)
        self.db.commit()
        row = self.get_one(1)
        assert row['title'] == currtuple.title
        assert row['updated'].replace(tzinfo=None) == currtuple.updated

    def test_relvar_update_one_from_current(self):
        "Update a single tuple from a fetched tuple"
        self.insert_one()
        keytuple = self.relvar.key_tuple(1)
        currtuple = self.relvar.get_one(keytuple)
        newtuple = copy(currtuple)
        newtuple.title = "Jane Doe"
        newtuple.updated = datetime.now()
        self.relvar.update_one(newtuple, keytuple, currtuple)
        self.db.commit()
        row = self.get_one(1)
        assert row['title'] == newtuple.title
        assert row['xmin'] != newtuple._tuple_version
        assert row['updated'].replace(tzinfo=None) == newtuple.updated

    def test_relvar_update_one_no_change(self):
        "Update a single tuple but without really changing anything"
        self.insert_one()
        keytuple = self.relvar.key_tuple(1)
        currtuple = self.relvar.get_one(keytuple)
        newtuple = copy(currtuple)
        newtuple.title = "John Doe"
        self.relvar.update_one(newtuple, keytuple, currtuple)
        self.db.commit()
        row = self.get_one(1)
        assert row['title'] == newtuple.title
        assert row['xmin'] == newtuple._tuple_version

    def test_relvar_update_missing(self):
        "Attempt to update a tuple that has been deleted since it was fetched"
        self.insert_one()
        keytuple = self.relvar.key_tuple(1)
        currtuple = self.relvar.get_one(keytuple)
        currtuple.title = "Jane Doe"
        self.delete_one(1)
        with pytest.raises(DatabaseError):
            self.relvar.update_one(currtuple, keytuple)

    def test_relvar_delete_one(self):
        "Delete a single tuple from a relvar"
        self.insert_one()
        keytuple = self.relvar.key_tuple(1)
        currtuple = self.relvar.get_one(keytuple)
        self.relvar.delete_one(keytuple, currtuple)
        self.db.commit()
        assert self.get_one(1) is None

    def test_relvar_delete_missing(self):
        "Attempt to delete a tuple that has been deleted since it was fetched"
        self.insert_one()
        keytuple = self.relvar.key_tuple(1)
        currtuple = self.relvar.get_one(keytuple)
        self.delete_one(1)
        with pytest.raises(DatabaseError):
            self.relvar.delete_one(currtuple, keytuple)


class TestRelvar2(RelationTestCase):

    @pytest.fixture(autouse=True)
    def setup(self):
        self.relvar = rv2
        self.relvar.connect(self.db)
        self.pgdb.execute("DROP TABLE IF EXISTS rv2, rv1 CASCADE")
        self.pgdb.execute("CREATE TABLE rv1 (id integer PRIMARY KEY, "
                          "title text NOT NULL)")
        self.pgdb.execute("CREATE TABLE rv2 (num integer PRIMARY KEY, "
                          "name text NOT NULL, "
                          "id integer NOT NULL REFERENCES rv1 (id))")
        self.pgdb.execute_commit("INSERT INTO rv1 VALUES (1, %(title)s)",
                                 (TEST_DATA1),)

    def insert_one(self):
        self.pgdb.execute_commit(
            "INSERT INTO rv2 VALUES (%(num)s, %(name)s, %(id)s)",
            (TEST_DATA2),)

    def get_one(self, num):
        return self.pgdb.fetchone("SELECT xmin, * FROM rv2 WHERE num = %s",
                                  (num,))

    def test_relvar_insert_fk(self):
        "Insert a tuple into a relvar that references another"
        # This also tests insert into non-sequenced primary key
        newtuple = self.relvar.tuple(**TEST_DATA2)
        self.relvar.insert_one(newtuple)
        self.db.commit()
        row = self.get_one(123)
        assert row['name'] == newtuple.name
        assert row['id'] == newtuple.id

    def test_relvar_dup_insert(self):
        "Insert a duplicate primary key value"
        self.insert_one()
        newtuple = self.relvar.tuple(**TEST_DATA2)
        with pytest.raises(IntegrityError):
            self.relvar.insert_one(newtuple)

    def test_relvar_insert_bad_fk(self):
        "Insert a tuple into a relvar with invalid foreign key"
        data = TEST_DATA2.copy()
        data['id'] = 2
        newtuple = self.relvar.tuple(**data)
        with pytest.raises(IntegrityError):
            self.relvar.insert_one(newtuple)

    def test_relvar_update_key(self):
        "Update a tuple's primary key"
        self.insert_one()
        keytuple = self.relvar.key_tuple(123)
        currtuple = self.relvar.get_one(keytuple)
        currtuple.num = 456
        self.relvar.update_one(currtuple, keytuple)
        self.db.commit()
        row = self.get_one(456)
        assert row['num'] == 456
        assert row['name'] == TEST_DATA2['name']
        assert row['id'] == TEST_DATA2['id']

    def test_relvar_update_fk(self):
        "Update a tuple's foreign key"
        self.pgdb.execute_commit("INSERT INTO rv1 VALUES (%(id)s, %(title)s)",
                                 (TEST_DATA1x))
        self.insert_one()
        keytuple = self.relvar.key_tuple(123)
        currtuple = self.relvar.get_one(keytuple)
        currtuple.id = 2
        self.relvar.update_one(currtuple, keytuple)
        self.db.commit()
        row = self.pgdb.fetchone(
            "SELECT title FROM rv1 NATURAL JOIN rv2 WHERE num = 123")
        assert row['title'] == TEST_DATA1x['title']

    def test_relvar_update_fk_fail(self):
        "Update a tuple's foreign key to an unknown value"
        self.pgdb.execute_commit("DELETE FROM rv1 WHERE id = 2")
        self.insert_one()
        keytuple = self.relvar.key_tuple(123)
        currtuple = self.relvar.get_one(keytuple)
        currtuple.id = 2
        with pytest.raises(IntegrityError):
            self.relvar.update_one(currtuple, keytuple)


class TestRelvar3(RelationTestCase):

    @pytest.fixture(autouse=True)
    def setup(self):
        self.relvar = rv3
        self.relvar.connect(self.db)
        self.pgdb.execute("DROP TABLE IF EXISTS rv3, rv1 CASCADE")
        self.pgdb.execute("CREATE TABLE rv1 (id integer PRIMARY KEY, "
                          "title text NOT NULL)")
        self.pgdb.execute(
            "CREATE TABLE rv3 (id1 integer NOT NULL REFERENCES rv1 (id), "
            "id2 integer NOT NULL REFERENCES rv1 (id), "
            "code char(2) NOT NULL, descr text NOT NULL, "
            "created date NOT NULL DEFAULT CURRENT_DATE, "
            "PRIMARY KEY (id1, code, id2))")
        self.pgdb.execute_commit("INSERT INTO rv1 VALUES (1, %(title)s)",
                                 (TEST_DATA1),)
        self.pgdb.execute_commit("INSERT INTO rv1 VALUES (%(id)s, %(title)s)",
                                 (TEST_DATA1x),)

    def insert_one(self):
        self.pgdb.execute_commit(
            "INSERT INTO rv3 VALUES (%(id1)s, %(id2)s, %(code)s, %(descr)s)",
            (TEST_DATA3),)

    def get_one(self, data):
        return self.pgdb.fetchone(
            "SELECT xmin, * FROM rv3 WHERE id1 = %(id1)s AND "
            "id2 = %(id2)s AND code = %(code)s", (data))

    def test_relvar_key_tuple(self):
        "Create a key tuple with both args and keyword args"
        tup = self.relvar.key_tuple(123, code='EN', id2=456)
        assert tup.id1 == 123
        assert tup.code == 'EN'
        assert tup.id2 == 456

    def test_relvar_insert_multi_key(self):
        "Insert a tuple into a relvar with a multi-attribute key"
        newtuple = self.relvar.tuple(**TEST_DATA3)
        self.relvar.insert_one(newtuple)
        self.db.commit()
        row = self.get_one(TEST_DATA3)
        assert row['id1'] == newtuple.id1
        assert row['code'] == newtuple.code
        assert row['id2'] == newtuple.id2
        assert row['descr'] == newtuple.descr
        assert row['created'] == date.today()

    def test_relvar_update_one(self):
        "Update a tuple in a relvar with a multi-attribute key"
        self.insert_one()
        keytuple = self.relvar.key_tuple(**TEST_DATA3)
        currtuple = self.relvar.get_one(keytuple)
        currtuple.code = 'FR'
        currtuple.descr = "Une description"
        self.relvar.update_one(currtuple, keytuple)
        self.db.commit()
        data = TEST_DATA3.copy()
        data['code'] = currtuple.code
        row = self.get_one(data)
        assert row['id1'] == currtuple.id1
        assert row['code'] == currtuple.code
        assert row['id2'] == currtuple.id2
        assert row['descr'] == currtuple.descr

########NEW FILE########
__FILENAME__ = test_tuple
# -*- coding: utf-8 -*-
"""Test Tuples"""
from pytest import raises

from pyrseas.relation import Attribute, Tuple
from pyrseas.relation.tuple import tuple_values_dict


def test_tuple_no_attribs():
    "Create a tuple with no attributes"
    tup = Tuple([])
    assert tup._heading == ()


def test_tuple_one_attrib():
    "Create a tuple with one attribute"
    tup = Tuple(Attribute('attr1'))
    assert tup.attr1 == ''
    assert tup._heading == (('attr1', str), )


def test_tuple_one_attrib_value():
    "Create a tuple with an attribute with a specified value"
    tup = Tuple(Attribute('attr1', int, 123))
    assert tup.attr1 == 123
    assert tup._heading == (('attr1', int), )


def test_tuple_multiple_attribs():
    "Create a tuple with multiple attributes"
    tup = Tuple([Attribute('attr1', int, 123), Attribute('attr2', str, 'abc'),
                 Attribute('attr3', float, 45.67)])
    assert tup.attr1 == 123
    assert tup.attr2 == 'abc'
    assert tup.attr3 == 45.67
    assert tup._sysdefault_attribs == []
    assert tup._nullable_attribs == []
    assert tup._tuple_version is None
    assert tup._heading == (('attr1', int), ('attr2', str), ('attr3', float))
    assert repr(tup) == "Tuple(attr1 int, attr2 str, attr3 float)"


def test_tuple_nullable_attribs():
    "Create a tuple with nullable attributes"
    tup = Tuple([Attribute('attr1', int, 123, nullable=True),
                 Attribute('attr2', str, 'abc'),
                 Attribute('attr3', float, 45.67, nullable=True)])
    assert tup._nullable_attribs == ['attr1', 'attr3']


def test_tuple_nullable_values():
    "Test attributes with nullable values"
    tup = Tuple([Attribute('attr1', int, 123),
                 Attribute('attr2', str, '', nullable=True),
                 Attribute('attr3', int, nullable=True)])
    assert tup.attr2 is None
    assert tup.attr3 is None


def test_tuple_sysdefault_attrib():
    "Create a tuple with a system default attribute"
    tup = Tuple([Attribute('attr1', int, sysdefault=True),
                 Attribute('attr2', str, 'abc')])
    assert tup.attr1 == 0
    assert tup._sysdefault_attribs == ['attr1']


def test_tuple_disallowed_name():
    "Test attribute names don't use internal Tuple attribute/method names"
    with raises(AssertionError):
        Tuple([Attribute('_heading', int, 123)])


def test_tuple_set_reserved_name():
    "Test reserved attributes cannot be set except for _tuple_version"
    tup = Tuple([Attribute('attr1', int, 123)])
    with raises(AssertionError):
        tup._heading = ['abc']
    tup._tuple_version = '1234567'
    assert tup._tuple_version == '1234567'


def test_tuple_set_unknown_attribute():
    "Test unknown attribute cannot be set"
    tup = Tuple([Attribute('attr1', int, 123)])
    with raises(AttributeError):
        tup.attr2 = 234


def test_tuple_set_attribute():
    tup = Tuple([Attribute('attr1', int, 123)])
    tup.attr1 = 456
    assert tup.attr1 == 456


def test_tuple_set_attribute_value_error():
    tup = Tuple([Attribute('attr1', int, 123)])
    with raises(ValueError):
        tup.attr1 = 'abc'


def test_tuple_set_nullable_attribute():
    tup = Tuple([Attribute('attr1', int, 123, nullable=True)])
    tup.attr1 = 0
    assert tup.attr1 is None


def test_tuple_values_dict():
    "Test tuple_values_dict function"
    tup = Tuple([Attribute('attr1', int, 123), Attribute('attr2', str, 'abc'),
                 Attribute('attr3', float, 45.67)])
    assert tuple_values_dict(tup) == {'attr1': 123, 'attr2': 'abc',
                                      'attr3': 45.67}


def test_tuple_changed_values_dict():
    "Test tuple_values_dict with second tuple"
    tup1 = Tuple([Attribute('attr1', int, 123), Attribute('attr2', str, 'abc'),
                  Attribute('attr3', float, 45.67)])
    tup2 = Tuple([Attribute('attr1', int, 123), Attribute('attr2', str, 'def'),
                  Attribute('attr3', float, 98.76),
                  Attribute('attr4', str, 'xyz')])
    assert tuple_values_dict(tup1, tup2) == {'attr2': 'def', 'attr3': 98.76,
                                             'attr4': 'xyz'}


def test_tuple_changed_nullable_values():
    "Test tuple_values_dict with changed nullable values"
    tup1 = Tuple([Attribute('attr1', int, 123), Attribute('attr2', str, 'abc'),
                 Attribute('attr3', float, 45.67)])
    tup2 = Tuple([Attribute('attr1', int, 123),
                  Attribute('attr2', str, '', nullable=True),
                 Attribute('attr3', float, 0, nullable=True)])
    assert tuple_values_dict(tup1, tup2) == {'attr2': None, 'attr3': None}

########NEW FILE########
__FILENAME__ = test_config
# -*- coding: utf-8 -*-
"""Test configuration files"""

import os
import sys

from pyrseas.config import Config
from pyrseas.cmdargs import cmd_parser, parse_args
from pyrseas.yamlutil import yamldump

USER_CFG_DATA = {'database': {'port': 5433},
                 'output': {'version_comment': True}}
CFG_TABLE_DATA = {'schema public': ['t1', 't2']}
CFG_DATA = {'datacopy': CFG_TABLE_DATA}
CFG_FILE = 'testcfg.yaml'


def test_defaults():
    "Create a configuration with defaults"
    cfg = Config()
    for key in ['audit_columns', 'functions', 'function_templates', 'columns',
                'triggers']:
        assert key in cfg['augmenter']
    for key in ['metadata', 'data']:
        assert key in cfg['repository']


def test_user_config(tmpdir):
    "Test a user configuration file"
    f = tmpdir.join(CFG_FILE)
    f.write(yamldump(USER_CFG_DATA))
    os.environ["PYRSEAS_USER_CONFIG"] = f.strpath
    cfg = Config()
    assert cfg['database'] == {'port': 5433}
    assert cfg['output'] == {'version_comment': True}


def test_repo_config(tmpdir):
    "Test a repository configuration file"
    ucfg = tmpdir.join(CFG_FILE)
    ucfg.write(yamldump({'repository': {'path': tmpdir.strpath}}))
    f = tmpdir.join("config.yaml")
    f.write(yamldump(CFG_DATA))
    os.environ["PYRSEAS_USER_CONFIG"] = ucfg.strpath
    cfg = Config()
    assert cfg['datacopy'] == CFG_TABLE_DATA


def test_cmd_parser(tmpdir):
    "Test parsing a configuration file specified on the command line"
    f = tmpdir.join(CFG_FILE)
    f.write(yamldump(CFG_DATA))
    sys.argv = ['testprog', 'testdb', '--config', f.strpath]
    os.environ["PYRSEAS_USER_CONFIG"] = ''
    parser = cmd_parser("Test description", '0.0.1')
    cfg = parse_args(parser)
    assert cfg['datacopy'] == CFG_TABLE_DATA


def test_parse_repo_config(tmpdir):
    "Test parsing a repository configuration file in the current directory"
    f = tmpdir.join('config.yaml')
    f.write(yamldump(CFG_DATA))
    os.chdir(tmpdir.strpath)
    sys.argv = ['testprog', 'testdb']
    os.environ["PYRSEAS_USER_CONFIG"] = ''
    parser = cmd_parser("Test description", '0.0.1')
    cfg = parse_args(parser)
    assert cfg['datacopy'] == CFG_TABLE_DATA


def test_repo_user_config(tmpdir):
    "Test a repository path specified in the user config"
    usercfg = {'repository': {'path': tmpdir.strpath}}
    userf = tmpdir.join("usercfg.yaml")
    userf.write(yamldump(usercfg))
    os.environ["PYRSEAS_USER_CONFIG"] = userf.strpath
    repof = tmpdir.join("config.yaml")
    repof.write(yamldump(CFG_DATA))
    cfg = Config()
    assert cfg['datacopy'] == CFG_TABLE_DATA

########NEW FILE########
