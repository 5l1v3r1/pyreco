__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# mincss documentation build configuration file, created by
# sphinx-quickstart on Fri Jan 11 14:08:28 2013.
#
# This file is execfile()d with the current directory set to its containing
# dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.


# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = []

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'mincss'
copyright = u'2013, Peter Bengtsson'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.1'
# The full version, including alpha/beta/rc tags.
release = '0.1'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all
# documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'mincssdoc'


# -- Options for LaTeX output --------------------------------------------

latex_elements = {
    # The paper size ('letterpaper' or 'a4paper').
    #'papersize': 'letterpaper',

    # The font size ('10pt', '11pt' or '12pt').
    #'pointsize': '10pt',

    # Additional stuff for the LaTeX preamble.
    #'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass
# [howto/manual]).
latex_documents = [
    ('index', 'mincss.tex', u'mincss Documentation',
     u'Peter Bengtsson', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'mincss', u'mincss Documentation',
     [u'Peter Bengtsson'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
    ('index', 'mincss', u'mincss Documentation',
     u'Peter Bengtsson', 'mincss', 'One line description of project.',
     'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

########NEW FILE########
__FILENAME__ = main
from __future__ import print_function

import io
import os
import time

from .processor import Processor


def run(args):
    options = {'debug': args.verbose}
    if args.phantomjs_path:
        options['phantomjs'] = args.phantomjs_path
    elif args.phantomjs:
        options['phantomjs'] = True
    p = Processor(**options)
    t0 = time.time()
    p.process(args.url)
    t1 = time.time()
    print('TOTAL TIME ', t1 - t0)
    for inline in p.inlines:
        print('ON', inline.url)
        print('AT line', inline.line)
        print('BEFORE '.ljust(79, '-'))
        print(inline.before)
        print('AFTER '.ljust(79, '-'))
        print(inline.after)
        print()

    output_dir = args.outputdir
    if not os.path.isdir(output_dir):
        os.mkdir(output_dir)
    for link in p.links:
        print('FOR', link.href)
        orig_name = link.href.split('/')[-1]
        with io.open(os.path.join(output_dir, orig_name), 'w') as f:
            f.write(link.after)
        before_name = 'before_' + link.href.split('/')[-1]
        with io.open(os.path.join(output_dir, before_name), 'w') as f:
            f.write(link.before)
        print('Files written to', output_dir)
        print()
        print(
            '(from %d to %d saves %d)' %
            (len(link.before), len(link.after),
             len(link.before) - len(link.after))
        )

    return 0


def main():
    import argparse
    parser = argparse.ArgumentParser()
    add = parser.add_argument
    add('url', type=str,
        help='URL to process')
    add('--outputdir', action='store',
        default='./output',
        help='directory where to put output (default ./output)')
    add('-v', '--verbose', action='store_true',
        help='increase output verbosity')
    add('--phantomjs', action='store_true',
        help='Use PhantomJS to download the source')
    add('--phantomjs-path', action='store',
        default='',
        help='Where is the phantomjs executable')

    args = parser.parse_args()
    return run(args)

########NEW FILE########
__FILENAME__ = processor
from __future__ import print_function

import contextlib
import functools
import os
import sys
import random
import re
import time
import subprocess

from lxml import etree
from lxml.cssselect import CSSSelector, SelectorSyntaxError, ExpressionError

try:
    from urllib.parse import urljoin
    from urllib.request import urlopen
except ImportError:
    from urlparse import urljoin
    from urllib import urlopen


try:
    unicode
except NameError:
    unicode = str


RE_FIND_MEDIA = re.compile('(@media.+?)(\{)', re.DOTALL | re.MULTILINE)
RE_NESTS = re.compile('@(-|keyframes).*?({)', re.DOTALL | re.M)
RE_CLASS_DEF = re.compile('\.([\w-]+)')
RE_ID_DEF = re.compile('#([\w-]+)')


EXCEPTIONAL_SELECTORS = (
    'html',
)


DOWNLOAD_JS = os.path.join(
    os.path.dirname(__file__),
    'download.js'
)


class ParserError(Exception):

    """happens when we fail to parse the HTML."""


class DownloadError(Exception):

    """happens when we fail to down the URL."""


def _get_random_string():
    p = 'abcdefghijklmopqrstuvwxyz'
    pl = list(p)
    random.shuffle(pl)
    return ''.join(pl)


class Processor(object):

    def __init__(self,
                 debug=False,
                 preserve_remote_urls=True,
                 phantomjs=False,
                 phantomjs_options=None,
                 optimize_lookup=True):
        self.debug = debug
        self.preserve_remote_urls = preserve_remote_urls
        self.blocks = {}
        self.inlines = []
        self.links = []
        self._bodies = []
        self.optimize_lookup = optimize_lookup
        self._all_ids = set()
        self._all_classes = set()
        self.phantomjs = phantomjs
        self.phantomjs_options = phantomjs_options

    def _download(self, url):
        try:
            with contextlib.closing(urlopen(url)) as response:
                if response.getcode() is not None:
                    if response.getcode() != 200:
                        raise DownloadError(
                            '%s -- %s ' % (url, response.getcode())
                        )
                content = response.read()
                return unicode(content,
                               get_charset(response))
        except IOError:
            raise IOError(url)

    def _download_with_phantomjs(self, url):
        if self.phantomjs is True:
            # otherwise, assume it's a path
            self.phantomjs = 'phantomjs'
        elif not os.path.isfile(self.phantomjs):
            raise IOError('%s is not a path to phantomjs' % self.phantomjs)

        command = [self.phantomjs]
        if self.phantomjs_options:
            if 'load-images' not in self.phantomjs_options:
                # not entirely sure if this helps but there can't be any point
                # at all to download image for mincss
                self.phantomjs_options['load-images'] = 'no'
            for key, value in self.phantomjs_options.items():
                command.append('--%s=%s' % (key, value))

        command.append(DOWNLOAD_JS)
        assert ' ' not in url
        command.append(url)

        t0 = time.time()
        process = subprocess.Popen(
            ' '.join(command),
            shell=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        out, err = process.communicate()
        t1 = time.time()
        if self.debug:
            print('Took', t1 - t0, 'seconds to download with PhantomJS')

        return unicode(out, 'utf-8')

    def process(self, *urls):
        for url in urls:
            self.process_url(url)

        for identifier in sorted(self.blocks.keys(), key=lambda x: str(x[0])):
            content = self.blocks[identifier]
            processed = self._process_content(content, self._bodies)

            if isinstance(identifier[0], int):
                line, url = identifier
                self.inlines.append(
                    InlineResult(
                        line,
                        url,
                        content,
                        processed
                    )
                )
            else:
                url, href = identifier
                self.links.append(
                    LinkResult(
                        href,
                        content,
                        processed
                    )
                )

    def process_url(self, url):
        if self.phantomjs:
            html = self._download_with_phantomjs(url)
        else:
            html = self._download(url)
        self.process_html(html.strip(), url=url)

    def process_html(self, html, url):
        parser = etree.HTMLParser(encoding='utf-8')
        tree = etree.fromstring(html.encode('utf-8'), parser).getroottree()
        page = tree.getroot()

        if page is None:
            print(repr(html))
            raise ParserError('Could not parse the html')

        lines = html.splitlines()
        body, = CSSSelector('body')(page)
        self._bodies.append(body)
        if self.optimize_lookup:
            for each in body.iter():
                identifier = each.attrib.get('id')
                if identifier:
                    self._all_ids.add(identifier)
                classes = each.attrib.get('class')
                if classes:
                    for class_ in classes.split():
                        self._all_classes.add(class_)

        for style in CSSSelector('style')(page):
            first_line = style.text.strip().splitlines()[0]
            for i, line in enumerate(lines):
                if line.count(first_line):
                    key = (i + 1, url)
                    self.blocks[key] = style.text
                    break

        for link in CSSSelector('link')(page):
            if (
                link.attrib.get('rel', '') == 'stylesheet' or
                link.attrib['href'].lower().split('?')[0].endswith('.css')
            ):
                link_url = self.make_absolute_url(url, link.attrib['href'])
                key = (link_url, link.attrib['href'])
                self.blocks[key] = self._download(link_url)
                if self.preserve_remote_urls:
                    self.blocks[key] = self._rewrite_urls(
                        self.blocks[key],
                        link_url
                    )

    def _rewrite_urls(self, content, link_url):
        """Suppose you run mincss on www.example.org and it references:

            <link href="http://cdn.example.org">

        and in the CSS it references an image as:

            background: url(/foo.png)

        Then rewrite this to become:

            background: url(http://cdn.example.org/foo.png)

        """
        css_url_regex = re.compile('url\(([^\)]+)\)')

        def css_url_replacer(match, href=None):
            filename = match.groups()[0]
            bail = match.group()
            if (
                (filename.startswith('"') and filename.endswith('"')) or
                (filename.startswith("'") and filename.endswith("'"))
            ):
                filename = filename[1:-1]
            if 'data:image' in filename or '://' in filename:
                return bail
            if filename == '.':
                # this is a known IE hack in CSS
                return bail

            new_filename = urljoin(href, filename)
            return 'url("%s")' % new_filename

        content = css_url_regex.sub(
            functools.partial(css_url_replacer, href=link_url),
            content
        )
        return content

    def _process_content(self, content, bodies):
        # Find all of the unique media queries

        comments = []
        _css_comments = re.compile(r'/\*.*?\*/', re.MULTILINE | re.DOTALL)
        no_mincss_blocks = []

        def commentmatcher(match):
            whole = match.group()
            # are we in a block or outside
            nearest_close = content[:match.start()].rfind('}')
            nearest_open = content[:match.start()].rfind('{')
            next_close = content[match.end():].find('}')
            next_open = content[match.end():].find('{')

            outside = False
            if nearest_open == -1 and nearest_close == -1:
                # it's at the very beginning of the file
                outside = True
            elif next_open == -1 and next_close == -1:
                # it's at the very end of the file
                outside = True
            elif nearest_close == -1 and nearest_open > -1:
                outside = False
            elif nearest_close > -1 and nearest_open > -1:
                outside = nearest_close > nearest_open
            else:
                raise Exception('can this happen?!')

            if outside:
                temp_key = '@%scomment{}' % _get_random_string()
            else:
                temp_key = '%sinnercomment' % _get_random_string()
                if re.findall(r'\bno mincss\b', match.group()):
                    no_mincss_blocks.append(temp_key)

            comments.append(
                (temp_key, whole)
            )
            return temp_key

        content = _css_comments.sub(commentmatcher, content)
        if no_mincss_blocks:
            no_mincss_regex = re.compile(
                '|'.join(re.escape(x) for x in no_mincss_blocks)
            )
        else:
            no_mincss_regex = None

        nests = [(m.group(1), m) for m in RE_NESTS.finditer(content)]
        _nests = []
        for _, m in nests:
            __, whole = self._get_contents(m, content)
            _nests.append(whole)
        # once all nests have been spotted, temporarily replace them

        queries = [(m.group(1), m) for m in RE_FIND_MEDIA.finditer(content)]
        inner_improvements = []

        for nest in _nests:
            temp_key = '@%snest{}' % _get_random_string()
            inner_improvements.append(
                (temp_key, nest, nest)
            )

        # Consolidate the media queries
        for (query, m) in queries:
            inner, whole = self._get_contents(m, content)
            improved_inner = self._process_content(inner, bodies)
            if improved_inner.strip():
                improved = query.rstrip() + ' {' + improved_inner + '}'
            else:
                improved = ''
            temp_key = '@%s{}' % _get_random_string()
            inner_improvements.append(
                (temp_key, whole, improved)
            )

        for temp_key, old, __ in inner_improvements:
            assert old in content
            content = content.replace(old, temp_key)

        _regex = re.compile('((.*?){(.*?)})', re.DOTALL | re.M)

        _already_found = set()
        _already_tried = set()

        def matcher(match):
            whole, selectors, bulk = match.groups()
            selectors = selectors.split('*/')[-1].lstrip()
            if selectors.strip().startswith('@'):
                return whole
            if no_mincss_regex and no_mincss_regex.findall(bulk):
                return no_mincss_regex.sub('', whole)

            improved = selectors
            perfect = True
            selectors_split = [
                x.strip() for x in
                selectors.split(',')
                if x.strip() and not x.strip().startswith(':')
            ]
            for selector in selectors_split:
                s = selector.strip()
                if s in EXCEPTIONAL_SELECTORS:
                    continue

                if s in _already_found:
                    found = True
                elif s in _already_tried:
                    found = False
                else:
                    found = self._found(bodies, s)

                if found:
                    _already_found.add(s)
                else:
                    _already_tried.add(s)
                    perfect = False
                    improved = re.sub(
                        '%s,?\s*' % re.escape(s),
                        '',
                        improved,
                        count=1
                    )

            if perfect:
                return whole
            if improved != selectors:
                if not improved.strip():
                    return ''
                else:
                    improved = re.sub(',\s*$', ' ', improved)
                    whole = whole.replace(selectors, improved)
            return whole

        fixed = _regex.sub(matcher, content)

        for temp_key, __, improved in inner_improvements:
            assert temp_key in fixed
            fixed = fixed.replace(temp_key, improved)
        for temp_key, whole in comments:
            # note, `temp_key` might not be in the `fixed` thing because the
            # comment could have been part of a selector that is entirely
            # removed
            fixed = fixed.replace(temp_key, whole)
        return fixed

    def _get_contents(self, match, original_content):
        # we are starting the character after the first opening brace
        open_braces = 1
        position = match.end()
        content = ''
        while open_braces > 0:
            c = original_content[position]
            if c == '{':
                open_braces += 1
            if c == '}':
                open_braces -= 1
            content += c
            position += 1
        return (
            content[:-1].strip(),
            # the last closing brace gets captured, drop it
            original_content[match.start():position]
        )

    def _found(self, bodies, selector):
        if self._all_ids:
            try:
                id_ = RE_ID_DEF.findall(selector)[0]
                if id_ not in self._all_ids:
                    # don't bother then
                    return False
            except IndexError:
                pass

        if self._all_classes:
            for class_ in RE_CLASS_DEF.findall(selector):
                if class_ not in self._all_classes:
                    # don't bother then
                    return False

        r = self._selector_query_found(bodies, selector)
        return r

    def _selector_query_found(self, bodies, selector):
        selector = selector.split(':')[0]

        if '}' in selector:
            # XXX does this ever happen any more?
            return

        for body in bodies:
            try:
                for _ in CSSSelector(selector)(body):
                    return True
            except SelectorSyntaxError:
                print('TROUBLEMAKER', file=sys.stderr)
                print(repr(selector), file=sys.stderr)
            except ExpressionError:
                print('EXPRESSIONERROR', file=sys.stderr)
                print(repr(selector), file=sys.stderr)
        return False

    @staticmethod
    def make_absolute_url(url, href):
        return urljoin(url, href)


class _Result(object):

    def __init__(self, before, after):
        self.before = before
        self.after = after


class InlineResult(_Result):

    def __init__(self, line, url, *args):
        self.line = line
        self.url = url
        super(InlineResult, self).__init__(*args)


class LinkResult(_Result):

    def __init__(self, href, *args):
        self.href = href
        super(LinkResult, self).__init__(*args)


def get_charset(response, default='utf-8'):
    """Return encoding."""
    try:
        # Python 3.
        return response.info().get_param('charset', default)
    except AttributeError:
        # Python 2.
        content_type = response.headers['content-type']
        split_on = 'charset='
        if split_on in content_type:
            return content_type.split(split_on)[-1]
        else:
            return default

########NEW FILE########
__FILENAME__ = __main__
#!/usr/bin/env python

import sys

from . import main


if __name__ == '__main__':
    sys.exit(main.main())

########NEW FILE########
__FILENAME__ = app
#!/usr/bin/env python

from __future__ import print_function
import codecs
import datetime
import os
import functools
import logging
import hashlib
import re
import shutil
import time

try:
    from urllib.parse import urljoin, urlparse
    from urllib.request import urlopen
except ImportError:
    from urlparse import urljoin, urlparse
    from urllib import urlopen

from lxml import etree
from lxml.cssselect import CSSSelector

from flask import Flask, abort, make_response, request
app = Flask(__name__)

import sys
# do this to help development
sys.path.insert(0, os.path.normpath('../'))
from mincss.processor import Processor


try:
    unicode
except NameError:
    unicode = str


CACHE_DIR = os.path.join(
    os.path.dirname(__file__),
    '.cache'
)


CLOSING_REGEX = re.compile(
    '(<(script|iframe|textarea|div)\s*[^>]+/>)',
    flags=re.M | re.DOTALL
)


@app.route('/cache/<path:path>')
def cache(path):
    source = os.path.join(CACHE_DIR, path)
    with open(source) as f:
        response = make_response(f.read())
        response.headers['Content-type'] = 'text/css'
        return response


def download(url):
    html = urlopen(url).read()
    return unicode(html, 'utf-8')


@app.route('/<path:path>')
def proxy(path):
    if path == 'favicon.ico':
        abort(404)
    url = path
    if not path.count('://'):
        url = 'http://' + url

    query = urlparse(request.url).query
    if query:
        url += '?%s' % query
    logging.info('Downloading %s' % url)
    t0 = time.time()
    html = download(url)
    t1 = time.time()
    print('%.4f seconds to download' % (t1 - t0))

    p = Processor(debug=False, optimize_lookup=True)
    # since we've already download the HTML
    t0 = time.time()
    p.process_html(html, url)
    t1 = time.time()
    p.process()
    t2 = time.time()
    print('%.4f seconds to parse and process' % (t2 - t1))

    collect_stats = request.args.get('MINCSS_STATS', False)
    stats = []
    css_url_regex = re.compile('url\(([^\)]+)\)')

    def css_url_replacer(match, href=None):
        filename = match.groups()[0]
        bail = match.group()

        if (
            (filename.startswith('"') and filename.endswith('"')) or
            (filename.startswith("'") and filename.endswith("'"))
        ):
            filename = filename[1:-1]
        if 'data:image' in filename or '://' in filename:
            return bail
        if filename == '.':
            # this is a known IE hack in CSS
            return bail

        new_filename = urljoin(url, filename)
        return 'url("%s")' % new_filename

    for i, each in enumerate(p.inlines):
        # this should be using CSSSelector instead
        new_inline = each.after
        new_inline = css_url_regex.sub(
            functools.partial(css_url_replacer, href=url),
            new_inline
        )
        stats.append(
            ('inline %s' % (i + 1), each.before, each.after)
        )
        html = html.replace(each.before, new_inline)

    parser = etree.HTMLParser()
    stripped = html.strip()
    tree = etree.fromstring(stripped, parser).getroottree()
    page = tree.getroot()

    # lxml inserts a doctype if none exists, so only include it in
    # the root if it was in the original html.
    was_doctype = tree.docinfo.doctype

    links = dict((x.href, x) for x in p.links)

    for link in CSSSelector('link')(page):
        if (
            link.attrib.get('rel', '') == 'stylesheet' or
            link.attrib['href'].lower().split('?')[0].endswith('.css')
        ):
            hash_ = hashlib.md5(url + link.attrib['href']).hexdigest()[:7]
            now = datetime.date.today()
            destination_dir = os.path.join(
                CACHE_DIR,
                str(now.year),
                str(now.month),
                str(now.day),
            )
            mkdir(destination_dir)
            new_css = links[link.attrib['href']].after
            stats.append((
                link.attrib['href'],
                links[link.attrib['href']].before,
                links[link.attrib['href']].after
            ))
            new_css = css_url_regex.sub(
                functools.partial(
                    css_url_replacer,
                    href=link.attrib['href']
                ),
                new_css
            )
            destination = os.path.join(destination_dir, hash_ + '.css')

            with codecs.open(destination, 'w', 'utf-8') as f:
                f.write(new_css)

            link.attrib['href'] = (
                '/cache%s' % destination.replace(CACHE_DIR, '')
            )

    for img in CSSSelector('img, script')(page):
        if 'src' in img.attrib:
            orig_src = urljoin(url, img.attrib['src'])
            img.attrib['src'] = orig_src

    for a in CSSSelector('a')(page):
        if 'href' not in a.attrib:
            continue
        href = a.attrib['href']

        if (
            '://' in href or
            href.startswith('#') or
            href.startswith('javascript:')
        ):
            continue

        if href.startswith('/'):
            a.attrib['href'] = (
                '/' +
                urljoin(url, a.attrib['href'])
                .replace('http://', '')
            )
        if collect_stats:
            a.attrib['href'] = add_collect_stats_qs(
                a.attrib['href'],
                collect_stats
            )

    html = etree.tostring(page, method='html')
    if collect_stats:
        html = re.sub(
            '<body[^>]*>',
            lambda m: m.group() + summorize_stats_html(stats),
            html,
            flags=re.I | re.M,
            count=1
        )

    return (was_doctype and was_doctype or '') + '\n' + html


def add_collect_stats_qs(url, value):
    """if :url is `page.html?foo=bar` return.

    `page.html?foo=bar&MINCSS_STATS=:value`

    """
    if '?' in url:
        url += '&'
    else:
        url += '?'
    url += 'MINCSS_STATS=%s' % value
    return url


def summorize_stats_html(stats):
    style = """
        font-size:10px;
        border:1px solid black;
        position:absolute;
        top:50px;
        right:5px;
        padding:4px;
        z-index:1001;
        background-color: white;
        color: black
    """
    template = """<div id="_mincss_stats"
    style="%s">
    <ul>
      %s
    </ul>
    </div>
    """
    lis = []
    total_before = total_after = 0
    for each, before, after in stats:
        total_before += len(before)
        total_after += len(after)
        lis.append(
            """<li>
            <strong>%s</strong>
            <ul>
              <li>before: %s</li>
              <li>after: %s</li>
            </ul>
            </li>""" %
            (
                each,
                sizeof(len(before)),
                sizeof(len(after))
            )
        )

    lis.append(
        """<li>
        <strong>TOTAL</strong>
        <ul>
          <li style="font-weigt:bold">before: %s</li>
          <li style="font-weigt:bold">after: %s</li>
          <li style="font-weigt:bold">saving: %s</li>
        </ul>
        </li>""" %
        (
            sizeof(total_before),
            sizeof(total_after),
            sizeof(total_before - total_after)
        )
    )
    style = style.strip().replace('\n', '')
    return template % (style, ('\n'.join(lis)))


def sizeof(num):
    for x in ['bytes', 'KB', 'MB', 'GB']:
        if num < 1024.0 and num > -1024.0:
            return '%3.1f%s' % (num, x)
        num /= 1024.0
    return '%3.1f%s' % (num, 'TB')


def mkdir(newdir):
    """works the way a good mkdir should :)

        - already exists, silently complete
        - regular file in the way, raise an exception
        - parent directory(ies) does not exist, make them as well

    """
    if os.path.isdir(newdir):
        return
    if os.path.isfile(newdir):
        raise OSError('a file with the same name as the desired '
                      "dir, '%s', already exists." % newdir)
    head, tail = os.path.split(newdir)
    if head and not os.path.isdir(head):
        mkdir(head)
    if tail:
        os.mkdir(newdir)


_link_regex = re.compile('<link .*?>')
_href_regex = re.compile('href=[\'"]([^\'"]+)[\'"]')


def _find_link(line, href):
    for each in _link_regex.findall(line):
        for each_href in _href_regex.findall(each):
            if each_href == href:
                return each


if __name__ == '__main__':
    app.run(debug=True)
    try:
        shutil.rmtree(CACHE_DIR)
    except Exception:
        pass

########NEW FILE########
__FILENAME__ = run
#!/usr/bin/env python
import os
import sys
import time
from __future__ import print_function

# make sure it's running the mincss here and not anything installed
sys.path.insert(0, os.path.dirname(__file__))
from mincss.processor import Processor


def run(args):
    options = {'debug': args.verbose}
    if args.phantomjs_path:
        options['phantomjs'] = args.phantomjs_path
    elif args.phantomjs:
        options['phantomjs'] = True
    p = Processor(**options)
    t0 = time.time()
    p.process(args.url)
    t1 = time.time()
    print("TOTAL TIME ", t1 - t0)
    for inline in p.inlines:
        print("ON", inline.url)
        print("AT line", inline.line)
        print("BEFORE ".ljust(79, '-'))
        print(inline.before)
        print("AFTER ".ljust(79, '-'))
        print(inline.after)
        print()

    output_dir = args.outputdir
    if not os.path.isdir(output_dir):
        os.mkdir(output_dir)
    for link in p.links:
        print("FOR", link.href)
        #print("BEFORE ".ljust(79, '-'))
        #print(link.before)
        #print("AFTER ".ljust(79, '-'))
        #print(link.after)
        orig_name = link.href.split('/')[-1]
        with open(os.path.join(output_dir, orig_name), 'w') as f:
            f.write(link.after)
        before_name = 'before_' + link.href.split('/')[-1]
        with open(os.path.join(output_dir, before_name), 'w') as f:
            f.write(link.before.encode('utf-8'))
        print("Files written to", output_dir
        print()
        print(
            '(from %d to %d saves %d)' %
            (len(link.before), len(link.after),
             len(link.before) - len(link.after))
        )

    return 0


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser()
    add = parser.add_argument
    add("url", type=str,
        help="URL to process")
    add("--outputdir", action="store",
        default="./output",
        help="directory where to put output (default ./output)")
    add("-v", "--verbose", action="store_true",
        help="increase output verbosity")
    add("--phantomjs", action="store_true",
        help="Use PhantomJS to download the source")
    add("--phantomjs-path", action="store",
        default="",
        help="Where is the phantomjs executable")

    args = parser.parse_args()
    sys.exit(run(args))

########NEW FILE########
__FILENAME__ = test_mincss
import os
import unittest
from nose.tools import eq_, ok_

# make sure it's running the mincss here and not anything installed
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
from mincss.processor import Processor


try:
    unicode
except NameError:
    unicode = str


HERE = os.path.dirname(__file__)

PHANTOMJS = os.path.join(HERE, 'fake_phantomjs')


class TestMinCSS(unittest.TestCase):

    def test_just_inline(self):
        html = os.path.join(HERE, 'one.html')
        url = 'file://' + html
        p = Processor()
        p.process(url)
        # on line 7 there inline css starts
        # one.html only has 1 block on inline CSS
        inline = p.inlines[0]
        lines_after = inline.after.strip().splitlines()
        eq_(inline.line, 7)
        ok_(len(inline.after) < len(inline.before))

        # compare line by line
        expect = '''
            h1, h2, h3 { text-align: center; }
            h3 { font-family: serif; }
            h2 { color:red }
        '''
        for i, line in enumerate(expect.strip().splitlines()):
            eq_(line.strip(), lines_after[i].strip())

    def test_just_one_link(self):
        html = os.path.join(HERE, 'two.html')
        url = 'file://' + html
        p = Processor()
        p.process(url)
        # two.html only has 1 link CSS ref
        link = p.links[0]
        eq_(link.href, 'two.css')
        ok_(len(link.after) < len(link.before))
        lines_after = link.after.splitlines()
        # compare line by line
        expect = '''
            body, html { margin: 0; }
            h1, h2, h3 { text-align: center; }
            h3 { font-family: serif; }
            h2 { color:red }
        '''
        for i, line in enumerate(expect.strip().splitlines()):
            eq_(line.strip(), lines_after[i].strip())

    def test_one_link_two_different_pages(self):
        html = os.path.join(HERE, 'two.html')
        url1 = 'file://' + html
        html_half = os.path.join(HERE, 'two_half.html')
        url2 = 'file://' + html_half
        p = Processor()
        p.process(url1, url2)
        # two.html only has 1 link CSS ref
        link = p.links[0]
        eq_(link.href, 'two.css')
        ok_(len(link.after) < len(link.before))
        lines_after = link.after.splitlines()
        # compare line by line
        expect = '''
            body, html { margin: 0; }
            h1, h2, h3 { text-align: center; }
            h3 { font-family: serif; }
            .foobar { delete:me }
            .foobar, h2 { color:red }
        '''
        for i, line in enumerate(expect.strip().splitlines()):
            eq_(line.strip(), lines_after[i].strip())

    def test_pseudo_selectors_hell(self):
        html = os.path.join(HERE, 'three.html')
        url = 'file://' + html
        p = Processor(preserve_remote_urls=False)
        p.process(url)
        # two.html only has 1 link CSS ref
        link = p.links[0]
        after = link.after
        ok_('a.three:hover' in after)
        ok_('a.hundred:link' not in after)

        ok_('.container > a.one' in after)
        ok_('.container > a.notused' not in after)
        ok_('input[type="button"]' not in after)

        ok_('input[type="search"]::-webkit-search-decoration' in after)
        ok_('input[type="reset"]::-webkit-search-decoration' not in after)

        ok_('@media (max-width: 900px)' in after)
        ok_('.container .two' in after)
        ok_('a.four' not in after)

        ok_('::-webkit-input-placeholder' in after)
        ok_(':-moz-placeholder {' in after)
        ok_('div::-moz-focus-inner' in after)
        ok_('button::-moz-focus-inner' not in after)

        ok_('@-webkit-keyframes progress-bar-stripes' in after)
        ok_('from {' in after)

        # some day perhaps this can be untangled and parsed too
        ok_('@import url(other.css)' in after)

    def test_media_query_simple(self):
        html = os.path.join(HERE, 'four.html')
        url = 'file://' + html
        p = Processor()
        p.process(url)

        link = p.links[0]
        after = link.after
        ok_('/* A comment */' in after, after)
        ok_('@media (max-width: 900px) {' in after, after)
        ok_('.container .two {' in after, after)
        ok_('.container .nine {' not in after, after)
        ok_('a.four' not in after, after)

    def test_double_classes(self):
        html = os.path.join(HERE, 'five.html')
        url = 'file://' + html
        p = Processor()
        p.process(url)

        after = p.links[0].after
        eq_(after.count('{'), after.count('}'))
        ok_('input.span6' in after)
        ok_('.uneditable-input.span9' in after)
        ok_('.uneditable-{' not in after)
        ok_('.uneditable-input.span3' not in after)

    def test_complicated_keyframes(self):
        html = os.path.join(HERE, 'six.html')
        url = 'file://' + html
        p = Processor()
        p.process(url)

        after = p.inlines[0].after
        eq_(after.count('{'), after.count('}'))
        ok_('.pull-left' in after)
        ok_('.pull-right' in after)
        ok_('.pull-middle' not in after)

    def test_ignore_annotations(self):
        html = os.path.join(HERE, 'seven.html')
        url = 'file://' + html
        p = Processor()
        p.process(url)

        after = p.inlines[0].after
        eq_(after.count('{'), after.count('}'))
        ok_('/* Leave this comment as is */' in after)
        ok_('/* Lastly leave this as is */' in after)
        ok_('/* Also stick around */' in after)
        ok_('/* leave untouched */' in after)
        ok_('.north' in after)
        ok_('.south' in after)
        ok_('.east' not in after)
        ok_('.west' in after)
        ok_('no mincss' not in after)

    def test_non_ascii_html(self):
        html = os.path.join(HERE, 'eight.html')
        url = 'file://' + html
        p = Processor()
        p.process(url)

        after = p.inlines[0].after
        ok_(isinstance(after, unicode))
        ok_(u'Varf\xf6r st\xe5r det h\xe4r?' in after)

    def test_preserve_remote_urls(self):
        html = os.path.join(HERE, 'nine.html')
        url = 'file://' + html
        p = Processor(preserve_remote_urls=True)
        p.process(url)

        after = p.links[0].after
        ok_("url('http://www.google.com/north.png')" in after)
        url = 'file://' + HERE + '/deeper/south.png'
        ok_('url("%s")' % url in after)
        # since local file URLs don't have a domain, this is actually expected
        ok_('url("file:///east.png")' in after)
        url = 'file://' + HERE + '/west.png'
        ok_('url("%s")' % url in after)

    @unittest.skip('This has always been failing')
    def test_download_with_phantomjs(self):
        html = os.path.join(HERE, 'one.html')
        url = 'file://' + html
        p = Processor(
            phantomjs=PHANTOMJS,
            phantomjs_options={'cookies-file': 'bla'}
        )
        p.process(url)
        # on line 7 there inline css starts
        # one.html only has 1 block on inline CSS
        inline = p.inlines[0]
        lines_after = inline.after.strip().splitlines()
        eq_(inline.line, 7)
        ok_(len(inline.after) < len(inline.before))

        # compare line by line
        expect = '''
            h1, h2, h3 { text-align: center; }
            h3 { font-family: serif; }
            h2 { color:red }
        '''
        for i, line in enumerate(expect.strip().splitlines()):
            eq_(line.strip(), lines_after[i].strip())

    def test_make_absolute_url(self):
        p = Processor()
        eq_(
            p.make_absolute_url('http://www.com/', './style.css'),
            'http://www.com/style.css'
        )
        eq_(
            p.make_absolute_url('http://www.com', './style.css'),
            'http://www.com/style.css'
        )
        eq_(
            p.make_absolute_url('http://www.com', '//cdn.com/style.css'),
            'http://cdn.com/style.css'
        )
        eq_(
            p.make_absolute_url('http://www.com/', '//cdn.com/style.css'),
            'http://cdn.com/style.css'
        )
        eq_(
            p.make_absolute_url('http://www.com/', '/style.css'),
            'http://www.com/style.css'
        )
        eq_(
            p.make_absolute_url('http://www.com/elsewhere', '/style.css'),
            'http://www.com/style.css'
        )
        eq_(
            p.make_absolute_url('http://www.com/elsewhere/', '/style.css'),
            'http://www.com/style.css'
        )
        eq_(
            p.make_absolute_url('http://www.com/elsewhere/', './style.css'),
            'http://www.com/elsewhere/style.css'
        )
        eq_(
            p.make_absolute_url('http://www.com/elsewhere', './style.css'),
            'http://www.com/style.css'
        )

########NEW FILE########
