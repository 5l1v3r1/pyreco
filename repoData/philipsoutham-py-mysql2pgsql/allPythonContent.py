__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# py-mysql2pgsql documentation build configuration file, created by
# sphinx-quickstart on Tue Aug  9 12:17:31 2011.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.todo']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'py-mysql2pgsql'
copyright = u'2011, Philip Southam'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#try:
#    import mysql2pgsql
#except ImportError:
sys.path.append(os.path.abspath('../'))
# The full version, including alpha/beta/rc tags.
from mysql2pgsql import __version__ as release
# The short X.Y version.
version = release



# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'py-mysql2pgsqldoc'


# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'py-mysql2pgsql.tex', u'py-mysql2pgsql Documentation',
   u'Philip Southam', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'py-mysql2pgsql', u'py-mysql2pgsql Documentation',
     [u'Philip Southam'], 1)
]

########NEW FILE########
__FILENAME__ = config
from __future__ import with_statement, absolute_import

import os.path

from yaml import load

try:
    from yaml import CLoader as Loader, CDumper as Dumper
except ImportError:
    from yaml import Loader, Dumper

from .errors import ConfigurationFileInitialized,\
    ConfigurationFileNotFound


class ConfigBase(object):
    def __init__(self, config_file_path):
        self.options = load(open(config_file_path))


class Config(ConfigBase):
    def __init__(self, config_file_path, generate_if_not_found=True):
        if not os.path.isfile(config_file_path):
            if generate_if_not_found:
                self.reset_configfile(config_file_path)
            if os.path.isfile(config_file_path):
                raise ConfigurationFileInitialized("""No configuration file found.
A new file has been initialized at: %s
Please review the configuration and retry...""" % config_file_path)
            else:
                raise ConfigurationFileNotFound("cannot load config file %s" % config_file_path)

        super(Config, self).__init__(config_file_path)

    def reset_configfile(self, file_path):
        with open(file_path, 'w') as f:
            f.write(CONFIG_TEMPLATE)

CONFIG_TEMPLATE = """
# if a socket is specified we will use that
# if tcp is chosen you can use compression
mysql:
 hostname: localhost
 port: 3306
 socket: /tmp/mysql.sock
 username: mysql2psql
 password: 
 database: mysql2psql_test
 compress: false
destination:
 # if file is given, output goes to file, else postgres
 file: 
 postgres:
  hostname: localhost
  port: 5432
  username: mysql2psql
  password: 
  database: mysql2psql_test

# if tables is given, only the listed tables will be converted.  leave empty to convert all tables.
#only_tables:
#- table1
#- table2
# if exclude_tables is given, exclude the listed tables from the conversion.
#exclude_tables:
#- table3
#- table4

# if supress_data is true, only the schema definition will be exported/migrated, and not the data
supress_data: false

# if supress_ddl is true, only the data will be exported/imported, and not the schema
supress_ddl: false

# if force_truncate is true, forces a table truncate before table loading
force_truncate: false

# if timezone is true, forces to append/convert to UTC tzinfo mysql data
timezone: false
"""

########NEW FILE########
__FILENAME__ = converter
from __future__ import absolute_import

from . import print_start_table


class Converter(object):
    def __init__(self, reader, writer, file_options, verbose=False):
        self.verbose = verbose
        self.reader = reader
        self.writer = writer
        self.file_options = file_options
        self.exclude_tables = file_options.get('exclude_tables', [])
        self.only_tables = file_options.get('only_tables', [])
        self.supress_ddl = file_options.get('supress_ddl', None)
        self.supress_data = file_options.get('supress_data', None)
        self.force_truncate = file_options.get('force_truncate', None)

    def convert(self):
        if self.verbose:
            print_start_table('>>>>>>>>>> STARTING <<<<<<<<<<\n\n')

        tables = [t for t in (t for t in self.reader.tables if t.name not in self.exclude_tables) if not self.only_tables or t.name in self.only_tables]
        if self.only_tables:
            tables.sort(key=lambda t: self.only_tables.index(t.name))
        
        if not self.supress_ddl:
            if self.verbose:
                print_start_table('START CREATING TABLES')

            for table in tables:
                self.writer.write_table(table)

            if self.verbose:
                print_start_table('DONE CREATING TABLES')

        if self.force_truncate and self.supress_ddl:
            if self.verbose:
                print_start_table('START TRUNCATING TABLES')

            for table in tables:
                self.writer.truncate(table)

            if self.verbose:
                print_start_table('DONE TRUNCATING TABLES')

        if not self.supress_data:
            if self.verbose:
                print_start_table('START WRITING TABLE DATA')

            for table in tables:
                self.writer.write_contents(table, self.reader)

            if self.verbose:
                print_start_table('DONE WRITING TABLE DATA')

        if not self.supress_ddl:
            if self.verbose:
                print_start_table('START CREATING INDEXES, CONSTRAINTS, AND TRIGGERS')

            for table in tables:
                self.writer.write_indexes(table)

            for table in tables:
                self.writer.write_constraints(table)

            for table in tables:
                self.writer.write_triggers(table)

            if self.verbose:
                print_start_table('DONE CREATING INDEXES, CONSTRAINTS, AND TRIGGERS')

        if self.verbose:
            print_start_table('\n\n>>>>>>>>>> FINISHED <<<<<<<<<<')

        self.writer.close()

########NEW FILE########
__FILENAME__ = errors
class GeneralException(Exception): pass


class ConfigurationException(Exception): pass


class UninitializedValueError(GeneralException): pass


class ConfigurationFileNotFound(ConfigurationException): pass


class ConfigurationFileInitialized(ConfigurationException): pass

########NEW FILE########
__FILENAME__ = mysql_reader
from __future__ import with_statement, absolute_import

import re
from contextlib import closing

import MySQLdb
import MySQLdb.cursors


re_column_length = re.compile(r'\((\d+)\)')
re_column_precision = re.compile(r'\((\d+),(\d+)\)')
re_key_1 = re.compile(r'CONSTRAINT `(\w+)` FOREIGN KEY \(`(\w+)`\) REFERENCES `(\w+)` \(`(\w+)`\)')
re_key_2 = re.compile(r'KEY `(\w+)` \((.*)\)')
re_key_3 = re.compile(r'PRIMARY KEY +\((.*)\)')


class DB:
    """
    Class that wraps MySQLdb functions that auto reconnects
    thus (hopefully) preventing the frustrating
    "server has gone away" error. Also adds helpful
    helper functions.
    """
    conn = None

    def __init__(self, options):
        args = {
            'user': str(options.get('username', 'root')),
            'db': options['database'],
            'use_unicode': True,
            'charset': 'utf8',
            }

        if options.get('password', None):
            args['passwd'] = str(options.get('password', None))

        if options.get('socket', None):
            args['unix_socket'] = str(options['socket'])
        else:
            args['host'] = str(options.get('hostname', 'localhost'))
            args['port'] = options.get('port', 3306)
            args['compress'] = options.get('compress', True)

        self.options = args

    def connect(self):
        self.conn = MySQLdb.connect(**self.options)

    def close(self):
        self.conn.close()

    def cursor(self, cursorclass=MySQLdb.cursors.Cursor):
        try:
            return self.conn.cursor(cursorclass)
        except (AttributeError, MySQLdb.OperationalError):
            self.connect()
            return self.conn.cursor(cursorclass)

    def list_tables(self):
        return self.query('SHOW TABLES;')

    def query(self, sql, args=(), one=False, large=False):
        return self.query_one(sql, args) if one\
            else self.query_many(sql, args, large)

    def query_one(self, sql, args):
        with closing(self.cursor()) as cur:
            cur.execute(sql, args)
            return cur.fetchone()

    def query_many(self, sql, args, large):
        with closing(self.cursor(MySQLdb.cursors.SSCursor if large else MySQLdb.cursors.Cursor)) as cur:
            cur.execute(sql, args)
            for row in cur:
                yield row


class MysqlReader(object):

    class Table(object):
        def __init__(self, reader, name):
            self.reader = reader
            self._name = name
            self._indexes = []
            self._foreign_keys = []
            self._triggers = []
            self._columns = self._load_columns()
            self._load_indexes()
            self._load_triggers()

        def _convert_type(self, data_type):
            """Normalize MySQL `data_type`"""
            if data_type.startswith('varchar'):
                return 'varchar'
            elif data_type.startswith('char'):
                return 'char'
            elif data_type in ('bit(1)', 'tinyint(1)', 'tinyint(1) unsigned'):
                return 'boolean'
            elif re.search(r'^smallint.* unsigned', data_type) or data_type.startswith('mediumint'):
                return 'integer'
            elif data_type.startswith('smallint'):
                return 'tinyint'
            elif data_type.startswith('tinyint') or data_type.startswith('year('):
                return 'tinyint'
            elif data_type.startswith('bigint') and 'unsigned' in data_type:
                return 'numeric'
            elif re.search(r'^int.* unsigned', data_type) or \
                    (data_type.startswith('bigint') and 'unsigned' not in data_type):
                return 'bigint'
            elif data_type.startswith('int'):
                return 'integer'
            elif data_type.startswith('float'):
                return 'float'
            elif data_type.startswith('decimal'):
                return 'decimal'
            elif data_type.startswith('double'):
                return 'double precision'
            else:
                return data_type

        def _load_columns(self):
            fields = []
            for row in self.reader.db.query('EXPLAIN `%s`' % self.name):
                res = ()
                for field in row:
                  if type(field) == unicode:
                    res += field.encode('utf8'),
                  else:
                    res += field,
                length_match = re_column_length.search(res[1])
                precision_match = re_column_precision.search(res[1])
                length = length_match.group(1) if length_match else \
                    precision_match.group(1) if precision_match else None
                name = res[0]
                field_type = self._convert_type(res[1])
                desc = {
                    'name': name,
                    'table_name': self.name,
                    'type': field_type,
                    'length': int(length) if length else None,
                    'decimals': precision_match.group(2) if precision_match else None,
                    'null': res[2] == 'YES' or field_type.startswith('enum') or field_type in ('date', 'datetime', 'timestamp'),
                    'primary_key': res[3] == 'PRI',
                    'auto_increment': res[5] == 'auto_increment',
                    'default': res[4] if not res[4] == 'NULL' else None,
                    'select': '`%s`' % name if not field_type.startswith('enum') else
                        'CASE `%(name)s` WHEN "" THEN NULL ELSE `%(name)s` END' % {'name': name},
                    }
                fields.append(desc)

            for field in (f for f in fields if f['auto_increment']):
                res = self.reader.db.query('SELECT MAX(`%s`) FROM `%s`;' % (field['name'], self.name), one=True)
                field['maxval'] = int(res[0]) if res[0] else 0

            return fields

        def _load_indexes(self):
            explain = self.reader.db.query('SHOW CREATE TABLE `%s`' % self.name, one=True)
            explain = explain[1]
            for line in explain.split('\n'):
                if ' KEY ' not in line:
                    continue
                index = {}
                match_data = re_key_1.search(line)
                if match_data:
                    index['name'] = match_data.group(1)
                    index['column'] = match_data.group(2)
                    index['ref_table'] = match_data.group(3)
                    index['ref_column'] = match_data.group(4)
                    self._foreign_keys.append(index)
                    continue
                match_data = re_key_2.search(line)
                if match_data:
                    index['name'] = match_data.group(1)
                    index['columns'] = [re.search(r'`(\w+)`', col).group(1) for col in match_data.group(2).split(',')]
                    index['unique'] = 'UNIQUE' in line
                    self._indexes.append(index)
                    continue
                match_data = re_key_3.search(line)
                if match_data:
                    index['primary'] = True
                    index['columns'] = [re.sub(r'\(\d+\)', '', col.replace('`', '')) for col in match_data.group(1).split(',')]
                    self._indexes.append(index)
                    continue

        def _load_triggers(self):
            explain = self.reader.db.query('SHOW TRIGGERS WHERE `table` = \'%s\'' % self.name)
            for row in explain:
                if type(row) is tuple:
                    trigger = {}
                    trigger['name'] = row[0]
                    trigger['event'] = row[1]
                    trigger['statement'] = row[3]
                    trigger['timing'] = row[4]

                    trigger['statement'] = re.sub('^BEGIN', '', trigger['statement'])
                    trigger['statement'] = re.sub('^END', '', trigger['statement'], flags=re.MULTILINE)
                    trigger['statement'] = re.sub('`', '', trigger['statement'])

                    self._triggers.append(trigger)

        @property
        def name(self):
            return self._name

        @property
        def columns(self):
            return self._columns

        @property
        def indexes(self):
            return self._indexes

        @property
        def foreign_keys(self):
            return self._foreign_keys

        @property
        def triggers(self):
            return self._triggers

        @property
        def query_for(self):
            return 'SELECT %(column_names)s FROM `%(table_name)s`' % {
                'table_name': self.name,
                'column_names': ', '. join(c['select'] for c in self.columns)}

    def __init__(self, options):
        self.db = DB(options)

    @property
    def tables(self):
        return (self.Table(self, t[0]) for t in self.db.list_tables())

    def read(self, table):
        return self.db.query(table.query_for, large=True)

    def close(self):
        self.db.close()

########NEW FILE########
__FILENAME__ = postgres_db_writer
from __future__ import with_statement, absolute_import

import time
from contextlib import closing

import psycopg2

from . import print_row_progress, status_logger
from .postgres_writer import PostgresWriter


class PostgresDbWriter(PostgresWriter):
    """Class used to stream DDL and/or data
    from a MySQL server to a PostgreSQL.

    :Parameters:
      - `db_options`: :py:obj:`dict` containing connection specific variables
      - `verbose`: whether or not to log progress to :py:obj:`stdout`

    """
    class FileObjFaker(object):
        """A file-like class to support streaming
        table data directly to :py:meth:`pscopg2.copy_from`.

        :Parameters:
          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.
          - `data`:
          - `processor`:
          - `verbose`: whether or not to log progress to :py:obj:`stdout`
        """
        def __init__(self, table, data, processor, verbose=False):
            self.data = iter(data)
            self.table = table
            self.processor = processor
            self.verbose = verbose

            if verbose:
                self.idx = 1
                self.start_time = time.time()
                self.prev_val_len = 0
                self.prev_idx = 0

        def readline(self, *args, **kwargs):
            try:
                row = list(self.data.next())
            except StopIteration:
                if self.verbose:
                    print('')
                return ''
            else:
                self.processor(self.table, row)
                try:
                    return '%s\n' % ('\t'.join(row))
                except UnicodeDecodeError:
                    return '%s\n' % ('\t'.join(r.decode('utf8') for r in row))
            finally:
                if self.verbose:
                    if (self.idx % 20000) == 0:
                        now = time.time()
                        elapsed = now - self.start_time
                        val = '%.2f rows/sec [%s] ' % ((self.idx - self.prev_idx) / elapsed, self.idx)
                        print_row_progress('%s%s' % (("\b" * self.prev_val_len), val)),
                        self.prev_val_len = len(val) + 3
                        self.start_time = now
                        self.prev_idx = self.idx + 0
                    self.idx += 1

        def read(self, *args, **kwargs):
            return self.readline(*args, **kwargs)

    def __init__(self, db_options, verbose=False, *args, **kwargs):
        super(PostgresDbWriter, self).__init__(*args, **kwargs)
        self.verbose = verbose
        self.db_options = {
            'host': str(db_options['hostname']),
            'port': db_options.get('port', 5432),
            'database': str(db_options['database']),
            'password': str(db_options.get('password', None)) or '',
            'user': str(db_options['username']),
            }
        if ':' in str(db_options['database']):
            self.db_options['database'], self.schema = self.db_options['database'].split(':')
        else:
            self.schema = None

        self.open()

    def open(self):
        self.conn = psycopg2.connect(**self.db_options)
        with closing(self.conn.cursor()) as cur:
            if self.schema:
                cur.execute('SET search_path TO %s' % self.schema)
            cur.execute('SET client_encoding = \'UTF8\'')
            if self.conn.server_version >= 80200:
                cur.execute('SET standard_conforming_strings = off')
            cur.execute('SET check_function_bodies = false')
            cur.execute('SET client_min_messages = warning')

    def query(self, sql, args=(), one=False):
        with closing(self.conn.cursor()) as cur:
            cur.execute(sql, args)
            return cur.fetchone() if one else cur

    def execute(self, sql, args=(), many=False):
        with closing(self.conn.cursor()) as cur:
            if many:
                cur.executemany(sql, args)
            else:
                cur.execute(sql, args)
            self.conn.commit()

    def copy_from(self, file_obj, table_name, columns):
        with closing(self.conn.cursor()) as cur:
            cur.copy_from(file_obj,
                          table=table_name,
                          columns=columns
                          )

        self.conn.commit()

    def close(self):
        """Closes connection to the PostgreSQL server"""
        self.conn.close()

    def exists(self, relname):
        rc = self.query('SELECT COUNT(!) FROM pg_class WHERE relname = %s', (relname, ), one=True)
        return rc and int(rc[0]) == 1

    @status_logger
    def truncate(self, table):
        """Send DDL to truncate the specified `table`

        :Parameters:
          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.

        Returns None
        """
        truncate_sql, serial_key_sql = super(PostgresDbWriter, self).truncate(table)
        self.execute(truncate_sql)
        if serial_key_sql:
            self.execute(serial_key_sql)

    @status_logger
    def write_table(self, table):
        """Send DDL to create the specified `table`

        :Parameters:
          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.

        Returns None
        """
        table_sql, serial_key_sql = super(PostgresDbWriter, self).write_table(table)
        for sql in serial_key_sql + table_sql:
            self.execute(sql)

    @status_logger
    def write_indexes(self, table):
        """Send DDL to create the specified `table` indexes

        :Parameters:
          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.

        Returns None
        """
        index_sql = super(PostgresDbWriter, self).write_indexes(table)
        for sql in index_sql:
            self.execute(sql)

    @status_logger
    def write_triggers(self, table):
        """Send DDL to create the specified `table` triggers

        :Parameters:
          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.

        Returns None
        """
        index_sql = super(PostgresDbWriter, self).write_triggers(table)
        for sql in index_sql:
            self.execute(sql)

    @status_logger
    def write_constraints(self, table):
        """Send DDL to create the specified `table` constraints

        :Parameters:
          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.

        Returns None
        """
        constraint_sql = super(PostgresDbWriter, self).write_constraints(table)
        for sql in constraint_sql:
            self.execute(sql)

    @status_logger
    def write_contents(self, table, reader):
        """Write the contents of `table`

        :Parameters:
          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.
          - `reader`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader` object that allows reading from the data source.

        Returns None
        """
        f = self.FileObjFaker(table, reader.read(table), self.process_row, self.verbose)
        self.copy_from(f, '"%s"' % table.name, ['"%s"' % c['name'] for c in table.columns])

########NEW FILE########
__FILENAME__ = postgres_file_writer
from __future__ import absolute_import

import time


from .postgres_writer import PostgresWriter

from . import print_row_progress, status_logger


class PostgresFileWriter(PostgresWriter):
    """Class used to ouput the PostgreSQL
    compatable DDL and/or data to the specified
    output :py:obj:`file` from a MySQL server.

    :Parameters:
      - `output_file`: the output :py:obj:`file` to send the DDL and/or data
      - `verbose`: whether or not to log progress to :py:obj:`stdout`

    """
    verbose = None

    def __init__(self, output_file, verbose=False, *args, **kwargs):
        super(PostgresFileWriter, self).__init__(*args, **kwargs)
        self.verbose = verbose
        self.f = output_file
        self.f.write("""
-- MySQL 2 PostgreSQL dump\n
SET client_encoding = 'UTF8';
SET standard_conforming_strings = off;
SET check_function_bodies = false;
SET client_min_messages = warning;
""")

    @status_logger
    def truncate(self, table):
        """Write DDL to truncate the specified `table`

        :Parameters:
          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.

        Returns None
        """
        truncate_sql, serial_key_sql = super(PostgresFileWriter, self).truncate(table)
        self.f.write("""
-- TRUNCATE %(table_name)s;
%(truncate_sql)s
""" % {'table_name': table.name, 'truncate_sql': truncate_sql})

        if serial_key_sql:
            self.f.write("""
%(serial_key_sql)s
""" % {
    'serial_key_sql': serial_key_sql})

    @status_logger
    def write_table(self, table):
        """Write DDL to create the specified `table`.

        :Parameters:
          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.

        Returns None
        """
        table_sql, serial_key_sql = super(PostgresFileWriter, self).write_table(table)
        if serial_key_sql:
            self.f.write("""
%(serial_key_sql)s
""" % {
    'serial_key_sql': '\n'.join(serial_key_sql)
    })

        self.f.write("""
-- Table: %(table_name)s
%(table_sql)s
""" % {
    'table_name': table.name,
    'table_sql': '\n'.join(table_sql),
    })

    @status_logger
    def write_indexes(self, table):
        """Write DDL of `table` indexes to the output file

        :Parameters:
          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.

        Returns None
        """
        self.f.write('\n'.join(super(PostgresFileWriter, self).write_indexes(table)))

    @status_logger
    def write_constraints(self, table):
        """Write DDL of `table` constraints to the output file

        :Parameters:
          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.

        Returns None
        """
        self.f.write('\n'.join(super(PostgresFileWriter, self).write_constraints(table)))

    @status_logger
    def write_triggers(self, table):
        """Write TRIGGERs existing on `table` to the output file

        :Parameters:
          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.

        Returns None
        """
        self.f.write('\n'.join(super(PostgresFileWriter, self).write_triggers(table)))

    @status_logger
    def write_contents(self, table, reader):
        """Write the data contents of `table` to the output file.

        :Parameters:
          - `table`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader.Table` object that represents the table to read/write.
          - `reader`: an instance of a :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader` object that allows reading from the data source.

        Returns None
        """
        # start variable optimiztions
        pr = self.process_row
        f_write = self.f.write
        verbose = self.verbose
        # end variable optimiztions

        f_write("""
--
-- Data for Name: %(table_name)s; Type: TABLE DATA;
--

COPY "%(table_name)s" (%(column_names)s) FROM stdin;
""" % {
                'table_name': table.name,
                'column_names': ', '.join(('"%s"' % col['name']) for col in table.columns)})
        if verbose:
            tt = time.time
            start_time = tt()
            prev_val_len = 0
            prev_row_count = 0
        for i, row in enumerate(reader.read(table), 1):
            row = list(row)
            pr(table, row)
            try:
                f_write(u'%s\n' % (u'\t'.join(row)))
            except UnicodeDecodeError:
                f_write(u'%s\n' % (u'\t'.join(r.decode('utf-8') for r in row)))
            if verbose:
                if (i % 20000) == 0:
                    now = tt()
                    elapsed = now - start_time
                    val = '%.2f rows/sec [%s] ' % ((i - prev_row_count) / elapsed, i)
                    print_row_progress('%s%s' % (("\b" * prev_val_len), val))
                    prev_val_len = len(val) + 3
                    start_time = now
                    prev_row_count = i

        f_write("\\.\n\n")
        if verbose:
            print('')

    def close(self):
        """Closes the output :py:obj:`file`"""
        self.f.close()

########NEW FILE########
__FILENAME__ = postgres_writer
from __future__ import absolute_import

import re
from cStringIO import StringIO
from datetime import datetime, date, timedelta

from psycopg2.extensions import QuotedString, Binary, AsIs
from pytz import timezone


class PostgresWriter(object):
    """Base class for :py:class:`mysql2pgsql.lib.postgres_file_writer.PostgresFileWriter`
    and :py:class:`mysql2pgsql.lib.postgres_db_writer.PostgresDbWriter`.
    """
    def __init__(self, tz=False):
        self.column_types = {}

        if tz:
            self.tz = timezone('UTC')
            self.tz_offset = '+00:00'
        else:
            self.tz = None
            self.tz_offset = ''

    def column_description(self, column):
        return '"%s" %s' % (column['name'], self.column_type_info(column))

    def column_type(self, column):
        hash_key = hash(frozenset(column.items()))
        self.column_types[hash_key] = self.column_type_info(column).split(" ")[0]
        return self.column_types[hash_key]

    def column_type_info(self, column):
        """
        """
        null = "" if column['null'] else " NOT NULL"

        def get_type(column):
            """This in conjunction with :py:class:`mysql2pgsql.lib.mysql_reader.MysqlReader._convert_type`
            determines the PostgreSQL data type. In my opinion this is way too fugly, will need
            to refactor one day.
            """
            t = lambda v: not v == None
            default = (' DEFAULT %s' % QuotedString(column['default']).getquoted()) if t(column['default']) else None

            if column['type'] == 'char':
                default = ('%s::char' % default) if t(default) else None
                return default, 'character(%s)' % column['length']
            elif column['type'] == 'varchar':
                default = ('%s::character varying' % default) if t(default) else None
                return default, 'character varying(%s)' % column['length']
            elif column['type'] == 'integer':
                default = (" DEFAULT %s" % (column['default'] if t(column['default']) else 'NULL')) if t(default) else None
                return default, 'integer'
            elif column['type'] == 'bigint':
                default = (" DEFAULT %s" % (column['default'] if t(column['default']) else 'NULL')) if t(default) else None
                return default, 'bigint'
            elif column['type'] == 'tinyint':
                default = (" DEFAULT %s" % (column['default'] if t(column['default']) else 'NULL')) if t(default) else None
                return default, 'smallint'
            elif column['type'] == 'boolean':
                default = (" DEFAULT %s" % ('true' if int(column['default']) == 1 else 'false')) if t(default) else None
                return default, 'boolean'
            elif column['type'] == 'float':
                default = (" DEFAULT %s" % (column['default'] if t(column['default']) else 'NULL')) if t(default) else None
                return default, 'real'
            elif column['type'] == 'float unsigned':
                default = (" DEFAULT %s" % (column['default'] if t(column['default']) else 'NULL')) if t(default) else None
                return default, 'real'
            elif column['type'] in ('numeric', 'decimal'):
                default = (" DEFAULT %s" % (column['default'] if t(column['default']) else 'NULL')) if t(default) else None
                return default, 'numeric(%s, %s)' % (column['length'] or 20, column['decimals'] or 0)
            elif column['type'] == 'double precision':
                default = (" DEFAULT %s" % (column['default'] if t(column['default']) else 'NULL')) if t(default) else None
                return default, 'double precision'
            elif column['type'] == 'datetime':
                default = None
                if self.tz:
                    return default, 'timestamp with time zone'
                else:
                    return default, 'timestamp without time zone'
            elif column['type'] == 'date':
                default = None
                return default, 'date'
            elif column['type'] == 'timestamp':
                if column['default'] == None:
                    default = None
                elif "CURRENT_TIMESTAMP" in column['default']:
                    default = ' DEFAULT CURRENT_TIMESTAMP'
                elif "0000-00-00 00:00" in  column['default']:
                    if self.tz:
                        default = " DEFAULT '1970-01-01T00:00:00.000000%s'" % self.tz_offset
                    elif "0000-00-00 00:00:00" in column['default']:
                        default = " DEFAULT '1970-01-01 00:00:00'"
                    else:
                        default = " DEFAULT '1970-01-01 00:00'"
                if self.tz:
                    return default, 'timestamp with time zone'
                else:
                    return default, 'timestamp without time zone'
            elif column['type'] == 'time':
                default = " DEFAULT NOW()" if t(default) else None
                if self.tz:
                    return default, 'time with time zone'
                else:
                    return default, 'time without time zone'
            elif column['type'] in ('blob', 'binary', 'varbinary'):
                return default, 'bytea'
            elif column['type'] in ('tinytext', 'mediumtext', 'longtext', 'text'):
                return default, 'text'
            elif column['type'].startswith('enum'):
                default = (' %s::character varying' % default) if t(default) else None
                enum = re.sub(r'^enum\(|\)$', '', column['type'])
                # TODO: will work for "'.',',',''''" but will fail for "'.'',','.'"
                max_enum_size = max([len(e.replace("''", "'")) for e in enum.split("','")])
                return default, ' character varying(%s) check(%s in (%s))' % (max_enum_size, column['name'], enum)
            elif column['type'].startswith('bit('):
                return ' DEFAULT %s' % column['default'].upper() if column['default'] else column['default'], 'varbit(%s)' % re.search(r'\((\d+)\)', column['type']).group(1)
            elif column['type'].startswith('set('):
                if default:
                    default = ' DEFAULT ARRAY[%s]::text[]' % ','.join(QuotedString(v).getquoted() for v in re.search(r"'(.*)'", default).group(1).split(','))
                return default, 'text[]'
            else:
                raise Exception('unknown %s' % column['type'])

        default, column_type = get_type(column)

        if column.get('auto_increment', None):
            return '%s DEFAULT nextval(\'"%s_%s_seq"\'::regclass) NOT NULL' % (
                   column_type, column['table_name'], column['name'])
                    
        return '%s%s%s' % (column_type, (default if not default == None else ''), null)

    def process_row(self, table, row):
        """Examines row data from MySQL and alters
        the values when necessary to be compatible with
        sending to PostgreSQL via the copy command
        """
        for index, column in enumerate(table.columns):
            hash_key = hash(frozenset(column.items()))
            column_type = self.column_types[hash_key] if hash_key in self.column_types else self.column_type(column)
            if row[index] == None and ('timestamp' not in column_type or not column['default']):
                row[index] = '\N'
            elif row[index] == None and column['default']:
                if self.tz:
                    row[index] = '1970-01-01T00:00:00.000000' + self.tz_offset
                else:
                    row[index] = '1970-01-01 00:00:00'
            elif 'bit' in column_type:
                row[index] = bin(ord(row[index]))[2:]
            elif isinstance(row[index], (str, unicode, basestring)):
                if column_type == 'bytea':
                    row[index] = Binary(row[index]).getquoted()[1:-8] if row[index] else row[index]
                elif 'text[' in column_type:
                    row[index] = '{%s}' % ','.join('"%s"' % v.replace('"', r'\"') for v in row[index].split(','))
                else:
                    row[index] = row[index].replace('\\', r'\\').replace('\n', r'\n').replace('\t', r'\t').replace('\r', r'\r').replace('\0', '')
            elif column_type == 'boolean':
                # We got here because you used a tinyint(1), if you didn't want a bool, don't use that type
                row[index] = 't' if row[index] not in (None, 0) else 'f' if row[index] == 0 else row[index]
            elif  isinstance(row[index], (date, datetime)):
                if  isinstance(row[index], datetime) and self.tz:
                    try:
                        if row[index].tzinfo:
                            row[index] = row[index].astimezone(self.tz).isoformat()
                        else:
                            row[index] = datetime(*row[index].timetuple()[:6], tzinfo=self.tz).isoformat()
                    except Exception as e:
                        print e.message
                else:
                    row[index] = row[index].isoformat()
            elif isinstance(row[index], timedelta):
                row[index] = datetime.utcfromtimestamp(row[index].total_seconds()).time().isoformat()
            else:
                row[index] = AsIs(row[index]).getquoted()

    def table_attributes(self, table):
        primary_keys = []
        serial_key = None
        maxval = None
        columns = StringIO()

        for column in table.columns:
            if column['auto_increment']:
                serial_key = column['name']
                maxval = 1 if column['maxval'] < 1 else column['maxval'] + 1
            if column['primary_key']:
                primary_keys.append(column['name'])
            columns.write('  %s,\n' % self.column_description(column))
        return primary_keys, serial_key, maxval, columns.getvalue()[:-2]

    def truncate(self, table):
        serial_key = None
        maxval = None

        for column in table.columns:
            if column['auto_increment']:
                serial_key = column['name']
                maxval = 1 if column['maxval'] < 1 else column['maxval'] + 1

        truncate_sql = 'TRUNCATE "%s" CASCADE;' % table.name
        serial_key_sql = None

        if serial_key:
            serial_key_sql = "SELECT pg_catalog.setval(pg_get_serial_sequence(%(table_name)s, %(serial_key)s), %(maxval)s, true);" % {
                'table_name': QuotedString('"%s"' % table.name).getquoted(),
                'serial_key': QuotedString(serial_key).getquoted(),
                'maxval': maxval}

        return (truncate_sql, serial_key_sql)

    def write_table(self, table):
        primary_keys, serial_key, maxval, columns = self.table_attributes(table)
        serial_key_sql = []
        table_sql = []
        if serial_key:
            serial_key_seq = '%s_%s_seq' % (table.name, serial_key)
            serial_key_sql.append('DROP SEQUENCE IF EXISTS "%s" CASCADE;' % serial_key_seq)
            serial_key_sql.append("""CREATE SEQUENCE "%s" INCREMENT BY 1
                                  NO MAXVALUE NO MINVALUE CACHE 1;""" % serial_key_seq)
            serial_key_sql.append('SELECT pg_catalog.setval(\'"%s"\', %s, true);' % (serial_key_seq, maxval))

        table_sql.append('DROP TABLE IF EXISTS "%s" CASCADE;' % table.name)
        table_sql.append('CREATE TABLE "%s" (\n%s\n)\nWITHOUT OIDS;' % (table.name.encode('utf8'), columns))
        return (table_sql, serial_key_sql)

    def write_indexes(self, table):
        index_sql = []
        primary_index = [idx for idx in table.indexes if idx.get('primary', None)]
        if primary_index:
            index_sql.append('ALTER TABLE "%(table_name)s" ADD CONSTRAINT "%(index_name)s_pkey" PRIMARY KEY(%(column_names)s);' % {
                    'table_name': table.name,
                    'index_name': '%s_%s' % (table.name, '_'.join(primary_index[0]['columns'])),
                    'column_names': ', '.join('"%s"' % col for col in primary_index[0]['columns']),
                    })
        for index in table.indexes:
            if 'primary' in index:
                continue
            unique = 'UNIQUE ' if index.get('unique', None) else ''
            index_name = '%s_%s' % (table.name, '_'.join(index['columns']))
            index_sql.append('DROP INDEX IF EXISTS "%s" CASCADE;' % index_name)
            index_sql.append('CREATE %(unique)sINDEX "%(index_name)s" ON "%(table_name)s" (%(column_names)s);' % {
                    'unique': unique,
                    'index_name': index_name,
                    'table_name': table.name,
                    'column_names': ', '.join('"%s"' % col for col in index['columns']),
                    })

        return index_sql

    def write_constraints(self, table):
        constraint_sql = []
        for key in table.foreign_keys:
            constraint_sql.append("""ALTER TABLE "%(table_name)s" ADD FOREIGN KEY ("%(column_name)s")
            REFERENCES "%(ref_table_name)s"(%(ref_column_name)s);""" % {
                'table_name': table.name,
                'column_name': key['column'],
                'ref_table_name': key['ref_table'],
                'ref_column_name': key['ref_column']})
        return constraint_sql

    def write_triggers(self, table):
        trigger_sql = []
        for key in table.triggers:
            trigger_sql.append("""CREATE OR REPLACE FUNCTION %(fn_trigger_name)s RETURNS TRIGGER AS $%(trigger_name)s$
            BEGIN
                %(trigger_statement)s
            RETURN NULL;
            END;
            $%(trigger_name)s$ LANGUAGE plpgsql;""" % {
                'table_name': table.name,
                'trigger_time': key['timing'],
                'trigger_event': key['event'],
                'trigger_name': key['name'],
                'fn_trigger_name': 'fn_' + key['name'] + '()',
                'trigger_statement': key['statement']})

            trigger_sql.append("""CREATE TRIGGER %(trigger_name)s %(trigger_time)s %(trigger_event)s ON %(table_name)s
            FOR EACH ROW
            EXECUTE PROCEDURE fn_%(trigger_name)s();""" % {
                'table_name': table.name,
                'trigger_time': key['timing'],
                'trigger_event': key['event'],
                'trigger_name': key['name']})

        return trigger_sql

    def close(self):
        raise NotImplementedError

    def write_contents(self, table, reader):
        raise NotImplementedError

########NEW FILE########
__FILENAME__ = mysql2pgsql
from __future__ import absolute_import

import codecs

from .lib import print_red
from .lib.mysql_reader import MysqlReader
from .lib.postgres_file_writer import PostgresFileWriter
from .lib.postgres_db_writer import PostgresDbWriter
from .lib.converter import Converter
from .lib.config import Config
from .lib.errors import ConfigurationFileInitialized


class Mysql2Pgsql(object):
    def __init__(self, options):
        self.run_options = options
        try:
            self.file_options = Config(options.file, True).options
        except ConfigurationFileInitialized, e:
            print_red(e.message)
            raise e

    def convert(self):
        reader = MysqlReader(self.file_options['mysql'])

        if self.file_options['destination']['file']:
            writer = PostgresFileWriter(self._get_file(self.file_options['destination']['file']), self.run_options.verbose, tz=self.file_options.get('timezone', False))
        else:
            writer = PostgresDbWriter(self.file_options['destination']['postgres'], self.run_options.verbose, tz=self.file_options.get('timezone', False))

        Converter(reader, writer, self.file_options, self.run_options.verbose).convert()

    def _get_file(self, file_path):
        return codecs.open(file_path, 'wb', 'utf-8')

########NEW FILE########
__FILENAME__ = test_config
import sys
import os
import unittest
import tempfile

sys.path.append(os.path.abspath('../'))

from mysql2pgsql.lib.config import Config, CONFIG_TEMPLATE
from mysql2pgsql.lib.errors import ConfigurationFileInitialized,\
    ConfigurationFileNotFound

class TestMissingConfig(unittest.TestCase):
    def setUp(self):
        self.temp_file_1 = tempfile.NamedTemporaryFile().name
        temp_file_2 = tempfile.NamedTemporaryFile()
        self.temp_file_2 = temp_file_2.name
        temp_file_2.close()
        
    def test_create_new_file(self):
        self.assertRaises(ConfigurationFileInitialized, Config, self.temp_file_1, True)
        self.assertEqual(CONFIG_TEMPLATE, open(self.temp_file_1).read())

    def test_dont_create_new_file(self):
        self.assertRaises(ConfigurationFileNotFound, Config, self.temp_file_2, False)


class TestDefaultConfig(unittest.TestCase):
    def setUp(self):
        self.temp_file = tempfile.NamedTemporaryFile(delete=False)
        self.temp_file.write(CONFIG_TEMPLATE)
        self.temp_file.close()
        
    def tearDown(self):
        os.remove(self.temp_file.name)

    def test_config(self):
        c = Config(self.temp_file.name)
        assert c
        
        options = c.options
        assert options
        self.assertIsInstance(options, dict)
        
        assert 'mysql' in options
        assert 'hostname' in options['mysql']
        assert 'destination' in options
        assert 'file' in options['destination']
        assert 'postgres' in options['destination']
        assert 'supress_data' in options
        assert 'supress_ddl' in options
        assert 'force_truncate' in options
        assert 'only_tables' not in options
        assert 'exclude_tables' not in options

########NEW FILE########
__FILENAME__ = test_converter
from __future__ import with_statement, absolute_import
import os
import sys
import re

from . import WithReader

sys.path.append(os.path.abspath('../'))

from mysql2pgsql.lib.postgres_writer import PostgresWriter
from mysql2pgsql.lib.converter import Converter

class TestConverter(WithReader):
    def setUp(self):
        super(self.__class__, self).setUp()
        mock_writer = type('MockWriter', (PostgresWriter, ), {'close': lambda s: None,
                                                               'write_contents': lambda s, t, r: None})
        self.writer = mock_writer()

    def test_converter(self):
        Converter(self.reader, self.writer, {}, True).convert()
        Converter(self.reader, self.writer, {'force_truncate':True, 'supress_ddl': True}, True).convert()


########NEW FILE########
__FILENAME__ = test_db_writer
from __future__ import with_statement
import sys
import os
import unittest

sys.path.append(os.path.abspath('../'))

########NEW FILE########
__FILENAME__ = test_file_writer
from __future__ import with_statement
import sys
import os
import unittest

sys.path.append(os.path.abspath('../'))

########NEW FILE########
__FILENAME__ = test_mysql2pgsql
import sys
import os
import unittest
import tempfile

sys.path.append(os.path.abspath('../'))

from mysql2pgsql import Mysql2Pgsql
from mysql2pgsql.lib.errors import ConfigurationFileInitialized
class TestFullBoat(unittest.TestCase):
    def setUp(self):
        mock_options = type('MockOptions', (), {'file': os.path.join(os.path.dirname(__file__), 'mysql2pgsql-test.yml'),
                                                 'verbose': False})
        mock_missing_options = type('MockMissingOptions', (), {'file': os.path.join(os.path.dirname(__file__), 'mysql2pgsql-missing.yml'),
                                                 'verbose': False})
        self.options = mock_options()
        self.missing_options = mock_missing_options()

    def test_mysql2pgsql(self):
        m = Mysql2Pgsql(self.options)
        m._get_file = lambda f: tempfile.NamedTemporaryFile()
        m.convert()

        m.file_options['destination']['file'] = None

        m.convert()

    def test_missing_config(self):
        self.assertRaises(ConfigurationFileInitialized, Mysql2Pgsql, self.missing_options)
        
        
    def tearDown(self):
        if os.path.exists(self.missing_options.file):
            os.remove(self.missing_options.file)

########NEW FILE########
__FILENAME__ = test_reader
from __future__ import with_statement
import sys
import os
import unittest

from contextlib import closing

import MySQLdb

sys.path.append(os.path.abspath('../'))

from mysql2pgsql.lib.config import Config
from mysql2pgsql.lib.mysql_reader import MysqlReader
from mysql2pgsql.lib.errors import ConfigurationFileNotFound

class TestMysqlReader(unittest.TestCase):
    def setUp(self):
        try:
            self.config_file = os.path.join(os.path.dirname(__file__), 'mysql2pgsql-test.yml')
            config = Config(self.config_file, False)
        except ConfigurationFileNotFound:
            print("In order to run this test you must create the file %s" % config)
            sys.exit(-1)
        self.options = config.options['mysql']

        self.args = {            
            'user': self.options.get('username', 'root'),
            'db': self.options['database'],
            'use_unicode': True,
            'charset': 'utf8',
            }

        if self.options.get('password', None):
            self.args['passwd'] = self.options.get('password', None)

        if self.options.get('socket', None):
            self.args['unix_socket'] = self.options['socket']
        else:
            self.args['host'] = self.options.get('hostname', 'localhost')
            self.args['port'] = self.options.get('port', 3306)
            self.args['compress'] = self.options.get('compress', True)

        with open(os.path.join(os.path.dirname(__file__), 'schema.sql')) as sql:
            self.sql = sql.read()
            with closing(MySQLdb.connect(**self.args)) as conn:
                with closing(conn.cursor()) as cur:
                    for cmd in self.sql.split('-- SPLIT'):
                        cur.execute(cmd)
                        conn.commit()
        self.reader = MysqlReader(self.options)
        self.type_to_pos = {
            'text': (21, 22),
            'float': (83, 84, 85, 86, 87, 88, 89, 90),
            'numeric': (75, 76, 77, 78),
            'datetime': (113, 114, 115, 116, 117, 118),
            'char': (9, 10, 11, 12),
            'boolean': (49, 50),
            "enum('small','medium','large')": (1, 2, 3, 4),
            'bit(8)': (37, 38, 39, 40),
            'mediumblob': (27, 28),
            'mediumtext': (19, 20),
            'blob': (29, 30),
            "set('a','b','c','d','e')": (5, 6, 7, 8),
            'varchar': (13, 14, 15, 16),
            'timestamp': (125, 126, 127, 128, 129, 130),
            'binary(3)': (33, 34),
            'varbinary(50)': (35, 36),
            'date': (107, 108, 109, 110, 111, 112),
            'integer': (0, 51, 52, 53, 54, 59, 60, 61, 62, 63, 64, 65, 66, 71, 72, 73, 74),
            'double precision': (91, 92, 93, 94, 95, 96, 97, 98),
            'tinytext': (17, 18),
            'decimal': (99, 100, 101, 102, 103, 104, 105, 106, 136, 137, 138, 139, 140, 141, 142, 143),
            'longtext': (23, 24),
            'tinyint': (41, 42, 43, 44, 45, 46, 47, 48, 55, 56, 57, 58, 131, 132, 133, 134, 135),
            'bigint': (67, 68, 69, 70, 79, 80, 81, 82),
            'time': (119, 120, 121, 122, 123, 124),
            'tinyblob': (25, 26),
            'longblob': (31, 32)
            }

    def tearDown(self):
        self.reader.close()
        '''
        with closing(MySQLdb.connect(**self.args)) as conn:
            with closing(conn.cursor()) as cur:
                for cmd in self.sql.split('-- SPLIT')[:2]:
                    cur.execute(cmd)
                conn.commit()
'''
        
    def test_tables(self):
        table_list = list(self.reader.tables)
        assert table_list
        assert len(table_list) == 2

    def test_columns(self):
        for table in self.reader.tables:
            columns = table.columns
            if table.name == 'type_conversion_test_1':
                for k, v in self.type_to_pos.iteritems():
                    assert all(columns[i]['type'] == k for i in v)

            

    def test_indexes(self):
        for table in self.reader.tables:
            assert table.indexes

    def test_constraints(self):
        assert list(self.reader.tables)[1].foreign_keys


########NEW FILE########
__FILENAME__ = test_writer
from __future__ import with_statement, absolute_import
import os
import sys
import re
import tempfile
import unittest

from . import WithReader

sys.path.append(os.path.abspath('../'))

from mysql2pgsql.lib.postgres_writer import PostgresWriter
from mysql2pgsql.lib.postgres_file_writer import PostgresFileWriter
from mysql2pgsql.lib.postgres_db_writer import PostgresDbWriter

def squeeze(val):
    return re.sub(r"[\x00-\x20]+", " ", val).strip()
        
class WithTables(WithReader):
    def setUp(self):
        super(WithTables, self).setUp()
        self.table1 = next((t for t in self.reader.tables if t.name == 'type_conversion_test_1'), None)
        self.table2 = next((t for t in self.reader.tables if t.name == 'type_conversion_test_2'), None)
        assert self.table1
        assert self.table2


class TestPostgresWriter(WithTables):
    def setUp(self):
        super(self.__class__, self).setUp()
        self.writer = PostgresWriter()
        assert self.writer
        
    def test_truncate(self):
        trunc_cmds = self.writer.truncate(self.table1)
        assert len(trunc_cmds) == 2
        trunc_stmt, reset_seq = trunc_cmds
        assert squeeze(trunc_stmt) == 'TRUNCATE "%s" CASCADE;' % self.table1.name
        if reset_seq:
            self.assertRegexpMatches(squeeze(reset_seq),
                                 "^SELECT pg_catalog.setval\(pg_get_serial_sequence\('%s', 'id'\), \d+, true\);$" % self.table1.name)

    def test_write_table(self):
        write_table_cmds = self.writer.write_table(self.table1)
        assert len(write_table_cmds) == 2
        table_cmds, seq_cmds = write_table_cmds
        assert len(table_cmds) == 2
        assert squeeze(table_cmds[0]) == 'DROP TABLE IF EXISTS "%s" CASCADE;' % self.table1.name
        assert 'CREATE TABLE "%s"' % self.table1.name in table_cmds[1]
#        assert self.assertRegexpMatches(squeeze(table_cmds[1]),
#                                        '^CREATE TABLE "%s" \(.*\) WITHOUT OIDS;$' % self.table1.name)

        if seq_cmds:
            assert len(seq_cmds) == 3
            self.assertRegexpMatches(squeeze(seq_cmds[0]),
                                     '^DROP SEQUENCE IF EXISTS %s_([^\s]+)_seq CASCADE;$' % self.table1.name)
            self.assertRegexpMatches(squeeze(seq_cmds[1]),
                                     '^CREATE SEQUENCE %s_([^\s]+)_seq INCREMENT BY 1 NO MAXVALUE NO MINVALUE CACHE 1;$' % self.table1.name)
            self.assertRegexpMatches(squeeze(seq_cmds[2]),
                                     "^SELECT pg_catalog.setval\('%s_([^\s]+)_seq', \d+, true\);$" % self.table1.name)

    def test_write_indexex(self):
        index_cmds = self.writer.write_indexes(self.table1)
        assert len(index_cmds) == 9
        
    def test_write_constraints(self):
        constraint_cmds = self.writer.write_constraints(self.table2)
        assert constraint_cmds


class WithOutput(WithTables):

    def setUp(self):
        super(WithOutput, self).setUp()

    def tearDown(self):
        super(WithOutput, self).tearDown()



class TestPostgresFileWriter(WithOutput):
    def setUp(self):
        super(self.__class__, self).setUp()
        self.outfile = tempfile.NamedTemporaryFile()
        self.writer = PostgresFileWriter(self.outfile)

    def tearDown(self):
        super(self.__class__, self).tearDown()
        self.writer.close()

    def test_truncate(self):
        self.writer.truncate(self.table1)

    def test_write_table(self):
        self.writer.write_table(self.table1)

    def test_write_indexes(self):
        self.writer.write_indexes(self.table1)

    def test_write_constraints(self):
        self.writer.write_constraints(self.table2)

    def test_write_contents(self):
        self.writer.write_contents(self.table1, self.reader)


class TestPostgresDbWriter(WithOutput):
    def setUp(self):
        super(self.__class__, self).setUp()
        self.writer = PostgresDbWriter(self.config.options['destination']['postgres'], True)
    def tearDown(self):
        super(self.__class__, self).tearDown()
        self.writer.close()

    def test_truncate(self):
        self.writer.truncate(self.table1)

    def test_write_table_indexes_and_constraints(self):
        self.writer.write_table(table=self.table1)
        self.writer.write_indexes(self.table1)
        self.writer.write_constraints(self.table2)

    def test_write_contents(self):
        self.writer.write_contents(self.table1, self.reader)

########NEW FILE########
