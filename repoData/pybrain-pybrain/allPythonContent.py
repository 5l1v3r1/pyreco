__FILENAME__ = code2tut
#!/bin/env python
""" Utility script to convert Python source code into tutorials.

Synopsis:
    code2tut.py basename

Output:
    Will convert  tutorials/basename.py  into  sphinx/basename.txt

Conventions:
1. All textual comments must be enclosed in triple quotation marks.
2. First line of file is ignored, second line of file shall contain title in "",
   the following lines starting with # are ignored.
3. Lines following paragraph-level markup (e.g. .. seealso::) must be indented.
   Paragraph ends with a blank line.
4. If the code after a comment starts with a higher indentation level, you have
   to manually edit the resulting file, e.g. by inserting "   ..." at the
   beginning of these sections.

See tutorials/fnn.py for example.
"""

__author__ = "Martin Felder, felder@in.tum.de"
__version__ = "$Id$"

import sys
import os


f_in = file(os.path.join("tutorials",sys.argv[1])+".py")
f_out = file(os.path.join("sphinx",sys.argv[1])+".txt", "w+")

# write the header
f_out.write(".. _"+sys.argv[1]+":\n\n")
f_in.readline()                                  #  ######################
line = f_in.readline()
line= line.split('"')[1]             #  # PyBrain Tutorial "Classification ..."
f_out.write(line+"\n")
f_out.write("="*len(line)+'\n\n')

linecomment = False
comment = 0
begin = True
inblock = False

# the following is an ugly hack - don't look at it!
for line in f_in:
    linecomment = False
    # crop #-comments at start of file
    if line.startswith('#'):
        if begin:
            continue
    elif begin:
        begin = False

    if '"""' in line:
        for i in range(line.count('"""')):
            comment = 1 - comment
        if line.count('"""')==2:
            linecomment = True

        line = line.replace('"""','')
        if comment==0:
            line += '::'
            if not inblock:
                line = line.strip()

    elif comment==0 and line!='\n':
        line = "  "+line

    if line.startswith('..'):
        inblock = True
    elif line=="\n":
        inblock = False

    if (comment or linecomment) and not inblock:
        line = line.strip()+"\n"

    if line.endswith("::"):
        line +='\n\n'
    elif line.endswith("::\n"):
        line +='\n'

    f_out.write(line)


f_in.close()
f_out.close()

########NEW FILE########
__FILENAME__ = autodoc_hack
# replace the function below in sphinx.ext.autodoc.py (tested with Sphinx version 0.4.1)
__author__ = "Martin Felder"

def prepare_docstring(s):
    """
    Convert a docstring into lines of parseable reST.  Return it as a list of
    lines usable for inserting into a docutils ViewList (used as argument
    of nested_parse().)  An empty line is added to act as a separator between
    this docstring and following content.
    """
    if not s or s.isspace():
        return ['']
    s = s.expandtabs()

    # [MF] begin pydoc hack **************
    idxpar = s.find('@param')
    if idxpar > 0:
        # insert blank line before keyword list
        idx = s.rfind('\n',0,idxpar)
        s = s[:idx]+'\n'+s[idx:]
        # replace pydoc with sphinx notation
        s = s.replace("@param", ":param")
    # [MF] end pydoc hack **************

    nl = s.rstrip().find('\n')
    if nl == -1:
        # Only one line...
        return [s.strip(), '']
    # The first line may be indented differently...
    firstline = s[:nl].strip()
    otherlines = textwrap.dedent(s[nl+1:]) #@UndefinedVariable
    return [firstline] + otherlines.splitlines() + ['']

########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# PyBrain documentation build configuration file, created by
# sphinx-quickstart on Wed Jun  4 14:19:05 2008.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# The contents of this file are pickled, so don't put values in the namespace
# that aren't pickleable (module imports are okay, they're removed automatically).
#
# All configuration values have a default value; values that are commented out
# serve to show the default value.

# If your extensions are in another directory, add it here. If the directory
# is relative to the documentation root, use os.path.abspath to make it
# absolute, like shown here.
import sys
sys.path.append(__file__.rsplit('docs', 1)[0])

# General configuration
# ---------------------

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc',
              'sphinx.ext.inheritance_diagram',
              'sphinx.ext.pngmath']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.txt'

# The master toctree document.
master_doc = 'index'

# General substitutions.
project = 'PyBrain'
copyright = '2009, CogBotLab & Idsia'

# The default replacements for |version| and |release|, also used in various
# other places throughout the built documents.
#
# The short X.Y version.
version = '0.3'
# The full version, including alpha/beta/rc tags.
release = '0.3'

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
today_fmt = '%B %d, %Y'

# List of documents that shouldn't be included in the build.
#unused_docs = []

# List of directories, relative to source directories, that shouldn't be searched
# for source files.
#exclude_dirs = []

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'


# Options for HTML output
# -----------------------

# The style sheet to use for HTML and HTML Help pages. A file of that name
# must exist either in Sphinx' static/ path, or in one of the custom paths
# given in html_static_path.
html_style = 'default.css'

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# The name of an image file (within the static path) to place at the top of
# the sidebar.
html_logo = 'pics/pybrain_logo.gif'

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_use_modindex = True

# If true, the reST sources are included in the HTML build as _sources/<name>.
#html_copy_source = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'PyBraindoc'


# Options for LaTeX output
# ------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, document class [howto/manual]).
latex_documents = [
  ('index', 'PyBrain.tex', 'PyBrain Documentation', 'IDSIA/CogBotLab', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
# latex_logo = 'pics/pybrain_logo.gif'

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
latex_use_modindex = True

########NEW FILE########
__FILENAME__ = blackboxoptimization
############################################################################
# PyBrain Tutorial "Black Box Optimization"
# 
# Author: Tom Schaul, tom@idsia.ch
############################################################################

__author__ = 'Tom Schaul, tom@idsia.ch'

""" A script that attempts to illustrate a large variety of 
use-cases for different kinds of black-box learning algorithms. 
"""

from pybrain.structure.networks.network import Network
from pybrain.optimization import * #@UnusedWildImport

""" The problem we would like to solve can be anything that 
has something like a fitness function. 
The following switches between two different examples. 

The variable 'theparams' contains the trainable 
parameters that affect the fitness. """

if False:
    """ Simple function optimization:
    here the parameters are learned directly. """    
    from scipy import randn
    from pybrain.rl.environments.functions import SphereFunction
    thetask = SphereFunction(3)
    theparams = randn(3)
    
else:
    """ Simple pole-balancing task:
    here we learn the weights of a neural network controller."""   
    from pybrain.tools.shortcuts import buildNetwork
    from pybrain.rl.environments.cartpole.balancetask import BalanceTask
    thetask = BalanceTask()
    theparams = buildNetwork(thetask.outdim, thetask.indim, bias=False)    
    
print 'Subsequently, we attempt to solve the following task:'
print thetask.__class__.__name__


if isinstance(theparams, Network):
    print '\nby finding good weights for this (simple) network:'
    print theparams
    print '\nwhich has', theparams.paramdim, 'trainable parameters. (the dimensions of its layers are:',
    for m in theparams.modules:
        print m.indim, ',',
    print ')\n'
        
""" We allow every algorithm a limited number of evaluations. """

maxEvals = 1000

""" Standard function minimization: """

print 'fmin', NelderMead(thetask, theparams, maxEvaluations=maxEvals).learn()

""" The same, using some other algorithms 
(note that the syntax for invoking them is always the same) """

print 'CMA', CMAES(thetask, theparams, maxEvaluations=maxEvals).learn()
print 'NES', ExactNES(thetask, theparams, maxEvaluations=maxEvals).learn()
print 'FEM', FEM(thetask, theparams, maxEvaluations=maxEvals).learn()
print 'Finite Differences', FiniteDifferences(thetask, theparams, maxEvaluations=maxEvals).learn()
print 'SPSA', SimpleSPSA(thetask, theparams, maxEvaluations=maxEvals).learn()
print 'PGPE', PGPE(thetask, theparams, maxEvaluations=maxEvals).learn()

""" Evolutionary methods fall in the Learner framework as well. 
All the following are examples."""

print 'HillClimber', HillClimber(thetask, theparams, maxEvaluations=maxEvals).learn()
print 'WeightGuessing', WeightGuessing(thetask, theparams, maxEvaluations=maxEvals).learn()
print 'ES 50+50', ES(thetask, theparams, maxEvaluations=maxEvals).learn()

""" We can change some default parameters, e.g."""
print 'ES 5+5', ES(thetask, theparams, mu=5, lambada=5, maxEvaluations=maxEvals).learn()

""" Memetic algorithms are a kind of meta-algorithm, doing topology mutations 
on the top-level, and using other algorithms internally 
as a kind of local search (default there: hill-climbing)."""

print 'Memetic Climber', MemeticSearch(thetask, theparams, maxEvaluations=maxEvals).learn()
print 'Memetic ES 50+50', MemeticSearch(thetask, theparams, maxEvaluations=maxEvals,
                                        localSearch=ES, localSteps=200).learn()
print 'Memetic ES 5+5', MemeticSearch(thetask, theparams, maxEvaluations=maxEvals,
                                      localSearch=ES,
                                      localSearchArgs={'mu': 5, 'lambada': 5}).learn()
print 'Memetic NES', MemeticSearch(thetask, theparams, maxEvaluations=maxEvals,
                                   localSearch=ExactNES,
                                   localSearchArgs={'batchSize': 20}).learn()

""" Inner memetic is the population based variant (on the topology level). """

print 'Inner Memetic Climber', InnerMemeticSearch(thetask, theparams, maxEvaluations=maxEvals).learn()
print 'Inner Memetic CMA', InnerMemeticSearch(thetask, theparams, maxEvaluations=maxEvals,
                                              localSearch=CMAES).learn()

""" Inverse memetic algorithms do local search on topology mutations, 
and weight changes in the top-level search. """

print 'Inverse Memetic Climber', InverseMemeticSearch(thetask, theparams, maxEvaluations=maxEvals).learn()


########NEW FILE########
__FILENAME__ = fnn
############################################################################
# PyBrain Tutorial "Classification with Feed-Forward Neural Networks"
#
# Author: Martin Felder, felder@in.tum.de
############################################################################

""" This tutorial walks you through the process of setting up a dataset
for classification, and train a network on it while visualizing the results
online.

First we need to import the necessary components from PyBrain."""

from pybrain.datasets            import ClassificationDataSet
from pybrain.utilities           import percentError
from pybrain.tools.shortcuts     import buildNetwork
from pybrain.supervised.trainers import BackpropTrainer
from pybrain.structure.modules   import SoftmaxLayer

""" Furthermore, pylab is needed for the graphical output. """
from pylab import ion, ioff, figure, draw, contourf, clf, show, hold, plot
from scipy import diag, arange, meshgrid, where
from numpy.random import multivariate_normal

""" To have a nice dataset for visualization, we produce a set of
points in 2D belonging to three different classes. You could also
read in your data from a file, e.g. using pylab.load(). """

means = [(-1, 0), (2, 4), (3, 1)]
cov = [diag([1, 1]), diag([0.5, 1.2]), diag([1.5, 0.7])]
alldata = ClassificationDataSet(2, 1, nb_classes=3)
for n in xrange(400):
    for klass in range(3):
        input = multivariate_normal(means[klass], cov[klass])
        alldata.addSample(input, [klass])

""" Randomly split the dataset into 75% training and 25% test data sets. Of course, we
could also have created two different datasets to begin with."""
tstdata, trndata = alldata.splitWithProportion(0.25)

""" For neural network classification, it is highly advisable to encode classes
with one output neuron per class. Note that this operation duplicates the original
targets and stores them in an (integer) field named 'class'."""
trndata._convertToOneOfMany()
tstdata._convertToOneOfMany()

""" Test our dataset by printing a little information about it. """
print "Number of training patterns: ", len(trndata)
print "Input and output dimensions: ", trndata.indim, trndata.outdim
print "First sample (input, target, class):"
print trndata['input'][0], trndata['target'][0], trndata['class'][0]

""" Now build a feed-forward network with 5 hidden units. We use the a convenience
function for this. The input and output
layer size must match the dataset's input and target dimension. You could add
additional hidden layers by inserting more numbers giving the desired layer sizes.
The output layer uses a softmax function because we are doing classification.
There are more options to explore here, e.g. try changing the hidden layer transfer
function to linear instead of (the default) sigmoid.

.. seealso:: Desciption :func:`buildNetwork` for more info on options,
   and the Network tutorial :ref:`netmodcon` for info on how to build
   your own non-standard networks.

"""
fnn = buildNetwork(trndata.indim, 5, trndata.outdim, outclass=SoftmaxLayer)

""" Set up a trainer that basically takes the network and training dataset as input.
Currently the backpropagation and RPROP learning algorithms are implemented. See their
description for possible parameters. If you don't want to deal with this, just use RPROP
with default parameters. """
trainer = BackpropTrainer(fnn, dataset=trndata, momentum=0.1, verbose=True, weightdecay=0.01)
#trainer = RPropMinusTrainer( fnn, dataset=trndata, verbose=True)

""" Now generate a square grid of data points and put it into a dataset,
which we can then classifiy to obtain a nice contour field for visualization.
Therefore the target values for this data set can be ignored."""
ticks = arange(-3., 6., 0.2)
X, Y = meshgrid(ticks, ticks)
# need column vectors in dataset, not arrays
griddata = ClassificationDataSet(2, 1, nb_classes=3)
for i in xrange(X.size):
    griddata.addSample([X.ravel()[i], Y.ravel()[i]], [0])
griddata._convertToOneOfMany()  # this is still needed to make the fnn feel comfy

""" Start the training iterations. """
for i in range(20):

    """ Train the network for some epochs. Usually you would set something like 5 here,
    but for visualization purposes we do this one epoch at a time."""
    trainer.trainEpochs(1)

    """ Evaluate the network on the training and test data. There are several ways to do this - check
    out the :mod:`pybrain.tools.validation` module, for instance. Here we let the trainer do the test. """
    trnresult = percentError(trainer.testOnClassData(),
                              trndata['class'])
    tstresult = percentError(trainer.testOnClassData(
           dataset=tstdata), tstdata['class'])

    print "epoch: %4d" % trainer.totalepochs, \
          "  train error: %5.2f%%" % trnresult, \
          "  test error: %5.2f%%" % tstresult

    """ Run our grid data through the FNN, get the most likely class
    and shape it into a square array again. """
    out = fnn.activateOnDataset(griddata)
    out = out.argmax(axis=1)  # the highest output activation gives the class
    out = out.reshape(X.shape)

    """ Now plot the test data and the underlying grid as a filled contour. """
    figure(1)
    ioff()  # interactive graphics off
    clf()   # clear the plot
    hold(True) # overplot on
    for c in [0, 1, 2]:
        here, _ = where(tstdata['class'] == c)
        plot(tstdata['input'][here, 0], tstdata['input'][here, 1], 'o')
    if out.max() != out.min():  # safety check against flat field
        contourf(X, Y, out)   # plot the contour
    ion()   # interactive graphics on
    draw()  # update the plot

""" Finally, keep showing the plot until user kills it. """
ioff()
show()


########NEW FILE########
__FILENAME__ = networks
############################################################################
# PyBrain Tutorial "Networks, Modules, Connections"
#
# Author: Tom Schaul, tom@idsia.ch
############################################################################

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.structure import FeedForwardNetwork, LinearLayer, SigmoidLayer, FullConnection, RecurrentNetwork
from pybrain.tools.shortcuts import buildNetwork

""" This tutorial will attempt to guide you for using one of PyBrain's most basic structural elements:
Networks, and with them Modules and Connections.

Let us start with a simple example, building a multi-layer-perceptron (MLP).

First we make a new network object: """

n = FeedForwardNetwork()

""" Next, we're constructing the input, hidden and output layers. """

inLayer = LinearLayer(2)
hiddenLayer = SigmoidLayer(3)
outLayer = LinearLayer(1)

""" (Note that we could also have used a hidden layer of type TanhLayer, LinearLayer, etc.)

Let's add them to the network: """

n.addInputModule(inLayer)
n.addModule(hiddenLayer)
n.addOutputModule(outLayer)

""" We still need to explicitly determine how they should be connected. For this we use the most
common connection type, which produces a full connectivity between two layers (or Modules, in general):
the 'FullConnection'. """

in2hidden = FullConnection(inLayer, hiddenLayer)
hidden2out = FullConnection(hiddenLayer, outLayer)
n.addConnection(in2hidden)
n.addConnection(hidden2out)

""" All the elements are in place now, so we can do the final step that makes our MLP usable,
which is to call the 'sortModules()' method. """

n.sortModules()

""" Let's see what we did. """

print n

""" One way of using the network is to call its 'activate()' method with an input to be transformed. """

print n.activate([1, 2])

""" We can access the trainable parameters (weights) of a connection directly, or read
all weights of the network at once. """

print hidden2out.params
print n.params

""" The former are the last slice of the latter. """

print n.params[-3:] == hidden2out.params

""" Ok, after having covered the basics, let's move on to some additional concepts.
First of all, we encourage you to name all modules, or connections you create, because that gives you
more readable printouts, and a very concise way of accessing them.

We now build an equivalent network to the one before, but with a more concise syntax:
"""
n2 = RecurrentNetwork(name='net2')
n2.addInputModule(LinearLayer(2, name='in'))
n2.addModule(SigmoidLayer(3, name='h'))
n2.addOutputModule(LinearLayer(1, name='out'))
n2.addConnection(FullConnection(n2['in'], n2['h'], name='c1'))
n2.addConnection(FullConnection(n2['h'], n2['out'], name='c2'))
n2.sortModules()

""" Printouts look more concise and readable: """
print n2

""" There is an even quicker way to build networks though, as long as their structure is nothing
more fancy than a stack of fully connected layers: """

n3 = buildNetwork(2, 3, 1, bias=False)

""" Recurrent networks are working in the same way, except that the recurrent connections
need to be explicitly declared upon construction.

We can modify our existing network 'net2' and add a recurrent connection on the hidden layer: """

n2.addRecurrentConnection(FullConnection(n2['h'], n2['h'], name='rec'))

""" After every structural modification, if we want ot use the network, we call 'sortModules()' again"""

n2.sortModules()
print n2

""" As the network is now recurrent, successive activations produce different outputs: """

print n2.activate([1, 2]),
print n2.activate([1, 2]),
print n2.activate([1, 2])

""" The 'reset()' method re-initializes the network, and with it sets the recurrent
activations to zero, so now we get the same results: """

n2.reset()
print n2.activate([1, 2]),
print n2.activate([1, 2]),
print n2.activate([1, 2])

""" This is already a good coverage of the basics, but if you're an advanced user
you might want to find out about the possibilities of nesting networks within
others, using weight-sharing, and more exotic types of networks, connections
and modules... but that goes beyond the scope of this tutorial.
"""


########NEW FILE########
__FILENAME__ = rl
############################################################################
# PyBrain Tutorial "Reinforcement Learning"
#
# Author: Thomas Rueckstiess, ruecksti@in.tum.de
############################################################################

__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

"""
A reinforcement learning (RL) task in pybrain always consists of a few
components that interact with each other: Environment, Agent, Task, and
Experiment. In this tutorial we will go through each of them, create
the instances and explain what they do.

But first of all, we need to import some general packages and the RL
components from PyBrain:
"""

from scipy import * #@UnusedWildImport
import pylab

from pybrain.rl.environments.mazes import Maze, MDPMazeTask
from pybrain.rl.learners.valuebased import ActionValueTable
from pybrain.rl.agents import LearningAgent
from pybrain.rl.learners import Q, SARSA #@UnusedImport
from pybrain.rl.experiments import Experiment


"""
For later visualization purposes, we also need to initialize the
plotting engine.
"""

pylab.gray()
pylab.ion()


"""
The Environment is the world, in which the agent acts. It receives input
with the .performAction() method and returns an output with
.getSensors(). All environments in PyBrain are located under
pybrain/rl/environments.

One of these environments is the maze environment, which we will use for
this tutorial. It creates a labyrinth with free fields, walls, and an
goal point. An agent can move over the free fields and needs to find the
goal point. Let's define the maze structure, a simple 2D numpy array, where
1 is a wall and 0 is a free field:
"""
structure = array([[1, 1, 1, 1, 1, 1, 1, 1, 1],
                   [1, 0, 0, 1, 0, 0, 0, 0, 1],
                   [1, 0, 0, 1, 0, 0, 1, 0, 1],
                   [1, 0, 0, 1, 0, 0, 1, 0, 1],
                   [1, 0, 0, 1, 0, 1, 1, 0, 1],
                   [1, 0, 0, 0, 0, 0, 1, 0, 1],
                   [1, 1, 1, 1, 1, 1, 1, 0, 1],
                   [1, 0, 0, 0, 0, 0, 0, 0, 1],
                   [1, 1, 1, 1, 1, 1, 1, 1, 1]])

"""
Then we create the environment with the structure as first parameter
and the goal field tuple as second parameter:
"""
environment = Maze(structure, (7, 7))

"""
Next, we need an agent. The agent is where the learning happens. It can
interact with the environment with its .getAction() and
.integrateObservation() methods.

The agent itself consists of a controller, which maps states to actions,
a learner, which updates the controller parameters according to the
interaction it had with the world, and an explorer, which adds some
explorative behaviour to the actions. All standard agents already have a
default explorer, so we don't need to take care of that in this
tutorial.

The controller in PyBrain is a module, that takes states as inputs and
transforms them into actions. For value-based methods, like the
Q-Learning algorithm we will use here, we need a module that implements
the ActionValueInterface. There are currently two modules in PyBrain
that do this: The ActionValueTable for discrete actions and the
ActionValueNetwork for continuous actions. Our maze uses discrete
actions, so we need a table:
"""

controller = ActionValueTable(81, 4)
controller.initialize(1.)

"""
The table needs the number of states and actions as parameters. The standard
maze environment comes with the following 4 actions: north, south, east, west.

Then, we initialize the table with 1 everywhere. This is not always necessary
but will help converge faster, because unvisited state-action pairs have a
promising positive value and will be preferred over visited ones that didn't
lead to the goal.

Each agent also has a learner component. Several classes of RL learners
are currently implemented in PyBrain: black box optimizers, direct
search methods, and value-based learners. The classical Reinforcement
Learning mostly consists of value-based learning, in which of the most
well-known algorithms is the Q-Learning algorithm. Let's now create
the agent and give it the controller and learner as parameters.
"""

learner = Q()
agent = LearningAgent(controller, learner)

"""
So far, there is no connection between the agent and the environment. In fact,
in PyBrain, there is a special component that connects environment and agent: the
task. A task also specifies what the goal is in an environment and how the
agent is rewarded for its actions. For episodic experiments, the Task also
decides when an episode is over. Environments usually bring along their own
tasks. The Maze environment for example has a MDPMazeTask, that we will use.
MDP stands for "markov decision process" and means here, that the agent knows
its exact location in the maze. The task receives the environment as parameter.
"""

task = MDPMazeTask(environment)

"""
Finally, in order to learn something, we create an experiment, tell it both
task and agent (it knows the environment through the task) and let it run
for some number of steps or infinitely, like here:
"""

experiment = Experiment(task, agent)

while True:
    experiment.doInteractions(100)
    agent.learn()
    agent.reset()

    pylab.pcolor(controller.params.reshape(81,4).max(1).reshape(9,9))
    pylab.draw()


"""
Above, the experiment executes 100 interactions between agent and
environment, or, to be exact, between the agent and the task. The task
will process the agent's actions, possibly scale it and hand it over to
the environment. The environment responds, returns the new state back to
the task which decides what information should be given to the agent.
The task also gives a reward value for each step to the agent.

After 100 steps, we call the agent's .learn() method and then reset it.
This will make the agent forget the previously executed steps but of
course it won't undo the changes it learned.

Then the loop is repeated, until a desired behaviour is learned.

In order to observe the learning progress, we visualize the controller
with the last two code lines in the loop. The ActionValueTable consists
of a scalar value for each state/action pair, in this case 81x4 values.
A nice way to visualize learning is to only consider the maximum value
over all actions for each state. This value is called the state-value V
and is defined as V(s) = max_a Q(s, a).

We plot the new table after learning and resetting the agent, INSIDE the
while loop. Running this code, you should see the shape of the maze and
a change of colors for the free fields. During learning, colors may jump
and change back and forth, but eventually the learning should converge
to the true state values, having higher scores (brigher fields) the
closer they are to the goal.
"""


########NEW FILE########
__FILENAME__ = benchmarkplots
#!/usr/bin/env python
""" A little script to do contour-plots of a couple of the widely used optimization benchmarks. """

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.tools.plotting.fitnesslandscapes import FitnessPlotter
from pybrain.rl.environments.functions.multimodal import BraninFunction,\
    RastriginFunction
from pybrain.rl.environments.functions.unimodal import RosenbrockFunction,\
    GlasmachersFunction
import pylab


FitnessPlotter(BraninFunction(), -5., 10., 0., 15.).plotAll(popup = False)
pylab.title('Branin')

FitnessPlotter(RastriginFunction(2)).plotAll(popup = False)
pylab.title('Rastrigin')

FitnessPlotter(RosenbrockFunction(2), -2., 2., -2., 2.).plotAll(popup = False)
pylab.title('Rosenbrock')

FitnessPlotter(GlasmachersFunction(2), -2., 2., -2., 2.).plotAll(popup = False)
pylab.title('Glasmachers')

pylab.show()
########NEW FILE########
__FILENAME__ = constnsga2jpq
#!/usr/bin/env python
""" An illustration of using the NSGA-II multi-objective optimization algorithm
on Constrained Multi-Objective Optimization benchmark function. """

__author__ = 'Jean Pierre Queau, jeanpierre.queau@sbmoffshore.com'

from pybrain.optimization import ConstMultiObjectiveGA
from pybrain.rl.environments.functions.multiobjective import ConstDeb,ConstSrn, \
     ConstOsy,ConstTnk,ConstBnh
import pylab
from scipy import zeros, array

# The Deb function
#f = ConstDeb()
# The Srinivas & Deb function
#f = ConstSrn()
# The Osyczka & Kundu function
#f = ConstOsy()
# The Tanaka function
#f = ConstTnk()
# The Binh & Korn function
f = ConstBnh()
# start at the origin
x0 = zeros(f.indim)

x0 = array([min_ for min_, max_ in f.xbound])

# the optimization for a maximum of 25 generations
n = ConstMultiObjectiveGA(f, x0, storeAllEvaluations = True, populationSize = 100, eliteProportion = 1.0,
    topProportion = 1.0, mutationProb = 1.0, mutationStdDev = 0.3, storeAllPopulations = True, allowEquality = False)
print 'Start Learning'
n.learn(50)
print 'End Learning'
# plotting the results (blue = all evaluated points, red = resulting pareto front)
print 'Plotting the Results'
print 'All Evaluations.... take some time'
for x in n._allEvaluations:
    if x[1]:
        pylab.plot([x[0][0]], [x[0][1]], 'b.')
    else:
        pylab.plot([x[0][0]], [x[0][1]], 'r.')
for x in n.bestEvaluation: pylab.plot([x[0][0]], [x[0][1]], 'go')
pylab.show()
print 'Pareto Front'
for x in n.bestEvaluation: pylab.plot([x[0][0]], [x[0][1]], 'go')
pylab.show()

print '==========='
print '= Results =' 
print '==========='
'''
i=0
for gen in n._allGenerations:
    print 'Generation: ',i
    for j in range(len(gen[1])):
        print gen[1].keys()[j],gen[1].values()[j]
    i+=1
'''
print 'Population size ',n.populationSize
print 'Elitism Proportion ',n.eliteProportion
print 'Mutation Probability ',n.mutationProb
print 'Mutation Std Deviation ',n.mutationStdDev
print 'Objective Evaluation number ',n.numEvaluations
print 'last generation Length of bestEvaluation ',len(n.bestEvaluation)
print 'Best Evaluable : Best Evaluation'
for i in range(len(n.bestEvaluation)):
    assert len(n.bestEvaluation) == len(n.bestEvaluable)
    print n.bestEvaluable[i],':',n.bestEvaluation[i]
########NEW FILE########
__FILENAME__ = nsga2
#!/usr/bin/env python
""" An illustration of using the NSGA-II multi-objective optimization algorithm
on a simple standard benchmark function. """

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.optimization import MultiObjectiveGA
from pybrain.rl.environments.functions.multiobjective import KurBenchmark
import pylab
from scipy import zeros

# The benchmark function
f = KurBenchmark()

# start at the origin
x0 = zeros(f.indim)

# the optimization for a maximum of 25 generations
n = MultiObjectiveGA(f, x0, storeAllEvaluations = True)
n.learn(25)

# plotting the results (blue = all evaluated points, red = resulting pareto front)
for x in n._allEvaluations: pylab.plot([x[1]], [x[0]], 'b+')
for x in n.bestEvaluation: pylab.plot([x[1]], [x[0]], 'ro')
pylab.show()
########NEW FILE########
__FILENAME__ = nsga2jpq
#!/usr/bin/env python
""" An illustration of using the NSGA-II multi-objective optimization algorithm
on Unconstrained Multi-Objective Optimization benchmark function. """

__author__ = 'Jean Pierre Queau, jeanpierre.queau@sbmoffshore.com'

from pybrain.optimization import MultiObjectiveGA
from pybrain.rl.environments.functions.multiobjective import Deb, Pol
import pylab
from scipy import zeros, array

# The Deb function
#f = Deb()
# The Pol function
f = Pol()

# start at the origin
x0 = zeros(f.indim)

x0 = array([min_ for min_, max_ in f.xbound])

# the optimization for a maximum of 25 generations
n = MultiObjectiveGA(f, x0, storeAllEvaluations = True, populationSize = 50, eliteProportion = 1.0,
    topProportion = 1.0, mutationProb = 0.5, mutationStdDev = 0.1, storeAllPopulations = True, allowEquality = False)
print 'Start Learning'
n.learn(30)
print 'End Learning'

# plotting the results (blue = all evaluated points, red = resulting pareto front)
print 'Plotting the Results'
print 'All Evaluations'
for x in n._allEvaluations: pylab.plot([x[0]], [x[1]], 'b.')
for x in n.bestEvaluation: pylab.plot([x[0]], [x[1]], 'ro')
pylab.show()
print 'Pareto Front'
for x in n.bestEvaluation: pylab.plot([x[0]], [x[1]], 'ro')
pylab.show()
print '==========='
print '= Results =' 
print '==========='
'''
i=0
for gen in n._allGenerations:
    print 'Generation: ',i
    for j in range(len(gen[1])):
        print gen[1].keys()[j],gen[1].values()[j]
    i+=1
'''
print 'Population size ',n.populationSize
print 'Elitism Proportion ',n.eliteProportion
print 'Mutation Probability ',n.mutationProb
print 'Mutation Std Deviation ',n.mutationStdDev
print 'Objective Evaluation number ',n.numEvaluations
print 'last generation Length of bestEvaluation ',len(n.bestEvaluation)
print 'Best Evaluable : Best Evaluation'
for i in range(len(n.bestEvaluation)):
    assert len(n.bestEvaluation) == len(n.bestEvaluable)
    print n.bestEvaluable[i],':',n.bestEvaluation[i]
########NEW FILE########
__FILENAME__ = optimizerinterface
#!/usr/bin/env python
"""
Illustrating the interface of black-box optimizers on a few simple problems:
- how to initialize when:
    * optimizing the parameters for a function
    * optimizing a neural network controller for a task
- how to set meta-parameters
- how to learn
- how to interrupt learning and continue where you left off
- how to access the information gathered during learning
"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import array

from pybrain.optimization import * #@UnusedWildImport
from pybrain.structure.parametercontainer import ParameterContainer
from pybrain.rl.environments.functions.unimodal import TabletFunction
from pybrain.rl.environments.shipsteer.northwardtask import GoNorthwardTask
from pybrain.tools.shortcuts import buildNetwork


# this script uses a very simple learning algorithm: hill-climbing.
# all other black-box optimizers can be user in the same way.
# Try it yourself, change the following line to use e.g. GA, CMAES, MemeticSearch or NelderMead
algo = HillClimber
#algo = GA
#algo = MemeticSearch
#algo = NelderMead
algo = CMAES
print 'Algorithm:', algo.__name__


# ------------------------
# ---- Initialization ----
# ------------------------

# here's the default way of setting it up: provide a function and an initial point
f = TabletFunction(2)
x0 = [2.1, 4]
l = algo(f, x0)

# f can also be a simple lambda function
l = algo(lambda x: sum(x)**2, x0)

# in the case of continuous optimization, the initial point
# can be provided as a list (above), an array...
l = algo(f, array(x0))

# ... or a ParameterContainer
pcontainer = ParameterContainer(2)
pcontainer._setParameters(x0)
l = algo(f, pcontainer)

# the initial point can be omitted if:
# a) the problem dimension is specified manually
l = algo(f, numParameters = 2)

# b) the function is a FunctionEnvironment that specifies the problem dimension itself
l = algo(f)

# but if none is the case this leads to an error:
try:
    l = algo(lambda x: sum(x)**2)
except ValueError, e:
    print 'Error caught:', e

# Initialization can also take place in 2 steps, first with the settings and then with the
# evaluator function:
l = algo()
l.setEvaluator(f)

# Learning can only be called after the second step, otherwise:
l = algo()
try:
    l.learn(0)
except AssertionError, e:
    print 'Error caught:', e
l.setEvaluator(f)
# no error anymore
l.learn(0)

# a very similar interface can be used to optimize the parameters of a Module
# (here a neural network controller) on an EpisodicTask
task = GoNorthwardTask()
nnet = buildNetwork(task.outdim, 2, task.indim)
l = algo(task, nnet)

# Normally optimization algorithms have reasonable default settings (meta-parameters).
# In case you want to be more specific, use keyword arguments, like this:

l = ES(f, mu = 10, lambada = 20)
l = OriginalNES(f, batchSize = 25, importanceMixing = False)

# if you mistype a keyword, or specify one that is not applicable,
# you will see a warning (but the initialization still takes place, ignoring those).
l = algo(f, batchSise = 10, theMiddleOfTheTutorial = 'here')


# -----------------------
# ----   Learning    ----
# -----------------------

# Learning is even simpler:
print l.learn(5)

# The return values are the best point found, and its fitness
# (the argument indicates the number of learning steps/generations).

# The argument is not mandatory, in that case it will run until
# one of the stopping criteria is reached. For example:
# a) maximal number of evaluations (accessible in .numEvaluations)
l = algo(f, maxEvaluations = 20)
l.learn()
print l.learn(), 'in', l.numEvaluations, 'evaluations.'

# b) desiredValue
l = algo(f, desiredEvaluation = 10)
print l.learn(), ': fitness below 10 (we minimize the function).'

# c) maximal number of learning steps
l = algo(f, maxLearningSteps = 25)
l.learn()
print l.learn(), 'in', l.numLearningSteps, 'learning steps.'

# it is possible to continue learning from where we left off, for a
# specific number of additional learning steps:
print l.learn(75), 'in', l.numLearningSteps, 'total learning steps.'

# Finally you can set storage settings and then access all evaluations made
# during learning, e.g. for plotting:
l = algo(f, x0, storeAllEvaluations = True, storeAllEvaluated = True, maxEvaluations = 150)
l.learn()
try:
    import pylab
    pylab.plot(map(abs,l._allEvaluations))
    pylab.semilogy()
    pylab.show()
except ImportError, e:
    print 'No plotting:', e


########NEW FILE########
__FILENAME__ = optimizers_for_rl
#!/usr/bin/env python
"""
Illustrating how to use optimization algorithms in a reinforcement learning framework.
"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.utilities import fListToString
from pybrain.rl.environments.cartpole.balancetask import BalanceTask
from pybrain.tools.shortcuts import buildNetwork
from pybrain.optimization import HillClimber, CMAES #@UnusedImport
# from pybrain.rl.learners.continuous.policygradients import ENAC
# from pybrain.rl.agents.learning import LearningAgent
from pybrain.rl.agents import OptimizationAgent
from pybrain.rl.experiments import EpisodicExperiment


# any episodic task
task = BalanceTask()

# any neural network controller
net = buildNetwork(task.outdim, 1, task.indim)

# any optimization algorithm to be plugged in, for example:
# learner = CMAES(storeAllEvaluations = True)
# or:
learner = HillClimber(storeAllEvaluations = True)

# in a non-optimization case the agent would be a LearningAgent:
# agent = LearningAgent(net, ENAC())
# here it is an OptimizationAgent:
agent = OptimizationAgent(net, learner)

# the agent and task are linked in an Experiment
# and everything else happens under the hood.
exp = EpisodicExperiment(task, agent)
exp.doEpisodes(100)

print 'Episodes learned from:', len(learner._allEvaluations)
n, fit = learner._bestFound()
print 'Best fitness found:', fit
print 'with this network:'
print n
print 'containing these parameters:'
print fListToString(n.params, 4)

########NEW FILE########
__FILENAME__ = evolvingplayer
#!/usr/bin/env python
""" A script illustrating how to evolve a simple Capture-Game Player
which uses a MDRNN as network, with a simple ES algorithm."""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.rl.environments.twoplayergames import CaptureGameTask
from pybrain.structure.evolvables.cheaplycopiable import CheaplyCopiable
from pybrain.optimization import ES
from pybrain.utilities import storeCallResults
from pybrain.rl.environments.twoplayergames.capturegameplayers.killing import KillingPlayer

# task settings: opponent, averaging to reduce noise, board size, etc.
size = 5
simplenet = False
task = CaptureGameTask(size, averageOverGames = 40, opponent = KillingPlayer)

# keep track of evaluations for plotting
res = storeCallResults(task)

if simplenet:
    # simple network
    from pybrain.tools.shortcuts import buildNetwork
    from pybrain import SigmoidLayer
    net = buildNetwork(task.outdim, task.indim, outclass = SigmoidLayer)
else:
    # specialized mdrnn variation
    from pybrain.structure.networks.custom.capturegame import CaptureGameNetwork
    net = CaptureGameNetwork(size = size, hsize = 2, simpleborders = True)

net = CheaplyCopiable(net)
print net.name, 'has', net.paramdim, 'trainable parameters.'

learner = ES(task, net, mu = 5, lambada = 5,
             verbose = True, evaluatorIsNoisy = True,
             maxEvaluations = 50)
newnet, f = learner.learn()

# now, let's take the result, and compare it's performance on a larger game-baord (to the original one)
newsize = 7
bignew = newnet.getBase().resizedTo(newsize)
bigold = net.getBase().resizedTo(newsize)

print 'The rescaled network,', bignew.name, ', has', bignew.paramdim, 'trainable parameters.'

newtask = CaptureGameTask(newsize, averageOverGames = 50, opponent = KillingPlayer)
print 'Old net on big board score:', newtask(bigold)
print 'New net on big board score:', newtask(bignew)


# plot the progression
from pylab import plot, show
plot(res)
show()
########NEW FILE########
__FILENAME__ = minitournament
#!/usr/bin/env python
""" A little example script showing a Capture-Game tournament between
 - a random player
 - a kill-on-sight player
 - a small-network-player with random weights
"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.rl.environments.twoplayergames import CaptureGame
from pybrain.rl.environments.twoplayergames.capturegameplayers import RandomCapturePlayer, KillingPlayer, ModuleDecidingPlayer
from pybrain.rl.environments.twoplayergames.capturegameplayers.clientwrapper import ClientCapturePlayer
from pybrain.rl.experiments.tournament import Tournament
from pybrain.tools.shortcuts import buildNetwork
from pybrain import SigmoidLayer

game = CaptureGame(5)
randAgent = RandomCapturePlayer(game, name = 'rand')
killAgent = KillingPlayer(game, name = 'kill')

# the network's outputs are probabilities of choosing the action, thus a sigmoid output layer
net = buildNetwork(game.outdim, game.indim, outclass = SigmoidLayer)
netAgent = ModuleDecidingPlayer(net, game, name = 'net')

# same network, but greedy decisions:
netAgentGreedy = ModuleDecidingPlayer(net, game, name = 'greedy', greedySelection = True)

agents = [randAgent, killAgent, netAgent, netAgentGreedy]

try:
    javaAgent = ClientCapturePlayer(game, name = 'java')
    agents.append(javaAgent)
except:
    print 'No Java server available.'

print
print 'Starting tournament...'
tourn = Tournament(game, agents)
tourn.organize(50)
print tourn

# try a different network, and play again:
net.randomize()
tourn.reset()
tourn.organize(50)
print tourn




########NEW FILE########
__FILENAME__ = pente
#!/usr/bin/env python
""" A little script illustrating how to use a (randomly initialized)
convolutional network to play a game of Pente. """

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.rl.environments.twoplayergames.pente import PenteGame
from pybrain.rl.environments.twoplayergames.gomokuplayers.randomplayer import RandomGomokuPlayer
from pybrain.rl.environments.twoplayergames.gomokuplayers.moduledecision import ModuleDecidingPlayer
from pybrain.structure.networks.custom.convboard import ConvolutionalBoardNetwork

dim = 7
g = PenteGame((dim, dim))
print g
n = ConvolutionalBoardNetwork(dim, 5, 3)
p1 = ModuleDecidingPlayer(n, g)
p2 = RandomGomokuPlayer(g)
p2.color = g.WHITE
g.playToTheEnd(p1, p2)
print g

########NEW FILE########
__FILENAME__ = cart_all
#!/usr/bin/env python
#########################################################################
# Reinforcement Learning with several optimization algorithms
# on the CartPoleEnvironment 
#
# Requirements: pylab (for plotting only). If not available, comment the
# last 3 lines out
#########################################################################

__author__ = "Thomas Rueckstiess, Frank Sehnke"


from pybrain.tools.example_tools import ExTools
from pybrain.tools.shortcuts import buildNetwork
from pybrain.rl.environments.cartpole import CartPoleEnvironment, BalanceTask
from pybrain.rl.agents import OptimizationAgent
from pybrain.optimization import PGPE #@UnusedImport
from pybrain.optimization import ExactNES #@UnusedImport
from pybrain.optimization import FEM #@UnusedImport
from pybrain.optimization import CMAES #@UnusedImport

from pybrain.rl.experiments import EpisodicExperiment

batch=2 #number of samples per learning step
prnts=100 #number of learning steps after results are printed
epis=4000/batch/prnts #number of roleouts
numbExp=40 #number of experiments
et = ExTools(batch, prnts) #tool for printing and plotting
expList = ["PGPE(storeAllEvaluations = True)", "ExactNES(storeAllEvaluations = True)", "FEM(storeAllEvaluations = True)", "CMAES(storeAllEvaluations = True)"]
for e in expList:
    for runs in range(numbExp):
        # create environment
        env = CartPoleEnvironment()    
        # create task
        task = BalanceTask(env, 200, desiredValue=None)
        # create controller network
        net = buildNetwork(4, 1, bias=False)
        # create agent with controller and learner (and its options)
        agent = OptimizationAgent(net, eval(e))
        et.agent = agent
        # create the experiment
        experiment = EpisodicExperiment(task, agent)

        #Do the experiment
        for updates in range(epis):
            for i in range(prnts):
                experiment.doEpisodes(batch)
            et.printResults((agent.learner._allEvaluations)[-50:-1], runs, updates)
        et.addExps()
    et.nextExps()
et.showExps()

########NEW FILE########
__FILENAME__ = cart_cma
#!/usr/bin/env python
#########################################################################
# Reinforcement Learning with CMA-ES on the CartPoleEnvironment 
#
# Requirements: pylab (for plotting only). If not available, comment the
# last 3 lines out
#########################################################################

__author__ = "Thomas Rueckstiess, Frank Sehnke"


from pybrain.tools.example_tools import ExTools
from pybrain.tools.shortcuts import buildNetwork
from pybrain.rl.environments.cartpole import CartPoleEnvironment, BalanceTask
from pybrain.rl.agents import OptimizationAgent
from pybrain.optimization import CMAES
from pybrain.rl.experiments import EpisodicExperiment

batch=2 #number of samples per learning step
prnts=100 #number of learning steps after results are printed
epis=4000/batch/prnts #number of roleouts
numbExp=10 #number of experiments
et = ExTools(batch, prnts) #tool for printing and plotting

for runs in range(numbExp):
    # create environment
    env = CartPoleEnvironment()    
    # create task
    task = BalanceTask(env, 200, desiredValue=None)
    # create controller network
    net = buildNetwork(4, 1, bias=False)
    # create agent with controller and learner (and its options)
    agent = OptimizationAgent(net, CMAES(storeAllEvaluations = True))
    et.agent = agent
    # create the experiment
    experiment = EpisodicExperiment(task, agent)

    #Do the experiment
    for updates in range(epis):
        for i in range(prnts):
            experiment.doEpisodes(batch)
        et.printResults((agent.learner._allEvaluations)[-50:-1], runs, updates)
    et.addExps()
et.showExps()

########NEW FILE########
__FILENAME__ = cart_enac
#!/usr/bin/env python
#########################################################################
# Reinforcement Learning with ENAC on the CartPoleEnvironment 
#
# Requirements: pylab (for plotting only). If not available, comment the
# last 3 lines out
#########################################################################

__author__ = "Thomas Rueckstiess, Frank Sehnke"


from pybrain.tools.example_tools import ExTools
from pybrain.tools.shortcuts import buildNetwork
from pybrain.rl.environments.cartpole import CartPoleEnvironment, BalanceTask
from pybrain.rl.agents import LearningAgent
from pybrain.rl.learners import ENAC
from pybrain.rl.experiments import EpisodicExperiment

batch=50 #number of samples per learning step
prnts=4 #number of learning steps after results are printed
epis=4000/batch/prnts #number of roleouts
numbExp=10 #number of experiments
et = ExTools(batch, prnts, kind = "learner") #tool for printing and plotting

for runs in range(numbExp):
    # create environment
    env = CartPoleEnvironment()    
    # create task
    task = BalanceTask(env, 200, desiredValue=None)
    # create controller network
    net = buildNetwork(4, 1, bias=False)
    # create agent with controller and learner (and its options)
    agent = LearningAgent(net, ENAC())
    et.agent = agent
    # create the experiment
    experiment = EpisodicExperiment(task, agent)

    #Do the experiment
    for updates in range(epis):
        for i in range(prnts):
            experiment.doEpisodes(batch)
        state, action, reward = agent.learner.dataset.getSequence(agent.learner.dataset.getNumSequences()-1)
        et.printResults(reward.sum(), runs, updates)
    et.addExps()
et.showExps()

########NEW FILE########
__FILENAME__ = cart_fem
#!/usr/bin/env python
#########################################################################
# Reinforcement Learning with FEM on the CartPoleEnvironment 
#
# Requirements: pylab (for plotting only). If not available, comment the
# last 3 lines out
#########################################################################

__author__ = "Thomas Rueckstiess, Frank Sehnke"


from pybrain.tools.example_tools import ExTools
from pybrain.tools.shortcuts import buildNetwork
from pybrain.rl.environments.cartpole import CartPoleEnvironment, BalanceTask
from pybrain.rl.agents import OptimizationAgent
from pybrain.optimization import FEM
from pybrain.rl.experiments import EpisodicExperiment

batch=2 #number of samples per learning step
prnts=100 #number of learning steps after results are printed
epis=4000/batch/prnts #number of roleouts
numbExp=10 #number of experiments
et = ExTools(batch, prnts) #tool for printing and plotting

for runs in range(numbExp):
    # create environment
    env = CartPoleEnvironment()    
    # create task
    task = BalanceTask(env, 200, desiredValue=None)
    # create controller network
    net = buildNetwork(4, 1, bias=False)
    # create agent with controller and learner (and its options)
    agent = OptimizationAgent(net, FEM(storeAllEvaluations = True))
    et.agent = agent
    # create the experiment
    experiment = EpisodicExperiment(task, agent)

    #Do the experiment
    for updates in range(epis):
        for i in range(prnts):
            experiment.doEpisodes(batch)
        #print "Epsilon   : ", agent.learner.sigma
        et.printResults((agent.learner._allEvaluations)[-50:-1], runs, updates)
    et.addExps()
et.showExps()

########NEW FILE########
__FILENAME__ = cart_nes
#!/usr/bin/env python
#########################################################################
# Reinforcement Learning with NES on the CartPoleEnvironment 
#
# Requirements: pylab (for plotting only). If not available, comment the
# last 3 lines out
#########################################################################

__author__ = "Thomas Rueckstiess, Frank Sehnke"


from pybrain.tools.example_tools import ExTools
from pybrain.tools.shortcuts import buildNetwork
from pybrain.rl.environments.cartpole import CartPoleEnvironment, BalanceTask
from pybrain.rl.agents import OptimizationAgent
from pybrain.optimization import ExactNES
from pybrain.rl.experiments import EpisodicExperiment

batch=2 #number of samples per learning step
prnts=100 #number of learning steps after results are printed
epis=4000/batch/prnts #number of roleouts
numbExp=10 #number of experiments
et = ExTools(batch, prnts) #tool for printing and plotting

for runs in range(numbExp):
    # create environment
    env = CartPoleEnvironment()    
    # create task
    task = BalanceTask(env, 200, desiredValue=None)
    # create controller network
    net = buildNetwork(4, 1, bias=False)
    # create agent with controller and learner (and its options)
    agent = OptimizationAgent(net, ExactNES(storeAllEvaluations = True))
    et.agent = agent
    # create the experiment
    experiment = EpisodicExperiment(task, agent)

    #Do the experiment
    for updates in range(epis):
        for i in range(prnts):
            experiment.doEpisodes(batch)
        print "Epsilon   : ", agent.learner.sigma
        et.printResults((agent.learner._allEvaluations)[-50:-1], runs, updates)
    et.addExps()
et.showExps()

########NEW FILE########
__FILENAME__ = cart_pgpe
#!/usr/bin/env python
#########################################################################
# Reinforcement Learning with PGPE on the CartPoleEnvironment 
#
# Requirements: pylab (for plotting only). If not available, comment the
# last 3 lines out
#########################################################################
__author__ = "Thomas Rueckstiess, Frank Sehnke"
__version__ = '$Id$' 

from pybrain.tools.example_tools import ExTools
from pybrain.tools.shortcuts import buildNetwork
from pybrain.rl.environments.cartpole import CartPoleEnvironment, BalanceTask
from pybrain.rl.agents import OptimizationAgent
from pybrain.optimization import PGPE
from pybrain.rl.experiments import EpisodicExperiment

batch=1 #number of samples per learning step
prnts=100 #number of learning steps after results are printed
epis=4000/batch/prnts #number of roleouts
numbExp=10 #number of experiments
et = ExTools(batch, prnts) #tool for printing and plotting

for runs in range(numbExp):
    # create environment
    env = CartPoleEnvironment()    
    # create task
    task = BalanceTask(env, 200, desiredValue=None)
    # create controller network
    net = buildNetwork(4, 1, bias=False)
    # create agent with controller and learner (and its options)
    agent = OptimizationAgent(net, PGPE(storeAllEvaluations = True))
    et.agent = agent
    # create the experiment
    experiment = EpisodicExperiment(task, agent)

    #Do the experiment
    for updates in range(epis):
        for i in range(prnts):
            experiment.doEpisodes(batch)
        et.printResults((agent.learner._allEvaluations)[-50:-1], runs, updates)
    et.addExps()
et.showExps()

########NEW FILE########
__FILENAME__ = cart_reinf
#!/usr/bin/env python
#########################################################################
# Reinforcement Learning with REINFORCE on the CartPoleEnvironment 
#
# Requirements: pylab (for plotting only). If not available, comment the
# last 3 lines out
#########################################################################
__author__ = "Thomas Rueckstiess, Frank Sehnke"
__version__ = '$Id$' 

from pybrain.tools.example_tools import ExTools
from pybrain.tools.shortcuts import buildNetwork
from pybrain.rl.environments.cartpole import CartPoleEnvironment, BalanceTask
from pybrain.rl.agents import LearningAgent
from pybrain.rl.learners import Reinforce
from pybrain.rl.experiments import EpisodicExperiment

batch=50 #number of samples per learning step
prnts=4 #number of learning steps after results are printed
epis=4000/batch/prnts #number of roleouts
numbExp=10 #number of experiments
et = ExTools(batch, prnts, kind = "learner") #tool for printing and plotting

for runs in range(numbExp):
    # create environment
    env = CartPoleEnvironment()    
    # create task
    task = BalanceTask(env, 200, desiredValue=None)
    # create controller network
    net = buildNetwork(4, 1, bias=False)
    # create agent with controller and learner (and its options)
    agent = LearningAgent(net, Reinforce())
    et.agent = agent
    # create the experiment
    experiment = EpisodicExperiment(task, agent)

    #Do the experiment
    for updates in range(epis):
        for i in range(prnts):
            experiment.doEpisodes(batch)
        state, action, reward = agent.learner.dataset.getSequence(agent.learner.dataset.getNumSequences()-1)
        et.printResults(reward.sum(), runs, updates)
    et.addExps()
et.showExps()

########NEW FILE########
__FILENAME__ = play_cartpole
#!/usr/bin/env python
###########################################################################
# This program takes 4 parameters at the command line and runs the
# (single) cartpole environment with it, visualizing the cart and the pole.
# if cart is green, no penalty is given. if the cart is blue, a penalty of
# -1 per step is given. the program ends with the end of the episode. if
# the variable "episodes" is changed to a bigger number, the task is executed
# faster and the mean return of all episodes is printed.
###########################################################################
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from pybrain.tools.shortcuts import buildNetwork
from pybrain.rl.environments.cartpole import CartPoleEnvironment, CartPoleRenderer, BalanceTask
from pybrain.rl.agents.learning import LearningAgent
from pybrain.rl.experiments import EpisodicExperiment
from scipy import mean
import sys

episodes = 1
epilen = 200

if len(sys.argv) < 5:
    sys.exit('please give 4 parameters. run: "python play_catpole.py <p1> <p2> <p3> <p4>"\n')

# create environment
env = CartPoleEnvironment()
env.setRenderer(CartPoleRenderer())
env.getRenderer().start()
env.delay = (episodes == 1)

# create task
task = BalanceTask(env, epilen)

# create controller network
net = buildNetwork(4, 1, bias=False)

# create agent and set parameters from command line
agent = LearningAgent(net, None)
agent.module._setParameters([float(sys.argv[1]), float(sys.argv[2]), float(sys.argv[3]), float(sys.argv[4])])

# create experiment
experiment = EpisodicExperiment(task, agent)
experiment.doEpisodes(episodes)

# run environment
ret = []
for n in range(agent.history.getNumSequences()):
    returns = agent.history.getSequence(n)
    reward = returns[2]
    ret.append( sum(reward, 0).item() )

# print results
print ret, "mean:",mean(ret)
#env.getRenderer().stop()




########NEW FILE########
__FILENAME__ = flexcube_pgpe
#!/usr/bin/env python
#########################################################################
# Reinforcement Learning with PGPE on the FlexCube Environment 
#
# The FlexCube Environment is a Mass-Spring-System composed of 8 mass points.
# These resemble a cube with flexible edges.
#
# Control/Actions:
# The agent can control the 12 equilibrium edge lengths. 
#
# A wide variety of sensors are available for observation and reward:
# - 12 edge lengths
# - 12 wanted edge lengths (the last action)
# - vertexes contact with floor
# - vertexes min height (distance of closest vertex to the floor)
# - distance to origin
# - distance and angle to target
#
# Task available are:
# - GrowTask, agent has to maximize the volume of the cube
# - JumpTask, agent has to maximize the distance of the lowest mass point during the episode
# - WalkTask, agent has to maximize the distance to the starting point
# - WalkDirectionTask, agent has to minimize the distance to a target point.
# - TargetTask, like the previous task but with several target points
# 
# Requirements: pylab (for plotting only). If not available, comment the
# last 3 lines out
# Author: Frank Sehnke, sehnke@in.tum.de
#########################################################################
__author__ = "Frank Sehnke"
__version__ = '$Id$' 

from pybrain.tools.example_tools import ExTools
from pybrain.structure.modules.tanhlayer import TanhLayer
from pybrain.tools.shortcuts import buildNetwork
from pybrain.rl.environments.flexcube import FlexCubeEnvironment, WalkTask
from pybrain.rl.agents import OptimizationAgent
from pybrain.optimization import PGPE 
from pybrain.rl.experiments import EpisodicExperiment

hiddenUnits = 4
batch=1 #number of samples per learning step
prnts=1 #number of learning steps after results are printed
epis=5000000/batch/prnts #number of roleouts
numbExp=10 #number of experiments
et = ExTools(batch, prnts) #tool for printing and plotting

env = None
for runs in range(numbExp):
    # create environment
    #Options: Bool(OpenGL), Bool(Realtime simu. while client is connected), ServerIP(default:localhost), Port(default:21560)
    if env != None: env.closeSocket()
    env = FlexCubeEnvironment()
    # create task
    task = WalkTask(env)
    # create controller network
    net = buildNetwork(len(task.getObservation()), hiddenUnits, env.actLen, outclass=TanhLayer)    
    # create agent with controller and learner (and its options)
    agent = OptimizationAgent(net, PGPE(storeAllEvaluations = True))
    et.agent = agent
    # create the experiment
    experiment = EpisodicExperiment(task, agent)

    #Do the experiment
    for updates in range(epis):
        for i in range(prnts):
            experiment.doEpisodes(batch)
        et.printResults((agent.learner._allEvaluations)[-50:-1], runs, updates)
    et.addExps()
et.showExps()
#To view what the simulation is doing at the moment, go to pybrain/rl/environments/flexcube/ and start renderer.py (python-openGL musst be installed)

########NEW FILE########
__FILENAME__ = flexcube_spsa
#!/usr/bin/env python
#########################################################################
# Reinforcement Learning with PGPE on the FlexCube Environment 
#
# The FlexCube Environment is a Mass-Spring-System composed of 8 mass points.
# These resemble a cube with flexible edges.
#
# Control/Actions:
# The agent can control the 12 equilibrium edge lengths. 
#
# A wide variety of sensors are available for observation and reward:
# - 12 edge lengths
# - 12 wanted edge lengths (the last action)
# - vertexes contact with floor
# - vertexes min height (distance of closest vertex to the floor)
# - distance to origin
# - distance and angle to target
#
# Task available are:
# - GrowTask, agent has to maximize the volume of the cube
# - JumpTask, agent has to maximize the distance of the lowest mass point during the episode
# - WalkTask, agent has to maximize the distance to the starting point
# - WalkDirectionTask, agent has to minimize the distance to a target point.
# - TargetTask, like the previous task but with several target points
# 
# Requirements: pylab (for plotting only). If not available, comment the
# last 3 lines out
# Author: Frank Sehnke, sehnke@in.tum.de
#########################################################################
__author__ = "Frank Sehnke"
__version__ = '$Id$' 

from pybrain.tools.example_tools import ExTools
from pybrain.structure.modules.tanhlayer import TanhLayer
from pybrain.tools.shortcuts import buildNetwork
from pybrain.rl.environments.flexcube import FlexCubeEnvironment, WalkTask
from pybrain.rl.agents import OptimizationAgent
from pybrain.optimization import SimpleSPSA 
from pybrain.rl.experiments import EpisodicExperiment

hiddenUnits = 4
batch=2 #number of samples per learning step
prnts=1 #number of learning steps after results are printed
epis=5000000/batch/prnts #number of roleouts
numbExp=10 #number of experiments
et = ExTools(batch, prnts) #tool for printing and plotting

for runs in range(numbExp):
    # create environment
    #Options: Bool(OpenGL), Bool(Realtime simu. while client is connected), ServerIP(default:localhost), Port(default:21560)
    env = FlexCubeEnvironment()
    # create task
    task = WalkTask(env)
    # create controller network
    net = buildNetwork(len(task.getObservation()), hiddenUnits, env.actLen, outclass=TanhLayer)    
    # create agent with controller and learner (and its options)
    agent = OptimizationAgent(net, SimpleSPSA(storeAllEvaluations = True))
    et.agent = agent
     # create the experiment
    experiment = EpisodicExperiment(task, agent)

    #Do the experiment
    for updates in range(epis):
        for i in range(prnts):
            experiment.doEpisodes(batch)
        et.printResults((agent.learner._allEvaluations)[-50:-1], runs, updates)
    et.addExps()
et.showExps()
#To view what the simulation is doing at the moment, go to pybrain/rl/environments/flexcube/ and start renderer.py (python-openGL musst be installed)

########NEW FILE########
__FILENAME__ = bicycle
"""An attempt to implement Randlov and Alstrom (1998). They successfully
use reinforcement learning to balance a bicycle, and to control it to drive
to a specified goal location. Their work has been used since then by a few
researchers as a benchmark problem.

We only implement the balance task. This implementation differs at least
slightly, since Randlov and Alstrom did not mention anything about how they
annealed/decayed their learning rate, etc. As a result of differences, the
results do not match those obtained by Randlov and Alstrom.

"""

__author__ = 'Chris Dembia, Bruce Cam, Johnny Israeli'

from scipy import asarray
from numpy import sin, cos, tan, sqrt, arcsin, arctan, sign, clip, argwhere
from matplotlib import pyplot as plt

import pybrain.rl.environments
from pybrain.rl.environments.environment import Environment
from pybrain.rl.learners.valuebased.linearfa import SARSALambda_LinFA
from pybrain.rl.agents.linearfa import LinearFA_Agent
from pybrain.rl.experiments import EpisodicExperiment
from pybrain.utilities import one_to_n

class BicycleEnvironment(Environment):
    """Randlov and Alstrom's bicycle model. This code matches nearly exactly
    some c code we found online for simulating Randlov and Alstrom's
    bicycle. The bicycle travels at a fixed speed.

    """

    # For superclass.
    indim = 2
    outdim = 10

    # Environment parameters.
    time_step = 0.01

    # Goal position and radius
    # Lagouakis (2002) uses angle to goal, not heading, as a state
    max_distance = 1000.

    # Acceleration on Earth's surface due to gravity (m/s^2):
    g = 9.82

    # See the paper for a description of these quantities:
    # Distances (in meters):
    c = 0.66
    dCM = 0.30
    h = 0.94
    L = 1.11
    r = 0.34
    # Masses (in kilograms):
    Mc = 15.0
    Md = 1.7
    Mp = 60.0
    # Velocity of a bicycle (in meters per second), equal to 10 km/h:
    v = 10.0 * 1000.0 / 3600.0

    # Derived constants.
    M = Mc + Mp # See Randlov's code.
    Idc = Md * r**2
    Idv = 1.5 * Md * r**2
    Idl = 0.5 * Md * r**2
    Itot = 13.0 / 3.0 * Mc * h**2 + Mp * (h + dCM)**2
    sigmad = v / r

    def __init__(self):
        Environment.__init__(self)
        self.reset()
        self.actions = [0.0, 0.0]
        self._save_wheel_contact_trajectories = False

    def performAction(self, actions):
        self.actions = actions
        self.step()

    def saveWheelContactTrajectories(self, opt):
        self._save_wheel_contact_trajectories = opt

    def step(self):
        # Unpack the state and actions.
        # -----------------------------
        # Want to ignore the previous value of omegadd; it could only cause a
        # bug if we assign to it.

        (theta, thetad, omega, omegad, _,
                xf, yf, xb, yb, psi) = self.sensors
        (T, d) = self.actions

        # For recordkeeping.
        # ------------------
        if self._save_wheel_contact_trajectories:
            self.xfhist.append(xf)
            self.yfhist.append(yf)
            self.xbhist.append(xb)
            self.ybhist.append(yb)

        # Intermediate time-dependent quantities.
        # ---------------------------------------
        # Avoid divide-by-zero, just as Randlov did.
        if theta == 0:
            rf = 1e8
            rb = 1e8
            rCM = 1e8
        else:
            rf = self.L / np.abs(sin(theta))
            rb = self.L / np.abs(tan(theta))
            rCM = sqrt((self.L - self.c)**2 + self.L**2 / tan(theta)**2)

        phi = omega + np.arctan(d / self.h)

        # Equations of motion.
        # --------------------
        # Second derivative of angular acceleration:
        omegadd = 1 / self.Itot * (self.M * self.h * self.g * sin(phi)
                - cos(phi) * (self.Idc * self.sigmad * thetad
                    + sign(theta) * self.v**2 * (
                        self.Md * self.r * (1.0 / rf + 1.0 / rb)
                        + self.M * self.h / rCM)))
        thetadd = (T - self.Idv * self.sigmad * omegad) / self.Idl

        # Integrate equations of motion using Euler's method.
        # ---------------------------------------------------
        # yt+1 = yt + yd * dt.
        # Must update omega based on PREVIOUS value of omegad.
        omegad += omegadd * self.time_step
        omega += omegad * self.time_step
        thetad += thetadd * self.time_step
        theta += thetad * self.time_step

        # Handlebars can't be turned more than 80 degrees.
        theta = np.clip(theta, -1.3963, 1.3963)

        # Wheel ('tyre') contact positions.
        # ---------------------------------

        # Front wheel contact position.
        front_temp = self.v * self.time_step / (2 * rf)
        # See Randlov's code.
        if front_temp > 1:
            front_temp = sign(psi + theta) * 0.5 * np.pi
        else:
            front_temp = sign(psi + theta) * arcsin(front_temp)
        xf += self.v * self.time_step * -sin(psi + theta + front_temp)
        yf += self.v * self.time_step * cos(psi + theta + front_temp)

        # Rear wheel.
        back_temp = self.v * self.time_step / (2 * rb)
        # See Randlov's code.
        if back_temp > 1:
            back_temp = np.sign(psi) * 0.5 * np.pi
        else:
            back_temp = np.sign(psi) * np.arcsin(back_temp)
        xb += self.v * self.time_step * -sin(psi + back_temp)
        yb += self.v * self.time_step * cos(psi + back_temp)

        # Preventing numerical drift.
        # ---------------------------
        # Copying what Randlov did.
        current_wheelbase = sqrt((xf - xb)**2 + (yf - yb)**2)
        if np.abs(current_wheelbase - self.L) > 0.01:
            relative_error = self.L / current_wheelbase - 1.0
            xb += (xb - xf) * relative_error
            yb += (yb - yf) * relative_error

        # Update heading, psi.
        # --------------------
        delta_y = yf - yb
        if (xf == xb) and delta_y < 0.0:
            psi = np.pi
        else:
            if delta_y > 0.0:
                psi = arctan((xb - xf) / delta_y)
            else:
                psi = sign(xb - xf) * 0.5 * np.pi - arctan(delta_y / (xb - xf))

        self.sensors = np.array([theta, thetad, omega, omegad, omegadd,
                xf, yf, xb, yb, psi])

    def reset(self):
        theta = 0
        thetad = 0
        omega = 0
        omegad = 0
        omegadd = 0
        xf = 0
        yf = self.L
        xb = 0
        yb = 0

        psi = np.arctan((xb - xf) / (yf - yb))
        self.sensors = np.array([theta, thetad, omega, omegad, omegadd,
                xf, yf, xb, yb, psi])

        self.xfhist = []
        self.yfhist = []
        self.xbhist = []
        self.ybhist = []

    def getSteer(self):
        return self.sensors[0]
    def getTilt(self):
        return self.sensors[2]
    def get_xfhist(self):
        return self.xfhist
    def get_yfhist(self):
        return self.yfhist
    def get_xbhist(self):
        return self.xbhist
    def get_ybhist(self):
        return self.ybhist
    def getSensors(self):
        return self.sensors

class BalanceTask(pybrain.rl.environments.EpisodicTask):
    """The rider is to simply balance the bicycle while moving with the
    speed perscribed in the environment. This class uses a continuous 5
    dimensional state space, and a discrete state space.

    This class is heavily guided by
    pybrain.rl.environments.cartpole.balancetask.BalanceTask.

    """
    max_tilt = np.pi / 6.
    nactions = 9

    def __init__(self, max_time=1000.0):
        super(BalanceTask, self).__init__(BicycleEnvironment())
        self.max_time = max_time
        # Keep track of time in case we want to end episodes based on number of
        # time steps.
        self.t = 0

    @property
    def indim(self):
        return 1

    @property
    def outdim(self):
        return 5

    def reset(self):
        super(BalanceTask, self).reset()
        self.t = 0

    def performAction(self, action):
        """Incoming action is an int between 0 and 8. The action we provide to
        the environment consists of a torque T in {-2 N, 0, 2 N}, and a
        displacement d in {-.02 m, 0, 0.02 m}.

        """
        self.t += 1
        assert round(action[0]) == action[0]

        # -1 for action in {0, 1, 2}, 0 for action in {3, 4, 5}, 1 for
        # action in {6, 7, 8}
        torque_selector = np.floor(action[0] / 3.0) - 1.0
        T = 2 * torque_selector
        # Random number in [-1, 1]:
        p = 2.0 * np.random.rand() - 1.0
        # -1 for action in {0, 3, 6}, 0 for action in {1, 4, 7}, 1 for
        # action in {2, 5, 8}
        disp_selector = action[0] % 3 - 1.0
        d = 0.02 * disp_selector + 0.02 * p
        super(BalanceTask, self).performAction([T, d])

    def getObservation(self):
        (theta, thetad, omega, omegad, omegadd,
                xf, yf, xb, yb, psi) = self.env.getSensors()
        return self.env.getSensors()[0:5]

    def isFinished(self):
        # Criterion for ending an episode. From Randlov's paper:
        # "When the agent can balance for 1000 seconds, the task is considered
        # learned."
        if np.abs(self.env.getTilt()) > self.max_tilt:
            return True
        elapsed_time = self.env.time_step * self.t
        if elapsed_time > self.max_time:
            return True
        return False

    def getReward(self):
        # -1 reward for falling over; no reward otherwise.
        if np.abs(self.env.getTilt()) > self.max_tilt:
            return -1.0
        return 0.0

class LinearFATileCoding3456BalanceTask(BalanceTask):
    """An attempt to exactly implement Randlov's function approximation. He
    discretized (tiled) the state space into 3456 bins. We use the same action
    space as in the superclass.

    """
    # From Randlov, 1998:
    theta_bounds = np.array(
            [-0.5 * np.pi, -1.0, -0.2, 0, 0.2, 1.0, 0.5 * np.pi])
    thetad_bounds = np.array(
            [-np.inf, -2.0, 0, 2.0, np.inf])
    omega_bounds = np.array(
            [-BalanceTask.max_tilt, -0.15, -0.06, 0, 0.06, 0.15,
                BalanceTask.max_tilt])
    omegad_bounds = np.array(
            [-np.inf, -0.5, -0.25, 0, 0.25, 0.5, np.inf])
    omegadd_bounds = np.array(
            [-np.inf, -2.0, 0, 2.0, np.inf])
    # http://stackoverflow.com/questions/3257619/numpy-interconversion-between-multidimensional-and-linear-indexing
    nbins_across_dims = [
            len(theta_bounds) - 1,
            len(thetad_bounds) - 1,
            len(omega_bounds) - 1,
            len(omegad_bounds) - 1,
            len(omegadd_bounds) - 1]
    # This array, when dotted with the 5-dim state vector, gives a 'linear'
    # index between 0 and 3455.
    magic_array = np.cumprod([1] + nbins_across_dims)[:-1]

    @property
    def outdim(self):
        # Used when constructing LinearFALearner's.
        return 3456

    def getBin(self, theta, thetad, omega, omegad, omegadd):
        bin_indices = [
                np.digitize([theta], self.theta_bounds)[0] - 1,
                np.digitize([thetad], self.thetad_bounds)[0] - 1,
                np.digitize([omega], self.omega_bounds)[0] - 1,
                np.digitize([omegad], self.omegad_bounds)[0] - 1,
                np.digitize([omegadd], self.omegadd_bounds)[0] - 1,
                ]
        return np.dot(self.magic_array, bin_indices)

    def getBinIndices(self, linear_index):
        """Given a linear index (integer between 0 and outdim), returns the bin
        indices for each of the state dimensions.

        """
        return linear_index / self.magic_array % self.nbins_across_dims

    def getObservation(self):
        (theta, thetad, omega, omegad, omegadd,
                xf, yf, xb, yb, psi) = self.env.getSensors()
        state = one_to_n(self.getBin(theta, thetad, omega, omegad, omegadd),
                self.outdim)
        return state

class SARSALambda_LinFA_ReplacingTraces(SARSALambda_LinFA):
    """Randlov used replacing traces, but this doesn't exist in PyBrain's
    SARSALambda.

    """
    def _updateEtraces(self, state, action, responsibility=1.):
        self._etraces *= self.rewardDiscount * self._lambda * responsibility
        # This assumes that state is an identity vector (like, from one_to_n).
        self._etraces[action] = clip(self._etraces[action] + state, -np.inf, 1.)
        # Set the trace for all other actions in this state to 0:
        action_bit = one_to_n(action, self.num_actions)

        for argstate in argwhere(state == 1) :
        	self._etraces[argwhere(action_bit != 1), argstate] = 0.


task = LinearFATileCoding3456BalanceTask()
env = task.env

# The learning is very sensitive to the learning rate decay.
learner = SARSALambda_LinFA_ReplacingTraces(task.nactions, task.outdim,
        learningRateDecay=2000)
learner._lambda = 0.95

task.discount = learner.rewardDiscount

agent = LinearFA_Agent(learner)
agent.logging = False

exp = EpisodicExperiment(task, agent)

performance_agent = LinearFA_Agent(learner)
performance_agent.logging = False
performance_agent.greedy = True
performance_agent.learning = False

env.saveWheelContactTrajectories(True)
plt.ion()
plt.figure(figsize=(8, 4))

ax1 = plt.subplot(1, 2, 1)
ax2 = plt.subplot(1, 2, 2)

def update_wheel_trajectories():
    front_lines = ax2.plot(env.get_xfhist(), env.get_yfhist(), 'r')
    back_lines = ax2.plot(env.get_xbhist(), env.get_ybhist(), 'b')
    plt.axis('equal')

perform_cumrewards = []
for irehearsal in range(7000):

    # Learn.
    # ------
    r = exp.doEpisodes(1)
    # Discounted reward.
    cumreward = exp.task.getTotalReward()
    #print 'cumreward: %.4f; nsteps: %i; learningRate: %.4f' % (
    #        cumreward, len(r[0]), exp.agent.learner.learningRate)

    if irehearsal % 50 == 0:
        # Perform (no learning).
        # ----------------------
        # Swap out the agent.
        exp.agent = performance_agent
    
        # Perform.
        r = exp.doEpisodes(1)
        perform_cumreward = task.getTotalReward()
        perform_cumrewards.append(perform_cumreward)
        print 'PERFORMANCE: cumreward:', perform_cumreward, 'nsteps:', len(r[0])
    
        # Swap back the learning agent.
        performance_agent.reset()
        exp.agent = agent
    
        ax1.cla()
        ax1.plot(perform_cumrewards, '.--')
        # Wheel trajectories.
        update_wheel_trajectories()
    
        plt.pause(0.001)

########NEW FILE########
__FILENAME__ = xor
""" Toy example for RL with linear function approximation. 
This illustrates how a 'AND'-state-space can be solved, but not 
an 'XOR' space. 
"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.rl.learners.valuebased.linearfa import Q_LinFA
from pybrain.rl.environments.classic.xor import XORTask
from pybrain.rl.experiments.experiment import Experiment
from pybrain.rl.agents.learning import LearningAgent
from random import random, randint

class LinFA_QAgent(LearningAgent):
    """ Customization of the Agent class for linear function approximation learners. """

    epsilon = 0.1
    logging = False
    
    def __init__(self, learner):
        self.learner = learner
        self.previousobs = None
        
    def getAction(self):
        if random() < self.epsilon:
            a = randint(0, self.learner.num_actions-1)
        else:
            a = self.learner._greedyAction(self.lastobs)  
        self.lastaction = a
        return a
    
    def giveReward(self, r):
        LearningAgent.giveReward(self, r)
        if self.previousobs is not None:
            #print  self.previousobs, a, self.lastreward, self.lastobs
            self.learner._updateWeights(self.previousobs, self.previousaction, self.previousreward, self.lastobs)
        self.previousobs = self.lastobs
        self.previousaction = self.lastaction
        self.previousreward = self.lastreward
        
    

def runExp(gamma=0, epsilon=0.1, xor=False, lr = 0.02):    
    if xor: 
        print "Attempting the XOR task"
    else:
        print "Attempting the AND task"
        
    task = XORTask()
    task.and_task = not xor
    
    l = Q_LinFA(task.nactions, task.nsenses)
    l.rewardDiscount = gamma
    l.learningRate = lr

    agent = LinFA_QAgent(l)
    agent.epsilon = epsilon
    exp = Experiment(task, agent)    
            
    sofar = 0
    for i in range(30):
        exp.doInteractions(100)
        print exp.task.cumreward - sofar,
        if i%10 == 9: 
            print                
        sofar = exp.task.cumreward          
        l._decayLearningRate()


if __name__ == "__main__":
    runExp(xor=False)
    print 
    runExp(xor=True)
    print 
    runExp(xor=True)
########NEW FILE########
__FILENAME__ = td
#!/usr/bin/env python
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

""" This example demonstrates how to use the discrete Temporal Difference
Reinforcement Learning algorithms (SARSA, Q, Q(lambda)) in a classical
fully observable MDP maze task. The goal point is the top right free
field. """

from scipy import *
import sys, time
import pylab

from pybrain.rl.environments.mazes import Maze, MDPMazeTask
from pybrain.rl.learners.valuebased import ActionValueTable
from pybrain.rl.agents import LearningAgent
from pybrain.rl.learners import Q, QLambda, SARSA #@UnusedImport
from pybrain.rl.explorers import BoltzmannExplorer #@UnusedImport
from pybrain.rl.experiments import Experiment
from pybrain.rl.environments import Task


# create the maze with walls (1)
envmatrix = array([[1, 1, 1, 1, 1, 1, 1, 1, 1],
                   [1, 0, 0, 1, 0, 0, 0, 0, 1],
                   [1, 0, 0, 1, 0, 0, 1, 0, 1],
                   [1, 0, 0, 1, 0, 0, 1, 0, 1],
                   [1, 0, 0, 1, 0, 1, 1, 0, 1],
                   [1, 0, 0, 0, 0, 0, 1, 0, 1],
                   [1, 1, 1, 1, 1, 1, 1, 0, 1],
                   [1, 0, 0, 0, 0, 0, 0, 0, 1],
                   [1, 1, 1, 1, 1, 1, 1, 1, 1]])

env = Maze(envmatrix, (7, 7))

# create task
task = MDPMazeTask(env)

# create value table and initialize with ones
table = ActionValueTable(81, 4)
table.initialize(1.)

# create agent with controller and learner - use SARSA(), Q() or QLambda() here
learner = SARSA()

# standard exploration is e-greedy, but a different type can be chosen as well
# learner.explorer = BoltzmannExplorer()

# create agent
agent = LearningAgent(table, learner)

# create experiment
experiment = Experiment(task, agent)

# prepare plotting
pylab.gray()
pylab.ion()

for i in range(1000):

    # interact with the environment (here in batch mode)
    experiment.doInteractions(100)
    agent.learn()
    agent.reset()

    # and draw the table
    pylab.pcolor(table.params.reshape(81,4).max(1).reshape(9,9))
    pylab.draw()

########NEW FILE########
__FILENAME__ = acrobot_fd
#!/usr/bin/env python
#########################################################################
# Reinforcement Learning with PGPE on the Acrobot Environment 
#
# The Acrobot Environment is a 1 DoF system.
# The goal is to swing up the pole and balance it.
# The motor is underpowered so that the pole can not go directly to the upright position.
# It has to swing several times to gain enough momentum.
#
# Control/Actions:
# The agent can control 1 joint. 
#
# Requirements: pylab (for plotting only). If not available, comment the
# last 3 lines out
# Author: Thomas Rueckstiess, rueckst@in.tum.de
#########################################################################
__author__ = "Thomas Rueckstiess, Frank Sehnke"
__version__ = '$Id$' 

from pybrain.tools.example_tools import ExTools
from pybrain.rl.environments.ode import AcrobotEnvironment
from pybrain.rl.environments.ode.tasks import GradualRewardTask
from pybrain.structure.modules.tanhlayer import TanhLayer
from pybrain.tools.shortcuts import buildNetwork
from pybrain.rl.agents import OptimizationAgent
from pybrain.optimization import FiniteDifferences
from pybrain.rl.experiments import EpisodicExperiment

batch=2 #number of samples per learning step
prnts=1 #number of learning steps after results are printed
epis=4000/batch/prnts #number of roleouts
numbExp=10 #number of experiments
et = ExTools(batch, prnts) #tool for printing and plotting

for runs in range(numbExp):
    # create environment
    #Options: Bool(OpenGL), Bool(Realtime simu. while client is connected), ServerIP(default:localhost), Port(default:21560)
    env = AcrobotEnvironment() 
    # create task
    task = GradualRewardTask(env)
    # create controller network
    net = buildNetwork(len(task.getObservation()), env.actLen, outclass=TanhLayer)    
    # create agent with controller and learner (and its options)
    agent = OptimizationAgent(net, FiniteDifferences(storeAllEvaluations = True))
    et.agent = agent
     # create the experiment
    experiment = EpisodicExperiment(task, agent)

    #Do the experiment
    for updates in range(epis):
        for i in range(prnts):
            experiment.doEpisodes(batch)
        et.printResults((agent.learner._allEvaluations)[-50:-1], runs, updates)
    et.addExps()
et.showExps()
#To view what the simulation is doing at the moment, go to pybrain/rl/environments/ode/ and start viewer.py (python-openGL musst be installed, see PyBrain documentation)

########NEW FILE########
__FILENAME__ = acrobot_pgpe
#!/usr/bin/env python
#########################################################################
# Reinforcement Learning with PGPE on the Acrobot Environment 
#
# The Acrobot Environment is a 1 DoF system.
# The goal is to swing up the pole and balance it.
# The motor is underpowered so that the pole can not go directly to the upright position.
# It has to swing several times to gain enough momentum.
#
# Control/Actions:
# The agent can control 1 joint. 
#
# Requirements: pylab (for plotting only). If not available, comment the
# last 3 lines out
# Author: Frank Sehnke, sehnke@in.tum.de
#########################################################################
__author__ = "Frank Sehnke"
__version__ = '$Id$' 

from pybrain.tools.example_tools import ExTools
from pybrain.rl.environments.ode import AcrobotEnvironment
from pybrain.rl.environments.ode.tasks import GradualRewardTask
from pybrain.structure.modules.tanhlayer import TanhLayer
from pybrain.tools.shortcuts import buildNetwork
from pybrain.rl.agents import OptimizationAgent
from pybrain.optimization import PGPE
from pybrain.rl.experiments import EpisodicExperiment
from time import sleep

batch=1 #number of samples per learning step
prnts=1 #number of learning steps after results are printed
epis=4000/batch/prnts #number of roleouts
numbExp=40 #number of experiments
et = ExTools(batch, prnts) #tool for printing and plotting

env = None
for runs in range(numbExp):
    # create environment
    #Options: Bool(OpenGL), Bool(Realtime simu. while client is connected), ServerIP(default:localhost), Port(default:21560)
    if env != None: env.closeSocket()
    env = AcrobotEnvironment()
    # create task
    task = GradualRewardTask(env)
    # create controller network
    net = buildNetwork(len(task.getObservation()), env.actLen, outclass=TanhLayer)    
    # create agent with controller and learner (and its options)
    agent = OptimizationAgent(net, PGPE(storeAllEvaluations = True,
                                    learningRate = 0.05,
                                    sigmaLearningRate = 0.025,
                                    momentum = 0.0,
                                    epsilon = 6.0,
                                    rprop = False,))
    et.agent = agent
    #agent.learner.bestEvaluation = 1500
    # create the experiment
    experiment = EpisodicExperiment(task, agent)

    #Do the experiment
    for updates in range(epis):
        for i in range(prnts):
            experiment.doEpisodes(batch)
        et.printResults((agent.learner._allEvaluations)[-50:-1], runs, updates)
    et.addExps()
et.showExps()
#To view what the simulation is doing at the moment, go to pybrain/rl/environments/ode/ and start viewer.py (python-openGL musst be installed, see PyBrain documentation)

########NEW FILE########
__FILENAME__ = ccrl_glass_pgpe
#!/usr/bin/env python
#########################################################################
# Reinforcement Learning with PGPE on the CCRL ODE Environment 
#
# The CCRL robot is a body structure with 2x 7 DoF Arms.
# Complex grasping tasks can be learned with this environment.
#
# Control/Actions:
# The agent can control all 14 DOF of the robot arms plus the 2 hands. 
#
# A wide variety of sensors are available for observation and reward:
# - 16 angles of joints
# - 16 angle velocitys of joints
# - Number of hand parts that have contact to target object
# - collision with table
# - distance of hand to target
# - angle of hand to horizontal and vertical plane
#
# Task available are:
# - Grasp Task, agent has to get hold of the object with avoiding collision with table
# 
# Requirements: pylab (for plotting only). If not available, comment the
# last 3 lines out
# Author: Frank Sehnke, sehnke@in.tum.de
#########################################################################
__author__ = "Frank Sehnke"
__version__ = '$Id$' 

from pybrain.tools.example_tools import ExTools
from pybrain.rl.environments.ode import CCRLEnvironment
from pybrain.rl.environments.ode.tasks import CCRLGlasTask
from pybrain.structure.modules.tanhlayer import TanhLayer
from pybrain.tools.shortcuts import buildNetwork
from pybrain.rl.agents import OptimizationAgent
from pybrain.optimization import PGPE
from pybrain.rl.experiments import EpisodicExperiment

hiddenUnits = 4
batch=1 #number of samples per learning step
prnts=1 #number of learning steps after results are printed
epis=2000/batch/prnts #number of roleouts
numbExp=10 #number of experiments
et = ExTools(batch, prnts) #tool for printing and plotting

env = None
for runs in range(numbExp):
    # create environment
    #Options: XML-Model, Bool(OpenGL), Bool(Realtime simu. while client is connected), ServerIP(default:localhost), Port(default:21560)
    if env != None: env.closeSocket()
    env = CCRLEnvironment()
    # create task
    task = CCRLGlasTask(env)
    # create controller network
    net = buildNetwork(len(task.getObservation()), hiddenUnits, env.actLen, outclass=TanhLayer) #, hiddenUnits    
    # create agent with controller and learner (and its options)
    agent = OptimizationAgent(net, PGPE(storeAllEvaluations = True))
    et.agent = agent
    # create the experiment
    experiment = EpisodicExperiment(task, agent)

    #Do the experiment
    for updates in range(epis):
        for i in range(prnts):
            experiment.doEpisodes(batch)
        et.printResults((agent.learner._allEvaluations)[-50:-1], runs, updates)
    et.addExps()
et.showExps()
#To view what the simulation is doing at the moment, go to pybrain/rl/environments/ode/ and start viewer.py (python-openGL musst be installed, see PyBrain documentation)

########NEW FILE########
__FILENAME__ = ccrl_plate_pgpe
#!/usr/bin/env python
#########################################################################
# Reinforcement Learning with PGPE on the CCRL ODE Environment 
#
# The CCRL robot is a body structure with 2x 7 DoF Arms.
# Complex grasping tasks can be learned with this environment.
#
# Control/Actions:
# The agent can control all 14 DOF of the robot arms plus the 2 hands. 
#
# A wide variety of sensors are available for observation and reward:
# - 16 angles of joints
# - 16 angle velocitys of joints
# - Number of hand parts that have contact to target object
# - collision with table
# - distance of hand to target
# - angle of hand to horizontal and vertical plane
#
# Task available are:
# - Grasp Task, agent has to get hold of the object with avoiding collision with table
# 
# Requirements: pylab (for plotting only). If not available, comment the
# last 3 lines out
# Author: Frank Sehnke, sehnke@in.tum.de
#########################################################################
__author__ = "Frank Sehnke"
__version__ = '$Id$' 

from pybrain.tools.example_tools import ExTools
from pybrain.rl.environments.ode import CCRLEnvironment
from pybrain.rl.environments.ode.tasks import CCRLPlateTask
from pybrain.structure.modules.tanhlayer import TanhLayer
from pybrain.tools.shortcuts import buildNetwork
from pybrain.rl.agents import OptimizationAgent
from pybrain.optimization import PGPE
from pybrain.rl.experiments import EpisodicExperiment

hiddenUnits = 10
batch=1 #number of samples per learning step
prnts=1 #number of learning steps after results are printed
epis=5000000/batch/prnts #number of roleouts
numbExp=10 #number of experiments
et = ExTools(batch, prnts) #tool for printing and plotting

env = None
for runs in range(numbExp):
    # create environment
    #Options: XML-Model, Bool(OpenGL), Bool(Realtime simu. while client is connected), ServerIP(default:localhost), Port(default:21560)
    if env != None: env.closeSocket()
    env = CCRLEnvironment("ccrlPlate.xode")
    # create task
    task = CCRLPlateTask(env)
    # create controller network
    net = buildNetwork(len(task.getObservation()), hiddenUnits, env.actLen, outclass=TanhLayer)    
    # create agent with controller and learner (and its options)
    agent = OptimizationAgent(net, PGPE(storeAllEvaluations = True))
    et.agent = agent
    # create the experiment
    experiment = EpisodicExperiment(task, agent)

    #Do the experiment
    for updates in range(epis):
        for i in range(prnts):
            experiment.doEpisodes(batch)
        et.printResults((agent.learner._allEvaluations)[-50:-1], runs, updates)
    et.addExps()
et.showExps()
#To view what the simulation is doing at the moment, go to pybrain/rl/environments/ode/ and start viewer.py (python-openGL musst be installed, see PyBrain documentation)

########NEW FILE########
__FILENAME__ = johnnie_pgpe
#!/usr/bin/env python
#########################################################################
# Reinforcement Learning with PGPE on the Johnnie Environment 
#
# The Johnnie robot is a body structure with 11 DoF .
# Complex balancing tasks can be learned with this environment.
#
# Control/Actions:
# The agent can control all 11 DOF of the robot. 
#
# A wide variety of sensors are available for observation and reward:
# - 11 angles of joints
# - 11 angle velocitys of joints
# - Number of foot parts that have contact to floor
# - Height sensor in head for reward calculation
# - Rotation sensor in 3 dimesnions
#
# Task available are:
# - StandTask, agent has not to fall by himself
# - Robust standing Task, agent has not to fall even then hit by reasonable random forces
# - JumpTask, agent has to maximize the head-vertical position during the episode
# 
# Requirements: pylab (for plotting only). If not available, comment the
# last 3 lines out
# Author: Frank Sehnke, sehnke@in.tum.de
#########################################################################
__author__ = "Frank Sehnke"
__version__ = '$Id$' 

from pybrain.tools.example_tools import ExTools
from pybrain.rl.environments.ode import JohnnieEnvironment
from pybrain.rl.environments.ode.tasks import StandingTask
from pybrain.structure.modules.tanhlayer import TanhLayer
from pybrain.tools.shortcuts import buildNetwork
from pybrain.rl.agents import OptimizationAgent
from pybrain.optimization import PGPE
from pybrain.rl.experiments import EpisodicExperiment

hiddenUnits = 4
batch=1 #number of samples per learning step
prnts=1 #number of learning steps after results are printed
epis=5000000/batch/prnts #number of roleouts
numbExp=10 #number of experiments
et = ExTools(batch, prnts) #tool for printing and plotting

env = None
for runs in range(numbExp):
    # create environment
    #Options: Bool(OpenGL), Bool(Realtime simu. while client is connected), ServerIP(default:localhost), Port(default:21560)
    if env != None: env.closeSocket()
    env = JohnnieEnvironment() 
    # create task
    task = StandingTask(env)
    # create controller network
    net = buildNetwork(len(task.getObservation()), hiddenUnits, env.actLen, outclass=TanhLayer)    
    # create agent with controller and learner (and its options)
    agent = OptimizationAgent(net, PGPE(storeAllEvaluations = True))
    et.agent = agent
    # create the experiment
    experiment = EpisodicExperiment(task, agent)

    #Do the experiment
    for updates in range(epis):
        for i in range(prnts):
            experiment.doEpisodes(batch)
        et.printResults((agent.learner._allEvaluations)[-50:-1], runs, updates)
    et.addExps()
et.showExps()
#To view what the simulation is doing at the moment, go to pybrain/rl/environments/ode/ and start viewer.py (python-openGL musst be installed, see PyBrain documentation)

########NEW FILE########
__FILENAME__ = johnnie_reinforce
#!/usr/bin/env python
#########################################################################
# Reinforcement Learning with PGPE on the Johnnie Environment 
#
# The Johnnie robot is a body structure with 11 DoF .
# Complex balancing tasks can be learned with this environment.
#
# Control/Actions:
# The agent can control all 11 DOF of the robot. 
#
# A wide variety of sensors are available for observation and reward:
# - 11 angles of joints
# - 11 angle velocitys of joints
# - Number of foot parts that have contact to floor
# - Height sensor in head for reward calculation
# - Rotation sensor in 3 dimesnions
#
# Task available are:
# - StandTask, agent has not to fall by himself
# - Robust standing Task, agent has not to fall even then hit by reasonable random forces
# - JumpTask, agent has to maximize the head-vertical position during the episode
# 
# Requirements: pylab (for plotting only). If not available, comment the
# last 3 lines out
# Author: Frank Sehnke, sehnke@in.tum.de
#########################################################################
__author__ = "Frank Sehnke"
__version__ = '$Id$' 

from pybrain.tools.example_tools import ExTools
from pybrain.rl.environments.ode import JohnnieEnvironment
from pybrain.rl.environments.ode.tasks import StandingTask
from pybrain.structure.modules.tanhlayer import TanhLayer
from pybrain.tools.shortcuts import buildNetwork
from pybrain.rl.agents import LearningAgent
from pybrain.rl.learners import Reinforce
from pybrain.rl.experiments import EpisodicExperiment

hiddenUnits = 4
batch=2 #number of samples per learning step
prnts=1 #number of learning steps after results are printed
epis=5000000/batch/prnts #number of roleouts
numbExp=10 #number of experiments
et = ExTools(batch, prnts, kind = "learner")#tool for printing and plotting

for runs in range(numbExp):
    # create environment
    #Options: Bool(OpenGL), Bool(Realtime simu. while client is connected), ServerIP(default:localhost), Port(default:21560)
    env = JohnnieEnvironment() 
    # create task
    task = StandingTask(env)
    # create controller network
    net = buildNetwork(len(task.getObservation()), hiddenUnits, env.actLen, outclass=TanhLayer)    
    # create agent with controller and learner (and its options)
    agent = LearningAgent(net, Reinforce())
    et.agent = agent
    # create the experiment
    experiment = EpisodicExperiment(task, agent)

    #Do the experiment
    for updates in range(epis):
        for i in range(prnts):
            experiment.doEpisodes(batch)
        state, action, reward = agent.learner.dataset.getSequence(agent.learner.dataset.getNumSequences()-1)
        et.printResults(reward.sum(), runs, updates)
    et.addExps()
et.showExps()
#To view what the simulation is doing at the moment, go to pybrain/rl/environments/ode/ and start viewer.py (python-openGL musst be installed, see PyBrain documentation)

########NEW FILE########
__FILENAME__ = shipbench_pgpe
#!/usr/bin/env python
#########################################################################
# Reinforcement Learning with PGPE on the ShipSteering Environment
#
# Requirements: pylab (for plotting only). If not available, comment the
# last 3 lines out
# Author: Frank Sehnke, sehnke@in.tum.de
#########################################################################
__author__ = "Martin Felder, Frank Sehnke"
__version__ = '$Id$' 

from pybrain.tools.example_tools import ExTools
from pybrain.structure.modules.tanhlayer import TanhLayer
from pybrain.tools.shortcuts import buildNetwork
from pybrain.rl.environments.shipsteer import ShipSteeringEnvironment
from pybrain.rl.environments.shipsteer import GoNorthwardTask
from pybrain.rl.agents import OptimizationAgent
from pybrain.optimization import PGPE 
from pybrain.rl.experiments import EpisodicExperiment

batch=1 #number of samples per learning step
prnts=50 #number of learning steps after results are printed
epis=2000/batch/prnts #number of roleouts
numbExp=10 #number of experiments
et = ExTools(batch, prnts) #tool for printing and plotting

env = None
for runs in range(numbExp):
    # create environment
    #Options: Bool(OpenGL), Bool(Realtime simu. while client is connected), ServerIP(default:localhost), Port(default:21560)
    if env != None: env.closeSocket()
    env = ShipSteeringEnvironment()
    # create task
    task = GoNorthwardTask(env,maxsteps = 500)
    # create controller network
    net = buildNetwork(task.outdim, task.indim, outclass=TanhLayer)
    # create agent with controller and learner (and its options)
    agent = OptimizationAgent(net, PGPE(learningRate = 0.3,
                                    sigmaLearningRate = 0.15,
                                    momentum = 0.0,
                                    epsilon = 2.0,
                                    rprop = False,
                                    storeAllEvaluations = True))
    et.agent = agent
    #create experiment
    experiment = EpisodicExperiment(task, agent)

    #Do the experiment
    for updates in range(epis):
        for i in range(prnts):
            experiment.doEpisodes(batch)
        et.printResults((agent.learner._allEvaluations)[-50:-1], runs, updates)
    et.addExps()
et.showExps()
#To view what the simulation is doing at the moment set the environment with True, go to pybrain/rl/environments/ode/ and start viewer.py (python-openGL musst be installed, see PyBrain documentation)

########NEW FILE########
__FILENAME__ = shipbench_sde
#!/usr/bin/env python
#########################################################################
# Reinforcement Learning with SPE on the ShipSteering Environment
#
# Requirements:
#   pybrain (tested on rev. 1195, ship env rev. 1202)
# Synopsis:
#   shipbenchm.py [<True|False> [logfile]]
#       (first argument is graphics flag)
#########################################################################
__author__ = "Martin Felder, Thomas Rueckstiess"
__version__ = '$Id$'

#---
# default backend GtkAgg does not plot properly on Ubuntu 8.04
import matplotlib
matplotlib.use('TkAgg')
#---

from pybrain.rl.environments.shipsteer import ShipSteeringEnvironment
from pybrain.rl.environments.shipsteer import GoNorthwardTask
from pybrain.rl.agents import LearningAgent
from pybrain.rl.learners.directsearch.enac import ENAC
from pybrain.rl.experiments.episodic import EpisodicExperiment

from pybrain.tools.shortcuts import buildNetwork
from pybrain.tools.plotting import MultilinePlotter

from pylab import figure, ion
from scipy import mean
import sys

if len(sys.argv) > 1:
    useGraphics = eval(sys.argv[1])
else:
    useGraphics = False

# create task
env=ShipSteeringEnvironment()
maxsteps = 500
task = GoNorthwardTask(env=env, maxsteps = maxsteps)
# task.env.setRenderer( CartPoleRenderer())

# create controller network
#net = buildNetwork(task.outdim, 7, task.indim, bias=True, outputbias=False)
net = buildNetwork(task.outdim, task.indim, bias=False)
#net.initParams(0.0)


# create agent
learner = ENAC()
learner.gd.rprop = True
# only relevant for RP
learner.gd.deltamin = 0.0001
#agent.learner.gd.deltanull = 0.05
# only relevant for BP
learner.gd.alpha = 0.01
learner.gd.momentum = 0.9

agent = LearningAgent(net, learner)
agent.actaspg = False

# create experiment
experiment = EpisodicExperiment(task, agent)

# print weights at beginning
print agent.module.params

rewards = []
if useGraphics:
    figure()
    ion()
    pl = MultilinePlotter(autoscale=1.2, xlim=[0, 50], ylim=[0, 1])
    pl.setLineStyle(linewidth=2)

# queued version
# experiment._fillQueue(30)
# while True:
#     experiment._stepQueueLoop()
#     # rewards.append(mean(agent.history.getSumOverSequences('reward')))
#     print agent.module.getParameters(),
#     print mean(agent.history.getSumOverSequences('reward'))
#     clf()
#     plot(rewards)

# episodic version
x = 0
batch = 30 #number of samples per gradient estimate (was: 20; more here due to stochastic setting)
while x<5000:
#while True:
    experiment.doEpisodes(batch)
    x += batch
    reward = mean(agent.history.getSumOverSequences('reward'))*task.rewardscale
    if useGraphics:
        pl.addData(0,x,reward)
    print agent.module.params
    print reward
    #if reward > 3:
    #    pass
    agent.learn()
    agent.reset()
    if useGraphics:
        pl.update()


if len(sys.argv) > 2:
    agent.history.saveToFile(sys.argv[1], protocol=-1, arraysonly=True)
if useGraphics:
    pl.show( popup = True)

#To view what the simulation is doing at the moment set the environment with True, go to pybrain/rl/environments/ode/ and start viewer.py (python-openGL musst be installed, see PyBrain documentation)

## performance:
## experiment.doEpisodes(5) * 100 without weave:
##    real    2m39.683s
##    user    2m33.358s
##    sys     0m5.960s
## experiment.doEpisodes(5) * 100 with weave:
##real    2m41.275s
##user    2m35.310s
##sys     0m5.192s
##

########NEW FILE########
__FILENAME__ = nfq
#!/usr/bin/env python
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'


from pybrain.rl.environments.cartpole import CartPoleEnvironment, DiscreteBalanceTask, CartPoleRenderer
from pybrain.rl.agents import LearningAgent
from pybrain.rl.experiments import EpisodicExperiment
from pybrain.rl.learners.valuebased import NFQ, ActionValueNetwork
from pybrain.rl.explorers import BoltzmannExplorer

from numpy import array, arange, meshgrid, pi, zeros, mean
from matplotlib import pyplot as plt

# switch this to True if you want to see the cart balancing the pole (slower)
render = False

plt.ion()

env = CartPoleEnvironment()
if render:
    renderer = CartPoleRenderer()
    env.setRenderer(renderer)
    renderer.start()

module = ActionValueNetwork(4, 3)

task = DiscreteBalanceTask(env, 100)
learner = NFQ()
learner.explorer.epsilon = 0.4

agent = LearningAgent(module, learner)
testagent = LearningAgent(module, None)
experiment = EpisodicExperiment(task, agent)

def plotPerformance(values, fig):
    plt.figure(fig.number)
    plt.clf()
    plt.plot(values, 'o-')
    plt.gcf().canvas.draw()
    # Without the next line, the pyplot plot won't actually show up.
    plt.pause(0.001)

performance = []

if not render:
    pf_fig = plt.figure()

while(True):
	# one learning step after one episode of world-interaction
    experiment.doEpisodes(1)
    agent.learn(1)

    # test performance (these real-world experiences are not used for training)
    if render:
        env.delay = True
    experiment.agent = testagent
    r = mean([sum(x) for x in experiment.doEpisodes(5)])
    env.delay = False
    testagent.reset()
    experiment.agent = agent

    performance.append(r)
    if not render:
        plotPerformance(performance, pf_fig)

    print "reward avg", r
    print "explorer epsilon", learner.explorer.epsilon
    print "num episodes", agent.history.getNumSequences()
    print "update step", len(performance)


########NEW FILE########
__FILENAME__ = td
#!/usr/bin/env python
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

""" This example demonstrates how to use the discrete Temporal Difference
Reinforcement Learning algorithms (SARSA, Q, Q(lambda)) in a classical
fully observable MDP maze task. The goal point is the top right free
field. """

from scipy import * #@UnusedWildImport
import pylab

from pybrain.rl.environments.mazes import Maze, MDPMazeTask
from pybrain.rl.learners.valuebased import ActionValueTable
from pybrain.rl.agents import LearningAgent
from pybrain.rl.learners import Q, QLambda, SARSA #@UnusedImport
from pybrain.rl.explorers import BoltzmannExplorer #@UnusedImport
from pybrain.rl.experiments import Experiment


# create the maze with walls (1)
envmatrix = array([[1, 1, 1, 1, 1, 1, 1, 1, 1],
                   [1, 0, 0, 1, 0, 0, 0, 0, 1],
                   [1, 0, 0, 1, 0, 0, 1, 0, 1],
                   [1, 0, 0, 1, 0, 0, 1, 0, 1],
                   [1, 0, 0, 1, 0, 1, 1, 0, 1],
                   [1, 0, 0, 0, 0, 0, 1, 0, 1],
                   [1, 1, 1, 1, 1, 1, 1, 0, 1],
                   [1, 0, 0, 0, 0, 0, 0, 0, 1],
                   [1, 1, 1, 1, 1, 1, 1, 1, 1]])

env = Maze(envmatrix, (7, 7))

# create task
task = MDPMazeTask(env)

# create value table and initialize with ones
table = ActionValueTable(81, 4)
table.initialize(1.)

# create agent with controller and learner - use SARSA(), Q() or QLambda() here
learner = SARSA()

# standard exploration is e-greedy, but a different type can be chosen as well
# learner.explorer = BoltzmannExplorer()

# create agent
agent = LearningAgent(table, learner)

# create experiment
experiment = Experiment(task, agent)

# prepare plotting
pylab.gray()
pylab.ion()

for i in range(1000):

    # interact with the environment (here in batch mode)
    experiment.doInteractions(100)
    agent.learn()
    agent.reset()

    # and draw the table
    pylab.pcolor(table.params.reshape(81,4).max(1).reshape(9,9))
    pylab.draw()

########NEW FILE########
__FILENAME__ = backpropanbncn
#!/usr/bin/env python
# A simple recurrent neural network that learns a simple sequential data set.

__author__ = 'Tom Schaul, tom@idsia.ch and Daan Wierstra'

from datasets import AnBnCnDataSet #@UnresolvedImport
from pybrain.supervised import BackpropTrainer
from pybrain.structure import FullConnection, RecurrentNetwork, TanhLayer, LinearLayer, BiasUnit


def testTraining():
    # the AnBnCn dataset (sequential)
    d = AnBnCnDataSet()

    # build a recurrent network to be trained
    hsize = 2
    n = RecurrentNetwork()
    n.addModule(TanhLayer(hsize, name = 'h'))
    n.addModule(BiasUnit(name = 'bias'))
    n.addOutputModule(LinearLayer(1, name = 'out'))
    n.addConnection(FullConnection(n['bias'], n['h']))
    n.addConnection(FullConnection(n['h'], n['out']))
    n.addRecurrentConnection(FullConnection(n['h'], n['h']))
    n.sortModules()

    # initialize the backprop trainer and train
    t = BackpropTrainer(n, learningrate = 0.1, momentum = 0.0, verbose = True)
    t.trainOnDataset(d, 200)

    # the resulting weights are in the network:
    print 'Final weights:', n.params

if __name__ == '__main__':
    testTraining()
########NEW FILE########
__FILENAME__ = backpropxor
#!/usr/bin/env python
# A simple feedforward neural network that learns XOR.

__author__ = 'Tom Schaul, tom@idsia.ch'

from datasets import SequentialXORDataSet #@UnresolvedImport
from pybrain.tools.shortcuts import buildNetwork
from pybrain.supervised import BackpropTrainer


def testTraining():
    d = SequentialXORDataSet()
    n = buildNetwork(d.indim, 4, d.outdim, recurrent=True)
    t = BackpropTrainer(n, learningrate = 0.01, momentum = 0.99, verbose = True)
    t.trainOnDataset(d, 1000)
    t.testOnData(verbose= True)


if __name__ == '__main__':
    testTraining()
########NEW FILE########
__FILENAME__ = anbncn
#!/usr/bin/env python
__author__ = 'Tom Schaul, tom@idsia.ch and Daan Wierstra'

from pybrain.datasets import SequentialDataSet

# TODO: make it *real* AnBnCn

class AnBnCnDataSet(SequentialDataSet):
    """ A Dataset partially modeling an AnBnCn grammar. """

    def __init__(self):
        SequentialDataSet.__init__(self, 0, 1)

        self.newSequence()
        self.addSample([],[0])
        self.addSample([],[1])
        self.addSample([],[0])
        self.addSample([],[1])
        self.addSample([],[0])
        self.addSample([],[1])

        self.newSequence()
        self.addSample([],[0])
        self.addSample([],[1])
        self.addSample([],[0])
        self.addSample([],[1])
        self.addSample([],[0])
        self.addSample([],[1])

########NEW FILE########
__FILENAME__ = parity
#!/usr/bin/env python
__author__ = 'Tom Schaul (tom@idsia.ch)'

from pybrain.datasets import SequentialDataSet

class ParityDataSet(SequentialDataSet):
    """ Determine whether the bitstring up to the current point conains a pair number of 1s or not."""
    def __init__(self):
        SequentialDataSet.__init__(self, 1,1)

        self.newSequence()
        self.addSample([-1], [-1])
        self.addSample([1], [1])
        self.addSample([1], [-1])

        self.newSequence()
        self.addSample([1], [1])
        self.addSample([1], [-1])

        self.newSequence()
        self.addSample([1], [1])
        self.addSample([1], [-1])
        self.addSample([1], [1])
        self.addSample([1], [-1])
        self.addSample([1], [1])
        self.addSample([1], [-1])
        self.addSample([1], [1])
        self.addSample([1], [-1])
        self.addSample([1], [1])
        self.addSample([1], [-1])

        self.newSequence()
        self.addSample([1], [1])
        self.addSample([1], [-1])
        self.addSample([-1], [-1])
        self.addSample([-1], [-1])
        self.addSample([-1], [-1])
        self.addSample([-1], [-1])
        self.addSample([-1], [-1])
        self.addSample([-1], [-1])
        self.addSample([-1], [-1])
        self.addSample([1], [1])
        self.addSample([-1], [1])
        self.addSample([-1], [1])
        self.addSample([-1], [1])
        self.addSample([-1], [1])
        self.addSample([-1], [1])

        self.newSequence()
        self.addSample([-1], [-1])
        self.addSample([-1], [-1])
        self.addSample([1], [1])

########NEW FILE########
__FILENAME__ = xor
#!/usr/bin/env python
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.datasets import SupervisedDataSet, ImportanceDataSet


class XORDataSet(SupervisedDataSet):
    """ A dataset for the XOR function."""
    def __init__(self):
        SupervisedDataSet.__init__(self, 2, 1)
        self.addSample([0,0],[0])
        self.addSample([0,1],[1])
        self.addSample([1,0],[1])
        self.addSample([1,1],[0])


class SequentialXORDataSet(ImportanceDataSet):
    """ same thing, but sequential, and having no importance on a second output"""
    def __init__(self):
        ImportanceDataSet.__init__(self, 2, 2)
        self.addSample([0,0],[0, 1],  [1,0])
        self.addSample([0,1],[1, 10], [1,0])
        self.addSample([1,0],[1, -1], [1,0])
        self.addSample([1,1],[0, 0],  [1,0])

########NEW FILE########
__FILENAME__ = parityrnn
#!/usr/bin/env python
""" A simple recurrent neural network that detects parity for arbitrary sequences. """

__author__ = 'Tom Schaul (tom@idsia.ch)'

from datasets import ParityDataSet #@UnresolvedImport
from pybrain.supervised.trainers.backprop import BackpropTrainer
from pybrain.structure import RecurrentNetwork, LinearLayer, TanhLayer, BiasUnit, FullConnection


def buildParityNet():
    net = RecurrentNetwork()
    net.addInputModule(LinearLayer(1, name = 'i'))
    net.addModule(TanhLayer(2, name = 'h'))
    net.addModule(BiasUnit('bias'))
    net.addOutputModule(TanhLayer(1, name = 'o'))
    net.addConnection(FullConnection(net['i'], net['h']))
    net.addConnection(FullConnection(net['bias'], net['h']))
    net.addConnection(FullConnection(net['bias'], net['o']))
    net.addConnection(FullConnection(net['h'], net['o']))
    net.addRecurrentConnection(FullConnection(net['o'], net['h']))
    net.sortModules()

    p = net.params
    p[:] = [-0.5, -1.5, 1, 1, -1, 1, 1, -1, 1]
    p *= 10.

    return net

def evalRnnOnSeqDataset(net, DS, verbose = False, silent = False):
    """ evaluate the network on all the sequences of a dataset. """
    r = 0.
    samples = 0.
    for seq in DS:
        net.reset()
        for i, t in seq:
            res = net.activate(i)
            if verbose:
                print t, res
            r += sum((t-res)**2)
            samples += 1
        if verbose:
            print '-'*20
    r /= samples
    if not silent:
        print 'MSE:', r
    return r

if __name__ == "__main__":
    N = buildParityNet()
    DS = ParityDataSet()
    evalRnnOnSeqDataset(N, DS, verbose = True)
    print '(preset weights)'
    N.randomize()
    evalRnnOnSeqDataset(N, DS)
    print '(random weights)'


    # Backprop improves the network performance, and sometimes even finds the global optimum.
    N.reset()
    bp = BackpropTrainer(N, DS, verbose = True)
    bp.trainEpochs(5000)
    evalRnnOnSeqDataset(N, DS)
    print '(backprop-trained weights)'

########NEW FILE########
__FILENAME__ = data_generator
__author__ = 'Michael Isik'



from pybrain.datasets.sequential import SequentialDataSet

from numpy import array, sin, apply_along_axis, ones



class SuperimposedSine(object):
    """ Small class for generating superimposed sine signals
    """
    def __init__(self, lambdas=[1.]):
        self.lambdas = array(lambdas, float)
        self.yScales = ones(len(self.lambdas))

    def getFuncValue(self, x):
        val = 0.
        for i,l in enumerate(self.lambdas):
            val += sin(x*l)*self.yScales[i]
        return val

    def getFuncValues(self, x_array):
        values = apply_along_axis(self.getFuncValue, 0, x_array)
        return values


def generateSuperimposedSineData( sinefreqs, space, yScales=None ):
    sine = SuperimposedSine( sinefreqs )
    if yScales is not None:
        sine.yScales = array(yScales)
    dataset = SequentialDataSet(0,1)
    data = sine.getFuncValues(space)

    dataset.newSequence()
    for i in xrange(len(data)):
        dataset.addSample([], data[i])

    return dataset



########NEW FILE########
__FILENAME__ = superimposed_sine
#!/usr/bin/env python


__author__ = 'Michael Isik'

from pylab import plot, show, ion, cla, subplot, title, figlegend, draw
import numpy

from pybrain.structure.modules.evolinonetwork import EvolinoNetwork
from pybrain.supervised.trainers.evolino      import EvolinoTrainer
from lib.data_generator import generateSuperimposedSineData

print
print "=== Learning to extrapolate 5 superimposed sine waves ==="
print
sinefreqs = ( 0.2, 0.311, 0.42, 0.51, 0.74 )
# sinefreqs = ( 0.2, 0.311, 0.42, 0.51, 0.74, 0.81 )
metascale = 8.



scale    = 0.5 * metascale
stepsize = 0.1 * metascale


# === create training dataset
# the sequences must be stored in the target field
# the input field will be ignored
print "creating training data"
trnInputSpace = numpy.arange( 0*scale , 190*scale , stepsize )
trnData = generateSuperimposedSineData(sinefreqs, trnInputSpace)

# === create testing dataset
print "creating test data"
tstInputSpace = numpy.arange( 400*scale , 540*scale , stepsize)
tstData = generateSuperimposedSineData(sinefreqs, tstInputSpace)



# === create the evolino-network
print "creating EvolinoNetwork"
net = EvolinoNetwork( trnData.outdim, 40 )



wtRatio = 1./3.

# === instantiate an evolino trainer
# it will train our network through evolutionary algorithms
print "creating EvolinoTrainer"
trainer = EvolinoTrainer(
    net,
    dataset=trnData,
    subPopulationSize = 20,
    nParents = 8,
    nCombinations = 1,
    initialWeightRange = ( -0.01 , 0.01 ),
#    initialWeightRange = ( -0.1 , 0.1 ),
#    initialWeightRange = ( -0.5 , -0.2 ),
    backprojectionFactor = 0.001,
    mutationAlpha = 0.001,
#    mutationAlpha = 0.0000001,
    nBurstMutationEpochs = numpy.Infinity,
    wtRatio = wtRatio,
    verbosity = 2)



# === prepare sequences for extrapolation and plotting
trnSequence = trnData.getField('target')
separatorIdx = int(len(trnSequence)*wtRatio)
trnSequenceWashout = trnSequence[0:separatorIdx]
trnSequenceTarget  = trnSequence[separatorIdx:]

tstSequence = tstData.getField('target')
separatorIdx = int(len(tstSequence)*wtRatio)
tstSequenceWashout = tstSequence[0:separatorIdx]
tstSequenceTarget  = tstSequence[separatorIdx:]


ion() # switch matplotlib to interactive mode
for i in range(3000):
    print "======================"
    print "====== NEXT RUN ======"
    print "======================"

    print "=== TRAINING"
    # train the network for 1 epoch
    trainer.trainEpochs( 1 )


    print "=== PLOTTING\n"
    # calculate the nets output for train and the test data
    trnSequenceOutput = net.extrapolate(trnSequenceWashout, len(trnSequenceTarget))
    tstSequenceOutput = net.extrapolate(tstSequenceWashout, len(tstSequenceTarget))

    # plot training data
    sp = subplot(211) # switch to the first subplot
    cla() # clear the subplot
    title("Training Set") # set the subplot's title
    sp.set_autoscale_on( True ) # enable autoscaling
    targetline = plot(trnSequenceTarget,"r-") # plot the targets
    sp.set_autoscale_on( False ) # disable autoscaling
    outputline = plot(trnSequenceOutput,"b-") # plot the actual output


    # plot test data
    sp = subplot(212)
    cla()
    title("Test Set")
    sp.set_autoscale_on( True )
    plot(tstSequenceTarget,"r-")
    sp.set_autoscale_on( False )
    plot(tstSequenceOutput,"b-")

    # create a legend
    figlegend((targetline, outputline),('target','output'),('upper right'))

    # draw everything
    draw()


show()





########NEW FILE########
__FILENAME__ = datagenerator
#!/usr/bin/env python
# generates some simple example data sets
__author__ = "Martin Felder"
__version__ = '$Id$'

import numpy as np
from numpy.random import multivariate_normal, rand
from scipy import diag
from pylab import show, hold, plot

from pybrain.datasets import ClassificationDataSet, SequenceClassificationDataSet

def generateClassificationData(size, nClasses=3):
    """ generate a set of points in 2D belonging to two or three different classes """
    if nClasses==3:
        means = [(-1,0),(2,4),(3,1)]
    else:
        means = [(-2,0),(2,1),(6,0)]

    cov = [diag([1,1]), diag([0.5,1.2]), diag([1.5,0.7])]
    dataset = ClassificationDataSet(2, 1, nb_classes=nClasses)
    for _ in xrange(size):
        for c in range(3):
            input = multivariate_normal(means[c],cov[c])
            dataset.addSample(input, [c%nClasses])
    dataset.assignClasses()
    return dataset


def generateGridData(x,y, return_ticks=False):
    """ Generates a dataset containing a regular grid of points. The x and y arguments
    contain start, end, and step each. Returns the dataset and the x and y mesh or ticks."""
    x = np.arange(x[0], x[1], x[2])
    y = np.arange(y[0], y[1], y[2])
    X, Y = np.meshgrid(x, y)
    shape = X.shape
    # need column vectors in dataset, not arrays
    ds = ClassificationDataSet(2,1)
    ds.setField('input',  np.concatenate((X.reshape(X.size, 1),Y.reshape(X.size, 1)), 1))
    ds.setField('target', np.zeros([X.size,1]))
    ds._convertToOneOfMany()
    if return_ticks:
        return (ds, x, y)
    else:
        return (ds, X, Y)


def generateNoisySines( npoints, nseq, noise=0.3 ):
    """ construct a 2-class dataset out of noisy sines """
    x = np.arange(npoints)/float(npoints) * 20.
    y1 = np.sin(x+rand(1)*3.)
    y2 = np.sin(x/2.+rand(1)*3.)
    DS = SequenceClassificationDataSet(1,1, nb_classes=2)
    for _ in xrange(nseq):
        DS.newSequence()
        buf = rand(npoints)*noise + y1 + (rand(1)-0.5)*noise
        for i in xrange(npoints):
            DS.addSample([buf[i]],[0])
        DS.newSequence()
        buf = rand(npoints)*noise + y2 + (rand(1)-0.5)*noise
        for i in xrange(npoints):
            DS.addSample([buf[i]],[1])
    return DS

def plotData(ds):
    hold(True)
    for c in range(ds.nClasses):
        here, _ = np.where(ds['class']==c)
        plot(ds['input'][here,0],ds['input'][here,1],'o')


if __name__ == '__main__':
    plotData(generateClassificationData(150))
    show()

########NEW FILE########
__FILENAME__ = example_fnn
#!/usr/bin/env python
# Example script for feed-forward network usage in PyBrain.
__author__ = "Martin Felder"
__version__ = '$Id$'

from pylab import figure, ioff, clf, contourf, ion, draw, show
from pybrain.utilities           import percentError
from pybrain.tools.shortcuts     import buildNetwork
from pybrain.supervised.trainers import BackpropTrainer
from pybrain.structure.modules   import SoftmaxLayer

from datasets import generateGridData, generateClassificationData, plotData

# load the training data set
trndata = generateClassificationData(250)

# neural networks work better if classes are encoded using
# one output neuron per class
trndata._convertToOneOfMany( bounds=[0,1] )

# same for the independent test data set
tstdata = generateClassificationData(100)
tstdata._convertToOneOfMany( bounds=[0,1] )

# build a feed-forward network with 20 hidden units, plus
# a corresponding trainer
fnn = buildNetwork( trndata.indim, 5, trndata.outdim, outclass=SoftmaxLayer )
trainer = BackpropTrainer( fnn, dataset=trndata, momentum=0.1, verbose=True, weightdecay=0.01)

# generate a grid of data points for visualization
griddata, X, Y = generateGridData([-3.,6.,0.2],[-3.,6.,0.2])

# repeat 20 times
for i in range(20):
    # train the network for 1 epoch
    trainer.trainEpochs( 1 )

    # evaluate the result on the training and test data
    trnresult = percentError( trainer.testOnClassData(),
                              trndata['class'] )
    tstresult = percentError( trainer.testOnClassData(
           dataset=tstdata ), tstdata['class'] )

    # print the result
    print "epoch: %4d" % trainer.totalepochs, \
          "  train error: %5.2f%%" % trnresult, \
          "  test error: %5.2f%%" % tstresult

    # run our grid data through the FNN, get the most likely class
    # and shape it into an array
    out = fnn.activateOnDataset(griddata)
    out = out.argmax(axis=1)
    out = out.reshape(X.shape)

    # plot the test data and the underlying grid as a filled contour
    figure(1)
    ioff()  # interactive graphics off
    clf()
    # plot the datapoints
    plotData(tstdata)
    # overlay a contour plot of the functional margin
    if out.max()!=out.min():
        CS = contourf(X, Y, out)
    ion()   # interactive graphics on
    draw()  # update the plot

# show the plot until user kills it
ioff()
show()


########NEW FILE########
__FILENAME__ = example_rnn
#!/usr/bin/env python
# Example script for recurrent network usage in PyBrain.
__author__ = "Martin Felder"
__version__ = '$Id$'

from pylab import plot, hold, show
from scipy import sin, rand, arange
from pybrain.datasets            import SequenceClassificationDataSet
from pybrain.structure.modules   import LSTMLayer, SoftmaxLayer
from pybrain.supervised          import RPropMinusTrainer
from pybrain.tools.validation    import testOnSequenceData
from pybrain.tools.shortcuts     import buildNetwork

from datasets import generateNoisySines

# create training and test data
trndata = generateNoisySines(50, 40)
trndata._convertToOneOfMany( bounds=[0.,1.] )
tstdata = generateNoisySines(50, 20)
tstdata._convertToOneOfMany( bounds=[0.,1.] )

# construct LSTM network - note the missing output bias
rnn = buildNetwork( trndata.indim, 5, trndata.outdim, hiddenclass=LSTMLayer, outclass=SoftmaxLayer, outputbias=False, recurrent=True)

# define a training method
trainer = RPropMinusTrainer( rnn, dataset=trndata, verbose=True )
# instead, you may also try
##trainer = BackpropTrainer( rnn, dataset=trndata, verbose=True, momentum=0.9, learningrate=0.00001 )

# carry out the training
for i in xrange(100):
    trainer.trainEpochs( 2 )
    trnresult = 100. * (1.0-testOnSequenceData(rnn, trndata))
    tstresult = 100. * (1.0-testOnSequenceData(rnn, tstdata))
    print "train error: %5.2f%%" % trnresult, ",  test error: %5.2f%%" % tstresult

# just for reference, plot the first 5 timeseries
plot(trndata['input'][0:250,:],'-o')
hold(True)
plot(trndata['target'][0:250,0])
show()


########NEW FILE########
__FILENAME__ = example_svm
#!/usr/bin/env python
""" Example script for SVM classification using PyBrain and LIBSVM
CAVEAT: Needs the libsvm Python file svm.py and the corresponding (compiled) library to reside in the Python path! """

__author__ = "Martin Felder"
__version__ = '$Id$'

import pylab as p
import logging
from os.path import join

# load the necessary components
from pybrain.datasets            import ClassificationDataSet
from pybrain.utilities           import percentError

from pybrain.structure.modules.svmunit        import SVMUnit
from pybrain.supervised.trainers.svmtrainer   import SVMTrainer

# import some local stuff
from datasets               import generateClassificationData, plotData, generateGridData

logging.basicConfig(level=logging.INFO, filename=join('.','testrun.log'),
                    format='%(asctime)s %(levelname)s %(message)s')
logging.getLogger('').addHandler(logging.StreamHandler())


# load the training and test data sets
trndata = generateClassificationData(20, nClasses=2)
tstdata = generateClassificationData(100, nClasses=2)

# initialize the SVM module and a corresponding trainer
svm = SVMUnit()
trainer = SVMTrainer( svm, trndata )

# train the with fixed meta-parameters
log2C=0.   # degree of slack
log2g=1.1  # width of RBF kernels
trainer.train( log2C=log2C, log2g=log2g )
# alternatively, could train the SVM using design-of-experiments grid search
##trainer.train( search="GridSearchDOE" )

# pass data sets through the SVM to get performance
trnresult = percentError( svm.activateOnDataset(trndata), trndata['target'] )
tstresult = percentError( svm.activateOnDataset(tstdata), tstdata['target'] )
print "sigma: %7g,  C: %7g,  train error: %5.2f%%,  test error: %5.2f%%" % (2.0**log2g, 2.0**log2C, trnresult, tstresult)

# generate a grid dataset
griddat, X, Y = generateGridData(x=[-4,8,0.1],y=[-2,3,0.1])

# pass the grid through the SVM, but this time get the raw distance
# from the boundary, not the class
Z = svm.activateOnDataset(griddat, values=True)

# the output format is a bit weird... make it into a decent array
Z = p.array([z.values()[0] for z in Z]).reshape(X.shape)

# make a 2d plot of training data with an decision value contour overlay
fig = p.figure()
plotData(trndata)
p.contourf(X, Y, Z)
p.show()

########NEW FILE########
__FILENAME__ = jpq2layersReader
from pybrain.structure import FeedForwardNetwork
from pybrain.tools.validation    import ModuleValidator,Validator
from pybrain.utilities           import percentError
from pybrain.tools.customxml     import NetworkReader
from pybrain.datasets            import SupervisedDataSet
import numpy
import pylab
import os

def myplot(trns,ctrns = None,tsts = None,ctsts = None,iter = 0):
  plotdir = os.path.join(os.getcwd(),'plot')
  pylab.clf()
  try:
    assert len(tsts) > 1
    tstsplot = True
  except:
    tstsplot = False
  try:
    assert len(ctsts) > 1
    ctstsplot = True
  except:
    ctstsplot = False
  try:
    assert len(ctrns) > 1
    ctrnsplot = True
  except:
    ctrnsplot = False
  if tstsplot:
    pylab.plot(tsts['input'],tsts['target'],c='b')
  pylab.scatter(trns['input'],trns['target'],c='r')
  if ctrnsplot:
    pylab.scatter(trns['input'],ctrns,c='y')
  if tstsplot and ctstsplot:
    pylab.plot(tsts['input'], ctsts,c='g')
 
  pylab.xlabel('x')
  pylab.ylabel('y')
  pylab.title('Neuron Number:'+str(nneuron))
  pylab.grid(True)
  plotname = os.path.join(plotdir,('jpq2layers_plot'+ str(iter)))
  pylab.savefig(plotname)


# set-up the neural network
nneuron = 5
mom = 0.98
netname="LSL-"+str(nneuron)+"-"+str(mom)
mv=ModuleValidator()
v = Validator()


#create the test DataSet
x = numpy.arange(0.0, 1.0+0.01, 0.01)
s = 0.5+0.4*numpy.sin(2*numpy.pi*x)
tsts = SupervisedDataSet(1,1)
tsts.setField('input',x.reshape(len(x),1))
tsts.setField('target',s.reshape(len(s),1))
#read the train DataSet from file
trndata = SupervisedDataSet.loadFromFile(os.path.join(os.getcwd(),'trndata'))

myneuralnet = os.path.join(os.getcwd(),'myneuralnet.xml')
if os.path.isfile(myneuralnet):
  n = NetworkReader.readFrom(myneuralnet,name=netname)
  #calculate the test DataSet based on the trained Neural Network
  ctsts = mv.calculateModuleOutput(n,tsts)
  tserr = v.MSE(ctsts,tsts['target'])
  print 'MSE error on TSTS:',tserr
  myplot(trndata,tsts = tsts,ctsts = ctsts)

  pylab.show()

########NEW FILE########
__FILENAME__ = jpq2layersWriter
from pybrain.structure import FeedForwardNetwork
from pybrain.structure           import LinearLayer, SigmoidLayer
from pybrain.structure           import BiasUnit,TanhLayer
from pybrain.structure           import FullConnection
from pybrain.datasets            import SupervisedDataSet
from pybrain.supervised.trainers import BackpropTrainer, RPropMinusTrainer
from pybrain.tools.validation    import ModuleValidator,Validator
from pybrain.utilities           import percentError
from pybrain.tools.customxml     import NetworkWriter
import numpy
import pylab
import os

def myplot(trns,ctrns,tsts = None,ctsts = None,iter = 0):
  plotdir = os.path.join(os.getcwd(),'plot')
  pylab.clf()
  try:
    assert len(tsts) > 1
    tstsplot = True
  except:
    tstsplot = False
  try:
    assert len(ctsts) > 1
    ctstsplot = True
  except:
    ctstsplot = False
  if tstsplot:
    pylab.plot(tsts['input'],tsts['target'],c='b')
  pylab.scatter(trns['input'],trns['target'],c='r')
  pylab.scatter(trns['input'],ctrns,c='y')
  if tstsplot and ctstsplot:
    pylab.plot(tsts['input'], ctsts,c='g')
 
  pylab.xlabel('x')
  pylab.ylabel('y')
  pylab.title('Neuron Number:'+str(nneuron))
  pylab.grid(True)
  plotname = os.path.join(plotdir,('jpq2layers_plot'+ str(iter)))
  pylab.savefig(plotname)


# set-up the neural network
nneuron = 5
mom = 0.98
netname="LSL-"+str(nneuron)+"-"+str(mom)
mv=ModuleValidator()
v = Validator()
n=FeedForwardNetwork(name=netname)
inLayer = LinearLayer(1,name='in')
hiddenLayer = SigmoidLayer(nneuron,name='hidden0')
outLayer = LinearLayer(1,name='out')
biasinUnit = BiasUnit(name="bhidden0")
biasoutUnit = BiasUnit(name="bout")
n.addInputModule(inLayer)
n.addModule(hiddenLayer)
n.addModule(biasinUnit)
n.addModule(biasoutUnit)
n.addOutputModule(outLayer)
in_to_hidden = FullConnection(inLayer,hiddenLayer)
bias_to_hidden = FullConnection(biasinUnit,hiddenLayer)
bias_to_out = FullConnection(biasoutUnit,outLayer)
hidden_to_out = FullConnection(hiddenLayer,outLayer)
n.addConnection(in_to_hidden)
n.addConnection(bias_to_hidden)
n.addConnection(bias_to_out)
n.addConnection(hidden_to_out)

n.sortModules()
n.reset()

#read the initail weight values from myparam2.txt
filetoopen = os.path.join(os.getcwd(),'myparam2.txt')
if os.path.isfile(filetoopen):
  myfile = open('myparam2.txt','r')
  c=[]
  for line in myfile:
    c.append(float(line))
  n._setParameters(c)
else:
  myfile = open('myparam2.txt','w')
  for i in n.params:
    myfile.write(str(i)+'\n')
myfile.close()

#activate the neural networks
act = SupervisedDataSet(1,1)
act.addSample((0.2,),(0.880422606518061,))
n.activateOnDataset(act)
#create the test DataSet
x = numpy.arange(0.0, 1.0+0.01, 0.01)
s = 0.5+0.4*numpy.sin(2*numpy.pi*x)
tsts = SupervisedDataSet(1,1)
tsts.setField('input',x.reshape(len(x),1))
tsts.setField('target',s.reshape(len(s),1))

#read the train DataSet from file
trndata = SupervisedDataSet.loadFromFile(os.path.join(os.getcwd(),'trndata'))

#create the trainer

t = BackpropTrainer(n, learningrate = 0.01 ,
                    momentum = mom)
#train the neural network from the train DataSet

cterrori=1.0
print "trainer momentum:"+str(mom)
for iter in range(25):
  t.trainOnDataset(trndata, 1000)
  ctrndata = mv.calculateModuleOutput(n,trndata)
  cterr = v.MSE(ctrndata,trndata['target'])
  relerr = abs(cterr-cterrori)
  cterrori = cterr
  print 'iteration:',iter+1,'MSE error:',cterr
  myplot(trndata,ctrndata,iter=iter+1)
  if cterr < 1.e-5 or relerr < 1.e-7:
    break
#write the network using xml file     
myneuralnet = os.path.join(os.getcwd(),'myneuralnet.xml')
if os.path.isfile(myneuralnet):
    NetworkWriter.appendToFile(n,myneuralnet)
else:
    NetworkWriter.writeToFile(n,myneuralnet)
    
#calculate the test DataSet based on the trained Neural Network
ctsts = mv.calculateModuleOutput(n,tsts)
tserr = v.MSE(ctsts,tsts['target'])
print 'MSE error on TSTS:',tserr
myplot(trndata,ctrndata,tsts,ctsts)

pylab.show()

########NEW FILE########
__FILENAME__ = gp
#!/usr/bin/env python
""" A simple example on how to use the GaussianProcess class
in pybrain, for one and two dimensions. """

__author__ = "Thomas Rueckstiess, ruecksti@in.tum.de"

from pybrain.auxiliary import GaussianProcess
from pybrain.datasets import SupervisedDataSet
from scipy import mgrid, sin, cos, array, ravel
from pylab import show, figure

ds = SupervisedDataSet(1, 1)
gp = GaussianProcess(indim=1, start=-3, stop=3, step=0.05)
figure()

x = mgrid[-3:3:0.2]
y = 0.1*x**2 + x + 1
z = sin(x) + 0.5*cos(y)

ds.addSample(-2.5, -1)
ds.addSample(-1.0, 3)
gp.mean = 0

# new feature "autonoise" adds uncertainty to data depending on
# it's distance to other points in the dataset. not tested much yet.
# gp.autonoise = True

gp.trainOnDataset(ds)
gp.plotCurves(showSamples=True)

# you can also test the gp on single points, but this deletes the
# original testing grid. it can be restored with a call to _buildGrid()
print gp.testOnArray(array([[0.4]]))


# --- example on how to use the GP in 2 dimensions

ds = SupervisedDataSet(2,1)
gp = GaussianProcess(indim=2, start=0, stop=5, step=0.25)
figure()

x,y = mgrid[0:5:4j, 0:5:4j]
z = cos(x)*sin(y)
(x, y, z) = map(ravel, [x, y, z])

for i,j,k in zip(x, y, z):
    ds.addSample([i, j], [k])

print "preparing plots. this can take a few seconds..."
gp.trainOnDataset(ds)
gp.plotCurves()

show()
########NEW FILE########
__FILENAME__ = kohonen
#!/usr/bin/env python
##################################################
# Example for Kohonen Map
#
# Clusters random 2D coordinates in range [0,1]
# with a Kohonen Map of 5x5 neurons.
#
# Note: you need pylab to show the results
##################################################

__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

import pylab
from scipy import random
from pybrain.structure.modules import KohonenMap

som = KohonenMap(2, 5)

pylab.ion()
p = pylab.plot(som.neurons[:,:,0].flatten(), som.neurons[:,:,1].flatten(), 's')

for i in range(25000):
    # one forward and one backward (training) pass
    som.activate(random.random(2))
    som.backward()

    # plot every 100th step
    if i % 100 == 0:
        p[0].set_data(som.neurons[:,:,0].flatten(), som.neurons[:,:,1].flatten())
        pylab.draw()
########NEW FILE########
__FILENAME__ = lsh
#!/usr/bin/env python
from __future__ import division

__author__ = 'Justin Bayer, bayer.justin@googlemail.com'


import logging
from random import shuffle

from pylab import show, plot, clf
from pybrain.supervised.knn.lsh.nearoptimal import MultiDimHash
from scipy import random, array, dot, zeros
from scipy.linalg import orth


def randomRotation(dim):
    """Return a random rotation matrix of rank dim."""
    return orth(random.random((dim, dim)))


def makeData(amount = 10000):
    """Return 2D dataset of points in (0, 1) where points in a circle of
    radius .4 around the center are blue and all the others are red."""
    center = array([0.5, 0.5])

    def makePoint():
        """Return a random point and its satellite information.

        Satellite is 'blue' if point is in the circle, else 'red'."""
        point = random.random((2,)) * 10
        vectorLength = lambda x: dot(x.T, x)
        return point, 'blue' if vectorLength(point - center) < 25 else 'red'

    return [makePoint() for _ in xrange(amount)]


if __name__ == '__main__':
    # Amount of dimensions to test with
    dimensions = 3

    loglevel = logging.DEBUG
    logging.basicConfig(level=loglevel,
                        format='%(asctime)s %(levelname)s %(message)s')

    logging.info("Making dataset...")
    data = makeData(1000)

    logging.info("Making random projection...")
    proj = zeros((2, dimensions))
    proj[0, 0] = 1
    proj[1, 1] = 1
    randRot = randomRotation(dimensions)
    proj = dot(proj, randRot)

    logging.info("Initializing data structure...")
    m = MultiDimHash(dimensions, 2, 0.80)

    logging.info("Putting data into hash...")
    for point, satellite in data:
        point = dot(point, proj)
        m.insert(point, satellite)

    logging.info("Retrieve nearest neighbours...")
    result = []
    width, height = 2**5, 2**5
    grid = (array([i / width * 10, j / height * 10])
            for i in xrange(width)
            for j in xrange(height))
    projected_grid = [(p, dot(p, proj)) for p in grid]

    # Just to fake random access
    shuffle(projected_grid)

    for p, pp in projected_grid:
        nns = m.knn(pp, 1)
        if nns == []:
            continue
        _, color = nns[0]
        result.append((p, color))

    # Visualize it
    visualize = True
    if visualize:
        clf()
        result = [((x, y), color)
                  for (x, y), color in result
                  if color is not None]

        xs_red = [x for ((x, y), color) in result if color == 'red']
        ys_red = [y for ((x, y), color) in result if color == 'red']
        xs_blue = [x for ((x, y), color) in result if color == 'blue']
        ys_blue = [y for ((x, y), color) in result if color == 'blue']

        plot(xs_red, ys_red, 'ro')
        plot(xs_blue, ys_blue, 'bo')
        show()

    ballsizes = (len(ball) for ball in m.balls.itervalues())
    logging.info("Sizes of the balls: " + " ".join(str(i) for i in ballsizes))

    logging.info("Finished")

########NEW FILE########
__FILENAME__ = rbm
#!/usr/bin/env python
""" Miniscule restricted Boltzmann machine usage example """

__author__ = 'Justin S Bayer, bayer.justin@googlemail.com'


from pybrain.structure.networks.rbm import Rbm
from pybrain.unsupervised.trainers.rbm import (RbmGibbsTrainerConfig,
                                               RbmBernoulliTrainer)
from pybrain.datasets import UnsupervisedDataSet


ds = UnsupervisedDataSet(6)
ds.addSample([0, 1] * 3)
ds.addSample([1, 0] * 3)

cfg = RbmGibbsTrainerConfig()
cfg.maxIter = 3

rbm = Rbm.fromDims(6, 1)
trainer = RbmBernoulliTrainer(rbm, ds, cfg)
print rbm.params, rbm.biasParams
for _ in xrange(50):
    trainer.train()

print rbm.params, rbm.biasParams

########NEW FILE########
__FILENAME__ = gaussprocess
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de; Christian Osendorfer, osendorf@in.tum.de'


from scipy import r_, exp, zeros, eye, array, asarray, random, ravel, diag, sqrt, sin, cos, sort, mgrid, dot, floor
from scipy import c_ #@UnusedImport
from scipy.linalg import solve, inv
from pybrain.datasets import SupervisedDataSet
from scipy.linalg import norm


class GaussianProcess:
    """ This class represents a basic n-dimensional Gaussian Process. The implementation
        follows the book 'Gaussian Processes for Machine Learning' by Carl E. Rasmussen
        (an online version is available at: http://www.gaussianprocess.org/gpml/chapters/).
        The hyper parameters of the GP can be adjusted by setting the self.hyper varible,
        which must be a tuple of size 3.
    """

    def __init__(self, indim, start=0, stop=1, step=0.1):
        """ initializes the gaussian process object.

            :arg indim: input dimension
            :key start: start of interval for sampling the GP.
            :key stop: stop of interval for sampling the GP.
            :key step: stepsize for sampling interval.
            :note: start, stop, step can either be scalars or tuples of size 'indim'.
        """
        self.mean = 0
        self.start = start
        self.stop = stop
        self.step = step
        self.indim = indim
        self.trainx = zeros((0, indim), float)
        self.trainy = zeros((0), float)
        self.noise = zeros((0), float)
        self.testx = self._buildGrid()
        self.calculated = True
        self.pred_mean = zeros(len(self.testx))
        self.pred_cov = eye(len(self.testx))
        self.autonoise = False
        self.hyper = (0.5, 2.0, 0.1)

    def _kernel(self, a, b):
        """ kernel function, here RBF kernel """
        (l, sigma_f, _sigma_n) = self.hyper
        r = sigma_f ** 2 * exp(-1.0 / (2 * l ** 2) * norm(a - b, 2) ** 2)
        # if a == b:
        #   r += sigma_n**2
        return r

    def _buildGrid(self):
        (start, stop, step) = (self.start, self.stop, self.step)
        """ returns a mgrid type of array for 'dim' dimensions """
        if isinstance(start, (int, long, float, complex)):
            dimstr = 'start:stop:step, '*self.indim
        else:
            assert len(start) == len(stop) == len(step)
            dimstr = ["start[%i]:stop[%i]:step[%i], " % (i, i, i) for i in range(len(start))]
            dimstr = ''.join(dimstr)
        return eval('c_[map(ravel, mgrid[' + dimstr + '])]').T

    def _buildCov(self, a, b):
        K = zeros((len(a), len(b)), float)
        for i in range(len(a)):
            for j in range(len(b)):
                K[i, j] = self._kernel(a[i, :], b[j, :])
        return K

    def reset(self):
        self.trainx = zeros((0, self.indim), float)
        self.trainy = zeros((0), float)
        self.noise = zeros((0), float)
        self.pred_mean = zeros(len(self.testx))
        self.pred_cov = eye(len(self.testx))

    def trainOnDataset(self, dataset):
        """ takes a SequentialDataSet with indim input dimension and scalar target """
        assert (dataset.getDimension('input') == self.indim)
        assert (dataset.getDimension('target') == 1)

        self.trainx = dataset.getField('input')
        self.trainy = ravel(dataset.getField('target'))
        self.noise = array([0.001] * len(self.trainx))
        # print(self.trainx, self.trainy)
        self.calculated = False

    def addDataset(self, dataset):
        """ adds the points from the dataset to the training set """
        assert (dataset.getDimension('input') == self.indim)
        assert (dataset.getDimension('target') == 1)

        self.trainx = r_[self.trainx, dataset.getField('input')]
        self.trainy = r_[self.trainy, ravel(dataset.getField('target'))]
        self.noise = array([0.001] * len(self.trainx))
        self.calculated = False

    def addSample(self, train, target):
        self.trainx = r_[self.trainx, asarray([train])]
        self.trainy = r_[self.trainy, asarray(target)]
        self.noise = r_[self.noise, array([0.001])]
        self.calculated = False

    def testOnArray(self, arr):
        self.testx = arr
        self._calculate()
        return self.pred_mean

    def _calculate(self):
        # calculate only of necessary
        if len(self.trainx) == 0:
            return

        # build covariance matrices
        train_train = self._buildCov(self.trainx, self.trainx)
        train_test = self._buildCov(self.trainx, self.testx)
        test_train = train_test.T
        test_test = self._buildCov(self.testx, self.testx)

        # calculate predictive mean and covariance
        K = train_train + self.noise * eye(len(self.trainx))

        if self.autonoise:
            # calculate average neighboring distance for auto-noise
            avgdist = 0
            sort_trainx = sort(self.trainx)
            for i, d in enumerate(sort_trainx):
                if i == 0:
                    continue
                avgdist += d - sort_trainx[i - 1]
            avgdist /= len(sort_trainx) - 1
            # sort(self.trainx)

            # add auto-noise from neighbouring samples (not standard gp)
            for i in range(len(self.trainx)):
                for j in range(len(self.trainx)):
                    if norm(self.trainx[i] - self.trainx[j]) > avgdist:
                        continue

                    d = norm(self.trainy[i] - self.trainy[j]) / (exp(norm(self.trainx[i] - self.trainx[j])))
                    K[i, i] += d

        self.pred_mean = self.mean + dot(test_train, solve(K, self.trainy - self.mean, sym_pos=0))
        self.pred_cov = test_test - dot(test_train, dot(inv(K), train_test))
        self.calculated = True

    def draw(self):
        if not self.calculated:
            self._calculate()

        return self.pred_mean + random.multivariate_normal(zeros(len(self.testx)), self.pred_cov)

    def plotCurves(self, showSamples=False, force2D=True):
        from pylab import clf, hold, plot, fill, title, gcf, pcolor, gray

        if not self.calculated:
            self._calculate()

        if self.indim == 1:
            clf()
            hold(True)
            if showSamples:
                # plot samples (gray)
                for _ in range(5):
                    plot(self.testx, self.pred_mean + random.multivariate_normal(zeros(len(self.testx)), self.pred_cov), color='gray')

            # plot training set
            plot(self.trainx, self.trainy, 'bx')
            # plot mean (blue)
            plot(self.testx, self.pred_mean, 'b', linewidth=1)
            # plot variance (as "polygon" going from left to right for upper half and back for lower half)
            fillx = r_[ravel(self.testx), ravel(self.testx[::-1])]
            filly = r_[self.pred_mean + 2 * diag(self.pred_cov), self.pred_mean[::-1] - 2 * diag(self.pred_cov)[::-1]]
            fill(fillx, filly, facecolor='gray', edgecolor='white', alpha=0.3)
            title('1D Gaussian Process with mean and variance')

        elif self.indim == 2 and not force2D:
            from matplotlib import axes3d as a3

            fig = gcf()
            fig.clear()
            ax = a3.Axes3D(fig) #@UndefinedVariable

            # plot training set
            ax.plot3D(ravel(self.trainx[:, 0]), ravel(self.trainx[:, 1]), ravel(self.trainy), 'ro')

            # plot mean
            (x, y, z) = map(lambda m: m.reshape(sqrt(len(m)), sqrt(len(m))), (self.testx[:, 0], self.testx[:, 1], self.pred_mean))
            ax.plot_wireframe(x, y, z, colors='gray')
            return ax

        elif self.indim == 2 and force2D:
            # plot mean on pcolor map
            gray()
            # (x, y, z) = map(lambda m: m.reshape(sqrt(len(m)), sqrt(len(m))), (self.testx[:,0], self.testx[:,1], self.pred_mean))
            m = floor(sqrt(len(self.pred_mean)))
            pcolor(self.pred_mean.reshape(m, m)[::-1, :])

        else: print("plotting only supported for indim=1 or indim=2.")


if __name__ == '__main__':

    from pylab import figure, show

    # --- example on how to use the GP in 1 dimension
    ds = SupervisedDataSet(1, 1)
    gp = GaussianProcess(indim=1, start= -3, stop=3, step=0.05)
    figure()

    x = mgrid[-3:3:0.2]
    y = 0.1 * x ** 2 + x + 1
    z = sin(x) + 0.5 * cos(y)

    ds.addSample(-2.5, -1)
    ds.addSample(-1.0, 3)
    gp.mean = 0

    # new feature "autonoise" adds uncertainty to data depending on
    # it's distance to other points in the dataset. not tested much yet.
    # gp.autonoise = True

    gp.trainOnDataset(ds)
    gp.plotCurves(showSamples=True)

    # you can also test the gp on single points, but this deletes the
    # original testing grid. it can be restored with a call to _buildGrid()
    print(gp.testOnArray(array([[0.4]])))


    # --- example on how to use the GP in 2 dimensions

    ds = SupervisedDataSet(2, 1)
    gp = GaussianProcess(indim=2, start=0, stop=5, step=0.2)
    figure()

    x, y = mgrid[0:5:4j, 0:5:4j]
    z = cos(x) * sin(y)
    (x, y, z) = map(ravel, [x, y, z])

    for i, j, k in zip(x, y, z):
        ds.addSample([i, j], [k])

    gp.trainOnDataset(ds)
    gp.plotCurves()

    show()

########NEW FILE########
__FILENAME__ = gradientdescent
__author__ = ('Thomas Rueckstiess, ruecksti@in.tum.de'
              'Justin Bayer, bayer.justin@googlemail.com')


from scipy import zeros, asarray, sign, array, cov, dot, clip, ndarray
from scipy.linalg import inv


class GradientDescent(object):

    def __init__(self):
        """ initialize algorithms with standard parameters (typical values given in parentheses)"""

        # --- BackProp parameters ---
        # learning rate (0.1-0.001, down to 1e-7 for RNNs)
        self.alpha = 0.1

        # alpha decay (0.999; 1.0 = disabled)
        self.alphadecay = 1.0

        # momentum parameters (0.1 or 0.9)
        self.momentum = 0.0
        self.momentumvector = None

        # --- RProp parameters ---
        self.rprop = False
        # maximum step width (1 - 20)
        self.deltamax = 5.0
        # minimum step width (0.01 - 1e-6)
        self.deltamin = 0.01
        # the remaining parameters do not normally need to be changed
        self.deltanull = 0.1
        self.etaplus = 1.2
        self.etaminus = 0.5
        self.lastgradient = None

    def init(self, values):
        """ call this to initialize data structures *after* algorithm to use
        has been selected

        :arg values: the list (or array) of parameters to perform gradient descent on
                       (will be copied, original not modified)
        """
        assert isinstance(values, ndarray)
        self.values = values.copy()
        if self.rprop:
            self.lastgradient = zeros(len(values), dtype='float64')
            self.rprop_theta = self.lastgradient + self.deltanull
            self.momentumvector = None
        else:
            self.lastgradient = None
            self.momentumvector = zeros(len(values))

    def __call__(self, gradient, error=None):
        """ calculates parameter change based on given gradient and returns updated parameters """
        # check if gradient has correct dimensionality, then make array """
        assert len(gradient) == len(self.values)
        gradient_arr = asarray(gradient)

        if self.rprop:
            rprop_theta = self.rprop_theta

            # update parameters
            self.values += sign(gradient_arr) * rprop_theta

            # update rprop meta parameters
            dirSwitch = self.lastgradient * gradient_arr
            rprop_theta[dirSwitch > 0] *= self.etaplus
            idx =  dirSwitch < 0
            rprop_theta[idx] *= self.etaminus
            gradient_arr[idx] = 0

            # upper and lower bound for both matrices
            rprop_theta = rprop_theta.clip(min=self.deltamin, max=self.deltamax)

            # save current gradients to compare with in next time step
            self.lastgradient = gradient_arr.copy()

            self.rprop_theta = rprop_theta

        else:
            # update momentum vector (momentum = 0 clears it)
            self.momentumvector *= self.momentum

            # update parameters (including momentum)
            self.momentumvector += self.alpha * gradient_arr
            self.alpha *= self.alphadecay

            # update parameters
            self.values += self.momentumvector

        return self.values

    descent = __call__


class NaturalGradient(object):

    def __init__(self, samplesize):
        # Counter after how many samples a new gradient estimate will be
        # returned.
        self.samplesize = samplesize
        # Samples of the gradient are held in this datastructure.
        self.samples = []

    def init(self, values):
        self.values = values.copy()

    def __call__(self, gradient, error=None):
        # Append a copy to make sure this one is not changed after by the
        # client.
        self.samples.append(array(gradient))
        # Return None if no new estimate is being given.
        if len(self.samples) < self.samplesize:
            return None
        # After all the samples have been put into a single array, we can
        # delete them.
        gradientarray = array(self.samples).T
        inv_covar = inv(cov(gradientarray))
        self.values += dot(inv_covar, gradientarray.sum(axis=1))
        return self.values


class IRpropPlus(object):

    def __init__(self, upfactor=1.1, downfactor=0.9, bound=0.5):
        self.upfactor = upfactor
        self.downfactor = downfactor
        if not bound > 0:
            raise ValueError("bound greater than 0 needed.")

    def init(self, values):
        self.values = values.copy()
        self.prev_values = values.copy()
        self.more_prev_values = values.copy()
        self.previous_gradient = zeros(values.shape)
        self.step = zeros(values.shape)
        self.previous_error = float("-inf")

    def __call__(self, gradient, error):
        products = self.previous_gradient * gradient
        signs = sign(gradient)

        # For positive gradient parts.
        positive = (products > 0).astype('int8')
        pos_step = self.step * self.upfactor * positive
        clip(pos_step, -self.bound, self.bound)
        pos_update = self.values - signs * pos_step

        # For negative gradient parts.
        negative = (products < 0).astype('int8')
        neg_step = self.step * self.downfactor * negative
        clip(neg_step, -self.bound, self.bound)
        if error <= self.previous_error:
            # If the error has decreased, do nothing.
            neg_update = zeros(gradient.shape)
        else:
            # If it has increased, move back 2 steps.
            neg_update = self.more_prev_values
        # Set all negative gradients to zero for the next step.
        gradient *= positive

        # Bookkeeping.
        self.previous_gradient = gradient
        self.more_prev_values = self.prev_values
        self.prev_values = self.values.copy()
        self.previous_error = error

        # Updates.
        self.step[:] = pos_step + neg_step
        self.values[:] = positive * pos_update + negative * neg_update

        return self.values

########NEW FILE########
__FILENAME__ = importancemixing
__author__ = 'Tom Schaul, tom@idsia.ch'

from random import uniform
from scipy import array
from scipy.stats.distributions import norm


def importanceMixing(oldpoints, oldpdf, newpdf, newdistr, forcedRefresh = 0.01):
    """ Implements importance mixing. Given a set of points, an old and a new pdf-function for them
    and a generator function for new points, it produces a list of indices of the old points to be reused and a list of new points.
    Parameter (optional): forced refresh rate.
    """
    reuseindices = []
    batch = len(oldpoints)
    for i, sample in enumerate(oldpoints):
        r = uniform(0, 1)
        if r < (1-forcedRefresh) * newpdf(sample) / oldpdf(sample):
            reuseindices.append(i)
        # never use only old samples
        if batch - len(reuseindices) <= max(1, batch * forcedRefresh):
            break
    newpoints = []
    # add the remaining ones
    while len(reuseindices)+len(newpoints) < batch:
        r = uniform(0, 1)
        sample = newdistr()
        if r < forcedRefresh:
            newpoints.append(sample)
        else:
            if r < 1 - oldpdf(sample)/newpdf(sample):
                newpoints.append(sample)
    return reuseindices, newpoints


def testImportanceMixing(popsize = 5000, forcedRefresh = 0.0):
    import pylab
    distr1 = norm()
    distr2 = norm(loc = 1.5)
    p1 = distr1.rvs(popsize)
    inds, np = importanceMixing(p1, distr1.pdf, distr2.pdf, lambda: distr2.rvs()[0], forcedRefresh)
    reuse = [p1[i] for i in inds]
    p2 = reuse + np
    p2b = distr2.rvs(popsize)
    pylab.hist(array([p2, p2b]).T,
               20, normed=1, histtype='bar')
    pylab.show()


if __name__ == '__main__':
    testImportanceMixing()
########NEW FILE########
__FILENAME__ = kmeans
#######################################################################
# k-means++
#
# this is a k-means clustering algorithm that selects its
# initial cluster centers in a smart way to speed up convergence.
# see: Arthur, D. and Vassilvitskii, S. "k-means++: the advantages
# of careful seeding". ACM-SIAM symposium on Discrete algorithms. 2007
#
# Implementation from Yong Sun's website
# http://blogs.sun.com/yongsun/entry/k_means_and_k_means
#######################################################################

__author__ = "Thomas Rueckstiess, ruecksti@in.tum.de"

from scipy.cluster.vq import kmeans2
from scipy import random, array
from scipy.linalg import norm

def kinit(X, k):
    'init k seeds according to kmeans++'
    n = X.shape[0]

    'choose the 1st seed randomly, and store D(x)^2 in D[]'
    centers = [X[random.randint(n)]]
    D = [norm(x - centers[0]) ** 2 for x in X]

    for _ in range(k - 1):
        bestDsum = bestIdx = -1

        for i in range(n):
            'Dsum = sum_{x in X} min(D(x)^2,||x-xi||^2)'
            Dsum = reduce(lambda x, y:x + y,
                          (min(D[j], norm(X[j] - X[i]) ** 2) for j in xrange(n)))

            if bestDsum < 0 or Dsum < bestDsum:
                bestDsum, bestIdx = Dsum, i

        centers.append (X[bestIdx])
        D = [min(D[i], norm(X[i] - X[bestIdx]) ** 2) for i in xrange(n)]

    return array (centers)

def kmeanspp(Y, k):
    return kmeans2(Y, kinit(Y, k), minit='points')


########NEW FILE########
__FILENAME__ = pca
# -*- coding: utf-8 -*-


"""Module that contains functionality for calculating the principal components
of a dataset."""


__author__ = 'Justin S Bayer, bayerj@in.tum.de'


from scipy import asmatrix, cov
from scipy.linalg import inv, eig
from numpy.random import standard_normal


def reduceDim(data, dim, func='pca'):
    """Reduce the dimension of datapoints to dim via principal component
    analysis.

    A matrix of shape (n, d) specifies n points of dimension d.
    """
    try:
        pcaFunc = globals()[func]
    except KeyError:
        raise ValueError('Unknown function to calc principal components')
    pc = pcaFunc(data, dim)
    return (pc * asmatrix(makeCentered(data)).T).T


def makeCentered(data):
    """Move the mean of the data matrix into the origin.

    Rows are perceived as datapoints.
    """
    return data - data.mean(axis=0)


def pca(data, dim):
    """ Return the first dim principal components as colums of a matrix.

    Every row of the matrix resembles a point in the data space.
    """

    assert dim <= data.shape[1], \
        "dim must be less or equal than the original dimension"

    # We have to make a copy of the original data and substract the mean
    # of every entry
    data = makeCentered(data)
    cm = cov(data.T)

    # OPT only calculate the dim first eigenvectors here
    # The following calculation may seem a bit "weird" but also correct to me.
    # The eigenvectors with the dim highest eigenvalues have to be selected
    # We keep track of the indexes via enumerate to restore the right ordering
    # later.
    eigval, eigvec = eig(cm)
    eigval = [(val, ind) for ind, val  in enumerate(eigval)]
    eigval.sort()
    eigval[:-dim] = []  # remove all but the highest dim elements

    # now we have to bring them back in the right order
    eig_indexes = [(ind, val) for val, ind in eigval]
    eig_indexes.sort(reverse=True)
    eig_indexes = [ind for ind, val in eig_indexes]

    return eigvec.take(eig_indexes, 1).T


def pPca(data, dim):
    """Return a matrix which contains the first `dim` dimensions principal
    components of data.

    data is a matrix which's rows correspond to datapoints. Implementation of
    the 'probabilistic PCA' algorithm.
    """
    num = data.shape[1]
    data = asmatrix(makeCentered(data))
    # Pick a random reduction
    W = asmatrix(standard_normal((num, dim)))
    # Save for convergence check
    W_ = W[:]
    while True:
        E = inv(W.T * W) * W.T * data.T
        W, W_ = data.T * E.T * inv(E * E.T), W
        if abs(W - W_).max() < 0.001:
            break
    return W.T

########NEW FILE########
__FILENAME__ = classification
__author__ = "Martin Felder, felder@in.tum.de"

from numpy import zeros, where, ravel, r_, single
from numpy.random import permutation
from pybrain.datasets import SupervisedDataSet, SequentialDataSet

class ClassificationDataSet(SupervisedDataSet):
    """ Specialized data set for classification data. Classes are to be numbered from 0 to nb_classes-1. """

    def __init__(self, inp, target=1, nb_classes=0, class_labels=None):
        """Initialize an empty dataset.

        `inp` is used to specify the dimensionality of the input. While the
        number of targets is given by implicitly by the training samples, it can
        also be set explicity by `nb_classes`. To give the classes names, supply
        an iterable of strings as `class_labels`."""
        # FIXME: hard to keep nClasses synchronized if appendLinked() etc. is used.
        SupervisedDataSet.__init__(self, inp, target)
        self.addField('class', 1)
        self.nClasses = nb_classes
        if len(self) > 0:
            # calculate class histogram, if we already have data
            self.calculateStatistics()
        self.convertField('target', int)
        if class_labels is None:
            self.class_labels = list(set(self.getField('target').flatten()))
        else:
            self.class_labels = class_labels
        # copy classes (may be changed into other representation)
        self.setField('class', self.getField('target'))


    @classmethod
    def load_matlab(cls, fname):
        """Create a dataset by reading a Matlab file containing one variable
        called 'data' which is an array of nSamples * nFeatures + 1 and
        contains the class in the first column."""
        from mlabwrap import mlab #@UnresolvedImport
        d = mlab.load(fname)
        return cls(d.data[:, 0], d.data[:, 1:])

    @classmethod
    def load_libsvm(cls, f):
        """Create a dataset by reading a sparse LIBSVM/SVMlight format file
        (with labels only)."""
        nFeat = 0
        # find max. number of features
        for line in f:
            n = int(line.split()[-1].split(':')[0])
            if n > nFeat:
                nFeat = n
        f.seek(0)
        labels = []
        features = []
        # read all data
        for line in f:
            # format is:
            # <class>  <featnr>:<featval>  <featnr>:<featval> ...
            # (whereby featnr starts at 1)
            if not line: break
            line = line.split()
            label = int(line[0])
            feat = []
            nextidx = 1
            for r in line[1:]:
                # construct list of features, taking care of sparsity
                (idx, val) = r.split(':')
                idx = int(idx)
                for _ in range(nextidx, idx):
                    feat.append(0.0)
                feat.append(float(val))
                nextidx = idx + 1
            for _ in range(nextidx, nFeat + 1):
                feat.append(0.0)
            features.append(feat[:])    # [:] causes copy
            labels.append([label])

        DS = cls(features, labels)
        return DS

    def __add__(self, other):
        """Adds the patterns of two datasets, if dimensions and type match."""
        if type(self) != type(other):
            raise TypeError('DataSets to be added must agree in type')
        elif self.indim != other.indim:
            raise TypeError('DataSets to be added must agree in input dimensions')
        elif self.outdim != 1 or other.outdim != 1:
            raise TypeError('Cannot add DataSets in 1-of-k representation')
        elif self.nClasses != other.nClasses:
            raise IndexError('Number of classes does not agree')
        else:
            result = self.copy()
            for pat in other:
                result.addSample(*pat)
            result.assignClasses()
        return result

    def assignClasses(self):
        """Ensure that the class field is properly defined and nClasses is set.
        """
        if len(self['class']) < len(self['target']):
            if self.outdim > 1:
                raise IndexError('Classes and 1-of-k representation out of sync!')
            else:
                self.setField('class', self.getField('target').astype(int))

        if self.nClasses <= 0:
            flat_labels = list(ravel(self['class']))
            classes = list(set(flat_labels))
            self.nClasses = len(classes)

    def calculateStatistics(self):
        """Return a class histogram."""
        self.assignClasses()
        self.classHist = {}
        flat_labels = list(ravel(self['class']))
        for class_ in range(self.nClasses):
            self.classHist[class_] = flat_labels.count(class_)
        return self.classHist

    def getClass(self, idx):
        """Return the label of given class."""
        try:
            return self.class_labels[idx]
        except IndexError:
            print("error: classes not defined yet!")

    def _convertToOneOfMany(self, bounds=(0, 1)):
        """Converts the target classes to a 1-of-k representation, retaining the
        old targets as a field `class`.

        To supply specific bounds, set the `bounds` parameter, which consists of
        target values for non-membership and membership."""
        if self.outdim != 1:
            # we already have the correct representation (hopefully...)
            return
        if self.nClasses <= 0:
            self.calculateStatistics()
        oldtarg = self.getField('target')
        newtarg = zeros([len(self), self.nClasses], dtype='Int32') + bounds[0]
        for i in range(len(self)):
            newtarg[i, int(oldtarg[i])] = bounds[1]
        self.setField('target', newtarg)
        self.setField('class', oldtarg)
        # probably better not to link field, otherwise there may be confusion
        # if getLinked() is called?
        ##self.linkFields(self.link.append('class'))

    def _convertToClassNb(self):
        """The reverse of _convertToOneOfMany. Target field is overwritten."""
        newtarg = self.getField('class')
        self.setField('target', newtarg)

    def __reduce__(self):
        _, _, state, _lst, _dct = super(ClassificationDataSet, self).__reduce__()
        creator = self.__class__
        args = self.indim, self.outdim, self.nClasses, self.class_labels
        return creator, args, state, iter([]), iter({})

    def splitByClass(self, cls_select):
        """Produce two new datasets, the first one comprising only the class
        selected (0..nClasses-1), the second one containing the remaining
        samples."""
        leftIndices, dummy = where(self['class'] == cls_select)
        rightIndices, dummy = where(self['class'] != cls_select)
        leftDs = self.copy()
        leftDs.clear()
        rightDs = leftDs.copy()
        # check which fields to split
        splitThis = []
        for f in ['input', 'target', 'class', 'importance', 'aux']:
            if self.hasField(f):
                splitThis.append(f)
        # need to synchronize input, target, and class fields
        for field in splitThis:
            leftDs.setField(field, self[field][leftIndices, :])
            leftDs.endmarker[field] = len(leftIndices)
            rightDs.setField(field, self[field][rightIndices, :])
            rightDs.endmarker[field] = len(rightIndices)
        leftDs.assignClasses()
        rightDs.assignClasses()
        return leftDs, rightDs

    def castToRegression(self, values):
        """Converts data set into a SupervisedDataSet for regression. Classes
        are used as indices into the value array given."""
        regDs = SupervisedDataSet(self.indim, 1)
        fields = self.getFieldNames()
        fields.remove('target')
        for f in fields:
            regDs.setField(f, self[f])
        regDs.setField('target', values[self['class'].astype(int)])
        return regDs


class SequenceClassificationDataSet(SequentialDataSet, ClassificationDataSet):
    """Defines a dataset for sequence classification. Each sample in the
    sequence still needs its own target value."""

    def __init__(self, inp, target, nb_classes=0, class_labels=None):
        """Initialize an empty dataset.

        `inp` is used to specify the dimensionality of the input. While the
        number of targets is given by implicitly by the training samples, it can
        also be set explicity by `nb_classes`. To give the classes names, supply
        an iterable of strings as `class_labels`."""
        # FIXME: hard to keep nClasses synchronized if appendLinked() etc. is used.
        SequentialDataSet.__init__(self, inp, target)
        # we want integer class numbers as targets
        self.convertField('target', int)
        if len(self) > 0:
            # calculate class histogram, if we already have data
            self.calculateStatistics()
        self.nClasses = nb_classes
        self.class_labels = range(self.nClasses) if class_labels is None else class_labels
        # copy classes (targets may be changed into other representation)
        self.setField('class', self.getField('target'))

    def __add__(self, other):
        """ NOT IMPLEMENTED """
        raise NotImplementedError

    def stratifiedSplit(self, testfrac=0.15, evalfrac=0):
        """Stratified random split of a sequence data set, i.e. (almost) same
        proportion of sequences in each class for all fragments. Return
        (training, test[, eval]) data sets.

        The parameter `testfrac` specifies the fraction of total sequences in
        the test dataset, while `evalfrac` specifies the fraction of sequences
        in the validation dataset. If `evalfrac` equals 0, no validationset is
        returned.

        It is assumed that the last target for each class is the class of the
        sequence. Also mind that the data will be sorted by class in the
        resulting data sets."""
        lastidx = ravel(self['sequence_index'][1:] - 1).astype(int)
        classes = ravel(self['class'][lastidx])
        trnDs = self.copy()
        trnDs.clear()
        tstDs = trnDs.copy()
        valDs = trnDs.copy()
        for c in range(self.nClasses):
            # scramble available sequences for current class
            idx, = where(classes == c)
            nCls = len(idx)
            perm = permutation(nCls).tolist()
            nTst, nVal = (int(testfrac * nCls), int(evalfrac * nCls))
            for count, ds in zip([nTst, nVal, nCls - nTst - nVal], [tstDs, valDs, trnDs]):
                for _ in range(count):
                    feat = self.getSequence(idx[perm.pop()])[0]
                    ds.newSequence()
                    for s in feat:
                        ds.addSample(s, [c])
                ds.assignClasses()
            assert perm == []
        if len(valDs) > 0:
            return trnDs, tstDs, valDs
        else:
            return trnDs, tstDs

    def getSequenceClass(self, index=None):
        """Return a flat array (or single scalar) comprising one class per
        sequence as given by last pattern in each sequence."""
        lastSeq = self.getNumSequences() - 1
        if index is None:
            classidx = r_[self['sequence_index'].astype(int)[1:, 0] - 1, len(self) - 1]
            return self['class'][classidx, 0]
        else:
            if index < lastSeq:
                return self['class'][self['sequence_index'].astype(int)[index + 1, 0] - 1, 0]
            elif index == lastSeq:
                return self['class'][len(self) - 1, 0]
            raise IndexError("Sequence index out of range!")

    def removeSequence(self, index):
        """Remove sequence (including class field) from the dataset."""
        self.assignClasses()
        self.linkFields(['input', 'target', 'class'])
        SequentialDataSet.removeSequence(self, index)
        self.unlinkFields(['class'])

    def save_netcdf(self, flo, **kwargs):
        """Save the current dataset to the given file as a netCDF dataset to be
        used with Alex Graves nnl_ndim program in
        task="sequence classification" mode."""
        # make sure classes are defined properly
        assert len(self['class']) == len(self['target'])
        if self.nClasses > 10:
            raise
        from pycdf import CDF, NC

        # need to regenerate the file name
        filename = flo.name
        flo.close()

        # Create the file. Raise the automode flag, so that
        # we do not need to worry about setting the define/data mode.
        d = CDF(filename, NC.WRITE | NC.CREATE | NC.TRUNC)
        d.automode()

        # Create 2 global attributes, one holding a string,
        # and the other one 2 floats.
        d.title = 'Sequential data exported from PyBrain (www.pybrain.org)'

        # create the dimensions
        dimsize = { 'numTimesteps':        len(self),
                    'inputPattSize':       self.indim,
                    'numLabels':           self.nClasses,
                    'numSeqs':             self.getNumSequences(),
                    'maxLabelLength':      2 }
        dims = {}
        for name, sz in dimsize.iteritems():
            dims[name] = d.def_dim(name, sz)

        # Create a netCDF record variables
        inputs = d.def_var('inputs', NC.FLOAT, (dims['numTimesteps'], dims['inputPattSize']))
        targetStrings = d.def_var('targetStrings', NC.CHAR, (dims['numSeqs'], dims['maxLabelLength']))
        seqLengths = d.def_var('seqLengths', NC.INT, (dims['numSeqs']))
        labels = d.def_var('labels', NC.CHAR, (dims['numLabels'], dims['maxLabelLength']))

        # Switch to data mode (automatic)

        # assign float and integer arrays directly
        inputs.put(self['input'].astype(single))
        # strings must be written as scalars (sucks!)
        for i in range(dimsize['numSeqs']):
            targetStrings.put_1(i, str(self.getSequenceClass(i)))
        for i in range(self.nClasses):
            labels.put_1(i, str(i))
        # need colon syntax for assigning list
        seqLengths[:] = [self.getSequenceLength(i) for i in range(self.getNumSequences())]

        # Close file
        print("wrote netCDF file " + filename)
        d.close()


if __name__ == "__main__":
    dataset = ClassificationDataSet(2, 1, class_labels=['Urd', 'Verdandi', 'Skuld'])
    dataset.appendLinked([ 0.1, 0.5 ]   , [0])
    dataset.appendLinked([ 1.2, 1.2 ]   , [1])
    dataset.appendLinked([ 1.4, 1.6 ]   , [1])
    dataset.appendLinked([ 1.6, 1.8 ]   , [1])
    dataset.appendLinked([ 0.10, 0.80 ] , [2])
    dataset.appendLinked([ 0.20, 0.90 ] , [2])

    dataset.calculateStatistics()
    print("class histogram:", dataset.classHist)
    print("# of classes:", dataset.nClasses)
    print("class 1 is: ", dataset.getClass(1))
    print("targets: ", dataset.getField('target'))
    dataset._convertToOneOfMany(bounds=[0, 1])
    print("converted targets: ")
    print(dataset.getField('target'))
    dataset._convertToClassNb()
    print("reconverted to original:", dataset.getField('target'))




########NEW FILE########
__FILENAME__ = dataset
from __future__ import with_statement

__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

import random
import pickle
from itertools import chain
from scipy import zeros, resize, ravel, asarray
import scipy

from pybrain.utilities import Serializable


class OutOfSyncError(Exception): pass
class VectorFormatError(Exception): pass
class NoLinkedFieldsError(Exception): pass


class DataSet(Serializable):
    """DataSet is a general base class for other data set classes
    (e.g. SupervisedDataSet, SequentialDataSet, ...). It consists of several
    fields. A field is a NumPy array with a label (a string) attached to it.
    Fields can be linked together which means they must have the same length."""

    def __init__(self):
        self.data = {}
        self.endmarker = {}
        self.link = []
        self.index = 0

        # row vectors returned by getLinked can have different formats:
        # '1d'       example: array([1, 2, 3])
        # '2d'       example: array([[1, 2, 3]])
        # 'list'     example: [1, 2, 3]
        self.vectorformat = 'none'

    def __str__(self):
        """Return a string representation of a dataset."""
        s = ""
        for key in self.data:
            s = s + key + ": dim" + str(self.data[key].shape) + "\n" + str(self.data[key][:self.endmarker[key]]) + "\n\n"
        return s

    def __getitem__(self, field):
        """Return the given field."""
        return self.getField(field)

    def __iter__(self):
        self.reset()
        while not self.endOfData():
            yield self.getLinked()

    def getVectorFormat(self):
        """Returns the current vector format."""
        return self.__vectorformat

    def setVectorFormat(self, vf):
        """Determine which format to use for returning vectors. Use the property vectorformat.

            :key type: possible types are '1d', '2d', 'list'
                  '1d' - example: array([1,2,3])
                  '2d' - example: array([[1,2,3]])
                'list' - example: [1,2,3]
                'none' - no conversion
         """
        switch = {
            '1d': self._convertArray1d,
            '2d': self._convertArray2d,
            'list': self._convertList,
            'none': lambda x:x
        }
        try:
            self._convert = switch[vf]
            self.__vectorformat = vf
        except KeyError:
            raise VectorFormatError("vector format must be one of '1d', '2d', 'list'. given: %s" % vf)

    vectorformat = property(getVectorFormat, setVectorFormat, None, "vectorformat can be '1d', '2d' or 'list'")

    def _convertList(self, vector):
        """Converts the incoming vector to a python list."""
        return ravel(vector).tolist()

    def _convertArray1d(self, vector):
        """Converts the incoming vector to a 1d vector with shape (x,) where x
        is the number of elements."""
        return ravel(vector)

    def _convertArray2d(self, vector, column=False):
        """Converts the incoming `vector` to a 2d vector with shape (1,x), or
        (x,1) if `column` is set, where x is the number of elements."""
        a = asarray(vector)
        sh = a.shape
        # also reshape scalar values to 2d-index
        if len(sh) == 0:
            sh = (1,)
        if len(sh) == 1:
            # use reshape to add extra dimension
            if column:
                return a.reshape((sh[0], 1))
            else:
                return a.reshape((1, sh[0]))
        else:
            # vector is not 1d, return a without change
            return a

    def addField(self, label, dim):
        """Add a field to the dataset.

        A field consists of a string `label`  and a numpy ndarray of dimension
        `dim`."""
        self.data[label] = zeros((0, dim), float)
        self.endmarker[label] = 0

    def setField(self, label, arr):
        """Set the given array `arr` as the new array of field `label`,"""
        as_arr = asarray(arr)
        self.data[label] = as_arr
        self.endmarker[label] = as_arr.shape[0]

    def linkFields(self, linklist):
        """Link the length of several fields given by the list of strings
        `linklist`."""
        length = self[linklist[0]].shape[0]
        for l in linklist:
            if self[l].shape[0] != length:
                raise OutOfSyncError
        self.link = linklist

    def unlinkFields(self, unlinklist=None):
        """Remove fields from the link list or clears link given by the list of
        string `linklist`.

        This method has no effect if fields are not linked."""
        link = self.link
        if unlinklist is not None:
            for l in unlinklist:
                if l in self.link:
                    link.remove(l)
            self.link = link
        else:
            self.link = []

    def getDimension(self, label):
        """Return the dimension/number of columns for the field given by
        `label`."""
        try:
            dim = self.data[label].shape[1]
        except KeyError:
            raise KeyError('dataset field %s not found.' % label)
        return dim

    def __len__(self):
        """Return the length of the linked data fields. If no linked fields exist,
        return the length of the longest field."""
        return self.getLength()

    def getLength(self):
        """Return the length of the linked data fields. If no linked fields exist,
        return the length of the longest field."""
        if self.link == []:
            try:
                length = self.endmarker[max(self.endmarker)]
            except ValueError:
                return 0
            return length
        else:
            # all linked fields have equal length. return the length of the first.
            l = self.link[0]
            return self.endmarker[l]

    def _resize(self, label=None):
        if label:
            label = [label]
        elif self.link:
            label = self.link
        else:
            label = self.data

        for l in label:
            self.data[l] = self._resizeArray(self.data[l])

    def _resizeArray(self, a):
        """Increase the buffer size. It should always be one longer than the
        current sequence length and double on every growth step."""
        shape = list(a.shape)
        shape[0] = (shape[0] + 1) * 2
        return resize(a, shape)

    def _appendUnlinked(self, label, row):
        """Append `row` to the field array with the given `label`.

        Do not call this function from outside, use ,append() instead.
        Automatically casts vector to a 2d (or higher) shape."""
        if self.data[label].shape[0] <= self.endmarker[label]:
            self._resize(label)

        self.data[label][self.endmarker[label], :] = row
        self.endmarker[label] += 1

    def append(self, label, row):
        """Append `row` to the array given by `label`.

        If the field is linked with others, the function throws an
        `OutOfSyncError` because all linked fields always have to have the same
        length. If you want to add a row to all linked fields, use appendLink
        instead."""
        if label in self.link:
            raise OutOfSyncError
        self._appendUnlinked(label, row)

    def appendLinked(self, *args):
        """Add rows to all linked fields at once."""
        assert len(args) == len(self.link)
        for i, l in enumerate(self.link):
            self._appendUnlinked(l, args[i])

    def getLinked(self, index=None):
        """Access the dataset randomly or sequential.

        If called with `index`, the appropriate line consisting of all linked
        fields is returned and the internal marker is set to the next line.
        Otherwise the marked line is returned and the marker is moved to the
        next line."""
        if self.link == []:
            raise NoLinkedFieldsError('The dataset does not have any linked fields.')

        if index == None:
            # no index given, return the currently marked line and step marker one line forward
            index = self.index
            self.index += 1
        else:
            # return the indexed line and move marker to next line
            self.index = index + 1
        if index >= self.getLength():
            raise IndexError('index out of bounds of the dataset.')

        return [self._convert(self.data[l][index]) for l in self.link]

    def getField(self, label):
        """Return the entire field given by `label` as an array or list,
        depending on user settings."""
        # Note: label_data should always be a np.array, so this will never
        # actually clone a list (performances are O(1)).
        label_data = self.data[label][:self.endmarker[label]]

        # Convert to list if requested.
        if self.vectorformat == 'list':
            label_data = label_data.tolist()

        return label_data

    def hasField(self, label):
        """Tell whether the field given by `label` exists."""
        return self.data.has_key(label)

    def getFieldNames(self):
        """Return the names of the currently defined fields."""
        return self.data.keys()

    def convertField(self, label, newtype):
        """Convert the given field to a different data type."""
        try:
            self.setField(label, self.data[label].astype(newtype))
        except KeyError:
            raise KeyError('convertField: dataset field %s not found.' % label)

    def endOfData(self):
        """Tell if the end of the data set is reached."""
        return self.index == self.getLength()

    def reset(self):
        """Reset the marker to the first line."""
        self.index = 0

    def clear(self, unlinked=False):
        """Clear the dataset.

        If linked fields exist, only the linked fields will be deleted unless
        `unlinked` is set to True. If no fields are linked, all data will be
        deleted."""
        self.reset()
        keys = self.link
        if keys == [] or unlinked:
            # iterate over all fields instead
            keys = self.data

        for k in keys:
            shape = list(self.data[k].shape)
            # set to zero rows
            shape[0] = 0
            self.data[k] = zeros(shape)
            self.endmarker[k] = 0

    @classmethod
    def reconstruct(cls, filename):
        """Read an incomplete data set (option arraysonly) into the given one. """
        # FIXME: Obsolete! Kept here because of some old files...
        obj = cls(1, 1)
        for key, val in pickle.load(file(filename)).iteritems():
            obj.setField(key, val)
        return obj

    def save_pickle(self, flo, protocol=0, compact=False):
        """Save data set as pickle, removing empty space if desired."""
        if compact:
            # remove padding of zeros for each field
            for field in self.getFieldNames():
                temp = self[field][0:self.endmarker[field] + 1, :]
                self.setField(field, temp)
        Serializable.save_pickle(self, flo, protocol)

    def __reduce__(self):
        def creator():
            obj = self.__class__()
            obj.vectorformat = self.vectorformat
            return obj
        args = tuple()
        state = {
            'data': self.data,
            'link': self.link,
            'endmarker': self.endmarker,
        }
        return creator, args, state, iter([]), iter({})

    def copy(self):
        """Return a deep copy."""
        import copy
        return copy.deepcopy(self)

    def batches(self, label, n, permutation=None):
        """Yield batches of the size of n from the dataset.

        A single batch is an array of with dim columns and n rows. The last
        batch is possibly smaller.

        If permutation is given, batches are yielded in the corresponding
        order."""
        # First calculate how many batches we will have
        full_batches, rest = divmod(len(self), n)
        number_of_batches = full_batches if rest == 0 else full_batches + 1

        # We make one iterator for the startindexes ...
        startindexes = (i * n for i in xrange(number_of_batches))
        # ... and one for the stop indexes
        stopindexes = (((i + 1) * n) for i in xrange(number_of_batches - 1))
        # The last stop index is the last element of the list (last batch
        # might not be filled completely)
        stopindexes = chain(stopindexes, [len(self)])
        # Now combine them
        indexes = zip(startindexes, stopindexes)

        # Shuffle them according to the permutation if one is given
        if permutation is not None:
            indexes = [indexes[i] for i in permutation]

        for start, stop in indexes:
            yield self.data[label][start:stop]

    def randomBatches(self, label, n):
        """Like .batches(), but the order is random."""
        permutation = random.shuffle(range(len(self)))
        return self.batches(label, n, permutation)

    def replaceNansByMeans(self):
        """Replace all not-a-number entries in the dataset by the means of the
        corresponding column."""
        for d in self.data.itervalues():
            means = scipy.nansum(d[:self.getLength()], axis=0) / self.getLength()
            for i in xrange(self.getLength()):
                for j in xrange(d.dim):
                    if not scipy.isfinite(d[i, j]):
                        d[i, j] = means[j]

########NEW FILE########
__FILENAME__ = importance
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import ones, dot

from pybrain.datasets.sequential import SequentialDataSet
from pybrain.utilities import fListToString


# CHECKME: does this provide for importance-datasets in the non-sequential case
# maybe there should be a second class - or another structure!


class ImportanceDataSet(SequentialDataSet):
    """ Allows setting an importance value for each of the targets of a sample. """

    def __init__(self, indim, targetdim):
        SequentialDataSet.__init__(self, indim, targetdim)
        self.addField('importance', targetdim)
        self.link.append('importance')

    def addSample(self, inp, target, importance=None):
        """ adds a new sample consisting of input, target and importance.

            :arg inp: the input of the sample
            :arg target: the target of the sample
            :key importance: the importance of the sample. If left None, the
                 importance will be set to 1.0
        """
        if importance == None:
            importance = ones(len(target))
        self.appendLinked(inp, target, importance)

    def _evaluateSequence(self, f, seq, verbose = False):
        """ return the importance-ponderated MSE over one sequence. """
        totalError = 0
        ponderation = 0.
        for input, target, importance in seq:
            res = f(input)
            e = 0.5 * dot(importance.flatten(), ((target-res).flatten()**2))
            totalError += e
            ponderation += sum(importance)
            if verbose:
                print(    'out:       ', fListToString(list(res)))
                print(    'correct:   ', fListToString(target))
                print(    'importance:', fListToString(importance))
                print(    'error: % .8f' % e)
        return totalError, ponderation


########NEW FILE########
__FILENAME__ = reinforcement
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from pybrain.datasets.sequential import SequentialDataSet
from pybrain.datasets.dataset import DataSet
from scipy import zeros


class ReinforcementDataSet(SequentialDataSet):
    def __init__(self, statedim, actiondim):
        """ initialize the reinforcement dataset, add the 3 fields state, action and
            reward, and create an index marker. This class is basically a wrapper function
            that renames the fields of SupervisedDataSet into the more common reinforcement
            learning names. Instead of 'episodes' though, we deal with 'sequences' here. """
        DataSet.__init__(self)
        # add 3 fields: input, target, importance
        self.addField('state', statedim)
        self.addField('action', actiondim)
        self.addField('reward', 1)
        # link these 3 fields
        self.linkFields(['state', 'action', 'reward'])
        # reset the index marker
        self.index = 0
        # add field that stores the beginning of a new episode
        self.addField('sequence_index', 1)
        self.append('sequence_index', 0)
        self.currentSeq = 0
        self.statedim = statedim
        self.actiondim = actiondim

        # the input and target dimensions (for compatibility)
        self.indim = self.statedim
        self.outdim = self.actiondim

    def addSample(self, state, action, reward):
        """ adds a new sample consisting of state, action, reward.

            :key state: the current state of the world
            :key action: the executed action by the agent
            :key reward: the reward received for action in state """
        self.appendLinked(state, action, reward)

    def getSumOverSequences(self, field):
        sums = zeros((self.getNumSequences(), self.getDimension(field)))
        for n in range(self.getNumSequences()):
            sums[n, :] = sum(self._getSequenceField(n, field), 0)
        return sums

    def __reduce__(self):
        # FIXME: This does actually not feel right: We have to use the DataSet
        # method here, although we inherit from sequential dataset.
        _, _, state, _, _ = DataSet.__reduce__(self)
        creator = self.__class__
        args = self.statedim, self.actiondim
        return creator, args, state, iter([]), iter({})



########NEW FILE########
__FILENAME__ = sequential
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'
# $Id$

from itertools import izip
from scipy import ravel, r_
from random import sample

from pybrain.datasets.supervised import SupervisedDataSet


class EmptySequenceError(Exception): pass


class SequentialDataSet(SupervisedDataSet):
    """A SequentialDataSet is like a SupervisedDataSet except that it can keep
    track of sequences of samples. Indices of a new sequence are stored whenever
    the method newSequence() is called. The last (open) sequence is considered
    a normal sequence even though it does not have a following "new sequence"
    marker."""

    def __init__(self, indim, targetdim):
        SupervisedDataSet.__init__(self, indim, targetdim)
        # add field that stores the beginning of a new episode
        self.addField('sequence_index', 1)
        self.append('sequence_index', 0)
        self.currentSeq = 0

    def newSequence(self):
        """Marks the beginning of a new sequence. this function does nothing if
        called at the very start of the data set. Otherwise, it starts a new
        sequence. Empty sequences are not allowed, and an EmptySequenceError
        exception will be raised."""
        length = self.getLength()
        if length != 0:
            if ravel(self.getField('sequence_index'))[-1] == length:
                raise EmptySequenceError
            self._appendUnlinked('sequence_index', length)

    def _getSequenceField(self, index, field):
        """Return a sequence of one single field given by `field` and indexed by
        `index`."""
        seq = ravel(self.getField('sequence_index'))
        if len(seq) == index + 1:
            # user wants to access the last sequence, return until end of data
            return self.getField(field)[seq[index]:]
        if len(seq) < index + 1:
            # sequence index beyond number of sequences. raise exception
            raise IndexError('sequence does not exist.')
        return self.getField(field)[seq[index]:seq[index + 1]]

    def getSequence(self, index):
        """Returns the sequence given by `index`.

        A list of arrays is returned for the linked arrays. It is assumed that
        the last sequence goes until the end of the dataset."""
        return [self._getSequenceField(index, l) for l in self.link]

    def getSequenceIterator(self, index):
        """Return an iterator over the samples of the sequence specified by
        `index`.

        Each element is a tuple."""
        return izip(*self.getSequence(index))

    def endOfSequence(self, index):
        """Return True if the marker was moved over the last element of
        sequence `index`, False otherwise.

        Mostly used like .endOfData() with while loops."""
        seq = ravel(self.getField('sequence_index'))
        if len(seq) == index + 1:
            # user wants to access the last sequence, return until end of data
            return self.endOfData()
        if len(seq) < index + 1:
            # sequence index beyond number of sequences. raise exception
            raise IndexError('sequence does not exist.')
        else:
            return self.index >= seq[index + 1]

    def gotoSequence(self, index):
        """Move the internal marker to the beginning of sequence `index`."""
        try:
            self.index = ravel(self.getField('sequence_index'))[index]
        except IndexError:
            raise IndexError('sequence does not exist')

    def getCurrentSequence(self):
        """Return the current sequence, according to the marker position."""
        seq = ravel(self.getField('sequence_index'))
        return len(seq) - sum(seq > self.index) - 1

    def getNumSequences(self):
        """Return the number of sequences. The last (open) sequence is also
        counted in, even though there is no additional 'newSequence' marker."""
        return self.getField('sequence_index').shape[0]

    def getSequenceLength(self, index):
        """Return the length of the given sequence. If `index` is pointing
        to the last sequence, the sequence is considered to go until the end
        of the dataset."""
        seq = ravel(self.getField('sequence_index'))
        if len(seq) == index + 1:
            # user wants to access the last sequence, return until end of data
            return int(self.getLength() - seq[index])
        if len(seq) < index + 1:
            # sequence index beyond number of sequences. raise exception
            raise IndexError('sequence does not exist.')
        return int(seq[index + 1] - seq[index])

    def removeSequence(self, index):
        """Remove the `index`'th sequence from the dataset and places the
        marker to the sample following the removed sequence."""
        if index >= self.getNumSequences():
            # sequence doesn't exist, raise exception
            raise IndexError('sequence does not exist.')
        sequences = ravel(self.getField('sequence_index'))
        seqstart = sequences[index]
        if index == self.getNumSequences() - 1:
            # last sequence is going to be removed
            lastSeqDeleted = True
            seqend = self.getLength()
        else:
            lastSeqDeleted = False
            # sequence to remove is not last one (sequence_index exists)
            seqend = sequences[index + 1]

        # cut out data from all fields
        for label in self.link:
            # concatenate rows from start to seqstart and from seqend to end
            self.data[label] = r_[self.data[label][:seqstart, :], self.data[label][seqend:, :]]
            # update endmarkers of linked fields
            self.endmarker[label] -= seqend - seqstart

        # update sequence indices
        for i, val in enumerate(sequences):
            if val > seqstart:
                self.data['sequence_index'][i, :] -= seqend - seqstart

        # remove sequence index of deleted sequence and reduce its endmarker
        self.data['sequence_index'] = r_[self.data['sequence_index'][:index, :], self.data['sequence_index'][index + 1:, :]]
        self.endmarker['sequence_index'] -= 1

        if lastSeqDeleted:
            # last sequence was removed
            # move sequence marker to last remaining sequence
            self.currentSeq = index - 1
            # move sample marker to end of dataset
            self.index = self.getLength()
            # if there was only 1 sequence left, re-initialize sequence index
            if self.getLength() == 0:
                self.clear()
        else:
            # removed sequence was not last one (sequence_index exists)
            # move sequence marker to the new sequence at position 'index'
            self.currentSeq = index
            # move sample marker to beginning of sequence at position 'index'
            self.index = ravel(self.getField('sequence_index'))[index]


    def clear(self):
        SupervisedDataSet.clear(self, True)
        self._appendUnlinked('sequence_index', [0])
        self.currentSeq = 0

    def __iter__(self):
        """Create an iterator object over sequences which are themselves
        iterable objects."""
        for i in range(self.getNumSequences()):
            yield self.getSequenceIterator(i)

    def _provideSequences(self):
        """Return an iterator over sequence lists."""
        return iter(map(list, iter(self)))

    def evaluateModuleMSE(self, module, averageOver=1, **args):
        """Evaluate the predictions of a module on a sequential dataset
        and return the MSE (potentially average over a number of epochs)."""
        res = 0.
        for dummy in range(averageOver):
            ponderation = 0.
            totalError = 0
            for seq in self._provideSequences():
                module.reset()
                e, p = self._evaluateSequence(module.activate, seq, **args)
                totalError += e
                ponderation += p
            assert ponderation > 0
            res += totalError / ponderation
        return res / averageOver

    def splitWithProportion(self, proportion=0.5):
        """Produce two new datasets, each containing a part of the sequences.

        The first dataset will have a fraction given by `proportion` of the
        dataset."""
        l = self.getNumSequences()
        leftIndices = sample(range(l), int(l * proportion))
        leftDs = self.copy()
        leftDs.clear()
        rightDs = leftDs.copy()
        index = 0
        for seq in iter(self):
            if index in leftIndices:
                leftDs.newSequence()
                for sp in seq:
                    leftDs.addSample(*sp)
            else:
                rightDs.newSequence()
                for sp in seq:
                    rightDs.addSample(*sp)
            index += 1
        return leftDs, rightDs


########NEW FILE########
__FILENAME__ = supervised
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from random import sample
from scipy import isscalar

from pybrain.datasets.dataset import DataSet
from pybrain.utilities import fListToString


class SupervisedDataSet(DataSet):
    """SupervisedDataSets have two fields, one for input and one for the target.
    """

    def __init__(self, inp, target):
        """Initialize an empty supervised dataset.

        Pass `inp` and `target` to specify the dimensions of the input and
        target vectors."""
        DataSet.__init__(self)
        if isscalar(inp):
            # add input and target fields and link them
            self.addField('input', inp)
            self.addField('target', target)
        else:
            self.setField('input', inp)
            self.setField('target', target)

        self.linkFields(['input', 'target'])

        # reset the index marker
        self.index = 0

        # the input and target dimensions
        self.indim = self.getDimension('input')
        self.outdim = self.getDimension('target')

    def __reduce__(self):
        _, _, state, _, _ = super(SupervisedDataSet, self).__reduce__()
        creator = self.__class__
        args = self.indim, self.outdim
        return creator, args, state, iter([]), iter({})

    def addSample(self, inp, target):
        """Add a new sample consisting of `input` and `target`."""
        self.appendLinked(inp, target)

    def getSample(self, index=None):
        """Return a sample at `index` or the current sample."""
        return self.getLinked(index)

    def setField(self, label, arr, **kwargs):
        """Set the given array `arr` as the new array of the field specfied by
        `label`."""
        DataSet.setField(self, label, arr, **kwargs)
        # refresh dimensions, in case any of these fields were modified
        if label == 'input':
            self.indim = self.getDimension('input')
        elif label == 'target':
            self.outdim = self.getDimension('target')

    def _provideSequences(self):
        """Return an iterator over sequence lists, although the dataset contains
        only single samples."""
        return iter(map(lambda x: [x], iter(self)))

    def evaluateMSE(self, f, **args):
        """Evaluate the predictions of a function on the dataset and return the
        Mean Squared Error, incorporating importance."""
        ponderation = 0.
        totalError = 0
        for seq in self._provideSequences():
            e, p = self._evaluateSequence(f, seq, **args)
            totalError += e
            ponderation += p
        assert ponderation > 0
        return totalError/ponderation

    def _evaluateSequence(self, f, seq, verbose = False):
        """Return the ponderated MSE over one sequence."""
        totalError = 0.
        ponderation = 0.
        for input, target in seq:
            res = f(input)
            e = 0.5 * sum((target-res).flatten()**2)
            totalError += e
            ponderation += len(target)
            if verbose:
                print(    'out:    ', fListToString( list( res ) ))
                print(    'correct:', fListToString( target ))
                print(    'error: % .8f' % e)
        return totalError, ponderation

    def evaluateModuleMSE(self, module, averageOver = 1, **args):
        """Evaluate the predictions of a module on a dataset and return the MSE
        (potentially average over a number of epochs)."""
        res = 0.
        for dummy in range(averageOver):
            module.reset()
            res += self.evaluateMSE(module.activate, **args)
        return res/averageOver

    def splitWithProportion(self, proportion = 0.5):
        """Produce two new datasets, the first one containing the fraction given
        by `proportion` of the samples."""
        leftIndices = set(sample(range(len(self)), int(len(self)*proportion)))
        leftDs = self.copy()
        leftDs.clear()
        rightDs = leftDs.copy()
        index = 0
        for sp in self:
            if index in leftIndices:
                leftDs.addSample(*sp)
            else:
                rightDs.addSample(*sp)
            index += 1
        return leftDs, rightDs


########NEW FILE########
__FILENAME__ = unsupervised
# -*- coding: utf-8 -*-

__author__ = 'Justin S Bayer, bayer.justin@googlemail.com'
__version__ = '$Id$'


from pybrain.datasets.dataset import DataSet


class UnsupervisedDataSet(DataSet):
    """UnsupervisedDataSets have a single field 'sample'."""

    def __init__(self, dim):
        """Initialize an empty unsupervised dataset.

        Pass `dim` to specify the dimensionality of the samples."""
        super(UnsupervisedDataSet, self).__init__()
        self.addField('sample', dim)
        self.linkFields(['sample'])
        self.dim = dim

        # reset the index marker
        self.index = 0

    def __reduce__(self):
        _, _, state, _, _ = super(UnsupervisedDataSet, self).__reduce__()
        creator = self.__class__
        args = (self.dim,)
        return creator, args, state, iter([]), iter({})

    def addSample(self, sample):
        self.appendLinked(sample)

    def getSample(self, index):
        return self.getLinked(index)

########NEW FILE########
__FILENAME__ = cmaes
__author__ = 'Tom Schaul, tom@idsia.ch; Sun Yi, yi@idsia.ch'

from numpy import floor, log, eye, zeros, array, sqrt, sum, dot, tile, outer, real
from numpy import exp, diag, power, ravel
from numpy.linalg import eig, norm
from numpy.random import randn

from pybrain.optimization.optimizer import ContinuousOptimizer


class CMAES(ContinuousOptimizer):
    """ CMA-ES: Evolution Strategy with Covariance Matrix Adaptation for
    nonlinear function minimization.
    This code is a close transcription of the provided matlab code.
    """

    mustMinimize = True
    stopPrecision = 1e-6

    storeAllCenters = False
    initStepSize = 0.5

    def _additionalInit(self):
        self.center = self._initEvaluable
        self.stepSize = self.initStepSize  # coordinate wise standard deviation (sigma)
        if self.storeAllCenters:
            self._allCenters = []

        # Strategy parameter setting: Selection
        # population size, offspring number
        self.mu = int(floor(self.batchSize / 2))        # number of parents/points for recombination
        self.weights = log(self.mu + 1) - log(array(xrange(1, self.mu + 1)))      # use array
        self.weights /= sum(self.weights)     # normalize recombination weights array
        self.muEff = sum(self.weights) ** 2 / sum(power(self.weights, 2)) # variance-effective size of mu

        # Strategy parameter setting: Adaptation
        self.cumCov = 4 / float(self.numParameters + 4)                    # time constant for cumulation for covariance matrix
        self.cumStep = (self.muEff + 2) / (self.numParameters + self.muEff + 3)# t-const for cumulation for Size control
        self.muCov = self.muEff                   # size of mu used for calculating learning rate covLearningRate
        self.covLearningRate = ((1 / self.muCov) * 2 / (self.numParameters + 1.4) ** 2 + (1 - 1 / self.muCov) * # learning rate for
                 ((2 * self.muEff - 1) / ((self.numParameters + 2) ** 2 + 2 * self.muEff)))                       # covariance matrix
        self.dampings = 1 + 2 * max(0, sqrt((self.muEff - 1) / (self.numParameters + 1)) - 1) + self.cumStep
        # damping for stepSize usually close to 1 former damp == self.dampings/self.cumStep

        # Initialize dynamic (internal) strategy parameters and constants
        self.covPath = zeros(self.numParameters)
        self.stepPath = zeros(self.numParameters)                   # evolution paths for C and stepSize
        self.B = eye(self.numParameters, self.numParameters)         # B defines the coordinate system
        self.D = eye(self.numParameters, self.numParameters)         # diagonal matrix D defines the scaling
        self.C = dot(dot(self.B, self.D), dot(self.B, self.D).T)       # covariance matrix
        self.chiN = self.numParameters ** 0.5 * (1 - 1. / (4. * self.numParameters) + 1 / (21. * self.numParameters ** 2))
        # expectation of ||numParameters(0,I)|| == norm(randn(numParameters,1))

    def _learnStep(self):
        # Generate and evaluate lambda offspring
        arz = randn(self.numParameters, self.batchSize)
        arx = tile(self.center.reshape(self.numParameters, 1), (1, self.batchSize))\
                        + self.stepSize * dot(dot(self.B, self.D), arz)
        arfitness = zeros(self.batchSize)
        for k in xrange(self.batchSize):
            arfitness[k] = self._oneEvaluation(arx[:, k])

        # Sort by fitness and compute weighted mean into center
        arfitness, arindex = sorti(arfitness)  # minimization
        arz = arz[:, arindex]
        arx = arx[:, arindex]
        arzsel = arz[:, xrange(self.mu)]
        arxsel = arx[:, xrange(self.mu)]
        arxmut = arxsel - tile(self.center.reshape(self.numParameters, 1), (1, self.mu))

        zmean = dot(arzsel, self.weights)
        self.center = dot(arxsel, self.weights)

        if self.storeAllCenters:
            self._allCenters.append(self.center)

        # Cumulation: Update evolution paths
        self.stepPath = (1 - self.cumStep) * self.stepPath \
                + sqrt(self.cumStep * (2 - self.cumStep) * self.muEff) * dot(self.B, zmean)         # Eq. (4)
        hsig = norm(self.stepPath) / sqrt(1 - (1 - self.cumStep) ** (2 * self.numEvaluations / float(self.batchSize))) / self.chiN \
                    < 1.4 + 2. / (self.numParameters + 1)
        self.covPath = (1 - self.cumCov) * self.covPath + hsig * \
                sqrt(self.cumCov * (2 - self.cumCov) * self.muEff) * dot(dot(self.B, self.D), zmean) # Eq. (2)

        # Adapt covariance matrix C
        self.C = ((1 - self.covLearningRate) * self.C                    # regard old matrix   % Eq. (3)
             + self.covLearningRate * (1 / self.muCov) * (outer(self.covPath, self.covPath) # plus rank one update
                                   + (1 - hsig) * self.cumCov * (2 - self.cumCov) * self.C)
             + self.covLearningRate * (1 - 1 / self.muCov)                 # plus rank mu update
             * dot(dot(arxmut, diag(self.weights)), arxmut.T)
            )

        # Adapt step size self.stepSize
        self.stepSize *= exp((self.cumStep / self.dampings) * (norm(self.stepPath) / self.chiN - 1)) # Eq. (5)

        # Update B and D from C
        # This is O(n^3). When strategy internal CPU-time is critical, the
        # next three lines should be executed only every (alpha/covLearningRate/N)-th
        # iteration, where alpha is e.g. between 0.1 and 10
        self.C = (self.C + self.C.T) / 2 # enforce symmetry
        Ev, self.B = eig(self.C)          # eigen decomposition, B==normalized eigenvectors
        Ev = real(Ev)       # enforce real value
        self.D = diag(sqrt(Ev))      #diag(ravel(sqrt(Ev))) # D contains standard deviations now
        self.B = real(self.B)

        # convergence is reached
        if arfitness[0] == arfitness[-1] or (abs(arfitness[0] - arfitness[-1]) /
                                             (abs(arfitness[0]) + abs(arfitness[-1]))) <= self.stopPrecision:
            if self.verbose:
                print("Converged.")
            self.maxLearningSteps = self.numLearningSteps

        # or diverged, unfortunately
        if min(Ev) > 1e5:
            if self.verbose:
                print("Diverged.")
            self.maxLearningSteps = self.numLearningSteps

    @property
    def batchSize(self):
        return int(4 + floor(3 * log(self.numParameters)))


def sorti(vect):
    """ sort, but also return the indices-changes """
    tmp = sorted(map(lambda (x, y): (y, x), enumerate(ravel(vect))))
    res1 = array(map(lambda x: x[0], tmp))
    res2 = array(map(lambda x: int(x[1]), tmp))
    return res1, res2


########NEW FILE########
__FILENAME__ = distributionbased
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.optimization.optimizer import ContinuousOptimizer


class DistributionBasedOptimizer(ContinuousOptimizer):
    """ The parent class for all optimization algorithms that are based on
    iteratively updating a search distribution.

    Provides a number of potentially useful methods that could be used by subclasses. """

    online = False
    batchSize = 100

    # distribution types
    GAUSSIAN = 1
    CAUCHY = 2
    GENERALIZEDGAUSSIAN = 3
    STUDENTT = 4

    distributionType = GAUSSIAN

    storeAllDistributions = False

    def _updateDistribution(self, dparamDeltas):
        """ Update the parameters of the current distribution, directly. """

    def _generateSample(self):
        """ Generate 1 sample from the current distribution. """

    def _generateConformingBatch(self):
        """ Generate a batch of samples that conforms to the current distribution.
        If importance mixing is enabled, this can reuse old samples. """




########NEW FILE########
__FILENAME__ = fem
__author__ = 'Daan Wierstra and Tom Schaul'

from scipy import dot, rand, ones, eye, zeros, outer, isnan, multiply, argmax, product, log
from numpy.random import normal, multivariate_normal
from numpy import sort
from scipy.stats import norm
from copy import deepcopy

from pybrain.utilities import drawIndex, fListToString
from pybrain.tools.functions import multivariateNormalPdf
from pybrain.tools.rankingfunctions import TopLinearRanking
from pybrain.optimization.distributionbased.distributionbased import DistributionBasedOptimizer


class FEM(DistributionBasedOptimizer):
    """ Fitness Expectation-Maximization (PPSN 2008).
    """

    # fundamental parameters
    numberOfCenters = 1
    diagonalOnly = False
    forgetFactor = 0.1
    muMultiplier = 1.
    windowSize = 50

    adaptiveShaping = False

    shapingFunction = TopLinearRanking(topFraction=0.5)

    minimumCenterWeight = 0.01

    # advanced improvement parameters

    # elitism: always keep best mu in distribution
    elitism = False
    # sampleElitism: every $windowSize samples, produce best sample ever
    sampleElitism = False
    oneFifthRule = False

    useAnticipatedMeanShift = False

    # rank-mu update, presumably
    doMadnessUpdate = False

    mutative = False

    # initialization parameters
    rangemins = None
    rangemaxs = None
    initCovariances = None

    def _additionalInit(self):
        assert self.numberOfCenters == 1, 'Mixtures of Gaussians not supported yet.'

        xdim = self.numParameters
        self.alphas = ones(self.numberOfCenters) / float(self.numberOfCenters)
        self.mus = []
        self.sigmas = []

        if self.rangemins == None:
            self.rangemins = -ones(xdim)
        if self.rangemaxs == None:
            self.rangemaxs = ones(xdim)
        if self.initCovariances == None:
            if self.diagonalOnly:
                self.initCovariances = ones(xdim)
            else:
                self.initCovariances = eye(xdim)

        for _ in range(self.numberOfCenters):
            self.mus.append(rand(xdim) * (self.rangemaxs - self.rangemins) + self.rangemins)
            self.sigmas.append(dot(eye(xdim), self.initCovariances))

        self.samples = range(self.windowSize)
        self.fitnesses = zeros(self.windowSize)
        self.generation = 0
        self.allsamples = []
        self.muevals = []
        self.allmus = []
        self.allsigmas = []
        self.allalphas = []
        self.allUpdateSizes = []
        self.allfitnesses = []
        self.meanShifts = [zeros((self.numParameters)) for _ in range(self.numberOfCenters)]

        self._oneEvaluation(self._initEvaluable)


    def _produceNewSample(self):
        """ returns a new sample, its fitness and its densities """
        chosenOne = drawIndex(self.alphas, True)
        mu = self.mus[chosenOne]

        if self.useAnticipatedMeanShift:
            if len(self.allsamples) % 2 == 1 and len(self.allsamples) > 1:
                if not(self.elitism and chosenOne == self.bestChosenCenter):
                    mu += self.meanShifts[chosenOne]

        if self.diagonalOnly:
            sample = normal(mu, self.sigmas[chosenOne])
        else:
            sample = multivariate_normal(mu, self.sigmas[chosenOne])
        if self.sampleElitism and len(self.allsamples) > self.windowSize and len(self.allsamples) % self.windowSize == 0:
            sample = self.bestEvaluable.copy()
        fit = self._oneEvaluation(sample)

        if ((not self.minimize and fit >= self.bestEvaluation)
            or (self.minimize and fit <= self.bestEvaluation)
            or len(self.allsamples) == 0):
            # used to determine which center produced the current best
            self.bestChosenCenter = chosenOne
            self.bestSigma = self.sigmas[chosenOne].copy()
        if self.minimize:
            fit = -fit
        self.allfitnesses.append(fit)
        self.allsamples.append(sample)
        return sample, fit

    def _computeDensities(self, sample):
        """ compute densities, and normalize """
        densities = zeros(self.numberOfCenters)
        for c in range(self.numberOfCenters):
            if self.diagonalOnly:
                pdf = product([norm.pdf(x, self.mus[c][i], self.sigmas[c][i]) for i, x in enumerate(sample)])
            else:
                pdf = multivariateNormalPdf(sample, self.mus[c], self.sigmas[c])
            if pdf > 1e40:
                pdf = 1e40
            elif pdf < 1e-40:
                pdf = 1e-40
            if isnan(pdf):
                print('NaN!')
                pdf = 0.
            densities[c] = self.alphas[c] * pdf
        densities /= sum(densities)
        return densities

    def _computeUpdateSize(self, densities, sampleIndex):
        """ compute the  the center-update-size for each sample
        using transformed fitnesses """

        # determine (transformed) fitnesses
        transformedfitnesses = self.shapingFunction(self.fitnesses)
        # force renormaliziation
        transformedfitnesses /= max(transformedfitnesses)

        updateSize = transformedfitnesses[sampleIndex] * densities
        return updateSize * self.forgetFactor

    def _updateMus(self, updateSize, lastSample):
        for c in range(self.numberOfCenters):
            oldmu = self.mus[c]
            self.mus[c] *= 1. - self.muMultiplier * updateSize[c]
            self.mus[c] += self.muMultiplier * updateSize[c] * lastSample
            # don't update with the ones that were produced with a mean shift
            if ((self.useAnticipatedMeanShift and len(self.allsamples) % self.windowSize == 1)
                or (not self.useAnticipatedMeanShift and self.numberOfCenters > 1)):
                self.meanShifts[c] *= 1. - self.forgetFactor
                self.meanShifts[c] += self.mus[c] - oldmu

            if self.doMadnessUpdate and len(self.allsamples) > 2 * self.windowSize:
                self.mus[c] = zeros(self.numParameters)
                updateSum = 0.
                for i in range(self.windowSize):
                    self.mus[c] += self.allsamples[-i - 1] * self.allUpdateSizes[-i - 1][c]
                    updateSum += self.allUpdateSizes[-i - 1][c]
                self.mus[c] /= updateSum

        if self.elitism:
            # dirty hack! TODO: koshify
            self.mus[0] = self.bestEvaluable.copy()

    def _updateSigmas(self, updateSize, lastSample):
        for c in range(self.numberOfCenters):
            self.sigmas[c] *= (1. - updateSize[c])
            dif = self.mus[c] - lastSample
            if self.diagonalOnly:
                self.sigmas[c] += updateSize[c] * multiply(dif, dif)
            else:
                self.sigmas[c] += updateSize[c] * 1.2 * outer(dif, dif)

    def _updateAlphas(self, updateSize):
        for c in range(self.numberOfCenters):
            x = updateSize[c]
            x /= sum(updateSize)
            self.alphas[c] = (1.0 - self.forgetFactor) * self.alphas[c] + self.forgetFactor * x
        self.alphas /= sum(self.alphas)
        for c in range(self.numberOfCenters):
            if self.alphas[c] < self.minimumCenterWeight:
                # center-splitting
                if self.verbose:
                    print('Split!')
                bestCenter = argmax(self.alphas)
                totalWeight = self.alphas[c] + self.alphas[bestCenter]
                self.alphas[c] = totalWeight / 2
                self.alphas[bestCenter] = totalWeight / 2
                self.mus[c] = self.mus[bestCenter].copy()
                self.sigmas[c] = 4.0 * self.sigmas[bestCenter].copy()
                self.sigmas[bestCenter] *= 0.25
                break

    def _updateShaping(self):
        """ Daan: "This won't work. I like it!"  """
        assert self.numberOfCenters == 1
        possible = self.shapingFunction.getPossibleParameters(self.windowSize)
        matchValues = []
        pdfs = [multivariateNormalPdf(s, self.mus[0], self.sigmas[0])
                for s in self.samples]

        for p in possible:
            self.shapingFunction.setParameter(p)
            transformedFitnesses = self.shapingFunction(self.fitnesses)
            #transformedFitnesses /= sum(transformedFitnesses)
            sumValue = sum([x * log(y) for x, y in zip(pdfs, transformedFitnesses) if y > 0])
            normalization = sum([x * y for x, y in zip(pdfs, transformedFitnesses) if y > 0])
            matchValues.append(sumValue / normalization)


        self.shapingFunction.setParameter(possible[argmax(matchValues)])

        if len(self.allsamples) % 100 == 0:
            print(possible[argmax(matchValues)])
            print(fListToString(matchValues, 3))

    def _learnStep(self):
        k = len(self.allsamples) % self.windowSize
        sample, fit = self._produceNewSample()
        self.samples[k], self.fitnesses[k] = sample, fit
        self.generation += 1
        if len(self.allsamples) < self.windowSize:
            return
        if self.verbose and len(self.allsamples) % 100 == 0:
            print(len(self.allsamples), min(self.fitnesses), max(self.fitnesses))
            # print(len(self.allsamples), min(self.fitnesses), max(self.fitnesses)#, self.alphas)

        updateSize = self._computeUpdateSize(self._computeDensities(sample), k)
        self.allUpdateSizes.append(deepcopy(updateSize))
        if sum(updateSize) > 0:
            # update parameters
            if self.numberOfCenters > 1:
                self._updateAlphas(updateSize)
            if not self.mutative:
                self._updateMus(updateSize, sample)
                self._updateSigmas(updateSize, sample)
            else:
                self._updateSigmas(updateSize, sample)
                self._updateMus(updateSize, sample)

        if self.adaptiveShaping:
            self._updateShaping()

        # storage, e.g. for plotting
        self.allalphas.append(deepcopy(self.alphas))
        self.allsigmas.append(deepcopy(self.sigmas))
        self.allmus.append(deepcopy(self.mus))

        if self.oneFifthRule and len(self.allsamples) % 10 == 0  and len(self.allsamples) > 2 * self.windowSize:
            lastBatch = self.allfitnesses[-self.windowSize:]
            secondLast = self.allfitnesses[-2 * self.windowSize:-self.windowSize]
            sortedLast = sort(lastBatch)
            sortedSecond = sort(secondLast)
            index = int(self.windowSize * 0.8)
            if sortedLast[index] >= sortedSecond[index]:
                self.sigmas = [1.2 * sigma for sigma in self.sigmas]
                #print("+")
            else:
                self.sigmas = [0.5 * sigma for sigma in self.sigmas]
                #print("-")


########NEW FILE########
__FILENAME__ = nes
__author__ = 'Daan Wierstra, Tom Schaul and Sun Yi'


from ves import VanillaGradientEvolutionStrategies
from pybrain.utilities import triu2flat, blockCombine
from scipy.linalg import inv, pinv2
from scipy import outer, dot, multiply, zeros, diag, mat, sum


class ExactNES(VanillaGradientEvolutionStrategies):
    """ A new version of NES, using the exact instead of the approximate
    Fisher Information Matrix, as well as a number of other improvements.
    (GECCO 2009).
    """

    # 4 kinds of baselines can be used:
    NOBASELINE = 0
    UNIFORMBASELINE = 1
    SPECIFICBASELINE = 2
    BLOCKBASELINE = 3

    #: Type of baseline. The most robust one is also the default.
    baselineType = BLOCKBASELINE

    learningRate = 1.

    def _calcBatchUpdate(self, fitnesses):
        samples = self.allSamples[-self.batchSize:]
        d = self.numParameters
        invA = inv(self.factorSigma)
        invSigma = inv(self.sigma)
        diagInvA = diag(diag(invA))

        # efficient computation of V, which corresponds to inv(Fisher)*logDerivs
        V = zeros((self.numDistrParams, self.batchSize))
        # u is used to compute the uniform baseline
        u = zeros(self.numDistrParams)
        for i in range(self.batchSize):
            s = dot(invA.T, (samples[i] - self.x))
            R = outer(s, dot(invA, s)) - diagInvA
            flatR = triu2flat(R)
            u[:d] += fitnesses[i] * (samples[i] - self.x)
            u[d:] += fitnesses[i] * flatR
            V[:d, i] += samples[i] - self.x
            V[d:, i] += flatR

        j = self.numDistrParams - 1
        D = 1 / invSigma[-1, -1]
        # G corresponds to the blocks of the inv(Fisher)
        G = 1 / (invSigma[-1, -1] + invA[-1, -1] ** 2)

        u[j] = dot(G, u[j])
        V[j, :] = dot(G, V[j, :])
        j -= 1
        for k in reversed(range(d - 1)):
            p = invSigma[k + 1:, k]
            w = invSigma[k, k]
            wg = w + invA[k, k] ** 2
            q = dot(D, p)
            c = dot(p, q)
            r = 1 / (w - c)
            rg = 1 / (wg - c)
            t = -(1 + r * c) / w
            tg = -(1 + rg * c) / wg

            G = blockCombine([[rg, tg * q],
                              [mat(tg * q).T, D + rg * outer(q, q)]])
            D = blockCombine([[r , t * q],
                              [mat(t * q).T, D + r * outer(q, q)]])
            u[j - (d - k - 1):j + 1] = dot(G, u[j - (d - k - 1):j + 1])
            V[j - (d - k - 1):j + 1, :] = dot(G, V[j - (d - k - 1):j + 1, :])
            j -= d - k


        # determine the update vector, according to different baselines.
        if self.baselineType == self.BLOCKBASELINE:
            update = zeros(self.numDistrParams)
            vsquare = multiply(V, V)
            j = self.numDistrParams - 1
            for k in reversed(range(self.numParameters)):
                b0 = sum(vsquare[j - (d - k - 1):j + 1, :], 0)
                b = dot(b0, fitnesses) / sum(b0)
                update[j - (d - k - 1):j + 1] = dot(V[j - (d - k - 1):j + 1, :], (fitnesses - b))
                j -= d - k
            b0 = sum(vsquare[:j + 1, :], 0)
            b = dot(b0, fitnesses) / sum(b0)
            update[:j + 1] = dot(V[:j + 1, :], (fitnesses - b))

        elif self.baselineType == self.SPECIFICBASELINE:
            update = zeros(self.numDistrParams)
            vsquare = multiply(V, V)
            for j in range(self.numDistrParams):
                b = dot(vsquare[j, :], fitnesses) / sum(vsquare[j, :])
                update[j] = dot(V[j, :], (fitnesses - b))

        elif self.baselineType == self.UNIFORMBASELINE:
            v = sum(V, 1)
            update = u - dot(v, u) / dot(v, v) * v

        elif self.baselineType == self.NOBASELINE:
            update = dot(V, fitnesses)

        else:
            raise NotImplementedError('No such baseline implemented')

        return update / self.batchSize



class OriginalNES(VanillaGradientEvolutionStrategies):
    """ Reference implementation of the original Natural Evolution Strategies algorithm (CEC-2008). """

    learningRate = 1.

    def _calcBatchUpdate(self, fitnesses):
        xdim = self.numParameters
        invSigma = inv(self.sigma)
        samples = self.allSamples[-self.batchSize:]
        phi = zeros((self.batchSize, self.numDistrParams + 1))
        phi[:, :xdim] = self._logDerivsX(samples, self.x, invSigma)
        phi[:, xdim:-1] = self._logDerivsFactorSigma(samples, self.x, invSigma, self.factorSigma)
        phi[:, -1] = 1

        update = dot(pinv2(phi), fitnesses)[:-1]
        return update

    def _logDerivsFactorSigma(self, samples, mu, invSigma, factorSigma):
        """ Compute the log-derivatives w.r.t. the factorized covariance matrix components.
        This implementation should be faster than the one in Vanilla. """
        res = zeros((len(samples), self.numDistrParams - self.numParameters))
        invA = inv(factorSigma)
        diagInvA = diag(diag(invA))

        for i, sample in enumerate(samples):
            s = dot(invA.T, (sample - mu))
            R = outer(s, dot(invA, s)) - diagInvA
            res[i] = triu2flat(R)
        return res


########NEW FILE########
__FILENAME__ = rank1
__author__ = 'Tom Schaul, Tobias Glasmachers'


from scipy import dot, array, randn,  exp, floor, log, sqrt, ones, multiply, log2

from pybrain.tools.rankingfunctions import HansenRanking
from pybrain.optimization.distributionbased.distributionbased import DistributionBasedOptimizer



class Rank1NES(DistributionBasedOptimizer):
    """ Natural Evolution Strategies with rank-1 covariance matrices. 
    
    See http://arxiv.org/abs/1106.1998 for a description. """
    
    # parameters, which can be set but have a good (adapted) default value
    centerLearningRate = 1.0
    scaleLearningRate = None
    covLearningRate = None 
    batchSize = None 
    uniformBaseline = True
    shapingFunction = HansenRanking()
    
    # fixed settings
    mustMaximize = True
    storeAllEvaluations = True    
    storeAllEvaluated = True    
    storeAllDistributions = True
    storeAllRates = True
    verboseGaps = 1
    initVariance = 1.
    varianceCutoff = 1e-20            

    
    def _additionalInit(self):
        # heuristic settings       
        if self.covLearningRate is None:
            self.covLearningRate = self._initLearningRate()
        if self.scaleLearningRate is None:
            self.scaleLearningRate = self.covLearningRate   
        if self.batchSize is None: 
            self.batchSize = self._initBatchSize()          
            
        # other initializations
        self._center = self._initEvaluable.copy()          
        self._logDetA = log(self.initVariance) / 2
        self._principalVector = randn(self.numParameters)
        self._principalVector /= sqrt(dot(self._principalVector, self._principalVector))
        self._allDistributions = [(self._center.copy(), self._principalVector.copy(), self._logDetA)]
        self.covLearningRate = 0.1
        self.batchSize = int(max(5,max(4*log2(self.numParameters),0.2*self.numParameters)))
        self.uniformBaseline = False
        self.scaleLearningRate = 0.1
    
    def _stoppingCriterion(self):
        if DistributionBasedOptimizer._stoppingCriterion(self):
            return True
        elif self._getMaxVariance < self.varianceCutoff:   
            return True
        else:
            return False
            
    @property
    def _getMaxVariance(self):
        return exp(self._logDetA * 2 / self.numParameters)
        
    def _initLearningRate(self):
        return 0.6 * (3 + log(self.numParameters)) / self.numParameters / sqrt(self.numParameters)
    
    def _initBatchSize(self):
        return 4 + int(floor(3 * log(self.numParameters)))               
            
    @property
    def _population(self):
        return [self._allEvaluated[i] for i in self._pointers]
        
    @property
    def _currentEvaluations(self):        
        fits = [self._allEvaluations[i] for i in self._pointers]
        if self._wasOpposed:
            fits = map(lambda x:-x, fits)
        return fits
                        
    def _produceSample(self):
        return randn(self.numParameters + 1)
    
    def _produceSamples(self):
        """ Append batch size new samples and evaluate them. """
        tmp = [self._sample2base(self._produceSample()) for _ in range(self.batchSize)]
        map(self._oneEvaluation, tmp)            
        self._pointers = list(range(len(self._allEvaluated) - self.batchSize, len(self._allEvaluated)))                    
        
    def _notify(self):
        """ Provide some feedback during the run. """
        if self.verbose:
            if self.numEvaluations % self.verboseGaps == 0:
                print('Step:', self.numLearningSteps, 'best:', self.bestEvaluation,
                    'logVar', round(self._logDetA, 3),
                    'log|vector|', round(log(dot(self._principalVector, self._principalVector))/2, 3))
                  
        if self.listener is not None:
            self.listener(self.bestEvaluable, self.bestEvaluation)
    
    def _learnStep(self):            
        # concatenations of y vector and z value
        samples = [self._produceSample() for _ in range(self.batchSize)]
                
        u = self._principalVector
        a = self._logDetA
        
        # unnamed in paper (y+zu), or x/exp(lambda)
        W = [s[:-1] + u * s[-1] for s in samples]   
        points = [self._center+exp(a) *w for w in W]
    
        map(self._oneEvaluation, points)  
                  
        self._pointers = list(range(len(self._allEvaluated) - self.batchSize, len(self._allEvaluated)))                            
        
        utilities = self.shapingFunction(self._currentEvaluations)
        utilities /= sum(utilities)  # make the utilities sum to 1
        if self.uniformBaseline:
            utilities -= 1. / self.batchSize    
        
        W = [w for i,w in enumerate(W) if utilities[i] != 0]
        utilities = [uw for uw in utilities if uw != 0]
                    
        dim = self.numParameters        
                 
        r = sqrt(dot(u, u))
        v = u / r
        c = log(r)        
        
        #inner products, but not scaled with exp(lambda) 
        wws = array([dot(w, w) for w in W])
        wvs = array([dot(v, w) for w in W])
        wv2s = array([wv ** 2 for wv in wvs])
        
        dCenter = exp(self._logDetA) * dot(utilities, W)
        self._center += self.centerLearningRate * dCenter       
        
        kp = ((r ** 2 - dim + 2) * wv2s - (r ** 2 + 1) * wws) / (2 * r * (dim - 1.))     

        # natural gradient on lambda, equation (5)
        da = 1. / (2 * (dim - 1)) * dot((wws - dim) - (wv2s - 1), utilities)
        
        # natural gradient on u, equation (6)
        du = dot(kp, utilities) * v + dot(multiply(wvs / r, utilities), W)
                
        # equation (7)
        dc = dot(du, v) / r
                
        # equation (8)
        dv = du / r - dc * v
        
        epsilon = min(self.covLearningRate, 2 * sqrt(r ** 2 / dot(du, du)))
        if dc > 0: 
            # additive update
            self._principalVector += epsilon * du
        else: 
            # multiplicative update
            # prevents instability            
            c += epsilon * dc
            v += epsilon * dv
            v /= sqrt(dot(v, v))
            r = exp(c)
            self._principalVector = r * v        
              
        self._lastLogDetA = self._logDetA
        self._logDetA += self.scaleLearningRate * da

        if self.storeAllDistributions:
            self._allDistributions.append((self._center.copy(), self._principalVector.copy(), self._logDetA))
    
    
def test():
    """ Rank-1 NEX easily solves high-dimensional Rosenbrock functions. """
    from pybrain.rl.environments.functions.unimodal import RosenbrockFunction
    dim = 40
    f = RosenbrockFunction(dim)
    x0 = -ones(dim)    
    l = Rank1NES(f, x0, verbose=True, verboseGaps=500)
    l.learn()
    
            
if __name__ == '__main__':
    test()
########NEW FILE########
__FILENAME__ = snes
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.optimization.distributionbased.distributionbased import DistributionBasedOptimizer
from scipy import dot, exp, log, sqrt, floor, ones, randn
from pybrain.tools.rankingfunctions import HansenRanking


class SNES(DistributionBasedOptimizer):
    """ Separable NES (diagonal). 
    [As described in Schaul, Glasmachers and Schmidhuber (GECCO'11)]
    """
    
    # parameters, which can be set but have a good (adapted) default value
    centerLearningRate = 1.0
    covLearningRate = None     
    batchSize = None     
    uniformBaseline = True
    shapingFunction = HansenRanking()
    initVariance = 1.
    
    # fixed settings
    mustMaximize = True
    storeAllEvaluations = True    
    storeAllEvaluated = True
    
    # for very long runs, we don't want to run out of memory
    clearStorage = False    
            
    # minimal setting where to abort the search
    varianceCutoff = 1e-20
            
    def _stoppingCriterion(self):
        if DistributionBasedOptimizer._stoppingCriterion(self):
            return True
        elif max(abs(self._sigmas)) < self.varianceCutoff:   
            return True
        else:
            return False
            
    def _initLearningRate(self):
        """ Careful, robust default value. """
        return 0.6 * (3 + log(self.numParameters)) / 3 / sqrt(self.numParameters)
        
    def _initBatchSize(self):
        """ as in CMA-ES """
        return 4 + int(floor(3 * log(self.numParameters)))    
    
    def _additionalInit(self):
        if self.covLearningRate is None:
            self.covLearningRate = self._initLearningRate()        
        if self.batchSize is None:
            self.batchSize = self._initBatchSize()                           
            
        self._center = self._initEvaluable.copy()          
        self._sigmas = ones(self.numParameters) * self.initVariance
    
    @property
    def _population(self):
        if self._wasUnwrapped:
            return [self._allEvaluated[i].params for i in self._pointers]
        else:
            return [self._allEvaluated[i] for i in self._pointers]
            
    @property
    def _currentEvaluations(self):        
        fits = [self._allEvaluations[i] for i in self._pointers]
        if self._wasOpposed:
            fits = map(lambda x:-x, fits)
        return fits
                        
    def _produceSample(self):
        return randn(self.numParameters)
            
    def _sample2base(self, sample):       
        """ How does a sample look in the outside (base problem) coordinate system? """ 
        return self._sigmas * sample + self._center
              
    def _base2sample(self, e):
        """ How does the point look in the present one reference coordinates? """
        return (e - self._center) / self._sigmas
    
    def _produceSamples(self):
        """ Append batch size new samples and evaluate them. """
        if self.clearStorage:
            self._allEvaluated = []
            self._allEvaluations = []
            
        tmp = [self._sample2base(self._produceSample()) for _ in range(self.batchSize)]
        map(self._oneEvaluation, tmp)            
        self._pointers = list(range(len(self._allEvaluated) - self.batchSize, len(self._allEvaluated)))                    
            
    def _learnStep(self):
        # produce samples
        self._produceSamples()
        samples = map(self._base2sample, self._population) 
        
        #compute utilities
        utilities = self.shapingFunction(self._currentEvaluations)
        utilities /= sum(utilities)  # make the utilities sum to 1
        if self.uniformBaseline:
            utilities -= 1. / self.batchSize                           
                    
        # update center
        dCenter = dot(utilities, samples)
        self._center += self.centerLearningRate * self._sigmas * dCenter
        
        # update variances
        covGradient = dot(utilities, [s ** 2 - 1 for s in samples])        
        dA = 0.5 * self.covLearningRate * covGradient                                
        self._sigmas = self._sigmas * exp(dA)            
        
        
if __name__ == "__main__":
    from pybrain.rl.environments.functions.unimodal import ElliFunction
    print(SNES(ElliFunction(100), ones(100), verbose=True).learn())
    
########NEW FILE########
__FILENAME__ = ves
__author__ = 'Daan Wierstra and Tom Schaul'

from scipy import eye, multiply, ones, dot, array, outer, rand, zeros, diag, randn, exp
from scipy.linalg import cholesky, inv, det

from pybrain.optimization.distributionbased.distributionbased import DistributionBasedOptimizer
from pybrain.tools.rankingfunctions import TopLinearRanking
from pybrain.utilities import flat2triu, triu2flat
from pybrain.auxiliary import importanceMixing


class VanillaGradientEvolutionStrategies(DistributionBasedOptimizer):
    """ Vanilla gradient-based evolution strategy. """

    # mandatory parameters
    online = False
    learningRate = 0.01
    learningRateSigma = None # default: the same than learningRate

    initialFactorSigma = None # default: identity matrix

    # NOT YET SUPPORTED:
    diagonalOnly = False

    batchSize = 100
    momentum = None

    elitism = False

    shapingFunction = TopLinearRanking(topFraction=0.5)

    # initialization parameters
    rangemins = None
    rangemaxs = None
    initCovariances = None

    vanillaScale = False

    # use of importance sampling to get away with fewer samples:
    importanceMixing = True
    forcedRefresh = 0.01

    mustMaximize = True

    def _additionalInit(self):
        xdim = self.numParameters
        assert not self.diagonalOnly, 'Diagonal-only not yet supported'
        self.numDistrParams = xdim + xdim * (xdim + 1) / 2

        if self.momentum != None:
            self.momentumVector = zeros(self.numDistrParams)
        if self.learningRateSigma == None:
            self.learningRateSigma = self.learningRate
        if self.batchSize is None:
            self.batchSize = 10 * self.numParameters

        if self.rangemins == None:
            self.rangemins = -ones(xdim)
        if self.rangemaxs == None:
            self.rangemaxs = ones(xdim)
        if self.initCovariances == None:
            if self.diagonalOnly:
                self.initCovariances = ones(xdim)
            else:
                self.initCovariances = eye(xdim)

        self.x = rand(xdim) * (self.rangemaxs - self.rangemins) + self.rangemins
        self.sigma = dot(eye(xdim), self.initCovariances)
        self.factorSigma = cholesky(self.sigma)

        # keeping track of history
        self.allSamples = []
        self.allFitnesses = []

        self.allGenerated = [0]

        self.allCenters = [self.x.copy()]
        self.allFactorSigmas = [self.factorSigma.copy()]

        # for baseline computation
        self.phiSquareWindow = zeros((self.batchSize, self.numDistrParams))

        if self.storeAllDistributions:
            self._allDistributions = [(self.x.copy(), self.sigma.copy())]

    def _produceNewSample(self, z=None):
        if z == None:
            p = randn(self.numParameters)
            z = dot(self.factorSigma.T, p) + self.x
        self.allSamples.append(z)
        fit = self._oneEvaluation(z)
        self.allFitnesses.append(fit)
        return z, fit

    def _produceSamples(self):
        """ Append batchsize new samples and evaluate them. """
        if self.numLearningSteps == 0 or not self.importanceMixing:
            for _ in range(self.batchSize):
                self._produceNewSample()
            self.allGenerated.append(self.batchSize + self.allGenerated[-1])

        # using new importance mixing code
        else:
            oldpoints = self.allSamples[-self.batchSize:]
            oldDetFactorSigma = det(self.allFactorSigmas[-2])
            newDetFactorSigma = det(self.factorSigma)
            invA = inv(self.factorSigma)
            offset = len(self.allSamples) - self.batchSize
            oldInvA = inv(self.allFactorSigmas[-2])
            oldX = self.allCenters[-2]

            def oldpdf(s):
                p = dot(oldInvA.T, (s- oldX))
                return exp(-0.5 * dot(p, p)) / oldDetFactorSigma

            def newpdf(s):
                p = dot(invA.T, (s - self.x))
                return exp(-0.5 * dot(p, p)) / newDetFactorSigma

            def newSample():
                p = randn(self.numParameters)
                return dot(self.factorSigma.T, p) + self.x

            reused, newpoints = importanceMixing(oldpoints, oldpdf, newpdf,
                                                 newSample, self.forcedRefresh)

            self.allGenerated.append(self.allGenerated[-1]+len(newpoints))

            for i in reused:
                self.allSamples.append(self.allSamples[offset+i])
                self.allFitnesses.append(self.allFitnesses[offset+i])
            for s in newpoints:
                self._produceNewSample(s)

    def _learnStep(self):
        if self.online:
            self._onlineLearn()
        else:
            self._batchLearn()

    def _batchLearn(self):
        """ Batch learning. """
        xdim = self.numParameters
        # produce samples and evaluate them
        try:
            self._produceSamples()

            # shape their fitnesses
            shapedFits = self.shapingFunction(self.allFitnesses[-self.batchSize:])

            # update parameters (unbiased: divide by batchsize)
            update = self._calcBatchUpdate(shapedFits)
            if self.elitism:
                self.x = self.bestEvaluable
            else:
                self.x += self.learningRate * update[:xdim]
            self.factorSigma += self.learningRateSigma * flat2triu(update[xdim:], xdim)
            self.sigma = dot(self.factorSigma.T, self.factorSigma)

        except ValueError:
            print('Numerical Instability. Stopping.')
            self.maxLearningSteps = self.numLearningSteps

        if self._hasConverged():
            print('Premature convergence. Stopping.')
            self.maxLearningSteps = self.numLearningSteps

        if self.verbose:
            print('Evals:', self.numEvaluations,)

        self.allCenters.append(self.x.copy())
        self.allFactorSigmas.append(self.factorSigma.copy())

        if self.storeAllDistributions:
            self._allDistributions.append((self.x.copy(), self.sigma.copy()))


    def _onlineLearn(self):
        """ Online learning. """
        # produce one sample and evaluate
        xdim = self.numParameters
        self._produceNewSample()
        if len(self.allSamples) <= self.batchSize:
            return

        # shape the fitnesses of the last samples
        shapedFits = self.shapingFunction(self.allFitnesses[-self.batchSize:])

        # update parameters
        update = self._calcOnlineUpdate(shapedFits)
        self.x += self.learningRate * update[:xdim]
        self.factorSigma += self.learningRateSigma * flat2triu(update[xdim:], xdim)
        self.sigma = dot(self.factorSigma.T, self.factorSigma)

        if self.storeAllDistributions:
            self._allDistributions.append(self.x.copy(), self.sigma.copy())

    def _calcBatchUpdate(self, fitnesses):
        gradient = self._calcVanillaBatchGradient(self.allSamples[-self.batchSize:], fitnesses)
        if self.momentum != None:
            self.momentumVector *= self.momentum
            self.momentumVector += gradient
            return self.momentumVector
        else:
            return gradient

    def _calcOnlineUpdate(self, fitnesses):
        gradient = self._calcVanillaOnlineGradient(self.allSamples[-1], fitnesses[-self.batchSize:])
        if self.momentum != None:
            self.momentumVector *= self.momentum
            self.momentumVector += gradient
            return self.momentumVector
        else:
            return gradient

    def _logDerivX(self, sample, x, invSigma):
        return dot(invSigma, (sample - x))

    def _logDerivsX(self, samples, x, invSigma):
        samplesArray = array(samples)
        tmpX = multiply(x, ones((len(samplesArray), self.numParameters)))
        return dot(invSigma, (samplesArray - tmpX).T).T

    def _logDerivFactorSigma(self, sample, x, invSigma, factorSigma):
        logDerivSigma = 0.5 * dot(dot(invSigma, outer(sample - x, sample - x)), invSigma) - 0.5 * invSigma
        if self.vanillaScale:
            logDerivSigma = multiply(outer(diag(abs(self.factorSigma)), diag(abs(self.factorSigma))), logDerivSigma)
        return triu2flat(dot(factorSigma, (logDerivSigma + logDerivSigma.T)))

    def _logDerivsFactorSigma(self, samples, x, invSigma, factorSigma):
        return [self._logDerivFactorSigma(sample, x, invSigma, factorSigma) for sample in samples]

    def _calcVanillaBatchGradient(self, samples, shapedfitnesses):
        invSigma = inv(self.sigma)

        phi = zeros((len(samples), self.numDistrParams))
        phi[:, :self.numParameters] = self._logDerivsX(samples, self.x, invSigma)
        logDerivFactorSigma = self._logDerivsFactorSigma(samples, self.x, invSigma, self.factorSigma)
        phi[:, self.numParameters:] = array(logDerivFactorSigma)
        Rmat = outer(shapedfitnesses, ones(self.numDistrParams))

        # optimal baseline
        self.phiSquareWindow = multiply(phi, phi)
        baselineMatrix = self._calcBaseline(shapedfitnesses)

        gradient = sum(multiply(phi, (Rmat - baselineMatrix)), 0)
        return gradient

    def _calcVanillaOnlineGradient(self, sample, shapedfitnesses):
        invSigma = inv(self.sigma)
        phi = zeros(self.numDistrParams)
        phi[:self.numParameters] = self._logDerivX(sample, self.x, invSigma)
        logDerivSigma = self._logDerivFactorSigma(sample, self.x, invSigma, self.factorSigma)
        phi[self.numParameters:] = logDerivSigma.flatten()
        index = len(self.allSamples) % self.batchSize
        self.phiSquareWindow[index] = multiply(phi, phi)
        baseline = self._calcBaseline(shapedfitnesses)
        gradient = multiply((ones(self.numDistrParams) * shapedfitnesses[-1] - baseline), phi)
        return gradient

    def _calcBaseline(self, shapedfitnesses):
        paramWeightings = dot(ones(self.batchSize), self.phiSquareWindow)
        baseline = dot(shapedfitnesses, self.phiSquareWindow) / paramWeightings
        return baseline

    def _hasConverged(self):
        """ When the largest eigenvalue is smaller than 10e-20, we assume the
        algorithms has converged. """
        eigs = abs(diag(self.factorSigma))
        return min(eigs) < 1e-10

    def _revertToSafety(self):
        """ When encountering a bad matrix, this is how we revert to a safe one. """
        self.factorSigma = eye(self.numParameters)
        self.x = self.bestEvaluable
        self.allFactorSigmas[-1][:] = self.factorSigma
        self.sigma = dot(self.factorSigma.T, self.factorSigma)


########NEW FILE########
__FILENAME__ = xnes
__author__ = 'Tom Schaul, Sun Yi, Tobias Glasmachers'


from pybrain.tools.rankingfunctions import HansenRanking
from pybrain.optimization.distributionbased.distributionbased import DistributionBasedOptimizer
from pybrain.auxiliary.importancemixing import importanceMixing
from scipy.linalg import expm2
from scipy import dot, array, randn, eye, outer, exp, trace, floor, log, sqrt


class XNES(DistributionBasedOptimizer):
    """ NES with exponential parameter representation. """

    # parameters, which can be set but have a good (adapted) default value
    covLearningRate = None
    centerLearningRate = 1.0
    scaleLearningRate = None
    uniformBaseline = True
    batchSize = None
    shapingFunction = HansenRanking()
    importanceMixing = False
    forcedRefresh = 0.01

    # fixed settings
    mustMaximize = True
    storeAllEvaluations = True
    storeAllEvaluated = True
    storeAllDistributions = False

    def _additionalInit(self):
        # good heuristic default parameter settings
        dim = self.numParameters
        if self.covLearningRate is None:
            self.covLearningRate = 0.6*(3+log(dim))/dim/sqrt(dim)
        if self.scaleLearningRate is None:
            self.scaleLearningRate = self.covLearningRate
        if self.batchSize is None:
            if self.importanceMixing:
                self.batchSize = 10*dim
            else:
                self.batchSize = 4+int(floor(3*log(dim)))

        # some bookkeeping variables
        self._center = self._initEvaluable.copy()
        self._A = eye(self.numParameters) # square root of covariance matrix
        self._invA = eye(self.numParameters)
        self._logDetA = 0.
        self._allPointers = []
        self._allGenSteps = [0]
        if self.storeAllDistributions:
            self._allDistributions = [(self._center.copy(), self._A.copy())]

    def _learnStep(self):
        """ Main part of the algorithm. """
        I = eye(self.numParameters)
        self._produceSamples()
        utilities = self.shapingFunction(self._currentEvaluations)
        utilities /= sum(utilities)  # make the utilities sum to 1
        if self.uniformBaseline:
            utilities -= 1./self.batchSize
        samples = array(map(self._base2sample, self._population))

        dCenter = dot(samples.T, utilities)
        covGradient = dot(array([outer(s,s) - I for s in samples]).T, utilities)
        covTrace = trace(covGradient)
        covGradient -= covTrace/self.numParameters * I
        dA = 0.5 * (self.scaleLearningRate * covTrace/self.numParameters * I
                    +self.covLearningRate * covGradient)

        self._lastLogDetA = self._logDetA
        self._lastInvA = self._invA

        self._center += self.centerLearningRate * dot(self._A, dCenter)
        self._A = dot(self._A, expm2(dA))
        self._invA = dot(expm2(-dA), self._invA)
        self._logDetA += 0.5 * self.scaleLearningRate * covTrace
        if self.storeAllDistributions:
            self._allDistributions.append((self._center.copy(), self._A.copy()))

    @property
    def _lastA(self): return self._allDistributions[-2][1]
    @property
    def _lastCenter(self): return self._allDistributions[-2][0]
    @property
    def _population(self):
        if self._wasUnwrapped:
            return [self._allEvaluated[i].params for i in self._pointers]
        else:
            return [self._allEvaluated[i] for i in self._pointers]

    @property
    def _currentEvaluations(self):
        fits = [self._allEvaluations[i] for i in self._pointers]
        if self._wasOpposed:
            fits = map(lambda x:-x, fits)
        return fits

    def _produceSample(self):
        return randn(self.numParameters)

    def _sample2base(self, sample):
        """ How does a sample look in the outside (base problem) coordinate system? """
        return dot(self._A, sample)+self._center

    def _base2oldsample(self, e):
        """ How would the point have looked in the previous reference coordinates? """
        return dot(self._lastInvA, (e - self._lastCenter))

    def _base2sample(self, e):
        """ How does the point look in the present one reference coordinates? """
        return dot(self._invA, (e - self._center))

    def _oldpdf(self, s):
        s = self._base2oldsample(self._sample2base(s))
        return exp(-0.5*dot(s,s)- self._lastLogDetA)

    def _newpdf(self, s):
        return exp(-0.5*dot(s,s)- self._logDetA)

    def _produceSamples(self):
        """ Append batch size new samples and evaluate them. """
        reuseindices = []
        if self.numLearningSteps == 0 or not self.importanceMixing:
            [self._oneEvaluation(self._sample2base(self._produceSample())) for _ in range(self.batchSize)]
            self._pointers = list(range(len(self._allEvaluated)-self.batchSize, len(self._allEvaluated)))
        else:
            reuseindices, newpoints = importanceMixing(map(self._base2sample, self._currentEvaluations),
                                                       self._oldpdf, self._newpdf, self._produceSample, self.forcedRefresh)
            [self._oneEvaluation(self._sample2base(s)) for s in newpoints]
            self._pointers = ([self._pointers[i] for i in reuseindices]+
                              range(len(self._allEvaluated)-self.batchSize+len(reuseindices), len(self._allEvaluated)))
        self._allGenSteps.append(self._allGenSteps[-1]+self.batchSize-len(reuseindices))
        self._allPointers.append(self._pointers)



if __name__ == '__main__':
    from pybrain.rl.environments.functions.unimodal import RosenbrockFunction
    from scipy import ones
    dim = 10
    f = RosenbrockFunction(dim)
    l = XNES(f, -ones(dim))
    print(l.learn())
    print('Evaluations needed:', len(l._allEvaluations))

########NEW FILE########
__FILENAME__ = fd
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de, Tom Schaul'

from scipy import ones, zeros, dot, ravel, random
from scipy.linalg import pinv

from pybrain.auxiliary import GradientDescent
from pybrain.optimization.optimizer import ContinuousOptimizer


class FiniteDifferences(ContinuousOptimizer):
    """ Basic finite difference method. """

    epsilon = 1.0
    gamma = 0.999
    batchSize = 10

    # gradient descent parameters
    learningRate = 0.1
    learningRateDecay = None
    momentum = 0.0
    rprop = False

    def _setInitEvaluable(self, evaluable):
        ContinuousOptimizer._setInitEvaluable(self, evaluable)
        self.current = self._initEvaluable
        self.gd = GradientDescent()
        self.gd.alpha = self.learningRate
        if self.learningRateDecay is not None:
            self.gd.alphadecay = self.learningRateDecay
        self.gd.momentum = self.momentum
        self.gd.rprop = self.rprop
        self.gd.init(self._initEvaluable)

    def perturbation(self):
        """ produce a parameter perturbation """
        deltas = random.uniform(-self.epsilon, self.epsilon, self.numParameters)
        # reduce epsilon by factor gamma
        self.epsilon *= self.gamma
        return deltas

    def _learnStep(self):
        """ calls the gradient calculation function and executes a step in direction
            of the gradient, scaled with a small learning rate alpha. """

        # initialize matrix D and vector R
        D = ones((self.batchSize, self.numParameters))
        R = zeros((self.batchSize, 1))

        # calculate the gradient with pseudo inverse
        for i in range(self.batchSize):
            deltas = self.perturbation()
            x = self.current + deltas
            D[i, :] = deltas
            R[i, :] = self._oneEvaluation(x)
        beta = dot(pinv(D), R)
        gradient = ravel(beta)

        # update the weights
        self.current = self.gd(gradient)


########NEW FILE########
__FILENAME__ = pgpe
__author__ = 'Frank Sehnke, sehnke@in.tum.de, Tom Schaul'

from scipy import ones, random

from pybrain.auxiliary import GradientDescent
from fd import FiniteDifferences


class PGPE(FiniteDifferences):
    """ Policy Gradients with Parameter Exploration (ICANN 2008)."""
    
    #:exploration type
    exploration = "local"
    #: specific settings for sigma updates
    learningRate = 0.2    
    #: specific settings for sigma updates
    sigmaLearningRate = 0.1
    #: Initial value of sigmas
    epsilon = 2.0
    #:lasso weight decay (0 to deactivate)
    wDecay = 0.0
    #:momentum term (0 to deactivate)
    momentum = 0.0
    #:rprop decent (False to deactivate)
    rprop = False
    
    def _additionalInit(self):
        if self.sigmaLearningRate is None:
            self.sigmaLearningRate = self.learningRate    
        self.gdSig = GradientDescent()
        self.gdSig.alpha = self.sigmaLearningRate
        self.gdSig.rprop = self.rprop
        self.sigList = ones(self.numParameters) * self.epsilon #Stores the list of standard deviations (sigmas)
        self.gdSig.init(self.sigList)
        self.baseline = None
        
    def perturbation(self):
        """ Generate a difference vector with the given standard deviations """
        return random.normal(0., self.sigList)
            
    def _learnStep(self):
        """ calculates the gradient and executes a step in the direction
            of the gradient, scaled with a learning rate alpha. """
        deltas = self.perturbation()
        #reward of positive and negative perturbations
        reward1 = self._oneEvaluation(self.current + deltas)        
        reward2 = self._oneEvaluation(self.current - deltas)

        self.mreward = (reward1 + reward2) / 2.                
        if self.baseline is None: 
            # first learning step
            self.baseline = self.mreward
            fakt = 0.
            fakt2 = 0.          
        else: 
            #calc the gradients
            if reward1 != reward2:
                #gradient estimate alla SPSA but with likelihood gradient and normalization
                fakt = (reward1 - reward2) / (2. * self.bestEvaluation - reward1 - reward2) 
            else: 
                fakt=0.
            #normalized sigma gradient with moving average baseline
            norm = (self.bestEvaluation-self.baseline)
            if norm != 0.0:
                fakt2=(self.mreward-self.baseline)/(self.bestEvaluation-self.baseline)
            else:
                fakt2 = 0.0
        #update baseline        
        self.baseline = 0.9 * self.baseline + 0.1 * self.mreward             
        # update parameters and sigmas
        self.current = self.gd(fakt * deltas - self.current * self.sigList * self.wDecay)   
        if fakt2 > 0.: #for sigma adaption alg. follows only positive gradients
            if self.exploration == "global":         
                #apply sigma update globally        
                self.sigList = self.gdSig(fakt2 * ((self.deltas ** 2).sum() - (self.sigList ** 2).sum())
                                          / (self.sigList * float(self.numParameters)))
            elif self.exploration == "local":
                #apply sigma update locally
                self.sigList = self.gdSig(fakt2 * (deltas * deltas - self.sigList * self.sigList) / self.sigList) 
            elif self.exploration == "cma":
                #I have to think about that - needs also an option in perturbation
                raise NotImplementedError()
            else:
                raise NotImplementedError(str(self.exploration) + " not a known exploration parameter setting.")

########NEW FILE########
__FILENAME__ = spsa
__author__ = 'Frank Sehnke, sehnke@in.tum.de, Tom Schaul'

from scipy import random

from fd import FiniteDifferences


class SimpleSPSA(FiniteDifferences):
    """ Simultaneous Perturbation Stochastic Approximation.

    This class uses SPSA in general, but uses the likelihood gradient and a simpler exploration decay.
    """

    epsilon = 2. #Initial value of exploration size
    gamma = 0.9995 #Exploration decay factor
    batchSize = 2

    def _additionalInit(self):
        self.baseline = None #Moving average baseline, used just for visualisation

    def perturbation(self):
        # generates a uniform difference vector with the given epsilon
        deltas = (random.randint(0, 2, self.numParameters) * 2 - 1) * self.epsilon
        # reduce epsilon by factor gamma
        # as another simplification we let the exploration just decay with gamma.
        # Is similar to the decreasing exploration in SPSA but simpler.
        self.epsilon *= self.gamma
        return deltas

    def _learnStep(self):
        """ calculates the gradient and executes a step in the direction
            of the gradient, scaled with a learning rate alpha. """
        deltas = self.perturbation()
        #reward of positive and negative perturbations
        reward1 = self._oneEvaluation(self.current + deltas)
        reward2 = self._oneEvaluation(self.current - deltas)

        self.mreward = (reward1 + reward2) / 2.
        if self.baseline is None:
            # first learning step
            self.baseline = self.mreward * 0.99
            fakt = 0.
        else:
            #calc the gradients
            if reward1 != reward2:
                #gradient estimate alla SPSA but with likelihood gradient and normalization (see also "update parameters")
                fakt = (reward1 - reward2) / (2.0 * self.bestEvaluation - reward1 - reward2)
            else:
                fakt = 0.0
        self.baseline = 0.9 * self.baseline + 0.1 * self.mreward #update baseline

        # update parameters
        # as a simplification we use alpha = alpha * epsilon**2 for decaying the stepsize instead of the usual use method from SPSA
        # resulting in the same update rule like for PGPE
        self.current = self.gd(fakt * self.epsilon * self.epsilon / deltas)



########NEW FILE########
__FILENAME__ = hillclimber
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.optimization.optimizer import BlackBoxOptimizer
from scipy import exp
from random import random


class HillClimber(BlackBoxOptimizer):
    """ The simplest kind of stochastic search: hill-climbing in the fitness landscape. """

    evaluatorIsNoisy = False

    def _additionalInit(self):
        self._oneEvaluation(self._initEvaluable)

    def _learnStep(self):
        """ generate a new evaluable by mutation, compare them, and keep the best. """
        # re-evaluate the current individual in case the evaluator is noisy
        if self.evaluatorIsNoisy:
            self.bestEvaluation = self._oneEvaluation(self.bestEvaluable)

        # hill-climbing
        challenger = self.bestEvaluable.copy()
        challenger.mutate()
        self._oneEvaluation(challenger)

    @property
    def batchSize(self):
        if self.evaluatorIsNoisy:
            return 2
        else:
            return 1


class StochasticHillClimber(HillClimber):
    """ Stochastic hill-climbing always moves to a better point, but may also
    go to a worse point with a probability that decreases with increasing drop in fitness
    (and depends on a temperature parameter). """

    #: The larger the temperature, the more explorative (less greedy) it behaves.
    temperature = 1.

    def _learnStep(self):
        # re-evaluate the current individual in case the evaluator is noisy
        if self.evaluatorIsNoisy:
            self.bestEvaluation = self._oneEvaluation(self.bestEvaluable)

        # hill-climbing
        challenger = self.bestEvaluable.copy()
        challenger.mutate()
        newEval = self._oneEvaluation(challenger)

        # if the new evaluation was better, it got stored automatically. Otherwise:
        if ((not self.minimize and newEval < self.bestEvaluation) or
            (self.minimize and newEval > self.bestEvaluation)):
            acceptProbability = exp(-abs(newEval-self.bestEvaluation)/self.temperature)
            if random() < acceptProbability:
                self.bestEvaluable, self.bestEvaluation = challenger, newEval


########NEW FILE########
__FILENAME__ = innerinversememetic
__author__ = 'Tom Schaul, tom@idsia.ch'

from innermemetic import InnerMemeticSearch
from inversememetic import InverseMemeticSearch

class InnerInverseMemeticSearch(InnerMemeticSearch, InverseMemeticSearch):
    """ inverse of inner memetic search"""

    def _learnStep(self):
        self.switchMutations()
        InnerMemeticSearch._learnStep(self)
        self.switchMutations()
########NEW FILE########
__FILENAME__ = innermemetic
__author__ = 'Tom Schaul, tom@idsia.ch'

from memetic import MemeticSearch
from pybrain.optimization.populationbased.es import ES


class InnerMemeticSearch(ES, MemeticSearch):
    """ Population-based memetic search """

    mu = 5
    lambada = 5

    def _learnStep(self):
        self.switchMutations()
        ES._learnStep(self)
        self.switchMutations()

    @property
    def batchSize(self):
        if self.evaluatorIsNoisy:
            return (self.mu + self.lambada)*self.localSteps
        else:
            return self.lambada*self.localSteps

########NEW FILE########
__FILENAME__ = inversememetic
__author__ = 'Tom Schaul, tom@idsia.ch'

from memetic import MemeticSearch


class InverseMemeticSearch(MemeticSearch):
    """ Interleaving local search with topology search (inverse of memetic search) """

    def _learnStep(self):
        self.switchMutations()
        MemeticSearch._learnStep(self)
        self.switchMutations()

########NEW FILE########
__FILENAME__ = memetic
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.optimization.optimizer import BlackBoxOptimizer, TopologyOptimizer
from pybrain.optimization.hillclimber import HillClimber
from pybrain.structure.evolvables.maskedparameters import MaskedParameters


class MemeticSearch(HillClimber, TopologyOptimizer):
    """ Interleaving topology search with local search """

    localSteps = 50
    localSearchArgs = {}
    localSearch = HillClimber

    def switchMutations(self):
        """ interchange the mutate() and topologyMutate() operators """
        tm = self._initEvaluable.__class__.topologyMutate
        m = self._initEvaluable.__class__.mutate
        self._initEvaluable.__class__.topologyMutate = m
        self._initEvaluable.__class__.mutate = tm

    def _oneEvaluation(self, evaluable):
        if self.numEvaluations == 0:
            return BlackBoxOptimizer._oneEvaluation(self, evaluable)
        else:
            self.switchMutations()
            if isinstance(evaluable, MaskedParameters):
                evaluable.returnZeros = False
                x0 = evaluable.params
                evaluable.returnZeros = True
                def f(x):
                    evaluable._setParameters(x)
                    return BlackBoxOptimizer._oneEvaluation(self, evaluable)
            else:
                f = lambda x: BlackBoxOptimizer._oneEvaluation(self, x)
                x0 = evaluable
            outsourced = self.localSearch(f, x0,
                                          maxEvaluations = self.localSteps,
                                          desiredEvaluation = self.desiredEvaluation,
                                          minimize = self.minimize,
                                          **self.localSearchArgs)
            assert self.localSteps > outsourced.batchSize, 'localSteps too small ('+str(self.localSteps)+\
                                                '), because local search has a batch size of '+str(outsourced.batchSize)
            _, fitness = outsourced.learn()
            self.switchMutations()
            return fitness

    def _learnStep(self):
        self.switchMutations()
        HillClimber._learnStep(self)
        self.switchMutations()

    def _notify(self):
        HillClimber._notify(self)
        if self.verbose:
            print('  Bits on in best mask:', sum(self.bestEvaluable.mask))

    @property
    def batchSize(self):
        return self.localSteps

########NEW FILE########
__FILENAME__ = neldermead
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy.optimize import fmin

from pybrain.optimization.optimizer import ContinuousOptimizer


class DesiredFoundException(Exception):
    """ The desired target has been found. """


class NelderMead(ContinuousOptimizer):
    """Do the optimization using a simple wrapper for scipy's fmin."""

    # acceptable relative error in the evaluator for convergence.
    stopPrecision = 1e-6

    mustMinimize = True


    def _callback(self, *_):
        if self._stoppingCriterion():
            raise DesiredFoundException()

    def _learnStep(self):
        try:
            fmin(func = self._oneEvaluation,
                 x0 = self.bestEvaluable,
                 callback = self._callback,
                 ftol = self.stopPrecision,
                 maxfun = self.maxEvaluations-self.numEvaluations-1,
                 disp = self.verbose)
        except DesiredFoundException:
            pass
        # the algorithm has finished: no point in doing more steps.
        self.maxLearningSteps = self.numLearningSteps

########NEW FILE########
__FILENAME__ = optimizer
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import array, randn, ndarray, isinf, isnan, isscalar
import logging

from pybrain.utilities import setAllArgs, abstractMethod, DivergenceError
from pybrain.rl.learners.directsearch.directsearch import DirectSearchLearner
from pybrain.structure.parametercontainer import ParameterContainer
from pybrain.rl.environments.functions.function import FunctionEnvironment
from pybrain.rl.environments.fitnessevaluator import FitnessEvaluator
from pybrain.rl.environments.functions.transformations import oppositeFunction
from pybrain.structure.evolvables.maskedmodule import MaskedModule
from pybrain.structure.evolvables.maskedparameters import MaskedParameters
from pybrain.structure.evolvables.topology import TopologyEvolvable
from pybrain.structure.modules.module import Module


class BlackBoxOptimizer(DirectSearchLearner):
    """ The super-class for learning algorithms that treat the problem as a black box. 
    At each step they change the policy, and get a fitness value by invoking 
    the FitnessEvaluator (provided as first argument upon initialization).
    
    Evaluable objects can be lists or arrays of continuous values (also wrapped in ParameterContainer) 
    or subclasses of Evolvable (that define its methods).
    """    
    
    
    # some algorithms are designed for minimization only, those can put this flag:
    mustMinimize = False
    mustMaximize = False
        
    #: Is there a known value of sufficient fitness?
    desiredEvaluation = None    

    #: Stopping criterion based on number of evaluations.    
    maxEvaluations = 1e6 
        
    #: Stopping criterion based on number of learning steps. 
    maxLearningSteps = None    
    
    
    #: dimension of the search space, if applicable
    numParameters = None
    
    '''added by JPQ Boundaries of the search space, if applicable'''
    xBound = None
    feasible = None
    constrained = None
    violation = None
    # ---   
    
    #: Store all evaluations (in the ._allEvaluations list)?
    storeAllEvaluations = False
    #: Store all evaluated instances (in the ._allEvaluated list)?
    storeAllEvaluated = False
    
    # an optimizer can take different forms of evaluables, and depending on its
    # needs, wrap them into a ParameterContainer (which is also an Evolvable)
    # or unwrap them to act directly on the array of parameters (all ContinuousOptimizers)
    _wasWrapped = False
    _wasUnwrapped = False
    _wasOpposed = False
        
    listener = None
    
    #: provide console output during learning
    verbose = False
    
    # some algorithms have a predetermined (minimal) number of 
    # evaluations they will perform during each learningStep:
    batchSize = 1
    
    
    def __init__(self, evaluator = None, initEvaluable = None, **kwargs):
        """ The evaluator is any callable object (e.g. a lambda function). 
        Algorithm parameters can be set here if provided as keyword arguments. """
        # set all algorithm-specific parameters in one go:
        self.__minimize = None
        self.__evaluator = None
        setAllArgs(self, kwargs)
        # bookkeeping
        self.numEvaluations = 0      
        self.numLearningSteps = 0
        if self.storeAllEvaluated:
            self._allEvaluated = []
            self._allEvaluations = []
        elif self.storeAllEvaluations:
            self._allEvaluations = []
        
        if evaluator is not None:
            self.setEvaluator(evaluator, initEvaluable)        
            
    def _getMinimize(self): return self.__minimize
            
    def _setMinimize(self, flag):
        """ Minimization vs. maximization: priority to algorithm requirements, 
        then evaluator, default = maximize."""
        self.__minimize = flag
        opp = False
        if flag is True:
            if self.mustMaximize:
                opp = True
                self.__minimize = False
        if flag is False:
            if self.mustMinimize:
                opp = True
                self.__minimize = True       
        if self.__evaluator is not None:
            if opp is not self._wasOpposed: 
                self._flipDirection()
        self._wasOpposed = opp
        
    #: Minimize cost or maximize fitness? By default, all functions are maximized.    
    minimize = property(_getMinimize, _setMinimize)
        
    def setEvaluator(self, evaluator, initEvaluable = None):
        """ If not provided upon construction, the objective function can be given through this method.
        If necessary, also provide an initial evaluable."""
        
        # default settings, if provided by the evaluator:
        if isinstance(evaluator, FitnessEvaluator):
            if self.desiredEvaluation is None:
                self.desiredEvaluation = evaluator.desiredValue               
            if self.minimize is None:
                self.minimize = evaluator.toBeMinimized 
            # in some cases, we can deduce the dimension from the provided evaluator:
            if isinstance(evaluator, FunctionEnvironment):
                if self.numParameters is None:            
                    self.numParameters = evaluator.xdim
                elif self.numParameters is not evaluator.xdim:
                    raise ValueError("Parameter dimension mismatch: evaluator expects "+str(evaluator.xdim)\
                                     +" but it was set to "+str(self.numParameters)+".")
                '''added by JPQ to handle boundaries on the parameters'''
                self.evaluator = evaluator
                if self.xBound is None:            
                    self.xBound = evaluator.xbound
                if self.feasible is None:
                    self.feasible = evaluator.feasible
                if self.constrained is None:
                    self.constrained = evaluator.constrained
                if self.violation is None:
                    self.violation = evaluator.violation
                # ---
        # default: maximize
        if self.minimize is None:
            self.minimize = False
        self.__evaluator = evaluator
        if self._wasOpposed:
            self._flipDirection()
        #set the starting point for optimization (as provided, or randomly)
        self._setInitEvaluable(initEvaluable)        
        self.bestEvaluation = None
        self._additionalInit()
        self.bestEvaluable = self._initEvaluable
        
    def _flipDirection(self):
        self.__evaluator = oppositeFunction(self.__evaluator)
        if self.desiredEvaluation is not None:
            self.desiredEvaluation *= -1        
        
    def _additionalInit(self):
        """ a method for subclasses that need additional initialization code but don't want to redefine __init__ """

    def _setInitEvaluable(self, evaluable):
        if evaluable is None:
            # if there is no initial point specified, we start at one that's sampled 
            # normally around the origin.
            if self.numParameters is not None:
                evaluable = randn(self.numParameters)
            else:
                raise ValueError('Could not determine the dimensionality of the evaluator. '+\
                                 'Please provide an initial search point.')   
        if isinstance(evaluable, list):
            evaluable = array(evaluable)
        
        # If the evaluable is provided as a list of numbers or as an array,
        # we wrap it into a ParameterContainer.
        if isinstance(evaluable, ndarray):            
            pc = ParameterContainer(len(evaluable))
            pc._setParameters(evaluable)
            self._wasWrapped = True
            evaluable = pc
        self._initEvaluable = evaluable
        if isinstance(self._initEvaluable, ParameterContainer):
            if self.numParameters is None:            
                self.numParameters = len(self._initEvaluable)
            elif self.numParameters != len(self._initEvaluable):
                raise ValueError("Parameter dimension mismatch: evaluator expects "+str(self.numParameters)\
                                 +" but the evaluable has "+str(len(self._initEvaluable))+".")
                  
    
    def learn(self, additionalLearningSteps = None):
        """ The main loop that does the learning. """
        assert self.__evaluator is not None, "No evaluator has been set. Learning cannot start."
        if additionalLearningSteps is not None:
            self.maxLearningSteps = self.numLearningSteps + additionalLearningSteps - 1
        while not self._stoppingCriterion():
            try:
                self._learnStep()
                self._notify()
                self.numLearningSteps += 1
            except DivergenceError:
                logging.warning("Algorithm diverged. Stopped after "+str(self.numLearningSteps)+" learning steps.")
                break
            except ValueError:
                logging.warning("Something numerical went wrong. Stopped after "+str(self.numLearningSteps)+" learning steps.")
                break
        return self._bestFound()
        
    def _learnStep(self):
        """ The core method to be implemented by all subclasses. """
        abstractMethod()        
        
    def _bestFound(self):
        """ return the best found evaluable and its associated fitness. """
        bestE = self.bestEvaluable.params.copy() if self._wasWrapped else self.bestEvaluable
        if self._wasOpposed and isscalar(self.bestEvaluation):
            bestF = -self.bestEvaluation
        else:
            bestF = self.bestEvaluation
        return bestE, bestF
        
    def _oneEvaluation(self, evaluable):
        """ This method should be called by all optimizers for producing an evaluation. """
        if self._wasUnwrapped:
            self.wrappingEvaluable._setParameters(evaluable)
            res = self.__evaluator(self.wrappingEvaluable)
        elif self._wasWrapped:            
            res = self.__evaluator(evaluable.params)
        else:            
            res = self.__evaluator(evaluable)
            ''' added by JPQ '''
            if self.constrained :
                self.feasible = self.__evaluator.outfeasible
                self.violation = self.__evaluator.outviolation
            # ---
        if isscalar(res):
            # detect numerical instability
            if isnan(res) or isinf(res):
                raise DivergenceError
            # always keep track of the best
            if (self.numEvaluations == 0
                or self.bestEvaluation is None
                or (self.minimize and res <= self.bestEvaluation)
                or (not self.minimize and res >= self.bestEvaluation)):
                self.bestEvaluation = res
                self.bestEvaluable = evaluable.copy()
        
        self.numEvaluations += 1
        
        # if desired, also keep track of all evaluables and/or their fitness.                        
        if self.storeAllEvaluated:
            if self._wasUnwrapped:            
                self._allEvaluated.append(self.wrappingEvaluable.copy())
            elif self._wasWrapped:            
                self._allEvaluated.append(evaluable.params.copy())
            else:            
                self._allEvaluated.append(evaluable.copy())        
        if self.storeAllEvaluations:
            if self._wasOpposed and isscalar(res):
                ''' added by JPQ '''
                if self.constrained :
                    self._allEvaluations.append([-res,self.feasible,self.violation])
                # ---
                else:
                    self._allEvaluations.append(-res)
            else:
                ''' added by JPQ '''
                if self.constrained :
                    self._allEvaluations.append([res,self.feasible,self.violation])
                # ---
                else:
                    self._allEvaluations.append(res)
        ''' added by JPQ '''
        if self.constrained :
            return [res,self.feasible,self.violation]
        else:
        # ---
            return res
    
    def _stoppingCriterion(self):
        if self.maxEvaluations is not None and self.numEvaluations+self.batchSize > self.maxEvaluations:
            return True
        if self.desiredEvaluation is not None and self.bestEvaluation is not None and isscalar(self.bestEvaluation):
            if ((self.minimize and self.bestEvaluation <= self.desiredEvaluation)
                or (not self.minimize and self.bestEvaluation >= self.desiredEvaluation)):
                return True
        if self.maxLearningSteps is not None and self.numLearningSteps > self.maxLearningSteps:
            return True
        return False
    
    def _notify(self):
        """ Provide some feedback during the run. """
        if self.verbose:
            print('Step:', self.numLearningSteps, 'best:', self.bestEvaluation)
        if self.listener is not None:
            self.listener(self.bestEvaluable, self.bestEvaluation)
        
        
class ContinuousOptimizer(BlackBoxOptimizer):
    """ A more restricted class of black-box optimization algorithms
    that assume the parameters to be necessarily an array of continuous values 
    (which can be wrapped in a ParameterContainer). """    
            
    def _setInitEvaluable(self, evaluable):
        """ If the parameters are wrapped, we keep track of the wrapper explicitly. """
        if isinstance(evaluable, ParameterContainer):
            self.wrappingEvaluable = evaluable.copy()
            self._wasUnwrapped = True
        elif not (evaluable is None 
                  or isinstance(evaluable, list) 
                  or isinstance(evaluable, ndarray)):
            raise ValueError('Continuous optimization algorithms require a list, array or'+\
                             ' ParameterContainer as evaluable.')
        BlackBoxOptimizer._setInitEvaluable(self, evaluable)
        self._wasWrapped = False
        self._initEvaluable = self._initEvaluable.params.copy()     
        
    def _bestFound(self):
        """ return the best found evaluable and its associated fitness. """
        bestE, bestF = BlackBoxOptimizer._bestFound(self)
        if self._wasUnwrapped:
            self.wrappingEvaluable._setParameters(bestE)
            bestE = self.wrappingEvaluable.copy()
        return bestE, bestF
   

class TopologyOptimizer(BlackBoxOptimizer):
    """ A class of algorithms that changes the topology as well as the parameters.
    It does not accept an arbitrary Evolvable as initial point, only a 
    ParameterContainer (or a continuous vector). """
        
    def _setInitEvaluable(self, evaluable):
        BlackBoxOptimizer._setInitEvaluable(self, evaluable)
        # distinguish modules from parameter containers.
        if not isinstance(evaluable, TopologyEvolvable):
            if isinstance(evaluable, Module):
                self._initEvaluable = MaskedModule(self._initEvaluable)
            else:
                self._initEvaluable = MaskedParameters(self._initEvaluable, returnZeros = True)   
    
     

########NEW FILE########
__FILENAME__ = coevolution
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import argmax, array
from random import sample, choice, shuffle

from pybrain.utilities import fListToString, Named


class Coevolution(Named):
    """ Population-based generational evolutionary algorithm
    with fitness being based (paritally) on a relative measure. """

    # algorithm parameters
    populationSize = 50
    selectionProportion = 0.5
    elitism = False
    parentChildAverage = 1. # proportion of the child
    tournamentSize = None
    hallOfFameEvaluation = 0. # proportion of HoF evaluations in relative fitness
    useSharedSampling = False

    # an external absolute evaluator
    absEvaluator = None
    absEvalProportion = 0

    # execution settings
    maxGenerations = None
    maxEvaluations = None
    verbose = False

    def __init__(self, relEvaluator, seeds, **args):
        """
        :arg relevaluator: an anti-symmetric function that can evaluate 2 elements
        :arg seeds: a list of initial guesses
        """
        # set parameters
        self.setArgs(**args)
        self.relEvaluator = relEvaluator
        if self.tournamentSize == None:
            self.tournamentSize = self.populationSize

        # initialize algorithm variables
        self.steps = 0
        self.generation = 0

        # the best host and the best parasite from each generation
        self.hallOfFame = []

        # the relative fitnesses from each generation (of the selected individuals)
        self.hallOfFitnesses = []

        # this dictionary stores all the results between 2 players (first one starting):
        #  { (player1, player2): [games won, total games, cumulative score, list of scores] }
        self.allResults = {}

        # this dictionary stores the opponents a player has played against.
        self.allOpponents = {}

        # a list of all previous populations
        self.oldPops = []

        # build initial populations
        self._initPopulation(seeds)

    def learn(self, maxSteps=None):
        """ Toplevel function, can be called iteratively.

        :return: best evaluable found in the last generation. """
        if maxSteps != None:
            maxSteps += self.steps
        while True:
            if maxSteps != None and self.steps + self._stepsPerGeneration() > maxSteps:
                break
            if self.maxEvaluations != None and self.steps + self._stepsPerGeneration() > self.maxEvaluations:
                break
            if self.maxGenerations != None and self.generation >= self.maxGenerations:
                break
            self._oneGeneration()
        return self.hallOfFame[-1]

    def _oneGeneration(self):
        self.oldPops.append(self.pop)
        self.generation += 1
        fitnesses = self._evaluatePopulation()
        # store best in hall of fame
        besti = argmax(array(fitnesses))
        best = self.pop[besti]
        bestFits = sorted(fitnesses)[::-1][:self._numSelected()]
        self.hallOfFame.append(best)
        self.hallOfFitnesses.append(bestFits)

        if self.verbose:
            print('Generation', self.generation)
            print('        relat. fits:', fListToString(sorted(fitnesses), 4))
            if len(best.params) < 20:
                print('        best params:', fListToString(best.params, 4))

        self.pop = self._selectAndReproduce(self.pop, fitnesses)

    def _averageWithParents(self, pop, childportion):
        for i, p in enumerate(pop[:]):
            if p.parent != None:
                tmp = p.copy()
                tmp.parent = p.parent
                tmp._setParameters(p.params * childportion + p.parent.params * (1 - childportion))
                pop[i] = tmp

    def _evaluatePopulation(self):
        hoFtournSize = min(self.generation, int(self.tournamentSize * self.hallOfFameEvaluation))
        tournSize = self.tournamentSize - hoFtournSize
        if self.useSharedSampling:
            opponents = self._sharedSampling(tournSize, self.pop, self.oldPops[-1])
        else:
            opponents = self.pop
        if len(opponents) < tournSize:
            tournSize = len(opponents)
        self._doTournament(self.pop, opponents, tournSize)
        if hoFtournSize > 0:
            hoF = list(set(self.hallOfFame))
            self._doTournament(self.pop, hoF, hoFtournSize)
        fitnesses = []
        for p in self.pop:
            fit = 0
            for opp in opponents:
                fit += self._beats(p, opp)
            if hoFtournSize > 0:
                for opp in hoF:
                    fit += self._beats(p, opp)
            if self.absEvalProportion > 0 and self.absEvaluator != None:
                fit = (1 - self.absEvalProportion) * fit + self.absEvalProportion * self.absEvaluator(p)
            fitnesses.append(fit)
        return fitnesses

    def _initPopulation(self, seeds):
        if self.parentChildAverage < 1:
            for s in seeds:
                s.parent = None
        self.pop = self._extendPopulation(seeds, self.populationSize)

    def _extendPopulation(self, seeds, size):
        """ build a population, with mutated copies from the provided
        seed pool until it has the desired size. """
        res = seeds[:]
        for dummy in range(size - len(seeds)):
            chosen = choice(seeds)
            tmp = chosen.copy()
            tmp.mutate()
            if self.parentChildAverage < 1:
                tmp.parent = chosen
            res.append(tmp)
        return res

    def _selectAndReproduce(self, pop, fits):
        """ apply selection and reproduction to host population, according to their fitness."""
        # combine population with their fitness, then sort, only by fitness
        s = zip(fits, pop)
        shuffle(s)
        s.sort(key=lambda x:-x[0])
        # select...
        selected = map(lambda x: x[1], s[:self._numSelected()])
        # ... and reproduce
        if self.elitism:
            newpop = self._extendPopulation(selected, self.populationSize)
            if self.parentChildAverage < 1:
                self._averageWithParents(newpop, self.parentChildAverage)
        else:
            newpop = self._extendPopulation(selected, self.populationSize
                                            + self._numSelected()) [self._numSelected():]
            if self.parentChildAverage < 1:
                self._averageWithParents(newpop[self._numSelected():], self.parentChildAverage)
        return newpop

    def _beats(self, h, p):
        """ determine the empirically observed score of p playing opp (starting or not).
        If they never played, assume 0. """
        if (h, p) not in self.allResults:
            return 0
        else:
            hpgames, hscore = self.allResults[(h, p)][1:3]
            phgames, pscore = self.allResults[(p, h)][1:3]
            return (hscore - pscore) / float(hpgames + phgames)

    def _doTournament(self, pop1, pop2, tournamentSize=None):
        """ Play a tournament.

        :key tournamentSize: If unspecified, play all-against-all
        """
        # TODO: Preferably select high-performing opponents?
        for p in pop1:
            pop3 = pop2[:]
            while p in pop3:
                pop3.remove(p)
            if tournamentSize != None and tournamentSize < len(pop3):
                opps = sample(pop3, tournamentSize)
            else:
                opps = pop3
            for opp in opps:
                self._relEval(p, opp)
                self._relEval(opp, p)

    def _globalScore(self, p):
        """ The average score over all evaluations for a player. """
        if p not in self.allOpponents:
            return 0.
        scoresum, played = 0., 0
        for opp in self.allOpponents[p]:
            scoresum += self.allResults[(p, opp)][2]
            played += self.allResults[(p, opp)][1]
            scoresum -= self.allResults[(opp, p)][2]
            played += self.allResults[(opp, p)][1]
        # slightly bias the global score in favor of players with more games (just for tie-breaking)
        played += 0.01
        return scoresum / played

    def _sharedSampling(self, numSelect, selectFrom, relativeTo):
        """ Build a shared sampling set of opponents """
        if numSelect < 1:
            return []
        # determine the player of selectFrom with the most wins against players from relativeTo (and which ones)
        tmp = {}
        for p in selectFrom:
            beaten = []
            for opp in relativeTo:
                if self._beats(p, opp) > 0:
                    beaten.append(opp)
            tmp[p] = beaten
        beatlist = map(lambda (p, beaten): (len(beaten), self._globalScore(p), p), tmp.items())
        shuffle(beatlist)
        beatlist.sort(key=lambda x: x[:2])
        best = beatlist[-1][2]
        unBeaten = list(set(relativeTo).difference(tmp[best]))
        otherSelect = selectFrom[:]
        otherSelect.remove(best)
        return [best] + self._sharedSampling(numSelect - 1, otherSelect, unBeaten)

    def _relEval(self, p, opp):
        """ a single relative evaluation (in one direction) with the involved bookkeeping."""
        if p not in self.allOpponents:
            self.allOpponents[p] = []
        self.allOpponents[p].append(opp)
        if (p, opp) not in self.allResults:
            self.allResults[(p, opp)] = [0, 0, 0., []]
        res = self.relEvaluator(p, opp)
        if res > 0:
            self.allResults[(p, opp)][0] += 1
        self.allResults[(p, opp)][1] += 1
        self.allResults[(p, opp)][2] += res
        self.allResults[(p, opp)][3].append(res)
        self.steps += 1

    def __str__(self):
        s = 'Coevolution ('
        s += str(self._numSelected())
        if self.elitism:
            s += '+' + str(self.populationSize - self._numSelected())
        else:
            s += ',' + str(self.populationSize)
        s += ')'
        if self.parentChildAverage < 1:
            s += ' p_c_avg=' + str(self.parentChildAverage)
        return s

    def _numSelected(self):
        return int(self.populationSize * self.selectionProportion)

    def _stepsPerGeneration(self):
        res = self.populationSize * self.tournamentSize * 2
        return res





if __name__ == '__main__':
    # TODO: convert to unittest
    x = Coevolution(None, [None], populationSize=1)
    x.allResults[(1, 2)] = [1, 1, 1, []]
    x.allResults[(2, 1)] = [-1, 1, -1, []]
    x.allResults[(2, 5)] = [1, 1, 2, []]
    x.allResults[(5, 2)] = [-1, 1, -1, []]
    x.allResults[(2, 3)] = [1, 1, 3, []]
    x.allResults[(3, 2)] = [-1, 1, -1, []]
    x.allResults[(4, 3)] = [1, 1, 4, []]
    x.allResults[(3, 4)] = [-1, 1, -1, []]
    x.allOpponents[1] = [2]
    x.allOpponents[2] = [1, 5]
    x.allOpponents[3] = [2, 4]
    x.allOpponents[4] = [3]
    x.allOpponents[5] = [2]
    print(x._sharedSampling(4, [1, 2, 3, 4, 5], [1, 2, 3, 4, 6, 7, 8, 9]))
    print('should be', [4, 1, 2, 5])

########NEW FILE########
__FILENAME__ = competitivecoevolution
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.optimization.coevolution.coevolution import Coevolution


class CompetitiveCoevolution(Coevolution):
    """ Coevolution with 2 independent populations, and competitive fitness sharing. """

    def __str__(self):
        return 'Competitive' + Coevolution.__str__(self)

    def _initPopulation(self, seeds):
        """ one half for each population """
        if self.parentChildAverage < 1:
            for s in seeds:
                s.parent = None
        if len(seeds) > 1:
            s1 = seeds[:len(seeds) / 2]
            s2 = seeds[len(seeds) / 2:]
        else:
            # not enough seeds: randomize
            s1 = seeds
            tmp = seeds[0].copy()
            tmp.randomize()
            s2 = [tmp]
        self.pop = self._extendPopulation(s1, self.populationSize)
        self.parasitePop = self._extendPopulation(s2, self.populationSize)

    def _competitiveSharedFitness(self, hosts, parasites):
        """ determine the competitive shared fitness for the population of hosts, w.r. to
        the population of parasites. """
        if len(parasites) == 0:
            return [0] * len(hosts)

        # determine beat-sum for parasites (nb of games lost)
        beatsums = {}
        for p in parasites:
            beatsums[p] = 0.
            for h in hosts:
                if self._beats(h, p) > 0:
                    beatsums[p] += 1

        # determine fitnesses for hosts
        fitnesses = []
        for h in hosts:
            hsum = 0
            unplayed = 0
            for p in parasites:
                if self._beats(h, p) > 0:
                    assert beatsums[p] > 0
                    hsum += 1. / beatsums[p]
                elif self._beats(h, p) == 0:
                    unplayed += 1
            # take into account the number of parasites played, to avoid
            # biasing for old agents in the elitist case
            if len(parasites) > unplayed:
                hsum /= float(len(parasites) - unplayed)

            # this is purely for breaking ties in favor of globally better players:
            hsum += 1e-5 * self._globalScore(h)
            fitnesses.append(hsum)
        return fitnesses

    def _evaluatePopulation(self):
        hoFtournSize = min(self.generation, int(self.tournamentSize * self.hallOfFameEvaluation))
        tournSize = self.tournamentSize - hoFtournSize
        if self.useSharedSampling and self.generation > 2:
            opponents = self._sharedSampling(tournSize, self.parasitePop, self.oldPops[-2])
        else:
            opponents = self.parasitePop
        if len(opponents) < tournSize:
            tournSize = len(opponents)
        self._doTournament(self.pop, opponents, tournSize)
        if hoFtournSize > 0:
            self._doTournament(self.pop, self.hallOfFame, hoFtournSize)

        fit = self._competitiveSharedFitness(self.pop, self.parasitePop)
        if hoFtournSize > 0:
            fitHof = self._competitiveSharedFitness(self.pop, self.hallOfFame)
            fit = map(lambda (f1, f2): tournSize * f1 + hoFtournSize * f2, zip(fit, fitHof))
        return fit

    def _oneGeneration(self):
        Coevolution._oneGeneration(self)
        # change roles between parasites and hosts
        tmp = self.pop
        self.pop = self.parasitePop
        self.parasitePop = tmp






if __name__ == '__main__':
    from pybrain.utilities import fListToString
    # TODO: convert to unittest
    C = CompetitiveCoevolution(None, [1, 2, 3, 4, 5, 6, 7, 8], populationSize=4)
    def b(x, y):
        C.allResults[(x, y)] = [1, 1, 1, []]
        C.allResults[(y, x)] = [-1, 1, -1, []]
        if x not in C.allOpponents:
            C.allOpponents[x] = []
        if y not in C.allOpponents:
            C.allOpponents[y] = []
        C.allOpponents[x].append(y)
        C.allOpponents[y].append(x)

    b(1, 6)
    b(1, 7)
    b(8, 1)
    b(5, 2)
    b(6, 2)
    b(8, 2)
    b(3, 5)
    b(3, 6)
    b(3, 7)
    b(4, 5)
    b(4, 7)
    b(8, 4)
    print(C.pop)
    print(C.parasitePop)
    print('          ', fListToString(C._competitiveSharedFitness(C.pop, C.parasitePop), 2))
    print('should be:', fListToString([0.83, 0.00, 1.33, 0.83], 2))


########NEW FILE########
__FILENAME__ = multipopulationcoevolution
__author__ = 'Tom Schaul, tom@idsia.ch'

from random import choice

from pybrain.optimization.coevolution.coevolution import Coevolution


class MultiPopulationCoevolution(Coevolution):
    """ Coevolution with a number of independent populations. """

    numPops = 10

    def __str__(self):
        return 'MultiPop'+str(self.numPops)+Coevolution.__str__(self)

    def _initPopulation(self, seeds):
        """ one part of the seeds for each population, if there's not enough: randomize. """
        for s in seeds:
            s.parent = None
        while len(seeds) < self.numPops:
            tmp = choice(seeds).copy()
            tmp.randomize()
            seeds.append(tmp)
        self.pops = []
        for i in range(self.numPops):
            si = seeds[i::self.numPops]
            self.pops.append(self._extendPopulation(si, self.populationSize))
        self.mainpop = 0
        self.pop = self.pops[self.mainpop]

    def _evaluatePopulation(self):
        """Each individual in main pop plays against
        tournSize others of each other population (the best part of them). """
        for other in self.pops:
            if other == self.pop:
                continue
            # TODO: parametrize
            bestPart = len(other)/2
            if bestPart < 1:
                bestPart = 1
            self._doTournament(self.pop, other[:bestPart], self.tournamentSize)

        fitnesses = []
        for p in self.pop:
            fit = 0
            for other in self.pops:
                if other == self.pop:
                    continue
                for opp in other:
                    fit += self._beats(p, opp)
            if self.absEvalProportion > 0 and self.absEvaluator != None:
                fit = (1-self.absEvalProportion) * fit + self.absEvalProportion * self.absEvaluator(p)
            fitnesses.append(fit)
        return fitnesses

    def _oneGeneration(self):
        Coevolution._oneGeneration(self)
        # change the main pop
        self.pops[self.mainpop] = self.pop
        self.mainpop = self.generation % self.numPops
        self.pop = self.pops[self.mainpop]

    def _stepsPerGeneration(self):
        if self.tournamentSize == None:
            return 2 * self.populationSize** 2
        else:
            return Coevolution._stepsPerGeneration(self)

########NEW FILE########
__FILENAME__ = es
__author__ = 'Julian Togelius and Tom Schaul, tom@idsia.ch'

from random import shuffle

from pybrain.optimization.optimizer import BlackBoxOptimizer


class ES(BlackBoxOptimizer):
    """ Standard evolution strategy, (mu +/, lambda). """

    mu = 50
    lambada = 50

    evaluatorIsNoisy = False

    storeHallOfFame = True

    mustMaximize = True

    elitism = False

    def _additionalInit(self):
        assert self.lambada % self.mu == 0, 'lambda ('+str(self.lambada)+\
                                            ') must be multiple of mu ('+str(self.mu)+').'
        self.hallOfFame = []
        # population is a list of (fitness, individual) tuples.
        self.population = [(self._oneEvaluation(self._initEvaluable), self._initEvaluable)] * self._popsize
        map(self._replaceByMutation, range(1, self._popsize))
        self._sortPopulation()

    @property
    def _popsize(self):
        if self.elitism:
            return self.mu + self.lambada
        else:
            return self.lambada

    def _replaceByMutation(self, index):
        x = self.population[index][1].copy()
        x.mutate()
        self.population[index] = (self._oneEvaluation(x), x)

    def _learnStep(self):
        # re-evaluate the mu individuals if the fitness function is noisy
        if self.evaluatorIsNoisy:
            for i in range (self.mu):
                x = self.population[i][1]
                self.population[i] = (self._oneEvaluation(x), x)
            self._sortPopulation(noHallOfFame = True)

        # produce offspring from the the mu best ones
        self.population = self.population[:self.mu] * (self._popsize/self.mu)

        # mutate the offspring
        if self.elitism:
            map(self._replaceByMutation, range(self.mu, self._popsize))
        else:
            map(self._replaceByMutation, range(self._popsize))

        self._sortPopulation()

    def _sortPopulation(self, noHallOfFame = False):
        # shuffle-sort the population and fitnesses
        shuffle(self.population)
        self.population.sort(key = lambda x: -x[0])
        if self.storeHallOfFame and not noHallOfFame:
            # the best per generation stored here
            self.hallOfFame.append(self.population[0][1])

    @property
    def batchSize(self):
        if self.evaluatorIsNoisy:
            return self._popsize
        else:
            return self.lambada

    def __str__(self):
        if self.elitism:
            return 'ES('+str(self.mu)+'+'+str(self.lambada)+')'
        else:
            return 'ES('+str(self.mu)+','+str(self.lambada)+')'

########NEW FILE########
__FILENAME__ = evolution
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.utilities import abstractMethod
from pybrain.optimization.optimizer import BlackBoxOptimizer


class Evolution(BlackBoxOptimizer):
    """ Base class for evolutionary algorithms, seen as function optimizers. """

    populationSize = 10

    storeAllPopulations = False

    mustMaximize = True

    def _additionalInit(self):
        self.currentpop = []
        self.fitnesses = []
        self._allGenerations = []
        self.initPopulation()

    def _learnStep(self):
        """ do one generation step """
        self.fitnesses = [self._oneEvaluation(indiv) for indiv in self.currentpop]
        if self.storeAllPopulations:
            self._allGenerations.append((self.currentpop, self.fitnesses))
        self.produceOffspring()

    def initPopulation(self):
        """ initialize the population """
        abstractMethod()

    def produceOffspring(self):
        """ generate the new generation of offspring, given the current population, and their fitnesses """
        abstractMethod()

    @property
    def batchSize(self):
        return self.populationSize
########NEW FILE########
__FILENAME__ = ga
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import randn, zeros
from scipy import random as rd, array
from random import choice, random, gauss, shuffle, sample
from numpy import ndarray

from pybrain.optimization.populationbased.evolution import Evolution
from pybrain.optimization.optimizer import ContinuousOptimizer


class GA(ContinuousOptimizer, Evolution):
    """ Standard Genetic Algorithm. """

    #: selection scheme
    tournament = False
    tournamentSize = 2

    #: selection proportion
    topProportion = 0.2

    elitism = False
    eliteProportion = 0.5
    _eliteSize = None # override with an exact number

    #: mutation probability
    mutationProb = 0.1
    mutationStdDev = 0.5
    initRangeScaling = 10.

    initialPopulation = None
    
    mustMaximize = True

    '''added by JPQ'''
    def initBoundaries(self):
        assert len(self.xBound) == self.numParameters
        self.mins = array([min_ for min_, max_ in self.xBound])
        self.maxs = array([max_ for min_, max_ in self.xBound])
    # ---        
    def initPopulation(self):
        '''added by JPQ '''
        if self.xBound is not None:
            self.initBoundaries()
        # ---
        if self.initialPopulation is not None:
            self.currentpop = self.initialPopulation
        else:
            self.currentpop = [self._initEvaluable]
            for _ in range(self.populationSize-1):
                '''added by JPQ '''
                if self.xBound is None:
                # ---
                    self.currentpop.append(self._initEvaluable+randn(self.numParameters)
                                       *self.mutationStdDev*self.initRangeScaling)
                    '''added by JPQ '''
                else:
                    position = rd.random(self.numParameters)
                    position *= (self.maxs-self.mins)
                    position += self.mins
                    self.currentpop.append(position)
                    # ---

    def crossOverOld(self, parents, nbChildren):
        """ generate a number of children by doing 1-point cross-over """
        xdim = self.numParameters
        children = []
        for _ in range(nbChildren):
            p1 = choice(parents)
            if xdim < 2:
                children.append(p1)
            else:
                p2 = choice(parents)
                point = choice(range(xdim-1))
                point += 1
                res = zeros(xdim)
                res[:point] = p1[:point]
                res[point:] = p2[point:]
                children.append(res)
        return children
        
    def mutatedOld(self, indiv):
        """ mutate some genes of the given individual """
        res = indiv.copy()
        for i in range(self.numParameters):
            if random() < self.mutationProb:
                res[i] = indiv[i] + gauss(0, self.mutationStdDev)
        return res
        
    ''' added by JPQ in replacement of crossover and mutated '''    
    def crossOver(self, parents, nbChildren):
        """ generate a number of children by doing 1-point cross-over """
        """ change as the <choice> return quite often the same p1 and even
            several time p2 was return the same than p1 """
        xdim = self.numParameters
        shuffle(parents)
        children = []
        for i in range(len(parents)/2):
            p1 = parents[i]
            p2 = parents[i+(len(parents)/2)]
            if xdim < 2:
                children.append(p1)
                children.append(p2)
            else:
                point = choice(range(xdim-1))
                point += 1
                res = zeros(xdim)
                res[:point] = p1[:point]
                res[point:] = p2[point:]
                children.append(res)
                res = zeros(xdim)
                res[:point] = p2[:point]
                res[point:] = p1[point:]
                children.append(res)
        shuffle(children)
        if len(children) > nbChildren:
            children = children[:nbChildren]  
        elif len(children) < nbChildren:
            children +=sample(children,(nbChildren-len(children)))  
        return children
        
    def childexist(self,indiv,pop):
        if isinstance(pop,list):
            for i in range(len(pop)):
                if all((abs(indiv[k] - pop[i][k])/(self.maxs[k]-self.mins[k]))
                        < 1.e-7 for k in xrange(self.numParameters)):
                    return True
        return False
        
    def mutated(self, indiv):
        """ mutate some genes of the given individual """
        res = indiv.copy()
        #to avoid having a child identical to one of the currentpopulation'''
        for i in range(self.numParameters):
            if random() < self.mutationProb:
                if self.xBound is None:
                    res[i] = indiv[i] + gauss(0, self.mutationStdDev)
                else:
                    res[i] = max(min(indiv[i] + gauss(0, self.mutationStdDev),self.maxs[i]),
                             self.mins[i])
        return res

    def old_jpq_mutated(self, indiv, pop):
        """ mutate some genes of the given individual """
        res = indiv.copy()
        #to avoid having a child identical to one of the currentpopulation'''
        in_pop = self.childexist(indiv,pop)
        for i in range(self.numParameters):
            if random() < self.mutationProb:
                res[i] = max(min(indiv[i] + gauss(0, self.mutationStdDev),self.maxs[i]),
                             self.mins[i])
            
            if random() < self.mutationProb or in_pop:
                if self.xBound is None:
                    res[i] = indiv[i] + gauss(0, self.mutationStdDev)
                else:
                    if in_pop:
                        cmin = abs(indiv[i] - self.mins[i])/(self.maxs[i]-self.mins[i])
                        cmax = abs(indiv[i] - self.maxs[i])/(self.maxs[i]-self.mins[i])
                        if cmin < 1.e-7 or cmax < 1.e-7:
                            res[i] = self.mins[i] + random()*random()*(self.maxs[i]-self.mins[i])
                        else:
                            res[i] = max(min(indiv[i] + gauss(0, self.mutationStdDev),self.maxs[i]),
                             self.mins[i])
                    else:
                        res[i] = max(min(indiv[i] + gauss(0, self.mutationStdDev),self.maxs[i]),
                             self.mins[i])

        return res
    # ---
    
    @property
    def selectionSize(self):
        """ the number of parents selected from the current population """
        return int(self.populationSize * self.topProportion)

    @property
    def eliteSize(self):
        if self.elitism:
            if self._eliteSize != None:
                return self._eliteSize
            else:
                return int(self.populationSize * self.eliteProportion)
        else:
            return 0

    def select(self):
        """ select some of the individuals of the population, taking into account their fitnesses

        :return: list of selected parents """
        if not self.tournament:
            tmp = zip(self.fitnesses, self.currentpop)
            tmp.sort(key = lambda x: x[0])
            tmp2 = list(reversed(tmp))[:self.selectionSize]
            return map(lambda x: x[1], tmp2)
        else:
            # TODO: tournament selection
            raise NotImplementedError()

    def produceOffspring(self):
        """ produce offspring by selection, mutation and crossover. """
        parents = self.select()
        es = min(self.eliteSize, self.selectionSize)
        self.currentpop = parents[:es]
        '''Modified by JPQ '''
        nbchildren = self.populationSize - es
        if self.populationSize - es <= 0:
            nbchildren = len(parents)
        for child in self.crossOver(parents, nbchildren ):
            self.currentpop.append(self.mutated(child))
        # ---


########NEW FILE########
__FILENAME__ = constnsga2
__author__ = 'proposed by Jean Pierre Queau , jeanpierre.queau"sbmoffshore.com'


from scipy import array

from pybrain.optimization.optimizer import BlackBoxOptimizer
from pybrain.optimization.populationbased.ga import GA
from pybrain.tools.nondominated import const_non_dominated_front, const_crowding_distance, const_non_dominated_sort

# TODO: not very elegant, because of the conversions between tuples and arrays all the time...


class ConstMultiObjectiveGA(GA):
    """ Constrained Multi-objective Genetic Algorithm: the fitness is a vector with one entry per objective.
    By default we use NSGA-II selection. """

    topProportion = 0.5
    elitism = True

    populationSize = 100
    mutationStdDev = 1.

    allowEquality = True

    mustMaximize = True

    def _learnStep(self):
        """ do one generation step """
        # evaluate fitness
        if isinstance(self.fitnesses,dict):
            oldfitnesses = self.fitnesses
            self.fitnesses = dict()
            for indiv in self.currentpop:
                if tuple(indiv) in oldfitnesses:
                    self.fitnesses[tuple(indiv)] = oldfitnesses[tuple(indiv)]
                else:
                    self.fitnesses[tuple(indiv)] = self._oneEvaluation(indiv)
            del oldfitnesses
        else:
            self.fitnesses = dict([(tuple(indiv), self._oneEvaluation(indiv)) for indiv in self.currentpop])
        if self.storeAllPopulations:
            self._allGenerations.append((self.currentpop, self.fitnesses))

        if self.elitism:
            self.bestEvaluable = list(const_non_dominated_front(map(tuple, self.currentpop),
                                                          key=lambda x: self.fitnesses[x],
                                                          allowequality = self.allowEquality))
        else:
            self.bestEvaluable = list(const_non_dominated_front(map(tuple, self.currentpop)+self.bestEvaluable,
                                                          key=lambda x: self.fitnesses[x],
                                                          allowequality = self.allowEquality))
        self.bestEvaluation = [self.fitnesses[indiv] for indiv in self.bestEvaluable]
#        self.eliteProportion = float(len(self.bestEvaluable))/self.populationSize
#        number_of_feasible = const_number_of_feasible_pop(map(tuple, self.currentpop),
#                                                          key=lambda x: self.fitnesses[x],
#                                                          allowequality = self.allowEquality)
#        self.topProportion = float(number_of_feasible)/self.populationSize
#        print('Len bestEvaluable ',len(self.bestEvaluable))
#        for i in range(len(self.bestEvaluable)):
#            print(self.bestEvaluable[i],':',self.bestEvaluation[i])
        self.produceOffspring()

    def select(self):
        return map(array, nsga2select(map(tuple, self.currentpop), self.fitnesses,
                                      self.selectionSize, self.allowEquality))



def nsga2select(population, fitnesses, survivors, allowequality = True):
    """The NSGA-II selection strategy (Deb et al., 2002).
    The number of individuals that survive is given by the survivors parameter."""
    fronts = const_non_dominated_sort(population,
                                key=lambda x: fitnesses[x],
                                allowequality = allowequality)
    
    individuals = set()
    for front in fronts:
        remaining = survivors - len(individuals)
        if not remaining > 0:
            break
        if len(front) > remaining:
            # If the current front does not fit in the spots left, use those
            # that have the biggest crowding distance.
            crowd_dist = const_crowding_distance(front, fitnesses)
            front = sorted(front, key=lambda x: crowd_dist[x], reverse=True)
            front = set(front[:remaining])
        individuals |= front

    return list(individuals)

########NEW FILE########
__FILENAME__ = nsga2
__author__ = 'Justin Bayer, Tom Schaul, {justin,tom}@idsia.ch'


from scipy import array

from pybrain.optimization.populationbased.ga import GA
from pybrain.tools.nondominated import non_dominated_front, crowding_distance, non_dominated_sort

# TODO: not very elegant, because of the conversions between tuples and arrays all the time...


class MultiObjectiveGA(GA):
    """ Multi-objective Genetic Algorithm: the fitness is a vector with one entry per objective.
    By default we use NSGA-II selection. """

    topProportion = 0.5
    elitism = True

    populationSize = 100
    mutationStdDev = 1.

    allowEquality = True

    mustMaximize = True

    def _learnStep(self):
        """ do one generation step """
        # evaluate fitness
        """ added by JPQ """
        if isinstance(self.fitnesses,dict):
            oldfitnesses = self.fitnesses
            self.fitnesses = dict()
            for indiv in self.currentpop:
                if tuple(indiv) in oldfitnesses:
                    self.fitnesses[tuple(indiv)] = oldfitnesses[tuple(indiv)]
                else:
                    self.fitnesses[tuple(indiv)] = self._oneEvaluation(indiv)
            del oldfitnesses
        else:
        # ---
            self.fitnesses = dict([(tuple(indiv), self._oneEvaluation(indiv)) for indiv in self.currentpop])

        if self.storeAllPopulations:
            self._allGenerations.append((self.currentpop, self.fitnesses))

        if self.elitism:
            self.bestEvaluable = list(non_dominated_front(map(tuple, self.currentpop),
                                                          key=lambda x: self.fitnesses[x],
                                                          allowequality = self.allowEquality))
        else:
            self.bestEvaluable = list(non_dominated_front(map(tuple, self.currentpop)+self.bestEvaluable,
                                                          key=lambda x: self.fitnesses[x],
                                                          allowequality = self.allowEquality))
        self.bestEvaluation = [self.fitnesses[indiv] for indiv in self.bestEvaluable]

        self.produceOffspring()

    def select(self):
        return map(array, nsga2select(map(tuple, self.currentpop), self.fitnesses,
                                      self.selectionSize, self.allowEquality))



def nsga2select(population, fitnesses, survivors, allowequality = True):
    """The NSGA-II selection strategy (Deb et al., 2002).
    The number of individuals that survive is given by the survivors parameter."""
    fronts = non_dominated_sort(population,
                                key=lambda x: fitnesses[x],
                                allowequality = allowequality)
    
    individuals = set()
    for front in fronts:
        remaining = survivors - len(individuals)
        if not remaining > 0:
            break
        if len(front) > remaining:
            # If the current front does not fit in the spots left, use those
            # that have the biggest crowding distance.
            crowd_dist = crowding_distance(front, fitnesses)
            front = sorted(front, key=lambda x: crowd_dist[x], reverse=True)
            front = set(front[:remaining])
        individuals |= front

    return list(individuals)

########NEW FILE########
__FILENAME__ = pso
__author__ = ('Julian Togelius, julian@idsia.ch',
              'Justin S Bayer, bayer.justin@googlemail.com')

import scipy
import logging

from pybrain.optimization.optimizer import ContinuousOptimizer


def fullyConnected(lst):
    return dict((i, lst) for i in lst)

def ring(lst):
    leftist = lst[1:] + lst[0:1]
    rightist = lst[-1:] + lst[:-1]
    return dict((i, (j, k)) for i, j, k in zip(lst, leftist, rightist))

# TODO: implement some better neighborhoods


class ParticleSwarmOptimizer(ContinuousOptimizer):
    """ Particle Swarm Optimization

    `size` determines the number of particles.

    `boundaries` should be a list of (min, max) pairs with the length of the
    dimensionality of the vector to be optimized (default: +-10). Particles will be
    initialized with a position drawn uniformly in that interval.

    `memory` indicates how much the velocity of a particle is affected by
    its previous best position.

    `sociality` indicates how much the velocity of a particle is affected by
    its neighbours best position.

    `inertia` is a damping factor.
    """

    size = 20
    boundaries = None

    memory = 2.0
    sociality = 2.0
    inertia = 0.9

    neighbourfunction = None

    mustMaximize = True

    def _setInitEvaluable(self, evaluable):
        if evaluable is not None:
            logging.warning("Initial point provided was ignored.")
        ContinuousOptimizer._setInitEvaluable(self, evaluable)

    def _additionalInit(self):
        self.dim = self.numParameters
        if self.neighbourfunction is None:
            self.neighbourfunction = fullyConnected

        if self.boundaries is None:
            maxs = scipy.array([10] * self.dim)
            mins = scipy.array([-10] * self.dim)
        else:
            mins = scipy.array([min_ for min_, max_ in self.boundaries])
            maxs = scipy.array([max_ for min_, max_ in self.boundaries])

        self.particles = []
        for _ in range(self.size):
            startingPosition = scipy.random.random(self.dim)
            startingPosition *= (maxs - mins)
            startingPosition += mins
            self.particles.append(Particle(startingPosition, self.minimize))

        # Global neighborhood
        self.neighbours = self.neighbourfunction(self.particles)

    def best(self, particlelist):
        """Return the particle with the best fitness from a list of particles.
        """
        picker = min if self.minimize else max
        return picker(particlelist, key=lambda p: p.fitness)

    def _learnStep(self):
        for particle in self.particles:
            particle.fitness = self._oneEvaluation(particle.position.copy())

        for particle in self.particles:
            bestPosition = self.best(self.neighbours[particle]).position
            diff_social = self.sociality \
                          * scipy.random.random() \
                          * (bestPosition - particle.position)

            diff_memory = self.memory \
                          * scipy.random.random() \
                          * (particle.bestPosition - particle.position)

            particle.velocity *= self.inertia
            particle.velocity += diff_memory + diff_social
            particle.move()

    @property
    def batchSize(self):
        return self.size


class Particle(object):
    def __init__(self, start, minimize):
        """Initialize a Particle at the given start vector."""
        self.minimize = minimize
        self.dim = scipy.size(start)
        self.position = start
        self.velocity = scipy.zeros(scipy.size(start))
        self.bestPosition = scipy.zeros(scipy.size(start))
        self._fitness = None
        if self.minimize:
            self.bestFitness = scipy.inf
        else:
            self.bestFitness = -scipy.inf

    def _setFitness(self, value):
        self._fitness = value
        if ((self.minimize and value < self.bestFitness)
            or (not self.minimize and value > self.bestFitness)):
            self.bestFitness = value
            self.bestPosition = self.position.copy()

    def _getFitness(self):
        return self._fitness

    fitness = property(_getFitness, _setFitness)

    def move(self):
        self.position += self.velocity


########NEW FILE########
__FILENAME__ = randomsearch
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.optimization.optimizer import BlackBoxOptimizer, TopologyOptimizer


class RandomSearch(BlackBoxOptimizer):
    """ Every point is chosen randomly, independently of all previous ones. """

    def _additionalInit(self):
        self._oneEvaluation(self._initEvaluable)

    def _learnStep(self):
        new = self._initEvaluable.copy()
        new.randomize()
        self._oneEvaluation(new)


class WeightGuessing(RandomSearch):
    """ Just an Alias. """


class WeightMaskGuessing(WeightGuessing, TopologyOptimizer):
    """ random search, with a random mask that disables weights """

########NEW FILE########
__FILENAME__ = agent
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.utilities import abstractMethod, Named

class Agent(Named):
    """ An agent is an entity capable of producing actions, based on previous observations.
        Generally it will also learn from experience. It can interact directly with a Task.
    """

    def integrateObservation(self, obs):
        """ Integrate the current observation of the environment.
            :arg obs: The last observation returned from the environment
            :type obs: by default, this is assumed to be a numpy array of doubles
        """
        pass

    def getAction(self):
        """ Return a chosen action.
            :rtype: by default, this is assumed to ba a numpy array of doubles.
            :note: This method is abstract and needs to be implemented.
        """
        abstractMethod()

    def giveReward(self, r):
        """ Reward or punish the agent.
            :key r: reward, if C{r} is positive, punishment if C{r} is negative
            :type r: double
        """
        pass

    def newEpisode(self):
        """ Inform the agent that a new episode has started. """
        pass

########NEW FILE########
__FILENAME__ = learning
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from pybrain.rl.agents.logging import LoggingAgent


class LearningAgent(LoggingAgent):
    """ LearningAgent has a module, a learner, that modifies the module, and an explorer,
        which perturbs the actions. It can have learning enabled or disabled and can be
        used continuously or with episodes.
    """

    def __init__(self, module, learner = None):
        """
        :key module: the acting module
        :key learner: the learner (optional) """

        LoggingAgent.__init__(self, module.indim, module.outdim)

        self.module = module
        self.learner = learner

        # if learner is available, tell it the module and data
        if self.learner is not None:
            self.learner.module = self.module
            self.learner.dataset = self.history

        self.learning = True


    def _getLearning(self):
        """ Return whether the agent currently learns from experience or not. """
        return self.__learning


    def _setLearning(self, flag):
        """ Set whether or not the agent should learn from its experience """
        if self.learner is not None:
            self.__learning = flag
        else:
            self.__learning = False

    learning = property(_getLearning, _setLearning)


    def getAction(self):
        """ Activate the module with the last observation, add the exploration from
            the explorer object and store the result as last action. """
        LoggingAgent.getAction(self)

        self.lastaction = self.module.activate(self.lastobs)

        if self.learning:
            self.lastaction = self.learner.explore(self.lastobs, self.lastaction)

        return self.lastaction


    def newEpisode(self):
        """ Indicate the beginning of a new episode in the training cycle. """
        # reset the module when a new episode starts.
        self.module.reset()
        
        if self.logging:
            self.history.newSequence()

        # inform learner about the start of a new episode
        if self.learning:
            self.learner.newEpisode()

    def reset(self):
        """ Clear the history of the agent and resets the module and learner. """
        LoggingAgent.reset(self)
        self.module.reset()
        if self.learning:
            self.learner.reset()


    def learn(self, episodes=1):
        """ Call the learner's learn method, which has access to both module and history. """
        if self.learning:
            self.learner.learnEpisodes(episodes)


########NEW FILE########
__FILENAME__ = linearfa
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.rl.agents.logging import LoggingAgent
from pybrain.utilities import drawIndex
from scipy import array


class LinearFA_Agent(LoggingAgent):
    """ Agent class for using linear-FA RL algorithms. """    
        
    init_exploration = 0.1   # aka epsilon
    exploration_decay = 0.99 # per episode        
    
    init_temperature = 1.
    temperature_decay = 0.99 # per episode
    
    # default is boltzmann exploration 
    epsilonGreedy = False
           
    # flags for different modes
    learning = True    
    greedy = False
     
    def __init__(self, learner, **kwargs):
        LoggingAgent.__init__(self, learner.num_features, 1, **kwargs)
        self.learner = learner
        self.learner._behaviorPolicy = self._actionProbs
        self.reset()
        
    def _actionProbs(self, state):
        if self.greedy:
            return self.learner._greedyPolicy(state)
        elif self.epsilonGreedy:
            return (self.learner._greedyPolicy(state) * (1 - self._expl_proportion) 
                    + self._expl_proportion / float(self.learner.num_actions))
        else:
            return self.learner._boltzmannPolicy(state, self._temperature)                    
    
    def getAction(self):
        self.lastaction = drawIndex(self._actionProbs(self.lastobs), True)
        if self.learning and not self.learner.batchMode and self._oaro is not None:
            self.learner._updateWeights(*(self._oaro + [self.lastaction]))
            self._oaro = None          
        return array([self.lastaction])
        
    def integrateObservation(self, obs):
        if self.learning and not self.learner.batchMode and self.lastobs is not None:
            if self.learner.passNextAction:
                self._oaro = [self.lastobs, self.lastaction, self.lastreward, obs]
            else:
                self.learner._updateWeights(self.lastobs, self.lastaction, self.lastreward, obs)
        LoggingAgent.integrateObservation(self, obs)        
        
    def reset(self):
        LoggingAgent.reset(self)
        self._temperature = self.init_temperature
        self._expl_proportion = self.init_exploration
        self.learner.reset()    
        self._oaro = None
        self.newEpisode()
        
    def newEpisode(self):
        """ Indicate the beginning of a new episode in the training cycle. """
        if self.logging:
            self.history.newSequence()
        if self.learning and not self.learner.batchMode:
            self.learner.newEpisode()
        else:
            self._temperature *= self.temperature_decay
            self._expl_proportion *= self.exploration_decay      
            self.learner.newEpisode()

            
    def learn(self):
        if not self.learning:
            return
        if not self.learner.batchMode:
            print('Learning is done online, and already finished.')
            return
        for seq in self.history:
            for obs, action, reward in seq:
                if self.laststate is not None:
                    self.learner._updateWeights(self.lastobs, self.lastaction, self.lastreward, obs)
                self.lastobs = obs
                self.lastaction = action[0]
                self.lastreward = reward
            self.learner.newEpisode()

########NEW FILE########
__FILENAME__ = logging
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from pybrain.rl.agents.agent import Agent
from pybrain.datasets import ReinforcementDataSet


class LoggingAgent(Agent):
    """ This agent stores actions, states, and rewards encountered during
        interaction with an environment in a ReinforcementDataSet (which is
        a variation of SequentialDataSet).
        The stored history can be used for learning and is erased by resetting
        the agent. It also makes sure that integrateObservation, getAction and
        giveReward are called in exactly that order.
    """

    logging = True

    lastobs = None
    lastaction = None
    lastreward = None


    def __init__(self, indim, outdim, **kwargs):
        self.setArgs(**kwargs)
        
        # store input and output dimension
        self.indim = indim
        self.outdim = outdim

        # create the history dataset
        self.history = ReinforcementDataSet(indim, outdim)


    def integrateObservation(self, obs):
        """Step 1: store the observation received in a temporary variable until action is called and
        reward is given. """
        self.lastobs = obs
        self.lastaction = None
        self.lastreward = None


    def getAction(self):
        """Step 2: store the action in a temporary variable until reward is given. """
        assert self.lastobs != None
        assert self.lastaction == None
        assert self.lastreward == None

        # implement getAction in subclass and set self.lastaction


    def giveReward(self, r):
        """Step 3: store observation, action and reward in the history dataset. """
        # step 3: assume that state and action have been set
        assert self.lastobs != None
        assert self.lastaction != None
        assert self.lastreward == None

        self.lastreward = r

        # store state, action and reward in dataset if logging is enabled
        if self.logging:
            self.history.addSample(self.lastobs, self.lastaction, self.lastreward)


    def newEpisode(self):
        """ Indicate the beginning of a new episode in the training cycle. """
        if self.logging:
            self.history.newSequence()


    def reset(self):
        """ Clear the history of the agent. """
        self.lastobs = None
        self.lastaction = None
        self.lastreward = None

        self.history.clear()

########NEW FILE########
__FILENAME__ = optimization
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.rl.agents.agent import Agent

class OptimizationAgent(Agent):
    """ A simple wrapper to allow optimizers to conform to the RL interface.
        Works only in conjunction with EpisodicExperiment.
    """
    def __init__(self, module, learner):
        self.module = module
        self.learner = learner


########NEW FILE########
__FILENAME__ = balancetask
__author__ = 'Thomas Rueckstiess and Tom Schaul'

from scipy import pi, dot, array, ones, exp
from scipy.linalg import norm

from pybrain.rl.environments.cartpole.nonmarkovpole import NonMarkovPoleEnvironment
from pybrain.rl.environments.cartpole.doublepole import DoublePoleEnvironment
from pybrain.rl.environments import EpisodicTask
from cartpole import CartPoleEnvironment
from pybrain.utilities import crossproduct
        

class BalanceTask(EpisodicTask):
    """ The task of balancing some pole(s) on a cart """
    def __init__(self, env=None, maxsteps=1000, desiredValue = 0):
        """
        :key env: (optional) an instance of a CartPoleEnvironment (or a subclass thereof)
        :key maxsteps: maximal number of steps (default: 1000)
        """
        self.desiredValue = desiredValue
        if env == None:
            env = CartPoleEnvironment()
        EpisodicTask.__init__(self, env)
        self.N = maxsteps
        self.t = 0

        # scale position and angle, don't scale velocities (unknown maximum)
        self.sensor_limits = [(-3, 3)]
        for i in range(1, self.outdim):
            if isinstance(self.env, NonMarkovPoleEnvironment) and i % 2 == 0:
                self.sensor_limits.append(None)
            else:
                self.sensor_limits.append((-pi, pi))

        # self.sensor_limits = [None] * 4
        # actor between -10 and 10 Newton
        self.actor_limits = [(-50, 50)]

    def reset(self):
        EpisodicTask.reset(self)
        self.t = 0

    def performAction(self, action):
        self.t += 1
        EpisodicTask.performAction(self, action)

    def isFinished(self):
        if max(map(abs, self.env.getPoleAngles())) > 0.7:
            # pole has fallen
            return True
        elif abs(self.env.getCartPosition()) > 2.4:
            # cart is out of it's border conditions
            return True
        elif self.t >= self.N:
            # maximal timesteps
            return True
        return False

    def getReward(self):
        angles = map(abs, self.env.getPoleAngles())
        s = abs(self.env.getCartPosition())
        reward = 0
        if min(angles) < 0.05 and abs(s) < 0.05:
            reward = 0
        elif max(angles) > 0.7 or abs(s) > 2.4:
            reward = -2 * (self.N - self.t)
        else:
            reward = -1
        return reward

    def setMaxLength(self, n):
        self.N = n


class JustBalanceTask(BalanceTask):
    """ this task does not require the cart to be moved to the middle. """
    def getReward(self):
        angles = map(abs, self.env.getPoleAngles())
        s = abs(self.env.getCartPosition())
        if min(angles) < 0.05:
            reward = 0
        elif max(angles) > 0.7 or abs(s) > 2.4:
            reward = -2 * (self.N - self.t)
        else:
            reward = -1
        return reward


class EasyBalanceTask(BalanceTask):
    """ this task is a bit easier to learn because it gives gradual feedback
        about the distance to the centre. """
    def getReward(self):
        angles = map(abs, self.env.getPoleAngles())
        s = abs(self.env.getCartPosition())
        if min(angles) < 0.05 and abs(s) < 0.05:
            reward = 0
        elif max(angles) > 0.7 or abs(s) > 2.4:
            reward = -2 * (self.N - self.t)
        else:
            reward = -abs(s) / 2
        return reward


class DiscreteBalanceTask(BalanceTask):
    """ here there are 3 discrete actions, left, right, nothing. """

    numActions = 3

    def __init__(self, env=None, maxsteps=1000):
        """
        :key env: (optional) an instance of a CartPoleEnvironment (or a subclass thereof)
        :key maxsteps: maximal number of steps (default: 1000)
        """
        if env == None:
            env = CartPoleEnvironment()
        EpisodicTask.__init__(self, env)
        self.N = maxsteps
        self.t = 0

        # no scaling of sensors
        self.sensor_limits = [None] * self.env.outdim

        # scale actor
        self.actor_limits = [(-50, 50)]

    def getObservation(self):
        """ a filtered mapping to getSample of the underlying environment. """
        sensors = self.env.getSensors()
        if self.sensor_limits:
            sensors = self.normalize(sensors)
        return sensors

    def performAction(self, action):
        action = action - (self.numActions-1)/2.
        BalanceTask.performAction(self, action)

    def getReward(self):
        angles = map(abs, self.env.getPoleAngles())
        s = abs(self.env.getCartPosition())
        if min(angles) < 0.05: # and abs(s) < 0.05:
            reward = 1.0
        elif max(angles) > 0.7 or abs(s) > 2.4:
            reward = -1. * (self.N - self.t)
        else:
            reward = 0
        return reward


class DiscreteNoHelpTask(DiscreteBalanceTask):
    def getReward(self):
        angles = map(abs, self.env.getPoleAngles())
        s = abs(self.env.getCartPosition())
        if max(angles) > 0.7 or abs(s) > 2.4:
            reward = -1. * (self.N - self.t)
        else:
            reward = 0.0
        return reward
    

class DiscretePOMDPTask(DiscreteBalanceTask):
    def __init__(self, env=None, maxsteps=1000):
        """
        :key env: (optional) an instance of a CartPoleEnvironment (or a subclass thereof)
        :key maxsteps: maximal number of steps (default: 1000)
        """
        if env == None:
            env = CartPoleEnvironment()
        EpisodicTask.__init__(self, env)
        self.N = maxsteps
        self.t = 0

        # no scaling of sensors
        self.sensor_limits = [None] * 2

        # scale actor
        self.actor_limits = [(-50, 50)]

    @property
    def outdim(self):
        return 2

    def getObservation(self):
        """ a filtered mapping to getSample of the underlying environment. """
        sensors = [self.env.getSensors()[0], self.env.getSensors()[2]]

        if self.sensor_limits:
            sensors = self.normalize(sensors)
        return sensors


class LinearizedBalanceTask(BalanceTask):
    """ Here we follow the setup in
    Peters J, Vijayakumar S, Schaal S (2003) Reinforcement learning for humanoid robotics.
    TODO: This stuff is not yet compatible to any other cartpole environment. """

    Q = array([12., 0.25, 1.25, 1.0])

    def getReward(self):
        return dot(self.env.sensors ** 2, self.Q) + self.env.action[0] ** 2 * 0.01

    def isFinished(self):
        if abs(self.env.getPoleAngles()[0]) > 0.5235988:  # pi/6
            # pole has fallen
            return True
        elif abs(self.env.getCartPosition()) > 1.5:
            # cart is out of it's border conditions
            return True
        elif self.t >= self.N:
            # maximal timesteps
            return True
        return False


class DiscreteBalanceTaskRBF(DiscreteBalanceTask):
    """ From Lagoudakis & Parr, 2003:
    With RBF features to generate a 10-dimensional observation (including bias),
    also no cart-restrictions, no helpful rewards, and a single pole. """
    
    CENTERS = array(crossproduct([[-pi/4, 0, pi/4], [1, 0, -1]]))
    
    def getReward(self):
        angles = map(abs, self.env.getPoleAngles())
        if max(angles) > 1.6:
            reward = -1.
        else:
            reward = 0.0
        return reward
    
    def isFinished(self):
        if max(map(abs, self.env.getPoleAngles())) > 1.6:
            return True
        elif self.t >= self.N:
            return True
        return False
    
    def getObservation(self):
        res = ones(1+len(self.CENTERS))
        sensors = self.env.getSensors()[:-2]        
        res[1:] = exp(-array(map(norm, self.CENTERS-sensors))**2/2)
        return res
    
    @property
    def outdim(self):
        return 1+len(self.CENTERS)
    
    
class DiscreteDoubleBalanceTaskRBF(DiscreteBalanceTaskRBF):
    """ Same idea, but two poles. """
    
    CENTERS = array(crossproduct([[-pi/4, 0, pi/4], [1, 0, -1]]*2))  
    
    def __init__(self, env=None, maxsteps=1000):
        if env == None:
            env = DoublePoleEnvironment()
        DiscreteBalanceTask.__init__(self, env, maxsteps)
    

########NEW FILE########
__FILENAME__ = cartpole
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from matplotlib.mlab import rk4
from math import sin, cos
import time
from scipy import eye, matrix, random, asarray

from pybrain.rl.environments.graphical import GraphicalEnvironment


class CartPoleEnvironment(GraphicalEnvironment):
    """ This environment implements the cart pole balancing benchmark, as stated in:
        Riedmiller, Peters, Schaal: "Evaluation of Policy Gradient Methods and
        Variants on the Cart-Pole Benchmark". ADPRL 2007.
        It implements a set of differential equations, solved with a 4th order
        Runge-Kutta method.
    """

    indim = 1
    outdim = 4

    # some physical constants
    g = 9.81
    l = 0.5
    mp = 0.1
    mc = 1.0
    dt = 0.02

    randomInitialization = True

    def __init__(self, polelength=None):
        GraphicalEnvironment.__init__(self)
        if polelength != None:
            self.l = polelength

        # initialize the environment (randomly)
        self.reset()
        self.action = 0.0
        self.delay = False

    def getSensors(self):
        """ returns the state one step (dt) ahead in the future. stores the state in
            self.sensors because it is needed for the next calculation. The sensor return
            vector has 4 elements: theta, theta', s, s' (s being the distance from the
            origin).
        """
        return asarray(self.sensors)

    def performAction(self, action):
        """ stores the desired action for the next runge-kutta step.
        """
        self.action = action
        self.step()

    def step(self):
        self.sensors = rk4(self._derivs, self.sensors, [0, self.dt])
        self.sensors = self.sensors[-1]
        if self.hasRenderer():
            self.getRenderer().updateData(self.sensors)
            if self.delay:
                time.sleep(0.05)

    def reset(self):
        """ re-initializes the environment, setting the cart back in a random position.
        """
        if self.randomInitialization:
            angle = random.uniform(-0.2, 0.2)
            pos = random.uniform(-0.5, 0.5)
        else:
            angle = -0.2
            pos = 0.2
        self.sensors = (angle, 0.0, pos, 0.0)

    def _derivs(self, x, t):
        """ This function is needed for the Runge-Kutta integration approximation method. It calculates the
            derivatives of the state variables given in x. for each variable in x, it returns the first order
            derivative at time t.
        """
        F = self.action
        (theta, theta_, _s, s_) = x
        u = theta_
        sin_theta = sin(theta)
        cos_theta = cos(theta)
        mp = self.mp
        mc = self.mc
        l = self.l
        u_ = (self.g * sin_theta * (mc + mp) - (F + mp * l * theta ** 2 * sin_theta) * cos_theta) / (4 / 3 * l * (mc + mp) - mp * l * cos_theta ** 2)
        v = s_
        v_ = (F - mp * l * (u_ * cos_theta - (s_ ** 2 * sin_theta))) / (mc + mp)
        return (u, u_, v, v_)

    def getPoleAngles(self):
        """ auxiliary access to just the pole angle(s), to be used by BalanceTask """
        return [self.sensors[0]]

    def getCartPosition(self):
        """ auxiliary access to just the cart position, to be used by BalanceTask """
        return self.sensors[2]



class CartPoleLinEnvironment(CartPoleEnvironment):
    """ This is a linearized implementation of the cart-pole system, as described in
    Peters J, Vijayakumar S, Schaal S (2003) Reinforcement learning for humanoid robotics.
    Polelength is fixed, the order of sensors has been changed to the above."""

    tau = 1. / 60.   # sec

    def __init__(self, **kwargs):
        CartPoleEnvironment.__init__(self, **kwargs)
        nu = 13.2 #  sec^-2
        tau = self.tau

        # linearized movement equations
        self.A = matrix(eye(4))
        self.A[0, 1] = tau
        self.A[2, 3] = tau
        self.A[1, 0] = nu * tau
        self.b = matrix([0.0, nu * tau / 9.80665, 0.0, tau])


    def step(self):
        self.sensors = random.normal(loc=self.sensors * self.A + self.action * self.b, scale=0.001).flatten()
        if self.hasRenderer():
            self.getRenderer().updateData(self.sensors)
            if self.delay:
                time.sleep(self.tau)

    def reset(self):
        """ re-initializes the environment, setting the cart back in a random position.
        """
        self.sensors = random.normal(scale=0.1, size=4)

    def getSensors(self):
        return self.sensors.flatten()

    def getPoleAngles(self):
        """ auxiliary access to just the pole angle(s), to be used by BalanceTask """
        return [self.sensors[0]]

    def getCartPosition(self):
        """ auxiliary access to just the cart position, to be used by BalanceTask """
        return self.sensors[2]


########NEW FILE########
__FILENAME__ = doublepole
__author__ = 'Tom Schaul, tom@idsia.ch'

from cartpole import CartPoleEnvironment
from pybrain.rl.environments import Environment


class DoublePoleEnvironment(Environment):
    """ two poles to be balanced from the same cart. """

    indim = 1
    ooutdim = 6

    def __init__(self):
        self.p1 = CartPoleEnvironment()
        self.p2 = CartPoleEnvironment()
        self.p2.l = 0.05
        self.p2.mp = 0.01
        self.reset()

    def getSensors(self):
        """ returns the state one step (dt) ahead in the future. stores the state in
            self.sensors because it is needed for the next calculation. The sensor return
            vector has 6 elements: theta1, theta1', theta2, theta2', s, s'
            (s being the distance from the origin).
        """
        s1 = self.p1.getSensors()
        s2 = self.p2.getSensors()
        self.sensors = (s1[0], s1[1], s2[0], s2[1], s2[2], s2[3])
        return self.sensors

    def reset(self):
        """ re-initializes the environment, setting the cart back in a random position.
        """
        self.p1.reset()
        self.p2.reset()
        # put cart in the same place:
        self.p2.sensors = (self.p2.sensors[0], self.p2.sensors[1], self.p1.sensors[2], self.p1.sensors[3])
        self.getSensors()

    def performAction(self, action):
        """ stores the desired action for the next runge-kutta step.
        """
        self.p1.performAction(action)
        self.p2.performAction(action)

    def getCartPosition(self):
        """ auxiliary access to just the cart position, to be used by BalanceTask """
        return self.sensors[4]

    def getPoleAngles(self):
        """ auxiliary access to just the pole angle(s), to be used by BalanceTask """
        return [self.sensors[0], self.sensors[2]]


########NEW FILE########
__FILENAME__ = cartpolecompile
__author__ = 'Tom Schaul, tom@idsia.ch'


from distutils.core import setup
from Pyrex.Distutils.extension import Extension
from Pyrex.Distutils import build_ext
import platform, sys, os

# Mac users need an environment variable set:
if platform.system() == 'Darwin':
    # get mac os version
    version = platform.mac_ver()
    # don't care about sub-versions like 10.5.2
    baseversion = version[0].rsplit('.', 1)[0]
    os.environ["MACOSX_DEPLOYMENT_TARGET"] = baseversion

sys.argv.append('build_ext')
sys.argv.append('--inplace')

if platform.system() == 'Windows':
    sys.argv.append('-c')
    sys.argv.append('mingw32')

setup(ext_modules=[Extension('cartpolewrap', ['cartpolewrap.pyx', 'cartpole.cpp'],
                                pyrex_cplus=[True],
                                )],
      cmdclass={'build_ext': build_ext})


########NEW FILE########
__FILENAME__ = cartpoleenv
#@Pydev CodeAnalysisIgnore

__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import array, sin, cos, randn
import logging

from pybrain.rl.environments.episodic import EpisodicTask


try:
    import cartpolewrap as impl
except ImportError, e:
    logging.error("FastCartPoleTask is wrapping C code that needs to be compiled - it's simple: run .../cartpolecompile.py")
    raise e



class FastCartPoleTask(EpisodicTask):
    """ A Python wrapper of the standard C implementation of the pole-balancing task, directly using the
    reference code of Faustino Gomez. """

    indim = 1

    desiredValue = 100000

    # additional random observations
    extraRandoms = 0

    __single = None
    def __init__(self, numPoles=1, markov=True, verbose=False,
                 extraObservations=False, extraRandoms=0, maxSteps=100000):
        """ @extraObservations: if this flag is true, the observations include the Cartesian coordinates
        of the pole(s).
        """
        if self.__single != None:
            raise Exception('Singleton class - there is already an instance around', self.__single)
        self.__single = self
        impl.init(markov, numPoles, maxSteps)
        self.markov = markov
        self.numPoles = numPoles
        self.verbose = verbose
        self.extraObservations = extraObservations
        self.extraRandoms = extraRandoms
        self.desiredValue = maxSteps
        self.reset()

    def __str__(self):
        s = 'Cart-Pole-Balancing-Task, '
        if self.markov:
            s += 'markovian'
        else:
            s += 'non-markovian'
        s += ', with '
        if self.numPoles == 1:
            s += 'a single pole'
        else:
            s += str(self.numPoles) + ' poles'
        if self.extraObservations:
            s += ' and additional observations (cartesian coordinates of tip of pole(s))'
        if self.extraRandoms > 0:
            s += ' and ' + str(self.extraRandoms) + ' additional random observations'
        return s

    def reset(self):
        if self.verbose:
            print('** reset **')
        self.cumreward = 0
        impl.res()

    @property
    def outdim(self):
        res = 1 + self.numPoles
        if self.markov:
            res *= 2
        if self.extraObservations:
            res += 2 * self.numPoles
        res += self.extraRandoms
        return res

    def getReward(self):
        r = 1. + impl.getR()
        if self.verbose:
            print(' +r', r,)
        return r

    def isFinished(self):
        if self.verbose:
            print('  -finished?', impl.isFinished())
        return impl.isFinished()

    def getObservation(self):
        obs = array(impl.getObs())
        if self.verbose:
            print('obs', obs)
        obs.resize(self.outdim)
        if self.extraObservations:
            cartpos = obs[-1]
            if self.markov:
                angle1 = obs[1]
            else:
                angle1 = obs[0]
            obs[-1 + self.extraRandoms] = 0.1 * cos(angle1) + cartpos
            obs[-2 + self.extraRandoms] = 0.1 * sin(angle1) + cartpos
            if self.numPoles == 2:
                if self.markov:
                    angle2 = obs[3]
                else:
                    angle2 = obs[1]
                obs[-3 + self.extraRandoms] = 0.05 * cos(angle2) + cartpos
                obs[-4 + self.extraRandoms] = 0.05 * sin(angle2) + cartpos

        if self.extraRandoms > 0:
            obs[-self.extraRandoms:] = randn(self.extraRandoms)

        if self.verbose:
            print('obs', obs)
        return obs

    def performAction(self, action):
        if self.verbose:
            print('act', action)
        impl.performAction(action[0])
        self.addReward()





########NEW FILE########
__FILENAME__ = nonmarkovdoublepole
__author__ = 'Tom Schaul, tom@idsia.ch'

from doublepole import DoublePoleEnvironment
from nonmarkovpole import NonMarkovPoleEnvironment


class NonMarkovDoublePoleEnvironment(DoublePoleEnvironment, NonMarkovPoleEnvironment):
    """ DoublePoleEnvironment which does not give access to the derivatives. """

    outdim = 3

    def getSensors(self):
        """ returns the state one step (dt) ahead in the future. stores the state in
            self.sensors because it is needed for the next calculation. The sensor return
            vector has 3 elements: theta1, theta2, s
            (s being the distance from the origin).
        """
        tmp = DoublePoleEnvironment.getSensors(self)
        return (tmp[0], tmp[2], tmp[4])

    def getPoleAngles(self):
        """ auxiliary access to just the pole angle(s), to be used by BalanceTask """
        return [self.sensors[0], self.sensors[1]]

    def getCartPosition(self):
        """ auxiliary access to just the cart position, to be used by BalanceTask """
        return self.sensors[2]



########NEW FILE########
__FILENAME__ = nonmarkovpole
__author__ = 'Tom Schaul, tom@idsia.ch'

from cartpole import CartPoleEnvironment


class NonMarkovPoleEnvironment(CartPoleEnvironment):
    """ CartPoleEnvironment which does not give access to the derivatives. """

    outdim = 2

    def getSensors(self):
        """ returns the state one step (dt) ahead in the future. stores the state in
            self.sensors because it is needed for the next calculation. The sensor return
            vector has 2 elements: theta, s
            (s being the distance from the origin).
        """
        tmp = CartPoleEnvironment.getSensors(self)
        return (tmp[0], tmp[2])

    def getCartPosition(self):
        """ auxiliary access to just the cart position, to be used by BalanceTask """
        return self.sensors[1]



########NEW FILE########
__FILENAME__ = renderer
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from pylab import ion, figure, draw, Rectangle, Line2D
from scipy import cos, sin
from pybrain.rl.environments.renderer import Renderer
import threading
import time


class CartPoleRenderer(Renderer):
    def __init__(self):
        Renderer.__init__(self)

        self.dataLock = threading.Lock()
        self.angle = 0.0
        self.angle_vel = 0.0
        self.pos = 0.0
        self.pos_vel = 0.0
        self.stopRequest = False

        # some drawing constants
        self.cartheight = 0.2
        self.cartwidth = 1.0
        self.polelength = 0.5
        self.plotlimits = [-4, 4, -0.5, 3]
        self.box = None
        self.pole = None

    def updateData(self, data):
        self.dataLock.acquire()
        (self.angle, self.angle_vel, self.pos, self.pos_vel) = data
        self.dataLock.release()

    def stop(self):
        self.stopRequest = True

    def start(self):
        self.drawPlot()
        Renderer.start(self)

    def drawPlot(self):
        ion()
        fig = figure(1)
        # draw cart
        axes = fig.add_subplot(111, aspect='equal')
        self.box = Rectangle(xy=(self.pos - self.cartwidth / 2.0, -self.cartheight), width=self.cartwidth, height=self.cartheight)
        axes.add_artist(self.box)
        self.box.set_clip_box(axes.bbox)

        # draw pole
        self.pole = Line2D([self.pos, self.pos + sin(self.angle)], [0, cos(self.angle)], linewidth=3, color='black')
        axes.add_artist(self.pole)
        self.pole.set_clip_box(axes.bbox)

        # set axes limits
        axes.set_xlim(-2.5, 2.5)
        axes.set_ylim(-0.5, 2)

    def _render(self):
        while not self.stopRequest:
            if self.angle < 0.05 and abs(self.pos) < 0.05:
                self.box.set_facecolor('green')
            else:
                self.box.set_facecolor('blue')

            self.box.set_x(self.pos - self.cartwidth / 2.0)
            self.pole.set_xdata([self.pos, self.pos + self.polelength * sin(self.angle)])
            self.pole.set_ydata([0, self.polelength * cos(self.angle)])
            draw()
            time.sleep(0.05)
        self.stopRequest = False

########NEW FILE########
__FILENAME__ = acrobot
__author__ = 'Tom Schaul, tom@idsia.ch'

"""
Adaptation of the Acrobot Environment
from the "FAReinforcement" library
of Jose Antonio Martin H. (version 1.0).
"""
    
from scipy import pi, array, cos, sin
from pybrain.rl.environments.episodic import EpisodicTask


class AcrobotTask(EpisodicTask): 
    """ TODO: not currently episodic: success just reinitializes it. """   
    input_ranges = [[-pi, pi], [-pi, pi], [-4 * pi, 4 * pi], [-9 * pi, 9 * pi]]
    reward_ranges = [[-1.0, 1000.0]]

    # The current real values of the state
    maxSpeed1 = 4 * pi
    maxSpeed2 = 9 * pi
    m1 = 1.0
    m2 = 1.0
    l1 = 1.0
    l2 = 1.0
    l1Square = l1 * l1
    l2Square = l2 * l2
    lc1 = 0.5
    lc2 = 0.5
    lc1Square = lc1 * lc1
    lc2Square = lc2 * lc2
    I1 = 1.0
    I2 = 1.0
    g = 9.8
    delta_t = 0.05

    #The number of actions.
    action_list = (-1.0 , 0.0 , 1.0)
    nactions = len(action_list)
    
    # angles, velocities and a bias
    nsenses = 5

    # number of steps of the current trial
    steps = 0
    maxSteps = 999

    # number of the current episode
    episode = 0
    
    target = 1.5
    
    easy_rewards = False    
    
    resetOnSuccess = False
    
    def __init__(self):
        self.reset()
        self.cumreward = 0
                
    def getObservation(self):    
        return array(self.state + [pi])/(pi)
        
    def performAction(self, action):
        if self.done > 0:
            self.done += 1            
        else:
            self.state = self.DoAction(action, self.state)
            self.r, self.done = self.GetReward(self.state)        
            self.cumreward += self.r
            
    def reset(self):
        self.state = self.GetInitialState()
            
    def getReward(self):
        return self.r
    
    def isFinished(self):
        if self.done>=3 and self.resetOnSuccess:
            self.reset()
            return False
        else:
            return self.done>=3
    
    def GetInitialState(self):
        s = [0, 0, 0, 0]
        self.StartEpisode()
        return  s

    def StartEpisode(self):
        self.steps = 0
        self.episode = self.episode + 1
        self.done = 0

    def GetReward(self, x):
        # r: the returned reward.
        # f: true if the car reached the goal, otherwise f is false
        y_acrobot = [0, 0, 0]

        theta1 = x[0]
        theta2 = x[1]
        y_acrobot[1] = y_acrobot[0] - cos(theta1)
        y_acrobot[2] = y_acrobot[1] - cos(theta2)
        #print(y_acrobot)
        #goal
        goal = y_acrobot[0] + self.target
        if self.easy_rewards:
            r = y_acrobot[2]
        else:
            #r = -0.01
            r = 0
        f = 0

        if  y_acrobot[2] >= goal:
            if self.easy_rewards:
                r = 10 * y_acrobot[2]
            else:
                r = 1
            f = 1

        if self.steps >= self.maxSteps:
            f = 5
            #r = -1

        return r, f

    def DoAction(self, a, x):
        self.steps = self.steps + 1
        torque = self.action_list[a]

        # Parameters for simulation
        theta1, theta2, theta1_dot, theta2_dot = x

        for _ in range(4):
            d1 = self.m1 * self.lc1Square + self.m2 * (self.l1Square + self.lc2Square + 2 * self.l1 * self.lc2 * cos(theta2)) + self.I1 + self.I2
            d2 = self.m2 * (self.lc2Square + self.l1 * self.lc2 * cos(theta2)) + self.I2

            phi2 = self.m2 * self.lc2 * self.g * cos(theta1 + theta2 - pi / 2.0)
            phi1 = -self.m2 * self.l1 * self.lc2 * theta2_dot * sin(theta2) * (theta2_dot - 2 * theta1_dot) + (self.m1 * self.lc1 + self.m2 * self.l1) * self.g * cos(theta1 - (pi / 2.0)) + phi2

            accel2 = (torque + phi1 * (d2 / d1) - self.m2 * self.l1 * self.lc2 * theta1_dot * theta1_dot * sin(theta2) - phi2)
            accel2 = accel2 / (self.m2 * self.lc2Square + self.I2 - (d2 * d2 / d1))
            accel1 = -(d2 * accel2 + phi1) / d1

            theta1_dot = theta1_dot + accel1 * self.delta_t

            if theta1_dot < -self.maxSpeed1:
                theta1_dot = -self.maxSpeed1

            if theta1_dot > self.maxSpeed1:
                theta1_dot = self.maxSpeed1

            theta1 = theta1 + theta1_dot * self.delta_t
            theta2_dot = theta2_dot + accel2 * self.delta_t

            if theta2_dot < -self.maxSpeed2:
                theta2_dot = -self.maxSpeed2

            if theta2_dot > self.maxSpeed2:
                theta2_dot = self.maxSpeed2

            theta2 = theta2 + theta2_dot * self.delta_t

        # bounded angles?
        #if theta1 < -pi:
        #    theta1 = -pi
        #elif theta1 > pi:
        #    theta1 = pi
        if theta1 < -pi:
            theta1 += 2*pi
        elif theta1 > pi:
            theta1 -= 2*pi
        if theta2 < -pi:
            theta2 += 2*pi
        elif theta2 > pi:
            theta2 -= 2*pi

        xp = [theta1, theta2, theta1_dot, theta2_dot]

        return xp


class SimpleAcrobot(AcrobotTask):
    
    target = -0.5
    
class VerySimpleAcrobot(AcrobotTask):
    
    target = -1.


class SingleArmSwinger(AcrobotTask):
    """ Variant with one piece fixed."""
    
    nsenses = 3
    
    resetOnSuccess = False
    
    target = 1.95
    maxSteps = 99

    def GetInitialState(self):
        
        s = [pi, 0, 0, 0]
        self.StartEpisode()
        return  s
    
    def getObservation(self):    
        return array(self.state[2:] + [pi])/(pi)
        
    def performAction(self, action):
        AcrobotTask.performAction(self, action)
        # re-fix the upper part of the arm
        _, theta2, _, theta2_dot = self.state
        self.state = [pi, theta2, 0, theta2_dot]

        

########NEW FILE########
__FILENAME__ = mountaincar
__author__ = 'Tom Schaul, tom@idsia.ch'

"""
Adaptation of the MountainCar Environment
from the "FAReinforcement" library
of Jose Antonio Martin H. (version 1.0).
"""
    
from scipy import array, cos
from pybrain.rl.environments.episodic import EpisodicTask


class MountainCar(EpisodicTask): 
    # The current real values of the state
    cur_pos = -0.5
    cur_vel = 0.0
    cur_state = [cur_pos, cur_vel]

    #The number of actions.
    action_list = (-1.0 , 0.0 , 1.0)
    nactions = len(action_list)
    
    nsenses = 3

    # number of steps of the current trial
    steps = 0

    # number of the current episode
    episode = 0

    # Goal Position
    goalPos = 0.45
    
    maxSteps = 999
    
    resetOnSuccess = False

    def __init__(self):
        self.nactions = len(self.action_list)
        self.reset()
        self.cumreward = 0

    def reset(self):
        self.state = self.GetInitialState()
    
    def getObservation(self):    
        #print(array([self.state[0], self.state[1] * 100, 1]))
        return array([self.state[0], self.state[1] * 100, 1])
        
    def performAction(self, action):
        if self.done > 0:
            self.done += 1            
        else:
            self.state = self.DoAction(action, self.state)
            self.r, self.done = self.GetReward(self.state)
            self.cumreward += self.r
            
    def getReward(self):
        return self.r    

    def GetInitialState(self):
        self.StartEpisode()
        return [-0.5, 0.]

    def StartEpisode(self):
        self.steps = 0
        self.episode = self.episode + 1
        self.done = 0
        
    def isFinished(self):
        if self.done>=3 and self.resetOnSuccess:
            self.reset()
            return False
        else:
            return self.done>=3
    

    def GetReward(self, s):
        # MountainCarGetReward returns the reward at the current state
        # x: a vector of position and velocity of the car
        # r: the returned reward.
        # f: true if the car reached the goal, otherwise f is false

        position = s[0]
        vel = s[1]
        # bound for position; the goal is to reach position = 0.45
        bpright = self.goalPos

        r = 0
        f = 0

        if  position >= bpright:
            r = 1
            f = 1
            
        if self.steps >= self.maxSteps:
            f = 5

        return r, f

    def DoAction(self, a, s):
        #MountainCarDoAction: executes the action (a) into the mountain car
        # acti: is the force to be applied to the car
        # x: is the vector containning the position and speed of the car
        # xp: is the vector containing the new position and velocity of the car
        #print('action',a)
        #print('state',s)
        force = self.action_list[a]

        self.steps = self.steps + 1

        position = s[0]
        speed = s[1]

        # bounds for position
        bpleft = -1.4

        # bounds for speed
        bsleft = -0.07
        bsright = 0.07

        speedt1 = speed + (0.001 * force) + (-0.0025 * cos(3.0 * position))
        
        if speedt1 < bsleft:
            speedt1 = bsleft
        elif speedt1 > bsright:
            speedt1 = bsright

        post1 = position + speedt1

        if post1 <= bpleft:
            post1 = bpleft
            speedt1 = 0.0

        return [post1, speedt1]


########NEW FILE########
__FILENAME__ = xor
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.rl.environments.episodic import EpisodicTask
from scipy import array
from random import randint, random


class XORTask(EpisodicTask):
    """ Continuous task, producing binary observations, taking a single, binary action
    rewarding the agent whenever action = xor(obs).
    """
    
    nactions = 2
    nsenses = 3
    
    randomorder = False
    
    and_task = False
    stochasticity = 0
    
    def __init__(self):
        self.r = 0
        self._counter = 0
        
    def getObservation(self):
        if self.randomorder:
            self.obs = array([randint(0,1), randint(0,1), 1])
        else:
            self.obs = array([self._counter%2, (self._counter/2)%2, 1])    
        self._counter += 1    
        return self.obs
        
    def performAction(self, action):
        if ((self.and_task and (action == self.obs[0] & self.obs[1]))
            or (not self.and_task and action == self.obs[0] ^ self.obs[1])):
            self.r = 1
        else:
            self.r = -1
        #print(self.obs, action, self.r    )
        self.addReward()       
            
    def getReward(self):
        if random() < self.stochasticity:
            return -self.r
        else:
            return self.r
    
    def isFinished(self):
        return False
    
    
    
class XORChainTask(XORTask):
    """ Continuous task, producing binary observations, taking a single, binary action
    rewarding the agent whenever action = xor(obs).
    It is a chain, going back to the initial state whenever the bad action is taken.
    Reward increases as we move along the chain.
    """
    
    reward_cutoff = 0
    
    def __init__(self):
        self.r = 0
        self.state = 0
        
    def getObservation(self):
        self.obs = array([self.state%2, (self.state/2)%2, 1])    
        return self.obs
        
    def performAction(self, action):
        if ((self.and_task and action == self.obs[0] & self.obs[1])
            or (not self.and_task and action == self.obs[0] ^ self.obs[1])):
            self.r = -1+2*(self.state >= self.reward_cutoff)
            self.state = min(self.state+1, 3)
        else:
            self.r = -1
            self.state = 0   
        self.addReward()       
            
########NEW FILE########
__FILENAME__ = environment
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.utilities import abstractMethod


class Environment(object):
    """ The general interface for whatever we would like to model, learn about,
        predict, or simply interact in. We can perform actions, and access
        (partial) observations.
    """

    # the number of action values the environment accepts
    indim = 0

    # the number of sensor values the environment produces
    outdim = 0

    # discrete state space
    discreteStates = False

    # discrete action space
    discreteActions = False

    # number of possible actions for discrete action space
    numActions = None

    def getSensors(self):
        """ the currently visible state of the world (the observation may be
            stochastic - repeated calls returning different values)

            :rtype: by default, this is assumed to be a numpy array of doubles
            :note: This function is abstract and has to be implemented.
        """
        abstractMethod()

    def performAction(self, action):
        """ perform an action on the world that changes it's internal state (maybe
            stochastically).
            :key action: an action that should be executed in the Environment.
            :type action: by default, this is assumed to be a numpy array of doubles
            :note: This function is abstract and has to be implemented.
        """
        abstractMethod()

    def reset(self):
        """ Most environments will implement this optional method that allows for
            reinitialization.
        """



########NEW FILE########
__FILENAME__ = episodic
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import power

from pybrain.utilities import abstractMethod
from pybrain.rl.environments.task import Task
from pybrain.rl.agents.agent import Agent
from pybrain.structure.modules.module import Module
from pybrain.rl.environments.fitnessevaluator import FitnessEvaluator
from pybrain.rl.experiments.episodic import EpisodicExperiment


class EpisodicTask(Task, FitnessEvaluator):
    """ A task that consists of independent episodes. """

    # tracking cumulative reward
    cumreward = 0

    # tracking the number of samples
    samples = 0

    #: Discount factor
    discount = None

    batchSize = 1

    def reset(self):
        """ Re-initialize the environment """
        # Note: if a task needs to be reset at the start, the subclass constructor
        # should take care of that.
        self.env.reset()
        self.cumreward = 0
        self.samples = 0

    def isFinished(self):
        """ Is the current episode over? """
        abstractMethod()

    def performAction(self, action):
        """ Execute one action. """
        Task.performAction(self, action)
        self.samples += 1
        self.addReward()
        
    def addReward(self):
        """ A filtered mapping towards performAction of the underlying environment. """
        # by default, the cumulative reward is just the sum over the episode
        if self.discount:
            self.cumreward += power(self.discount, self.samples) * self.getReward()
        else:
            self.cumreward += self.getReward()

    def getTotalReward(self):
        """ Return the accumulated reward since the start of the episode """
        return self.cumreward

    def f(self, x):
        """ An episodic task can be used as an evaluation function of a module that produces actions
        from observations, or as an evaluator of an agent. """
        r = 0.
        for _ in range(self.batchSize):
            if isinstance(x, Module):
                x.reset()
                self.reset()
                while not self.isFinished():
                    self.performAction(x.activate(self.getObservation()))
            elif isinstance(x, Agent):
                EpisodicExperiment(self, x).doEpisodes()
            else:
                raise ValueError(self.__class__.__name__+' cannot evaluate the fitness of '+str(type(x)))
            r += self.getTotalReward()
        return r / float(self.batchSize)

########NEW FILE########
__FILENAME__ = fitnessevaluator
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.utilities import abstractMethod


class FitnessEvaluator(object):
    """ The superclass of all evaluators that produce a single output value,
    given an arbitrary input.
    """

    # what would be the desired fitness?
    desiredValue = None

    # what is the desirable direction?
    toBeMinimized = False

    def f(self, x):
        """ The function itself, to be defined by subclasses """
        abstractMethod()

    def __call__(self, x):
        """ All FitnessEvaluators are callable.

        :return: float """
        return self.f(x)

########NEW FILE########
__FILENAME__ = environment
__author__ = 'Frank Sehnke, sehnke@in.tum.de'

import sensors
import threading
from pybrain.utilities import threaded
from pybrain.tools.networking.udpconnection import UDPServer
from pybrain.rl.environments.environment import Environment
from scipy import ones, zeros, array, clip, arange, sqrt
from time import sleep

class FlexCubeEnvironment(Environment):
    def __init__(self, render=True, realtime=True, ip="127.0.0.1", port="21560"):
        # initialize base class
        self.render = render
        if self.render:
            self.updateDone = True
            self.updateLock = threading.Lock()
            self.server = UDPServer(ip, port)
        self.actLen = 12
        self.mySensors = sensors.Sensors(["EdgesReal"])
        self.dists = array([20.0, sqrt(2.0) * 20, sqrt(3.0) * 20])
        self.gravVect = array([0.0, -100.0, 0.0])
        self.centerOfGrav = zeros((1, 3), float)
        self.pos = ones((8, 3), float)
        self.vel = zeros((8, 3), float)
        self.SpringM = ones((8, 8), float)
        self.d = 60.0
        self.dt = 0.02
        self.startHight = 10.0
        self.dumping = 0.4
        self.fraktMin = 0.7
        self.fraktMax = 1.3
        self.minAkt = self.dists[0] * self.fraktMin
        self.maxAkt = self.dists[0] * self.fraktMax
        self.reset()
        self.count = 0
        self.setEdges()
        self.act(array([20.0] * 12))
        self.euler()
        self.realtime = realtime
        self.step = 0

    def closeSocket(self):
        self.server.UDPInSock.close()
        sleep(10)

    def setEdges(self):
        self.edges = zeros((12, 2), int)
        count = 0
        c1 = 0
        for i in range(2):
            for j in range(2):
                for k in range(2):
                    c2 = 0
                    for i2 in range(2):
                        for j2 in range(2):
                            for k2 in range(2):
                                sum = abs(i - i2) + abs(j - j2) + abs(k - k2)
                                if sum == 1 and i <= i2 and j <= j2 and k <= k2:
                                    self.edges[count] = [c1, c2]
                                    count += 1
                                c2 += 1
                    c1 += 1

    def reset(self):
        self.action = ones((1, 12), float) * self.dists[0]

        for i in range(2):
            for j in range(2):
                for k in range(2):
                    self.pos[i * 4 + j * 2 + k] = [i * self.dists[0] - self.dists[0] / 2.0, j * self.dists[0] - self.dists[0] / 2.0 + self.startHight, k * self.dists[0] - self.dists[0] / 2.0]
        self.vel = zeros((8, 3), float)

        idx0 = arange(8).repeat(8)
        idx1 = array(range(8) * 8)
        self.difM = self.pos[idx0, :] - self.pos[idx1, :] #vectors from all points to all other points
        self.springM = sqrt((self.difM ** 2).sum(axis=1)).reshape(64, 1)
        self.distM = self.springM.copy() #distance matrix
        self.step = 0
        self.mySensors.updateSensor(self.pos, self.vel, self.distM, self.centerOfGrav, self.step, self.action)
        if self.render:
            if self.server.clients > 0:
                # If there are clients send them reset signal
                self.server.send(["r", "r"])

    def performAction(self, action):
        action = self.normAct(action)
        self.action = action.copy()
        self.act(action)
        self.euler()
        self.step += 1

        if self.render:
            if self.updateDone:
                self.updateRenderer()
                if self.server.clients > 0 and self.realtime:
                    sleep(0.02)

    def getSensors(self):
        self.mySensors.updateSensor(self.pos, self.vel, self.distM, self.centerOfGrav, self.step, self.action)
        return self.mySensors.getSensor()[:]

    def normAct(self, s):
        return clip(s, self.minAkt, self.maxAkt)

    def act(self, a):
        count = 0
        for i in self.edges:
            self.springM[i[0] * 8 + i[1]] = a[count]
            self.springM[i[1] * 8 + i[0]] = a[count]
            count += 1

    def euler(self):
        self.count += 1
        #Inner Forces
        distM = self.distM.copy()
        disM = self.springM - distM #difference between wanted spring lengths and current ones
        disM = disM.reshape(64, 1)

        distM = distM + 0.0000000001 #hack to prevent divs by 0

        #Forces to Velos
        #spring vectors normalized to 1 times the actual force from deformation
        vel = self.difM / distM
        vel *= disM * self.d * self.dt
        idx2 = arange(8)

        #TODO: arggggg!!!!!
        for i in range(8):
            self.vel[i] += vel[idx2 + i * 8, :].sum(axis=0)

        #Gravity
        self.vel += self.gravVect * self.dt

        #Dumping
        self.vel -= self.vel * self.dumping * self.dt

        #velos to positions
        self.pos += self.dt * self.vel

        #Collisions and friction
        for i in range(8):
            if self.pos[i][1] < 0.0:
                self.pos[i][1] = 0.0
                self.vel[i] = self.vel[i] * [0.0, -1.0, 0.0]
        self.centerOfGrav = self.pos.sum(axis=0) / 8.0

        #Distances of new state
        idx0 = arange(8).repeat(8)
        idx1 = array(range(8) * 8)
        self.difM = self.pos[idx0, :] - self.pos[idx1, :] #vectors from all points to all other points
        self.distM = sqrt((self.difM ** 2).sum(axis=1)).reshape(64, 1) #distance matrix

    @threaded()
    def updateRenderer(self):
        self.updateDone = False
        if not self.updateLock.acquire(False): return

        # Listen for clients
        self.server.listen()
        if self.server.clients > 0:
            # If there are clients send them the new data
            self.server.send(repr([self.pos, self.centerOfGrav]))
        sleep(0.02)
        self.updateLock.release()
        self.updateDone = True


########NEW FILE########
__FILENAME__ = masspoint
__author__ = 'Frank Sehnke, sehnke@in.tum.de'


class MArray:
    def __init__(self):
        self.field = {}
    def get(self, i):
        return self.field[i]
    def set(self, i, val):
        self.field[i] = val

class MassPoint:
    def __init__(self):
        self.pos = [0.0, 0.0, 0.0]
        self.vel = [0.0, 0.0, 0.0]
    def setPos(self, pos):
        self.pos = pos
    def setVel(self, vel):
        self.vel = vel


########NEW FILE########
__FILENAME__ = objects3d
__author__ = 'Frank Sehnke, sehnke@in.tum.de'

from OpenGL.GL import * #@UnusedWildImport
from OpenGL.GLU import * #@UnusedWildImport
import math


class Objects3D:
    def normale(self, vect, centerOfGrav):
        vect = self.dumpVect(vect, 1.0 / 4.0)
        norm = self.difVect(vect, centerOfGrav)
        norm = self.normVect(norm, 1.0)
        return norm

    def drawCreature(self, cPoints, centerOfGrav):
        glBegin(GL_QUADS)

        #unten
        point = []
        summe = [0.0, 0.0, 0.0]
        for i in range(2):
            for k in range(2):
                grayI = i
                grayK = i ^ k
                point.append(cPoints[grayI * 4 + grayK])
                summe = self.addVect(summe, point[i * 2 + k])
        norm = self.normale(summe, centerOfGrav)
        glNormal(norm[0], norm[1], norm[2])
        for i in range(2):
            for k in range(2):
                glVertex3f(point[i * 2 + k][0], point[i * 2 + k][1], point[i * 2 + k][2]);

        #links
        point = []
        summe = [0.0, 0.0, 0.0]
        for j in range(2):
            for k in range(2):
                grayJ = j
                grayK = j ^ k
                point.append(cPoints[grayJ * 2 + grayK])
                summe = self.addVect(summe, point[j * 2 + k])
        norm = self.normale(summe, centerOfGrav)
        glNormal(norm[0], norm[1], norm[2])
        for j in range(2):
            for k in range(2):
                glVertex3f(point[j * 2 + k][0], point[j * 2 + k][1], point[j * 2 + k][2]);

        #rechts
        point = []
        summe = [0.0, 0.0, 0.0]
        for j in range(2):
            for k in range(2):
                grayJ = j
                grayK = j ^ k
                point.append(cPoints[4 + grayJ * 2 + grayK])
                summe = self.addVect(summe, point[j * 2 + k])
        norm = self.normale(summe, centerOfGrav)
        glNormal(norm[0], norm[1], norm[2])
        for j in range(2):
            for k in range(2):
                glVertex3f(point[j * 2 + k][0], point[j * 2 + k][1], point[j * 2 + k][2]);

        #oben
        point = []
        summe = [0.0, 0.0, 0.0]
        for i in range(2):
            for k in range(2):
                grayI = i
                grayK = i ^ k
                point.append(cPoints[grayI * 4 + 2 + grayK])
                summe = self.addVect(summe, point[i * 2 + k])
        norm = self.normale(summe, centerOfGrav)
        glNormal(norm[0], norm[1], norm[2])
        for i in range(2):
            for k in range(2):
                glVertex3f(point[i * 2 + k][0], point[i * 2 + k][1], point[i * 2 + k][2]);

        #vorne
        point = []
        summe = [0.0, 0.0, 0.0]
        for i in range(2):
            for j in range(2):
                grayI = i
                grayJ = i ^ j
                point.append(cPoints[grayI * 4 + grayJ * 2 + 1])
                summe = self.addVect(summe, point[i * 2 + j])
        norm = self.normale(summe, centerOfGrav)
        glNormal(norm[0], norm[1], norm[2])
        for i in range(2):
            for j in range(2):
                glVertex3f(point[i * 2 + j][0], point[i * 2 + j][1], point[i * 2 + j][2]);

        #hinten
        point = []
        summe = [0.0, 0.0, 0.0]
        for i in range(2):
            for j in range(2):
                grayI = i
                grayJ = i ^ j
                point.append(cPoints[grayI * 4 + grayJ * 2])
                summe = self.addVect(summe, point[i * 2 + j])
        norm = self.normale(summe, centerOfGrav)
        glNormal(norm[0], norm[1], norm[2])
        for i in range(2):
            for j in range(2):
                glVertex3f(point[i * 2 + j][0], point[i * 2 + j][1], point[i * 2 + j][2]);
        glEnd()

    def drawMirCreat(self, cPoints, centerOfGrav):
        glBegin(GL_QUADS)

        #unten
        point = []
        summe = [0.0, 0.0, 0.0]
        for i in range(2):
            for k in range(2):
                grayI = i
                grayK = i ^ k
                point.append(cPoints[grayI * 4 + grayK])
                summe = self.addVect(summe, point[i * 2 + k])
        norm = self.normale(summe, centerOfGrav)
        glNormal(norm[0], norm[1], norm[2])
        for i in range(2):
            for k in range(2):
                glVertex3f(point[i * 2 + k][0], -point[i * 2 + k][1], point[i * 2 + k][2])

        #links
        point = []
        summe = [0.0, 0.0, 0.0]
        for j in range(2):
            for k in range(2):
                grayJ = j
                grayK = j ^ k
                point.append(cPoints[grayJ * 2 + grayK])
                summe = self.addVect(summe, point[j * 2 + k])
        norm = self.normale(summe, centerOfGrav)
        glNormal(norm[0], norm[1], norm[2])
        for j in range(2):
            for k in range(2):
                glVertex3f(point[j * 2 + k][0], -point[j * 2 + k][1], point[j * 2 + k][2])

        #rechts
        point = []
        summe = [0.0, 0.0, 0.0]
        for j in range(2):
            for k in range(2):
                grayJ = j
                grayK = j ^ k
                point.append(cPoints[4 + grayJ * 2 + grayK])
                summe = self.addVect(summe, point[j * 2 + k])
        norm = self.normale(summe, centerOfGrav)
        glNormal(norm[0], norm[1], norm[2])
        for j in range(2):
            for k in range(2):
                glVertex3f(point[j * 2 + k][0], -point[j * 2 + k][1], point[j * 2 + k][2])

        #oben
        point = []
        summe = [0.0, 0.0, 0.0]
        for i in range(2):
            for k in range(2):
                grayI = i
                grayK = i ^ k
                point.append(cPoints[grayI * 4 + 2 + grayK])
                summe = self.addVect(summe, point[i * 2 + k])
        norm = self.normale(summe, centerOfGrav)
        glNormal(norm[0], norm[1], norm[2])
        for i in range(2):
            for k in range(2):
                glVertex3f(point[i * 2 + k][0], -point[i * 2 + k][1], point[i * 2 + k][2]);

        #vorne
        point = []
        summe = [0.0, 0.0, 0.0]
        for i in range(2):
            for j in range(2):
                grayI = i
                grayJ = i ^ j
                point.append(cPoints[grayI * 4 + grayJ * 2 + 1])
                summe = self.addVect(summe, point[i * 2 + j])
        norm = self.normale(summe, centerOfGrav)
        glNormal(norm[0], norm[1], norm[2])
        for i in range(2):
            for j in range(2):
                glVertex3f(point[i * 2 + j][0], -point[i * 2 + j][1], point[i * 2 + j][2])

        #hinten
        point = []
        summe = [0.0, 0.0, 0.0]
        for i in range(2):
            for j in range(2):
                grayI = i
                grayJ = i ^ j
                point.append(cPoints[grayI * 4 + grayJ * 2])
                summe = self.addVect(summe, point[i * 2 + j])
        norm = self.normale(summe, centerOfGrav)
        glNormal(norm[0], norm[1], norm[2])
        for i in range(2):
            for j in range(2):
                glVertex3f(point[i * 2 + j][0], -point[i * 2 + j][1], point[i * 2 + j][2])
        glEnd()

    def drawShadow(self, cPoints, centerOfGrav):
        glBegin(GL_QUADS)
        zPers = -0.5
        xPers = -0.33

        #schatten
        #unten
        point = []
        for i in range(2):
            for k in range(2):
                grayI = i
                grayK = i ^ k
                point.append([cPoints[grayI * 4 + grayK][0] + xPers * cPoints[grayI * 4 + grayK][1], -0.025, cPoints[grayI * 4 + grayK][2] + zPers * cPoints[grayI * 4 + grayK][1]])
        glNormal(0.0, 1.0, 0.0)
        for i in range(2):
            for k in range(2):
                glVertex3f(point[i * 2 + k][0], point[i * 2 + k][1], point[i * 2 + k][2])

        #links
        point = []
        summe = [0.0, 0.0, 0.0]
        for j in range(2):
            for k in range(2):
                grayJ = j
                grayK = j ^ k
                #point.append(cPoints[grayJ*2+grayK].pos)
                point.append([cPoints[grayJ * 2 + grayK][0] + xPers * cPoints[grayJ * 2 + grayK][1], -0.02, cPoints[grayJ * 2 + grayK][2] + zPers * cPoints[grayJ * 2 + grayK][1]])
                summe = self.addVect(summe, point[j * 2 + k])
        norm = self.normale(summe, centerOfGrav)
        glNormal(norm[0], norm[1], norm[2])
        for j in range(2):
            for k in range(2):
                glVertex3f(point[j * 2 + k][0], -point[j * 2 + k][1], point[j * 2 + k][2])

        #rechts
        point = []
        summe = [0.0, 0.0, 0.0]
        for j in range(2):
            for k in range(2):
                grayJ = j
                grayK = j ^ k
                #point.append(cPoints[4+grayJ*2+grayK].pos)
                point.append([cPoints[4 + grayJ * 2 + grayK][0] + xPers * cPoints[4 + grayJ * 2 + grayK][1], -0.015, cPoints[4 + grayJ * 2 + grayK][2] + zPers * cPoints[4 + grayJ * 2 + grayK][1]])
                summe = self.addVect(summe, point[j * 2 + k])
        norm = self.normale(summe, centerOfGrav)
        glNormal(norm[0], norm[1], norm[2])
        for j in range(2):
            for k in range(2):
                glVertex3f(point[j * 2 + k][0], -point[j * 2 + k][1], point[j * 2 + k][2])

        #oben
        point = []
        summe = [0.0, 0.0, 0.0]
        for i in range(2):
            for k in range(2):
                grayI = i
                grayK = i ^ k
                #point.append(cPoints[grayI*4+2+grayK].pos)
                point.append([cPoints[grayI * 4 + 2 + grayK][0] + xPers * cPoints[grayI * 4 + 2 + grayK][1], -0.01, cPoints[grayI * 4 + 2 + grayK][2] + zPers * cPoints[grayI * 4 + 2 + grayK][1]])
                summe = self.addVect(summe, point[i * 2 + k])
        norm = self.normale(summe, centerOfGrav)
        glNormal(norm[0], norm[1], norm[2])
        for i in range(2):
            for k in range(2):
                glVertex3f(point[i * 2 + k][0], -point[i * 2 + k][1], point[i * 2 + k][2]);

        #vorne
        point = []
        summe = [0.0, 0.0, 0.0]
        for i in range(2):
            for j in range(2):
                grayI = i
                grayJ = i ^ j
                #point.append(cPoints[grayI*4+grayJ*2+1].pos)
                point.append([cPoints[grayI * 4 + grayJ * 2 + 1][0] + xPers * cPoints[grayI * 4 + grayJ * 2 + 1][1], -0.005, cPoints[grayI * 4 + grayJ * 2 + 1][2] + zPers * cPoints[grayI * 4 + grayJ * 2 + 1][1]])
                summe = self.addVect(summe, point[i * 2 + j])
        norm = self.normale(summe, centerOfGrav)
        glNormal(norm[0], norm[1], norm[2])
        for i in range(2):
            for j in range(2):
                glVertex3f(point[i * 2 + j][0], -point[i * 2 + j][1], point[i * 2 + j][2]);

        #hinten
        point = []
        summe = [0.0, 0.0, 0.0]
        for i in range(2):
            for j in range(2):
                grayI = i
                grayJ = i ^ j
                #point.append(cPoints[grayI*4+grayJ*2].pos)
                point.append([cPoints[grayI * 4 + grayJ * 2][0] + xPers * cPoints[grayI * 4 + grayJ * 2][1], -0.0, cPoints[grayI * 4 + grayJ * 2][2] + zPers * cPoints[grayI * 4 + grayJ * 2][1]])
                summe = self.addVect(summe, point[i * 2 + j])
        norm = self.normale(summe, centerOfGrav)
        glNormal(norm[0], norm[1], norm[2])
        for i in range(2):
            for j in range(2):
                glVertex3f(point[i * 2 + j][0], -point[i * 2 + j][1], point[i * 2 + j][2])
        glEnd()

    def difVect(self, point1, point2):
        vect = [point1[0] - point2[0], point1[1] - point2[1], point1[2] - point2[2]]
        return vect

    def addVect(self, point1, point2):
        vect = [point1[0] + point2[0], point1[1] + point2[1], point1[2] + point2[2]]
        return vect

    def velDif(self, vect, dif, soll):
        zug = self.d * (soll - dif)
        dif = [vect[0] / dif * zug, vect[1] / dif * zug, vect[2] / dif * zug]
        return dif

    def dumpVect(self, vect, fakt):
        for i in range(3):
            vect[i] *= fakt
        return vect

    def normVect(self, vect, norm):
        summe = 0.0
        for i in range(3):
            summe += vect[i] * vect[i]
        vect = self.dumpVect(vect, norm / math.sqrt(summe))
        return vect

    def calcNormal(self, xVector, yVector):
        result = [0, 0, 0]
        result[0] = xVector[1] * yVector[2] - yVector[1] * xVector[2]
        result[1] = -xVector[0] * yVector[2] + yVector[0] * xVector[2]
        result[2] = xVector[0] * yVector[1] - yVector[0] * xVector[1]
        return [result[0], result[1], result[2]]

    def points2Vector(self, startPoint, endPoint):
        result = [0, 0, 0]
        result[0] = endPoint[0] - startPoint[0]
        result[1] = endPoint[1] - startPoint[1]
        result[2] = endPoint[2] - startPoint[2]
        return [result[0], result[1], result[2]]


########NEW FILE########
__FILENAME__ = sensors
__author__ = 'Frank Sehnke, sehnke@in.tum.de'

#########################################################################
# The sensors that are available for the FlexCube Environment
#
# The FlexCube Environment is a Mass-Spring-System composed of 8 mass points.
# These resemble a cube with flexible edges.
#
# A wide variety of sensors are available for observation and reward:
# - EdgesReal: 12 edge lengths
# - EdgesTarget: 12 wanted edge lengths (the last action)
# - VerticesContact: vertexes contact with floor
# - VerticesMinHight: distance of closest vertex to the floor
# - DistToOrigin: distance to origin (0.0, 0.0)
# - Target: distance and angle to target
# - Time: time dependend cyclic signals
#
#########################################################################

from scipy import sqrt, zeros, array, clip, sin

# Class that bundles the different sensors defined by their names in sensorList
class Sensors:
    def __init__(self, sensorList):
        self.sensors = []
        for i in sensorList:
            self.sensors.append(eval(i + "()"))

    def updateSensor(self, pos, vel, dist, center, step, wEdges):
        for i in self.sensors:
            i.updateSensor(pos, vel, dist, center, step, wEdges)

    def getSensor(self):
        output = []
        for i in self.sensors:
            output.append(i.getSensor()[:])
        return output

#Sensor basis class
class defaultSensor:
    def __init__(self):
        self.sensorOutput = ["defaultSensor", 0]
        self.targetList = [array([-80.0, 0.0, 0.0])]
        self.edges = array([1, 2, 4, 11, 13, 19, 22, 31, 37, 38, 47, 55])

    def updateSensor(self, pos, vel, dist, center, step, wEdges):
        self.pos = pos.copy()
        self.dist = dist.copy()
        self.centerOfGrav = center.copy().reshape(3)
        self.centerOfGrav[1] = 0.0
        self.step = step
        self.wantedEdges = wEdges.copy()

    def getSensor(self):
        return self.sensorOutput

#Gives back the length of the edges of the cube
class EdgesReal(defaultSensor):
    def getSensor(self):
        self.sensorOutput = ["EdgesReal", 12]
        self.sensorOutput.append(self.dist[self.edges].reshape(12) / 30.0) #/30 for normalization [0-1]
        return self.sensorOutput

#Rewardsensor for grow task - returns how much the sum of edge lengths exeeds initial sum
class EdgesSumReal(defaultSensor):
    def getSensor(self):
        self.sensorOutput = ["EdgesSumReal", 1]
        self.sensorOutput.append(array([(self.dist[self.edges].reshape(12)).sum(axis=0) - 240.0]))
        return self.sensorOutput

#Gives back the desired length of the edges (last action)
class EdgesTarget(defaultSensor):
    def getSensor(self):
        self.sensorOutput = ["EdgesTarget", 12]
        self.sensorOutput.append(self.wantedEdges.reshape(12) / 30.0)
        return self.sensorOutput

#Returns how close vertices are to the floor up to 1 pixel distance (contact sensor)
class VerticesContact(defaultSensor):
    def getSensor(self):
        self.sensorOutput = ["VerticesContact", 8]
        self.sensorOutput.append(clip((1.0 - self.pos[:, 1]), 0.0, 1.0))
        return self.sensorOutput

#Returns the distance of the closest vertex to the floor (reward for jump task)
class VerticesMinHight(defaultSensor):
    def getSensor(self):
        self.sensorOutput = ["VerticesMinHight", 1]
        self.sensorOutput.append(array([min(self.pos[:, 1])]))
        return self.sensorOutput

#Returns the distance to the origin of the cube (reward for walk task)
class DistToOrigin(defaultSensor):
    def getSensor(self):
        self.sensorOutput = ["DistToOrigin", 1]
        self.sensorOutput.append(array([sqrt((self.centerOfGrav ** 2).sum(axis=0)) * 0.00125]))
        return self.sensorOutput

#Returns the distance and angle to a target point (distance is reward for target task)
class Target(defaultSensor):
    def getSensor(self):
        self.sensorOutput = ["Target", 5]

        t = self.targetList[0] - self.centerOfGrav
        dist = sqrt((t ** 2).sum(axis=0))
        out = zeros(5, float)
        out[0] = dist * 0.00625

        for i in range(4):
            if i < 2:
                d = self.pos[i] - self.centerOfGrav
            else:
                d = self.pos[i + 2] - self.centerOfGrav
            sen = sqrt((d ** 2).sum(axis=0))
            norm = dist * sen
            cosA = (d[0] * t[0] + d[2] * t[2]) / norm
            sinA = (d[0] * t[2] - d[2] * t[0]) / norm
            if cosA < 0.0:
                if sinA > 0.0: sinA = 1.0
                else: sinA = -1.0
            out[i + 1] = sinA
        self.sensorOutput.append(out)
        return self.sensorOutput

#Returns 3 time dependend values given by the sin of the timestep in the current episode
class Time(defaultSensor):
    def getSensor(self):
        self.sensorOutput = ["Time", 3]
        self.sensorOutput.append(array([sin(float(self.step) / 4.0), sin(float(self.step) / 8.0), sin(float(self.step) / 16.0)]))
        return self.sensorOutput


########NEW FILE########
__FILENAME__ = tasks
__author__ = 'Frank Sehnke, sehnke@in.tum.de'

#########################################################################
# The tasks availabele in the FlexCube Environment
#
# The FlexCube Environment is a Mass-Spring-System composed of 8 mass points.
# These resemble a cube with flexible edges.
#
# Tasks available are:
# - GrowTask, agent has to maximize the volume of the cube
# - JumpTask, agent has to maximize the distance of the lowest mass point to the floor
# - WalkTask, agent has to maximize the distance to the starting point
# - WalkDirectionTask, agent has to minimize the distance to a target point.
# - TargetTask, like the previous task but with several target points
#
#########################################################################


from pybrain.rl.environments import EpisodicTask
from scipy import array, r_, clip
import sensors

#Task basis class
class NoRewardTask(EpisodicTask):
    ''' just a basic task, that doesn't return a reward '''
    def __init__(self, env):
        EpisodicTask.__init__(self, env)
        self.step = 0
        self.epiStep = 0
        self.reward = [0.0]
        self.rawReward = 0.0
        self.obsSensors = ["EdgesReal"]
        self.rewardSensor = [""]
        self.oldReward = 0.0
        self.plotString = ["World Interactions", "Reward", "Reward on NoReward Task"]
        self.inDim = len(self.getObservation())
        self.outDim = self.env.actLen
        self.dif = (self.env.fraktMax - self.env.fraktMin) * self.env.dists[0]
        self.maxSpeed = self.dif / 30.0
        self.picCount = 0
        self.epiLen = 1

    def incStep(self):
        self.step += 1
        self.epiStep += 1

    def getReward(self):
        # calculate reward and return self.reward
        self.reward[0] = self.rawReward - self.getPain()
        return self.reward[0]

    def getObservation(self):
        # do something with self.sensors and return observation
        self.oldReward = self.rawReward
        aktSensors = self.env.getSensors()
        output = array([])
        for i in aktSensors:
            for j in self.obsSensors:
                if i[0] == j:
                    momSense = i[2]
                    output = r_[output, momSense]
            if i[0] == self.rewardSensor[0]:
                self.rawReward = i[2][0]
            if i[0] == "EdgesReal":
                self.EdgeL = momSense.copy()
        return output[:]

    #An agent can find easily the resonance frequency of the cube
    #Most tasks can be tricked by realising a resonance catastrophy
    #Therefore edges longer than 30 pixels result in punishment for all tasks
    def getPain(self):
        self.EdgeL = clip(self.EdgeL, 1.0, 4.0)
        return ((self.EdgeL - 1.0) ** 2).sum(axis=0)

    def performAction(self, action):
        """ a filtered mapping towards performAction of the underlying environment. """
        # scaling
        self.incStep()
        action = (action + 1.0) / 2.0 * self.dif + self.env.fraktMin * self.env.dists[0]
        #Clipping the maximal change in actions (max force clipping)
        action = clip(action, self.action - self.maxSpeed, self.action + self.maxSpeed)
        EpisodicTask.performAction(self, action)
        self.action = action.copy()

    def reset(self):
        self.reward[0] = 0.0
        self.rawReward = 0.0
        self.env.reset()
        self.action = [self.env.dists[0]] * self.outDim
        self.epiStep = 0
        EpisodicTask.reset(self)

    def isFinished(self):
        return (self.epiStep >= self.epiLen)

#Aim is to maximize the edge lengths (best reward: 3096.167, PGPE)
class GrowTask(NoRewardTask):
    def __init__(self, env):
        NoRewardTask.__init__(self, env)
        self.rewardSensor = ["EdgesSumReal"]
        self.obsSensors = ["EdgesReal", "EdgesTarget"]
        self.inDim = len(self.getObservation())
        self.plotString = ["World Interactions", "Size", "Reward on Growing Task"]
        self.env.mySensors = sensors.Sensors(self.obsSensors + self.rewardSensor)
        self.epiLen = 200 #suggested episode length

#Aim is to maximize the distance to the starting point  (best reward: 406.43, PGPE)
class WalkTask(NoRewardTask):
    def __init__(self, env):
        NoRewardTask.__init__(self, env)
        self.rewardSensor = ["DistToOrigin"]
        self.obsSensors = ["EdgesTarget", "EdgesReal", "VerticesContact", "Time"]
        self.inDim = len(self.getObservation())
        self.plotString = ["World Interactions", "Distance", "Reward on Walking Task"]
        self.env.mySensors = sensors.Sensors(self.obsSensors + self.rewardSensor)
        self.epiLen = 2000  #suggested episode length

    def getReward(self):
        if self.epiStep < self.epiLen: self.reward[0] = -self.getPain()
        else: self.reward[0] = self.rawReward * 800.0 - self.getPain()
        return self.reward[0]

#Aim is to maximize the distance to the starting point, but after a high fall (fun task)
class RollingUpTask(WalkTask):
    def __init__(self, env):
        WalkTask.__init__(self, env)
        self.env.startHight = 200.0
        self.env.reset()
        self.epiLen = 500  #suggested episode length

#Aim is to minimize distance to a target point
class WalkDirectionTask(WalkTask):
    def __init__(self, env):
        WalkTask.__init__(self, env)
        self.rewardSensor = ["Target"]
        self.obsSensors.append("Target")
        self.inDim = len(self.getObservation())
        self.plotString = ["World Interactions", "Distance", "Reward on Target Approach Task"]
        self.env.mySensors = sensors.Sensors(self.obsSensors)
        self.env.mySensors.sensors[4].targetList = [array([160.0, 0.0, 0.0])]
        if self.env.hasRenderInterface(): self.env.getRenderInterface().target = self.env.mySensors.sensors[4].targetList[0]
        self.epiLen = 2000
        #self.epiFakt=1.0/float(self.epiLen)

    def getReward(self):
        if self.epiStep < self.epiLen:
            if self.rawReward < 0.5: self.reward[0] = (0.5 - self.rawReward) * 1.0 - self.getPain()
            else: self.reward[0] = -self.getPain()
        else: self.reward[0] = clip(160.0 * (1.0 - self.rawReward), 0.0, 160.0) - self.getPain()
        return self.reward[0]

    def reset(self):
        WalkTask.reset(self)
        self.env.mySensors.sensors[4].targetList = [array([160.0, 0.0, 0.0])]
        if self.env.hasRenderInterface(): self.env.getRenderInterface().target = self.env.mySensors.sensors[4].targetList[0]

#Aim is to minimize distance to a variable target p<aoint
class TargetTask(WalkDirectionTask):
    def __init__(self, env):
        WalkDirectionTask.__init__(self, env)
        self.epiLen = 6000
        self.epiFakt = 1.0 / self.epiLen

    def getReward(self):
        if self.epiStep == self.epiLen / 3 or self.epiStep == 2 * self.epiLen / 3 or self.epiStep == self.epiLen:
            self.reward[0] = clip(160.0 * (1.0 - self.rawReward), 0.0, 160.0) - self.getPain()
        else: self.reward[0] = -self.getPain()
        return self.reward[0]

    def isFinished(self):
        if self.epiStep == self.epiLen / 3:
            self.env.reset()
            self.env.mySensors.sensors[4].targetList = [array([113.2, 0.0, -113.2])]
            if self.env.hasRenderInterface(): self.env.getRenderInterface().target = self.env.mySensors.sensors[4].targetList[0]
        if self.epiStep == 2 * self.epiLen / 3:
            self.env.reset()
            self.env.mySensors.sensors[4].targetList = [array([113.2, 0.0, 113.2])]
            if self.env.hasRenderInterface(): self.env.getRenderInterface().target = self.env.mySensors.sensors[4].targetList[0]
        return (self.epiStep >= self.epiLen)

#Aim is to maximize distance to floor (closest vertex counts)
class JumpTask(NoRewardTask):
    def __init__(self, env):
        NoRewardTask.__init__(self, env)
        self.rewardSensor = ["VerticesMinHight"]
        self.obsSensors = ["EdgesTarget", "EdgesReal", "VerticesContact"]
        self.inDim = len(self.getObservation())
        self.plotString = ["World Interactions", "Distance", "Reward on Walking Task"]
        self.env.mySensors = sensors.Sensors(self.obsSensors + self.rewardSensor)
        self.epiLen = 500
        self.maxReward = 0.0
        self.maxSpeed = self.dif / 10.0

    def getReward(self):
        if self.epiStep < self.epiLen:
            if self.rawReward > self.maxReward: self.maxReward = self.rawReward
            self.reward[0] = -self.getPain()
        else: self.reward[0] = self.maxReward - self.getPain()
        return self.reward[0]

    def reset(self):
        NoRewardTask.reset(self)
        self.maxReward = 0.0


########NEW FILE########
__FILENAME__ = viewer
__author__ = 'Frank Sehnke, sehnke@in.tum.de'

#########################################################################
# OpenGL viewer for the FlexCube Environment 
#
# The FlexCube Environment is a Mass-Spring-System composed of 8 mass points.
# These resemble a cube with flexible edges.
#
# This viewer uses an UDP connection found in tools/networking/udpconnection.py
#
# The viewer recieves the position matrix of the 8 masspoints and the center of gravity.
# With this information it renders a Glut based 3d visualization of teh FlexCube
#
# Options: 
# - serverIP: The ip of the server to which the viewer should connect
# - ownIP: The IP of the computer running the viewer
# - port: The starting port (2 adjacent ports will be used)
#
# Saving the images is possible by setting self.savePics=True.
# Changing the point and angle of view is possible by using the mouse 
# while button 1 or 2 pressed.
# 
# Requirements: OpenGL
#
#########################################################################

from OpenGL.GLUT import * #@UnusedWildImport
from OpenGL.GL import * #@UnusedWildImport
from OpenGL.GLE import * #@UnusedWildImport
from OpenGL.GLU import * #@UnusedWildImport
import objects3d
from time import sleep
from scipy import ones, array
from pybrain.tools.networking.udpconnection import UDPClient

class FlexCubeRenderer(object): 
    #Options: ServerIP(default:localhost), OwnIP(default:localhost), Port(default:21560)
    def __init__(self, servIP="127.0.0.1", ownIP="127.0.0.1", port="21560"):
        self.oldScreenValues = None
        self.view = 0
        self.worldRadius = 400
        
        # Start of mousepointer 
        self.lastx = 0
        self.lasty = 15
        self.lastz = 300
        self.zDis = 1
   
        # Start of cube 
        self.cube = [0.0, 0.0, 0.0]
        self.bmpCount = 0
        self.actCount = 0
        self.calcPhysics = 0
        self.newPic = 1
        self.picCount = 0
        self.target = array([80.0, 0.0, 0.0])
      
        self.centerOfGrav = array([0.0, -2.0, 0.0])
        self.points = ones((8, 3), float)
        self.savePics = False
        self.drawCounter = 0
        self.fps = 25
        self.dt = 1.0 / float(self.fps)

        self.client = UDPClient(servIP, ownIP, port)

    # If self.savePics=True this method saves the produced images      
    def saveTo(self, filename, format="JPEG"):
        import Image # get PIL's functionality... @UnresolvedImport
        width, height = 800, 600
        glPixelStorei(GL_PACK_ALIGNMENT, 1)
        data = glReadPixels(0, 0, width, height, GL_RGB, GL_UNSIGNED_BYTE)
        image = Image.fromstring("RGB", (width, height), data)
        image = image.transpose(Image.FLIP_TOP_BOTTOM)
        image.save(filename, format)
        print('Saved image to ', filename)
        return image

    # the render method containing the Glut mainloop
    def _render(self):
        # Call init: Parameter(Window Position -> x, y, height, width)
        self.init_GL(self, 300, 300, 800, 600)    
        self.object = objects3d.Objects3D()
        self.quad = gluNewQuadric()
        glutMainLoop()

    # The Glut idle function
    def drawIdleScene(self):
        #recive data from server and update the points of the cube
        try:
            self.points, self.centerOfGrav = eval(self.client.listen([self.points, self.centerOfGrav]))
        except: pass
        if self.points == "r":
            self.target = array([80.0, 0.0, 0.0])
            self.centerOfGrav = array([0.0, -2.0, 0.0])
            self.points = ones((8, 3), float)
        self.drawScene()
        if self.savePics:
            self.saveTo("./screenshots/image_jump" + repr(10000 + self.picCount) + ".jpg")
            self.picCount += 1
        else: sleep(self.dt)
          
    def drawScene(self):
        ''' This methode describes the complete scene.'''
        # clear the buffer
        if self.zDis < 10: self.zDis += 0.25
        if self.lastz > 200: self.lastz -= self.zDis
        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)
        glLoadIdentity()
        
        # Point of view
        glRotatef(self.lastx, 0.0, 1.0, 0.0)
        glRotatef(self.lasty, 1.0, 0.0, 0.0)      
        #glRotatef(15, 0.0, 0.0, 1.0)      
        # direction of view is aimed to the center of gravity of the cube
        glTranslatef(-self.centerOfGrav[0], -self.centerOfGrav[1] - 50.0, -self.centerOfGrav[2] - self.lastz)
        
        #Objects
        #Target Ball
        glColor3f(1, 0.25, 0.25)
        glPushMatrix()
        glTranslate(self.target[0], 0.0, self.target[2])
        glutSolidSphere(1.5, 8, 8)
        glPopMatrix()
        
        #Massstab
        for lk in range(41):
            if float(lk - 20) / 10.0 == (lk - 20) / 10: 
                glColor3f(0.75, 0.75, 0.75)
                glPushMatrix()
                glRotatef(90, 1, 0, 0)
                glTranslate(self.worldRadius / 40.0 * float(lk) - self.worldRadius / 2.0, -40.0, -30)
                quad = gluNewQuadric()
                gluCylinder(quad, 2, 2, 60, 4, 1);
                glPopMatrix()
            else: 
                if float(lk - 20) / 5.0 == (lk - 20) / 5:       
                    glColor3f(0.75, 0.75, 0.75)
                    glPushMatrix()
                    glRotatef(90, 1, 0, 0)
                    glTranslate(self.worldRadius / 40.0 * float(lk) - self.worldRadius / 2.0, -40.0, -15.0)
                    quad = gluNewQuadric()
                    gluCylinder(quad, 1, 1, 30, 4, 1);
                    glPopMatrix()
                else:
                    glColor3f(0.75, 0.75, 0.75)
                    glPushMatrix()
                    glRotatef(90, 1, 0, 0)
                    glTranslate(self.worldRadius / 40.0 * float(lk) - self.worldRadius / 2.0, -40.0, -7.5)
                    quad = gluNewQuadric()
                    gluCylinder(quad, 0.5, 0.5, 15, 4, 1);
                    glPopMatrix()
    
        #Mirror Center Ball
        glColor3f(1, 1, 1)
        glPushMatrix()
        glTranslate(self.centerOfGrav[0], -self.centerOfGrav[1], self.centerOfGrav[2])
        glutSolidSphere(1.5, 8, 8)
        glPopMatrix()
        
        #Mirror Cube
        glEnable (GL_BLEND)
        glBlendFunc (GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA)
        
        glColor4f(0.5, 0.75, 0.5, 0.75)
        glPushMatrix()
        glTranslatef(0, -0.05, 0)
        self.object.drawMirCreat(self.points, self.centerOfGrav)
        glPopMatrix()
        
        # Floor
        tile = self.worldRadius / 40.0
        glEnable (GL_BLEND)
        glBlendFunc (GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA)
        
        for xF in range(40):
            for yF in range(40):
                if float(xF + yF) / 2.0 == (xF + yF) / 2: glColor3f(0.8, 0.8, 0.7)
                else: glColor4f(0.8, 0.8, 0.8, 0.7)
                glPushMatrix()
                glTranslatef(0.0, -0.03, 0.0)
                glBegin(GL_QUADS)
                glNormal(0.0, 1.0, 0.0)      
                for i in range(2):
                    for k in range(2):
                        glVertex3f((i + xF - 20) * tile, 0.0, ((k ^ i) + yF - 20) * tile);
                glEnd()
                glPopMatrix()
     
        #Center Ball
        glColor3f(1, 1, 1)
        glPushMatrix()
        glTranslate(self.centerOfGrav[0], self.centerOfGrav[1], self.centerOfGrav[2])
        glutSolidSphere(1.5, 8, 8)
        glPopMatrix()
        
        # Cube    
        glEnable (GL_BLEND)
        glBlendFunc (GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA)
        
        glColor4f(0.5, 0.75, 0.5, 0.75)
        glPushMatrix()
        self.object.drawCreature(self.points, self.centerOfGrav)
        glPopMatrix()
        
        glEnable (GL_BLEND)
        glBlendFunc (GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA)
        
        # Cubes shadow
        glColor4f(0, 0, 0, 0.5)
        glPushMatrix()
        self.object.drawShadow(self.points, self.centerOfGrav)
        glPopMatrix()
        
        # swap the buffer
        glutSwapBuffers()    
    
    def resizeScene(self, width, height):
        '''Needed if window size changes.'''
        if height == 0: # Prevent A Divide By Zero If The Window Is Too Small 
            height = 1
    
        glViewport(0, 0, width, height) # Reset The Current Viewport And Perspective Transformation
        glMatrixMode(GL_PROJECTION)
        glLoadIdentity()
        gluPerspective(45.0, float(width) / float(height), 0.1, 700.0)
        glMatrixMode(GL_MODELVIEW)

    def activeMouse(self, x, y):
        #Returns mouse coordinates while any mouse button is pressed.
        # store the mouse coordinate
        if self.mouseButton == GLUT_LEFT_BUTTON:
            self.lastx = x - self.xOffset
            self.lasty = y - self.yOffset
        if self.mouseButton == GLUT_RIGHT_BUTTON:
            self.lastz = y - self.zOffset 
        # redisplay
        glutPostRedisplay()
  
    def passiveMouse(self, x, y):
        '''Returns mouse coordinates while no mouse button is pressed.'''
        pass
    
    def completeMouse(self, button, state, x, y):
        #Returns mouse coordinates and which button was pressed resp. released.
        self.mouseButton = button
        if state == GLUT_DOWN:
            self.xOffset = x - self.lastx
            self.yOffset = y - self.lasty
            self.zOffset = y - self.lastz
        # redisplay
        glutPostRedisplay()
    
        #Initialise an OpenGL windows with the origin at x, y and size of height, width.
    def init_GL(self, pyWorld, x, y, height, width):
        # initialize GLUT 
        glutInit([])
          
        glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA | GLUT_DEPTH)
        glutInitWindowSize(height, width)
        glutInitWindowPosition(x, y)
        glutCreateWindow("The Curious Cube")
        glClearDepth(1.0)
        glEnable(GL_DEPTH_TEST)
        glClearColor(0.0, 0.0, 0.0, 0.0)
        glShadeModel(GL_SMOOTH)
        glMatrixMode(GL_MODELVIEW)
        # initialize lighting */
        glLightfv(GL_LIGHT0, GL_DIFFUSE, [1, 1, 1, 1.0])
        glLightModelfv(GL_LIGHT_MODEL_AMBIENT, [1.0, 1.0, 1.0, 1.0])
        glEnable(GL_LIGHTING)
        glEnable(GL_LIGHT0)
        # 
        glColorMaterial(GL_FRONT, GL_DIFFUSE)
        glEnable(GL_COLOR_MATERIAL)
        # Automatic vector normalise
        glEnable(GL_NORMALIZE)
        
        ### Instantiate the virtual world ###
        glutDisplayFunc(pyWorld.drawScene)
        glutMotionFunc(pyWorld.activeMouse)
        glutMouseFunc(pyWorld.completeMouse)
        glutReshapeFunc(pyWorld.resizeScene)
        glutIdleFunc(pyWorld.drawIdleScene)

if __name__ == '__main__':
    s = sys.argv[1:]
    r = FlexCubeRenderer(*s)
    r._render()


########NEW FILE########
__FILENAME__ = bbob2010
""" Implementation of all the benchmark functions used in the 2010 GECCO workshop BBOB
(Black-Box Optimization Benchmarking). 

Note: f_opt is fixed to 0 for all.

"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.rl.environments.functions.unimodal import * #@UnusedWildImport
from pybrain.rl.environments.functions.transformations import BBOBTransformationFunction
from pybrain.rl.environments.functions.multimodal import * #@UnusedWildImport



# --- separable ---

def bbob_f1(dim):
    return BBOBTransformationFunction(SphereFunction(dim))

def bbob_f2(dim):
    return BBOBTransformationFunction(ElliFunction(dim),
                                      oscillate=True)

def bbob_f3(dim):
    return BBOBTransformationFunction(RastriginFunction(dim), 
                                      oscillate=True,
                                      asymmetry=0.2)

def bbob_f4(dim):
    return BBOBTransformationFunction(BucheRastriginFunction(dim), 
                                      oscillate=True,
                                      penalized=100)   

def bbob_f5(dim):
    return BBOBTransformationFunction(BoundedLinear(dim), 
                                      translate=False)   


# --- moderate conditioning ---

def bbob_f6(dim):
    return BBOBTransformationFunction(AttractiveSectorFunction(dim), 
                                      rotate=True, 
                                      translate=False)

def bbob_f7(dim):
    return BBOBTransformationFunction(StepElliFunction(dim), 
                                      conditioning=10,
                                      penalized=1,
                                      rotate=True)

def bbob_f8(dim):
    return BBOBTransformationFunction(RosenbrockFunction(dim))

def bbob_f9(dim):
    return BBOBTransformationFunction(RosenbrockFunction(dim), 
                                      rotate=True)


# --- unimodal, high conditioning ---

def bbob_f10(dim):
    return BBOBTransformationFunction(ElliFunction(dim), 
                                      oscillate=True,
                                      rotate=True)

def bbob_f11(dim):
    return BBOBTransformationFunction(TabletFunction(dim), 
                                      oscillate=True,
                                      rotate=True)

def bbob_f12(dim):
    return BBOBTransformationFunction(CigarFunction(dim), 
                                      asymmetry=0.5,
                                      rotate=True)
    
def bbob_f13(dim):
    return BBOBTransformationFunction(SharpRFunctionBis(dim), 
                                      conditioning=10,
                                      rotate=True)

def bbob_f14(dim):
    return BBOBTransformationFunction(DiffPowFunction(dim, a=4), 
                                      rotate=True)


# --- multi-modal with global structure ---

def bbob_f15(dim):
    return BBOBTransformationFunction(RastriginFunction(dim), 
                                      conditioning=10,
                                      oscillate=True,
                                      asymmetry=0.2,
                                      rotate=True)

def bbob_f16(dim):
    return BBOBTransformationFunction(WeierstrassFunction(dim, kmax=11), 
                                      conditioning=0.01,
                                      oscillate=True,
                                      rotate=True)
    
    
def bbob_f17(dim):
    return BBOBTransformationFunction(SchaffersF7Function(dim), 
                                      conditioning=10,
                                      asymmetry=0.5,
                                      penalized=10,
                                      rotate=True)

def bbob_f18(dim):
    return BBOBTransformationFunction(SchaffersF7Function(dim), 
                                      conditioning=1000,
                                      asymmetry=0.5,
                                      penalized=10,
                                      rotate=True)

def bbob_f19(dim):
    return BBOBTransformationFunction(GriewankRosenbrockFunction(dim), 
                                      rotate=True)


# --- multi-modal with weak global structure ---    

def bbob_f20(dim):
    return BBOBTransformationFunction(Schwefel20Function(dim), 
                                      translate=False)
    
def bbob_f21(dim):
    return BBOBTransformationFunction(GallagherGauss101MeFunction(dim), 
                                      translate=False)
    
def bbob_f22(dim):
    return BBOBTransformationFunction(GallagherGauss21HiFunction(dim), 
                                      translate=False)

def bbob_f23(dim):
    return BBOBTransformationFunction(KatsuuraFunction(dim), 
                                      rotate=True,
                                      conditioning=100)

def bbob_f24(dim):
    return BBOBTransformationFunction(LunacekBiRastriginFunction(dim), 
                                      translate=False)
    

# all of them
bbob_collection = [bbob_f1, bbob_f2, bbob_f3, bbob_f4, 
                   bbob_f5, 
                   bbob_f6, bbob_f7, bbob_f8, bbob_f9, bbob_f10, 
                   bbob_f11, bbob_f12, bbob_f13, bbob_f14, bbob_f15, 
                   bbob_f16, 
                   bbob_f17, 
                   bbob_f18, 
                   bbob_f19, bbob_f20, 
                   bbob_f21, bbob_f22, bbob_f23, bbob_f24]

#moderate noise
def bbob_f101(dim):
    return BBOBTransformationFunction(SphereFunction(dim),
                                      gnoise=0.01,
                                      penalized=1)
def bbob_f102(dim):
    return BBOBTransformationFunction(SphereFunction(dim),
                                      unoise=0.01,
                                      penalized=1)  
def bbob_f103(dim):
    return BBOBTransformationFunction(SphereFunction(dim),
                                      cnoise=(0.01,0.05),
                                      penalized=1)
    
def bbob_f104(dim):
    return BBOBTransformationFunction(RosenbrockFunction(dim),
                                      gnoise=0.01,
                                      penalized=1)  
def bbob_f105(dim):
    return BBOBTransformationFunction(RosenbrockFunction(dim),
                                      unoise=0.01,
                                      penalized=1)   
def bbob_f106(dim):
    return BBOBTransformationFunction(RosenbrockFunction(dim),
                                      cnoise=(0.01,0.05),
                                      penalized=1)
    
# severe noise
def bbob_f107(dim):
    return BBOBTransformationFunction(SphereFunction(dim),
                                      gnoise=1,
                                      penalized=1)    
def bbob_f108(dim):
    return BBOBTransformationFunction(SphereFunction(dim),
                                      unoise=1,
                                      penalized=1)    
def bbob_f109(dim):
    return BBOBTransformationFunction(SphereFunction(dim),
                                      cnoise=(1,0.2),
                                      penalized=1)
    
def bbob_f110(dim):
    return BBOBTransformationFunction(RosenbrockFunction(dim),
                                      gnoise=1,
                                      penalized=1)    
def bbob_f111(dim):
    return BBOBTransformationFunction(RosenbrockFunction(dim),
                                      unoise=1,
                                      penalized=1)    
def bbob_f112(dim):
    return BBOBTransformationFunction(RosenbrockFunction(dim),
                                      cnoise=(1,0.2),
                                      penalized=1)
    
def bbob_f113(dim):
    return BBOBTransformationFunction(StepElliFunction(dim), 
                                      conditioning=10,
                                      penalized=1,
                                      rotate=True,
                                      gnoise=1)        
def bbob_f114(dim):
    return BBOBTransformationFunction(StepElliFunction(dim), 
                                      conditioning=10,
                                      penalized=1,
                                      rotate=True,
                                      unoise=1)        
def bbob_f115(dim):
    return BBOBTransformationFunction(StepElliFunction(dim), 
                                      conditioning=10,
                                      penalized=1,
                                      rotate=True,
                                      cnoise=(1,0.2)) 
    
def bbob_f116(dim):
    return BBOBTransformationFunction(ElliFunction(dim, a=100),                                       
                                      oscillate=True, 
                                      penalized=1,                                   
                                      rotate=True,
                                      gnoise=1)        
def bbob_f117(dim):
    return BBOBTransformationFunction(ElliFunction(dim, a=100),                                       
                                      oscillate=True, 
                                      penalized=1,
                                      rotate=True,
                                      unoise=1)        
def bbob_f118(dim):
    return BBOBTransformationFunction(ElliFunction(dim, a=100),                                       
                                      oscillate=True, 
                                      penalized=1,
                                      rotate=True,
                                      cnoise=(1,0.2))   
    
def bbob_f119(dim):
    return BBOBTransformationFunction(DiffPowFunction(dim),  
                                      penalized=1,                                   
                                      rotate=True,
                                      gnoise=1)        
def bbob_f120(dim):
    return BBOBTransformationFunction(DiffPowFunction(dim),  
                                      penalized=1,
                                      rotate=True,
                                      unoise=1)        
def bbob_f121(dim):
    return BBOBTransformationFunction(DiffPowFunction(dim),  
                                      penalized=1,
                                      rotate=True,
                                      cnoise=(1,0.2))    


# multi-modal with severe noise
def bbob_f122(dim):
    return BBOBTransformationFunction(SchaffersF7Function(dim), 
                                      conditioning=10,
                                      asymmetry=0.5,
                                      penalized=1,                                   
                                      rotate=True,
                                      gnoise=1)        
def bbob_f123(dim):
    return BBOBTransformationFunction(SchaffersF7Function(dim), 
                                      conditioning=10,
                                      asymmetry=0.5,
                                      penalized=1,
                                      rotate=True,
                                      unoise=1)        
def bbob_f124(dim):
    return BBOBTransformationFunction(SchaffersF7Function(dim), 
                                      conditioning=10,
                                      asymmetry=0.5,
                                      penalized=1,
                                      rotate=True,
                                      cnoise=(1,0.2)) 
    
def bbob_f125(dim):
    return BBOBTransformationFunction(GriewankRosenbrockFunction(dim),  
                                      penalized=1,                                   
                                      rotate=True,
                                      gnoise=1)        
def bbob_f126(dim):
    return BBOBTransformationFunction(GriewankRosenbrockFunction(dim),  
                                      penalized=1,
                                      rotate=True,
                                      unoise=1)        
def bbob_f127(dim):
    return BBOBTransformationFunction(GriewankRosenbrockFunction(dim),  
                                      penalized=1,
                                      rotate=True,
                                      cnoise=(1,0.2)) 
    
def bbob_f128(dim):
    return BBOBTransformationFunction(GallagherGauss101MeFunction(dim), 
                                      translate=False,
                                      penalized=1,                                   
                                      gnoise=1)        
def bbob_f129(dim):
    return BBOBTransformationFunction(GallagherGauss101MeFunction(dim), 
                                      translate=False,
                                      penalized=1,
                                      unoise=1)        
def bbob_f130(dim):
    return BBOBTransformationFunction(GallagherGauss101MeFunction(dim), 
                                      translate=False,
                                      penalized=1,
                                      cnoise=(1,0.2)) 
    
    

bbob_noise_collection = [bbob_f101, bbob_f102, bbob_f103, 
                         bbob_f104, bbob_f105, bbob_f106, 
                         bbob_f107, bbob_f108, bbob_f109, 
                         bbob_f110, bbob_f111, bbob_f112, 
                         bbob_f113, bbob_f114, bbob_f115, 
                         bbob_f116, bbob_f117, bbob_f118, 
                         bbob_f119, bbob_f120, bbob_f121, 
                         bbob_f122, bbob_f123, bbob_f124,
                         bbob_f125, bbob_f126, bbob_f127, 
                         bbob_f128, bbob_f129, bbob_f130
                         ]

########NEW FILE########
__FILENAME__ = function
from pybrain.utilities import setAllArgs
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import zeros, array, ndarray

from pybrain.rl.environments import Environment
from pybrain.structure.parametercontainer import ParameterContainer
from pybrain.rl.environments.fitnessevaluator import FitnessEvaluator


class FunctionEnvironment(Environment, FitnessEvaluator):
    """ A n-to-1 mapping function to be with a single minimum of value zero, at xopt. """

    # what input dimensions can the function have?
    xdimMin = 1
    xdimMax = None
    xdim = None

    # the (single) point where f = 0
    xopt = None

    # what would be the desired performance? by default: something close to zero
    desiredValue = 1e-10
    toBeMinimized = True
    
    # does the function already include a penalization term, to keep search near the origin?
    penalized = False

    def __init__(self, xdim = None, xopt = None, xbound=5, feasible=True, constrained=False, violation=False, **args):
        self.feasible=feasible
        self.constrained=constrained
        self.violation=violation
        self.xbound=xbound
        if xdim is None:
            xdim = self.xdim
        if xdim is None:
            xdim = self.xdimMin
        assert xdim >= self.xdimMin and not (self.xdimMax is not None and xdim > self.xdimMax)
        self.xdim = xdim
        if xopt is None:
            self.xopt = zeros(self.xdim)
        else:
            self.xopt = xopt
        setAllArgs(self, args)
        self.reset()

    def __call__(self, x):
        if isinstance(x, ParameterContainer):
            x = x.params
        assert type(x) == ndarray, 'FunctionEnvironment: Input not understood: '+str(type(x))
        return self.f(x)

    # methods for conforming to the Environment interface:
    def reset(self):
        self.result = None

    def getSensors(self):
        """ the one sensor is the function result. """
        tmp = self.result
        assert tmp is not None
        self.result = None
        return array([tmp])

    def performAction(self, action):
        """ the action is an array of values for the function """
        self.result = self(action)

    @property
    def indim(self):
        return self.xdim

    # does not provide any observations
    outdim = 0


########NEW FILE########
__FILENAME__ = lennardjones
__author__ = 'Tom Schaul, tom@idsia.ch, and Daan Wierstra'


from pybrain.rl.environments.functions.multimodal import MultiModalFunction
from scipy import sqrt, tile, swapaxes, ravel, eye, randn
import scipy 


class LennardJones(MultiModalFunction):
    """ The classical atom configuration problem. The problem dimension must be a multiple of 3, and the 
    input are the Cartesian coordinates of all atoms."""
    
    def f(self, x):
        N = self.xdim / 3
        coords = x.reshape((N, 3))
        distances = sqrt(scipy.sum((tile(coords, (N, 1, 1)) - swapaxes(tile(coords, (N, 1, 1)), 0, 1)) ** 2, axis=2)) + eye(N)
        return 2 * sum(ravel(distances ** -12 - distances ** -6))
        
    def _exampleConfig(self, numatoms, noise=0.05, edge=2.):
        """ Arranged in an approximate cube of certain edge length. """
        assert numatoms % 8 == 0
        x0 = randn(3, 2, 2, 2, numatoms / 8) * noise * edge - edge / 2
        x0[0, 0] += edge
        x0[1, :, 0] += edge
        x0[2, :, :, 0] += edge
        x0 = x0.reshape(3, numatoms).T
        return x0.flatten()
    
    @property
    def desiredValue(self):
        N = self.xdim / 3
        return self.BEST_KNOWN_TABLE[N] + 1e-5
    
    BEST_KNOWN_TABLE = {0:0, 1:0,
        2:-1.000000, 57:-288.342625,
        3:-3.000000, 58:-294.378148,
        4:-6.000000, 59:-299.738070,
        5:-9.103852, 60:-305.875476,
        6:-12.712062, 61:-312.008896,
        7:-16.505384, 62:-317.353901,
        8:-19.821489, 63:-323.489734,
        9:-24.113360, 64:-329.620147,
        10:-28.422532, 65:-334.971532,
        11:-32.765970, 66:-341.110599,
        12:-37.967600, 67:-347.252007,
        13:-44.326801, 68:-353.394542,
        14:-47.845157, 69:-359.882566,
        15:-52.322627, 70:-366.892251,
        16:-56.815742, 71:-373.349661,
        17:-61.317995, 72:-378.637253,
        18:-66.530949, 73:-384.789377,
        19:-72.659782, 74:-390.908500,
        20:-77.177043, 75:-397.492331,
        21:-81.684571, 76:-402.894866,
        22:-86.809782, 77:-409.083517,
        23:-92.844472, 78:-414.794401,
        24:-97.348815, 79:-421.810897,
        25:-102.372663, 80:-428.083564,
        26:-108.315616, 81:-434.343643,
        27:-112.873584, 82:-440.550425,
        28:-117.822402, 83:-446.924094,
        29:-123.587371, 84:-452.657214,
        30:-128.286571, 85:-459.055799,
        31:-133.586422, 86:-465.384493,
        32:-139.635524, 87:-472.098165,
        33:-144.842719, 88:-479.032630,
        34:-150.044528, 89:-486.053911,
        35:-155.756643, 90:-492.433908,
        36:-161.825363, 91:-498.811060,
        37:-167.033672, 92:-505.185309,
        38:-173.928427, 93:-510.877688,
        39:-180.033185, 94:-517.264131,
        40:-185.249839, 95:-523.640211,
        41:-190.536277, 96:-529.879146,
        42:-196.277534, 97:-536.681383,
        43:-202.364664, 98:-543.642957,
        44:-207.688728, 99:-550.666526,
        45:-213.784862, 100:-557.039820,
        46:-220.680330, 101:-563.411308,
        47:-226.012256, 102:-569.363652,
        48:-232.199529, 103:-575.766131,
        49:-239.091864, 104:-582.086642,
        50:-244.549926, 105:-588.266501,
        51:-251.253964, 106:-595.061072,
        52:-258.229991, 107:-602.007110,
        53:-265.203016, 108:-609.033011,
        54:-272.208631, 109:-615.411166,
        55:-279.248470, 110:-621.788224,
        56:-283.643105
        }




    

########NEW FILE########
__FILENAME__ = multimodal
""" The functions implemented here are standard benchmarks from literature. """

__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import power, exp, cos, sqrt, rand, sin, floor, dot, ones, sign, randn, prod
from scipy.linalg import orth
from math import pi
from random import shuffle

from pybrain.rl.environments.functions.function import FunctionEnvironment
from pybrain.rl.environments.functions.transformations import penalize, generateDiags


class MultiModalFunction(FunctionEnvironment):
    """ A function with more than one local optima. """
    xdimMin = 2
    desiredValue = -1e-3

class FunnelFunction(MultiModalFunction):
    funnelSize = 1.0
    funnelDepth = 1.

    def f(self, x):
        return min(dot(x - 2.5 * ones(self.xdim), x - 2.5 * ones(self.xdim)), \
            self.funnelDepth * self.xdim + self.funnelSize * dot(x + 2.5 * ones(self.xdim), x + 2.5 * ones(self.xdim)))



class RastriginFunction(MultiModalFunction):
    """ A classical multimodal benchmark with plenty of local minima, globally arranged on a bowl. """
    def __init__(self, xdim=1, a=1, xopt=None):
        # additional parameter
        self.a = a
        FunctionEnvironment.__init__(self, xdim, xopt)

    def f(self, x):
        s = 0
        for i, xi in enumerate(x):
            ai = power(self.a, (i - 1) / (self.xdim - 1))
            s += (ai * xi) ** 2 - 10 * cos(2 * pi * ai * xi)
        return s + 10 * len(x)


class WeierstrassFunction(MultiModalFunction):
    """ Global optimum is not unique. 
    
    Standard setting: kmax = 20 (Other variants: kmax=11)."""
        
    kmax = 20
    
    def f(self, x):
        a = 0.5
        b = 3
        res = 0
        for k in range(self.kmax):
            res += sum(a ** k * cos(2 * pi * b ** k * (x + 0.5)))
            res -= self.xdim * a ** k * cos(2 * pi * b ** k * 0.5)
        return res


class SchaffersF7Function(MultiModalFunction):
        
    def f(self, x):
        s = sqrt(x[:-1] ** 2 + x[1:] ** 2)
        return sum(sqrt(s) * (1 + sin(50 * power(s, 0.2)) ** 2)) ** 2


class AckleyFunction(MultiModalFunction):
    def f(self, x):
        res = -20 * exp(-0.2 * sqrt(1. / self.xdim * sum(x ** 2)))
        res -= exp((1. / self.xdim) * sum(cos(2 * pi * x)))
        res += 20 + exp(1)
        return res


class GriewankFunction(MultiModalFunction):
    def f(self, x):
        prod = 1
        for i, xi in enumerate(x):
            prod *= cos(xi / sqrt(i + 1))
        return 1 + sum(x ** 2) / 4000. - prod
    
class BucheRastriginFunction(MultiModalFunction):
    """ Deceptive and highly multi-modal."""
    
    def f(self, x):
        z = x[:]
        for i in range(self.xdim):
            e = i/(self.xdim-1.)/2.
            if x[i] <= 0 or i%2==0:
                e += 1
            z[i] *= power(10, e)
        return dot(z,z) + 10 * self.xdim - 10*sum(cos(2*pi*z))
    
class GriewankRosenbrockFunction(MultiModalFunction):
    """ Composite between the two. """
    
    def f(self, x):
        s = 100 * (x[:-1] ** 2 - x[1:]) ** 2 + (x[:-1] - 1) ** 2
        return 1/(self.xdim-1.) * sum(s / 4000. - cos(s)) +1
    


class Schwefel_2_13Function(MultiModalFunction):
    def __init__(self, *args, **kwargs):
        MultiModalFunction.__init__(self, *args, **kwargs)
        self.A = floor(rand(self.xdim, self.xdim) * 200) - 100
        self.B = floor(rand(self.xdim, self.xdim) * 200) - 100
        self.alphas = 2 * pi * rand(self.xdim) - pi

    def f(self, x):
        res = 0
        for i in range(self.xdim):
            Ai = sum(self.A[i] * sin(self.alphas) + self.B[i] * cos(self.alphas))
            Bix = sum(self.A[i] * sin(x) + self.B[i] * cos(x))
            res += (Ai - Bix) ** 2
        return res
    
class Schwefel20Function(MultiModalFunction):
    """ f20 in BBOB. """
    
    penalized = True
    
    _k = 4.189828872724339
    _k2 = 4.20966874633/2
    
    def __init__(self, *args, **kwargs):
        MultiModalFunction.__init__(self, *args, **kwargs)
        self._signs = sign(randn(self.xdim))
        self._diags = generateDiags(10, self.xdim)
        self.xopt = self._k2 * self._signs
        
    def f(self, x):
        z = 2* x * self._signs        
        z[1:] += (z[:-1]-self.xopt[:-1]) * 0.25
        z = 100 * (dot(self._diags, (z-self.xopt)) + self.xopt)
        return - 1. / float(self.xdim) * sum(z * sin(sqrt(abs(z)))) + self._k + 100 * penalize(z / 100.)
        
    
class GallagherGauss101MeFunction(MultiModalFunction):
    """ 101 random local optima (medium peaks). """
    
    numPeaks = 101
    maxCond = 1000.
    optCond = sqrt(1000)

    def __init__(self, *args, **kwargs):
        MultiModalFunction.__init__(self, *args, **kwargs)
        print(self.numPeaks, self.xdim)
        self._opts = [(rand(self.xdim) - 0.5) * 8]
        self._opts.extend([(rand(self.xdim) - 0.5) * 9.8 for _ in range(self.numPeaks-1)])
        alphas = [power(self.maxCond, 2 * i / float(self.numPeaks - 2)) for i in range(self.numPeaks - 1)]
        shuffle(alphas)
        self._covs = [generateDiags(alpha, self.xdim, shuffled=True) / power(alpha, 0.25) for alpha in [self.optCond] + alphas]
        self._R = orth(rand(self.xdim, self.xdim))
        self._ws = [10] + [1.1 + 8 * i / float(self.numPeaks - 2) for i in range(self.numPeaks - 1)]
        
        
    def f(self, x):
        rxy = [dot(self._R, (x - o)) for o in self._opts]
        return (10 - max([self._ws[i] * exp(-1 / (2. * self.xdim) * dot(rxy[i], dot(self._covs[i], rxy[i]))) 
                          for i in range(self.numPeaks)])) ** 2


class GallagherGauss21HiFunction(GallagherGauss101MeFunction):
    """ 21 random local optima (high peaks). """
    
    numPeaks = 21
    optCond = 1000.


class KatsuuraFunction(MultiModalFunction):
    """ Has more than 10^dim optima. """
    
    def f(self, x):
        return - 1 + prod([power(1 + (i + 1) * sum([abs(2 ** j * x[i] - int(2 ** j * x[i])) * 2 ** -j 
                                                    for j in range(1, 33)]),
                                 10. / power(self.xdim, 1.2)) 
                          for i in range(self.xdim)])
    
class LunacekBiRastriginFunction(MultiModalFunction):
    """ A deceptive double-funnel structure with many local optima. 
    The bad funnel has about 70% of the search volume. """
    
    def __init__(self, *args, **kwargs):
        MultiModalFunction.__init__(self, *args, **kwargs)
        self._mu0 = 2.5
        self._s = 1 - 1 / (2 * sqrt(self.xdim + 20) - 8.2)
        self._mu1 = -sqrt((self._mu0 ** 2 - 1) / self._s)
        self._signs = sign(randn(self.xdim))
        self._R1 = orth(rand(self.xdim, self.xdim))
        self._R2 = orth(rand(self.xdim, self.xdim))
        self._diags = generateDiags(100, self.xdim)
        
    def f(self, x):
        x_ = x * self._signs * 2
        z = dot(self._R1, dot(self._diags, dot(self._R2, x_ - self._mu0)))
        return (min(dot(x_ - self._mu0, x_ - self._mu0),
                    self.xdim + self._s * dot(x_ - self._mu1, x_ - self._mu1)) 
                + 10 * (self.xdim - sum(cos(2 * pi * z))))

         
        

class BraninFunction(MultiModalFunction):
    """ Has 3 global optima at (-pi, 12.275), (pi, 2.275), (9.42478, 2.475) """

    xdimMax = 2

    _a = 1.
    _b = 5.1 / (4 * pi ** 2)
    _c = 5 / pi
    _d = 6.
    _e = 10.
    _f = 1. / (8 * pi)

    vopt = 0.397887

    _globalOptima = [[-pi, 12.275], [pi, 2.275], [9.42478, 2.475]]

    def f(self, x):
        return self._a * (x[1] - self._b * x[0] ** 2 + self._c * x[0] - self._d) ** 2 + self._e * ((1 - self._f) * cos(x[0]) + 1) - self.vopt


########NEW FILE########
__FILENAME__ = multiobjective
""" Some multi-objective benchmark functions.
Implemented according to the classical reference paper of Deb et al. (Evolutionary Computation 2002) """

from scipy import array, exp, sqrt, sin, cos, power, pi, arctan, ndarray
from pybrain.rl.environments.functions.function import FunctionEnvironment
from pybrain.structure.parametercontainer import ParameterContainer


__author__ = 'Tom Schaul, tom@idsia.ch'


class MultiObjectiveFunction(FunctionEnvironment):
    """ A function with multiple outputs. """
    ydim = 2 # by default
    
    feasible = None
    xbound = None
    constrained = None
    violation = None
    
    @property
    def outfeasible(self):
        return self.feasible
        
    @property
    def outviolation(self):
        return self.violation

    @property
    def outdim(self):
        return self.ydim
        
    def __call__(self, x):
        if isinstance(x, ParameterContainer):
            x = x.params
        assert type(x) == ndarray, 'FunctionEnvironment: Input not understood: '+str(type(x))
        res = self.f(x)
        return res

class SchBenchmark(MultiObjectiveFunction):
    """ Schaffer 1987 """
    xdim = 1
    xdimMax = 1

    def f(self, x):
        return -array([x**2, (x-2)**2])


class FonBenchmark(MultiObjectiveFunction):
    """ Fonesca and Fleming 1993 """
    xdim = 3

    def f(self, x):
        f1 = 1 - exp(-sum((x-1/sqrt(3))**2))
        f2 = 1 - exp(-sum((x+1/sqrt(3))**2))
        return -array([f1, f2])


class PolBenchmark(MultiObjectiveFunction):
    """ Poloni 1997 """
    xdim = 2

    _A1 = 0.5 * sin(1) - 2*cos(1) + sin(2) -1.5*cos(2)
    _A2 = 1.5 * sin(1) - cos(1) + 2*sin(2) -0.5*cos(2)

    def f(self, x):
        B1 = 0.5 * sin(x[0]) - 2*cos(x[0]) + sin(x[1]) -1.5*cos(x[1])
        B2 = 1.5 * sin(x[0]) - cos(x[0]) + 2*sin(x[1]) -0.5*cos(x[1])

        f1 = 1 + (self._A1-B1)**2 + (self._A2-B2)**2
        f2 = (x[0]+3)**2 + (x[1]+1)**2
        return -array([f1, f2])


class KurBenchmark(MultiObjectiveFunction):
    """ Kursawe 1990 """
    xdim = 3

    def f(self, x):
        f1 = sum(-10*exp(-0.2*sqrt(x[:-1]**2+x[1:]**2)))
        f2 = sum(power(abs(x), 0.8)+5*sin(x**3))
        return -array([f1, f2])

''' added by JPQ'''
class Deb(MultiObjectiveFunction):
    """ Deb 2001 """
    xdim = 2
    def __init__(self):
        self.xbound = []
        self.xbound.append((0.1,1.0))
        self.xbound.append((0.0,5.0))
        self.constrained = False

    def f(self, x):
        f1 = x[0]
        f2 = (1+x[1])/x[0]
        return -array([f1, f2])

class ConstDeb(MultiObjectiveFunction):
    """ Deb 2001 """
    xdim = 2
    def __init__(self):
        self.feasible = None
        self.xbound = []
        self.xbound.append((0.1,1.0))
        self.xbound.append((0.0,5.0))
        self.constrained = True

    def g(self, x):
        g1 = x[1] + 9.0*x[0] - 6.0
        g2 = -x[1] + 9.0*x[0] - 1.0
        if g1 >= 0 and g2 >= 0:
            return True,array([0.,0.])
        return False,array([g1,g2])
    def f(self, x):
        self.feasible,self.violation = self.g(x)
        ''' 
        not nice, due to the fact that oppositeFunction does not used
        the instance of this class for the evaluator but creates a new instance
        of class MultiObjectiveFunction
        '''
        MultiObjectiveFunction.feasible = self.feasible
        MultiObjectiveFunction.violation = self.violation
        f1 = x[0]
        f2 = (1+x[1])/x[0]
        return -array([f1, f2])

class Pol(MultiObjectiveFunction):
    """ Poloni 1997 """
    xdim = 2
    _A1 = 0.5 * sin(1) - 2*cos(1) + sin(2) -1.5*cos(2)
    _A2 = 1.5 * sin(1) - cos(1) + 2*sin(2) -0.5*cos(2)
    
    def __init__(self):
        self.xbound = []
        self.xbound.append((-pi,pi))
        self.xbound.append((-pi,pi))
        self.constrained = False


    def f(self, x):
        B1 = 0.5 * sin(x[0]) - 2*cos(x[0]) + sin(x[1]) -1.5*cos(x[1])
        B2 = 1.5 * sin(x[0]) - cos(x[0]) + 2*sin(x[1]) -0.5*cos(x[1])

        f1 = 1 + (self._A1-B1)**2 + (self._A2-B2)**2
        f2 = (x[0]+3)**2 + (x[1]+1)**2
        return -array([f1, f2])

class ConstSrn(MultiObjectiveFunction):
    """ Srinivas and Deb 1994 """
    xdim = 2
    def __init__(self):
        self.feasible = None
        self.xbound = []
        self.xbound.append((-20.0,20.0))
        self.xbound.append((-20.0,20.0))
        self.constrained = True

    def g(self, x):
        g1 = 225 - (x[1]**2 + x[0]**2)
        g2 = 3*x[1] -x[0] - 10.0
        if g1 >= 0 and g2 >= 0:
            return True,array([0.,0.])
        return False,array([g1,g2])
    def f(self, x):
        self.feasible,self.violation = self.g(x)
        MultiObjectiveFunction.feasible = self.feasible
        MultiObjectiveFunction.violation = self.violation
        f1 = 2+(x[0]-2)**2+(x[1]-1)**2
        f2 = 9*x[0]-(x[1]-1)**2
        return -array([f1, f2])
        
class ConstOsy(MultiObjectiveFunction):
    """ Osyczka and Kundu 1995 """
    xdim = 6
    def __init__(self):
        self.feasible = None
        self.xbound = []
        self.xbound.append((0.0,10.0))
        self.xbound.append((0.0,10.0))
        self.xbound.append((1.0,5.0))
        self.xbound.append((0.0,6.0))
        self.xbound.append((1.0,5.0))
        self.xbound.append((0.0,10.0))
        self.constrained = True

    def g(self, x):
        g1 = x[0] + x[1] -2.0
        g2 = 6.0 -x[0] -x[1]
        g3 = 2.0 -x[1] +x[0]
        g4 = 1.0 -x[0] +3.0*x[1]
        g5 = 4.0 -(x[2]-3)**2 -x[3]
        g6 = (x[4]-3)**2 +x[5] -4.0
        if g1 >= 0 and g2 >= 0 and g3 >= 0 and g4 >= 0 and g5 >= 0 and g6 >= 0:
            return True,array([0.,0.])
        return False,array([g1,g2,g3,g4,g5,g6])
    def f(self, x):
        self.feasible,self.violation = self.g(x)
        MultiObjectiveFunction.feasible = self.feasible
        MultiObjectiveFunction.violation = self.violation
        f1 = -(25.0*(x[0]-2.0)**2+(x[1]-2.0)**2+(x[2]-1.0)**2+(x[3]-4.0)**2+(x[4]-1.0)**2)
        f2 = x[0]**2+x[1]**2+x[2]**2+x[3]**2+x[4]**2+x[5]**2
        return -array([f1, f2])

class ConstTnk(MultiObjectiveFunction):
    """ Tanaka 1995 """
    xdim = 2
    def __init__(self):
        self.feasible = None
        self.xbound = []
        self.xbound.append((0.0,pi))
        self.xbound.append((0.0,pi))
        self.constrained = True

    def g(self, x):
        if x[1] == 0.0:
            A = pi/2.0
        else:
            A = arctan(x[0]/x[1])
        g1 = x[1]**2 + x[0]**2 - 1.0 -0.1*cos(16.0*A)
        g2 = 0.5 -(x[0]-0.5)**2 -(x[1]-0.5)**2 
        if g1 >= 0 and g2 >= 0:
            return True,array([0.,0.])
        return False,array([g1,g2])
    def f(self, x):
        self.feasible,self.violation = self.g(x)
        MultiObjectiveFunction.feasible = self.feasible
        MultiObjectiveFunction.violation = self.violation
        f1 = x[0]
        f2 = x[1]
        return -array([f1, f2])
    
class ConstBnh(MultiObjectiveFunction):
    """ Binh & Korn 1997 """
    xdim = 2
    def __init__(self):
        self.feasible = None
        self.xbound = []
        self.xbound.append((0.0,5.0))
        self.xbound.append((0.0,3.0))
        self.constrained = True

    def g(self, x):
        g1 = 25 - (x[0]-5.0)**2 - x[1]**2
        g2 = (x[0]-8.0)**2 + (x[1]+3.0)**2 - 7.7
        if g1 >= 0 and g2 >= 0:
            return True,array([0.,0.])
        return False,array([g1,g2])
    def f(self, x):
        self.feasible,self.violation = self.g(x)
        MultiObjectiveFunction.feasible = self.feasible
        MultiObjectiveFunction.violation = self.violation
        f1 = 4*x[0]**2 + 4*x[1]**2
        f2 = (x[0]-5)**2 + (x[1]-5)**2
        return -array([f1, f2])
# ---
########NEW FILE########
__FILENAME__ = transformations
__author__ = 'Tom Schaul, tom@idsia.ch'


from scipy import rand, dot, power, diag, eye, sqrt, sin, log, exp, ravel, clip, arange
from scipy.linalg import orth, norm, inv
from random import shuffle, random, gauss

from pybrain.rl.environments.functions.function import FunctionEnvironment
from pybrain.structure.parametercontainer import ParameterContainer
from pybrain.rl.environments.fitnessevaluator import FitnessEvaluator
from pybrain.utilities import sparse_orth, dense_orth
from pybrain.rl.environments.functions.multiobjective import MultiObjectiveFunction


def oppositeFunction(basef):
    """ the opposite of a function """
    if isinstance(basef, FitnessEvaluator):
        if isinstance(basef, FunctionEnvironment):
            ''' added by JPQ '''
            if isinstance(basef, MultiObjectiveFunction):
                res = MultiObjectiveFunction()
            else:
            # ---
                res = FunctionEnvironment(basef.xdim, basef.xopt)
        else:
            res = FitnessEvaluator()        
        res.f = lambda x:-basef.f(x)
        if not basef.desiredValue is None:
            res.desiredValue = -basef.desiredValue
        res.toBeMinimized = not basef.toBeMinimized
        return res
    else:    
        return lambda x:-basef(x)
                

class TranslateFunction(FunctionEnvironment):
    """ change the position of the optimum """        
    
    def __init__(self, basef, distance=0.1, offset=None):
        """ by default the offset is random, with a distance of 0.1 to the old one """
        FunctionEnvironment.__init__(self, basef.xdim, basef.xopt)
        if offset == None:
            self._offset = rand(basef.xdim)
            self._offset *= distance / norm(self._offset)
        else:
            self._offset = offset
        self.xopt += self._offset
        self.desiredValue = basef.desiredValue            
        self.toBeMinimized = basef.toBeMinimized
        def tf(x):
            if isinstance(x, ParameterContainer):
                x = x.params
            return basef.f(x - self._offset)
        self.f = tf
    

class RotateFunction(FunctionEnvironment):
    """ make the dimensions non-separable, by applying a matrix transformation to 
    x before it is given to the function """
    
    def __init__(self, basef, rotMat=None):
        """ by default the rotation matrix is random. """
        FunctionEnvironment.__init__(self, basef.xdim, basef.xopt)
        if rotMat == None:
            # make a random orthogonal rotation matrix
            self._M = orth(rand(basef.xdim, basef.xdim))
        else:
            self._M = rotMat
        self.desiredValue = basef.desiredValue            
        self.toBeMinimized = basef.toBeMinimized   
        self.xopt = dot(inv(self._M), self.xopt)
        def rf(x):
            if isinstance(x, ParameterContainer):
                x = x.params
            return basef.f(dot(x, self._M))    
        self.f = rf
        

def penalize(x, distance=5):
    ax = abs(x)
    tmp = clip(ax-distance, 0, ax.max())
    return dot(tmp, tmp)
    #return sum([max(0, abs(xi) - distance) ** 2 for xi in x])
        

class SoftConstrainedFunction(FunctionEnvironment):
    """ Soft constraint handling through a penalization term. """
    
    penalized = True
    
    def __init__(self, basef, distance=5, penalizationFactor=1.):
        FunctionEnvironment.__init__(self, basef.xdim, basef.xopt)
        self.desiredValue = basef.desiredValue            
        self.toBeMinimized = basef.toBeMinimized
        if basef.penalized:
            # already OK
            self.f = basef.f
        else:
            if not self.toBeMinimized:
                penalizationFactor *= -1
                
            def scf(x):
                if isinstance(x, ParameterContainer):
                    x = x.params
                return basef.f(x) + penalize(x, distance) * penalizationFactor
            
            self.f = scf
    
    
def generateDiags(alpha, dim, shuffled=False):    
    diags = [power(alpha, i / (2 * dim - 2.)) for i in range(dim)]
    if shuffled:
        shuffle(diags)
    return diag(diags)


class BBOBTransformationFunction(FunctionEnvironment):
    """ Reimplementation of the relatively complex set of function and 
    variable transformations, and their non-trivial combinations from BBOB 2010.
    But in clean, reusable code.
    """    
    
    def __init__(self, basef,
                 translate=True,
                 rotate=False,
                 conditioning=None,
                 asymmetry=None,
                 oscillate=False,
                 penalized=0,
                 desiredValue=1e-8,
                 gnoise=None,
                 unoise=None,
                 cnoise=None,
                 sparse=True,
                 ):
        FunctionEnvironment.__init__(self, basef.xdim, basef.xopt)
        self._name = basef.__class__.__name__
        self.desiredValue = desiredValue            
        self.toBeMinimized = basef.toBeMinimized
        
        if self.xdim < 500:
            sparse = False
        
        if sparse:
            try:
                from scipy.sparse import csc_matrix
            except:
                sparse = False
        
        if translate:            
            self.xopt = (rand(self.xdim) - 0.5) * 9.8
                    
        if conditioning:
            prefix = generateDiags(conditioning, self.xdim)                
            if sparse:
                prefix = csc_matrix(prefix)
                if rotate:
                    prefix = prefix * sparse_orth(self.xdim)
                    if oscillate or not asymmetry:
                        prefix = sparse_orth(self.xdim) * prefix                
            else:
                if rotate:
                    prefix = dot(prefix, dense_orth(self.xdim))
                    if oscillate or not asymmetry:
                        prefix = dot(dense_orth(self.xdim), prefix)
                
        elif rotate and asymmetry and not oscillate:
            if sparse:
                prefix = sparse_orth(self.xdim)
            else:
                prefix = dense_orth(self.xdim)
        elif sparse:
            prefix = None
        else:
            prefix = eye(self.xdim)  
            
        if penalized != 0:
            if self.penalized:
                penalized = 0
            else:
                self.penalized = True
        
        # combine transformations    
        if rotate:
            if sparse:
                r = sparse_orth(self.xdim)
                tmp1 = lambda x: ravel(x * r)
            else:
                r = dense_orth(self.xdim)
                tmp1 = lambda x: dot(x, r)
        else:
            tmp1 = lambda x: x
            
        if oscillate:
            tmp2 = lambda x: BBOBTransformationFunction.oscillatify(tmp1(x))     
        else:
            tmp2 = tmp1            
        
        if asymmetry is not None:
            tmp3 = lambda x: BBOBTransformationFunction.asymmetrify(tmp2(x), asymmetry)
        else:
            tmp3 = tmp2
            
        # noise
        ntmp = None
        if gnoise:
            ntmp = lambda f: f * exp(gnoise * gauss(0, 1))
        elif unoise:
            alpha = 0.49 * (1. / self.xdim) * unoise
            ntmp = lambda f: f * power(random(), unoise) * max(1, power(1e9 / (f + 1e-99), alpha * random())) 
        elif cnoise:
            alpha, beta = cnoise
            ntmp = lambda f: f + alpha * max(0, 1000 * (random() < beta) * gauss(0, 1) / (abs(gauss(0, 1)) + 1e-199))
            
        def noisetrans(f):
            if ntmp is None or f < 1e-8:
                return f
            else:
                return ntmp(f) + 1.01e-8
            
        if sparse:
            if prefix is None:
                tmp4 = lambda x: tmp3(x - self.xopt)
            else:
                tmp4 = lambda x: ravel(prefix * tmp3(x - self.xopt))
        else:
            tmp4 = lambda x: dot(prefix, tmp3(x - self.xopt))
                            
        self.f = lambda x: (noisetrans(basef.f(tmp4(x))) 
                            + penalized * penalize(x))
        

    @staticmethod
    def asymmetrify(x, beta=0.2):
        dim = len(x)
        return x * (x<=0) + (x>0) * exp((1+beta*arange(dim)/(dim-1.)*sqrt(abs(x))) * log(abs(x)+1e-100))
        #res = x.copy()
        #for i, xi in enumerate(x):
        #    if xi > 0:
        #        res[i] = power(xi, 1 + beta * i / (dim - 1.) * sqrt(xi))
        #return res
    
    @staticmethod
    def _oscillatify(x):
        if isinstance(x, float):
            res = [x]
        else:
            res = x.copy()        
        for i, xi in enumerate(res):
            if xi == 0: 
                continue
            elif xi > 0:
                s = 1 
                c1 = 10
                c2 = 7.9
            else:
                s = 1
                c1 = 5.5
                c2 = 3.1
            res[i] = s * exp(log(abs(xi)) + 0.049 * (sin(c1 * xi) + sin(c2 * xi)))
        if isinstance(x, float):
            return res[0]
        else:
            return res

    @staticmethod
    def oscillatify(x):
        return exp(log(abs(x)+1e-100)
                   + (x>0) * 0.049 * (sin(10 * x) + sin(7.9 * x))
                   + (x<0) * 0.049 * (sin(5.5 * x) + sin(3.1 * x)))
        
########NEW FILE########
__FILENAME__ = unbounded
""" The functions implemented here are standard benchmarks from literature. """

__author__ = 'Tom Schaul, tom@idsia.ch'

from math import sqrt

from pybrain.rl.environments.functions.function import FunctionEnvironment


class UnboundedFunctionEnvironment(FunctionEnvironment):
    """ a function that does not have a minimum """
    desiredValue = -1e3
    toBeMinimized = True


class LinearFunction(UnboundedFunctionEnvironment):
    def f(self, x):
        return sum(x)


class ParabRFunction(UnboundedFunctionEnvironment):
    def f(self, x):
        return -x[0] + 100 * sum(x[1:]**2)


class SharpRFunction(UnboundedFunctionEnvironment):
    def f(self, x):
        return -x[0] + 100*sqrt(sum(x[1:]**2))


########NEW FILE########
__FILENAME__ = unimodal
""" The functions implemented here are standard benchmarks from literature. """

__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import ones, sqrt, dot, sign, randn, power, rand, floor, array
from scipy.linalg import norm, orth

from pybrain.rl.environments.functions.function import FunctionEnvironment


class SphereFunction(FunctionEnvironment):
    """ Simple quadratic function. """
    def f(self, x):
        return dot(x,x)


class SchwefelFunction(FunctionEnvironment):
    def f(self, x):
        s = 0
        for i in range(len(x)):
            s += sum(x[:i])**2
        return s


class CigarFunction(FunctionEnvironment):
    """ Bent Cigar function """
    xdimMin = 2

    def f(self, x):
        return x[0]**2 + 1e6*dot(x[1:],x[1:])


class TabletFunction(FunctionEnvironment):
    """ Also known as discus function."""
    xdimMin = 2

    def f(self, x):
        return 1e6*x[0]**2 + dot(x[1:],x[1:])


class ElliFunction(FunctionEnvironment):
    """ Ellipsoid. """
        
    a = 1000
    
    def __init__(self, *args, **kwargs):
        FunctionEnvironment.__init__(self, *args, **kwargs)
        self._as = array([power(self.a, 2*i/(self.xdim-1.)) for i in range(self.xdim)])
        
    def f(self, x):
        return dot(self._as*x, x)

class StepElliFunction(ElliFunction):
    """ Plateaus make for a zero derivative """
    
    a = 10
    
    def __init__(self, *args, **kwargs):
        ElliFunction.__init__(self, *args, **kwargs)
        self._R = orth(rand(self.xdim, self.xdim))
    
    def f(self, x):
        # rounding
        z = floor(0.5 + x) * (x>0.5) + floor(0.5 + 10*x)/10. * (x<=0.5)
        z = dot(self._R, z)         
        return 0.1 * max(1e-4 * abs(z[0]), ElliFunction.f(self, z))
    
    
    
class AttractiveSectorFunction(FunctionEnvironment):
    """ Only one of the quadrants has the good values. """
    
    def __init__(self, *args, **kwargs):
        FunctionEnvironment.__init__(self, *args, **kwargs)
        self.xopt = (rand(self.xdim) - 0.5) * 9.8   
    
    def f(self, x):
        from transformations import BBOBTransformationFunction
        quad = (x*self.xopt > 0)
        sz = 100 * x * quad + x * (quad==False)         
        return power(BBOBTransformationFunction.oscillatify(dot(sz, sz)), 0.9)
        
        

class SharpRFunctionBis(FunctionEnvironment):
    """ Bounded version of the Sharp ridge function. """
    def f(self, x):
        return x[0]**2 + 100*sqrt(dot(x[1:],x[1:]))
    

class DiffPowFunction(FunctionEnvironment):
    """ Different powers.
    Standard setting: a=10 (other variants: a=4)
    """
    
    a = 10
    
    def f(self, x):
        s = 0
        for i in range(len(x)):
            s += abs(x[i])**(2+self.a*i/(len(x)-1))
        return s


class RosenbrockFunction(FunctionEnvironment):
    """ Banana-shaped function with a tricky optimum in the valley at 1,1.
    (has another, local optimum in higher dimensions)."""
    def __init__(self, xdim = 2, xopt = None):
        assert xdim >= self.xdimMin and not (self.xdimMax != None and xdim > self.xdimMax)
        self.xdim = xdim
        if xopt == None:
            self.xopt = ones(self.xdim)
        else:
            self.xopt = xopt
        self.reset()

    def f(self, x):
        return sum(100*(x[:-1]**2-x[1:])**2 + (x[:-1]-1)**2)
        
        
class GlasmachersFunction(FunctionEnvironment):
    """ Tricky! Designed to make most algorithms fail. """
    c = .1
    xdimMin = 2

    def f(self, x):
        m = self.xdim/2
        a = self.c * norm(x[:m])
        b = norm(x[m:])
        return a + b + sqrt(2*a*b+b**2)
    
    
class BoundedLinear(FunctionEnvironment):
    """ Linear function within the domain [-5,5], constant from the 
    best corner onwards. """
    
    def __init__(self, *args, **kwargs):
        FunctionEnvironment.__init__(self, *args, **kwargs)
        self._w = [power(10, i/(self.xdim-1.)) for i in range(self.xdim)]        
        self._signs = sign(randn(self.xdim))       
    
    def f(self, x):
        x_ = x[:]
        for i, xi in enumerate(x):
            if xi*self._signs[i] > 5:
                x_[i] = self._signs[i]*5
        return 5*sum(self._w) - dot(self._w*self._signs, x_) 
        
        

########NEW FILE########
__FILENAME__ = graphical
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from environment import Environment

class GraphicalEnvironment(Environment):
    """ Special type of environment that has graphical output and therefore needs a renderer.
    """

    def __init__(self):
        self.renderer = None

    def setRenderer(self, renderer):
        """ set the renderer, which is an object of or inherited from class Renderer.

            :key renderer: The renderer that should display the Environment
            :type renderer: L{Renderer}
            :see: Renderer
        """
        self.renderer = renderer

    def getRenderer(self):
        """ returns the current renderer.

            :return: the current renderer
            :rtype: L{Renderer}
        """
        return self.renderer

    def hasRenderer(self):
        """ tells you whether or not a Renderer has been set previously

            :return: True if a renderer was set, False otherwise
            :rtype: Boolean
        """
        return (self.getRenderer() != None)

########NEW FILE########
__FILENAME__ = maze
__author__ = 'Tom Schaul, tom@idsia.ch'

from random import random, choice
from scipy import zeros

from pybrain.utilities import Named
from pybrain.rl.environments.environment import Environment

# TODO: mazes can have any number of dimensions?


class Maze(Environment, Named):
    """ 2D mazes, with actions being the direction of movement (N,E,S,W)
    and observations being the presence of walls in those directions.

    It has a finite number of states, a subset of which are potential starting states (default: all except goal states).
    A maze can have absorbing states, which, when reached end the episode (default: there is one, the goal).

    There is a single agent walking around in the maze (Theseus).
    The movement can succeed or not, or be stochastically determined.
    Running against a wall does not get you anywhere.

    Every state can have an an associated reward (default: 1 on goal, 0 elsewhere).
    The observations can be noisy.
    """

    # table of booleans
    mazeTable = None

    # single goal
    goal = None

    # current state
    perseus = None

    # list of possible initial states
    initPos = None

    # directions
    N = (1, 0)
    S = (-1, 0)
    E = (0, 1)
    W = (0, -1)

    allActions = [N, E, S, W]

    # stochasticity
    stochAction = 0.
    stochObs = 0.

    def __init__(self, topology, goal, **args):
        self.setArgs(**args)
        self.mazeTable = topology
        self.goal = goal
        if self.initPos == None:
            self.initPos = self._freePos()
            self.initPos.remove(self.goal)
        self.reset()

    def reset(self):
        """ return to initial position (stochastically): """
        self.bang = False
        self.perseus = choice(self.initPos)

    def _freePos(self):
        """ produce a list of the free positions. """
        res = []
        for i, row in enumerate(self.mazeTable):
            for j, p in enumerate(row):
                if p == False:
                    res.append((i, j))
        return res

    def _moveInDir(self, pos, dir):
        """ the new state after the movement in one direction. """
        return (pos[0] + dir[0], pos[1] + dir[1])

    def performAction(self, action):
        if self.stochAction > 0:
            if random() < self.stochAction:
                action = choice(range(len(self.allActions)))
        tmp = self._moveInDir(self.perseus, self.allActions[action])
        if self.mazeTable[tmp] == False:
            self.perseus = tmp
            self.bang = False
        else:
            self.bang = True

    def getSensors(self):
        obs = zeros(4)
        for i, a in enumerate(Maze.allActions):
            obs[i] = self.mazeTable[self._moveInDir(self.perseus, a)]
        if self.stochObs > 0:
            for i in range(len(obs)):
                if random() < self.stochObs:
                    obs[i] = not obs[i]
        return obs

    def __str__(self):
        """ Ascii representation of the maze, with the current state """
        s = ''
        for r, row in reversed(list(enumerate(self.mazeTable))):
            for c, p in enumerate(row):
                if (r, c) == self.goal:
                    s += '*'
                elif (r, c) == self.perseus:
                    s += '@'
                elif p == True:
                    s += '#'
                else:
                    s += ' '
            s += '\n'
        return s



########NEW FILE########
__FILENAME__ = polarmaze
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import zeros
from random import choice, random

from maze import Maze


class PolarMaze(Maze):
    """ Mazes with the emphasis on Perseus: allow him to turn, go forward or backward.
    Thus there are 4 states per position.
    """

    actions = 5

    Stay = 0
    Forward = 1
    TurnAround = 2
    TurnLeft = 3
    TurnRight = 4

    allActions = [Stay, Forward, TurnAround, TurnLeft, TurnRight]

    def reset(self):
        Maze.reset(self)
        self.perseusDir = choice(range(4))

    def performAction(self, action):
        if self.stochAction > 0:
            if random() < self.stochAction:
                action = choice(range(len(PolarMaze.allActions)))
        act = PolarMaze.allActions[action]
        self.bang = False
        if act == self.Forward:
            tmp = self._moveInDir(self.perseus, Maze.allActions[self.perseusDir])
            if self.mazeTable[tmp] == False:
                self.perseus = tmp
            else:
                self.bang = True
        elif act == self.TurnLeft:
            self.perseusDir = (self.perseusDir + 1) % 4
        elif act == self.TurnRight:
            self.perseusDir = (self.perseusDir - 1) % 4
        elif act == self.TurnAround:
            self.perseusDir = (self.perseusDir + 2) % 4

    def getSensors(self):
        obs = Maze.getSensors(self)
        res = zeros(4)
        res[:4 - self.perseusDir] = obs[self.perseusDir:]
        res[4 - self.perseusDir:] = obs[:self.perseusDir]
        return res

    def __str__(self):
        return Maze.__str__(self) + '(dir:' + str(self.perseusDir) + ')'

########NEW FILE########
__FILENAME__ = cheesemaze
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import zeros, array

from maze import MazeTask


class CheeseMaze(MazeTask):
    """
    #######
    #     #
    # # # #
    # #*# #
    #######
    """

    observations = 7
    discount = 0.95

    topology = array([[1] * 7,
                      [1, 0, 1, 0, 1, 0, 1],
                      [1, 0, 1, 0, 1, 0, 1],
                      [1, 0, 0, 0, 0, 0, 1],
                      [1] * 7])
    goal = (1, 3)

    def getObservation(self):
        """ observations are encoded in a 1-n encoding of possible wall combinations. """
        res = zeros(7)
        obs = self.env.getSensors()
        if self.env.perseus == self.env.goal:
            res[6] = 1
        elif sum(obs) == 3:
            res[0] = 1
        elif sum(obs) == 1:
            res[5] = 1
        elif obs[0] == obs[1]:
            res[1] = 1
        elif obs[0] == obs[3]:
            res[2] = 1
        elif obs[0] == obs[2]:
            if obs[0] == 1:
                res[3] = 1
            else:
                res[4] = 1
        return res

########NEW FILE########
__FILENAME__ = maze
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import array

from pomdp import POMDPTask
from pybrain.rl.environments.mazes import Maze
from pybrain.rl.environments.task import Task


class MazeTask(POMDPTask):
    """ a task corresponding to a maze environment """

    bangPenalty = 0
    defaultPenalty = 0
    finalReward = 1

    topology = None
    goal = None
    initPos = None
    mazeclass = Maze

    stochObs = 0
    stochAction = 0

    @property
    def noisy(self):
        return self.stochObs > 0

    def __init__(self, **args):
        self.setArgs(**args)
        Task.__init__(self, self.mazeclass(self.topology, self.goal, initPos=self.initPos,
                                           stochObs=self.stochObs, stochAction=self.stochAction))
        self.minReward = min(self.bangPenalty, self.defaultPenalty)
        self.reset()

    def getReward(self):
        if self.env.perseus == self.env.goal:
            return self.finalReward
        elif self.env.bang:
            return self.bangPenalty
        else:
            return self.defaultPenalty

    def isFinished(self):
        return self.env.perseus == self.env.goal or POMDPTask.isFinished(self)

    def __str__(self):
        return str(self.env)


class TrivialMaze(MazeTask):
    """
    #####
    #. *#
    #####
    """
    discount = 0.8
    initPos = [(1, 1)]
    topology = array([[1] * 5,
                      [1, 0, 0, 0, 1],
                      [1] * 5, ])
    goal = (1, 3)


########NEW FILE########
__FILENAME__ = maze4x3
__author__ = 'Tom Schaul, tom@idsia.ch'

from random import random, choice
from scipy import array, zeros

from maze import MazeTask


class FourByThreeMaze(MazeTask):
    """
    ######
    #   *#
    # # -#
    #    #
    ######

    The '-' spot if absorbing, and giving negative reward.
    """

    discount = 0.95

    defaultPenalty = -0.04
    bangPenalty = -0.04
    minReward = -1

    stochAction = 0.1

    initPos = [(1, 1), (1, 2), (1, 3), (2, 1), (3, 1), (3, 2), (2, 3), (3, 3), (1, 4)]
    topology = array([[1] * 6,
                      [1, 0, 0, 0, 0, 1],
                      [1, 0, 1, 0, 0, 1],
                      [1, 0, 0, 0, 0, 1],
                      [1] * 6, ])
    goal = (3, 4)

    def reset(self):
        MazeTask.reset(self)
        self.bad = False

    def performAction(self, action):
        poss = []
        for a in range(self.actions):
            if action - a % 4 != 2:
                poss.append(a)
        if random() < self.stochAction * len(poss):
            MazeTask.performAction(self, choice(poss))
        else:
            MazeTask.performAction(self, action)

    def getReward(self):
        if self.bad:
            return self.minReward
        else:
            return MazeTask.getReward(self)

    def getObservation(self):
        """only walls on w, E, both or neither are observed. """
        res = zeros(4)
        all = self.env.getSensors()
        res[0] = all[3]
        res[1] = all[1]
        res[2] = all[3] and all[1]
        res[3] = not all[3] and not all[1]
        return res

########NEW FILE########
__FILENAME__ = maze89state
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import array

from maze import MazeTask
from pybrain.rl.environments.mazes import PolarMaze


class EightyNineStateMaze(MazeTask):
    """
    #########
    ##     ##
    #  # #  #
    ## # # ##
    #  # # *#
    ##     ##
    #########
    """

    mazeclass = PolarMaze
    topology = array([[1]*9,
                      [1, 1, 0, 0, 0, 0, 0, 1, 1],
                      [1, 0, 0, 1, 0, 1, 0, 0, 1],
                      [1, 1, 0, 1, 0, 1, 0, 1, 1],
                      [1, 0, 0, 1, 0, 1, 0, 0, 1],
                      [1, 1, 0, 0, 0, 0, 0, 1, 1],
                      [1]*9])
    goal = (2, 7)
    stochAction = 0.1
    stochObs = 0.1

########NEW FILE########
__FILENAME__ = mdp
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from pybrain.rl.environments import Task
from scipy import array

class MDPMazeTask(Task):
    """ This is a MDP task for the MazeEnvironment. The state is fully observable,
        giving the agent the current position of perseus. Reward is given on reaching
        the goal, otherwise no reward. """

    def getReward(self):
        """ compute and return the current reward (i.e. corresponding to the last action performed) """
        if self.env.goal == self.env.perseus:
            self.env.reset()
            reward = 1.
        else:
            reward = 0.
        return reward

    def performAction(self, action):
        """ The action vector is stripped and the only element is cast to integer and given
            to the super class.
        """
        Task.performAction(self, int(action[0]))


    def getObservation(self):
        """ The agent receives its position in the maze, to make this a fully observable
            MDP problem.
        """
        obs = array([self.env.perseus[0] * self.env.mazeTable.shape[0] + self.env.perseus[1]])
        return obs




########NEW FILE########
__FILENAME__ = pomdp
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import ndarray

from pybrain.rl.environments import EpisodicTask
from pybrain.utilities import Named, drawIndex


class POMDPTask(EpisodicTask, Named):
    """ Partially observable episodic MDP (with discrete actions)
    Has actions that can be performed, and observations in every state.
    By default, the observation is a vector, and the actions are integers.
    """
    # number of observations
    observations = 4

    # number of possible actions
    actions = 4

    # maximal number of steps before the episode is stopped
    maxSteps = None

    # the lower bound on the reward value
    minReward = 0

    def __init__(self, **args):
        self.setArgs(**args)
        self.steps = 0

    @property
    def indim(self):
        return self.actions

    @property
    def outdim(self):
        return self.observations

    def reset(self):
        self.steps = 0
        EpisodicTask.reset(self)

    def isFinished(self):
        if self.maxSteps != None:
            return self.steps >= self.maxSteps
        return False

    def performAction(self, action):
        """ POMDP tasks, as they have discrete actions, can me used by providing either an index,
        or an array with a 1-in-n coding (which can be stochastic). """
        if type(action) == ndarray:
            action = drawIndex(action, tolerant = True)
        self.steps += 1
        EpisodicTask.performAction(self, action)
########NEW FILE########
__FILENAME__ = shuttle
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import array, zeros
from random import random

from maze import MazeTask
from pybrain.rl.environments.mazes import PolarMaze


class ShuttleDocking(MazeTask):
    """
    #######
    #.   *#
    #######

    The spaceship needs to dock backwards into the goal station.
    """

    actions = 3
    observations = 5
    discount = 0.95

    mazeclass = PolarMaze

    finalReward = 10
    bangPenalty = -3

    initPos = [(1, 1)]
    topology = array([[1] * 7,
                      [1, 0, 0, 0, 0, 0, 1],
                      [1] * 7, ])
    goal = (1, 5)

    Backup = 0
    Forward = 1
    TurnAround = 2

    def reset(self):
        MazeTask.reset(self)
        self.env.perseusDir = 1

    def getObservation(self):
        """ inold, seeold, black, seenew, innew """
        res = zeros(5)
        if self.env.perseus == self.env.goal:
            res[4] = 1
        elif self.env.perseus == self.env.initPos[0]:
            res[0] = 1
        elif self.env.perseus[1] == 3:
            if random() > 0.7:
                res[self.env.perseusDir] = 1
            else:
                res[(self.env.perseusDir + 2) % 4] = 1
        else:
            res[(self.env.perseusDir + 2) % 4] = 1
        return res

    def performAction(self, action):
        self.steps += 1
        if action == self.TurnAround:
            self._turn()
        elif action == self.Forward:
            self._forward()
        else: # noisy backup
            r = random()
            if self.env.perseus[1] == 3:
                # in space
                if r < 0.1:
                    self._turn()
                elif r < 0.9:
                    self._backup()
            elif ((self.env.perseus[1] == 2 and self.env.perseusDir == 3) or
                  (self.env.perseus[1] == 4 and self.env.perseusDir == 1)):
                # close to station, front to station
                if r < 0.3:
                    self._turn()
                elif r < 0.6:
                    self._backup()
            else:
                # close to station, back to station
                if r < 0.7:
                    self._backup()

    def _backup(self):
        self.env.performAction(PolarMaze.TurnAround)
        self.env.performAction(PolarMaze.Forward)
        self.env.performAction(PolarMaze.TurnAround)

    def _turn(self):
        self.env.performAction(PolarMaze.TurnAround)

    def _forward(self):
        old = self.env.perseus
        self.env.performAction(PolarMaze.TurnAround)
        if self.env.perseus == self.env.goal or self.env.perseus == self.env.initPos[0]:
            self.env.perseus = old
            self.env.bang = True

########NEW FILE########
__FILENAME__ = tiger
__author__ = 'Tom Schaul, tom@idsia.ch'

from random import choice, random
from scipy import array

from pomdp import POMDPTask


class TigerTask(POMDPTask):
    """ two doors, behind one is a tiger - we can listen or open. """

    observations = 2
    actions = 3
    minReward = -100
    discount = 0.75

    # allowed actions:
    Listen = 0
    OpenLeft = 1
    OpenRight = 2

    # stochsticity on the observation
    stochObs = 0.15

    def reset(self):
        self.steps = 0
        self.cumreward = 0
        self.tigerLeft = choice([True, False])
        self.done = False
        self.nextReward = -1

    def performAction(self, action):
        self.steps += 1
        if action != self.Listen:
            self.done = True
            if ((action == self.OpenLeft and self.tigerLeft)
                or (action == self.OpenRight and not self.tigerLeft)):
                self.nextReward = -100
            else:
                self.nextReward = 10

    def getObservation(self):
        """ do we think we heard something on the left or on the right?  """
        if self.tigerLeft:
            obs = array([1, 0])
        else:
            obs = array([0, 1])
        if random() < self.stochObs:
            obs = 1 - obs
        return obs

    def isFinished(self):
        return self.done or POMDPTask.isFinished(self)

    def getReward(self):
        return self.nextReward

########NEW FILE########
__FILENAME__ = tmaze
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import array, zeros
from random import choice

from maze import MazeTask


class TMaze(MazeTask):
    """
    #############
    ###########*#
    #.          #
    ########### #
    #############

    1-in-n encoding for observations.
    """

    discount = 0.98
    observations = 4

    finalReward = 4
    bangPenalty = -0.1

    length = 10

    def __init__(self, **args):
        self.initPos = [(2, 1)]
        self.setArgs(**args)
        columns = [[1] * 5]
        for dummy in range(self.length):
            columns.append([1, 1, 0, 1, 1])
        columns.append([1, 0, 0, 0, 1])
        columns.append([1] * 5)
        self.topology = array(columns).T
        MazeTask.__init__(self, **args)

    def reset(self):
        MazeTask.reset(self)
        goUp = choice([True, False])
        self.specialObs = goUp
        if goUp:
            self.env.goal = (3, self.length + 1)
        else:
            self.env.goal = (1, self.length + 1)

    def getObservation(self):
        res = zeros(4)
        if self.env.perseus == self.env.initPos[0]:
            if self.specialObs:
                res[0] = 1
            else:
                res[1] = 1
        elif self.env.perseus[1] == self.length + 1:
            res[2] = 1
        else:
            res[3] = 1
        return res

    def getReward(self):
        if self.env.perseus[1] == self.length + 1:
            if abs(self.env.perseus[0] - self.env.goal[0]) == 2:
                # bad choice taken
                self.env.perseus = self.env.goal
                return self.bangPenalty
        return MazeTask.getReward(self)

########NEW FILE########
__FILENAME__ = actuators
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

import ode, xode #@UnresolvedImport
from pybrain.utilities import Named
import sys, warnings

class Actuator(Named):
    """The base Actuator class. Every actuator has a name, and a list of values (even if it is
        only one value) with numValues entries. They can be added to the ODEEnvironment with
        its addActuator(...) function. Actuators receive the world model when added to the world."""

    def __init__(self, name, numValues):
        self._numValues = numValues
        self.name = name
        self._world = None

    def _update(self, action):
        pass

    def _connect(self, world):
        self._world = world

    def setNumValues(self, numValues):
        self._numValues = numValues

    def getNumValues(self):
        return self._numValues



class JointActuator(Actuator):
    ''' This actuator parses the xode root node for all joints and applies a torque
        to the angles of each of them. Different joints have a different number of values (e.g.
        a hinge2 joints has two degrees of freedom, whereas a slider joint has only one).
        However, calling the function getValues(), will return a flat list of all the
        degrees of freedom of all joints.'''

    def __init__(self, name='JointActuator'):
        Actuator.__init__(self, name, 0)
        self._joints = []

    def _parseJoints(self, node):
        if isinstance(node, xode.joint.Joint):
            # append joints to joint vector
            joint = node.getODEObject()
            joint.name = node.getName()
            self._joints.append(joint)
        # recursive call for children
        for c in node.getChildren():
            self._parseJoints(c)

    def _connect(self, world):
        Actuator._connect(self, world)

        # get XODE Root and parse its joints
        self._joints = []
        self._parseJoints(self._world.getXODERoot())

        # do initial update to get numValues
        self._numValues = self._countValues()

    def _countValues(self):
        num = 0
        for j in self._joints:
            if type(j) == ode.BallJoint:
                pass
            elif type(j) == ode.UniversalJoint:
                pass
            elif type(j) == ode.AMotor:
                num += j.getNumAxes()
            elif type(j) == ode.HingeJoint:
                num += 1
            elif type(j) == ode.Hinge2Joint:
                num += 2
            elif type(j) == ode.SliderJoint:
                pass
        return num

    def _update(self, action):
        assert (len(action) == self._numValues)
        for j in self._joints:
            if type(j) == ode.BallJoint:
                # ball joints can't be controlled yet
                pass
            elif type(j) == ode.UniversalJoint:
                # universal joints can't be controlled yet (use patch from mailing list)
                pass
            elif type(j) == ode.AMotor:
                num = j.getNumAxes()
                torques = []
                for _ in range(num):
                    torques.append(action[0])
                    action = action[1:]
                for _ in range(3 - num):
                    torques.append(0)
                (t1, t2, t3) = torques
                j.addTorques(t1, t2, t3)
            elif type(j) == ode.HingeJoint:
                # hinge joints have only one axis to add torque to
                j.addTorque(action[0])
                action = action[1:]
            elif type(j) == ode.Hinge2Joint:
                # hinge2 joints can receive 2 torques for their 2 axes
                t1, t2 = action[0:2]
                action = action[2:]
                j.addTorques(t1, t2)
            elif type(j) == ode.SliderJoint:
                # slider joints are currently only used for touch sensors
                # therefore, you can (must) set a torque but it is not applied
                # to the joint.
                pass


class SpecificJointActuator(JointActuator):
    ''' This sensor takes a list of joint names, and controlls only their values. '''

    def __init__(self, jointNames, name=None):
        Actuator.__init__(self, name, 0)
        self._names = jointNames
        self._joints = []

    def _parseJoints(self, node=None):
        for name in self._names:
            try:
                self._joints.append(self._world.getXODERoot().namedChild(name).getODEObject())
            except KeyError:
                # the given object name is not found. output warning and quit.
                warnings.warn("Joint with name '", name, "' not found.")
                sys.exit()

    def _connect(self, world):
        Actuator._connect(self, world)

        # get XODE Root and parse its joints
        self._joints = []
        self._parseJoints()

        # do initial update to get numValues
        self._numValues = self._countValues()


class CopyJointActuator(JointActuator):
    ''' This sensor takes a list of joint names and controls all joints at once (one single value,
        even for multiple hinges/amotors) '''

    def __init__(self, jointNames, name=None):
        Actuator.__init__(self, name, 0)
        self._names = jointNames
        self._joints = []

    def _parseJoints(self, node=None):
        for name in self._names:
            try:
                self._joints.append(self._world.getXODERoot().namedChild(name).getODEObject())
            except KeyError:
                # the given object name is not found. output warning and quit.
                warnings.warn("Joint with name '", name, "' not found.")
                sys.exit()

    def _connect(self, world):
        Actuator._connect(self, world)

        # get XODE Root and parse its joints
        self._joints = []
        self._parseJoints()

        # pretend to have only one single value
        self._numValues = 1

    def _update(self, action):
        assert (len(action) == self._numValues)
        for j in self._joints:
            if type(j) == ode.BallJoint:
                # ball joints can't be controlled yet
                pass
            elif type(j) == ode.UniversalJoint:
                # universal joints can't be controlled yet (use patch from mailing list)
                pass
            elif type(j) == ode.AMotor:
                num = j.getNumAxes()
                torques = []
                for _ in range(num):
                    torques.append(action[0])
                for _ in range(3 - num):
                    torques.append(0)
                (t1, t2, t3) = torques
                j.addTorques(t1 * 10, t2 * 10, t3 * 10)
            elif type(j) == ode.HingeJoint:
                # hinge joints have only one axis to add torque to
                j.addTorque(action[0])
            elif type(j) == ode.Hinge2Joint:
                # hinge2 joints can receive 2 torques for their 2 axes
                t1, t2 = (action[0], action[0])
                j.addTorques(t1, t2)
            elif type(j) == ode.SliderJoint:
                # slider joints are currently only used for touch sensors
                # therefore, you can (must) set a torque but it is not applied
                # to the joint.
                pass


########NEW FILE########
__FILENAME__ = environment
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

import sys, time
from scipy import random, asarray
import xode.parser, xode.body, xode.geom #@UnresolvedImport @UnusedImport @Reimport
import ode #@UnresolvedImport

from pybrain.rl.environments.environment import Environment
from tools.configgrab import ConfigGrabber
import sensors, actuators
from pybrain.utilities import threaded
import threading
import warnings
from pybrain.tools.networking.udpconnection import UDPServer

class ODEEnvironment(Environment):
    """
    Virtual simulation for rigid bodies. Uses ODE as a physics engine and OpenGL as graphics
    interface to simulate and display arbitrary objects. Virtual worlds are defined through
    an XML file in XODE format (see http://tanksoftware.com/xode/). This file can be loaded
    into the world with the "loadXODE(...)" function. Use performAction(...) or step() to advance
    the simulation by one step.
    """
    # objects with names within one tuple can pass each other
    passpairs = []
    # define a verbosity level for selective debug output (0=none)
    verbosity = 0

    def __init__(self, render=True, realtime=True, ip="127.0.0.1", port="21590", buf='16384'):
        """ initializes the virtual world, variables, the frame rate and the callback functions."""
        print("ODEEnvironment -- based on Open Dynamics Engine.")

        # initialize base class
        self.render = render
        if self.render:
            self.updateDone = True
            self.updateLock = threading.Lock()
            self.server = UDPServer(ip, port)
        self.realtime = realtime

        # initialize attributes
        self.resetAttributes()

        # initialize the textures dictionary
        self.textures = {}

        # initialize sensor and exclude list
        self.sensors = []
        self.excludesensors = []

        #initialize actuators list
        self.actuators = []

        # A joint group for the contact joints that are generated whenever two bodies collide
        self.contactgroup = ode.JointGroup()

        self.dt = 0.01
        self.FricMu = 8.0
        self.stepsPerAction = 1
        self.stepCounter = 0

    def closeSocket(self):
        self.server.UDPInSock.close()
        time.sleep(10)

    def resetAttributes(self):
        """resets the class attributes to their default values"""
        # initialize root node
        self.root = None

        # A list with (body, geom) tuples
        self.body_geom = []

    def reset(self):
        """resets the model and all its parameters to their original values"""
        self.loadXODE(self._currentXODEfile, reload=True)
        self.stepCounter = 0

    def setGravity(self, g):
        """set the world's gravity constant in negative y-direction"""
        self.world.setGravity((0, -g, 0))


    def _setWorldParameters(self):
        """ sets parameters for ODE world object: gravity, error correction (ERP, default=0.2),
        constraint force mixing (CFM, default=1e-5).  """
        self.world.setGravity((0, -9.81, 0))
        # self.world.setERP(0.2)
        # self.world.setCFM(1e-9)

    def _create_box(self, space, density, lx, ly, lz):
        """Create a box body and its corresponding geom."""
        # Create body and mass
        body = ode.Body(self.world)
        M = ode.Mass()
        M.setBox(density, lx, ly, lz)
        body.setMass(M)
        body.name = None
        # Create a box geom for collision detection
        geom = ode.GeomBox(space, lengths=(lx, ly, lz))
        geom.setBody(body)
        geom.name = None

        return (body, geom)

    def _create_sphere(self, space, density, radius):
        """Create a sphere body and its corresponding geom."""
        # Create body and mass
        body = ode.Body(self.world)
        M = ode.Mass()
        M.setSphere(density, radius)
        body.setMass(M)
        body.name = None

        # Create a sphere geom for collision detection
        geom = ode.GeomSphere(space, radius)
        geom.setBody(body)
        geom.name = None

        return (body, geom)

    def drop_object(self):
        """Drops a random object (box, sphere) into the scene."""
        # choose between boxes and spheres
        if random.uniform() > 0.5:
            (body, geom) = self._create_sphere(self.space, 10, 0.4)
        else:
            (body, geom) = self._create_box(self.space, 10, 0.5, 0.5, 0.5)
        # randomize position slightly
        body.setPosition((random.normal(-6.5, 0.5), 6.0, random.normal(-6.5, 0.5)))
        # body.setPosition( (0.0, 3.0, 0.0) )
        # randomize orientation slightly
        #theta = random.uniform(0,2*pi)
        #ct = cos (theta)
        #st = sin (theta)
        # rotate body and append to (body,geom) tuple list
        # body.setRotation([ct, 0., -st, 0., 1., 0., st, 0., ct])
        self.body_geom.append((body, geom))

    # -- sensor and actuator functions
    def addSensor(self, sensor):
        """ adds a sensor object to the list of sensors """
        if not isinstance(sensor, sensors.Sensor):
            raise TypeError("the given sensor is not an instance of class 'Sensor'.")
        # add sensor to sensors list
        self.sensors.append(sensor)
        # connect sensor and give it the virtual world object
        sensor._connect(self)

    def addActuator(self, actuator):
        """ adds a sensor object to the list of sensors """
        if not isinstance(actuator, actuators.Actuator):
            raise TypeError("the given actuator is not an instance of class 'Actuator'.")
        # add sensor to sensors list
        self.actuators.append(actuator)
        # connect actuator and give it the virtual world object
        actuator._connect(self)

    def addTexture(self, name, texture):
        """ adds a texture to the given ODE object name """
        self.textures[name] = texture

    def centerOn(self, name):
        return
        """ if set, keeps camera to the given ODE object name. """
        try:
            self.getRenderer().setCenterObj(self.root.namedChild(name).getODEObject())
        except KeyError:
            # name not found, unset centerObj
            print("Warning: Cannot center on " + name)
            self.centerObj = None

    def loadXODE(self, filename, reload=False):
        """ loads an XODE file (xml format) and parses it. """
        f = file(filename)
        self._currentXODEfile = filename
        p = xode.parser.Parser()
        self.root = p.parseFile(f)
        f.close()
        try:
            # filter all xode "world" objects from root, take only the first one
            world = filter(lambda x: isinstance(x, xode.parser.World), self.root.getChildren())[0]
        except IndexError:
            # malicious format, no world tag found
            print("no <world> tag found in " + filename + ". quitting.")
            sys.exit()
        self.world = world.getODEObject()
        self._setWorldParameters()
        try:
            # filter all xode "space" objects from world, take only the first one
            space = filter(lambda x: isinstance(x, xode.parser.Space), world.getChildren())[0]
        except IndexError:
            # malicious format, no space tag found
            print("no <space> tag found in " + filename + ". quitting.")
            sys.exit()
        self.space = space.getODEObject()

        # load bodies and geoms for painting
        self.body_geom = []
        self._parseBodies(self.root)

        if self.verbosity > 0:
            print("-------[body/mass list]-----")
            for (body, _) in self.body_geom:
                try:
                    print(body.name, body.getMass())
                except AttributeError:
                    print("<Nobody>")

        # now parse the additional parameters at the end of the xode file
        self.loadConfig(filename, reload)


    def loadConfig(self, filename, reload=False):
        # parameters are given in (our own brand of) config-file syntax
        self.config = ConfigGrabber(filename, sectionId="<!--odeenvironment parameters", delim=("<", ">"))

        # <passpairs>
        self.passpairs = []
        for passpairstring in self.config.getValue("passpairs")[:]:
            self.passpairs.append(eval(passpairstring))
        if self.verbosity > 0:
            print("-------[pass tuples]--------")
            print(self.passpairs)
            print("----------------------------")

        # <centerOn>
        # set focus of camera to the first object specified in the section, if any
        if self.render:
            try:
                self.centerOn(self.config.getValue("centerOn")[0])
            except IndexError:
                pass

        # <affixToEnvironment>
        for jointName in self.config.getValue("affixToEnvironment")[:]:
            try:
                # find first object with that name
                obj = self.root.namedChild(jointName).getODEObject()
            except IndexError:
                print("ERROR: Could not affix object '" + jointName + "' to environment!")
                sys.exit(1)
            if isinstance(obj, ode.Joint):
                # if it is a joint, use this joint to fix to environment
                obj.attach(obj.getBody(0), ode.environment)
            elif isinstance(obj, ode.Body):
                # if it is a body, create new joint and fix body to environment
                j = ode.FixedJoint(self.world)
                j.attach(obj, ode.environment)
                j.setFixed()

        # <colors>
        for coldefstring in self.config.getValue("colors")[:]:
            # ('name', (0.3,0.4,0.5))
            objname, coldef = eval(coldefstring)
            for (body, _) in self.body_geom:
                if hasattr(body, 'name'):
                    if objname == body.name:
                        body.color = coldef
                        break


        if not reload:
            # add the JointSensor as default
            self.sensors = []
            ## self.addSensor(self._jointSensor)

            # <sensors>
            # expects a list of strings, each of which is the executable command to create a sensor object
            # example: DistToPointSensor('legSensor', (0.0, 0.0, 5.0))
            sens = self.config.getValue("sensors")[:]
            for s in sens:
                try:
                    self.addSensor(eval('sensors.' + s))
                except AttributeError:
                    print(dir(sensors))
                    warnings.warn("Sensor name with name " + s + " not found. skipped.")
        else:
            for s in self.sensors:
                s._connect(self)
            for a in self.actuators:
                a._connect(self)

    def _parseBodies(self, node):
        """ parses through the xode tree recursively and finds all bodies and geoms for drawing. """

        # body (with nested geom)
        if isinstance(node, xode.body.Body):
            body = node.getODEObject()
            body.name = node.getName()
            try:
                # filter all xode geom objects and take the first one
                xgeom = filter(lambda x: isinstance(x, xode.geom.Geom), node.getChildren())[0]
            except IndexError:
                return() # no geom object found, skip this node
            # get the real ode object
            geom = xgeom.getODEObject()
            # if geom doesn't have own name, use the name of its body
            geom.name = node.getName()
            self.body_geom.append((body, geom))

        # geom on its own without body
        elif isinstance(node, xode.geom.Geom):
            try:
                node.getFirstAncestor(ode.Body)
            except xode.node.AncestorNotFoundError:
                body = None
                geom = node.getODEObject()
                geom.name = node.getName()
                self.body_geom.append((body, geom))

        # special cases for joints: universal, fixed, amotor
        elif isinstance(node, xode.joint.Joint):
            joint = node.getODEObject()

            if type(joint) == ode.UniversalJoint:
                # insert an additional AMotor joint to read the angles from and to add torques
                # amotor = ode.AMotor(self.world)
                # amotor.attach(joint.getBody(0), joint.getBody(1))
                # amotor.setNumAxes(3)
                # amotor.setAxis(0, 0, joint.getAxis2())
                # amotor.setAxis(2, 0, joint.getAxis1())
                # amotor.setMode(ode.AMotorEuler)
                # xode_amotor = xode.joint.Joint(node.getName() + '[amotor]', node.getParent())
                # xode_amotor.setODEObject(amotor)
                # node.getParent().addChild(xode_amotor, None)
                pass
            if type(joint) == ode.AMotor:
                # do the euler angle calculations automatically (ref. ode manual)
                joint.setMode(ode.AMotorEuler)

            if type(joint) == ode.FixedJoint:
                # prevent fixed joints from bouncing to center of first body
                joint.setFixed()

        # recursive call for all child nodes
        for c in node.getChildren():
            self._parseBodies(c)

    def excludeSensors(self, exclist):
        self.excludesensors.extend(exclist)

    def getSensors(self):
        """ returns combined sensor data """
        output = []
        for s in self.sensors:
            if not s.name in self.excludesensors:
                output.extend(s.getValues())
        return asarray(output)

    def getSensorNames(self):
        return [s.name for s in self.sensors]

    def getActuatorNames(self):
        return [a.name for a in self.actuators]

    def getSensorByName(self, name):
        try:
            idx = self.getSensorNames().index(name)
        except ValueError:
            warnings.warn('sensor ' + name + ' is not in sensor list.')
            return []

        return self.sensors[idx].getValues()

    @property
    def indim(self):
        num = 0
        for a in self.actuators:
            num += a.getNumValues()
        return num

    def getActionLength(self):
        print("getActionLength() is deprecated. use property 'indim' instead.")
        return self.indim

    @property
    def outdim(self):
        return len(self.getSensors())

    def performAction(self, action):
        """ sets the values for all actuators combined. """
        pointer = 0
        for a in self.actuators:
            val = a.getNumValues()
            a._update(action[pointer:pointer + val])
            pointer += val

        for _ in range(self.stepsPerAction):
            self.step()

    def getXODERoot(self):
        return self.root


    #--- callback functions ---#
    def _near_callback(self, args, geom1, geom2):
        """Callback function for the collide() method.
        This function checks if the given geoms do collide and
        creates contact joints if they do."""

        # only check parse list, if objects have name
        if geom1.name != None and geom2.name != None:
            # Preliminary checking, only collide with certain objects
            for p in self.passpairs:
                g1 = False
                g2 = False
                for x in p:
                    g1 = g1 or (geom1.name.find(x) != -1)
                    g2 = g2 or (geom2.name.find(x) != -1)
                if g1 and g2:
                    return()

        # Check if the objects do collide
        contacts = ode.collide(geom1, geom2)

        # Create contact joints
        world, contactgroup = args
        for c in contacts:
            p = c.getContactGeomParams()
            # parameters from Niko Wolf
            c.setBounce(0.2)
            c.setBounceVel(0.05) #Set the minimum incoming velocity necessary for bounce
            c.setSoftERP(0.6) #Set the contact normal "softness" parameter
            c.setSoftCFM(0.00005) #Set the contact normal "softness" parameter
            c.setSlip1(0.02) #Set the coefficient of force-dependent-slip (FDS) for friction direction 1
            c.setSlip2(0.02) #Set the coefficient of force-dependent-slip (FDS) for friction direction 2
            c.setMu(self.FricMu) #Set the Coulomb friction coefficient
            j = ode.ContactJoint(world, contactgroup, c)
            j.name = None
            j.attach(geom1.getBody(), geom2.getBody())

    def getCurrentStep(self):
        return self.stepCounter

    @threaded()
    def updateClients(self):
        self.updateDone = False
        if not self.updateLock.acquire(False):
            return

        # build message to send
        message = []
        for (body, geom) in self.body_geom:
            item = {}
            # real bodies (boxes, spheres, ...)
            if body != None:
                # transform (rotate, translate) body accordingly
                item['position'] = body.getPosition()
                item['rotation'] = body.getRotation()
                if hasattr(body, 'color'): item['color'] = body.color

                # switch different geom objects
                if type(geom) == ode.GeomBox:
                    # cube
                    item['type'] = 'GeomBox'
                    item['scale'] = geom.getLengths()
                elif type(geom) == ode.GeomSphere:
                    # sphere
                    item['type'] = 'GeomSphere'
                    item['radius'] = geom.getRadius()

                elif type(geom) == ode.GeomCCylinder:
                    # capped cylinder
                    item['type'] = 'GeomCCylinder'
                    item['radius'] = geom.getParams()[0]
                    item['length'] = geom.getParams()[1] - 2 * item['radius']

                elif type(geom) == ode.GeomCylinder:
                    # solid cylinder
                    item['type'] = 'GeomCylinder'
                    item['radius'] = geom.getParams()[0]
                    item['length'] = geom.getParams()[1]
                else:
                    # TODO: add other geoms here
                    pass

            else:
                # no body found, then it must be a plane (we only draw planes)
                if type(geom) == ode.GeomPlane:
                    # plane
                    item['type'] = 'GeomPlane'
                    item['normal'] = geom.getParams()[0] # the normal vector to the plane
                    item['distance'] = geom.getParams()[1] # the distance to the origin

            message.append(item)

        # Listen for clients
        self.server.listen()
        if self.server.clients > 0:
            # If there are clients send them the new data
            self.server.send(message)
        time.sleep(0.02)
        self.updateLock.release()
        self.updateDone = True

    def step(self):
        """ Here the ode physics is calculated by one step. """

        # call additional callback functions for all kinds of tasks (e.g. printing)
        self._printfunc()

        # Detect collisions and create contact joints
        self.space.collide((self.world, self.contactgroup), self._near_callback)

        # Simulation step
        self.world.step(float(self.dt))
        # Remove all contact joints
        self.contactgroup.empty()

        # update all sensors
        for s in self.sensors:
            s._update()

        # update clients
        if self.render and self.updateDone:
            self.updateClients()
            if self.server.clients > 0 and self.realtime:
                time.sleep(self.dt)

        # increase step counter
        self.stepCounter += 1
        return self.stepCounter

    def _printfunc (self):
        pass
        # print(self.root.namedChild('palm').getODEObject().getPosition())

    def specialkeyfunc(self, c, x, y):
        """Derived classes can implement extra functionality here"""
        pass


    #--- helper functions ---#
    def _print_help(self):
        """ prints out the keyboard shortcuts. """
        print("v   -> toggle view with mouse on/off")
        print("s   -> toggle screen capture on/off")
        print("d   -> drop an object")
        print("f   -> lift all objects")
        print("m   -> toggle mouse view (press button to zoom)")
        print("r   -> random torque at all joints")
        print("a/z -> negative/positive torque to all joints")
        print("g   -> print current state")
        print("n   -> reset environment")
        self.specialfunctionDoc()
        print("x,q -> exit program")

    def specialfunctionDoc(self):
        """Derived classes can implement extra functionality here"""
        pass


#--- main function, if called directly ---

if __name__ == '__main__' :
    """
    little example on how to use the virtual world.
    Synopsis: python environment.py [modelname]
    Parameters: modelname = base name of the xode file to use (default: johnnie)
    """

    print("ODEEnvironment -- test program")
    if len(sys.argv) > 1:
        modelName = sys.argv[1]
    else:
        modelName = "johnnie"

    # initialize world and renderer and attach renderer to world
    w = ODEEnvironment()
    # load model file
    w.loadXODE("models/" + modelName + ".xode")             # load XML file that describes the world

    w.addSensor(sensors.JointSensor())
    w.addActuator(actuators.JointActuator())

    # start simulating the world
    while True:
        w.step()


########NEW FILE########
__FILENAME__ = acrobot
__author__ = 'Frank Sehnke, sehnke@in.tum.de'

from pybrain.rl.environments.ode import ODEEnvironment, sensors, actuators
import imp
from scipy import array

class AcrobotEnvironment(ODEEnvironment):
    def __init__(self, renderer=True, realtime=True, ip="127.0.0.1", port="21590", buf='16384'):
        ODEEnvironment.__init__(self, renderer, realtime, ip, port, buf)
        # load model file
        self.loadXODE(imp.find_module('pybrain')[1] + "/rl/environments/ode/models/acrobot.xode")

        # standard sensors and actuators
        self.addSensor(sensors.JointSensor())
        self.addSensor(sensors.JointVelocitySensor())
        self.addActuator(actuators.JointActuator())

        #set act- and obsLength, the min/max angles and the relative max touques of the joints
        self.actLen = self.indim
        self.obsLen = len(self.getSensors())

        self.stepsPerAction = 1

if __name__ == '__main__' :
    w = AcrobotEnvironment()
    while True:
        w.step()
        if w.stepCounter == 1000: w.reset()


########NEW FILE########
__FILENAME__ = ccrl
__author__ = 'Frank Sehnke, sehnke@in.tum.de'

from pybrain.rl.environments.ode import ODEEnvironment, sensors, actuators
import imp
import xode #@UnresolvedImport
import ode #@UnresolvedImport
import sys
from scipy import array, asarray

class CCRLEnvironment(ODEEnvironment):
    def __init__(self, xodeFile="ccrlGlas.xode", renderer=True, realtime=False, ip="127.0.0.1", port="21590", buf='16384'):
        ODEEnvironment.__init__(self, renderer, realtime, ip, port, buf)
        # load model file
        self.pert = asarray([0.0, 0.0, 0.0])
        self.loadXODE(imp.find_module('pybrain')[1] + "/rl/environments/ode/models/" + xodeFile)

        # standard sensors and actuators
        self.addSensor(sensors.JointSensor())
        self.addSensor(sensors.JointVelocitySensor())
        self.addActuator(actuators.JointActuator())

        #set act- and obsLength, the min/max angles and the relative max touques of the joints
        self.actLen = self.indim
        self.obsLen = len(self.getSensors())
        #ArmLeft, ArmRight, Hip, PevelLeft, PevelRight, TibiaLeft, TibiaRight, KneeLeft, KneeRight, FootLeft, FootRight
        self.tourqueList = array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.8, 0.8, 0.8, 0.5, 0.5, 0.1],)
        #self.tourqueList=array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],)
        self.cHighList = array([0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.9],)
        self.cLowList = array([-1.0, -1.0, -1.0, -1.5, -1.0, -1.0, -1.0, -0.7, -1.0, 0.0, -1.0, -1.5, -1.0, -1.0, -1.0, 0.0],)
        self.stepsPerAction = 1

    def step(self):
        # Detect collisions and create contact joints
        self.tableSum = 0
        self.glasSum = 0
        ODEEnvironment.step(self)

    def _near_callback(self, args, geom1, geom2):
        """Callback function for the collide() method.
        This function checks if the given geoms do collide and
        creates contact joints if they do."""

        # only check parse list, if objects have name
        if geom1.name != None and geom2.name != None:
            # Preliminary checking, only collide with certain objects
            for p in self.passpairs:
                g1 = False
                g2 = False
                for x in p:
                    g1 = g1 or (geom1.name.find(x) != -1)
                    g2 = g2 or (geom2.name.find(x) != -1)
                if g1 and g2:
                    return()

        # Check if the objects do collide
        contacts = ode.collide(geom1, geom2)
        tmpStr = geom2.name[:-2]
        handStr = geom1.name[:-1]
        if geom1.name == 'plate' and tmpStr != 'objectP':
            self.tableSum += len(contacts)
        if tmpStr == 'objectP' and handStr == 'pressLeft':
            if len(contacts) > 0: self.glasSum += 1
        tmpStr = geom1.name[:-2]
        handStr = geom2.name[:-1]
        if geom2.name == 'plate' and tmpStr != 'objectP':
            self.tableSum += len(contacts)
        if tmpStr == 'objectP' and handStr == 'pressLeft':
            if len(contacts) > 0: self.glasSum += 1

        # Create contact joints
        world, contactgroup = args
        for c in contacts:
            p = c.getContactGeomParams()
            # parameters from Niko Wolf
            c.setBounce(0.2)
            c.setBounceVel(0.05) #Set the minimum incoming velocity necessary for bounce
            c.setSoftERP(0.6) #Set the contact normal "softness" parameter
            c.setSoftCFM(0.00005) #Set the contact normal "softness" parameter
            c.setSlip1(0.02) #Set the coefficient of force-dependent-slip (FDS) for friction direction 1
            c.setSlip2(0.02) #Set the coefficient of force-dependent-slip (FDS) for friction direction 2
            c.setMu(self.FricMu) #Set the Coulomb friction coefficient
            j = ode.ContactJoint(world, contactgroup, c)
            j.name = None
            j.attach(geom1.getBody(), geom2.getBody())

    def loadXODE(self, filename, reload=False):
        """ loads an XODE file (xml format) and parses it. """
        f = file(filename)
        self._currentXODEfile = filename
        p = xode.parser.Parser()
        self.root = p.parseFile(f)
        f.close()
        try:
            # filter all xode "world" objects from root, take only the first one
            world = filter(lambda x: isinstance(x, xode.parser.World), self.root.getChildren())[0]
        except IndexError:
            # malicious format, no world tag found
            print("no <world> tag found in " + filename + ". quitting.")
            sys.exit()
        self.world = world.getODEObject()
        self._setWorldParameters()
        try:
            # filter all xode "space" objects from world, take only the first one
            space = filter(lambda x: isinstance(x, xode.parser.Space), world.getChildren())[0]
        except IndexError:
            # malicious format, no space tag found
            print("no <space> tag found in " + filename + ". quitting.")
            sys.exit()
        self.space = space.getODEObject()

        # load bodies and geoms for painting
        self.body_geom = []
        self._parseBodies(self.root)

        for (body, _) in self.body_geom:
            if hasattr(body, 'name'):
                tmpStr = body.name[:-2]
                if tmpStr == "objectP":
                    body.setPosition(body.getPosition() + self.pert)

        if self.verbosity > 0:
            print("-------[body/mass list]-----")
            for (body, _) in self.body_geom:
                try:
                    print(body.name, body.getMass())
                except AttributeError:
                    print("<Nobody>")

        # now parse the additional parameters at the end of the xode file
        self.loadConfig(filename, reload)

    def reset(self):
        ODEEnvironment.reset(self)
        self.pert = asarray([1.5, 0.0, 1.0])

if __name__ == '__main__' :
    w = CCRLEnvironment()
    while True:
        w.step()
        if w.stepCounter == 1000: w.reset()


########NEW FILE########
__FILENAME__ = johnnie
__author__ = 'Frank Sehnke, sehnke@in.tum.de'

from pybrain.rl.environments.ode import ODEEnvironment, sensors, actuators
import imp
from scipy import array

class JohnnieEnvironment(ODEEnvironment):
    def __init__(self, renderer=True, realtime=False, ip="127.0.0.1", port="21590", buf='16384'):
        ODEEnvironment.__init__(self, renderer, realtime, ip, port, buf)
        # load model file
        self.loadXODE(imp.find_module('pybrain')[1] + "/rl/environments/ode/models/johnnie.xode")

        # standard sensors and actuators
        self.addSensor(sensors.JointSensor())
        self.addSensor(sensors.JointVelocitySensor())
        self.addActuator(actuators.JointActuator())

        #set act- and obsLength, the min/max angles and the relative max touques of the joints
        self.actLen = self.indim
        self.obsLen = len(self.getSensors())
        #ArmLeft, ArmRight, Hip, PevelLeft, PevelRight, TibiaLeft, TibiaRight, KneeLeft, KneeRight, FootLeft, FootRight
        self.tourqueList = array([0.2, 0.2, 0.2, 0.5, 0.5, 2.0, 2.0, 2.0, 2.0, 0.5, 0.5],)
        self.cHighList = array([1.0, 1.0, 0.5, 0.5, 0.5, 1.5, 1.5, 1.5, 1.5, 0.25, 0.25],)
        self.cLowList = array([-0.5, -0.5, -0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.25, -0.25],)

        self.stepsPerAction = 1

if __name__ == '__main__' :
    w = JohnnieEnvironment()
    while True:
        w.step()
        if w.stepCounter == 1000: w.reset()


########NEW FILE########
__FILENAME__ = sensors
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

import ode, sys, xode #@UnresolvedImport
import warnings
from scipy.linalg import norm
from pybrain.utilities import Named

class SizeError(Exception):
    def __init__(self, value):
        self.value = value

    def __str__(self):
        return 'size not correct: ', repr(self.value)


class Sensor(Named):
    """The base Sensor class. Every sensor has a name, and a list of values (even if it is
        only one value) with numValues entries. They can be added to the ODEEnvironment with
        its addSensor(...) function. Sensors get the world model when added to the world.
        Other information sources have to be taken manually by parsing the world object."""

    def __init__(self, name, numValues):
        self._numValues = numValues
        self.name = name
        self._values = [0] * self._numValues
        self._world = None

    def _update(self):
        pass

    def _connect(self, world):
        self._world = world

    def setNumValues(self, numValues):
        self._numValues = numValues

    def getNumValues(self):
        return self._numValues

    def setValues(self, values):
        if (len(values) != self._numValues):
            raise SizeError(self._numValues)
        self._values = values[:]

    def getValues(self):
        return self._values



class JointSensor(Sensor):
    ''' This sensor parses the xode root node for all joints and returns the angles
        for each of them. Different joints have a different number of values (e.g.
        a hinge2 joints has two degrees of freedom, whereas a slider joint has only one).
        However, calling the function getValues(), will return a flat list of all the
        degrees of freedom of all joints.'''

    def __init__(self, name='JointSensor'):
        Sensor.__init__(self, name, 0)
        self._joints = []

    def _parseJoints(self, node):
        if isinstance(node, xode.joint.Joint):
            # append joints to joint vector
            joint = node.getODEObject()
            joint.name = node.getName()
            self._joints.append(joint)
        # recursive call for children
        for c in node.getChildren():
            self._parseJoints(c)

    def _connect(self, world):
        Sensor._connect(self, world)

        # get XODE Root and parse its joints
        self._joints = []
        self._parseJoints(self._world.getXODERoot())

        # do initial update to get numValues
        self._update()
        self._numValues = len(self._values)

    def _update(self):
        self._values = []
        for j in self._joints:
            if type(j) == ode.BallJoint:
                # ball joints can't be controlled yet
                pass
            elif type(j) == ode.UniversalJoint:
                # universal joints not implemented (are covered by using AMotors instead
                pass
            elif type(j) == ode.AMotor:
                num = j.getNumAxes()
                for i in range(num):
                    self._values.append(j.getAngle(i))
            elif type(j) == ode.HingeJoint:
                # a hinge joint has only one angle
                self._values.append(j.getAngle())
            elif type(j) == ode.Hinge2Joint:
                # for hinge2 joints, we need to values
                self._values.append(j.getAngle1())
                self._values.append(j.getAngle2())
            elif type(j) == ode.SliderJoint:
                # slider joints have one value (the relative distance)
                self._values.append(j.getPosition())

    def getJoints(self):
        return self._joints


class JointVelocitySensor(JointSensor):

    def __init__(self, name='JointVelocitySensor'):
        Sensor.__init__(self, name, 0)
        self._joints = []

    def _update(self):
        self._values = []
        for j in self._joints:
            if type(j) == ode.BallJoint:
                # ball joints can't be controlled yet
                pass
            elif type(j) == ode.UniversalJoint:
                # universal joints not implemented (are covered by using AMotors instead
                pass
            elif type(j) == ode.AMotor:
                num = j.getNumAxes()
                for i in range(num):
                    self._values.append(j.getAngleRate(i))
            elif type(j) == ode.HingeJoint:
                # a hinge joint has only one angle
                self._values.append(j.getAngleRate())
            elif type(j) == ode.Hinge2Joint:
                # for hinge2 joints, we need to values
                self._values.append(j.getAngle1Rate())
                self._values.append(j.getAngle2Rate())
            elif type(j) == ode.SliderJoint:
                # slider joints have one value (the relative distance)
                self._values.append(j.getPositionRate())


class DistToPointSensor(Sensor):
    ''' This sensor takes a name (of a body) and possibly a point and returns
        the current distance of the body to this point. if no point is given,
        the distance to the origin is returned. '''

    def __init__(self, bodyName, name='DistToPointSensor', point=(0, 0, 0)):
        Sensor.__init__(self, name, 0)
        # initialize one return value
        self.setNumValues(1)
        self._values = [0]
        self._bodyName = bodyName
        self._point = point

    def _update(self):
        try:
            odeObj = self._world.getXODERoot().namedChild(self._bodyName).getODEObject()
        except KeyError:
            # the given object name is not found. output warning and return distance 0.
            warnings.warn("Object with name '", self._bodyName, "' not found.")
            return 0
        self.setValues([norm(odeObj.getPosRelPoint(self._point))])

class BodyDistanceSensor(Sensor):
    ''' This sensor takes two body names and returns the current distance between them. '''

    def __init__(self, bodyName1, bodyName2, name='BodyDistanceSensor'):
        Sensor.__init__(self, name, 0)
        # initialize one return value
        self.setNumValues(1)
        self._values = [0]
        self._bodyName1 = bodyName1
        self._bodyName2 = bodyName2

    def _update(self):
        try:
            odeObj1 = self._world.getXODERoot().namedChild(self._bodyName1).getODEObject()
            odeObj2 = self._world.getXODERoot().namedChild(self._bodyName2).getODEObject()
        except KeyError:
            # the given object name is not found. output warning and return distance 0.
            warnings.warn("One of the objects '", self._bodyName1, self._bodyName2, "' was not found.")
            return 0
        self.setValues([norm(odeObj1.getPosRelPoint(odeObj2.getPosition()))])


class BodyPositionSensor(Sensor):
    ''' This sensor parses the xode root node for all bodies and returns the positions in x, y, z
        for each of them. Calling the function getValues(), will return a flat list of all the
        bodies' coordinates.'''

    def __init__(self, name='BodyPositionSensor'):
        Sensor.__init__(self, name, 0)
        self._bodies = []

    def _parseBodies(self, node):
        if isinstance(node, xode.body.Body):
            # append bodies to body vector
            body = node.getODEObject()
            body.name = node.getName()
            self._bodies.append(body)
        # recursive call for children
        for c in node.getChildren():
            self._parseBodies(c)

    def _connect(self, world):
        Sensor._connect(self, world)

        # get XODE Root and parse its joints
        self._bodies = []
        self._parseBodies(self._world.getXODERoot())

        # do initial update to get numValues
        self._update()
        self._numValues = len(self._values)

    def _update(self):
        self._values = []
        for b in self._bodies:
            self._values.extend(b.getPosition())

    def getBodies(self):
        return self._bodies


class SpecificJointSensor(JointSensor):
    ''' This sensor takes a list of joint names, and returns only their values. '''

    def __init__(self, jointNames, name=None):
        Sensor.__init__(self, name, 0)
        self._names = jointNames
        self._joints = []
        self._values = []

    def _parseJoints(self, node=None):
        for name in self._names:
            try:
                self._joints.append(self._world.getXODERoot().namedChild(name).getODEObject())
            except KeyError:
                # the given object name is not found. output warning and quit.
                warnings.warn("Joint with name '", name, "' not found.")
                sys.exit()

    def _connect(self, world):
        """ Connects the sensor to the world and initializes the value list. """
        Sensor._connect(self, world)

        # initialize object list - this should not change during runtime
        self._joints = []
        self._parseJoints()

        # do initial update to get numValues
        self._update()
        self._numValues = len(self._values)


class SpecificJointVelocitySensor(JointVelocitySensor):
    ''' This sensor takes a list of joint names, and returns only their velocities. '''

    def __init__(self, jointNames, name=None):
        Sensor.__init__(self, name, 0)
        self._names = jointNames
        self._joints = []
        self._values = []

    def _parseJoints(self, node=None):
        for name in self._names:
            try:
                self._joints.append(self._world.getXODERoot().namedChild(name).getODEObject())
            except KeyError:
                # the given object name is not found. output warning and quit.
                warnings.warn("Joint with name '", name, "' not found.")
                sys.exit()

    def _connect(self, world):
        """ Connects the sensor to the world and initializes the value list. """
        Sensor._connect(self, world)

        # initialize object list - this should not change during runtime
        self._joints = []
        self._parseJoints()

        # do initial update to get numValues
        self._update()
        self._numValues = len(self._values)


class SpecificBodyPositionSensor(BodyPositionSensor):
    ''' This sensor takes a list of body names, and returns their positions. It must
        be given a custom name as well, for later identification.'''

    def __init__(self, bodyNames, name=None):
        Sensor.__init__(self, name, 0)
        self._names = bodyNames
        self._bodies = []
        self._values = [0]

    def _parseBodies(self, node=None):
        for name in self._names:
            try:
                self._bodies.append(self._world.getXODERoot().namedChild(name).getODEObject())
            except KeyError:
                # the given object name is not found. output warning and quit.
                warnings.warn("Body with name '", name, "' not found.")
                sys.exit()

    def _connect(self, world):
        """ Connects the sensor to the world and initializes the value list. """
        Sensor._connect(self, world)

        # initialize object list - this should not change during runtime
        self._bodies = []
        self._parseBodies()

        # do initial update to get numValues
        self._update()
        self._numValues = len(self._values)



########NEW FILE########
__FILENAME__ = acrobot
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from pybrain.rl.environments import EpisodicTask
from scipy import pi

class GradualRewardTask(EpisodicTask):
    ''' task gives more reward, the higher the bar is.'''
    def __init__(self, environment):
        EpisodicTask.__init__(self, environment)
        self.maxPower = 0.5
        self.reward_history = []
        self.count = 0
        # normalize to (-1, 1)
        self.sensor_limits = [(-pi, pi), (-20, 20)]
        #self.actor_limits = [(-1, 1)]
        self.actor_limits = None

    def isFinished(self):
        if self.count > 1000:
            self.count = 0
            self.reward_history.append(self.getTotalReward())
            return True
        else:
            self.count += 1
            return False

    def getReward(self):
        # calculate reward and return reward
        jointSense = self.env.getSensorByName('JointSensor')
        veloSense = self.env.getSensorByName('JointVelocitySensor')

        j = jointSense[0]
        v = veloSense[0]

        reward = (abs(j)) ** 2 - 0.2 * abs(v)
        # time.sleep(0.001)
        return reward

    def performAction(self, action):
        EpisodicTask.performAction(self, action*self.maxPower)

########NEW FILE########
__FILENAME__ = ccrl
__author__ = 'Frank Sehnke, sehnke@in.tum.de'

from pybrain.rl.environments import EpisodicTask
from pybrain.rl.environments.ode.sensors import SpecificBodyPositionSensor
from scipy import tanh, zeros, array, random, sqrt, asarray


#Basic class for all ccrl tasks
class CCRLTask(EpisodicTask):
    def __init__(self, env):
        EpisodicTask.__init__(self, env)
        #Overall maximal tourque - is multiplied with relative max tourque for individual joint.
        self.maxPower = 100.0
        self.reward_history = []
        self.count = 0 #timestep counter
        self.epiLen = 1500 #suggestet episodic length for normal Johnnie tasks
        self.incLearn = 0 #counts the task resets for incrementall learning
        self.env.FricMu = 20.0 #We need higher friction for CCRL
        self.env.dt = 0.002 #We also need more timly resolution

        # normalize standard sensors to (-1, 1)
        self.sensor_limits = []
        #Angle sensors
        for i in range(self.env.actLen):
            self.sensor_limits.append((self.env.cLowList[i], self.env.cHighList[i]))
        # Joint velocity sensors
        for i in range(self.env.actLen):
            self.sensor_limits.append((-20, 20))
        #Norm all actor dimensions to (-1, 1)
        self.actor_limits = [(-1, 1)] * env.actLen
        self.oldAction = zeros(env.actLen, float)
        self.dist = zeros(9, float)
        self.dif = array([0.0, 0.0, 0.0])
        self.target = array([-6.5, 1.75, -10.5])
        self.grepRew = 0.0
        self.tableFlag = 0.0
        self.env.addSensor(SpecificBodyPositionSensor(['objectP00'], "glasPos"))
        self.env.addSensor(SpecificBodyPositionSensor(['palmLeft'], "palmPos"))
        self.env.addSensor(SpecificBodyPositionSensor(['fingerLeft1'], "finger1Pos"))
        self.env.addSensor(SpecificBodyPositionSensor(['fingerLeft2'], "finger2Pos"))
        #we changed sensors so we need to update environments sensorLength variable
        self.env.obsLen = len(self.env.getSensors())
        #normalization for the task spezific sensors
        for i in range(self.env.obsLen - 2 * self.env.actLen):
            self.sensor_limits.append((-4, 4))
        self.actor_limits = None

    def getObservation(self):
        """ a filtered mapping to getSample of the underlying environment. """
        sensors = self.env.getSensors()
        #Sensor hand to target object
        for i in range(3):
            self.dist[i] = ((sensors[self.env.obsLen - 9 + i] + sensors[self.env.obsLen - 6 + i] + sensors[self.env.obsLen - 3 + i]) / 3.0 - (sensors[self.env.obsLen - 12 + i] + self.dif[i])) * 4.0 #sensors[self.env.obsLen-12+i]
        #Sensor hand angle to horizontal plane X-Axis
        for i in range(3):
            self.dist[i + 3] = (sensors[self.env.obsLen - 3 + i] - sensors[self.env.obsLen - 6 + i]) * 5.0
        #Sensor hand angle to horizontal plane Y-Axis
        for i in range(3):
            self.dist[i + 6] = ((sensors[self.env.obsLen - 3 + i] + sensors[self.env.obsLen - 6 + i]) / 2.0 - sensors[self.env.obsLen - 9 + i]) * 10.0
        if self.sensor_limits:
            sensors = self.normalize(sensors)
        sens = []
        for i in range(self.env.obsLen - 12):
            sens.append(sensors[i])
        for i in range(9):
            sens.append(self.dist[i])
        for i in self.oldAction:
            sens.append(i)
        return sens

    def performAction(self, action):
        #Filtered mapping towards performAction of the underlying environment
        #The standard CCRL task uses a PID controller to controll directly angles instead of forces
        #This makes most tasks much simpler to learn
        self.oldAction = action
        #Grasping as reflex depending on the distance to target - comment in for more easy grasping
        if abs(abs(self.dist[:3]).sum())<2.0: action[15]=1.0 #self.grepRew=action[15]*.01
        else: action[15]=-1.0 #self.grepRew=action[15]*-.03
        isJoints=array(self.env.getSensorByName('JointSensor')) #The joint angles
        isSpeeds=array(self.env.getSensorByName('JointVelocitySensor')) #The joint angular velocitys
        act=(action+1.0)/2.0*(self.env.cHighList-self.env.cLowList)+self.env.cLowList #norm output to action intervall
        action=tanh((act-isJoints-0.9*isSpeeds*self.env.tourqueList)*16.0)*self.maxPower*self.env.tourqueList #simple PID
        EpisodicTask.performAction(self, action)
        #self.env.performAction(action)

    def isFinished(self):
        #returns true if episode timesteps has reached episode length and resets the task
        if self.count > self.epiLen:
            self.res()
            return True
        else:
            self.count += 1
            return False

    def res(self):
        #sets counter and history back, increases incremental counter
        self.count = 0
        self.incLearn += 1
        self.reward_history.append(self.getTotalReward())
        self.tableFlag = 0.0

    def getReward(self):
        #rewarded for approaching the object
        dis = sqrt((self.dist[0:3] ** 2).sum())
        return (25.0 - dis) / float(self.epiLen) - float(self.env.tableSum) * 0.1

#Learn to grasp a glas at a fixed location
class CCRLGlasTask(CCRLTask):
    def __init__(self, env):
        CCRLTask.__init__(self, env)
        self.dif = array([0.0, 0.0, 0.0])
        self.epiLen = 1000 #suggestet episodic length for normal Johnnie tasks

    def isFinished(self):
        #returns true if episode timesteps has reached episode length and resets the task
        if self.count > self.epiLen:
            self.res()
            return True
        else:
            self.count += 1
            return False

    def getReward(self):
        if self.env.glasSum >= 2: grip = 1000.0
        else: grip = 0.0
        if self.env.tableSum > 0: self.tableFlag = -1.0
        else: tableFlag = 0.0
        self.dist[3] = 0.0
        self.dist[8] = 0.0
        dis = 100.0/((self.dist[:3] ** 2).sum()+0.1)
        nig = 10.0/((self.dist[3:] ** 2).sum()+0.1)
        if self.env.stepCounter == self.epiLen: print("Grip:", grip, "Dis:", dis, "Nig:", nig, "Table:", self.tableFlag)
        return (10 + grip + nig + dis + self.tableFlag) / float(self.epiLen) #-dis
        #else:
        #    return (25.0 - dis) / float(self.epiLen) + (grip / nig - float(self.env.tableSum)) * 0.1 #+self.grepRew (10.0-dis)/float(self.epiLen)+

#Learn to grasp a plate at a fixed location
class CCRLPlateTask(CCRLTask):
    def __init__(self, env):
        CCRLTask.__init__(self, env)
        self.dif = array([0.0, 0.2, 0.8])
        self.epiLen = 1000 #suggestet episodic length for normal Johnnie tasks

    def isFinished(self):
        #returns true if episode timesteps has reached episode length and resets the task
        if self.count > self.epiLen:
            self.res()
            return True
        else:
            if self.count == 1: self.pertGlasPos(0)
            self.count += 1
            return False

    def pertGlasPos(self, num):
        if num == 0: self.env.pert = asarray([0.0, 0.0, 0.5])

    def getReward(self):
        if self.env.glasSum >= 2: grip = 1.0
        else: grip = 0.0
        if self.env.tableSum > 0: self.tableFlag = 10.0
        #self.dist[4]=0.0
        #self.dist[8]=0.0
        dis = sqrt((self.dist[0:3] ** 2).sum())
        if self.count == self.epiLen:
            return 25.0 + grip - dis - self.tableFlag #/nig
        else:
            return (25.0 - dis) / float(self.epiLen) + (grip - float(self.env.tableSum)) * 0.1 #/nig -(1.0+self.oldAction[15])

#Learn to grasp a glas at 5 different locations
class CCRLGlasVarTask(CCRLGlasTask):
    def __init__(self, env):
        CCRLGlasTask.__init__(self, env)
        self.epiLen = 5000 #suggestet episodic length for normal Johnnie tasks

    def isFinished(self):
        #returns true if episode timesteps has reached episode length and resets the task
        if self.count > self.epiLen:
            self.res()
            return True
        else:
            if self.count == 1:
                self.pertGlasPos(0)
            if self.count == self.epiLen / 5 + 1:
                self.env.reset()
                self.pertGlasPos(1)
            if self.count == 2 * self.epiLen / 5 + 1:
                self.env.reset()
                self.pertGlasPos(2)
            if self.count == 3 * self.epiLen / 5 + 1:
                self.env.reset()
                self.pertGlasPos(3)
            if self.count == 4 * self.epiLen / 5 + 1:
                self.env.reset()
                self.pertGlasPos(4)
            self.count += 1
            return False

    def pertGlasPos(self, num):
        if num == 0: self.env.pert = asarray([1.0, 0.0, 0.5])
        if num == 1: self.env.pert = asarray([-1.0, 0.0, 0.5])
        if num == 2: self.env.pert = asarray([1.0, 0.0, 0.0])
        if num == 3: self.env.pert = asarray([-1.0, 0.0, 0.0])
        if num == 4: self.env.pert = asarray([0.0, 0.0, 0.25])

    def getReward(self):
        if self.env.glasSum >= 2: grip = 1.0
        else: grip = 0.0
        if self.env.tableSum > 0: self.tableFlag = 10.0
        self.dist[3] = 0.0
        self.dist[8] = 0.0
        dis = sqrt((self.dist ** 2).sum())
        nig = (abs(self.dist[4]) + 1.0)
        if self.count == self.epiLen or self.count == self.epiLen / 5 or self.count == 2 * self.epiLen / 5 or self.count == 3 * self.epiLen / 5 or self.count == 4 * self.epiLen / 5:
            return 25.0 + grip / nig - dis - self.tableFlag #/nig
        else:
            return (25.0 - dis) / float(self.epiLen) + (grip / nig - float(self.env.tableSum)) * 0.1 #/nig

#Learn to grasp a glas at random locations
class CCRLGlasVarRandTask(CCRLGlasVarTask):
    def pertGlasPos(self, num):
        self.env.pert = asarray([random.random()*2.0 - 1.0, 0.0, random.random()*0.5 + 0.5])


#Some experimental stuff
class CCRLPointTask(CCRLGlasVarTask):
    def __init__(self, env):
        CCRLGlasVarTask.__init__(self, env)
        self.epiLen = 1000 #suggestet episodic length for normal Johnnie tasks

    def isFinished(self):
        #returns true if episode timesteps has reached episode length and resets the task
        if self.count > self.epiLen:
            self.res()
            return True
        else:
            if self.count == 1:
                self.pertGlasPos(0)
            self.count += 1
            return False

    def getObservation(self):
        """ a filtered mapping to getSample of the underlying environment. """
        sensors = self.env.getSensors()
        sensSort = []
        #Angle and angleVelocity
        for i in range(32):
            sensSort.append(sensors[i])
        #Angles wanted (old action)
        for i in self.oldAction:
            sensSort.append(i)
        #Hand position
        for i in range(3):
            sensSort.append((sensors[38 + i] + sensors[41 + i]) / 2)
        #Hand orientation (Hack - make correkt!!!!)
        sensSort.append((sensors[38] - sensors[41]) / 2 - sensors[35]) #pitch
        sensSort.append((sensors[38 + 1] - sensors[41 + 1]) / 2 - sensors[35 + 1]) #yaw
        sensSort.append((sensors[38 + 1] - sensors[41 + 1])) #roll
        #Target position
        for i in range(3):
            sensSort.append(self.target[i])
        #Target orientation
        for i in range(3):
            sensSort.append(0.0)
        #Object type (start with random)
        sensSort.append(float(random.randint(-1, 1))) #roll

        #normalisation
        if self.sensor_limits:
            sensors = self.normalize(sensors)
        sens = []
        for i in range(32):
            sens.append(sensors[i])
        for i in range(29):
            sens.append(sensSort[i + 32])

        #calc dist to target
        self.dist = array([(sens[54] - sens[48]), (sens[55] - sens[49]), (sens[56] - sens[50]), sens[51], sens[52], sens[53], sens[15]])
        return sens

    def pertGlasPos(self, num):
        if num == 0: self.target = asarray([0.0, 0.0, 1.0])
        self.env.pert = self.target.copy()
        self.target = self.target.copy() + array([-6.5, 1.75, -10.5])

    def getReward(self):
        dis = sqrt((self.dist ** 2).sum())
        return (25.0 - dis) / float(self.epiLen) - float(self.env.tableSum) * 0.1

class CCRLPointVarTask(CCRLPointTask):
    def __init__(self, env):
        CCRLPointTask.__init__(self, env)
        self.epiLen = 2000 #suggestet episodic length for normal Johnnie tasks

    def isFinished(self):
        #returns true if episode timesteps has reached episode length and resets the task
        if self.count > self.epiLen:
            self.res()
            return True
        else:
            if self.count == 1:
                self.pertGlasPos(0)
            if self.count == self.epiLen / 2 + 1:
                self.env.reset()
                self.pertGlasPos(1)
            self.count += 1
            return False

    def getObservation(self):
        """ a filtered mapping to getSample of the underlying environment. """
        sensors = self.env.getSensors()
        sensSort = []
        #Angle and angleVelocity
        for i in range(32):
            sensSort.append(sensors[i])
        #Angles wanted (old action)
        for i in self.oldAction:
            sensSort.append(i)
        #Hand position
        for i in range(3):
            sensSort.append((sensors[38 + i] + sensors[41 + i]) / 2)
        #Hand orientation (Hack - make correkt!!!!)
        sensSort.append((sensors[38] - sensors[41]) / 2 - sensors[35]) #pitch
        sensSort.append((sensors[38 + 1] - sensors[41 + 1]) / 2 - sensors[35 + 1]) #yaw
        sensSort.append((sensors[38 + 1] - sensors[41 + 1])) #roll
        #Target position
        for i in range(3):
            sensSort.append(self.target[i])
        #Target orientation
        for i in range(3):
            sensSort.append(0.0)
        #Object type (start with random)
        sensSort.append(float(random.randint(-1, 1))) #roll

        #normalisation
        if self.sensor_limits:
            sensors = self.normalize(sensors)
        sens = []
        for i in range(32):
            sens.append(sensors[i])
        for i in range(29):
            sens.append(sensSort[i + 32])

        #calc dist to target
        self.dist = array([(sens[54] - sens[48]) * 10.0, (sens[55] - sens[49]) * 10.0, (sens[56] - sens[50]) * 10.0, sens[51], sens[52], sens[53], 1.0 + sens[15]])
        return sens

    def pertGlasPos(self, num):
        if num == 0: self.target = asarray([1.0, 0.0, 1.0])
        if num == 1: self.target = asarray([-1.0, 0.0, 1.0])
        if num == 2: self.target = asarray([1.0, 0.0, 0.0])
        if num == 3: self.target = asarray([-1.0, 0.0, 0.0])
        if num == 4: self.target = asarray([0.0, 0.0, 0.5])
        self.env.pert = self.target.copy()
        self.target = self.target.copy() + array([-6.5, 1.75, -10.5])

    def getReward(self):
        dis = sqrt((self.dist ** 2).sum())
        subEpi = self.epiLen / 2
        if self.count == self.epiLen or self.count == subEpi:
            return (25.0 - dis) / 2.0
        else:
            return (25.0 - dis) / float(self.epiLen) - float(self.env.tableSum) * 0.1


########NEW FILE########
__FILENAME__ = johnnie
__author__ = 'Frank Sehnke, sehnke@in.tum.de'

from pybrain.rl.environments import EpisodicTask
from pybrain.rl.environments.ode.sensors import * #@UnusedWildImport
from scipy import  ones, tanh, clip

#Basic class for all Johnnie tasks
class JohnnieTask(EpisodicTask):
    def __init__(self, env):
        EpisodicTask.__init__(self, env)
        self.maxPower = 100.0 #Overall maximal tourque - is multiplied with relative max tourque for individual joint to get individual max tourque
        self.reward_history = []
        self.count = 0 #timestep counter
        self.epiLen = 500 #suggestet episodic length for normal Johnnie tasks
        self.incLearn = 0 #counts the task resets for incrementall learning
        self.env.FricMu = 20.0 #We need higher friction for Johnnie
        self.env.dt = 0.01 #We also need more timly resolution

        # normalize standard sensors to (-1, 1)
        self.sensor_limits = []
        #Angle sensors
        for i in range(self.env.actLen):
            self.sensor_limits.append((self.env.cLowList[i], self.env.cHighList[i]))
        # Joint velocity sensors
        for i in range(self.env.actLen):
            self.sensor_limits.append((-20, 20))
        #Norm all actor dimensions to (-1, 1)
        #self.actor_limits = [(-1, 1)] * env.actLen
        self.actor_limits = None

    def performAction(self, action):
        #Filtered mapping towards performAction of the underlying environment
        #The standard Johnnie task uses a PID controller to controll directly angles instead of forces
        #This makes most tasks much simpler to learn
        isJoints=self.env.getSensorByName('JointSensor') #The joint angles
        isSpeeds=self.env.getSensorByName('JointVelocitySensor') #The joint angular velocitys
        act=(action+1.0)/2.0*(self.env.cHighList-self.env.cLowList)+self.env.cLowList #norm output to action intervall
        action=tanh((act-isJoints-isSpeeds)*16.0)*self.maxPower*self.env.tourqueList #simple PID
        EpisodicTask.performAction(self, action)
        #self.env.performAction(action)

    def isFinished(self):
        #returns true if episode timesteps has reached episode length and resets the task
        if self.count > self.epiLen:
            self.res()
            return True
        else:
            self.count += 1
            return False

    def res(self):
        #sets counter and history back, increases incremental counter
        self.count = 0
        self.incLearn += 1
        self.reward_history.append(self.getTotalReward())

#The standing tasks, just not falling on its own is the goal
class StandingTask(JohnnieTask):
    def __init__(self, env):
        JohnnieTask.__init__(self, env)
        #add task spezific sensors, TODO build attitude sensors
        self.env.addSensor(SpecificBodyPositionSensor(['footLeft'], "footLPos"))
        self.env.addSensor(SpecificBodyPositionSensor(['footRight'], "footRPos"))
        self.env.addSensor(SpecificBodyPositionSensor(['palm'], "bodyPos"))
        self.env.addSensor(SpecificBodyPositionSensor(['head'], "headPos"))

        #we changed sensors so we need to update environments sensorLength variable
        self.env.obsLen = len(self.env.getSensors())

        #normalization for the task spezific sensors
        for _ in range(self.env.obsLen - 2 * self.env.actLen):
            self.sensor_limits.append((-20, 20))
        self.epiLen = 1000 #suggested episode length for this task

    def getReward(self):
        # calculate reward and return reward
        reward = self.env.getSensorByName('headPos')[1] / float(self.epiLen) #reward is hight of head
        #to prevent jumping reward can't get bigger than head position while standing absolut upright
        reward = clip(reward, -14.0, 4.0)
        return reward

#Robust standing task suited for complete learning with already standable controller
class RStandingTask(StandingTask):
    def __init__(self, env):
        StandingTask.__init__(self, env)
        self.epiLen = 4000 #suggested episode length for this task
        self.h1 = self.epiLen / 4 #timestep of first perturbation
        self.h2 = self.epiLen / 2 #timestep of environment reset
        self.h3 = 3 * self.epiLen / 4 #timestep of second perturbation
        self.pVect1 = (0, -9.81, -9.81) #gravity vector for first perturbation
        self.pVect2 = (0, -9.81, 0) #gravity vector standard
        self.pVect3 = (0, -9.81, 9.81) #gravity vector for second perturbation

    def isFinished(self):
        if self.count > self.epiLen:
            self.res()
            return True
        else:
            self.count += 1
            self.disturb()
            return False

    #changes gravity vector for perturbation
    def disturb(self):
        disturb = self.getDisturb()
        if self.count == self.h1: self.env.world.setGravity(self.pVect1)
        if self.count == self.h1 + disturb: self.env.world.setGravity(self.pVect2)
        if self.count == self.h2: self.env.reset()
        if self.count == self.h3: self.env.world.setGravity(self.pVect3)
        if self.count == self.h3 + disturb: self.env.world.setGravity(self.pVect2)

    def getDisturb(self):
        return 50

#Robust standing task suited for incremental learning with already standable controller
class RobStandingTask(RStandingTask):
    #increases the amount of perturbation with the number of episodes
    def getDisturb(self):
        return clip((10 + self.incLearn / 50), 0.0, 50)

#Robust standing task suited for complete learning with an empty controller
class RobustStandingTask(RobStandingTask):
    #increases the amount of perturbation with the number of episodes
    def getDisturb(self):
        return clip((self.incLearn / 200), 0.0, 50)

#The jumping tasks, goal is to maximize the highest point the head reaches during episode
class JumpingTask(JohnnieTask):
    def __init__(self, env):
        JohnnieTask.__init__(self, env)
        #add task spezific sensors, TODO build attitude sensors
        self.env.addSensor(SpecificBodyPositionSensor(['footLeft']))
        self.env.addSensor(SpecificBodyPositionSensor(['footRight']))
        self.env.addSensor(SpecificBodyPositionSensor(['palm']))
        self.env.addSensor(SpecificBodyPositionSensor(['head']))

        #we changed sensors so we need to update environments sensorLength variable
        self.env.obsLen = len(self.env.getSensors())

        #normalization for the task spezific sensors
        for _ in range(self.env.obsLen - 2 * self.env.actLen):
            self.sensor_limits.append((-20, 20))
        self.epiLen = 400 #suggested episode length for this task
        self.maxHight = 4.0 #maximum hight reached during episode
        self.maxPower = 400.0 #jumping needs more power

    def getReward(self):
        # calculate reward and return reward
        reward = self.env.getSensorByName('SpecificBodyPositionSensor8')[1] #reward is hight of head
        if reward > self.maxHight:
            self.maxHight = reward
        if self.count == self.epiLen:
            reward = self.maxHight
        else:
            reward = 0.0
        return reward

    def res(self):
        self.count = 0
        self.reward_history.append(self.getTotalReward())
        self.maxHight = 4.0

#The standing up from prone task, goal is to stand up from prone in an upright position.
#Nearly unsolveable task - most learners achive to bring Johnnie in some kind of kneeling position
class StandingUpTask(StandingTask):
    def __init__(self, env):
        StandingTask.__init__(self, env)
        self.epiLen = 2000 #suggested episode length for this task
        self.env.tourqueList[0] = 2.5
        self.env.tourqueList[1] = 2.5

    def getReward(self):
        # calculate reward and return reward
        if self.count < 800:
            return 0.0
        else:
            reward = self.env.getSensorByName('SpecificBodyPositionSensor8')[1] / float(self.epiLen - 800) #reward is hight of head
            #to prevent jumping reward can't get bigger than head position while standing absolut upright
            reward = clip(reward, -14.0, 4.0)
            return reward

    def performAction(self, action):
        if self.count < 800:
            #provoke falling
            a = ones(self.env.actLen, int) * self.maxPower * self.env.tourqueList * -1
            StandingTask.performAction(self, a)
        else:
            StandingTask.performAction(self, action)


########NEW FILE########
__FILENAME__ = configgrab
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

import string

class ConfigGrabber:
    def __init__(self, filename, sectionId="", delim=("[", "]")):
        # filename:  name of the config file to be parsed
        # sectionId: start looking for parameters only after this string has
        #            been encountered in the file
        # delim:     tuple of delimiters to identify tags
        self.filename = filename
        self.sectionId = string.strip(sectionId)
        self.delim = delim

    def getValue(self, name):
        file = open(self.filename, "r")
        flag = -1
        output = []
        # ignore sectionId if not set
        if self.sectionId == "":
            self.sectionId = string.strip(file.readline())
            file.seek(0)

        for line in file:
            if flag == -1 and string.strip(line) == self.sectionId:
                flag = 0
            if flag > -1:
                if line[0] != self.delim[0]:
                    if flag == 1: output.append(string.strip(line))
                else:
                    if line == self.delim[0] + name + self.delim[1] + "\n": flag = 1
                    else: flag = 0
        file.close()
        #if len(output)==0: print("Attention: Config for ", name, "not found")
        return output


########NEW FILE########
__FILENAME__ = mathhelpers
# TODO - redundant file, and functionality, as we use scipy!

import math

def dotproduct(p, q):
	return p[0] * q[0] + p[1] * q[1] + p[2] * q[2]

def norm(p):
	sum = 0
	for i in p:
		sum += i ** 2
	return math.sqrt(sum)

def crossproduct(p, q):
	return (p[1] * q[2] - p[2] * q[1],
		    p[2] * q[0] - p[0] * q[2],
		    p[0] * q[1] - p[1] * q[0])


########NEW FILE########
__FILENAME__ = xmltools
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

import sys

class XMLstruct:
    """
    Defines an XML tag structure. Tags are added at the current level
    using the insert() method, with up() providing a way to get to the
    parent tag.

    $Id:xmltools.py 150 2007-04-11 13:42:47Z ruecksti $
    """

    _tab = "\t"

    def __init__(self, name, attr=None):
        """create a new tag at the topmost level using given name
        and (optional) attribute dictionary"""
        # XML tag structure is a dictionary containing all attributes plus
        # two special tags:
        #   myName = name of the tag
        #   Icontain = list of XML tags this one encloses
        self.tag = {}
        self.tag['myName'] = name
        if attr is not None:
            self.tag.update(attr)

        # to traverse the XML hierarchy, store the current tag and
        # the previously visited ones. Start at the top level.
        self.top()

    def __iter__(self):
        """returns the top-level list of tags (including internal tags 'Icontain' and 'myName')"""
        return self.tag


    def insert(self, name, attr=None):
        """Insert a new tag into the current one. The name can be either the
        new tag name or an XMLstruct object (in which case attr is ignored).
        Unless name is None, we descend into the new tag as a side effect.
        A dictionary is expected for attr."""
        if not self.current.hasSubtag():
            self.current.tag['Icontain'] = []
        if name == None:
            # empty subtag list inserted, return now
            # (this produces <tag></tag> in the output)
            return
        elif type(name) == str:
            # create a new subtag with given name and attributes
            newtag = XMLstruct(name, attr)
        else:
            # input assumed to be a tag structure
            newtag = name
        self.current.tag['Icontain'].append(newtag)
        self.stack.append(self.current)
        self.current = newtag


    def insertMulti(self, attrlist):
        """Inserts multiple subtags at once. A list of XMLstruct objects
        must be given; the tag hierarchy is not descended into."""
        if not self.current.hasSubtag():
            self.current.tag['Icontain'] = []
        self.current.tag['Icontain'] += attrlist


    def downTo(self, name, stack=None, current=None):
        """Traverse downward from current tag, until given named tag is found. Returns
        true if found and sets stack and current tag correspondingly."""
        if stack is None:
            stack = self.stack
            current = self.current
        if self.name == name:
            return(True)
        else:
            if not self.hasSubtag():
                return(False)
            else:
                # descend one level
                stack.append(self)
                found = self.getSubtag(name)
                if found is None:
                    # no subtag of correct name found; recursively check whether
                    # any of the subtags contain the looked for tag
                    for subtag in self.tag['Icontain']:
                        found = subtag.downTo(name, stack, current)
                        if found:
                            # everything was updated recursively, need only return
                            return(True)
                    # nothing found, revert stack and return
                    stack.pop()
                    return(False)

                else:
                    current.setCurrent(found)
                    return(True)


    def up(self, steps=1):
        """traverse upward a number of steps in tag stack"""
        for _ in range(steps):
            if self.stack != []:
                self.current = self.stack.pop()

    def top(self):
        """traverse upward to root level"""
        self.stack = []
        self.current = self

    def setCurrent(self, tag):
        self.current = tag

    def getName(self):
        """return tag's name"""
        return(self.tag['myName'])

    def getCurrentSubtags(self):
        if self.current.hasSubtag():
            return(self.current.tag['Icontain'])
        else:
            return([])

    def hasSubtag(self, name=None):
        """determine whether current tag contains other tags, and returns
        the tag with a matching name (if name is given) or True (if not)"""
        if self.tag.has_key('Icontain'):
            if name is None:
                return(True)
            else:
                for subtag in self.tag['Icontain']:
                    if subtag.name == name: return(True)
        return(False)


    def getSubtag(self, name=None):
        """determine whether current tag contains other tags, and returns
        the tag with a matching name (if name is given) or None (if not)"""
        if self.tag.has_key('Icontain'):
            for subtag in self.tag['Icontain']:
                if subtag.name == name: return(subtag)
        return(None)

    def nbAttributes(self):
        """return number of user attributes the current tag has"""
        nAttr = len(self.tag.keys()) - 1
        if self.hasSubtag():
            nAttr -= 1
        return nAttr


    def scale(self, sc, scaleset=set([]), exclude=set([])):
        """for all tags not in the exclude set, scale all attributes whose names are in scaleset by the given factor"""
        if self.name not in exclude:
            for name, val in self.tag.iteritems():
                if name in scaleset:
                    self.tag[name] = val * sc
        if self.hasSubtag():
            for subtag in self.tag['Icontain']:
                subtag.scale(sc, scaleset, exclude)


    def write(self, file, depth=0):
        """parse XML structure recursively and append to the output fileID,
        increasing the offset (tabs) while descending into the tree"""
        if not self.tag.has_key('myName'):
            print("Error parsing XML structure: Tag name missing!")
            sys.exit(1)
        # find number of attributes (disregarding special keys)
        nAttr = self.nbAttributes()
        endmark = '/>'
        if self.hasSubtag():
            endmark = '>'
        # print(start tag, with attributes if present)
        if nAttr > 0:
            file.write(self._tab * depth + "<" + self.tag['myName'] + " " + \
                ' '.join([name + '="' + str(val) + '"' for name, val in self.tag.iteritems() \
                if name != 'myName' and name != 'Icontain']) + endmark + '\n')
        else:
            file.write(self._tab * depth + "<" + self.tag['myName'] + ">\n")
        # print(enclosed tags, if any)
        if self.hasSubtag():
            for subtag in self.tag['Icontain']:
                subtag.write(file, depth=depth + 1)
            # finalize tag
            file.write(self._tab * depth + "</" + self.tag['myName'] + ">\n")



########NEW FILE########
__FILENAME__ = xodetools
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

import sys
from xmltools import XMLstruct
from math import asin, cos, sin, pi, degrees, radians, pow
from scipy import array, matrix, sqrt
import random

class XODEfile(XMLstruct):
    """
    Creates a (virtual) XODE file, into which bodies, joints and custom
    parameters can be inserted. This file can be merged at a defined level
    with other instances of itself, and written to disk in the
    standard format.

    $Id:xodetools.py 150 2007-04-11 13:42:47Z ruecksti $
    """

    def __init__(self, name, **kwargs):
        """initialize the XODE structure with a name and the world and
        space tags"""
        self._xodename = name
        self._centerOn = None
        self._affixToEnvironment = None
        # sensors is a list of ['type', [args], {kwargs}]
        self.sensors = []
        # sensor elements is a list of joints to be used as pressure sensors
        self.sensorElements = []
        self._nSensorElements = 0
        self._pass = {}     # dict of sets containing objects allowed to pass
        self._colors = []   # list of tuples ('name', (r,g,b))
        XMLstruct.__init__(self, 'world')
        self.insert('space')
        # TODO: insert palm, support, etc. (derived class)

    def _mass2dens(self, shape, size, mass):
        """converts a mass into a density"""
        if shape == 'box':
            return mass / float(size[0] * size[1] * size[2])
        elif shape == 'cylinder' or shape == 'cappedCylinder':
            if shape == 'cylinder':
                return mass / (12.56637061 * size[0] * size[0] * size[1])
            else:
                return mass / (size[0] * size[0] * (12.56637061 * (size[1] - 2 * size[0]) + 4.18879020 * size[0]))
        elif shape == 'sphere':
            return mass / (4.18879020 * pow(size[0], 3))
        else:
            print("Unknown shape: " + shape + " not implemented!")
            sys.exit(1)

    def _dens2mass(self, shape, size, dens):
        """converts a density into a mass"""
        if shape == 'box':
            return dens * float(size[0] * size[1] * size[2])
        elif shape == 'cylinder' or shape == 'cappedCylinder':
            if shape == 'cylinder':
                return dens * (12.56637061 * size[0] * size[0] * size[1])
            else:
                return dens * (size[0] * size[0] * (12.56637061 * (size[1] - 2 * size[0]) + 4.18879020 * size[0]))
        elif shape == 'sphere':
            return dens * (4.18879020 * pow(size[0], 3))
        else:
            print("Unknown shape: " + shape + " not implemented!")
            sys.exit(1)

            
    def insertBody(self, bname, shape, size, density, pos=[0, 0, 0], passSet=None, euler=None, mass=None, color=None):
        """Inserts a body with the given custom name and one of the standard
        shapes. The size and pos parameters are given as xyz-lists or tuples.
        euler are three rotation angles (degrees), 
        if mass is given, density is calculated automatically"""
        self.insert('body', {'name': bname})
        if color is not None:
            self._colors.append((bname, color))
        self.insert('transform')
        self.insert('position', {'x':pos[0], 'y':pos[1], 'z':pos[2]})
        if euler is not None:
            self.up()
            self.insert('rotation')
            self.insert('euler', {'x':euler[0], 'y':euler[1], 'z':euler[2], 'aformat':'degrees'})
            self.up()
        self.up(2)
        self.insert('mass')
        if shape == 'box':
            dims = {'sizex':size[0], 'sizey':size[1], 'sizez':size[2]}
        elif shape == 'cylinder' or shape == 'cappedCylinder':
            dims = {'radius':size[0], 'length':size[1]}
        elif shape == 'sphere':
            dims = {'radius':size[0]}
        else:
            print("Unknown shape: " + shape + " not implemented!")
            sys.exit(1)
        if mass is not None:
            density = self._mass2dens(shape, size, mass)
            
        self.insert('mass_shape', {'density': density})
        self.insert(shape, dims)
        self.up(3)
        self.insert('geom')
        self.insert(shape, dims)
        self.up(3)
        # add the body to a matching pass set
        if passSet is not None:
            for pset in passSet:
                try:
                    self._pass[pset].add(bname)
                except KeyError:
                    self._pass[pset] = set([bname])



    def insertJoint(self, body1, body2, type, axis=None, anchor=(0, 0, 0), rel=False, name=None):
        """Inserts a joint of given type linking the two bodies. Default name is
        a "_"-concatenation of the body names. The anchor is a xyz-tuple, rel is 
        a boolean specifying whether the anchor coordinates refer to the body's origin, 
        axis parameters have to be provided as a dictionary."""
        if name is None: name = body1 + "_" + body2
        if rel:
            abs = 'false'
        else:
            abs = 'true'
            
        self.insert('joint', {'name': name })
        self.insert('link1', {'body': body1})
        self.up()
        self.insert('link2', {'body': body2})
        self.up()
        self.insert(type)
        if type == 'fixed':
            self.insert(None)  # empty subtag, seems to be needed by xode parser
        elif type == 'ball':
            self.insert('anchor', {'x':anchor[0], 'y':anchor[1], 'z':anchor[2], 'absolute':abs})
            self.up()
        elif type == 'slider':
            self.insert('axis', axis)
            self.up()
        elif type == 'hinge':
            self.insert('axis', axis)
            self.up()
            self.insert('anchor', {'x':anchor[0], 'y':anchor[1], 'z':anchor[2], 'absolute':abs})
            self.up()
        else:
            print("Sorry, joint type " + type + " not yet implemented!")
            sys.exit()
        self.up(2)
        return name


    def insertFloor(self, y= -0.5):
        """inserts a bodiless floor at given y offset"""
        self.insert('geom', {'name': 'floor'})
        self.insert('plane', {'a': 0, 'b': 1, 'c': 0, 'd': y})
        self.up(2)

    def insertPressureSensorElement(self, parent, name=None, shape='cappedCylinder', size=[0.16, 0.5], pos=[0, 0, 0], euler=[0, 0, 0], dens=1, \
                             mass=None, passSet=[], stiff=10.0):
        """Insert one single pressure sensor element of the given shape, size, density, etc.
        The sliding axis is by default oriented along the z-axis, which is also the default for cylinder shapes.
        You have to rotate the sensor into the correct orientation - the sliding axis will be rotated accordingly.
        Stiffness of the sensor's spring is set via stiff, whereby damping is calculated automatically to prevent
        oscillations."""
        
        if name is None: 
            name = 'psens' + str(self._nSensorElements)
        if mass is None:
            mass = self._dens2mass(shape, size, dens)
        else:
            dens = self._mass2dens(shape, size, mass)
            
        self._nSensorElements += 1        
        h = 0.02  # temporal stepwidth

        self.insertBody(name, shape, size, dens, pos=pos, euler=euler, passSet=passSet)

        # In the aperiodic limit case, we have 
        # kp = a kd^2 / 4m     i.e.    kd = 2 sqrt(m kp/a)
        # where kp is the spring constant, kd is the dampening constant, and m is the mass of the oscillator.
        # For practical purposes, it is often better if kp is a few percent stronger such that the sensor
        # resets itself faster (but still does not oscillate a lot), thus a~=1.02.
        # ERP = h kp / (h kp + kd)
        # CFM = 1 / (h kp + kd) = ERP / h kp
        # For assumed mass of finger of 0.5kg, kp=10, kd=4.5 is approx. the non-oscillatory case.
        kd = 2.0 * sqrt(mass * stiff / 1.02) 
        ERP = h * stiff / (h * stiff + kd)
        CFM = ERP / (h * stiff)
        
        # Furthermore, compute the sliding axis direction from the Euler angles (x-convention, see
        # http://mathworld.wolfram.com/EulerAngles.html): Without rotation, the axis is along
        # the z axis, just like a cylinder's axis
        w = array(euler) * pi / 180
        #A  = matrix([[cos(w[0]), sin(w[0]), 0], [-sin(w[0]),cos(w[0]),0],[0,0,1]])
        #A = matrix([[1,0,0], [0,cos(w[1]), sin(w[1])], [0, -sin(w[1]),cos(w[1])]]) * A
        #A = matrix([[cos(w[2]), sin(w[2]), 0], [-sin(w[2]),cos(w[2]),0],[0,0,1]]) * A
        # hmmm, it seems XODE is rather using the y-convention here:
        A = matrix([[-sin(w[0]), cos(w[0]), 0], [-cos(w[0]), -sin(w[0]), 0], [0, 0, 1]])
        A = matrix([[1, 0, 0], [0, cos(w[1]), sin(w[1])], [0, -sin(w[1]), cos(w[1])]]) * A
        A = matrix([[sin(w[2]), -cos(w[2]), 0], [cos(w[2]), sin(w[2]), 0], [0, 0, 1]]) * A
        ax = ((A * matrix([0, 0, 1]).getT()).flatten().tolist())[0]
        
        jname = self.insertJoint(name, parent, 'slider', \
            axis={'x':ax[0], 'y':ax[1], 'z':ax[2], "HiStop":0.0, "LowStop":0.0, "StopERP":ERP, "StopCFM":CFM })
        self.sensorElements.append(jname)
        return name
        
    def attachSensor(self, type, *args, **kwargs):
        """adds a sensor with the given type, arguments and keywords, see sensors module for details"""
        self.sensors.append([type, args, kwargs])
        
        
    def merge(self, xodefile, joinLevel='space'):
        """Merge a second XODE file into this one, at the specified
        level (which must exist in both files). The passpair lists are
        also joined. Upon return, the current tag for both objects
        is the one given."""
        self.top()
        if not self.downTo(joinLevel):
            print("Error: Cannot merge " + self.name + " at level " + joinLevel)
        xodefile.top()
        if not xodefile.downTo(joinLevel):
            print("Error: Cannot merge " + xodefile.name + " at level " + joinLevel)
        self.insertMulti(xodefile.getCurrentSubtags())
        self._pass.update(xodefile.getPassList())

    def centerOn(self, name):
        self._centerOn = name


    def affixToEnvironment(self, name):
        self._affixToEnvironment = name

    def getPassList(self):
        return(self._pass)

    def scaleModel(self, sc):
        """scales all spatial dimensions by the given factor
        FIXME: quaternions may cause problems, which are currently ignored"""
        # scale these attributes...
        scaleset = set(['x', 'y', 'z', 'a', 'b', 'c', 'd', 'sizex', 'sizey', 'sizez', 'length', 'radius']) 
        # ... unless contained in these tags (in which case they specify angles)
        exclude = set(['euler', 'finiteRotation', 'axisangle'])
        self.scale(sc, scaleset, exclude)
        
        
    def writeCustomParameters(self, f):
        """writes our custom parameters into an XML comment"""
        f.write('<!--odeenvironment parameters\n')
        if len(self._pass) > 0:
            f.write('<passpairs>\n')
            for pset in self._pass.itervalues():
                f.write(str(tuple(pset)) + '\n')
        if self._centerOn is not None:
            f.write('<centerOn>\n')
            f.write(self._centerOn + '\n')
        if self._affixToEnvironment is not None:
            f.write('<affixToEnvironment>\n')
            f.write(self._affixToEnvironment + '\n')
        if self._nSensorElements > 0:
            f.write('<sensors>\n')
            f.write("SpecificJointSensor(" + str(self.sensorElements) + ",name='PressureElements')\n")
        # compile all sensor commands
        for sensor in self.sensors:
            outstr = sensor[0] + "("
            for val in sensor[1]:
                outstr += ',' + repr(val)
            for key, val in sensor[2].iteritems():
                outstr += ',' + key + '=' + repr(val)
            outstr = outstr.replace('(,', '(') + ")\n"
            f.write(outstr)
        if self._colors:
            f.write('<colors>\n')
            for col in self._colors:
                f.write(str(col) + "\n")

        f.write('<end>\n')
        f.write('-->\n')


    def writeXODE(self, filename=None):
        """writes the created structure (plus header and footer) to file with
        the given basename (.xode is appended)"""
        if filename is None: filename = self._xodename
        f = file(filename + '.xode', 'wb')  # <-- wb here ensures Linux compatibility
        f.write('<?xml version="1.0" encoding="UTF-8"?>\n')
        f.write('<xode version="1.0r23" name="' + self._xodename + '"\n')
        f.write('xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="http://tanksoftware.com/xode/1.0r23/xode.xsd">\n\n')
        self.write(f)
        f.write('</xode>\n')
        self.writeCustomParameters(f)
        f.close()
        print("Wrote " + filename + '.xode')

class XODEfinger(XODEfile):

    def __init__(self, name, **kwargs):
        """Creates one finger on a fixed palm, and adds some sensors"""
        XODEfile.__init__(self, name, **kwargs)
        # create the hand and finger
        self.insertBody('palm', 'box', [10, 2, 10], 5, pos=[3.75, 4, 0], passSet=['pal'])
        self.insertBody('sample', 'box', [10, 0.2, 40], 5, pos=[0, 0.4, 0], passSet=['sam'])
        self.insertJoint('palm', 'sample', 'fixed', name='palm_support')
        self.insertJoint('palm', 'sample', 'slider', axis={'x':0, 'y':0, 'z':1, 'FMax':11000.0, "HiStop":100.0, "LowStop":-100.0})
        self.insertBody('finger1_link0', 'cappedCylinder', [1, 7.5], 5, pos=[0, 4, 8.75], passSet=['pal', 'f1'])
        self.insertBody('finger1_link1', 'cappedCylinder', [1, 4], 5, pos=[0, 4, 14.5], passSet=['f1', 'f2'])
        self.insertBody('fingertip', 'cappedCylinder', [1, 2.9], 5, pos=[0, 4, 17.95], passSet=['f2', 'haptic'])
        self.insertJoint('palm', 'finger1_link0', 'hinge', \
            axis={'x':-1, 'y':0, 'z':0, "HiStop":8, "LowStop":0.0}, anchor=(0, 4, 5))
        self.insertJoint('finger1_link0', 'finger1_link1', 'hinge', \
            axis={'x':-1, 'y':0, 'z':0, "HiStop":15, "LowStop":0.0}, anchor=(0, 4, 12.5))
        self.insertJoint('finger1_link1', 'fingertip', 'hinge', \
            axis={'x':-1, 'y':0, 'z':0, "HiStop":15, "LowStop":0.0}, anchor=(0, 4, 16.5))
        self.centerOn('fingertip')
        self.affixToEnvironment('palm_support')
        self.insertFloor()
        # add one group of haptic sensors
        self._nSensorElements = 0
        self.sensorElements = []
        self.sensorGroupName = None
        self.insertHapticSensors()
        # give some structure to the sample
        self.insertSampleStructure(**kwargs)

    def insertHapticSensorsRandom(self):
        """insert haptic sensors at random locations"""
        self.sensorGroupName = 'haptic'
        for _ in range(5):
            self.insertHapticSensor(dx=random.uniform(-0.65, 0.65), dz=random.uniform(-0.4, 0.2))
        ##self.insertHapticSensor(dx=-0.055)


    def insertHapticSensors(self):
        """insert haptic sensors at predetermined locations
        (check using testhapticsensorslocations.py)"""
        self.sensorGroupName = 'haptic'
        x = [0.28484253596392306, -0.59653176701550947, -0.36877718203650889, 0.50549219349016294, -0.22467390532644882, 0.051978612692656596, -0.18287341960589126, 0.40477910340060383, 0.56041266484490182, -0.47806390012776134]
        z = [-0.20354546253333045, -0.23178541627964597, 0.04632154813480549, -0.27525024891443889, -0.20352571063065863, -0.07930554411063101, 0.025260779785407084, 0.091906227805625964, -0.031751424859005839, -0.0034220681106161277]
        nSens = len(x)
        for _ in range(nSens):
            self.insertHapticSensor(dx=x.pop(), dz=z.pop())


    def insertSampleStructure(self, angle=None):
        pass

    def getSensors(self):
        """return the list of haptic sensors defined"""
        return(self.sensorElements)

    def insertHapticSensor(self, ctr=(0, 4, 18.5), dx=0.0, dz=0.0):
        """insert one single haptic sensor"""
        name = 'haptic' + str(self._nSensorElements)
        self._nSensorElements += 1
        # centers of haptic sensors lie on a cylinder of fixed radius (slightly bigger than fingertip)
        R = 1.2
        alpha = asin(dx / R)
        dy = -R * cos(alpha)
        pos = [dx, dy, dz]
        rot = [90, degrees(alpha), 0]
        # ERP = h kp / (h kp + kd)
        # CFM = 1 / (h kp + kd) = ERP / h kp
        _h = 0.01  # temporal stepwidth
        # CHECKME: unused!
        # for assumed mass of finger of 0.5kg, kp=10, kd=4.5 is approx. the non-oscillatory case
        self.insertBody(name, 'cappedCylinder', [0.08, 0.5], 7, pos=[ctr[0] + pos[0], ctr[1] + pos[1], ctr[2] + pos[2]], \
            euler=rot, passSet=['haptic'])
        jname = 'finger1_' + name
        self.insertJoint('fingertip', name, 'slider', name=jname, \
            axis={'x':-dx, 'y':-dy, 'z':0, "HiStop":0.0, "LowStop":0.0, "StopERP":0.022, "StopCFM":0.22 })
        self.sensorElements.append(jname)

class XODEhand(XODEfile):

    def __init__(self, name, **kwargs):
        """Creates hand with fingertip and palm sensors -- palm up"""
        XODEfile.__init__(self, name, **kwargs)
        # create the hand and finger
        self.insertBody('palm', 'box', [10, 2, 10], 30, pos=[0, 0, 0], passSet=['pal'])
        self.insertBody('pressure', 'box', [8, 0.5, 8], 30, pos=[0, 1, 0], passSet=['pal'])
        self.insertBody('finger0_link0', 'cappedCylinder', [1, 7.5], 5, pos=[-8.75, 0, -2.5], euler=[0, 90, 0], passSet=['pal', 'f01'])
        self.insertBody('finger0_link1', 'cappedCylinder', [1, 4], 5, pos=[-14.5, 0, -2.5], euler=[0, 90, 0], passSet=['f01', 'f02'])
        self.insertBody('finger0_link2', 'cappedCylinder', [1, 2.9], 5, pos=[-17.95, 0, -2.5], euler=[0, 90, 0], passSet=['f02', 'f03'])
        self.insertBody('finger0_link3', 'sphere', [1], 5, pos=[-19, 0, -2.5], passSet=['f03'])
        self.insertBody('finger1_link0', 'cappedCylinder', [1, 7.5], 5, pos=[-3.75, 0, 8.75], passSet=['pal', 'f11'])
        self.insertBody('finger1_link1', 'cappedCylinder', [1, 4], 5, pos=[-3.75, 0, 14.5], passSet=['f11', 'f12'])
        self.insertBody('finger1_link2', 'cappedCylinder', [1, 2.9], 5, pos=[-3.75, 0, 17.95], passSet=['f12', 'f13'])
        self.insertBody('finger1_link3', 'sphere', [1], 5, pos=[-3.75, 0, 19], passSet=['f13'])
        self.insertBody('finger2_link0', 'cappedCylinder', [1, 7.5], 5, pos=[0, 0, 8.75], passSet=['pal', 'f21'])
        self.insertBody('finger2_link1', 'cappedCylinder', [1, 4], 5, pos=[0, 0, 14.5], passSet=['f21', 'f22'])
        self.insertBody('finger2_link2', 'cappedCylinder', [1, 2.9], 5, pos=[0, 0, 17.95], passSet=['f22', 'f23'])
        self.insertBody('finger2_link3', 'sphere', [1], 5, pos=[0, 0, 19], passSet=['f23'])
        self.insertBody('finger3_link0', 'cappedCylinder', [1, 7.5], 5, pos=[3.75, 0, 8.75], passSet=['pal', 'f31'])
        self.insertBody('finger3_link1', 'cappedCylinder', [1, 4], 5, pos=[3.75, 0, 14.5], passSet=['f31', 'f32'])
        self.insertBody('finger3_link2', 'cappedCylinder', [1, 2.9], 5, pos=[3.75, 0, 17.95], passSet=['f32', 'f33'])
        self.insertBody('finger3_link3', 'sphere', [1], 5, pos=[3.75, 0, 19], passSet=['f33'])
        self.insertJoint('palm', 'pressure', 'slider', axis={'x':0, 'y':1, 'z':0, "HiStop":0, "LowStop":-0.5, "StopERP":0.999, "StopCFM":0.002})
        self.insertJoint('palm', 'finger0_link0', 'hinge', axis={'x':0, 'y':0, 'z':1, "HiStop":1.5, "LowStop":0.0}, anchor=(-5, 0, -2.5))
        self.insertJoint('finger0_link0', 'finger0_link1', 'hinge', axis={'x':0, 'y':0, 'z':1, "HiStop":1.5, "LowStop":0.0}, anchor=(-12.5, 0, -2.5))
        self.insertJoint('finger0_link1', 'finger0_link2', 'hinge', axis={'x':0, 'y':0, 'z':1, "HiStop":1.5, "LowStop":0.0}, anchor=(-16.5, 0, -2.5))
        self.insertJoint('finger0_link2', 'finger0_link3', 'slider', axis={'x':1, 'y':0, 'z':0, "HiStop":0, "LowStop":-0.5, "StopERP":0.999, "StopCFM":0.002})
        self.insertJoint('palm', 'finger1_link0', 'hinge', axis={'x':1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(3.75, 0, 5))
        self.insertJoint('finger1_link0', 'finger1_link1', 'hinge', axis={'x':1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(3.75, 0, 12.5))
        self.insertJoint('finger1_link1', 'finger1_link2', 'hinge', axis={'x':1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(3.75, 0, 16.5))
        self.insertJoint('finger1_link2', 'finger1_link3', 'slider', axis={'x':0, 'y':0, 'z':1, "HiStop":0, "LowStop":-0.5, "StopERP":0.999, "StopCFM":0.002})
        self.insertJoint('palm', 'finger2_link0', 'hinge', axis={'x':1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(0, 0, 5))
        self.insertJoint('finger2_link0', 'finger2_link1', 'hinge', axis={'x':1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(0, 0, 12.5))
        self.insertJoint('finger2_link1', 'finger2_link2', 'hinge', axis={'x':1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(0, 0, 16.5))
        self.insertJoint('finger2_link2', 'finger2_link3', 'slider', axis={'x':0, 'y':0, 'z':1, "HiStop":0, "LowStop":-0.5, "StopERP":0.999, "StopCFM":0.002})
        self.insertJoint('palm', 'finger3_link0', 'hinge', axis={'x':1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(-3.75, 0, 5))
        self.insertJoint('finger3_link0', 'finger3_link1', 'hinge', axis={'x':1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(-3.75, 0, 12.5))
        self.insertJoint('finger3_link1', 'finger3_link2', 'hinge', axis={'x':1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(-3.75, 0, 16.5))
        self.insertJoint('finger3_link2', 'finger3_link3', 'slider', axis={'x':0, 'y':0, 'z':1, "HiStop":0, "LowStop":-0.5, "StopERP":0.999, "StopCFM":0.002})
        self.centerOn('palm')
        self.insertFloor(y= -1)
        # add one group of haptic sensors
        self._nSensorElements = 0
        self.sensorElements = []
        self.sensorGroupName = None


class XODEhandflip(XODEfile):

    def __init__(self, name, **kwargs):
        """Creates hand with fingertip and palm sensors -- palm down"""
        XODEfile.__init__(self, name, **kwargs)
        # create the hand and finger
        self.insertBody('palm', 'box', [10, 2, 10], 10, pos=[0, 0, 0], passSet=['pal'])
        self.insertBody('pressure', 'box', [8, 0.5, 8], 10, pos=[0, -1, 0], passSet=['pal'])
        self.insertBody('finger0_link0', 'cappedCylinder', [1, 7.5], 5, pos=[-8.75, 0, -2.5], euler=[0, 90, 0], passSet=['pal', 'f01'])
        self.insertBody('finger0_link1', 'cappedCylinder', [1, 4], 5, pos=[-14.5, 0, -2.5], euler=[0, 90, 0], passSet=['f01', 'f02'])
        self.insertBody('finger0_link2', 'cappedCylinder', [1, 2.9], 5, pos=[-17.95, 0, -2.5], euler=[0, 90, 0], passSet=['f02', 'f03'])
        self.insertBody('finger0_link3', 'sphere', [1], 5, pos=[-19, 0, -2.5], passSet=['f03'])
        self.insertBody('finger1_link0', 'cappedCylinder', [1, 7.5], 5, pos=[-3.75, 0, 8.75], passSet=['pal', 'f11'])
        self.insertBody('finger1_link1', 'cappedCylinder', [1, 4], 5, pos=[-3.75, 0, 14.5], passSet=['f11', 'f12'])
        self.insertBody('finger1_link2', 'cappedCylinder', [1, 2.9], 5, pos=[-3.75, 0, 17.95], passSet=['f12', 'f13'])
        self.insertBody('finger1_link3', 'sphere', [1], 5, pos=[-3.75, 0, 19], passSet=['f13'])
        self.insertBody('finger2_link0', 'cappedCylinder', [1, 7.5], 5, pos=[0, 0, 8.75], passSet=['pal', 'f21'])
        self.insertBody('finger2_link1', 'cappedCylinder', [1, 4], 5, pos=[0, 0, 14.5], passSet=['f21', 'f22'])
        self.insertBody('finger2_link2', 'cappedCylinder', [1, 2.9], 5, pos=[0, 0, 17.95], passSet=['f22', 'f23'])
        self.insertBody('finger2_link3', 'sphere', [1], 5, pos=[0, 0, 19], passSet=['f23'])
        self.insertBody('finger3_link0', 'cappedCylinder', [1, 7.5], 5, pos=[3.75, 0, 8.75], passSet=['pal', 'f31'])
        self.insertBody('finger3_link1', 'cappedCylinder', [1, 4], 5, pos=[3.75, 0, 14.5], passSet=['f31', 'f32'])
        self.insertBody('finger3_link2', 'cappedCylinder', [1, 2.9], 5, pos=[3.75, 0, 17.95], passSet=['f32', 'f33'])
        self.insertBody('finger3_link3', 'sphere', [1], 5, pos=[3.75, 0, 19], passSet=['f33'])
        ## funny finger config with bestNetwork provided (try it ;)
        ##self.insertJoint('palm','pressure','slider', axis={'x':0,'y':-1,'z':0,"HiStop":0,"LowStop":-0.5, "StopERP":0.999,"StopCFM":0.002})
        self.insertJoint('palm', 'pressure', 'slider', axis={'x':0, 'y':1, 'z':0, "HiStop":0, "LowStop":0, "StopERP":0.999, "StopCFM":0.002})
        self.insertJoint('palm', 'finger0_link0', 'hinge', axis={'x':0, 'y':0, 'z':-1, "HiStop":1.5, "LowStop":0.0}, anchor=(-5, 0, -2.5))
        self.insertJoint('finger0_link0', 'finger0_link1', 'hinge', axis={'x':0, 'y':0, 'z':-1, "HiStop":1.5, "LowStop":0.0}, anchor=(-12.5, 0, -2.5))
        self.insertJoint('finger0_link1', 'finger0_link2', 'hinge', axis={'x':0, 'y':0, 'z':-1, "HiStop":1.5, "LowStop":0.0}, anchor=(-16.5, 0, -2.5))
        self.insertJoint('finger0_link2', 'finger0_link3', 'slider', axis={'x':1, 'y':0, 'z':0, "HiStop":0, "LowStop":-0.5, "StopERP":0.999, "StopCFM":0.002})
        self.insertJoint('palm', 'finger1_link0', 'hinge', axis={'x':-1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(3.75, 0, 5))
        self.insertJoint('finger1_link0', 'finger1_link1', 'hinge', axis={'x':-1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(3.75, 0, 12.5))
        self.insertJoint('finger1_link1', 'finger1_link2', 'hinge', axis={'x':-1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(3.75, 0, 16.5))
        self.insertJoint('finger1_link2', 'finger1_link3', 'slider', axis={'x':0, 'y':0, 'z':1, "HiStop":0, "LowStop":-0.5, "StopERP":0.999, "StopCFM":0.002})
        self.insertJoint('palm', 'finger2_link0', 'hinge', axis={'x':-1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(0, 0, 5))
        self.insertJoint('finger2_link0', 'finger2_link1', 'hinge', axis={'x':-1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(0, 0, 12.5))
        self.insertJoint('finger2_link1', 'finger2_link2', 'hinge', axis={'x':-1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(0, 0, 16.5))
        self.insertJoint('finger2_link2', 'finger2_link3', 'slider', axis={'x':0, 'y':0, 'z':1, "HiStop":0, "LowStop":-0.5, "StopERP":0.999, "StopCFM":0.002})
        self.insertJoint('palm', 'finger3_link0', 'hinge', axis={'x':-1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(-3.75, 0, 5))
        self.insertJoint('finger3_link0', 'finger3_link1', 'hinge', axis={'x':-1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(-3.75, 0, 12.5))
        self.insertJoint('finger3_link1', 'finger3_link2', 'hinge', axis={'x':-1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(-3.75, 0, 16.5))
        self.insertJoint('finger3_link2', 'finger3_link3', 'slider', axis={'x':0, 'y':0, 'z':1, "HiStop":0, "LowStop":-0.5, "StopERP":0.999, "StopCFM":0.002})
        self.centerOn('palm')
        self.insertFloor(y= -1.25)
        # add one group of haptic sensors
        self._nSensorElements = 0
        self.sensorElements = []
        self.sensorGroupName = None


class HapticTestSetupWithRidges(XODEfinger):

    def insertSampleStructure(self, angle=30, std=0.05, dist=0.9, **kwargs):
        """create some ridges on the sample"""
        for i in range(16):
            name = 'ridge' + str(i)
            self.insertBody(name, 'cappedCylinder', [0.2, 10], 5, pos=[0, 0.5, random.gauss(15 - dist * i, std)], euler=[0, angle, 0], passSet=['sam'])
            self.insertJoint('sample', name, 'fixed')


class HapticTestSetupWithSpheres(XODEfinger):

    def insertSampleStructure(self, xoffs=0.0, std=0.025, dist=0.9, **kwargs):
        """create four rows of spheres on the sample"""
        dx = [dist * k for k in [-1, 0, 1]]
        dz = [dist * k * 0.5 for k in [0, 1, 0]]
        for i in range(16):
            for k in range(3):
                x = random.gauss(dx[k] + xoffs, std)
                z = random.gauss(15 - dist * i + dz[k], std)
                name = 'sphere' + str(i) + str(k)
                self.insertBody(name, 'sphere', [0.2], 5, pos=[x, 0.5, z], passSet=['sam'])
                self.insertJoint('sample', name, 'fixed')


class HapticTestSetupWithSpirals(XODEfinger):

    def insertSampleStructure(self, std=0.05, xoffs=0.0, dist=1.0, **kwargs):
        """create elongated spiral pattern"""
        rg = 50
        phi = [2.3 + sqrt(f) * pi * 10 / sqrt(rg) for f in range(rg)]
        r = [sqrt(f) * 2.5 / sqrt(rg) for f in range(rg)]
        for k in range(rg):
            x = random.gauss(cos(phi[k]) * r[k], std)
            z = random.gauss(5 + sin(phi[k]) * r[k] * 3, std)
            name = 'sphere' + str(k)
            self.insertBody(name, 'sphere', [0.2], 5, pos=[x, 0.5, z], passSet=['sam'])
            self.insertJoint('sample', name, 'fixed')


class HapticTestSetupWithSine(XODEfinger):

    def insertSampleStructure(self, angle=0, std=0.05, xoffs=0.0, dist=1.0, **kwargs):
        """create rotated sine pattern"""
        rg = 50
        z = [f * 10.0 / rg for f in range(rg)]
        x = [sin(f * 2) * sin(f / 3) * 3.5 for f in z]
        z = [f - 5 for f in z]
        for i in range(rg):
            r = sqrt(x[i] * x[i] + z[i] * z[i])
            if r > 0:
                phi = asin(x[i] / r)
                if z[i] < 0: phi = pi - phi
                phi += radians(angle)
                x[i] = random.gauss(sin(phi) * r, std)
                z[i] = random.gauss(cos(phi) * r, std)
            name = 'sphere' + str(i)
            self.insertBody(name, 'sphere', [0.2], 5, pos=[x[i], 0.5, z[i]], passSet=['sam'])
            self.insertJoint('sample', name, 'fixed')

class XODEJohnnie(XODEfile):

    def __init__(self, name, **kwargs):
        """Creates hand with fingertip and palm sensors -- palm up"""
        XODEfile.__init__(self, name, **kwargs)
        # create the hand and finger
        self.insertBody('palm', 'box', [4.12, 3.0, 2], 30, pos=[0, 0, 0], passSet=['total'], mass=3.356)
        self.insertBody('neck', 'cappedCylinder', [0.25, 5.6], 5, pos=[0, 2.8, 0], euler=[90, 0, 0], passSet=['total'], mass=0.1)
        self.insertBody('head', 'box', [3.0, 1.2, 1.5], 30, pos=[0, 4.0, 0], passSet=['total'], mass=0.1)
        self.insertBody('arm_left', 'cappedCylinder', [0.25, 7.5], 5, pos=[2.06, -2.89, 0], euler=[90, 0, 0], passSet=['total'], mass=2.473)
        self.insertBody('arm_right', 'cappedCylinder', [0.25, 7.5], 5, pos=[-2.06, -2.89, 0], euler=[90, 0, 0], passSet=['total'], mass=2.473)
        self.insertBody('hip', 'cappedCylinder', [0.25, 3.2], 5, pos=[0, -1.6, 0], euler=[90, 0, 0], passSet=['total'], mass=0.192)
        self.insertBody('pelvis', 'cappedCylinder', [0.25, 2.4], 5, pos=[0, -3.2, 0], euler=[0, 90, 0], passSet=['total'], mass=1.0)
        self.insertBody('pelLeft', 'cappedCylinder', [0.25, 0.8], 5, pos=[1.2, -3.6, 0], euler=[90, 0, 0], passSet=['total'], mass=2.567)
        self.insertBody('pelRight', 'cappedCylinder', [0.25, 0.8], 5, pos=[-1.2, -3.6, 0], euler=[90, 0, 0], passSet=['total'], mass=2.567)
        self.insertBody('tibiaLeft', 'cappedCylinder', [0.25, 4.4], 5, pos=[1.2, -6.2, 0], euler=[90, 0, 0], passSet=['total'], mass=5.024)
        self.insertBody('tibiaRight', 'cappedCylinder', [0.25, 4.4], 5, pos=[-1.2, -6.2, 0], euler=[90, 0, 0], passSet=['total'], mass=5.024)
        self.insertBody('sheenLeft', 'cappedCylinder', [0.25, 3.8], 5, pos=[1.2, -10.3, 0], euler=[90, 0, 0], passSet=['total'], mass=3.236)
        self.insertBody('sheenRight', 'cappedCylinder', [0.25, 3.8], 5, pos=[-1.2, -10.3, 0], euler=[90, 0, 0], passSet=['total'], mass=3.236)
        self.insertBody('footLeft', 'box', [2.2, 0.4, 2.6], 3, pos=[1.2, -12.2, 0.75], passSet=['total'], mass=1.801)
        self.insertBody('footRight', 'box', [2.2, 0.4, 2.6], 3, pos=[-1.2, -12.2, 0.75], passSet=['total'], mass=1.801)
        self.insertJoint('palm', 'neck', 'fixed', axis={'x':0, 'y':0, 'z':0}, anchor=(0, 0, 0))
        self.insertJoint('neck', 'head', 'fixed', axis={'x':0, 'y':0, 'z':0}, anchor=(0, 2.8, 0))
        self.insertJoint('palm', 'arm_left', 'hinge', axis={'x':1, 'y':0, 'z':0}, anchor=(2.06, 0.86, 0))
        self.insertJoint('palm', 'arm_right', 'hinge', axis={'x':1, 'y':0, 'z':0}, anchor=(-2.06, 0.86, 0))
        self.insertJoint('palm', 'hip', 'hinge', axis={'x':0, 'y':1, 'z':0, "HiStop":0.5, "LowStop":-0.5}, anchor=(0, -1.6, 0))
        self.insertJoint('hip', 'pelvis', 'fixed', axis={'x':0, 'y':0, 'z':0}, anchor=(0, -3.2, 0))
        self.insertJoint('pelvis', 'pelLeft', 'hinge', axis={'x':0, 'y':0, 'z':-1, "HiStop":0.5, "LowStop":0.0}, anchor=(1.2, -3.2, 0))
        self.insertJoint('pelvis', 'pelRight', 'hinge', axis={'x':0, 'y':0, 'z':1, "HiStop":0.5, "LowStop":0.0}, anchor=(-1.2, -3.2, 0))
        self.insertJoint('pelLeft', 'tibiaLeft', 'hinge', axis={'x':1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(1.2, -4.0, 0))
        self.insertJoint('pelRight', 'tibiaRight', 'hinge', axis={'x':1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(-1.2, -4.0, 0))
        self.insertJoint('tibiaLeft', 'sheenLeft', 'hinge', axis={'x':-1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(1.2, -8.4, 0))
        self.insertJoint('tibiaRight', 'sheenRight', 'hinge', axis={'x':-1, 'y':0, 'z':0, "HiStop":1.5, "LowStop":0.0}, anchor=(-1.2, -8.4, 0))
        self.insertJoint('sheenLeft', 'footLeft', 'hinge', axis={'x':1, 'y':0, 'z':0, "HiStop":0.25, "LowStop":-0.25}, anchor=(1.2, -12.2, 0))
        self.insertJoint('sheenRight', 'footRight', 'hinge', axis={'x':1, 'y':0, 'z':0, "HiStop":0.25, "LowStop":-0.25}, anchor=(-1.2, -12.2, 0))
        self.centerOn('palm')
        self.insertFloor(y= -12.7)
        # add one group of haptic sensors
        self._nSensorElements = 0
        self.sensorElements = []
        self.sensorGroupName = None

class XODESLR(XODEfile):
    def __init__(self, name, **kwargs):
        """Creates hand with fingertip and palm sensors -- palm up"""
        XODEfile.__init__(self, name, **kwargs)
        # create the hand and finger
        self.insertBody('body', 'box', [7.0, 16.0, 10.0], 30, pos=[0, 0, 2.0], passSet=['total'], mass=15.0, color=(0.5, 0.5, 0.4, 1.0))
        #right arm
        self.insertBody('shoulderUpRight', 'cappedCylinder', [0.5, 2.0], 5, pos=[2.5, 7.0, -3.5], euler=[0, 90, 0], passSet=['rightSh', 'total'], mass=0.25)
        self.insertBody('shoulderLRRight', 'cappedCylinder', [0.5, 2.0], 5, pos=[3.5, 6.0, -3.5], euler=[90, 0, 0], passSet=['rightSh'], mass=0.25)
        self.insertBody('shoulderPRRight', 'cappedCylinder', [0.5, 2.0], 5, pos=[3.5, 4.0, -3.5], euler=[90, 0, 0], passSet=['rightSh'], mass=0.25)
        self.insertBody('armUpRight', 'cappedCylinder', [0.5, 2.0], 5, pos=[3.5, 2.0, -3.5], euler=[90, 0, 0], passSet=['rightAr', 'rightSh'], mass=0.25)
        self.insertBody('armPRRight', 'cappedCylinder', [0.5, 2.0], 5, pos=[3.5, 0.0, -3.5], euler=[90, 0, 0], passSet=['rightAr'], mass=0.25)
        self.insertBody('handUpRight', 'cappedCylinder', [0.5, 2.0], 5, pos=[3.5, -2.0, -3.5], euler=[90, 0, 0], passSet=['rightAr', 'rightHa'], mass=0.25)
        #right hand
        self.insertBody('palmRight', 'box', [1.5, 0.25, 0.5], 30, pos=[3.5, -3.0, -3.5], passSet=['rightHa'], mass=0.1, color=(0.6, 0.6, 0.3, 1.0))
        self.insertBody('fingerRight1', 'box', [0.25, 1.0, 0.5], 30, pos=[4.0, -3.5, -3.5], passSet=['rightHa'], mass=0.1, color=(0.6, 0.6, 0.3, 1.0))
        self.insertBody('fingerRight2', 'box', [0.25, 1.0, 0.5], 30, pos=[3.0, -3.5, -3.5], passSet=['rightHa'], mass=0.1, color=(0.6, 0.6, 0.3, 1.0))
        
        #left arm
        self.insertBody('shoulderUpLeft', 'cappedCylinder', [0.5, 2.0], 5, pos=[-2.5, 7.0, -3.5], euler=[0, 90, 0], passSet=['leftSh', 'total'], mass=0.25)
        self.insertBody('shoulderLRLeft', 'cappedCylinder', [0.5, 2.0], 5, pos=[-3.5, 6.0, -3.5], euler=[90, 0, 0], passSet=['leftSh'], mass=0.25)
        self.insertBody('shoulderPRLeft', 'cappedCylinder', [0.5, 2.0], 5, pos=[-3.5, 4.0, -3.5], euler=[90, 0, 0], passSet=['leftSh'], mass=0.25)
        self.insertBody('armUpLeft', 'cappedCylinder', [0.5, 2.0], 5, pos=[-3.5, 2.0, -3.5], euler=[90, 0, 0], passSet=['leftAr', 'leftSh'], mass=0.25)
        self.insertBody('armPRLeft', 'cappedCylinder', [0.5, 2.0], 5, pos=[-3.5, 0.0, -3.5], euler=[90, 0, 0], passSet=['leftAr'], mass=0.25)
        self.insertBody('handUpLeft', 'cappedCylinder', [0.5, 2.0], 5, pos=[-3.5, -2.0, -3.5], euler=[90, 0, 0], passSet=['leftAr', 'leftHa'], mass=0.25)
        #left hand
        self.insertBody('palmLeft', 'box', [1.5, 0.25, 0.5], 30, pos=[-3.5, -3.0, -3.5], passSet=['leftHa'], mass=0.1, color=(0.6, 0.6, 0.3, 1.0))
        self.insertBody('fingerLeft1', 'box', [0.25, 1.0, 0.5], 30, pos=[-4.0, -3.5, -3.5], passSet=['leftHa'], mass=0.1, color=(0.6, 0.6, 0.3, 1.0))
        self.insertBody('fingerLeft2', 'box', [0.25, 1.0, 0.5], 30, pos=[-3.0, -3.5, -3.5], passSet=['leftHa'], mass=0.1, color=(0.6, 0.6, 0.3, 1.0))
        
        #Joints right        
        self.insertJoint('body', 'shoulderUpRight', 'hinge', axis={'x':1, 'y':0, 'z':0}, anchor=(2.5, 7.0, -3.5))
        self.insertJoint('shoulderUpRight', 'shoulderLRRight', 'hinge', axis={'x':0, 'y':0, 'z':1}, anchor=(3.5, 7.0, -3.5))
        self.insertJoint('shoulderLRRight', 'shoulderPRRight', 'hinge', axis={'x':0, 'y':1, 'z':0}, anchor=(3.5, 5.0, -3.5))
        self.insertJoint('shoulderPRRight', 'armUpRight', 'hinge', axis={'x':1, 'y':0, 'z':0}, anchor=(3.5, 3.0, -3.5))
        self.insertJoint('armUpRight', 'armPRRight', 'hinge', axis={'x':0, 'y':1, 'z':0}, anchor=(3.5, 1.0, -3.5))
        self.insertJoint('armPRRight', 'handUpRight', 'hinge', axis={'x':1, 'y':0, 'z':0}, anchor=(3.5, -1.0, -3.5))
        self.insertJoint('handUpRight', 'palmRight', 'hinge', axis={'x':0, 'y':1, 'z':0}, anchor=(3.5, -3.0, -3.5))
        self.insertJoint('palmRight', 'fingerRight1', 'fixed', axis={'x':0, 'y':0, 'z':0}, anchor=(4.0, -3.5, -3.5))
        self.insertJoint('palmRight', 'fingerRight2', 'hinge', axis={'x':0, 'y':0, 'z':1}, anchor=(3.0, -3.0, -3.5))
        
        #Joints left        
        self.insertJoint('body', 'shoulderUpLeft', 'hinge', axis={'x':1, 'y':0, 'z':0}, anchor=(-2.5, 7.0, -3.5))
        self.insertJoint('shoulderUpLeft', 'shoulderLRLeft', 'hinge', axis={'x':0, 'y':0, 'z':1}, anchor=(-3.5, 7.0, -3.5))
        self.insertJoint('shoulderLRLeft', 'shoulderPRLeft', 'hinge', axis={'x':0, 'y':1, 'z':0}, anchor=(-3.5, 5.0, -3.5))
        self.insertJoint('shoulderPRLeft', 'armUpLeft', 'hinge', axis={'x':1, 'y':0, 'z':0}, anchor=(-3.5, 3.0, -3.5))
        self.insertJoint('armUpLeft', 'armPRLeft', 'hinge', axis={'x':0, 'y':1, 'z':0}, anchor=(-3.5, 1.0, -3.5))
        self.insertJoint('armPRLeft', 'handUpLeft', 'hinge', axis={'x':1, 'y':0, 'z':0}, anchor=(-3.5, -1.0, -3.5))
        self.insertJoint('handUpLeft', 'palmLeft', 'hinge', axis={'x':0, 'y':1, 'z':0}, anchor=(-3.5, -3.0, -3.5))
        self.insertJoint('palmLeft', 'fingerLeft1', 'fixed', axis={'x':0, 'y':0, 'z':0}, anchor=(-4.0, -3.5, -3.5))
        self.insertJoint('palmLeft', 'fingerLeft2', 'hinge', axis={'x':0, 'y':0, 'z':1}, anchor=(-3.0, -3.0, -3.5))
               
        self.centerOn('body')
        self.insertFloor(y= -8.0)
        # add one group of haptic sensors
        self._nSensorElements = 0
        self.sensorElements = []
        self.sensorGroupName = None

class XODELSRTable(XODESLR): #XODESLR
    def __init__(self, name, **kwargs):
        XODESLR.__init__(self, name, **kwargs)        
        # create table
        self.insertBody('plate', 'box', [15.0, 1.0, 8.0], 30, pos=[-12.5, 0.5, -14.0], passSet=['table'], mass=2.0, color=(0.4, 0.25, 0.0, 1.0))
        self.insertBody('leg1', 'box', [0.5, 8.0, 0.5], 30, pos=[-19.5, -4.0, -17.5], passSet=['table'], mass=0.3, color=(0.6, 0.8, 0.8, 0.8))
        self.insertJoint('plate', 'leg1', 'fixed', axis={'x':0, 'y':0, 'z':0}, anchor=(-19.5, 0.0, -17.5))
        self.insertBody('leg2', 'box', [0.5, 8.0, 0.5], 30, pos=[-5.5, -4.0, -17.5], passSet=['table'], mass=0.3, color=(0.6, 0.8, 0.8, 0.8))
        self.insertJoint('plate', 'leg2', 'fixed', axis={'x':0, 'y':0, 'z':0}, anchor=(-5.5, 0.0, -17.5))
        self.insertBody('leg3', 'box', [0.5, 8.0, 0.5], 30, pos=[-5.5, -4.0, -10.5], passSet=['table'], mass=0.3, color=(0.6, 0.8, 0.8, 0.8))
        self.insertJoint('plate', 'leg3', 'fixed', axis={'x':0, 'y':0, 'z':0}, anchor=(-5.5, 0.0, -10.5))
        self.insertBody('leg4', 'box', [0.5, 8.0, 0.5], 30, pos=[-19.5, -4.0, -10.5], passSet=['table'], mass=0.3, color=(0.6, 0.8, 0.8, 0.8))
        self.insertJoint('plate', 'leg4', 'fixed', axis={'x':0, 'y':0, 'z':0}, anchor=(-19.5, 0.0, -10.5))

class XODELSRGlas(XODELSRTable): #XODESLR
    def __init__(self, name, **kwargs):
        XODELSRTable.__init__(self, name, **kwargs)
        # create glass + coaster (necessary because cylinder collision has a bug)
        self.insertBody('objectP00', 'cylinder', [0.2, 1], 30, pos=[-6.5, 1.51 , -11.0], passSet=['object'], mass=0.2, euler=[90, 0, 0], color=(0.6, 0.6, 0.8, 0.5))
        self.insertBody('objectP01', 'box', [0.45, 0.02, 0.45], 30, pos=[-6.5, 1.01, -11.0], passSet=['object'], mass=0.01)
        self.insertBody('objectP02', 'box', [0.45, 0.02, 0.45], 30, pos=[-6.5, 2.01, -11.0], passSet=['object'], mass=0.01)
        self.insertJoint('objectP00', 'objectP01', 'fixed', axis={'x':0, 'y':0, 'z':0}, anchor=(-6.5, 1.01, -11.0))
        self.insertJoint('objectP00', 'objectP02', 'fixed', axis={'x':0, 'y':0, 'z':0}, anchor=(-6.5, 2.01, -11.0))

class XODELSRPlate(XODELSRTable): #XODESLR
    def __init__(self, name, **kwargs):
        XODELSRTable.__init__(self, name, **kwargs)
        # create plate
        # plate ground
        bX = 1.0 #width of plate floor
        bY = 0.05 #height of plate floor
        bZ = 1.0 #depth of plate floor
        #plate sides
        sX = 0.5 #width of plate side
        sY = bY #height of plate side
        sZ = 1.0 #depth of plate side
        #position of plate
        pX = -6.5
        pY = 1.02
        pZ = -11.0
        #stuff
        m = 0.05 #mass per part
        c = (0.6, 0.6, 0.8, 0.95) #color of object
        dif = sX / (2.0 * sqrt(5)) #

        self.insertBody('objectP00', 'box', [bX, bY, bZ], 30, pos=[pX, pY, pZ], passSet=['object'], mass=m, color=c)
        self.insertBody('objectP01', 'box', [sX, sY, sZ], 30, pos=[pX - bX * 0.5 - 2.0 * dif, pY + dif, pZ], passSet=['object'], mass=m, euler=[0, 0, 22.5], color=c)
        self.insertJoint('objectP00', 'objectP01', 'fixed', axis={'x':0, 'y':0, 'z':0}, anchor=(pX - bX * 0.5, pY, pZ))

        self.insertBody('objectP02', 'box', [sX, sY, sZ], 30, pos=[pX + bX * 0.5 + 2.0 * dif, pY + dif, pZ], passSet=['object'], mass=m, euler=[0, 0, -22.5], color=c)
        self.insertJoint('objectP00', 'objectP02', 'fixed', axis={'x':0, 'y':0, 'z':0}, anchor=(pX + bX * 0.5, pY, pZ))

        self.insertBody('objectP03', 'box', [sX, sY, sZ], 30, pos=[pX, pY + dif, pZ + bZ * 0.5 + 2.0 * dif], passSet=['object'], mass=m, euler=[0, 90, -22.5], color=c)
        self.insertJoint('objectP00', 'objectP03', 'fixed', axis={'x':0, 'y':0, 'z':0}, anchor=(pX, pY, pZ + bZ * 0.5))

        self.insertBody('objectP04', 'box', [sX, sY, sZ], 30, pos=[pX, pY + dif, pZ - bZ * 0.5 - 2.0 * dif], passSet=['object'], mass=m, euler=[0, 90, 22.5], color=c)
        self.insertJoint('objectP00', 'objectP04', 'fixed', axis={'x':0, 'y':0, 'z':0}, anchor=(pX, pY, pZ - bZ * 0.5))
        
if __name__ == '__main__' :

    table = XODELSRPlate('../models/ccrlPlate')
    
    #z = XODESLR('../models/slr')
    #z = XODEhand('hand_mal_10')
    #z = XODEhandflip('handflip')
    #z = XODEhandflip('handflip')
    #z.scaleModel(0.5)
    
    table.writeXODE()


########NEW FILE########
__FILENAME__ = viewer
__author__ = 'Martin Felder, felder@in.tum.de'

from OpenGL.GL import * #@UnusedWildImport
from OpenGL.GLU import * #@UnusedWildImport
from OpenGL.GLUT import * #@UnusedWildImport

from math import acos, pi, sqrt
from tools.mathhelpers import crossproduct, norm, dotproduct

import time
import Image #@UnresolvedImport

from pybrain.tools.networking.udpconnection import UDPClient


class ODEViewer(object):
    def __init__(self, servIP="127.0.0.1", ownIP="127.0.0.1", port="21590", buf="16384"):

        # initialize the viewport size
        self.width = 800
        self.height = 600

        # initialize object which the camera follows
        self.centerObj = None
        self.mouseView = True
        self.viewDistance = 30
        self.lastx = -0.5
        self.lasty = 1
        self.lastz = -1

        self.dt = 1
        self.fps = 50
        self.lasttime = time.time()
        self.starttime = time.time()
        self.captureScreen = False
        self.isCapturing = False
        self.isFloorGreen = True

        self.message = None
        self.keyboardCallback = None

        # capture only every frameT. frame
        self.counter = 0
        self.frameT = 1

        self.init_GL()

        # set own callback functions
        glutMotionFunc (self._motionfunc)
        glutPassiveMotionFunc(self._passivemotionfunc)
        glutDisplayFunc (self._drawfunc)
        glutIdleFunc (self._idlefunc)
        glutKeyboardFunc (self._keyfunc)


        self.dt = 1.0 / self.fps
        self.lasttime = time.time()
        self.starttime = self.lasttime

        # initialize udp client
        self.client = UDPClient(servIP, ownIP, port, buf)


    def start(self):
        # start the OpenGL main loop
        while True:
            glutMainLoop()

    def setFrameRate(self, fps):
        self.fps = fps
        self.dt = 1.0 / self.fps

    def setCaptureScreen(self, capture):
        self.captureScreen = capture

    def getCaptureScreen(self):
        return self.captureScreen

    def waitScreenCapturing(self):
        self.isCapturing = True

    def isScreenCapturing(self):
        return self.isCapturing

    def setCenterObj(self, obj):
        self.centerObj = obj


    def updateData(self):
        try:
            self.message = self.client.listen()
        except:
            pass


    def init_GL(self, width=800, height=600):
        """ initialize OpenGL. This function has to be called only once before drawing. """
        glutInit([])

        # Open a window
        glutInitDisplayMode (GLUT_RGB | GLUT_DOUBLE | GLUT_DEPTH)
        self.width = width
        self.height = height
        glutInitWindowPosition (500, 0)
        glutInitWindowSize (self.width, self.height)
        self._myWindow = glutCreateWindow ("ODE Viewer")

        # Initialize Viewport and Shading
        glViewport(0, 0, self.width, self.height)
        glShadeModel(GL_SMOOTH)
        glHint(GL_PERSPECTIVE_CORRECTION_HINT, GL_NICEST)
        glClearColor(1.0, 1.0, 1.0, 0.0)

        # Initialize Depth Buffer
        glClearDepth(1.0)
        glEnable(GL_DEPTH_TEST)
        glDepthFunc(GL_LESS)

        # Initialize Lighting
        glEnable(GL_LIGHTING)
        glLightfv(GL_LIGHT1, GL_AMBIENT, [0.5, 0.5, 0.5, 1.0])
        glLightfv(GL_LIGHT1, GL_DIFFUSE, [1.0, 1.0, 1.0, 1.0])
        glLightfv(GL_LIGHT1, GL_POSITION, [0.0, 5.0, 5.0, 1.0])
        glEnable(GL_LIGHT1)

        # enable material coloring
        glEnable(GL_COLOR_MATERIAL)
        glColorMaterial(GL_FRONT, GL_AMBIENT_AND_DIFFUSE)

        glEnable(GL_NORMALIZE)

    def prepare_GL(self):
        """Prepare drawing. This function is called in every step. It clears the screen and sets the new camera position"""
        # Clear the screen
        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)

        # Projection mode
        glMatrixMode(GL_PROJECTION)
        glLoadIdentity()
        gluPerspective (45, 1.3333, 0.2, 500)

        # Initialize ModelView matrix
        glMatrixMode(GL_MODELVIEW)
        glLoadIdentity()

        # View transformation (if "centerOn(...)" is set, keep camera to specific object)
        if self.centerObj is not None:
            (centerX, centerY, centerZ) = self.centerObj.getPosition()
        else:
            centerX = centerY = centerZ = 0
        # use the mouse to shift eye sensor on a hemisphere
        eyeX = self.viewDistance * self.lastx
        eyeY = self.viewDistance * self.lasty + centerY
        eyeZ = self.viewDistance * self.lastz
        gluLookAt (eyeX, eyeY, eyeZ, centerX, centerY, centerZ, 0, 1, 0)

    def draw_item(self, item):
        """ draws an object (spere, cube, plane, ...) """
        glDisable(GL_TEXTURE_2D)

        glPushMatrix()

        if item['type'] in ['GeomBox', 'GeomSphere', 'GeomCylinder', 'GeomCCylinder']:
            # set color of object (currently dark gray)
            if item.has_key('color'):
                glEnable (GL_BLEND)
                glBlendFunc (GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA)
                glColor4f(*(item['color']))
            else: glColor3f(0.1, 0.1, 0.1)

            # transform (rotate, translate) body accordingly
            (x, y, z) = item['position']
            R = item['rotation']

            rot = [R[0], R[3], R[6], 0.0,
                   R[1], R[4], R[7], 0.0,
                   R[2], R[5], R[8], 0.0,
                      x, y, z, 1.0]

            glMultMatrixd(rot)

            # switch different geom objects
            if item['type'] == 'GeomBox':
                # cube
                (sx, sy, sz) = item['scale']
                glScaled(sx, sy, sz)
                glutSolidCube(1)
            elif item['type'] == 'GeomSphere':
                # sphere
                glutSolidSphere(item['radius'], 20, 20)

            elif item['type'] == 'GeomCCylinder':
                quad = gluNewQuadric()
                # draw cylinder and two spheres, one at each end
                glTranslate(0.0, 0.0, -item['length'] / 2)
                gluCylinder(quad, item['radius'], item['radius'], item['length'], 32, 32)
                glutSolidSphere(item['radius'], 20, 20)
                glTranslate(0.0, 0.0, item['length'])
                glutSolidSphere(item['radius'], 20, 20)

            elif item['type'] == 'GeomCylinder':
                glTranslate(0.0, 0.0, -item['length'] / 2)
                quad = gluNewQuadric()
                gluDisk(quad, 0, item['radius'], 32, 1)
                quad = gluNewQuadric()
                gluCylinder(quad, item['radius'], item['radius'], item['length'], 32, 32)
                glTranslate(0.0, 0.0, item['length'])
                quad = gluNewQuadric()
                gluDisk(quad, 0, item['radius'], 32, 1)
            else:
                # TODO: add other geoms here
                pass

        elif item['type'] == 'GeomPlane':
            # set color of plane (currently green)
            if self.isFloorGreen:
                glColor3f(0.2, 0.6, 0.3)
            else:
                glColor3f(0.2, 0.3, 0.8)

            # for planes, we need a Quadric object
            quad = gluNewQuadric()
            gluQuadricTexture(quad, GL_TRUE)

            p = item['normal']      # the normal vector to the plane
            d = item['distance']    # the distance to the origin
            q = (0.0, 0.0, 1.0)     # the normal vector of default gluDisks (z=0 plane)

            # calculate the cross product to get the rotation axis
            c = crossproduct(p, q)
            # calculate the angle between default normal q and plane normal p
            theta = acos(dotproduct(p, q) / (norm(p) * norm(q))) / pi * 180

            # rotate the plane
            glPushMatrix()
            glTranslate(d * p[0], d * p[1], d * p[2])
            glRotate(-theta, c[0], c[1], c[2])
            gluDisk(quad, 0, 20, 20, 1)
            glPopMatrix()

        glPopMatrix()

    @staticmethod
    def _loadTexture(textureFile):
        image = open(textureFile)
        ix = image.size[0]
        iy = image.size[1]

        image = image.tostring("raw", "RGBX", 0, -1)

        # Create Texture
        textures = glGenTextures(3)
        glBindTexture(GL_TEXTURE_2D, textures[0])       # 2d texture (x and y size)

        glPixelStorei(GL_UNPACK_ALIGNMENT, 1)
        glTexImage2D(GL_TEXTURE_2D, 0, 3, ix, iy, 0, GL_RGBA, GL_BYTE, image)
        glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP)
        glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP)
        glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT)
        glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT)
        glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST)
        glTexParameterf(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST)
        glTexEnvf(GL_TEXTURE_ENV, GL_TEXTURE_ENV_MODE, GL_DECAL)

        # Create Linear Filtered Texture
        glBindTexture(GL_TEXTURE_2D, textures[1])
        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR)
        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR)
        glTexImage2D(GL_TEXTURE_2D, 0, 3, ix, iy, 0, GL_RGBA, GL_UNSIGNED_BYTE, image)

        # Create MipMapped Texture
        glBindTexture(GL_TEXTURE_2D, textures[2])
        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR)
        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR_MIPMAP_NEAREST)
        gluBuild2DMipmaps(GL_TEXTURE_2D, 3, ix, iy, GL_RGBA, GL_UNSIGNED_BYTE, image)
        return textures



    def _drawfunc (self):
        """ draw callback function """
        # Draw the scene
        self.prepare_GL()

        if self.message:
            for item in self.message:
                self.draw_item(item)

        glutSwapBuffers()
        if self.captureScreen:
            self._screenshot()

    def _idlefunc(self):
        self.updateData()
        t = self.dt - (time.time() - self.lasttime)
        if (t > 0):
            time.sleep(t)
        self.lasttime = time.time()
        glutPostRedisplay ()

    def _keyfunc (self, c, x, y):
        """ keyboard call-back function. """
        if c == 's':
            self.setCaptureScreen(not self.getCaptureScreen())
            print("Screen Capture: " + (self.getCaptureScreen() and "on" or "off"))
        if c in ['x', 'q']:
            sys.exit()
        if c == 'v':
            self.mouseView = not self.mouseView

    def _motionfunc(self, x, z):
        """Control the zoom factor"""
        if not self.mouseView: return
        zn = 2.75 * float(z) / self.height + 0.25   # [0.25,3]
        self.viewDistance = 3.0 * zn * zn
        self._passivemotionfunc(x, z)

    def _passivemotionfunc(self, x, z):
        """ Store the mouse coordinates (relative to center and normalized)
         the eye does not exactly move on a unit hemisphere; we fudge the projection
         a little by shifting the hemisphere into the ground by 0.1 units,
         such that approaching the perimeter dows not cause a huge change in the
         viewing direction. The limit for l is thus cos(arcsin(0.1))."""
        if not self.mouseView: return
        x1 = 3 * float(x) / self.width - 1.5
        z1 = -3 * float(z) / self.height + 1.5
        lsq = x1 * x1 + z1 * z1
        l = sqrt(lsq)
        if l > 0.994987:
            # for mouse outside window, project onto the unit circle
            x1 = x1 / l
            z1 = z1 / l
            y1 = 0
        else:
            y1 = max(0.0, sqrt(1.0 - x1 * x1 - z1 * z1) - 0.1)
        self.lasty = y1
        self.lastx = x1
        self.lastz = z1

    def _screenshot(self, path_prefix='.', format='PNG'):
        """Saves a screenshot of the current frame buffer.
        The save path is <path_prefix>/.screenshots/shot<num>.png
        The path is automatically created if it does not exist.
        Shots are automatically numerated based on how many files
        are already in the directory."""

        if self.counter == self.frameT:
            self.counter = 1
            dir = os.path.join(path_prefix, 'screenshots')
            if not os.path.exists(dir):
                os.makedirs(dir)

            num_present = len(os.listdir(dir))
            num_digits = len(str(num_present))
            index = '0' * (5 - num_digits) + str(num_present)

            path = os.path.join(dir, 'shot' + index + '.' + format.lower())
            glPixelStorei(GL_PACK_ALIGNMENT, 1)
            data = glReadPixels(0, 0, self.width, self.height, GL_RGB, GL_UNSIGNED_BYTE)
            image = Image.fromstring("RGB", (self.width, self.height), data)
            image = image.transpose(Image.FLIP_TOP_BOTTOM)
            image.save(path, format)
            print('Image saved to %s' % (os.path.basename(path)))
        else:
            self.counter += 1

        self.isCapturing = False


if __name__ == '__main__':
    s = sys.argv[1:]
    odeview = ODEViewer(*s)
    odeview.start()


########NEW FILE########
__FILENAME__ = body
#@PydevCodeAnalysisIgnore

######################################################################
# Python Open Dynamics Engine Wrapper
# Copyright (C) 2004 PyODE developers (see file AUTHORS)
# All rights reserved.
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of EITHER:
#   (1) The GNU Lesser General Public License as published by the Free
#       Software Foundation; either version 2.1 of the License, or (at
#       your option) any later version. The text of the GNU Lesser
#       General Public License is included with this library in the
#       file LICENSE.
#   (2) The BSD-style license that is included with this library in
#       the file LICENSE-BSD.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the files
# LICENSE and LICENSE-BSD for more details.
######################################################################

# XODE Importer for PyODE

"""
XODE Body and Mass Parser
@author: U{Timothy Stranex<mailto:timothy@stranex.com>}
"""

import ode
import errors, node, joint, transform, geom

class Body(node.TreeNode):
    """
    Represents an ode.Body object and corresponds to the <body> tag.
    """

    def __init__(self, name, parent, attrs):
        node.TreeNode.__init__(self, name, parent)
        world = parent.getFirstAncestor(ode.World)
        self.setODEObject(ode.Body(world.getODEObject()))

        enabled = attrs.get('enabled', 'true')
        if (enabled not in ['true', 'false']):
            raise errors.InvalidError("Enabled attribute must be either 'true'"\
                                      " or 'false'.")
        else:
            if (enabled == 'false'):
                self.getODEObject().disable()

        gravitymode = int(attrs.get('gravitymode', 1))
        if (gravitymode == 0):
            self.getODEObject().setGravityMode(0)

        self._mass = None
        self._transformed = False

    def takeParser(self, parser):
        """
        Handle further parsing. It should be called immediately after the <body>
        tag has been encountered.
        """

        self._parser = parser
        self._parser.push(startElement=self._startElement,
                          endElement=self._endElement)

    def _applyTransform(self):
        if (self._transformed): return

        t = self.getTransform()

        body = self.getODEObject()
        body.setPosition(t.getPosition())
        body.setRotation(t.getRotation())

        self._transformed = True

    def _startElement(self, name, attrs):
        nodeName = attrs.get('name', None)

        if (name == 'transform'):
            t = transform.Transform()
            t.takeParser(self._parser, self, attrs)
        else:
            self._applyTransform()

        if (name == 'torque'):
            self.getODEObject().setTorque(self._parser.parseVector(attrs))
        elif (name == 'force'):
            self.getODEObject().setForce(self._parser.parseVector(attrs))
        elif (name == 'finiteRotation'):
            mode = int(attrs['mode'])

            try:
                axis = (float(attrs['xaxis']),
                        float(attrs['yaxis']),
                        float(attrs['zaxis']))
            except KeyError:
                raise errors.InvalidError('finiteRotation element must have' \
                                          ' xaxis, yaxis and zaxis attributes')

            if (mode not in [0, 1]):
                raise errors.InvalidError('finiteRotation mode attribute must' \
                                          ' be either 0 or 1.')

            self.getODEObject().setFiniteRotationMode(mode)
            self.getODEObject().setFiniteRotationAxis(axis)
        elif (name == 'linearVel'):
            self.getODEObject().setLinearVel(self._parser.parseVector(attrs))
        elif (name == 'angularVel'):
            self.getODEObject().setAngularVel(self._parser.parseVector(attrs))
        elif (name == 'mass'):
            self._mass = Mass(nodeName, self)
            self._mass.takeParser(self._parser)
        elif (name == 'joint'):
            j = joint.Joint(nodeName, self)
            j.takeParser(self._parser)
        elif (name == 'body'):
            b = Body(nodeName, self, attrs)
            b.takeParser(self._parser)
        elif (name == 'geom'):
            g = geom.Geom(nodeName, self)
            g.takeParser(self._parser)
        elif (name == 'transform'): # so it doesn't raise ChildError
            pass
        else:
            raise errors.ChildError('body', name)

    def _endElement(self, name):
        if (name == 'body'):
            self._parser.pop()

            self._applyTransform()
            if (self._mass is not None):
                self.getODEObject().setMass(self._mass.getODEObject())

class Mass(node.TreeNode):
    """
    Represents an ode.Mass object and corresponds to the <mass> tag.
    """

    def __init__(self, name, parent):
        node.TreeNode.__init__(self, name, parent)

        mass = ode.Mass()
        mass.setSphere(1.0, 1.0)
        self.setODEObject(mass)

        body = self.getFirstAncestor(ode.Body)
        body.getODEObject().setMass(mass)

    def takeParser(self, parser):
        """
        Handle further parsing. It should be called immediately after the <mass>
        tag is encountered.
        """

        self._parser = parser
        self._parser.push(startElement=self._startElement,
                          endElement=self._endElement)

    def _startElement(self, name, attrs):
        nodeName = attrs.get('name', None)

        if (name == 'mass_struct'):
            pass
        elif (name == 'mass_shape'):
            self._parseMassShape(attrs)
        elif (name == 'transform'):
            # parse transform
            pass
        elif (name == 'adjust'):
            total = float(attrs['total'])
            self.getODEObject().adjust(total)
        elif (name == 'mass'):
            mass = Mass(nodeName, self)
            mass.takeParser(self._parser)
        else:
            raise errors.ChildError('mass', name)

    def _endElement(self, name):
        if (name == 'mass'):
            try:
                mass = self.getFirstAncestor(ode.Mass)
            except node.AncestorNotFoundError:
                pass
            else:
                mass.getODEObject().add(self.getODEObject())
            self._parser.pop()

    def _parseMassShape(self, attrs):
        density = attrs.get('density', None)
        mass = self.getODEObject()

        def start(name, attrs):
            if (name == 'sphere'):
                radius = float(attrs.get('radius', 1.0))
                if (density is not None):
                    mass.setSphere(float(density), radius)
            elif (name == 'box'):
                lx = float(attrs['sizex'])
                ly = float(attrs['sizey'])
                lz = float(attrs['sizez'])
                if (density is not None):
                    mass.setBox(float(density), lx, ly, lz)
            elif (name == 'cappedCylinder'):
                radius = float(attrs.get('radius', 1.0))
                length = float(attrs['length'])
                if (density is not None):
                    mass.setCappedCylinder(float(density), 3, radius, length)
            elif (name == 'cylinder'):
                radius = float(attrs.get('radius', 1.0))
                length = float(attrs['length'])
                if (density is not None):
                    mass.setCylinder(float(density), 3, radius, length)
            else:
                # FIXME: Implement remaining mass shapes.
                raise NotImplementedError()

        def end(name):
            if (name == 'mass_shape'):
                self._parser.pop()

        self._parser.push(startElement=start, endElement=end)


########NEW FILE########
__FILENAME__ = geom
#@PydevCodeAnalysisIgnore

######################################################################
# Python Open Dynamics Engine Wrapper
# Copyright (C) 2004 PyODE developers (see file AUTHORS)
# All rights reserved.
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of EITHER:
#   (1) The GNU Lesser General Public License as published by the Free
#       Software Foundation; either version 2.1 of the License, or (at
#       your option) any later version. The text of the GNU Lesser
#       General Public License is included with this library in the
#       file LICENSE.
#   (2) The BSD-style license that is included with this library in
#       the file LICENSE-BSD.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the files
# LICENSE and LICENSE-BSD for more details.
######################################################################

# XODE Importer for PyODE

"""
XODE Geom Parser
@author: U{Timothy Stranex<mailto:timothy@stranex.com>}
"""

import ode
import errors, node, joint, body, transform

class Geom(node.TreeNode):
    """
    Represents an C{ode.Geom} object and corresponds to the <geom> tag.
    """

    def __init__(self, name, parent):
        node.TreeNode.__init__(self, name, parent)

        self._space = self.getFirstAncestor(ode.SpaceBase).getODEObject()
        self._transformed = False

        try:
            self._body = self.getFirstAncestor(ode.Body)
        except node.AncestorNotFoundError:
            self._body = None

    def takeParser(self, parser):
        """
        Handle further parsing. It should be called immediately after the <geom>
        tag has been encountered.
        """

        self._parser = parser
        self._parser.push(startElement=self._startElement,
                          endElement=self._endElement)

    def _startElement(self, name, attrs):
        nodeName = attrs.get('name', None)

        if (name == 'transform'):
            t = transform.Transform()
            t.takeParser(self._parser, self, attrs)
            self._transformed = True
        elif (name == 'box'):
            self._parseGeomBox(attrs)
        elif (name == 'cappedCylinder'):
            self._parseGeomCCylinder(attrs)
        elif (name == 'cone'):
            raise NotImplementedError()
        elif (name == 'cylinder'):
            self._parseGeomCylinder(attrs)
        elif (name == 'plane'):
            self._parseGeomPlane(attrs)
        elif (name == 'ray'):
            self._parseGeomRay(attrs)
        elif (name == 'sphere'):
            self._parseGeomSphere(attrs)
        elif (name == 'trimesh'):
            self._parseTriMesh(attrs)
        elif (name == 'geom'):
            g = Geom(nodeName, self)
            g.takeParser(self._parser)
        elif (name == 'body'):
            b = body.Body(nodeName, self, attrs)
            b.takeParser(self._parser)
        elif (name == 'joint'):
            j = joint.Joint(nodeName, self)
            j.takeParser(self._parser)
        elif (name == 'jointgroup'):
            pass
        elif (name == 'ext'):
            pass
        else:
            raise errors.ChildError('geom', name)

    def _endElement(self, name):
        if (name == 'geom'):
            obj = self.getODEObject()

            if (obj is None):
                raise errors.InvalidError('No geom type element found.')

            self._parser.pop()

    def _setObject(self, kclass, **kwargs):
        """
        Create the Geom object and apply transforms. Only call for placeable
        Geoms.
        """

        if (self._body is None):
            # The Geom is independant so it can have its own transform

            kwargs['space'] = self._space
            obj = kclass(**kwargs)

            t = self.getTransform()
            obj.setPosition(t.getPosition())
            obj.setRotation(t.getRotation())

            self.setODEObject(obj)

        elif (self._transformed):
            # The Geom is attached to a body so to transform it, it must
            # by placed in a GeomTransform and its transform is relative
            # to the body.

            kwargs['space'] = None
            obj = kclass(**kwargs)

            t = self.getTransform(self._body)
            obj.setPosition(t.getPosition())
            obj.setRotation(t.getRotation())

            trans = ode.GeomTransform(self._space)
            trans.setGeom(obj)
            trans.setBody(self._body.getODEObject())

            self.setODEObject(trans)
        else:
            kwargs['space'] = self._space
            obj = kclass(**kwargs)
            obj.setBody(self._body.getODEObject())
            self.setODEObject(obj)

    def _parseGeomBox(self, attrs):
        def start(name, attrs):
            if (name == 'ext'):
                pass
            else:
                raise errors.ChildError('box', name)

        def end(name):
            if (name == 'box'):
                self._parser.pop()

        lx = float(attrs['sizex'])
        ly = float(attrs['sizey'])
        lz = float(attrs['sizez'])

        self._setObject(ode.GeomBox, lengths=(lx, ly, lz))
        self._parser.push(startElement=start, endElement=end)

    def _parseGeomCCylinder(self, attrs):
        def start(name, attrs):
            if (name == 'ext'):
                pass
            else:
                raise errors.ChildError('cappedCylinder', name)

        def end(name):
            if (name == 'cappedCylinder'):
                self._parser.pop()

        radius = float(attrs['radius'])
        length = float(attrs['length'])

        self._setObject(ode.GeomCCylinder, radius=radius, length=length)
        self._parser.push(startElement=start, endElement=end)

    def _parseGeomCylinder(self, attrs):
        def start(name, attrs):
            if (name == 'ext'):
                pass
            else:
                raise errors.ChildError('cylinder', name)

        def end(name):
            if (name == 'cylinder'):
                self._parser.pop()

        radius = float(attrs['radius'])
        length = float(attrs['length'])

        self._setObject(ode.GeomCylinder, radius=radius, length=length)
        self._parser.push(startElement=start, endElement=end)

    def _parseGeomSphere(self, attrs):
        def start(name, attrs):
            if (name == 'ext'):
                pass
            else:
                raise errors.ChildError('sphere', name)

        def end(name):
            if (name == 'sphere'):
                self._parser.pop()

        radius = float(attrs['radius'])

        self._setObject(ode.GeomSphere, radius=radius)
        self._parser.push(startElement=start, endElement=end)

    def _parseGeomPlane(self, attrs):
        def start(name, attrs):
            if (name == 'ext'):
                pass
            else:
                raise errors.ChildError('plane', name)

        def end(name):
            if (name == 'plane'):
                self._parser.pop()

        a = float(attrs['a'])
        b = float(attrs['b'])
        c = float(attrs['c'])
        d = float(attrs['d'])

        self.setODEObject(ode.GeomPlane(self._space, (a, b, c), d))
        self._parser.push(startElement=start, endElement=end)

    def _parseGeomRay(self, attrs):
        def start(name, attrs):
            if (name == 'ext'):
                pass
            else:
                raise errors.ChildError('ray', name)

        def end(name):
            if (name == 'ray'):
                self._parser.pop()

        length = float(attrs['length'])

        self.setODEObject(ode.GeomRay(self._space, length))
        self._parser.push(startElement=start, endElement=end)

    def _parseTriMesh(self, attrs):
        vertices = []
        triangles = []

        def start(name, attrs):
            if (name == 'vertices'):
                pass
            elif (name == 'triangles'):
                pass
            elif (name == 'v'):
                vertices.append(self._parser.parseVector(attrs))
            elif (name == 't'):
                tri = int(attrs['ia']) - 1, int(attrs['ib']) - 1, int(attrs['ic']) - 1
                triangles.append(tri)
            else:
                raise errors.ChildError('trimesh', name)

        def end(name):
            if (name == 'trimesh'):
                data = ode.TriMeshData()
                data.build(vertices, triangles)
                self._setObject(ode.GeomTriMesh, data=data)
                self._parser.pop()

        self._parser.push(startElement=start, endElement=end)


########NEW FILE########
__FILENAME__ = renderer
# obsolete - should be deleted if there are no objections.

__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from pybrain.utilities import abstractMethod
import threading

class Renderer(threading.Thread):
    """ The general interface for a class displays what is happening in an environment.
        The renderer is executed as concurrent thread. Start the renderer with the function
        start() inherited from Thread, and check with isAlive(), if the thread is running.
    """

    def __init__(self):
        """ initializes some variables and parent init functions """
        threading.Thread.__init__(self)

    def updateData(self):
        """ overwrite this class to update whatever data the renderer needs to display the current
            state of the world. """
        abstractMethod()

    def _render(self):
        """ Here, the render methods are called. This function has to be implemented by subclasses. """
        abstractMethod()

    def start(self):
        """ wrapper for Thread.start(). only calls start if thread has not been started before. """
        if not self.isAlive():
            threading.Thread.start(self)

    def run(self):
        """ Don't call this function on its own. Use start() instead. """
        self._render()

    def stop(self):
        """ stop signal requested. stop current thread.
            @note: only if possible. OpenGL glutMainLoop is not stoppable.
        """
        pass


########NEW FILE########
__FILENAME__ = serverinterface
__author__ = 'Frank Sehnke, sehnke@in.tum.de'

from environment import Environment

class GraphicalEnvironment(Environment):
    """ Special type of environment that has graphical output and therefore needs a renderer.
    """

    def __init__(self):
        self.renderInterface = None

    def setRenderInterface(self, renderer):
        """ set the renderer, which is an object of or inherited from class Renderer.

            :arg renderer: The renderer that should display the Environment
            :type renderer: L{Renderer}
            .. seealso:: :class:`Renderer`
        """
        self.renderInterface = renderer

    def getRenderInterface(self):
        """ returns the current renderer.

            :return: the current renderer
            :rtype: L{Renderer}
        """
        return self.renderInterface

    def hasRenderInterface(self):
        """ tells you, if a Renderer has been set previously or not

            :return: True if a renderer was set, False otherwise
            :rtype: Boolean
        """
        return (self.getRenderInterface() != None)

########NEW FILE########
__FILENAME__ = northwardtask
__author__ = 'Martin Felder, felder@in.tum.de'

from pybrain.rl.environments import EpisodicTask
from shipsteer import ShipSteeringEnvironment


class GoNorthwardTask(EpisodicTask):

    """ The task of balancing some pole(s) on a cart """
    def __init__(self, env=None, maxsteps=1000):
        """
        :key env: (optional) an instance of a ShipSteeringEnvironment (or a subclass thereof)
        :key maxsteps: maximal number of steps (default: 1000)
        """
        if env == None:
            env = ShipSteeringEnvironment(render=False)
        EpisodicTask.__init__(self, env)
        self.N = maxsteps
        self.t = 0

        # scale sensors
        #                          [h,              hdot,           v]
        self.sensor_limits = [(-180.0, +180.0), (-180.0, +180.0), (-10.0, +40.0)]

        # actions:              thrust,       rudder
        self.actor_limits = [(-1.0, +2.0), (-90.0, +90.0)]
        # scale reward over episode, such that max. return = 100
        self.rewardscale = 100. / maxsteps / self.sensor_limits[2][1]

    def reset(self):
        EpisodicTask.reset(self)
        self.t = 0

    def performAction(self, action):
        self.t += 1
        EpisodicTask.performAction(self, action)

    def isFinished(self):
        if self.t >= self.N:
            # maximal timesteps
            return True
        return False

    def getReward(self):
        if abs(self.env.getHeading()) < 5.:
            return self.env.getSpeed() * self.rewardscale
        else:
            return 0

    def setMaxLength(self, n):
        self.N = n


########NEW FILE########
__FILENAME__ = shipsteer
__author__ = 'Martin Felder, felder@in.tum.de'

from scipy import random
from pybrain.tools.networking.udpconnection import UDPServer
import threading
from pybrain.utilities import threaded
from time import sleep

from pybrain.rl.environments.environment import Environment


class ShipSteeringEnvironment(Environment):
    """
    Simulates an ocean going ship with substantial inertia in both forward
    motion and rotation, plus noise.

    State space (continuous):
        h       heading of ship in degrees (North=0)
        hdot    angular velocity of heading in degrees/minute
        v       velocity of ship in knots
    Action space (continuous):
        rudder  angle of rudder
        thrust  propulsion of ship forward
    """

    # some (more or less) physical constants
    dt = 4.        # simulated time (in seconds) per step
    mass = 1000.   # mass of ship in unclear units
    I = 1000.      # rotational inertia of ship in unclear units

    def __init__(self, render=True, ip="127.0.0.1", port="21580", numdir=1):
        # initialize the environment (randomly)
        self.action = [0.0, 0.0]
        self.delay = False
        self.numdir = numdir  # number of directions in which ship starts
        self.render = render
        if self.render:
            self.updateDone = True
            self.updateLock = threading.Lock()
            self.server = UDPServer(ip, port)
        self.reset()

    def step(self):
        """ integrate state using simple rectangle rule """
        thrust = float(self.action[0])
        rudder = float(self.action[1])
        h, hdot, v = self.sensors
        rnd = random.normal(0, 1.0, size=3)

        thrust = min(max(thrust, -1), +2)
        rudder = min(max(rudder, -90), +90)
        drag = 5 * h + (rudder ** 2 + rnd[0])
        force = 30.0 * thrust - 2.0 * v - 0.02 * v * drag + rnd[1] * 3.0
        v = v + self.dt * force / self.mass
        v = min(max(v, -10), +40)
        torque = -v * (rudder + h + 1.0 * hdot + rnd[2] * 10.)
        last_hdot = hdot
        hdot += torque / self.I
        hdot = min(max(hdot, -180), 180)
        h += (hdot + last_hdot) / 2.0
        if h > 180.:
            h -= 360.
        elif h < -180.:
            h += 360.
        self.sensors = (h, hdot, v)

    def closeSocket(self):
        self.server.UDPInSock.close()
        sleep(10)

    def reset(self):
        """ re-initializes the environment, setting the ship to rest at a random orientation.
        """
        #               [h,                           hdot, v]
        self.sensors = [random.uniform(-30., 30.), 0.0, 0.0]
        if self.render:
            if self.server.clients > 0:
                # If there are clients send them reset signal
                self.server.send(["r", "r", "r"])

    def getHeading(self):
        """ auxiliary access to just the heading, to be used by GoNorthwardTask """
        return self.sensors[0]

    def getSpeed(self):
        """ auxiliary access to just the speed, to be used by GoNorthwardTask """
        return self.sensors[2]


    def getSensors(self):
        """ returns the state one step (dt) ahead in the future. stores the state in
            self.sensors because it is needed for the next calculation.
        """
        return self.sensors

    def performAction(self, action):
        """ stores the desired action for the next time step.
        """
        self.action = action
        self.step()
        if self.render:
            if self.updateDone:
                self.updateRenderer()
                if self.server.clients > 0:
                    sleep(0.2)

    @threaded()
    def updateRenderer(self):
        self.updateDone = False
        if not self.updateLock.acquire(False): return

        # Listen for clients
        self.server.listen()
        if self.server.clients > 0:
            # If there are clients send them the new data
            self.server.send(self.sensors)
        sleep(0.02)
        self.updateLock.release()
        self.updateDone = True

    @property
    def indim(self):
        return len(self.action)

    @property
    def outdim(self):
        return len(self.sensors)



########NEW FILE########
__FILENAME__ = viewer
__author__ = 'Frank Sehnke, sehnke@in.tum.de'

#@PydevCodeAnalysisIgnore
#########################################################################
# OpenGL viewer for the FlexCube Environment
#
# The FlexCube Environment is a Mass-Spring-System composed of 8 mass points.
# These resemble a cube with flexible edges.
#
# This viewer uses an UDP connection found in tools/networking/udpconnection.py
#
# The viewer recieves the position matrix of the 8 masspoints and the center of gravity.
# With this information it renders a Glut based 3d visualization of teh FlexCube
#
# Options:
# - serverIP: The ip of the server to which the viewer should connect
# - ownIP: The IP of the computer running the viewer
# - port: The starting port (2 adjacent ports will be used)
#
# Saving the images is possible by setting self.savePics=True.
# Changing the point and angle of view is possible by using the mouse
# while button 1 or 2 pressed.
#
# Requirements: OpenGL
#
#########################################################################

from OpenGL.GLUT import *
from OpenGL.GL import *
from OpenGL.GLE import *
from OpenGL.GLU import *
from time import sleep
from scipy import ones, array, cos, sin
from pybrain.tools.networking.udpconnection import UDPClient

class FlexCubeRenderer(object):
    #Options: ServerIP(default:localhost), OwnIP(default:localhost), Port(default:21560)
    def __init__(self, servIP="127.0.0.1", ownIP="127.0.0.1", port="21580"):
        self.oldScreenValues = None
        self.view = 0
        self.worldRadius = 400

        # Start of mousepointer
        self.lastx = 0
        self.lasty = 15
        self.lastz = 300
        self.zDis = 1

        # Start of cube
        self.cube = [0.0, 0.0, 0.0]
        self.bmpCount = 0
        self.actCount = 0
        self.calcPhysics = 0
        self.newPic = 1
        self.picCount = 0
        self.sensors = [0.0, 0.0, 0.0]
        self.centerOfGrav = array([0.0, 5.0, 0.0])

        self.savePics = False
        self.drawCounter = 0
        self.fps = 50
        self.dt = 1.0 / float(self.fps)
        self.step = 0

        self.client = UDPClient(servIP, ownIP, port)

    # If self.savePics=True this method saves the produced images
    def saveTo(self, filename, format="JPEG"):
        import Image # get PIL's functionality...
        width, height = 800, 600
        glPixelStorei(GL_PACK_ALIGNMENT, 1)
        data = glReadPixels(0, 0, width, height, GL_RGB, GL_UNSIGNED_BYTE)
        image = Image.fromstring("RGB", (width, height), data)
        image = image.transpose(Image.FLIP_TOP_BOTTOM)
        image.save(filename, format)
        print('Saved image to ', filename)
        return image

    # the render method containing the Glut mainloop
    def _render(self):
        # Call init: Parameter(Window Position -> x, y, height, width)
        self.init_GL(self, 300, 300, 800, 600)
        self.quad = gluNewQuadric()
        glutMainLoop()

    # The Glut idle function
    def drawIdleScene(self):
        #recive data from server and update the points of the cube
        try: self.sensors = self.client.listen(self.sensors)
        except: pass
        if self.sensors == ["r", "r", "r"]: self.centerOfGrav = array([0.0, 5.0, 0.0])
        else:
            self.step += 1
            a = self.sensors[0] / 360.0 * 3.1428
            dir = array([cos(a), 0.0, -sin(a)])
            self.centerOfGrav += self.sensors[2] * dir * 0.02
            self.drawScene()
        if self.savePics:
            self.saveTo("./screenshots/image_jump" + repr(10000 + self.picCount) + ".jpg")
            self.picCount += 1
        else: sleep(self.dt)

    def drawScene(self):
        ''' This methode describes the complete scene.'''
        # clear the buffer
        if self.zDis < 10: self.zDis += 0.25
        if self.lastz > 100: self.lastz -= self.zDis
        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)
        glLoadIdentity()

        # Point of view
        glRotatef(self.lastx, 0.0, 1.0, 0.0)
        glRotatef(self.lasty, 1.0, 0.0, 0.0)
        #glRotatef(15, 0.0, 0.0, 1.0)
        # direction of view is aimed to the center of gravity of the cube
        glTranslatef(-self.centerOfGrav[0], -self.centerOfGrav[1] - 50.0, -self.centerOfGrav[2] - self.lastz)

        #Objects

        #Massstab
        for lk in range(41):
            if float(lk - 20) / 10.0 == (lk - 20) / 10:
                glColor3f(0.75, 0.75, 0.75)
                glPushMatrix()
                glRotatef(90, 1, 0, 0)
                glTranslate(self.worldRadius / 40.0 * float(lk) - self.worldRadius / 2.0, -40.0, -30)
                quad = gluNewQuadric()
                gluCylinder(quad, 2, 2, 60, 4, 1)
                glPopMatrix()
            else:
                if float(lk - 20) / 5.0 == (lk - 20) / 5:
                    glColor3f(0.75, 0.75, 0.75)
                    glPushMatrix()
                    glRotatef(90, 1, 0, 0)
                    glTranslate(self.worldRadius / 40.0 * float(lk) - self.worldRadius / 2.0, -40.0, -15.0)
                    quad = gluNewQuadric()
                    gluCylinder(quad, 1, 1, 30, 4, 1)
                    glPopMatrix()
                else:
                    glColor3f(0.75, 0.75, 0.75)
                    glPushMatrix()
                    glRotatef(90, 1, 0, 0)
                    glTranslate(self.worldRadius / 40.0 * float(lk) - self.worldRadius / 2.0, -40.0, -7.5)
                    quad = gluNewQuadric()
                    gluCylinder(quad, 0.5, 0.5, 15, 4, 1)
                    glPopMatrix()

        # Floor
        tile = self.worldRadius / 40.0
        glEnable (GL_BLEND)
        glBlendFunc (GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA)

        glColor3f(0.8, 0.8, 0.5)
        glPushMatrix()
        glTranslatef(0.0, -3.0, 0.0)
        glBegin(GL_QUADS)
        glNormal(0.0, 1.0, 0.0)
        glVertex3f(-self.worldRadius, 0.0, -self.worldRadius)
        glVertex3f(-self.worldRadius, 0.0, self.worldRadius)
        glVertex3f(self.worldRadius, 0.0, self.worldRadius)
        glVertex3f(self.worldRadius, 0.0, -self.worldRadius)
        glEnd()
        glPopMatrix()

        #Water
        for xF in range(40):
            for yF in range(40):
                if float(xF + yF) / 2.0 == (xF + yF) / 2: glColor4f(0.7, 0.7, 1.0, 0.5)
                else: glColor4f(0.9, 0.9, 1.0, 0.5)
                glPushMatrix()
                glTranslatef(0.0, -0.03, 0.0)
                glBegin(GL_QUADS)
                glNormal(0.5 + sin(float(xF) + float(self.step) / 4.0) * 0.5, 0.5 + cos(float(xF) + float(self.step) / 4.0) * 0.5, 0.0)
                for i in range(2):
                    for k in range(2):
                        glVertex3f((i + xF - 20) * tile, sin(float(xF + i) + float(self.step) / 4.0) * 3.0, ((k ^ i) + yF - 20) * tile)
                glEnd()
                glPopMatrix()

        self.ship()
                  	
        # swap the buffer
        glutSwapBuffers()

    def ship(self):
        glColor3f(0.4, 0.1, 0.2)
        glPushMatrix()
        glTranslate(self.centerOfGrav[0] + 14, self.centerOfGrav[1], self.centerOfGrav[2])
        glRotatef(180 - self.sensors[0], 0.0, 1.0, 0.0)

        self.cuboid(0, 0, 0, 20, 5, 5)
        #bow of ship
        glBegin(GL_TRIANGLES)
        glNormal3fv(self.calcNormal(self.points2Vector([-5, 6, 2.5], [0, 5, 5]), self.points2Vector([-5, 6, 2.5], [0, 5, 0])))
        glVertex3f(-5, 6, 2.5), glVertex3f(0, 5, 0), glVertex3f(0, 5, 5)
        glNormal3fv(self.calcNormal(self.points2Vector([-5, 6, 2.5], [0, 0, 5]), self.points2Vector([-5, 6, 2.5], [0, 5, 5])))
        glVertex3f(-5, 6, 2.5), glVertex3f(0, 0, 5), glVertex3f(0, 5, 5)
        glNormal3fv(self.calcNormal(self.points2Vector([-5, 6, 2.5], [0, 0, 0]), self.points2Vector([-5, 6, 2.5], [0, 0, 5])))
        glVertex3f(-5, 6, 2.5), glVertex3f(0, 0, 5), glVertex3f(0, 0, 0)
        glNormal3fv(self.calcNormal(self.points2Vector([-5, 6, 2.5], [0, 5, 0]), self.points2Vector([-5, 6, 2.5], [0, 0, 0])))
        glVertex3f(-5, 6, 2.5), glVertex3f(0, 0, 0), glVertex3f(0, 5, 0)
        glEnd()
        # stern
        glPushMatrix()
        glRotatef(-90, 1.0, 0.0, 0.0)
        glTranslatef(15, -2.5, 0)
        gluCylinder(self.quad, 2.5, 2.5, 5, 10, 1)
        glTranslatef(0, 0, 5)
        gluDisk(self.quad, 0, 2.5, 10, 1)
        glPopMatrix()
        # deck
        if abs(self.sensors[0]) < 5.0: reward = (self.sensors[2] + 10.0) / 50.0
        else: reward = 0.2
        glColor3f(1.0 - reward, reward, 0)
        self.cuboid(5, 5, 1, 10, 8, 4)
        glPushMatrix()
        glRotatef(-90, 1.0, 0.0, 0.0)
        glTranslatef(13, -2.5, 5)
        glColor3f(1, 1, 1)
        gluCylinder(self.quad, 1, 0.8, 5, 20, 1)
        glPopMatrix()
        glPopMatrix()

    def cuboid(self, x0, y0, z0, x1, y1, z1):
        glBegin(GL_QUADS)
        glNormal(0, 0, 1)
        glVertex3f(x0, y0, z1); glVertex3f(x0, y1, z1); glVertex3f(x1, y1, z1); glVertex3f(x1, y0, z1)  #front
        glNormal(-1, 0, 0)
        glVertex3f(x0, y0, z0); glVertex3f(x0, y0, z1); glVertex3f(x0, y1, z1); glVertex3f(x0, y1, z0) # left
        glNormal(0, -1, 0)
        glVertex3f(x0, y0, z0); glVertex3f(x0, y0, z1); glVertex3f(x1, y0, z1); glVertex3f(x1, y0, z0) # bottom
        glNormal(0, 0, -1)
        glVertex3f(x0, y0, z0); glVertex3f(x1, y0, z0); glVertex3f(x1, y1, z0); glVertex3f(x0, y1, z0) # back
        glNormal(0, 1, 0)
        glVertex3f(x0, y1, z0); glVertex3f(x1, y1, z0); glVertex3f(x1, y1, z1); glVertex3f(x0, y1, z1) # top
        glNormal(1, 0, 0)
        glVertex3f(x1, y0, z0); glVertex3f(x1, y0, z1); glVertex3f(x1, y1, z1); glVertex3f(x1, y1, z0) # right
        glEnd()

    def calcNormal(self, xVector, yVector):
        result = [0, 0, 0]
        result[0] = xVector[1] * yVector[2] - yVector[1] * xVector[2]
        result[1] = -xVector[0] * yVector[2] + yVector[0] * xVector[2]
        result[2] = xVector[0] * yVector[1] - yVector[0] * xVector[1]
        return [result[0], result[1], result[2]]

    def points2Vector(self, startPoint, endPoint):
        result = [0, 0, 0]
        result[0] = endPoint[0] - startPoint[0]
        result[1] = endPoint[1] - startPoint[1]
        result[2] = endPoint[2] - startPoint[2]
        return [result[0], result[1], result[2]]

    def resizeScene(self, width, height):
        '''Needed if window size changes.'''
        if height == 0: # Prevent A Divide By Zero If The Window Is Too Small
            height = 1

        glViewport(0, 0, width, height) # Reset The Current Viewport And Perspective Transformation
        glMatrixMode(GL_PROJECTION)
        glLoadIdentity()
        gluPerspective(45.0, float(width) / float(height), 0.1, 700.0)
        glMatrixMode(GL_MODELVIEW)

    def activeMouse(self, x, y):
        #Returns mouse coordinates while any mouse button is pressed.
        # store the mouse coordinate
        if self.mouseButton == GLUT_LEFT_BUTTON:
            self.lastx = x - self.xOffset
            self.lasty = y - self.yOffset
        if self.mouseButton == GLUT_RIGHT_BUTTON:
            self.lastz = y - self.zOffset
        # redisplay
        glutPostRedisplay()

    def passiveMouse(self, x, y):
        '''Returns mouse coordinates while no mouse button is pressed.'''
        pass

    def completeMouse(self, button, state, x, y):
        #Returns mouse coordinates and which button was pressed resp. released.
        self.mouseButton = button
        if state == GLUT_DOWN:
            self.xOffset = x - self.lastx
            self.yOffset = y - self.lasty
            self.zOffset = y - self.lastz
        # redisplay
        glutPostRedisplay()

    #Initialise an OpenGL windows with the origin at x, y and size of height, width.
    def init_GL(self, pyWorld, x, y, height, width):
        # initialize GLUT
        glutInit([])

        glutInitDisplayMode(GLUT_DOUBLE | GLUT_RGBA | GLUT_DEPTH)
        glutInitWindowSize(height, width)
        glutInitWindowPosition(x, y)
        glutCreateWindow("The Curious Cube")
        glClearDepth(1.0)
        glEnable(GL_DEPTH_TEST)
        glClearColor(0.0, 0.0, 0.0, 0.0)
        glShadeModel(GL_SMOOTH)
        glMatrixMode(GL_MODELVIEW)
        # initialize lighting */
        glLightfv(GL_LIGHT0, GL_DIFFUSE, [1, 1, 1, 1.0])
        glLightModelfv(GL_LIGHT_MODEL_AMBIENT, [1.0, 1.0, 1.0, 1.0])
        glEnable(GL_LIGHTING)
        glEnable(GL_LIGHT0)
        #
        glColorMaterial(GL_FRONT, GL_DIFFUSE)
        glEnable(GL_COLOR_MATERIAL)
        # Automatic vector normalise
        glEnable(GL_NORMALIZE)

        ### Instantiate the virtual world ###
        glutDisplayFunc(pyWorld.drawScene)
        glutMotionFunc(pyWorld.activeMouse)
        glutMouseFunc(pyWorld.completeMouse)
        glutReshapeFunc(pyWorld.resizeScene)
        glutIdleFunc(pyWorld.drawIdleScene)

if __name__ == '__main__':
    s = sys.argv[1:]
    r = FlexCubeRenderer(*s)
    r._render()


########NEW FILE########
__FILENAME__ = environment
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from scipy import random, zeros
from pybrain.rl.environments import Environment


class SimpleEnvironment(Environment):
    def __init__(self, dim=1):
        Environment.__init__(self)
        self.dim = dim
        self.indim = dim
        self.outdim = dim
        self.noise = None
        self.reset()

    def setNoise(self, variance):
        self.noise = variance

    def getSensors(self):
        if not self.updated:
            self.update()
        return self.state

    def performAction(self, action):
        self.updated = False
        self.action = action

    def update(self):
        self.state = [s + 0.1 * a for s, a in zip(self.state, self.action)]
        if self.noise:
            self.state += random.normal(0, self.noise, self.dim)

    def f(self, x):
        return [v ** 2 for v in x]

    def reset(self):
        self.state = random.uniform(2, 2, self.dim)

        self.action = zeros(self.dim, float)
        self.updated = True



########NEW FILE########
__FILENAME__ = renderer
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from pylab import plot, figure, ion, Line2D, draw, arange
from pybrain.rl.environments.renderer import Renderer
import threading
import time


class SimpleRenderer(Renderer):
    def __init__(self):
        Renderer.__init__(self)

        self.dataLock = threading.Lock()
        self.stopRequest = False
        self.pathx = []
        self.pathy = []
        self.f = None
        self.min = -1
        self.max = 1
        self.fig = None
        self.color = 'red'

    def setFunction(self, f, rmin, rmax):
        self.dataLock.acquire()
        self.f = f
        self.min = rmin
        self.max = rmax
        self.dataLock.release()

    def updateData(self, data):
        self.dataLock.acquire()
        (x, y) = data
        self.pathx.append(x)
        self.pathy.append(y)
        self.dataLock.release()

    def reset(self):
        self.dataLock.acquire()
        self.pathx = []
        self.pathy = []
        self.dataLock.release()

    def stop(self):
        self.dataLock.acquire()
        self.stopRequest = True
        self.dataLock.release()

    def start(self):
        self.drawPlot()
        Renderer.start(self)

    def drawPlot(self):
        ion()
        self.fig = figure()
        axes = self.fig.add_subplot(111)

        # draw function
        xvalues = arange(self.min, self.max, 0.1)
        yvalues = map(self.f, xvalues)
        plot(xvalues, yvalues)

        # draw exploration path
        self.line = Line2D([], [], linewidth=3, color='red')
        axes.add_artist(self.line)
        self.line.set_clip_box(axes.bbox)

        # set axes limits
        axes.set_xlim(min(xvalues) - 0.5, max(xvalues) + 0.5)
        axes.set_ylim(min(yvalues) - 0.5, max(yvalues) + 0.5)

    def _render(self):
        while not self.stopRequest:
            self.dataLock.acquire()
            self.line.set_data(self.pathx, self.pathy)
            self.line.set_color(self.color)
            figure(self.fig.number)
            draw()
            self.dataLock.release()

            time.sleep(0.05)
        self.stopRequest = False


########NEW FILE########
__FILENAME__ = tasks
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from pybrain.rl.environments import EpisodicTask


class MinimizeTask(EpisodicTask):
    def __init__(self, environment):
        EpisodicTask.__init__(self, environment)
        self.N = 15
        self.t = 0
        self.state = [0.0] * environment.dim
        self.action = [0.0] * environment.dim

    def reset(self):
        EpisodicTask.reset(self)
        self.t = 0

    def isFinished(self):
        if self.t >= self.N:
            self.t = 0
            return True
        else:
            self.t += 1
            return False

    def getObservation(self):
        self.state = EpisodicTask.getObservation(self)
        return self.state

    def performAction(self, action):
        EpisodicTask.performAction(self, action)
        self.action = action

    def getReward(self):
        # sleep(0.01)
        # print(self.state, self.action)
        reward = self.env.f([s + 0.1 * a for s, a in zip(self.state, self.action)])
        return - sum(reward)


########NEW FILE########
__FILENAME__ = simplecontroller
__author__ = 'Julian Togelius, julian@idsia.ch'

from scipy import array

from pybrain.rl.agents.agent import Agent


class SimpleController(Agent):

    def integrateObservation(self, obs):
        self.speed = obs[0]
        self.angleToCurrentWP = obs[1]
        self.distanceToCurrentWP = obs[2]
        self.angleToNextWP = obs[3]
        self.distanceToNextWP = obs[4]
        self.angleToOtherVehicle = obs[5]
        self.distanceToOtherVehicle = obs[6]


    def getAction(self):
        if self.speed < 10:
            driving = 1
        else:
            driving = 0
        if self.angleToCurrentWP > 0:
            steering = -1
        else:
            steering = 1
        print("speed", self.speed, "angle", self.angleToCurrentWP, "driving", driving, "steering", steering)
        return array([driving, steering])


########NEW FILE########
__FILENAME__ = simpleracetask
__author__ = 'Julian Togelius, julian@idsia.ch'

from pybrain.rl.environments import EpisodicTask
from simpleracetcp import SimpleraceEnvironment

class SimpleraceTask(EpisodicTask):

    def getTotalReward(self):
        #score handled by environment?
        return self.environment.firstCarScore

    def getReward(self):
        return 0

    def isFinished(self):
        #this task can't really fail, a bad policy will just get a low score
        return False

    def setMaxLength(self, n):
        # I don't think this can be done
        pass

    def reset(self):
        EpisodicTask.reset(self)
        self.t = 0

    def performAction(self, action):
        self.t += 1
        EpisodicTask.performAction(self, action)

    def __init__(self):
        self.environment = SimpleraceEnvironment()
        EpisodicTask.__init__(self, self.environment)
        self.t = 0


########NEW FILE########
__FILENAME__ = simpleracetcp
__author__ = 'Julian Togelius, julian@idsia.ch'

from pybrain.rl.environments import Environment
from math import sqrt
import socket
import string
from scipy import zeros

class SimpleraceEnvironment(Environment):

    firstCarScore = 0
    secondCarScore = 0
    lastStepCurrentWp = [0, 0]
    lastStepNextWp = [0, 0]

    indim = 2
    outdim = 7

    def __init__(self, host="127.0.0.1", port=6524):
        self.theSocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.theSocket.connect((host, port))
        self.step = 0
        print("Connected to a simplerace server")
        self.reset()
        self.serverIsReady = False

    def getSensors(self):
        return self.sensors

    def performAction(self, action):
        # there is a nicer way of doing the following, but i'll wait with that until
        # i'm a bit more fluent in Python
        if (action[0] > 0.3):
            if(action[1]) > 0.3:
                command = 8
            elif(action[1]) < -0.3:
                command = 6
            else:
                command = 7
        elif (action[0] < -0.3):
            if(action[1]) > 0.3:
                command = 2
            elif(action[1]) < -0.3:
                command = 0
            else:
                command = 1
        else:
            if(action[1]) > 0.3:
                command = 5
            elif(action[1]) < -0.3:
                command = 3
            else:
                command = 4
        if self.waitOne:
            print('Waiting one step')
            self.waitOne = False
        elif self.serverIsReady:
            self.theSocket.send (str(command) + "\n")
        else:
            print("not sending")
        # get and process the answer
        data = ""
        while len (data) < 2:
            data = self.theSocket.recv(1000)
        #print("received", data)
        inputs = string.split(str(data), " ")
        if (inputs[0][:5] == "reset"):
            print("Should we reset the scores here?")
            self.reset ()
            self.serverIsReady = True
            self.waitOne = True
        elif (inputs[0] == "data"):
            inputs[2:20] = map(float, inputs[2:20])
            self.sensors = inputs[2:9]
            currentWp = [inputs[18], inputs[19]]
            # check that this is not the first step of an episode
            if (self.lastStepCurrentWp[0] != 0):
                # check if a way point position has changed
                if (currentWp[0] != self.lastStepCurrentWp[0]):
                    # check that we don't have a server side change of episode
                    if (currentWp[0] != self.lastStepNextWp[0]):
                        print("%.3f   %.3f   %.3f   %.3f   " % (currentWp[0], currentWp[1], self.lastStepNextWp[0], self.lastStepNextWp[1]))
                        raise Exception("Unexpected episode change")
                    else:
                        # all is fine, increase score. but for who?
                        ownPosition = [inputs[9], inputs[10]]
                        otherPosition = [inputs[14], inputs[15]]
                        if (self.euclideanDistance(ownPosition, self.lastStepCurrentWp) < self.euclideanDistance(otherPosition, self.lastStepCurrentWp)):
                            self.firstCarScore += 1
                        else:
                            self.secondCarScore += 1
            # store old way point positions
            self.lastStepCurrentWp = currentWp
            self.step += 1
        elif (len (inputs[0]) < 2):
            print("impossible!")
        else:
            print("incomprehensible and thus roundly ignored", data)

    def reset(self):
        self.step = 0
        self.firstCarScore = 0
        self.secondCarScore = 0
        self.lastStepCurrentWp = [0, 0]
        self.lastStepNextWp = [0, 0]
        self.sensors = zeros(self.outdim)
        self.waitOne = False

    def euclideanDistance(self, firstPoint, secondPoint):
        return sqrt ((firstPoint[0] - secondPoint[0]) ** 2 + (firstPoint[1] - secondPoint[1]) ** 2)

########NEW FILE########
__FILENAME__ = task
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import clip, asarray

from pybrain.utilities import abstractMethod

class Task(object):
    """ A task is associating a purpose with an environment. It decides how to evaluate the
    observations, potentially returning reinforcement rewards or fitness values.
    Furthermore it is a filter for what should be visible to the agent.
    Also, it can potentially act as a filter on how actions are transmitted to the environment. """

    def __init__(self, environment):
        """ All tasks are coupled to an environment. """
        self.env = environment

        # limits for scaling of sensors and actors (None=disabled)
        self.sensor_limits = None
        self.actor_limits = None
        self.clipping = True

    def setScaling(self, sensor_limits, actor_limits):
        """ Expects scaling lists of 2-tuples - e.g. [(-3.14, 3.14), (0, 1), (-0.001, 0.001)] -
            one tuple per parameter, giving min and max for that parameter. The functions
            normalize and denormalize scale the parameters between -1 and 1 and vice versa.
            To disable this feature, use 'None'. """
        self.sensor_limits = sensor_limits
        self.actor_limits = actor_limits

    def performAction(self, action):
        """ A filtered mapping towards performAction of the underlying environment. """
        if self.actor_limits:
            action = self.denormalize(action)
        self.env.performAction(action)

    def getObservation(self):
        """ A filtered mapping to getSensors of the underlying environment. """
        sensors = self.env.getSensors()
        if self.sensor_limits:
            sensors = self.normalize(sensors)
        return sensors

    def getReward(self):
        """ Compute and return the current reward (i.e. corresponding to the last action performed) """
        return abstractMethod()

    def normalize(self, sensors):
        """ The function scales the parameters to be between -1 and 1. e.g. [(-pi, pi), (0, 1), (-0.001, 0.001)] """
        assert(len(self.sensor_limits) == len(sensors))
        result = []
        for l, s in zip(self.sensor_limits, sensors):
            if not l:
                result.append(s)
            else:
                result.append((s - l[0]) / (l[1] - l[0]) * 2 - 1.0)
        if self.clipping:
            clip(result, -1, 1)
        return asarray(result)

    def denormalize(self, actors):
        """ The function scales the parameters from -1 and 1 to the given interval (min, max) for each actor. """
        assert(len(self.actor_limits) == len(actors))
        result = []
        for l, a in zip(self.actor_limits, actors):
            if not l:
                result.append(a)
            else:
                r = (a + 1.0) / 2 * (l[1] - l[0]) + l[0]
                if self.clipping:
                    r = clip(r, l[0], l[1])
                result.append(r)

        return result

    @property
    def indim(self):
        return self.env.indim

    @property
    def outdim(self):
        return self.env.outdim



########NEW FILE########
__FILENAME__ = capturegame
__author__ = 'Tom Schaul, tom@idsia.ch'

from random import choice
from scipy import zeros

from twoplayergame import TwoPlayerGame


# TODO: undo operation


class CaptureGame(TwoPlayerGame):
    """ the capture game is a simplified version of the Go game: the first player to capture a stone wins!
    Pass moves are forbidden."""
    # CHECKME: suicide allowed?

    BLACK = 1
    WHITE = -1
    EMPTY = 0

    startcolor = BLACK

    def __init__(self, size, suicideenabled=True):
        """ the size of the board is generally between 3 and 19. """
        self.size = size
        self.suicideenabled = suicideenabled
        self.reset()

    def _iterPos(self):
        """ an iterator over all the positions of the board. """
        for i in range(self.size):
            for j in range(self.size):
                yield (i, j)

    def reset(self):
        """ empty the board. """
        TwoPlayerGame.reset(self)
        self.movesDone = 0
        self.b = {}
        for p in self._iterPos():
            self.b[p] = self.EMPTY
        # which stone belongs to which group
        self.groups = {}
        # how many liberties does each group have
        self.liberties = {}

    @property
    def indim(self):
        return self.size ** 2

    @property
    def outdim(self):
        return 2 * self.size ** 2

    def getBoardArray(self):
        """ an array with two boolean values per position, indicating
        'white stone present' and 'black stone present' respectively. """
        a = zeros(self.outdim)
        for i, p in enumerate(self._iterPos()):
            if self.b[p] == self.WHITE:
                a[2 * i] = 1
            elif self.b[p] == self.BLACK:
                a[2 * i + 1] = 1
        return a

    def isLegal(self, c, pos):
        if pos not in self.b:
            return False
        elif self.b[pos] != self.EMPTY:
            return False
        elif not self.suicideenabled:
            return not self._suicide(c, pos)
        return True

    def doMove(self, c, pos):
        """ the action is a (color, position) tuple, for the next stone to move.
        returns True if the move was legal. """
        self.movesDone += 1
        if pos == 'resign':
            self.winner = -c
            return True
        elif not self.isLegal(c, pos):
            return False
        elif self._suicide(c, pos):
            assert self.suicideenabled
            self.b[pos] = 'y'
            self.winner = -c
            return True
        elif self._capture(c, pos):
            self.winner = c
            self.b[pos] = 'x'
            return True
        else:
            self._setStone(c, pos)
            return True

    def getSensors(self):
        """ just a list of the board position states. """
        return map(lambda x: x[1], sorted(self.b.items()))

    def __str__(self):
        s = ''
        for i in range(self.size):
            for j in range(self.size):
                val = self.b[(i, j)]
                if val == self.EMPTY: s += ' .'
                elif val == self.BLACK: s += ' X'
                elif val == self.WHITE: s += ' O'
                else: s += ' ' + str(val)
            s += '\n'
        if self.winner:
            if self.winner == self.BLACK:
                w = 'Black (#)'
            elif self.winner == self.WHITE:
                w = 'White (*)'
            else:
                w = self.winner
            s += 'Winner: ' + w
            s += ' (moves done:' + str(self.movesDone) + ')\n'
        return s

    def _neighbors(self, pos):
        """ the 4 neighboring positions """
        res = []
        if pos[1] < self.size - 1: res.append((pos[0], pos[1] + 1))
        if pos[1] > 0: res.append((pos[0], pos[1] - 1))
        if pos[0] < self.size - 1: res.append((pos[0] + 1, pos[1]))
        if pos[0] > 0: res.append((pos[0] - 1, pos[1]))
        return res

    def _setStone(self, c, pos):
        """ set stone, and update liberties and groups. """
        self.b[pos] = c
        merge = False
        self.groups[pos] = self.size * pos[0] + pos[1]
        freen = filter(lambda n: self.b[n] == self.EMPTY, self._neighbors(pos))
        self.liberties[self.groups[pos]] = set(freen)
        for n in self._neighbors(pos):
            if self.b[n] == -c:
                self.liberties[self.groups[n]].difference_update([pos])
            elif self.b[n] == c:
                if merge:
                    newg = self.groups[pos]
                    oldg = self.groups[n]
                    if newg == oldg:
                        self.liberties[newg].difference_update([pos])
                    else:
                        # merging 2 groups
                        for p in self.groups.keys():
                            if self.groups[p] == oldg:
                                self.groups[p] = newg
                        self.liberties[newg].update(self.liberties[oldg])
                        self.liberties[newg].difference_update([pos])
                        del self.liberties[oldg]
                else:
                    # connect to this group
                    del self.liberties[self.groups[pos]]
                    self.groups[pos] = self.groups[n]
                    self.liberties[self.groups[n]].update(freen)
                    self.liberties[self.groups[n]].difference_update([pos])
                    merge = True

    def _suicide(self, c, pos):
        """ would putting a stone here be suicide for c? """
        # any free neighbors?
        for n in self._neighbors(pos):
            if self.b[n] == self.EMPTY:
                return False

        # any friendly neighbor with extra liberties?
        for n in self._neighbors(pos):
            if self.b[n] == c:
                if len(self.liberties[self.groups[n]]) > 1:
                    return False

        # capture all surrounding ennemies?
        if self._capture(c, pos):
            return False

        return True

    def _capture(self, c, pos):
        """ would putting a stone here lead to a capture? """
        for n in self._neighbors(pos):
            if self.b[n] == -c:
                if len(self.liberties[self.groups[n]]) == 1:
                    return True
        return False

    def getLiberties(self, pos):
        """ how many liberties does the stone at pos have? """
        if self.b[pos] == self.EMPTY:
            return None
        return len(self.liberties[self.groups[pos]])

    def getGroupSize(self, pos):
        """ what size is the worm that this stone is part of? """
        if self.b[pos] == self.EMPTY:
            return None
        g = self.groups[pos]
        return len(filter(lambda x: x == g, self.groups.values()))

    def getLegals(self, c):
        """ return all the legal positions for a color """
        return filter(lambda p: self.b[p] == self.EMPTY, self._iterPos())

    def getAcceptable(self, c):
        """ return all legal positions for a color that don't commit suicide. """
        return filter(lambda p: not self._suicide(c, p), self.getLegals(c))

    def getKilling(self, c):
        """ return all legal positions for a color that immediately kill the opponent. """
        return filter(lambda p: self._capture(c, p), self.getAcceptable(c))

    def randomBoard(self, nbmoves):
        """ produce a random, undecided and legal capture-game board, after at most nbmoves.
        :return: the number of moves actually done. """
        c = self.BLACK
        self.reset()
        for i in range(nbmoves):
            l = set(self.getAcceptable(c))
            l.difference_update(self.getKilling(c))
            if len(l) == 0:
                return i
            self._setStone(c, choice(list(l)))
            c = -c
        return nbmoves

    def giveHandicap(self, h, color=BLACK):
        i = 0
        for pos in self._handicapIterator():
            i += 1
            if i > h:
                return
            if self.isLegal(color, pos):
                self._setStone(color, pos)

    def _handicapIterator(self):
        s = self.size
        assert s > 2
        yield (1, 1)
        if s > 3:
            # 4 corners
            yield (s - 2, s - 2)
            yield (1, s - 2)
            yield (s - 2, 1)
        if s > 4:
            for i in range(2, s - 2):
                yield (i, 1)
                yield (i, s - 2)
                yield (1, i)
                yield (s - 2, i)

    def playToTheEnd(self, p1, p2):
        """ alternate playing moves between players until the game is over. """
        assert p1.color == -p2.color
        i = 0
        p1.game = self
        p2.game = self
        players = [p1, p2]
        while not self.gameOver():
            p = players[i]
            self.performAction(p.getAction())
            i = (i + 1) % 2


########NEW FILE########
__FILENAME__ = captureplayer
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.rl.agents.agent import Agent
from pybrain.rl.environments.twoplayergames import CaptureGame


class CapturePlayer(Agent):
    """ a class of agent that can play the capture game, i.e. provides actions in the format:
    (playerid, position)
    playerid is self.color, by convention.
    It generally also has access to the game object. """
    def __init__(self, game, color = CaptureGame.BLACK, **args):
        self.game = game
        self.color = color
        self.setArgs(**args)



########NEW FILE########
__FILENAME__ = clientwrapper
__author__ = 'Tom Schaul, tom@idsia.ch'

import socket

from captureplayer import CapturePlayer
from pybrain.rl.environments.twoplayergames import CaptureGame

# TODO: allow partially forced random moves.

class ClientCapturePlayer(CapturePlayer):
    """ A wrapper class for using external code to play the capture game,
    interacting via a TCP socket. """

    verbose = False

    def __init__(self, game, color=CaptureGame.BLACK, player='AtariGreedy', **args):
        '''player: AtariGreedy, AtariMinMax, AtariAlphaBeta possible'''
        CapturePlayer.__init__(self, game, color, **args)
        # build connection
        host = "127.0.0.1"
        port = 6524
        self.player = player
        try:
            self.theSocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            self.theSocket.connect((host, port))
            if self.verbose:
                print("Connected to server")
        except:
            print('Failed to connect')

        #define player
        self.theSocket.send(player + '-' + str(color) + '\n')
        if self.verbose:
            print('Sending:', player + '-' + str(color))
        accept = ""
        while len (accept) < 2:
            accept = self.theSocket.recv(1000)
        assert accept == 'OK'


    def getAction(self):
        # build a java string
        if self.color == CaptureGame.BLACK:
            js = '1-'
        else:
            js = '2-'
        for i, p in enumerate(self.game._iterPos()):
            if i % self.game.size == 0:
                js += '-'
            if self.game.b[p] == CaptureGame.BLACK:
                js += '1'
            elif self.game.b[p] == CaptureGame.WHITE:
                js += '2'
            else:
                js += '0'

        # get the suggested move from the java player:
        if self.verbose:
            print('Sending:', js)
        self.theSocket.send(js + '\n')
        jr = ""
        if self.verbose:
            print('Waiting for server',)
        while len (jr) < 2:
            jr = self.theSocket.recv(1000)
            if self.verbose:
                print('.',)
        if self.verbose:
            print(" received.", jr)

        chosen = eval(jr)
        assert self.game.isLegal(self.color, chosen)
        return [self.color, chosen]



########NEW FILE########
__FILENAME__ = killing
__author__ = 'Tom Schaul, tom@idsia.ch'

from random import choice

from nonsuicide import NonSuicidePlayer


class KillingPlayer(NonSuicidePlayer):
    """ do random moves, but always instant-kill if possible,
    and never suicide, if avoidable. """
    def getAction(self):
        p = self.game.getKilling(self.color)
        if len(p) > 0:
            return [self.color, choice(p)]
        else:
            return NonSuicidePlayer.getAction(self)
########NEW FILE########
__FILENAME__ = moduledecision
__author__ = 'Tom Schaul, tom@idsia.ch'


from scipy import zeros, ones

from pybrain.rl.environments.twoplayergames import CaptureGame
from randomplayer import RandomCapturePlayer
from pybrain.utilities import drawGibbs


class ModuleDecidingPlayer(RandomCapturePlayer):
    """ A Capture-game player that plays according to the rules, but choosing its moves
    according to the output of a module that takes as input the current state of the board. """

    greedySelection = False

    # if the selection is not greedy, use Gibbs-sampling with this temperature
    temperature = 1.

    def __init__(self, module, *args, **kwargs):
        RandomCapturePlayer.__init__(self, *args, **kwargs)
        self.module = module
        if self.greedySelection:
            self.temperature = 0.

    def getAction(self):
        """ get suggested action, return them if they are legal, otherwise choose randomly. """
        ba = self.game.getBoardArray()
        # network is given inputs with self/other as input, not black/white
        if self.color != CaptureGame.BLACK:
            # invert values
            tmp = zeros(len(ba))
            tmp[:len(ba)-1:2] = ba[1:len(ba):2]
            tmp[1:len(ba):2] = ba[:len(ba)-1:2]
            ba = tmp
        self.module.reset()
        return [self.color, self._legalizeIt(self.module.activate(ba))]

    def newEpisode(self):
        self.module.reset()

    def _legalizeIt(self, a):
        """ draw index from an array of values, filtering out illegal moves. """
        if not min(a) >= 0:
            print(a)
            print(min(a))
            print(self.module.params)
            print(self.module.inputbuffer)
            print(self.module.outputbuffer)
            raise Exception('Non-positive value in array?')
        legals = self.game.getLegals(self.color)
        vals = ones(len(a))*(-100)*(1+self.temperature)
        for i in map(self._convertPosToIndex, legals):
            vals[i] = a[i]
        drawn = self._convertIndexToPos(drawGibbs(vals, self.temperature))
        assert drawn in legals
        return drawn

    def _convertIndexToPos(self, i):
        return (i/self.game.size, i%self.game.size)

    def _convertPosToIndex(self, p):
        return p[0]*self.game.size+p[1]

    def integrateObservation(self, obs = None):
        pass



########NEW FILE########
__FILENAME__ = nonsuicide
__author__ = 'Tom Schaul, tom@idsia.ch'

from random import choice

from randomplayer import RandomCapturePlayer


class NonSuicidePlayer(RandomCapturePlayer):
    """ do random non-suicide moves in the capture game """
    def getAction(self):
        p = self.game.getAcceptable(self.color)
        if len(p) > 0:
            return [self.color, choice(p)]
        else:
            return RandomCapturePlayer.getAction(self)
########NEW FILE########
__FILENAME__ = randomplayer
__author__ = 'Tom Schaul, tom@idsia.ch'

from random import choice

from captureplayer import CapturePlayer


class RandomCapturePlayer(CapturePlayer):
    """ do random moves in the capture game"""

    def getAction(self):
        return [self.color, choice(self.game.getLegals(self.color))]
########NEW FILE########
__FILENAME__ = gomoku
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import zeros

from twoplayergame import TwoPlayerGame

# TODO: factor out the similarities with the CaptureGame and Go.

class GomokuGame(TwoPlayerGame):
    """ The game of Go-Moku, alias Connect-Five. """

    BLACK = 1
    WHITE = -1
    EMPTY = 0

    startcolor = BLACK

    def __init__(self, size):
        """ the size of the board is a tuple, where each dimension must be minimum 5. """
        self.size = size
        assert size[0] >= 5
        assert size[1] >= 5
        self.reset()

    def _iterPos(self):
        """ an iterator over all the positions of the board. """
        for i in range(self.size[0]):
            for j in range(self.size[1]):
                yield (i, j)

    def reset(self):
        """ empty the board. """
        TwoPlayerGame.reset(self)
        self.movesDone = 0
        self.b = {}
        for p in self._iterPos():
            self.b[p] = self.EMPTY

    def _fiveRow(self, color, pos):
        """ Is this placement the 5th in a row? """
        # TODO: more efficient...
        for dir in [(0, 1), (1, 0), (1, 1), (1, -1)]:
            found = 1
            for d in [-1, 1]:
                for i in range(1, 5):
                    next = (pos[0] + dir[0] * i * d, pos[1] + dir[1] * i * d)
                    if (next[0] < 0 or next[0] >= self.size[0]
                        or next[1] < 0 or next[1] >= self.size[1]
                        or self.b[next] != color):
                        break
                    else:
                        found += 1
            if found >= 5:
                return True
        return False

    @property
    def indim(self):
        return self.size[0] * self.size[1]

    @property
    def outdim(self):
        return 2 * self.size[0] * self.size[1]

    def getBoardArray(self):
        """ an array with thow boolean values per position, indicating
        'white stone present' and 'black stone present' respectively. """
        a = zeros(self.outdim)
        for i, p in enumerate(self._iterPos()):
            if self.b[p] == self.WHITE:
                a[2 * i] = 1
            elif self.b[p] == self.BLACK:
                a[2 * i + 1] = 1
        return a

    def isLegal(self, c, pos):
        return self.b[pos] == self.EMPTY

    def doMove(self, c, pos):
        """ the action is a (color, position) tuple, for the next stone to move.
        returns True if the move was legal. """
        self.movesDone += 1
        if not self.isLegal(c, pos):
            return False
        elif self._fiveRow(c, pos):
            self.winner = c
            self.b[pos] = 'x'
            return True
        else:
            self._setStone(c, pos)
            if self.movesDone == self.size[0] * self.size[1]:
                # DRAW
                self.winner = self.DRAW
            return True

    def getSensors(self):
        """ just a list of the board position states. """
        return map(lambda x: x[1], sorted(self.b.items()))

    def __str__(self):
        s = ''
        for i in range(self.size[0]):
            for j in range(self.size[1]):
                val = self.b[(i, j)]
                if val == self.EMPTY: s += ' _'
                elif val == self.BLACK: s += ' #'
                elif val == self.WHITE: s += ' *'
                else: s += ' ' + str(val)
            s += '\n'
        if self.winner:
            if self.winner == self.BLACK:
                w = 'Black (#)'
            elif self.winner == self.WHITE:
                w = 'White (*)'
            else:
                w = self.winner
            s += 'Winner: ' + w
            s += ' (moves done:' + str(self.movesDone) + ')\n'
        return s

    def _neighbors(self, pos):
        """ the 4 neighboring positions """
        res = []
        if pos[1] < self.size - 1: res.append((pos[0], pos[1] + 1))
        if pos[1] > 0: res.append((pos[0], pos[1] - 1))
        if pos[0] < self.size - 1: res.append((pos[0] + 1, pos[1]))
        if pos[0] > 0: res.append((pos[0] - 1, pos[1]))
        return res

    def _setStone(self, c, pos):
        """ set stone """
        self.b[pos] = c

    def getLegals(self, c):
        """ return all the legal positions for a color """
        return filter(lambda p: self.b[p] == self.EMPTY, self._iterPos())

    def getKilling(self, c):
        """ return all legal positions for a color that immediately kill the opponent. """
        return filter(lambda p: self._fiveRow(c, p), self.getLegals(c))

    def playToTheEnd(self, p1, p2):
        """ alternate playing moves between players until the game is over. """
        assert p1.color == -p2.color
        i = 0
        p1.game = self
        p2.game = self
        players = [p1, p2]
        while not self.gameOver():
            p = players[i]
            self.performAction(p.getAction())
            i = (i + 1) % 2



########NEW FILE########
__FILENAME__ = gomokuplayer
__author__ = 'Tom Schaul, tom@idsia.ch'


from pybrain.rl.agents.agent import Agent
from pybrain.rl.environments.twoplayergames import GomokuGame


class GomokuPlayer(Agent):
    """ a class of agent that can play Go-Moku, i.e. provides actions in the format:
    (playerid, position)
    playerid is self.color, by convention.
    It generally also has access to the game object. """
    def __init__(self, game, color = GomokuGame.BLACK, **args):
        self.game = game
        self.color = color
        self.setArgs(**args)


########NEW FILE########
__FILENAME__ = killing
__author__ = 'Tom Schaul, tom@idsia.ch'

from random import choice

from randomplayer import RandomGomokuPlayer


class KillingGomokuPlayer(RandomGomokuPlayer):
    """ do random moves, but always instant-kill if possible. """
    def getAction(self):
        p = self.game.getKilling(self.color)
        if len(p) > 0:
            return [self.color, choice(p)]
        else:
            return RandomGomokuPlayer.getAction(self)
########NEW FILE########
__FILENAME__ = moduledecision
__author__ = 'Tom Schaul, tom@idsia.ch'


from scipy import zeros, ones

from pybrain.rl.environments.twoplayergames import GomokuGame
from randomplayer import RandomGomokuPlayer
from pybrain.utilities import drawGibbs


class ModuleDecidingPlayer(RandomGomokuPlayer):
    """ A Go-Moku player that plays according to the rules, but choosing its moves
    according to the output of a module that takes as input the current state of the board. """

    greedySelection = False

    # if the selection is not greedy, use Gibbs-sampling with this temperature
    temperature = 1.

    def __init__(self, module, *args, **kwargs):
        RandomGomokuPlayer.__init__(self, *args, **kwargs)
        self.module = module
        if self.greedySelection:
            self.temperature = 0.

    def getAction(self):
        """ get suggested action, return them if they are legal, otherwise choose randomly. """
        ba = self.game.getBoardArray()
        # network is given inputs with self/other as input, not black/white
        if self.color != GomokuGame.BLACK:
            # invert values
            tmp = zeros(len(ba))
            tmp[:len(ba)-1:2] = ba[1:len(ba):2]
            tmp[1:len(ba):2] = ba[:len(ba)-1:2]
            ba = tmp
        self.module.reset()
        return [self.color, self._legalizeIt(self.module.activate(ba))]

    def newEpisode(self):
        self.module.reset()

    def _legalizeIt(self, a):
        """ draw index from an array of values, filtering out illegal moves. """
        if not min(a) >= 0:
            print(a)
            print(min(a))
            print(self.module.params)
            print(self.module.inputbuffer)
            print(self.module.outputbuffer)
            raise Exception('No positve value in array?')
        legals = self.game.getLegals(self.color)
        vals = ones(len(a))*(-100)*(1+self.temperature)
        for i in map(self._convertPosToIndex, legals):
            vals[i] = a[i]
        drawn = self._convertIndexToPos(drawGibbs(vals, self.temperature))
        assert drawn in legals
        return drawn

    def _convertIndexToPos(self, i):
        return (i/self.game.size[0], i%self.game.size[0])

    def _convertPosToIndex(self, p):
        return p[0]*self.game.size[0]+p[1]

    def integrateObservation(self, obs = None):
        pass



########NEW FILE########
__FILENAME__ = randomplayer
__author__ = 'Tom Schaul, tom@idsia.ch'

from random import choice

from gomokuplayer import GomokuPlayer


class RandomGomokuPlayer(GomokuPlayer):
    """ do random moves in Go-Moku"""

    def getAction(self):
        return [self.color, choice(self.game.getLegals(self.color))]
########NEW FILE########
__FILENAME__ = pente
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.rl.environments.twoplayergames.gomoku import GomokuGame


class PenteGame(GomokuGame):
    """ The game of Pente.

    The rules are similar to Go-Moku, except that it is now possible to capture
    stones, in pairs, by putting stones at both ends of a pair of the opponent.
    The game is won by the first player who either has 5 connected stones, or
    has captured 5 pairs.
    """

    def reset(self):
        GomokuGame.reset(self)
        self.pairsTaken = {self.BLACK: 0, self.WHITE: 0}
        center = (self.size[0] / 2, self.size[1] / 2)
        self._setStone(-self.startcolor, center)
        self.movesDone += 1

    def getKilling(self, c):
        """ return all legal positions for a color that immediately kill the opponent. """
        res = GomokuGame.getKilling(self, c)
        for p in self.getLegals(c):
            k = self._killsWhich(c, p)
            if self.pairsTaken[c] + len(k) / 2 >= 5:
                res.append(p)
        return res

    def _killsWhich(self, c, pos):
        """ placing a stone of color c at pos would kill which enemy stones? """
        res = []
        for dir in [(0, 1), (1, 0), (1, 1), (1, -1)]:
            for d in [-1, 1]:
                killcands = []
                for i in [1, 2, 3]:
                    next = (pos[0] + dir[0] * i * d, pos[1] + dir[1] * i * d)
                    if (next[0] < 0 or next[0] >= self.size[0]
                        or next[1] < 0 or next[1] >= self.size[1]):
                        break
                    if i == 3 and self.b[next] == c:
                        res += killcands
                        break
                    if i != 3 and self.b[next] != -c:
                        break
                    killcands.append(next)
        return res

    def doMove(self, c, pos):
        """ the action is a (color, position) tuple, for the next stone to move.
        returns True if the move was legal. """
        self.movesDone += 1
        if not self.isLegal(c, pos):
            return False
        elif self._fiveRow(c, pos):
            self.winner = c
            self.b[pos] = 'x'
            return True
        else:
            tokill = self._killsWhich(c, pos)
            if self.pairsTaken[c] + len(tokill) / 2 >= 5:
                self.winner = c
                self.b[pos] = 'x'
                return True

            self._setStone(c, pos, tokill)
            if self.movesDone == (self.size[0] * self.size[1]
                                  + 2 * (self.pairsTaken[self.BLACK] + self.pairsTaken[self.WHITE])):
                # DRAW
                self.winner = self.DRAW
            return True

    def _setStone(self, c, pos, tokill=None):
        """ set stone, and potentially kill stones. """
        if tokill == None:
            tokill = self._killsWhich(c, pos)
        GomokuGame._setStone(self, c, pos)
        for p in tokill:
            self.b[p] = self.EMPTY
        self.pairsTaken[c] += len(tokill) / 2

    def __str__(self):
        s = GomokuGame.__str__(self)
        s += 'Black captured:' + str(self.pairsTaken[self.BLACK]) + ', white captured:' + str(self.pairsTaken[self.WHITE]) + '.'
        return s


########NEW FILE########
__FILENAME__ = capturetask
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.rl.environments.episodic import EpisodicTask
from inspect import isclass
from pybrain.utilities import  Named
from pybrain.rl.environments.twoplayergames import CaptureGame
from pybrain.rl.environments.twoplayergames.capturegameplayers import RandomCapturePlayer, ModuleDecidingPlayer
from pybrain.rl.environments.twoplayergames.capturegameplayers.captureplayer import CapturePlayer
from pybrain.structure.modules.module import Module


class CaptureGameTask(EpisodicTask, Named):
    """ The task of winning the maximal number of capture games against a fixed opponent. """

    # first game, opponent is black
    opponentStart = True

    # on subsequent games, starting players are alternating
    alternateStarting = False

    # numerical reward value attributed to winning
    winnerReward = 1.

    # coefficient determining the importance of long vs. short games w.r. to winning/losing
    numMovesCoeff = 0.

    # average over some games for evaluations
    averageOverGames = 10

    noisy = True

    def __init__(self, size, opponent = None, **args):
        EpisodicTask.__init__(self, CaptureGame(size))
        self.setArgs(**args)
        if opponent == None:
            opponent = RandomCapturePlayer(self.env)
        elif isclass(opponent):
            # assume the agent can be initialized without arguments then.
            opponent = opponent(self.env)
        else:
            opponent.game = self.env
        if not self.opponentStart:
            opponent.color = CaptureGame.WHITE
        self.opponent = opponent
        self.maxmoves = self.env.size * self.env.size
        self.minmoves = 3
        self.reset()

    def reset(self):
        self.switched = False
        EpisodicTask.reset(self)
        if self.opponent.color == CaptureGame.BLACK:
            # first move by opponent
            EpisodicTask.performAction(self, self.opponent.getAction())

    def isFinished(self):
        res = self.env.gameOver()
        if res and self.alternateStarting and not self.switched:
            # alternate starting player
            self.opponent.color *= -1
            self.switched = True
        return res

    def getReward(self):
        """ Final positive reward for winner, negative for loser. """
        if self.isFinished():
            win = (self.env.winner != self.opponent.color)
            moves = self.env.movesDone
            res = self.winnerReward - self.numMovesCoeff * (moves -self.minmoves)/(self.maxmoves-self.minmoves)
            if not win:
                res *= -1
            if self.alternateStarting and self.switched:
                # opponent color has been inverted after the game!
                res *= -1
            return res
        else:
            return 0

    def performAction(self, action):
        EpisodicTask.performAction(self, action)
        if not self.isFinished():
            EpisodicTask.performAction(self, self.opponent.getAction())

    def f(self, x):
        """ If a module is given, wrap it into a ModuleDecidingAgent before evaluating it.
        Also, if applicable, average the result over multiple games. """
        if isinstance(x, Module):
            agent = ModuleDecidingPlayer(x, self.env, greedySelection = True)
        elif isinstance(x, CapturePlayer):
            agent = x
        else:
            raise NotImplementedError('Missing implementation for '+x.__class__.__name__+' evaluation')
        res = 0
        agent.game = self.env
        self.opponent.game = self.env
        for _ in range(self.averageOverGames):
            agent.color = -self.opponent.color
            x = EpisodicTask.f(self, agent)
            res += x
        return res / float(self.averageOverGames)




########NEW FILE########
__FILENAME__ = gomokutask
__author__ = 'Tom Schaul, tom@idsia.ch'


from inspect import isclass
from pybrain.utilities import  Named
from pybrain.rl.environments.twoplayergames import GomokuGame
from pybrain.rl.environments.twoplayergames.gomokuplayers import RandomGomokuPlayer, ModuleDecidingPlayer
from pybrain.rl.environments.twoplayergames.gomokuplayers.gomokuplayer import GomokuPlayer
from pybrain.structure.modules.module import Module
from pybrain.rl.environments.episodic import EpisodicTask


class GomokuTask(EpisodicTask, Named):
    """ The task of winning the maximal number of Gomoku games against a fixed opponent. """

    # first game, opponent is black
    opponentStart = True

    # on subsequent games, starting players are alternating
    alternateStarting = False

    # numerical reward value attributed to winning
    winnerReward = 1.

    # coefficient determining the importance of long vs. short games w.r. to winning/losing
    numMovesCoeff = 0.

    # average over some games for evaluations
    averageOverGames = 10

    noisy = True

    def __init__(self, size, opponent = None, **args):
        EpisodicTask.__init__(self, GomokuGame((size, size)))
        self.setArgs(**args)
        if opponent == None:
            opponent = RandomGomokuPlayer(self.env)
        elif isclass(opponent):
            # assume the agent can be initialized without arguments then.
            opponent = opponent(self.env)
        if not self.opponentStart:
            opponent.color = GomokuGame.WHITE
        self.opponent = opponent
        self.minmoves = 9
        self.maxmoves = self.env.size[0] * self.env.size[1]
        self.reset()

    def reset(self):
        self.switched = False
        EpisodicTask.reset(self)
        if self.opponent.color == GomokuGame.BLACK:
            # first move by opponent
            EpisodicTask.performAction(self, self.opponent.getAction())

    def isFinished(self):
        res = self.env.gameOver()
        if res and self.alternateStarting and not self.switched:
            # alternate starting player
            self.opponent.color *= -1
            self.switched = True
        return res

    def getReward(self):
        """ Final positive reward for winner, negative for loser. """
        if self.isFinished():
            if self.env.winner == self.env.DRAW:
                return 0
            win = (self.env.winner != self.opponent.color)
            moves = self.env.movesDone
            res = self.winnerReward - self.numMovesCoeff * (moves -self.minmoves)/(self.maxmoves-self.minmoves)
            if not win:
                res *= -1
            if self.alternateStarting and self.switched:
                # opponent color has been inverted after the game!
                res *= -1
            return res
        else:
            return 0

    def performAction(self, action):
        EpisodicTask.performAction(self, action)
        if not self.isFinished():
            EpisodicTask.performAction(self, self.opponent.getAction())

    def f(self, x):
        """ If a module is given, wrap it into a ModuleDecidingAgent before evaluating it.
        Also, if applicable, average the result over multiple games. """
        if isinstance(x, Module):
            agent = ModuleDecidingPlayer(x, self.env, greedySelection = True)
        elif isinstance(x, GomokuPlayer):
            agent = x
        else:
            raise NotImplementedError('Missing implementation for '+x.__class__.__name__+' evaluation')
        res = 0
        agent.game = self.env
        self.opponent.game = self.env
        for dummy in range(self.averageOverGames):
            agent.color = -self.opponent.color
            res += EpisodicTask.f(self, agent)
        return res / float(self.averageOverGames)




########NEW FILE########
__FILENAME__ = handicaptask
__author__ = 'Tom Schaul, tom@idsia.ch'

from capturetask import CaptureGameTask
from pybrain.rl.environments.twoplayergames.capturegameplayers.captureplayer import CapturePlayer
from pybrain.rl.environments.twoplayergames.capturegameplayers import ModuleDecidingPlayer
from pybrain.rl.environments.twoplayergames.capturegame import CaptureGame

# TODO: parametrize hard-coded variables.
# TODO: also allow handicap-advantage

class HandicapCaptureTask(CaptureGameTask):
    """ Play against an opponent, and try to beat it with it having the maximal
    number of handicap stones:

    The score for this task is not the percentage of wins, but the achieved handicap against
    the opponent when the results stabilize.
    Stabilize: if after minimum of 6 games at the same handicap H, > 80% were won
    by the player, increase the handicap. If <20% decrease it.

    If the system fluctuates between H and H+1, with at least 10 games played on each level,
    assert H+0.5 as handicap.
    the score = 2 * #handicaps + proportion of wins at that level. """

    maxGames = 200
    averageOverGames = 1
    minEvals = 5

    def __init__(self, *args, **kargs):
        CaptureGameTask.__init__(self, *args, **kargs)
        self.size = self.env.size
        # the maximal handicap given is a full line of stones along the second line.
        self.maxHandicaps = (self.size - 2) * 2 + (self.size - 4) * 2

    def winProp(self, h):
        w, t, wms, lms = self.results[h]
        if t > 0:
            res = (w - t / 2.) / (t / 2.)
            if wms > 0:
                res += 0.1 / (t * wms)
            if lms > 0:
                res -= 1. / (t * lms)
            return res
        else:
            return 0.

    def goUp(self, h):
        """ ready to go up one handicap? """
        if self.results[h][1] >= self.minEvals:
            return self.winProp(h) > 0.6
        return False

    def goDown(self, h):
        """ have to go down one handicap? """
        if self.results[h][1] >= self.minEvals:
            return self.winProp(h) < -0.6
        return False

    def bestHandicap(self):
        return max(self.results.keys()) - 1

    def fluctuating(self):
        """ Is the highest handicap unstable? """
        high = self.bestHandicap()
        if high > 0:
            if self.results[high][1] >= 2 * self.minEvals and self.results[high - 1][1] >= 2 * self.minEvals:
                return self.goUp(high - 1) and self.goDown(high)
        return False

    def stable(self, h):
        return (self.fluctuating()
                or (self.results[h][1] >= 2 * self.minEvals and (not self.goUp(h)) and (not self.goDown(h)))
                or (self.results[h][1] >= 2 * self.minEvals and self.goUp(h) and h >= self.maxHandicaps)
                or (self.results[h][1] >= 2 * self.minEvals and self.goDown(h) and h == 0))

    def addResult(self, h, win, moves):
        if h + 1 not in self.results:
            self.results[h + 1] = [0, 0, 0, 0]
        self.results[h][1] += 1
        if win == True:
            self.results[h][0] += 1
            self.results[h][2] += moves
        else:
            self.results[h][3] += moves

    def reset(self):
        # stores [wins, total, sum(moves-til-win), sum(moves-til-lose)]
        # for each handicap-key
        self.results = {0: [0, 0, 0, 0]}

    def f(self, player):
        if not isinstance(player, CapturePlayer):
            player = ModuleDecidingPlayer(player, self.env, greedySelection=True)
        player.color = CaptureGame.WHITE
        self.opponent.color = CaptureGame.BLACK
        self.reset()
        current = 0
        games = 0
        while games < self.maxGames and not self.stable(current):
            games += 1
            self.env.reset()
            self.env.giveHandicap(current , self.opponent.color)
            self.env.playToTheEnd(self.opponent, player)
            win = self.env.winner == player.color
            self.addResult(current, win, self.env.movesDone)
            if self.goUp(current) and current < self.maxHandicaps:
                current += 1
            elif self.goDown(current) and current > 1:
                current -= 1

        high = self.bestHandicap()
        # the scale goes from -1 to (the highest handicap + 1)
        if not self.fluctuating():
            return high + self.winProp(high)
        else:
            return (high - 0.5) + (self.winProp(high) + self.winProp(high - 1)) / 2.


if __name__ == '__main__':
    from pybrain.rl.environments.twoplayergames.capturegameplayers import RandomCapturePlayer, KillingPlayer
    h = HandicapCaptureTask(4, opponentStart=False)
    p1 = RandomCapturePlayer(h.env)
    p1 = KillingPlayer(h.env)
    print(h(p1))
    print(h.results)
    print(h.winProp(0))
    print(h.winProp(1))


########NEW FILE########
__FILENAME__ = pentetask

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.rl.environments.twoplayergames.pente import PenteGame
from pybrain.rl.environments.episodic import EpisodicTask
from gomokutask import GomokuTask
from pybrain.rl.environments.twoplayergames.gomokuplayers import RandomGomokuPlayer
from inspect import isclass


class PenteTask(GomokuTask):
    """ The task of winning the maximal number of Gomoku games against a fixed opponent. """

    def __init__(self, size, opponent = None, **args):
        EpisodicTask.__init__(self, PenteGame((size, size)))
        self.setArgs(**args)
        if opponent == None:
            opponent = RandomGomokuPlayer(self.env)
        elif isclass(opponent):
            # assume the agent can be initialized without arguments then.
            opponent = opponent(self.env)
        if not self.opponentStart:
            opponent.color = PenteGame.WHITE
        self.opponent = opponent
        self.minmoves = 9
        self.maxmoves = self.env.size[0] * self.env.size[1]
        self.reset()


########NEW FILE########
__FILENAME__ = relativegomokutask
__author__ = 'Tom Schaul, tom@idsia.ch'

from gomokutask import GomokuTask
from pybrain.rl.environments.twoplayergames.gomokuplayers import ModuleDecidingPlayer
from pybrain.rl.environments.twoplayergames import GomokuGame
from pybrain.rl.environments.twoplayergames.gomokuplayers.gomokuplayer import GomokuPlayer
from pybrain.structure.networks.custom.capturegame import CaptureGameNetwork


class RelativeGomokuTask(GomokuTask):
    """ returns the (anti-symmetric) relative score of p1 with respect to p2.
    (p1 and p2 are CaptureGameNetworks)
    The score depends on:
    - greedy play
    - moves-until-win or moves-until-defeat (winning faster is better)
    - play with noisy moves (e.g. adjusting softmax temperature)

    """

    # are networks provided?
    useNetworks = False

    # maximal number of games per evaluation
    maxGames = 3

    minTemperature = 0
    maxTemperature = 0.2

    verbose = False

    # coefficient determining the importance of long vs. short games w.r. to winning/losing
    numMovesCoeff = 0.5

    def __init__(self, size, **args):
        self.setArgs(**args)
        self.size = size
        self.task = GomokuTask(self.size)
        self.env = self.task.env
        self.maxmoves = self.env.size[0] * self.env.size[1]
        self.minmoves = 9


    def __call__(self, p1, p2):
        self.temp = self.minTemperature
        if self.useNetworks:
            p1 = ModuleDecidingPlayer(p1, self.task.env, temperature = self.temp)
            p2 = ModuleDecidingPlayer(p2, self.task.env, temperature = self.temp)
        else:
            assert isinstance(p1, GomokuPlayer)
            assert isinstance(p2, GomokuPlayer)
            p1.game = self.task.env
            p2.game = self.task.env
        p1.color = GomokuGame.BLACK
        p2.color = -p1.color
        self.player = p1
        self.opponent = p2

        # the games with increasing temperatures and lower coefficients
        coeffSum = 0.
        res = 0.
        for i in range(self.maxGames):
            coeff = 1/(10*self.temp+1)
            res += coeff * self._oneGame()
            coeffSum += coeff
            if i > 0:
                self._globalWarming()

        return res / coeffSum

    def _globalWarming(self):
        """ increase temperature """
        if self.temp == 0:
            self.temp = 0.02
        else:
            self.temp *= 1.2
        if self.temp > self.maxTemperature:
            return False
        elif self._setTemperature() == False:
            # not adjustable, keep it fixed then.
            self.temp = self.minTemperature
            return False
        return True

    def _setTemperature(self):
        if self.useNetworks:
            self.opponent.temperature = self.temp
            self.player.temperature = self.temp
            return True
        elif hasattr(self.opponent, 'randomPartMoves'):
            # an approximate conversion of temperature into random proportion:
            randPart = self.temp/(self.temp+1)
            self.opponent.randomPartMoves = randPart
            self.player.randomPartMoves = randPart
            return True
        else:
            return False

    def _oneGame(self, preset = None):
        """ a single black stone can be set as the first move. """
        self.env.reset()
        if preset != None:
            self.env._setStone(GomokuGame.BLACK, preset)
            self.env.movesDone += 1
            self.env.playToTheEnd(self.opponent, self.player)
        else:
            self.env.playToTheEnd(self.player, self.opponent)
        moves = self.env.movesDone
        win = self.env.winner == self.player.color
        if self.verbose:
            print('Preset:', preset, 'T:', self.temp, 'Win:', win, 'after', moves, 'moves.')
        res = 1 - self.numMovesCoeff * (moves -self.minmoves)/(self.maxmoves-self.minmoves)
        if win:
            return res
        else:
            return -res



if __name__ == '__main__':
    net1 = CaptureGameNetwork(hsize = 1)
    net2 = CaptureGameNetwork(hsize = 1)
    r = RelativeGomokuTask(7, maxGames = 10, useNetworks = True)
    print(r(net1, net2))
    print(r(net2, net1))
    print(r.env)
    r.maxGames = 50
    print(r(net1, net2))
    print(r(net2, net1))
    print(r.env)


########NEW FILE########
__FILENAME__ = relativetask
__author__ = 'Tom Schaul, tom@idsia.ch'

from capturetask import CaptureGameTask
from pybrain.rl.environments.twoplayergames.capturegameplayers import ModuleDecidingPlayer
from pybrain.rl.environments.twoplayergames import CaptureGame
from pybrain.rl.environments.twoplayergames.capturegameplayers.captureplayer import CapturePlayer
from pybrain.structure.networks.custom.capturegame import CaptureGameNetwork


class RelativeCaptureTask(CaptureGameTask):
    """ returns the (anti-symmetric) relative score of p1 with respect to p2.
    (p1 and p2 are CaptureGameNetworks)
    The score depends on:
    - greedy play
    - play with fixed starting positions (only first stone)
    - moves-until-win or moves-until-defeat (winning faster is better)
    - play with noisy moves (e.g. adjusting softmax temperature)

    """

    # are networks provided?
    useNetworks = False

    # maximal number of games per evaluation
    maxGames = 3

    presetGamesProportion = 0.5

    minTemperature = 0
    maxTemperature = 0.2

    verbose = False

    # coefficient determining the importance of long vs. short games w.r. to winning/losing
    numMovesCoeff = 0.5

    def __init__(self, size, **args):
        self.setArgs(**args)
        self.size = size
        self.task = CaptureGameTask(self.size)
        self.env = self.task.env
        if self.presetGamesProportion > 0:
            self.sPos = self._fixedStartingPos()
            self.cases = int(len(self.sPos) / self.presetGamesProportion)
        else:
            self.cases = 1
        self.maxmoves = self.size * self.size
        self.minmoves = 3

    def __call__(self, p1, p2):
        self.temp = self.minTemperature
        if self.useNetworks:
            p1 = ModuleDecidingPlayer(p1, self.task.env, temperature=self.temp)
            p2 = ModuleDecidingPlayer(p2, self.task.env, temperature=self.temp)
        else:
            assert isinstance(p1, CapturePlayer)
            assert isinstance(p2, CapturePlayer)
            p1.game = self.task.env
            p2.game = self.task.env
        p1.color = CaptureGame.BLACK
        p2.color = -p1.color
        self.player = p1
        self.opponent = p2

        # the games with increasing temperatures and lower coefficients
        coeffSum = 0.
        score = 0.
        np = int(self.cases * (1 - self.presetGamesProportion))
        for i in range(self.maxGames):
            coeff = 1 / (10 * self.temp + 1)
            preset = None
            if self.cases > 1:
                if i % self.cases >= np:
                    preset = self.sPos[(i - np) % self.cases]
                elif i < self.cases:
                    # greedy, no need to repeat, just increase the coefficient
                    if i == 0:
                        coeff *= np
                    else:
                        continue
            res = self._oneGame(preset)
            score += coeff * res
            coeffSum += coeff
            if self.cases == 1 or (i % self.cases == 0 and i > 0):
                self._globalWarming()

        return score / coeffSum

    def _globalWarming(self):
        """ increase temperature """
        if self.temp == 0:
            self.temp = 0.02
        else:
            self.temp *= 1.5
        if self.temp > self.maxTemperature:
            return False
        elif self._setTemperature() == False:
            # not adjustable, keep it fixed then.
            self.temp = self.minTemperature
            return False
        return True

    def _setTemperature(self):
        if self.useNetworks:
            self.opponent.temperature = self.temp
            self.player.temperature = self.temp
            return True
        elif hasattr(self.opponent, 'randomPartMoves'):
            # an approximate conversion of temperature into random proportion:
            randPart = self.temp / (self.temp + 1)
            self.opponent.randomPartMoves = randPart
            self.player.randomPartMoves = randPart
            return True
        else:
            return False

    def _fixedStartingPos(self):
        """ a list of starting positions, not along the border, and respecting symmetry. """
        res = []
        if self.size < 3:
            return res
        for x in range(1, (self.size + 1) / 2):
            for y in range(x, (self.size + 1) / 2):
                res.append((x, y))
        return res

    def _oneGame(self, preset=None):
        """ a single black stone can be set as the first move. """
        self.env.reset()
        if preset != None:
            self.env._setStone(CaptureGame.BLACK, preset)
            self.env.movesDone += 1
            self.env.playToTheEnd(self.opponent, self.player)
        else:
            self.env.playToTheEnd(self.player, self.opponent)
        moves = self.env.movesDone
        win = self.env.winner == self.player.color
        if self.verbose:
            print('Preset:', preset, 'T:', self.temp, 'Win:', win, 'after', moves, 'moves.')
        res = 1 - self.numMovesCoeff * (moves - self.minmoves) / (self.maxmoves - self.minmoves)
        if win:
            return res
        else:
            return - res


if __name__ == '__main__':
    assert RelativeCaptureTask(5)._fixedStartingPos() == [(1, 1), (1, 2), (2, 2)]
    assert RelativeCaptureTask(8)._fixedStartingPos() == [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]

    net1 = CaptureGameNetwork(hsize=1)
    net2 = CaptureGameNetwork(hsize=1)
    #print(net1.params)
    #print(net2.params)

    r = RelativeCaptureTask(5, maxGames=40, useNetworks=True,
                            presetGamesProportion=0.5)

    print(r(net1, net2))
    print(r(net2, net1))
    r.maxGames = 200
    print(r(net1, net2))
    print(r(net2, net1))




########NEW FILE########
__FILENAME__ = twoplayergame
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.utilities import abstractMethod
from pybrain.rl.environments import Environment


class CompetitiveEnvironment(Environment):
    """ an environment in which multiple agents interact, competitively.
    This class is only for conceptual grouping, it only constrains the format of action input.
    """

    def performAction(self, action):
        """ perform an action on the world that changes it's internal state (maybe stochastically)

            :key action: an action that should be executed in the Environment, by an agent.
            :type action: tuple: (agentID, action value)
            :note: This function is abstract and has to be implemented.
        """
        abstractMethod()


class TwoPlayerGame(CompetitiveEnvironment):
    """ a game between 2 players, alternating turns. Outcome can be one winner or a draw. """

    DRAW = 'draw'

    def reset(self):
        self.winner = None
        self.lastplayer = None

    def performAction(self, action):
        self.lastplayer = action[0]
        self.doMove(*action)

    def doMove(self, player, action):
        """ the core method to be implemented bu all TwoPlayerGames:
        what happens when a player performs an action. """
        abstractMethod()

    def isLegal(self, player, action):
        """ is this a legal move? By default, everything is allowed. """
        return True

    def gameOver(self):
        """ is the game over? """
        return self.winner != None

    def getWinner(self):
        """ returns the id of the winner, 'draw' if it's a draw, and None if the game is undecided. """
        return self.winner

########NEW FILE########
__FILENAME__ = continuous
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from pybrain.rl.experiments.experiment import Experiment


class ContinuousExperiment(Experiment):
    """ The extension of Experiment to handle continuous tasks. """

    def doInteractionsAndLearn(self, number = 1):
        """ Execute a number of steps while learning continuously.
            no reset is performed, such that consecutive calls to
            this function can be made.
        """
        for _ in range(number):
            self._oneInteraction()
            self.agent.learn()
        return self.stepid

########NEW FILE########
__FILENAME__ = episodic
__author__ = 'Tom Schaul, tom@idsia.ch'


from pybrain.rl.experiments.experiment import Experiment
from pybrain.rl.agents.optimization import OptimizationAgent


class EpisodicExperiment(Experiment):
    """ The extension of Experiment to handle episodic tasks. """

    doOptimization = False

    def __init__(self, task, agent):
        if isinstance(agent, OptimizationAgent):
            self.doOptimization = True
            self.optimizer = agent.learner
            self.optimizer.setEvaluator(task, agent.module)
            self.optimizer.maxEvaluations = self.optimizer.numEvaluations
        else:
            Experiment.__init__(self, task, agent)

    def _oneInteraction(self):
        """ Do an interaction between the Task and the Agent. """
        if self.doOptimization:
            raise Exception('When using a black-box learning algorithm, only full episodes can be done.')
        else:
            return Experiment._oneInteraction(self)

    def doEpisodes(self, number = 1):
        """ Do one episode, and return the rewards of each step as a list. """
        if self.doOptimization:
            self.optimizer.maxEvaluations += number
            self.optimizer.learn()
        else:
            all_rewards = []
            for dummy in range(number):
                self.agent.newEpisode()
                rewards = []
                self.stepid = 0
                self.task.reset()
                while not self.task.isFinished():
                    r = self._oneInteraction()
                    rewards.append(r)
                all_rewards.append(rewards)

            return all_rewards

########NEW FILE########
__FILENAME__ = experiment
__author__ = 'Tom Schaul, tom@idsia.ch'


class Experiment(object):
    """ An experiment matches up a task with an agent and handles their interactions.
    """

    def __init__(self, task, agent):
        self.task = task
        self.agent = agent
        self.stepid = 0

    def doInteractions(self, number = 1):
        """ The default implementation directly maps the methods of the agent and the task.
            Returns the number of interactions done.
        """
        for _ in range(number):
            self._oneInteraction()
        return self.stepid

    def _oneInteraction(self):
        """ Give the observation to the agent, takes its resulting action and returns
            it to the task. Then gives the reward to the agent again and returns it.
        """
        self.stepid += 1
        self.agent.integrateObservation(self.task.getObservation())
        self.task.performAction(self.agent.getAction())
        reward = self.task.getReward()
        self.agent.giveReward(reward)
        return reward

########NEW FILE########
__FILENAME__ = queued
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from episodic import EpisodicExperiment
from scipy import arange


class QueuedExperiment(EpisodicExperiment):
    """ This experiment type runs n episodes at the beginning, followed by a learning step.
        From then on it removes the oldest episode, learns a new one, and executes another
        training step with the n current episodes. This way, learning happens after each
        episode, and each episode is considered n times for learning until discarded. """

    def run(self, queuelength, learningcycles=-1):
        # fill the queue with given number of episodes
        self._fillQueue(queuelength)

        # start the queue loop
        if learningcycles == -1:
            while True:
                # indefinite learning
                self._stepQueueLoop()
        else:
            for _ in arange(learningcycles):
                # learn the given number of times
                self._stepQueueLoop()


    def _fillQueue(self, queuelength):
        # reset agent (empty queue)
        self.agent.reset()
        # fill queue with first n episodes
        self.doEpisodes(queuelength)


    def _stepQueueLoop(self):
        # let agent learn with full queue
        self.agent.learn()
        # remove oldest episode
        self.agent.history.removeSequence(0)
        # execute one new episode
        self.doEpisodes(1)


########NEW FILE########
__FILENAME__ = tournament
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.rl.environments.twoplayergames.twoplayergame import TwoPlayerGame
from pybrain.utilities import Named


class Tournament(Named):
    """ the tournament class is a specific kind of experiment, that takes a pool of agents
    and has them compete against each other in a TwoPlayerGame. """

    # do all moves need to be checked for legality?
    forcedLegality = False

    def __init__(self, env, agents):
        assert isinstance(env, TwoPlayerGame)
        self.startcolor = env.startcolor
        self.env = env
        self.agents = agents
        for a in agents:
            a.game = self.env
        self.reset()

    def reset(self):
        # a dictionnary attaching a list of outcomes to a player-couple-key
        self.results = {}
        self.rounds = 0
        self.numGames = 0

    def _produceAllPairs(self):
        """ produce a list of all pairs of agents (assuming ab <> ba)"""
        res = []
        for a in self.agents:
            for b in self.agents:
                if a != b:
                    res.append((a, b))
        return res

    def _oneGame(self, p1, p2):
        """ play one game between two agents p1 and p2."""
        self.numGames += 1
        self.env.reset()
        players = (p1, p2)
        p1.color = self.startcolor
        p2.color = -p1.color
        p1.newEpisode()
        p2.newEpisode()
        i = 0
        while not self.env.gameOver():
            p = players[i]
            i = (i + 1) % 2 # alternate
            act = p.getAction()

            if self.forcedLegality:
                tries = 0
                while not self.env.isLegal(*act):
                    tries += 1
                    # CHECKME: maybe the legality check is too specific?
                    act = p.getAction()
                    if tries > 50:
                        raise Exception('No legal move produced!')

            self.env.performAction(act)

        if players not in self.results:
            self.results[players] = []
        wincolor = self.env.getWinner()
        if wincolor == p1.color:
            winner = p1
        else:
            winner = p2
        self.results[players].append(winner)

    def organize(self, repeat=1):
        """ have all agents play all others in all orders, and repeat. """
        for dummy in range(repeat):
            self.rounds += 1
            for p1, p2 in self._produceAllPairs():
                self._oneGame(p1, p2)
        return self.results

    def eloScore(self, startingscore=1500, k=32):
        """ compute the elo score of all the agents, given the games played in the tournament.
        Also checking for potentially initial scores among the agents ('elo' variable). """
        # initialize
        elos = {}
        for a in self.agents:
            if 'elo' in a.__dict__:
                elos[a] = a.elo
            else:
                elos[a] = startingscore
        # adjust ratings
        for i, a1 in enumerate(self.agents[:-1]):
            for a2 in self.agents[i + 1:]:
                # compute score (in favor of a1)
                s = 0
                outcomes = self.results[(a1, a2)] + self.results[(a2, a1)]
                for r in outcomes:
                    if r == a1:
                        s += 1.
                    elif r == self.env.DRAW:
                        s += 0.5
                # what score would have been estimated?
                est = len(outcomes) / (1. + 10 ** ((elos[a2] - elos[a1]) / 400.))
                delta = k * (s - est)
                elos[a1] += delta
                elos[a2] -= delta
        for a, e in elos.items():
            a.elo = e
        return elos

    def __str__(self):
        s = 'Tournament results (' + str(self.rounds) + ' rounds, ' + str(self.numGames) + ' games):\n'
        for p1, p2 in self._produceAllPairs():
            wins = len(filter(lambda x: x == p1, self.results[(p1, p2)]))
            losses = len(filter(lambda x: x == p2, self.results[(p1, p2)]))
            s += ' ' * 3 + p1.name + ' won ' + str(wins) + ' times and lost ' + str(losses) + ' times against ' + p2.name + '\n'
        return s

########NEW FILE########
__FILENAME__ = normal
__author__ = "Thomas Rueckstiess, ruecksti@in.tum.de"

from scipy import random

from pybrain.rl.explorers.explorer import Explorer
from pybrain.tools.functions import expln, explnPrime
from pybrain.structure.parametercontainer import ParameterContainer


class NormalExplorer(Explorer, ParameterContainer):
    """ A continuous explorer, that perturbs the resulting action with
        additive, normally distributed random noise. The exploration
        has parameter(s) sigma, which are related to the distribution's
        standard deviation. In order to allow for negative values of sigma,
        the real std. derivation is a transformation of sigma according
        to the expln() function (see pybrain.tools.functions).
    """

    def __init__(self, dim, sigma=0.):
        Explorer.__init__(self, dim, dim)
        self.dim = dim

        # initialize parameters to sigma
        ParameterContainer.__init__(self, dim, stdParams=0)
        self.sigma = [sigma] * dim

    def _setSigma(self, sigma):
        """ Wrapper method to set the sigmas (the parameters of the module) to a
            certain value.
        """
        assert len(sigma) == self.dim
        self._params *= 0
        self._params += sigma

    def _getSigma(self):
        return self.params

    sigma = property(_getSigma, _setSigma)

    def _forwardImplementation(self, inbuf, outbuf):
        outbuf[:] = random.normal(inbuf, expln(self.sigma))

    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        expln_sigma = expln(self.sigma)
        self._derivs += ((outbuf - inbuf) ** 2 - expln_sigma ** 2) / expln_sigma * explnPrime(self.sigma)
        inerr[:] = (outbuf - inbuf)

        # auto-alpha
        # inerr /= expln_sigma**2
        # self._derivs /= expln_sigma**2



########NEW FILE########
__FILENAME__ = sde
__author__ = "Thomas Rueckstiess, ruecksti@in.tum.de"

from scipy import random, dot

from pybrain.structure.modules.module import Module
from pybrain.rl.explorers.explorer import Explorer
from pybrain.tools.functions import expln, explnPrime
from pybrain.structure.parametercontainer import ParameterContainer


class StateDependentExplorer(Explorer, ParameterContainer):
    """ A continuous explorer, that perturbs the resulting action with
        additive, normally distributed random noise. The exploration
        has parameter(s) sigma, which are related to the distribution's
        standard deviation. In order to allow for negative values of sigma,
        the real std. derivation is a transformation of sigma according
        to the expln() function (see pybrain.tools.functions).
    """

    def __init__(self, statedim, actiondim, sigma= -2.):
        Explorer.__init__(self, actiondim, actiondim)
        self.statedim = statedim
        self.actiondim = actiondim

        # initialize parameters to sigma
        ParameterContainer.__init__(self, actiondim, stdParams=0)
        self.sigma = [sigma] * actiondim

        # exploration matrix (linear function)
        self.explmatrix = random.normal(0., expln(self.sigma), (statedim, actiondim))

        # store last state
        self.state = None

    def _setSigma(self, sigma):
        """ Wrapper method to set the sigmas (the parameters of the module) to a
            certain value.
        """
        assert len(sigma) == self.actiondim
        self._params *= 0
        self._params += sigma

    def _getSigma(self):
        return self.params

    sigma = property(_getSigma, _setSigma)

    def newEpisode(self):
        """ Randomize the matrix values for exploration during one episode. """
        self.explmatrix = random.normal(0., expln(self.sigma), self.explmatrix.shape)

    def activate(self, state, action):
        """ The super class commonly ignores the state and simply passes the
            action through the module. implement _forwardImplementation()
            in subclasses.
        """
        self.state = state
        return Module.activate(self, action)

    def _forwardImplementation(self, inbuf, outbuf):
        outbuf[:] = inbuf + dot(self.state, self.explmatrix)

    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        expln_params = expln(self.params
                        ).reshape(len(outbuf), len(self.state))
        explnPrime_params = explnPrime(self.params
                        ).reshape(len(outbuf), len(self.state))

        idx = 0
        for j in xrange(len(outbuf)):
            sigma_subst2 = dot(self.state ** 2, expln_params[j, :]**2)
            for i in xrange(len(self.state)):
                self._derivs[idx] = ((outbuf[j] - inbuf[j]) ** 2 - sigma_subst2) / sigma_subst2 * \
                    self.state[i] ** 2 * expln_params[j, i] * explnPrime_params[j, i]
                # if self.autoalpha and sigma_subst2 != 0:
                # self._derivs[idx] /= sigma_subst2
                idx += 1
            inerr[j] = (outbuf[j] - inbuf[j])
            # if not self.autoalpha and sigma_subst2 != 0:
            #     inerr[j] /= sigma_subst2
        # auto-alpha
        # inerr /= expln_sigma**2
        # self._derivs /= expln_sigma**2



########NEW FILE########
__FILENAME__ = boltzmann
__author__ = "Thomas Rueckstiess, ruecksti@in.tum.de"

from scipy import array

from pybrain.rl.explorers.discrete.discrete import DiscreteExplorer
from pybrain.utilities import drawGibbs

class BoltzmannExplorer(DiscreteExplorer):
    """ A discrete explorer, that executes the actions with probability
        that depends on their action values. The boltzmann explorer has
        a parameter tau (the temperature). for high tau, the actions are
        nearly equiprobable. for tau close to 0, this action selection
        becomes greedy.
    """

    def __init__(self, tau = 2., decay = 0.9995):
        DiscreteExplorer.__init__(self)
        self.tau = tau
        self.decay = decay
        self._state = None

    def activate(self, state, action):
        """ The super class ignores the state and simply passes the
            action through the module. implement _forwardImplementation()
            in subclasses.
        """
        self._state = state
        return DiscreteExplorer.activate(self, state, action)


    def _forwardImplementation(self, inbuf, outbuf):
        """ Draws a random number between 0 and 1. If the number is less
            than epsilon, a random action is chosen. If it is equal or
            larger than epsilon, the greedy action is returned.
        """
        assert self.module

        values = self.module.getActionValues(self._state)
        action = drawGibbs(values, self.tau)

        self.tau *= self.decay

        outbuf[:] = array([action])

########NEW FILE########
__FILENAME__ = discrete
__author__ = "Thomas Rueckstiess, ruecksti@in.tum.de"

from pybrain.rl.explorers.explorer import Explorer
# from pybrain.rl.learners.valuebased.interface import ActionValueInterface

class DiscreteExplorer(Explorer):
    """ Discrete explorers choose one of the available actions from the
        set of actions. In order to know which actions are available and
        which action to choose, discrete explorers need access to the
        module (which has to of class ActionValueTable).
    """

    _module = None

    def __init__(self):
        Explorer.__init__(self, 1, 1)

    def _setModule(self, module):
        """ Tells the explorer the module (which has to be ActionValueTable). """
        # removed: cause for circular import
        # assert isinstance(module, ActionValueInterface)
        self._module = module

    def _getModule(self):
        return self._module

    module = property(_getModule, _setModule)
########NEW FILE########
__FILENAME__ = discretesde
__author__ = "Thomas Rueckstiess, ruecksti@in.tum.de"


from pybrain.rl.explorers.discrete.discrete import DiscreteExplorer
from pybrain.rl.learners.valuebased.interface import ActionValueTable, ActionValueNetwork
from copy import deepcopy
from numpy import random, array


class DiscreteStateDependentExplorer(DiscreteExplorer):
    """ A discrete explorer, that directly manipulates the ActionValue
        estimator (table or network) and keeps the changes fixed for one
        full episode (if episodic) or slowly changes it over time.

        TODO: currently only implemented for episodes
    """

    def __init__(self, epsilon = 0.2, decay = 0.9998):
        """ TODO: the epsilon and decay parameters are currently
            not implemented.
        """
        DiscreteExplorer.__init__(self)
        self.state = None

    def _setModule(self, module):
        """ Tell the explorer the module. """
        self._module = module
        # copy the original module for exploration
        self.explorerModule = deepcopy(module)

    def _getModule(self):
        return self._module

    module = property(_getModule, _setModule)


    def activate(self, state, action):
        """ Save the current state for state-dependent exploration. """
        self.state = state
        return DiscreteExplorer.activate(self, state, action)

    def _forwardImplementation(self, inbuf, outbuf):
        """ Activate the copied module instead of the original and
            feed it with the current state.
        """
        if random.random() < 0.001:
            outbuf[:] = array([random.randint(self.module.numActions)])
        else:
            outbuf[:] = self.explorerModule.activate(self.state)

    def newEpisode(self):
        """ Inform the explorer about the start of a new episode. """
        self.explorerModule = deepcopy(self.module)

        if isinstance(self.explorerModule, ActionValueNetwork):

            self.explorerModule.network.mutationStd = 0.01
            self.explorerModule.network.mutate()

        elif isinstance(self.explorerModule, ActionValueTable):
            self.explorerModule.mutationStd = 0.01
            self.explorerModule.mutate()

########NEW FILE########
__FILENAME__ = egreedy
__author__ = "Thomas Rueckstiess, ruecksti@in.tum.de"

from scipy import random, array

from pybrain.rl.explorers.discrete.discrete import DiscreteExplorer

class EpsilonGreedyExplorer(DiscreteExplorer):
    """ A discrete explorer, that executes the original policy in most cases,
        but sometimes returns a random action (uniformly drawn) instead. The
        randomness is controlled by a parameter 0 <= epsilon <= 1. The closer
        epsilon gets to 0, the more greedy (and less explorative) the agent
        behaves.
    """

    def __init__(self, epsilon = 0.3, decay = 0.9999):
        DiscreteExplorer.__init__(self)
        self.epsilon = epsilon
        self.decay = decay

    def _forwardImplementation(self, inbuf, outbuf):
        """ Draws a random number between 0 and 1. If the number is less
            than epsilon, a random action is chosen. If it is equal or
            larger than epsilon, the greedy action is returned.
        """
        assert self.module

        if random.random() < self.epsilon:
            outbuf[:] = array([random.randint(self.module.numActions)])
        else:
            outbuf[:] = inbuf

        self.epsilon *= self.decay



########NEW FILE########
__FILENAME__ = explorer
__author__ = "Thomas Rueckstiess, ruecksti@in.tum.de"


from pybrain.structure.modules.module import Module


class Explorer(Module):
    """ An Explorer object is used in Agents, receives the current state
        and action (from the controller Module) and returns an explorative
        action that is executed instead the given action.
        
        Continous explorer will produce continous action states, discrete
        once discrete actions accordingly. 
        
        Explorer                        action    episodic?
        =============================== ========= =========
        NormalExplorer                  continous no
        StateDependentExplorer          continous yes
        BoltzmannExplorer               discrete  no
        EpsilonGreedyExplorer           discrete  no
        DiscreteStateDependentExplorer  discrete  yes
        

        Explorer has to be added to the learner before adding the learner
        to the LearningAgent.

        For Example::

            controller = ActionValueNetwork(2, 100)
            learner = SARSA()
            learner.explorer = NormalExplorer(1, 0.1)
            self.learning_agent = LearningAgent(controller, learner)
    """

    def activate(self, state, action):
        """ The super class commonly ignores the state and simply passes the
            action through the module. implement _forwardImplementation()
            in subclasses.
        """
        return Module.activate(self, action)


    def newEpisode(self):
        """ Inform the explorer about the start of a new episode. """
        pass

########NEW FILE########
__FILENAME__ = directsearch
# The code for all black-box optimization algorithms is located in
# the pybrain/optimization directory (to avoid duplicating files).
# Those algorithms can perfectly be used on (episodic) RL tasks anyway.
#
# See also examples/optimization/optimizers_for_rl.py

__author__ = 'Tom Schaul and Thomas Rueckstiess, tom@idsia.ch, ruecksti@in.tum.de'

from pybrain.rl.learners.learner import Learner, EpisodicLearner


class DirectSearchLearner(Learner):
    """ The class of learners that (in contrast to value-based learners)
    searches directly in policy space.
    """

########NEW FILE########
__FILENAME__ = enac
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'


from pybrain.rl.learners.directsearch.policygradient import PolicyGradientLearner
from scipy import ones, dot, ravel
from scipy.linalg import pinv


class ENAC(PolicyGradientLearner):
    """ Episodic Natural Actor-Critic. See J. Peters "Natural Actor-Critic", 2005.
        Estimates natural gradient with regression of log likelihoods to rewards.
    """

    def calculateGradient(self):
        # normalize rewards
        # self.dataset.data['reward'] /= max(ravel(abs(self.dataset.data['reward'])))

        # initialize variables
        R = ones((self.dataset.getNumSequences(), 1), float)
        X = ones((self.dataset.getNumSequences(), self.loglh.getDimension('loglh') + 1), float)

        # collect sufficient statistics
        print(self.dataset.getNumSequences())
        for n in range(self.dataset.getNumSequences()):
            _state, _action, reward = self.dataset.getSequence(n)
            seqidx = ravel(self.dataset['sequence_index'])
            if n == self.dataset.getNumSequences() - 1:
                # last sequence until end of dataset
                loglh = self.loglh['loglh'][seqidx[n]:, :]
            else:
                loglh = self.loglh['loglh'][seqidx[n]:seqidx[n + 1], :]

            X[n, :-1] = sum(loglh, 0)
            R[n, 0] = sum(reward, 0)

        # linear regression
        beta = dot(pinv(X), R)
        return beta[:-1]


########NEW FILE########
__FILENAME__ = gpomdp
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from policygradient import PolicyGradientLearner
from scipy import zeros, mean

### NOT WORKING YET ###

class GPOMDP(PolicyGradientLearner):
    def __init__(self):
        PolicyGradientLearner.__init__(self)

    def calculateGradient(self):

        # normalize rewards
        # self.ds.data['reward'] /= max(ravel(abs(self.ds.data['reward'])))

        g = zeros((self.ds.getNumSequences(), self.ds.getDimension('loglh')), float)

        # get maximal length
        maxlen = max([self.ds.getSequenceLength(n) for n in range(self.ds.getNumSequences())])
        baselines = zeros((maxlen, self.ds.getDimension('loglh')), float)
        seqcount = zeros((maxlen, 1))

        # calculcate individual baseline for each timestep and episode
        for seq in range(self.ds.getNumSequences()):
            _, _, rewards, loglhs = self.ds.getSequence(seq)
            for t in range(len(rewards)):
                baselines[t, :] += mean(sum(loglhs[:t + 1, :], 0) ** 2 * rewards[t, :], 0) / mean(sum(loglhs[:t + 1, :], 0) ** 2, 0)
                seqcount[t, :] += 1

        baselines = baselines / seqcount
        # print(baselines)
        for seq in range(self.ds.getNumSequences()):
            _, _, rewards, loglhs = self.ds.getSequence(seq)
            for t in range(len(rewards)):
                g[seq, :] += sum(loglhs[:t + 1, :], 0) * (rewards[t, :] - baselines[t])

        gradient = mean(g, 0)
        return gradient

########NEW FILE########
__FILENAME__ = policygradient
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from pybrain.rl.learners.directsearch.directsearch import DirectSearchLearner
from pybrain.rl.learners.learner import DataSetLearner, ExploringLearner
from pybrain.utilities import abstractMethod
from pybrain.auxiliary import GradientDescent
from pybrain.rl.explorers.continuous.normal import NormalExplorer
from pybrain.datasets.dataset import DataSet
from pybrain.structure.networks import FeedForwardNetwork
from pybrain.structure.connections import IdentityConnection


class LoglhDataSet(DataSet):
    def __init__(self, dim):
        DataSet.__init__(self)
        self.addField('loglh', dim)
        self.linkFields(['loglh'])
        self.index = 0


class PolicyGradientLearner(DirectSearchLearner, DataSetLearner, ExploringLearner):
    """ PolicyGradientLearner is a super class for all continuous direct search
        algorithms that use the log likelihood of the executed action to update
        the weights. Subclasses are ENAC, GPOMDP, or REINFORCE.
    """

    _module = None

    def __init__(self):
        # gradient descender
        self.gd = GradientDescent()

        # create default explorer
        self._explorer = None

        # loglh dataset
        self.loglh = None

        # network to tie module and explorer together
        self.network = None


    def _setLearningRate(self, alpha):
        """ pass the alpha value through to the gradient descent object """
        self.gd.alpha = alpha

    def _getLearningRate(self):
        return self.gd.alpha

    learningRate = property(_getLearningRate, _setLearningRate)

    def _setModule(self, module):
        """ initialize gradient descender with module parameters and
            the loglh dataset with the outdim of the module. """
        self._module = module

        # initialize explorer
        self._explorer = NormalExplorer(module.outdim)

        # build network
        self._initializeNetwork()

    def _getModule(self):
        return self._module

    module = property(_getModule, _setModule)

    def _setExplorer(self, explorer):
        """ assign non-standard explorer to the policy gradient learner.
            requires the module to be set beforehand.
        """
        assert self._module

        self._explorer = explorer

        # build network
        self._initializeNetwork()

    def _getExplorer(self):
        return self._explorer

    explorer = property(_getExplorer, _setExplorer)


    def _initializeNetwork(self):
        """ build the combined network consisting of the module and
            the explorer and initializing the log likelihoods dataset.
        """
        self.network = FeedForwardNetwork()
        self.network.addInputModule(self._module)
        self.network.addOutputModule(self._explorer)
        self.network.addConnection(IdentityConnection(self._module, self._explorer))
        self.network.sortModules()

        # initialize gradient descender
        self.gd.init(self.network.params)

        # initialize loglh dataset
        self.loglh = LoglhDataSet(self.network.paramdim)


    def learn(self):
        """ calls the gradient calculation function and executes a step in direction
            of the gradient, scaled with a small learning rate alpha. """
        assert self.dataset != None
        assert self.module != None

        # calculate the gradient with the specific function from subclass
        gradient = self.calculateGradient()

        # scale gradient if it has too large values
        if max(gradient) > 1000:
            gradient = gradient / max(gradient) * 1000

        # update the parameters of the module
        p = self.gd(gradient.flatten())
        self.network._setParameters(p)
        self.network.reset()

    def explore(self, state, action):
        # forward pass of exploration
        explorative = ExploringLearner.explore(self, state, action)

        # backward pass through network and store derivs
        self.network.backward()
        self.loglh.appendLinked(self.network.derivs.copy())

        return explorative

    def reset(self):
        self.loglh.clear()

    def calculateGradient(self):
        abstractMethod()

########NEW FILE########
__FILENAME__ = reinforce
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from pybrain.rl.learners.directsearch.policygradient import PolicyGradientLearner
from scipy import mean, ravel, array


class Reinforce(PolicyGradientLearner):
    """ Reinforce is a gradient estimator technique by Williams (see
        "Simple Statistical Gradient-Following Algorithms for
        Connectionist Reinforcement Learning"). It uses optimal
        baselines and calculates the gradient with the log likelihoods
        of the taken actions. """

    def calculateGradient(self):
        # normalize rewards
        # self.ds.data['reward'] /= max(ravel(abs(self.ds.data['reward'])))

        # initialize variables
        returns = self.dataset.getSumOverSequences('reward')
        seqidx = ravel(self.dataset['sequence_index'])

        # sum of sequences up to n-1
        loglhs = [sum(self.loglh['loglh'][seqidx[n]:seqidx[n + 1], :]) for n in range(self.dataset.getNumSequences() - 1)]
        # append sum of last sequence as well
        loglhs.append(sum(self.loglh['loglh'][seqidx[-1]:, :]))
        loglhs = array(loglhs)

        baselines = mean(loglhs ** 2 * returns, 0) / mean(loglhs ** 2, 0)
        # TODO: why gradient negative?
        gradient = -mean(loglhs * (returns - baselines), 0)

        return gradient


########NEW FILE########
__FILENAME__ = rwr
__author__ = 'Tom Schaul, tom@idsia.ch and Daan Wiertra, daan@idsia.ch'

from scipy import zeros, array, mean, randn, exp, dot, argmax

from pybrain.datasets import ReinforcementDataSet, ImportanceDataSet, SequentialDataSet
from pybrain.supervised import BackpropTrainer
from pybrain.utilities import drawIndex
from pybrain.rl.learners.directsearch.directsearch import DirectSearchLearner


# TODO: greedy runs: start once in every possible starting state!
# TODO: supervised: train-set, test-set, early stopping -> actual convergence!


class RWR(DirectSearchLearner):
    """ Reward-weighted regression.

    The algorithm is currently limited to discrete-action episodic tasks, subclasses of POMDPTasks.
    """

    # parameters
    batchSize = 20

    # feedback settings
    verbose = True
    greedyRuns = 20
    supervisedPlotting = False

    # settings for the supervised training
    learningRate = 0.005
    momentum = 0.9
    maxEpochs = 20
    validationProportion = 0.33
    continueEpochs = 2

    # parameters for the variation that uses a value function
    # TODO: split into 2 classes.
    valueLearningRate = None
    valueMomentum = None
    #valueTrainEpochs = 5
    resetAllWeights = False
    netweights = 0.01

    def __init__(self, net, task, valueNetwork=None, **args):
        self.net = net
        self.task = task
        self.setArgs(**args)
        if self.valueLearningRate == None:
            self.valueLearningRate = self.learningRate
        if self.valueMomentum == None:
            self.valueMomentum = self.momentum
        if self.supervisedPlotting:
            from pylab import ion
            ion()

        # adaptive temperature:
        self.tau = 1.

        # prepare the datasets to be used
        self.weightedDs = ImportanceDataSet(self.task.outdim, self.task.indim)
        self.rawDs = ReinforcementDataSet(self.task.outdim, self.task.indim)
        self.valueDs = SequentialDataSet(self.task.outdim, 1)

        # prepare the supervised trainers
        self.bp = BackpropTrainer(self.net, self.weightedDs, self.learningRate,
                                  self.momentum, verbose=False,
                                  batchlearning=True)

        # CHECKME: outsource
        self.vnet = valueNetwork
        if valueNetwork != None:
            self.vbp = BackpropTrainer(self.vnet, self.valueDs, self.valueLearningRate,
                                       self.valueMomentum, verbose=self.verbose)

        # keep information:
        self.totalSteps = 0
        self.totalEpisodes = 0

    def shapingFunction(self, R):
        return exp(self.tau * R)

    def updateTau(self, R, U):
        self.tau = sum(U) / dot((R - self.task.minReward), U)

    def reset(self):
        self.weightedDs.clear()
        self.valueDs.clear()
        self.rawDs.clear()
        self.bp.momentumvector *= 0.0
        if self.vnet != None:
            self.vbp.momentumvector *= 0.0
            if self.resetAllWeights:
                self.vnet.params[:] = randn(len(self.vnet.params)) * self.netweights

    def greedyEpisode(self):
        """ run one episode with greedy decisions, return the list of rewards recieved."""
        rewards = []
        self.task.reset()
        self.net.reset()
        while not self.task.isFinished():
            obs = self.task.getObservation()
            act = self.net.activate(obs)
            chosen = argmax(act)
            self.task.performAction(chosen)
            reward = self.task.getReward()
            rewards.append(reward)
        return rewards

    def learn(self, batches):
        self.greedyAvg = []
        self.rewardAvg = []
        self.lengthAvg = []
        self.initr0Avg = []
        for b in range(batches):
            if self.verbose:
                print
                print('Batch', b + 1)
            self.reset()
            self.learnOneBatch()
            self.totalEpisodes += self.batchSize

            # greedy measure (avg over some greedy runs)
            rws = 0.
            for dummy in range(self.greedyRuns):
                tmp = self.greedyEpisode()
                rws += (sum(tmp) / float(len(tmp)))
            self.greedyAvg.append(rws / self.greedyRuns)
            if self.verbose:
                print('::', round(rws / self.greedyRuns, 5), '::')

    def learnOneBatch(self):
        # collect a batch of runs as experience
        r0s = []
        lens = []
        avgReward = 0.
        for dummy in range(self.batchSize):
            self.rawDs.newSequence()
            self.valueDs.newSequence()
            self.task.reset()
            self.net.reset()
            acts, obss, rewards = [], [], []
            while not self.task.isFinished():
                obs = self.task.getObservation()
                act = self.net.activate(obs)
                chosen = drawIndex(act)
                self.task.performAction(chosen)
                reward = self.task.getReward()
                obss.append(obs)
                y = zeros(len(act))
                y[chosen] = 1
                acts.append(y)
                rewards.append(reward)
            avgReward += sum(rewards) / float(len(rewards))

            # compute the returns from the list of rewards
            current = 0
            returns = []
            for r in reversed(rewards):
                current *= self.task.discount
                current += r
                returns.append(current)
            returns.reverse()
            for i in range(len(obss)):
                self.rawDs.addSample(obss[i], acts[i], returns[i])
                self.valueDs.addSample(obss[i], returns[i])
            r0s.append(returns[0])
            lens.append(len(returns))

        r0s = array(r0s)
        self.totalSteps += sum(lens)
        avgLen = sum(lens) / float(self.batchSize)
        avgR0 = mean(r0s)
        avgReward /= self.batchSize
        if self.verbose:
            print('***', round(avgLen, 3), '***', '(avg init exp. return:', round(avgR0, 5), ')',)
            print('avg reward', round(avgReward, 5), '(tau:', round(self.tau, 3), ')')
            print(lens)
        # storage:
        self.rewardAvg.append(avgReward)
        self.lengthAvg.append(avgLen)
        self.initr0Avg.append(avgR0)


#        if self.vnet == None:
#            # case 1: no value estimator:

        # prepare the dataset for training the acting network
        shaped = self.shapingFunction(r0s)
        self.updateTau(r0s, shaped)
        shaped /= max(shaped)
        for i, seq in enumerate(self.rawDs):
            self.weightedDs.newSequence()
            for sample in seq:
                obs, act, dummy = sample
                self.weightedDs.addSample(obs, act, shaped[i])

#        else:
#            # case 2: value estimator:
#
#
#            # train the value estimating network
#            if self.verbose: print('Old value error:  ', self.vbp.testOnData())
#            self.vbp.trainEpochs(self.valueTrainEpochs)
#            if self.verbose: print('New value error:  ', self.vbp.testOnData())
#
#            # produce the values and analyze
#            rminusvs = []
#            sizes = []
#            for i, seq in enumerate(self.valueDs):
#                self.vnet.reset()
#                seq = list(seq)
#                for sample in seq:
#                    obs, ret = sample
#                    val = self.vnet.activate(obs)
#                    rminusvs.append(ret-val)
#                    sizes.append(len(seq))
#
#            rminusvs = array(rminusvs)
#            shapedRminusv = self.shapingFunction(rminusvs)
#            # CHECKME: here?
#            self.updateTau(rminusvs, shapedRminusv)
#            shapedRminusv /= array(sizes)
#            shapedRminusv /= max(shapedRminusv)
#
#            # prepare the dataset for training the acting network
#            rvindex = 0
#            for i, seq in enumerate(self.rawDs):
#                self.weightedDs.newSequence()
#                self.vnet.reset()
#                for sample in seq:
#                    obs, act, ret = sample
#                    self.weightedDs.addSample(obs, act, shapedRminusv[rvindex])
#                    rvindex += 1

        # train the acting network
        tmp1, tmp2 = self.bp.trainUntilConvergence(maxEpochs=self.maxEpochs,
                                                   validationProportion=self.validationProportion,
                                                   continueEpochs=self.continueEpochs,
                                                   verbose=self.verbose)
        if self.supervisedPlotting:
            from pylab import plot, legend, figure, clf, draw
            figure(1)
            clf()
            plot(tmp1, label='train')
            plot(tmp2, label='valid')
            legend()
            draw()

        return avgLen, avgR0



########NEW FILE########
__FILENAME__ = learner
""" The top of the learner hierarchy is more conceptual than functional.
The different classes distinguish algorithms in such a way that we can automatically
determine when an algorithm is not applicable for a problem. """

__author__ = 'Tom Schaul, tom@idsia.ch, Thomas Rueckstiess, ruecksti@in.tum.de'


from pybrain.utilities import abstractMethod
import logging


class Learner(object):
    """ Top-level class for all reinforcement learning algorithms.
    Any learning algorithm changes a policy (in some way) in order
    to increase the expected reward/fitness.
    """

    module = None

    def learn(self):
        """ The main method, that invokes a learning step. """
        abstractMethod()


class ExploringLearner(Learner):
    """ A Learner determines how to change the adaptive parameters of a module.
    """

    explorer = None

    def explore(self, state, action):
        if self.explorer is not None:
            return self.explorer.activate(state, action)
        else:
            # logging.warning("No explorer found: no exploration could be done.")
            return action


class EpisodicLearner(Learner):
    """ Assumes the task is episodic, not life-long,
    and therefore does a learning step only after the end of each episode. """

    def learnEpisodes(self, episodes = 1, *args, **kwargs):
        """ learn on the current dataset, for a number of episodes """
        for _ in range(episodes):
            self.learn(*args, **kwargs)

    def newEpisode(self):
        """ informs the learner that a new episode has started. """
        if self.explorer is not None:
            self.explorer.newEpisode()

    def reset(self):
        pass


class DataSetLearner(EpisodicLearner):
    """ A class for learners that learn from a dataset, which has no target output but
        only a reinforcement signal for each sample. It requires a
        ReinforcementDataSet object (which provides state-action-reward tuples). """

    dataset = None

    def learnOnDataset(self, dataset, *args, **kwargs):
        """ set the dataset, and learn """
        self.dataset = dataset
        self.learnEpisodes(*args, **kwargs)


########NEW FILE########
__FILENAME__ = levinsearch
__author__ = 'Tom Schaul, tom@idsia.ch'


def timeBoundExecution(algo, maxtime):
    """ wrap the algo, to stop execution after it has used all its allocated time """
    # TODO
    return algo


class LevinSeach:
    """ a.k.a. Universal Search

    Note: don't run this, it's a bit slow... but it will solve all your problems! """

    def stoppingCriterion(self, val):
        return val == True

    def run(self, input, generator):
        complexities = {}

        # an iterator over all valid programs, by increasing complexity, and in
        # lexicographical order, together with its code's complexity.
        piter = generator.orderedEnumeration()

        maxLevin = 1
        # every phase goes through all programs of a certain Levin-complexity.
        while True:

            # generate all programs that might be needed in this phase.
            c = 0
            while c <= maxLevin:
                try:
                    c, p = piter.next()
                except StopIteration:
                    break
                if c not in complexities:
                    complexities[c] = [p]
                else:
                    complexities[c].append(p)

            # execute all programs, but with a set time-limit:
            # every phase the total time used doubles (= 2**maxLevin)
            for c in range(maxLevin):
                for p in complexities[c]:
                    boundP = timeBoundExecution(p, 2**(maxLevin-c)/maxLevin)
                    res = boundP.run(input)
                    if self.stoppingCriterion(res):
                        return res

            maxLevin += 1
########NEW FILE########
__FILENAME__ = meta
__author__ = 'Tom Schaul, tom@idsia.ch'


from pybrain.rl.learners.learner import Learner


class MetaLearner(Learner):
    """ Learners that make use of other Learners, or learn how to learn. """
########NEW FILE########
__FILENAME__ = leastsquares
__author__ = 'Tom Schaul, tom@idsia.ch'

""" 
Doing RL when an environment model (transition matrices and rewards) are available,
and the states are observed by a feature vector for each state:

 - a feature map (fMap) is a 2D array of features, one row per state.

(otherwise same representation as for in policyiteration.py)

Here we provide some algorithms when the values are estimated linearly from the features.

"""

# TODO: optimize LSTDQ by vectorization

import random
from scipy import ravel, zeros, outer, dot, tile, transpose, tensordot
from scipy.linalg import lstsq
from numpy.matlib import repmat


from pybrain.rl.learners.modelbased.policyiteration import randomPolicy, greedyQPolicy, collapsedTransitions, policyIteration


def trueFeatureStats(T, R, fMap, discountFactor, stateProp=1, MAT_LIMIT=1e8):
    """ Gather the statistics needed for LSTD,
    assuming infinite data (true probabilities).
    Option: if stateProp is  < 1, then only a proportion of all 
    states will be seen as starting state for transitions """
    dim = len(fMap)
    numStates = len(T)
    statMatrix = zeros((dim, dim))
    statResidual = zeros(dim)
    ss = range(numStates)
    repVersion = False
    
    if stateProp < 1:
        ss = random.sample(ss, int(numStates * stateProp))
    elif dim * numStates**2 < MAT_LIMIT:
        repVersion = True
    
    # two variants, depending on how large we can afford our matrices to become.        
    if repVersion:    
        tmp1 = tile(fMap, (numStates,1,1))
        tmp2 = transpose(tmp1, (2,1,0))
        tmp3 = tmp2 - discountFactor * tmp1            
        tmp4 = tile(T, (dim,1,1))
        tmp4 *= transpose(tmp1, (1,2,0))
        statMatrix = tensordot(tmp3, tmp4, axes=[[0,2], [1,2]]).T
        statResidual = dot(R, dot(fMap, T).T)
    else:
        for sto in ss:
            tmp = fMap - discountFactor * repmat(fMap[:, sto], numStates, 1).T
            tmp2 = fMap * repmat(T[:, sto], dim, 1)
            statMatrix += dot(tmp2, tmp.T)             
            statResidual += R[sto] * dot(fMap, T[:, sto])
    return statMatrix, statResidual


def LSTD_values(T, R, fMap, discountFactor, **kwargs):
    """ Least-squares temporal difference algorithm. """
    statMatrix, statResidual = trueFeatureStats(T, R, fMap, discountFactor,**kwargs)
    weights = lstsq(statMatrix, statResidual)[0]
    return dot(weights, fMap)


def LSTD_Qvalues(Ts, policy, R, fMap, discountFactor):
    """ LSTDQ is like LSTD, but with features replicated 
    once for each possible action.
    
    Returns Q-values in a 2D array. """
    numA = len(Ts)
    dim = len(Ts[0])
    numF = len(fMap)
    fMapRep = zeros((numF * numA, dim * numA))
    for a in range(numA):
        fMapRep[numF * a:numF * (a + 1), dim * a:dim * (a + 1)] = fMap

    statMatrix = zeros((numF * numA, numF * numA))
    statResidual = zeros(numF * numA)
    for sto in range(dim):
        r = R[sto]
        fto = zeros(numF * numA)
        for nextA in range(numA):
            fto += fMapRep[:, sto + nextA * dim] * policy[sto][nextA]
        for sfrom in range(dim):
            for a in range(numA):
                ffrom = fMapRep[:, sfrom + a * dim]
                prob = Ts[a][sfrom, sto]
                statMatrix += outer(ffrom, ffrom - discountFactor * fto) * prob
                statResidual += ffrom * r * prob

    Qs = zeros((dim, numA))
    w = lstsq(statMatrix, statResidual)[0]
    for a in range(numA):
        Qs[:,a] = dot(w[numF*a:numF*(a+1)], fMap)
    return Qs


def LSPI_policy(fMap, Ts, R, discountFactor, initpolicy=None, maxIters=20):
    """ LSPI is like policy iteration, but Q-values are estimated based 
    on the feature map. 
    Returns the best policy found. """
    if initpolicy is None:
        policy, _ = randomPolicy(Ts) 
    else:
        policy = initpolicy
    
    while maxIters > 0:
        Qs = LSTD_Qvalues(Ts, policy, R, fMap, discountFactor)
        newpolicy = greedyQPolicy(Qs)
        if sum(ravel(abs(newpolicy - policy))) < 1e-3:
            return policy, collapsedTransitions(Ts, policy)
        policy = newpolicy
        maxIters -= 1
    return policy, collapsedTransitions(Ts, policy)


def LSTD_PI_policy(fMap, Ts, R, discountFactor, initpolicy=None, maxIters=20):
    """ Alternative version of LSPI using value functions
    instead of state-action values as intermediate.
    """
    def veval(T):
        return LSTD_values(T, R, fMap, discountFactor)
    return policyIteration(Ts, R, discountFactor, VEvaluator=veval, 
                           initpolicy=initpolicy, maxIters=maxIters)

########NEW FILE########
__FILENAME__ = policyiteration
__author__ = 'Tom Schaul, tom@idsia.ch'

""" 
Doing RL when an environment model (transition matrices and rewards) are available.


Representation:
 - a policy is a 2D-array of probabilities, 
    one row per state (summing to 1), one column per action.
 
 - a transition matrix (T) maps from originating states to destination states 
    (probabilities in each row sum to 1).
 
 - a reward vector (R) maps each state to the reward value obtained when entering (or staying in) a state.
 
 - a feature map (fMap) is a 2D array of features, one row per state.
 
 - a task model is defined by a list of transition matrices (Ts), one per action, a 
    reward vector R, a discountFactor
    
Note: a task model combined with a policy is again a transition matrix ("collapsed" dynamics).
        
 - a value function (V) is a vector of expected discounted rewards (one per state).
 
 - a set of state-action values (Qs) is a 2D array, one row per action.
 
"""

# TODO: we may use an alternative, more efficient representation if all actions are deterministic 
# TODO: it may be worth considering a sparse representation of T matrices.
# TODO: optimize some of this code with vectorization
    

from scipy import dot, zeros, zeros_like, ones, mean, array, ravel, rand
from numpy.matlib import repmat

from pybrain.utilities import all_argmax


def trueValues(T, R, discountFactor):
    """ Compute the true discounted value function for each state,
    given a policy (encoded as collapsed transition matrix). """
    assert discountFactor < 1
    distr = T.copy()
    res = dot(T, R)
    for i in range(1, int(10 / (1. - discountFactor))):
        distr = dot(distr, T)
        res += (discountFactor ** i) * dot(distr, R)
    return res


def trueQValues(Ts, R, discountFactor, policy):
    """ The true Q-values, given a model and a policy. """
    T = collapsedTransitions(Ts, policy)
    V = trueValues(T, R, discountFactor)
    Vnext = V*discountFactor+R
    numA = len(Ts)
    dim = len(R)
    Qs = zeros((dim, numA))
    for si in range(dim):
        for a in range(numA):
            Qs[si, a] = dot(Ts[a][si], Vnext)   
    return Qs


def collapsedTransitions(Ts, policy):
    """ Collapses a list of transition matrices (one per action) and a list 
        of action probability vectors into a single transition matrix."""
    res = zeros_like(Ts[0])
    dim = len(Ts[0])
    for ai, ap in enumerate(policy.T):
        res += Ts[ai] * repmat(ap, dim, 1).T
    return res


def greedyPolicy(Ts, R, discountFactor, V):
    """ Find the greedy policy, (soft tie-breaking)
    given a value function and full transition model. """
    dim = len(V)
    numA = len(Ts)
    Vnext = V*discountFactor+R
    policy = zeros((dim, numA))
    for si in range(dim):
        actions = all_argmax([dot(T[si, :], Vnext) for T in Ts])
        for a in actions:
            policy[si, a] = 1. / len(actions)        
    return policy, collapsedTransitions(Ts, policy)    


def greedyQPolicy(Qs):
    """ Find the greedy deterministic policy, 
    given the Q-values. """
    dim = len(Qs)
    numA = len(Qs[0])
    policy = zeros((dim, numA))
    for si in range(dim):
        actions = all_argmax(Qs[si])
        for a in actions:
            policy[si, a] = 1. / len(actions)    
    return policy


def randomPolicy(Ts):
    """ Each action is equally likely. """
    numA = len(Ts)
    dim = len(Ts[0])
    return ones((dim, numA)) / float(numA), mean(array(Ts), axis=0)


def randomDeterministic(Ts):
    """ Pick a random deterministic action for each state. """
    numA = len(Ts)
    dim = len(Ts[0])
    choices = (rand(dim) * numA).astype(int)
    policy = zeros((dim, numA))
    for si, a in choices:
        policy[si, a] = 1
    return policy, collapsedTransitions(Ts, policy)


def policyIteration(Ts, R, discountFactor, VEvaluator=None, initpolicy=None, maxIters=20):
    """ Given transition matrices (one per action),
    produce the optimal policy, using the policy iteration algorithm.
    
    A custom function that maps policies to value functions can be provided. """
    if initpolicy is None:
        policy, T = randomPolicy(Ts) 
    else:
        policy = initpolicy
        T = collapsedTransitions(Ts, policy)
        
    if VEvaluator is None:
        VEvaluator = lambda T: trueValues(T, R, discountFactor)
    
    while maxIters > 0:
        V = VEvaluator(T)
        newpolicy, T = greedyPolicy(Ts, R, discountFactor, V)
        # if the probabilities are not changing more than by 0.001, we're done.
        if sum(ravel(abs(newpolicy - policy))) < 1e-3:
            return policy, T
        policy = newpolicy
        maxIters -= 1
    return policy, T


########NEW FILE########
__FILENAME__ = interface
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from pybrain.utilities import abstractMethod
from pybrain.structure.modules import Table, Module, TanhLayer, LinearLayer, BiasUnit
from pybrain.structure.connections import FullConnection
from pybrain.structure.networks import FeedForwardNetwork
from pybrain.structure.parametercontainer import ParameterContainer
from pybrain.tools.shortcuts import buildNetwork
from pybrain.utilities import one_to_n

from scipy import argmax, array, r_, asarray, where
from random import choice


class ActionValueInterface(object):
    """ Interface for different ActionValue modules, like the
        ActionValueTable or the ActionValueNetwork.
    """

    numActions = None

    def getMaxAction(self, state):
        abstractMethod()

    def getActionValues(self, state):
        abstractMethod()


class ActionValueTable(Table, ActionValueInterface):
    """ A special table that is used for Value Estimation methods
        in Reinforcement Learning. This table is used for value-based
        TD algorithms like Q or SARSA.
    """

    def __init__(self, numStates, numActions, name=None):
        Module.__init__(self, 1, 1, name)
        ParameterContainer.__init__(self, numStates * numActions)
        self.numRows = numStates
        self.numColumns = numActions

    @property
    def numActions(self):
        return self.numColumns

    def _forwardImplementation(self, inbuf, outbuf):
        """ Take a vector of length 1 (the state coordinate) and return
            the action with the maximum value over all actions for this state.
        """
        outbuf[0] = self.getMaxAction(inbuf[0])

    def getMaxAction(self, state):
        """ Return the action with the maximal value for the given state. """
        values = self.params.reshape(self.numRows, self.numColumns)[state, :].flatten()
        action = where(values == max(values))[0]
        action = choice(action)
        return action

    def getActionValues(self, state):
        return self.params.reshape(self.numRows, self.numColumns)[state, :].flatten()

    def initialize(self, value=0.0):
        """ Initialize the whole table with the given value. """
        self._params[:] = value


class ActionValueNetwork(Module, ActionValueInterface):
    """ A network that approximates action values for continuous state /
        discrete action RL environments. To receive the maximum action
        for a given state, a forward pass is executed for all discrete
        actions, and the maximal action is returned. This network is used
        for the NFQ algorithm. """

    def __init__(self, dimState, numActions, name=None):
        Module.__init__(self, dimState, 1, name)
        self.network = buildNetwork(dimState + numActions, dimState + numActions, 1)
        self.numActions = numActions

    def _forwardImplementation(self, inbuf, outbuf):
        """ takes the state vector and return the discrete action with
            the maximum value over all actions for this state.
        """
        outbuf[0] = self.getMaxAction(asarray(inbuf))

    def getMaxAction(self, state):
        """ Return the action with the maximal value for the given state. """
        return argmax(self.getActionValues(state))

    def getActionValues(self, state):
        """ Run forward activation for each of the actions and returns all values. """
        values = array([self.network.activate(r_[state, one_to_n(i, self.numActions)]) for i in range(self.numActions)])
        return values

    def getValue(self, state, action):
        return self.network.activate(r_[state, one_to_n(action, self.numActions)])
########NEW FILE########
__FILENAME__ = linearfa
__author__ = 'Tom Schaul, tom@idsia.ch'

""" RL with linear function approximation. 

In part inspired by pseudo-code from Szepesvari's 'Algorithms for RL' (2010)
"""

from pybrain.rl.learners.valuebased.valuebased import ValueBasedLearner
from scipy import zeros, dot, outer, exp, clip, ravel, ones, rand, array, randn
from scipy.linalg import pinv2
from pybrain.utilities import r_argmax, fListToString, setAllArgs
import unittest
from random import choice, randint

def rv_dot(x, y):
    return dot(ravel(x), ravel(y))


class LinearFALearner(ValueBasedLearner):
    """ A reinforcement learner using linear function approximation (FA),
    on the states (the actions remain discrete/tabular).
     
    The weights (theta) are arranged in a 2D-array: state-features by actions.
    
    The 'state' field in the ReinforcementDataSet is to be interpreted as state-features now.
    
    Superclass for all the actual algorithms.
    """
    
    learningRate = 0.5      # aka alpha: make sure this is being decreased by calls from the learning agent!
    learningRateDecay = 100 # aka n_0, but counting decay-calls
    
    randomInit = True
    
    rewardDiscount = 0.99 # aka gamma
    
    batchMode = False
    passNextAction = False # for the _updateWeights method    
    
    def __init__(self, num_actions, num_features, **kwargs):
        ValueBasedLearner.__init__(self)
        setAllArgs(self, kwargs)
        self.explorer = None        
        self.num_actions = num_actions
        self.num_features = num_features
        if self.randomInit:
            self._theta = randn(self.num_actions, self.num_features) / 10.
        else:
            self._theta = zeros((self.num_actions, self.num_features))
        self._additionalInit()
        self._behaviorPolicy = self._boltzmannPolicy
        self.reset()
        
    def _additionalInit(self):
        pass
        
    def _qValues(self, state):
        """ Return vector of q-values for all actions, 
        given the state(-features). """
        return dot(self._theta, state)
    
    def _greedyAction(self, state):
        return r_argmax(self._qValues(state))
    
    def _greedyPolicy(self, state):
        tmp = zeros(self.num_actions)
        tmp[self._greedyAction(state)] = 1
        return tmp
    
    def _boltzmannPolicy(self, state, temperature=1.):
        tmp = self._qValues(state)
        return LinearFALearner._boltzmannProbs(tmp, temperature)
    
    @staticmethod
    def _boltzmannProbs(qvalues, temperature=1.):
        if temperature == 0:
            tmp = zeros(len(qvalues))        
            tmp[r_argmax(qvalues)] = 1.
        else:
            tmp = qvalues / temperature            
            tmp -= max(tmp)        
            tmp = exp(clip(tmp, -20, 0))
        return tmp / sum(tmp)
        
    def reset(self):        
        ValueBasedLearner.reset(self)        
        self._callcount = 0
        self.newEpisode()
    
    def newEpisode(self):  
        ValueBasedLearner.newEpisode(self)      
        self._callcount += 1
        self.learningRate *= ((self.learningRateDecay + self._callcount) 
                              / (self.learningRateDecay + self._callcount + 1.))

    
class Q_LinFA(LinearFALearner):
    """ Standard Q-learning with linear FA. """
    
    def _updateWeights(self, state, action, reward, next_state):
        """ state and next_state are vectors, action is an integer. """
        td_error = reward + self.rewardDiscount * max(dot(self._theta, next_state)) - dot(self._theta[action], state) 
        #print(action, reward, td_error,self._theta[action], state, dot(self._theta[action], state))
        #print(self.learningRate * td_error * state)
        #print()
        self._theta[action] += self.learningRate * td_error * state 
          

class QLambda_LinFA(LinearFALearner):
    """ Q-lambda with linear FA. """
    
    _lambda = 0.9
    
    def newEpisode(self):
        """ Reset eligibilities after each episode. """
        LinearFALearner.newEpisode(self)
        self._etraces = zeros((self.num_actions, self.num_features))
        
    def _updateEtraces(self, state, action, responsibility=1.):
        self._etraces *= self.rewardDiscount * self._lambda * responsibility
        self._etraces[action] += state 
            
    def _updateWeights(self, state, action, reward, next_state):
        """ state and next_state are vectors, action is an integer. """
        self._updateEtraces(state, action)
        td_error = reward + self.rewardDiscount * max(dot(self._theta, next_state)) - dot(self._theta[action], state)
        self._theta += self.learningRate * td_error * self._etraces  
        
    
class SARSALambda_LinFA(QLambda_LinFA):
    
    passNextAction = True
    
    def _updateWeights(self, state, action, reward, next_state, next_action):
        """ state and next_state are vectors, action is an integer. """
        td_error = reward + self.rewardDiscount * dot(self._theta[next_action], next_state) - dot(self._theta[action], state)
        self._updateEtraces(state, action)
        self._theta += self.learningRate * td_error * self._etraces  
        
        
class LSTDQLambda(QLambda_LinFA):
    """ Least-squares Q(lambda)"""
        
    def _additionalInit(self):
        phi_size = self.num_actions * self.num_features
        self._A = zeros((phi_size, phi_size))
        self._b = zeros(phi_size)                           
    
    def _updateWeights(self, state, action, reward, next_state, learned_policy=None):
        """ Policy is a function that returns a probability vector for all actions, 
        given the current state(-features). """
        if learned_policy is None:
            learned_policy = self._greedyPolicy
        
        self._updateEtraces(state, action)
        
        phi = zeros((self.num_actions, self.num_features))
        phi[action] += state        
        phi_n = outer(learned_policy(next_state), next_state)
        
        self._A += outer(ravel(self._etraces), ravel(phi - self.rewardDiscount * phi_n))
        self._b += reward * ravel(self._etraces)       
        
        self._theta = dot(pinv2(self._A), self._b).reshape(self.num_actions, self.num_features)
        
        
class LSPILambda(LSTDQLambda):
    """ Least-squares policy iteration (incomplete). """        
    # TODO: batch version: iterate until policy converges
        
        
class LSPI(LinearFALearner):
    """ LSPI without eligibility traces. (Mark's version) """
    
    exploring = False
    explorationReward = 1.
    
    passNextAction = True    
    
    lazyInversions = 20
    
    def _additionalInit(self):
        phi_size = self.num_actions * self.num_features
        if self.randomInit:
            self._A = randn(phi_size, phi_size) / 100.
            self._b = randn(phi_size) / 100.
        else:
            self._A = zeros((phi_size, phi_size))
            self._b = zeros(phi_size)          
        self._untouched = ones(phi_size, dtype=bool)
        self._count = 0
    
    def _updateWeights(self, state, action, reward, next_state, next_action):
        phi = zeros((self.num_actions, self.num_features))
        phi[action] += state        
        phi = ravel(phi)
        
        phi_n = zeros((self.num_actions, self.num_features))
        phi_n[next_action] += next_state
        phi_n = ravel(phi_n)
        
        self._A += outer(phi, phi - self.rewardDiscount * phi_n)
        self._b += reward * phi
        
        
        if self.lazyInversions is None or self._count % self.lazyInversions == 0:        
            if self.exploring:
                # add something to all the entries that are untouched
                self._untouched &= (phi == 0)
                res = dot(pinv2(self._A), self._b + self.explorationReward * self._untouched)
            else:
                res = dot(pinv2(self._A), self._b)
            self._theta = res.reshape(self.num_actions, self.num_features)    
            
        self._count += 1
        
        
class GQLambda(QLambda_LinFA):
    """ From Maei/Sutton 2010, with additional info from Adam White. """
    
    sec_learningRate = 1.
    
    def _additionalInit(self):
        self._sec_weights = zeros((self.num_actions, self.num_features)) # w        
    
    def _updateWeights(self, state, action, reward, next_state, behavior_policy=None, learned_policy=None, outcome_reward=None):
        if learned_policy is None:
            learned_policy = self._greedyPolicy
        if behavior_policy is None:
            behavior_policy = self._behaviorPolicy
        
        responsibility = learned_policy(state)[action] / behavior_policy(state)[action]
        self._updateEtraces(state, action, responsibility)
        
        phi_bar = outer(learned_policy(next_state), next_state)
        
        td_error = reward + self.rewardDiscount * rv_dot(self._theta, phi_bar) - dot(self._theta[action], state)
        if outcome_reward is not None:
            td_error += (1 - self.rewardDiscount) * outcome_reward
            
        self._theta += self.learningRate * (td_error * self._etraces - 
                                            self.rewardDiscount * (1 - self._lambda) * rv_dot(self._sec_weights, self._etraces) * phi_bar)
        
        phi = zeros((self.num_actions, self.num_features))
        phi[action] += state
        
        self._sec_weights += self.learningRate * self.sec_learningRate * (td_error * self._etraces - rv_dot(self._sec_weights, phi) * phi)
    


class LearningTester(unittest.TestCase):
    """ Some simple test cases. 
        Note: this can still fail from time to time!"""
        
    verbose = False
    
    algos = [Q_LinFA, QLambda_LinFA, SARSALambda_LinFA, LSPI, LSTDQLambda, GQLambda]    
    need_next_action = [LSPI, SARSALambda_LinFA]
    
    def testSingleStateFullDiscounted(self):
        r = self.runSequences(num_actions=4, num_features=3, num_states=1, num_interactions=500,
                              gamma=0, lr=0.25)
        if self.verbose:
            for x, l in r:
                print(x)
                for a in l:
                    print(fListToString(a[0], 2)        )
        for _, l in r:        
            self.assertAlmostEquals(min(l[0][0]), 1, places=0) 
            self.assertAlmostEquals(max(l[0][0]), 1, places=0) 
            self.assertAlmostEquals(2 * min(l[1][0]), 1, places=0) 
            self.assertAlmostEquals(2 * max(l[1][0]), 1, places=0) 
            self.assertAlmostEquals(min(l[2][0]), 0, places=0) 
            self.assertAlmostEquals(max(l[2][0]), len(l[2][0]) - 1, places=0) 
            self.assertAlmostEquals(min(l[3][0]), max(l[3][0]), places=0)                 
    
    def testSingleState(self):
        r = self.runSequences(num_actions=3, num_features=2, num_states=1, num_interactions=1000,
                              lr=0.2, _lambda=0.5, gamma=0.5)
        if self.verbose:
            for x, l in r:
                print(x)
                for a in l:
                    print(fListToString(a[0], 2)        )
        for _, l in r:
            self.assertAlmostEquals(min(l[0][0]), max(l[0][0]), places=0) 
            self.assertAlmostEquals(min(l[1][0]), max(l[1][0]), places=0)
            self.assertAlmostEquals(min(l[2][0]) + len(l[2][0]) - 1, max(l[2][0]), places=0)             
            self.assertAlmostEquals(min(l[3][0]), max(l[3][0]), places=0)                         
                        
    def testSingleAction(self):        
        r = self.runSequences(num_actions=1, r_states=map(array, [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]]),
                              num_interactions=1000, lr=0.1, _lambda=0.5, gamma=0.5)
        if self.verbose:
            for x, l in r:
                print(x)
                for a in l:
                    print(fListToString(a, 2)        )
        for _, l in r:
            self.assertAlmostEquals(min(l[0]), max(l[0]), places=0) 
            self.assertAlmostEquals(min(l[1]), max(l[1]), places=0)
            self.assertAlmostEquals(min(l[2]), max(l[2]), places=0)             
            self.assertAlmostEquals(max(l[3]) - 1, min(l[3]), places=0) 
            
    def testSimple(self):        
        r = self.runSequences(num_actions=3, num_features=5, num_states=4, num_interactions=2000,
                              lr=0.1, _lambda=0.5, gamma=0.5)        
        if self.verbose:
            for x, l in r:
                print(x)
                for a in l:
                    print(fListToString(a[0], 2)        )
        for _, l in r:
            self.assertAlmostEquals(min(l[0][0]), max(l[0][0]), places=0) 
            self.assertAlmostEquals(min(l[1][0]), max(l[1][0]), places=0)
            self.assertAlmostEquals(min(l[2][0]) + len(l[2][0]) - 1, max(l[2][0]), places=0)             
            self.assertAlmostEquals(min(l[3][0]), max(l[3][0]), places=0) 
            
    def runSequences(self, num_actions=1, num_features=1, num_states=1,
                     num_interactions=10000, gamma=None, _lambda=None, lr=None, r_states=None):
        if r_states is None:
            r_states = [rand(num_features) for _ in range(num_states)]
        else:
            num_features = len(r_states[0])
            num_states = len(r_states)
        state_seq = [choice(r_states) for  _ in range(num_interactions)]
        action_seq = [randint(0, num_actions - 1) for  _ in range(num_interactions)]
        rewards = [ones(num_interactions), rand(num_interactions), action_seq, [s[0] for s in state_seq]]
        datas = [zip(state_seq, action_seq, r) for r in rewards]        
        res = []        
        for algo in self.algos:
            res.append((algo.__name__, []))
            for d in datas:
                l = algo(num_actions, num_features)
                if gamma is not None:       
                    l.rewardDiscount = gamma
                if _lambda is not None:
                    l._lambda = _lambda
                if lr is not None:
                    l.learningRate = lr                     
                self.trainWith(l, d)
                res[-1][-1].append([dot(l._theta, s) for s in r_states])
        return res
            
    def trainWith(self, algo, data):
        last_s = None
        last_a = None
        last_r = None
        for s, a, r in data:
            if last_s is not None:
                if algo.__class__ in self.need_next_action:
                    algo._updateWeights(last_s, last_a, last_r, s, a)
                else:
                    algo._updateWeights(last_s, last_a, last_r, s)
            last_s = s
            last_a = a
            last_r = r        

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = nfq
from scipy import r_

from pybrain.rl.learners.valuebased.valuebased import ValueBasedLearner
from pybrain.datasets import SupervisedDataSet
from pybrain.supervised.trainers.rprop import RPropMinusTrainer
from pybrain.supervised.trainers import BackpropTrainer
from pybrain.utilities import one_to_n


class NFQ(ValueBasedLearner):
    """ Neuro-fitted Q-learning"""

    def __init__(self, maxEpochs=20):
        ValueBasedLearner.__init__(self)
        self.gamma = 0.9
        self.maxEpochs = maxEpochs

    def learn(self):
        # convert reinforcement dataset to NFQ supervised dataset
        supervised = SupervisedDataSet(self.module.network.indim, 1)

        for seq in self.dataset:
            lastexperience = None
            for state, action, reward in seq:
                if not lastexperience:
                    # delay each experience in sequence by one
                    lastexperience = (state, action, reward)
                    continue

                # use experience from last timestep to do Q update
                (state_, action_, reward_) = lastexperience

                Q = self.module.getValue(state_, action_[0])

                inp = r_[state_, one_to_n(action_[0], self.module.numActions)]
                tgt = Q + 0.5*(reward_ + self.gamma * max(self.module.getActionValues(state)) - Q)
                supervised.addSample(inp, tgt)

                # update last experience with current one
                lastexperience = (state, action, reward)

        # train module with backprop/rprop on dataset
        trainer = RPropMinusTrainer(self.module.network, dataset=supervised, batchlearning=True, verbose=False)
        trainer.trainUntilConvergence(maxEpochs=self.maxEpochs)

        # alternative: backprop, was not as stable as rprop
        # trainer = BackpropTrainer(self.module.network, dataset=supervised, learningrate=0.005, batchlearning=True, verbose=True)
        # trainer.trainUntilConvergence(maxEpochs=self.maxEpochs)




########NEW FILE########
__FILENAME__ = q
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from pybrain.rl.learners.valuebased.valuebased import ValueBasedLearner


class Q(ValueBasedLearner):

    offPolicy = True
    batchMode = True

    def __init__(self, alpha=0.5, gamma=0.99):
        ValueBasedLearner.__init__(self)

        self.alpha = alpha
        self.gamma = gamma

        self.laststate = None
        self.lastaction = None

    def learn(self):
        """ Learn on the current dataset, either for many timesteps and
            even episodes (batchMode = True) or for a single timestep
            (batchMode = False). Batch mode is possible, because Q-Learning
            is an off-policy method.

            In batchMode, the algorithm goes through all the samples in the
            history and performs an update on each of them. if batchMode is
            False, only the last data sample is considered. The user himself
            has to make sure to keep the dataset consistent with the agent's
            history.
        """
        if self.batchMode:
            samples = self.dataset
        else:
            samples = [[self.dataset.getSample()]]

        for seq in samples:
            # information from the previous episode (sequence)
            # should not influence the training on this episode
            self.laststate = None
            self.lastaction = None
            self.lastreward = None

            for state, action, reward in seq:

                state = int(state)
                action = int(action)

                # first learning call has no last state: skip
                if self.laststate == None:
                    self.lastaction = action
                    self.laststate = state
                    self.lastreward = reward
                    continue

                qvalue = self.module.getValue(self.laststate, self.lastaction)
                maxnext = self.module.getValue(state, self.module.getMaxAction(state))
                self.module.updateValue(self.laststate, self.lastaction, qvalue + self.alpha * (self.lastreward + self.gamma * maxnext - qvalue))

                # move state to oldstate
                self.laststate = state
                self.lastaction = action
                self.lastreward = reward


########NEW FILE########
__FILENAME__ = qlambda
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from pybrain.rl.learners.valuebased.valuebased import ValueBasedLearner


class QLambda(ValueBasedLearner):
    """ Q-lambda is a variation of Q-learning that uses an eligibility trace. """

    offPolicy = True
    batchMode = False

    def __init__(self, alpha=0.5, gamma=0.99, qlambda=0.9):
        ValueBasedLearner.__init__(self)

        self.alpha = alpha
        self.gamma = gamma
        self.qlambda = qlambda

        self.laststate = None
        self.lastaction = None


    def learn(self):
        states = self.dataset['state']
        actions = self.dataset['action']
        rewards = self.dataset['reward']

        for i in range(states.shape[0] - 1, 0, -1):
            lbda = self.qlambda ** (states.shape[0] - 1 - i)
            # if eligibility trace gets too long, break
            if lbda < 0.0001:
                break

            state = int(states[i])
            laststate = int(states[i - 1])
            # action = int(actions[i])
            lastaction = int(actions[i - 1])
            lastreward = int(rewards[i - 1])

            qvalue = self.module.getValue(laststate, lastaction)
            maxnext = self.module.getValue(state, self.module.getMaxAction(state))
            self.module.updateValue(laststate, lastaction, qvalue + self.alpha * lbda * (lastreward + self.gamma * maxnext - qvalue))

########NEW FILE########
__FILENAME__ = sarsa
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from pybrain.rl.learners.valuebased.valuebased import ValueBasedLearner


class SARSA(ValueBasedLearner):
    """ State-Action-Reward-State-Action (SARSA) algorithm.

    In batchMode, the algorithm goes through all the samples in the
    history and performs an update on each of them. if batchMode is
    False, only the last data sample is considered. The user himself
    has to make sure to keep the dataset consistent with the agent's
    history."""

    offPolicy = False
    batchMode = True

    def __init__(self, alpha=0.5, gamma=0.99):
        ValueBasedLearner.__init__(self)

        self.alpha = alpha
        self.gamma = gamma

        self.laststate = None
        self.lastaction = None

    def learn(self):
        if self.batchMode:
            samples = self.dataset
        else:
            samples = [[self.dataset.getSample()]]

        for seq in samples:
            # information from the previous episode (sequence)
            # should not influence the training on this episode
            self.laststate = None
            self.lastaction = None
            self.lastreward = None
            for state, action, reward in seq:

                state = int(state)
                action = int(action)

                # first learning call has no last state: skip
                if self.laststate == None:
                    self.lastaction = action
                    self.laststate = state
                    self.lastreward = reward
                    continue

                qvalue = self.module.getValue(self.laststate, self.lastaction)
                qnext = self.module.getValue(state, action)
                self.module.updateValue(self.laststate, self.lastaction, qvalue + self.alpha * (self.lastreward + self.gamma * qnext - qvalue))

                # move state to oldstate
                self.laststate = state
                self.lastaction = action
                self.lastreward = reward


########NEW FILE########
__FILENAME__ = valuebased
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from pybrain.rl.learners.learner import ExploringLearner, DataSetLearner, EpisodicLearner
from pybrain.rl.explorers.discrete.egreedy import EpsilonGreedyExplorer


class ValueBasedLearner(ExploringLearner, DataSetLearner, EpisodicLearner):
    """ An RL algorithm based on estimating a value-function."""

    #: Does the algorithm work on-policy or off-policy?
    offPolicy = False

    #: Does the algorithm run in batch mode or online?
    batchMode = True

    _module = None
    _explorer = None

    def __init__(self):
        """ Create a default explorer for discrete learning tasks. """
        self.explorer = EpsilonGreedyExplorer()

    def _setModule(self, module):
        """ Set module and tell explorer about the module. """
        if self.explorer:
            self.explorer.module = module
        self._module = module

    def _getModule(self):
        """ Return the internal module. """
        return self._module

    module = property(_getModule, _setModule)

    def _setExplorer(self, explorer):
        """ Set explorer and tell it the module, if already available. """
        self._explorer = explorer
        if self.module:
            self._explorer.module = self.module

    def _getExplorer(self):
        """ Return the internal explorer. """
        return self._explorer

    explorer = property(_getExplorer, _setExplorer)



########NEW FILE########
__FILENAME__ = connection
__author__ = 'Daan Wierstra and Tom Schaul'

from pybrain.utilities import abstractMethod, Named
from pybrain.structure.moduleslice import ModuleSlice


class Connection(Named):
    """ A connection links 2 modules, more precisely: the output of the first module
    to the input of the second. It can potentially transform the information on the way.
    It also transmits errors backwards between the same modules. """

    inmod = None
    outmod = None
    paramdim = 0

    def __init__(self, inmod, outmod, name = None,
                 inSliceFrom = 0, inSliceTo = None, outSliceFrom = 0, outSliceTo = None):
        """ Every connection requires an input and an output module. Optionally, it is possible to define slices on the buffers.

            :arg inmod: input module
            :arg outmod: output module
            :key inSliceFrom: starting index on the buffer of inmod (default = 0)
            :key inSliceTo: ending index on the buffer of inmod (default = last)
            :key outSliceFrom: starting index on the buffer of outmod (default = 0)
            :key outSliceTo: ending index on the buffer of outmod (default = last)
        """
        self._name = name
        self.inSliceFrom = inSliceFrom
        self.outSliceFrom = outSliceFrom
        if inSliceTo is not None:
            self.inSliceTo = inSliceTo
        else:
            self.inSliceTo = inmod.outdim
        if outSliceTo is not None:
            self.outSliceTo = outSliceTo
        else:
            self.outSliceTo = outmod.indim

        if isinstance(inmod, ModuleSlice):
            self.inmod = inmod.base
            self.inSliceFrom += inmod.outOffset
            self.inSliceTo += inmod.outOffset
        else:
            self.inmod = inmod

        if isinstance(outmod, ModuleSlice):
            self.outmod = outmod.base
            self.outSliceFrom += outmod.inOffset
            self.outSliceTo += outmod.inOffset
        else:
            self.outmod = outmod

        self.indim = self.inSliceTo - self.inSliceFrom
        self.outdim = self.outSliceTo - self.outSliceFrom

        # arguments for for xml
        self.setArgs(inmod = self.inmod, outmod = self.outmod)
        if self.inSliceFrom > 0:
            self.setArgs(inSliceFrom = self.inSliceFrom)
        if self.outSliceFrom > 0:
            self.setArgs(outSliceFrom = self.outSliceFrom)
        if self.inSliceTo < self.inmod.outdim:
            self.setArgs(inSliceTo = self.inSliceTo)
        if self.outSliceTo < self.outmod.indim:
            self.setArgs(outSliceTo = self.outSliceTo)


    def forward(self, inmodOffset=0, outmodOffset=0):
        """Propagate the information from the incoming module's output buffer,
        adding it to the outgoing node's input buffer, and possibly transforming
        it on the way.

        For this transformation use inmodOffset as an offset for the inmod and
        outmodOffset as an offset for the outmodules offset."""
        self._forwardImplementation(
            self.inmod.outputbuffer[inmodOffset, self.inSliceFrom:self.inSliceTo],
            self.outmod.inputbuffer[outmodOffset, self.outSliceFrom:self.outSliceTo])


    def backward(self, inmodOffset=0, outmodOffset=0):
        """Propagate the error found at the outgoing module, adding it to the
        incoming module's output-error buffer and doing the inverse
        transformation of forward propagation.

        For this transformation use inmodOffset as an offset for the inmod and
        outmodOffset as an offset for the outmodules offset.

        If appropriate, also compute the parameter derivatives. """

        self._backwardImplementation(
            self.outmod.inputerror[outmodOffset, self.outSliceFrom:self.outSliceTo],
            self.inmod.outputerror[inmodOffset, self.inSliceFrom:self.inSliceTo],
            self.inmod.outputbuffer[inmodOffset, self.inSliceFrom:self.inSliceTo])

    def _forwardImplementation(self, inbuf, outbuf):
        abstractMethod()

    def _backwardImplementation(self, outerr, inerr, inbuf):
        abstractMethod()

    def __repr__(self):
        """A simple representation (this should probably be expanded by
        subclasses). """
        params = {
            'class': self.__class__.__name__,
            'name': self.name,
            'inmod': self.inmod.name,
            'outmod': self.outmod.name
        }
        return "<%(class)s '%(name)s': '%(inmod)s' -> '%(outmod)s'>" % params
########NEW FILE########
__FILENAME__ = full
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import reshape, dot, outer

from pybrain.structure.connections.connection import Connection
from pybrain.structure.parametercontainer import ParameterContainer


class FullConnection(Connection, ParameterContainer):
    """Connection which fully connects every element from the first module's
    output buffer to the second module's input buffer in a matrix multiplicative
    manner."""

    def __init__(self, *args, **kwargs):
        Connection.__init__(self, *args, **kwargs)
        ParameterContainer.__init__(self, self.indim*self.outdim)

    def _forwardImplementation(self, inbuf, outbuf):
        outbuf += dot(reshape(self.params, (self.outdim, self.indim)), inbuf)

    def _backwardImplementation(self, outerr, inerr, inbuf):
        inerr += dot(reshape(self.params, (self.outdim, self.indim)).T, outerr)
        ds = self.derivs
        ds += outer(inbuf, outerr).T.flatten()

    def whichBuffers(self, paramIndex):
        """Return the index of the input module's output buffer and
        the output module's input buffer for the given weight."""
        return paramIndex % self.inmod.outdim, paramIndex / self.inmod.outdim

########NEW FILE########
__FILENAME__ = fullnotself
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from scipy import reshape, dot, outer, eye
from pybrain.structure.connections import FullConnection


class FullNotSelfConnection(FullConnection):
    """Connection which connects every element from the first module's
    output buffer to the second module's input buffer in a matrix multiplicative
    manner, EXCEPT the corresponding elements with the same index of each buffer
    (the diagonal of the parameter matrix is 0). Asserts that in and out dimensions
    are equal. """
    #:TODO: the values on the diagonal are counted as parameters but not used! FIX!

    def __init__(self, *args, **kwargs):
        FullConnection.__init__(self, *args, **kwargs)
        assert self.indim == self.outdim, \
            "Indim (%i) does not equal outdim (%i)" % (
            self.indim, self.outdim)

    def _forwardImplementation(self, inbuf, outbuf):
        p = reshape(self.params, (self.outdim, self.indim)) * (1-eye(self.outdim))
        outbuf += dot(p, inbuf)

    def _backwardImplementation(self, outerr, inerr, inbuf):
        p = reshape(self.params, (self.outdim, self.indim)) * (1-eye(self.outdim))
        inerr += dot(p.T, outerr)
        ds = self.derivs
        ds += outer(inbuf, outerr).T.flatten()

########NEW FILE########
__FILENAME__ = identity
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.structure.connections.connection import Connection


class IdentityConnection(Connection):
    """Connection which connects the i'th element from the first module's output
    buffer to the i'th element of the second module's input buffer."""

    def __init__(self, *args, **kwargs):
        Connection.__init__(self, *args, **kwargs)
        assert self.indim == self.outdim, \
               "Indim (%i) does not equal outdim (%i)" % (
               self.indim, self.outdim)

    def _forwardImplementation(self, inbuf, outbuf):
        outbuf += inbuf

    def _backwardImplementation(self, outerr, inerr, inbuf):
        inerr += outerr
########NEW FILE########
__FILENAME__ = linear
__author__ = 'Justin S Bayer, bayer.justin@googlemail.com'


from pybrain.structure.connections.connection import Connection
from pybrain.structure.parametercontainer import ParameterContainer


class LinearConnection(Connection, ParameterContainer):
    """Connection that just forwards by multiplying the output of the inmodule
    with a parameter and adds it to the input of the outmodule."""

    def __init__(self, inmod, outmod, name=None,
                 inSliceFrom=0, inSliceTo=None, outSliceFrom=0, outSliceTo=None):
        if inSliceTo is None:
            inSliceTo = inmod.outdim
        size = inSliceTo - inSliceFrom
        Connection.__init__(self, inmod, outmod, name,
                            inSliceFrom, inSliceTo, outSliceFrom, outSliceTo)
        ParameterContainer.__init__(self, size)

    def _forwardImplementation(self, inbuf, outbuf):
        outbuf += inbuf * self.params

    def _backwardImplementation(self, outerr, inerr, inbuf):
        #CHECKME: not setting derivatives -- this means the multiplicative weight is never updated!
        inerr += outerr * self.params

########NEW FILE########
__FILENAME__ = permutation
# -*- coding: utf-8 -_*-

__author__ = 'Justin Bayer, bayer.justin@googlemail.com'
__version__ = '$Id$'


from scipy import array

from pybrain.structure.connections.connection import Connection
from pybrain.utilities import permute


class PermutationConnection(Connection):
    """Connection that permutes the input by a given permutation."""

    def __init__(self, inmod, outmod, permutation, blocksize, *args, **kwargs):
        Connection.__init__(self, inmod, outmod, *args, **kwargs)
        if self.indim != self.outdim:
            raise ValueError("Indim (%i) does not equal outdim (%i)" % (
               self.indim, self.outdim))
        if len(permutation) * blocksize != self.indim:
            raise ValueError(
                "Permutation has wrong size: should be %i but is %i." %(
                (self.indim / blocksize), len(permutation)))

        self.permutation = array(permutation)
        self.invpermutation = permute(range(len(permutation)), permutation)
        self.blocksize = blocksize

    def _forwardImplementation(self, inbuf, outbuf):
        inbuf = inbuf.reshape(self.indim / self.blocksize, self.blocksize)
        inbuf = permute(inbuf, self.permutation)
        inbuf.shape = self.indim,
        outbuf += inbuf

    def _backwardImplementation(self, outerr, inerr, inbuf):
        outerr = outerr.reshape(self.indim / self.blocksize, self.blocksize)
        outerr = permute(outerr, self.invpermutation)
        outerr.shape = self.indim,
        inerr += outerr

########NEW FILE########
__FILENAME__ = shared
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.structure.parametercontainer import ParameterContainer
from pybrain.structure.connections.connection import Connection
from pybrain.structure.connections.full import FullConnection
from pybrain.structure.connections.subsampling import SubsamplingConnection


class OwnershipViolation(Exception):
    """Exception raised when one attempts to write-access the parameters of the
    SharedConnection, instead of its mother."""
    pass


class MotherConnection(ParameterContainer):
    """The container for the shared parameters of connections (just a container
    with a constructor, actually)."""

    hasDerivatives = True
    nbparams = None

    def __init__(self, nbparams, **args):
        assert nbparams > 0
        ParameterContainer.__init__(self, nbparams, **args)
        self.setArgs(nbparams = self.paramdim)


class SharedConnection(Connection):
    """A shared connection can link different couples of modules, with a single
    set of parameters (encapsulated in a MotherConnection)."""

    #: pointer to MotherConnection
    mother = None

    def __init__(self, mother, *args, **kwargs):
        Connection.__init__(self, *args, **kwargs)
        self._replaceParamsByMother(mother)

    def _replaceParamsByMother(self, mother):
        self.setArgs(mother = mother)
        self.paramdim = self.mother.paramdim

    def initParams(self, *args): raise OwnershipViolation
    @property
    def params(self): return self.mother.params

    @property
    def derivs(self): return self.mother.derivs

    def _getName(self):
        return self.mother.name if self._name is None else self._name

    def _setName(self, newname):
        self._name = newname

    name = property(_getName, _setName)


class SharedFullConnection(SharedConnection, FullConnection):
    """Shared version of FullConnection."""

    def _forwardImplementation(self, inbuf, outbuf):
        FullConnection._forwardImplementation(self, inbuf, outbuf)

    def _backwardImplementation(self, outerr, inerr, inbuf):
        FullConnection._backwardImplementation(self, outerr, inerr, inbuf)


class SharedSubsamplingConnection(SharedConnection, SubsamplingConnection):
    """Shared version of SubsamplingConnection."""

    def __init__(self, mother, inmod, outmod, **kwargs):
        SubsamplingConnection.__init__(self, inmod, outmod, **kwargs)
        self._replaceParamsByMother(mother)

    def _forwardImplementation(self, inbuf, outbuf):
        SubsamplingConnection._forwardImplementation(self, inbuf, outbuf)

    def _backwardImplementation(self, outerr, inerr, inbuf):
        SubsamplingConnection._backwardImplementation(self, outerr, inerr, inbuf)


########NEW FILE########
__FILENAME__ = subsampling
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.structure.connections.connection import Connection
from pybrain.structure.parametercontainer import ParameterContainer
from scipy import average

#:TODO: backward pass

class SubsamplingConnection(Connection, ParameterContainer):
    """Connection that just averages all the inputs before forwarding."""

    def __init__(self, inmod, outmod, name=None,
                 inSliceFrom=0, inSliceTo=None, outSliceFrom=0, outSliceTo=None):
        if outSliceTo is None:
            outSliceTo = outmod.indim
        size = outSliceTo - outSliceFrom
        Connection.__init__(self, inmod, outmod, name,
                            inSliceFrom, inSliceTo, outSliceFrom, outSliceTo)
        ParameterContainer.__init__(self, size)

    def _forwardImplementation(self, inbuf, outbuf):
        outbuf += average(inbuf) * self.params

    def _backwardImplementation(self, outerr, inerr, inbuf):
        raise NotImplementedError()


########NEW FILE########
__FILENAME__ = cheaplycopiable
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.structure.parametercontainer import ParameterContainer
from pybrain.structure.modules.module import Module


class CheaplyCopiable(ParameterContainer, Module):
    """ a shallow version of a module, that it only copies/mutates the params, not the structure. """

    def __init__(self, module):
        self.__stored = module
        self._params = module.params.copy()
        self.paramdim = module.paramdim
        self.name = module.name+'-COPY'
        self.indim = module.indim
        self.outdim = module.outdim

    def copy(self):
        self.__stored._params[:] = self._params
        cp = CheaplyCopiable(self.__stored)
        return cp

    def convertToFastNetwork(self):
        self.__stored._params[:] = self._params
        cp = CheaplyCopiable(self.__stored.convertToFastNetwork())
        return cp

    @property
    def derivs(self):
        return self.__stored.derivs

    @property
    def _derivs(self):
        return self.__stored.derivs

    @property
    def outputbuffer(self):
        return self.__stored.outputbuffer

    @property
    def inputerror(self):
        return self.__stored.inputerror


    def reset(self):
        self.__stored.reset()

    def _resetBuffers(self):
        self.__stored._resetBuffers()

    def forward(self, *args, **kwargs):
        self.__stored._params[:] = self._params
        return self.__stored.forward(*args, **kwargs)

    def backward(self, *args, **kwargs):
        self.__stored._params[:] = self._params
        return self.__stored.backward(*args, **kwargs)

    def activate(self, *args, **kwargs):
        self.__stored._params[:] = self._params
        return self.__stored.activate(*args, **kwargs)

    def backActivate(self, *args, **kwargs):
        self.__stored._params[:] = self._params
        return self.__stored.backActivate(*args, **kwargs)

    def randomize(self, *args, **kwargs):
        ParameterContainer.randomize(self, *args, **kwargs)
        self.__stored._params[:] = self._params

    def mutate(self, *args, **kwargs):
        ParameterContainer.mutate(self, *args, **kwargs)
        self.__stored._params[:] = self._params

    def getBase(self):
        self.__stored._params[:] = self._params
        return self.__stored

    def resetDerivatives(self):
        self.__stored.resetDerivatives()


########NEW FILE########
__FILENAME__ = evolvable
__author__ = 'Tom Schaul, tom@idsia.ch'

import copy

from pybrain.utilities import abstractMethod, Named


class Evolvable(Named):
    """ The interface for all Evolvables, i.e. which implement mutation, randomize and copy operators. """

    def mutate(self, **args):
        """ Vary some properties of the underlying module, so that it's behavior
        changes, (but not too abruptly). """
        abstractMethod()

    def copy(self):
        """ By default, returns a full deep-copy - subclasses should implement something faster, if appropriate. """
        return copy.deepcopy(self)

    def randomize(self):
        """ Sets all variable parameters to random values. """
        abstractMethod()

    def newSimilarInstance(self):
        """ Generates a new Evolvable of the same kind."""
        res = self.copy()
        res.randomize()
        return res

########NEW FILE########
__FILENAME__ = maskedmodule
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.structure.evolvables.maskedparameters import MaskedParameters
from pybrain.structure.modules.module import Module


class MaskedModule(MaskedParameters, Module):
    """ an extension of masked-parameters, that wraps a module, and forwards the functionality. """

    def reset(self):
        return self.pcontainer.reset()

    def _resetBuffers(self):
        return self.pcontainer._resetBuffers()

    def activate(self, *args, **kwargs):
        return self.pcontainer.activate(*args, **kwargs)

    def backActivate(self, *args, **kwargs):
        return self.pcontainer.backActivate(*args, **kwargs)

    def forward(self, *args, **kwargs):
        return self.pcontainer.forward(*args, **kwargs)

    def backward(self, *args, **kwargs):
        return self.pcontainer.backward(*args, **kwargs)

    def activateOnDataset(self, *args, **kwargs):
        return self.pcontainer.activateOnDataset(*args, **kwargs)







########NEW FILE########
__FILENAME__ = maskedparameters
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import zeros, randn
from random import random, sample, gauss

from pybrain.structure.evolvables.topology import TopologyEvolvable


class MaskedParameters(TopologyEvolvable):
    """ A module with a binary mask that can disable (=zero) parameters.
    If no maximum is set, the mask can potentially have all parameters enabled.
    The maxComplexity represents the number of allowed enabled parameters. """

    maskFlipProbability = 0.05
    mutationStdev = 0.1

    # number of bits in the mask that can be maximally on at once (None = all)
    # Note: there must always be at least one on
    maxComplexity = None

    # probability of mask bits being on in a random mask (subject to the constraint above)
    maskOnProbability = 0.5

    # when accessed through .params, the masked values are included (and have value zero).
    returnZeros = False

    def __init__(self, pcontainer, **args):
        TopologyEvolvable.__init__(self, pcontainer, **args)
        if self.maxComplexity == None:
            self.maxComplexity = self.pcontainer.paramdim
        self.randomize()
        self.maskableParams = self.pcontainer.params.copy()
        self._applyMask()

    def _applyMask(self):
        """ apply the mask to the module. """
        self.pcontainer._params[:] = self.mask*self.maskableParams

    @property
    def paramdim(self):
        if self.returnZeros:
            return self.pcontainer.paramdim
        else:
            return sum(self.mask)

    @property
    def params(self):
        """ returns an array with (usually) only the unmasked parameters """
        if self.returnZeros:
            return self.pcontainer.params
        else:
            x = zeros(self.paramdim)
            paramcount = 0
            for i in range(len(self.maskableParams)):
                if self.mask[i] == True:
                    x[paramcount] = self.maskableParams[i]
                    paramcount += 1
            return x

    def _setParameters(self, x):
        """ sets only the unmasked parameters """
        paramcount = 0
        for i in range(len(self.maskableParams)):
            if self.mask[i] == True:
                self.maskableParams[i] = x[paramcount]
                paramcount += 1
        self._applyMask()

    def randomize(self, **args):
        """ an initial, random mask (with random params)
        with as many parameters enabled as allowed"""
        self.mask = zeros(self.pcontainer.paramdim, dtype=bool)
        onbits = []
        for i in range(self.pcontainer.paramdim):
            if random() > self.maskOnProbability:
                self.mask[i] = True
                onbits.append(i)
        over = len(onbits) - self.maxComplexity
        if over > 0:
            for i in sample(onbits, over):
                self.mask[i] = False
        self.maskableParams = randn(self.pcontainer.paramdim)*self.stdParams
        self._applyMask()

    def topologyMutate(self):
        """ flips some bits on the mask
        (but do not exceed the maximum of enabled parameters). """
        for i in range(self.pcontainer.paramdim):
            if random() < self.maskFlipProbability:
                self.mask[i] = not self.mask[i]
        tooMany = sum(self.mask) - self.maxComplexity
        for i in range(tooMany):
            while True:
                ind = int(random()*self.pcontainer.paramdim)
                if self.mask[ind]:
                    self.mask[ind] = False
                    break
        if sum(self.mask) == 0:
            # CHECKME: minimum of one needs to be on
            ind = int(random()*self.pcontainer.paramdim)
            self.mask[ind] = True

        self._applyMask()

    def mutate(self):
        """ add some gaussian noise to all parameters."""
        # CHECKME: could this be partly outsourced to the pcontainer directly?
        for i in range(self.pcontainer.paramdim):
            self.maskableParams[i] += gauss(0, self.mutationStdev)
        self._applyMask()

########NEW FILE########
__FILENAME__ = topology
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.utilities import abstractMethod
from pybrain.structure.evolvables.evolvable import Evolvable
from pybrain.structure.parametercontainer import ParameterContainer


class TopologyEvolvable(ParameterContainer):
    """ An evolvable object, with higher-level mutations,
    that change the topology (in the broadest sense).
    It contains an instance of ParameterContainer. """

    pcontainer = None

    def __init__(self, pcontainer, **args):
        self.setArgs(**args)
        self.pcontainer = pcontainer

    @property
    def params(self):
        return self.pcontainer.params

    def _setParameters(self, x):
        self.pcontainer._setParameters(x)

    def topologyMutate(self):
        abstractMethod()

    def newSimilarInstance(self):
        """ generate a new Evolvable with the same topology """
        res = self.copy()
        res.randomize()
        return res

    def copy(self):
        """ copy everything, except the pcontainer """
        # CHECKME: is this correct, or might it be misleading?
        tmp = self.pcontainer
        self.pcontainer = None
        cp = Evolvable.copy(self)
        cp.pcontainer = tmp
        self.pcontainer = tmp
        return cp
########NEW FILE########
__FILENAME__ = modulemesh
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.utilities import iterCombinations, Named
from pybrain.structure.moduleslice import ModuleSlice


class ModuleMesh(Named):
    """ An multi-dimensional array of modules, accessible by their coordinates.
    All modules need to have the same indim and outdim """

    def __init__(self, constructor, dimensions, name = None, baserename = False):
        """:arg constructor: a constructor method that returns a module
        :arg dimensions: tuple of dimensions. """
        self.dims = dimensions
        if name != None:
            self.name = name
        # a dict where the tuple of coordinates is the key
        self.components = {}
        for coord in iterCombinations(self.dims):
            tmp = constructor()
            self.components[coord] = tmp
            tmp.name = self.name + str(coord)
            if baserename and isinstance(tmp, ModuleSlice):
                tmp.base.name = tmp.name
        self.componentIndim = tmp.indim
        self.componentOutdim = tmp.outdim

    @staticmethod
    def constructWithLayers(layerclass, layersize, dimensions, name = None):
        """ create the mesh using constructors that build layers of a specified size and class. """
        c = lambda: layerclass(layersize)
        return ModuleMesh(c, dimensions, name)

    @staticmethod
    def viewOnFlatLayer(layer, dimensions, name = None):
        """ Produces a ModuleMesh that is a mesh-view on a flat module. """
        assert max(dimensions) > 1, "At least one dimension needs to be larger than one."
        def slicer():
            nbunits = reduce(lambda x, y: x*y, dimensions, 1)
            insize = layer.indim / nbunits
            outsize = layer.outdim / nbunits
            for index in range(nbunits):
                yield ModuleSlice(layer, insize*index, insize*(index+1), outsize*index, outsize*(index+1))
        c = slicer()
        return ModuleMesh(lambda: c.next(), dimensions, name)

    def __iter__(self):
        for coord in iterCombinations(self.dims):
            yield self.components[coord]

    def __getitem__(self, coord):
        return self.components[coord]


########NEW FILE########
__FILENAME__ = biasunit
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.structure.modules.neuronlayer import NeuronLayer
from pybrain.structure.modules.module import Module


class BiasUnit(NeuronLayer):
    """A simple bias unit with a single constant output."""

    dim = 1

    def __init__(self, name=None):
        Module.__init__(self, 0, 1, name = name)

    def _forwardImplementation(self, inbuf, outbuf):
        outbuf[:] = 1
########NEW FILE########
__FILENAME__ = evolinonetwork
__author__ = 'Michael Isik'


from pybrain.structure.networks.recurrent import RecurrentNetwork
from pybrain.structure.modules.lstm import LSTMLayer
from pybrain.structure.modules.linearlayer import LinearLayer
from pybrain.structure.connections.full import FullConnection
from pybrain.structure.modules.module import Module
from pybrain.structure.modules.biasunit import BiasUnit

from numpy import zeros, array, reshape
from copy import copy, deepcopy


class EvolinoNetwork(Module):
    """ Model class to be trained by the EvolinoTrainer."""

    def __init__(self, outdim, hiddim=15):
        """ Create an EvolinoNetwork with for sequences of dimension outdim and
        hiddim dimension of the RNN Layer."""
        indim = 0
        Module.__init__(self, indim, outdim)

        self._network = RecurrentNetwork()
        self._in_layer = LinearLayer(indim + outdim)
        self._hid_layer = LSTMLayer(hiddim)
        self._out_layer = LinearLayer(outdim)
        self._bias = BiasUnit()

        self._network.addInputModule(self._in_layer)
        self._network.addModule(self._hid_layer)
        self._network.addModule(self._bias)
        self._network.addOutputModule(self._out_layer)

        self._in_to_hid_connection = FullConnection(self._in_layer,
                                                    self._hid_layer)
        self._bias_to_hid_connection = FullConnection(self._bias,
                                                      self._hid_layer)
        self._hid_to_out_connection = FullConnection(self._hid_layer,
                                                     self._out_layer)
        self._network.addConnection(self._in_to_hid_connection)
        self._network.addConnection(self._bias_to_hid_connection)
        self._network.addConnection(self._hid_to_out_connection)

        self._recurrent_connection = FullConnection(self._hid_layer,
                                                    self._hid_layer)
        self._network.addRecurrentConnection(self._recurrent_connection)

        self._network.sortModules()
        self._network.reset()

        self.offset = self._network.offset
        self.backprojectionFactor = 0.01

    def reset(self):
        """ Resets the underlying network """
        self._network.reset()

    def washout(self, sequence):
        """ Force the network to process the sequence instead of the
        backprojection values. Used for adjusting the RNN's state. Returns the
        outputs of the RNN that are needed for linear regression."""
        assert len(sequence) != 0
        assert self.outdim == len(sequence[0])

        raw_outputs = []
        for val in sequence:
            backprojection = self._getLastOutput()
            backprojection *= self.backprojectionFactor
            self._activateNetwork(backprojection)
            raw_out = self._getRawOutput()
            raw_outputs.append(raw_out)
            self._setLastOutput(val)

        return array(raw_outputs)

    def _activateNetwork(self, input):
        """ Run the activate method of the underlying network."""
        assert len(input) == self._network.indim
        output = array(self._network.activate(input))
        self.offset = self._network.offset
        return output

    def activate(self, input):
        raise NotImplementedError(
            '.activate() is not supported, use .extrapolate()')

    def extrapolate(self, sequence, length):
        """ Extrapolate 'sequence' for 'length' steps and return the
        extrapolated sequence as array.

        Extrapolating is realized by reseting the network, then washing it out
        with the supplied  sequence, and then generating a sequence."""
        self.reset()
        self.washout(sequence)
        return self.generate(length)

    def generate(self, length):
        """ Generate a sequence of specified length.

        Use .reset() and .washout() before."""
        generated_sequence = [] #empty(length)
        for _ in xrange(length):
            backprojection = self._getLastOutput()
            backprojection *= self.backprojectionFactor
            out = self._activateNetwork(backprojection)
            generated_sequence.append(out)

        return array(generated_sequence)

    def _getLastOutput(self):
        """Return the current output of the linear output layer."""
        if self.offset == 0:
            return zeros(self.outdim)
        else:
            return self._out_layer.outputbuffer[self.offset - 1]

    def _setLastOutput(self, output):
        """Force the current output of the linear output layer to 'output'."""
        self._out_layer.outputbuffer[self.offset - 1][:] = output

    #
    # Genome related
    #

    def _validateGenomeLayer(self, layer):
        """Validate the type and state of a layer."""
        assert isinstance(layer, LSTMLayer)
        assert not layer.peepholes

    def getGenome(self):
        """Return the RNN's Genome."""
        return self._getGenomeOfLayer(self._hid_layer)

    def setGenome(self, weights):
        """Set the RNN's Genome."""
        weights = deepcopy(weights)
        self._setGenomeOfLayer(self._hid_layer, weights)

    def _getGenomeOfLayer(self, layer):
        """Return the genome of a single layer."""
        self._validateGenomeLayer(layer)

        connections = self._getInputConnectionsOfLayer(layer)

        layer_weights = []
        # iterate cells of layer
        for cell_idx in range(layer.outdim):
            # todo: the evolino paper uses a different order of weights for the genotype of a lstm cell
            cell_weights = []
            # iterate weight types (ingate, forgetgate, cell and outgate)
            for t in range(4):
                # iterate connections
                for c in connections:
                    # iterate sources of connection
                    for i in range(c.indim):
                        idx = i + cell_idx * c.indim + t * layer.outdim * c.indim
                        cell_weights.append(c.params[idx])

            layer_weights.append(cell_weights)

        return layer_weights

    def _setGenomeOfLayer(self, layer, weights):
        """Set the genome of a single layer."""
        self._validateGenomeLayer(layer)

        connections = self._getInputConnectionsOfLayer(layer)

        # iterate cells of layer
        for cell_idx in range(layer.outdim):
            # todo: the evolino paper uses a different order of weights for the genotype of a lstm cell
            cell_weights = weights[cell_idx]
            # iterate weight types (ingate, forgetgate, cell and outgate)
            for t in range(4):
                # iterate connections
                for c in connections:
                    # iterate sources of connection
                    for i in range(c.indim):
                        idx = i + cell_idx * c.indim + t * layer.outdim * c.indim
                        c.params[idx] = cell_weights.pop(0)

    #
    #  Linear Regression related
    #

    def setOutputWeightMatrix(self, W):
        """Set the weight matrix of the linear output layer."""
        c = self._hid_to_out_connection
        c.params[:] = W.flatten()

    def getOutputWeightMatrix(self):
        """Return the weight matrix of the linear output layer."""
        c = self._hid_to_out_connection
        p = c.params
        return reshape(p, (c.outdim, c.indim))

    def _getRawOutput(self):
        """Return the current output of the RNN. This is needed for linear
        regression, which calculates the weight matrix of the linear output
        layer."""
        return copy(self._hid_layer.outputbuffer[self.offset - 1])

    #
    # Topology Helper
    #

    def _getInputConnectionsOfLayer(self, layer):
        """Return a list of all input connections for the layer."""
        connections = []
        all_cons = list(self._network.recurrentConns)
        all_cons += sum(self._network.connections.values(), [])
        for c in all_cons:
            if c.outmod is layer:
                if not isinstance(c, FullConnection):
                    raise NotImplementedError(
                        "Only FullConnections are supported")
                connections.append(c)
        return connections

########NEW FILE########
__FILENAME__ = gate
# -*- coding: utf-8 -*-


__author__ = 'Justin S Bayer, bayer.justin@googlemail.com'
__version__ = '$Id$'


from pybrain.structure.modules.module import Module
from pybrain.structure.modules.neuronlayer import NeuronLayer
from pybrain.tools.functions import sigmoid, sigmoidPrime


class MultiplicationLayer(NeuronLayer):
    """Layer that implements pairwise multiplication."""

    def __init__(self, dim, name=None):
        Module.__init__(self, 2 * dim, dim, name)
        self.setArgs(dim=dim, name=self.name)

    def _forwardImplementation(self, inbuf, outbuf):
        outbuf += inbuf[:self.outdim] * inbuf[self.outdim:]

    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        inerr[:self.outdim] += inbuf[self.outdim:] * outerr
        inerr[self.outdim:] += inbuf[:self.outdim] * outerr


class GateLayer(NeuronLayer):
    """Layer that implements pairwise input multiplication, with one element of
    the pair being squashed.

    If a GateLayer of size n is created, it will have 2 * n inputs and n
    outputs. The i'th output is calculated as sigmoid(I_i) * I_(i + n) where I
    is the vector of inputs."""

    def __init__(self, dim, name=None):
        Module.__init__(self, 2 * dim, dim, name)
        self.setArgs(dim=dim, name=self.name)

    def _forwardImplementation(self, inbuf, outbuf):
        outbuf += sigmoid(inbuf[:self.outdim]) * inbuf[self.outdim:]

    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        inerr[:self.outdim] += (sigmoidPrime(inbuf[:self.outdim])
                                * inbuf[self.outdim:]
                                * outerr)
        inerr[self.outdim:] += (sigmoid(inbuf[:self.outdim])
                                * outerr)


class DoubleGateLayer(NeuronLayer):
    """Layer that implements a continuous if-then-else.

    If a DoubleGateLayer of size n is created, it will have 2 * n inputs and
    2 * n outputs. The i'th output is calculated as sigmoid(I_i) * I_(i + n) for
    i < n and as (1 - sigmoid(I_i) * I_(i + n) for i >= n where I is the vector
    of inputs."""

    def __init__(self, dim, name=None):
        Module.__init__(self, 2 * dim, 2 * dim, name)
        self.setArgs(dim=dim, name=self.name)

    def _forwardImplementation(self, inbuf, outbuf):
        dim = self.indim / 2
        outbuf[:dim] += sigmoid(inbuf[:dim]) * inbuf[dim:]
        outbuf[dim:] += (1 - sigmoid(inbuf[:dim])) * inbuf[dim:]

    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        dim = self.indim / 2
        in0 = inbuf[:dim]
        in1 = inbuf[dim:]
        out0 = outerr[:dim]
        out1 = outerr[dim:]
        inerr[:dim] += sigmoidPrime(in0) * in1 * out0
        inerr[dim:] += sigmoid(in0) * out0

        inerr[:dim] -= sigmoidPrime(in0) * in1 * out1
        inerr[dim:] += (1 - sigmoid(in0)) * out1


class SwitchLayer(NeuronLayer):
    """Layer that implements pairwise multiplication."""
    #:TODO: Misleading docstring

    def __init__(self, dim, name=None):
        Module.__init__(self, dim, dim * 2, name)
        self.setArgs(dim=dim, name=self.name)

    def _forwardImplementation(self, inbuf, outbuf):
        outbuf[:self.indim] += sigmoid(inbuf)
        outbuf[self.indim:] += 1 - sigmoid(inbuf)

    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        inerr += sigmoidPrime(inbuf) * outerr[:self.indim]
        inerr -= sigmoidPrime(inbuf) * outerr[self.indim:]



########NEW FILE########
__FILENAME__ = gaussianlayer
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from scipy import random
from pybrain.structure.modules.neuronlayer import NeuronLayer
from pybrain.tools.functions import expln, explnPrime
from pybrain.structure.parametercontainer import ParameterContainer


class GaussianLayer(NeuronLayer, ParameterContainer):
    """ A layer implementing a gaussian interpretation of the input. The mean is
    the input, the sigmas are stored in the module parameters."""

    def __init__(self, dim, name=None):
        NeuronLayer.__init__(self, dim, name)
        # initialize sigmas to 0
        ParameterContainer.__init__(self, dim, stdParams = 0)
        # if autoalpha is set to True, alpha_sigma = alpha_mu = alpha*sigma^2
        self.autoalpha = False
        self.enabled = True

    def setSigma(self, sigma):
        """Wrapper method to set the sigmas (the parameters of the module) to a
        certain value. """
        assert len(sigma) == self.indim
        self._params *= 0
        self._params += sigma

    def _forwardImplementation(self, inbuf, outbuf):
        if not self.enabled:
            outbuf[:] = inbuf
        else:
            outbuf[:] = random.normal(inbuf, expln(self.params))

    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        expln_params = expln(self.params)
        self._derivs += ((outbuf - inbuf)**2 - expln_params**2) / expln_params * explnPrime(self.params)
        inerr[:] = (outbuf - inbuf)

        if not self.autoalpha:
            inerr /= expln_params**2
            self._derivs /= expln_params**2

########NEW FILE########
__FILENAME__ = kohonen
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from scipy import random
from scipy.ndimage import minimum_position
from scipy import mgrid, zeros, tile, array, floor, sum

from pybrain.structure.modules.module import Module


class KohonenMap(Module):
    """ Implements a Self-Organizing Map (SOM), also known as a Kohonen Map.
        Clusters the inputs in unsupervised fashion while conserving their
        neighbourhood relationship on a 2-dimensional grid. There are two
        versions: With the outputFullMap option set to True, it outputs
        the full Kohonen map to the next layer, set to False it will only
        return 2 values: the x and y coordinate of the winner neuron. """

    def __init__(self, dim, nNeurons, name=None, outputFullMap=False):
        if outputFullMap:
            outdim = nNeurons ** 2
        else:
            outdim = 2
        Module.__init__(self, dim, outdim, name)

        # switch modes
        self.outputFullMap = outputFullMap

        # create neurons
        self.neurons = random.random((nNeurons, nNeurons, dim))
        self.difference = zeros(self.neurons.shape)
        self.winner = zeros(2)
        self.nInput = dim
        self.nNeurons = nNeurons
        self.neighbours = nNeurons
        self.learningrate = 0.01
        self.neighbourdecay = 0.9999

        # distance matrix
        distx, disty = mgrid[0:self.nNeurons, 0:self.nNeurons]
        self.distmatrix = zeros((self.nNeurons, self.nNeurons, 2))
        self.distmatrix[:, :, 0] = distx
        self.distmatrix[:, :, 1] = disty


    def _forwardImplementation(self, inbuf, outbuf):
        """ assigns one of the neurons to the input given in inbuf and writes
            the neuron's coordinates to outbuf. """
        # calculate the winner neuron with lowest error (square difference)
        self.difference = self.neurons - tile(inbuf, (self.nNeurons, self.nNeurons, 1))
        error = sum(self.difference ** 2, 2)
        self.winner = array(minimum_position(error))
        if not self.outputFullMap:
            outbuf[:] = self.winner


    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        """ trains the kohonen map in unsupervised manner, moving the
            closest neuron and its neighbours closer to the input pattern. """

        # calculate neighbourhood and limit to edge of matrix
        n = floor(self.neighbours)
        self.neighbours *= self.neighbourdecay
        tl = (self.winner - n)
        br = (self.winner + n + 1)
        tl[tl < 0] = 0
        br[br > self.nNeurons + 1] = self.nNeurons + 1

        # calculate distance matrix
        tempm = 1 - sum(abs(self.distmatrix - self.winner.reshape(1, 1, 2)), 2) / self.nNeurons
        tempm[tempm < 0] = 0
        distm = zeros((self.nNeurons, self.nNeurons, self.nInput))
        for i in range(self.nInput):
            distm[:, :, i] = tempm
            distm[:, :, i] = tempm

        self.neurons[tl[0]:br[0], tl[1]:br[1]] -= self.learningrate * self.difference[tl[0]:br[0], tl[1]:br[1]] * distm[tl[0]:br[0], tl[1]:br[1]]


########NEW FILE########
__FILENAME__ = linearlayer
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.structure.modules.neuronlayer import NeuronLayer


class LinearLayer(NeuronLayer):
    """ The simplest kind of module, not doing any transformation. """

    def _forwardImplementation(self, inbuf, outbuf):
        outbuf[:] = inbuf

    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        inerr[:] = outerr
########NEW FILE########
__FILENAME__ = lstm
__author__ = 'Daan Wierstra and Tom Schaul'

from scipy import tanh

from pybrain.structure.modules.neuronlayer import NeuronLayer
from pybrain.structure.modules.module import Module
from pybrain.structure.parametercontainer import ParameterContainer
from pybrain.tools.functions import sigmoid, sigmoidPrime, tanhPrime


class LSTMLayer(NeuronLayer, ParameterContainer):
    """Long short-term memory cell layer.

    The input consists of 4 parts, in the following order:
    - input gate
    - forget gate
    - cell input
    - output gate

    """

    sequential = True
    peepholes = False
    maxoffset = 0

    # Transfer functions and their derivatives
    f = lambda _, x: sigmoid(x)
    fprime = lambda _, x: sigmoidPrime(x)
    g = lambda _, x: tanh(x)
    gprime = lambda _, x: tanhPrime(x)
    h = lambda _, x: tanh(x)
    hprime = lambda _, x: tanhPrime(x)


    def __init__(self, dim, peepholes = False, name = None):
        """
        :arg dim: number of cells
        :key peepholes: enable peephole connections (from state to gates)? """
        self.setArgs(dim = dim, peepholes = peepholes)

        # Internal buffers, created dynamically:
        self.bufferlist = [
            ('ingate', dim),
            ('outgate', dim),
            ('forgetgate', dim),
            ('ingatex', dim),
            ('outgatex', dim),
            ('forgetgatex', dim),
            ('state', dim),
            ('ingateError', dim),
            ('outgateError', dim),
            ('forgetgateError', dim),
            ('stateError', dim),
        ]

        Module.__init__(self, 4*dim, dim, name)
        if self.peepholes:
            ParameterContainer.__init__(self, dim*3)
            self._setParameters(self.params)
            self._setDerivatives(self.derivs)


    def _setParameters(self, p, owner = None):
        ParameterContainer._setParameters(self, p, owner)
        dim = self.outdim
        self.ingatePeepWeights = self.params[:dim]
        self.forgetgatePeepWeights = self.params[dim:dim*2]
        self.outgatePeepWeights = self.params[dim*2:]

    def _setDerivatives(self, d, owner = None):
        ParameterContainer._setDerivatives(self, d, owner)
        dim = self.outdim
        self.ingatePeepDerivs = self.derivs[:dim]
        self.forgetgatePeepDerivs = self.derivs[dim:dim*2]
        self.outgatePeepDerivs = self.derivs[dim*2:]


    def _isLastTimestep(self):
        """Tell wether the current offset is the maximum offset."""
        return self.maxoffset == self.offset

    def _forwardImplementation(self, inbuf, outbuf):
        self.maxoffset = max(self.offset + 1, self.maxoffset)

        dim = self.outdim
        # slicing the input buffer into the 4 parts
        try:
            self.ingatex[self.offset] = inbuf[:dim]
        except IndexError:
            raise str((self.offset, self.ingatex.shape))

        self.forgetgatex[self.offset] = inbuf[dim:dim*2]
        cellx = inbuf[dim*2:dim*3]
        self.outgatex[self.offset] = inbuf[dim*3:]

        # peephole treatment
        if self.peepholes and self.offset > 0:
            self.ingatex[self.offset] += self.ingatePeepWeights * self.state[self.offset-1]
            self.forgetgatex[self.offset] += self.forgetgatePeepWeights * self.state[self.offset-1]

        self.ingate[self.offset] = self.f(self.ingatex[self.offset])
        self.forgetgate[self.offset] = self.f(self.forgetgatex[self.offset])

        self.state[self.offset] = self.ingate[self.offset] * self.g(cellx)
        if self.offset > 0:
            self.state[self.offset] += self.forgetgate[self.offset] * self.state[self.offset-1]

        if self.peepholes:
            self.outgatex[self.offset] += self.outgatePeepWeights * self.state[self.offset]
        self.outgate[self.offset] = self.f(self.outgatex[self.offset])

        outbuf[:] = self.outgate[self.offset] * self.h(self.state[self.offset])

    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        dim = self.outdim
        cellx = inbuf[dim*2:dim*3]

        self.outgateError[self.offset] = self.fprime(self.outgatex[self.offset]) * outerr * self.h(self.state[self.offset])
        self.stateError[self.offset] = outerr * self.outgate[self.offset] * self.hprime(self.state[self.offset])
        if not self._isLastTimestep():
            self.stateError[self.offset] += self.stateError[self.offset+1] * self.forgetgate[self.offset+1]
            if self.peepholes:
                self.stateError[self.offset] += self.ingateError[self.offset+1] * self.ingatePeepWeights
                self.stateError[self.offset] += self.forgetgateError[self.offset+1] * self.forgetgatePeepWeights
        if self.peepholes:
            self.stateError[self.offset] += self.outgateError[self.offset] * self.outgatePeepWeights
        cellError = self.ingate[self.offset] * self.gprime(cellx) * self.stateError[self.offset]
        if self.offset > 0:
            self.forgetgateError[self.offset] = self.fprime(self.forgetgatex[self.offset]) * self.stateError[self.offset] * self.state[self.offset-1]

        self.ingateError[self.offset] = self.fprime(self.ingatex[self.offset]) * self.stateError[self.offset] * self.g(cellx)

        # compute derivatives
        if self.peepholes:
            self.outgatePeepDerivs += self.outgateError[self.offset] * self.state[self.offset]
            if self.offset > 0:
                self.ingatePeepDerivs += self.ingateError[self.offset] * self.state[self.offset-1]
                self.forgetgatePeepDerivs += self.forgetgateError[self.offset] * self.state[self.offset-1]

        inerr[:dim] = self.ingateError[self.offset]
        inerr[dim:dim*2] = self.forgetgateError[self.offset]
        inerr[dim*2:dim*3] = cellError
        inerr[dim*3:] = self.outgateError[self.offset]

    def whichNeuron(self, inputIndex = None, outputIndex = None):
        if inputIndex != None:
            return inputIndex % self.dim
        if outputIndex != None:
            return outputIndex

########NEW FILE########
__FILENAME__ = mdlstm
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import zeros, tanh

from pybrain.structure.modules.neuronlayer import NeuronLayer
from pybrain.structure.modules.module import Module
from pybrain.structure.parametercontainer import ParameterContainer
from pybrain.tools.functions import sigmoid, sigmoidPrime, tanhPrime
from pybrain.structure.moduleslice import ModuleSlice


class MDLSTMLayer(NeuronLayer, ParameterContainer):
    """Multi-dimensional long short-term memory cell layer.

    The cell-states are explicitly passed on through a part of
    the input/output buffers (which should be connected correctly with IdentityConnections).

    The input consists of 4 parts, in the following order:
    - input gate
    - forget gates (1 per dim)
    - cell input
    - output gate
    - previous states (1 per dim)

    The output consists of two parts:
    - cell output
    - current statte


    Attention: this module has to be used with care: it's last <size> input and
    outputs are reserved for transmitting internal states on flattened recursive
    multi-dim networks, and so its connections have always to be sliced!
    """

    peepholes = False
    dimensions = 1
    maxoffset = 0

    # Transfer functions and their derivatives
    def f(self, x): return sigmoid(x)
    def fprime(self, x): return sigmoidPrime(x)
    def g(self, x): return tanh(x)
    def gprime(self, x): return tanhPrime(x)
    def h(self, x): return tanh(x)
    def hprime(self, x): return tanhPrime(x)

    def __init__(self, dim, dimensions=1, peepholes=False, name=None):
        self.setArgs(dim=dim, peepholes=peepholes, dimensions=dimensions)

        # Internal buffers:
        self.bufferlist = [
            ('ingate', dim),
            ('outgate', dim),
            ('forgetgate', dim * dimensions),
            ('ingatex', dim),
            ('outgatex', dim),
            ('forgetgatex', dim * dimensions),
            ('state', dim),
            ('ingateError', dim),
            ('outgateError', dim),
            ('forgetgateError', dim * dimensions),
            ('stateError', dim),
        ]

        Module.__init__(self, (3 + 2 * dimensions) * dim, dim * 2, name)

        if self.peepholes:
            ParameterContainer.__init__(self, dim * (2 + dimensions))
            self._setParameters(self.params)
            self._setDerivatives(self.derivs)

    def _setParameters(self, p, owner=None):
        ParameterContainer._setParameters(self, p, owner)
        size = self.dim
        self.ingatePeepWeights = self.params[:size]
        self.forgetgatePeepWeights = self.params[size:size*(1 + self.dimensions)]
        self.outgatePeepWeights = self.params[size*(1 + self.dimensions):]

    def _setDerivatives(self, d, owner=None):
        ParameterContainer._setDerivatives(self, d, owner)
        size = self.dim
        self.ingatePeepDerivs = self.derivs[:size]
        self.forgetgatePeepDerivs = \
            self.derivs[size:size * (1 + self.dimensions)]
        self.outgatePeepDerivs = \
            self.derivs[size * (1 + self.dimensions):]

    def _forwardImplementation(self, inbuf, outbuf):
        self.maxoffset = max(self.offset + 1, self.maxoffset)
        size = self.dim
        # slicing the input buffer into the 4 parts.
        self.ingatex[self.offset] = inbuf[:size]
        self.forgetgatex[self.offset] = inbuf[size:size*(1+self.dimensions)]
        cellx = inbuf[size*(1+self.dimensions):size*(2+self.dimensions)]
        self.outgatex[self.offset] = inbuf[size*(2+self.dimensions):size*(3+self.dimensions)]
        laststates = inbuf[size*(3+self.dimensions):]

        # Peephole treatment
        if self.peepholes:
            for i in range(self.dimensions):
                self.ingatex[self.offset] += self.ingatePeepWeights * laststates[size * i:size * (i + 1)]
            self.forgetgatex[self.offset] += self.forgetgatePeepWeights * laststates

        self.ingate[self.offset] = self.f(self.ingatex[self.offset])
        self.forgetgate[self.offset] = self.f(self.forgetgatex[self.offset])

        self.state[self.offset] = self.ingate[self.offset] * self.g(cellx)
        for i in range(self.dimensions):
            self.state[self.offset] += self.forgetgate[self.offset, size*i:size*(i+1)] * laststates[size*i:size*(i+1)]

        if self.peepholes:
            self.outgatex[self.offset] += self.outgatePeepWeights * self.state[self.offset]
        self.outgate[self.offset] = self.f(self.outgatex[self.offset])

        outbuf[:size] = self.outgate[self.offset] * self.h(self.state[self.offset])
        outbuf[size:] = self.state[self.offset]

    def _backwardImplementation(self, outerr2, inerr, outbuf, inbuf):
        size = self.dim
        cellx = inbuf[size*(1+self.dimensions):size*(2+self.dimensions)]
        laststates = inbuf[size*(3+self.dimensions):]
        outerr = outerr2[:size]
        nextstateerr = outerr2[size:]

        self.outgateError[self.offset] = self.fprime(self.outgatex[self.offset]) * outerr * self.h(self.state[self.offset])
        self.stateError[self.offset] = outerr * self.outgate[self.offset] * self.hprime(self.state[self.offset])
        self.stateError[self.offset] += nextstateerr
        if self.peepholes:
            self.stateError[self.offset] += self.outgateError[self.offset] * self.outgatePeepWeights
        cellError = self.ingate[self.offset] * self.gprime(cellx) * self.stateError[self.offset]
        for i in range(self.dimensions):
            self.forgetgateError[self.offset, size*i:size*(i+1)] = (self.fprime(self.forgetgatex[self.offset, size*i:size*(i+1)])
                                                                  * self.stateError[self.offset] * laststates[size*i:size*(i+1)])

        self.ingateError[self.offset] = self.fprime(self.ingatex[self.offset]) * self.stateError[self.offset] * self.g(cellx)

        # compute derivatives
        if self.peepholes:
            self.outgatePeepDerivs += self.outgateError[self.offset] * self.state[self.offset]
            for i in range(self.dimensions):
                self.ingatePeepDerivs += self.ingateError[self.offset] * laststates[size*i:size*(i+1)]
                self.forgetgatePeepDerivs[size*i:size*(i+1)] += (self.forgetgateError[self.offset, size*i:size*(i+1)]
                                                                 * laststates[size*i:size*(i+1)])

        instateErrors = zeros((size * self.dimensions))
        for i in range(self.dimensions):
            instateErrors[size * i:size * (i + 1)] = (self.stateError[self.offset] *
                                                      self.forgetgate[self.offset, size*i:size*(i+1)])
            if self.peepholes:
                instateErrors[size * i:size * (i + 1)] += self.ingateError[self.offset] * self.ingatePeepWeights
                instateErrors[size * i:size * (i + 1)] += self.forgetgateError[self.offset, size*i:size*(i+1)] * \
                                                          self.forgetgatePeepWeights[size*i:size*(i+1)]

        inerr[:size] = self.ingateError[self.offset]
        inerr[size:size*(1+self.dimensions)] = self.forgetgateError[self.offset]
        inerr[size*(1+self.dimensions):size*(2+self.dimensions)] = cellError
        inerr[size*(2+self.dimensions):size*(3+self.dimensions)] = self.outgateError[self.offset]
        inerr[size * (3 + self.dimensions):] = instateErrors

    def meatSlice(self):
        """Return a moduleslice that wraps the meat part of the layer."""
        return ModuleSlice(self,
                           inSliceTo=self.dim * (3 + self.dimensions),
                           outSliceTo=self.dim)

    def stateSlice(self):
        """Return a moduleslice that wraps the state transfer part of the layer.
        """
        return ModuleSlice(self,
                           inSliceFrom=self.dim * (3 + self.dimensions),
                           outSliceFrom=self.dim)

    def whichNeuron(self, inputIndex=None, outputIndex=None):
        if inputIndex != None:
            return inputIndex % self.dim
        if outputIndex != None:
            return outputIndex % self.dim
########NEW FILE########
__FILENAME__ = mdrnnlayer
"""The Mdrnn is a bogus layer that only works with fast networks.

It takes an input which is then treated as a multidimensional sequence. E.G. you
might give it an input of `01011010` and specify that its shape is (3, 3), which
results in a 2-dimensional input:

 010
 111
 010
"""


__author__ = 'Justin S Bayer, bayer.justin@googlemail.com'

import operator

from pybrain.structure.modules import MDLSTMLayer, LinearLayer, BiasUnit
from pybrain.structure.modules.module import Module
from pybrain.structure.modules.neuronlayer import NeuronLayer
from pybrain.structure.parametercontainer import ParameterContainer


class MdrnnLayer(NeuronLayer, ParameterContainer):
    """Layer that acts as a Multi-Dimensional Recurrent Neural Network, but can
    be integrated more easily into a network.

    Works only for fast networks."""

    # The parameters can be acces via some shortcuts. These are implemented as
    # properties, since the network containing this layer may change the
    # parameters.

    @property
    def inParams(self):
        return self.params[0:self.num_in_params]

    @property
    def predParams(self):
        offset = self.num_in_params
        rest = self.params[offset:]
        return [rest[(i * self.num_rec_params):(i + 1) * self.num_rec_params]
                for i in xrange(self.timedim)]

    @property
    def outParams(self):
        offset = self.num_in_params + self.num_rec_params
        return self.params[offset:offset + self.num_out_params]

    @property
    def biasParams(self):
        offset = self.num_in_params + self.num_rec_params + self.num_out_params
        return self.params[offset:offset + self.num_bias_params]

    def __init__(self, timedim, shape,
                 hiddendim, outsize, blockshape=None, name=None):
        """Initialize an MdrnnLayer.

        The dimensionality of the sequence - for example 2 for a
        picture or 3 for a video - is given by `timedim`, while the sidelengths
        along each dimension are given by the tuple `shape`.

        The layer will have `hiddendim` hidden units per swiping direction. The
        number of swiping directions is given by 2**timedim, which corresponds
        to one swipe from each corner to its opposing corner and back.

        To indicate how many outputs per timesteps are used, you have to specify
        `outsize`.

        In order to treat blocks of the input and not single voxels, you can
        also specify `blockshape`. For example the layer will then feed (2, 2)
        chunks into the network at each timestep which correspond to the (2, 2)
        rectangles that the input can be split into.
        """
        self.timedim = timedim
        self.shape = shape
        blockshape = tuple([1] * timedim) if blockshape is None else blockshape
        self.blockshape = shape
        self.hiddendim = hiddendim
        self.outsize = outsize
        self.indim = reduce(operator.mul, shape, 1)
        self.blocksize = reduce(operator.mul, blockshape, 1)
        self.sequenceLength = self.indim / self.blocksize
        self.outdim = self.sequenceLength * self.outsize

        self.bufferlist = [('cellStates', self.sequenceLength * self.hiddendim)]

        Module.__init__(self, self.indim, self.outdim, name=name)

        # Amount of parameters that are required for the input to the hidden
        self.num_in_params = self.blocksize * self.hiddendim * (3 + self.timedim)

        # Amount of parameters that are needed for the recurrent connections.
        # There is one of the parameter for every time dimension.
        self.num_rec_params = outsize * hiddendim * (3 + self.timedim)

        # Amount of parameters that are needed for the output.
        self.num_out_params = outsize * hiddendim

        # Amount of parameters that are needed from the bias to the hidden and
        # the output
        self.num_bias_params = (3 + self.timedim) * self.hiddendim + self.outsize

        # Total list of parameters.
        self.num_params = sum((self.num_in_params,
                               self.timedim * self.num_rec_params,
                               self.num_out_params,
                               self.num_bias_params))

        ParameterContainer.__init__(self, self.num_params)

        # Some layers for internal use.
        self.hiddenlayer = MDLSTMLayer(self.hiddendim, self.timedim)

        # Every point in the sequence has timedim predecessors.
        self.predlayers = [LinearLayer(self.outsize) for _ in xrange(timedim)]

        # We need a single layer to hold the input. We will swipe a connection
        # over the corrects part of it, in order to feed the correct input in.
        self.inlayer = LinearLayer(self.indim)
        # Make some layers the same to save memory.
        self.inlayer.inputbuffer = self.inlayer.outputbuffer = self.inputbuffer

        # In order to allocate not too much memory, we just set the size of the
        # layer to 1 and correct it afterwards.
        self.outlayer = LinearLayer(self.outdim)
        self.outlayer.inputbuffer = self.outlayer.outputbuffer = self.outputbuffer

        self.bias = BiasUnit()

    def _forwardImplementation(self, inbuf, outbuf):
        raise NotImplementedError("Only for fast networks.")

    def _growBuffers(self):
        super(MdrnnLayer, self)._growBuffers()
        self.inlayer.inputbuffer = self.inlayer.outputbuffer = self.inputbuffer
        self.outlayer.inputbuffer = self.outlayer.outputbuffer = self.outputbuffer

    def _resetBuffers(self, length=1):
        super(MdrnnLayer, self)._resetBuffers()
        if getattr(self, 'inlayer', None) is not None:
            # Don't do this if the buffers have not been set before.
            self.inlayer.inputbuffer = self.inlayer.outputbuffer = self.inputbuffer
            self.outlayer.inputbuffer = self.outlayer.outputbuffer = self.outputbuffer

########NEW FILE########
__FILENAME__ = module
__author__ = 'Daan Wierstra and Tom Schaul'

from scipy import append, zeros

from pybrain.utilities import abstractMethod, Named


class Module(Named):
    """A module has an input and an output buffer and does some processing
    to produce the output from the input -- the "forward" method.
    Optionally it can have a "backward" method too, which processes a given
    output error to derive the input error.

    Input, output and errors are (flat) scipy arrays.

    A module memorizes the buffers for all input-output pairs it encounters
    until .reset() is called."""

    # Flag that marks modules that treat a sequence of samples not as
    # independent.
    sequential = False

    # Flag which at the same time provides info on how many trainable parameters
    # the module might contain.
    paramdim = 0

    # An offset that is added upon any array access. Useful for implementing
    # things like time.
    offset = 0

    bufferlist = None

    def __init__(self, indim, outdim, name=None, **args):
        """Create a Module with an input dimension of indim and an output
        dimension of outdim."""
        self.setArgs(name=name, **args)

        # Make sure that it does not matter whether Module.__init__ is called
        # before or after adding elements to bufferlist in subclasses.
        # TODO: it should be possible to use less than these buffers. For some
        # methods, an error is not completely necessary. (e.g. evolution)
        self.bufferlist = [] if not self.bufferlist else self.bufferlist
        self.bufferlist += [('inputbuffer', indim),
                            ('inputerror', indim),
                            ('outputbuffer', outdim),
                            ('outputerror', outdim), ]

        self.indim = indim
        self.outdim = outdim
        # Those buffers are 2D arrays (time, dim)
        self._resetBuffers()

    def _resetBuffers(self, length=1):
        """Reset buffers to a length (in time dimension) of 1."""
        for buffername, dim in self.bufferlist:
            setattr(self, buffername, zeros((length, dim)))
        if length==1:
            self.offset = 0

    def _growBuffers(self):
        """Double the size of the modules buffers in its first dimension and
        keep the current values."""
        currentlength = getattr(self, self.bufferlist[0][0]).shape[0]
        # Save the current buffers
        tmp = [getattr(self, n) for n, _ in self.bufferlist]
        Module._resetBuffers(self, currentlength * 2)

        for previous, (buffername, _dim) in zip(tmp, self.bufferlist):
            buffer_ = getattr(self, buffername)
            buffer_[:currentlength] = previous

    def forward(self):
        """Produce the output from the input."""
        self._forwardImplementation(self.inputbuffer[self.offset],
                                    self.outputbuffer[self.offset])

    def backward(self):
        """Produce the input error from the output error."""
        self._backwardImplementation(self.outputerror[self.offset],
                                     self.inputerror[self.offset],
                                     self.outputbuffer[self.offset],
                                     self.inputbuffer[self.offset])

    def reset(self):
        """Set all buffers, past and present, to zero."""
        self.offset = 0
        for buffername, l  in self.bufferlist:
            buf = getattr(self, buffername)
            buf[:] = zeros(l)

    def shift(self, items):
        """Shift all buffers up or down a defined number of items on offset axis.
        Negative values indicate backward shift."""
        if items == 0:
            return
        self.offset += items
        for buffername, _  in self.bufferlist:
            buf = getattr(self, buffername)
            assert abs(items) <= len(buf), "Cannot shift further than length of buffer."
            fill = zeros((abs(items), len(buf[0])))
            if items < 0:
                buf[:] = append(buf[-items:], fill, 0)
            else:
                buf[:] = append(fill ,buf[0:-items] , 0)

    def activateOnDataset(self, dataset):
        """Run the module's forward pass on the given dataset unconditionally
        and return the output."""
        dataset.reset()
        self.reset()
        out = zeros((len(dataset), self.outdim))
        for i, sample in enumerate(dataset):
            # FIXME: Can we always assume that sample[0] is the input data?
            out[i, :] = self.activate(sample[0])
        self.reset()
        dataset.reset()
        return out

    def activate(self, inpt):
        """Do one transformation of an input and return the result."""
        assert len(self.inputbuffer[self.offset]) == len(inpt), str((len(self.inputbuffer[self.offset]), len(inpt)))
        self.inputbuffer[self.offset] = inpt
        self.forward()
        return self.outputbuffer[self.offset].copy()

    def backActivate(self, outerr):
        """Do one transformation of an output error outerr backward and return
        the error on the input."""
        self.outputerror[self.offset] = outerr
        self.backward()
        return self.inputerror[self.offset].copy()

    def _forwardImplementation(self, inbuf, outbuf):
        """Actual forward transformation function. To be overwritten in
        subclasses."""
        abstractMethod()

    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        """Converse of the module's transformation function. Can be overwritten
        in subclasses, does not have to.

        Should also compute the derivatives of the parameters."""

########NEW FILE########
__FILENAME__ = neuronlayer
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.structure.modules.module import Module


class NeuronLayer(Module):
    """Module conceptually representing a layer of units """

    # Number of neurons
    dim = 0

    def __init__(self, dim, name=None):
        """Create a layer with dim number of units."""
        Module.__init__(self, dim, dim, name=name)
        self.setArgs(dim=dim)

    def whichNeuron(self, inputIndex=None, outputIndex=None):
        """Determine which neuron a position in the input/output buffer
        corresponds to. """
        if inputIndex is not None:
            return inputIndex
        if outputIndex is not None:
            return outputIndex
########NEW FILE########
__FILENAME__ = relulayer
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.structure.modules.neuronlayer import NeuronLayer

class ReluLayer(NeuronLayer):
    """ Layer of rectified linear units (relu). """

    def _forwardImplementation(self, inbuf, outbuf):
        outbuf[:] = inbuf * (inbuf > 0)

    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        inerr[:] = outerr * (inbuf > 0)
########NEW FILE########
__FILENAME__ = samplelayer
# -*- coding: utf-8 -*-


__author__ = ('Christian Osendorfer, osendorf@in.tum.de; '
              'Justin S Bayer, bayerj@in.tum.de')


from scipy import random

from pybrain.structure.modules.neuronlayer import NeuronLayer


class SampleLayer(NeuronLayer):
    """Baseclass for all layers that have stochastic output depending on the
    incoming weight."""


class BernoulliLayer(SampleLayer):

    def _forwardImplementation(self, inbuf, outbuf):
        outbuf[:] = inbuf <= random.random(inbuf.shape)

########NEW FILE########
__FILENAME__ = sigmoidlayer
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.structure.modules.neuronlayer import NeuronLayer
from pybrain.tools.functions import sigmoid


class SigmoidLayer(NeuronLayer):
    """Layer implementing the sigmoid squashing function."""

    def _forwardImplementation(self, inbuf, outbuf):
        outbuf[:] = sigmoid(inbuf)

    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        inerr[:] = outbuf * (1 - outbuf) * outerr


########NEW FILE########
__FILENAME__ = softmax
__author__ = 'Tom Schaul, tom@idsia.ch'


import scipy

from pybrain.structure.modules.neuronlayer import NeuronLayer
from pybrain.tools.functions import safeExp


class SoftmaxLayer(NeuronLayer):
    """ A layer implementing a softmax distribution over the input."""

    # TODO: collapsing option?
    # CHECKME: temperature parameter?

    def _forwardImplementation(self, inbuf, outbuf):
        outbuf[:] = safeExp(inbuf)
        outbuf /= sum(outbuf)

    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        inerr[:] = outerr


class PartialSoftmaxLayer(NeuronLayer):
    """Layer implementing a softmax distribution over slices of the input."""

    def __init__(self, size, slicelength):
        super(PartialSoftmaxLayer, self).__init__(size)
        self.slicelength = slicelength

    def _forwardImplementation(self, inbuf, outbuf):
        outbuf[:] = safeExp(inbuf)
        outbuf.shape = scipy.size(outbuf) / self.slicelength, self.slicelength
        s = outbuf.sum(axis=1)
        outbuf = (outbuf.T / s).T.flatten()

    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        inerr[:] = outerr

########NEW FILE########
__FILENAME__ = softsign
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from pybrain.structure.modules.neuronlayer import NeuronLayer

class SoftSignLayer(NeuronLayer):
    """ softsign activation function as described in X. Glorot and Y.
        Bengio. Understanding the difficulty of training deep feedforward neural
        networks. In Proceedings of the 13th International Workshop on
        Artificial Intelligence and Statistics, 2010. """

    def _forwardImplementation(self, inbuf, outbuf):
        outbuf[:] = inbuf / (1 + abs(inbuf))

    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        inerr[:] = (1 - abs(outbuf))**2 * outerr

########NEW FILE########
__FILENAME__ = statedependentlayer
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'

from scipy import random, asarray, zeros, dot

from pybrain.structure.modules.neuronlayer import NeuronLayer
from pybrain.tools.functions import expln, explnPrime
from pybrain.structure.parametercontainer import ParameterContainer


class StateDependentLayer(NeuronLayer, ParameterContainer):

    def __init__(self, dim, module, name=None, onesigma=True):
        NeuronLayer.__init__(self, dim, name)
        self.exploration = zeros(dim, float)
        self.state = None
        self.onesigma = onesigma

        if self.onesigma:
            # one single parameter: sigma
            ParameterContainer.__init__(self, 1)
        else:
            # sigmas for all parameters in the exploration module
            ParameterContainer.__init__(self, module.paramdim)

        # a module for the exploration
        assert module.outdim == dim, (
            "Passed module does not have right dimension")
        self.module = module
        self.autoalpha = False
        self.enabled = True

    def setState(self, state):
        self.state = asarray(state)
        self.exploration[:] = self.module.activate(self.state)
        self.module.reset()

    def drawRandomWeights(self):
        self.module._setParameters(
            random.normal(0, expln(self.params), self.module.paramdim))

    def _forwardImplementation(self, inbuf, outbuf):
        assert self.exploration != None
        if not self.enabled:
            outbuf[:] = inbuf
        else:
            outbuf[:] = inbuf + self.exploration
        self.exploration = zeros(self.dim, float)

    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        if self.onesigma:
            # algorithm for one global sigma for all mu's
            expln_params = expln(self.params)
            sumxsquared = dot(self.state, self.state)
            self._derivs += (
                sum((outbuf - inbuf) ** 2 - expln_params ** 2 * sumxsquared)
                / expln_params * explnPrime(self.params)
            )
            inerr[:] = (outbuf - inbuf)

            if not self.autoalpha and sumxsquared != 0:
                inerr /= expln_params ** 2 * sumxsquared
                self._derivs /= expln_params ** 2 * sumxsquared
        else:
            # Algorithm for seperate sigma for each mu
            expln_params = expln(self.params
                            ).reshape(len(outbuf), len(self.state))
            explnPrime_params = explnPrime(self.params
                            ).reshape(len(outbuf), len(self.state))

            idx = 0
            for j in xrange(len(outbuf)):
                sigma_subst2 = dot(self.state ** 2, expln_params[j, :]**2)
                for i in xrange(len(self.state)):
                    self._derivs[idx] = ((outbuf[j] - inbuf[j]) ** 2 - sigma_subst2) / sigma_subst2 * \
                        self.state[i] ** 2 * expln_params[j, i] * explnPrime_params[j, i]
                    if self.autoalpha and sigma_subst2 != 0:
                        self._derivs[idx] /= sigma_subst2
                    idx += 1
                inerr[j] = (outbuf[j] - inbuf[j])
                if not self.autoalpha and sigma_subst2 != 0:
                    inerr[j] /= sigma_subst2


########NEW FILE########
__FILENAME__ = svmunit
__author__ = "Martin Felder"
__version__ = '$Id: exampleRNN.py 1503 2008-09-13 15:25:06Z bayerj $'
try:
    from svm import svm_model
except ImportError:
    raise ImportError("Cannot find LIBSVM installation. Make sure svm.py and svmc.* are in the PYTHONPATH!")

class SVMUnit(object):
    """ This unit represents an Support Vector Machine and is implemented through the
    LIBSVM Python interface. It functions somewhat like a Model or a Network, but combining
    it with other PyBrain Models is currently discouraged. Its main function is to compare
    against feed-forward network classifications. You cannot get or set model parameters, but
    you can load and save the entire model in LIBSVM format. Sequential data and backward
    passes are not supported. See the corresponding example code for usage. """

    def __init__(self, indim=0, outdim=0, model=None):
        """ Initializes as empty module.

        If `model` is given, initialize using this LIBSVM model instead. `indim`
        and `outdim` are for compatibility only, and ignored."""
        self.reset()
        # set some dummy input/ouput dimensions - these become obsolete when
        # the SVM is initialized
        self.indim = 0
        self.outdim = 0
        self.setModel(model)

    def reset(self):
        """ Reset input and output buffers """
        self.input = None
        self.output = None

    def setModel(self, model):
        """ Set the SVM model. """
        self.model = model

    def loadModel(self, filename):
        """ Read the SVM model description from a file """
        self.model = svm_model(filename)

    def saveModel(self, filename):
        """ Save the SVM model description from a file """
        self.model.save(filename)

    def forwardPass(self, values=False):
        """ Produce the output from the current input vector, or process a
        dataset.

        If `values` is False or 'class', output is set to the number of the
        predicted class. If True or 'raw', produces decision values instead.
        These are stored in a dictionary for multi-class SVM. If `prob`, class
        probabilities are produced. This works only if probability option was
        set for SVM training."""
        if values == "class" or values == False:
            # predict the output class right away
            self.output = self.model.predict(self.input)
        elif values == 'raw' or values == True:
            # return a dict of decision values for each one-on-one class
            # combination (i,j)
            self.output = self.model.predict_values(self.input)
        else:  # values == "prob"
            # return probability (works only for multiclass!)
            self.output = self.model.predict_probability(self.input)

    def activateOnDataset(self, dataset, values=False):
        """ Run the module's forward pass on the given dataset unconditionally
        and return the output as a list.

        :arg dataset: A non-sequential supervised data set.
        :key values: Passed trough to forwardPass() method."""
        out = []
        inp = dataset['input']
        for i in range(inp.shape[0]):
            self.input = inp[i, :]
            # carry out forward pass to get decision values for each class combo
            self.forwardPass(values=values)
            out.append(self.output)
        return out

    def getNbClasses(self):
        """ return number of classes the current model uses """
        return self.model.get_nr_class()



########NEW FILE########
__FILENAME__ = table
__author__ = 'Thomas Rueckstiess, ruecksti@in.tum.de'


from pybrain.structure.modules.module import Module
from pybrain.structure.parametercontainer import ParameterContainer

class Table(Module, ParameterContainer):
    """ implements a simple 2D table with dimensions rows x columns,
        which is basically a wrapper for a numpy array.
    """

    def __init__(self, numRows, numColumns, name=None):
        """ initialize with the number of rows and columns. the table
            values are all set to zero.
        """
        Module.__init__(self, 2, 1, name)
        ParameterContainer.__init__(self, numRows*numColumns)

        self.numRows = numRows
        self.numColumns = numColumns

    def _forwardImplementation(self, inbuf, outbuf):
        """ takes two coordinates, row and column, and returns the
            value in the table.
        """
        outbuf[0] = self.params.reshape(self.numRows, self.numColumns)[inbuf[0], inbuf[1]]

    def updateValue(self, row, column, value):
        """ set the value at a certain location in the table. """
        self.params.reshape(self.numRows, self.numColumns)[row, column] = value

    def getValue(self, row, column):
        """ return the value at a certain location in the table. """
        return self.params.reshape(self.numRows, self.numColumns)[row, column]


########NEW FILE########
__FILENAME__ = tanhlayer
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import tanh

from pybrain.structure.modules.neuronlayer import NeuronLayer


class TanhLayer(NeuronLayer):
    """ A layer implementing the tanh squashing function. """

    def _forwardImplementation(self, inbuf, outbuf):
        outbuf[:] = tanh(inbuf)

    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        inerr[:] = (1 - outbuf**2) * outerr

########NEW FILE########
__FILENAME__ = moduleslice
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.utilities import Named


class ModuleSlice(Named):
    """ A wrapper for using a particular input-output slice of a module's buffers.
    The constructors of connections between ModuleSlices need to ensure a correct use
    (i.e) do the slicing on the base module directly. """

    def __init__(self, base, inSliceFrom = 0, inSliceTo = None, outSliceFrom = 0, outSliceTo = None):
        """ :key base: the base module that is sliced """
        if isinstance(base, ModuleSlice):
            # tolerantly handle the case of a slice of another slice
            self.base = base.base
            self.inOffset = inSliceFrom + base.inSliceFrom
            self.outOffset = outSliceFrom + base.outSliceFrom
            if inSliceTo == None:
                inSliceTo = self.base.indim + base.inSliceFrom
            if outSliceTo == None:
                outSliceTo = self.base.outdim + base.outSliceFrom
            self.name = base.base.name
        else:
            self.base = base
            self.inOffset = inSliceFrom
            self.outOffset = outSliceFrom
            if inSliceTo == None:
                inSliceTo = self.base.indim
            if outSliceTo == None:
                outSliceTo = self.base.outdim
            self.name = base.name
        assert self.inOffset >= 0 and self.outOffset >= 0
        self.indim = inSliceTo - inSliceFrom
        self.outdim = outSliceTo - outSliceFrom
        self.name += ('-slice:('+str(self.inOffset)+','+str(self.indim+self.inOffset)+')('
                     +str(self.outOffset)+','+str(self.outdim+self.outOffset)+')')
        # some slicing is required
        assert self.indim+self.outdim < base.indim+base.outdim

########NEW FILE########
__FILENAME__ = bidirectional
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.structure.modules import TanhLayer, SigmoidLayer
from pybrain.structure.networks.feedforward import FeedForwardNetwork
from pybrain.structure.connections.shared import MotherConnection, SharedFullConnection
from pybrain.structure.modules.linearlayer import LinearLayer
from pybrain.structure.modulemesh import ModuleMesh


class BidirectionalNetwork(FeedForwardNetwork):
    """ A bi-directional recurrent neural network, implemented as unfolded in time. """

    #: should the weights for the forward-direction be the same than for the backward-direction?
    symmetric = False

    #: class for the hidden layers
    componentclass = TanhLayer

    #: class for the output layers
    outcomponentclass = SigmoidLayer

    #: number of inputs for each component of the sequence
    inputsize = 1

    #: number of outputs for each component of the sequence
    outputsize = 1

    #: number of hidden neurons in each hiddne layer
    hiddensize = 5

    #: length of the sequences
    seqlen = None

    def __init__(self, predefined = None, **kwargs):
        """ For the current implementation, the sequence length
        needs to be fixed, and given at construction time. """
        if predefined is not None:
            self.predefined = predefined
        else:
            self.predefined = {}
        FeedForwardNetwork.__init__(self, **kwargs)
        assert self.seqlen is not None

        # the input is a 1D-mesh (as a view on a flat input layer)
        inmod = LinearLayer(self.inputsize * self.seqlen, name='input')
        inmesh = ModuleMesh.viewOnFlatLayer(inmod, (self.seqlen,), 'inmesh')

        # the output is also a 1D-mesh
        outmod = self.outcomponentclass(self.outputsize * self.seqlen, name='output')
        outmesh = ModuleMesh.viewOnFlatLayer(outmod, (self.seqlen,), 'outmesh')

        # the hidden layers are places in a 2xseqlen mesh
        hiddenmesh = ModuleMesh.constructWithLayers(self.componentclass, self.hiddensize,
                                                    (2, self.seqlen), 'hidden')

        # add the modules
        for c in inmesh:
            self.addInputModule(c)
        for c in outmesh:
            self.addOutputModule(c)
        for c in hiddenmesh:
            self.addModule(c)

        # set the connections weights to be shared
        inconnf = MotherConnection(inmesh.componentOutdim * hiddenmesh.componentIndim, name='inconn')
        outconnf = MotherConnection(outmesh.componentIndim * hiddenmesh.componentOutdim, name='outconn')
        forwardconn = MotherConnection(hiddenmesh.componentIndim * hiddenmesh.componentOutdim, name='fconn')
        if self.symmetric:
            backwardconn = forwardconn
            inconnb = inconnf
            outconnb = outconnf
        else:
            backwardconn = MotherConnection(hiddenmesh.componentIndim * hiddenmesh.componentOutdim, name='bconn')
            inconnb = MotherConnection(inmesh.componentOutdim * hiddenmesh.componentIndim, name='inconn')
            outconnb = MotherConnection(outmesh.componentIndim * hiddenmesh.componentOutdim, name='outconn')

        # build the connections
        for i in range(self.seqlen):
            # input to hidden
            self.addConnection(SharedFullConnection(inconnf, inmesh[(i,)], hiddenmesh[(0, i)]))
            self.addConnection(SharedFullConnection(inconnb, inmesh[(i,)], hiddenmesh[(1, i)]))
            # hidden to output
            self.addConnection(SharedFullConnection(outconnf, hiddenmesh[(0, i)], outmesh[(i,)]))
            self.addConnection(SharedFullConnection(outconnb, hiddenmesh[(1, i)], outmesh[(i,)]))
            if i > 0:
                # forward in time
                self.addConnection(SharedFullConnection(forwardconn, hiddenmesh[(0, i - 1)], hiddenmesh[(0, i)]))
            if i < self.seqlen - 1:
                # backward in time
                self.addConnection(SharedFullConnection(backwardconn, hiddenmesh[(1, i + 1)], hiddenmesh[(1, i)]))

        self.sortModules()



########NEW FILE########
__FILENAME__ = borderswiping
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import zeros

from pybrain.structure.networks.swiping import SwipingNetwork
from pybrain.structure.modules import BiasUnit
from pybrain.structure.connections.shared import MotherConnection, SharedFullConnection
from pybrain.utilities import iterCombinations, tupleRemoveItem, reachable, decrementAny


class BorderSwipingNetwork(SwipingNetwork):
    """ Expands the swiping network architecture by border units (bias) and connections. """

    # if this flag is set, we extrapolate the values of unknown border connection weights
    # by initializing them to the closest match.
    extrapolateBorderValues = True

    # all border weights the same?
    simpleborders = False

    def __init__(self, inmesh = None, hiddenmesh = None, outmesh = None, **args):
        if not self.symmetricdirections:
            raise NotImplementedError("BorderSwipingNetworks are currently limited so direction-symmetric weights.")
        if inmesh != None:
            args['dims'] = inmesh.dims
        SwipingNetwork.__init__(self, **args)
        if inmesh != None:
            self._buildBorderStructure(inmesh, hiddenmesh, outmesh)
            self.sortModules()

    def _buildBorderStructure(self, inmesh, hiddenmesh, outmesh):
        self._buildSwipingStructure(inmesh, hiddenmesh, outmesh)
        self.addModule(BiasUnit(name = 'bias'))

        # build the motherconnections for the borders
        if self.simpleborders:
            if not 'borderconn' in self.predefined:
                self.predefined['borderconn'] = MotherConnection(hiddenmesh.componentIndim, name = 'bconn')
        else:
            if not 'bordconns' in self.predefined:
                self.predefined['bordconns'] = {}
            for dim, maxval in enumerate(self.dims):
                if dim > 0 and self.symmetricdimensions:
                    self.predefined['bordconns'][dim] = self.predefined['bordconns'][0]
                elif dim not in self.predefined['bordconns']:
                    self.predefined['bordconns'][dim] = {}
                tmp = self.predefined['bordconns'][dim].copy()
                if len(self.dims) == 1 and () not in tmp:
                    tmp[()] = MotherConnection(hiddenmesh.componentIndim, name = 'bconn')
                for t in iterCombinations(tupleRemoveItem(self.dims, dim)):
                    tc = self._canonicForm(t, dim)
                    if t == tc and t not in tmp:
                        # the connections from the borders are symmetrical,
                        # so we need separate ones only up to the middle
                        tmp[t] = MotherConnection(hiddenmesh.componentIndim, name = 'bconn'+str(dim)+str(t))
                        if self.extrapolateBorderValues:
                            p = self._extrapolateBorderAt(t, self.predefined['bordconns'][dim])
                            if p != None:
                                tmp[t].params[:] = p
                self.predefined['bordconns'][dim] = tmp

        # link the bordering units to the bias, using the correct connection
        for dim, maxval in enumerate(self.dims):
            for unit in self._iterateOverUnits():
                if self.simpleborders:
                    bconn = self.predefined['borderconn']
                else:
                    tc = self._canonicForm(tupleRemoveItem(unit, dim), dim)
                    bconn = self.predefined['bordconns'][dim][tc]
                hunits = []
                if unit[dim] == 0:
                    for swipe in range(self.swipes):
                        if (swipe/2**dim) % 2 == 0:
                            hunits.append(tuple(list(unit)+[swipe]))
                if unit[dim] == maxval-1:
                    for swipe in range(self.swipes):
                        if (swipe/2**dim) % 2 == 1:
                            hunits.append(tuple(list(unit)+[swipe]))
                for hunit in hunits:
                    self.addConnection(SharedFullConnection(bconn, self['bias'], hiddenmesh[hunit]))

    def _canonicForm(self, tup, dim):
        """ determine if there is a symmetrical tuple of lower coordinates

        :key dim: the removed coordinate. """
        if not self.symmetricdimensions:
            return tup
        canonic = []
        for dim, maxval in enumerate(tupleRemoveItem(self.dims, dim)):
            canonic.append(min(maxval-1-tup[dim], tup[dim]))
        return tuple(canonic)

    def _extrapolateBorderAt(self, t, using):
        """ maybe we can use weights that are similar to neighboring borderconnections
        as initialization. """
        closest = reachable(decrementAny, [t], using.keys())
        if len(closest) > 0:
            params = zeros(using[closest.keys()[0]].paramdim)
            normalize = 0.
            for c, dist in closest.items():
                params += using[c].params / dist
                normalize += 1./dist
            params /= normalize
            return params
        return None

########NEW FILE########
__FILENAME__ = convolutional
from pybrain.structure.modules.linearlayer import LinearLayer
from pybrain.structure.modules.tanhlayer import TanhLayer
from pybrain.structure.moduleslice import ModuleSlice
from pybrain.structure.networks.feedforward import FeedForwardNetwork
from pybrain.structure.connections.shared import MotherConnection, SharedFullConnection
from pybrain.structure.modules.sigmoidlayer import SigmoidLayer

__author__ = 'Tom Schaul, tom@idsia.ch'

# TODO: code up a more general version
# TODO: use modulemash.viewonflatlayer()


class SimpleConvolutionalNetwork(FeedForwardNetwork):
    """ A network with a specific form of weight-sharing, on a single 2D layer,
    convoluting neighboring inputs (within a square). """

    def __init__(self, inputdim, insize, convSize, numFeatureMaps, **args):
        FeedForwardNetwork.__init__(self, **args)
        inlayer = LinearLayer(inputdim * insize * insize)
        self.addInputModule(inlayer)
        self._buildStructure(inputdim, insize, inlayer, convSize, numFeatureMaps)
        self.sortModules()


    def _buildStructure(self, inputdim, insize, inlayer, convSize, numFeatureMaps):
        #build layers
        outdim = insize - convSize + 1
        hlayer = TanhLayer(outdim * outdim * numFeatureMaps, name='h')
        self.addModule(hlayer)

        outlayer = SigmoidLayer(outdim * outdim, name='out')
        self.addOutputModule(outlayer)

        # build shared weights
        convConns = []
        for i in range(convSize):
            convConns.append(MotherConnection(convSize * numFeatureMaps * inputdim, name='conv' + str(i)))
        outConn = MotherConnection(numFeatureMaps)

        # establish the connections.
        for i in range(outdim):
            for j in range(outdim):
                offset = i * outdim + j
                outmod = ModuleSlice(hlayer, inSliceFrom=offset * numFeatureMaps, inSliceTo=(offset + 1) * numFeatureMaps,
                                     outSliceFrom=offset * numFeatureMaps, outSliceTo=(offset + 1) * numFeatureMaps)
                self.addConnection(SharedFullConnection(outConn, outmod, outlayer, outSliceFrom=offset, outSliceTo=offset + 1))

                for k, mc in enumerate(convConns):
                    offset = insize * (i + k) + j
                    inmod = ModuleSlice(inlayer, outSliceFrom=offset * inputdim, outSliceTo=offset * inputdim + convSize * inputdim)
                    self.addConnection(SharedFullConnection(mc, inmod, outmod))


if __name__ == '__main__':
    from scipy import array, ravel
    from custom.convboard import ConvolutionalBoardNetwork
    from pybrain.rl.environments.twoplayergames.tasks import CaptureGameTask

    N = ConvolutionalBoardNetwork(4, 3, 5)
    input = [[[0, 0], [0, 0], [0, 0], [0, 0]],
             [[0, 0], [0, 0], [0, 0], [1, 1]],
             [[0, 0], [1, 1], [0, 0], [0, 1]],
             [[0, 0], [1, 0], [1, 1], [0, 1]],
             ]
    res = N.activate(ravel(array(input)))
    res = res.reshape(4, 4)
    print(N['pad'].inputbuffer[0].reshape(6, 6, 2)[:, :, 0])
    print(res)

    t = CaptureGameTask(4)
    print(t(N))

    if False:
        N = SimpleConvolutionalNetwork(4, 2, 5)
        print(N)
        res = N.activate(ravel(array(input)))
        res = res.reshape(3, 3)
        print(res)






########NEW FILE########
__FILENAME__ = capturegame
__author__ = 'Tom Schaul, tom@idsia.ch'

import random

from pybrain import SharedFullConnection, MotherConnection, MDLSTMLayer, IdentityConnection
from pybrain import ModuleMesh, LinearLayer, TanhLayer, SigmoidLayer
from pybrain.structure.networks import BorderSwipingNetwork

# TODO: incomplete implementation: missing clusters, combined outputs, etc.

class CaptureGameNetwork(BorderSwipingNetwork):
    """ a custom-made swiping network for the Capture-Game.
    As an input it takes an array of values corresponding to the occupation state on
    the board positions (black, white, empty) -  the output produced is an array
    of values for positions that correspond to the preference of moving there. """

    size = 5
    insize = 2
    hsize = 5
    predefined = None
    directlink = False
    componentclass = TanhLayer
    outcomponentclass = SigmoidLayer
    peepholes = False
    outputs = 1
    comboutputs = 0
    combinputs = 0

    #bnecksize = 1
    #combbnecksize = 1
    #clusterssize = 1
    #clusteroverlap = 0

    # a flag purely for xml reading to avoid full reconstruction:
    rebuilt = False

    def __init__(self, **args):
        """
        :key clusterssize: the side of the square for clustering: if > 1, an extra layer for cluster-construction is added
        :key clusteroverlap: by how much should the cluster overlap (default = 0)
        :key directlink: should connections from the input directly to the bottleneck be included?
        """
        if 'size' in args:
            self.size = args['size']
        args['dims'] = (self.size, self.size)
        assert self.size > 1, 'Minimal board size is 2.'
        BorderSwipingNetwork.__init__(self, **args)

        if not self.rebuilt:
            self._buildCaptureNetwork()
            self.sortModules()
            self.rebuilt = True
            self.setArgs(rebuilt = True)

    def _buildCaptureNetwork(self):
        # the input is a 2D-mesh (as a view on a flat input layer)
        inmod = LinearLayer(self.insize*self.size*self.size, name = 'input')
        inmesh = ModuleMesh.viewOnFlatLayer(inmod, (self.size, self.size), 'inmesh')

        # the output is a 2D-mesh (as a view on a flat sigmoid output layer)
        outmod = self.outcomponentclass(self.outputs*self.size*self.size, name = 'output')
        outmesh = ModuleMesh.viewOnFlatLayer(outmod, (self.size, self.size), 'outmesh')

        if self.componentclass is MDLSTMLayer:
            c = lambda: MDLSTMLayer(self.hsize, 2, self.peepholes).meatSlice()
            hiddenmesh = ModuleMesh(c, (self.size, self.size, 4), 'hidden', baserename = True)
        else:
            hiddenmesh = ModuleMesh.constructWithLayers(self.componentclass, self.hsize, (self.size, self.size, 4), 'hidden')

        self._buildBorderStructure(inmesh, hiddenmesh, outmesh)

        # add the identity connections for the states
        for m in self.modules:
            if isinstance(m, MDLSTMLayer):
                tmp = m.stateSlice()
                index = 0
                for c in list(self.connections[m]):
                    if isinstance(c.outmod, MDLSTMLayer):
                        self.addConnection(IdentityConnection(tmp, c.outmod.stateSlice(),
                                                              outSliceFrom = self.hsize*(index),
                                                              outSliceTo = self.hsize*(index+1)))
                        index += 1
        # direct connections between input and output
        if self.directlink:
            self._buildDirectLink(inmesh, outmesh)

        # combined inputs
        if self.combinputs > 0:
            cin = LinearLayer(self.combinputs, name = 'globalin')
            self.addInputModule(cin)
            if 'globalinconn' not in self.predefined:
                self.predefined['globalinconn'] = MotherConnection(cin.componentOutdim*hiddenmesh.componentIndim, 'globalinconn')
            self._linkToAll(cin, hiddenmesh, self.predefined['globalinconn'])

    def _buildDirectLink(self, inmesh, outmesh):
        if not 'directconn' in self.predefined:
            self.predefined['directconn'] = MotherConnection(inmesh.componentOutdim*outmesh.componentIndim, 'inconn')
        for unit in self._iterateOverUnits():
            self.addConnection(SharedFullConnection(self.predefined['directconn'], inmesh[unit], outmesh[unit]))

    def _linkToAll(self, inmod, mesh, conn):
        for unit in self._iterateOverUnits():
            self.addConnection(SharedFullConnection(conn, inmod, mesh[unit]))

    def _generateName(self):
        """ generate a quasi unique name, using construction parameters """
        name = self.__class__.__name__
        #if self.size != 5:
        name += '-s'+str(self.size)
        name += '-h'+str(self.hsize)
        if self.directlink:
            name += '-direct'
        if self.componentclass != TanhLayer:
            name += '-'+self.componentclass.__name__
        if self.outputs > 1:
            name += '-o'+str(self.outputs)
        if self.combinputs > 0:
            name += '-combin'+str(self.combinputs)
        #if self.bnecksize != 1:
        #    name += '-bn'+str(self.bnecksize)
        #if self.combbnecksize > 0:
        #    name += '-combbn'+str(self.combbnecksize)
        #if self.clusterssize != 1:
        #    name += '-cluster'+str(self.clusterssize)+'ov'+str(self.clusteroverlap)
        # add a 6-digit random number, for distinction:
        name += '--'+str(int(random.random()*9e5+1e5))
        # TODO: use hash of the weights.
        return name

    def resizedTo(self, newsize):
        """ Produce a copy of the network, with a different size but with the same (shared) weights,
        extrapolating on the borders as necessary. """
        if newsize == self.size:
            return self.copy()
        else:
            import copy
            # TODO: ugly hack!
            # remove recurrent references
            for mc in self.motherconnections:
                mc.owner = None
            # copy the connections from the self.predefined dictionnary:
            cdict = copy.deepcopy(self.predefined)
            args = self.argdict.copy()
            args['size'] = newsize
            del args['rebuilt']
            # put the references back in
            for mc in self.motherconnections:
                mc.owner = self
            return CaptureGameNetwork(predefined = cdict, **args)


########NEW FILE########
__FILENAME__ = convboard
from pybrain.structure.modules.linearlayer import LinearLayer
from pybrain.structure.moduleslice import ModuleSlice
from pybrain.structure.connections.identity import IdentityConnection
from pybrain.structure.networks.feedforward import FeedForwardNetwork
from pybrain.structure.connections.shared import MotherConnection, SharedFullConnection
from pybrain.structure.modules.biasunit import BiasUnit
from pybrain.utilities import crossproduct
from pybrain.structure.networks.convolutional import SimpleConvolutionalNetwork

__author__ = 'Tom Schaul, tom@idsia.ch'


class ConvolutionalBoardNetwork(SimpleConvolutionalNetwork):
    """ A type of convolutional network, designed for handling game boards.
    It pads the borders with a uniform bias input to allow one output per board position.
    """

    def __init__(self, boardSize, convSize, numFeatureMaps, **args):
        inputdim = 2
        FeedForwardNetwork.__init__(self, **args)
        inlayer = LinearLayer(inputdim*boardSize*boardSize, name = 'in')
        self.addInputModule(inlayer)

        # we need some treatment of the border too - thus we pad the direct board input.
        x = convSize/2
        insize = boardSize+2*x
        if convSize % 2 == 0:
            insize -= 1
        paddedlayer = LinearLayer(inputdim*insize*insize, name = 'pad')
        self.addModule(paddedlayer)

        # we connect a bias to the padded-parts (with shared but trainable weights).
        bias = BiasUnit()
        self.addModule(bias)
        biasConn = MotherConnection(inputdim)

        paddable = []
        if convSize % 2 == 0:
            xs = range(x)+range(insize-x+1, insize)
        else:
            xs = range(x)+range(insize-x, insize)
        paddable.extend(crossproduct([range(insize), xs]))
        paddable.extend(crossproduct([xs, range(x, boardSize+x)]))

        for (i, j) in paddable:
            self.addConnection(SharedFullConnection(biasConn, bias, paddedlayer,
                                                    outSliceFrom = (i*insize+j)*inputdim,
                                                    outSliceTo = (i*insize+j+1)*inputdim))

        for i in range(boardSize):
            inmod = ModuleSlice(inlayer, outSliceFrom = i*boardSize*inputdim,
                                outSliceTo = (i+1)*boardSize*inputdim)
            outmod = ModuleSlice(paddedlayer, inSliceFrom = ((i+x)*insize+x)*inputdim,
                                 inSliceTo = ((i+x)*insize+x+boardSize)*inputdim)
            self.addConnection(IdentityConnection(inmod, outmod))

        self._buildStructure(inputdim, insize, paddedlayer, convSize, numFeatureMaps)
        self.sortModules()

########NEW FILE########
__FILENAME__ = feedforward
# -*- coding: utf-8 -*-

"""Module that contains the FeedForwardNetwork class."""


__author__ = 'Justin Bayer, bayer.justin@googlemail.com'

from pybrain.structure.networks.network import Network


class FeedForwardNetworkComponent(object):

    def __init__(self, name=None, **args):
        pass

    def activate(self, inpt):
        """Do one transformation of an input and return the result."""
        self.reset()
        return super(FeedForwardNetworkComponent, self).activate(inpt)

    def _forwardImplementation(self, inbuf, outbuf):
        assert self.sorted, ".sortModules() has not been called"
        index = 0
        offset = self.offset
        for m in self.inmodules:
            m.inputbuffer[offset] = inbuf[index:index + m.indim]
            index += m.indim

        for m in self.modulesSorted:
            m.forward()
            for c in self.connections[m]:
                c.forward()

        index = 0
        for m in self.outmodules:
            outbuf[index:index + m.outdim] = m.outputbuffer[offset]
            index += m.outdim

    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        assert self.sorted, ".sortModules() has not been called"
        index = 0
        offset = self.offset
        for m in self.outmodules:
            m.outputerror[offset] = outerr[index:index + m.outdim]
            index += m.outdim

        for m in reversed(self.modulesSorted):
            for c in self.connections[m]:
                c.backward()
            m.backward()

        index = 0
        for m in self.inmodules:
            inerr[index:index + m.indim] = m.inputerror[offset]
            index += m.indim


class FeedForwardNetwork(FeedForwardNetworkComponent, Network):
    """FeedForwardNetworks are networks that do not work for sequential data.
    Every input is treated as independent of any previous or following inputs.
    """

    def __init__(self, *args, **kwargs):
        Network.__init__(self, *args, **kwargs)
        FeedForwardNetworkComponent.__init__(self, *args, **kwargs)

########NEW FILE########
__FILENAME__ = mdrnn
# -*- coding: utf-8 -*-

""" WARNING: this file is a construction site. The classes are currently placeholders for stuff to come. """

__author__ = 'Justin S Bayer, bayer.justin@googlemail.com'
__version__ = '$Id$'

import operator
import scipy

try:
    from arac.pybrainbridge import _FeedForwardNetwork #@UnresolvedImport
except:
    _FeedForwardNetwork = object
from pybrain.structure.modules.mdrnnlayer import MdrnnLayer
from pybrain.structure import LinearLayer
from pybrain.structure.connections.permutation import PermutationConnection
from pybrain.utilities import crossproduct, permute, permuteToBlocks


class _Mdrnn(_FeedForwardNetwork):

    def __init__(self, timedim, shape,
                 hiddendim, outsize, blockshape=None, name=None,
                 inlayerclass=LinearLayer, outlayerclass=LinearLayer):
        super(_Mdrnn, self).__init__()
        # Initialize necessary member variables
        self.timedim = timedim
        self.shape = shape
        self.hiddendim = hiddendim
        self.outsize = outsize
        self.blockshape = blockshape
        self.indim = reduce(operator.mul, shape, 1)
        self.blocksize = reduce(operator.mul, blockshape, 1)
        self.sequenceLength = self.indim / self.blocksize
        self.inlayerclass = inlayerclass
        self.outlayerclass = outlayerclass

        # Build up topology
        self._buildTopology()

    def _makeMdrnnLayer(self):
        """Return an MdrnnLayer suitable for this network."""
        return MdrnnLayer(self.timedim, self.shape, self.hiddendim,
                          self.outsize, self.blockshape)

    def _standardPermutation(self):
        """Return the permutation of input data that is suitable for this
        network."""
        # TODO: include blockpermute here
        return scipy.array(range(self.sequenceLength))

    def _buildTopology(self):
        inlayer = self.inlayerclass(self.indim)
        outlayer = self.outlayerclass(self.sequenceLength * self.outsize)
        self.hiddenlayers = []
        # Add connections and layers
        self.addInputModule(inlayer)
        for p in self._permsForSwiping():
            i = self._makeMdrnnLayer()
            self.hiddenlayers.append(i)
            # Make a connection that permutes the input...
            in_pc = PermutationConnection(inlayer, i, p, self.blocksize)
            # .. and one that permutes it back.
            pinv = permute(range(len(p)), p)
            out_pc = PermutationConnection(i, outlayer, pinv, self.outsize)
            self.addModule(i)
            self.addConnection(in_pc)
            self.addConnection(out_pc)
        self.addOutputModule(outlayer)

    def _permsForSwiping(self):
        """Return the correct permutations of blocks for all swiping direction.
        """
        # We use an identity permutation to generate the permutations from by
        # slicing correctly.
        return [self._standardPermutation()]

    def activate(self, inpt):
        inpt.shape = self.shape
        inpt_ = permuteToBlocks(inpt, self.blockshape)
        inpt.shape = scipy.size(inpt),
        return super(_Mdrnn, self).activate(inpt_)

    def filterResult(self, inpt):
        return inpt


class _MultiDirectionalMdrnn(_Mdrnn):

    def _permsForSwiping(self):
        """Return the correct permutations of blocks for all swiping direction.
        """
        # We use an identity permutation to generate the permutations from by
        # slicing correctly.
        identity = scipy.array(range(self.sequenceLength))
        identity.shape = tuple(s / b for s, b in zip(self.shape, self.blockshape))
        permutations = []
        # Loop over all possible directions: from each corner to each corner
        for direction in crossproduct([('+', '-')] * self.timedim):
            axises = []
            for _, axisdir in enumerate(direction):
                # Use a normal complete slice for forward...
                if axisdir == '+':
                    indices = slice(None, None, 1)
                # ...and a reversed complete slice for backward
                else:
                    indices = slice(None, None, -1)
                axises.append(indices)
            permutations.append(operator.getitem(identity, axises).flatten())
        return permutations


class _AccumulatingMdrnn(_Mdrnn):

    def activate(self, inpt):
        res = super(_AccumulatingMdrnn, self).activate(inpt)
        res.shape = self.outsize, self.indim
        res = res.sum()



########NEW FILE########
__FILENAME__ = multidimensional
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.structure.networks.swiping import SwipingNetwork
from pybrain import MDLSTMLayer, IdentityConnection
from pybrain import ModuleMesh, LinearLayer, TanhLayer, SigmoidLayer
from scipy import product


class MultiDimensionalRNN(SwipingNetwork):
    """ One possible implementation of Multi-dimensional Recurrent Neural Networks."""

    insize = 1
    outputs = 1
    hsize = 5
    componentclass = TanhLayer
    outcomponentclass = SigmoidLayer

    def __init__(self, dims, **args):
        """ The one required argument specifies the sizes of each dimension (minimum 2) """

        SwipingNetwork.__init__(self, dims = dims, **args)

        pdims = product(dims)
        # the input is a 2D-mesh (as a view on a flat input layer)
        inmod = LinearLayer(self.insize*pdims, name = 'input')
        inmesh = ModuleMesh.viewOnFlatLayer(inmod, dims, 'inmesh')

        # the output is a 2D-mesh (as a view on a flat sigmoid output layer)
        outmod = self.outcomponentclass(self.outputs*pdims, name = 'output')
        outmesh = ModuleMesh.viewOnFlatLayer(outmod, dims, 'outmesh')

        if self.componentclass is MDLSTMLayer:
            c = lambda: MDLSTMLayer(self.hsize, 2, self.peepholes).meatSlice()
            adims = tuple(list(dims)+[4])
            hiddenmesh = ModuleMesh(c, adims, 'hidden', baserename = True)
        else:
            hiddenmesh = ModuleMesh.constructWithLayers(self.componentclass, self.hsize, tuple(list(dims)+[self.swipes]), 'hidden')

        self._buildSwipingStructure(inmesh, hiddenmesh, outmesh)

        # add the identity connections for the states
        for m in self.modules:
            if isinstance(m, MDLSTMLayer):
                tmp = m.stateSlice()
                index = 0
                for c in list(self.connections[m]):
                    if isinstance(c.outmod, MDLSTMLayer):
                        self.addConnection(IdentityConnection(tmp, c.outmod.stateSlice(),
                                                              outSliceFrom = self.hsize*(index),
                                                              outSliceTo = self.hsize*(index+1)))
                        index += 1

        self.sortModules()

class MultiDimensionalLSTM(MultiDimensionalRNN):
    """ The same, but with LSTM cells in the hidden layer. """
    componentclass = MDLSTMLayer
    peepholes = False


########NEW FILE########
__FILENAME__ = network
from __future__ import with_statement


__author__ = 'Daan Wierstra and Tom Schaul'

import scipy

import logging
from itertools import chain

from pybrain.structure.moduleslice import ModuleSlice
from pybrain.structure.modules.module import Module
from pybrain.structure.parametercontainer import ParameterContainer
from pybrain.structure.connections.shared import SharedConnection
from pybrain.structure.evolvables.evolvable import Evolvable


class NetworkConstructionException(Exception):
    """Exception that indicates that the structure of the network is invalid."""


class Network(Module, ParameterContainer):
    """Abstract class for linking different modules with connections."""

    __offset = 0

    def __getOffset(self):
        return self.__offset

    def __setOffset(self, x):
        self.__offset = x
        for m in self.modules:
            m.offset = x

    offset = property(__getOffset, __setOffset)


    def __init__(self, name=None, **args):
        ParameterContainer.__init__(self, **args)
        self.name = name
        # Due to the necessity of regular testing for membership, modules are
        # stored in a set.
        self.modules = set()
        self.modulesSorted = []
        # The connections are stored in a dictionary: the key is the module
        # where the connection leaves from, the value is a list of the
        # corresponding connections.
        self.connections = {}
        self.inmodules = []
        self.outmodules = []
        # Special treatment of weight-shared connections.
        self.motherconnections = []
        # This flag is used to make sure that the modules are reordered when
        # new connections are added.
        self.sorted = False

    def __str__(self):
        sortedByName = lambda itr: sorted(itr, key=lambda i: i.name)

        params = {
            'name': self.name,
            'modules': self.modulesSorted,
            'connections':
                sortedByName(chain(*(sortedByName(self.connections[m])
                                     for m in self.modulesSorted))),
        }

        s = ("%(name)s\n" +
             "   Modules:\n    %(modules)s\n" +
             "   Connections:\n    %(connections)s\n") % params

        return s

    def __getitem__(self, name):
        """Return the module with the given name."""
        for m in self.modules:
            if m.name == name:
                return m
        return None

    def _containerIterator(self):
        """Return an iterator over the non-empty ParameterContainers of the
        network.

        The order IS deterministic."""
        for m in self.modulesSorted:
            if m.paramdim:
                yield m
            for c in self.connections[m]:
                if c.paramdim and not isinstance(c, SharedConnection):
                    yield c
        for mc in self.motherconnections:
            if mc.paramdim:
                yield mc

    def addModule(self, m):
        """Add the given module to the network."""
        if isinstance(m, ModuleSlice):
            m = m.base
        if m not in self.modules:
            self.modules.add(m)
        if not m in self.connections:
            self.connections[m] = []
        if m.paramdim > 0:
            m.owner = self
        if m.sequential and not self.sequential:
            logging.warning(
                ("Module %s is sequential, and added to a FFN. Are you sure " +
                "you know what you're doing?") % m)
        self.sorted = False

    def addInputModule(self, m):
        """Add the given module to the network and mark it as an input module.
        """
        if isinstance(m, ModuleSlice): m = m.base
        if m not in self.inmodules:
            self.inmodules.append(m)
        self.addModule(m)

    def addOutputModule(self, m):
        """Add the given module to the network and mark it as an output module.
        """
        if isinstance(m, ModuleSlice):
            m = m.base
        if m not in self.outmodules:
            self.outmodules.append(m)
        self.addModule(m)

    def addConnection(self, c):
        """Add the given connection to the network."""
        if not c.inmod in self.connections:
            self.connections[c.inmod] = []
        self.connections[c.inmod].append(c)
        if isinstance(c, SharedConnection):
            if c.mother not in self.motherconnections:
                self.motherconnections.append(c.mother)
                c.mother.owner = self
        elif c.paramdim > 0:
            c.owner = self
        self.sorted = False

    def _growBuffers(self):
        for m in self.modules:
            m._growBuffers()
        super(Network, self)._growBuffers()

    def reset(self):
        """Reset all component modules and the network."""
        Module.reset(self)
        for m in self.modules:
            m.reset()

    def _setParameters(self, p, owner=None):
        """ put slices of this array back into the modules """
        ParameterContainer._setParameters(self, p, owner)
        index = 0
        for x in self._containerIterator():
            x._setParameters(self.params[index:index + x.paramdim], self)
            index += x.paramdim

    def _setDerivatives(self, d, owner=None):
        """ put slices of this array back into the modules """
        ParameterContainer._setDerivatives(self, d, owner)
        index = 0
        for x in self._containerIterator():
            x._setDerivatives(self.derivs[index:index + x.paramdim], self)
            index += x.paramdim

    def _forwardImplementation(self, inbuf, outbuf):
        raise NotImplemented("Must be implemented by subclass.")

    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        raise NotImplemented("Must be implemented by subclass.")

    def _topologicalSort(self):
        """Update the network structure and make .modulesSorted a topologically
        sorted list of the modules."""
        # Algorithm: R. E. Tarjan (1972), stolen from:
        #     http://www.bitformation.com/art/python_toposort.html

        # Create a directed graph, including a counter of incoming connections.
        graph = {}
        for node in self.modules:
            if node not in graph:
                # Zero incoming connections.
                graph[node] = [0]
        for c in chain(*self.connections.values()):
            graph[c.inmod].append(c.outmod)
            # Update the count of incoming arcs in outnode.
            graph[c.outmod][0] += 1

        # Find all roots (nodes with zero incoming arcs).
        roots = [node for (node, nodeinfo) in graph.items() if nodeinfo[0] == 0]

        # Make sure the ordering on all runs is the same.
        roots.sort(key=lambda x: x.name)

        # Repeatedly emit a root and remove it from the graph. Removing
        # a node may convert some of the node's direct children into roots.
        # Whenever that happens, we append the new roots to the list of
        # current roots.
        self.modulesSorted = []
        while len(roots) != 0:
            root = roots[0]
            roots = roots[1:]
            self.modulesSorted.append(root)
            for child in graph[root][1:]:
                graph[child][0] -= 1
                if graph[child][0] == 0:
                    roots.append(child)
            del graph[root]

        if graph:
            raise NetworkConstructionException("Loop in network graph.")

    def sortModules(self):
        """Prepare the network for activation by sorting the internal
        datastructure.

        Needs to be called before activation."""
        if self.sorted:
            return
        # Sort the modules.
        self._topologicalSort()
        # Sort the connections by name.
        for m in self.modules:
            self.connections[m].sort(key=lambda x: x.name)
        self.motherconnections.sort(key=lambda x: x.name)

        # Create a single array with all parameters.
        tmp = [pc.params for pc in self._containerIterator()]
        total_size = sum(scipy.size(i) for i in tmp)
        ParameterContainer.__init__(self, total_size)
        if total_size > 0:
            self.params[:] = scipy.concatenate(tmp)
            self._setParameters(self.params)

            # Create a single array with all derivatives.
            tmp = [pc.derivs for pc in self._containerIterator()]
            self.resetDerivatives()
            self.derivs[:] = scipy.concatenate(tmp)
            self._setDerivatives(self.derivs)

        # TODO: make this a property; indim and outdim are invalid before
        # .sortModules is called!
        # Determine the input and output dimensions of the network.
        self.indim = sum(m.indim for m in self.inmodules)
        self.outdim = sum(m.outdim for m in self.outmodules)

        self.indim = 0
        for m in self.inmodules:
            self.indim += m.indim
        self.outdim = 0
        for m in self.outmodules:
            self.outdim += m.outdim

        # Initialize the network buffers.
        self.bufferlist = []
        Module.__init__(self, self.indim, self.outdim, name=self.name)
        self.sorted = True

    def _resetBuffers(self, length=1):
        super(Network, self)._resetBuffers(length)
        for m in self.modules:
            m._resetBuffers(length)

    def copy(self, keepBuffers=False):
        if not keepBuffers:
            self._resetBuffers()
        cp = Evolvable.copy(self)
        if self.paramdim > 0:
            cp._setParameters(self.params.copy())
        return cp

    def convertToFastNetwork(self):
        """ Attempt to transform the network into a fast network. If fast networks are not available,
        or the network cannot be converted, it returns None. """

        from pybrain.structure.networks import FeedForwardNetwork, RecurrentNetwork
        try:
            from arac.pybrainbridge import _RecurrentNetwork, _FeedForwardNetwork #@UnresolvedImport
        except:
            print("No fast networks available.")
            return None

        net = self.copy()
        if isinstance(net, FeedForwardNetwork):
            cnet = _FeedForwardNetwork()
        elif isinstance(net, RecurrentNetwork):
            cnet = _RecurrentNetwork()

        for m in net.inmodules:
            cnet.addInputModule(m)
        for m in net.outmodules:
            cnet.addOutputModule(m)
        for m in net.modules:
            cnet.addModule(m)

        for clist in net.connections.values():
            for c in clist:
                cnet.addConnection(c)
        if isinstance(net, RecurrentNetwork):
            for c in net.recurrentConns:
                cnet.addRecurrentConnection(c)

        try:
            cnet.sortModules()
        except ValueError:
            print("Network cannot be converted.")
            return None

        cnet.owner = cnet
        return cnet

########NEW FILE########
__FILENAME__ = neurondecomposable
__author__ = 'Daan Wierstra and Tom Schaul'

from itertools import chain

from scipy import zeros

from pybrain.structure.networks.feedforward import FeedForwardNetwork
from pybrain.structure.networks.recurrent import RecurrentNetwork
from pybrain.structure.modules.neuronlayer import NeuronLayer
from pybrain.structure.connections import FullConnection

# CHECKME: allow modules that do not inherit from NeuronLayer? and treat them as single neurons?


class NeuronDecomposableNetwork(object):
    """ A Network, that allows accessing parameters decomposed by their
    corresponding individual neuron. """

    # ESP style treatment:
    espStyleDecomposition = True

    def addModule(self, m):
        assert isinstance(m, NeuronLayer)
        super(NeuronDecomposableNetwork, self).addModule(m)

    def sortModules(self):
        super(NeuronDecomposableNetwork, self).sortModules()
        self._constructParameterInfo()

        # contains a list of lists of indices
        self.decompositionIndices = {}
        for neuron in self._neuronIterator():
            self.decompositionIndices[neuron] = []
        for w in range(self.paramdim):
            inneuron, outneuron = self.paramInfo[w]
            if self.espStyleDecomposition and outneuron[0] in self.outmodules:
                self.decompositionIndices[inneuron].append(w)
            else:
                self.decompositionIndices[outneuron].append(w)

    def _neuronIterator(self):
        for m in self.modules:
            for n in range(m.dim):
                yield (m, n)

    def _constructParameterInfo(self):
        """ construct a dictionnary with information about each parameter:
        The key is the index in self.params, and the value is a tuple containing
        (inneuron, outneuron), where a neuron is a tuple of it's module and an index.
        """
        self.paramInfo = {}
        index = 0
        for x in self._containerIterator():
            if isinstance(x, FullConnection):
                for w in range(x.paramdim):
                    inbuf, outbuf = x.whichBuffers(w)
                    self.paramInfo[index + w] = ((x.inmod, x.inmod.whichNeuron(outputIndex=inbuf)),
                                               (x.outmod, x.outmod.whichNeuron(inputIndex=outbuf)))
            elif isinstance(x, NeuronLayer):
                for n in range(x.paramdim):
                    self.paramInfo[index + n] = ((x, n), (x, n))
            else:
                raise
            index += x.paramdim

    def getDecomposition(self):
        """ return a list of arrays, each corresponding to one neuron's relevant parameters """
        res = []
        for neuron in self._neuronIterator():
            nIndices = self.decompositionIndices[neuron]
            if len(nIndices) > 0:
                tmp = zeros(len(nIndices))
                for i, ni in enumerate(nIndices):
                    tmp[i] = self.params[ni]
                res.append(tmp)
        return res

    def setDecomposition(self, decomposedParams):
        """ set parameters by neuron decomposition,
        each corresponding to one neuron's relevant parameters """
        nindex = 0
        for neuron in self._neuronIterator():
            nIndices = self.decompositionIndices[neuron]
            if len(nIndices) > 0:
                for i, ni in enumerate(nIndices):
                    self.params[ni] = decomposedParams[nindex][i]
                nindex += 1

    @staticmethod
    def convertNormalNetwork(n):
        """ convert a normal network into a decomposable one """
        if isinstance(n, RecurrentNetwork):
            res = RecurrentDecomposableNetwork()
            for c in n.recurrentConns:
                res.addRecurrentConnection(c)
        else:
            res = FeedForwardDecomposableNetwork()
        for m in n.inmodules:
            res.addInputModule(m)
        for m in n.outmodules:
            res.addOutputModule(m)
        for m in n.modules:
            res.addModule(m)
        for c in chain(*n.connections.values()):
            res.addConnection(c)
        res.name = n.name
        res.sortModules()
        return res


class FeedForwardDecomposableNetwork(NeuronDecomposableNetwork, FeedForwardNetwork):
    pass


class RecurrentDecomposableNetwork(NeuronDecomposableNetwork, RecurrentNetwork):
    pass

########NEW FILE########
__FILENAME__ = rbm
# -*- coding: utf-8 -*-

__author__ = 'Justin S Bayer, bayer.justin@googlemail.com'
__version__ = '$Id$'


from pybrain.structure import (LinearLayer, SigmoidLayer, FullConnection,
                               BiasUnit, FeedForwardNetwork)


class Rbm(object):
    """Class that holds a network and offers some shortcuts."""

    @property
    def params(self):
        return self.con.params
        pass

    @property
    def biasParams(self):
        return self.biascon.params

    @property
    def visibleDim(self):
        return self.net.indim

    @property
    def hiddenDim(self):
        return self.net.outdim

    def __init__(self, net):
        self.net = net
        self.net.sortModules()
        self.bias = [i for i in self.net.modules if isinstance(i, BiasUnit)][0]
        self.biascon = self.net.connections[self.bias][0]
        self.visible = net['visible']
        self.hidden = net['hidden']
        self.con = self.net.connections[self.visible][0]

    @classmethod
    def fromDims(cls, visibledim, hiddendim, params=None, biasParams=None):
        """Return a restricted Boltzmann machine of the given dimensions with the
        given distributions."""
        net = FeedForwardNetwork()
        bias = BiasUnit('bias')
        visible = LinearLayer(visibledim, 'visible')
        hidden = SigmoidLayer(hiddendim, 'hidden')
        con1 = FullConnection(visible, hidden)
        con2 = FullConnection(bias, hidden)
        if params is not None:
            con1.params[:] = params
        if biasParams is not None:
            con2.params[:] = biasParams

        net.addInputModule(visible)
        net.addModule(bias)
        net.addOutputModule(hidden)
        net.addConnection(con1)
        net.addConnection(con2)
        net.sortModules()
        return cls(net)

    @classmethod
    def fromModules(cls, visible, hidden, bias, con, biascon):
        net = FeedForwardNetwork()
        net.addInputModule(visible)
        net.addModule(bias)
        net.addOutputModule(hidden)
        net.addConnection(con)
        net.addConnection(biascon)
        net.sortModules()
        return cls(net)

    def invert(self):
        """Return the inverse rbm."""
        # TODO: check if shape is correct
        return self.__class__.fromDims(self.hiddenDim, self.visibleDim,
                                       params=self.params)

    def activate(self, inpt):
        return self.net.activate(inpt)

########NEW FILE########
__FILENAME__ = recurrent
# -*- coding: utf-8 -*-


"""Module that contains the RecurrentNetwork class."""


__author__ = 'Justin Bayer, bayer.justin@googlemail.com'


from pybrain.structure.networks.network import Network
from pybrain.structure.connections.shared import SharedConnection


class RecurrentNetworkComponent(object):

    sequential = True

    def __init__(self, forget=None, name=None, *args, **kwargs):
        self.recurrentConns = []
        self.maxoffset = 0
        self.forget = forget

    def __str__(self):
        s = super(RecurrentNetworkComponent, self).__str__()
        s += "   Recurrent Connections:\n    %s" % (
                sorted(self.recurrentConns, key=lambda c: c.name))
        return s

    def _containerIterator(self):
        for c in super(RecurrentNetworkComponent, self)._containerIterator():
            yield c
        for c in self.recurrentConns:
            if c.paramdim and not isinstance(c, SharedConnection):
                yield c

    def addRecurrentConnection(self, c):
        """Add a connection to the network and mark it as a recurrent one."""
        if isinstance(c, SharedConnection):
            if c.mother not in self.motherconnections:
                self.motherconnections.append(c.mother)
                c.mother.owner = self
        elif c.paramdim > 0:
            c.owner = self
        self.recurrentConns.append(c)
        self.sorted = False

    def activate(self, inpt):
        """Do one transformation of an input and return the result."""
        self.inputbuffer[self.offset] = inpt
        self.forward()
        if self.forget:
            return self.outputbuffer[self.offset].copy()
        else:
            return self.outputbuffer[self.offset - 1].copy()

    def backActivate(self, outerr):
        """Do one transformation of an output error outerr backward and return
        the error on the input."""
        self.outputerror[self.offset - 1] = outerr
        self.backward()
        return self.inputerror[self.offset].copy()

    def forward(self):
        """Produce the output from the input."""
        if not (self.offset + 1 < self.inputbuffer.shape[0]):
            self._growBuffers()
        super(RecurrentNetworkComponent, self).forward()
        self.offset += 1
        self.maxoffset = max(self.offset, self.maxoffset)

    def backward(self):
        """Produce the input error from the output error."""
        self.offset -= 1
        super(RecurrentNetworkComponent, self).backward()

    def _isLastTimestep(self):
        return self.offset == self.maxoffset

    def _forwardImplementation(self, inbuf, outbuf):
        assert self.sorted, ".sortModules() has not been called"

        if self.forget:
            self.offset += 1

        index = 0
        offset = self.offset
        for m in self.inmodules:
            m.inputbuffer[offset] = inbuf[index:index + m.indim]
            index += m.indim

        if offset > 0:
            for c in self.recurrentConns:
                c.forward(offset - 1, offset)

        for m in self.modulesSorted:
            m.forward()
            for c in self.connections[m]:
                c.forward(offset, offset)

        if self.forget:
            for m in self.modules:
                m.shift(-1)
            offset -= 1
            self.offset -= 2

        index = 0
        for m in self.outmodules:
            outbuf[index:index + m.outdim] = m.outputbuffer[offset]
            index += m.outdim

    def _backwardImplementation(self, outerr, inerr, outbuf, inbuf):
        assert not self.forget, "Cannot back propagate a forgetful network"
        assert self.sorted, ".sortModules() has not been called"
        index = 0
        offset = self.offset
        for m in self.outmodules:
            m.outputerror[offset] = outerr[index:index + m.outdim]
            index += m.outdim

        if not self._isLastTimestep():
            for c in self.recurrentConns:
                c.backward(offset, offset + 1)

        for m in reversed(self.modulesSorted):
            for c in self.connections[m]:
                c.backward(offset, offset)
            m.offset = offset
            m.backward()

        index = 0
        for m in self.inmodules:
            inerr[index:index + m.indim] = m.inputerror[offset]
            index += m.indim

    def sortModules(self):
        self.recurrentConns.sort(key=lambda x: x.name)
        super(RecurrentNetworkComponent, self).sortModules()


class RecurrentNetwork(RecurrentNetworkComponent, Network):
    """Class that implements networks which can work with sequential data.

    Until .reset() is called, the network keeps track of all previous inputs and
    thus allows the use of recurrent connections and layers that look back in
    time, unless forget is set to True."""

    bufferlist = Network.bufferlist

    def __init__(self, *args, **kwargs):
        Network.__init__(self, *args, **kwargs)
        if 'forget' in kwargs:
            forget = kwargs['forget']
        else:
            forget = False
        RecurrentNetworkComponent.__init__(self, forget, *args, **kwargs)

########NEW FILE########
__FILENAME__ = swiping
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.structure.networks.feedforward import FeedForwardNetwork
from pybrain.structure.connections.shared import MotherConnection, SharedFullConnection
from pybrain.utilities import iterCombinations

# TODO: special treatment for multi-dimensional lstm cells: identity connections on state buffers


class SwipingNetwork(FeedForwardNetwork):
    """ A network architecture that establishes shared connections between ModuleMeshes (of identical dimensions)
    so that the behavior becomes equivalent to one unit (in+hidden+out components at the same coordinate) swiping
    over a multidimensional input space and producing a multidimensional output. """

    # if all dimensions should be considered symmetric, their weights are shared
    symmetricdimensions = True

    # should the forward and backward directions be symmetric (for each dimension)?
    symmetricdirections = True

    # dimensions of the swiping grid
    dims = None

    def __init__(self, inmesh=None, hiddenmesh=None, outmesh=None, predefined=None, **args):
        if predefined != None:
            self.predefined = predefined
        else:
            self.predefined = {}
        super(SwipingNetwork, self).__init__(**args)

        # determine the dimensions
        if inmesh != None:
            self.setArgs(dims=inmesh.dims)
        elif self.dims == None:
            raise Exception('No dimensions specified, or derivable')

        self.swipes = 2 ** len(self.dims)

        if inmesh != None:
            self._buildSwipingStructure(inmesh, hiddenmesh, outmesh)
            self.sortModules()

    def _verifyDimensions(self, inmesh, hiddenmesh, outmesh):
        """ verify dimension matching between the meshes """
        assert self.dims == inmesh.dims
        assert outmesh.dims == self.dims
        assert tuple(hiddenmesh.dims[:-1]) == self.dims, '%s <-> %s' % (
                hiddenmesh.dims[:-1], self.dims)
        assert hiddenmesh.dims[-1] == self.swipes
        assert min(self.dims) > 1

    def _buildSwipingStructure(self, inmesh, hiddenmesh, outmesh):
        """
        :key inmesh: a mesh of input units
        :key hiddenmesh: a mesh of hidden units
        :key outmesh: a mesh of output units
        """
        self._verifyDimensions(inmesh, hiddenmesh, outmesh)

        # add the modules
        for c in inmesh:
            self.addInputModule(c)
        for c in outmesh:
            self.addOutputModule(c)
        for c in hiddenmesh:
            self.addModule(c)

        # create the motherconnections if they are not provided
        if 'inconn' not in self.predefined:
            self.predefined['inconn'] = MotherConnection(inmesh.componentOutdim * hiddenmesh.componentIndim, name='inconn')
        if 'outconn' not in self.predefined:
            self.predefined['outconn'] = MotherConnection(outmesh.componentIndim * hiddenmesh.componentOutdim, name='outconn')
        if 'hconns' not in self.predefined:
            self.predefined['hconns'] = {}
            for s in range(len(self.dims)):
                if self.symmetricdirections:
                    if s > 0 and self.symmetricdimensions:
                        self.predefined['hconns'][s] = self.predefined['hconns'][0]
                    else:
                        self.predefined['hconns'][s] = MotherConnection(hiddenmesh.componentIndim *
                                                        hiddenmesh.componentOutdim, name='hconn' + str(s))
                else:
                    for dir in ['-', '+']:
                        if s > 0 and self.symmetricdimensions:
                            self.predefined['hconns'][(s, dir)] = self.predefined['hconns'][(0, dir)]
                        else:
                            self.predefined['hconns'][(s, dir)] = MotherConnection(hiddenmesh.componentIndim *
                                                        hiddenmesh.componentOutdim, name='hconn' + str(s) + dir)

        # establish the connections
        for unit in self._iterateOverUnits():
            for swipe in range(self.swipes):
                hunit = tuple(list(unit) + [swipe])
                self.addConnection(SharedFullConnection(self.predefined['inconn'], inmesh[unit], hiddenmesh[hunit]))
                self.addConnection(SharedFullConnection(self.predefined['outconn'], hiddenmesh[hunit], outmesh[unit]))
                # one swiping connection along every dimension
                for dim, maxval in enumerate(self.dims):
                    # determine where the swipe is coming from in this direction:
                    # swipe directions are towards higher coordinates on dim D if the swipe%(2**D) = 0
                    # and towards lower coordinates otherwise.
                    previousunit = list(hunit)
                    if (swipe / 2 ** dim) % 2 == 0:
                        previousunit[dim] -= 1
                        dir = '+'
                    else:
                        previousunit[dim] += 1
                        dir = '-'

                    if self.symmetricdirections:
                        hconn = self.predefined['hconns'][dim]
                    else:
                        hconn = self.predefined['hconns'][(dim, dir)]

                    previousunit = tuple(previousunit)
                    if previousunit[dim] >= 0 and previousunit[dim] < maxval:
                        self.addConnection(SharedFullConnection(hconn, hiddenmesh[previousunit], hiddenmesh[hunit]))

    def _iterateOverUnits(self):
        """ iterate over the coordinates defines by the ranges of self.dims. """
        return iterCombinations(self.dims)

    def _printPredefined(self, dic=None, indent=0):
        """ print the weights of the Motherconnections in the self.predefined dictionary (recursively)"""
        if dic == None:
            dic = self.predefined
        for k, val in sorted(dic.items()):
            print(' ' * indent, k,)
            if isinstance(val, dict):
                print(':')
                self._printPredefined(val, indent + 2)
            elif isinstance(val, MotherConnection):
                print(val.params)
            else:
                print(val)


########NEW FILE########
__FILENAME__ = parametercontainer
__author__ = 'Daan Wierstra and Tom Schaul'

from scipy import size, zeros, ndarray, array
from numpy.random import randn

from pybrain.structure.evolvables.evolvable import Evolvable


class ParameterContainer(Evolvable):
    """ A common interface implemented by all classes which
    contains data that can change during execution (i.e. trainable parameters)
    and should be losslessly storable and retrievable to files.  """

    # standard deviation for random values, and for mutation
    stdParams = 1.
    mutationStd = 0.1

    # if this variable is set, then only the owner can set the params or the derivs of the container
    owner = None

    # a flag that enables storage of derivatives
    hasDerivatives = False

    def __init__(self, paramdim = 0, **args):
        """ initialize all parameters with random values, normally distributed around 0

            :key stdParams: standard deviation of the values (default: 1).
        """
        self.setArgs(**args)
        self.paramdim = paramdim
        if paramdim > 0:
            self._params = zeros(self.paramdim)
            # enable derivatives if it is a instance of Module or Connection
            # CHECKME: the import can not be global?
            from pybrain.structure.modules.module import Module
            from pybrain.structure.connections.connection import Connection
            if isinstance(self, Module) or isinstance(self, Connection):
                self.hasDerivatives = True
            if self.hasDerivatives:
                self._derivs = zeros(self.paramdim)
            self.randomize()

    @property
    def params(self):
        """ @rtype: an array of numbers. """
        return self._params

    def __len__(self):
        return self.paramdim

    def _setParameters(self, p, owner = None):
        """ :key p: an array of numbers """
        if isinstance(p, list):
            p = array(p)
        assert isinstance(p, ndarray)

        if self.owner == self:
            # the object owns it parameter array, which means it cannot be set,
            # only updated with new values.
            self._params[:] = p
        elif self.owner != owner:
            raise Exception("Parameter ownership mismatch: cannot set to new array.")
        else:
            self._params = p
            self.paramdim = size(self.params)

    @property
    def derivs(self):
        """ :rtype: an array of numbers. """
        return self._derivs

    def _setDerivatives(self, d, owner = None):
        """ :key d: an array of numbers of self.paramdim """
        assert self.owner == owner
        assert size(d) == self.paramdim
        self._derivs = d

    def resetDerivatives(self):
        """ :note: this method only sets the values to zero, it does not initialize the array. """
        assert self.hasDerivatives
        self._derivs *= 0

    def randomize(self):
        self._params[:] = randn(self.paramdim)*self.stdParams
        if self.hasDerivatives:
            self.resetDerivatives()

    def mutate(self):
        self._params += randn(self.paramdim)*self.mutationStd

########NEW FILE########
__FILENAME__ = filter
__author__ = 'Michael Isik'

from pybrain.supervised.evolino.gfilter import Filter, SimpleMutation
from pybrain.supervised.evolino.variate import CauchyVariate
from pybrain.supervised.evolino.population import SimplePopulation
from pybrain.tools.validation import Validator
from pybrain.tools.kwargsprocessor import KWArgsProcessor

from numpy import array, dot, concatenate, Infinity
from scipy.linalg import pinv2
from copy import deepcopy





class EvolinoEvaluation(Filter):
    """ Evaluate all individuals of the Evolino population, and store their
        fitness value inside the population.
    """

    def __init__(self, evolino_network, dataset, **kwargs):
        """ :key evolino_network: an instance of NetworkWrapper()
            :key dataset: The evaluation dataset
            :key evalfunc: Compares output to target values and returns a scalar, denoting the fitness.
                             Defaults to -mse(output, target).
            :key wtRatio: Float array of two values denoting the ratio between washout and training length.
                            Defaults to [1,2]
            :key verbosity: Verbosity level. Defaults to 0
        """
        Filter.__init__(self)
        ap = KWArgsProcessor(self, kwargs)

        ap.add('verbosity', default=0)
        ap.add('evalfunc', default=lambda output, target:-Validator.MSE(output, target))
        ap.add('wtRatio', default=array([1, 2], float))

        self.network = evolino_network
        self.dataset = dataset
        self.max_fitness = -Infinity


    def _evaluateNet(self, net, dataset, wtRatio):
        """ Evaluates the performance of net on the given dataset.
            Returns the fitness value.

            :key net: Instance of EvolinoNetwork to evaluate
            :key dataset: Sequences to test the net on
            :key wtRatio: See __init__
        """

        # === extract sequences from dataset ===
        numSequences = dataset.getNumSequences()
        washout_sequences = []
        training_sequences = []
        for i in xrange(numSequences):
            sequence = dataset.getSequence(i)[1]
            training_start = int(wtRatio * len(sequence))
            washout_sequences.append(sequence[                  : training_start   ])
            training_sequences.append(sequence[ training_start   :                  ])


        # === collect raw output (denoted by phi) ===
        phis = []
        for i in range(numSequences):
            net.reset()
            net.washout(washout_sequences[i])
            phi = net.washout(training_sequences[i])
            phis.append(phi)


        # === calculate and set weights of linear output layer ===
        PHI = concatenate(phis).T
        PHI_INV = pinv2(PHI)
        TARGET = concatenate(training_sequences).T
        W = dot(TARGET, PHI_INV)
        net.setOutputWeightMatrix(W)


        # === collect outputs by applying the newly configured network ===
        outputs = []
        for i in range(numSequences):
            out = net.extrapolate(washout_sequences[i], len(training_sequences[i]))
            outputs.append(out)


        # === calculate fitness value ===
        OUTPUT = concatenate(outputs)
        TARGET = concatenate(training_sequences)
        fitness = self.evalfunc(OUTPUT, TARGET)


        return fitness



    def apply(self, population):
        """ Evaluate each individual, and store fitness inside population.
            Also calculate and set the weight matrix W of the linear output layer.

            :arg population: Instance of EvolinoPopulation
        """
        net = self.network
        dataset = self.dataset
        population.clearFitness()
        best_W = None
        best_fitness = -Infinity


        # iterate all individuals. Note, that these individuals are created on the fly
        for individual in population.getIndividuals():

            # load the individual's genome into the weights of the net
            net.setGenome(individual.getGenome())
            fitness = self._evaluateNet(net, dataset, self.wtRatio)
            if self.verbosity > 1:
                print("Calculated fitness for individual", id(individual), " is ", fitness)

            # set the individual fitness
            population.setIndividualFitness(individual, fitness)

            if best_fitness < fitness:
                best_fitness = fitness
                best_genome = deepcopy(individual.getGenome())
                best_W = deepcopy(net.getOutputWeightMatrix())

        net.reset()
        net.setGenome(best_genome)
        net.setOutputWeightMatrix(best_W)


        # store fitness maximum to use it for triggering burst mutation
        self.max_fitness = best_fitness





class EvolinoSelection(Filter):
    """ Evolino's selection operator.
        Set its nParents attribute at any time.
        nParents specifies the number of individuals not to be deleted.
        If nParents equals None, EvolinoSubSelection will use its
        default value.
    """
    def __init__(self):
        Filter.__init__(self)
        self.nParents = None
        self.sub_selection = EvolinoSubSelection()

    def apply(self, population):
        """ The subpopulations of the EvolinoPopulation are iterated and forwarded
            to the EvolinoSubSelection() operator.

            :arg population: object of type EvolinoPopulation
        """
        self.sub_selection.nParents = self.nParents
        for sp in population.getSubPopulations():
            self.sub_selection.apply(sp)




class EvolinoReproduction(Filter):
    """ Evolino's reproduction operator """
    def __init__(self, **kwargs):
        """ :key **kwargs: will be forwarded to the EvolinoSubReproduction constructor
        """
        Filter.__init__(self)
        self._kwargs = kwargs


    def apply(self, population):
        """ The subpopulations of the EvolinoPopulation are iterated and forwarded
            to the EvolinoSubReproduction() operator.

            :arg population: object of type EvolinoPopulation
        """
        sps = population.getSubPopulations()
        reproduction = EvolinoSubReproduction(**self._kwargs)
        for sp in sps:
            reproduction.apply(sp)


class EvolinoBurstMutation(Filter):
    """ The burst mutation operator for evolino """
    def __init__(self, **kwargs):
        """ :key **kwargs: will be forwarded to the EvolinoSubReproduction constructor
        """
        Filter.__init__(self)
        self._kwargs = kwargs

    def apply(self, population):
        """ Keeps just the best fitting individual of each subpopulation.
            All other individuals are erased. After that, the kept best fitting
            individuals will be used for reproduction, in order to refill the
            sub-populations.
        """
        sps = population.getSubPopulations()
        for sp in sps:
            n_toremove = sp.getIndividualsN() - 1
            sp.removeWorstIndividuals(n_toremove)
            reproduction = EvolinoSubReproduction(**self._kwargs)
            reproduction.apply(sp)



# ==================================================== SubPopulation related ===



class EvolinoSubSelection(Filter):
    """ Selection operator for EvolinoSubPopulation objects
        Specify its nParents attribute at any time. See EvolinoSelection.
    """
    def __init__(self):
        Filter.__init__(self)

    def apply(self, population):
        """ Simply removes some individuals with lowest fitness values
        """

        n = population.getIndividualsN()
        if self.nParents is None:
            nKeep = n / 4
        else:
            nKeep = self.nParents

        assert nKeep >= 0
        assert nKeep <= n

        population.removeWorstIndividuals(n - nKeep)





class EvolinoSubReproduction(Filter):
    """ Reproduction operator for EvolinoSubPopulation objects.
    """
    def __init__(self, **kwargs):
        """ :key verbosity: Verbosity level
            :key mutationVariate: Variate used for mutation. Defaults to None
            :key mutation: Defaults to EvolinoSubMutation
        """
        Filter.__init__(self)

        ap = KWArgsProcessor(self, kwargs)
        ap.add('verbosity', default=0)
        ap.add('mutationVariate', default=None)
        ap.add('mutation', default=EvolinoSubMutation())

        if self.mutationVariate is not None:
            self.mutation.mutationVariate = self.mutationVariate




    def apply(self, population):
        """ First determines the number of individuals to be created.
            Then clones the fittest individuals (=parents), mutates these clones
            and adds them to the population.
        """
        max_n = population.getMaxNIndividuals()
        n = population.getIndividualsN()
        freespace = max_n - n

        best = population.getBestIndividualsSorted(freespace)
        children = set()
        while True:
            if len(children) >= freespace: break
            for parent in best:
                children.add(parent.copy())
                if len(children) >= freespace: break

        dummy_population = SimplePopulation()
        dummy_population.addIndividuals(children)
        self.mutation.apply(dummy_population)
        population.addIndividuals(dummy_population.getIndividuals())

        assert population.getMaxNIndividuals() == population.getIndividualsN()





class EvolinoSubMutation(SimpleMutation):
    """ Mutation operator for EvolinoSubPopulation objects.
        Like SimpleMutation, except, that CauchyVariate is used by default.
    """
    def __init__(self, **kwargs):
        SimpleMutation.__init__(self)

        ap = KWArgsProcessor(self, kwargs)
        ap.add('mutationVariate', default=CauchyVariate())
        self.mutationVariate.alpha = 0.001





########NEW FILE########
__FILENAME__ = gfilter
__author__ = 'Michael Isik'


from pybrain.supervised.evolino.variate import UniformVariate, GaussianVariate

class Filter(object):
    """ Base class for all kinds of operators on the population during the
        evolutionary process like mutation, selection or evaluation.
    """
    def __init__(self):
        pass
    def apply(self, population):
        """ Applies an operation on a population. """
        raise NotImplementedError()

def isiter(obj):
    try:
        iter(obj)
        return True
    except TypeError:
        return False



class SimpleGenomeManipulation(Filter):
    """ Abstract filter class for simple genome manipulation. """
    def __init__(self):
        Filter.__init__(self)

    def _manipulateGenome(self, genome, manfunc=None):
        """ Manipulates the genome inplace by calling the abstract _manipulateValue()
            method on each float found.

            :key genome: Arbitrary netsted iterateable container whose leaf
                           elements may be floats or empty containers.
                           E.g. [ [1.] , [1. , 2. , 2 , [3. , 4.] ] , [] ]
            :key manfunc: function that manipulates the found floats.
                            If omitted, self._manipulateValue() is used.
                            See its documentation for the signature description.
        """
        assert isiter(genome)
        if manfunc is None:  manfunc = self._manipulateValue

        for i, v in enumerate(genome):
            if isiter(v):
                self._manipulateGenome(v, manfunc)
            else:
                genome[i] = manfunc(v)

    def _manipulateValue(self, value):
        """ Abstract Method, which should manipulate a value.
            Should return the manipulated value
        """
        raise NotImplementedError()



class SimpleMutation(SimpleGenomeManipulation):
    mutationVariate = None
    """ A simple mutation filter, which uses a gaussian variate per default
        for mutation.
    """
    def __init__(self):
        """ :key kwargs: See setArgs() method documentation
        """
        SimpleGenomeManipulation.__init__(self)
        self.mutationVariate = GaussianVariate()
        self.mutationVariate.alpha = 0.1
        self.verbosity = 0


    def apply(self, population):
        """ Apply the mutation to the population

            :key population: must implement the getIndividuals() method
        """
        for individual in population.getIndividuals():
            self._mutateIndividual(individual)

    def _mutateIndividual(self, individual):
        """ Mutate a single individual

            :key individual: must implement the getGenome() method
        """
        genome = individual.getGenome()
        self._manipulateGenome(genome)

    def _manipulateValue(self, value):
        """ Implementation of the abstract method of class SimpleGenomeManipulation
            Set's the x0 value of the variate to value and takes a new sample
            value and returns it.
        """
        self.mutationVariate.x0 = value
        newval = self.mutationVariate.getSample()
#        print("MUTATED: ", value, "--->", newval)
        return newval





class Randomization(SimpleGenomeManipulation):
    """ Randomizes the genome of all individuals of a population
        Uses UniformVariate to do so.
    """
    def __init__(self, minval=0., maxval=1.):
        SimpleGenomeManipulation.__init__(self)
        self._minval = minval
        self._maxval = maxval

    def apply(self, population):
        self._uniform_variate = UniformVariate(self._minval, self._maxval)
        for individual in population.getIndividuals():
            self._manipulateGenome(individual.getGenome())

    def _manipulateValue(self, value):
        """ See SimpleGenomeManipulation._manipulateValue() for more information """
        return self._uniform_variate.getSample()






########NEW FILE########
__FILENAME__ = gindividual
__author__ = 'Michael Isik'


class Individual(object):
    """ Simple abstract template for a minimal individual """
    def getGenome(self):
        """ Should return a reference to the genome.
        """
        raise NotImplementedError()

    def copy(self):
        """ Should return a full copy of the individual
        """
        raise NotImplementedError()



########NEW FILE########
__FILENAME__ = gpopulation
__author__ = 'Michael Isik'

from numpy import Infinity


class Population:
    """ Abstract template for a minimal Population.
        Implement just the methods you need.
    """
    def __init__(self):pass
    def getIndividuals(self):
        """ Should return a shallow copy of the individuals container, so that
            individuals can be manipulated, but not the set of individuals itself.
            For removing or appending individuals to the population, use methods
            like removeIndividual() or addIndividual().
        """
        raise NotImplementedError()

    def addIndividual(self, individual):
        """ Should add an individual to the individual container.
        """
        raise NotImplementedError()

    def addIndividuals(self, individuals):
        """ Should add a set of individuals.
        """
        raise NotImplementedError()

    def removeIndividual(self, individual):
        """ Should remove an individual from the individual container.
        """
        raise NotImplementedError()

    def removeIndividuals(self, individuals):
        """ Should remove a set of individuals.
        """
        raise NotImplementedError()

    def setIndividualFitness(self, individual, fitness):
        """ Should associate the fitness value to the specified individual.
        """
        raise NotImplementedError()

    def getIndividualFitness(self, individual):
        """ Should return the associated fitness value of the specified individual.
        """
        raise NotImplementedError()


class SimplePopulation(Population):
    """ A simple implementation of the abstract Population class.
        Sets are used as individual container. The fitness values are
        stored in a separate dictionary, which maps individuals to fitness values.
        For descriptions of the methods please refer to the Population documentation.
    """
    def __init__(self):
        self._individuals = set()
#        self._fitness = collections.defaultdict( lambda: 0. )
#        self._fitness = collections.defaultdict( lambda: -Infinity )
        self._fitness = {}

    def getIndividuals(self):
        return self._individuals.copy()

    def addIndividual(self, individual):
        self._individuals.add(individual)
        self._fitness[individual] = -Infinity

    def addIndividuals(self, individuals):
        for individual in individuals:
            self.addIndividual(individual)
#        self._individuals = self._individuals.union(set(individuals))

    def removeIndividual(self, individual):
        self._individuals.discard(individual)
        del self._fitness[individual]
#        self._fitness[individual] = -Infinity
#        if self._fitness.has_key(individual):
#            self._fitness[individual] = -Infinity
#            del self._fitness[individual]

    def removeIndividuals(self, individuals):
        for individual in individuals:
            self.removeIndividual(individual)
#        self._individuals.difference_update(set(individuals))


    def setIndividualFitness(self, individual, fitness):
        self._fitness[individual] = fitness

    def getIndividualFitness(self, individual):
#        assert self._fitness.has_key(individual)
        return self._fitness[individual]


    def clearFitness(self):
        """ Clears all stored fitness values """
        for (ind, _) in self._fitness.iteritems():
            self._fitness[ind] = -Infinity
#        self._fitness.clear()

    def getFitnessMap(self):
        """ Returns the fitness dictionary """
        return self._fitness.copy()

    def getMaxFitness(self):
        """ Returns the maximal fitness value """
        return self.getIndividualFitness(self.getBestIndividuals(1))



    def getBestIndividuals(self, n):
        """ Returns n individuals with the highest fitness ranking.
            If n is greater than the number of individuals inside the population
            all individuals are returned.
        """
        return set(self.getBestIndividualsSorted(n))

    def getBestIndividualsSorted(self, n):
        return self.getSortedIndividualList()[:n]


    def getWorstIndividuals(self, n):
        """ Returns the n individuals with the lowest fitness ranking.
            If n is greater than the number of individuals inside the population
            all individuals are returned.
        """
        return set(self.getSortedIndividualList()[-n:])

    def removeWorstIndividuals(self, n):
        """ Removes the n individuals with the lowest fitness ranking.
            If n is greater than the number of individuals inside the population
            all individuals are removed.
        """
        inds = self.getWorstIndividuals(n)
        self.removeIndividuals(inds)


    def getSortedIndividualList(self):
        """ Returns a sorted list of all individuals with descending fitness values. """
        fitness = self._fitness
        return sorted(fitness.iterkeys(), key=lambda(k):-fitness[k])


    def getIndividualsN(self):
        """ Returns the number of individuals inside the population """
        return len(self._individuals)

    def getAverageFitness(self):
        return sum(self._fitness.values()) / float(len(self._fitness))





########NEW FILE########
__FILENAME__ = individual
__author__ = 'Michael Isik'


from pybrain.supervised.evolino.gindividual import Individual
from copy import copy, deepcopy


class EvolinoIndividual(Individual):
    """ Individual of the Evolino framework, that consists of a list of
        sub-individuals. The genomes of the sub-individuals are used as
        the cromosomes for the main individual's genome.
        The genome of an individual encodes the RNN's connection weights.
    """
    def __init__(self, sub_individuals):
        """ :key sub_individuals: sequence (e.g. list) of sub-individuals
        """
        self._sub_individuals = list(sub_individuals)

    def getGenome(self):
        """ Returns the genome created by concatenating the chromosomes supplied
            by the sub-individuals.
        """
        genome = []
        for sub_individual in self._sub_individuals:
            genome.append(deepcopy(sub_individual.getGenome()))
        return genome

    def getSubIndividuals(self):
        """ Returns a shallow copy of the list of sub-individuals """
        return copy(self._sub_individuals)



class EvolinoSubIndividual(Individual):
    """ The sub-individual class of evolino
    """
    _next_id = 0
    def __init__(self, genome):
        """ :key genome: Any kind of nested iteratable container containing
                           floats as leafs
        """
        self.setGenome(genome)
        self.id = EvolinoSubIndividual._next_id
        EvolinoSubIndividual._next_id += 1

    def getGenome(self):
        """ Returns the genome. """
        return self._genome

    def setGenome(self, genome):
        """ Sets the genome. """
        self._genome = genome

    def copy(self):
        """ Returns a complete copy of the individual. """
        return copy(self)

    def __copy__(self):
        """ Returns a complete copy of the individual. """
        return EvolinoSubIndividual(deepcopy(self._genome))





########NEW FILE########
__FILENAME__ = networkwrapper
__author__ = 'Michael Isik'


from pybrain.structure.networks.network     import Network
from pybrain.structure.modules.lstm         import LSTMLayer
from pybrain.structure.modules.linearlayer  import LinearLayer
from pybrain.structure.connections.full     import FullConnection
from pybrain.structure.modules.module       import Module
from pybrain.structure.modules.biasunit     import BiasUnit

from numpy import zeros, array, append


class EvolinoNetwork(Module):
    def __init__(self, indim, outdim, hiddim=6):
        Module.__init__(self, indim, outdim)

        self._network = Network()
        self._in_layer = LinearLayer(indim + outdim)
        self._hid_layer = LSTMLayer(hiddim)
        self._out_layer = LinearLayer(outdim)
        self._bias = BiasUnit()

        self._network.addInputModule(self._in_layer)
        self._network.addModule(self._hid_layer)
        self._network.addModule(self._bias)
        self._network.addOutputModule(self._out_layer)


        self._hid_to_out_connection = FullConnection(self._hid_layer , self._out_layer)
        self._in_to_hid_connection = FullConnection(self._in_layer  , self._hid_layer)
        self._network.addConnection(self._hid_to_out_connection)
        self._network.addConnection(self._in_to_hid_connection)
        self._network.addConnection(FullConnection(self._bias, self._hid_layer))

        self._network.sortModules()

        self.offset = self._network.offset
        self.backprojectionFactor = 0.01

    def reset(self):
        self._network.reset()


    def _washout(self, input, target, first_idx=None, last_idx=None):
        assert self.indim == len(input[0])
        assert self.outdim == len(target[0])
        assert len(input) == len(target)

        if first_idx is None: first_idx = 0
        if last_idx  is None: last_idx = len(target) - 1
        raw_outputs = []
        for i in xrange(first_idx, last_idx + 1):
            backprojection = self._getLastOutput()
            backprojection *= self.backprojectionFactor
            full_inp = self._createFullInput(input[i], backprojection)
            self._activateNetwork(full_inp)
            raw_out = self._getRawOutput()
#            print("RAWOUT: ", full_inp, " --> ", raw_out, self._getLastOutput())
            raw_outputs.append(array(raw_out))
            self._setLastOutput(target[i])

        return array(raw_outputs)



    def _activateNetwork(self, input):
        assert len(input) == self._network.indim
        output = self._network.activate(input)
        self.offset = self._network.offset
#        print("INNNNNNN=", input, "   OUTPP=", output)
        return output

    def activate(self, input):
        assert len(input) == self.indim

        backprojection = self._getLastOutput()
        backprojection *= self.backprojectionFactor
        full_inp = self._createFullInput(input, backprojection)
        out = self._activateNetwork(full_inp)
#        print("AAAAAACT: ", full_inp, "-->", out)

#        self._setLastOutput(last_out*5)

        return out


    def calculateOutput(self, dataset, washout_calculation_ratio=(1, 2)):
        washout_calculation_ratio = array(washout_calculation_ratio, float)
        ratio = washout_calculation_ratio / sum(washout_calculation_ratio)

        # iterate through all sequences
        collected_input = None
        collected_output = None
        collected_target = None
        for i in range(dataset.getNumSequences()):

            seq = dataset.getSequence(i)
            input = seq[0]
            target = seq[1]

            washout_steps = int(len(input) * ratio[0])

            washout_input = input  [               : washout_steps ]
            washout_target = target [               : washout_steps ]
            calculation_target = target [ washout_steps :               ]


            # reset
            self.reset()

            # washout
            self._washout(washout_input, washout_target)


            # collect calculation data
            outputs = []
            inputs = []
#            for i in xrange(washout_steps, len(input)):
            for inp in input[washout_steps:]:
                out = self.activate(inp)
#                    print(out)
#                print(inp)
                inputs.append(inp)
                outputs.append(out)

            # collect output and targets
            if collected_input is not None:
                collected_input = append(collected_input, inputs, axis=0)
            else:
                collected_input = array(inputs)
#            print(collected_input; exit())

            if collected_output is not None:
                collected_output = append(collected_output, outputs, axis=0)
            else:
                collected_output = array(outputs)

            if collected_target is not None:
                collected_target = append(collected_target, calculation_target, axis=0)
            else:
                collected_target = calculation_target

        return collected_input, collected_output, collected_target

    def _createFullInput(self, input, output):
        if self.indim > 0:
            return append(input, output)
        else:
            return array(output)



    def _getLastOutput(self):
        if self.offset == 0:
            return zeros(self.outdim)
        else:
            return self._out_layer.outputbuffer[self.offset - 1]

    def _setLastOutput(self, output):
        self._out_layer.outputbuffer[self.offset - 1][:] = output


    # ======================================================== Genome related ===


    def _validateGenomeLayer(self, layer):
        """ Validates the type and state of a layer
        """
        assert isinstance(layer, LSTMLayer)
        assert not layer.peepholes


    def getGenome(self):
        """ Returns the Genome of the network.
            See class description for more details.
        """
        return self._getGenomeOfLayer(self._hid_layer)


    def setGenome(self, weights):
        """ Sets the Genome of the network.
            See class description for more details.
        """
        weights = deepcopy(weights)
        self._setGenomeOfLayer(self._hid_layer, weights)



    def _getGenomeOfLayer(self, layer):
        """ Returns the genome of a single layer.
        """
        self._validateGenomeLayer(layer)

        dim = layer.outdim
        layer_weights = []

        connections = self._getInputConnectionsOfLayer(layer)

        for cell_idx in range(dim):
            # todo: the evolino paper uses a different order of weights for the genotype of a lstm cell
            cell_weights = []
            for c in connections:
                cell_weights += [
                    c.params[ cell_idx + 0 * dim ],
                    c.params[ cell_idx + 1 * dim ],
                    c.params[ cell_idx + 2 * dim ],
                    c.params[ cell_idx + 3 * dim ] ]

            layer_weights.append(cell_weights)
        return layer_weights





    def _setGenomeOfLayer(self, layer, weights):
        """ Sets the genome of a single layer.
        """
        self._validateGenomeLayer(layer)

        dim = layer.outdim

        connections = self._getInputConnectionsOfLayer(layer)

        for cell_idx in range(dim):
            cell_weights = weights.pop(0)
            for c in connections:
                params = c.params
                params[cell_idx + 0 * dim] = cell_weights.pop(0)
                params[cell_idx + 1 * dim] = cell_weights.pop(0)
                params[cell_idx + 2 * dim] = cell_weights.pop(0)
                params[cell_idx + 3 * dim] = cell_weights.pop(0)
            assert not len(cell_weights)





    # ============================================ Linear Regression related ===

    def setOutputWeightMatrix(self, W):
        """ Sets the weight matrix of the output layer's input connection.
        """
        c = self._hid_to_out_connection
        c.params[:] = W.flatten()

    def getOutputWeightMatrix(self):
        """ Sets the weight matrix of the output layer's input connection.
        """
        c = self._hid_to_out_connection
        p = c.getParameters()
        return reshape(p, (c.outdim, c.indim))




    def _getRawOutput(self):
        """ Returns the current output of the last hidden layer.
            This is needed for linear regression, which calculates
            the weight matrix W of the full connection between this layer
            and the output layer.
        """
        return copy(self._hid_layer.outputbuffer[self.offset - 1])






    # ====================================================== Topology Helper ===



    def _getInputConnectionsOfLayer(self, layer):
        """ Returns a list of all input connections for the layer. """
        connections = []
        for c in sum(self._network.connections.values(), []):
            if c.outmod is layer:
                if not isinstance(c, FullConnection):
                    raise NotImplementedError("At the time there is only support for FullConnection")
                connections.append(c)
        return connections















from numpy import reshape
from copy  import copy, deepcopy


class NetworkWrapper(object):
    """ Network wrapper class for Evolino Networks

        This class implements methods for extracting and setting the genome of
        the supplied network to allow its evolving.
        The genome of the network consists of the input weights of each hidden
        lstm neuron. The structure of the genome will be a list of lists,
        where the inner lists bundle all input weights of on neuron:
            [ [ neuron1's inweights ] , [ neuron2's inweights ] , ... ]
        The inner lists will be used as chromosomes inside the evolino framework.

        Also there are methods that help with the linear regression part.
        They can extract end set the weight matrix W for the last full-connection.

        At the moment the network must meet following constraints:
            - All hidden layers that have input connections must be of type LSTMLayer
            - The LSTMLayer do not use peepholes
            - There must be exactly one output-layer
            - There must be exactly one input-layer
            - There must be only one layer, that is connected to the output layer
            - The input layer must be connected to only one hidden layer
            - All used connections must be of type FullConnection

        When the network is supplied it will be augmented with a
        recurrent full connection from the output layer to the first hidden layer.
        So do not do this yourself.

    """
    def __init__(self, network):
        """ :key network: The network to be wrapped
        """
        self.network = network
        self._output_connection = None
        self._last_hidden_layer = None
        self._first_hidden_layer = None
        self._establishRecurrence()

    def getNetwork(self):
        """ Returns the Network """
        return self.network

    def _establishRecurrence(self):
        """ Adds a recurrent full connection from the output layer to the first
            hidden layer.
        """
        network = self.network
        outlayer = self.getOutputLayer()
        hid1layer = self.getFirstHiddenLayer()
        network.addRecurrentConnection(FullConnection(outlayer, hid1layer))


    # ======================================================== Genome related ===


    def _validateGenomeLayer(self, layer):
        """ Validates the type and state of a layer
        """
        assert isinstance(layer, LSTMLayer)
        assert not layer.peepholes


    def getGenome(self):
        """ Returns the Genome of the network.
            See class description for more details.
        """
        weights = []
        for layer in self.getHiddenLayers():
            if isinstance(layer, LSTMLayer):
#                 if layer is not self._recurrence_layer:
                weights += self._getGenomeOfLayer(layer)
        return weights

    def setGenome(self, weights):
        """ Sets the Genome of the network.
            See class description for more details.
        """
        weights = deepcopy(weights)
        for layer in self.getHiddenLayers():
            if isinstance(layer, LSTMLayer):
#               if layer is not self._recurrence_layer:
                self._setGenomeOfLayer(layer, weights)



    def _getGenomeOfLayer(self, layer):
        """ Returns the genome of a single layer.
        """
        self._validateGenomeLayer(layer)

        dim = layer.outdim
        layer_weights = []

        connections = self._getInputConnectionsOfLayer(layer)

        for cell_idx in range(dim):
            # todo: the evolino paper uses a different order of weights for the genotype of a lstm cell
            cell_weights = []
            for c in connections:
                cell_weights += [
                    c.getParameters()[ cell_idx + 0 * dim ],
                    c.getParameters()[ cell_idx + 1 * dim ],
                    c.getParameters()[ cell_idx + 2 * dim ],
                    c.getParameters()[ cell_idx + 3 * dim ] ]

            layer_weights.append(cell_weights)
        return layer_weights





    def _setGenomeOfLayer(self, layer, weights):
        """ Sets the genome of a single layer.
        """
        self._validateGenomeLayer(layer)

        dim = layer.outdim

        connections = self._getInputConnectionsOfLayer(layer)

        for cell_idx in range(dim):
            cell_weights = weights.pop(0)
            for c in connections:
                params = c.getParameters()
                params[cell_idx + 0 * dim] = cell_weights.pop(0)
                params[cell_idx + 1 * dim] = cell_weights.pop(0)
                params[cell_idx + 2 * dim] = cell_weights.pop(0)
                params[cell_idx + 3 * dim] = cell_weights.pop(0)
            assert not len(cell_weights)



    # ============================================ Linear Regression related ===

    def setOutputWeightMatrix(self, W):
        """ Sets the weight matrix of the output layer's input connection.
        """
        c = self.getOutputConnection()
        p = c.getParameters()
        p[:] = W.flatten()

    def getOutputWeightMatrix(self):
        """ Sets the weight matrix of the output layer's input connection.
        """
        c = self.getOutputConnection()
        p = c.getParameters()
        return reshape(p, (c.outdim, c.indim))


    def injectBackproject(self, injection):
        """ Injects a vector into the recurrent connection.
            This will be used in the evolino trainingsphase, where the target
            values need to be backprojected instead of the real output of the net.

            :key injection: vector of length self.network.outdim
        """
        outlayer = self.getOutputLayer()
        outlayer.outputbuffer[self.network.offset - 1][:] = injection


    def _getRawOutput(self):
        """ Returns the current output of the last hidden layer.
            This is needed for linear regression, which calculates
            the weight matrix W of the full connection between this layer
            and the output layer.
        """
        return copy(self.getLastHiddenLayer().outputbuffer[self.network.offset - 1])


    # ====================================================== Topology Helper ===


    def getOutputLayer(self):
        """ Returns the output layer """
        assert len(self.network.outmodules) == 1
        return self.network.outmodules[0]



    def getOutputConnection(self):
        """ Returns the input connection of the output layer. """
        if self._output_connection is None:
            outlayer = self.getOutputLayer()
            lastlayer = self.getLastHiddenLayer()
            for c in self.getConnections():
                if c.outmod is outlayer:
                    assert c.inmod is lastlayer
                    self._output_connection = c

        return self._output_connection



    def getLastHiddenLayer(self):
        """ Returns the last hidden layer. """
        if self._last_hidden_layer is None:
            outlayer = self.getOutputLayer()
            layers = []
            for c in self.getConnections():
                if c.outmod is outlayer:
#                    print(c.inmod)
                    layers.append(c.inmod)

            assert len(layers) == 1
            self._last_hidden_layer = layers[0]
        return self._last_hidden_layer



    def getFirstHiddenLayer(self):
        """ Returns the first hidden layer. """
        if self._first_hidden_layer is None:
            inlayer = self.getInputLayer()
            layers = []
            for c in self.getConnections():
                if c.inmod is inlayer:
                    layers.append(c.outmod)

            assert len(layers) == 1
            self._first_hidden_layer = layers[0]
        return self._first_hidden_layer



    def getConnections(self):
        """ Returns a list of all connections. """
        return sum(self.network.connections.values(), [])

    def getInputLayer(self):
        """ Returns the input layer. """
        assert len(self.network.inmodules) == 1
        return self.network.inmodules[0]

    def _getInputConnectionsOfLayer(self, layer):
        """ Returns a list of all input connections for the layer. """
        connections = []
        for c in sum(self.network.connections.values(), []):
            if c.outmod is layer:
                if not isinstance(c, FullConnection):
                    raise NotImplementedError("At the time there is only support for FullConnection")
                connections.append(c)
        return connections



    def getHiddenLayers(self):
        """ Returns a list of all hidden layers. """
        layers = []
        network = self.network
        for m in network.modules:
            if m not in network.inmodules and m not in network.outmodules:
                layers.append(m)
        return layers






########NEW FILE########
__FILENAME__ = population
__author__ = 'Michael Isik'

from pybrain.supervised.evolino.gpopulation import Population, SimplePopulation
from pybrain.supervised.evolino.gfilter import Randomization
from pybrain.supervised.evolino.individual import EvolinoIndividual, EvolinoSubIndividual

from pybrain.tools.kwargsprocessor import KWArgsProcessor

from copy   import copy
from random import randrange


class EvolinoPopulation(Population):
    """ Evolino's population class.

        EvolinoIndividuals aren't stored directly, but there is a list of
        subpopulations.
        These subpopulations are used to generate EvolinoIndividuals on demand.

        On initialization, a prototype individual must be supplied. Its genome
        should be a list of chromosomes. A chromosome should be a list of floats.

        A subpopulation of size subPopulationSize is created for each of these
        chromosomes.

        :key nCombinations: Denotes the number of times each subindividual should
                              be built into an individual. default=1
        :key valueInitializer:
    """
    def __init__(self, individual, subPopulationSize, nCombinations=1, valueInitializer=Randomization(-0.1, 0.1), **kwargs):
        """ :key individual: A prototype individual which is used to determine
                               the structure of the genome.
            :key subPopulationSize: integer describing the size of the subpopulations
        """
        Population.__init__(self)

        self._subPopulations = []

        self.nCombinations = nCombinations

        ap = KWArgsProcessor(self, kwargs)

        ap.add('verbosity', default=0)



        genome = individual.getGenome()
        for chromosome in genome:
            self._subPopulations.append(
                EvolinoSubPopulation(chromosome, subPopulationSize, valueInitializer))

    def getIndividuals(self):
        """ Returns a set of individuals of type EvolinoIndividual. The individuals
            are generated on the fly. Note that each subpopulation has the same size.
            So the number of resulting EvolinoIndividuals is subPopulationSize,
            since each chromosome of each subpopulation will be assembled once.

            The subpopulation container is a sequence with strict order. This
            sequence is iterated subPopulationSize times. In each iteration
            one random EvolinoSubIndividual is taken from each sub population.
            After each iteration the resulting sequence of sub individuals
            is supplied to the constructor of a new EvolinoIndividual.
            All EvolinoIndividuals are collected in a set, which is finally returned.
        """
        assert len(self._subPopulations)

        individuals = set()

        for _ in range(self.nCombinations):
            subIndividualsList = [ list(sp.getIndividuals()) for sp in self._subPopulations ]

            nIndividuals = len(subIndividualsList[0])

            for _ in range(nIndividuals):
                subIndividualCombination = []
                for subIndividuals in subIndividualsList:
                    sub_individual = subIndividuals.pop(randrange(len(subIndividuals)))
                    subIndividualCombination.append(sub_individual)
                individuals.add(EvolinoIndividual(subIndividualCombination))

        return individuals

    def getSubPopulations(self):
        """ Returns a shallow copy of the list of subpopulation. """
        return copy(self._subPopulations)



    def setIndividualFitness(self, individual, fitness):
        """ The fitness value is not stored directly inside this population,
            but is propagated to the subpopulations of all the subindividuals
            of which the individual consists of.
            The individual's fitness value is only adjusted if its bigger than
            the old value.
            To reset these values use clearFitness().
        """


        # additive fitness distribution
#        subIndividuals = individual.getSubIndividuals()
#        for i,sp in enumerate(self._subPopulations):
#            sp.addIndividualFitness( subIndividuals[i], fitness )

        # max fitness distribution
        subIndividuals = individual.getSubIndividuals()
        for i, sp in enumerate(self._subPopulations):
            sub_individual = subIndividuals[i]
            old_fitness = sp.getIndividualFitness(sub_individual)
            if old_fitness < fitness:
                sp.setIndividualFitness(sub_individual, fitness)



    def clearFitness(self):
        """ Clears all fitness values of all subpopulations. """
        for sp in self._subPopulations:
            sp.clearFitness()




class EvolinoSubPopulation(SimplePopulation):
    """ The class for Evolino subpopulations. Mostly the same as SimplePopulation
        but with a few extensions.
        It contains a set of EvolinoSubIndividuals.

        On initialization, a prototype individual is created from the prototype
        chromosome. This individual is then cloned and added so that the
        population exists of maxNIndividuals individuals.

        The genomes of these clones are then randomized by the Randomization
        operator.
    """
    def __init__(self, chromosome, maxNIndividuals, valueInitializer=Randomization(-0.1, 0.1), **kwargs):
        """ :key chromosome: The prototype chromosome
            :key maxNIndividuals: The maximum allowed number of individuals
        """
        SimplePopulation.__init__(self)

        self._prototype = EvolinoSubIndividual(chromosome)

        self._maxNIndividuals = maxNIndividuals
        self._valueInitializer = valueInitializer


        self.setArgs(**kwargs)


        for _ in range(maxNIndividuals):
            self.addIndividual(self._prototype.copy())
        self._valueInitializer.apply(self)


    def setArgs(self, **kwargs):
        for key, val in kwargs.iteritems():
            getattr(self, key)
            setattr(self, key, val)


    def getMaxNIndividuals(self):
        """ Returns the maximum allowed number of individuals """
        return self._maxNIndividuals

    def addIndividualFitness(self, individual, fitness):
        """ Add fitness to the individual's fitness value.
            :key fitness: a float value denoting the fitness
        """
        self._fitness[individual] += fitness




########NEW FILE########
__FILENAME__ = variate
__author__ = 'Michael Isik'

from random import uniform, random, gauss
from numpy  import tan, pi


class UniformVariate:
    def __init__(self, min_val=0., max_val=1.):
        """ Initializes the uniform variate with a min and a max value.
        """
        self._min_val = min_val
        self._max_val = max_val

    def getSample(self, min_val=None, max_val=None):
        """ Returns a random value between min_val and max_val.
        """
        if min_val is None: min_val = self._min_val
        if max_val is None: max_val = self._max_val
        return uniform(min_val, max_val)

class CauchyVariate:
    def __init__(self, x0=0., alpha=1.):
        """ :key x0: Median and mode of the Cauchy distribution
            :key alpha: scale
        """
        self.x0 = x0
        self.alpha = alpha

    def getSample(self, x0=None, alpha=None):
        if x0    is None: x0 = self.x0
        if alpha is None: alpha = self.alpha
        uniform_variate = random()
        cauchy_variate = x0 + alpha * tan(pi * (uniform_variate - 0.5))
        return cauchy_variate


class GaussianVariate:
    def __init__(self, x0=0., alpha=1.):
        """ :key x0: Mean
            :key alpha: standard deviation
        """
        self.x0 = x0
        self.alpha = alpha

    def getSample(self, x0=None, alpha=None):
        if x0    is None: x0 = self.x0
        if alpha is None: alpha = self.alpha
        return gauss(x0, alpha)



########NEW FILE########
__FILENAME__ = minhash
from __future__ import division


"""Module that provides functionality for locality sensitive hashing in hamming
spaces."""


__author__ = 'Justin Bayer, bayer.justin@googlemail.com'
__version__ = '$Id$'


from collections import defaultdict
from scipy import array
from numpy.random import permutation


def arrayPermutation(permutation):
    """Return a permutation function.

    The function permutes any array as specified by the supplied permutation.
    """
    assert permutation.ndim == 1, \
           "Only one dimensional permutaton arrays are supported"

    def permute(arr):
        assert arr.ndim == 1, "Only one dimensional arrays are supported"
        assert arr.shape == permutation.shape, "Array shapes don't match"
        return array([arr[i] for i in permutation])

    return permute


def jacardCoefficient(a, b):
    """Return the Jacard coefficient of a and b.

    The jacard coefficient is defined as the overlap between two sets: the sum
    of all equal elements divided by the size of the sets.

    Mind that a and b must b in Hamming space, so every element must either be
    1 or 0.
    """
    if a.shape != b.shape:
        raise ValueError("Arrays must be of same shape")

    length = a.shape[0]
    a = a.astype(bool)
    b = b.astype(bool)

    return (a == b).sum() / length


class MinHash(object):
    """Class for probabilistic hashing of items in the hamming space.

    Introduced in

        E. Cohen. Size-Estimation Framework with Applications to
        Transitive Closure and Reachability. Journal of Computer and System
        Sciences 55 (1997): 441-453"""

    def __setPermutations(self, permutations):
        self._permutations = permutations
        self._permFuncs = [arrayPermutation(i) for i in permutations]

    def __getPermutations(self):
        return self._permutations

    permutations = property(__getPermutations,
                            __setPermutations)

    def __init__(self, dim, nPermutations):
        """Create a hash structure that can hold arrays of size dim and
        hashes with nPermutations permutations.

        The number of buckets is dim * nPermutations."""
        self.dim = dim
        self.permutations = array([permutation(dim)
                                   for _ in xrange(nPermutations)])

        self.buckets = defaultdict(lambda: [])

    def _firstOne(self, arr):
        """Return the index of the first 1 in the array."""
        for i, elem in enumerate(arr):
            if elem == 1:
                return i
        return i + 1

    def _checkItem(self, item):
        if item.ndim != 1:
            raise ValueError("Only one dimensional arrays are supported")
        if item.shape != (self.dim,):
            raise ValueError("Array has wrong size")

    def _hash(self, item):
        """Return a hash for item based on the internal permutations.

        That hash is a tuple of ints.
        """
        self._checkItem(item)

        result = []
        for perm in self._permFuncs:
            permuted = perm(item)
            result.append(self._firstOne(permuted))
        return tuple(result)

    def put(self, item, satellite):
        """Put an item into the hash structure and attach any object satellite
        to it."""
        self._checkItem(item)

        item = item.astype(bool)
        bucket = self._hash(item)
        self.buckets[bucket].append((item, satellite))


    def knn(self, item, k):
        """Return the k nearest neighbours of the item in the current hash.

        Mind that the probabilistic nature of the data structure might not
        return a nearest neighbor at all.
        """
        self._checkItem(item)

        candidates = self.buckets[self._hash(item)]
        candidates.sort(key=lambda x: jacardCoefficient(x[0], item),
                        reverse=True)
        return candidates[:k]



########NEW FILE########
__FILENAME__ = nearoptimal
from __future__ import division

"""This module provides functionality for locality sensitive hashing in high
dimensional euclidean spaces.

It is based on the work of Andoni and Indyk, 'Near-Optimal Hashing Algorithms
for Approximate Nearest Neighbor in High Dimensions'."""


__author__ = 'Justin Bayer, bayer.justin@googlemail.com'


import logging

from collections import defaultdict
from heapq import nlargest
from math import sqrt, log, ceil

from scipy import array, dot, random, ones
from scipy import weave


class MultiDimHash(object):
    """Class that represents a datastructure that enables nearest neighbours
    search and methods to do so."""

    # If the dimension of a dataset is bigger than this bound, the
    # dimensionality will be reduced by a random projection into 24dimensional
    # space
    lowerDimensionBound = 24

    def _getRadius(self):
        return self._radius

    def _setRadius(self, value):
        self._radius = abs(value)
        self.radiusSquared = value ** 2

    radius = property(_getRadius, _setRadius)

    def __init__(self, dim, omega=4, prob=0.8):
        """Create a hash for arrays of dimension dim.

        The hyperspace will be split into hypercubes with a sidelength of
        omega * sqrt(sqrt(dim)), that is omega * radius.

        Every point in the dim-dimensional euclidean space will be hashed to
        its correct bucket with a probability of prob.

        """
        message = ("Creating Hash with %i dimensions, sidelength %.2f and " +
                  "cNN-probability %.2f") % (dim, omega, prob)
        logging.debug(message)

        self.dim = dim
        self.omega = omega
        self.prob = prob

        self.radius = sqrt(sqrt(min(dim, self.lowerDimensionBound)))
        logging.debug("Radius set to %.2f" % self.radius)

        self._initializeGrids()
        self._initializeProjection()

        self.balls = defaultdict(lambda: [])

    def _findAmountOfGrids(self):
        w = self.radius
        omega = self.omega
        d = self.dim
        prob = self.prob

        N = ((omega * w) / (w / sqrt(d))) ** d
        result = int(ceil(log((1 - prob) / N, 1 - 1 / N)))
        logging.debug("Number of grids: %i" % result)
        return result

    def _initializeGrids(self):
        offset = self.omega * self.radius
        radius_offset = ones(self.dim) * self.radius
        self.gridBalls = random.random((self._findAmountOfGrids(), self.dim))
        self.gridBalls *= offset
        self.gridBalls += radius_offset

    def _initializeProjection(self):
        if self.dim <= self.lowerDimensionBound:
            # We only need to reduce the dimension if it's bigger than
            # lowerDimensionBound; otherwise, chose identity
            self.projection = 1
        else:
            projection_shape = self.dim, self.lowerDimensionBound
            self.projection = random.standard_normal(projection_shape)
            self.projection /= sqrt(self.lowerDimensionBound)

    def _findHypercube(self, point):
        """Return where a point lies in what hypercube.

        The result is a pair of two arrays. The first array is an array of
        integers that indicate the multidimensional index of the hypercube it
        is in. The second array is an array of floats, specifying the
        coordinates of the point in that hypercube.
        """
        offset = self.omega * self.radius
        divmods = (divmod(p, offset) for p in point)
        hypercube_indices, relative_point = [], []
        for index, rest in divmods:
            hypercube_indices.append(index)
            relative_point.append(rest)
        return array(hypercube_indices, dtype=int), array(relative_point)

    def _findLocalBall_noinline(self, point):
        """Return the index of the ball that the point lies in."""
        for i, ball in enumerate(self.gridBalls):
            distance = point - ball
            if dot(distance.T, distance) <= self.radiusSquared:
                return i

    def _findLocalBall_inline(self, point):
        """Return the index of the ball that the point lies in."""
        balls = self.gridBalls
        nBalls, dim = balls.shape #@UnusedVariable
        radiusSquared = self.radiusSquared #@UnusedVariable

        code = """
            #line 121 "nearoptimal.py"
            return_val = -1;
            for (long i = 0; i < nBalls; i++)
            {
                double distance = 0.0;
                for (long j = 0; j < dim; j++)
                {
                    double diff = balls(i, j) - point(j);
                    distance += diff * diff;
                }
                if (distance <= radiusSquared) {
                    return_val = i;
                    break;
                }
            }
        """

        variables = 'point', 'balls', 'nBalls', 'dim', 'radiusSquared',
        result = weave.inline(
            code,
            variables,
            type_converters=weave.converters.blitz,
            compiler='gcc')

        return result if result != -1 else None

    _findLocalBall = _findLocalBall_noinline

    def findBall(self, point):
        hypercube_index, relative_point = self._findHypercube(point)
        ball_index = self._findLocalBall(relative_point)
        return tuple(hypercube_index), ball_index

    def insert(self, point, satellite):
        """Put a point and its satellite information into the hash structure.
        """
        point = dot(self.projection, point)
        index = self.findBall(point)
        self.balls[index].append((point, satellite))

    def _findKnnCandidates(self, point):
        """Return a set of candidates that might be nearest neighbours of a
        query point."""
        index = self.findBall(point)
        logging.debug("Found %i candidates for cNN" % len(self.balls[index]))
        return self.balls[index]

    def knn(self, point, k):
        """Return the k approximate nearest neighbours of the item in the
        current hash.

        Mind that the probabilistic nature of the data structure might not
        return a nearest neighbor at all and not the nearest neighbour."""

        candidates = self._findKnnCandidates(point)

        def sortKey((point_, satellite_)):
            distance = point - point_
            return - dot(distance.T, distance)

        return nlargest(k, candidates, key=sortKey)

########NEW FILE########
__FILENAME__ = backprop
__author__ = 'Daan Wierstra and Tom Schaul'

from scipy import dot, argmax
from random import shuffle
from math import isnan
from pybrain.supervised.trainers.trainer import Trainer
from pybrain.utilities import fListToString
from pybrain.auxiliary import GradientDescent


class BackpropTrainer(Trainer):
    """Trainer that trains the parameters of a module according to a
    supervised dataset (potentially sequential) by backpropagating the errors
    (through time)."""

    def __init__(self, module, dataset=None, learningrate=0.01, lrdecay=1.0,
                 momentum=0., verbose=False, batchlearning=False,
                 weightdecay=0.):
        """Create a BackpropTrainer to train the specified `module` on the
        specified `dataset`.

        The learning rate gives the ratio of which parameters are changed into
        the direction of the gradient. The learning rate decreases by `lrdecay`,
        which is used to to multiply the learning rate after each training
        step. The parameters are also adjusted with respect to `momentum`, which
        is the ratio by which the gradient of the last timestep is used.

        If `batchlearning` is set, the parameters are updated only at the end of
        each epoch. Default is False.

        `weightdecay` corresponds to the weightdecay rate, where 0 is no weight
        decay at all.
        """
        Trainer.__init__(self, module)
        self.setData(dataset)
        self.verbose = verbose
        self.batchlearning = batchlearning
        self.weightdecay = weightdecay
        self.epoch = 0
        self.totalepochs = 0
        # set up gradient descender
        self.descent = GradientDescent()
        self.descent.alpha = learningrate
        self.descent.momentum = momentum
        self.descent.alphadecay = lrdecay
        self.descent.init(module.params)

    def train(self):
        """Train the associated module for one epoch."""
        assert len(self.ds) > 0, "Dataset cannot be empty."
        self.module.resetDerivatives()
        errors = 0
        ponderation = 0.
        shuffledSequences = []
        for seq in self.ds._provideSequences():
            shuffledSequences.append(seq)
        shuffle(shuffledSequences)
        for seq in shuffledSequences:
            e, p = self._calcDerivs(seq)
            errors += e
            ponderation += p
            if not self.batchlearning:
                gradient = self.module.derivs - self.weightdecay * self.module.params
                new = self.descent(gradient, errors)
                if new is not None:
                    self.module.params[:] = new
                self.module.resetDerivatives()

        if self.verbose:
            print("Total error:", errors / ponderation)
        if self.batchlearning:
            self.module._setParameters(self.descent(self.module.derivs))
        self.epoch += 1
        self.totalepochs += 1
        return errors / ponderation


    def _calcDerivs(self, seq):
        """Calculate error function and backpropagate output errors to yield
        the gradient."""
        self.module.reset()
        for sample in seq:
            self.module.activate(sample[0])
        error = 0
        ponderation = 0.
        for offset, sample in reversed(list(enumerate(seq))):
            # need to make a distinction here between datasets containing
            # importance, and others
            target = sample[1]
            outerr = target - self.module.outputbuffer[offset]
            if len(sample) > 2:
                importance = sample[2]
                error += 0.5 * dot(importance, outerr ** 2)
                ponderation += sum(importance)
                self.module.backActivate(outerr * importance)
            else:
                error += 0.5 * sum(outerr ** 2)
                ponderation += len(target)
                # FIXME: the next line keeps arac from producing NaNs. I don't
                # know why that is, but somehow the __str__ method of the
                # ndarray class fixes something,
                str(outerr)
                self.module.backActivate(outerr)

        return error, ponderation

    def _checkGradient(self, dataset=None, silent=False):
        """Numeric check of the computed gradient for debugging purposes."""
        if dataset:
            self.setData(dataset)
        res = []
        for seq in self.ds._provideSequences():
            self.module.resetDerivatives()
            self._calcDerivs(seq)
            e = 1e-6
            analyticalDerivs = self.module.derivs.copy()
            numericalDerivs = []
            for p in range(self.module.paramdim):
                storedoldval = self.module.params[p]
                self.module.params[p] += e
                righterror, dummy = self._calcDerivs(seq)
                self.module.params[p] -= 2 * e
                lefterror, dummy = self._calcDerivs(seq)
                approxderiv = (righterror - lefterror) / (2 * e)
                self.module.params[p] = storedoldval
                numericalDerivs.append(approxderiv)
            r = zip(analyticalDerivs, numericalDerivs)
            res.append(r)
            if not silent:
                print(r)
        return res

    def testOnData(self, dataset=None, verbose=False):
        """Compute the MSE of the module performance on the given dataset.

        If no dataset is supplied, the one passed upon Trainer initialization is
        used."""
        if dataset == None:
            dataset = self.ds
        dataset.reset()
        if verbose:
            print('\nTesting on data:')
        errors = []
        importances = []
        ponderatedErrors = []
        for seq in dataset._provideSequences():
            self.module.reset()
            e, i = dataset._evaluateSequence(self.module.activate, seq, verbose)
            importances.append(i)
            errors.append(e)
            ponderatedErrors.append(e / i)
        if verbose:
            print('All errors:', ponderatedErrors)
        assert sum(importances) > 0
        avgErr = sum(errors) / sum(importances)
        if verbose:
            print('Average error:', avgErr)
            print(('Max error:', max(ponderatedErrors), 'Median error:',
                   sorted(ponderatedErrors)[len(errors) / 2]))
        return avgErr

    def testOnClassData(self, dataset=None, verbose=False,
                        return_targets=False):
        """Return winner-takes-all classification output on a given dataset.

        If no dataset is given, the dataset passed during Trainer
        initialization is used. If return_targets is set, also return
        corresponding target classes.
        """
        if dataset == None:
            dataset = self.ds
        dataset.reset()
        out = []
        targ = []
        for seq in dataset._provideSequences():
            self.module.reset()
            for input, target in seq:
                res = self.module.activate(input)
                out.append(argmax(res))
                targ.append(argmax(target))
        if return_targets:
            return out, targ
        else:
            return out

    def trainUntilConvergence(self, dataset=None, maxEpochs=None, verbose=None,
                              continueEpochs=10, validationProportion=0.25,
                              trainingData=None, validationData=None,
                              convergence_threshold=10):
        """Train the module on the dataset until it converges.

        Return the module with the parameters that gave the minimal validation
        error.

        If no dataset is given, the dataset passed during Trainer
        initialization is used. validationProportion is the ratio of the dataset
        that is used for the validation dataset.
        
        If the training and validation data is already set, the splitPropotion is ignored

        If maxEpochs is given, at most that many epochs
        are trained. Each time validation error hits a minimum, try for
        continueEpochs epochs to find a better one."""
        epochs = 0
        if dataset is None:
            dataset = self.ds
        if verbose is None:
            verbose = self.verbose
        if trainingData is None or validationData is None:
            # Split the dataset randomly: validationProportion of the samples for
            # validation.
            trainingData, validationData = (
                dataset.splitWithProportion(1 - validationProportion))
        if not (len(trainingData) > 0 and len(validationData)):
            raise ValueError("Provided dataset too small to be split into training " +
                             "and validation sets with proportion " + str(validationProportion))
        self.ds = trainingData
        bestweights = self.module.params.copy()
        bestverr = self.testOnData(validationData)
        bestepoch = 0
        self.trainingErrors = []
        self.validationErrors = [bestverr]
        while True:
            trainingError = self.train()
            validationError = self.testOnData(validationData)
            if isnan(trainingError) or isnan(validationError):
                raise Exception("Training produced NaN results")
            self.trainingErrors.append(trainingError)
            self.validationErrors.append(validationError)
            if epochs == 0 or self.validationErrors[-1] < bestverr:
                # one update is always done
                bestverr = self.validationErrors[-1]
                bestweights = self.module.params.copy()
                bestepoch = epochs

            if maxEpochs != None and epochs >= maxEpochs:
                self.module.params[:] = bestweights
                break
            epochs += 1

            if len(self.validationErrors) >= continueEpochs * 2:
                # have the validation errors started going up again?
                # compare the average of the last few to the previous few
                old = self.validationErrors[-continueEpochs * 2:-continueEpochs]
                new = self.validationErrors[-continueEpochs:]
                if min(new) > max(old):
                    self.module.params[:] = bestweights
                    break
                elif reduce(lambda x, y: x + (y - round(new[-1], convergence_threshold)), [round(y, convergence_threshold) for y in new]) == 0:
                    self.module.params[:] = bestweights
                    break
        #self.trainingErrors.append(self.testOnData(trainingData))
        self.ds = dataset
        if verbose:
            print('train-errors:', fListToString(self.trainingErrors, 6))
            print('valid-errors:', fListToString(self.validationErrors, 6))
        return self.trainingErrors[:bestepoch], self.validationErrors[:1 + bestepoch]

########NEW FILE########
__FILENAME__ = evolino
__author__ = 'Michael Isik'


from numpy import Infinity

from pybrain.supervised.trainers.trainer import Trainer
from pybrain.supervised.evolino.population import EvolinoPopulation
from pybrain.supervised.evolino.individual import EvolinoSubIndividual
from pybrain.supervised.evolino.filter import EvolinoEvaluation, EvolinoSelection, EvolinoReproduction, EvolinoBurstMutation
from pybrain.supervised.evolino.gfilter import Randomization
from pybrain.supervised.evolino.variate import CauchyVariate
from pybrain.tools.kwargsprocessor import KWArgsProcessor


class EvolinoTrainer(Trainer):
    """ The Evolino trainer class.

        Use a network as module that should be trained. There are some restrictions
        the network must follow. Basically, it should be a simple lstm network.
        For more details on these restrictions read NetworkWrapper's documentaion.
    """
    initialWeightRange = property(lambda self: self._initialWeightRange)
    subPopulationSize = property(lambda self: self._subPopulationSize)
    nCombinations = property(lambda self: self._nCombinations)
    nParents = property(lambda self: self._nParents)
    initialWeightRange = property(lambda self: self._initialWeightRange)
    mutationAlpha = property(lambda self: self._mutationAlpha)
    mutationVariate = property(lambda self: self._mutationVariate)
    wtRatio = property(lambda self: self._wtRatio)
    weightInitializer = property(lambda self: self._weightInitializer)
#    burstMutation        = property(lambda self: self._burstMutation)
    backprojectionFactor = property(lambda self: self._backprojectionFactor)

    def __init__(self, evolino_network, dataset, **kwargs):
        """
            :key subPopulationSize: Size of the subpopulations.
            :key nCombinations: Number of times each chromosome is built into an individual. default=1
            :key nParents: Number of individuals left in a subpopulation after selection.
            :key initialWeightRange: Range of the weights of the RNN after initialization. default=(-0.1,0.1)
            :key weightInitializer: Initializer object for the weights of the RNN. default=Randomization(...)
            :key mutationAlpha: The mutation's intensity. default=0.01
            :key mutationVariate: The variate used for mutation. default=CauchyVariate(...)
            :key wtRatio: The quotient: washout-time/training-time. Needed to
                            split the sequences into washout phase and training phase.
            :key nBurstMutationEpochs: Number of epochs without increase of fitness in a row,
                                         before burstmutation is applied. default=Infinity
            :key backprojectionFactor: Weight of the backprojection. Usually
                                         supplied through evolino_network.
            :key selection: Selection object for evolino
            :key reproduction: Reproduction object for evolino
            :key burstMutation: BurstMutation object for evolino
            :key evaluation: Evaluation object for evolino
            :key verbosity: verbosity level
        """
        Trainer.__init__(self, evolino_network)

        self.network = evolino_network
        self.setData(dataset)

        ap = KWArgsProcessor(self, kwargs)

        # misc
        ap.add('verbosity', default=0)

        # population
        ap.add('subPopulationSize', private=True, default=8)
        ap.add('nCombinations', private=True, default=4)
        ap.add('nParents', private=True, default=None)
        ap.add('initialWeightRange', private=True, default=(-0.1, 0.1))
        ap.add('weightInitializer', private=True, default=Randomization(self._initialWeightRange[0], self._initialWeightRange[1]))

        # mutation
        ap.add('mutationAlpha', private=True, default=0.01)
        ap.add('mutationVariate', private=True, default=CauchyVariate(0, self._mutationAlpha))

        # evaluation
        ap.add('wtRatio', private=True, default=(1, 3))

        # burst mutation
        ap.add('nBurstMutationEpochs', default=Infinity)

        # network
        ap.add('backprojectionFactor', private=True, default=float(evolino_network.backprojectionFactor))
        evolino_network.backprojectionFactor = self._backprojectionFactor

        # aggregated objects
        ap.add('selection', default=EvolinoSelection())
        ap.add('reproduction', default=EvolinoReproduction(mutationVariate=self.mutationVariate))
        ap.add('burstMutation', default=EvolinoBurstMutation())
        ap.add('evaluation', default=EvolinoEvaluation(evolino_network, self.ds, **kwargs))

        self.selection.nParents = self.nParents

        self._population = EvolinoPopulation(
            EvolinoSubIndividual(evolino_network.getGenome()),
            self._subPopulationSize,
            self._nCombinations,
            self._weightInitializer
            )

        filters = []
        filters.append(self.evaluation)
        filters.append(self.selection)
        filters.append(self.reproduction)

        self._filters = filters

        self.totalepochs = 0
        self._max_fitness = self.evaluation.max_fitness
        self._max_fitness_epoch = self.totalepochs

    def setDataset(self, dataset):
        self.evaluation.dataset = dataset

    def trainOnDataset(self, *args, **kwargs):
        """ Not implemented """
        raise NotImplementedError()

    def train(self):
        """ Evolve for one epoch. """
        self.totalepochs += 1

        if self.totalepochs - self._max_fitness_epoch >= self.nBurstMutationEpochs:
            if self.verbosity: print("RUNNING BURST MUTATION")
            self.burstMutate()
            self._max_fitness_epoch = self.totalepochs


        for filter in self._filters:
            filter.apply(self._population)

        if self._max_fitness < self.evaluation.max_fitness:
            if self.verbosity: print("GAINED FITNESS: ", self._max_fitness, " -->" , self.evaluation.max_fitness, "\n")
            self._max_fitness = self.evaluation.max_fitness
            self._max_fitness_epoch = self.totalepochs
        else:
            if self.verbosity: print("DIDN'T GAIN FITNESS:", "best =", self._max_fitness, "    current-best = ", self.evaluation.max_fitness, "\n")

    def burstMutate(self):
        self.burstMutation.apply(self._population)


########NEW FILE########
__FILENAME__ = rprop
# $Id$
__author__ = 'Martin Felder'

from scipy import sqrt

from pybrain.supervised.trainers import BackpropTrainer


class RPropMinusTrainer(BackpropTrainer):
    """ Train the parameters of a module according to a supervised dataset (possibly sequential)
        by RProp without weight backtracking (aka RProp-, cf. [Igel&Huesken, Neurocomputing 50, 2003])
        and without ponderation, ie. all training samples have the same weight. """

    def __init__(self, module, etaminus=0.5, etaplus=1.2, deltamin=1.0e-6, deltamax=5.0, delta0=0.1, **kwargs):
        """ Set up training algorithm parameters, and objects associated with the trainer.

            :arg module: the module whose parameters should be trained.
            :key etaminus: factor by which step width is decreased when overstepping (0.5)
            :key etaplus: factor by which step width is increased when following gradient (1.2)
            :key delta: step width for each weight
            :key deltamin: minimum step width (1e-6)
            :key deltamax: maximum step width (5.0)
            :key delta0: initial step width (0.1)
        """
        BackpropTrainer.__init__(self, module, **kwargs)
        self.epoch = 0
        # set descender to RPROP mode and update parameters
        self.descent.rprop = True
        self.descent.etaplus = etaplus
        self.descent.etaminus = etaminus
        self.descent.deltamin = deltamin
        self.descent.deltamax = deltamax
        self.descent.deltanull = delta0
        self.descent.init(module.params)  # reinitialize, since mode changed

    def train(self):
        """ Train the network for one epoch """
        self.module.resetDerivatives()
        errors = 0
        ponderation = 0
        for seq in self.ds._provideSequences():
            e, p = self._calcDerivs(seq)
            errors += e
            ponderation += p
        if self.verbose:
            print("epoch {epoch:6d}  total error {error:12.5g}   avg weight  {weight:12.5g}".format(
                epoch=self.epoch,
                error=errors / ponderation,
                weight=sqrt((self.module.params ** 2).mean())))
        self.module._setParameters(self.descent(self.module.derivs - self.weightdecay * self.module.params))
        self.epoch += 1
        self.totalepochs += 1
        return errors / ponderation



########NEW FILE########
__FILENAME__ = svmtrainer
__author__ = "Martin Felder, felder@in.tum.de"


try:
    from svm import svm_model, svm_parameter, svm_problem, cross_validation #@UnresolvedImport
    from svm import C_SVC, NU_SVC, ONE_CLASS, EPSILON_SVR, NU_SVR #@UnresolvedImport @UnusedImport
    from svm import LINEAR, POLY, RBF, SIGMOID, PRECOMPUTED #@UnresolvedImport @UnusedImport
except ImportError:
    raise ImportError("Cannot find LIBSVM installation. Make sure svm.py and svmc.* are in the PYTHONPATH!")

from numpy import * #@UnusedWildImport
import logging


class SVMTrainer(object):
    """A class performing supervised learning of a DataSet by an SVM unit. See 
    the remarks on :class:`SVMUnit` above. This whole class is a bit of a hack,
    and provided mostly for convenience of comparisons."""
    
    def __init__(self, svmunit, dataset, modelfile=None, plot=False):
        """ Initialize data and unit to be trained, and load the model, if 
        provided.
        
        The passed `svmunit` has to be an object of class :class:`SVMUnit` 
        that is going to be trained on the :class:`ClassificationDataSet` object
        dataset. 
        Compared to FNN training we do not use a test data set, instead 5-fold 
        cross-validation is performed if needed.
        
        If `modelfile` is provided, this model is loaded instead of training.
        If `plot` is True, a grid search is performed and the resulting pattern
        is plotted."""
        self.svm = svmunit
        self.ds = dataset
        self.svmtarget = dataset['target'].flatten()
        self.plot = plot
        self.searchlog = 'gridsearch_results.txt'
        # set default parameters for training
        self.params = {
            'kernel_type':RBF
            }

        if modelfile is not None:  self.load(modelfile)
        
        
    def train(self, search=False, **kwargs):
        """ Train the SVM on the dataset. For RBF kernels (the default), an optional meta-parameter search can be performed.

        :key search: optional name of grid search class to use for RBF kernels: 'GridSearch' or 'GridSearchDOE' 
        :key log2g: base 2 log of the RBF width parameter
        :key log2C: base 2 log of the slack parameter
        :key searchlog: filename into which to dump the search log
        :key others: ...are passed through to the grid search and/or libsvm 
        """
        
        self.setParams(**kwargs)
        problem = svm_problem(self.ds['target'].flatten(), self.ds['input'].tolist())
        if search:
            # this is a bit of a hack...
            model = eval(search + "(problem, self.svmtarget, cmin=[0,-7],cmax=[25,1], cstep=[0.5,0.2],plotflag=self.plot,searchlog=self.searchlog,**self.params)")
        else:
            param = svm_parameter(**self.params)
            model = svm_model(problem, param)
            logging.info("Training completed with parameters:")
            logging.info(repr(param))

        self.svm.setModel(model)
        
        
    def save(self, filename):
        """ save the trained SVM """
        self.svm.saveModel(filename)
        
    
    def load(self, filename):
        """ no training at all - just load the SVM model from a file """
        self.svm.loadModel(filename)
    
    def setParams(self, **kwargs):
        """ Set parameters for SVM training. Apart from the ones below, you can use all parameters 
        defined for the LIBSVM svm_model class, see their documentation.

        :key searchlog: Save a list of coordinates and the achieved CV accuracy to this file."""
        if kwargs.has_key('weight'):
            self.params['nr_weight'] = len(kwargs['weight'])
        if kwargs.has_key('log2C'):
            self.params['C'] = 2 ** kwargs['log2C']
            kwargs.pop('log2C')
        if kwargs.has_key('log2g'):
            self.params['gamma'] = 2 ** kwargs['log2g']
            kwargs.pop('log2g')
        if kwargs.has_key('searchlog'):
            self.searchlog = kwargs['searchlog']
            kwargs.pop('searchlog')
        self.params.update(kwargs)

        
        
class GridSearch(svm_model):
    """Helper class used by :class:`SVMTrainer` to perform an exhaustive grid search, and plot the
    resulting accuracy surface, if desired. Adapted from the LIBSVM python toolkit."""
    
    allPts = []
    allScores = []
    
    def __init__(self, problem, targets, cmin, cmax, cstep=None, crossval=5,
                 plotflag=False, maxdepth=8, searchlog='gridsearch_results.txt', **params):
        """ Set up (log) grid search over the two RBF kernel parameters C and gamma.

        :arg problem: the LIBSVM svm_problem to be optimized, ie. the input and target data
        :arg targets: unfortunately, the targets used in the problem definition have to be given again here
        :arg cmin: lower left corner of the log2C/log2gamma window to search
        :arg cmax: upper right corner of the log2C/log2gamma window to search
        :key cstep: step width for log2C and log2gamma (ignored for DOE search)
        :key crossval: split dataset into this many parts for cross-validation
        :key plotflag: if True, plot the error surface contour (regular) or search pattern (DOE)
        :key maxdepth: maximum window bisection depth (DOE only)
        :key searchlog: Save a list of coordinates and the achieved CV accuracy to this file
        :key others: ...are passed through to the cross_validation method of LIBSVM
        """
        self.nPars = len(cmin)
        self.usermin = cmin
        self.usermax = cmax
        self.userstep = cstep
        self.crossval = crossval
        self.plotflag = plotflag
        self.maxdepth = maxdepth  # number of zoom-in steps (DOE search only!)
        
        # set default parameters for training
        self.params = params
        
        if self.plotflag:
            import pylab as p 
            p.ion()
            p.figure(figsize=[12, 8])
        
        assert isinstance(problem, svm_problem)
        self.problem = problem
        self.targets = targets
        
        self.resfile = open(searchlog, 'w')

        # do the parameter searching
        param = self.search()
        
        if self.plotflag: 
            p.ioff()
            p.show()
        
        self.resfile.close()
        svm_model.__init__(self, problem, param)
        
    def setParams(self, **kwargs):
        """ set parameters for SVM training """
        if kwargs.has_key('weight'):
            self.params['nr_weight'] = len(kwargs['weight'])
        self.params.update(kwargs)
    
    def search(self):
        """ iterate successive parameter grid refinement and evaluation; adapted from LIBSVM grid search tool """
        jobs = self.calculate_jobs()
        scores = []
        for line in jobs:
            for (c, g) in line:
                # run cross-validation for this point
                self.setParams(C=2 ** c, gamma=2 ** g)
                param = svm_parameter(**self.params)
                cvresult = array(cross_validation(self.problem, param, self.crossval))
                corr, = where(cvresult == self.targets)
                res = (c, g, float(corr.size) / self.targets.size)                
                scores.append(res)
                self._save_points(res)
            self._redraw(scores)
        scores = array(scores)
        best = scores[scores[:, 0].argmax(), 1:]
        self.setParams(C=2 ** best[0], gamma=2 ** best[1])
        logging.info("best log2C=%12.7g, log2g=%11.7g " % (best[0], best[1]))
        param = svm_parameter(**self.params)
        return param
    
        
    def _permute_sequence(self, seq):
        """ helper function to create a nice sequence of refined regular grids; from LIBSVM grid search tool """
        n = len(seq)
        if n <= 1: return seq
    
        mid = int(n / 2)
        left = self._permute_sequence(seq[:mid])
        right = self._permute_sequence(seq[mid + 1:])
    
        ret = [seq[mid]]
        while left or right:
            if left: ret.append(left.pop(0))
            if right: ret.append(right.pop(0))
    
        return ret

    def _range_f(self, begin, end, step):
        """ like range, but works on non-integer too; from LIBSVM grid search tool """ 
        seq = []
        while 1:
            if step > 0 and begin > end: break
            if step < 0 and begin < end: break
            seq.append(begin)
            begin = begin + step
        return seq

    def calculate_jobs(self):
        """ like range, but works on non-integer too; from LIBSVM grid search tool """ 
        c_seq = self._permute_sequence(self._range_f(self.usermin[0], self.usermax[0], self.userstep[0]))
        g_seq = self._permute_sequence(self._range_f(self.usermin[1], self.usermax[1], self.userstep[1]))
        nr_c = float(len(c_seq))
        nr_g = float(len(g_seq))
        global total_points
        total_points = (nr_g + 1) * (nr_g)
        i = 0
        j = 0
        jobs = []
    
        while i < nr_c or j < nr_g:
            if i / nr_c < j / nr_g:
                # increase C resolution
                line = []
                for k in range(0, j):
                    line.append((c_seq[i], g_seq[k]))
                i = i + 1
                jobs.append(line)
            else:
                # increase g resolution
                line = []
                for k in range(0, i):
                    line.append((c_seq[k], g_seq[j]))
                j = j + 1
                jobs.append(line)
        return jobs

    def _save_points(self, res):
        """ save the list of points and corresponding scores into a file """
        self.resfile.write("%g, %g, %g\n" % res)
        logging.info("log2C=%g, log2g=%g, res=%g" % res)
        self.resfile.flush()
        
    def _redraw(self, db, tofile=0, eta=None):
        """ redraw the updated grid interactively """
        import pylab as p 
        if len(db) <= 3 or not self.plotflag: return
        #begin_level = round(max(map(lambda(x):x[2],db))) - 3
        #step_size = 0.25
        nContours = 25
        #suffix = ''
        #if eta is not None:
        #    suffix = " (ETA: %5.2f min)" % eta
        def cmp (x, y):
            if x[0] < y[0]: return - 1
            if x[0] > y[0]: return 1
            if x[1] > y[1]: return - 1
            if x[1] < y[1]: return 1
            return 0
        db.sort(cmp)
        dbarr = p.asarray(db)
        # reconstruct grid: array is ordered along first and second dimension
        x = dbarr[:, 0]
        dimy = len(x[x == x[0]])
        dimx = x.size / dimy
        print('plotting: ', dimx, dimy)
        x = x.reshape(dimx, dimy)
        y = dbarr[:, 1]
        y = y.reshape(dimx, dimy)
        z = dbarr[:, 2].reshape(dimx, dimy)
    
        # plot using manual double buffer
        p.ioff()
        p.clf()
        p.contourf(x, y, z, nContours)
        p.hsv()
        p.colorbar()
        p.xlim(self.usermin[0], self.usermax[0])
        p.ylim(self.usermin[1], self.usermax[1])
        p.xlabel(r'$\rm{log}_2(C)$')
        p.ylabel(r'$\rm{log}_2(\gamma)$')
        p.ion()
        p.draw_if_interactive()

        

class GridSearchDOE(GridSearch):
    """ Same as GridSearch, but implements a design-of-experiments based search pattern, as
    described by C. Staelin, http://www.hpl.hp.com/techreports/2002/HPL-2002-354R1.pdf """

    # DOE pattern; the last 5 points do not need to be calculated when refining the grid 
    doepat = array([[0.5, 1], [0.25, 0.75], [0.75, 0.75], [0, 0.5], [1, 0.5], \
              [0.25, 0.25], [0.75, 0.25], [0.5, 0], [0, 1], [1, 1], [0.5, 0.5], [0, 0], [1, 0]])
    nPts = 13
    depth = 0
    
    def search(self, cmin=None, cmax=None):
        """ iterate parameter grid refinement and evaluation recursively """
        if self.depth > self.maxdepth:
            # maximum search depth reached - finish up
            best = self.allPts[self.allScores.argmax(), :]
            logging.info("best log2C=%12.7g, log2g=%11.7g " % (best[0], best[1]))
            self.setParams(C=2 ** best[0], gamma=2 ** best[1])
            param = svm_parameter(**self.params)
            logging.info("Grid search completed! Final parameters:")
            logging.info(repr(param))
            return param
        
        # generate DOE gridpoints using current range
        if cmin is None:
            # use initial values, if none given
            cmin = array(self.usermin)
            cmax = array(self.usermax)
        points = self.refineGrid(cmin, cmax)
        
        # calculate scores for all grid points using n-fold cross-validation
        scores = []
        isnew = array([True] * self.nPts)
        for i in range(self.nPts):
            idx = self._findIndex(points[i, :])
            if idx >= 0:
                # point already exists
                isnew[i] = False
                scores.append(self.allScores[idx])
            else: 
                # new point, run cross-validation
                self.setParams(C=2 ** points[i, 0], gamma=2 ** points[i, 1])
                param = svm_parameter(**self.params)
                cvresult = array(cross_validation(self.problem, param, self.crossval))
                # save cross validation result as "% correct" 
                corr, = where(cvresult == self.targets)
                corr = float(corr.size) / self.targets.size
                scores.append(corr)
                self._save_points((points[i, 0], points[i, 1], corr))

        scores = array(scores)
        
        # find max and new ranges by halving the old ones, whereby
        # entire search region must lie within original search range
        newctr = points[scores.argmax(), :].copy()
        newdiff = (cmax - cmin) / 4.0
        for i in range(self.nPars):
            newctr[i] = min([max([newctr[i], self.usermin[i] + newdiff[i]]), self.usermax[i] - newdiff[i]])
        cmin = newctr - newdiff
        cmax = newctr + newdiff
        logging.info("depth:\t%3d\tcrange:\t%g\tscore:\t%g" % (self.depth, cmax[0] - cmin[0], scores.max()))
        
        # append points and scores to the full list
        if self.depth == 0:
            self.allPts = points[isnew, :].copy()
            self.allScores = scores[isnew].copy()
        else:
            self.allPts = append(self.allPts, points[isnew, :], axis=0)
            self.allScores = append(self.allScores, scores[isnew], axis=0)
        
        if self.plotflag:
            import pylab as p 
            if self.depth == 0:
                self.oPlot = p.plot(self.allPts[:, 0], self.allPts[:, 1], 'o')[0]
            # insert new data into plot
            self.oPlot.set_data(self.allPts[:, 0], self.allPts[:, 1])
            p.draw()
        
        # recursively call ourselves
        self.depth += 1    
        return self.search(cmin, cmax)
    
    
    def refineGrid(self, cmin, cmax):
        """ given grid boundaries, generate the corresponding DOE pattern from template"""
        diff = array((cmax - cmin).tolist()*self.nPts).reshape(self.nPts, self.nPars)
        return self.doepat * diff + array(cmin.tolist()*self.nPts).reshape(self.nPts, self.nPars)
    
    def _findIndex(self, point):
        """ determines whether given point already exists in list of all calculated points.
        raises exception if more than one point is found, returns -1 if no point is found """
        if self.depth == 0: return - 1
        check = self.allPts[:, 0] == point[0]
        for i in range(1, point.size):
            check = check & (self.allPts[:, i] == point[i])
        idx, = where(check)
        if idx.size == 0:
            return - 1
        elif idx.size > 1:
            logging.error("Something went wrong - found more than one matching point!")
            logging.error(str(point))
            logging.error(str(self.allPts))
            raise
        else:
            return idx[0]



########NEW FILE########
__FILENAME__ = trainer
__author__ = 'Tom Schaul, tom@idsia.ch'
__version__ = '$Id$'

from pybrain.utilities import Named, abstractMethod


class Trainer(Named):
    """ A trainer determines how to change the adaptive parameters of a module.
    It requires access to a DataSet object (which provides input-target tuples). """
    # e.g. bptt, rtrl, svm

    ds = None
    module = None

    def __init__(self, module):
        self.module = module

    def setData(self, dataset):
        """Associate the given dataset with the trainer."""
        self.ds = dataset
        if dataset:
            assert dataset.indim == self.module.indim
            assert dataset.outdim == self.module.outdim

    def trainOnDataset(self, dataset, *args, **kwargs):
        """Set the dataset and train.

        Additional arguments are passed on to the train method."""
        self.setData(dataset)
        self.trainEpochs(*args, **kwargs)

    def trainEpochs(self, epochs=1, *args, **kwargs):
        """Train on the current dataset for the given number of `epochs`.

        Additional arguments are passed on to the train method."""
        for dummy in range(epochs):
            self.train(*args, **kwargs)

    def train(self):
        """Train on the current dataset, for a single epoch."""
        abstractMethod()



########NEW FILE########
__FILENAME__ = auxiliary
# -*- coding: utf-8 -*-

__author__ = 'Justin Bayer, bayer.justin@googlemail.com'
__version__ = '$Id$'


""""This module is a place to hold functionality that _has_ to be outside of a
test module but is required by it."""

########NEW FILE########
__FILENAME__ = helpers
__author__ = 'Tom Schaul, tom@idsia.ch'

from os import unlink, getcwd
import os.path
import profile
import pstats
import tempfile

from scipy import randn, zeros

from pybrain.structure.networks.network import Network
from pybrain.datasets import SequentialDataSet, SupervisedDataSet
from pybrain.supervised import BackpropTrainer
from pybrain.tools.customxml import NetworkWriter, NetworkReader



def epsilonCheck(x, epsilon=1e-6):
    """Checks that x is in (-epsilon, epsilon)."""
    epsilon = abs(epsilon)
    return -epsilon < x < epsilon


def buildAppropriateDataset(module):
    """ build a sequential dataset with 2 sequences of 3 samples, with arndom input and target values,
    but the appropriate dimensions to be used on the provided module. """
    if module.sequential:
        d = SequentialDataSet(module.indim, module.outdim)
        for dummy in range(2):
            d.newSequence()
            for dummy in range(3):
                d.addSample(randn(module.indim), randn(module.outdim))
    else:
        d = SupervisedDataSet(module.indim, module.outdim)
        for dummy in range(3):
            d.addSample(randn(module.indim), randn(module.outdim))
    return d


def gradientCheck(module, tolerance=0.0001, dataset=None):
    """ check the gradient of a module with a randomly generated dataset,
    (and, in the case of a network, determine which modules contain incorrect derivatives). """
    if module.paramdim == 0:
        print('Module has no parameters')
        return True
    if dataset:
        d = dataset
    else:
        d = buildAppropriateDataset(module)
    b = BackpropTrainer(module)
    res = b._checkGradient(d, True)
    # compute average precision on every parameter
    precision = zeros(module.paramdim)
    for seqres in res:
        for i, p in enumerate(seqres):
            if p[0] == 0 and p[1] == 0:
                precision[i] = 0
            else:
                precision[i] += abs((p[0] + p[1]) / (p[0] - p[1]))
    precision /= len(res)
    if max(precision) < tolerance:
        print('Perfect gradient')
        return True
    else:
        print('Incorrect gradient', precision)
        if isinstance(module, Network):
            index = 0
            for m in module._containerIterator():
                if max(precision[index:index + m.paramdim]) > tolerance:
                    print('Incorrect module:', m, res[-1][index:index + m.paramdim])
                index += m.paramdim
        else:
            print(res)
        return False


def netCompare(net1, net2, forwardpasses=1, verbose=False):
    identical = True
    if str(net2) == str(net1):
        if verbose:
            print('Same representation')
    else:
        identical = False
        if verbose:
            print(net2)
            print('-' * 80)
            print(net1)

    outN = zeros(net2.outdim)
    outEnd = zeros(net1.outdim)
    net2.reset()
    net1.reset()
    for dummy in range(forwardpasses):
        inp = randn(net2.indim)
        outN += net2.activate(inp)
        outEnd += net1.activate(inp)

    if sum(map(abs, outN - outEnd)) < 1e-9:
        if verbose:
            print('Same function')
    else:
        identical = False
        if verbose:
            print(outN)
            print(outEnd)

    if net2.__class__ == net1.__class__:
        if verbose:
            print('Same class')
    else:
        identical = False
        if verbose:
            print(net2.__class__)
            print(net1.__class__)

    return identical


def xmlInvariance(n, forwardpasses = 1):
    """ try writing a network to an xml file, reading it, rewrite it, reread it, and compare
    if the result looks the same (compare string representation, and forward processing
    of some random inputs) """
    # We only use this for file creation.
    tmpfile = tempfile.NamedTemporaryFile(dir='.')
    f = tmpfile.name
    tmpfile.close()

    NetworkWriter.writeToFile(n, f)
    tmpnet = NetworkReader.readFrom(f)
    NetworkWriter.writeToFile(tmpnet, f)
    endnet = NetworkReader.readFrom(f)

    # Unlink temporary file.
    os.unlink(f)

    netCompare(tmpnet, endnet, forwardpasses, True)




def sortedProfiling(code, maxfunctions=50):
    f = 'temp/profilingInfo.tmp'
    if os.path.split(os.path.abspath(''))[1] != 'tests':
        f = '../' + f
    profile.run(code, f)
    p = pstats.Stats(f)
    p.sort_stats('time').print_stats(maxfunctions)

########NEW FILE########
__FILENAME__ = optimizationtest
#! /usr/bin/env python
""" This test script will test the set of optimization algorithms.

It tests
 - the conformity of interface
 - the behavior on simple functions
 - the behavior on FitnessEvaluators
 - the behavior when optimizing a list or an array
 - the behavior when optimizing an Evolvable
 - the behavior when optimizing a ParameterContainer
 - consistency w.r.t. minimization/maximization

Tests to be added:
 - tolerance of problems that have a constant fitness
 - tolerance of problems that have adversarial (strictly decreasing) fitness
 - handling one-dimensional and high-dimensional spaces
 - reasonable results on the linear function
"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from inspect import isclass
from scipy import sum, array, ndarray, log10
from random import random, choice

import pybrain.optimization.optimizer as bbo
import pybrain.optimization.populationbased.multiobjective as mobj
import pybrain.optimization as allopts

from pybrain.rl.environments.functions.unimodal import SphereFunction
from pybrain.structure.parametercontainer import ParameterContainer
from pybrain.structure.evolvables.evolvable import Evolvable
from pybrain.rl.environments.cartpole.balancetask import BalanceTask
from pybrain.tools.shortcuts import buildNetwork
from pybrain.structure.modules.module import Module


# Tasks to be optimized:
# ----------------------

# simple function
sf = lambda x:-sum((x + 1) ** 2)
# FunctionEnvironment class
fe = SphereFunction
# initialized FE
ife1 = fe(1)
ife2 = fe(2)
ife100 = fe(100)
# a Task object
task = BalanceTask()
task.N = 10
# for the simple evolvable class defined below
evoEval = lambda e: e.x


# starting points
# ----------------------
xlist1 = [2.]
xlist2 = [0.2, 10]
xlist100 = list(range(12, 112))

xa1 = array(xlist1)
xa2 = array(xlist2)
xa100 = array(xlist100)

pc1 = ParameterContainer(1)
pc2 = ParameterContainer(2)
pc100 = ParameterContainer(100)
pc1._setParameters(xa1)
pc2._setParameters(xa2)
pc100._setParameters(xa100)

# for the task object, we need a module
nnet = buildNetwork(task.outdim, 2, task.indim)

# a mimimalistic Evolvable subclass that is not (like usual) a ParameterContainer
class SimpleEvo(Evolvable):
    def __init__(self, x): self.x = x
    def mutate(self):      self.x += random() - 0.3
    def copy(self):        return SimpleEvo(self.x)
    def randomize(self):   self.x = 10 * random() - 2
    def __repr__(self):     return '--%.3f--' % self.x

evo1 = SimpleEvo(-3.)


# the test functions
# ----------------------

def testInterface(algo):
    """ Tests whether the algorithm is properly implementing the
    correct Blackbox-optimization interface."""
    # without any arguments, initialization has to work
    emptyalgo = algo()
    try:
        # but not learning
        emptyalgo.learn(0)
        return "Failed to throw missing evaluator error?"
    except AssertionError:
        pass

    emptyalgo.setEvaluator(sf, xa1)
    # not it can run
    emptyalgo.learn(0)

    # simple functions don't check for dimension mismatch
    algo(sf, xa1)
    algo(sf, xa100)

    # for these, either an initial point or a dimension parameter is required
    algo(sf, numParameters=2)

    try:
        algo(sf)
        return "Failed to throw unknown dimension error"
    except ValueError:
        pass

    # FitnessEvaluators do not require that
    algo(ife1)

    # parameter containers can be used too
    algo(ife2, pc2)

    return True


def testContinuousInterface(algo):
    """ Test the specifics for the interface for ContinuousOptimizers """
    if not issubclass(algo, bbo.ContinuousOptimizer):
        return True
    # list starting points are internally converted to arrays
    x = algo(sf, xlist2)
    assert isinstance(x.bestEvaluable, ndarray), 'not converted to array'

    # check for dimension mismatch
    try:
        algo(ife1, xa2)
        return "Failed to throw dimension mismatch error"
    except ValueError:
        pass

    return True


def testMinMax(algo):
    """ Verify that the algorithm is doing the minimization/maximization consistently. """
    if (issubclass(algo, bbo.TopologyOptimizer)
        or algo == allopts.StochasticHillClimber):
        # TODO
        return True

    xa1[0] = 2
    evalx = sf(xa1)

    amax1 = algo(sf, xa1, minimize=False)
    amax2 = algo(sf, xa1)
    amax2.minimize = False
    amax3 = algo()
    amax3.setEvaluator(sf, xa1)
    amax3.minimize = False
    amax4 = algo()
    amax4.minimize = False
    amax4.setEvaluator(sf, xa1)
    for i, amax in enumerate([amax1, amax2, amax3, amax4]):
        assert amax.minimize is False or amax.mustMinimize, 'Max: Attribute not set correctly.' \
                                            + str(amax.minimize) + str(amax.mustMinimize) + str(i)
        x, xv = amax.learn(1)
        assert sf(x) == xv, 'Evaluation does not fit: ' + str((sf(x), xv))
        assert xv >= evalx, 'Evaluation did not increase: ' + str(xv) + ' (init: ' + str(evalx) + ')'

    xa1[0] = 2
    amin1 = algo(sf, xa1, minimize=True)
    amin2 = algo(sf, xa1)
    amin2.minimize = True
    amin3 = algo()
    amin3.setEvaluator(sf, xa1)
    amin3.minimize = True
    amin4 = algo()
    amin4.minimize = True
    amin4.setEvaluator(sf, xa1)
    for i, amin in enumerate([amin1, amin2, amin3, amin4]):
        assert amin.minimize is True or amin.mustMaximize, 'Min: Attribute not set correctly.' \
                                            + str(amin.minimize) + str(amin.mustMaximize) + str(i)
        x, xv = amin.learn(1)
        assert sf(x) == xv, 'Evaluation does not fit: ' + str((sf(x), xv)) + str(i)
        assert xv <= evalx, 'Evaluation did not decrease: ' + str(xv) + ' (init: ' + str(evalx) + ')' + str(i)
        assert ((amin.minimize is not amax.minimize)
                or not (amin._wasOpposed is amax._wasOpposed)), 'Inconsistent flags.'

    return True




def testOnModuleAndTask(algo):
    l = algo(task, nnet)
    assert isinstance(l._bestFound()[0], Module), 'Did not return a module.'
    return True


def testOnEvolvable(algo):
    if issubclass(algo, bbo.ContinuousOptimizer):
        return True
    if issubclass(algo, bbo.TopologyOptimizer):
        try:
            algo(evoEval, evo1).learn(1)
            return "Topology optimizers should not accept arbitrary Evolvables"
        except AttributeError:
            return True
    else:
        algo(evoEval, evo1).learn(1)
        return True



# the main test procedure
# ------------------------

def testAll(tests, allalgos, tolerant=True):
    countgood = 0
    for i, algo in enumerate(sorted(allalgos)):
        print("%d, %s:" % (i + 1, algo.__name__))
        print(' ' * int(log10(i + 1) + 2),)
        good = True
        messages = []
        for t in tests:
            try:
                res = t(algo)
            except Exception, e:
                if not tolerant:
                    raise e
                res = e

            if res is True:
                print('.',)
            else:
                good = False
                messages.append(res)
                print('F',)
        if good:
            countgood += 1
            print('--- OK.')
        else:
            print('--- NOT OK.')
            for m in messages:
                if m is not None:
                    print(' ' * int(log10(i + 1) + 2), '->', m)
    print
    print('Summary:', countgood, '/', len(allalgos), 'of test were passed.')



if __name__ == '__main__':
    from pybrain.optimization import *  #@UnusedWildImport
    #from pybrain.optimization import CMAES #@UnusedImport
    allalgos = filter(lambda c: (isclass(c)
                                 and issubclass(c, bbo.BlackBoxOptimizer)
                                 and not issubclass(c, mobj.MultiObjectiveGA)
                                 ),
                      globals().values())

    print('Optimization algorithms to be tested:', len(allalgos))
    print
    print('Note: this collection of tests may take quite some time.')
    print

    tests = [testInterface,
             testContinuousInterface,
             testOnModuleAndTask,
             testOnEvolvable,
             testMinMax,
             ]

    testAll(tests, allalgos, tolerant=True)


########NEW FILE########
__FILENAME__ = runtests
#! /usr/bin/env python
# -*- coding: utf-8 -*-

"""Script to run the pybrain testsuite."""

__author__ = 'Justin Bayer, bayerj@in.tum.de'
__version__ = '$Id$'


import doctest
import logging
import os
import sys
from copy import copy

from unittest import TestLoader, TestSuite, TextTestRunner


def setUpLogging():
    logging.basicConfig(level=logging.INFO,
                        format='%(levelname)s %(message)s')


def testImport(module_name):
    """Tell wether a module can be imported.

    This function has a cache, so modules are only tested once on
    importability.
    """
    try:
        return testImport.cache[module_name]
    except KeyError:
        try:
            __import__(module_name)
        except ImportError:
            result = False
        else:
            result = True
    testImport.cache[module_name] = result
    return result
testImport.cache = {}   # Import checks are expensive, so cache results


def missingDependencies(target_module):
    """Returns a list of dependencies of the module that the current
    interpreter cannot import.

    This does not inspect the code, but instead check for a list of strings
    called _dependencies in the target_module. This list should contain module
    names that the module depends on."""
    dependencies = getattr(target_module, '_dependencies', [])
    return [i for i in dependencies if not testImport(i)]

def getSubDirectories(testdir):
    """Recursively builds a list of all subdirectories in the test suite."""
    subdirs = [os.path.join(testdir,d) for d in
               filter(os.path.isdir,[os.path.join(testdir,dd) for dd in os.listdir(testdir)])]
    
    for d in copy(subdirs):
        subdirs.extend(getSubDirectories(os.path.join(testdir,d)))
    
    return subdirs

def make_test_suite():
    """Load unittests placed in pybrain/tests/unittests, then return a
    TestSuite object of those."""
    # [...]/pybrain/pybrain [cut] /tests/runtests.py
    path = os.path.abspath(__file__).rsplit(os.sep+'tests', 1)[0]

    sys.path.append(path.rstrip('pybrain'))

    top_testdir = os.path.join(path, 'tests', 'unittests')
    testdirs = getSubDirectories(top_testdir)
    
    # Initialize the testsuite to add to
    suite = TestSuite()
    optionflags = doctest.ELLIPSIS | doctest.NORMALIZE_WHITESPACE
    
    for testdir in testdirs:
        # All unittest modules have to start with 'test_' and have to be, of
        # course, python files
        module_names = [f[:-3] for f in os.listdir(testdir)
                        if f.startswith('test_') and f.endswith('.py')]
        
        if not module_names:
            logging.info('No tests found in %s' % testdir)
            continue
        
        # "Magically" import the tests package and its test-modules that we've
        # found
        test_package_path = 'pybrain.tests.unittests'
        sub_path = os.path.relpath(testdir, top_testdir).split(os.sep)
        test_package_path = '.'.join([test_package_path]+sub_path)
        test_package = __import__(test_package_path, fromlist=module_names)
    
        # Put the test modules in a list that can be passed to the testsuite
        modules = (getattr(test_package, n) for n in module_names)
        modules = [(m, missingDependencies(m)) for m in modules]
        untests = [(m, md) for m, md in modules if md]
        modules = [m for m, md in modules if not md]
    
        # print(out modules that are missing dependencies)
        for module, miss_dep in untests:    # Mr Dep is not around, though
            logging.warning('Module %s is missing dependencies: %s' % (
                            module.__name__, ', '.join(miss_dep)))
    
        # print(out a list of tests that are found)
        for m in modules:
            logging.info('Tests found: %s' % m.__name__)
    
        # Build up the testsuite
        suite.addTests([TestLoader().loadTestsFromModule(m) for m in modules])

        # Add doctests from the unittest modules to the suite
        for mod in modules:
            try:
                suite.addTest(doctest.DocTestSuite(mod, optionflags=optionflags))
            except ValueError:
                # No tests found.
                pass

    return suite


if __name__ == '__main__':
    setUpLogging()
    runner = TextTestRunner()
    runner.run(make_test_suite())

########NEW FILE########
__FILENAME__ = testsuites
"""Module that contains several utilities for testing."""

__author__ = 'Justin Bayer, bayer.justin@googlemail.com'
__version__ = '$Id$'


from doctest import DocTestSuite, ELLIPSIS, REPORT_ONLY_FIRST_FAILURE, \
NORMALIZE_WHITESPACE
from unittest import TestSuite, TestLoader, TextTestRunner


def runModuleTestSuite(module):
    """Runs a test suite for all local tests."""
    suite = TestSuite([TestLoader().loadTestsFromModule(module)])

    # Add local doctests
    optionflags = ELLIPSIS | NORMALIZE_WHITESPACE | REPORT_ONLY_FIRST_FAILURE

    try:
        suite.addTest(DocTestSuite(module, optionflags=optionflags))
    except ValueError:
        # No tests have been found in that module.
        pass

    TextTestRunner().run(suite)


########NEW FILE########
__FILENAME__ = test_pca
"""

    >>> from scipy import array, matrix

    >>> from pybrain.auxiliary.pca import makeCentered

    >>> data = array([[2.5, 2.4],
    ...               [0.5, 0.7],
    ...               [2.2, 2.9],
    ...               [1.9, 2.2],
    ...               [3.1, 3.0],
    ...               [2.3, 2.7],
    ...               [2.0, 1.6],
    ...               [1.0, 1.1],
    ...               [1.5, 1.6],
    ...               [1.1, 0.9]])

    >>> makeCentered(data)
    array([[ 0.69,  0.49],
           [-1.31, -1.21],
           [ 0.39,  0.99],
           [ 0.09,  0.29],
           [ 1.29,  1.09],
           [ 0.49,  0.79],
           [ 0.19, -0.31],
           [-0.81, -0.81],
           [-0.31, -0.31],
           [-0.71, -1.01]])


Tests for regular PCA
---------------------

    >>> from pybrain.auxiliary.pca import pca, reduceDim

    >>> pca(data, 1)
    array([[-0.6778734 , -0.73517866]])

    >>> reduceDim(data, 1)
    matrix([[-0.82797019],
            [ 1.77758033],
            [-0.99219749],
            [-0.27421042],
            [-1.67580142],
            [-0.9129491 ],
            [ 0.09910944],
            [ 1.14457216],
            [ 0.43804614],
            [ 1.22382056]])

    >>> reduceDim(data, 2)
    matrix([[-0.82797019, -0.17511531],
            [ 1.77758033,  0.14285723],
            [-0.99219749,  0.38437499],
            [-0.27421042,  0.13041721],
            [-1.67580142, -0.20949846],
            [-0.9129491 ,  0.17528244],
            [ 0.09910944, -0.3498247 ],
            [ 1.14457216,  0.04641726],
            [ 0.43804614,  0.01776463],
            [ 1.22382056, -0.16267529]])

    >>> data2 = matrix([
    ... [2.4, 2.5],
    ... [0.7, 0.5],
    ... [2.9, 2.2],
    ... [2.2, 1.9],
    ... [3.0, 3.1],
    ... [2.7, 2.3],
    ... [1.6, 2.0],
    ... [1.1, 1.0],
    ... [1.6, 1.5],
    ... [0.9, 1.1]])

    >>> reduceDim(data2, 2)
    matrix([[ 0.17511531,  0.82797019],
            [-0.14285723, -1.77758033],
            [-0.38437499,  0.99219749],
            [-0.13041721,  0.27421042],
            [ 0.20949846,  1.67580142],
            [-0.17528244,  0.9129491 ],
            [ 0.3498247 , -0.09910944],
            [-0.04641726, -1.14457216],
            [-0.01776463, -0.43804614],
            [ 0.16267529, -1.22382056]])

    >>> data3 = matrix([
    ... [7.0, 4.0, 3.0],
    ... [4.0, 1.0, 8.0],
    ... [6.0, 3.0, 5.0],
    ... [8.0, 6.0, 1.0],
    ... [8.0, 5.0, 7.0],
    ... [7.0, 2.0, 9.0],
    ... [5.0, 3.0, 3.0],
    ... [9.0, 5.0, 8.0],
    ... [7.0, 4.0, 5.0],
    ... [8.0, 2.0, 2.0]])

    >>> reduceDim(data3, 1)
    matrix([[-2.15142276],
            [ 3.80418259],
            [ 0.15321328],
            [-4.7065185 ],
            [ 1.29375788],
            [ 4.0993133 ],
            [-1.62582148],
            [ 2.11448986],
            [-0.2348172 ],
            [-2.74637697]])

Tests for probabilistic PCA
---------------------------

    >>> from pybrain.auxiliary.pca import pPca

    >>> pc = pPca(data, 1)
    >>> x, y = pc[0, 0], pc[0, 1]
    >>> x / y
    0.92...

"""


__author__ = 'Justin Bayer, bayerj@in.tum.de'

from pybrain.tests import runModuleTestSuite

if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))


########NEW FILE########
__FILENAME__ = test_datasets_datasets
# -*- coding: utf-8 -*-

"""
    >>> from scipy import array
    >>> from pybrain import datasets
    >>> from copy import deepcopy
    >>> d = datasets.dataset.DataSet()
    >>> d.addField('input', 2)
    >>> type(d.data['input'])
	<type 'numpy.ndarray'>
	
    >>> len(d.data['input'])
    0
	
    >>> x, y = d.data['input'].shape
	>>> str(x)
	0
	>>> str(y)
    2
	
Build up a DataSet for testing:

    >>> d.append('input', (array((0, 0))))
    >>> d.append('input', (array((1, 1))))
    >>> d.append('input', (array((2, 2))))
    >>> d.append('input', (array((3, 3))))
    >>> d.append('input', (array((4, 4))))
    >>> d.append('input', (array((5, 5))))
    >>> d.append('input', (array((6, 6))))
    >>> d.append('input', (array((7, 7))))

    >>> list(d.batches('input', 3))
    [array([[ 0.,  0.],
               [ 1.,  1.],
               [ 2.,  2.]]), array([[ 3.,  3.],
               [ 4.,  4.],
               [ 5.,  5.]]), array([[ 6.,  6.],
               [ 7.,  7.]])]

    >>> list(d.batches('input', 2))
    [array([[ 0.,  0.],
               [ 1.,  1.]]), array([[ 2.,  2.],
               [ 3.,  3.]]), array([[ 4.,  4.],
               [ 5.,  5.]]), array([[ 6.,  6.],
               [ 7.,  7.]])]

    >>> p = reversed(range(4))
    >>> print('\\n'.join(repr(b) for b in d.batches('input', 2, p)))
    array([[ 6.,  6.],
           [ 7.,  7.]])
    array([[ 4.,  4.],
           [ 5.,  5.]])
    array([[ 2.,  2.],
           [ 3.,  3.]])
    array([[ 0.,  0.],
           [ 1.,  1.]])


Serialization
=============

    >>> from cStringIO import StringIO


UnsupervisedDataSet
-----------------

    >>> d = datasets.UnsupervisedDataSet(2)
    >>> d.addSample([0,0])
    >>> d.addSample([0,1])
    >>> d.addSample([1,0])
    >>> d.addSample([1,1])
    >>> for sample in d:
    ...   print(sample)
    ...
    [array([ 0.,  0.])]
    [array([ 0.,  1.])]
    [array([ 1.,  0.])]
    [array([ 1.,  1.])]






ClassificationDataSet
---------------------

    >>> class_labels = 'Urd', 'Verdandi', 'Skuld'
    >>> d = datasets.ClassificationDataSet(2,1, class_labels=class_labels)
    >>> d.appendLinked( [ 0.1, 0.5 ]   , [0] )
    >>> d.appendLinked( [ 1.2, 1.2 ]   , [1] )
    >>> d.appendLinked( [ 1.4, 1.6 ]   , [1] )
    >>> d.appendLinked( [ 1.6, 1.8 ]   , [1] )
    >>> d.appendLinked( [ 0.10, 0.80 ] , [2] )
    >>> d.appendLinked( [ 0.20, 0.90 ] , [2] )

    >>> saveInvariant(d)
    True


ImportanceDataSet
-----------------


SequentialDataSet
-----------------

      >>> d = datasets.SequentialDataSet(0, 1)
      >>> d.addSample([],[0])
      >>> d.addSample([],[1])
      >>> d.addSample([],[0])
      >>> d.addSample([],[1])
      >>> d.addSample([],[0])
      >>> d.addSample([],[1])
      >>> d.newSequence()
      >>> d.addSample([],[0])
      >>> d.addSample([],[1])
      >>> d.addSample([],[0])
      >>> d.addSample([],[1])
      >>> d.addSample([],[0])
      >>> d.addSample([],[1])

      >>> saveInvariant(d)
      True


ReinforcementDataSet
--------------------

    >>> d = datasets.ReinforcementDataSet(1, 1)
    >>> d.addSample([1,], [1,], [1,])
    >>> d.addSample([1,], [1,], [1,])
    >>> d.addSample([1,], [1,], [1,])
    >>> saveInvariant(d)
    True



"""


__author__ = 'Justin Bayer, bayerj@in.tum.de'


from cStringIO import StringIO

from pybrain.tests import runModuleTestSuite


def saveInvariant(dataset):
    # Save and reconstruct
    s = StringIO()
    dataset.saveToFileLike(s)
    s.seek(0)
    reconstructed = dataset.__class__.loadFromFileLike(s)

    orig_array_data = sorted(dataset.data.items())
    rec_array_data = sorted(reconstructed.data.items())
    equal = True
    for (k, v), (k_, v_) in zip(orig_array_data, rec_array_data):
        if k != k_:
            print("Differing keys: %s <=> %s" % (dataset.dataset.keys(),
                                                 rec_array_data.dataset.keys()))
            equal = False
            break
        if not (v == v_).all():
            print("Differing values for %s" % k)
            print(v)
            print(v_)
            equal = False
            break

    if not equal:
        return False

    rec_dict = reconstructed.__dict__
    orig_dict = dataset.__dict__

    del rec_dict['_convert']
    del orig_dict['_convert']
    del rec_dict['data']
    del orig_dict['data']

    if rec_dict == orig_dict:
        return True
    else:
        print(rec_dict)
        print(orig_dict)
        return False


if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))

########NEW FILE########
__FILENAME__ = test_pso_ring
"""


    >>> from pybrain.optimization.populationbased.pso import ring
    >>> ring(range(9))
    {0: (1, 8), 1: (2, 0), 2: (3, 1), 3: (4, 2), 4: (5, 3), 5: (6, 4), 6: (7, 5), 7: (8, 6), 8: (0, 7)}

    Disabled:
    lattice(range(9))
    {0: (1, 2),
     1: (0, 3),
     2: (0, 3),
     3: (1, 2)
     4:
     5:
     6:
     7:
     8:
     9:}

"""

__author__ = ('Justin Bayer, bayer.justin@googlemail.com;'
              'Julian Togelius, julian@idsia.ch')

from pybrain.tests import runModuleTestSuite

if __name__ == '__main__':
    runModuleTestSuite(__import__('__main__'))



########NEW FILE########
__FILENAME__ = test_capture_game
"""

Initialize a capturegame
    >>> from pybrain.rl.environments.twoplayergames import CaptureGame
    >>> c = CaptureGame(5)
    >>> print(c)
     . . . . .
     . . . . .
     . . . . .
     . . . . .
     . . . . .


Do some moves to produce a situation
    >>> c.performAction([1, (1,0)])
    >>> c.performAction([1, (0,1)])
    >>> c.performAction([1, (1,1)])
    >>> c.performAction([-1, (2,0)])
    >>> c.performAction([-1, (0,2)])
    >>> c.performAction([-1, (1,2)])
    >>> c.performAction([-1, (2,1)])

Now it is almost decided, white has a killing move!
    >>> c.getKilling(-1)
    [(0, 0)]

    >>> c.getWinner()

Do it!
    >>> c.performAction([-1, (0,0)])

White wins.
    >>> c.getWinner()
    -1

Check if all the values are right:
    >>> print(c)
     x X O . .
     X X O . .
     O O . . .
     . . . . .
     . . . . .
    Winner: White (*) (moves done:8)


    >>> c.groups
    {(0, 1): 5, (1, 2): 2, (2, 1): 10, (0, 2): 2, (2, 0): 10, (1, 0): 5, (1, 1): 5}

    >>> c.liberties
    {2: set([(0, 3), (1, 3), (2, 2)]), 5: set([(0, 0)]), 10: set([(3, 0), (3, 1), (2, 2)])}

"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.tests import runModuleTestSuite

if __name__ == '__main__':
    runModuleTestSuite(__import__('__main__'))

########NEW FILE########
__FILENAME__ = test_pente
"""

Initialize a game of Pente.

    >>> from pybrain.rl.environments.twoplayergames.pente import PenteGame
    >>> dim = 5
    >>> c = PenteGame((dim, dim))
    >>> print(c)
     _ _ _ _ _
     _ _ _ _ _
     _ _ * _ _
     _ _ _ _ _
     _ _ _ _ _
    Black captured:0, white captured:0.


Do some moves to produce a situation
    >>> c.performAction([1, (0,1)])
    >>> c.performAction([-1, (1,0)])
    >>> c.performAction([1, (1,1)])
    >>> c.performAction([-1, (1,2)])
    >>> c.performAction([1, (2,0)])
    >>> c.performAction([-1, (2,1)])
    >>> c.performAction([1, (0,2)])

Show the updated board:
    >>> print(c)
     _ # # _ _
     * # * _ _
     # * * _ _
     _ _ _ _ _
     _ _ _ _ _
    Black captured:0, white captured:0.


Do some captures:
    >>> c.performAction([-1, (0,3)])
    >>> c.performAction([1, (3,2)])
    >>> c.performAction([-1, (0,0)])

Stepping between black stones is not deadly though:
    >>> c.performAction([1, (2,3)])
    >>> c.performAction([-1, (2,2)])
    >>> print(c)
     * _ _ * _
     * # _ _ _
     # * * # _
     _ _ # _ _
     _ _ _ _ _
    Black captured:1, white captured:1.

Fast forward to the end:
    >>> c.performAction([-1, (0,4)])
    >>> c.performAction([-1, (3,1)])
    >>> c.performAction([-1, (4,0)])

Now it is almost decided, white has a killing move!
    >>> c.getKilling(-1)
    [(1, 3)]

    >>> c.getWinner()

Do it!
    >>> c.performAction([-1, (1,3)])

White wins.
    >>> c.getWinner()
    -1

Check if all the values are right:
    >>> print(c)
     * _ _ * *
     * # _ x _
     # * * # _
     _ * # _ _
     * _ _ _ _
    Winner: White (*) (moves done:17)
    Black captured:1, white captured:1.


"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.tests import runModuleTestSuite

if __name__ == '__main__':
    runModuleTestSuite(__import__('__main__'))



########NEW FILE########
__FILENAME__ = test_shared_connections
"""
Trying to build a network with shared connections:

    >>> from random import random
    >>> n = buildSharedCrossedNetwork()

Check if the parameters are the same:

    >>> (n.connections[n['a']][0].params == n.connections[n['a']][1].params).all()
    True

    >>> (n.connections[n['b']][0].params == n.connections[n['c']][0].params).all()
    True

    >>> from pybrain.tools.customxml.networkwriter import NetworkWriter

The transformation of the first input to the second output is identical to the transformation of the
second towards the first:

    >>> r1, r2 = random(), random()
    >>> v1, v2 = n.activate([r1, r2])
    >>> v3, v4 = n.activate([r2, r1])

    >> n['b'].inputbuffer, n['c'].inputbuffer, n['b'].outputbuffer, n['c'].outputbuffer

    >>> v1 == v4
    True
    >>> v2 == v3
    True

Check its gradient:

    >>> from pybrain.tests import gradientCheck
    >>> gradientCheck(n)
    Perfect gradient
    True

Try writing it to an xml file, reread it and determine if it looks the same:

    >>> from pybrain.tests import xmlInvariance
    >>> xmlInvariance(n)
    Same representation
    Same function
    Same class

"""

__author__ = 'Tom Schaul, tom@idsia.ch'

import scipy

from pybrain.structure.networks.feedforward import FeedForwardNetwork
from pybrain import LinearLayer, SharedFullConnection, MotherConnection
from pybrain.tests import runModuleTestSuite


def buildSharedCrossedNetwork():
    """ build a network with shared connections. Two hidden modules are
    symmetrically linked, but to a different input neuron than the
    output neuron. The weights are random. """
    N = FeedForwardNetwork('shared-crossed')
    h = 1
    a = LinearLayer(2, name = 'a')
    b = LinearLayer(h, name = 'b')
    c = LinearLayer(h, name = 'c')
    d = LinearLayer(2, name = 'd')
    N.addInputModule(a)
    N.addModule(b)
    N.addModule(c)
    N.addOutputModule(d)

    m1 = MotherConnection(h)
    m1.params[:] = scipy.array((1,))

    m2 = MotherConnection(h)
    m2.params[:] = scipy.array((2,))

    N.addConnection(SharedFullConnection(m1, a, b, inSliceTo = 1))
    N.addConnection(SharedFullConnection(m1, a, c, inSliceFrom = 1))
    N.addConnection(SharedFullConnection(m2, b, d, outSliceFrom = 1))
    N.addConnection(SharedFullConnection(m2, c, d, outSliceTo = 1))
    N.sortModules()
    return N


if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))


########NEW FILE########
__FILENAME__ = test_sliced_connections
"""
    >>> from scipy import array
    >>> from pybrain.tests import epsilonCheck

Trying to build a network with shared connections:

    >>> from random import random
    >>> n = buildSlicedNetwork()
    >>> n.params[:] = array((2, 2))

The transformation of the first input to the second output is identical to the transformation of the
second towards the first:

    >>> r1, r2 = 2.5, 3.2
    >>> v1, v2 = n.activate([r1, r2])
    >>> epsilonCheck(6.4 - v1)
    True
    >>> epsilonCheck(5 - v2)
    True


"""

__author__ = 'Tom Schaul, tom@idsia.ch'


from pybrain.structure.networks.feedforward import FeedForwardNetwork
from pybrain import LinearLayer, FullConnection
from pybrain.tests import runModuleTestSuite


def buildSlicedNetwork():
    """ build a network with shared connections. Two hidden modules are
    symmetrically linked, but to a different input neuron than the
    output neuron. The weights are random. """
    N = FeedForwardNetwork('sliced')
    a = LinearLayer(2, name = 'a')
    b = LinearLayer(2, name = 'b')
    N.addInputModule(a)
    N.addOutputModule(b)

    N.addConnection(FullConnection(a, b, inSliceTo=1, outSliceFrom=1))
    N.addConnection(FullConnection(a, b, inSliceFrom=1, outSliceTo=1))
    N.sortModules()
    return N


if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))

########NEW FILE########
__FILENAME__ = test_subsampling_connection
"""
    >>> from pybrain.tests import epsilonCheck
    >>> n = buildSubsamplingNetwork()

All those inputs will be averaged in two blocks (first 4 and last 2),
so they should produce the same outputs.

    >>> x1 = n.activate([3,0,0,0,0,2])[0]
    >>> x2 = n.activate([0,0,0,3,2,0])[0]
    >>> x3 = n.activate([1,1,-2,3,1,1])[0]

    >>> epsilonCheck(x1 - x2)
    True
    >>> epsilonCheck(x1 - x3)
    True


"""

__author__ = 'Tom Schaul, tom@idsia.ch'


from pybrain.structure.connections.subsampling import SubsamplingConnection
from pybrain.structure.networks.feedforward import FeedForwardNetwork
from pybrain import LinearLayer
from pybrain.tests import runModuleTestSuite


def buildSubsamplingNetwork():
    """ Builds a network with subsampling connections. """
    n = FeedForwardNetwork()
    n.addInputModule(LinearLayer(6, 'in'))
    n.addOutputModule(LinearLayer(1, 'out'))
    n.addConnection(SubsamplingConnection(n['in'], n['out'], inSliceTo=4))
    n.addConnection(SubsamplingConnection(n['in'], n['out'], inSliceFrom=4))
    n.sortModules()
    return n


if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))

########NEW FILE########
__FILENAME__ = test_peephole_lstm
"""

    >>> from pybrain.tools.functions import tanh
    >>> from pybrain.utilities import fListToString
    >>> from scipy import arctanh
    >>> from random import random

Test the LSTMLayer behavior when using peepholes.

    >>> N = buildMinimalLSTMNetwork()
    >>> N.params[:] = [3,4,5]

    >>> s1 = 0.2
    >>> s2 = 0.345
    >>> s3 = -0.135
    >>> big = 10000

Set the state to s1
    >>> predictOutcome(N, [big, big, arctanh(s1), -big], 0)
    True

Verify that the state is conserved
    >>> predictOutcome(N, [-big, big, big*random(), big], tanh(s1))
    True

Add s2 to the state
    >>> predictOutcome(N, [big, big, arctanh(s2), big], tanh(s1+s2))
    True

Verify the peephole connection to the forgetgate (weight = 4) by neutralizing its contibution
and therefore dividing the state value by 2
    >>> predictOutcome(N, [-big, -(s1+s2) * 4, big*random(), big], tanh((s1+s2)/2))
    True

Verify the peephole connection to the inputgate (weight = 3) by neutralizing its contibution
and therefore dividing the provided input by 2. Also clearing the old state.
    >>> predictOutcome(N, [-(s1+s2)/2 * 3, -big, arctanh(s3), big], tanh(s3/2))
    True

Verify the peephole connection to the outputgate (weight = 5) by neutralizing its contibution
and therefore dividing the provided output by 2. Also clearing the old state.
    >>> predictOutcome(N, [-big, big, big*random(), -s3/2 * 5], tanh(s3/2)/2)
    True

List all the states again, explicitly (buffer size is 8 by now).
    >>> fListToString(N['lstm'].state, 4)
    '[0.2    , 0.2    , 0.545  , 0.2725 , -0.0675, -0.0675, 0      , 0      ]'


"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.tests import runModuleTestSuite
from pybrain.structure import LinearLayer, IdentityConnection, LSTMLayer, RecurrentNetwork
from pybrain.tests.helpers import epsilonCheck


def buildMinimalLSTMNetwork():
    N = RecurrentNetwork('simpleLstmNet')
    i = LinearLayer(4, name='i')
    h = LSTMLayer(1, peepholes=True, name='lstm')
    o = LinearLayer(1, name='o')
    N.addInputModule(i)
    N.addModule(h)
    N.addOutputModule(o)
    N.addConnection(IdentityConnection(i, h))
    N.addConnection(IdentityConnection(h, o))
    N.sortModules()
    return N

def predictOutcome(net, input, output):
    res = net.activate(input)[0]
    if epsilonCheck(res - output):
        return True
    else:
        print('expected:', round(output, 7), '- got:', round(res, 7))
        return False


if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))


########NEW FILE########
__FILENAME__ = test_peephole_mdlstm
"""
    >>> from pybrain.tests.helpers import epsilonCheck
    >>> from pybrain.tools.functions import tanh
    >>> from pybrain.utilities import fListToString
    >>> from test_peephole_lstm import predictOutcome
    >>> from scipy import arctanh
    >>> from random import random

Test the MDLSTMLayer behavior when using peepholes.

    >>> N = buildMinimalMDLSTMNetwork()
    >>> N.params[:] = [.3,.4,.5]

    >>> s1 = 0.4
    >>> s2 = 0.414
    >>> s3 = -0.305
    >>> big = 10000

Set the state to s1
    >>> predictOutcome(N, [big, big, arctanh(s1), -big], 0)
    True

Verify that the state is conserved
    >>> predictOutcome(N, [-big, big, big*random(), big], tanh(s1))
    True

Add s2 to the state
    >>> predictOutcome(N, [big, big, arctanh(s2), big], tanh(s1+s2))
    True

Verify the peephole connection to the forgetgate (weight = .4) by neutralizing its contibution
and therefore dividing the state value by 2
    >>> predictOutcome(N, [-big, -(s1+s2) * .4, big*random(), big], tanh((s1+s2)/2))
    True

Verify the peephole connection to the inputgate (weight = .3) by neutralizing its contibution
and therefore dividing the provided input by 2. Also clearing the old state.
    >>> predictOutcome(N, [-(s1+s2)/2 * .3, -big, arctanh(s3), big], tanh(s3/2))
    True

Verify the peephole connection to the outputgate (weight = .5) by neutralizing its contibution
and therefore dividing the provided output by 2. Also clearing the old state.
    >>> predictOutcome(N, [-big, big, big*random(), -s3/2 * .5], tanh(s3/2)/2)
    True

List all the states again, explicitly (buffer size is 8 by now).
    >>> fListToString(N['mdlstm'].outputbuffer[:,1], 2)
    '[0.4  , 0.4  , 0.81 , 0.41 , -0.15, -0.15, 0    , 0    ]'

"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.tests import runModuleTestSuite
from pybrain.structure import LinearLayer, IdentityConnection, MDLSTMLayer, RecurrentNetwork


def buildMinimalMDLSTMNetwork():
    N = RecurrentNetwork('simpleMdLstmNet')
    i = LinearLayer(4, name = 'i')
    h = MDLSTMLayer(1, peepholes = True, name = 'mdlstm')
    o = LinearLayer(1, name = 'o')
    N.addInputModule(i)
    N.addModule(h)
    N.addOutputModule(o)
    N.addConnection(IdentityConnection(i, h, outSliceTo = 4))
    N.addRecurrentConnection(IdentityConnection(h, h, outSliceFrom = 4, inSliceFrom = 1))
    N.addConnection(IdentityConnection(h, o, inSliceTo = 1))
    N.sortModules()
    return N

if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))

########NEW FILE########
__FILENAME__ = test_samplelayer
"""

    >>> from pybrain.structure.modules.samplelayer import BernoulliLayer
    >>> from scipy import random, array, empty

Set the random seed so we can predict the random variables.

    >>> random.seed(0)

Create a layer.

    >>> layer = BernoulliLayer(3)
    >>> input = array((0.8, 0.5, 0.2))
    >>> output = empty((3,))

Now test some forwards:

    >>> layer._forwardImplementation(input, output)
    >>> output
    array([ 0.,  1.,  1.])

    >>> layer._forwardImplementation(input, output)
    >>> output
    array([ 0.,  0.,  1.])

    >>> layer._forwardImplementation(input, output)
    >>> output
    array([ 0.,  1.,  1.])

"""

__author__ = 'Justin Bayer, bayerj@in.tum.de'

from pybrain.tests import runModuleTestSuite

if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))


########NEW FILE########
__FILENAME__ = test_simple_lstm_network
"""

Build a simple lstm network with peepholes:

    >>> n = buildSimpleLSTMNetwork(True)
    >>> print(n)
    simpleLstmNet
       Modules:
        [<BiasUnit 'bias'>, <LinearLayer 'i'>, <LSTMLayer 'lstm'>, <LinearLayer 'o'>]
       Connections:
        [<FullConnection 'f1': 'i' -> 'lstm'>, <FullConnection 'f2': 'bias' -> 'lstm'>, <FullConnection 'r1': 'lstm' -> 'o'>]
       Recurrent Connections:
        [<FullConnection 'r1': 'lstm' -> 'lstm'>]

Check its gradient:

    >>> from pybrain.tests import gradientCheck
    >>> gradientCheck(n)
    Perfect gradient
    True

    >>> net = RecurrentNetwork()
    >>> l = LSTMLayer(1)
    >>> net.addRecurrentConnection(FullConnection(l, l))
    >>> net.addInputModule(l)
    >>> net.outmodules.append(l)
    >>> net.sortModules()
    >>> gradientCheck(net)
    Perfect gradient
    True


Try writing it to an xml file, reread it and determine if it looks the same:

    >>> from pybrain.tests import xmlInvariance
    >>> xmlInvariance(n)
    Same representation
    Same function
    Same class




"""


__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.structure.networks.recurrent import RecurrentNetwork
from pybrain import LinearLayer, FullConnection, LSTMLayer, BiasUnit
from pybrain.tests import runModuleTestSuite


def buildSimpleLSTMNetwork(peepholes = False):
    N = RecurrentNetwork('simpleLstmNet')
    i = LinearLayer(1, name = 'i')
    h = LSTMLayer(1, peepholes = peepholes, name = 'lstm')
    o = LinearLayer(1, name = 'o')
    b = BiasUnit('bias')
    N.addModule(b)
    N.addOutputModule(o)
    N.addInputModule(i)
    N.addModule(h)
    N.addConnection(FullConnection(i, h, name = 'f1'))
    N.addConnection(FullConnection(b, h, name = 'f2'))
    N.addRecurrentConnection(FullConnection(h, h, name = 'r1'))
    N.addConnection(FullConnection(h, o, name = 'r1'))
    N.sortModules()
    return N


if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))


########NEW FILE########
__FILENAME__ = test_simple_mdlstm
"""

Build a simple mdlstm network with peepholes:

    >>> n = buildSimpleMDLSTMNetwork(True)
    >>> print(n)
    simpleMDLstmNet
       Modules:
        [<BiasUnit 'bias'>, <LinearLayer 'i'>, <MDLSTMLayer 'MDlstm'>, <LinearLayer 'o'>]
       Connections:
        [<FullConnection 'f1': 'i' -> 'MDlstm'>, <FullConnection 'f2': 'bias' -> 'MDlstm'>, <FullConnection 'f3': 'MDlstm' -> 'o'>]
       Recurrent Connections:
        [<FullConnection 'r1': 'MDlstm' -> 'MDlstm'>, <IdentityConnection 'rstate': 'MDlstm' -> 'MDlstm'>]

Check its gradient:

    >>> from pybrain.tests import gradientCheck
    >>> gradientCheck(n)
    Perfect gradient
    True

Try writing it to an xml file, reread it and determine if it looks the same:

    >>> from pybrain.tests import xmlInvariance
    >>> xmlInvariance(n)
    Same representation
    Same function
    Same class

"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.structure.networks.recurrent import RecurrentNetwork
from pybrain import LinearLayer, FullConnection, MDLSTMLayer, BiasUnit, IdentityConnection
from pybrain.tests import runModuleTestSuite


def buildSimpleMDLSTMNetwork(peepholes = False):
    N = RecurrentNetwork('simpleMDLstmNet')
    i = LinearLayer(1, name = 'i')
    dim = 1
    h = MDLSTMLayer(dim, peepholes = peepholes, name = 'MDlstm')
    o = LinearLayer(1, name = 'o')
    b = BiasUnit('bias')
    N.addModule(b)
    N.addOutputModule(o)
    N.addInputModule(i)
    N.addModule(h)
    N.addConnection(FullConnection(i, h, outSliceTo = 4*dim, name = 'f1'))
    N.addConnection(FullConnection(b, h, outSliceTo = 4*dim, name = 'f2'))
    N.addRecurrentConnection(FullConnection(h, h, inSliceTo = dim, outSliceTo = 4*dim, name = 'r1'))
    N.addRecurrentConnection(IdentityConnection(h, h, inSliceFrom = dim, outSliceFrom = 4*dim, name = 'rstate'))
    N.addConnection(FullConnection(h, o, inSliceTo = dim, name = 'f3'))
    N.sortModules()
    return N


if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))


########NEW FILE########
__FILENAME__ = test_capturegame_network
"""
Build a CaptureGameNetwork with LSTM cells

    >>> from pybrain.structure.networks.custom import CaptureGameNetwork
    >>> from pybrain import MDLSTMLayer
    >>> size = 2
    >>> n = CaptureGameNetwork(size = size, componentclass = MDLSTMLayer, hsize = 1, peepholes = False)

Check it's string representation
    >>> print(n)
    CaptureGameNetwork-s2-h1-MDLSTMLayer--...
      Modules:
        [<BiasUnit 'bias'>, <LinearLayer 'input'>, <MDLSTMLayer 'hidden(0, 0, 0)'>, ... <MDLSTMLayer 'hidden(0, 0, 3)'>, <SigmoidLayer 'output'>]
      Connections:
        [<IdentityConnection ...


Check some of the connections dimensionalities
    >>> c1 = n.connections[n['hidden(1, 0, 3)']][0]
    >>> c2 = n.connections[n['hidden(0, 1, 2)']][-1]
    >>> print(c1.indim, c1.outdim)
    1 1
    >>> print(c2.indim, c2.outdim)
    1 1
    >>> n.paramdim
    21

Try writing it to an xml file, reread it and determine if it looks the same:

    >>> from pybrain.tests import xmlInvariance
    >>> xmlInvariance(n)
    Same representation
    Same function
    Same class

Check its gradient:

    >>> from pybrain.tests import gradientCheck
    >>> gradientCheck(n)
    Perfect gradient
    True

"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.tests import runModuleTestSuite

if __name__ == '__main__':
    runModuleTestSuite(__import__('__main__'))

########NEW FILE########
__FILENAME__ = test_convolutional_nets
"""

Let's build a convolutional network designed for board games:

    >>> from pybrain.structure.networks.custom.convboard import ConvolutionalBoardNetwork
    >>> from scipy import array, ravel, var
    >>> N = ConvolutionalBoardNetwork(4, 3, 5)
    >>> print(N.paramdim)
    97

This is what a typical input would look like (on a 4x4 board)

    >>> input = [[[0,0],[0,0],[0,0],[0,0]],\
                 [[0,0],[0,0],[0,0],[0,0]],\
                 [[0,0],[1,1],[0,0],[0,1]],\
                 [[0,0],[1,0],[1,1],[0,1]],\
                 ]

We let the network process the input:

    >>> res = N.activate(ravel(array(input)))
    >>> res = res.reshape(4,4)
    >>> inp =  N['pad'].inputbuffer[0].reshape(6,6,2)[:,:,0]

The input of the first features (e.g. white stone presence) is in the middle, like we set it:

    >>> print(inp[1:5,1:5])
    [[ 0.  0.  0.  0.]
     [ 0.  0.  0.  0.]
     [ 0.  1.  0.  0.]
     [ 0.  1.  1.  0.]]

The rest of that array has been padded with an arbitrary, but identical bias weight:

    >>> var(inp[0,:]) < 1e-20
    True

    >>> inp[0,0] != 0.0
    True

On the output, all the values should be distinct, except for two in the middle above
because a cluster-size of 3x3 makes their input look identical.

    >>> res[0,1] - res[0,2]
    0.0

    >>> res[0,1] == res[0,3]
    False

    >>> res[1,1] == res[0,0]
    False

    >>> res[0,2] == res[3,2]
    False

Now let's use the network, and play a game with it:

    >>> from pybrain.rl.environments.twoplayergames import CaptureGameTask
    >>> t = CaptureGameTask(4)
    >>> tmp = t(N)

"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.tests import runModuleTestSuite

if __name__ == '__main__':
    runModuleTestSuite(__import__('__main__'))









########NEW FILE########
__FILENAME__ = test_bidirectional_network
"""

Build a bi-directional Network for sequences (each sample a single value) of length 20:

    >>> n = BidirectionalNetwork(seqlen=20, inputsize=1, hiddensize=5, symmetric=False)

It should have 2x1x5 + 2x1x5 + 2x5x5 = 70 weights

    >>> n.paramdim
    70

Now let's build a symmetric network:

    >>> n = BidirectionalNetwork(seqlen=12, inputsize=2, hiddensize=3, symmetric=True)
    >>> n.indim
    24

It should have 1x2x3 + 1x1x3 + 1x3x3 = 18 weights

    >>> n.paramdim
    18

A forward pass:

    >>> from numpy import ones
    >>> r = n.activate(ones(24))
    >>> len(r)
    12

The result should be symmetric (although the weights are random)

    >>> r[0]-r[-1]
    0.0

Check its gradient:

    >>> from pybrain.tests import gradientCheck
    >>> gradientCheck(n)
    Perfect gradient
    True


"""

__author__ = 'Tom Schaul, tom@idsia.ch'


from pybrain.structure.networks import BidirectionalNetwork #@UnusedImport
from pybrain.tests import runModuleTestSuite


if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))


########NEW FILE########
__FILENAME__ = test_borderswipingnetwork
""" A few tests for BorderSwipingNetworks
    >>> from pybrain import MotherConnection
    >>> from scipy import ones, array

We will use a simple 3-dimensional network:

    >>> dim = 3
    >>> size = 3
    >>> hsize = 1

It is possible to define some weights before construction:

    >>> predefined = {'outconn': MotherConnection(1)}
    >>> predefined['outconn']._setParameters([0.5])

Building it with the helper function below:

    >>> net = buildSimpleBorderSwipingNet(size, dim, hsize, predefined)
    >>> net.name
    'BorderSwipingNetwork-...

    >>> net.paramdim
    7

    >>> net.dims
    (3, 3, 3)


Did the weight get set correctly?

    >>> net.params[0]
    0.5

Now we'll set all weights to a sequence of values:

    >>> net._setParameters(array(range(net.paramdim))/10.+.1)
    >>> nearlyEqual(list(net.params), [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7])
    True

Now we want to use the same weights to build a bigger network

    >>> size2 = size + 2
    >>> net2 = buildSimpleBorderSwipingNet(size2, dim, hsize, net.predefined)

It has a few more parameters:

    >>> net2.paramdim
    12

But the values are the same than before except numerical differences.

    >>> nearlyEqual(list(net2.params), [0.1, 0.2, 0.3, 0.3, 0.4, 0.5, 0.4333, 0.40, 0.46666, 0.4142857, 0.6, 0.7])
    True

Let's attempt a couple of activations:

    >>> res = net.activate(array(range(net.indim))/10.)
    >>> res2 = net2.activate(array(range(net2.indim))/10.)
    >>> min(res), min(res2)
    (0.625..., 0.631...)

    >>> max(res), max(res2)
    (0.797..., 0.7999...)


"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.tests import runModuleTestSuite
from pybrain.structure.networks import BorderSwipingNetwork
from pybrain import ModuleMesh, LinearLayer, TanhLayer


def nearlyEqual(lst1, lst2, tolerance=0.001):
    """Tell whether the itemwise differences of the two lists is never bigger
    than tolerance."""
    return all(abs(i - j) <= tolerance for i, j in zip(lst1, lst2))


def buildSimpleBorderSwipingNet(size = 3, dim = 3, hsize = 1, predefined = {}):
    """ build a simple swiping network,of given size and dimension, using linear inputs and output"""
    # assuming identical size in all dimensions
    dims = tuple([size]*dim)
    # also includes one dimension for the swipes
    hdims = tuple(list(dims)+[2**dim])
    inmod = LinearLayer(size**dim, name = 'input')
    inmesh = ModuleMesh.viewOnFlatLayer(inmod, dims, 'inmesh')
    outmod = LinearLayer(size**dim, name = 'output')
    outmesh = ModuleMesh.viewOnFlatLayer(outmod, dims, 'outmesh')
    hiddenmesh = ModuleMesh.constructWithLayers(TanhLayer, hsize, hdims, 'hidden')
    return BorderSwipingNetwork(inmesh, hiddenmesh, outmesh, predefined = predefined)


if __name__ == '__main__':
    runModuleTestSuite(__import__('__main__'))

########NEW FILE########
__FILENAME__ = test_cyclic_network
"""

Trying to build a cyclic network (should fail):

    >>> buildCyclicNetwork(False)
    Traceback (most recent call last):
        ...
    NetworkConstructionException: Loop in network graph.

If one connection is recurrent, it should work:

    >>> buildCyclicNetwork(True)
    <RecurrentNetwork 'cyc'>

"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain import FeedForwardNetwork, RecurrentNetwork, LinearLayer, \
    FullConnection
from pybrain.tests import runModuleTestSuite


def buildCyclicNetwork(recurrent):
    """ build a cyclic network with 4 modules

    :key recurrent: make one of the connections recurrent """
    Network = RecurrentNetwork if recurrent else FeedForwardNetwork
    N = Network('cyc')
    a = LinearLayer(1, name='a')
    b = LinearLayer(2, name='b')
    c = LinearLayer(3, name='c')
    d = LinearLayer(4, name='d')
    N.addInputModule(a)
    N.addModule(b)
    N.addModule(d)
    N.addOutputModule(c)
    N.addConnection(FullConnection(a, b))
    N.addConnection(FullConnection(b, c))
    N.addConnection(FullConnection(c, d))
    if recurrent:
        N.addRecurrentConnection(FullConnection(d, a))
    else:
        N.addConnection(FullConnection(d, a))
    N.sortModules()
    return N


if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))


########NEW FILE########
__FILENAME__ = test_nested_ffn_and_rnn
"""

Build a mixed nested network:

    >>> n = buildMixedNestedNetwork()
    >>> inner = n['inner']

Some specific tests:
The feed-forward part should have its buffers increased in size, and
keep the correct offset.

    >>> len(inner.outputbuffer)
    1

    >>> o1 = n.activate([1])
    >>> o2 = n.activate([2])
    >>> o2 = n.activate([3])
    >>> (o1 == o2).any()
    False

    >>> n.offset
    3
    >>> inner.offset
    3
    >>> len(inner.outputbuffer)
    4

Verify everything is still fine after reset
    >>> n.reset()

    >>> n.offset
    0
    >>> inner.offset
    0



Check its gradient:

    >>> from pybrain.tests import gradientCheck
    >>> gradientCheck(n)
    Perfect gradient
    True

Try writing it to an xml file, reread it and determine if it looks the same:

    >>> from pybrain.tests import xmlInvariance
    >>> xmlInvariance(n)
    Same representation
    Same function
    Same class


"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.structure import RecurrentNetwork
from pybrain import LinearLayer, FullConnection
from pybrain.tools.shortcuts import buildNetwork
from pybrain.tests import runModuleTestSuite


def buildMixedNestedNetwork():
    """ build a nested network with the inner one being a ffn and the outer one being recurrent. """
    N = RecurrentNetwork('outer')
    a = LinearLayer(1, name='a')
    b = LinearLayer(2, name='b')
    c = buildNetwork(2, 3, 1)
    c.name = 'inner'
    N.addInputModule(a)
    N.addModule(c)
    N.addOutputModule(b)
    N.addConnection(FullConnection(a, b))
    N.addConnection(FullConnection(b, c))
    N.addRecurrentConnection(FullConnection(c, c))
    N.sortModules()
    return N


if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))


########NEW FILE########
__FILENAME__ = test_nested_network
"""

Build a nested network:

    >>> n = buildNestedNetwork()

Check its gradient:

    >>> from pybrain.tests import gradientCheck
    >>> gradientCheck(n)
    Perfect gradient
    True

Try writing it to an xml file, reread it and determine if it looks the same:

    >>> from pybrain.tests import xmlInvariance
    >>> xmlInvariance(n)
    Same representation
    Same function
    Same class

"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.structure import FeedForwardNetwork
from pybrain import LinearLayer, FullConnection
from pybrain.tools.shortcuts import buildNetwork
from pybrain.tests import runModuleTestSuite


def buildNestedNetwork():
    """ build a nested network. """
    N = FeedForwardNetwork('outer')
    a = LinearLayer(1, name='a')
    b = LinearLayer(2, name='b')
    c = buildNetwork(2, 3, 1)
    c.name = 'inner'
    N.addInputModule(a)
    N.addModule(c)
    N.addOutputModule(b)
    N.addConnection(FullConnection(a, b))
    N.addConnection(FullConnection(b, c))
    N.sortModules()
    return N


if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))


########NEW FILE########
__FILENAME__ = test_network_decomposition
"""

Build a decomposable network

    >>> n = buildDecomposableNetwork()

Check if it was built correctly
    >>> print(n.paramdim)
    12
    >>> tmp = n.getDecomposition()
    >>> tmp[2]
    array([ 1.,  1.,  1.,  1.])

Let's keep the output value for later
    >>> act = n.activate([-1.2,0.5])


Now, change the values for the first neuron
    >>> tmp[0] *= 0

The network has not changed yet
    >>> n.getDecomposition()[0]
    array([ 1.,  1.,  1.,  1.])

Now it has:
    >>> n.setDecomposition(tmp)
    >>> n.getDecomposition()[0]
    array([ 0.,  0.,  0.,  0.])

The new output value should be 2/3 of the original one, with one neuron disabled.

    >>> act2 = n.activate([-1.2,0.5])
    >>> (act2 * 3 / 2 - act)[0]
    0.0

Check its gradient:

    >>> from pybrain.tests import gradientCheck
    >>> gradientCheck(n)
    Perfect gradient
    True

Try writing it to an xml file, reread it and determine if it looks the same:

    >>> from pybrain.tests import xmlInvariance
    >>> xmlInvariance(n)
    Same representation
    Same function
    Same class

"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import ones

from pybrain.structure.networks import NeuronDecomposableNetwork
from pybrain.tools.shortcuts import buildNetwork
from pybrain.tests import runModuleTestSuite


def buildDecomposableNetwork():
    """ three hidden neurons, with 2 in- and 2 outconnections each. """
    n = buildNetwork(2, 3, 2, bias = False)
    ndc = NeuronDecomposableNetwork.convertNormalNetwork(n)
    # set all the weights to 1
    ndc._setParameters(ones(12))
    return ndc

if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))


########NEW FILE########
__FILENAME__ = test_network_forward_backward
"""

Test the forward and backward passes through a linear network.

    >>> from scipy import array
    >>> from pybrain import LinearLayer
    >>> from pybrain.tools.shortcuts import buildNetwork
    >>> n = buildNetwork(2, 4, 3, bias = False, hiddenclass = LinearLayer, recurrent=True)


The forward passes (2 timesteps), by two different but equivalent methods
    >>> input = array([1,2])
    >>> n.inputbuffer[0] = input
    >>> n.forward()
    >>> tmp = n.activate(input * 2)

The backward passes, also by two different but equivalent methods
    >>> outerr = array([-0.1, 0, 1])
    >>> n.outputerror[1] = outerr * 3
    >>> n.backward()
    >>> tmp = n.backActivate(outerr)

Verify that the inputs and outputs are proportional
    >>> sum(n.outputbuffer[1]/n.outputbuffer[0])
    6.0
    >>> abs((n.inputerror[1]/n.inputerror[0])[1] - 3.0) < 0.0001
    True

"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.tests import runModuleTestSuite

if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))


########NEW FILE########
__FILENAME__ = test_network_sort
"""

Determine if the modules in a Network are always sorted in the same way, even if the connections
don't constrain a particular order.

Build a number of modules and connections, to be used for all constructions

    >>> from pybrain.structure.networks.network import Network
    >>> mods = buildSomeModules(10)
    >>> conns = buildSomeConnections(mods)

Construct a network, normally, and sort it
    >>> n = Network()
    >>> for m in mods:
    ...    n.addModule(m)
    ...
    >>> for c in conns:
    ...    n.addConnection(c)
    ...
    >>> n.sortModules()
    >>> ord = str(n.modulesSorted)

Is the order the same, if we sort it again?

    >>> n.sortModules()
    >>> ord2 = str(n.modulesSorted)
    >>> ord == ord2
    True

What if we construct it in a different order?

    >>> n = Network()
    >>> for m in reversed(mods):
    ...    n.addModule(m)
    ...
    >>> for c in reversed(conns):
    ...    n.addConnection(c)
    ...
    >>> n.sortModules()
    >>> ord3 = str(n.modulesSorted)
    >>> ord == ord3
    True

Is it the same ordering than our reference?

    >>> print(ord3)
    [<LinearLayer 'l0'>, <LinearLayer 'l2'>, <LinearLayer 'l3'>, <LinearLayer 'l5'>, <LinearLayer 'l6'>, <LinearLayer 'l7'>, <LinearLayer 'l8'>, <LinearLayer 'l9'>, <LinearLayer 'l1'>, <LinearLayer 'l4'>]

"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain import LinearLayer, FullConnection
from pybrain.tests import runModuleTestSuite


def buildSomeModules(number = 4):
    res = []
    for i in range(number):
        res.append(LinearLayer(1, 'l'+str(i)))
    return res

def buildSomeConnections(modules):
    """ add a connection from every second to every third module """
    res = []
    for i in range(len(modules)/3-1):
        res.append(FullConnection(modules[i*2], modules[i*3+1]))
    return res


if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))


########NEW FILE########
__FILENAME__ = test_no_gravity_network
"""
The library should be able to handle networks without any weight:

    >>> n1= buildNonGravityNet(False)
    >>> n1.paramdim
    0

    >>> n1.activate([0.2,0.4])[0]
    1.289...
    >>> n1.activate([0.2,0.4])[0]
    1.289...

Now let's verify the recurrent one as well:

    >>> n2= buildNonGravityNet(True)
    >>> n2.paramdim
    0

    >>> n2.activate([0.2,0.4])[0]
    1.289...
    >>> n2.activate([0.2,0.4])[0]
    3.478...

"""

from pybrain.structure import RecurrentNetwork, FeedForwardNetwork, IdentityConnection, LinearLayer, SigmoidLayer
from pybrain.tests.testsuites import runModuleTestSuite

def buildNonGravityNet(recurrent = False):
    if recurrent:
        net = RecurrentNetwork()
    else:
        net = FeedForwardNetwork()
    l1 = LinearLayer(2)
    l2 = LinearLayer(3)
    s1 = SigmoidLayer(2)
    l3 = LinearLayer(1)
    net.addInputModule(l1)
    net.addModule(l2)
    net.addModule(s1)
    net.addOutputModule(l3)
    net.addConnection(IdentityConnection(l1, l2, outSliceFrom = 1))
    net.addConnection(IdentityConnection(l1, l2, outSliceTo = 2))
    net.addConnection(IdentityConnection(l2, l3, inSliceFrom = 2))
    net.addConnection(IdentityConnection(l2, l3, inSliceTo = 1))
    net.addConnection(IdentityConnection(l1, s1))
    net.addConnection(IdentityConnection(l2, s1, inSliceFrom = 1))
    net.addConnection(IdentityConnection(s1, l3, inSliceFrom = 1))
    if recurrent:
        net.addRecurrentConnection(IdentityConnection(s1, l1))
        net.addRecurrentConnection(IdentityConnection(l2, l2, inSliceFrom = 1, outSliceTo = 2))
    net.sortModules()
    return net

if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))


########NEW FILE########
__FILENAME__ = test_recurrent_network
"""

Build a simple recurrent network:

    >>> n = buildRecurrentNetwork()
    >>> print(n)
    RecurrentNetwork
       Modules:
        [<LinearLayer 'in'>, <LinearLayer 'hidden0'>, <LinearLayer 'out'>]
       ...
       Recurrent Connections:
        [<FullConnection ...: 'hidden0' -> 'hidden0'>]

Check its gradient:

    >>> from pybrain.tests import gradientCheck
    >>> gradientCheck(n)
    Perfect gradient
    True

Try writing it to an xml file, reread it and determine if it looks the same:

    >>> from pybrain.tests import xmlInvariance
    >>> xmlInvariance(n)
    Same representation
    Same function
    Same class

Set all the weights to one, and the recurrent one to 0.5, and then do some checks.
    >>> n.params[:] = [1,1,0.5]
    >>> n.reset()
    >>> n.activate(4)[0]
    4.0
    >>> n.activate(-1)[0]
    1.0
    >>> n.activate(0)[0]
    0.5
    >>> n.reset()
    >>> n.activate(0)[0]
    0.0

"""
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import ones #@UnusedImport
from pybrain import FullConnection
from pybrain.tools.shortcuts import buildNetwork
from pybrain.structure import LinearLayer
from pybrain.tests import runModuleTestSuite


def buildRecurrentNetwork():
    N = buildNetwork(1, 1, 1, recurrent=True, bias=False, hiddenclass=LinearLayer, outputbias=False)
    h = N['hidden0']
    N.addRecurrentConnection(FullConnection(h, h))
    N.sortModules()
    N.name = 'RecurrentNetwork'
    return N

if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))


########NEW FILE########
__FILENAME__ = test_swiping_network
"""

Build a 2-dimensional BorderSwipingNetwork:

    >>> n = buildSwipingNetwork(2)

Check its gradient:

    >>> from pybrain.tests import gradientCheck
    >>> gradientCheck(n)
    Perfect gradient
    True

Try writing it to an xml file, reread it and determine if it looks the same:

    >>> from pybrain.tests import xmlInvariance
    >>> xmlInvariance(n)
    Same representation
    Same function
    Same class

"""

__author__ = 'Tom Schaul, tom@idsia.ch'


from pybrain import ModuleMesh, LinearLayer
from pybrain.structure.networks import BorderSwipingNetwork
from pybrain.tests import runModuleTestSuite


def buildSwipingNetwork(dimensions = 3):
    d = tuple([2] * dimensions)
    inmesh = ModuleMesh.constructWithLayers(LinearLayer, 1, d, 'in')
    hmesh = ModuleMesh.constructWithLayers(LinearLayer, 1, tuple(list(d)+[2**len(d)]), 'h')
    outmesh = ModuleMesh.constructWithLayers(LinearLayer, 1, d, 'out')
    return BorderSwipingNetwork(inmesh, hmesh, outmesh)



if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))


########NEW FILE########
__FILENAME__ = _test_mdrnn
"""

    >>> m = _MultiDirectionalMdrnn(2, (4, 4), 1, 1, (2, 2))
    >>> m._permsForSwiping()[0]
    array([0, 1, 2, 3])
    >>> m._permsForSwiping()[1]
    array([1, 0, 3, 2])
    >>> m._permsForSwiping()[2]
    array([2, 3, 0, 1])
    >>> m._permsForSwiping()[3]
    array([3, 2, 1, 0])

    >>> m = _Mdrnn(2, (4, 4), 1, 1, (2, 2))
    >>> m._permsForSwiping()
    [array([0, 1, 2, 3])]
"""


from pybrain.structure.networks.mdrnn import _MultiDirectionalMdrnn, _Mdrnn #@UnusedImport
from pybrain.tests import runModuleTestSuite


if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))

########NEW FILE########
__FILENAME__ = _test_rbm
"""

    >>> import scipy

    >>> from pybrain.structure.networks.rbm import Rbm
    >>> rbm = Rbm.fromDims(3, 2,
    ...                    weights=scipy.array((0, 1, 2, 3, 4, 5)))
    ...
    >>> scipy.size(rbm.params)
    8
    >>> rbmi = Rbm.invert()
    >>> rbmi.connections[rbmi['visible']][0].params
    array([ 0.,  3.,  1.,  4.,  2.,  5.])

"""

__author__ = 'Justin S Bayer, bayer.justin@googlemail.com'
__version__ = '$Id$'


from pybrain.tests import runModuleTestSuite


if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))

########NEW FILE########
__FILENAME__ = test_minhash
# -*- coding: utf-8 -*-

"""
Internal Tests:

    >>> from scipy import array
    >>> from pybrain.supervised.knn.lsh.minhash import arrayPermutation
    >>> permutation = array([4, 3, 2, 1, 0])
    >>> permute = arrayPermutation(permutation)
    >>> permute(array([5, 2, 3, 1, 4]))
    array([4, 1, 3, 2, 5])

    >>> from pybrain.supervised.knn.lsh.minhash import jacardCoefficient
    >>> a = array([0, 0, 0, 1])
    >>> b = array([1, 1, 1, 1])
    >>> c = array([1, 1, 0, 1])
    >>> d = array([0, 0, 1, 1])
    >>> jacardCoefficient(a, b)
    0.25
    >>> jacardCoefficient(b, c)
    0.75
    >>> jacardCoefficient(a, a)
    1.0
    >>> jacardCoefficient(a, d)
    0.75


Example Usage:

    >>> from pybrain.supervised.knn.lsh.minhash import MinHash

We need to specify the length of the inputs and how many permutations should be
used:

    >>> m = MinHash(5, 1)

The permutation is initialized randomly

    >>> m.permutations
    array([...])

But for the tests, we will justify it to our means:

    >>> m.permutations = array([[0, 1, 2, 4, 3], [0, 1, 2, 3, 4]])

So let's put in some values that will hash to the same bucket

    >>> m.put(array([1, 1, 1, 1, 1]), 'red')

Some "introspection" to check if everything went right

    >>> m.buckets
    defaultdict(<function <lambda> at ...>, {(0, 0): [(array([...True], dtype=bool), 'red')]})
    >>> m._hash(array([1, 1, 0, 0, 0]))
    (0, 0)

Put another one in

    >>> m.put(array([1, 1, 1, 1, 0]), 'red')

An check if this one is favored above the other

    >>> m.knn(array([1, 1, 0, 0, 0]), 1)
    [(array([... True, False], dtype=bool), 'red')]
    >>> m.knn(array([1, 1, 0, 0, 0]), 1)
    [(array([... True, False], dtype=bool), 'red')]



Let's make a hash that returns nothing

    >>> m.knn(array([0, 0, 0, 0, 0]), 1)
    []

"""

from pybrain.tests import runModuleTestSuite

if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))

########NEW FILE########
__FILENAME__ = test_nearoptimal
# -*- coding: utf-8 -*-

"""
    >>> from scipy import array
    >>> from math import sqrt
    >>> from pybrain.tests import epsilonCheck

Internal Tests:
====================

Let's make a hypercube for 2 dimensions.

    >>> dim = 2

To make some nice sidelengths, we cheat on omega

    >>> omega = 5 / sqrt(sqrt(dim))

    >>> m = nearoptimal.MultiDimHash(dim=dim, omega=omega, prob=0.8)

    >>> m.radius
    1.189207115002...
    >>> m.radiusSquared
    1.41421356237309...

This gives us hypercube sidelength of

    >>> SIDELENGTH = sqrt(sqrt(2)) * omega
    >>> epsilonCheck(SIDELENGTH - 5)
    True

Define some points to work with

    >>> a = array([0, 0])
    >>> m._findHypercube(a)
    (array([0, 0]), array([ 0.,  0.]))

    >>> b = array([0.14 + 3 * SIDELENGTH, .5])
    >>> m._findHypercube(b)
    (array([3, 0]), array([ 0.14,  0.5 ]))

    >>> c = array([.5, 42 * SIDELENGTH + 0.1])
    >>> m._findHypercube(c)
    (array([ 0, 42]), array([ 0.5,  0.1]))

    >>> d = array([-1 * SIDELENGTH + 0.1, 2 * SIDELENGTH + 0.1])
    >>> m._findHypercube(d)
    (array([-1,  2]), array([ 0.1,  0.1]))

Overwrite the balls of the hash to make test the ball intersection function

    >>> m.gridBalls = array([[.3, .3], [ 3., 3.]])

Tests for points within the hypercube [0, 1)^n

    >>> u, v = array([.29, .31]), array([2.9, 2.71])

We discard the first result, since it might trigger a compilation and thus
output some noise.

    >>> _ = m._findLocalBall(u)
    ...

    >>> m._findLocalBall(u)
    0

    >>> m._findLocalBall(v)
    1

Point outside of the hypercube don't return a result

    >>> m._findLocalBall(array([20, 0]))     # Returns None

As do points that are in not within any ball

    >>> m._findLocalBall(array([5.4, .9]))

Testing the composition of _findLocalBall and _findHypercube

    >>> m.findBall(u + array([2 * SIDELENGTH, 4 * SIDELENGTH]))
    ((2, 4), 0)

    >>> m.findBall(u + array([-2 * SIDELENGTH, 4 * SIDELENGTH]))
    ((-2, 4), 0)

    >>> m.findBall(u + array([-2 * SIDELENGTH, -4 * SIDELENGTH]))
    ((-2, -4), 0)

    >>> m.findBall(u + array([2 * SIDELENGTH, -4 * SIDELENGTH]))
    ((2, -4), 0)

    >>> m.findBall(v + array([2 * SIDELENGTH, 4 * SIDELENGTH]))
    ((2, 4), 1)

    >>> m.findBall(v + array([-2 * SIDELENGTH, 4 * SIDELENGTH]))
    ((-2, 4), 1)

    >>> m.findBall(v + array([-2 * SIDELENGTH, -4 * SIDELENGTH]))
    ((-2, -4), 1)

    >>> m.findBall(v + array([2 * SIDELENGTH, -4 * SIDELENGTH]))
    ((2, -4), 1)


Example Usage:
====================

    >>> m = nearoptimal.MultiDimHash(dim=2)
    >>> m.insert(array([0.9585762, 1.15822724]), 'red')
    >>> m.insert(array([1.02331605,  0.95385982]), 'red')
    >>> m.insert(array([0.80838576, 1.07507294]), 'red')
    >>> m.knn(array([0.9585762, 1.15822724]), 1)
    [(array([ 0.9585762 ,  1.15822724]), 'red')]


"""

from pybrain.tests import runModuleTestSuite
from pybrain.supervised.knn.lsh import nearoptimal

if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))

########NEW FILE########
__FILENAME__ = test_backprop
"""
    >>> from pybrain.datasets.supervised import SupervisedDataSet
    >>> from pybrain.supervised.trainers import BackpropTrainer
    >>> from pybrain import FeedForwardNetwork
    >>> from pybrain.structure import LinearLayer, SigmoidLayer, FullConnection
    >>> from random import randrange
    >>> dataset = SupervisedDataSet(6, 2)
    >>> for i in range(1000):
    ...     state = [randrange(0, 15), 
    ...              randrange(-70, 50), 
    ...              randrange(-70, 50), 
    ...              randrange(-70, 50), 
    ...              randrange(-70, 50), 
    ...              float(randrange(1, 5))/20.]
    ...     action = [float(randrange(-1, 1))/10.0, 
    ...               randrange(0, 1)]
    ...     dataset.addSample(state, action)
    >>> 
    >>> net = FeedForwardNetwork()
    >>> 
    >>> net.addInputModule(LinearLayer(6, name='in'))
    >>> net.addModule(SigmoidLayer(40, name='hidden_0'))
    >>> net.addModule(SigmoidLayer(16, name='hidden_1'))
    >>> net.addOutputModule(LinearLayer(2, name='out'))
    >>> 
    >>> net.addConnection(FullConnection(net['in'], net['hidden_0']))
    >>> net.addConnection(FullConnection(net['hidden_0'], net['hidden_1']))
    >>> net.addConnection(FullConnection(net['hidden_1'], net['out']))
    >>> 
    >>> net.sortModules()
    >>> 
    >>> trainer = BackpropTrainer(net,
    ...                           dataset=dataset,
    ...                           learningrate=0.01,
    ...                           lrdecay=1,
    ...                           momentum=0.5,
    ...                           verbose=False,
    ...                           weightdecay=0,
    ...                           batchlearning=False)
    >>> 
    >>> trainingErrors, validationErrors = trainer.trainUntilConvergence(
    ...    dataset=dataset, 
    ...    maxEpochs=10)
"""



__author__ = 'Steffen Kampmann, steffen.kampmann@gmail.com'

from pybrain.tests import runModuleTestSuite

if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))

########NEW FILE########
__FILENAME__ = test_evolino
"""
    >>> import numpy
    >>> from pybrain.datasets.sequential              import SequentialDataSet
    >>> from pybrain.structure.modules.evolinonetwork import EvolinoNetwork
    >>> from pybrain.supervised.trainers.evolino      import EvolinoTrainer

    >>> dataset = SequentialDataSet(0,1)
    >>> dataset.newSequence()
    >>> for x in numpy.arange( 0 , 30 , 0.2 ):
    ...     dataset.addSample([], [x])
    ...


Tests for Construction of Module and Trainer
--------------------------------------------
    >>> net = EvolinoNetwork( dataset.outdim, 7 )
    >>> trainer = EvolinoTrainer(
    ...     net,
    ...     dataset=dataset,
    ...     subPopulationSize = 5,
    ...     nParents = 2,
    ...     nCombinations = 1,
    ...     initialWeightRange = ( -0.01 , 0.01 ),
    ...     backprojectionFactor = 0.001,
    ...     mutationAlpha = 0.001,
    ...     nBurstMutationEpochs = numpy.Infinity,
    ...     wtRatio = 1./3.,
    ...     verbosity = 0)


Tests for Training and Applying
-------------------------------
    >>> trainer.trainEpochs( 1 )
    >>> trainer.evaluation.max_fitness > -100
    True

    >>> sequence = dataset.getField('target')
    >>> net.extrapolate(sequence, len(sequence))[-1][0]>55
    True
"""



__author__ = 'Michael Isik, isikmichael@gmx.net'

from pybrain.tests import runModuleTestSuite

if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))


########NEW FILE########
__FILENAME__ = test_rprop
"""
    >>> from pybrain.tools.shortcuts     import buildNetwork
    >>> from pybrain.supervised.trainers import BackpropTrainer, RPropMinusTrainer
    >>> from pybrain.datasets import SupervisedDataSet, ImportanceDataSet
    >>> from scipy import random, array

Initialize random number generator

    >>> random.seed(42)

Create an XOR-dataset and a recurrent network

    >>> ds = ImportanceDataSet(2,2)
    >>> ds.addSample([0,0],[0, 1],  [1,0])
    >>> ds.addSample([0,1],[1, 10],  [1,0])
    >>> ds.addSample([1,0],[1, -1],  [1,0])
    >>> ds.addSample([1,1],[0, 0],  [1,0])
    >>> n = buildNetwork(ds.indim, 4, ds.outdim, recurrent=True)

Create and test backprop trainer

    >>> t = BackpropTrainer(n, learningrate = 0.01, momentum = 0.99, verbose = True)
    >>> t.trainOnDataset(ds, 4)
    Total error: 2.44696473875
    Total error: 1.97570498879
    Total error: 1.23940309483
    Total error: 0.546129967878
    >>> abs(n.params[10:15] - array([ -0.53868206, -0.54185834,  0.26726394, -1.90008234, -1.12114946])).round(5)
    array([ 0.,  0.,  0.,  0.,  0.])

Now the same for RPROP

    >>> t = RPropMinusTrainer(n, verbose = True)
    >>> t.trainOnDataset(ds, 4)
    epoch      0  total error      0.16818   avg weight       0.92638
    epoch      1  total error      0.15007   avg weight       0.92202
    epoch      2  total error      0.15572   avg weight       0.92684
    epoch      3  total error      0.13036   avg weight       0.92604
    >>> abs(n.params[5:10] - array([ -0.19241111,  1.43404022,  0.23062397, -0.40105413,  0.62100109])).round(5)
    array([ 0.,  0.,  0.,  0.,  0.])

"""

__author__ = 'Martin Felder, felder@in.tum.de'


from pybrain.tests import runModuleTestSuite

if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))


########NEW FILE########
__FILENAME__ = test_utilities
"""
    >>> from pybrain.utilities import memoize
    >>> call_count = 0
    >>> @memoize
    ... def longComp():
    ...   global call_count
    ...   call_count += 1
    ...   return 'result'
    >>> longComp()
    'result'
    >>> call_count
    1
    >>> longComp()
    'result'
    >>> call_count
    1


Tests for Serializable
======================

    >>> from cStringIO import StringIO
    >>> s = StringIO()
    >>> p = P()
    >>> p.x = 2
    >>> p.saveToFileLike(s)
    >>> s.seek(0)
    >>> q = P.loadFromFileLike(s)
    >>> q.x
    2



Tests for permute
=================

    >>> from pybrain.utilities import permute
    >>> permute(array((0, 1, 2)), [2, 1, 0])
    array([2, 1, 0])
    >>> permute(array(((0, 0, 0), (1, 1, 1), (2, 2, 2))), (2, 0, 1))
    array([[2, 2, 2],
           [0, 0, 0],
           [1, 1, 1]])


Tests for permuteToBlocks
=========================

    >>> from pybrain.utilities import permuteToBlocks
    >>> arr = array([[0, 1, 2, 3], [4, 5 ,6 ,7], [8, 9, 10, 11], [12, 13,14, 15]])
    >>> permuteToBlocks(arr, (2, 2))
    array([  0.,   1.,   4.,   5.,   2.,   3.,   6.,   7.,   8.,   9.,  12.,
            13.,  10.,  11.,  14.,  15.])
    >>> arr = array(range(32)).reshape(2, 4, 4)
    >>> permuteToBlocks(arr, (2, 2, 2)).astype('int8').tolist()
    [0, 1, 4, 5, 16, 17, 20, 21, 2, 3, 6, 7, 18, 19, 22, 23, 8, 9, 12, 13, 24, 25, 28, 29, 10, 11, 14, 15, 26, 27, 30, 31]


"""


from scipy import array #@UnusedImport
from pybrain.utilities import Serializable
from pybrain.tests import runModuleTestSuite


class P(Serializable):

    def __getstate__(self):
        return {'x': self.x}

    def __setstate__(self, dct):
        self.x = dct['x']


if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))

########NEW FILE########
__FILENAME__ = test_utilities_dictionaries
"""
    >>> from pybrain.utilities import dictCombinations, subDict, matchingDict
    >>> d1 = {'ones':[1,1,1,'one'], 2:[2,4,6,8], 4:4, 8:['eight']}
    >>> d2 = {1:1, 2:2, 4:4, 8:8}

subDict produces a sub-dictionary, by removing some keys.

    >>> d3 = subDict(d1, ['ones', 2, 4])
    >>> print(sorted(d3.items()))
    [(2, [2, 4, 6, 8]), (4, 4), ('ones', [1, 1, 1, 'one'])]

We can also flip the selection, and limit the keys to the ones NOT in the list:
    >>> d4 = subDict(d1, [8], flip=True)
    >>> d4 == d3
    True


matchingDict determines whether the values of a dictionary match the selection.
No selection always works:

    >>> matchingDict(d2, {})
    True

Not all elements must be present:

    >>> matchingDict(d2, {3:3})
    True

But those that are in both must fit (here 8 is wrong)
    >>> matchingDict(d2, d1)
    False

Without the 8 key:
    >>> matchingDict(d2, d3)
    True

dictCombinations will produce all the combinations of the elements in lists
with their keys, not allowing for identical items,
but dealing with non-lists, and any types of keys and values.

    >>> for x in dictCombinations(d1): print(sorted(x.items()))
    [(2, 2), (4, 4), (8, 'eight'), ('ones', 1)]
    [(2, 4), (4, 4), (8, 'eight'), ('ones', 1)]
    [(2, 6), (4, 4), (8, 'eight'), ('ones', 1)]
    [(2, 8), (4, 4), (8, 'eight'), ('ones', 1)]
    [(2, 2), (4, 4), (8, 'eight'), ('ones', 'one')]
    [(2, 4), (4, 4), (8, 'eight'), ('ones', 'one')]
    [(2, 6), (4, 4), (8, 'eight'), ('ones', 'one')]
    [(2, 8), (4, 4), (8, 'eight'), ('ones', 'one')]


"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.tests import runModuleTestSuite

if __name__ == '__main__':
    runModuleTestSuite(__import__('__main__'))

########NEW FILE########
__FILENAME__ = test_utilities_flood
"""
    >>> from pybrain.utilities import flood

The reachable-search can only get to 3 of the points.

    >>> sorted(flood(step, range(10), [2]))
    [2, 4, 5, 7, 8]

Early stopping with relevance argument:

    >>> sorted(flood(step, range(100), [2], relevant=[5]))
    [2, 4, 5]

If the initial point must be included for it to work:

    >>> sorted(flood(step, range(10), [-1]))
    []

"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.tests import runModuleTestSuite

def step(x):
    """ A neighbor of x is either 2*x or x+3"""
    return [x+3, 2*x]

if __name__ == '__main__':
    runModuleTestSuite(__import__('__main__'))



########NEW FILE########
__FILENAME__ = test_utilities_foundafter
"""
    >>> from pybrain.utilities import avgFoundAfter

A sequence of decreasing target values
    >>> dess = [20,10,3,1,0]

A list of sequences of encountered values
    >>> ls = [[11,11,11,11,11,11,11,11,1,1,1,10,1,0],\
              [11,9,7,5,2,0.5,-2],\
              [2,2,2,2,2,0,2,2,0,2,2,2,-1]]

Average index where each value is encountered.
    >>> avgFoundAfter(dess, ls)
    array([ 0.,  3.,  4.,  6.,  8.])

If a value is not always encountered, the length of the longest sequence is used:
    >>> avgFoundAfter([10,0], [[20],[20,1,1,1,-2]])
    array([ 3. ,  4.5])


"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.tests import runModuleTestSuite

if __name__ == '__main__':
    runModuleTestSuite(__import__('__main__'))




########NEW FILE########
__FILENAME__ = test_utilities_reachable
"""
    >>> from pybrain.utilities import reachable, decrementAny, flood

The reachable-search can only get to 3 of the points.

    >>> dests = [(1,3), (2,2), (3,2), (3,1), (1,0), (0,2), (2,0), (0,1)]
    >>> sorted(reachable(decrementAny, [(3,3)], dests).items())
    [((1, 3), 2), ((2, 2), 2), ((3, 2), 1)]

    >>> d2 = map(lambda i: (i,), range(10))
    >>> reachable(decrementAny, [(12,), (29,)], d2)
    {(9,): 3}

"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.tests import runModuleTestSuite

if __name__ == '__main__':
    runModuleTestSuite(__import__('__main__'))

########NEW FILE########
__FILENAME__ = test_ibp_leftordered
"""
    >>> from pybrain.tools.ibp import leftordered
    >>> from scipy import rand, array

Build a random binary matrix

    >>> M = array(rand(10,20)<0.4, dtype=bool)
    >>> L = leftordered(M)

Reordering rows gives the same result

    >>> M2 = M[:, ::-1]
    >>> sum(sum(L == leftordered(M2))) == 200
    True

Reordering columns does not
    >>> M3 = M[::-1, :]
    >>> sum(sum(L == leftordered(M3))) < 200
    True

"""

__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.tests import runModuleTestSuite

if __name__ == '__main__':
    runModuleTestSuite(__import__('__main__'))

########NEW FILE########
__FILENAME__ = test_rlgluebridge
"""

    >>> from pybrain.tools.rlgluebridge import adaptAgent
    >>> from rlglue.types import Action as RLGlueAction
    >>> from rlglue.types import Observation as RLGlueObservation

Let's take a pseudo agent to test with and make an object of it

    >>> klass = adaptAgent(PseudoPybrainAgent)
    >>> rlglue_agent = klass()

We can access the attributes of the original agent via the held attribute
`agent`

    >>> rlglue_agent
    <pybrain.tools.rlgluebridge.RlglueAgentAdapter object at ...>
    >>> type(rlglue_agent)
    <class 'pybrain.tools.rlgluebridge.RlglueAgentAdapter'>

Now let's see how the observations, actions and rewards are proxied to the
inner agent

    >>> rlglue_agent.agent_init()
    I was reseted

We need an observation to test with

    >>> obs = RLGlueObservation()
    >>> obs.doubleArray = [3.14, 42]

A first step

    >>> action = rlglue_agent.agent_start(obs)
    I saw [  3.14  42.  ]
    I did [ 2.7 -1. ]
    >>> action
    <rlglue.types.Action instance at ...>
    >>> action.doubleArray[0]
    2.7000000000000002
    >>> action.doubleArray[1]
    -1.0

Another step

    >>> action = rlglue_agent.agent_step(1, obs)
    I was given 1.00
    I saw [  3.14  42.  ]
    I did [ 2.7 -1. ]
    >>> action
    <rlglue.types.Action instance at ...>
    >>> action.doubleArray[0]
    2.7000000000000002
    >>> action.doubleArray[1]
    -1.0

And a last step

    >>> rlglue_agent.agent_end(0)
    I was given 0.00
    I got a new episode
    I was reseted


Now let's have a look on how we can save statistics of an agent running
rlglue.

    >>> from pybrain.tools.rlgluebridge import BenchmarkingAgent
    >>> agent = PseudoPybrainAgent()
    >>> agent = BenchmarkingAgent(agent)

    >>> agent.integrateObservation('obs')
    I saw obs
    >>> agent.getAction()
    I did [ 2.7 -1. ]
    array([ 2.7, -1. ])
    >>> agent.giveReward(0)
    I was given 0.00

    >>> agent.integrateObservation('obs2')
    I saw obs2
    >>> agent.getAction()
    I did [ 2.7 -1. ]
    array([ 2.7, -1. ])
    >>> agent.giveReward(1)
    I was given 1.00

    >>> agent.newEpisode()
    I got a new episode
    >>> print(agent.benchmark)
    Average Reward: dim(2, 1)
    [[ 0.5]]
    <BLANKLINE>
    Episode Length: dim(2, 1)
    [[ 2.]]
    <BLANKLINE>
    <BLANKLINE>

"""

__author__ = 'Justin Bayer, bayerj@in.tum.de'
_dependencies = ['rlglue']

from scipy import array

from pybrain.rl.agents import LearningAgent
from pybrain.tests import runModuleTestSuite


class PseudoPybrainAgent(LearningAgent):
    """A little Agent that follows the pybrain API for testing."""

    attribute = "my-attribute"
    learner = None
    learning = False

    def __init__(self): pass

    def integrateObservation(self, obs):
        print("I saw %s" % obs)

    def getAction(self):
        action = array([2.7, -1])
        print("I did %s" % action)
        return action

    def giveReward(self, r):
        """ Reward or punish the agent.

            :key r: reward, if C{r} is positive, punishment if C{r} is
                      negative
            :type r: double
        """
        print("I was given %.2f" % float(r))

    def newEpisode(self):
        print("I got a new episode")

    def reset(self):
        print("I was resetted")


if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))


########NEW FILE########
__FILENAME__ = _test_equivalence_to_ctypes
"""

    >>> from pybrain.tools.shortcuts import buildNetwork
    >>> from test_recurrent_network import buildRecurrentNetwork
    >>> from test_peephole_lstm import buildMinimalLSTMNetwork
    >>> from test_peephole_mdlstm import buildMinimalMDLSTMNetwork
    >>> from test_nested_network import buildNestedNetwork
    >>> from test_simple_lstm_network import buildSimpleLSTMNetwork
    >>> from test_simple_mdlstm import buildSimpleMDLSTMNetwork
    >>> from test_swiping_network import buildSwipingNetwork
    >>> from test_shared_connections import buildSharedCrossedNetwork
    >>> from test_sliced_connections import buildSlicedNetwork
    >>> from test_borderswipingnetwork import buildSimpleBorderSwipingNet

Test a number of network architectures, and compare if they produce the same output,
whether the Python implementation is used, or CTYPES.

Use the network construction scripts in other test files to build a number of networks,
and then test the equivalence of each.

Simple net
    >>> testEquivalence(buildNetwork(2,2))
    True

A lot of layers
    >>> net = buildNetwork(2,3,4,3,2,3,4,3,2)
    >>> testEquivalence(net)
    True

Nonstandard components
    >>> from pybrain.structure import TanhLayer
    >>> net = buildNetwork(2,3,2, bias = True, outclass = TanhLayer)
    >>> testEquivalence(net)
    True

Shared connections
    >>> net = buildSharedCrossedNetwork()
    >>> testEquivalence(net)
    True

Sliced connections
    >>> net = buildSlicedNetwork()
    >>> testEquivalence(net)
    True

Nested networks (not supposed to work yet!)
    >>> net = buildNestedNetwork()
    >>> testEquivalence(net)
    Network cannot be converted.

Recurrent networks
    >>> net = buildRecurrentNetwork()
    >>> net.name = '22'
    >>> net.params[:] = [1,1,0.5]
    >>> testEquivalence(net)
    True

Swiping networks
    >>> net = buildSwipingNetwork()
    >>> testEquivalence(net)
    True

Border-swiping networks
    >>> net = buildSimpleBorderSwipingNet()
    >>> testEquivalence(net)
    True

Lstm
    >>> net = buildSimpleLSTMNetwork()
    >>> testEquivalence(net)
    True

Mdlstm
    >>> net = buildSimpleMDLSTMNetwork()
    >>> testEquivalence(net)
    True

Lstm with peepholes
    >>> net = buildMinimalLSTMNetwork(True)
    >>> testEquivalence(net)
    True

Mdlstm with peepholes
    >>> net = buildMinimalMDLSTMNetwork(True)
    >>> testEquivalence(net)
    True


TODO:
- heavily nested
- exotic module use

"""

__author__ = 'Tom Schaul, tom@idsia.ch'
_dependencies = ['arac']


from pybrain.tests.helpers import buildAppropriateDataset, epsilonCheck
from pybrain.tests import runModuleTestSuite

def testEquivalence(net):
    cnet = net.convertToFastNetwork()
    if cnet == None:
        return None
    ds = buildAppropriateDataset(net)
    if net.sequential:
        for seq in ds:
            net.reset()
            cnet.reset()
            for input, _ in seq:
                res = net.activate(input)
                cres = cnet.activate(input)
                if net.name == '22':
                    h = net['hidden0']
                    ch = cnet['hidden0']
                    print('ni', input, net.inputbuffer.T)
                    print('ci', input, cnet.inputbuffer.T)
                    print('hni', h.inputbuffer.T[0])
                    print('hci', ch.inputbuffer.T[0])
                    print('hnout', h.outputbuffer.T[0])
                    print('hcout', ch.outputbuffer.T[0])
                    print

    else:
        for input, _ in ds:
            res = net.activate(input)
            cres = cnet.activate(input)
    if epsilonCheck(sum(res - cres), 0.001):
        return True
    else:
        print('in-net', net.inputbuffer.T)
        print('in-arac', cnet.inputbuffer.T)
        print('out-net', net.outputbuffer.T)
        print('out-arac', cnet.outputbuffer.T)
        return (res, cres)


if __name__ == "__main__":
    runModuleTestSuite(__import__('__main__'))


########NEW FILE########
__FILENAME__ = aptativeresampling
__author__ = "Tom Schaul, tom@idsia.ch"


from scipy import median


class AdaptiveResampler(object):
    """ A simplified version of the uncertainty handling method described in
    Hansen, Niederberger, Guzzella and Koumoutsakos, 2009."""
            
    def __init__(self, f, batchsize, update_factor=1.5, threshold=0.2, max_resampling=None):
        self.f = f
        self.batchsize = batchsize
        self.update_factor = update_factor
        self.threshold = threshold
        self.max_resampling = max_resampling
        
        self.recents = [None]*self.batchsize
        self.resample_over = 1
        self.num_evals = 0
                
    def __call__(self, x):
        res = median([self.f(x) for _ in range(int(self.resample_over))])
        if self.num_evals%self.batchsize == 0 and self.num_evals > 0:
            alt_res = median([self.f(x) for _ in range(int(self.resample_over))])
            self._adaptResampling(res, alt_res)
            res = 0.5*res+0.5*alt_res
        self.recents[self.num_evals%self.batchsize] = res
        self.num_evals += 1
        return res
    
    def _adaptResampling(self, res, alt):
        #compute rank change
        rc = sum([(x-res) * (x-alt) < 0 for x in self.recents[1:]])
        if rc >= self.threshold*(self.batchsize-1):
            self.resample_over *= self.update_factor 
            if self.max_resampling is not None and self.resample_over > self.max_resampling:
                self.resample_over = self.max_resampling
        elif rc == 0:
            self.resample_over = max(self.resample_over/self.update_factor, 1)
        

    
def testnes():
    from pybrain.optimization.distributionbased.xnes import XNES
    from scipy import ones
    from random import gauss    
    import pylab  
    noise = 0.1
    x0 = ones(5)
    fun = lambda x: -sum(x**2) - gauss(0,noise)
    
    fun2 = AdaptiveResampler(fun, 10)
    l = XNES(fun, x0, maxEvaluations=1100, storeAllEvaluations=True)
    res = l.learn()
    print(sum(res[0]**2) )
    pylab.plot(map(abs, l._allEvaluations))
    
    l2 = XNES(fun2, x0, maxEvaluations=1100, storeAllEvaluations=True)
    res = l2.learn()
    print(sum(res[0]**2) )
    print(fun2.resample_over)
    pylab.plot(map(abs,l2._allEvaluations))
    pylab.semilogy()
    pylab.show()
    
if __name__ == "__main__":
    testnes()    
    
########NEW FILE########
__FILENAME__ = benchmark
__author__ = 'Justin Bayer, bayerj@in.tum.de'


from pybrain.datasets.dataset import DataSet


class BenchmarkDataSet(DataSet):

    def __init__(self):
        super(BenchmarkDataSet, self).__init__()
        self.addField('Average Reward', 1)
        self.addField('Episode Length', 1)
        self.linkFields(['Average Reward', 'Episode Length'])

    def _initialValues(self):
        return tuple(), dict()

########NEW FILE########
__FILENAME__ = handling
__author__ = 'Tom Schaul, tom@idsia.ch'

from xml.dom.minidom import parse, getDOMImplementation
from pybrain.utilities import fListToString
from scipy import zeros
import string

class XMLHandling:
    """ general purpose methods for reading, writing and editing XML files.
    This class should wrap all the XML-specific code, and then be subclassed
    by specialized readers/writers that use its methods.

    The priority is on readability and usability for the subclasses, not efficiency.
    """

    def __init__(self, filename, newfile):
        """ :key newfile: is the file to be read or is it a new file? """
        self.filename = filename
        if not newfile:
            self.dom = parse(filename)
            if self.dom.firstChild.nodeName != 'PyBrain':
                raise Exception, 'Not a correct PyBrain XML file'
        else:
            domimpl = getDOMImplementation()
            self.dom = domimpl.createDocument(None, 'PyBrain', None)
        self.root = self.dom.documentElement

    def save(self):
        file = open(self.filename, 'w')
        file.write(self.dom.toprettyxml())
        file.close()

    def readAttrDict(self, node, transform = None):
        """ read a dictionnary of attributes
        :key transform: optionally function transforming the attribute values on reading """
        args = {}
        for name, val in node.attributes.items():
            name = str(name)
            if transform != None:
                args[name] = transform(val, name)
            else:
                args[name] = val
        return args

    def writeAttrDict(self, node, adict, transform = None):
        """ read a dictionnary of attributes

        :key transform: optionally transform the attribute values on writing """
        for name, val in adict.items():
            if val != None:
                if transform != None:
                    node.setAttribute(name, transform(val, name))
                else:
                    node.setAttribute(name, val)

    def newRootNode(self, name):
        return self.newChild(self.root, name)

    def newChild(self, node, name):
        """ create a new child of node with the provided name. """
        elem = self.dom.createElement(name)
        node.appendChild(elem)
        return elem

    def addTextNode(self, node, text):
        tmp = self.dom.createTextNode(text)
        node.appendChild(tmp)

    def getChild(self, node, name):
        """ get the child with the given name """
        for n in node.childNodes:
            if name and n.nodeName == name:
                return n

    def getChildrenOf(self, node):
        """ get the element children """
        return filter(lambda x: x.nodeType == x.ELEMENT_NODE, node.childNodes)

    def findNode(self, name, index = 0, root = None):
        """ return the toplevel node with the provided name (if there are more, choose the
        index corresponding one). """
        if root == None:
            root = self.root
        for n in root.childNodes:
            if n.nodeName == name:
                if index == 0:
                    return n
                index -= 1
        return None

    def findNamedNode(self, name, nameattr, root = None):
        """ return the toplevel node with the provided name, and the fitting 'name' attribute. """
        if root == None:
            root = self.root
        for n in root.childNodes:
            if n.nodeName == name:
# modif JPQ
#                if 'name' in n.attributes:
                if n.attributes['name']:
# modif JPQ
#                    if n.attributes['name'] == nameattr:
                    if n.attributes['name'].value == nameattr:
                        return n
        return None

    def writeDoubles(self, node, l, precision = 6):
        self.addTextNode(node, fListToString(l, precision)[2:-1])

    def writeMatrix(self, node, m, precision = 6):
        for i, row in enumerate(m):
            r = self.newChild(node, 'row')
            self.writeAttrDict(r, {'number':str(i)})
            self.writeDoubles(r, row, precision)

    def readDoubles(self, node):
        dstrings = string.split(node.firstChild.data)
        return map(lambda s: float(s), dstrings)

    def readMatrix(self, node):
        rows = []
        for c in self.getChildrenOf(node):
            rows.append(self.readDoubles(c))
        if len(rows) == 0:
            return None
        res = zeros((len(rows), len(rows[0])))
        for i, r in enumerate(rows):
            res[i] = r
        return res


def baseTransform(val):
    """ back-conversion: modules are encoded by their name
    and classes by the classname """
    from pybrain.structure.modules.module import Module
    from inspect import isclass

    if isinstance(val, Module):
        return val.name
    elif isclass(val):
        return val.__name__
    else:
        return str(val)

########NEW FILE########
__FILENAME__ = networkreader
__author__ = 'Tom Schaul, tom@idsia.ch'


from handling import XMLHandling

# those imports are necessary for the eval() commands to find the right classes
import pybrain #@UnusedImport
from scipy import array #@UnusedImport


try:
    import arac.pybrainbridge #@UnusedImport
except ImportError:
    pass


class NetworkReader(XMLHandling):
    """ A class that can take read a network from an XML file """

    mothers = {}
    modules = {}

    @staticmethod
    def readFrom(filename, name = None, index = 0):
        """ append the network to an existing xml file

        :key name: if this parameter is specified, read the network with this name
        :key index: which network in the file shall be read (if there is more than one)
        """
        r = NetworkReader(filename, newfile = False)
        if name:
            netroot = r.findNamedNode('Network', name)
        else:
            netroot = r.findNode('Network', index)

        return r.readNetwork(netroot)

    def readNetwork(self, node):
        # TODO: why is this necessary?
        import pybrain.structure.networks.custom #@Reimport @UnusedImport
        nclass = eval(str(node.getAttribute('class')))
        argdict = self.readArgs(node)
        n = nclass(**argdict)
        n.name = node.getAttribute('name')

        for mnode in self.getChildrenOf(self.getChild(node, 'Modules')):
            m, inmodule, outmodule = self.readModule(mnode)
            if inmodule:
                n.addInputModule(m)
            elif outmodule:
                n.addOutputModule(m)
            else:
                n.addModule(m)

        mconns = self.getChild(node, 'MotherConnections')
        if mconns:
            for mcnode in self.getChildrenOf(mconns):
                m = self.readBuildable(mcnode)
                self.mothers[m.name] = m

        for cnode in self.getChildrenOf(self.getChild(node, 'Connections')):
            c, recurrent = self.readConnection(cnode)
            if recurrent:
                n.addRecurrentConnection(c)
            else:
                n.addConnection(c)

        n.sortModules()
        return n

    def readModule(self, mnode):
        if mnode.nodeName == 'Network':
            m = self.readNetwork(mnode)
        else:
            m = self.readBuildable(mnode)
        self.modules[m.name] = m
        inmodule = mnode.hasAttribute('inmodule')
        outmodule = mnode.hasAttribute('outmodule')
        return m, inmodule, outmodule

    def readConnection(self, cnode):
        c = self.readBuildable(cnode)
        recurrent = cnode.hasAttribute('recurrent')
        return c, recurrent

    def readBuildable(self, node):
        mclass = node.getAttribute('class')
        argdict = self.readArgs(node)
        try:
            m = eval(mclass)(**argdict)
        except:
            print('Could not construct', mclass)
            print('with arguments:', argdict)
            return None
        m.name = node.getAttribute('name')
        self.readParams(node, m)
        return m

    def readArgs(self, node):
        res = {}
        for c in self.getChildrenOf(node):
            val = c.getAttribute('val')
            if val in self.modules:
                res[str(c.nodeName)] = self.modules[val]
            elif val in self.mothers:
                res[str(c.nodeName)] = self.mothers[val]
            elif val != '':
                res[str(c.nodeName)] = eval(val)
        return res

    def readParams(self, node, m):
        import string
        pnode = self.getChild(node, 'Parameters')
        if pnode:
            params = eval(string.strip(pnode.firstChild.data))
            m._setParameters(params)

########NEW FILE########
__FILENAME__ = networkwriter
__author__ = 'Tom Schaul, tom@idsia.ch'

from inspect import isclass

from handling import XMLHandling
from pybrain.structure.connections.shared import SharedConnection
from pybrain.structure.networks.network import Network
from pybrain.structure.networks.recurrent import RecurrentNetwork
from pybrain.utilities import canonicClassString

# TODO: higher precision on writing parameters


class NetworkWriter(XMLHandling):
    """ A class that can take a network and write it to an XML file """

    @staticmethod
    def appendToFile(net, filename):
        """ append the network to an existing xml file """
        w = NetworkWriter(filename, newfile = False)
        netroot = w.newRootNode('Network')
        w.writeNetwork(net, netroot)
        w.save()

    @staticmethod
    def writeToFile(net, filename):
        """ write the network as a new xml file """
        w = NetworkWriter(filename, newfile = True)
        netroot = w.newRootNode('Network')
        w.writeNetwork(net, netroot)
        w.save()

    def writeNetwork(self, net, netroot):
        """ write a Network into a new XML node """
        netroot.setAttribute('name', net.name)
        netroot.setAttribute('class', canonicClassString(net))
        if net.argdict:
            self.writeArgs(netroot, net.argdict)

        # the modules
        mods = self.newChild(netroot, 'Modules')
        # first write the input modules (in order)
        for im in net.inmodules:
            self.writeModule(mods, im, True, im in net.outmodules)
        # now the output modules (in order)
        for om in net.outmodules:
            if om not in net.inmodules:
                self.writeModule(mods, om, False, True)
        # now the rest
        for m in net.modulesSorted:
            if m not in net.inmodules and m not in net.outmodules:
                self.writeModule(mods, m, False, False)

        # the motherconnections
        if len(net.motherconnections) > 0:
            mothers = self.newChild(netroot, 'MotherConnections')
            for m in net.motherconnections:
                self.writeBuildable(mothers, m)

        # the connections
        conns = self.newChild(netroot, 'Connections')
        for m in net.modulesSorted:
            for c in net.connections[m]:
                self.writeConnection(conns, c, False)
        if hasattr(net, "recurrentConns"):
            for c in net.recurrentConns:
                self.writeConnection(conns, c, True)

    def writeModule(self, rootnode, m, inmodule, outmodule):
        if isinstance(m, Network):
            mnode = self.newChild(rootnode, 'Network')
            self.writeNetwork(m, mnode)
        else:
            mnode = self.writeBuildable(rootnode, m)
        if inmodule:
            mnode.setAttribute('inmodule', 'True')
        elif outmodule:
            mnode.setAttribute('outmodule', 'True')

    def writeConnection(self, rootnode, c, recurrent):
        mnode = self.writeBuildable(rootnode, c)
        if recurrent:
            mnode.setAttribute('recurrent', 'True')

    def writeBuildable(self, rootnode, m):
        """ store the class (with path) and name in a new child. """
        mname = m.__class__.__name__
        mnode = self.newChild(rootnode, mname)
        mnode.setAttribute('name', m.name)
        mnode.setAttribute('class', canonicClassString(m))
        if m.argdict:
            self.writeArgs(mnode, m.argdict)
        if m.paramdim > 0 and not isinstance(m, SharedConnection):
            self.writeParams(mnode, m.params)
        return mnode

    def writeArgs(self, node, argdict):
        """ write a dictionnary of arguments """
        for name, val in argdict.items():
            if val != None:
                tmp = self.newChild(node, name)
                if isclass(val):
                    s = canonicClassString(val)
                else:
                    s = getattr(val, 'name', repr(val))
                tmp.setAttribute('val', s)

    def writeParams(self, node, params):
        # TODO: might be insufficient precision
        pnode = self.newChild(node, 'Parameters')
        self.addTextNode(pnode, str(list(params)))

########NEW FILE########
__FILENAME__ = mnist
import itertools
import os
import scipy
import struct

from pybrain.datasets import SupervisedDataSet


def labels(filename):
    fp = file(filename)
    magicnumber, length = struct.unpack('>ii', fp.read(8))
    assert magicnumber in (2049, 2051), ("Not an MNIST file: %i" % magicnumber)
    for _ in xrange(length):
        label, = struct.unpack('B', fp.read(1))
        yield label


def images(filename):
    fp = file(filename,'rb')
    chunk = fp.read(16)
    magicnumber, length, numrows, numcols = struct.unpack('>iiii', chunk)
    assert magicnumber in (2049, 2051), ("Not an MNIST file: %i" % magicnumber)
    imagesize = numrows * numcols
    for _ in xrange(length):
        imagestring = fp.read(imagesize)
        image = struct.unpack('B' * imagesize, imagestring)
        yield scipy.array(image)


def flaggedArrayByIndex(idx, length):
    arr = scipy.zeros(length)
    arr[idx] = 1.
    return arr


def makeMnistDataSets(path):
    """Return a pair consisting of two datasets, the first being the training
    and the second being the test dataset."""
    test = SupervisedDataSet(28 * 28, 10)
    test_image_file = os.path.join(path, 't10k-images-idx3-ubyte')
    test_label_file = os.path.join(path, 't10k-labels-idx1-ubyte')
    test_images = images(test_image_file)
    test_labels = (flaggedArrayByIndex(l, 10) for l in labels(test_label_file))

    for image, label in itertools.izip(test_images, test_labels):
        test.addSample(image, label)

    train = SupervisedDataSet(28 * 28, 10)
    train_image_file = os.path.join(path, 'train-images-idx3-ubyte')
    train_label_file = os.path.join(path, 'train-labels-idx1-ubyte')
    train_images = images(train_image_file)
    train_labels = (flaggedArrayByIndex(l, 10) for l in labels(train_label_file))
    for image, label in itertools.izip(train_images, train_labels):
        train.addSample(image, label)

    return train, test

########NEW FILE########
__FILENAME__ = datasettools
# This tool converts a sequential data set into a number of equally sized windows,
# to be used for supervised training.
__author__ = "Martin Felder"


from numpy import r_, array, isfinite
from pybrain.datasets import SequentialDataSet


def convertSequenceToTimeWindows(DSseq, NewClass, winsize):
    """ Converts a sequential classification dataset into time windows of fixed length.
    Assumes the correct class is given at the last timestep of each sequence. Incomplete windows at the
    sequence end are pruned. No overlap between windows.

    :arg DSseq: the sequential data set to cut up
    :arg winsize: size of the data window
    :arg NewClass: class of the windowed data set to be returned (gets initialised with indim*winsize, outdim)"""
    assert isinstance(DSseq, SequentialDataSet)
    #assert isinstance(DSwin, SupervisedDataSet)

    DSwin = NewClass(DSseq.indim * winsize, DSseq.outdim)
    nsamples = 0
    nseqs = 0
    si = r_[DSseq['sequence_index'].flatten(), DSseq.endmarker['sequence_index']]
    for i in xrange(DSseq.getNumSequences()):
        # get one sequence as arrays
        input = DSseq['input'][si[i]:si[i + 1], :]
        target = DSseq['target'][si[i]:si[i + 1], :]
        nseqs += 1
        # cut this sequence into windows, assuming class is given at the last step of each sequence
        for k in range(winsize, input.shape[0], winsize):
            inp_win = input[k - winsize:k, :]
            tar_win = target[k - 1, :]
            DSwin.addSample(inp_win.flatten(), tar_win.flatten())
            nsamples += 1
            ##print("added sample %d from sequence %d: %d - %d" %( nsamples, nseqs, k-winsize, k-1))
    print("samples in original dataset: ", len(DSseq))
    print("window size * nsamples = ", winsize * nsamples)
    print("total data points in original data: ", len(DSseq) * DSseq.indim)
    print("total data points in windowed dataset: ", len(DSwin) * DSwin.indim)
    return DSwin

def windowSequenceEval(DS, winsz, result):
    """ take results of a window-based classification and assess/plot them on the sequence
    WARNING: NOT TESTED!"""
    si_old = 0
    idx = 0
    x = []
    y = []
    seq_res = []
    for i, si in enumerate(DS['sequence_index'][1:].astype(int)):
        tar = DS['target'][si - 1]
        curr_x = si_old
        correct = 0.
        wrong = 0.
        while curr_x < si:
            x.append(curr_x)
            if result[idx] == tar:
                correct += 1.
                y += [1., 1.]
            else:
                wrong += 1.
                y += [0., 0.]
            idx += 1
            #print("winidx: ", idx)
            curr_x += winsz
            x.append(curr_x)

        seq_res.append(100. * correct / (correct + wrong))
        print("sequence %d correct: %g12.2%%" % (i, seq_res[-1]))

    seq_res = array(seq_res)
    print("total fraction of correct sequences: ", 100. * float((seq_res >= 0.5).sum()) / seq_res.size)


class DataSetNormalizer(object):
    """ normalize a dataset according to a stored LIBSVM normalization file """
    def __init__(self, fname=None, meanstd=False):
        self.dim = 0
        self.meanstd = meanstd
        if fname is not None:
            self.load(fname)

    def load(self, fname):
        f = file(fname)
        c = []
        # the first line determines whether we interpret the file as
        # giving min/max of features or mean/std
        x = f.readline()
        self.meanstd = False if x == 'x' else True

        # the next line gives the normalization bounds
        bounds = array(f.readline().split()).astype(float)
        for line in f:
            c.append(array(line.split()).astype(float)[1:])
        self.dim = len(c)
        c = array(c)
        self.par1 = c[:, 0]
        self.par2 = c[:, 1]
        self.scale = (bounds[1] - bounds[0]) / (c[:, 1] - c[:, 0])
        self.newmin = bounds[0]
        self.newmax = bounds[1]

    def save(self, fname):
        f = file(fname, "w+")
        f.write('x\n')
        f.write('%g %g' % (self.newmin, self.newmax))
        for i in range(self.dim):
            f.write('%d %g %g' % (i + 1, self.par1[i], self.par2[i]))
        f.close()

    def normalizePattern(self, y):
        return (y - self.par1) * self.scale + self.newmin

    def normalize(self, ds, field='input'):
        """ normalize dataset or vector wrt. to stored min and max """
        if self.dim <= 0:
            raise IndexError("No normalization parameters defined!")
        dsdim = ds[field].shape[1]
        if self.dim != dsdim:
            raise IndexError("Dimension of normalization params does not match DataSet field!")
        newfeat = ds[field]
        if self.meanstd:
            for i in range(dsdim):
                divisor = self.par2[i] if self.par2[i] > 0 else 1.0
                newfeat[:, i] = (newfeat[:, i] - self.par1[i]) / divisor
        else:
            for i in range(dsdim):
                scale = self.scale[i] if isfinite(self.scale[i]) else 1.0
                newfeat[:, i] = (newfeat[:, i] - self.par1[i]) * scale + self.newmin
        ds.setField(field, newfeat)

    def calculate(self, ds, bounds=[-1, 1], field='input'):
        self.dim = ds[field].shape[1]
        if self.meanstd:
            self.par1 = ds[field].mean(axis=0)
            self.par2 = ds[field].std(axis=0)
        else:
            self.par1 = ds[field].min(axis=0)
            self.par2 = ds[field].max(axis=0)
            self.scale = (bounds[1] - bounds[0]) / (self.par2 - self.par1)
        self.newmin = bounds[0]
        self.newmax = bounds[1]



########NEW FILE########
__FILENAME__ = example_tools
#########################################################################
# Reinforcement Tools for printing, saving and loading for RL examples 
# 
# Requirements: scipy
#
# Author: Frank Sehnke, sehnke@in.tum.de
#########################################################################

from cPickle import load, dump
from scipy import array, sqrt
from pylab import errorbar, show


class ExTools():
    agent = None
    loadName = "none.wgt"
    saveName = "none.wgt"
    resuName = "none.dat"
    rl = []
    rll = []

    def __init__(self, batch = 2, prnts = 1, kind = "optimizer"):
        self.batch = batch
        self.prnts = prnts
        self.kind = kind

    # Method for loading a weight matrix and initialize the network
    def loadWeights(self, filename):
        filepointer = file(filename)
        self.agent.learner.current = load(filepointer)
        filepointer.close()
        self.agent.learner.gd.init(self.agent.learner.current)
        self.agent.learner.epsilon = 0.2
        self.agent.learner.initSigmas()

    # Method for saving the weight matrix    
    def saveWeights(self, filename, w):
        filepointer = file(filename, 'w+')
        dump(w, filepointer)
        filepointer.close()

    # Method for saving the weight matrix    
    def saveResults(self, filename, results):
        filepointer = file(filename, 'w+')
        dump(results, filepointer)
        filepointer.close()

    def printResults(self,resList, runs, updates):
        if self.kind == "optimizer":
            rLen = len(resList)
            avReward = array(resList).sum()/rLen
            print("Parameters:", self.agent.learner._bestFound())
            print("Experiment:", runs,
                " Evaluation:", (updates+1)*self.batch*self.prnts,
                " BestReward:", self.agent.learner.bestEvaluation,
                " AverageReward:", avReward)
            print()
            self.rl.append(avReward)
        else:
            avReward = resList
            #print("Parameters: ", self.agent.learner._bestFound())
            print(
                "Step: ", runs, "/", (updates+1)*self.batch*self.prnts,
                #"Best: ", self.agent.learner.bestEvaluation,
                "Base: ", avReward)
            #print()
            self.rl.append(avReward)

    def addExps(self):
        self.rll.append(self.rl)
        self.rl = []

    def showExps(self):
        nEx = len(self.rll)
        self.rll = array(self.rll)
        r = self.rll.sum(axis=0)/nEx
        d = self.rll-r
        v = (d**2).sum(axis=0)
        v = v/nEx
        stand = sqrt(v)
        errorbar(array(range(len(self.rll[0])))*self.prnts*self.batch+self.prnts*self.batch,r,stand)
        show()

########NEW FILE########
__FILENAME__ = filehandling
__author__ = 'Tom Schaul, tom@idsia.ch'

import os
import pickle


def getAllFilesIn(dir, tag='', extension='.pickle'):
    """ return a list of all filenames in the specified directory
    (with the given tag and/or extension). """
    allfiles = os.listdir(dir)
    res = []
    for f in allfiles:
        if f[-len(extension):] == extension and f[:len(tag)] == tag:
            res.append(f[:-len(extension)])
    return res


def selectSome(strings, requiredsubstrings=[], requireAll=True):
    """ Filter the list of strings to only contain those that have at least
    one of the required substrings. """
    if len(requiredsubstrings) == 0:
        return strings
    res = []
    for s in strings:
        if requireAll:
            bad = False
            for rs in requiredsubstrings:
                if s.find(rs) < 0:
                    bad = True
                    break
            if not bad:
                res.append(s)
        else:
            for rs in requiredsubstrings:
                if s.find(rs) >= 0:
                    res.append(s)
                    break
    return res


def pickleDumpDict(name, d):
    """ pickle-dump a variable into a file """
    try:
        f = open(name + '.pickle', 'w')
        pickle.dump(d, f)
        f.close()
        return True
    except Exception, e:
        print('Error writing into', name, ':', str(e))
        return False


def pickleReadDict(name):
    """ pickle-read a (default: dictionnary) variable from a file """
    try:
        f = open(name + '.pickle')
        val = pickle.load(f)
        f.close()
    except Exception, e:
        print('Nothing read from', name, ':', str(e))
        val = {}
    return val


def addToDictFile(name, key, data, verbose=False):
    if verbose:
        print('.',)
    d = pickleReadDict(name)
    if key not in d:
        d[key] = []
    d[key].append(data)
    pickleDumpDict(name, d)
    if verbose:
        print(':')



########NEW FILE########
__FILENAME__ = fisher
__author__ = 'Tom Schaul, tom@idsia.ch'

from pybrain.utilities import blockCombine
from scipy import mat, dot, outer
from scipy.linalg import inv, cholesky



def calcFisherInformation(sigma, invSigma=None, factorSigma=None):
    """ Compute the exact Fisher Information Matrix of a Gaussian distribution,
    given its covariance matrix.
    Returns a list of the diagonal blocks. """
    if invSigma == None:
        invSigma = inv(sigma)
    if factorSigma == None:
        factorSigma = cholesky(sigma)
    dim = sigma.shape[0]
    fim = [invSigma]
    for k in range(dim):
        D = invSigma[k:, k:].copy()
        D[0, 0] += factorSigma[k, k] ** -2
        fim.append(D)
    return fim



def calcInvFisher(sigma, invSigma=None, factorSigma=None):
    """ Efficiently compute the exact inverse of the FIM of a Gaussian.
    Returns a list of the diagonal blocks. """
    if invSigma == None:
        invSigma = inv(sigma)
    if factorSigma == None:
        factorSigma = cholesky(sigma)
    dim = sigma.shape[0]

    invF = [mat(1 / (invSigma[-1, -1] + factorSigma[-1, -1] ** -2))]
    invD = 1 / invSigma[-1, -1]
    for k in reversed(range(dim - 1)):
        v = invSigma[k + 1:, k]
        w = invSigma[k, k]
        wr = w + factorSigma[k, k] ** -2
        u = dot(invD, v)
        s = dot(v, u)
        q = 1 / (w - s)
        qr = 1 / (wr - s)
        t = -(1 + q * s) / w
        tr = -(1 + qr * s) / wr
        invF.append(blockCombine([[qr, tr * u], [mat(tr * u).T, invD + qr * outer(u, u)]]))
        invD = blockCombine([[q , t * u], [mat(t * u).T, invD + q * outer(u, u)]])

    invF.append(sigma)
    invF.reverse()
    return invF


########NEW FILE########
__FILENAME__ = functions
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import array, exp, tanh, clip, log, dot, sqrt, power, pi, tan, diag, rand, real_if_close
from scipy.linalg import inv, det, svd, logm, expm2


def semilinear(x):
    """ This function ensures that the values of the array are always positive. It is
        x+1 for x=>0 and exp(x) for x<0. """
    try:
        # assume x is a numpy array
        shape = x.shape
        x.flatten()
        x = x.tolist()
    except AttributeError:
        # no, it wasn't: build shape from length of list
        shape = (1, len(x))
    def f(val):
        if val < 0:
            # exponential function for x<0
            return safeExp(val)
        else:
            # linear function for x>=0
            return val + 1.0
    return array(map(f, x)).reshape(shape)


def semilinearPrime(x):
    """ This function is the first derivative of the semilinear function (above).
        It is needed for the backward pass of the module. """
    try:
        # assume x is a numpy array
        shape = x.shape
        x.flatten()
        x = x.tolist()
    except AttributeError:
        # no, it wasn't: build shape from length of list
        shape = (1, len(x))
    def f(val):
        if val < 0:
            # exponential function for x<0
            return safeExp(val)
        else:
            # linear function for x>=0
            return 1.0
    return array(map(f, x)).reshape(shape)


def safeExp(x):
    """ Bounded range for the exponential function (won't produce inf or NaN). """
    return exp(clip(x, -500, 500))


def sigmoid(x):
    """ Logistic sigmoid function. """
    return 1. / (1. + safeExp(-x))


def sigmoidPrime(x):
    """ Derivative of logistic sigmoid. """
    tmp = sigmoid(x)
    return tmp * (1 - tmp)


def tanhPrime(x):
    """ Derivative of tanh. """
    tmp = tanh(x)
    return 1 - tmp * tmp


def ranking(R):
    """ Produces a linear ranking of the values in R. """
    l = sorted(list(enumerate(R)), cmp=lambda a, b: cmp(a[1], b[1]))
    l = sorted(list(enumerate(l)), cmp=lambda a, b: cmp(a[1], b[1]))
    return array(map(lambda kv: kv[0], l))


def expln(x):
    """ This continuous function ensures that the values of the array are always positive.
        It is ln(x+1)+1 for x >= 0 and exp(x) for x < 0. """
    def f(val):
        if val < 0:
            # exponential function for x < 0
            return exp(val)
        else:
            # natural log function for x >= 0
            return log(val + 1.0) + 1
    try:
        result = array(map(f, x))
    except TypeError:
        result = array(f(x))

    return result


def explnPrime(x):
    """ This function is the first derivative of the expln function (above).
        It is needed for the backward pass of the module. """
    def f(val):
        if val < 0:
            # exponential function for x<0
            return exp(val)
        else:
            # linear function for x>=0
            return 1.0 / (val + 1.0)
    try:
        result = array(map(f, x))
    except TypeError:
        result = array(f(x))

    return result


def multivariateNormalPdf(z, x, sigma):
    """ The pdf of a multivariate normal distribution (not in scipy).
    The sample z and the mean x should be 1-dim-arrays, and sigma a square 2-dim-array. """
    assert len(z.shape) == 1 and len(x.shape) == 1 and len(x) == len(z) and sigma.shape == (len(x), len(z))
    tmp = -0.5 * dot(dot((z - x), inv(sigma)), (z - x))
    res = (1. / power(2.0 * pi, len(z) / 2.)) * (1. / sqrt(det(sigma))) * exp(tmp)
    return res


def simpleMultivariateNormalPdf(z, detFactorSigma):
    """ Assuming z has been transformed to a mean of zero and an identity matrix of covariances.
    Needs to provide the determinant of the factorized (real) covariance matrix. """
    dim = len(z)
    return exp(-0.5 * dot(z, z)) / (power(2.0 * pi, dim / 2.) * detFactorSigma)


def multivariateCauchy(mu, sigma, onlyDiagonal=True):
    """ Generates a sample according to a given multivariate Cauchy distribution. """
    if not onlyDiagonal:
        u, s, d = svd(sigma)
        coeffs = sqrt(s)
    else:
        coeffs = diag(sigma)
    r = rand(len(mu))
    res = coeffs * tan(pi * (r - 0.5))
    if not onlyDiagonal:
        res = dot(d, dot(res, u))
    return res + mu


def approxChiFunction(dim):
    """ Returns Chi (expectation of the length of a normal random vector)
    approximation according to: Ostermeier 1997. """
    dim = float(dim)
    return sqrt(dim) * (1 - 1 / (4 * dim) + 1 / (21 * dim ** 2))


def sqrtm(M):
    """ Returns the symmetric semi-definite positive square root of a matrix. """
    r = real_if_close(expm2(0.5 * logm(M)), 1e-8)
    return (r + r.T) / 2


########NEW FILE########
__FILENAME__ = gridsearch
__author__ = 'Michael Isik'


from pybrain.tools.validation import CrossValidator
from numpy import linspace, append, ones, zeros, array, where, apply_along_axis
import copy

class GridSearch2D:
    """ Abstract class providing a method for searching optimal metaparmeters
        of a training algorithm.

        It is especially designed for searching two metaparameters, which
        explains the "2D" in "GridSearch2D". To use it, one must create a
        subclass and implement the _validate() method.
        See GridSearchCostGamma for an example.

        On construction, the minima, maxima and granularity of the search space
        must be defined. Then a list of jobs are calculated. Each job stands for
        a metaparameter setting, that will be validated in advance.
        The special job execution order makes it possible to visualize the
        progress of the Gridsearch at several intermediate steps.
    """
    def __init__(self, min_params, max_params, n_steps=7, **kwargs):
        """ :key min_params: Tuple of two elements specifying the minima
                               of the two metaparameters
            :key max_params: Tuple of two elements specifying the minima
                               of the two metaparameters
            :key max_param:  Tuple of two elements, specifying the number of
                               steps between the minimum and maximum of each search
                               dimension. Alternative, specify a scalar to set
                               the same granularity for each dimension.
            :key **kwargs:   See setArgs()
        """
        assert len(min_params) == len(max_params)

        self._min_params = array(min_params, float)
        self._max_params = array(max_params, float)
        self._n_dim = len(min_params)
        self._n_steps = append([], n_steps) * ones(self._n_dim)
        self._range = self._max_params - self._min_params
        self._performances = {}
        self._verbosity = 0
        self.setArgs(**kwargs)

    def setArgs(self, **kwargs):
        """ :key **kwargs:
                verbosity : set verbosity
        """
        for key, value in kwargs.items():
            if key in ("verbose", "verbosity", "ver", "v"):
                self._verbosity = value

    def getPerformances(self):
        """ Returns the performances calculated so far. They are stored inside
            a dictionary, mapping jobs to performances. A job is a tuple of
            metaparameters.
        """
        return copy.copy(self._performances)

    def search(self):
        """ The main search method, that validates all calculated metaparameter
            settings (=jobs) by calling the abstract _validate() method.
            After enough new jobs were validated in order to visualize a grid,
            the _onStep() callback method is called.
        """
        jobs = self._calculateJobs()
        perfs = self._performances
        for line in jobs:
            for params in line:
                perf = self._validate(params)
                perfs[params] = perf
                if self._verbosity > 0:
                    print("validated:", params, " performance = ", perf)

            self._onStep()

        max_idx = array(perfs.values()).argmax()
        return perfs.keys()[max_idx]

    def _validate(self, params):
        """ Abstract validation method. Should validate the supplied metaparameters,
            and return this performance on the learning task.
        """
        raise NotImplementedError()

    def _onStep(self):
        """ Callback function, that gets called after a gridvisualization
            could be updated because of new performance values.
            Overwrite this function to make use of it.
        """
        pass

    def _calculateJobs(self):
        """ Calculate and return the metaparameter settings to be validated (=jobs).
        """
        ndim = len(self._min_params)
        linspaces = []
        for i in range(ndim):
            linspaces.append(
                self._permuteSequence(
                    list(linspace(self._min_params[i], self._max_params[i], self._n_steps[i]))))
#        print(linspaces; exit(0))
#        linspaces = array(linspaces,float)
        nr_c = len(linspaces[0])
        nr_g = len(linspaces[1])
        i = 0
        j = 0
        jobs = []

        while i < nr_c or j < nr_g:
            if i / float(nr_c) < j / float(nr_g):
                line = []
                for k in range(0, j):
                    line.append((linspaces[0][i], linspaces[1][k]))
                i += 1
                jobs.append(line)
            else:
                line = []
                for k in range(0, i):
                    line.append((linspaces[0][k], linspaces[1][j]))
                j += 1
                jobs.append(line)
        return jobs
#        return grid


    def _permuteSequence(self, seq):
        """ Helper function for calculating the job list
        """
        n = len(seq)
        if n <= 1: return seq

        mid = int(n / 2)
        left = self._permuteSequence(seq[:mid])
        right = self._permuteSequence(seq[mid + 1:])

        ret = [seq[mid]]
        while left or right:
            if left:  ret.append(left.pop(0))
            if right: ret.append(right.pop(0))

        return ret




class GridSearchDOE:
    """ Abstract class providing a method for searching optimal metaparmeters
        of a training algorithm after the DOE principle.
        Read: "Parameter selection for support vector machines"
              Carl Staelin, Senior Member IEEE
        for more information

        To use this class, one must create a subclass and implement the
        _validate() method. See GridSearchDOECostGamma for an example.
    """
    _doe_pat = array([ [ -1.0 , +1.0 ]                , [ 0.0, +1.0 ]                , [ +1.0 , +1.0 ] ,
                                       [ -0.5, +0.5 ] , [ +0.5, +0.5 ] ,
                       [ -1.0 , 0.0 ]                , [ 0.0, 0.0 ]                , [ +1.0 , 0.0 ] ,
                                       [ -0.5, -0.5 ] , [ +0.5, -0.5 ] ,
                       [ -1.0 , -1.0 ]                , [ 0.0, -1.0 ]                , [ +1.0 , -1.0 ]   ])

    def __init__(self, min_params, max_params, n_iterations=5, **kwargs):
        """ See GridSearch.init()
        """
        assert len(min_params) == len(max_params)
        self._min_params = array(min_params, float)
        self._max_params = array(max_params, float)
        self._n_iterations = n_iterations
        self._refine_factor = 2.
        self._range = self._max_params - self._min_params
        self._performances = {}
        self._verbosity = 0
        self.setArgs(**kwargs)

    def setArgs(self, **kwargs):
        """ :key **kwargs:
                verbosity : set verbosity
        """
        for key, value in kwargs.items():
            if key in ("verbose", "ver", "v"):
                self._verbosity = value

    def search(self):
        """ The main search method, that validates all calculated metaparameter
            settings by calling the abstract _validate() method.
        """
        self._n_params = len(self._min_params)

        center = self._min_params + self._range / 2.
        for level in range(self._n_iterations):
            grid = self._calcGrid(center, level)
            local_perf = apply_along_axis(self._validateWrapper, 1, grid)

            max_idx = local_perf.argmax()
            center = grid[max_idx]
            if self._verbosity > 0:
                print
                print("Found maximum at:", center, "   performance = ", local_perf[max_idx])
                print

        return center


    def _validateWrapper(self, params):
        """ Helper function that wraps the _validate() method.
        """
        perf = self._validate(params)
        if self._verbosity > 0:
            print("validated:", params, " performance = ", perf)
        self._performances[tuple(params)] = perf
        return perf

    def _calcGrid(self, center, level):
        """ Calculate the next grid to validate.

            :arg center: The central position of the grid
            :arg level:  The iteration number
        """
        local_range = self._range / (self._refine_factor ** level)
        scale = local_range / 2
        translation = center
        grid = self._doe_pat * scale + translation

        grid = self._moveGridIntoBounds(grid)

        return grid

    def _moveGridIntoBounds(self, grid):
        """ If the calculated grid is out of bounds,
            this method moves it back inside, and returns the new grid.
        """
        grid = array(grid)
        local_min_params = grid.min(axis=0)
        local_max_params = grid.max(axis=0)
        tosmall_idxs, = where(local_min_params < self._min_params)
        togreat_idxs, = where(local_max_params > self._max_params)
        translation = zeros(self._n_params)
        for idx in tosmall_idxs:
            translation[idx] = self._min_params[idx] - local_min_params[idx]
        for idx in togreat_idxs:
            translation[idx] = self._max_params[idx] - local_max_params[idx]
        grid += translation
        return grid

    def _validate(self, params):
        """ Abstract validation method. Should validate the supplied metaparameters,
            and return this performance on the learning task.
        """
        raise NotImplementedError()


class GridSearchCostGamma(GridSearch2D):
    """ GridSearch class, that searches for the optimal cost and gamma values
        of a support vector machine. See SVMTrainer and the SVM module for
        more information on these metaparameters.
        The parameters are searched in log2-space. Crossvalidation is used
        to determine the performance values.
    """
    def __init__(self, trainer, dataset, min_params=[-5, -15], max_params=[15, 3], n_steps=7, **kwargs):
        """ The parameter boundaries are specified in log2-space.

            :arg trainer: The SVM trainer including the SVM module.
                            (Could be any kind of trainer and module)
            :arg dataset: Dataset used for crossvalidation
        """
        GridSearch2D.__init__(self, min_params, max_params, n_steps)
        self._trainer = trainer
        self._dataset = dataset

        self._validator_kwargs = {}
        self._n_folds = 5
        self.setArgs(**kwargs)


    def setArgs(self, **kwargs):
        """ :key **kwargs:
                nfolds    : Number of folds of crossvalidation
                max_epochs: Maximum number of epochs for training
                verbosity : set verbosity
        """
        for key, value in kwargs.items():
            if key in ("folds", "nfolds"):
                self._n_folds = int(value)
            elif key in ("max_epochs"):
                self._validator_kwargs['max_epochs'] = value
            elif key in ("verbose", "ver", "v"):
                self._verbosity = value
            else:
                GridSearch2D.setArgs(self, **{key:value})

    def _validate(self, params):
        """ The overridden validate function, that uses cross-validation in order
            to determine the params' performance value.
        """
        trainer = self._getTrainerForParams(params)
        return CrossValidator(trainer, self._dataset, self._n_folds, **self._validator_kwargs).validate()

    def _getTrainerForParams(self, params):
        """ Returns a trainer, loaded with the supplied metaparameters.
        """
        trainer = copy.deepcopy(self._trainer)
        trainer.setArgs(cost=2 ** params[0], gamma=2 ** params[1], ver=0)
        return trainer




class GridSearchDOECostGamma(GridSearchDOE):
    """ Same as GridSearchCostGamma, except, that it uses the Design of Experiment (DOE)
        algorithm.
    """
    def __init__(self, trainer, dataset, min_params=[-5, -15], max_params=[15, 3], n_iterations=5, **kwargs):
        """ See GridSearchCostGamma and GridSearchDOE """
        GridSearchDOE.__init__(self, min_params, max_params, n_iterations)
        assert len(min_params) == 2
        self._trainer = trainer
        self._dataset = dataset

        self._n_folds = 5
        self._validator_kwargs = {}
        self.setArgs(**kwargs)

    def setArgs(self, **kwargs):
        """ See GridSearchCostGamma """
        for key, value in kwargs.items():
            if key in ("folds", "nfolds"):
                self._n_folds = int(value)
            elif key in ("max_epochs"):
                self._validator_kwargs['max_epochs'] = value
            else:
                GridSearchDOE.setArgs(self, **{key:value})


    def _validate(self, params):
        """ See GridSearchCostGamma """
        glob_idx = tuple(params)
        perf = self._performances

        if not perf.has_key(glob_idx):
            trainer = self._getTrainerForParams(params)
            local_perf = CrossValidator(trainer, self._dataset, self._n_folds, **self._validator_kwargs).validate()
            perf[glob_idx] = local_perf
        else:
            local_perf = perf[glob_idx]
        return local_perf


    def _getTrainerForParams(self, params):
        """ See GridSearchCostGamma """
        trainer = copy.deepcopy(self._trainer)
        trainer.setArgs(cost=2 ** params[0], gamma=2 ** params[1], ver=0)
        return trainer






########NEW FILE########
__FILENAME__ = ibp
""" A few utilities for Indian Buffet Processes. """

__author__ = 'Tom Schaul, tom@idsia.ch'


from scipy import zeros, rand, array, sqrt
from numpy.random import beta


def leftordered(M):
    """ Returns the given matrix in left-ordered-form. """
    l = list(M.T)
    l.sort(key=tuple)
    return array(l)[::-1].T


def generateIBP(customers, alpha=10, reducedprop=1.):
    """ Simple implementation of the Indian Buffet Process. Generates a binary matrix with
    customers rows and an expected number of columns of alpha * sum(1,1/2,...,1/customers).
    This implementation uses a stick-breaking construction.
    An additional parameter permits reducing the expected number of times a dish is tried. """
    # max number of dishes is distributed according to Poisson(alpha*sum(1/i))
    _lambda = alpha * sum(1. / array(range(1, customers + 1)))
    alpha /= reducedprop

    # we give it 2 standard deviations as cutoff
    maxdishes = int(_lambda + sqrt(_lambda) * 2) + 1

    res = zeros((customers, maxdishes), dtype=bool)
    stickprops = beta(alpha, 1, maxdishes) # nu_i

    currentstick = 1.
    dishesskipped = 0

    for i, nu in enumerate(stickprops):
        currentstick *= nu
        dishestaken = rand(customers) < currentstick * reducedprop
        if sum(dishestaken) > 0:
            res[:, i - dishesskipped] = dishestaken
        else:
            dishesskipped += 1

    return res[:, :maxdishes - dishesskipped]


def testIBP():
    """ Plot matrices generated by an IBP, for a few different settings. """

    from pybrain.tools.plotting.colormaps import ColorMap
    import pylab

    # always 50 customers
    n = 50

    # define parameter settings
    ps = [(10, 0.1),
          (10,), (50,),
          (50, 0.5),
          ]

    # generate a few matrices, on for each parameter setting
    ms = []
    for p in ps:
        if len(p) > 1:
            m = generateIBP(n, p[0], p[1])
        else:
            m = generateIBP(n, p[0])
        ms.append(leftordered(m))

    # plot the matrices
    for m in ms:
        ColorMap(m, pixelspervalue=3)
    pylab.show()


if __name__ == '__main__':
    testIBP()


########NEW FILE########
__FILENAME__ = kwargsprocessor
__author__ = 'Michael Isik'


class KWArgDsc(object):
    def __init__(self, name, **kwargs):
        self.name = name
        self.private = False
        self.mandatory = False
        keys = ['private', 'default', 'mandatory']
        for key in keys:
            if kwargs.has_key(key):
                setattr(self, key, kwargs[key])

        assert not (self.mandatory and self.hasDefault())

    def hasDefault(self):
        return hasattr(self, 'default')



class KWArgsProcessor(object):
    def __init__(self, obj, kwargs):
#       self.argDscs = []
        self._object = obj
        self._obj_kwargs = kwargs

    def add(self, name, **kwargs):
        kwargDsc = KWArgDsc(name, **kwargs)
#        self.argDscs.append(ad)

        # determine attribute name
        name = kwargDsc.name
        if kwargDsc.private:
            attrname = "_" + name
        else:
            attrname = name

        # set the objects attribute
        if self._obj_kwargs.has_key(name):
            # set attribute supplied value
            setattr(self._object, attrname, self._obj_kwargs[name])
        elif kwargDsc.hasDefault():
            # set attribute to default value
            setattr(self._object, attrname, kwargDsc.default)
        elif kwargDsc.mandatory:
            raise KeyError('Mandatory Keyword argument "%s" missing!' % name)
        # del kwargs[name]



if __name__ == '__main__':
    class C(object):

        b = property(lambda self: self._b) # b will be readonly

        def __init__(self, **kwargs):
            kp = KWArgsProcessor(self, kwargs)
            kp.add('simple')

            kp.add('a', default=33)
            kp.add('b', private=True, default=55)
            kp.add('c', default=self.a + self._b)
            kp.add('m', mandatory=True)

        def __str__(self):
            return str(dict(self.__dict__))

    c1 = C(m=1)
    print('c1 =', c1)

    c2 = C(m=1, a=1, b=2)
    print('c2 =', c2)

    c3 = C(m=1, simple="hallo", a=11, b=22, c=55)
    print('c3 =', c3)


    print("\nc3.b = ", c3.b)

    try:
        C() # will raise KeyError because mandatory keyword argument "m" is missing
    except KeyError, k:
        print(k)




########NEW FILE########
__FILENAME__ = mixtureofgaussian
__author__ = 'Frank Sehnke, sehnke@in.tum.de'


from scipy import random, zeros, ones, sqrt, exp, sin, cos, log

stND = zeros(1000)
for i in range(1000):
    x = -4.0 + float(i) * 8.0 / 1000.0
    stND[i] = 1.0 / 2.51 * exp(-0.5 * (x) ** 2)

class MixtureOfGaussians:
    def __init__(self, typ, numOGaus=10, alphaA=0.02, alphaM=0.02, alphaS=0.02):
        self.typ = typ
        self.alphaA = alphaA
        self.alphaM = alphaM
        self.alphaS = alphaS
        self.minSig = 0.000001
        self.numOGaus = numOGaus #Number of Gaussians
        self.rangeMin = -20.0
        self.rangeMax = 20.0
        self.epsilon = (self.rangeMax - self.rangeMin) / (sqrt(2.0) * float(self.numOGaus - 1)) #Initial value of sigmas

        self.propFakt = 1.0 / float(self.numOGaus)
        self.distFakt = 1.0 / float(self.numOGaus - 1)
        self.distRange = self.rangeMax - self.rangeMin

        self.sigma = ones(self.numOGaus)
        self.mue = zeros(self.numOGaus)
        self.alpha = ones(self.numOGaus)
        self.sigma *= self.epsilon
        self.alpha /= float(self.numOGaus)
        self.alpha = self.invSigmo(self.alpha)
        for i in range(self.numOGaus):
            self.mue[i] = self.distRange * float(i) * self.distFakt + self.rangeMin
        self.baseline = 0.0
        self.best = 0.000001

    def getStND(self, x, mue=0.0, sig=1.0):
        x = (x - mue) / sig
        if abs(x) >= 4.0: return 0.000000001
        x = int((x + 4.0) / 8.0 * 1000)
        return stND[x] / sig

    #generate complete mixture for plotting
    def plotGaussian(self, col, dm):
        from pylab import plot
        if dm == "max": scal = 1.0
        else: scal = 10.0
        ret = []
        xList = []
        for i in range(1000):
            x = float(i) * 1.5 * self.distRange / 1000.0 + self.rangeMin * 1.5
            xList.append(x)
            ret.append(self.getGaus(self.alpha, self.mue, self.sigma, x) * scal)
        plot(xList, ret, col)

    #get mixture for point x
    def getGaus(self, alpha, mue, sigma, x):
        sigmoA = self.sigmo(self.alpha)
        dens = zeros(self.numOGaus)
        for g in range(self.numOGaus):
            dens[g] = self.getStND(x, mue[g], sigma[g])
        return sum(sigmoA * dens)

    #reward functions for testing
    def testRewardFunction(self, x, typ, noise=0.000001):
        if typ == "growSin":
            return (sin((x - self.rangeMin) / 3.0) + 1.5 + x / self.distRange) / 4.0 + random.normal(0, noise)
        if typ == "rastrigin":
            n = x / self.distRange * 10.0
            if abs(n) > 5.0: n = 0.0
            # FIXME: imprecise reimplementation of the Rastrigin function that exists already
            # in rl/environments/functions...
            return (20.0 + n ** 2 - 10.0 * cos(2.0 * 3.1416 * n)) / 55.0 + random.normal(0, noise)
        if typ == "singleGaus":
            return self.getStND(x) + random.normal(0, noise)
        return 0.0

    def drawSample(self, dm):
        sum = 0.0
        rndFakt = random.random()
        if dm == "max":
            for g in range(self.numOGaus):
                sum += self.sigmo(self.alpha[g])
                if rndFakt < sum:
                    if self.sigma[g] < self.minSig: self.sigma[g] = self.minSig
                    x = random.normal(self.mue[g], self.sigma[g])
                    break
            return x
        if dm == "dist":
            return rndFakt * self.distRange + self.rangeMin
        return 0.0

    def learn(self, x, y, dm="max", typ="logLiklihood"):
        #learning overalls
        norm = zeros(self.numOGaus)
        nOver = self.getGaus(self.alpha, self.mue, self.sigma, x)
        for g in range(self.numOGaus):
            norm[g] = self.getStND(x, self.mue[g], self.sigma[g]) / nOver
        if dm == "max": self.baseline = 0.99 * self.baseline + 0.01 * y
        if y > self.best: self.best = y
        fakt = (y - self.baseline) / (self.best - self.baseline)
        if fakt < -1.0: fakt = -1.0


        #alpha learning
        sigmoA = self.sigmo(self.alpha)
        self.alpha += self.alphaA * self.propFakt * sigmoA * (1.0 - sigmoA) * fakt * norm #(1.0-sigmoA)*
        sigmoA = self.sigmo(self.alpha)
        sigmoA /= sum(sigmoA)
        self.alpha = self.invSigmo(sigmoA)

        #mue learning
        sigmoA = self.sigmo(self.alpha)
        self.mue += self.alphaM * fakt * (x - self.mue) * sigmoA * norm

        #sigma learning
        if fakt > 0.0:
            self.sigma += self.alphaS * fakt * ((x - self.mue) ** 2 - self.sigma ** 2) / self.sigma * sigmoA * norm

    def sigmo(self, a):
        return 1.0 / (1.0 + exp(-1.0 * a))

    def invSigmo(self, a):
        return - log(1.0 / a - 1.0)

    #plots the choosen reward function without noise
    def plotReward(self, col):
        from pylab import plot
        xList = []
        yList = []
        for i in range(1000):
            x = float(i) * 1.5 * self.distRange / 1000.0 + self.rangeMin * 1.5
            xList.append(x)
            yList.append(self.testRewardFunction(x, self.typ))
        plot(xList, yList, col)

    def getSample(self, dm="max"):
        sampleX = self.drawSample(dm)
        return sampleX

    def sample(self, wi, dm, learning="logLiklihood", noise=0.2, plt=True):
        if plt:
            self.plotGaussian('r', dm)
            self.plotReward('y')
        xList = []
        yList = []
        for i in range(wi):
            sampleX = self.getSample(dm)
            sampleY = self.testRewardFunction(sampleX, self.typ, noise)
            self.learn(sampleX, sampleY, dm, "logLiklihood")

            if plt:
                if i / 1 == float(i) / 1.0:
                    xList.append(sampleX)
                    yList.append(sampleY)
                if i == wi / 4:
                    self.plotGaussian('g', dm)
                if i == wi / 2:
                    self.plotGaussian('b', dm)

        if plt:
            from pylab import show, scatter, legend, axis
            self.plotGaussian('k', dm)
            scatter(xList, yList, 1)
            v = [-30.5, 30.5, -0.5, 1.5]
            axis(v)
            s4 = repr(wi / 4) + 'Sample'
            s2 = repr(wi / 2) + 'Sample'
            s1 = repr(wi) + 'Sample'
            legend(('meanReward', 'InitMixture', s4, s2, s1), loc=0, shadow=True)
            show()

if __name__ == '__main__':
    #m=MixtureOfGaussians("rastrigin", 20)
    #m.sample(10000, "dist")
    m = MixtureOfGaussians("rastrigin", 10)
    m.sample(10000, "max")


########NEW FILE########
__FILENAME__ = mogpuremax
__author__ = 'Frank Sehnke, sehnke@in.tum.de'


from scipy import random, zeros, ones, exp, sqrt, cos, log

stND = zeros(1000)
for i in range(1000):
    x = -4.0 + float(i) * 8.0 / 1000.0
    stND[i] = 1.0 / 2.51 * exp(-0.5 * (x) ** 2)


# FIXME: different class name?
class MixtureOfGaussians:
    def __init__(self, numOGaus=10, alphaA=0.02, alphaM=0.02, alphaS=0.02):
        self.alphaA = alphaA
        self.alphaM = alphaM
        self.alphaS = alphaS
        self.minSig = 0.000001
        self.numOGaus = numOGaus #Number of Gaussians
        self.rangeMin = -20.0
        self.rangeMax = 20.0
        self.epsilon = (self.rangeMax - self.rangeMin) / (sqrt(2.0) * float(self.numOGaus - 1)) #Initial value of sigmas

        self.propFakt = 1.0 / float(self.numOGaus)
        self.distFakt = 1.0 / float(self.numOGaus - 1)
        self.distRange = self.rangeMax - self.rangeMin

        self.sigma = ones(self.numOGaus)
        self.mue = zeros(self.numOGaus)
        self.alpha = ones(self.numOGaus)
        self.sigma *= self.epsilon
        self.alpha /= float(self.numOGaus)
        for i in range(self.numOGaus):
            self.mue[i] = self.distRange * float(i) * self.distFakt + self.rangeMin
        self.baseline = 0.0
        self.best = 0.000001

    def getStND(self, x, mue=0.0, sig=1.0):
        x = (x - mue) / sig
        if abs(x) >= 4.0: return 0.000000001
        x = int((x + 4.0) / 8.0 * 1000)
        return stND[x] / sig

    #get mixture for point x
    def getGaus(self, alpha, mue, sigma, x):
        dens = zeros(self.numOGaus)
        for g in range(self.numOGaus):
            dens[g] = self.getStND(x, mue[g], sigma[g])
        return sum(self.sigmo(alpha) * dens)

    def drawSample(self):
        sum = 0.0
        rndFakt = random.random()
        for g in range(self.numOGaus):
            sum += self.sigmo(self.alpha[g])
            if rndFakt < sum:
                if self.sigma[g] < self.minSig: self.sigma[g] = self.minSig
                x = random.normal(self.mue[g], self.sigma[g])
                break
        return x

    def learn(self, x, y):
        #learning overalls
        norm = zeros(self.numOGaus)
        nOver = self.getGaus(self.alpha, self.mue, self.sigma, x)
        for g in range(self.numOGaus):
            norm[g] = self.getStND(x, self.mue[g], self.sigma[g]) / nOver
        self.baseline = 0.99 * self.baseline + 0.01 * y
        if y > self.best: self.best = y
        fakt = (y - self.baseline) / (self.best - self.baseline)
        if fakt < -1.0: fakt = -1.0

        #alpha learning
        sigmoA = self.sigmo(self.alpha)
        self.alpha += self.alphaA * self.propFakt * sigmoA * (1.0 - sigmoA) * fakt * norm #(1.0-sigmoA)*
        sigmoA = self.sigmo(self.alpha)
        sigmoA /= sum(sigmoA)
        self.alpha = self.invSigmo(sigmoA)

        #mue learning
        sigmoA = self.sigmo(self.alpha)
        self.mue += self.alphaM * fakt * (x - self.mue) * sigmoA * norm

        #sigma learning
        if fakt > 0.0:
            self.sigma += self.alphaS * fakt * ((x - self.mue) ** 2 - self.sigma ** 2) / self.sigma * sigmoA * norm

    def sigmo(self, a):
        return 1.0 / (1.0 + exp(-1.0 * a))

    def invSigmo(self, a):
        return - log(1.0 / a - 1.0)

    def getSample(self):
        sampleX = self.drawSample()
        return sampleX

if __name__ == '__main__':
    m = MixtureOfGaussians()
    for i in range(10000):
        x = m.getSample()
        n = x / m.distRange * 10.0
        if abs(n) > 5.0: n = 0.0
        y = (20.0 + n ** 2 - 10.0 * cos(2.0 * 3.1416 * n)) / 55.0 + random.normal(0, 0.2) #one dimensional rastrigin
        m.learn(x, y)
    print(m.alpha)
    print(m.mue)
    print(m.sigma)


########NEW FILE########
__FILENAME__ = udpconnection
__author__ = 'Frank Sehnke, sehnke@in.tum.de'

#############################################################################################################
# UDP Connection classes                                                                                    #
#                                                                                                           #
# UDPServer waits till at least one client is connected.                                                    #
# It then sends a list to the connected clients (can also be a list of scipy arrays!)                       #
# There can connect several clients to the server but the same data is sent to all clients.                 #
# Options for the constructor are the server IP and the starting port (2 adjacent ports will be used)       #
#                                                                                                           #
# UDPClient trys to connect to a UDPServer till the connection is established.                              #
# The client then recives data from the server and parses it into an list of the original shape.            #
# Options for the cunstructor are server-, client IP and the starting port (2 adjacent ports will be used)  #
#                                                                                                           #
# Requirements: sockets and scipy.                                                                          #
# Example: FlexCubeEnvironment and FlexCubeRenderer (env sends data to renderer for OpenGL output)          #
#                                                                                                           #
#############################################################################################################

import socket

# The server class
class UDPServer(object):
    def __init__(self, ip="127.0.0.1", port="21560", buf="1024"):
        #Socket settings
        self.host = ip
        self.inPort = eval(port) + 1
        self.outPort = eval(port)
        self.buf = eval(buf) #16384
        self.addr = (self.host, self.inPort)

        #Create socket and bind to address
        self.UDPInSock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.UDPInSock.bind(self.addr)

        #Client lists
        self.clients = 0
        self.cIP = []
        self.addrList = []
        self.UDPOutSockList = []
        print("listening on port", self.inPort)

    # Adding a client to the list
    def addClient(self, cIP):
        self.cIP.append(cIP)
        self.addrList.append((cIP, self.outPort))
        self.UDPOutSockList.append(socket.socket(socket.AF_INET, socket.SOCK_DGRAM))
        print("client", cIP, "connected")
        self.clients += 1

    # Listen for clients
    def listen(self):
        if self.clients < 1:
            self.UDPInSock.settimeout(10)
            try:
                cIP = self.UDPInSock.recv(self.buf)
                self.addClient(cIP)
            except:
                pass
        else:
            # At least one client has to send a sign of life during 2 seconds
            self.UDPInSock.settimeout(2)
            try:
                cIP = self.UDPInSock.recv(self.buf)
                newClient = True
                for i in self.cIP:
                    if cIP == i:
                        newClient = False
                        break
                #Adding new client
                if newClient:
                    self.addClient(cIP)
            except:
                print("All clients disconnected")
                self.clients = 0
                self.cIP = []
                self.addrList = []
                self.UDPOutSockList = []
                print("listening on port", self.inPort)


    # Sending the actual data too all clients
    def send(self, arrayList):
        sendString = repr(arrayList)
        count = 0
        for i in self.UDPOutSockList:
            i.sendto(sendString, self.addrList[count])
            count += 1

# The client class
class UDPClient(object):
    def __init__(self, servIP="127.0.0.1", ownIP="127.0.0.1", port="21560", buf="1024"):
        #UDP Sttings
        self.host = servIP
        self.inPort = eval(port)
        self.outPort = eval(port) + 1
        self.inAddr = (ownIP, self.inPort)
        self.outAddr = (self.host, self.outPort)
        self.ownIP = ownIP
        self.buf = eval(buf) #16384

        # Create sockets
        self.createSockets()

    # Listen for data from server
    def listen(self, arrayList=None):
        # Send alive signal (own IP adress)
        self.UDPOutSock.sendto(self.ownIP, self.outAddr)
        # if there is no data from Server for 10 seconds server is propably down
        self.UDPInSock.settimeout(10)
        try:
            data = self.UDPInSock.recv(self.buf)

            try:
                arrayList = eval(data)
                return arrayList
            except:
                print("Unsupported data format received from", self.outAddr, "!")
                return None

        except:
            print("Server has quit!")
            return None
            # Try to recreate sockets
            #self.createSockets()

    # Creating the sockets
    def createSockets(self):
        self.UDPOutSock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.UDPOutSock.sendto(self.ownIP, self.outAddr)
        self.UDPInSock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.UDPInSock.bind(self.inAddr)


########NEW FILE########
__FILENAME__ = neuralnets
# Neural network data analysis tool collection. Makes heavy use of the logging module.
# Can generate training curves during the run (from properly setup IPython and/or with
# TkAgg backend and interactive mode - see matplotlib documentation).
__author__ = "Martin Felder"
__version__ = "$Id$"

from pylab import ion, figure, draw
import csv
from numpy import Infinity
import logging

from pybrain.datasets                  import ClassificationDataSet, SequentialDataSet
from pybrain.tools.shortcuts           import buildNetwork
from pybrain.supervised                import BackpropTrainer, RPropMinusTrainer, Trainer
from pybrain.structure                 import SoftmaxLayer, LSTMLayer
from pybrain.utilities                 import setAllArgs
from pybrain.tools.plotting            import MultilinePlotter
from pybrain.tools.validation          import testOnSequenceData, ModuleValidator, Validator
from pybrain.tools.customxml           import NetworkWriter


class NNtools(object):
    """ Abstract class providing basic functionality to make neural network training more comfortable """

    def __init__(self, DS, **kwargs):
        """ Initialize with the training data set DS. All keywords given are set as member variables.
        The following are particularly important:

        :key hidden: number of hidden units
        :key TDS: test data set for checking convergence
        :key VDS: validation data set for final performance evaluation
        :key epoinc: number of epochs to train for, before checking convergence (default: 5)
        """
        self.DS = DS
        self.hidden = 10
        self.maxepochs = 1000
        self.Graph = None
        self.TDS = None
        self.VDS = None
        self.epoinc = 5
        setAllArgs(self, kwargs)
        self.trainCurve = None


    def initGraphics(self, ymax=10, xmax= -1):
        """ initialize the interactive graphics output window, and return a handle to the plot """
        if xmax < 0:
            xmax = self.maxepochs
        figure(figsize=[12, 8])
        ion()
        draw()
        #self.Graph = MultilinePlotter(autoscale=1.2 ) #xlim=[0, self.maxepochs], ylim=[0, ymax])
        self.Graph = MultilinePlotter(xlim=[0, xmax], ylim=[0, ymax])
        self.Graph.setLineStyle([0, 1], linewidth=2)
        return self.Graph


    def set(self, **kwargs):
        """ convenience method to set several member variables at once """
        setAllArgs(self, kwargs)


    def saveTrainingCurve(self, learnfname):
        """ save the training curves into a file with the given name (CSV format) """
        logging.info('Saving training curves into ' + learnfname)
        if self.trainCurve is None:
            logging.error('No training curve available for saving!')
        learnf = open(learnfname, "wb")
        writer = csv.writer(learnf, dialect='excel')
        nDataSets = len(self.trainCurve)
        for i in range(1, len(self.trainCurve[0]) - 1):
            writer.writerow([self.trainCurve[k][i] for k in range(nDataSets)])
        learnf.close()

    def saveNetwork(self, fname):
        """ save the trained network to a file """
        NetworkWriter.writeToFile(self.Trainer.module, fname)
        logging.info("Network saved to: " + fname)


#=======================================================================================================

class NNregression(NNtools):
    """ Learns to numerically predict the targets of a set of data, with optional online progress plots. """


    def setupNN(self, trainer=RPropMinusTrainer, hidden=None, **trnargs):
        """ Constructs a 3-layer FNN for regression. Optional arguments are passed on to the Trainer class. """
        if hidden is not None:
            self.hidden = hidden
        logging.info("Constructing FNN with following config:")
        FNN = buildNetwork(self.DS.indim, self.hidden, self.DS.outdim)
        logging.info(str(FNN) + "\n  Hidden units:\n    " + str(self.hidden))
        logging.info("Training FNN with following special arguments:")
        logging.info(str(trnargs))
        self.Trainer = trainer(FNN, dataset=self.DS, **trnargs)


    def runTraining(self, convergence=0, **kwargs):
        """ Trains the network on the stored dataset. If convergence is >0, check after that many epoch increments
        whether test error is going down again, and stop training accordingly.
        CAVEAT: No support for Sequential datasets!"""
        assert isinstance(self.Trainer, Trainer)
        if self.Graph is not None:
            self.Graph.setLabels(x='epoch', y='normalized regression error')
            self.Graph.setLegend(['training', 'test'], loc='upper right')
        epoch = 0
        inc = self.epoinc
        best_error = Infinity
        best_epoch = 0
        learncurve_x = [0]
        learncurve_y = [0.0]
        valcurve_y = [0.0]
        converged = False
        convtest = 0
        if convergence > 0:
            logging.info("Convergence criterion: %d batches of %d epochs w/o improvement" % (convergence, inc))
        while epoch <= self.maxepochs and not converged:
            self.Trainer.trainEpochs(inc)
            epoch += inc
            learncurve_x.append(epoch)
            # calculate errors on TRAINING data
            err_trn = ModuleValidator.validate(Validator.MSE, self.Trainer.module, self.DS)
            learncurve_y.append(err_trn)
            if self.TDS is None:
                logging.info("epoch: %6d,  err_trn: %10g" % (epoch, err_trn))
            else:
                # calculate same errors on TEST data
                err_tst = ModuleValidator.validate(Validator.MSE, self.Trainer.module, self.TDS)
                valcurve_y.append(err_tst)
                if err_tst < best_error:
                    # store best error and parameters
                    best_epoch = epoch
                    best_error = err_tst
                    bestweights = self.Trainer.module.params.copy()
                    convtest = 0
                else:
                    convtest += 1
                logging.info("epoch: %6d,  err_trn: %10g,  err_tst: %10g,  best_tst: %10g" % (epoch, err_trn, err_tst, best_error))
                if self.Graph is not None:
                    self.Graph.addData(1, epoch, err_tst)

                # check if convegence criterion is fulfilled (no improvement after N epoincs)
                if convtest >= convergence:
                    converged = True

            if self.Graph is not None:
                self.Graph.addData(0, epoch, err_trn)
                self.Graph.update()

        # training finished!
        logging.info("Best epoch: %6d, with error: %10g" % (best_epoch, best_error))
        if self.VDS is not None:
            # calculate same errors on VALIDATION data
            self.Trainer.module.params[:] = bestweights.copy()
            err_val = ModuleValidator.validate(Validator.MSE, self.Trainer.module, self.VDS)
            logging.info("Result on evaluation data: %10g" % err_val)
        # store training curve for saving into file
        self.trainCurve = (learncurve_x, learncurve_y, valcurve_y)

#=======================================================================================================

class NNclassifier(NNtools):
    """ Learns to classify a set of data, with optional online progress plots. """

    def __init__(self, DS, **kwargs):
        """ Initialize the classifier: the least we need is the dataset to be classified. All keywords given are set as member variables. """
        if not isinstance(DS, ClassificationDataSet):
            raise TypeError('Need a ClassificationDataSet to do classification!')
        NNtools.__init__(self, DS, **kwargs)
        self.nClasses = self.DS.nClasses  # need this because targets may be altered later
        self.clsnames = None
        self.targetsAreOneOfMany = False


    def _convertAllDataToOneOfMany(self, values=[0, 1]):
        """ converts all datasets associated with self into 1-out-of-many representations,
        e.g. with original classes 0 to 4, the new target for class 1 would be [0,1,0,0,0],
        or accordingly with other upper and lower bounds, as given by the values keyword """
        if self.targetsAreOneOfMany:
            return
        else:
            # convert all datasets to one-of-many ("winner takes all") representation
            for dsname in ["DS", "TDS", "VDS"]:
                d = getattr(self, dsname)
                if d is not None:
                    if d.outdim < d.nClasses:
                        d._convertToOneOfMany(values)
            self.targetsAreOneOfMany = True


    def setupNN(self, trainer=RPropMinusTrainer, hidden=None, **trnargs):
        """ Setup FNN and trainer for classification. """
        self._convertAllDataToOneOfMany()
        if hidden is not None:
            self.hidden = hidden
        FNN = buildNetwork(self.DS.indim, self.hidden, self.DS.outdim, outclass=SoftmaxLayer)
        logging.info("Constructing classification FNN with following config:")
        logging.info(str(FNN) + "\n  Hidden units:\n    " + str(self.hidden))
        logging.info("Trainer received the following special arguments:")
        logging.info(str(trnargs))
        self.Trainer = trainer(FNN, dataset=self.DS, **trnargs)


    def setupRNN(self, trainer=BackpropTrainer, hidden=None, **trnargs):
        """ Setup an LSTM RNN and trainer for sequence classification. """
        if hidden is not None:
            self.hidden = hidden
        self._convertAllDataToOneOfMany()

        RNN = buildNetwork(self.DS.indim, self.hidden, self.DS.outdim, hiddenclass=LSTMLayer, 
                           recurrent=True, outclass=SoftmaxLayer)
        logging.info("Constructing classification RNN with following config:")
        logging.info(str(RNN) + "\n  Hidden units:\n    " + str(self.hidden))
        logging.info("Trainer received the following special arguments:")
        logging.info(str(trnargs))
        self.Trainer = trainer(RNN, dataset=self.DS, **trnargs)


    def runTraining(self, convergence=0, **kwargs):
        """ Trains the network on the stored dataset. If convergence is >0, check after that many epoch increments
        whether test error is going down again, and stop training accordingly. """
        assert isinstance(self.Trainer, Trainer)
        if self.Graph is not None:
            self.Graph.setLabels(x='epoch', y='% classification error')
            self.Graph.setLegend(['training', 'test'], loc='lower right')
        epoch = 0
        inc = self.epoinc
        best_error = 100.0
        best_epoch = 0
        learncurve_x = [0]
        learncurve_y = [0.0]
        valcurve_y = [0.0]
        converged = False
        convtest = 0
        if convergence > 0:
            logging.info("Convergence criterion: %d batches of %d epochs w/o improvement" % (convergence, inc))
        while epoch <= self.maxepochs and not converged:
            self.Trainer.trainEpochs(inc)
            epoch += inc
            learncurve_x.append(epoch)
            # calculate errors on TRAINING data
            if isinstance(self.DS, SequentialDataSet):
                r_trn = 100. * (1.0 - testOnSequenceData(self.Trainer.module, self.DS))
            else:
                # FIXME: messy - validation does not belong into the Trainer...
                out, trueclass = self.Trainer.testOnClassData(return_targets=True)
                r_trn = 100. * (1.0 - Validator.classificationPerformance(out, trueclass))
            learncurve_y.append(r_trn)
            if self.TDS is None:
                logging.info("epoch: %6d,  err_trn: %5.2f%%" % (epoch, r_trn))
            else:
                # calculate errors on TEST data
                if isinstance(self.DS, SequentialDataSet):
                    r_tst = 100. * (1.0 - testOnSequenceData(self.Trainer.module, self.TDS))
                else:
                    # FIXME: messy - validation does not belong into the Trainer...
                    out, trueclass = self.Trainer.testOnClassData(return_targets=True, dataset=self.TDS)
                    r_tst = 100. * (1.0 - Validator.classificationPerformance(out, trueclass))
                valcurve_y.append(r_tst)
                if r_tst < best_error:
                    best_epoch = epoch
                    best_error = r_tst
                    bestweights = self.Trainer.module.params.copy()
                    convtest = 0
                else:
                    convtest += 1
                logging.info("epoch: %6d,  err_trn: %5.2f%%,  err_tst: %5.2f%%,  best_tst: %5.2f%%" % (epoch, r_trn, r_tst, best_error))
                if self.Graph is not None:
                    self.Graph.addData(1, epoch, r_tst)

                # check if convegence criterion is fulfilled (no improvement after N epoincs)
                if convtest >= convergence:
                    converged = True

            if self.Graph is not None:
                self.Graph.addData(0, epoch, r_trn)
                self.Graph.update()

        logging.info("Best epoch: %6d, with error: %5.2f%%" % (best_epoch, best_error))
        if self.VDS is not None:
            # calculate errors on VALIDATION data
            self.Trainer.module.params[:] = bestweights.copy()
            if isinstance(self.DS, SequentialDataSet):
                r_val = 100. * (1.0 - testOnSequenceData(self.Trainer.module, self.VDS))
            else:
                out, trueclass = self.Trainer.testOnClassData(return_targets=True, dataset=self.VDS)
                r_val = 100. * (1.0 - Validator.classificationPerformance(out, trueclass))
            logging.info("Result on evaluation data: %5.2f%%" % r_val)

        self.trainCurve = (learncurve_x, learncurve_y, valcurve_y)



########NEW FILE########
__FILENAME__ = nondominated
__author__ = 'Justin Bayer, Tom Schaul, {justin,tom}@idsia.ch'


import collections
from scipy import array, tile, sum


def crowding_distance(individuals, fitnesses):
    """ Crowding distance-measure for multiple objectives. """
    distances = collections.defaultdict(lambda: 0)
    individuals = list(individuals)
    # Infer the number of objectives by looking at the fitness of the first.
    n_obj = len(fitnesses[individuals[0]])
    for i in xrange(n_obj):
        individuals.sort(key=lambda x: fitnesses[x][i])
        # normalization between 0 and 1.
        normalization = float(fitnesses[individuals[0]][i] - fitnesses[individuals[-1]][i])
        # Make sure the boundary points are always selected.
        distances[individuals[0]] = 1e100
        distances[individuals[-1]] = 1e100
        tripled = zip(individuals, individuals[1:-1], individuals[2:])
        for pre, ind, post in tripled:
            distances[ind] += (fitnesses[pre][i] - fitnesses[post][i]) / normalization
    return distances


def _non_dominated_front_old(iterable, key=lambda x: x, allowequality=True):
    """Return a subset of items from iterable which are not dominated by any
    other item in iterable."""
    items = list(iterable)
    keys = dict((i, key(i)) for i in items)
    dim = len(keys.values()[0])
    if any(dim != len(k) for k in keys.values()):
        raise ValueError("Wrong tuple size.")

    # Make a dictionary that holds the items another item dominates.
    dominations = collections.defaultdict(lambda: [])
    for i in items:
        for j in items:
            if allowequality:
                if all(keys[i][k] < keys[j][k] for k in xrange(dim)):
                    dominations[i].append(j)
            else:
                if all(keys[i][k] <= keys[j][k] for k in xrange(dim)):
                    dominations[i].append(j)

    dominates = lambda i, j: j in dominations[i]

    res = set()
    items = set(items)
    for i in items:
        res.add(i)
        for j in list(res):
            if i is j:
                continue
            if dominates(j, i):
                res.remove(i)
                break
            elif dominates(i, j):
                res.remove(j)
    return res


def _non_dominated_front_fast(iterable, key=lambda x: x, allowequality=True):
    """Return a subset of items from iterable which are not dominated by any
    other item in iterable.

    Faster version.
    """
    items = list(iterable)
    keys = dict((i, key(i)) for i in items)
    dim = len(keys.values()[0])
    dominations = {}
    for i in items:
        for j in items:
            good = True
            if allowequality:
                for k in xrange(dim):
                    if keys[i][k] >= keys[j][k]:
                        good = False
                        break
            else:
                for k in xrange(dim):
                    if keys[i][k] > keys[j][k]:
                        good = False
                        break
            if good:
                dominations[(i, j)] = None
    res = set()
    items = set(items)
    for i in items:
        res.add(i)
        for j in list(res):
            if i is j:
                continue
            if (j, i) in dominations:
                res.remove(i)
                break
            elif (i, j) in dominations:
                res.remove(j)
    return res


def _non_dominated_front_merge(iterable, key=lambda x: x, allowequality=True):
    items = list(iterable)
    l = len(items)
    if l > 20:
        part1 = list(_non_dominated_front_merge(items[:l / 2], key, allowequality))
        part2 = list(_non_dominated_front_merge(items[l / 2:], key, allowequality))
        if len(part1) >= l / 3 or len(part2) >= l / 3:
            return _non_dominated_front_fast(part1 + part2, key, allowequality)
        else:
            return _non_dominated_front_merge(part1 + part2, key, allowequality)
    else:
        return _non_dominated_front_fast(items, key, allowequality)


def _non_dominated_front_arr(iterable, key=lambda x: x, allowequality=True):
    """Return a subset of items from iterable which are not dominated by any
    other item in iterable.

    Faster version, based on boolean matrix manipulations.
    """
    items = list(iterable)
    fits = map(key, items)
    l = len(items)
    x = array(fits)
    a = tile(x, (l, 1, 1))
    b = a.transpose((1, 0, 2))
    if allowequality:
        ndom = sum(a <= b, axis=2)
    else:
        ndom = sum(a < b, axis=2)
    ndom = array(ndom, dtype=bool)
    res = set()
    for ii in range(l):
        res.add(ii)
        for ij in list(res):
            if ii == ij:
                continue
            if not ndom[ij, ii]:
                res.remove(ii)
                break
            elif not ndom[ii, ij]:
                res.remove(ij)
    return set(map(lambda i: items[i], res))


def _non_dominated_front_merge_arr(iterable, key=lambda x: x, allowequality=True):
    items = list(iterable)
    l = len(items)
    if l > 100:
        part1 = list(_non_dominated_front_merge_arr(items[:l / 2], key, allowequality))
        part2 = list(_non_dominated_front_merge_arr(items[l / 2:], key, allowequality))
        if len(part1) >= l / 3 or len(part2) >= l / 3:
            return _non_dominated_front_arr(part1 + part2, key, allowequality)
        else:
            return _non_dominated_front_merge_arr(part1 + part2, key, allowequality)
    else:
        return _non_dominated_front_arr(items, key, allowequality)


non_dominated_front = _non_dominated_front_merge_arr

def non_dominated_sort(iterable, key=lambda x: x, allowequality=True):
    """Return a list that is sorted in a non-dominating fashion.
    Keys have to be n-tuple."""
    items = set(iterable)
    fronts = []
    while items:
        front = non_dominated_front(items, key, allowequality)
        items -= front
        fronts.append(front)
    return fronts
    
''' added by JPQ for Constrained Multi-objective Optimization '''


def _const_non_dominated_front_merge_arr(iterable, key=lambda x: x, allowequality=True):
    items = list(iterable)
    l = len(items)
    if l > 100:
        part1 = list(_const_non_dominated_front_merge_arr(items[:l / 2], key, allowequality))
        part2 = list(_const_non_dominated_front_merge_arr(items[l / 2:], key, allowequality))
        if len(part1) >= l / 3 or len(part2) >= l / 3:
            return _const_non_dominated_front_arr(part1 + part2, key, allowequality)
        else:
            return _const_non_dominated_front_merge_arr(part1 + part2, key, allowequality)
    else:
        return _const_non_dominated_front_arr(items, key, allowequality)
        
def _const_non_dominated_front_arr(iterable, key=lambda x: x, allowequality=True):
    """Return a subset of items from iterable which are not dominated by any
    other item in iterable.

    Faster version, based on boolean matrix manipulations.
    """
    items = list(iterable)  # pop
 
    fits = map(key, items)  # fitness

    x = array([fits[i][0] for i in range(len(fits))])
    v = array([fits[i][1] for i in range(len(fits))])
    c = array([fits[i][2] for i in range(len(fits))])
    
    l = len(items)
    a = tile(x, (l, 1, 1))
    b = a.transpose((1, 0, 2))
    if allowequality:
        ndom = sum(a <= b, axis=2)
    else:
        ndom = sum(a < b, axis=2)
    ndom = array(ndom, dtype=bool)
    res = set()
    for ii in range(l):
        res.add(ii)
        for ij in list(res):
            if ii == ij:
                continue
            if not ndom[ij, ii] and v[ij] and v[ii]:
                res.remove(ii)
                break
            elif not ndom[ii, ij] and v[ij] and v[ii]:
                res.remove(ij)
            elif v[ij] and not v[ii]:
                res.remove(ii)
                break
            elif v[ii] and not v[ij]:
                res.remove(ij)
            elif not v[ii] and not v[ij]:
                cii = abs(sum(c[ii]))
                cij = abs(sum(c[ij]))
                if cii < cij:
                   res.remove(ij)
                else:
                   res.remove(ii)
                   break

    return set(map(lambda i: items[i], res))
    
const_non_dominated_front = _const_non_dominated_front_merge_arr

def const_non_dominated_sort(iterable, key=lambda x: x, allowequality=True):
    """Return a list that is sorted in a non-dominating fashion.
    Keys have to be n-tuple."""
    
    items = set(iterable)
    
    fronts = []
    while items:
        front = const_non_dominated_front(items, key, allowequality)
        items -= front
        fronts.append(front)
    return fronts

def const_crowding_distance(individuals, fitnesses):
    """ Crowding distance-measure for multiple objectives. """
    distances = collections.defaultdict(lambda: 0)
    individuals = list(individuals)
    # Infer the number of objectives by looking at the fitness of the first.
    n_obj = len(fitnesses[individuals[0]][0])
    
    for i in xrange(n_obj):
        individuals.sort(key=lambda x: fitnesses[x][0][i])
        # normalization between 0 and 1.
        normalization = float(fitnesses[individuals[0]][0][i] - fitnesses[individuals[-1]][0][i])
        # Make sure the boundary points are always selected.
        distances[individuals[0]] = 1e100
        distances[individuals[-1]] = 1e100
        tripled = zip(individuals, individuals[1:-1], individuals[2:])
        for pre, ind, post in tripled:
            distances[ind] += (fitnesses[pre][0][i] - fitnesses[post][0][i]) / normalization
    return distances

def const_number_of_feasible_pop(iterable, key=lambda x: x, allowequality=True):
    """Return a subset of items from iterable which are not dominated by any
    other item in iterable.

    Faster version, based on boolean matrix manipulations.
    """
    items = list(iterable)  # pop
 
    fits = map(key, items)  # fitness

    v = list([fits[i][1] for i in range(len(fits))])
    n = v.count(True)
    return n
# ---
########NEW FILE########
__FILENAME__ = ciaoplot
__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import zeros, array, amin, amax, sqrt

from colormaps import ColorMap

class CiaoPlot(ColorMap):
    """ CIAO plot of coevolution performance with respect to the best
    individuals from previous generations (Hall of Fame).
    Requires 2 populations.  """

    @staticmethod
    def generateData(evaluator, hof1, hof2, symmetric=True):
        assert len(hof1) == len(hof2)
        gens = len(hof1)
        res = zeros((gens, gens))
        for g1, ind1 in enumerate(hof1):
            for g2, ind2 in enumerate(hof2[:g1 + 1]):
                res[g1, g2] = evaluator(ind1, ind2)
                if symmetric:
                    res[g2, g1] = res[g1, g2]
                elif g1 == g2:
                    # TODO: chack this!
                    res[g1, g2] += evaluator(ind2, ind1)
                else:
                    res[g2, g1] = evaluator(ind2, ind1)
        return res


    def __init__(self, evaluator, hof1, hof2, **args):
        if 'symmetric' in args:
            M = CiaoPlot.generateData(evaluator, hof1, hof2, symmetric=args['symmetric'])
            del args['symmetric']
        else:
            M = CiaoPlot.generateData(evaluator, hof1, hof2)
        M *= 1 / (amin(M) - amax(M))
        M -= amin(M)
        self.relData = M
        ColorMap.__init__(self, M, minvalue=0, maxvalue=1, **args)


if __name__ == '__main__':
    x = array(range(100))
    h1 = x * 4
    h2 = x + 20 * sqrt(x)
    def evo(x, y):
        return x - y
    from pylab import cm
    p = CiaoPlot(evo, h1, h2, cmap=cm.hot).show()

########NEW FILE########
__FILENAME__ = classification
"""
matplotlib helpers for ClassificationDataSet and classifiers in general.
"""
__author__ = 'Werner Beroux <werner@beroux.com>'

import numpy as np
import matplotlib.pyplot as plt

class ClassificationDataSetPlot(object):
    @staticmethod
    def plot_module_classification_sequence_performance(module, dataset, sequence_index, bounds=(0, 1)):
        """Plot all outputs and fill the value of the output of the correct category.

        The grapth of a good classifier should be like all white, with all other
        values very low. A graph with lot of black is a bad sign.

        :param module: The module/network to plot.
        :type module: pybrain.structure.modules.module.Module
        :param dataset: Training dataset used as inputs and expected outputs.
        :type dataset: SequenceClassificationDataSet
        :param sequence_index: Sequence index to plot in the dataset.
        :type sequence_index: int
        :param bounds: Outputs lower and upper bound.
        :type bounds: list
        """
        outputs = []
        valid_output = []
        module.reset()
        for sample in dataset.getSequenceIterator(sequence_index):
            out = module.activate(sample[0])
            outputs.append(out)
            valid_output.append(out[sample[1].argmax()])
        plt.fill_between(range(len(valid_output)), 1, valid_output, facecolor='k', alpha=0.8)
        plt.plot(outputs, linewidth=4, alpha=0.7)
        plt.yticks(bounds)

    @staticmethod
    def plot_module_classification_dataset_performance(module, dataset, cols=4, bounds=(0, 1)):
        """Do a plot_module_classification_sequence_performance() for all sequences in the dataset.
        :param module: The module/network to plot.
        :type module: pybrain.structure.modules.module.Module
        :param dataset: Training dataset used as inputs and expected outputs.
        :type dataset: SequenceClassificationDataSet
        :param bounds: Outputs lower and upper bound.
        :type bounds: list
        """
        # Outputs and detected category error for each sequence.
        for i in range(dataset.getNumSequences()):
            plt.subplot(ceil(dataset.getNumSequences() / float(cols)), cols, i)
            ClassificationDataSetPlot.plot_module_classification_sequence_performance(module, dataset, i, bounds)

    @staticmethod
    def punchcard_module_classification_performance(module, dataset, s=800):
        """Punshcard-like clasification performances.__add__(

        Actual dataset target vs. estimated target by the module.
        The graph of a good classfier module should a have no red dots visible:
        - Red Dots: Target (only visible if the black dot doesn't cover it).
        - Green Dots: Estimated classes confidences (size = outputs means).
        - Black Dots: Single winnter-takes-all estimated target.

        :param module: An object that has at least reset() and activate() methods.
        :param dataset: A classification dataset. It should, for any given sequence, have a constant target.
        :type dataset: ClassificationDataSet
        """
        # TODO: Could also show the variation for each dot
        #       (e.g., vertical errorbar of 2*stddev).
        # TODO: Could keep together all sequences of a given class and somehow
        #       arrange them closer togther. Could then aggregate them and
        #       include horizontal errorbar.

        def calculate_module_output_mean(module, inputs):
            """Returns the mean of the module's outputs for a given input list."""
            outputs = np.zeros(module.outdim)
            module.reset()
            for inpt in inputs:
                outputs += module.activate(inpt)
            return outputs / len(inputs)

        num_sequences = dataset.getNumSequences()
        actual = []
        expected = []
        confidence_x = []
        confidence_s = []
        correct = 0

        for seq_i in xrange(num_sequences):
            seq = dataset.getSequence(seq_i)
            outputs_mean = calculate_module_output_mean(module, seq[0])
            actual.append(np.argmax(outputs_mean))
            confidence_s.append(np.array(outputs_mean))
            confidence_x.append(np.ones(module.outdim) * seq_i)
            # FIXME: np.argmax(seq[1]) == dataset.getSequenceClass(seq_i) is bugged for split SequenceClassificationDataSet.
            expected.append(np.argmax(seq[1]))
            if actual[-1] == expected[-1]:
                correct += 1

        plt.title('{}% Correct Classification (red dots mean bad classification)'.format(correct * 100 / num_sequences))
        plt.xlabel('Sequence')
        plt.ylabel('Class')
        plt.scatter(range(num_sequences), expected, s=s, c='r', linewidths=0)
        plt.scatter(range(num_sequences), actual, s=s, c='k')
        plt.scatter(confidence_x, range(module.outdim) * num_sequences, s=s*np.array(confidence_s), c='g', linewidths=0, alpha=0.66)
        plt.yticks(range(dataset.nClasses), dataset.class_labels)

########NEW FILE########
__FILENAME__ = colormaps
__author__ = 'Tom Schaul, tom@idsia.ch'

from pylab import figure, savefig, imshow, axes, axis, cm, show
from scipy import array, amin, amax


class ColorMap:
    def __init__(self, mat, cmap=None, pixelspervalue=20, minvalue=None, maxvalue=None):
        """ Make a colormap image of a matrix

        :key mat: the matrix to be used for the colormap.
        """
        if minvalue == None:
            minvalue = amin(mat)
        if maxvalue == None:
            maxvalue = amax(mat)
        if not cmap:
            cmap = cm.hot

        figsize = (array(mat.shape) / 100. * pixelspervalue)[::-1]
        self.fig = figure(figsize=figsize)
        axes([0, 0, 1, 1]) # Make the plot occupy the whole canvas
        axis('off')
        self.fig.set_size_inches(figsize)
        imshow(mat, cmap=cmap, clim=(minvalue, maxvalue), interpolation='nearest')

    def show(self):
        """ have the image popup """
        show()

    def save(self, filename):
        """ save colormap to file"""
        savefig(filename, fig=self.fig, facecolor='black', edgecolor='black')


########NEW FILE########
__FILENAME__ = fitnesslandscapes
# some utility code for nicely plotting 3D images of function fitness landscapes.

__author__ = 'Tom Schaul, tom@idsia.ch'

from scipy import zeros, r_, cos, sin, pi, array, dot, sqrt, diag
from scipy.linalg import svd
from pylab import figure, plot, show, meshgrid, contour, savefig, colorbar
from pybrain.rl.environments.functions import FunctionEnvironment
from inspect import isclass


def plotCovEllipse(emat, center, segments=50, color='y', transp=1.):
    """ Plots a covariance ellipse. """
    # compute a nb of points on the ellipse
    ex = zeros(segments + 1)
    ey = zeros(segments + 1)
    u, s, d = svd(emat)
    sm = dot(d, dot(diag(sqrt(s)), u))
    for i in range(segments + 1):
        circlex = cos((2 * pi * i) / float(segments))
        circley = sin((2 * pi * i) / float(segments))
        ex[i] = center[0] + sm[0, 0] * circlex + sm[0, 1] * circley
        ey[i] = center[1] + sm[1, 0] * circlex + sm[1, 1] * circley

    # plot them
    plot([center[0]], [center[1]], '+', color=color, alpha=transp)
    plot(ex, ey, '-', color=color, alpha=transp)
    return ex, ey


class FitnessPlotter:
    """ plot the function's values in the rectangular region specified by ranges. By default, plot in [-1,1] """
    def __init__(self, f, xmin= -1, xmax=1, ymin= -1, ymax=1, precision=50, newfig=True,
                 colorbar=False, cblabel=None):
        """ :key precision: how many steps along every dimension """
        if isinstance(f, FunctionEnvironment):
            assert f.xdim == 2
            self.f = lambda x, y: f(array([x, y]))
        elif isclass(f) and issubclass(f, FunctionEnvironment):
            tmp = f(2)
            self.f = lambda x, y: tmp(array([x, y]))
        else:
            self.f = f

        self.precision = precision
        self.colorbar = colorbar
        self.cblabel = cblabel
        self.xs = r_[xmin:xmax:self.precision * 1j]
        self.ys = r_[ymin:ymax:self.precision * 1j]
        self.zs = self._generateValMap()
        if newfig:
            self.fig = figure()

    def _generateValMap(self):
        """ generate the function fitness values for the current grid of x and y """
        vals = zeros((len(self.xs), len(self.ys)))
        for i, x in enumerate(self.xs):
            for j, y in enumerate(self.ys):
                vals[j, i] = self.f(x, y)
        return vals

    def plotAll(self, levels=50, popup=True):
        """ :key levels: how many fitness levels should be drawn."""
        tmp = contour(self.xs, self.ys, self.zs, levels)
        if self.colorbar:
            cb = colorbar(tmp)
            if self.cblabel != None:
                cb.set_label(self.cblabel)

        if popup: show()

    def addSamples(self, samples, rescale=True, color=''):
        """plot some sample points on the fitness landscape.

        :key rescale: should the plotting ranges be adjusted? """
        # split samples into x and y
        sx = zeros(len(samples))
        sy = zeros(len(samples))
        for i, s in enumerate(samples):
            sx[i] = s[0]
            sy[i] = s[1]
        if rescale:
            self._rescale(min(sx), max(sx), min(sy), max(sy))

        plot(sx, sy, color + '+')

    def _rescale(self, xmin, xmax, ymin, ymax):
        self.xs = r_[min(xmin * 1.1, min(self.xs)):max(xmax * 1.1, max(self.xs)):self.precision * 1j]
        self.ys = r_[min(ymin * 1.1, min(self.ys)):max(ymax * 1.1, max(self.ys)):self.precision * 1j]
        self.zs = self._generateValMap()

    def addCovEllipse(self, emat, center, segments=50, rescale=True, color='c', transp=1.):
        """plot a covariance ellipse """
        ex, ey = plotCovEllipse(emat, center, segments, color, transp)
        if rescale:
            self._rescale(min(ex), max(ex), min(ey), max(ey))

    def saveAs(self, filename, format='.jpg'):
        savefig(filename + format)




########NEW FILE########
__FILENAME__ = fitnessprogression
""" a type of plots used so frequently that I think they merit their own utility """

__author__ = 'Tom Schaul, tom@idsia.ch'

import pylab
from pylab import xlabel, ylabel, legend, plot, semilogy
from scipy import array, zeros, power, log10
from pybrain.utilities import avgFoundAfter

plotsymbols = ['-', ':', '-.']
psymbol = '-'


def plotFitnessProgession(fitdict, batchsize=1, semilog=True,
                          targetcutoff=1e-10, minimize=True,
                          title=None, verbose=True,
                          varyplotsymbols=False,
                          averageOverEvaluations=True,
                          onlysuccessful=False,
                          useMedian=False,
                          resolution=1000):
    """ Plot multiple fitness curves on a single figure, with the following customizations:

        :arg fitdict: a dictionary mapping a name to a list of fitness-arrays
        :key batchsize: the number of evaluations between two points in fitness-arrays
                          specific batch sizes can also be given given in fitdict
        :key targetcutoff: this gives the cutoff point at the best fitness
        :key averageOverEvaluations: averaging is done over fitnesses (for a given number of evaluations)
                                    or over evaluations required to reach a certain fitness.
        :key resolution: resolution when averaging over evaluations
        :key onlysuccessful: consider only successful runs
        :key title: specify a title.
        :key varyplotsymbols: used different line types for each curve.
        """

    def isSuccessful(l):
        """ criterion for successful run """
        if targetcutoff == None:
            return True
        elif minimize:
            return min(l) <= targetcutoff
        else:
            return max(l) >= targetcutoff

    def paddedClipped(l, maxLen):
        assert len(l) <= maxLen
        res = zeros(maxLen)
        if targetcutoff == None:
            res[:len(l)] += l
        elif minimize:
            res[:len(l)] += l.clip(min=targetcutoff, max=1e100)
        else:
            res[:len(l)] += l.clip(max=targetcutoff, min= -1e100)
        return res

    def relevantPart(l):
        """ the part of the vector that's above the cutoff. """
        if targetcutoff != None:
            for i, val in enumerate(l):
                if minimize and val <= targetcutoff:
                    return l[:i + 1]
                elif not minimize and val >= targetcutoff:
                    return l[:i + 1]
        return l



    i = 0
    for name, flist in sorted(fitdict.items()):
        if isinstance(flist, tuple):
            batchsize = flist[1]
            flist = flist[0]

        i += 1
        nbRuns = len(flist)
        print(name, nbRuns, 'runs',)

        if targetcutoff != None:
            if onlysuccessful:
                # filter out unsuccessful runs
                flist = filter(isSuccessful, flist)
                print(',', len(flist), 'of which were successful.')
            else:
                print
            # cut off irrelevant part
            flist = map(relevantPart, flist)

        if len(flist) == 0:
            continue

        if averageOverEvaluations:
            worstPerf = max(map(max, flist))
            if semilog:
                yPlot = list(reversed(power(10, ((array(range(resolution + 1)) / float(resolution)) *
                                             (log10(worstPerf) - log10(targetcutoff)) + log10(targetcutoff)))))
            else:
                yPlot = list(reversed((array(range(resolution + 1)) / float(resolution)) *
                                             (worstPerf - targetcutoff) + targetcutoff))
            xPlot = avgFoundAfter(yPlot, flist, batchsize, useMedian=useMedian)

        else:
            longestRun = max(map(len, flist))
            xPlot = array(range(longestRun)) * batchsize
            summed = zeros(longestRun)
            for l in flist:
                summed += paddedClipped(l, longestRun)
            yPlot = paddedClipped(summed / len(flist), longestRun)

        if semilog:
            semilogy()

        if varyplotsymbols:
            psymbol = plotsymbols[i % len(plotsymbols)]
        else:
            psymbol = '-'

        plot(xPlot, yPlot, psymbol, label=name)

    ylabel('-fitness')
    xlabel('number of evaluations')
    pylab.title(title)
    legend()


########NEW FILE########
__FILENAME__ = multiline
# $Id$
__author__ = 'Martin Felder and Frank Sehnke'

import math, imp
from matplotlib.lines import Line2D
from pylab import clf, plot, axes, show, xlabel, ylabel, savefig, ioff, draw_if_interactive


class MultilinePlotter:
    """  Basic plotting class build on pylab
  Implementing by instancing the class with the number of different plots to show.
  Every plot has an id so adding data is done by addData(id, xValue, yValue) of the given data point

  :todo: Add possibility to stick markers to the plots
  :todo: Some error checking and documentation
  :todo: Derive from this to make classes for trn/tst data plotting with different linestyles
  """

    # some nice color definitions for graphs (from colorbrewer.org)
    graphColor = [(0.894117647, 0.101960784, 0.109803922), \
        (0.215686275, 0.494117647, 0.721568627), \
        (0.301960784, 0.68627451, 0.290196078), \
        (0.596078431, 0.305882353, 0.639215686), \
        (1, 0.498039216, 0), \
        (1, 1, 0.2), \
        (0.650980392, 0.337254902, 0.156862745), \
        (0.968627451, 0.505882353, 0.749019608), \
        (0.6, 0.6, 0.6)]

    def __init__(self, maxLines=1, autoscale=0.0, **kwargs):
        """
    :key maxLines: Number of Plots to draw and so max ID.
    :key autoscale: If set to a factor > 1, axes are automatically expanded whenever out-range data points are added
    :var indexList: The x-component of the data points
    :var DataList: The y-component of the data points"""
        self.indexList = []
        self.dataList = []
        self.Lines = []
        self.autoscale = autoscale
        clf()
        self.Axes = axes(**kwargs)
        self.nbLines = 0
        self.defaultLineStyle = {}
        self._checkMaxId(maxLines - 1)
        self.replot = True           # is the plot still current?
        self.currentID = None
        self.offset = 0              # external references to IDs are modified by this

    def setOffset(self, offs):
        """ Set an offset that modifies all subsequent references to line IDs

    :key offs: The desired offset """
        self.offset = offs

    #def createFigure(self, size=[12,8], interactive=True):
        #""" initialize the graphics output window """
        ## FIXME: doesn work, because axes() in the constructor already creates a figure
        #pylab.figure(figsize=size)
        #if interactive: pylab.ion()

    def _checkMaxId(self, id):
        """ Appends additional lines as necessary

    :key id: Lines up to this id are added automatically """
        if id >= self.nbLines:
            for i in range(self.nbLines, id + 1):
                # create a new line with corresponding x/y data, and attach it to the plot
                l = Line2D([], [], color=self.graphColor[i % 9], **self.defaultLineStyle)
                self.Lines.append(l)
                self.Axes.add_line(l)
                self.indexList.append([])
                self.dataList.append([])
            self.nbLines = id + 1


    def addData(self, id0, x, y):
        """ The given data point or points is appended to the given line.

    :key id0: The plot ID (counted from 0) the data point(s) belong to.
    :key x: The x-component of the data point(s)
    :key y: The y-component of the data point(s)"""
        id = id0 + self.offset
        if not (isinstance(x, list) | isinstance(x, tuple)):
            self._checkMaxId(id)
            self.indexList[id].append(x)
            self.dataList[id].append(y)
            self.currentID = id
        else:
            for i, xi in enumerate(x):
                self.addData(id0, xi, y[i])
        self.replot = True

    def setData(self, id0, x, y):
        """ Data series id0 is replaced by the given lists

    :key id0: The plot ID (counted from 0) the data point(s) belong to.
    :key x: The x-component of the data points
    :key y: The y-component of the data points"""
        id = id0 + self.offset
        self._checkMaxId(id)
        self.indexList[id] = x
        self.dataList[id] = y
        self.replot = True

    def saveData(self, filename):
        """ Writes the data series for all points to a file

    :key filename: The name of the output file """
        file = open(filename, "w")
        for i in range(self.nbLines):
            datLen = len(self.indexList[i])
            for j in range(datLen):
                file.write(repr(self.indexList[i][j]) + "\n")
                file.write(repr(self.dataList[i][j]) + "\n")
        file.close()


    def setLabels(self, x='', y='', title=''):
        """ set axis labels and title """
        self.Axes.set_xlabel(x)
        self.Axes.set_ylabel(y)
        self.Axes.set_title(title)

    def setLegend(self, *args, **kwargs):
        """ hand parameters to the legend """
        self.Axes.legend(*args, **kwargs)

    def setLineStyle(self, id=None, **kwargs):
        """ hand parameters to the specified line(s), and set them as default for new lines

    :key id: The line or lines (list!) to be modified - defaults to last one added """
        if id is None:
            id = self.currentID

        if isinstance(id, list) | isinstance(id, tuple):
            # apply to specified list of lines
            self._checkMaxId(max(id) + self.offset)
            for i in id:
                self.Lines[i + self.offset].set(**kwargs)
        elif id >= 0:
            # apply to selected line
            self._checkMaxId(id + self.offset)
            self.Lines[id + self.offset].set(**kwargs)
        else:
            # apply to all lines
            for l in self.Lines:
                l.set(**kwargs)

        # set as new default linestyle
        if kwargs.has_key('color'):
            kwargs.popitem('color')
        self.defaultLineStyle = kwargs


    def update(self):
        """ Updates the current plot, if necessary """
        if not self.replot:
            return
        xr = list(self.Axes.get_xlim())
        yr = list(self.Axes.get_ylim())
        for i in range(self.nbLines):
            self.Lines[i].set_data(self.indexList[i], self.dataList[i])
            if self.autoscale > 1.0:
                if self.indexList[i][0] < xr[0]:
                    xr[0] = self.indexList[i][0]
                ymn = min(self.dataList[i])
                if ymn < yr[0]:
                    yr[0] = ymn
                while self.indexList[i][-1] > xr[1]:
                    xr[1] = (xr[1] - xr[0]) * self.autoscale + xr[0]
                ymx = max(self.dataList[i])
                while ymx > yr[1]:
                    yr[1] = (yr[1] - yr[0]) * self.autoscale + yr[0]
        if self.autoscale > 1.0:
            self.Axes.set_xlim(tuple(xr))
            self.Axes.set_ylim(tuple(yr))
            #self.Axes.draw()
        #pylab.show()
        draw_if_interactive()
        self.replot = False


    def show(self, xLabel='', yLabel='', Title='', popup=False, imgfile=None):
        """ Plots the data internally and saves an image of it to the plotting directory.

    :key title: The title of the plot.
    :key xLable: The label for the x-axis
    :key yLable: The label for the y-axis
    :key popup: also produce a popup window with the image?"""
        clf()
        for i in range(self.nbLines):
            plot(self.indexList[i], self.dataList[i])
        xlabel(xLabel)
        ylabel(yLabel)
        title(Title)
        if imgfile == None:
            imgfile = imp.find_module('pybrain')[1] + "/tools/plotting/plot.png"
        savefig(imgfile)
        if popup:
            ioff()
            show()


"""Small example to demonstrate how the plot class can be used"""
if __name__ == "__main__":
    pbplot = MultilinePlotter(7)
    for i in range(400000):
        if i / 100000 == i / 100000.0:
            for j in range(7):
                pbplot.addData(j, i, math.sqrt(float(i * (j + 1))))
    pbplot.show("WorldInteractions", "Fitness", "Example Plot", True)


########NEW FILE########
__FILENAME__ = quickvariations
__author__ = 'Tom Schaul, tom@idsia.ch'


from pybrain.utilities import subDict, dictCombinations
import pylab


def plotVariations(datalist, titles, genFun, varyperplot=None, prePlotFun=None, postPlotFun=None,
                   _differentiator=0.0, **optionlists):
    """ A tool for quickly generating a lot of variations of a plot.
    Generates a number of figures from a list of data (and titles).
    For each data item it produces one or more figures, each with one or more plots, while varying
    all options in optionlists (trying all combinations).
    :arg genFun: is the function that generates the curve to be plotted, for each set of options.
    :key varyperplot: determines which options are varied within a figure.
    :key prePlotFun: is called before the plots of a figure
    :key postPlotFun: is called after the plots of a figure (e.g. for realigning axes).
    """
    odl = subDict(optionlists, varyperplot, False)
    fdl = subDict(optionlists, varyperplot, True)
    # title contains file and non-varying parameters
    titadd1 = ''.join([k+'='+str(vs[0])[:min(5, len(str(vs[0])))]+' '
                    for k,vs in odl.items()
                    if len(vs) == 1])
    for x, tit in zip(datalist, titles):
        for figdict in sorted(dictCombinations(fdl.copy())):
            pylab.figure()

            # it also contains the parameters that don't vary per figure
            titadd2 = ''.join([k+'='+str(v)[:min(5, len(str(v)))]+' '
                               for k,v in figdict.items()])
            pylab.title(tit+'\n'+titadd1+titadd2)

            # code initializing the plot
            if prePlotFun is not None:
                prePlotFun(x)

            for i, odict in enumerate(sorted(dictCombinations(odl.copy()))):
                # concise labels
                lab = ''.join([k[:3]+'='+str(v)[:min(5, len(str(v)))]+'-'
                               for k,v in odict.items()
                               if len(odl[k]) > 1])
                if len(lab) > 0:
                    lab = lab[:-1]  # remove trailing '-'
                else:
                    lab = None
                generated = genFun(x, **dict(odict, **figdict))
                if generated is not None:
                    if len(generated) == 2:
                        xs, ys = generated
                    else:
                        ys = generated
                        xs = range(len(ys))
                    # the differentiator can slightly move the curves to be able to tell them apart if they overlap
                    if _differentiator != 0.0:
                        ys = generated+_differentiator*i

                    pylab.plot(xs, ys, label=lab)

            if postPlotFun is not None:
                postPlotFun(tit)
            # a legend is only necessary, if there are multiple plots
            if lab is not None:
                pylab.legend()

########NEW FILE########
__FILENAME__ = rankingfunctions
""" Ranking functions that are used in Black-box optimization, or for selection. """

__author__ = 'Daan Wierstra and Tom Schaul'

from pybrain.utilities import Named
from random import randint
from scipy import zeros, argmax, array, power, exp, sqrt, var, zeros_like, arange, mean, log


def rankedFitness(R):
    """ produce a linear ranking of the fitnesses in R.

    (The highest rank is the best fitness)"""
    #l = sorted(list(enumerate(R)), cmp = lambda a,b: cmp(a[1],b[1]))
    #l = sorted(list(enumerate(l)), cmp = lambda a,b: cmp(a[1],b[1]))
    #return array(map(lambda (r, dummy): r, l))
    res = zeros_like(R)
    l = zip(R, range(len(R)))
    l.sort()
    for i, (_, j) in enumerate(l):
        res[j] = i
    return res


def normalizedFitness(R):
    return array((R - mean(R)) / sqrt(var(R))).flatten()


class RankingFunction(Named):
    """ Default: ranked and scaled to [0,1]."""

    def __init__(self, **args):
        self.setArgs(**args)
        n = self.__class__.__name__
        for k, val in args.items():
            n += '-' + str(k) + '=' + str(val)
        self.name = n

    def __call__(self, R):
        """ :key R: one-dimensional array containing fitnesses. """
        res = rankedFitness(R)
        return res / float(max(res))


class TournamentSelection(RankingFunction):
    """ Standard evolution tournament selection, the returned array contains intergers for the samples that
    are selected indicating how often they are. """

    tournamentSize = 2

    def __call__(self, R):
        res = zeros(len(R))
        for i in range(len(R)):
            l = [i]
            for dummy in range(self.tournamentSize - 1):
                randindex = i
                while randindex == i:
                    randindex = randint(0, len(R) - 1)
                l.append(randindex)
            fits = map(lambda x: R[x], l)
            res[argmax(fits)] += 1
        return res


class SmoothGiniRanking(RankingFunction):
    """ a smooth ranking function that gives more importance to examples with better fitness.

    Rescaled to be between 0 and 1"""

    gini = 0.1
    linearComponent = 0.

    def __call__(self, R):
        def smoothup(x):
            """ produces a mapping from [0,1] to [0,1], with a specific gini coefficient. """
            return power(x, 2 / self.gini - 1)
        ranks = rankedFitness(R)
        res = zeros(len(R))
        for i in range(len(ranks)):
            res[i] = ranks[i] * self.linearComponent + smoothup(ranks[i] / float(len(R) - 1)) * (1 - self.linearComponent)
        res /= max(res)
        return res


class ExponentialRanking(RankingFunction):
    """ Exponential transformation (with a temperature parameter) of the rank values. """

    temperature = 10.

    def __call__(self, R):
        ranks = rankedFitness(R)
        ranks = ranks / (len(R) - 1.0)
        return exp(ranks * self.temperature)

class HansenRanking(RankingFunction):
    """ Ranking, as used in CMA-ES """

    def __call__(self, R):
        ranks = rankedFitness(R)
        return array([max(0., x) for x in log(len(R)/2.+1.0)-log(len(R)-array(ranks))])


class TopSelection(RankingFunction):
    """ Select the fraction of the best ranked fitnesses. """

    topFraction = 0.1

    def __call__(self, R):
        res = zeros(len(R))
        ranks = rankedFitness(R)
        cutoff = len(R) * (1. - self.topFraction)
        for i in range(len(R)):
            if ranks[i] >= cutoff:
                res[i] = 1.0
            else:
                res[i] = 0.0
        return res


class TopLinearRanking(TopSelection):
    """ Select the fraction of the best ranked fitnesses
    and scale them linearly between 0 and 1.  """

    topFraction = 0.2

    def __call__(self, R):
        res = zeros(len(R))
        ranks = rankedFitness(R)
        cutoff = len(R) * (1. - self.topFraction)
        for i in range(len(R)):
            if ranks[i] >= cutoff:
                res[i] = ranks[i] - cutoff
            else:
                res[i] = 0.0
        res /= max(res)
        return res

    def getPossibleParameters(self, numberOfSamples):
        x = 1. / float(numberOfSamples)
        return arange(x * 2, 1 + x, x)

    def setParameter(self, p):
        self.topFraction = p


class BilinearRanking(RankingFunction):
    """ Bi-linear transformation, rescaled. """

    bilinearFactor = 20

    def __call__(self, R):
        ranks = rankedFitness(R)
        res = zeros(len(R))
        transitionpoint = 4 * len(ranks) / 5
        for i in range(len(ranks)):
            if ranks[i] < transitionpoint:
                res[i] = ranks[i]
            else:
                res[i] = ranks[i] + (ranks[i] - transitionpoint) * self.bilinearFactor
        res /= max(res)
        return res



########NEW FILE########
__FILENAME__ = rlgluebridge
from __future__ import division

"""This module provides functionality to use pybrain with rlglue and to use it
for the rlcompetition.

The whole module has quite a hacky feel, which is due to the fact that
communication with subprocesses is not always easy and less often intended by
the original author.

So make changes with care and don't be surprised if you see some rude lines of
code.
"""


__author__ = 'Justin Bayer, bayerj@in.tum.de'


import exceptions
import logging
import os

from signal import SIGKILL #@UnresolvedImport
from subprocess import Popen, PIPE

from rlglue.agent.ClientAgent import ClientAgent #@UnresolvedImport
from rlglue.network.Network import kRetryTimeout as CLIENT_TIMEOUT #@UnresolvedImport
from rlglue.network.Network import kDefaultPort as DEFAULT_PORT #@UnresolvedImport
from rlglue.network.Network import kLocalHost as DEFAULT_HOST #@UnresolvedImport
from rlglue.types import Action as RLGlueAction #@UnresolvedImport
from scipy import array

from pybrain.structure.modules.module import Module
from pybrain.rl.agents import LearningAgent
from pybrain.utilities import threaded
from pybrain.tools.benchmark import BenchmarkDataSet


class RLGlueError(Exception): pass
class RLCompetitionNotFound(RLGlueError): pass


def adaptAgent(agent_klass):
    """Return a factory function that instantiates a pybrain agent and adapts
    it to the rlglue framework interface.

    :type   agent_klass:    subclass of some pybrain agent
    :key  agent_klass:    Some class that is to be adapted to the rlglue
                            framework

    """
    # TODO: return a real class instead of a function, so docstrings and such
    # are not lost.
    def inner(*args, **kwargs):
        return RlglueAgentAdapter(agent_klass, *args, **kwargs)
    return inner


def adaptAgentObject(agent_object):
    """Return an object that adapts a pybrain agent to the rlglue interface.
    """
    # This is pretty hacky: We first take a bogus agent with a bogus module
    # for our function adaptAgent to work, then substitue the bogus agent with
    # our actual agent.
    agent = adaptAgent(LearningAgent)(Module(1, 1))
    agent.agent = agent_object
    return agent


class RlglueAgentAdapter(object):
    """Wrapper class to use pybrain agents with the RLGlue library."""

    def __init__(self, klass, *args, **kwargs):
        """
        Create an object that adapts an object of class klass to the
        protocol of rlglue agents.

        :type   klass:    subclass of some pybrain agent
        :key  klass:    Some class that is to be adapted to the rlglue
                          framework
        """
        if not issubclass(klass, LearningAgent):
            raise ValueError("Supply a LearningAgent as first argument")

        self.agent = klass(*args, **kwargs)

        # TODO: At the  moment, learning is done after a certain amount of
        # steps - this is somehow logic of the agent, and not of the wrapper
        # Maybe there are some changes in the agent API needed.
        self.learnCycle = 1
        self.episodeCount = 1

    def agent_init(self, task_specification=None):
        """Give the agent a specification of the action and state space.

        Since pybrain agents are not using task specifications
        (they are already set up to the problem domain) the task_specification
        parameter is only there for API consistency, but it will be ignored.

        The specification for the specifications can be found here:
        http://rlai.cs.ualberta.ca/RLBB/TaskSpecification.html

        :type task_specification:   string
        """
        # This is (for now) actually a dummy method to satisfy the
        # RLGlue interface. It is the programmer's job to check wether an
        # experiment fits the agent object.
        self.agent.reset()

    def agent_start(self, firstObservation):
        """
        Return an action depending on the first observation.

        :type firstObservation:     Observation
        """
        self._integrateObservation(firstObservation)
        return self._getAction()

    def agent_step(self, reward, observation):
        """
        Return an action depending on an observation and a reward.

        :type reward:               number
        :type firstObservation:     Observation
        """
        self._giveReward(reward)
        self._integrateObservation(observation)
        return self._getAction()

    def agent_end(self, reward):
        """
        Give the last reward to the agent.

        :type reward: number
        """
        self._giveReward(reward)
        self.agent.newEpisode()
        self.episodeCount += 1
        if self.episodeCount % self.learnCycle == 0:
            self.agent.learn()
            self.agent.reset()

    def agent_cleanup(self):
        """This is called when an episode ends.

        Should be in one ratio to agent_init."""

    def agent_freeze(self):
        """Tell the agent to end training.

        Learning and exploration is stopped."""
        self.agent.disableTraining()

    def agent_message(self, message):
        # Originally thought to enable dynamic methods for agents, but this
        # does not make a lot of sense in a dynamic language (and in OO?)
        print("Message:", message)

    def _getAction(self):
        """
        Return a RLGlue action that is made out of a numpy array yielded by
        the hold pybrain agent.
        """
        action = RLGlueAction()
        action.doubleArray = self.agent.getAction().tolist()
        action.intArray = []
        return action

    def _integrateObservation(self, observation):
        """
        Take an RLGlue observation and convert it into a numpy array to feed
        it into the pybrain agent.

        :type observation:     Observation
        """
        observation = array(observation.doubleArray)
        self.agent.integrateObservation(observation)

    def _giveReward(self, reward):
        self.agent.giveReward(reward)


class RLCExperiment(object):
    """Class to abstract a subprocess that runs an rl-competition experiment
    on a given port."""

    def __init__(self, path, port=None, autoreconnect=None):
        """Instantiate an object with the given variables."""
        if os.name not in ('posix', 'mac'):
            raise NotImplementedError(
                    "Killing processes under win32 not supported")
        self.path = path
        self.port = port
        self.autoreconnect = autoreconnect
        self.running = False

    def start(self):
        """Start the experiment."""
        self.running = True
        env = {}
        if self.port:
            env['RLGLUE_PORT'] = self.port
        if self.autoreconnect:
            env['RLGLUE_AUTORECONNECT'] = self.autoreconnect

        cwd = self.path[:self.path.rfind("/") + 1]
        self.process = Popen(self.path, env=env, shell=True, cwd=cwd,
                             bufsize=0, stdin=PIPE, stdout=PIPE, stderr=PIPE)

        # We have to fetch some PIDs from the subprocess' output to kill the
        # processes afterwards.
        rlg_pidstr = self.process.stdout.readline()
        de_pidstr = self.process.stdout.readline()
        get_pid = lambda s: int(s[s.find("PID=") + 4:].strip())
        self.rlglue_pid = get_pid(rlg_pidstr)
        self.dynenv_pid = get_pid(de_pidstr)

        logging.info("Environment started (%i, %i. %i)" %
                     (self.process.pid, self.rlglue_pid, self.dynenv_pid))

        # We need to consume the processes standard output and error output,
        # otherwise the program will wait for it to be consumed. This is
        # actually pretty nasty, but it has to be done.
        @threaded(lambda x: None, True)
        def consume(flo):
            while self.running:
                flo.read(512)

        consume(self.process.stdout)
        consume(self.process.stderr)

    def stop(self):
        """Stop the experiment."""
        # There is some bad bad process shooting going on here. It seems as if
        # the rl-competition software has some hickups when the process is
        # started via subprocess and does not always end its childprocesses
        # properly. Luckily, their pids are printed out, and we can grab those
        # and kill the processes.

        # Shoot child processes mercilessly
        self.running = False
        self._killProcess(self.rlglue_pid)
        self._killProcess(self.dynenv_pid)
        self._killProcess(self.process.pid)

        logging.info("Environment ended.")

    def _killProcess(self, pid):
        try:
            os.kill(pid, SIGKILL) #@UndefinedVariable
        except exceptions.OSError:
            # Explicitly silence if the process has already been killed
            pass

    def __del__(self):
        if self.running:
            self.stop()


class RlCompBenchmark(object):
    """Class to run benchmarks of pybrain agents on the rl-competition 2007
    environments.
    """

    port = DEFAULT_PORT
    overwrite = False

    def __init__(self, agents, port=None):
        self.agents = agents
        if port: self.port = port

    def run(self):
        """Run the benchmark. All agents are tested loop times against the
        environment and statistics for each run are saved into the benchmark
        directory benchmarkDir.

        """
        # Create benchmark directory: the desired name plus the current date
        # and time.
        try:
            os.makedirs(self.benchmarkDir)
        except OSError, e:
            if not "File exists" in str(e):
                raise e

        for name, agent_klass in self.agents:
            todo = xrange(self.loops)
            if not self.overwrite:
                # If overwrite is set to false, we will only do the experiments
                # that have not been done.

                # index gets the index of a benchmark file out of the filename.
                index = lambda x: int(x[x.rfind("-") + 1:])
                done = set(index(i) for i in os.listdir(self.benchmarkDir)
                           if i.startswith("%s-" % name))
                todo = (i for i in todo if i not in done)
            for j in todo:
                logging.info("Starting agent %s's loop #%i" % (name, j + 1))
                # Make a clean copy of the agent for every run
                # Start subprocess that gives us the experiment
                agent = agent_klass()
                stats = self.testAgent(agent)
                # Dump stats to the given directory
                self.saveStats(name + "-%i" % j, stats)

    def testAgent(self, agent):
        """Test an agent once on the experiment and return a benchmark
        dataset."""
        return testAgent(self.path, agent, self.port)

    def saveStats(self, name, dataset):
        """Save the given dataset to a"""
        filename = os.path.join(self.benchmarkDir, name)
        dataset.saveToFile(filename, arraysonly=True)
        logging.info("Saved statistics to %s" % filename)


def testAgent(path, agent, port=DEFAULT_PORT):
    """Test an agent once on a rlcompetition experiment.

    Path specifies the executable file of the rl competition.
    """
    agent = adaptAgentObject(BenchmarkingAgent(agent))

    experiment = RLCExperiment(path, str(port))
    experiment.start()
    # This method is provided by rlglue and makes a client to be runnable
    # over the network.
    clientAgent = ClientAgent(agent)
    clientAgent.connect(DEFAULT_HOST, port, CLIENT_TIMEOUT)
    logging.info("Agent connected")
    clientAgent.runAgentEventLoop()
    clientAgent.close()
    logging.info("Agent finished")
    experiment.stop()

    return agent.agent.benchmark


# This class is defined here and not in benchmarks, since use of it is
# discouraged when not interacting with the rlglue framework. When using
# pybrain environments, other ways should be found.
class BenchmarkingAgent(object):
    """Agent that is used as a middleware to record benchmarks into a
    BenchmarkDataSet.
    """

    def __init__(self, agent):
        """Return a wrapper around the given agent."""
        if hasattr(agent, 'benchmark') or  hasattr(agent, 'agent'):
            raise ValueError("Wrapped agent must not define a benchmark or" +
                             "an agent attribute.")
        self.agent = agent
        self.benchmark = BenchmarkDataSet()

        # For episodewide statistics
        self.__rewards = []

    def giveReward(self, reward):
        self.agent.giveReward(reward)
        self.__rewards.append(reward)

    def newEpisode(self):
        episodeLength = len(self.__rewards)
        avgReward = sum(self.__rewards) / episodeLength
        self.benchmark.appendLinked(avgReward, episodeLength)
        self.__rewards = []
        return self.agent.newEpisode()

    def __getattribute__(self, key):
        try:
            return super(BenchmarkingAgent, self).__getattribute__(key)
        except AttributeError:
            agent = super(BenchmarkingAgent, self).__getattribute__('agent')
            return getattr(agent, key)

    def __setattribute__(self, key, value):
        if hasattr(self, key):
            setattr(self, key, value)
        else:
            agent = super(BenchmarkingAgent, self).__getattribute__('agent')
            setattr(agent, key, value)



########NEW FILE########
__FILENAME__ = shortcuts
__author__ = 'Tom Schaul and Thomas Rueckstiess'


from itertools import chain
import logging
from sys import exit as errorexit
from pybrain.structure.networks.feedforward import FeedForwardNetwork
from pybrain.structure.networks.recurrent import RecurrentNetwork
from pybrain.structure.modules import BiasUnit, SigmoidLayer, LinearLayer, LSTMLayer
from pybrain.structure.connections import FullConnection, IdentityConnection

try:
    from arac.pybrainbridge import _RecurrentNetwork, _FeedForwardNetwork
except ImportError as e:
    logging.info("No fast networks available: %s" % e)


class NetworkError(Exception): pass


def buildNetwork(*layers, **options):
    """Build arbitrarily deep networks.

    `layers` should be a list or tuple of integers, that indicate how many
    neurons the layers should have. `bias` and `outputbias` are flags to
    indicate whether the network should have the corresponding biases; both
    default to True.

    To adjust the classes for the layers use the `hiddenclass` and  `outclass`
    parameters, which expect a subclass of :class:`NeuronLayer`.

    If the `recurrent` flag is set, a :class:`RecurrentNetwork` will be created,
    otherwise a :class:`FeedForwardNetwork`.

    If the `fast` flag is set, faster arac networks will be used instead of the
    pybrain implementations."""
    # options
    opt = {'bias': True,
           'hiddenclass': SigmoidLayer,
           'outclass': LinearLayer,
           'outputbias': True,
           'peepholes': False,
           'recurrent': False,
           'fast': False,
    }
    for key in options:
        if key not in opt.keys():
            raise NetworkError('buildNetwork unknown option: %s' % key)
        opt[key] = options[key]

    if len(layers) < 2:
        raise NetworkError('buildNetwork needs 2 arguments for input and output layers at least.')

    # Bind the right class to the Network name
    network_map = {
        (False, False): FeedForwardNetwork,
        (True, False): RecurrentNetwork,
    }
    try:
        network_map[(False, True)] = _FeedForwardNetwork
        network_map[(True, True)] = _RecurrentNetwork
    except NameError:
        if opt['fast']:
            raise NetworkError("No fast networks available.")
    if opt['hiddenclass'].sequential or opt['outclass'].sequential:
        if not opt['recurrent']:
            # CHECKME: a warning here?
            opt['recurrent'] = True
    Network = network_map[opt['recurrent'], opt['fast']]
    n = Network()
    # linear input layer
    n.addInputModule(LinearLayer(layers[0], name='in'))
    # output layer of type 'outclass'
    n.addOutputModule(opt['outclass'](layers[-1], name='out'))
    if opt['bias']:
        # add bias module and connection to out module, if desired
        n.addModule(BiasUnit(name='bias'))
        if opt['outputbias']:
            n.addConnection(FullConnection(n['bias'], n['out']))
    # arbitrary number of hidden layers of type 'hiddenclass'
    for i, num in enumerate(layers[1:-1]):
        layername = 'hidden%i' % i
        if issubclass(opt['hiddenclass'], LSTMLayer):
            n.addModule(opt['hiddenclass'](num, peepholes=opt['peepholes'], name=layername))
        else:
            n.addModule(opt['hiddenclass'](num, name=layername))
        if opt['bias']:
            # also connect all the layers with the bias
            n.addConnection(FullConnection(n['bias'], n[layername]))
    # connections between hidden layers
    for i in range(len(layers) - 3):
        n.addConnection(FullConnection(n['hidden%i' % i], n['hidden%i' % (i + 1)]))
    # other connections
    if len(layers) == 2:
        # flat network, connection from in to out
        n.addConnection(FullConnection(n['in'], n['out']))
    else:
        # network with hidden layer(s), connections from in to first hidden and last hidden to out
        n.addConnection(FullConnection(n['in'], n['hidden0']))
        n.addConnection(FullConnection(n['hidden%i' % (len(layers) - 3)], n['out']))

    # recurrent connections
    if issubclass(opt['hiddenclass'], LSTMLayer):
        if len(layers) > 3:
            errorexit("LSTM networks with > 1 hidden layers are not supported!")
        n.addRecurrentConnection(FullConnection(n['hidden0'], n['hidden0']))

    n.sortModules()
    return n


def _buildNetwork(*layers, **options):
    """This is a helper function to create different kinds of networks.

    `layers` is a list of tuples. Each tuple can contain an arbitrary number of
    layers, each being connected to the next one with IdentityConnections. Due
    to this, all layers have to have the same dimension. We call these tuples
    'parts.'

    Afterwards, the last layer of one tuple is connected to the first layer of
    the following tuple by a FullConnection.

    If the keyword argument bias is given, BiasUnits are added additionally with
    every FullConnection.

    Example:

        _buildNetwork(
            (LinearLayer(3),),
            (SigmoidLayer(4), GaussianLayer(4)),
            (SigmoidLayer(3),),
        )
    """
    bias = options['bias'] if 'bias' in options else False

    net = FeedForwardNetwork()
    layerParts = iter(layers)
    firstPart = iter(layerParts.next())
    firstLayer = firstPart.next()
    net.addInputModule(firstLayer)

    prevLayer = firstLayer

    for part in chain(firstPart, layerParts):
        new_part = True
        for layer in part:
            net.addModule(layer)
            # Pick class depending on whether we entered a new part
            if new_part:
                ConnectionClass = FullConnection
                if bias:
                    biasUnit = BiasUnit('BiasUnit for %s' % layer.name)
                    net.addModule(biasUnit)
                    net.addConnection(FullConnection(biasUnit, layer))
            else:
                ConnectionClass = IdentityConnection
            new_part = False
            conn = ConnectionClass(prevLayer, layer)
            net.addConnection(conn)
            prevLayer = layer
    net.addOutputModule(layer)
    net.sortModules()
    return net



########NEW FILE########
__FILENAME__ = svmdata
__author__ = 'Michael Isik'

from pybrain.datasets import SupervisedDataSet


class SVMData(SupervisedDataSet):
    """ Reads data files in LIBSVM/SVMlight format """
    def __init__(self, filename=None):
        SupervisedDataSet.__init__(self, 0, 0)

        self.nCls = 0
        self.nSamples = 0
        self.classHist = {}
        self.filename = ''
        if filename is not None:
            self.loadData(filename)


    def loadData(self, fname):
        """ decide which format the data is in """
        self.filename = fname
        if fname.find('.mat') >= 0:
            self.loadMATdata(fname)
        elif fname.find('.svm') >= 0:
            self.loadSVMdata(fname)
        else:
            # dataset consists of raw ascii columns
            self.loadRawData(fname)


    def _setDataFields(self, x, y):
        if not len(x): raise Exception("no input data found")
        SupervisedDataSet.__init__(self, len(x[0]), 1)
        self.setField('input'  , x)
        self.setField('target' , y)

        flat_labels = list(self.getField('target').flatten())
        classes = list(set(flat_labels))
        self._classes = classes
        self.nClasses = len(classes)
        for class_ in classes:
            self.classHist[class_] = flat_labels.count(class_)



    def loadMATdata(self, fname):
        """ read Matlab file containing one variable called 'data' which is an array
            nSamples x nFeatures+1 and contains the class in the first column """
        from mlabwrap import mlab #@UnresolvedImport
        from numpy import float
        d = mlab.load(fname)
        self.nSamples = d.data.shape[0]
        x = []
        y = []
        for i in range(self.nSamples):
            label = int(d.data[i, 0])


            x.append(d.data[i, 1:].astype(float).tolist())
            y.append([ float(label) ])
        self._setDataFields(x, y)

    def loadSVMdata(self, fname):
        """ read svm sparse format from file 'fname' (with labels only)
            output: [attributes[], labels[]] """

        x = []
        y = []
        nFeatMax = 0
        for line in open(fname, 'r').readlines():
            # format is:
            # <class>  <featnr>:<featval>  <featnr>:<featval> ...
            # (whereby featnr starts at 1)
            if not line: break
            line = line.split()
            label = float(line[0])


            feat = []
            nextidx = 1
            for r in line[1:]:
                # construct list of features, taking care of sparsity
                (idx, val) = r.split(':')
                idx = int(idx)
                for _ in range(nextidx, idx):
                    feat.append(0) # zzzzwar hier ein bug??
                feat.append(float(val))
                nextidx = idx + 1
            nFeat = len(feat)
            if nFeatMax < nFeat: nFeatMax = nFeat

            x.append(feat)
            y.append([ label ])
            self.nSamples += 1

        for xi in x:
            while len(xi) < nFeatMax:
                xi.append(0.)

        self._setDataFields(x, y)


    def loadRawData(self, fname):
        """ read svm sparse format from file 'fname' (with labels only)
            output: [attributes[], labels[]] """
        targetfile = open(fname.replace('data', 'targets'), 'r')
        x = []
        y = []
        for line in open(fname, 'r').readlines():
            if not line: break
            targline = targetfile.readline()
            targline = map(int, targline.split())
            for i, v in enumerate(targline):
                if v:
                    label = i
                    break
            feat = map(float, line.split())
            x.append(feat)
            y.append([float(label)])
            self.nSamples += 1
        self.nCls = len(targline)
        targetfile.close()
        self._setDataFields(x, y)

    def getNbClasses(self):
        return self.nCls

    def getNbSamples(self):
        return self.nSamples

    def getTargets(self):
        """ return the targets of the dataset, preserving the current sample pointer """
        self.storePointer()
        self.reset()
        targets = []
        while not self.endOfSequences():
            input, target, dummy = self.getSample()
            targets.append(target)
        self.recallPointer()
        return targets

    def getFileName(self):
        return self.filename

    def getClass(self, idx):
        return self._classes[idx]

    def getClassHistogram(self):
        """ return number of values per class as list of integers """
        return self.classHist


############################################################################
if __name__ == '__main__':
    d = SVMData()
    d.clear()
    d.loadSVMdata(r'M:\Data\Johan\svm\trials_scale.svm')
    print(d.getSample())
    print(d.getSample())


########NEW FILE########
__FILENAME__ = validation
__author__ = 'Michael Isik'


from numpy.random import permutation
from numpy import array, array_split, apply_along_axis, concatenate, ones, dot, delete, append, zeros, argmax
import copy
from pybrain.datasets.importance import ImportanceDataSet
from pybrain.datasets.sequential import SequentialDataSet
from pybrain.datasets.supervised import SupervisedDataSet



class Validator(object):
    """ This class provides methods for the validation of calculated output
        values compared to their destined target values. It does
        not know anything about modules or other pybrain stuff. It just works
        on arrays, hence contains just the core calculations.

        The class has just classmethods, as it is used as kind of namespace
        instead of an object definition.
    """
    @classmethod
    def classificationPerformance(cls, output, target):
        """ Returns the hit rate of the outputs compared to the targets.

            :arg output: array of output values
            :arg target: array of target values
        """
        output = array(output)
        target = array(target)
        assert len(output) == len(target)
        n_correct = sum(output == target)
        return float(n_correct) / float(len(output))

    @classmethod
    def ESS(cls, output, target):
        """ Returns the explained sum of squares (ESS).

            :arg output: array of output values
            :arg target: array of target values
        """
        return sum((output - target) ** 2)

    @classmethod
    def MSE(cls, output, target, importance=None):
        """ Returns the mean squared error. The multidimensional arrays will get
            flattened in order to compare them.

            :arg output: array of output values
            :arg target: array of target values
            :key importance: each squared error will be multiplied with its
                corresponding importance value. After summing
                up these values, the result will be divided by the
                sum of all importance values for normalization
                purposes.
        """
        # assert equal shapes
        output = array(output)
        target = array(target)
        assert output.shape == target.shape
        if importance is not None:
            assert importance.shape == target.shape
            importance = importance.flatten()

        # flatten structures
        output = output.flatten()
        target = target.flatten()

        if importance is None:
            importance = ones(len(output))


        # calculate mse
        squared_error = (output - target) ** 2
        mse = dot(squared_error, importance) / sum(importance)


        return mse



class ClassificationHelper(object):
    """ This class provides helper methods for classification, like the
        conversion of one-of-many data to class indices data.

        The class has just classmethods, as it is used as kind of namespace
        instead of an object definition.
    """
    @classmethod
    def oneOfManyToClasses(cls, data):
        """ Converts data in one-of-many format to class indices format and
            and returns the result.

            :arg data: array of vectors, that are in the one-of-many format.
                         Each vector will be converted to the index of the
                         component with the maximum value.
        """
        return apply_along_axis(argmax, 1, data)





class SequenceHelper(object):
    """ This class provides helper methods for sequence handling.

        The class has just classmethods, as it is used as kind of namespace
        instead of an object definition.
    """
    @classmethod
    def getSequenceEnds(cls, dataset):
        """ Returns the indices of the last elements of the sequences stored
            inside dataset.

            :arg dataset: Must implement :class:`SequentialDataSet`
        """
        sequence_ends = delete(dataset.getField('sequence_index') - 1, 0)
        sequence_ends = append(sequence_ends, dataset.getLength() - 1)
#        print(sequence_ends; exit())
        sequence_ends = array(sequence_ends)
        return sequence_ends

    @classmethod
    def getSequenceStarts(cls, dataset):
        """ Returns the indices of the first elements of the sequences stored
            inside dataset.

            :arg dataset: Must implement :class:`SequentialDataSet`
        """
        return  list(dataset.getField('sequence_index'))

    @classmethod
    def getSequenceEndsImportance(cls, dataset):
        """ Returns the importance values of the last elements of the sequences
            stored inside dataset.

            :arg dataset: Must implement :class:`ImportanceDataSet`
        """
        importance = zeros(dataset.getLength())
        importance[cls.getSequenceEnds(dataset)] = 1.
        return importance





class ModuleValidator(object):
    """ This class provides methods for the validation of calculated output
        values compared to their destined target values. It especially handles
        pybrains modules and dataset classes.
        For the core calculations, the Validator class is used.

        The class has just classmethods, as it is used as kind of namespace
        instead of an object definition.
    """
    @classmethod
    def classificationPerformance(cls, module, dataset):
        """ Returns the hit rate of the module's output compared to the targets
            stored inside dataset.

            :arg module: Object of any subclass of pybrain's Module type
            :arg dataset: Dataset object at least containing the fields
                'input' and 'target' (for example SupervisedDataSet)
        """
        return ModuleValidator.validate(
            Validator.classificationPerformance,
            module,
            dataset)

    @classmethod
    def MSE(cls, module, dataset):
        """ Returns the mean squared error.

            :arg module: Object of any subclass of pybrain's Module type
            :arg dataset: Dataset object at least containing the fields
                'input' and 'target' (for example SupervisedDataSet)
        """
        return ModuleValidator.validate(
            Validator.MSE,
            module,
            dataset)


    @classmethod
    def validate(cls, valfunc, module, dataset):
        """ Abstract validate function, that is heavily used by this class.
            First, it calculates the module's output on the dataset.
            In advance, it compares the output to the target values of the dataset
            through the valfunc function and returns the result.

            :arg valfunc: A function expecting arrays for output, target and
                importance (optional). See Validator.MSE for an example.
            :arg module:  Object of any subclass of pybrain's Module type
            :arg dataset: Dataset object at least containing the fields
                'input' and 'target' (for example SupervisedDataSet)
        """
        target = dataset.getField('target')
        output = ModuleValidator.calculateModuleOutput(module, dataset)

        if isinstance(dataset, ImportanceDataSet):
            importance = dataset.getField('importance')
            return valfunc(output, target, importance)
        else:
            return valfunc(output, target)


    @classmethod
    def _calculateModuleOutputSequential(cls, module, dataset):
        """ Calculates the module's output on the dataset. Especially designed
            for datasets storing sequences.
            After a sequence is fed to the module, it has to be resetted.

            :arg dataset: Dataset object of type SequentialDataSet or subclass.
        """
        outputs = []
        for seq in dataset._provideSequences():
            module.reset()
            for i in xrange(len(seq)):
                output = module.activate(seq[i][0])
                outputs.append(output.copy())
        outputs = array(outputs)
        return outputs


    @classmethod
    def calculateModuleOutput(cls, module, dataset):
        """ Calculates the module's output on the dataset. Can be called with
            any type of dataset.

            :arg dataset: Any Dataset object containing an 'input' field.
        """
        if isinstance(dataset, SequentialDataSet) or isinstance(dataset, ImportanceDataSet):
            return cls._calculateModuleOutputSequential(module, dataset)
        else:
            module.reset()
            input = dataset.getField('input')
            output = array([module.activate(inp) for inp in input])
            return output





class CrossValidator(object):
    """ Class for crossvalidating data.
        An object of CrossValidator must be supplied with a trainer that contains
        a module and a dataset.
        Then the dataset ist shuffled and split up into n parts of equal length.

        A clone of the trainer and its module is made, and trained with n-1 parts
        of the split dataset. After training, the module is validated with
        the n'th part of the dataset that was not used during training.

        This is done for each possible combination of n-1 dataset pieces.
        The the mean of the calculated validation results will be returned.
    """
    def __init__(self, trainer, dataset, n_folds=5, valfunc=ModuleValidator.classificationPerformance, **kwargs):
        """ :arg trainer: Trainer containing a module to be trained
            :arg dataset: Dataset for training and testing
            :key n_folds: Number of pieces, the dataset will be splitted to
            :key valfunc: Validation function. Should expect a module and a dataset.
                            E.g. ModuleValidator.MSE()
            :key others: see setArgs() method
        """
        self._trainer = trainer
        self._dataset = dataset
        self._n_folds = n_folds
        self._calculatePerformance = valfunc
        self._max_epochs = None
        self.setArgs(**kwargs)

    def setArgs(self, **kwargs):
        """ Set the specified member variables.

        :key max_epochs: maximum number of epochs the trainer should train the module for.
        :key verbosity: set verbosity level
        """
        for key, value in kwargs.items():
            if key in ("verbose", "ver", "v"):
                self._verbosity = value
            elif key in ("max_epochs"):
                self._max_epochs = value

    def validate(self):
        """ The main method of this class. It runs the crossvalidation process
            and returns the validation result (e.g. performance).
        """
        dataset = self._dataset
        trainer = self._trainer
        n_folds = self._n_folds
        l = dataset.getLength()
        inp = dataset.getField("input")
        tar = dataset.getField("target")
        indim = dataset.indim
        outdim = dataset.outdim
        assert l > n_folds

        perms = array_split(permutation(l), n_folds)

        perf = 0.
        for i in range(n_folds):
            # determine train indices
            train_perms_idxs = range(n_folds)
            train_perms_idxs.pop(i)
            temp_list = []
            for train_perms_idx in train_perms_idxs:
                temp_list.append(perms[ train_perms_idx ])
            train_idxs = concatenate(temp_list)

            # determine test indices
            test_idxs = perms[i]

            # train
            #print("training iteration", i)
            train_ds = SupervisedDataSet(indim, outdim)
            train_ds.setField("input"  , inp[train_idxs])
            train_ds.setField("target" , tar[train_idxs])
            trainer = copy.deepcopy(self._trainer)
            trainer.setData(train_ds)
            if not self._max_epochs:
                trainer.train()
            else:
                trainer.trainEpochs(self._max_epochs)

            # test
            #print("testing iteration", i)
            test_ds = SupervisedDataSet(indim, outdim)
            test_ds.setField("input"  , inp[test_idxs])
            test_ds.setField("target" , tar[test_idxs])
#            perf += self.getPerformance( trainer.module, dataset )
            perf += self._calculatePerformance(trainer.module, dataset)

        perf /= n_folds
        return perf

#    def getPerformance( self, module, dataset ):
#        inp    = dataset.getField("input")
#        tar    = dataset.getField("target")
#        indim  = module.indim
#        outdim = module.outdim

#        def forward(inp):
#            out = empty(outdim)
#            module._forwardImplementation(inp,out)
#            return out

#        out = apply_along_axis(forward,1,inp)
#        return self._calculatePerformance(out,tar)
#        return self._calculatePerformance(module, dataset)


    def _calculatePerformance(self, output, target):
        raise NotImplementedError()



def testOnSequenceData(module, dataset):
    """ Fetch targets and calculate the modules output on dataset.
    Output and target are in one-of-many format. The class for each sequence is
    determined by first summing the probabilities for each individual sample over
    the sequence, and then finding its maximum."""
    target = dataset.getField("target")
    output = ModuleValidator.calculateModuleOutput(module, dataset)

    # determine last indices of the sequences inside dataset
    ends = SequenceHelper.getSequenceEnds(dataset)
    ##format = "%d"*len(ends)
    summed_output = zeros(dataset.outdim)
    # class_output and class_target will store class labels instead of
    # one-of-many values
    class_output = []
    class_target = []
    for j in xrange(len(output)):
        # sum up the output values of one sequence
        summed_output += output[j]
#            print(j, output[j], " --> ", summed_output)
        # if we reached the end of the sequence
        if j in ends:
            # convert summed_output and target to class labels
            class_output.append(argmax(summed_output))
            class_target.append(argmax(target[j]))

            # reset the summed_output to zeros
            summed_output = zeros(dataset.outdim)

    ##print(format % tuple(class_output))
    ##print(format % tuple(class_target))

    class_output = array(class_output)
    class_target = array(class_target)
#    print(class_target)
#    print(class_output)
    return Validator.classificationPerformance(class_output, class_target)






########NEW FILE########
__FILENAME__ = deepbelief
# -*- coding: utf-8 -*-

__author__ = 'Justin S Bayer, bayer.justin@googlemail.com'
__version__ = '$Id$'


import copy

from pybrain.datasets import SupervisedDataSet, UnsupervisedDataSet
from pybrain.structure import BiasUnit, FeedForwardNetwork, FullConnection
from pybrain.structure.networks.rbm import Rbm
from pybrain.structure.modules.neuronlayer import NeuronLayer
from pybrain.supervised.trainers import Trainer
from pybrain.unsupervised.trainers.rbm import (RbmBernoulliTrainer,
                                               RbmGaussTrainer)


class DeepBeliefTrainer(Trainer):
    """Trainer for deep networks.

    Trains the network by greedily training layer after layer with the
    RbmGibbsTrainer.

    The network that is being trained is assumed to be a chain of layers that
    are connected with full connections and feature a bias each.

    The behaviour of the trainer is undefined for other cases.
    """

    trainers = {
        'bernoulli': RbmBernoulliTrainer,
        'gauss': RbmGaussTrainer,
    }

    def __init__(self, net, dataset, epochs=50,
                 cfg=None, distribution='bernoulli'):
        if isinstance(dataset, SupervisedDataSet):
            self.datasetfield = 'input'
        elif isinstance(dataset, UnsupervisedDataSet):
            self.datasetfield = 'sample'
        else:
            raise ValueError("Wrong dataset class.")
        self.net = net
        self.net.sortModules()
        self.dataset = dataset
        self.epochs = epochs
        self.cfg = cfg
        self.trainerKlass = self.trainers[distribution]

    def trainRbm(self, rbm, dataset):
        trainer = self.trainerKlass(rbm, dataset, self.cfg)
        for _ in xrange(self.epochs):
            trainer.train()
        return rbm

    def iterRbms(self):
        """Yield every two layers as an rbm."""
        layers = [i for i in self.net.modulesSorted
                  if isinstance(i, NeuronLayer) and not isinstance(i, BiasUnit)]
        # There will be a single bias.
        bias = [i for i in self.net.modulesSorted if isinstance(i, BiasUnit)][0]
        layercons = (self.net.connections[i][0] for i in layers)
        # The biascons will not be sorted; we have to sort them to zip nicely
        # with the corresponding layers.
        biascons = self.net.connections[bias]
        biascons.sort(key=lambda c: layers.index(c.outmod))
        modules = zip(layers, layers[1:], layercons, biascons)
        for visible, hidden, layercon, biascon in modules:
            rbm = Rbm.fromModules(visible, hidden, bias,
                                  layercon, biascon)
            yield rbm

    def train(self):
        # We will build up a network piecewise in order to create a new dataset
        # for each layer.
        dataset = self.dataset
        piecenet = FeedForwardNetwork()
        piecenet.addInputModule(copy.deepcopy(self.net.inmodules[0]))
        # Add a bias
        bias = BiasUnit()
        piecenet.addModule(bias)
        # Add the first visible layer
        firstRbm = self.iterRbms().next()
        visible = copy.deepcopy(firstRbm.visible)
        piecenet.addModule(visible)
        # For saving the rbms and their inverses
        self.invRbms = []
        self.rbms = []
        for rbm in self.iterRbms():
            self.net.sortModules()
            # Train the first layer with an rbm trainer for `epoch` epochs.
            trainer = self.trainerKlass(rbm, dataset, self.cfg)
            for _ in xrange(self.epochs):
                trainer.train()
            self.invRbms.append(trainer.invRbm)
            self.rbms.append(rbm)
            # Add the connections and the hidden layer of the rbm to the net.
            hidden = copy.deepcopy(rbm.hidden)
            biascon = FullConnection(bias, hidden)
            biascon.params[:] = rbm.biasWeights
            con = FullConnection(visible, hidden)
            con.params[:] = rbm.weights

            piecenet.addConnection(biascon)
            piecenet.addConnection(con)
            piecenet.addModule(hidden)
            # Overwrite old outputs
            piecenet.outmodules = [hidden]
            piecenet.outdim = rbm.hiddenDim
            piecenet.sortModules()

            dataset = UnsupervisedDataSet(rbm.hiddenDim)
            for sample, in self.dataset:
                new_sample = piecenet.activate(sample)
                dataset.addSample(new_sample)
            visible = hidden

########NEW FILE########
__FILENAME__ = rbm
__author__ = ('Christian Osendorfer, osendorf@in.tum.de;'
              'Justin S Bayer, bayerj@in.tum.de'
              'SUN Yi, yi@idsia.ch')

from scipy import random, outer, zeros, ones

from pybrain.datasets import SupervisedDataSet, UnsupervisedDataSet
from pybrain.supervised.trainers import Trainer
from pybrain.utilities import abstractMethod


class RbmGibbsTrainerConfig:
    def __init__(self):
        self.batchSize = 10		# how many samples in a batch

        # training rate
        self.rWeights = 0.1
        self.rHidBias = 0.1
        self.rVisBias = 0.1

        # Several configurations, I have no idea why they are here...
        self.weightCost = 0.0002

        self.iniMm = 0.5		# initial momentum
        self.finMm = 0.9		# final momentum
        self.mmSwitchIter = 5	# at which iteration we switch the momentum
        self.maxIter = 9		# how many iterations

        self.visibleDistribution = 'bernoulli'


class RbmGibbsTrainer(Trainer):
    """Class for training rbms with contrastive divergence."""

    def __init__(self, rbm, dataset, cfg=None):
        self.rbm = rbm
        self.invRbm = rbm.invert()
        self.dataset = dataset
        self.cfg = RbmGibbsTrainerConfig() if cfg is None else cfg

        if isinstance(self.dataset, SupervisedDataSet):
            self.datasetField = 'input'
        elif isinstance(self.dataset, UnsupervisedDataSet):
            self.datasetField = 'sample'

    def train(self):
        self.trainOnDataset(self.dataset)

    def trainOnDataset(self, dataset):
        """This function trains the RBM using the same algorithm and
        implementation presented in:
        http://www.cs.toronto.edu/~hinton/MatlabForSciencePaper.html"""
        cfg = self.cfg
        for rows in dataset.randomBatches(self.datasetField, cfg.batchSize):
            olduw, olduhb, olduvb = \
                zeros((self.rbm.visibleDim, self.rbm.hiddenDim)), \
                zeros(self.rbm.hiddenDim), zeros(self.rbm.visibleDim)

            for t in xrange(cfg.maxIter):
                #print("*** Iteration %2d **************************************" % t)

                params = self.rbm.params
                params = params.reshape((self.rbm.visibleDim, self.rbm.hiddenDim))
                biasParams = self.rbm.biasParams

                mm = cfg.iniMm if t < cfg.mmSwitchIter else cfg.finMm

                w, hb, vb = self.calcUpdateByRows(rows)

                #print("Delta: ")
                #print("Weight: ",)
                #print(w)
                #print("Visible bias: ",)
                #print(vb)
                #print("Hidden bias: ",)
                #print(hb)
                #print("")

                olduw = uw = olduw * mm + \
                	cfg.rWeights * (w - cfg.weightCost * params)
                olduhb = uhb = olduhb * mm + cfg.rHidBias * hb
                olduvb = uvb = olduvb * mm + cfg.rVisBias * vb

                #print("Delta after momentum: ")
                #print("Weight: ",)
                #print(uw)
                #print("Visible bias: ",)
                #print(uvb)
                #print("Hidden bias: ",)
                #print(uhb)
                #print("")

                # update the parameters of the original rbm
                params += uw
                biasParams += uhb

                # Create a new inverted rbm with correct parameters
                invBiasParams = self.invRbm.biasParams
                invBiasParams += uvb
                self.invRbm = self.rbm.invert()
                self.invRbm.biasParams[:] = invBiasParams

                #print("Updated ")
                #print("Weight: ",)
                #print(self.rbm.connections[self.rbm['visible']][0].params.reshape( \)
                #    (self.rbm.indim, self.rbm.outdim))
                #print("Visible bias: ",)
                #print(self.invRbm.connections[self.invRbm['bias']][0].params)
                #print("Hidden bias: ",)
                #print(self.rbm.connections[self.rbm['bias']][0].params)
                #print("")

    def calcUpdateByRow(self, row):
        """This function trains the RBM using only one data row.
        Return a 3-tuple consiting of updates for (weightmatrix,
        hidden bias weights, visible bias weights)."""

        # a) positive phase
        poshp = self.rbm.activate(row)	# compute the posterior probability
        pos = outer(row, poshp)       	# fraction from the positive phase
        poshb = poshp
        posvb = row

        # b) the sampling & reconstruction
        sampled = self.sampler(poshp)
        recon = self.invRbm.activate(sampled)	# the re-construction of data

        # c) negative phase
        neghp = self.rbm.activate(recon)
        neg = outer(recon, neghp)
        neghb = neghp
        negvb = recon

        # compute the raw delta
        # !!! note that this delta is only the 'theoretical' delta
        return self.updater(pos, neg, poshb, neghb, posvb, negvb)

    def sampler(self, probabilities):
        abstractMethod()

    def updater(self, pos, neg, poshb, neghb, posvb, negvb):
        abstractMethod()

    def calcUpdateByRows(self, rows):
        """Return a 3-tuple constisting of update for (weightmatrix,
        hidden bias weights, visible bias weights)."""

        delta_w, delta_hb, delta_vb = \
            zeros((self.rbm.visibleDim, self.rbm.hiddenDim)), \
            zeros(self.rbm.hiddenDim), zeros(self.rbm.visibleDim)

        for row in rows:
            dw, dhb, dvb = self.calcUpdateByRow(row)
            delta_w += dw
            delta_hb += dhb
            delta_vb += dvb

        delta_w /= len(rows)
        delta_hb /= len(rows)
        delta_vb /= len(rows)

        # !!! note that this delta is only the 'theoretical' delta
        return delta_w, delta_hb, delta_vb


class RbmBernoulliTrainer(RbmGibbsTrainer):

    def sampler(self, probabilities):
        result = probabilities > random.rand(self.rbm.hiddenDim)
        return result.astype('int32')

    def updater(self, pos, neg, poshb, neghb, posvb, negvb):
        return pos - neg, poshb - neghb, posvb - negvb


class RbmGaussTrainer(RbmGibbsTrainer):

    def __init__(self, rbm, dataset, cfg=None):
        super(RbmGaussTrainer, self).__init__(rbm, dataset, cfg)
        #samples = self.dataset[self.datasetField]
        # self.visibleVariances = samples.var(axis=0)
        self.visibleVariances = ones(rbm.net.outdim)

    def sampler(self, probabilities):
        return random.normal(probabilities, self.visibleVariances)

    def updater(self, pos, neg, poshb, neghb, posvb, negvb):
        pos = pos / self.visibleVariances
        return pos - neg, poshb - neghb, posvb - negvb




########NEW FILE########
__FILENAME__ = utilities
from __future__ import with_statement

__author__ = 'Tom Schaul, tom@idsia.ch; Justin Bayer, bayerj@in.tum.de'

import gc
import pickle
import logging
import threading
import os
import operator

from itertools import count
from math import sqrt
from random import random, choice

from scipy import where, array, exp, zeros, size, mat, median

# file extension for load/save protocol mapping
known_extensions = {
    'mat': 'matlab',
    'txt': 'ascii',
    'svm': 'libsvm',
    'pkl': 'pickle',
    'nc' : 'netcdf' }


def abstractMethod():
    """ This should be called when an abstract method is called that should have been
    implemented by a subclass. It should not be called in situations where no implementation
    (i.e. a 'pass' behavior) is acceptable. """
    raise NotImplementedError('Method not implemented!')


def drawIndex(probs, tolerant=False):
    """ Draws an index given an array of probabilities.

    :key tolerant: if set to True, the array is normalized to sum to 1.  """
    if not sum(probs) < 1.00001 or not sum(probs) > 0.99999:
        if tolerant:
            probs /= sum(probs)
        else:
            print(probs, 1 - sum(probs))
            raise ValueError()
    r = random()
    s = 0
    for i, p in enumerate(probs):
        s += p
        if s > r:
            return i
    return choice(range(len(probs)))


def drawGibbs(vals, temperature=1.):
    """ Return the index of the sample drawn by a softmax (Gibbs). """
    if temperature == 0:
        # randomly pick one of the values with the max value.
        m = max(vals)
        best = []
        for i, v in enumerate(vals):
            if v == m:
                best.append(i)
        return choice(best)
    else:
        temp = vals / temperature

        # make sure we keep the exponential bounded (between +20 and -20)
        temp += 20 - max(temp)
        if min(temp) < -20:
            for i, v in enumerate(temp):
                if v < -20:
                    temp[i] = -20
        temp = exp(temp)
        temp /= sum(temp)
        return drawIndex(temp)


def iterCombinations(tup):
    """ all possible of integer tuples of the same dimension than tup, and each component being
    positive and strictly inferior to the corresponding entry in tup. """
    if len(tup) == 1:
        for i in range(tup[0]):
            yield (i,)
    elif len(tup) > 1:
        for prefix in iterCombinations(tup[:-1]):
            for i in range(tup[-1]):
                yield tuple(list(prefix) + [i])


def setAllArgs(obj, argdict):
    """ set all those internal variables which have the same name than an entry in the
    given object's dictionary.
    This function can be useful for quick initializations. """

    xmlstore = isinstance(obj, XMLBuildable)
    for n in argdict.keys():
        if hasattr(obj, n):
            setattr(obj, n, argdict[n])
            if xmlstore:
                obj.argdict[n] = argdict[n]
        else:
            print('Warning: parameter name', n, 'not found!')
            if xmlstore:
                if not hasattr(obj, '_unknown_argdict'):
                    obj._unknown_argdict = {}
                obj._unknown_argdict[n] = argdict[n]


def linscale(d, lim):
    """ utility function to linearly scale array d to the interval defined by lim """
    return (d - d.min())*(lim[1] - lim[0]) + lim[0]


def percentError(out, true):
    """ return percentage of mismatch between out and target values (lists and arrays accepted) """
    arrout = array(out).flatten()
    wrong = where(arrout != array(true).flatten())[0].size
    return 100. * float(wrong) / float(arrout.size)


def formatFromExtension(fname):
    """Tries to infer a protocol from the file extension."""
    _base, ext = os.path.splitext(fname)
    if not ext:
        return None
    try:
        format = known_extensions[ext.replace('.', '')]
    except KeyError:
        format = None
    return format


class XMLBuildable(object):
    """ subclasses of this can be losslessly stored in XML, and
    automatically reconstructed on reading. For this they need to store
    their construction arguments in the variable <argdict>. """

    argdict = None

    def setArgs(self, **argdict):
        if not self.argdict:
            self.argdict = {}
        setAllArgs(self, argdict)


class Serializable(object):
    """Class that implements shortcuts to serialize an object.

    Serialization is done by various formats. At the moment, only 'pickle' is
    supported.
    """

    def saveToFileLike(self, flo, format=None, **kwargs):
        """Save the object to a given file like object in the given format.
        """
        format = 'pickle' if format is None else format
        save = getattr(self, "save_%s" % format, None)
        if save is None:
            raise ValueError("Unknown format '%s'." % format)
        save(flo, **kwargs)

    @classmethod
    def loadFromFileLike(cls, flo, format=None):
        """Load the object to a given file like object with the given protocol.
        """
        format = 'pickle' if format is None else format
        load = getattr(cls, "load_%s" % format, None)
        if load is None:
            raise ValueError("Unknown format '%s'." % format)
        return load(flo)

    def saveToFile(self, filename, format=None, **kwargs):
        """Save the object to file given by filename."""
        if format is None:
            # try to derive protocol from file extension
            format = formatFromExtension(filename)
        with file(filename, 'wb') as fp:
            self.saveToFileLike(fp, format, **kwargs)

    @classmethod
    def loadFromFile(cls, filename, format=None):
        """Return an instance of the class that is saved in the file with the
        given filename in the specified format."""
        if format is None:
            # try to derive protocol from file extension
            format = formatFromExtension(filename)
        with file(filename, 'rbU') as fp:
            obj = cls.loadFromFileLike(fp, format)
            obj.filename = filename
            return obj

    def save_pickle(self, flo, protocol=0):
        pickle.dump(self, flo, protocol)

    @classmethod
    def load_pickle(cls, flo):
        return pickle.load(flo)


class Named(XMLBuildable):
    """Class whose objects are guaranteed to have a unique name."""

    _nameIds = count(0)

    def getName(self):
        logging.warning("Deprecated, use .name property instead.")
        return self.name

    def setName(self, newname):
        logging.warning("Deprecated, use .name property instead.")
        self.name = newname

    def _getName(self):
        """Returns the name, which is generated if it has not been already."""
        if self._name is None:
            self._name = self._generateName()
        return self._name

    def _setName(self, newname):
        """Change name to newname. Uniqueness is not guaranteed anymore."""
        self._name = newname

    _name = None
    name = property(_getName, _setName)

    def _generateName(self):
        """Return a unique name for this object."""
        return "%s-%i" % (self.__class__.__name__, next(self._nameIds))

    def __repr__(self):
        """ The default representation of a named object is its name. """
        return "<%s '%s'>" % (self.__class__.__name__, self.name)


def fListToString(a_list, a_precision=3):
    """ Returns a string representing a list of floats with a given precision """
    s_list = ", ".join(("%g" % round(x, a_precision)).ljust(a_precision+3)
                      for x in a_list)
    return "[%s]" % s_list

def tupleRemoveItem(tup, index):
    """ remove the item at position index of the tuple and return a new tuple. """
    l = list(tup)
    return tuple(l[:index] + l[index + 1:])


def confidenceIntervalSize(stdev, nbsamples):
    """ Determine the size of the confidence interval, given the standard deviation and the number of samples.
    t-test-percentile: 97.5%, infinitely many degrees of freedom,
    therefore on the two-sided interval: 95% """
    # CHECKME: for better precision, maybe get the percentile dynamically, from the scipy library?
    return 2 * 1.98 * stdev / sqrt(nbsamples)


def trace(func):
    def inner(*args, **kwargs):
        print("%s: %s, %s" % (func.__name__, args, kwargs))
        return func(*args, **kwargs)
    return inner


def threaded(callback=lambda * args, **kwargs: None, daemonic=False):
    """Decorate  a function to run in its own thread and report the result
    by calling callback with it."""
    def innerDecorator(func):
        def inner(*args, **kwargs):
            target = lambda: callback(func(*args, **kwargs))
            t = threading.Thread(target=target)
            t.setDaemon(daemonic)
            t.start()
        return inner
    return innerDecorator


def garbagecollect(func):
    """Decorate a function to invoke the garbage collector after each execution.
    """
    def inner(*args, **kwargs):
        result = func(*args, **kwargs)
        gc.collect()
        return result
    return inner


def memoize(func):
    """Decorate a function to 'memoize' results by holding it in a cache that
    maps call arguments to returns."""
    cache = {}
    def inner(*args, **kwargs):
        # Dictionaries and lists are unhashable
        args = tuple(args)
        # Make a set for checking in the cache, since the order of
        # .iteritems() is undefined
        kwargs_set = frozenset(kwargs.iteritems())
        if (args, kwargs_set) in cache:
            result = cache[args, kwargs_set]
        else:
            result = func(*args, **kwargs)
            cache[args, kwargs_set] = result
        return result
    return inner


def storeCallResults(obj, verbose=False):
    """Pseudo-decorate an object to store all evaluations of the function in the returned list."""
    results = []
    oldcall = obj.__class__.__call__
    def newcall(*args, **kwargs):
        result = oldcall(*args, **kwargs)
        results.append(result)
        if verbose:
            print(result)
        return result
    obj.__class__.__call__ = newcall
    return results


def multiEvaluate(repeat):
    """Decorate a function to evaluate repeatedly with the same arguments, and return the average result """
    def decorator(func):
        def inner(*args, **kwargs):
            result = 0.
            for dummy in range(repeat):
                result += func(*args, **kwargs)
            return result / repeat
        return inner
    return decorator


def _import(name):
    """Return module from a package.

    These two are equivalent:

        > from package import module as bar
        > bar = _import('package.module')

    """
    mod = __import__(name)
    components = name.split('.')
    for comp in components[1:]:
        try:
            mod = getattr(mod, comp)
        except AttributeError:
            raise ImportError("No module named %s" % mod)
    return mod


# tools for binary Gray code manipulation:

def int2gray(i):
    """ Returns the value of an integer in Gray encoding."""
    return i ^ (i >> 1)


def gray2int(g, size):
    """ Transforms a Gray code back into an integer. """
    res = 0
    for i in reversed(range(size)):
        gi = (g >> i) % 2
        if i == size - 1:
            bi = gi
        else:
            bi = bi ^ gi
        res += bi * 2 ** i
    return res


def asBinary(i):
    """ Produces a string from an integer's binary representation.
    (preceding zeros removed). """
    if i > 1:
        if i % 2 == 1:
            return asBinary(i >> 1) + '1'
        else:
            return asBinary(i >> 1) + '0'
    else:
        return str(i)


def one_to_n(val, maxval):
    """ Returns a 1-in-n binary encoding of a non-negative integer. """
    a = zeros(maxval, float)
    a[val] = 1.
    return a


def n_to_one(arr):
    """ Returns the reverse of a 1-in-n binary encoding. """
    return where(arr == 1)[0][0]


def canonicClassString(x):
    """ the __class__ attribute changed from old-style to new-style classes... """
    if isinstance(x, object):
        return repr(x.__class__).split("'")[1]
    else:
        return repr(x.__class__)


def decrementAny(tup):
    """ the closest tuples to tup: decrementing by 1 along any dimension.
    Never go into negatives though. """
    res = []
    for i, x in enumerate(tup):
        if x > 0:
            res.append(tuple(list(tup[:i]) + [x - 1] + list(tup[i + 1:])))
    return res


def reachable(stepFunction, start, destinations, _alreadyseen=None):
    """ Determines the subset of destinations that can be reached from a set of starting positions,
    while using stepFunction (which produces a list of neighbor states) to navigate.
    Uses breadth-first search.
    Returns a dictionary with reachable destinations and their distances.
    """
    if len(start) == 0 or len(destinations) == 0:
        return {}
    if _alreadyseen is None:
        _alreadyseen = []
    _alreadyseen.extend(start)

    # dict with distances to destinations
    res = {}
    for s in start:
        if s in destinations:
            res[s] = 0
            start.remove(s)

    # do one step
    new = set()
    for s in start:
        new.update(stepFunction(s))
    new.difference_update(_alreadyseen)
    ndestinations = list(destinations)

    for s in list(new):
        if s in destinations:
            res[s] = 1
            new.remove(s)
            ndestinations.remove(s)
            _alreadyseen.append(s)

    # recursively do the rest
    deeper = reachable(stepFunction, new, ndestinations, _alreadyseen)

    # adjust distances
    for k, val in deeper.items():
        res[k] = val + 1
    return res


def flood(stepFunction, fullSet, initSet, relevant=None):
    """ Returns a list of elements of fullSet linked to some element of initSet
    through the neighborhood-setFunction (which must be defined on all elements of fullSet).

    :key relevant: (optional) list of relevant elements: stop once all relevant elements are found.
    """
    if fullSet is None:
        flooded = set(initSet)
    else:
        full = set(fullSet)
        flooded = full.intersection(set(initSet))
        if relevant is None:
            relevant = full.copy()
    if relevant:
        relevant = set(relevant)

    change = flooded.copy()
    while len(change)>0:
        new = set()
        for m in change:
            if fullSet is None:
                new.update(stepFunction(m))
            else:
                new.update(full.intersection(stepFunction(m)))
        change = new.difference(flooded)
        flooded.update(change)
        if relevant is not None and relevant.issubset(flooded):
            break
    return list(flooded)


def crossproduct(ss, row=None, level=0):
    """Returns the cross-product of the sets given in `ss`."""
    if row is None:
        row = []
    if len(ss) > 1:
        return reduce(operator.add,
                      [crossproduct(ss[1:], row + [i], level + 1) for i in ss[0]])
    else:
        return [row + [i] for i in ss[0]]


def permute(arr, permutation):
    """Return an array like arr but with elements permuted.

    Only the first dimension is permuted, which makes it possible to permute
    blocks of the input.

    arr can be anything as long as it's indexable."""
    return array([arr[i] for i in permutation])


def permuteToBlocks(arr, blockshape):
    """Permute an array so that it consists of linearized blocks.

    Example: A two-dimensional array of the form

        0  1  2  3
        4  5  6  7
        8  9  10 11
        12 13 14 15

    would be turned into an array like this with (2, 2) blocks:

        0 1 4 5 2 3 6 7 8 9 12 13 10 11 14 15
    """
    if len(blockshape) < 2:
        raise ValueError("Need more than one dimension.")
    elif len(blockshape) == 2:
        blockheight, blockwidth = blockshape
        return permuteToBlocks2d(arr, blockheight, blockwidth)
    elif len(blockshape) == 3:
        blockdepth, blockheight, blockwidth = blockshape
        return permuteToBlocks3d(arr, blockdepth, blockheight, blockwidth)
    else:
        raise NotImplementedError("Only for dimensions 2 and 3.")


def permuteToBlocks3d(arr, blockdepth, blockheight, blockwidth):
    depth, height, width = arr.shape
    arr_ = arr.reshape(height * depth, width)
    arr_ = permuteToBlocks2d(arr_, blockheight, blockwidth)
    arr_.shape = depth, height * width
    return permuteToBlocks2d(arr_, blockdepth, blockwidth * blockheight)


def permuteToBlocks2d(arr, blockheight, blockwidth):
    _height, width = arr.shape
    arr = arr.flatten()
    new = zeros(size(arr))
    for i in xrange(size(arr)):
        blockx = (i % width) / blockwidth
        blocky = i / width / blockheight
        blockoffset = blocky * width / blockwidth + blockx
        blockoffset *= blockwidth * blockheight
        inblockx = i % blockwidth
        inblocky = (i / width) % blockheight
        j = blockoffset + inblocky * blockwidth + inblockx
        new[j] = arr[i]
    return new


def triu2flat(m):
    """ Flattens an upper triangular matrix, returning a vector of the
    non-zero elements. """
    dim = m.shape[0]
    res = zeros(dim * (dim + 1) / 2)
    index = 0
    for row in range(dim):
        res[index:index + dim - row] = m[row, row:]
        index += dim - row
    return res


def flat2triu(a, dim):
    """ Produces an upper triangular matrix of dimension dim from the elements of the given vector. """
    res = zeros((dim, dim))
    index = 0
    for row in range(dim):
        res[row, row:] = a[index:index + dim - row]
        index += dim - row
    return res


def blockList2Matrix(l):
    """ Converts a list of matrices into a corresponding big block-diagonal one. """
    dims = [m.shape[0] for m in l]
    s = sum(dims)
    res = zeros((s, s))
    index = 0
    for i in range(len(l)):
        d = dims[i]
        m = l[i]
        res[index:index + d, index:index + d] = m
        index += d
    return res


def blockCombine(l):
    """ Produce a matrix from a list of lists of its components. """
    l = [map(mat, row) for row in l]
    hdims = [m.shape[1] for m in l[0]]
    hs = sum(hdims)
    vdims = [row[0].shape[0] for row in l]
    vs = sum(vdims)
    res = zeros((hs, vs))
    vindex = 0
    for i, row in enumerate(l):
        hindex = 0
        for j, m in enumerate(row):
            res[vindex:vindex + vdims[i], hindex:hindex + hdims[j]] = m
            hindex += hdims[j]
        vindex += vdims[i]
    return res


def avgFoundAfter(decreasingTargetValues, listsOfActualValues, batchSize=1, useMedian=False):
    """ Determine the average number of steps to reach a certain value (for the first time),
    given a list of value sequences.
    If a value is not always encountered, the length of the longest sequence is used.
    Returns an array. """
    from scipy import sum
    numLists = len(listsOfActualValues)
    longest = max(map(len, listsOfActualValues))
    # gather a list of indices of first encounters
    res = [[0] for _ in range(numLists)]
    for tval in decreasingTargetValues:
        for li, l in enumerate(listsOfActualValues):
            lres = res[li]
            found = False
            for i in range(lres[-1], len(l)):
                if l[i] <= tval:
                    lres.append(i)
                    found = True
                    break
            if not found:
                lres.append(longest)
    tmp = array(res)
    if useMedian:
        resx = median(tmp, axis=0)[1:]
    else:
        resx = sum(tmp, axis=0)[1:] / float(numLists)
    return resx * batchSize


class DivergenceError(Exception):
    """ Raised when an algorithm diverges. """


def matchingDict(d, selection, require_existence=False):
    """ Determines if the dictionary d conforms to the specified selection,
    i.e. if a (key, x) is in the selection, then if key is in d as well it must be x
    or contained in x (if x is a list). """
    for k, v in selection.items():
        if k in d:
            if isinstance(v, list):
                if d[k] not in v:
                    return False
            else:
                if d[k] != v:
                    return False
        elif require_existence:
            return False
    return True


def subDict(d, allowedkeys, flip=False):
    """ Returns a new dictionary with a subset of the entries of d
    that have on of the (dis-)allowed keys."""
    res = {}
    for k, v in d.items():
        if (k in allowedkeys) ^ flip:
            res[k] = v
    return res


def dictCombinations(listdict):
    """ Iterates over dictionaries that go through every possible combination
    of key-value pairs as specified in the lists of values for each key in listdict."""
    listdict = listdict.copy()
    if len(listdict) == 0:
        return [{}]
    k, vs = listdict.popitem()
    res = dictCombinations(listdict)
    if isinstance(vs, list) or isinstance(vs, tuple):
        res = [dict(d, **{k:v}) for d in res for v in sorted(set(vs))]
    else:
        res = [dict(d, **{k:vs}) for d in res]
    return res


def r_argmax(v):
    """ Acts like scipy argmax, but break ties randomly. """
    if len(v) == 1:
        return 0
    maxbid = max(v)
    maxbidders = [i for (i, b) in enumerate(v) if b==maxbid]
    return choice(maxbidders)

def all_argmax(x):
    """ Return the indices of all values that are equal to the maximum: no breaking ties. """
    m = max(x)
    return [i for i, v in enumerate(x) if v == m]

def dense_orth(dim):
    """ Constructs a dense orthogonal matrix. """
    from scipy import rand
    from scipy.linalg import orth
    return orth(rand(dim, dim))
    
def sparse_orth(d):
    """ Constructs a sparse orthogonal matrix.
    
    The method is described in:
    Gi-Sang Cheon et al., Constructions for the sparsest orthogonal matrices,
    Bull. Korean Math. Soc 36 (1999) No.1 pp.199-129
    """
    from scipy.sparse import eye
    from scipy import r_, pi, sin, cos
    if d%2 == 0:
        seq = r_[0:d:2,1:d-1:2]
    else:
        seq = r_[0:d-1:2,1:d:2]
    Q = eye(d,d).tocsc()
    for i in seq:
        theta = random() * 2 * pi
        flip = (random() - 0.5)>0;
        Qi = eye(d,d).tocsc()
        Qi[i,i] = cos(theta)
        Qi[(i+1),i] = sin(theta)
        if flip > 0:
            Qi[i,(i+1)] = -sin(theta)
            Qi[(i+1),(i+1)] = cos(theta)
        else:
            Qi[i,(i+1)] = sin(theta)
            Qi[(i+1),(i+1)] = -cos(theta)            
        Q = Q*Qi;
    return Q

def xhash(arr):
    """ Hashing function for arrays. Use with care. """
    import hashlib
    return hashlib.sha1(arr).hexdigest()

def binArr2int(arr):
    """ Convert a binary array into its (long) integer representation. """
    from numpy import packbits
    tmp2 = packbits(arr.astype(int))
    return sum(val * 256 ** i for i, val in enumerate(tmp2[::-1])) 
        
def uniqueArrays(vs):
    """ create a set of arrays """
    resdic = {}
    for v in vs:
        resdic[xhash(v)] = v
    return resdic.values()    
    

def seedit(seed=0):
    """ Fixed seed makes for repeatability, but there may be two different
    random number generators involved. """
    import random
    import numpy
    random.seed(seed)
    numpy.random.seed(seed)


    
def weightedUtest(g1, w1, g2, w2):
    """ Determines the confidence level of the assertion:
    'The values of g2 are higher than those of g1'.  
    (adapted from the scipy.stats version)
    
    Twist: here the elements of each group have associated weights, 
    corresponding to how often they are present (i.e. two identical entries with 
    weight w are equivalent to one entry with weight 2w).
    Reference: "Studies in Continuous Black-box Optimization", Schaul, 2011 [appendix B].
    
    TODO: make more efficient for large sets. 
    """
    from scipy.stats.distributions import norm
    import numpy
    n1 = sum(w1)
    n2 = sum(w2)
    u1 = 0.
    for x1, wx1 in zip(g1, w1):
        for x2, wx2 in zip(g2, w2):
            if x1 == x2:
                u1 += 0.5 * wx1 * wx2
            elif x1 > x2:
                u1 += wx1 * wx2
    mu = n1*n2/2.
    sigu = numpy.sqrt(n1*n2*(n1+n2+1)/12.)
    z = (u1 - mu) / sigu
    conf = norm.cdf(z)
    return conf 


########NEW FILE########
