__FILENAME__ = conf
# -*- coding: utf-8 -*-

# General information about the project.
project = u'patsy'
copyright = u'2011-2013, Nathaniel J. Smith'

import sys
print "python exec:", sys.executable
print "sys.path:", sys.path
try:
    import numpy
    print "numpy: %s, %s" % (numpy.__version__, numpy.__file__)
except ImportError:
    print "no numpy"
try:
    import matplotlib
    print "matplotlib: %s, %s" % (matplotlib.__version__, matplotlib.__file__)
except ImportError:
    print "no matplotlib"
try:
    import IPython
    print "ipython: %s, %s" % (IPython.__version__, IPython.__file__)
except ImportError:
    print "no ipython"

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
import sys, os
sys.path.insert(0, os.getcwd() + "/..")
import patsy
version = patsy.__version__
# The full version, including alpha/beta/rc tags.
release = version

#
# scikits.sparse documentation build configuration file, created by
# sphinx-quickstart on Sat Dec 12 22:10:41 2009.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.append(os.path.abspath('sphinxext'))

# -- General configuration -----------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.doctest', 'sphinx.ext.pngmath',
              'sphinx.ext.intersphinx',
              'IPython.sphinxext.ipython_directive',
              'IPython.sphinxext.ipython_console_highlighting',
              ]

# Undocumented trick: if we def setup here in conf.py, it gets called just
# like an extension's setup function.
def setup(app):
    app.add_javascript("show-code.js")
    app.add_javascript("facebox.js")
    app.add_stylesheet("facebox.css")

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8'

# The master toctree document.
master_doc = 'index'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of documents that shouldn't be included in the build.
#unused_docs = []

# List of directories, relative to source directory, that shouldn't be searched
# for source files.
exclude_trees = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  Major themes that come with
# Sphinx are currently 'default' and 'sphinxdoc'.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_use_modindex = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'patsydoc'


# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'patsy.tex', u'patsy Documentation',
   u'Nathaniel J. Smith', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_use_modindex = True

# -- Custom extra options

autoclass_content = "both"

intersphinx_mapping = {"python": ("http://docs.python.org", None),
                       "numpy": ("http://docs.scipy.org/doc/numpy",
                                 None),
                       "pandas": ('http://pandas.pydata.org/pandas-docs/stable/',
                                  None),
                       }

autodoc_member_order = "source"

########NEW FILE########
__FILENAME__ = add_predictors
def add_predictors(base_formula, extra_predictors):
    # Interpret formula in caller's environment:
    env = EvalEnvironment.capture(1)
    desc = ModelDesc.from_formula(base_formula, env)
    # Using LookupFactor here ensures that everything will work correctly even
    # if one of the column names in extra_columns is named like "weight.in.kg"
    # or "sys.exit()" or "LittleBobbyTables()".
    desc.rhs_termlist += [Term([LookupFactor(p)]) for p in extra_predictors]
    return desc

########NEW FILE########
__FILENAME__ = example_lm
import numpy as np
from patsy import dmatrices, build_design_matrices

class LM(object):
    """An example ordinary least squares linear model class, analogous to R's
    lm() function. Don't use this in real life, it isn't properly tested."""
    def __init__(self, formula_like, data={}):
        y, x = dmatrices(formula_like, data, 1)
        self.nobs = x.shape[0]
        self.betas, self.rss, _, _ = np.linalg.lstsq(x, y)
        self._y_design_info = y.design_info
        self._x_design_info = x.design_info

    def __repr__(self):
        summary = ("Ordinary least-squares regression\n"
                   "  Model: %s ~ %s\n"
                   "  Regression (beta) coefficients:\n"
                   % (self._y_design_info.describe(),
                      self._x_design_info.describe()))
        for name, value in zip(self._x_design_info.column_names, self.betas):
            summary += "    %s:  %0.3g\n" % (name, value[0])
        return summary

    def predict(self, new_data):
        (new_x,) = build_design_matrices([self._x_design_info.builder],
                                         new_data)
        return np.dot(new_x, self.betas)

    def loglik(self, new_data):
        (new_y, new_x) = build_design_matrices([self._y_design_info.builder,
                                                self._x_design_info.builder],
                                               new_data)
        new_pred = np.dot(new_x, self.betas)
        sigma2 = self.rss / self.nobs
        # It'd be more elegant to use scipy.stats.norm.logpdf here, but adding
        # a dependency on scipy makes the docs build more complicated:
        Z = -0.5 * np.log(2 * np.pi * sigma2)
        return Z + -0.5 * (new_y - new_x) ** 2/sigma2

########NEW FILE########
__FILENAME__ = example_treatment
import numpy as np

class MyTreat(object):
    def __init__(self, reference=0):
        self.reference = reference

    def code_with_intercept(self, levels):
        return ContrastMatrix(np.eye(len(levels)),
                              ["[My.%s]" % (level,) for level in levels])

    def code_without_intercept(self, levels):
        eye = np.eye(len(levels) - 1)
        contrasts = np.vstack((eye[:self.reference, :],
                               np.zeros((1, len(levels) - 1)),
                               eye[self.reference:, :]))
        suffixes = ["[MyT.%s]" % (level,) for level in
                    levels[:self.reference] + levels[self.reference + 1:]]
        return ContrastMatrix(contrasts, suffixes)

########NEW FILE########
__FILENAME__ = build
# This file is part of Patsy
# Copyright (C) 2011-2013 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# This file defines the core design matrix building functions.

# These are made available in the patsy.* namespace
__all__ = ["design_matrix_builders", "DesignMatrixBuilder",
           "build_design_matrices"]

import numpy as np
from patsy import PatsyError
from patsy.categorical import (guess_categorical,
                               CategoricalSniffer,
                               categorical_to_int)
from patsy.util import (atleast_2d_column_default,
                        have_pandas, have_pandas_categorical,
                        asarray_or_pandas)
from patsy.design_info import DesignMatrix, DesignInfo
from patsy.redundancy import pick_contrasts_for_term
from patsy.desc import ModelDesc
from patsy.eval import EvalEnvironment
from patsy.contrasts import code_contrast_matrix, Treatment
from patsy.compat import itertools_product, OrderedDict
from patsy.missing import NAAction

if have_pandas:
    import pandas

class _MockFactor(object):
    def __init__(self, name="MOCKMOCK"):
        self._name = name

    def eval(self, state, env):
        return env["mock"]

    def name(self):
        return self._name

def _max_allowed_dim(dim, arr, factor):
    if arr.ndim > dim:
        msg = ("factor '%s' evaluates to an %s-dimensional array; I only "
               "handle arrays with dimension <= %s"
               % (factor.name(), arr.ndim, dim))
        raise PatsyError(msg, factor)

def test__max_allowed_dim():
    from nose.tools import assert_raises
    f = _MockFactor()
    _max_allowed_dim(1, np.array(1), f)
    _max_allowed_dim(1, np.array([1]), f)
    assert_raises(PatsyError, _max_allowed_dim, 1, np.array([[1]]), f)
    assert_raises(PatsyError, _max_allowed_dim, 1, np.array([[[1]]]), f)
    _max_allowed_dim(2, np.array(1), f)
    _max_allowed_dim(2, np.array([1]), f)
    _max_allowed_dim(2, np.array([[1]]), f)
    assert_raises(PatsyError, _max_allowed_dim, 2, np.array([[[1]]]), f)

class _NumFactorEvaluator(object):
    def __init__(self, factor, state, expected_columns):
        # This one instance variable is part of our public API:
        self.factor = factor
        self._state = state
        self._expected_columns = expected_columns

    # Returns either a 2d ndarray, or a DataFrame, plus is_NA mask
    def eval(self, data, NA_action):
        result = self.factor.eval(self._state, data)
        result = atleast_2d_column_default(result, preserve_pandas=True)
        _max_allowed_dim(2, result, self.factor)
        if result.shape[1] != self._expected_columns:
            raise PatsyError("when evaluating factor %s, I got %s columns "
                                "instead of the %s I was expecting"
                                % (self.factor.name(), self._expected_columns,
                                   result.shape[1]),
                                self.factor)
        if not np.issubdtype(np.asarray(result).dtype, np.number):
            raise PatsyError("when evaluating numeric factor %s, "
                                "I got non-numeric data of type '%s'"
                                % (self.factor.name(), result.dtype),
                                self.factor)
        return result, NA_action.is_numerical_NA(result)

def test__NumFactorEvaluator():
    from nose.tools import assert_raises
    naa = NAAction()
    f = _MockFactor()
    nf1 = _NumFactorEvaluator(f, {}, 1)
    assert nf1.factor is f
    eval123, is_NA = nf1.eval({"mock": [1, 2, 3]}, naa)
    assert eval123.shape == (3, 1)
    assert np.all(eval123 == [[1], [2], [3]])
    assert is_NA.shape == (3,)
    assert np.all(~is_NA)
    assert_raises(PatsyError, nf1.eval, {"mock": [[[1]]]}, naa)
    assert_raises(PatsyError, nf1.eval, {"mock": [[1, 2]]}, naa)
    assert_raises(PatsyError, nf1.eval, {"mock": ["a", "b"]}, naa)
    assert_raises(PatsyError, nf1.eval, {"mock": [True, False]}, naa)
    nf2 = _NumFactorEvaluator(_MockFactor(), {}, 2)
    eval123321, is_NA = nf2.eval({"mock": [[1, 3], [2, 2], [3, 1]]}, naa)
    assert eval123321.shape == (3, 2)
    assert np.all(eval123321 == [[1, 3], [2, 2], [3, 1]])
    assert is_NA.shape == (3,)
    assert np.all(~is_NA)
    assert_raises(PatsyError, nf2.eval, {"mock": [1, 2, 3]}, naa)
    assert_raises(PatsyError, nf2.eval, {"mock": [[1, 2, 3]]}, naa)

    ev_nan, is_NA = nf1.eval({"mock": [1, 2, np.nan]},
                             NAAction(NA_types=["NaN"]))
    assert np.array_equal(is_NA, [False, False, True])
    ev_nan, is_NA = nf1.eval({"mock": [1, 2, np.nan]},
                             NAAction(NA_types=[]))
    assert np.array_equal(is_NA, [False, False, False])

    if have_pandas:
        eval_ser, _ = nf1.eval({"mock":
                                pandas.Series([1, 2, 3], index=[10, 20, 30])},
                               naa)
        assert isinstance(eval_ser, pandas.DataFrame)
        assert np.array_equal(eval_ser, [[1], [2], [3]])
        assert np.array_equal(eval_ser.index, [10, 20, 30])
        eval_df1, _ = nf1.eval({"mock":
                                    pandas.DataFrame([[2], [1], [3]],
                                                     index=[20, 10, 30])},
                               naa)
        assert isinstance(eval_df1, pandas.DataFrame)
        assert np.array_equal(eval_df1, [[2], [1], [3]])
        assert np.array_equal(eval_df1.index, [20, 10, 30])
        eval_df2, _ = nf2.eval({"mock":
                                    pandas.DataFrame([[2, 3], [1, 4], [3, -1]],
                                                     index=[20, 30, 10])},
                               naa)
        assert isinstance(eval_df2, pandas.DataFrame)
        assert np.array_equal(eval_df2, [[2, 3], [1, 4], [3, -1]])
        assert np.array_equal(eval_df2.index, [20, 30, 10])
        
        assert_raises(PatsyError,
                      nf2.eval,
                      {"mock": pandas.Series([1, 2, 3], index=[10, 20, 30])},
                      naa)
        assert_raises(PatsyError,
                      nf1.eval,
                      {"mock":
                       pandas.DataFrame([[2, 3], [1, 4], [3, -1]],
                                        index=[20, 30, 10])},
                      naa)


class _CatFactorEvaluator(object):
    def __init__(self, factor, state, levels):
        # This one instance variable is part of our public API:
        self.factor = factor
        self._state = state
        self._levels = tuple(levels)

    # returns either a 1d ndarray or a pandas.Series, plus is_NA mask
    def eval(self, data, NA_action):
        result = self.factor.eval(self._state, data)
        result = categorical_to_int(result, self._levels, NA_action,
                                    origin=self.factor)
        assert result.ndim == 1
        return result, np.asarray(result == -1)

def test__CatFactorEvaluator():
    from nose.tools import assert_raises
    from patsy.categorical import C
    naa = NAAction()
    f = _MockFactor()
    cf1 = _CatFactorEvaluator(f, {}, ["a", "b"])
    assert cf1.factor is f
    cat1, _ = cf1.eval({"mock": ["b", "a", "b"]}, naa)
    assert cat1.shape == (3,)
    assert np.all(cat1 == [1, 0, 1])
    assert_raises(PatsyError, cf1.eval, {"mock": ["c"]}, naa)
    assert_raises(PatsyError, cf1.eval, {"mock": C(["a", "c"])}, naa)
    assert_raises(PatsyError, cf1.eval,
                  {"mock": C(["a", "b"], levels=["b", "a"])}, naa)
    assert_raises(PatsyError, cf1.eval, {"mock": [1, 0, 1]}, naa)
    bad_cat = np.asarray(["b", "a", "a", "b"])
    bad_cat.resize((2, 2))
    assert_raises(PatsyError, cf1.eval, {"mock": bad_cat}, naa)

    cat1_NA, is_NA = cf1.eval({"mock": ["a", None, "b"]},
                              NAAction(NA_types=["None"]))
    assert np.array_equal(is_NA, [False, True, False])
    assert np.array_equal(cat1_NA, [0, -1, 1])
    assert_raises(PatsyError, cf1.eval,
                  {"mock": ["a", None, "b"]}, NAAction(NA_types=[]))

    cf2 = _CatFactorEvaluator(_MockFactor(), {}, [False, True])
    cat2, _ = cf2.eval({"mock": [True, False, False, True]}, naa)
    assert cat2.shape == (4,)
    assert np.all(cat2 == [1, 0, 0, 1])

    if have_pandas:
        s = pandas.Series(["b", "a"], index=[10, 20])
        cat_s, _ = cf1.eval({"mock": s}, naa)
        assert isinstance(cat_s, pandas.Series)
        assert np.array_equal(cat_s, [1, 0])
        assert np.array_equal(cat_s.index, [10, 20])
        sbool = pandas.Series([True, False], index=[11, 21])
        cat_sbool, _ = cf2.eval({"mock": sbool}, naa)
        assert isinstance(cat_sbool, pandas.Series)
        assert np.array_equal(cat_sbool, [1, 0])
        assert np.array_equal(cat_sbool.index, [11, 21])

def _column_combinations(columns_per_factor):
    # For consistency with R, the left-most item iterates fastest:
    iterators = [xrange(n) for n in reversed(columns_per_factor)]
    for reversed_combo in itertools_product(*iterators):
        yield reversed_combo[::-1]

def test__column_combinations():
    assert list(_column_combinations([2, 3])) == [(0, 0),
                                                  (1, 0),
                                                  (0, 1),
                                                  (1, 1),
                                                  (0, 2),
                                                  (1, 2)]
    assert list(_column_combinations([3])) == [(0,), (1,), (2,)]

# This class is responsible for producing some columns in a final design
# matrix output:
class _ColumnBuilder(object):
    def __init__(self, factors, num_columns, cat_contrasts):
        self._factors = factors
        self._num_columns = num_columns
        self._cat_contrasts = cat_contrasts
        self._columns_per_factor = []
        for factor in self._factors:
            if factor in self._cat_contrasts:
                columns = self._cat_contrasts[factor].matrix.shape[1]
            else:
                columns = num_columns[factor]
            self._columns_per_factor.append(columns)
        self.total_columns = np.prod(self._columns_per_factor, dtype=int)

    def column_names(self):
        if not self._factors:
            return ["Intercept"]
        column_names = []
        for i, column_idxs in enumerate(_column_combinations(self._columns_per_factor)):
            name_pieces = []
            for factor, column_idx in zip(self._factors, column_idxs):
                if factor in self._num_columns:
                    if self._num_columns[factor] > 1:
                        name_pieces.append("%s[%s]"
                                           % (factor.name(), column_idx))
                    else:
                        assert column_idx == 0
                        name_pieces.append(factor.name())
                else:
                    contrast = self._cat_contrasts[factor]
                    suffix = contrast.column_suffixes[column_idx]
                    name_pieces.append("%s%s" % (factor.name(), suffix))
            column_names.append(":".join(name_pieces))
        assert len(column_names) == self.total_columns
        return column_names

    def build(self, factor_values, out):
        assert self.total_columns == out.shape[1]
        out[:] = 1
        for i, column_idxs in enumerate(_column_combinations(self._columns_per_factor)):
            for factor, column_idx in zip(self._factors, column_idxs):
                if factor in self._cat_contrasts:
                    contrast = self._cat_contrasts[factor]
                    if np.any(factor_values[factor] < 0):
                        raise PatsyError("can't build a design matrix "
                                         "containing missing values", factor)
                    out[:, i] *= contrast.matrix[factor_values[factor],
                                                 column_idx]
                else:
                    assert (factor_values[factor].shape[1]
                            == self._num_columns[factor])
                    out[:, i] *= factor_values[factor][:, column_idx]

def test__ColumnBuilder():
    from nose.tools import assert_raises
    from patsy.contrasts import ContrastMatrix
    from patsy.categorical import C
    f1 = _MockFactor("f1")
    f2 = _MockFactor("f2")
    f3 = _MockFactor("f3")
    contrast = ContrastMatrix(np.array([[0, 0.5],
                                        [3, 0]]),
                              ["[c1]", "[c2]"])
                             
    cb = _ColumnBuilder([f1, f2, f3], {f1: 1, f3: 1}, {f2: contrast})
    mat = np.empty((3, 2))
    assert cb.column_names() == ["f1:f2[c1]:f3", "f1:f2[c2]:f3"]
    cb.build({f1: atleast_2d_column_default([1, 2, 3]),
              f2: np.asarray([0, 0, 1]),
              f3: atleast_2d_column_default([7.5, 2, -12])},
             mat)
    assert np.allclose(mat, [[0, 0.5 * 1 * 7.5],
                             [0, 0.5 * 2 * 2],
                             [3 * 3 * -12, 0]])
    # Check that missing categorical values blow up
    assert_raises(PatsyError, cb.build,
                  {f1: atleast_2d_column_default([1, 2, 3]),
                   f2: np.asarray([0, -1, 1]),
                   f3: atleast_2d_column_default([7.5, 2, -12])},
                  mat)

    cb2 = _ColumnBuilder([f1, f2, f3], {f1: 2, f3: 1}, {f2: contrast})
    mat2 = np.empty((3, 4))
    cb2.build({f1: atleast_2d_column_default([[1, 2], [3, 4], [5, 6]]),
               f2: np.asarray([0, 0, 1]),
               f3: atleast_2d_column_default([7.5, 2, -12])},
              mat2)
    assert cb2.column_names() == ["f1[0]:f2[c1]:f3",
                                  "f1[1]:f2[c1]:f3",
                                  "f1[0]:f2[c2]:f3",
                                  "f1[1]:f2[c2]:f3"]
    assert np.allclose(mat2, [[0, 0, 0.5 * 1 * 7.5, 0.5 * 2 * 7.5],
                              [0, 0, 0.5 * 3 * 2, 0.5 * 4 * 2],
                              [3 * 5 * -12, 3 * 6 * -12, 0, 0]])
    # Check intercept building:
    cb_intercept = _ColumnBuilder([], {}, {})
    assert cb_intercept.column_names() == ["Intercept"]
    mat3 = np.empty((3, 1))
    cb_intercept.build({f1: [1, 2, 3], f2: [1, 2, 3], f3: [1, 2, 3]}, mat3)
    assert np.allclose(mat3, 1)

def _factors_memorize(factors, data_iter_maker):
    # First, start off the memorization process by setting up each factor's
    # state and finding out how many passes it will need:
    factor_states = {}
    passes_needed = {}
    for factor in factors:
        state = {}
        which_pass = factor.memorize_passes_needed(state)
        factor_states[factor] = state
        passes_needed[factor] = which_pass
    # Now, cycle through the data until all the factors have finished
    # memorizing everything:
    memorize_needed = set()
    for factor, passes in passes_needed.iteritems():
        if passes > 0:
            memorize_needed.add(factor)
    which_pass = 0
    while memorize_needed:
        for data in data_iter_maker():
            for factor in memorize_needed:
                state = factor_states[factor]
                factor.memorize_chunk(state, which_pass, data)
        for factor in list(memorize_needed):
            factor.memorize_finish(factor_states[factor], which_pass)
            if which_pass == passes_needed[factor] - 1:
                memorize_needed.remove(factor)
        which_pass += 1
    return factor_states

def test__factors_memorize():
    class MockFactor(object):
        def __init__(self, requested_passes, token):
            self._requested_passes = requested_passes
            self._token = token
            self._chunk_in_pass = 0
            self._seen_passes = 0

        def memorize_passes_needed(self, state):
            state["calls"] = []
            state["token"] = self._token
            return self._requested_passes

        def memorize_chunk(self, state, which_pass, data):
            state["calls"].append(("memorize_chunk", which_pass))
            assert data["chunk"] == self._chunk_in_pass
            self._chunk_in_pass += 1

        def memorize_finish(self, state, which_pass):
            state["calls"].append(("memorize_finish", which_pass))
            self._chunk_in_pass = 0

    class Data(object):
        CHUNKS = 3
        def __init__(self):
            self.calls = 0
            self.data = [{"chunk": i} for i in xrange(self.CHUNKS)]
        def __call__(self):
            self.calls += 1
            return iter(self.data)
    data = Data()
    f0 = MockFactor(0, "f0")
    f1 = MockFactor(1, "f1")
    f2a = MockFactor(2, "f2a")
    f2b = MockFactor(2, "f2b")
    factor_states = _factors_memorize(set([f0, f1, f2a, f2b]), data)
    assert data.calls == 2
    mem_chunks0 = [("memorize_chunk", 0)] * data.CHUNKS
    mem_chunks1 = [("memorize_chunk", 1)] * data.CHUNKS
    expected = {
        f0: {
            "calls": [],
            "token": "f0",
            },
        f1: {
            "calls": mem_chunks0 + [("memorize_finish", 0)],
            "token": "f1",
            },
        f2a: {
            "calls": mem_chunks0 + [("memorize_finish", 0)]
                     + mem_chunks1 + [("memorize_finish", 1)],
            "token": "f2a",
            },
        f2b: {
            "calls": mem_chunks0 + [("memorize_finish", 0)]
                     + mem_chunks1 + [("memorize_finish", 1)],
            "token": "f2b",
            },
        }
    assert factor_states == expected

def _examine_factor_types(factors, factor_states, data_iter_maker, NA_action):
    num_column_counts = {}
    cat_sniffers = {}
    examine_needed = set(factors)
    for data in data_iter_maker():
        for factor in list(examine_needed):
            value = factor.eval(factor_states[factor], data)
            if factor in cat_sniffers or guess_categorical(value):
                if factor not in cat_sniffers:
                    cat_sniffers[factor] = CategoricalSniffer(NA_action,
                                                              factor.origin)
                done = cat_sniffers[factor].sniff(value)
                if done:
                    examine_needed.remove(factor)
            else:
                # Numeric
                value = atleast_2d_column_default(value)
                _max_allowed_dim(2, value, factor)
                column_count = value.shape[1]
                num_column_counts[factor] = column_count
                examine_needed.remove(factor)
        if not examine_needed:
            break
    # Pull out the levels
    cat_levels_contrasts = {}
    for factor, sniffer in cat_sniffers.iteritems():
        cat_levels_contrasts[factor] = sniffer.levels_contrast()
    return (num_column_counts, cat_levels_contrasts)

def test__examine_factor_types():
    from patsy.categorical import C
    class MockFactor(object):
        def __init__(self):
            # You should check this using 'is', not '=='
            from patsy.origin import Origin
            self.origin = Origin("MOCK", 1, 2)

        def eval(self, state, data):
            return state[data]

        def name(self):
            return "MOCK MOCK"

    # This hacky class can only be iterated over once, but it keeps track of
    # how far it got.
    class DataIterMaker(object):
        def __init__(self):
            self.i = -1

        def __call__(self):
            return self

        def __iter__(self):
            return self

        def next(self):
            self.i += 1
            if self.i > 1:
                raise StopIteration
            return self.i

    num_1dim = MockFactor()
    num_1col = MockFactor()
    num_4col = MockFactor()
    categ_1col = MockFactor()
    bool_1col = MockFactor()
    string_1col = MockFactor()
    object_1col = MockFactor()
    object_levels = (object(), object(), object())
    factor_states = {
        num_1dim: ([1, 2, 3], [4, 5, 6]),
        num_1col: ([[1], [2], [3]], [[4], [5], [6]]),
        num_4col: (np.zeros((3, 4)), np.ones((3, 4))),
        categ_1col: (C(["a", "b", "c"], levels=("a", "b", "c"),
                       contrast="MOCK CONTRAST"),
                     C(["c", "b", "a"], levels=("a", "b", "c"),
                       contrast="MOCK CONTRAST")),
        bool_1col: ([True, True, False], [False, True, True]),
        # It has to read through all the data to see all the possible levels:
        string_1col: (["a", "a", "a"], ["c", "b", "a"]),
        object_1col: ([object_levels[0]] * 3, object_levels),
        }

    it = DataIterMaker()
    (num_column_counts, cat_levels_contrasts,
     ) = _examine_factor_types(factor_states.keys(), factor_states, it,
                               NAAction())
    assert it.i == 2
    iterations = 0
    assert num_column_counts == {num_1dim: 1, num_1col: 1, num_4col: 4}
    assert cat_levels_contrasts == {
        categ_1col: (("a", "b", "c"), "MOCK CONTRAST"),
        bool_1col: ((False, True), None),
        string_1col: (("a", "b", "c"), None),
        object_1col: (tuple(sorted(object_levels, key=id)), None),
        }

    # Check that it doesn't read through all the data if that's not necessary:
    it = DataIterMaker()
    no_read_necessary = [num_1dim, num_1col, num_4col, categ_1col, bool_1col]
    (num_column_counts, cat_levels_contrasts,
     ) = _examine_factor_types(no_read_necessary, factor_states, it,
                               NAAction())
    assert it.i == 0
    assert num_column_counts == {num_1dim: 1, num_1col: 1, num_4col: 4}
    assert cat_levels_contrasts == {
        categ_1col: (("a", "b", "c"), "MOCK CONTRAST"),
        bool_1col: ((False, True), None),
        }

    # Illegal inputs:
    bool_3col = MockFactor()
    num_3dim = MockFactor()
    # no such thing as a multi-dimensional Categorical
    # categ_3dim = MockFactor()
    string_3col = MockFactor()
    object_3col = MockFactor()
    illegal_factor_states = {
        bool_3col: (np.zeros((3, 3), dtype=bool), np.ones((3, 3), dtype=bool)),
        num_3dim: (np.zeros((3, 3, 3)), np.ones((3, 3, 3))),
        string_3col: ([["a", "b", "c"]], [["b", "c", "a"]]),
        object_3col: ([[[object()]]], [[[object()]]]),
        }
    from nose.tools import assert_raises
    for illegal_factor in illegal_factor_states:
        it = DataIterMaker()
        try:
            _examine_factor_types([illegal_factor], illegal_factor_states, it,
                                  NAAction())
        except PatsyError, e:
            assert e.origin is illegal_factor.origin
        else:
            assert False

def _make_term_column_builders(terms,
                               num_column_counts,
                               cat_levels_contrasts):
    # Sort each term into a bucket based on the set of numeric factors it
    # contains:
    term_buckets = OrderedDict()
    bucket_ordering = []
    for term in terms:
        num_factors = []
        for factor in term.factors:
            if factor in num_column_counts:
                num_factors.append(factor)
        bucket = frozenset(num_factors)
        if bucket not in term_buckets:
            bucket_ordering.append(bucket)
        term_buckets.setdefault(bucket, []).append(term)
    # Special rule: if there is a no-numerics bucket, then it always comes
    # first:
    if frozenset() in term_buckets:
        bucket_ordering.remove(frozenset())
        bucket_ordering.insert(0, frozenset())
    term_to_column_builders = {}
    new_term_order = []
    # Then within each bucket, work out which sort of contrasts we want to use
    # for each term to avoid redundancy
    for bucket in bucket_ordering:
        bucket_terms = term_buckets[bucket]
        # Sort by degree of interaction
        bucket_terms.sort(key=lambda t: len(t.factors))
        new_term_order += bucket_terms
        used_subterms = set()
        for term in bucket_terms:
            column_builders = []
            factor_codings = pick_contrasts_for_term(term,
                                                     num_column_counts,
                                                     used_subterms)
            # Construct one _ColumnBuilder for each subterm
            for factor_coding in factor_codings:
                builder_factors = []
                num_columns = {}
                cat_contrasts = {}
                # In order to preserve factor ordering information, the
                # coding_for_term just returns dicts, and we refer to
                # the original factors to figure out which are included in
                # each subterm, and in what order
                for factor in term.factors:
                    # Numeric factors are included in every subterm
                    if factor in num_column_counts:
                        builder_factors.append(factor)
                        num_columns[factor] = num_column_counts[factor]
                    elif factor in factor_coding:
                        builder_factors.append(factor)
                        levels, contrast = cat_levels_contrasts[factor]
                        # This is where the default coding is set to
                        # Treatment:
                        coded = code_contrast_matrix(factor_coding[factor],
                                                     levels, contrast,
                                                     default=Treatment)
                        cat_contrasts[factor] = coded
                column_builder = _ColumnBuilder(builder_factors,
                                                num_columns,
                                                cat_contrasts)
                column_builders.append(column_builder)
            term_to_column_builders[term] = column_builders
    return new_term_order, term_to_column_builders

def design_matrix_builders(termlists, data_iter_maker, NA_action="drop"):
    """Construct several :class:`DesignMatrixBuilders` from termlists.

    This is one of Patsy's fundamental functions. This function and
    :func:`build_design_matrices` together form the API to the core formula
    interpretation machinery.

    :arg termlists: A list of termlists, where each termlist is a list of
      :class:`Term` objects which together specify a design matrix.
    :arg data_iter_maker: A zero-argument callable which returns an iterator
      over dict-like data objects. This must be a callable rather than a
      simple iterator because sufficiently complex formulas may require
      multiple passes over the data (e.g. if there are nested stateful
      transforms).
    :arg NA_action: An :class:`NAAction` object or string, used to determine
      what values count as 'missing' for purposes of determining the levels of
      categorical factors.
    :returns: A list of :class:`DesignMatrixBuilder` objects, one for each
      termlist passed in.

    This function performs zero or more iterations over the data in order to
    sniff out any necessary information about factor types, set up stateful
    transforms, pick column names, etc.

    See :ref:`formulas` for details.

    .. versionadded:: 0.2.0
       The ``NA_action`` argument.
    """
    if isinstance(NA_action, basestring):
        NA_action = NAAction(NA_action)
    all_factors = set()
    for termlist in termlists:
        for term in termlist:
            all_factors.update(term.factors)
    factor_states = _factors_memorize(all_factors, data_iter_maker)
    # Now all the factors have working eval methods, so we can evaluate them
    # on some data to find out what type of data they return.
    (num_column_counts,
     cat_levels_contrasts) = _examine_factor_types(all_factors,
                                                   factor_states,
                                                   data_iter_maker,
                                                   NA_action)
    # Now we need the factor evaluators, which encapsulate the knowledge of
    # how to turn any given factor into a chunk of data:
    factor_evaluators = {}
    for factor in all_factors:
        if factor in num_column_counts:
            evaluator = _NumFactorEvaluator(factor,
                                            factor_states[factor],
                                            num_column_counts[factor])
        else:
            assert factor in cat_levels_contrasts
            levels = cat_levels_contrasts[factor][0]
            evaluator = _CatFactorEvaluator(factor, factor_states[factor],
                                            levels)
        factor_evaluators[factor] = evaluator
    # And now we can construct the DesignMatrixBuilder for each termlist:
    builders = []
    for termlist in termlists:
        result = _make_term_column_builders(termlist,
                                            num_column_counts,
                                            cat_levels_contrasts)
        new_term_order, term_to_column_builders = result
        assert frozenset(new_term_order) == frozenset(termlist)
        term_evaluators = set()
        for term in termlist:
            for factor in term.factors:
                term_evaluators.add(factor_evaluators[factor])
        builders.append(DesignMatrixBuilder(new_term_order,
                                            term_evaluators,
                                            term_to_column_builders))
    return builders

class DesignMatrixBuilder(object):
    """An opaque class representing Patsy's knowledge about
    how to build a specific design matrix.

    You get these objects from :func:`design_matrix_builders`, and pass them
    to :func:`build_design_matrices`.
    """
    def __init__(self, terms, evaluators, term_to_column_builders):
        self._termlist = terms
        self._evaluators = evaluators
        self._term_to_column_builders = term_to_column_builders
        term_column_count = []
        self._column_names = []
        for term in self._termlist:
            column_builders = self._term_to_column_builders[term]
            this_count = 0
            for column_builder in column_builders:
                this_names = column_builder.column_names()
                this_count += len(this_names)
                self._column_names += this_names
            term_column_count.append(this_count)
        term_column_starts = np.concatenate(([0], np.cumsum(term_column_count)))
        self._term_slices = []
        for i, term in enumerate(self._termlist):
            span = slice(term_column_starts[i], term_column_starts[i + 1])
            self._term_slices.append((term, span))
        self.total_columns = np.sum(term_column_count, dtype=int)

    # Generate this on demand, to avoid a reference loop:
    @property
    def design_info(self):
        """A :class:`DesignInfo` object giving information about the design
        matrices that this DesignMatrixBuilder can be used to create."""
        return DesignInfo(self._column_names, self._term_slices,
                          builder=self)

    def subset(self, which_terms):
        """Create a new :class:`DesignMatrixBuilder` that includes only a
        subset of the terms that this object does.

        For example, if `builder` has terms `x`, `y`, and `z`, then::

          builder2 = builder.subset(["x", "z"])

        will return a new builder that will return design matrices with only
        the columns corresponding to the terms `x` and `z`. After we do this,
        then in general these two expressions will return the same thing (here
        we assume that `x`, `y`, and `z` each generate a single column of the
        output)::

          build_design_matrix([builder], data)[0][:, [0, 2]]
          build_design_matrix([builder2], data)[0]

        However, a critical difference is that in the second case, `data` need
        not contain any values for `y`. This is very useful when doing
        prediction using a subset of a model, in which situation R usually
        forces you to specify dummy values for `y`.

        If using a formula to specify the terms to include, remember that like
        any formula, the intercept term will be included by default, so use
        `0` or `-1` in your formula if you want to avoid this.

        :arg which_terms: The terms which should be kept in the new
          :class:`DesignMatrixBuilder`. If this is a string, then it is parsed
          as a formula, and then the names of the resulting terms are taken as
          the terms to keep. If it is a list, then it can contain a mixture of
          term names (as strings) and :class:`Term` objects.

        .. versionadded: 0.2.0
        """
        factor_to_evaluators = {}
        for evaluator in self._evaluators:
            factor_to_evaluators[evaluator.factor] = evaluator
        design_info = self.design_info
        term_name_to_term = dict(zip(design_info.term_names,
                                     design_info.terms))
        if isinstance(which_terms, basestring):
            # We don't use this EvalEnvironment -- all we want to do is to
            # find matching terms, and we can't do that use == on Term
            # objects, because that calls == on factor objects, which in turn
            # compares EvalEnvironments. So all we do with the parsed formula
            # is pull out the term *names*, which the EvalEnvironment doesn't
            # effect. This is just a placeholder then to allow the ModelDesc
            # to be created:
            env = EvalEnvironment({})
            desc = ModelDesc.from_formula(which_terms, env)
            if desc.lhs_termlist:
                raise PatsyError("right-hand-side-only formula required")
            which_terms = [term.name() for term in desc.rhs_termlist]
        terms = []
        evaluators = set()
        term_to_column_builders = {}
        for term_or_name in which_terms:
            if isinstance(term_or_name, basestring):
                if term_or_name not in term_name_to_term:
                    raise PatsyError("requested term %r not found in "
                                     "this DesignMatrixBuilder"
                                     % (term_or_name,))
                term = term_name_to_term[term_or_name]
            else:
                term = term_or_name
            if term not in self._termlist:
                raise PatsyError("requested term '%s' not found in this "
                                 "DesignMatrixBuilder" % (term,))
            for factor in term.factors:
                evaluators.add(factor_to_evaluators[factor])
            terms.append(term)
            column_builder = self._term_to_column_builders[term]
            term_to_column_builders[term] = column_builder
        return DesignMatrixBuilder(terms,
                                   evaluators,
                                   term_to_column_builders)

    def _build(self, evaluator_to_values, dtype):
        factor_to_values = {}
        need_reshape = False
        num_rows = None
        for evaluator, value in evaluator_to_values.iteritems():
            if evaluator in self._evaluators:
                factor_to_values[evaluator.factor] = value
                if num_rows is not None:
                    assert num_rows == value.shape[0]
                else:
                    num_rows = value.shape[0]
        if num_rows is None:
            # We have no dependence on the data -- e.g. an empty termlist, or
            # only an intercept term.
            num_rows = 1
            need_reshape = True
        m = DesignMatrix(np.empty((num_rows, self.total_columns), dtype=dtype),
                         self.design_info)
        start_column = 0
        for term in self._termlist:
            for column_builder in self._term_to_column_builders[term]:
                end_column = start_column + column_builder.total_columns
                m_slice = m[:, start_column:end_column]
                column_builder.build(factor_to_values, m_slice)
                start_column = end_column
        assert start_column == self.total_columns
        return need_reshape, m

class _CheckMatch(object):
    def __init__(self, name, eq_fn):
        self._name = name
        self._eq_fn = eq_fn
        self.value = None
        self._value_desc = None
        self._value_origin = None

    def check(self, seen_value, desc, origin):
        if self.value is None:
            self.value = seen_value
            self._value_desc = desc
            self._value_origin = origin
        else:
            if not self._eq_fn(self.value, seen_value):
                msg = ("%s mismatch between %s and %s"
                       % (self._name, self._value_desc, desc))
                if isinstance(self.value, int):
                    msg += " (%r versus %r)" % (self.value, seen_value)
                # XX FIXME: this is a case where having discontiguous Origins
                # would be useful...
                raise PatsyError(msg, origin)

def build_design_matrices(builders, data,
                          NA_action="drop",
                          return_type="matrix",
                          dtype=np.dtype(float)):
    """Construct several design matrices from :class:`DesignMatrixBuilder`
    objects.

    This is one of Patsy's fundamental functions. This function and
    :func:`design_matrix_builders` together form the API to the core formula
    interpretation machinery.

    :arg builders: A list of :class:`DesignMatrixBuilders` specifying the
      design matrices to be built.
    :arg data: A dict-like object which will be used to look up data.
    :arg NA_action: What to do with rows that contain missing values. You can
      ``"drop"`` them, ``"raise"`` an error, or for customization, pass an
      :class:`NAAction` object. See :class:`NAAction` for details on what
      values count as 'missing' (and how to alter this).
    :arg return_type: Either ``"matrix"`` or ``"dataframe"``. See below.
    :arg dtype: The dtype of the returned matrix. Useful if you want to use
      single-precision or extended-precision.

    This function returns either a list of :class:`DesignMatrix` objects (for
    ``return_type="matrix"``) or a list of :class:`pandas.DataFrame` objects
    (for ``return_type="dataframe"``). In both cases, all returned design
    matrices will have ``.design_info`` attributes containing the appropriate
    :class:`DesignInfo` objects.

    Note that unlike :func:`design_matrix_builders`, this function takes only
    a simple data argument, not any kind of iterator. That's because this
    function doesn't need a global view of the data -- everything that depends
    on the whole data set is already encapsulated in the `builders`. If you
    are incrementally processing a large data set, simply call this function
    for each chunk.

    Index handling: This function always checks for indexes in the following
    places:

    * If ``data`` is a :class:`pandas.DataFrame`, its ``.index`` attribute.
    * If any factors evaluate to a :class:`pandas.Series` or
      :class:`pandas.DataFrame`, then their ``.index`` attributes.

    If multiple indexes are found, they must be identical (same values in the
    same order). If no indexes are found, then a default index is generated
    using ``np.arange(num_rows)``. One way or another, we end up with a single
    index for all the data. If ``return_type="dataframe"``, then this index is
    used as the index of the returned DataFrame objects. Examining this index
    makes it possible to determine which rows were removed due to NAs.

    Determining the number of rows in design matrices: This is not as obvious
    as it might seem, because it's possible to have a formula like "~ 1" that
    doesn't depend on the data (it has no factors). For this formula, it's
    obvious what every row in the design matrix should look like (just the
    value ``1``); but, how many rows like this should there be? To determine
    the number of rows in a design matrix, this function always checks in the
    following places:

    * If ``data`` is a :class:`pandas.DataFrame`, then its number of rows.
    * The number of entries in any factors present in any of the design
    * matrices being built.

    All these values much match. In particular, if this function is called to
    generate multiple design matrices at once, then they must all have the
    same number of rows.

    .. versionadded:: 0.2.0
       The ``NA_action`` argument.
    """
    if isinstance(NA_action, basestring):
        NA_action = NAAction(NA_action)
    if return_type == "dataframe" and not have_pandas:
        raise PatsyError("pandas.DataFrame was requested, but pandas "
                            "is not installed")
    if return_type not in ("matrix", "dataframe"):
        raise PatsyError("unrecognized output type %r, should be "
                            "'matrix' or 'dataframe'" % (return_type,))
    # Evaluate factors
    evaluator_to_values = {}
    evaluator_to_isNAs = {}
    import operator
    rows_checker = _CheckMatch("Number of rows", lambda a, b: a == b)
    index_checker = _CheckMatch("Index", lambda a, b: a.equals(b))
    if have_pandas and isinstance(data, pandas.DataFrame):
        index_checker.check(data.index, "data.index", None)
        rows_checker.check(data.shape[0], "data argument", None)
    for builder in builders:
        # We look at evaluators rather than factors here, because it might
        # happen that we have the same factor twice, but with different
        # memorized state.
        for evaluator in builder._evaluators:
            if evaluator not in evaluator_to_values:
                value, is_NA = evaluator.eval(data, NA_action)
                evaluator_to_isNAs[evaluator] = is_NA
                # value may now be a Series, DataFrame, or ndarray
                name = evaluator.factor.name()
                origin = evaluator.factor.origin
                rows_checker.check(value.shape[0], name, origin)
                if (have_pandas
                    and isinstance(value, (pandas.Series, pandas.DataFrame))):
                    index_checker.check(value.index, name, origin)
                # Strategy: we work with raw ndarrays for doing the actual
                # combining; DesignMatrixBuilder objects never sees pandas
                # objects. Then at the end, if a DataFrame was requested, we
                # convert. So every entry in this dict is either a 2-d array
                # of floats, or a 1-d array of integers (representing
                # categories).
                value = np.asarray(value)
                evaluator_to_values[evaluator] = value
    # Handle NAs
    values = evaluator_to_values.values()
    is_NAs = evaluator_to_isNAs.values()
    origins = [evaluator.factor.origin for evaluator in evaluator_to_values]
    pandas_index = index_checker.value
    num_rows = rows_checker.value
    # num_rows is None iff evaluator_to_values (and associated sets like
    # 'values') are empty, i.e., we have no actual evaluators involved
    # (formulas like "~ 1").
    if return_type == "dataframe" and num_rows is not None:
        if pandas_index is None:
            pandas_index = np.arange(num_rows)
        values.append(pandas_index)
        is_NAs.append(np.zeros(len(pandas_index), dtype=bool))
        origins.append(None)
    new_values = NA_action.handle_NA(values, is_NAs, origins)
    # NA_action may have changed the number of rows.
    if new_values:
        num_rows = new_values[0].shape[0]
    if return_type == "dataframe" and num_rows is not None:
        pandas_index = new_values.pop()
    evaluator_to_values = dict(zip(evaluator_to_values, new_values))
    # Build factor values into matrices
    results = []
    for builder in builders:
        results.append(builder._build(evaluator_to_values, dtype))
    matrices = []
    for need_reshape, matrix in results:
        if need_reshape:
            # There is no data-dependence, at all -- a formula like "1 ~ 1".
            # In this case the builder just returns a single-row matrix, and
            # we have to broadcast it vertically to the appropriate size. If
            # we can figure out what that is...
            assert matrix.shape[0] == 1
            if num_rows is not None:
                matrix = DesignMatrix(np.repeat(matrix, num_rows, axis=0),
                                      matrix.design_info)
            else:
                raise PatsyError(
                    "No design matrix has any non-trivial factors, "
                    "the data object is not a DataFrame. "
                    "I can't tell how many rows the design matrix should "
                    "have!"
                    )
        matrices.append(matrix)
    if return_type == "dataframe":
        assert have_pandas
        for i, matrix in enumerate(matrices):
            di = matrix.design_info
            matrices[i] = pandas.DataFrame(matrix,
                                           columns=di.column_names,
                                           index=pandas_index)
            matrices[i].design_info = di
    return matrices

# It should be possible to do just the factors -> factor evaluators stuff
# alone, since that, well, makes logical sense to do. though categorical
# coding has to happen afterwards, hmm.

########NEW FILE########
__FILENAME__ = builtins
# This file is part of Patsy
# Copyright (C) 2011-2013 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# This module sets up the namespace of stuff that is available to formulas by
# default. All formulas are interpreted in an environment that acts as if
#   from patsy.builtins import *
# has been executed. (Of course, you can also execute this yourself if you
# want to use these in your regular code for some reason.)

__all__ = ["I", "Q"]

from patsy.contrasts import ContrastMatrix, Treatment, Poly, Sum, Helmert, Diff
__all__ += ["ContrastMatrix", "Treatment", "Poly", "Sum", "Helmert", "Diff"]

from patsy.categorical import C
__all__ += ["C"]

from patsy.state import center, standardize, scale
__all__ += ["center", "standardize", "scale"]

from patsy.splines import bs
__all__ += ["bs"]

def I(x):
    """The identity function. Simply returns its input unchanged.

    Since Patsy's formula parser ignores anything inside a function call
    syntax, this is useful to 'hide' arithmetic operations from it. For
    instance::

      y ~ x1 + x2

    has ``x1`` and ``x2`` as two separate predictors. But in::

      y ~ I(x1 + x2)

    we instead have a single predictor, defined to be the sum of ``x1`` and
    ``x2``."""
    return x

def test_I():
    assert I(1) == 1
    assert I(None) is None

def Q(name):
    """A way to 'quote' variable names, especially ones that do not otherwise
    meet Python's variable name rules.

    If ``x`` is a variable, ``Q("x")`` returns the value of ``x``. (Note that
    ``Q`` takes the *string* ``"x"``, not the value of ``x`` itself.) This
    works even if instead of ``x``, we have a variable name that would not
    otherwise be legal in Python.

    For example, if you have a column of data named ``weight.in.kg``, then you
    can't write::

      y ~ weight.in.kg

    because Python will try to find a variable named ``weight``, that has an
    attribute named ``in``, that has an attribute named ``kg``. (And worse
    yet, ``in`` is a reserved word, which makes this example doubly broken.)
    Instead, write::

      y ~ Q("weight.in.kg")

    and all will be well. Note, though, that this requires embedding a Python
    string inside your formula, which may require some care with your quote
    marks. Some standard options include::

      my_fit_function("y ~ Q('weight.in.kg')", ...)
      my_fit_function('y ~ Q("weight.in.kg")', ...)
      my_fit_function("y ~ Q(\\"weight.in.kg\\")", ...)

    Note also that ``Q`` is an ordinary Python function, which means that you
    can use it in more complex expressions. For example, this is a legal
    formula::

      y ~ np.sqrt(Q("weight.in.kg"))
    """
    from patsy.eval import EvalEnvironment
    env = EvalEnvironment.capture(1)
    try:
        return env.namespace[name]
    except KeyError:
        raise NameError, "no data named %r found" % (name,)

def test_Q():
    a = 1
    assert Q("a") == 1
    assert Q("Q") is Q
    from nose.tools import assert_raises
    assert_raises(NameError, Q, "asdfsadfdsad")
    

########NEW FILE########
__FILENAME__ = categorical
# This file is part of Patsy
# Copyright (C) 2011-2013 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

__all__ = ["C", "guess_categorical", "CategoricalSniffer",
           "categorical_to_int"]

# How we handle categorical data: the big picture
# -----------------------------------------------
#
# There is no Python/NumPy standard for how to represent categorical data.
# There is no Python/NumPy standard for how to represent missing data.
#
# Together, these facts mean that when we receive some data object, we must be
# able to heuristically infer what levels it has -- and this process must be
# sensitive to the current missing data handling, because maybe 'None' is a
# level and maybe it is missing data.
#
# We don't know how missing data is represented until we get into the actual
# builder code, so anything which runs before this -- e.g., the 'C()' builtin
# -- cannot actually do *anything* meaningful with the data.
#
# Therefore, C() simply takes some data and arguments, and boxes them all up
# together into an object called (appropriately enough) _CategoricalBox. All
# the actual work of handling the various different sorts of categorical data
# (lists, string arrays, bool arrays, pandas.Categorical, etc.) happens inside
# the builder code, and we just extend this so that it also accepts
# _CategoricalBox objects as yet another categorical type.
#
# Originally this file contained a container type (called 'Categorical'), and
# the various sniffing, conversion, etc., functions were written as methods on
# that type. But we had to get rid of that type, so now this file just
# provides a set of plain old functions which are used by patsy.build to
# handle the different stages of categorical data munging.

import numpy as np
from patsy import PatsyError
from patsy.state import stateful_transform
from patsy.util import (SortAnythingKey,
                        have_pandas, have_pandas_categorical,
                        safe_scalar_isnan,
                        iterable)

if have_pandas:
    import pandas

# Objects of this type will always be treated as categorical, with the
# specified levels and contrast (if given).
class _CategoricalBox(object):
    def __init__(self, data, contrast, levels):
        self.data = data
        self.contrast = contrast
        self.levels = levels

def C(data, contrast=None, levels=None):
    """
    Marks some `data` as being categorical, and specifies how to interpret
    it.

    This is used for three reasons:

    * To explicitly mark some data as categorical. For instance, integer data
      is by default treated as numerical. If you have data that is stored
      using an integer type, but where you want patsy to treat each different
      value as a different level of a categorical factor, you can wrap it in a
      call to `C` to accomplish this. E.g., compare::

        dmatrix("a", {"a": [1, 2, 3]})
        dmatrix("C(a)", {"a": [1, 2, 3]})

    * To explicitly set the levels or override the default level ordering for
      categorical data, e.g.::

        dmatrix("C(a, levels=["a2", "a1"])", balanced(a=2))
    * To override the default coding scheme for categorical data. The
      `contrast` argument can be any of:

      * A :class:`ContrastMatrix` object
      * A simple 2d ndarray (which is treated the same as a ContrastMatrix
        object except that you can't specify column names)
      * An object with methods called `code_with_intercept` and
        `code_without_intercept`, like the built-in contrasts
        (:class:`Treatment`, :class:`Diff`, :class:`Poly`, etc.). See
        :ref:`categorical-coding` for more details.
      * A callable that returns one of the above.
    """
    if isinstance(data, _CategoricalBox):
        if contrast is None:
            contrast = data.contrast
        if levels is None:
            levels = data.levels
        data = data.data
    return _CategoricalBox(data, contrast, levels)

def test_C():
    c1 = C("asdf")
    assert isinstance(c1, _CategoricalBox)
    assert c1.data == "asdf"
    assert c1.levels is None
    assert c1.contrast is None
    c2 = C("DATA", "CONTRAST", "LEVELS")
    assert c2.data == "DATA"
    assert c2.contrast == "CONTRAST"
    assert c2.levels == "LEVELS"
    c3 = C(c2, levels="NEW LEVELS")
    assert c3.data == "DATA"
    assert c3.contrast == "CONTRAST"
    assert c3.levels == "NEW LEVELS"
    c4 = C(c2, "NEW CONTRAST")
    assert c4.data == "DATA"
    assert c4.contrast == "NEW CONTRAST"
    assert c4.levels == "LEVELS"

def guess_categorical(data):
    if have_pandas_categorical and isinstance(data, pandas.Categorical):
        return True
    if isinstance(data, _CategoricalBox):
        return True
    data = np.asarray(data)
    if np.issubdtype(data.dtype, np.number):
        return False
    return True

def test_guess_categorical():
    if have_pandas_categorical:
        assert guess_categorical(pandas.Categorical.from_array([1, 2, 3]))
    assert guess_categorical(C([1, 2, 3]))
    assert guess_categorical([True, False])
    assert guess_categorical(["a", "b"])
    assert guess_categorical(["a", "b", np.nan])
    assert guess_categorical(["a", "b", None])
    assert not guess_categorical([1, 2, 3])
    assert not guess_categorical([1, 2, 3, np.nan])
    assert not guess_categorical([1.0, 2.0, 3.0])
    assert not guess_categorical([1.0, 2.0, 3.0, np.nan])

class CategoricalSniffer(object):
    def __init__(self, NA_action, origin=None):
        self._NA_action = NA_action
        self._origin = origin
        self._contrast = None
        self._levels = None
        self._level_set = set()

    def levels_contrast(self):
        if self._levels is None:
            levels = list(self._level_set)
            levels.sort(key=SortAnythingKey)
            self._levels = levels
        return tuple(self._levels), self._contrast

    def sniff(self, data):
        if hasattr(data, "contrast"):
            self._contrast = data.contrast
        # returns a bool: are we confident that we found all the levels?
        if have_pandas_categorical and isinstance(data, pandas.Categorical):
            # pandas.Categorical has its own NA detection, so don't try to
            # second-guess it.
            self._levels = tuple(data.levels)
            return True
        if isinstance(data, _CategoricalBox):
            if data.levels is not None:
                self._levels = tuple(data.levels)
                return True
            else:
                # unbox and fall through
                data = data.data
        for value in data:
            if self._NA_action.is_categorical_NA(value):
                continue
            if value is True or value is False:
                self._level_set.update([True, False])
            else:
                try:
                    self._level_set.add(value)
                except TypeError:
                    raise PatsyError("Error interpreting categorical data: "
                                     "all items must be hashable",
                                     self._origin)
        # If everything we've seen is boolean, assume that everything else
        # would be too. Otherwise we need to keep looking.
        return self._level_set == set([True, False])

def test_CategoricalSniffer():
    from patsy.missing import NAAction
    def t(NA_types, datas, exp_finish_fast, exp_levels, exp_contrast=None):
        sniffer = CategoricalSniffer(NAAction(NA_types=NA_types))
        for data in datas:
            done = sniffer.sniff(data)
            if done:
                assert exp_finish_fast
                break
            else:
                assert not exp_finish_fast
        assert sniffer.levels_contrast() == (exp_levels, exp_contrast)
    
    if have_pandas_categorical:
        t([], [pandas.Categorical.from_array([1, 2, None])],
          True, (1, 2))
        # check order preservation
        t([], [pandas.Categorical([1, 0], ["a", "b"])],
          True, ("a", "b"))
        t([], [pandas.Categorical([1, 0], ["b", "a"])],
          True, ("b", "a"))
        # check that if someone sticks a .contrast field onto a Categorical
        # object, we pick it up:
        c = pandas.Categorical.from_array(["a", "b"])
        c.contrast = "CONTRAST"
        t([], [c], True, ("a", "b"), "CONTRAST")

    t([], [C([1, 2]), C([3, 2])], False, (1, 2, 3))
    # check order preservation
    t([], [C([1, 2], levels=[1, 2, 3]), C([4, 2])], True, (1, 2, 3))
    t([], [C([1, 2], levels=[3, 2, 1]), C([4, 2])], True, (3, 2, 1))

    # do some actual sniffing with NAs in
    t(["None", "NaN"], [C([1, np.nan]), C([10, None])],
      False, (1, 10))
    # But 'None' can be a type if we don't make it represent NA:
    sniffer = CategoricalSniffer(NAAction(NA_types=["NaN"]))
    sniffer.sniff(C([1, np.nan, None]))
    # The level order here is different on py2 and py3 :-( Because there's no
    # consistent way to sort mixed-type values on both py2 and py3. Honestly
    # people probably shouldn't use this, but I don't know how to give a
    # sensible error.
    levels, _ = sniffer.levels_contrast()
    assert set(levels) == set([None, 1])

    # bool special case
    t(["None", "NaN"], [C([True, np.nan, None])],
      True, (False, True))
    t([], [C([10, 20]), C([False]), C([30, 40])],
      False, (False, True, 10, 20, 30, 40))

    # check tuples too
    t(["None", "NaN"], [C([("b", 2), None, ("a", 1), np.nan, ("c", None)])],
      False, (("a", 1), ("b", 2), ("c", None)))

    # contrasts
    t([], [C([10, 20], contrast="FOO")], False, (10, 20), "FOO")

    # unhashable level error:
    from nose.tools import assert_raises
    sniffer = CategoricalSniffer(NAAction())
    assert_raises(PatsyError, sniffer.sniff, [{}])

# returns either a 1d ndarray or a pandas.Series
def categorical_to_int(data, levels, NA_action, origin=None):
    assert isinstance(levels, tuple)
    # In this function, missing values are always mapped to -1
    if have_pandas_categorical and isinstance(data, pandas.Categorical):
        data_levels_tuple = tuple(data.levels)
        if not data_levels_tuple == levels:
            raise PatsyError("mismatching levels: expected %r, got %r"
                             % (levels, data_levels_tuple), origin)
        # pandas.Categorical also uses -1 to indicate NA, and we don't try to
        # second-guess its NA detection, so we can just pass it back.
        return data.labels
    if isinstance(data, _CategoricalBox):
        if data.levels is not None and tuple(data.levels) != levels:
            raise PatsyError("mismatching levels: expected %r, got %r"
                             % (levels, tuple(data.levels)), origin)
        data = data.data
    if hasattr(data, "shape") and len(data.shape) > 1:
        raise PatsyError("categorical data must be 1-dimensional",
                         origin)
    if not iterable(data) or isinstance(data, basestring):
        raise PatsyError("categorical data must be an iterable container")
    try:
        level_to_int = dict(zip(levels, xrange(len(levels))))
    except TypeError:
        raise PatsyError("Error interpreting categorical data: "
                         "all items must be hashable", origin)
    out = np.empty(len(data), dtype=int)
    for i, value in enumerate(data):
        if NA_action.is_categorical_NA(value):
            out[i] = -1
        else:
            try:
                out[i] = level_to_int[value]
            except KeyError:
                SHOW_LEVELS = 4
                level_strs = []
                if len(levels) <= SHOW_LEVELS:
                    level_strs += [repr(level) for level in levels]
                else:
                    level_strs += [repr(level)
                                   for level in levels[:SHOW_LEVELS//2]]
                    level_strs.append("...")
                    level_strs += [repr(level)
                                   for level in levels[-SHOW_LEVELS//2:]]
                level_str = "[%s]" % (", ".join(level_strs))
                raise PatsyError("Error converting data to categorical: "
                                 "observation with value %r does not match "
                                 "any of the expected levels (expected: %s)"
                                 % (value, level_str), origin)
            except TypeError:
                raise PatsyError("Error converting data to categorical: "
                                 "encountered unhashable value %r"
                                 % (value,), origin)
    if have_pandas and isinstance(data, pandas.Series):
        out = pandas.Series(out, index=data.index)
    return out

def test_categorical_to_int():
    from nose.tools import assert_raises
    from patsy.missing import NAAction
    if have_pandas:
        s = pandas.Series(["a", "b", "c"], index=[10, 20, 30])
        c_pandas = categorical_to_int(s, ("a", "b", "c"), NAAction())
        assert np.all(c_pandas == [0, 1, 2])
        assert np.all(c_pandas.index == [10, 20, 30])
        # Input must be 1-dimensional
        assert_raises(PatsyError,
                      categorical_to_int,
                      pandas.DataFrame({10: s}), ("a", "b", "c"), NAAction())
    if have_pandas_categorical:
        cat = pandas.Categorical([1, 0, -1], ("a", "b"))
        conv = categorical_to_int(cat, ("a", "b"), NAAction())
        assert np.all(conv == [1, 0, -1])
        # Trust pandas NA marking
        cat2 = pandas.Categorical([1, 0, -1], ("a", "None"))
        conv2 = categorical_to_int(cat, ("a", "b"), NAAction(NA_types=["None"]))
        assert np.all(conv2 == [1, 0, -1])
        # But levels must match
        assert_raises(PatsyError,
                      categorical_to_int,
                      pandas.Categorical([1, 0], ("a", "b")),
                      ("a", "c"),
                      NAAction())
        assert_raises(PatsyError,
                      categorical_to_int,
                      pandas.Categorical([1, 0], ("a", "b")),
                      ("b", "a"),
                      NAAction())

    def t(data, levels, expected, NA_action=NAAction()):
        got = categorical_to_int(data, levels, NA_action)
        assert np.array_equal(got, expected)

    t(["a", "b", "a"], ("a", "b"), [0, 1, 0])
    t(np.asarray(["a", "b", "a"]), ("a", "b"), [0, 1, 0])
    t(np.asarray(["a", "b", "a"], dtype=object), ("a", "b"), [0, 1, 0])
    t([0, 1, 2], (1, 2, 0), [2, 0, 1])
    t(np.asarray([0, 1, 2]), (1, 2, 0), [2, 0, 1])
    t(np.asarray([0, 1, 2], dtype=float), (1, 2, 0), [2, 0, 1])
    t(np.asarray([0, 1, 2], dtype=object), (1, 2, 0), [2, 0, 1])
    t(["a", "b", "a"], ("a", "d", "z", "b"), [0, 3, 0])
    t([("a", 1), ("b", 0), ("a", 1)], (("a", 1), ("b", 0)), [0, 1, 0])

    assert_raises(PatsyError, categorical_to_int,
                  ["a", "b", "a"], ("a", "c"), NAAction())

    t(C(["a", "b", "a"]), ("a", "b"), [0, 1, 0])
    t(C(["a", "b", "a"]), ("b", "a"), [1, 0, 1])
    t(C(["a", "b", "a"], levels=["b", "a"]), ("b", "a"), [1, 0, 1])
    # Mismatch between C() levels and expected levels
    assert_raises(PatsyError, categorical_to_int,
                  C(["a", "b", "a"], levels=["a", "b"]),
                  ("b", "a"), NAAction())

    # ndim == 2 is disallowed
    assert_raises(PatsyError, categorical_to_int,
                  np.asarray([["a", "b"], ["b", "a"]]),
                  ("a", "b"), NAAction())
    # ndim == 0 is disallowed likewise
    assert_raises(PatsyError, categorical_to_int,
                  "a",
                  ("a", "b"), NAAction())

    # levels must be hashable
    assert_raises(PatsyError, categorical_to_int,
                  ["a", "b"], ("a", "b", {}), NAAction())
    assert_raises(PatsyError, categorical_to_int,
                  ["a", "b", {}], ("a", "b"), NAAction())

    t(["b", None, np.nan, "a"], ("a", "b"), [1, -1, -1, 0],
      NAAction(NA_types=["None", "NaN"]))
    t(["b", None, np.nan, "a"], ("a", "b", None), [1, -1, -1, 0],
      NAAction(NA_types=["None", "NaN"]))
    t(["b", None, np.nan, "a"], ("a", "b", None), [1, 2, -1, 0],
      NAAction(NA_types=["NaN"]))

    # Smoke test for the branch that formats the ellipsized list of levels in
    # the error message:
    assert_raises(PatsyError, categorical_to_int,
                  ["a", "b", "q"],
                  ("a", "b", "c", "d", "e", "f", "g", "h"),
                  NAAction())

########NEW FILE########
__FILENAME__ = compat
# This file is part of Patsy
# Copyright (C) 2012 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# This file contains compatibility code for supporting old versions of Python
# and numpy. (If we can concentrate it here, hopefully it'll make it easier to
# get rid of weird hacks once we drop support for old versions).

##### Numpy

import os
# To force use of the compat code, set this env var to a non-empty value:
optional_dep_ok = not os.environ.get("PATSY_AVOID_OPTIONAL_DEPENDENCIES")

# The *_indices functions were added in numpy 1.4
import numpy as np
if optional_dep_ok and hasattr(np, "triu_indices"):
    from numpy import triu_indices
    from numpy import tril_indices
    from numpy import diag_indices
else:
    def triu_indices(n):
        return np.triu(np.ones((n, n))).nonzero()
    def tril_indices(n):
        return np.tril(np.ones((n, n))).nonzero()
    def diag_indices(n):
        return (np.arange(n), np.arange(n))

##### Python standard library

# The Python license requires that all derivative works contain a "brief
# summary of the changes made to Python". Both for license compliance, and for
# our own sanity, therefore, please add a note at the top of any snippets you
# add here explaining their provenance, any changes made, and what versions of
# Python require them:

# Copied unchanged from Python 2.7.3's re.py module; all I did was add the
# import statements at the top.
# This code seems to be included in Python 2.5+.
import re
if optional_dep_ok and hasattr(re, "Scanner"):
    Scanner = re.Scanner
else:
    import sre_parse
    import sre_compile
    class Scanner:
        def __init__(self, lexicon, flags=0):
            from sre_constants import BRANCH, SUBPATTERN
            self.lexicon = lexicon
            # combine phrases into a compound pattern
            p = []
            s = sre_parse.Pattern()
            s.flags = flags
            for phrase, action in lexicon:
                p.append(sre_parse.SubPattern(s, [
                    (SUBPATTERN, (len(p)+1, sre_parse.parse(phrase, flags))),
                    ]))
            s.groups = len(p)+1
            p = sre_parse.SubPattern(s, [(BRANCH, (None, p))])
            self.scanner = sre_compile.compile(p)
        def scan(self, string):
            result = []
            append = result.append
            match = self.scanner.scanner(string).match
            i = 0
            while 1:
                m = match()
                if not m:
                    break
                j = m.end()
                if i == j:
                    break
                action = self.lexicon[m.lastindex-1][1]
                if hasattr(action, '__call__'):
                    self.match = m
                    action = action(self, m.group())
                if action is not None:
                    append(action)
                i = j
            return result, string[i:]

# itertools.product available in Python 2.6+
import itertools
if optional_dep_ok and hasattr(itertools, "product"):
    itertools_product = itertools.product
else:
    # Copied directly from the Python documentation:
    def itertools_product(*args, **kwds):
        # product('ABCD', 'xy') --> Ax Ay Bx By Cx Cy Dx Dy
        # product(range(2), repeat=3) --> 000 001 010 011 100 101 110 111
        pools = map(tuple, args) * kwds.get('repeat', 1)
        result = [[]]
        for pool in pools:
            result = [x+[y] for x in result for y in pool]
        for prod in result:
            yield tuple(prod)    

# functools available in Python 2.5+
# This is just a cosmetic thing, so don't bother emulating it if we don't
# have it.
def compat_wraps(f1):
    def do_wrap(f2):
        return f2
    return do_wrap
if optional_dep_ok:
    try:
        from functools import wraps
    except ImportError:
        wraps = compat_wraps
else:
    wraps = compat_wraps

# collections.Mapping available in Python 2.6+
import collections
if optional_dep_ok and hasattr(collections, "Mapping"):
    Mapping = collections.Mapping
else:
    Mapping = dict

# OrderedDict is only available in Python 2.7+. compat_ordereddict.py has
# comments at the top.
import collections
if optional_dep_ok and hasattr(collections, "OrderedDict"):
    from collections import OrderedDict
else:
    from patsy.compat_ordereddict import OrderedDict

# 'raise from' available in Python 3+
import sys
from patsy import PatsyError
def call_and_wrap_exc(msg, origin, f, *args, **kwargs):
    try:
        return f(*args, **kwargs)
    except Exception, e:
        if sys.version_info[0] >= 3:
            new_exc = PatsyError("%s: %s: %s"
                                 % (msg, e.__class__.__name__, e),
                                 origin)
            # Use 'exec' to hide this syntax from the Python 2 parser:
            exec("raise new_exc from e")
        else:
            # In python 2, we just let the original exception escape -- better
            # than destroying the traceback. But if it's a PatsyError, we can
            # at least set the origin properly.
            if isinstance(e, PatsyError):
                e.set_origin(origin)
            raise

########NEW FILE########
__FILENAME__ = compat_ordereddict
# Backport of OrderedDict() class that runs on Python 2.4, 2.5, 2.6, 2.7 and pypy.
# Passes Python2.7's test suite and incorporates all the latest updates.

#Author: Raymond Hettinger
#License: MIT License
#http://code.activestate.com/recipes/576693/ revision 9, downloaded 2012-03-28

try:
    from thread import get_ident as _get_ident
except ImportError:
    from dummy_thread import get_ident as _get_ident

try:
    from _abcoll import KeysView, ValuesView, ItemsView
except ImportError:
    pass


class OrderedDict(dict): # pragma: no cover
    'Dictionary that remembers insertion order'
    # An inherited dict maps keys to values.
    # The inherited dict provides __getitem__, __len__, __contains__, and get.
    # The remaining methods are order-aware.
    # Big-O running times for all methods are the same as for regular dictionaries.

    # The internal self.__map dictionary maps keys to links in a doubly linked list.
    # The circular doubly linked list starts and ends with a sentinel element.
    # The sentinel element never gets deleted (this simplifies the algorithm).
    # Each link is stored as a list of length three:  [PREV, NEXT, KEY].

    def __init__(self, *args, **kwds):
        '''Initialize an ordered dictionary.  Signature is the same as for
        regular dictionaries, but keyword arguments are not recommended
        because their insertion order is arbitrary.

        '''
        if len(args) > 1:
            raise TypeError('expected at most 1 arguments, got %d' % len(args))
        try:
            self.__root
        except AttributeError:
            self.__root = root = []                     # sentinel node
            root[:] = [root, root, None]
            self.__map = {}
        self.__update(*args, **kwds)

    def __setitem__(self, key, value, dict_setitem=dict.__setitem__):
        'od.__setitem__(i, y) <==> od[i]=y'
        # Setting a new item creates a new link which goes at the end of the linked
        # list, and the inherited dictionary is updated with the new key/value pair.
        if key not in self:
            root = self.__root
            last = root[0]
            last[1] = root[0] = self.__map[key] = [last, root, key]
        dict_setitem(self, key, value)

    def __delitem__(self, key, dict_delitem=dict.__delitem__):
        'od.__delitem__(y) <==> del od[y]'
        # Deleting an existing item uses self.__map to find the link which is
        # then removed by updating the links in the predecessor and successor nodes.
        dict_delitem(self, key)
        link_prev, link_next, key = self.__map.pop(key)
        link_prev[1] = link_next
        link_next[0] = link_prev

    def __iter__(self):
        'od.__iter__() <==> iter(od)'
        root = self.__root
        curr = root[1]
        while curr is not root:
            yield curr[2]
            curr = curr[1]

    def __reversed__(self):
        'od.__reversed__() <==> reversed(od)'
        root = self.__root
        curr = root[0]
        while curr is not root:
            yield curr[2]
            curr = curr[0]

    def clear(self):
        'od.clear() -> None.  Remove all items from od.'
        try:
            for node in self.__map.itervalues():
                del node[:]
            root = self.__root
            root[:] = [root, root, None]
            self.__map.clear()
        except AttributeError:
            pass
        dict.clear(self)

    def popitem(self, last=True):
        '''od.popitem() -> (k, v), return and remove a (key, value) pair.
        Pairs are returned in LIFO order if last is true or FIFO order if false.

        '''
        if not self:
            raise KeyError('dictionary is empty')
        root = self.__root
        if last:
            link = root[0]
            link_prev = link[0]
            link_prev[1] = root
            root[0] = link_prev
        else:
            link = root[1]
            link_next = link[1]
            root[1] = link_next
            link_next[0] = root
        key = link[2]
        del self.__map[key]
        value = dict.pop(self, key)
        return key, value

    # -- the following methods do not depend on the internal structure --

    def keys(self):
        'od.keys() -> list of keys in od'
        return list(self)

    def values(self):
        'od.values() -> list of values in od'
        return [self[key] for key in self]

    def items(self):
        'od.items() -> list of (key, value) pairs in od'
        return [(key, self[key]) for key in self]

    def iterkeys(self):
        'od.iterkeys() -> an iterator over the keys in od'
        return iter(self)

    def itervalues(self):
        'od.itervalues -> an iterator over the values in od'
        for k in self:
            yield self[k]

    def iteritems(self):
        'od.iteritems -> an iterator over the (key, value) items in od'
        for k in self:
            yield (k, self[k])

    def update(*args, **kwds):
        '''od.update(E, **F) -> None.  Update od from dict/iterable E and F.

        If E is a dict instance, does:           for k in E: od[k] = E[k]
        If E has a .keys() method, does:         for k in E.keys(): od[k] = E[k]
        Or if E is an iterable of items, does:   for k, v in E: od[k] = v
        In either case, this is followed by:     for k, v in F.items(): od[k] = v

        '''
        if len(args) > 2:
            raise TypeError('update() takes at most 2 positional '
                            'arguments (%d given)' % (len(args),))
        elif not args:
            raise TypeError('update() takes at least 1 argument (0 given)')
        self = args[0]
        # Make progressively weaker assumptions about "other"
        other = ()
        if len(args) == 2:
            other = args[1]
        if isinstance(other, dict):
            for key in other:
                self[key] = other[key]
        elif hasattr(other, 'keys'):
            for key in other.keys():
                self[key] = other[key]
        else:
            for key, value in other:
                self[key] = value
        for key, value in kwds.items():
            self[key] = value

    __update = update  # let subclasses override update without breaking __init__

    __marker = object()

    def pop(self, key, default=__marker):
        '''od.pop(k[,d]) -> v, remove specified key and return the corresponding value.
        If key is not found, d is returned if given, otherwise KeyError is raised.

        '''
        if key in self:
            result = self[key]
            del self[key]
            return result
        if default is self.__marker:
            raise KeyError(key)
        return default

    def setdefault(self, key, default=None):
        'od.setdefault(k[,d]) -> od.get(k,d), also set od[k]=d if k not in od'
        if key in self:
            return self[key]
        self[key] = default
        return default

    def __repr__(self, _repr_running={}):
        'od.__repr__() <==> repr(od)'
        call_key = id(self), _get_ident()
        if call_key in _repr_running:
            return '...'
        _repr_running[call_key] = 1
        try:
            if not self:
                return '%s()' % (self.__class__.__name__,)
            return '%s(%r)' % (self.__class__.__name__, self.items())
        finally:
            del _repr_running[call_key]

    def __reduce__(self):
        'Return state information for pickling'
        items = [[k, self[k]] for k in self]
        inst_dict = vars(self).copy()
        for k in vars(OrderedDict()):
            inst_dict.pop(k, None)
        if inst_dict:
            return (self.__class__, (items,), inst_dict)
        return self.__class__, (items,)

    def copy(self):
        'od.copy() -> a shallow copy of od'
        return self.__class__(self)

    @classmethod
    def fromkeys(cls, iterable, value=None):
        '''OD.fromkeys(S[, v]) -> New ordered dictionary with keys from S
        and values equal to v (which defaults to None).

        '''
        d = cls()
        for key in iterable:
            d[key] = value
        return d

    def __eq__(self, other):
        '''od.__eq__(y) <==> od==y.  Comparison to another OD is order-sensitive
        while comparison to a regular mapping is order-insensitive.

        '''
        if isinstance(other, OrderedDict):
            return len(self)==len(other) and self.items() == other.items()
        return dict.__eq__(self, other)

    def __ne__(self, other):
        return not self == other

    # -- the following methods are only used in Python 2.7 --

    def viewkeys(self):
        "od.viewkeys() -> a set-like object providing a view on od's keys"
        return KeysView(self)

    def viewvalues(self):
        "od.viewvalues() -> an object providing a view on od's values"
        return ValuesView(self)

    def viewitems(self):
        "od.viewitems() -> a set-like object providing a view on od's items"
        return ItemsView(self)

########NEW FILE########
__FILENAME__ = constraint
# This file is part of Patsy
# Copyright (C) 2011-2012 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# Interpreting linear constraints like "2*x1 + x2 = 0"

# These are made available in the patsy.* namespace
__all__ = ["LinearConstraint"]

import re
import numpy as np
from patsy import PatsyError
from patsy.origin import Origin
from patsy.util import (atleast_2d_column_default,
                           repr_pretty_delegate, repr_pretty_impl,
                           SortAnythingKey)
from patsy.infix_parser import Token, Operator, ParseNode, infix_parse
from patsy.compat import Scanner, Mapping

class LinearConstraint(object):
    """A linear constraint in matrix form.

    This object represents a linear constraint of the form `Ax = b`.

    Usually you won't be constructing these by hand, but instead get them as
    the return value from :meth:`DesignInfo.linear_constraint`.

    .. attribute:: coefs

       A 2-dimensional ndarray with float dtype, representing `A`.

    .. attribute:: constants

       A 2-dimensional single-column ndarray with float dtype, representing
       `b`.

    .. attribute:: variable_names

       A list of strings giving the names of the variables being
       constrained. (Used only for consistency checking.)
    """
    def __init__(self, variable_names, coefs, constants=None):
        self.variable_names = list(variable_names)
        self.coefs = np.atleast_2d(np.asarray(coefs, dtype=float))
        if constants is None:
            constants = np.zeros(self.coefs.shape[0], dtype=float)
        constants = np.asarray(constants, dtype=float)
        self.constants = atleast_2d_column_default(constants)
        if self.constants.ndim != 2 or self.constants.shape[1] != 1:
            raise ValueError("constants is not (convertible to) a column matrix")
        if self.coefs.ndim != 2 or self.coefs.shape[1] != len(variable_names):
            raise ValueError("wrong shape for coefs")
        if self.coefs.shape[0] == 0:
            raise ValueError("must have at least one row in constraint matrix")
        if self.coefs.shape[0] != self.constants.shape[0]:
            raise ValueError("shape mismatch between coefs and constants")
        if np.any(np.all(self.coefs == 0, axis=1)):
            raise ValueError("can't test a constant constraint")

    __repr__ = repr_pretty_delegate
    def _repr_pretty_(self, p, cycle):
        assert not cycle
        return repr_pretty_impl(p, self,
                                [self.variable_names, self.coefs, self.constants])

    @classmethod
    def combine(cls, constraints):
        """Create a new LinearConstraint by ANDing together several existing
        LinearConstraints.

        :arg constraints: An iterable of LinearConstraint objects. Their
          :attr:`variable_names` attributes must all match.
        :returns: A new LinearConstraint object.
        """
        if not constraints:
            raise ValueError("no constraints specified")
        variable_names = constraints[0].variable_names
        for constraint in constraints:
            if constraint.variable_names != variable_names:
                raise ValueError("variable names don't match")
        coefs = np.row_stack([c.coefs for c in constraints])
        constants = np.row_stack([c.constants for c in constraints])
        return cls(variable_names, coefs, constants)

def test_LinearConstraint():
    from numpy.testing.utils import assert_equal
    lc = LinearConstraint(["foo", "bar"], [1, 1])
    assert lc.variable_names == ["foo", "bar"]
    assert_equal(lc.coefs, [[1, 1]])
    assert_equal(lc.constants, [[0]])

    lc = LinearConstraint(["foo", "bar"], [[1, 1], [2, 3]], [10, 20])
    assert_equal(lc.coefs, [[1, 1], [2, 3]])
    assert_equal(lc.constants, [[10], [20]])
    
    assert lc.coefs.dtype == np.dtype(float)
    assert lc.constants.dtype == np.dtype(float)

    from nose.tools import assert_raises
    assert_raises(ValueError, LinearConstraint, ["a"], [[1, 2]])
    assert_raises(ValueError, LinearConstraint, ["a"], [[[1]]])
    assert_raises(ValueError, LinearConstraint, ["a"], [[1, 2]], [3, 4])
    assert_raises(ValueError, LinearConstraint, ["a", "b"], [[1, 2]], [3, 4])
    assert_raises(ValueError, LinearConstraint, ["a"], [[0]])
    assert_raises(ValueError, LinearConstraint, ["a"], [[1]], [[]])
    assert_raises(ValueError, LinearConstraint, ["a", "b"], [])
    assert_raises(ValueError, LinearConstraint, ["a", "b"],
                  np.zeros((0, 2)))

def test_LinearConstraint_combine():
    comb = LinearConstraint.combine([LinearConstraint(["a", "b"], [1, 0]),
                                     LinearConstraint(["a", "b"], [0, 1], [1])])
    assert comb.variable_names == ["a", "b"]
    from numpy.testing.utils import assert_equal
    assert_equal(comb.coefs, [[1, 0], [0, 1]])
    assert_equal(comb.constants, [[0], [1]])

    from nose.tools import assert_raises
    assert_raises(ValueError, LinearConstraint.combine, [])
    assert_raises(ValueError, LinearConstraint.combine,
                  [LinearConstraint(["a"], [1]), LinearConstraint(["b"], [1])])
    

_ops = [
    Operator(",", 2, -100),

    Operator("=", 2, 0),
    
    Operator("+", 1, 100),
    Operator("-", 1, 100),
    Operator("+", 2, 100),
    Operator("-", 2, 100),

    Operator("*", 2, 200),
    Operator("/", 2, 200),
    ]

_atomic = ["NUMBER", "VARIABLE"]

def _token_maker(type, string):
    def make_token(scanner, token_string):
        if type == "__OP__":
            actual_type = token_string
        else:
            actual_type = type
        return Token(actual_type,
                     Origin(string, *scanner.match.span()),
                     token_string)
    return make_token

def _tokenize_constraint(string, variable_names):
    lparen_re = r"\("
    rparen_re = r"\)"
    op_re = "|".join([re.escape(op.token_type) for op in _ops])
    num_re = r"[-+]?[0-9]*\.?[0-9]+([eE][-+]?[0-9]+)?"
    whitespace_re = r"\s+"

    # Prefer long matches:
    variable_names = sorted(variable_names, key=len, reverse=True)
    variable_re = "|".join([re.escape(n) for n in variable_names])

    lexicon = [
        (lparen_re, _token_maker(Token.LPAREN, string)),
        (rparen_re, _token_maker(Token.RPAREN, string)),
        (op_re, _token_maker("__OP__", string)),
        (variable_re, _token_maker("VARIABLE", string)),
        (num_re, _token_maker("NUMBER", string)),
        (whitespace_re, None),
        ]

    scanner = Scanner(lexicon)
    tokens, leftover = scanner.scan(string)
    if leftover:
        offset = len(string) - len(leftover)
        raise PatsyError("unrecognized token in constraint",
                            Origin(string, offset, offset + 1))

    return tokens

def test__tokenize_constraint():
    code = "2 * (a + b) = q"
    tokens = _tokenize_constraint(code, ["a", "b", "q"])
    expecteds = [("NUMBER", 0, 1, "2"),
                 ("*", 2, 3, "*"),
                 (Token.LPAREN, 4, 5, "("),
                 ("VARIABLE", 5, 6, "a"),
                 ("+", 7, 8, "+"),
                 ("VARIABLE", 9, 10, "b"),
                 (Token.RPAREN, 10, 11, ")"),
                 ("=", 12, 13, "="),
                 ("VARIABLE", 14, 15, "q")]
    for got, expected in zip(tokens, expecteds):
        assert isinstance(got, Token)
        assert got.type == expected[0]
        assert got.origin == Origin(code, expected[1], expected[2])
        assert got.extra == expected[3]

    from nose.tools import assert_raises
    assert_raises(PatsyError, _tokenize_constraint, "1 + @b", ["b"])
    # Shouldn't raise an error:
    _tokenize_constraint("1 + @b", ["@b"])

    # Check we aren't confused by names which are proper prefixes of other
    # names:
    for names in (["a", "aa"], ["aa", "a"]):
        tokens = _tokenize_constraint("a aa a", names)
        assert len(tokens) == 3
        assert [t.extra for t in tokens] == ["a", "aa", "a"]

    # Check that embedding ops and numbers inside a variable name works
    tokens = _tokenize_constraint("2 * a[1,1],", ["a[1,1]"])
    assert len(tokens) == 4
    assert [t.type for t in tokens] == ["NUMBER", "*", "VARIABLE", ","]
    assert [t.extra for t in tokens] == ["2", "*", "a[1,1]", ","]

def parse_constraint(string, variable_names):
    return infix_parse(_tokenize_constraint(string, variable_names),
                       _ops, _atomic)

class _EvalConstraint(object):
    def __init__(self, variable_names):
        self._variable_names = variable_names
        self._N = len(variable_names)

        self._dispatch = {
            ("VARIABLE", 0): self._eval_variable,
            ("NUMBER", 0): self._eval_number,
            ("+", 1): self._eval_unary_plus,
            ("-", 1): self._eval_unary_minus,
            ("+", 2): self._eval_binary_plus,
            ("-", 2): self._eval_binary_minus,
            ("*", 2): self._eval_binary_multiply,
            ("/", 2): self._eval_binary_div,
            ("=", 2): self._eval_binary_eq,
            (",", 2): self._eval_binary_comma,
            }

    # General scheme: there are 2 types we deal with:
    #   - linear combinations ("lincomb"s) of variables and constants,
    #     represented as ndarrays with size N+1
    #     The last entry is the constant, so [10, 20, 30] means 10x + 20y +
    #     30.
    #   - LinearConstraint objects

    def is_constant(self, coefs):
        return np.all(coefs[:self._N] == 0)

    def _eval_variable(self, tree):
        var = tree.token.extra
        coefs = np.zeros((self._N + 1,), dtype=float)
        coefs[self._variable_names.index(var)] = 1
        return coefs

    def _eval_number(self, tree):
        coefs = np.zeros((self._N + 1,), dtype=float)
        coefs[-1] = float(tree.token.extra)
        return coefs

    def _eval_unary_plus(self, tree):
        return self.eval(tree.args[0])

    def _eval_unary_minus(self, tree):
        return -1 * self.eval(tree.args[0])

    def _eval_binary_plus(self, tree):
        return self.eval(tree.args[0]) + self.eval(tree.args[1])

    def _eval_binary_minus(self, tree):
        return self.eval(tree.args[0]) - self.eval(tree.args[1])

    def _eval_binary_div(self, tree):
        left = self.eval(tree.args[0])
        right = self.eval(tree.args[1])
        if not self.is_constant(right):
            raise PatsyError("Can't divide by a variable in a linear "
                                "constraint", tree.args[1])
        return left / right[-1]

    def _eval_binary_multiply(self, tree):
        left = self.eval(tree.args[0])
        right = self.eval(tree.args[1])
        if self.is_constant(left):
            return left[-1] * right
        elif self.is_constant(right):
            return left * right[-1]
        else:
            raise PatsyError("Can't multiply one variable by another "
                                "in a linear constraint", tree)

    def _eval_binary_eq(self, tree):
        # Handle "a1 = a2 = a3", which is parsed as "(a1 = a2) = a3"
        args = list(tree.args)
        constraints = []
        for i, arg in enumerate(args):
            if arg.type == "=":
                constraints.append(self.eval(arg, constraint=True))
                # make our left argument be their right argument, or
                # vice-versa
                args[i] = arg.args[1 - i]
        left = self.eval(args[0])
        right = self.eval(args[1])
        coefs = left[:self._N] - right[:self._N]
        if np.all(coefs == 0):
            raise PatsyError("no variables appear in constraint", tree)
        constant = -left[-1] + right[-1]
        constraint = LinearConstraint(self._variable_names, coefs, constant)
        constraints.append(constraint)
        return LinearConstraint.combine(constraints)

    def _eval_binary_comma(self, tree):
        left = self.eval(tree.args[0], constraint=True)
        right = self.eval(tree.args[1], constraint=True)
        return LinearConstraint.combine([left, right])

    def eval(self, tree, constraint=False):
        key = (tree.type, len(tree.args))
        assert key in self._dispatch
        val = self._dispatch[key](tree)
        if constraint:
            # Force it to be a constraint
            if isinstance(val, LinearConstraint):
                return val
            else:
                assert val.size == self._N + 1
                if np.all(val[:self._N] == 0):
                    raise PatsyError("term is constant, with no variables",
                                        tree)
                return LinearConstraint(self._variable_names,
                                        val[:self._N],
                                        -val[-1])
        else:
            # Force it to *not* be a constraint
            if isinstance(val, LinearConstraint):
                raise PatsyError("unexpected constraint object", tree)
            return val

def linear_constraint(constraint_like, variable_names):
    """This is the internal interface implementing
    DesignInfo.linear_constraint, see there for docs."""
    if isinstance(constraint_like, LinearConstraint):
        if constraint_like.variable_names != variable_names:
            raise ValueError("LinearConstraint has wrong variable_names "
                             "(got %r, expected %r)"
                             % (constraint_like.variable_names,
                                variable_names))
        return constraint_like

    if isinstance(constraint_like, Mapping):
        # Simple conjunction-of-equality constraints can be specified as
        # dicts. {"x": 1, "y": 2} -> tests x = 1 and y = 2. Keys can be
        # either variable names, or variable indices.
        coefs = np.zeros((len(constraint_like), len(variable_names)),
                         dtype=float)
        constants = np.zeros(len(constraint_like))
        used = set()
        for i, (name, value) in enumerate(constraint_like.iteritems()):
            if name in variable_names:
                idx = variable_names.index(name)
            elif isinstance(name, (int, long)):
                idx = name
            else:
                raise ValueError("unrecognized variable name/index %r"
                                 % (name,))
            if idx in used:
                raise ValueError("duplicated constraint on %r"
                                 % (variable_names[idx],))
            used.add(idx)
            coefs[i, idx] = 1
            constants[i] = value
        return LinearConstraint(variable_names, coefs, constants)

    if isinstance(constraint_like, basestring):
        constraint_like = [constraint_like]
        # fall-through

    if (isinstance(constraint_like, list)
        and constraint_like
        and isinstance(constraint_like[0], basestring)):
        constraints = []
        for code in constraint_like:
            if not isinstance(code, basestring):
                raise ValueError("expected a string, not %r" % (code,))
            tree = parse_constraint(code, variable_names)
            evaluator = _EvalConstraint(variable_names)
            constraints.append(evaluator.eval(tree, constraint=True))
        return LinearConstraint.combine(constraints)

    if isinstance(constraint_like, tuple):
        if len(constraint_like) != 2:
            raise ValueError, "constraint tuple must have length 2"
        coef, constants = constraint_like
        return LinearConstraint(variable_names, coef, constants)

    # assume a raw ndarray
    coefs = np.asarray(constraint_like, dtype=float)
    return LinearConstraint(variable_names, coefs)

def _check_lincon(input, varnames, coefs, constants):
    from numpy.testing.utils import assert_equal
    got = linear_constraint(input, varnames)
    print "got", got
    expected = LinearConstraint(varnames, coefs, constants)
    print "expected", expected
    assert_equal(got.variable_names, expected.variable_names)
    assert_equal(got.coefs, expected.coefs)
    assert_equal(got.constants, expected.constants)
    assert_equal(got.coefs.dtype, np.dtype(float))
    assert_equal(got.constants.dtype, np.dtype(float))

def test_linear_constraint():
    from nose.tools import assert_raises
    from patsy.compat import OrderedDict
    t = _check_lincon

    t(LinearConstraint(["a", "b"], [2, 3]), ["a", "b"], [[2, 3]], [[0]])
    assert_raises(ValueError, linear_constraint,
                  LinearConstraint(["b", "a"], [2, 3]),
                  ["a", "b"])

    t({"a": 2}, ["a", "b"], [[1, 0]], [[2]])
    t(OrderedDict([("a", 2), ("b", 3)]),
      ["a", "b"], [[1, 0], [0, 1]], [[2], [3]])
    t(OrderedDict([("a", 2), ("b", 3)]),
      ["b", "a"], [[0, 1], [1, 0]], [[2], [3]])

    t({0: 2}, ["a", "b"], [[1, 0]], [[2]])
    t(OrderedDict([(0, 2), (1, 3)]), ["a", "b"], [[1, 0], [0, 1]], [[2], [3]])

    t(OrderedDict([("a", 2), (1, 3)]),
      ["a", "b"], [[1, 0], [0, 1]], [[2], [3]])

    assert_raises(ValueError, linear_constraint, {"q": 1}, ["a", "b"])
    assert_raises(ValueError, linear_constraint, {"a": 1, 0: 2}, ["a", "b"])

    t(np.array([2, 3]), ["a", "b"], [[2, 3]], [[0]])
    t(np.array([[2, 3], [4, 5]]), ["a", "b"], [[2, 3], [4, 5]], [[0], [0]])

    t("a = 2", ["a", "b"], [[1, 0]], [[2]])
    t("a - 2", ["a", "b"], [[1, 0]], [[2]])
    t("a + 1 = 3", ["a", "b"], [[1, 0]], [[2]])
    t("a + b = 3", ["a", "b"], [[1, 1]], [[3]])
    t("a = 2, b = 3", ["a", "b"], [[1, 0], [0, 1]], [[2], [3]])
    t("b = 3, a = 2", ["a", "b"], [[0, 1], [1, 0]], [[3], [2]])

    t(["a = 2", "b = 3"], ["a", "b"], [[1, 0], [0, 1]], [[2], [3]])

    assert_raises(ValueError, linear_constraint, ["a", {"b": 0}], ["a", "b"])

    # Actual evaluator tests
    t("2 * (a + b/3) + b + 2*3/4 = 1 + 2*3", ["a", "b"],
      [[2, 2.0/3 + 1]], [[7 - 6.0/4]])
    t("+2 * -a", ["a", "b"], [[-2, 0]], [[0]])
    t("a - b, a + b = 2", ["a", "b"], [[1, -1], [1, 1]], [[0], [2]])
    t("a = 1, a = 2, a = 3", ["a", "b"],
      [[1, 0], [1, 0], [1, 0]], [[1], [2], [3]])
    t("a * 2", ["a", "b"], [[2, 0]], [[0]])
    t("-a = 1", ["a", "b"], [[-1, 0]], [[1]])
    t("(2 + a - a) * b", ["a", "b"], [[0, 2]], [[0]])

    t("a = 1 = b", ["a", "b"], [[1, 0], [0, -1]], [[1], [-1]])
    t("a = (1 = b)", ["a", "b"], [[0, -1], [1, 0]], [[-1], [1]])
    t("a = 1, a = b = c", ["a", "b", "c"],
      [[1, 0, 0], [1, -1, 0], [0, 1, -1]], [[1], [0], [0]])

    # One should never do this of course, but test that it works anyway...
    t("a + 1 = 2", ["a", "a + 1"], [[0, 1]], [[2]])

    t(([10, 20], [30]), ["a", "b"], [[10, 20]], [[30]])
    t(([[10, 20], [20, 40]], [[30], [35]]), ["a", "b"],
      [[10, 20], [20, 40]], [[30], [35]])
    # wrong-length tuple
    assert_raises(ValueError, linear_constraint,
                  ([1, 0], [0], [0]), ["a", "b"])
    assert_raises(ValueError, linear_constraint, ([1, 0],), ["a", "b"])

    t([10, 20], ["a", "b"], [[10, 20]], [[0]])
    t([[10, 20], [20, 40]], ["a", "b"], [[10, 20], [20, 40]], [[0], [0]])
    t(np.array([10, 20]), ["a", "b"], [[10, 20]], [[0]])
    t(np.array([[10, 20], [20, 40]]), ["a", "b"],
      [[10, 20], [20, 40]], [[0], [0]])

    # unknown object type
    assert_raises(ValueError, linear_constraint, None, ["a", "b"])

_parse_eval_error_tests = [
    # Bad token
    "a + <f>oo",
    # No pure constant equalities
    "a = 1, <1 = 1>, b = 1",
    "a = 1, <b * 2 - b + (-2/2 * b)>",
    "a = 1, <1>, b = 2",
    "a = 1, <2 * b = b + b>, c",
    # No non-linearities
    "a + <a * b> + c",
    "a + 2 / <b> + c",
    # Constraints are not numbers
    "a = 1, 2 * <(a = b)>, c",
    "a = 1, a + <(a = b)>, c",
    "a = 1, <(a, b)> + 2, c",
]

from patsy.parse_formula import _parsing_error_test
def test_eval_errors():
    def doit(bad_code):
        return linear_constraint(bad_code, ["a", "b", "c"])
    _parsing_error_test(doit, _parse_eval_error_tests)

########NEW FILE########
__FILENAME__ = contrasts
# This file is part of Patsy
# Copyright (C) 2011-2012 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm
# http://www.ats.ucla.edu/stat/sas/webbooks/reg/chapter5/sasreg5.htm

# These are made available in the patsy.* namespace
__all__ = ["ContrastMatrix", "Treatment", "Poly", "Sum", "Helmert", "Diff"]

import sys
import numpy as np
from patsy import PatsyError
from patsy.compat import triu_indices, tril_indices, diag_indices
from patsy.util import repr_pretty_delegate, repr_pretty_impl

class ContrastMatrix(object):
    """A simple container for a matrix used for coding categorical factors.

    Attributes:

    .. attribute:: matrix

       A 2d ndarray, where each column corresponds to one column of the
       resulting design matrix, and each row contains the entries for a single
       categorical variable level. Usually n-by-n for a full rank coding or
       n-by-(n-1) for a reduced rank coding, though other options are
       possible.

    .. attribute:: column_suffixes

       A list of strings to be appended to the factor name, to produce the
       final column names. E.g. for treatment coding the entries will look
       like ``"[T.level1]"``.
    """
    def __init__(self, matrix, column_suffixes):
        self.matrix = np.asarray(matrix)
        self.column_suffixes = column_suffixes
        if self.matrix.shape[1] != len(column_suffixes):
            raise PatsyError, "matrix and column_suffixes don't conform"

    __repr__ = repr_pretty_delegate
    def _repr_pretty_(self, p, cycle):
        repr_pretty_impl(p, self, [self.matrix, self.column_suffixes])

def test_ContrastMatrix():
    cm = ContrastMatrix([[1, 0], [0, 1]], ["a", "b"])
    assert np.array_equal(cm.matrix, np.eye(2))
    assert cm.column_suffixes == ["a", "b"]
    # smoke test
    repr(cm)

    from nose.tools import assert_raises
    assert_raises(PatsyError, ContrastMatrix, [[1], [0]], ["a", "b"])

# This always produces an object of the type that Python calls 'str' (whether
# that be a Python 2 string-of-bytes or a Python 3 string-of-unicode). It does
# *not* make any particular guarantees about being reversible or having other
# such useful programmatic properties -- it just produces something that will
# be nice for users to look at.
def _obj_to_readable_str(obj):
    if isinstance(obj, str):
        return obj
    elif sys.version_info >= (3,) and isinstance(obj, bytes):
        try:
            return obj.decode("utf-8")
        except UnicodeDecodeError:
            return repr(obj)
    elif sys.version_info < (3,) and isinstance(obj, unicode):
        try:
            return obj.encode("ascii")
        except UnicodeEncodeError:
            return repr(obj)
    else:
        return repr(obj)

def test__obj_to_readable_str():
    def t(obj, expected):
        got = _obj_to_readable_str(obj)
        assert type(got) is str
        assert got == expected
    t(1, "1")
    t(1.0, "1.0")
    t("asdf", "asdf")
    t(u"asdf", "asdf")
    if sys.version_info >= (3,):
        # a utf-8 encoded euro-sign comes out as a real euro sign. We have to
        # use u""-style strings here, even though this is a py3-only block,
        # because otherwise 2to3 may be clever enough to realize that in py2
        # "\u20ac" produces a literal \, and double it for us when
        # converting.
        t(u"\u20ac".encode("utf-8"), u"\u20ac")
        # but a iso-8859-15 euro sign can't be decoded, and we fall back on
        # repr()
        t(u"\u20ac".encode("iso-8859-15"), "b'\\xa4'")
    else:
        t(u"\u20ac", "u'\\u20ac'")

def _name_levels(prefix, levels):
    return ["[%s%s]" % (prefix, _obj_to_readable_str(level)) for level in levels]

def test__name_levels():
    assert _name_levels("a", ["b", "c"]) == ["[ab]", "[ac]"]

def _dummy_code(levels):
    return ContrastMatrix(np.eye(len(levels)), _name_levels("", levels))

def _get_level(levels, level_ref):
    if level_ref in levels:
        return levels.index(level_ref)
    if isinstance(level_ref, int):
        if level_ref < 0:
            level_ref += len(levels)
        if not (0 <= level_ref < len(levels)):
            raise PatsyError("specified level %r is out of range"
                                % (level_ref,))
        return level_ref
    raise PatsyError, "specified level %r not found" % (level_ref,)

def test__get_level():
    assert _get_level(["a", "b", "c"], 0) == 0
    assert _get_level(["a", "b", "c"], -1) == 2
    assert _get_level(["a", "b", "c"], "b") == 1
    # For integer levels, we check identity before treating it as an index
    assert _get_level([2, 1, 0], 0) == 2
    from nose.tools import assert_raises
    assert_raises(PatsyError, _get_level, ["a", "b"], 2)
    assert_raises(PatsyError, _get_level, ["a", "b"], -3)
    assert_raises(PatsyError, _get_level, ["a", "b"], "c")

class Treatment(object):
    """Treatment coding (also known as dummy coding).

    This is the default coding.

    For reduced-rank coding, one level is chosen as the "reference", and its
    mean behaviour is represented by the intercept. Each column of the
    resulting matrix represents the difference between the mean of one level
    and this reference level.

    For full-rank coding, classic "dummy" coding is used, and each column of
    the resulting matrix represents the mean of the corresponding level.

    The reference level defaults to the first level, or can be specified
    explicitly.

    .. ipython:: python

       # reduced rank
       dmatrix("C(a, Treatment)", balanced(a=3))
       # full rank
       dmatrix("0 + C(a, Treatment)", balanced(a=3))
       # Setting a reference level
       dmatrix("C(a, Treatment(1))", balanced(a=3))
       dmatrix("C(a, Treatment('a2'))", balanced(a=3))

    Equivalent to R ``contr.treatment``. The R documentation suggests that
    using ``Treatment(reference=-1)`` will produce contrasts that are
    "equivalent to those produced by many (but not all) SAS procedures".
    """
    def __init__(self, reference=None):
        self.reference = reference

    def code_with_intercept(self, levels):
        return _dummy_code(levels)

    def code_without_intercept(self, levels):
        if self.reference is None:
            reference = 0
        else:
            reference = _get_level(levels, self.reference)
        eye = np.eye(len(levels) - 1)
        contrasts = np.vstack((eye[:reference, :],
                                np.zeros((1, len(levels) - 1)),
                                eye[reference:, :]))
        names = _name_levels("T.", levels[:reference] + levels[reference + 1:])
        return ContrastMatrix(contrasts, names)

def test_Treatment():
    t1 = Treatment()
    matrix = t1.code_with_intercept(["a", "b", "c"])
    assert matrix.column_suffixes == ["[a]", "[b]", "[c]"]
    assert np.allclose(matrix.matrix, [[1, 0, 0], [0, 1, 0], [0, 0, 1]])
    matrix = t1.code_without_intercept(["a", "b", "c"])
    assert matrix.column_suffixes == ["[T.b]", "[T.c]"]
    assert np.allclose(matrix.matrix, [[0, 0], [1, 0], [0, 1]])
    matrix = Treatment(reference=1).code_without_intercept(["a", "b", "c"])
    assert matrix.column_suffixes == ["[T.a]", "[T.c]"]
    assert np.allclose(matrix.matrix, [[1, 0], [0, 0], [0, 1]])
    matrix = Treatment(reference=-2).code_without_intercept(["a", "b", "c"])
    assert matrix.column_suffixes == ["[T.a]", "[T.c]"]
    assert np.allclose(matrix.matrix, [[1, 0], [0, 0], [0, 1]])
    matrix = Treatment(reference="b").code_without_intercept(["a", "b", "c"])
    assert matrix.column_suffixes == ["[T.a]", "[T.c]"]
    assert np.allclose(matrix.matrix, [[1, 0], [0, 0], [0, 1]])
    # Make sure the default is always the first level, even if there is a
    # different level called 0.
    matrix = Treatment().code_without_intercept([2, 1, 0])
    assert matrix.column_suffixes == ["[T.1]", "[T.0]"]
    assert np.allclose(matrix.matrix, [[0, 0], [1, 0], [0, 1]])

class Poly(object):
    """Orthogonal polynomial contrast coding.

    This coding scheme treats the levels as ordered samples from an underlying
    continuous scale, whose effect takes an unknown functional form which is
    `Taylor-decomposed`__ into the sum of a linear, quadratic, etc. components.

    .. __: https://en.wikipedia.org/wiki/Taylor_series

    For reduced-rank coding, you get a linear column, a quadratic column,
    etc., up to the number of levels provided.

    For full-rank coding, the same scheme is used, except that the zero-order
    constant polynomial is also included. I.e., you get an intercept column
    included as part of your categorical term.

    By default the levels are treated as equally spaced, but you can override
    this by providing a value for the `scores` argument.

    Examples:

    .. ipython:: python

       # Reduced rank
       dmatrix("C(a, Poly)", balanced(a=4))
       # Full rank
       dmatrix("0 + C(a, Poly)", balanced(a=3))
       # Explicit scores
       dmatrix("C(a, Poly([1, 2, 10]))", balanced(a=3))

    This is equivalent to R's ``contr.poly``. (But note that in R, reduced
    rank encodings are always dummy-coded, regardless of what contrast you
    have set.)
    """
    def __init__(self, scores=None):
        self.scores = scores

    def _code_either(self, intercept, levels):
        n = len(levels)
        scores = self.scores
        if scores is None:
            scores = np.arange(n)
        scores = np.asarray(scores, dtype=float)
        if len(scores) != n:
            raise PatsyError("number of levels (%s) does not match"
                                " number of scores (%s)"
                                % (n, len(scores)))
        # Strategy: just make a matrix whose columns are naive linear,
        # quadratic, etc., functions of the raw scores, and then use 'qr' to
        # orthogonalize each column against those to its left.
        scores -= scores.mean()
        raw_poly = scores.reshape((-1, 1)) ** np.arange(n).reshape((1, -1))
        q, r = np.linalg.qr(raw_poly)
        q *= np.sign(np.diag(r))
        q /= np.sqrt(np.sum(q ** 2, axis=1))
        # The constant term is always all 1's -- we don't normalize it.
        q[:, 0] = 1
        names = [".Constant", ".Linear", ".Quadratic", ".Cubic"]
        names += ["^%s" % (i,) for i in xrange(4, n)]
        names = names[:n]
        if intercept:
            return ContrastMatrix(q, names)
        else:
            # We always include the constant/intercept column as something to
            # orthogonalize against, but we don't always return it:
            return ContrastMatrix(q[:, 1:], names[1:])

    def code_with_intercept(self, levels):
        return self._code_either(True, levels)

    def code_without_intercept(self, levels):
        return self._code_either(False, levels)

def test_Poly():
    t1 = Poly()
    matrix = t1.code_with_intercept(["a", "b", "c"])
    assert matrix.column_suffixes == [".Constant", ".Linear", ".Quadratic"]
    # Values from R 'options(digits=15); contr.poly(3)'
    expected = [[1, -7.07106781186548e-01, 0.408248290463863],
                [1, 0, -0.816496580927726],
                [1, 7.07106781186547e-01, 0.408248290463863]]
    print matrix.matrix
    assert np.allclose(matrix.matrix, expected)
    matrix = t1.code_without_intercept(["a", "b", "c"])
    assert matrix.column_suffixes == [".Linear", ".Quadratic"]
    # Values from R 'options(digits=15); contr.poly(3)'
    print matrix.matrix
    assert np.allclose(matrix.matrix,
                       [[-7.07106781186548e-01, 0.408248290463863],
                        [0, -0.816496580927726],
                        [7.07106781186547e-01, 0.408248290463863]])

    matrix = Poly(scores=[0, 10, 11]).code_with_intercept(["a", "b", "c"])
    assert matrix.column_suffixes == [".Constant", ".Linear", ".Quadratic"]
    # Values from R 'options(digits=15); contr.poly(3, scores=c(0, 10, 11))'
    print matrix.matrix
    assert np.allclose(matrix.matrix,
                       [[1, -0.813733471206735, 0.0671156055214024],
                        [1, 0.348742916231458, -0.7382716607354268],
                        [1, 0.464990554975277, 0.6711560552140243]])

    # we had an integer/float handling bug for score vectors whose mean was
    # non-integer, so check one of those:
    matrix = Poly(scores=[0, 10, 12]).code_with_intercept(["a", "b", "c"])
    assert matrix.column_suffixes == [".Constant", ".Linear", ".Quadratic"]
    # Values from R 'options(digits=15); contr.poly(3, scores=c(0, 10, 12))'
    print matrix.matrix
    assert np.allclose(matrix.matrix,
                       [[1, -0.806559132617443, 0.127000127000191],
                        [1, 0.293294230042706, -0.762000762001143],
                        [1, 0.513264902574736, 0.635000635000952]])

    matrix = t1.code_with_intercept(range(6))
    assert matrix.column_suffixes == [".Constant", ".Linear", ".Quadratic",
                                      ".Cubic", "^4", "^5"]


class Sum(object):
    """Deviation coding (also known as sum-to-zero coding).

    Compares the mean of each level to the mean-of-means. (In a balanced
    design, compares the mean of each level to the overall mean.)

    For full-rank coding, a standard intercept term is added.

    One level must be omitted to avoid redundancy; by default this is the last
    level, but this can be adjusted via the `omit` argument.

    .. warning:: There are multiple definitions of 'deviation coding' in
       use. Make sure this is the one you expect before trying to interpret
       your results!

    Examples:

    .. ipython:: python

       # Reduced rank
       dmatrix("C(a, Sum)", balanced(a=4))
       # Full rank
       dmatrix("0 + C(a, Sum)", balanced(a=4))
       # Omit a different level
       dmatrix("C(a, Sum(1))", balanced(a=3))
       dmatrix("C(a, Sum('a1'))", balanced(a=3))

    This is equivalent to R's `contr.sum`.
    """
    def __init__(self, omit=None):
        self.omit = omit

    def _omit_i(self, levels):
        if self.omit is None:
            # We assume below that this is positive
            return len(levels) - 1
        else:
            return _get_level(levels, self.omit)

    def _sum_contrast(self, levels):
        n = len(levels)
        omit_i = self._omit_i(levels)
        eye = np.eye(n - 1)
        out = np.empty((n, n - 1))
        out[:omit_i, :] = eye[:omit_i, :]
        out[omit_i, :] = -1
        out[omit_i + 1:, :] = eye[omit_i:, :]
        return out

    def code_with_intercept(self, levels):
        contrast = self.code_without_intercept(levels)
        matrix = np.column_stack((np.ones(len(levels)),
                                  contrast.matrix))
        column_suffixes = ["[mean]"] + contrast.column_suffixes
        return ContrastMatrix(matrix, column_suffixes)

    def code_without_intercept(self, levels):
        matrix = self._sum_contrast(levels)
        omit_i = self._omit_i(levels)
        included_levels = levels[:omit_i] + levels[omit_i + 1:]
        return ContrastMatrix(matrix, _name_levels("S.", included_levels))

def test_Sum():
    t1 = Sum()
    matrix = t1.code_with_intercept(["a", "b", "c"])
    assert matrix.column_suffixes == ["[mean]", "[S.a]", "[S.b]"]
    assert np.allclose(matrix.matrix, [[1, 1, 0], [1, 0, 1], [1, -1, -1]])
    matrix = t1.code_without_intercept(["a", "b", "c"])
    assert matrix.column_suffixes == ["[S.a]", "[S.b]"]
    assert np.allclose(matrix.matrix, [[1, 0], [0, 1], [-1, -1]])
    # Check that it's not thrown off by negative integer term names
    matrix = t1.code_without_intercept([-1, -2, -3])
    assert matrix.column_suffixes == ["[S.-1]", "[S.-2]"]
    assert np.allclose(matrix.matrix, [[1, 0], [0, 1], [-1, -1]])
    t2 = Sum(omit=1)
    matrix = t2.code_with_intercept(["a", "b", "c"])
    assert matrix.column_suffixes == ["[mean]", "[S.a]", "[S.c]"]
    assert np.allclose(matrix.matrix, [[1, 1, 0], [1, -1, -1], [1, 0, 1]])
    matrix = t2.code_without_intercept(["a", "b", "c"])
    assert matrix.column_suffixes == ["[S.a]", "[S.c]"]
    assert np.allclose(matrix.matrix, [[1, 0], [-1, -1], [0, 1]])
    matrix = t2.code_without_intercept([1, 0, 2])
    assert matrix.column_suffixes == ["[S.0]", "[S.2]"]
    assert np.allclose(matrix.matrix, [[-1, -1], [1, 0], [0, 1]])
    t3 = Sum(omit=-3)
    matrix = t3.code_with_intercept(["a", "b", "c"])
    assert matrix.column_suffixes == ["[mean]", "[S.b]", "[S.c]"]
    assert np.allclose(matrix.matrix, [[1, -1, -1], [1, 1, 0], [1, 0, 1]])
    matrix = t3.code_without_intercept(["a", "b", "c"])
    assert matrix.column_suffixes == ["[S.b]", "[S.c]"]
    assert np.allclose(matrix.matrix, [[-1, -1], [1, 0], [0, 1]])
    t4 = Sum(omit="a")
    matrix = t3.code_with_intercept(["a", "b", "c"])
    assert matrix.column_suffixes == ["[mean]", "[S.b]", "[S.c]"]
    assert np.allclose(matrix.matrix, [[1, -1, -1], [1, 1, 0], [1, 0, 1]])
    matrix = t3.code_without_intercept(["a", "b", "c"])
    assert matrix.column_suffixes == ["[S.b]", "[S.c]"]
    assert np.allclose(matrix.matrix, [[-1, -1], [1, 0], [0, 1]])

class Helmert(object):
    """Helmert contrasts.

    Compares the second level with the first, the third with the average of
    the first two, and so on.

    For full-rank coding, a standard intercept term is added.

    .. warning:: There are multiple definitions of 'Helmert coding' in
       use. Make sure this is the one you expect before trying to interpret
       your results!

    Examples:

    .. ipython:: python

       # Reduced rank
       dmatrix("C(a, Helmert)", balanced(a=4))
       # Full rank
       dmatrix("0 + C(a, Helmert)", balanced(a=4))

    This is equivalent to R's `contr.helmert`.
    """
    def _helmert_contrast(self, levels):
        n = len(levels)
        #http://www.ats.ucla.edu/stat/sas/webbooks/reg/chapter5/sasreg5.htm#HELMERT
        #contr = np.eye(n - 1)
        #int_range = np.arange(n - 1., 1, -1)
        #denom = np.repeat(int_range, np.arange(n - 2, 0, -1))
        #contr[np.tril_indices(n - 1, -1)] = -1. / denom

        #http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm#HELMERT
        #contr = np.zeros((n - 1., n - 1))
        #int_range = np.arange(n, 1, -1)
        #denom = np.repeat(int_range[:-1], np.arange(n - 2, 0, -1))
        #contr[np.diag_indices(n - 1)] = (int_range - 1.) / int_range
        #contr[np.tril_indices(n - 1, -1)] = -1. / denom
        #contr = np.vstack((contr, -1./int_range))

        #r-like
        contr = np.zeros((n, n - 1))
        contr[1:][diag_indices(n - 1)] = np.arange(1, n)
        contr[triu_indices(n - 1)] = -1
        return contr

    def code_with_intercept(self, levels):
        contrast = np.column_stack((np.ones(len(levels)),
                                    self._helmert_contrast(levels)))
        column_suffixes = _name_levels("H.", ["intercept"] + list(levels[1:]))
        return ContrastMatrix(contrast, column_suffixes)

    def code_without_intercept(self, levels):
        contrast = self._helmert_contrast(levels)
        return ContrastMatrix(contrast,
                              _name_levels("H.", levels[1:]))

def test_Helmert():
    t1 = Helmert()
    for levels in (["a", "b", "c", "d"], ("a", "b", "c", "d")):
        matrix = t1.code_with_intercept(levels)
        assert matrix.column_suffixes == ["[H.intercept]",
                                          "[H.b]",
                                          "[H.c]",
                                          "[H.d]"]
        assert np.allclose(matrix.matrix, [[1, -1, -1, -1],
                                           [1, 1, -1, -1],
                                           [1, 0, 2, -1],
                                           [1, 0, 0, 3]])
        matrix = t1.code_without_intercept(levels)
        assert matrix.column_suffixes == ["[H.b]", "[H.c]", "[H.d]"]
        assert np.allclose(matrix.matrix, [[-1, -1, -1],
                                           [1, -1, -1],
                                           [0, 2, -1],
                                           [0, 0, 3]])

class Diff(object):
    """Backward difference coding.

    This coding scheme is useful for ordered factors, and compares the mean of
    each level with the preceding level. So you get the second level minus the
    first, the third level minus the second, etc.

    For full-rank coding, a standard intercept term is added (which gives the
    mean value for the first level).

    Examples:

    .. ipython:: python

       # Reduced rank
       dmatrix("C(a, Diff)", balanced(a=3))
       # Full rank
       dmatrix("0 + C(a, Diff)", balanced(a=3))
    """
    def _diff_contrast(self, levels):
        nlevels = len(levels)
        contr = np.zeros((nlevels, nlevels-1))
        int_range = np.arange(1, nlevels)
        upper_int = np.repeat(int_range, int_range)
        row_i, col_i = triu_indices(nlevels-1)
        # we want to iterate down the columns not across the rows
        # it would be nice if the index functions had a row/col order arg
        col_order = np.argsort(col_i)
        contr[row_i[col_order],
              col_i[col_order]] = (upper_int-nlevels)/float(nlevels)
        lower_int = np.repeat(int_range, int_range[::-1])
        row_i, col_i = tril_indices(nlevels-1)
        # we want to iterate down the columns not across the rows
        col_order = np.argsort(col_i)
        contr[row_i[col_order]+1, col_i[col_order]] = lower_int/float(nlevels)
        return contr

    def code_with_intercept(self, levels):
        contrast = np.column_stack((np.ones(len(levels)),
                                    self._diff_contrast(levels)))
        return ContrastMatrix(contrast, _name_levels("D.", levels))

    def code_without_intercept(self, levels):
        contrast = self._diff_contrast(levels)
        return ContrastMatrix(contrast, _name_levels("D.", levels[:-1]))

def test_diff():
    t1 = Diff()
    matrix = t1.code_with_intercept(["a", "b", "c", "d"])
    assert matrix.column_suffixes == ["[D.a]", "[D.b]", "[D.c]",
                                      "[D.d]"]
    assert np.allclose(matrix.matrix, [[1, -3/4., -1/2., -1/4.],
                                        [1, 1/4., -1/2., -1/4.],
                                        [1, 1/4., 1./2, -1/4.],
                                        [1, 1/4., 1/2., 3/4.]])
    matrix = t1.code_without_intercept(["a", "b", "c", "d"])
    assert matrix.column_suffixes == ["[D.a]", "[D.b]", "[D.c]"]
    assert np.allclose(matrix.matrix, [[-3/4., -1/2., -1/4.],
                                        [1/4., -1/2., -1/4.],
                                        [1/4., 2./4, -1/4.],
                                        [1/4., 1/2., 3/4.]])

# contrast can be:
#   -- a ContrastMatrix
#   -- a simple np.ndarray
#   -- an object with code_with_intercept and code_without_intercept methods
#   -- a function returning one of the above
#   -- None, in which case the above rules are applied to 'default'
# This function always returns a ContrastMatrix.
def code_contrast_matrix(intercept, levels, contrast, default=None):
    if contrast is None:
        contrast = default
    if callable(contrast):
        contrast = contrast()
    if isinstance(contrast, ContrastMatrix):
        return contrast
    as_array = np.asarray(contrast)
    if np.issubdtype(as_array.dtype, np.number):
        return ContrastMatrix(as_array,
                              _name_levels("custom", range(as_array.shape[1])))
    if intercept:
        return contrast.code_with_intercept(levels)
    else:
        return contrast.code_without_intercept(levels)


########NEW FILE########
__FILENAME__ = desc
# This file is part of Patsy
# Copyright (C) 2011-2012 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# This file defines the ModelDesc class, which describes a model at a high
# level, as a list of interactions of factors. It also has the code to convert
# a formula parse tree (from patsy.parse_formula) into a ModelDesc.

from patsy import PatsyError
from patsy.parse_formula import ParseNode, Token, parse_formula
from patsy.eval import EvalEnvironment, EvalFactor
from patsy.util import uniqueify_list
from patsy.util import repr_pretty_delegate, repr_pretty_impl

# These are made available in the patsy.* namespace
__all__ = ["Term", "ModelDesc", "INTERCEPT"]

# One might think it would make more sense for 'factors' to be a set, rather
# than a tuple-with-guaranteed-unique-entries-that-compares-like-a-set. The
# reason we do it this way is that it preserves the order that the user typed
# and is expecting, which then ends up producing nicer names in our final
# output, nicer column ordering, etc. (A similar comment applies to the
# ordering of terms in ModelDesc objects as a whole.)
class Term(object):
    """The interaction between a collection of factor objects.

    This is one of the basic types used in representing formulas, and
    corresponds to an expression like ``"a:b:c"`` in a formula string.
    For details, see :ref:`formulas` and :ref:`expert-model-specification`.

    Terms are hashable and compare by value.

    Attributes:
    
    .. attribute:: factors

       A tuple of factor objects.
    """
    def __init__(self, factors):
        self.factors = tuple(uniqueify_list(factors))

    def __eq__(self, other):
        return (isinstance(other, Term)
                and frozenset(other.factors) == frozenset(self.factors))

    def __ne__(self, other):
        return not self == other

    def __hash__(self):
        return hash((Term, frozenset(self.factors)))

    __repr__ = repr_pretty_delegate
    def _repr_pretty_(self, p, cycle):
        assert not cycle
        repr_pretty_impl(p, self, [list(self.factors)])

    def name(self):
        """Return a human-readable name for this term."""
        if self.factors:
            return ":".join([f.name() for f in self.factors])
        else:
            return "Intercept"

INTERCEPT = Term([])

class _MockFactor(object):
    def __init__(self, name):
        self._name = name

    def name(self):
        return self._name

def test_Term():
    assert Term([1, 2, 1]).factors == (1, 2)
    assert Term([1, 2]) == Term([2, 1])
    assert hash(Term([1, 2])) == hash(Term([2, 1]))
    f1 = _MockFactor("a")
    f2 = _MockFactor("b")
    assert Term([f1, f2]).name() == "a:b"
    assert Term([f2, f1]).name() == "b:a"
    assert Term([]).name() == "Intercept"

_builtins_dict = {}
exec "from patsy.builtins import *" in {}, _builtins_dict
# This is purely to make the existence of patsy.builtins visible to systems
# like py2app and py2exe. It's basically free, since the above line guarantees
# that patsy.builtins will be present in sys.modules in any case.
import patsy.builtins

class ModelDesc(object):
    """A simple container representing the termlists parsed from a formula.

    This is a simple container object which has exactly the same
    representational power as a formula string, but is a Python object
    instead. You can construct one by hand, and pass it to functions like
    :func:`dmatrix` or :func:`incr_dbuilder` that are expecting a formula
    string, but without having to do any messy string manipulation. For
    details see :ref:`expert-model-specification`.

    Attributes:

    .. attribute:: lhs_termlist
                   rhs_termlist

       Two termlists representing the left- and right-hand sides of a
       formula, suitable for passing to :func:`design_matrix_builders`.
    """
    def __init__(self, lhs_termlist, rhs_termlist):
        self.lhs_termlist = uniqueify_list(lhs_termlist)
        self.rhs_termlist = uniqueify_list(rhs_termlist)

    __repr__ = repr_pretty_delegate
    def _repr_pretty_(self, p, cycle):
        assert not cycle
        return repr_pretty_impl(p, self,
                                [],
                                [("lhs_termlist", self.lhs_termlist),
                                 ("rhs_termlist", self.rhs_termlist)])

    def describe(self):
        """Returns a human-readable representation of this :class:`ModelDesc`
        in pseudo-formula notation.

        .. warning:: There is no guarantee that the strings returned by this
           function can be parsed as formulas. They are best-effort
           descriptions intended for human users. However, if this ModelDesc
           was created by parsing a formula, then it should work in
           practice. If you *really* have to.
        """
        def term_code(term):
            if term == INTERCEPT:
                return "1"
            else:
                return term.name()
        result = " + ".join([term_code(term) for term in self.lhs_termlist])
        if result:
            result += " ~ "
        else:
            result += "~ "
        if self.rhs_termlist == [INTERCEPT]:
            result += term_code(INTERCEPT)
        else:
            term_names = []
            if INTERCEPT not in self.rhs_termlist:
                term_names.append("0")
            term_names += [term_code(term) for term in self.rhs_termlist
                           if term != INTERCEPT]
            result += " + ".join(term_names)
        return result
            
    @classmethod
    def from_formula(cls, tree_or_string, factor_eval_env):
        """Construct a :class:`ModelDesc` from a formula string.

        :arg tree_or_string: A formula string. (Or an unevaluated formula
          parse tree, but the API for generating those isn't public yet. Shh,
          it can be our secret.)
        :arg factor_eval_env: A :class:`EvalEnvironment`, to be used for
          constructing :class:`EvalFactor` objects while parsing this
          formula.
        :returns: A new :class:`ModelDesc`.
        """
        if isinstance(tree_or_string, ParseNode):
            tree = tree_or_string
        else:
            tree = parse_formula(tree_or_string)
        factor_eval_env.add_outer_namespace(_builtins_dict)
        value = Evaluator(factor_eval_env).eval(tree, require_evalexpr=False)
        assert isinstance(value, cls)
        return value

def test_ModelDesc():
    f1 = _MockFactor("a")
    f2 = _MockFactor("b")
    m = ModelDesc([INTERCEPT, Term([f1])], [Term([f1]), Term([f1, f2])])
    assert m.lhs_termlist == [INTERCEPT, Term([f1])]
    assert m.rhs_termlist == [Term([f1]), Term([f1, f2])]
    print m.describe()
    assert m.describe() == "1 + a ~ 0 + a + a:b"

    assert ModelDesc([], []).describe() == "~ 0"
    assert ModelDesc([INTERCEPT], []).describe() == "1 ~ 0"
    assert ModelDesc([INTERCEPT], [INTERCEPT]).describe() == "1 ~ 1"
    assert (ModelDesc([INTERCEPT], [INTERCEPT, Term([f2])]).describe()
            == "1 ~ b")

def test_ModelDesc_from_formula():
    for input in ("y ~ x", parse_formula("y ~ x")):
        eval_env = EvalEnvironment.capture(0)
        md = ModelDesc.from_formula(input, eval_env)
        assert md.lhs_termlist == [Term([EvalFactor("y", eval_env)]),]
        assert md.rhs_termlist == [INTERCEPT, Term([EvalFactor("x", eval_env)])]

class IntermediateExpr(object):
    "This class holds an intermediate result while we're evaluating a tree."
    def __init__(self, intercept, intercept_origin, intercept_removed, terms):
        self.intercept = intercept
        self.intercept_origin = intercept_origin
        self.intercept_removed =intercept_removed
        self.terms = tuple(uniqueify_list(terms))
        if self.intercept:
            assert self.intercept_origin
        assert not (self.intercept and self.intercept_removed)

    __repr__ = repr_pretty_delegate
    def _pretty_repr_(self, p, cycle): # pragma: no cover
        assert not cycle
        return repr_pretty_impl(p, self,
                                [self.intercept, self.intercept_origin,
                                 self.intercept_removed, self.terms])

def _maybe_add_intercept(doit, terms):
    if doit:
        return (INTERCEPT,) + terms
    else:
        return terms

def _eval_any_tilde(evaluator, tree):
    exprs = [evaluator.eval(arg) for arg in tree.args]    
    if len(exprs) == 1:
        # Formula was like: "~ foo"
        # We pretend that instead it was like: "0 ~ foo"
        exprs.insert(0, IntermediateExpr(False, None, True, []))
    assert len(exprs) == 2
    # Note that only the RHS gets an implicit intercept:
    return ModelDesc(_maybe_add_intercept(exprs[0].intercept, exprs[0].terms),
                     _maybe_add_intercept(not exprs[1].intercept_removed,
                                          exprs[1].terms))

def _eval_binary_plus(evaluator, tree):
    left_expr = evaluator.eval(tree.args[0])
    if tree.args[1].type == "ZERO":
        return IntermediateExpr(False, None, True, left_expr.terms)
    else:
        right_expr = evaluator.eval(tree.args[1])
        if right_expr.intercept:
            return IntermediateExpr(True, right_expr.intercept_origin, False,
                                    left_expr.terms + right_expr.terms)
        else:
            return IntermediateExpr(left_expr.intercept,
                                    left_expr.intercept_origin,
                                    left_expr.intercept_removed,
                                    left_expr.terms + right_expr.terms)
    

def _eval_binary_minus(evaluator, tree):
    left_expr = evaluator.eval(tree.args[0])
    if tree.args[1].type == "ZERO":
        return IntermediateExpr(True, tree.args[1], False,
                                left_expr.terms)
    elif tree.args[1].type == "ONE":
        return IntermediateExpr(False, None, True, left_expr.terms)
    else:
        right_expr = evaluator.eval(tree.args[1])
        terms = [term for term in left_expr.terms
                 if term not in right_expr.terms]
        if right_expr.intercept:
            return IntermediateExpr(False, None, True, terms)
        else:
            return IntermediateExpr(left_expr.intercept,
                                    left_expr.intercept_origin,
                                    left_expr.intercept_removed,
                                    terms)

def _check_interactable(expr):
    if expr.intercept:
        raise PatsyError("intercept term cannot interact with "
                            "anything else", expr.intercept_origin)

def _interaction(left_expr, right_expr):
    for expr in (left_expr, right_expr):
        _check_interactable(expr)
    terms = []
    for l_term in left_expr.terms:
        for r_term in right_expr.terms:
            terms.append(Term(l_term.factors + r_term.factors))
    return IntermediateExpr(False, None, False, terms)

def _eval_binary_prod(evaluator, tree):
    exprs = [evaluator.eval(arg) for arg in tree.args]
    return IntermediateExpr(False, None, False,
                            exprs[0].terms
                            + exprs[1].terms
                            + _interaction(*exprs).terms)

# Division (nesting) is right-ward distributive:
#   a / (b + c) -> a/b + a/c -> a + a:b + a:c
# But left-ward, in S/R it has a quirky behavior:
#   (a + b)/c -> a + b + a:b:c
# This is because it's meaningless for a factor to be "nested" under two
# different factors. (This is documented in Chambers and Hastie (page 30) as a
# "Slightly more subtle..." rule, with no further elaboration. Hopefully we
# will do better.)
def _eval_binary_div(evaluator, tree):
    left_expr = evaluator.eval(tree.args[0])
    right_expr = evaluator.eval(tree.args[1])
    terms = list(left_expr.terms)
    _check_interactable(left_expr)
    # Build a single giant combined term for everything on the left:
    left_factors = []
    for term in left_expr.terms:
        left_factors += list(term.factors)
    left_combined_expr = IntermediateExpr(False, None, False,
                                          [Term(left_factors)])
    # Then interact it with everything on the right:
    terms += list(_interaction(left_combined_expr, right_expr).terms)
    return IntermediateExpr(False, None, False, terms)

def _eval_binary_interact(evaluator, tree):
    exprs = [evaluator.eval(arg) for arg in tree.args]
    return _interaction(*exprs)

def _eval_binary_power(evaluator, tree):
    left_expr = evaluator.eval(tree.args[0])
    _check_interactable(left_expr)
    power = -1
    if tree.args[1].type in ("ONE", "NUMBER"):
        expr = tree.args[1].token.extra
        try:
            power = int(expr)
        except ValueError:
            pass
    if power < 1:
        raise PatsyError("'**' requires a positive integer", tree.args[1])
    all_terms = left_expr.terms
    big_expr = left_expr
    # Small optimization: (a + b)**100 is just the same as (a + b)**2.
    power = min(len(left_expr.terms), power)
    for i in xrange(1, power):
        big_expr = _interaction(left_expr, big_expr)
        all_terms = all_terms + big_expr.terms
    return IntermediateExpr(False, None, False, all_terms)

def _eval_unary_plus(evaluator, tree):
    return evaluator.eval(tree.args[0])

def _eval_unary_minus(evaluator, tree):
    if tree.args[0].type == "ZERO":
        return IntermediateExpr(True, tree.origin, False, [])
    elif tree.args[0].type == "ONE":
        return IntermediateExpr(False, None, True, [])
    else:
        raise PatsyError("Unary minus can only be applied to 1 or 0", tree)

def _eval_zero(evaluator, tree):
    return IntermediateExpr(False, None, True, [])
    
def _eval_one(evaluator, tree):
    return IntermediateExpr(True, tree.origin, False, [])

def _eval_number(evaluator, tree):
    raise PatsyError("numbers besides '0' and '1' are "
                        "only allowed with **", tree)

def _eval_python_expr(evaluator, tree):
    factor = EvalFactor(tree.token.extra, evaluator._factor_eval_env,
                        origin=tree.origin)
    return IntermediateExpr(False, None, False, [Term([factor])])

class Evaluator(object):
    def __init__(self, factor_eval_env):
        self._evaluators = {}
        self._factor_eval_env = factor_eval_env
        self.add_op("~", 2, _eval_any_tilde)
        self.add_op("~", 1, _eval_any_tilde)

        self.add_op("+", 2, _eval_binary_plus)
        self.add_op("-", 2, _eval_binary_minus)
        self.add_op("*", 2, _eval_binary_prod)
        self.add_op("/", 2, _eval_binary_div)
        self.add_op(":", 2, _eval_binary_interact)
        self.add_op("**", 2, _eval_binary_power)

        self.add_op("+", 1, _eval_unary_plus)
        self.add_op("-", 1, _eval_unary_minus)

        self.add_op("ZERO", 0, _eval_zero)
        self.add_op("ONE", 0, _eval_one)
        self.add_op("NUMBER", 0, _eval_number)
        self.add_op("PYTHON_EXPR", 0, _eval_python_expr)

        # Not used by Patsy -- provided for the convenience of eventual
        # user-defined operators.
        self.stash = {}

    # This should not be considered a public API yet (to use for actually
    # adding new operator semantics) because I wrote in some of the relevant
    # code sort of speculatively, but it isn't actually tested.
    def add_op(self, op, arity, evaluator):
        self._evaluators[op, arity] = evaluator

    def eval(self, tree, require_evalexpr=True):
        result = None
        assert isinstance(tree, ParseNode)
        key = (tree.type, len(tree.args))
        if key not in self._evaluators:
            raise PatsyError("I don't know how to evaluate this "
                                "'%s' operator" % (tree.type,),
                                tree.token)
        result = self._evaluators[key](self, tree)
        if require_evalexpr and not isinstance(result, IntermediateExpr):
            if isinstance(result, ModelDesc):
                raise PatsyError("~ can only be used once, and "
                                    "only at the top level",
                                    tree)
            else:
                raise PatsyError("custom operator returned an "
                                    "object that I don't know how to "
                                    "handle", tree)
        return result

#############

_eval_tests = {
    "": (True, []),
    " ": (True, []),
    " \n ": (True, []),
    "a": (True, ["a"]),

    "1": (True, []),
    "0": (False, []),
    "- 1": (False, []),
    "- 0": (True, []),
    "+ 1": (True, []),
    "+ 0": (False, []),
    "0 + 1": (True, []),
    "1 + 0": (False, []),
    "1 - 0": (True, []),
    "0 - 1": (False, []),
    
    "1 + a": (True, ["a"]),
    "0 + a": (False, ["a"]),
    "a - 1": (False, ["a"]),
    "a - 0": (True, ["a"]),
    "1 - a": (True, []),

    "a + b": (True, ["a", "b"]),
    "(a + b)": (True, ["a", "b"]),
    "a + ((((b))))": (True, ["a", "b"]),
    "a + ((((+b))))": (True, ["a", "b"]),
    "a + ((((b - a))))": (True, ["a", "b"]),

    "a + a + a": (True, ["a"]),

    "a + (b - a)": (True, ["a", "b"]),

    "a + np.log(a, base=10)": (True, ["a", "np.log(a, base=10)"]),
    # Note different spacing:
    "a + np.log(a, base=10) - np . log(a , base = 10)": (True, ["a"]),
    
    "a + (I(b) + c)": (True, ["a", "I(b)", "c"]),
    "a + I(b + c)": (True, ["a", "I(b + c)"]),

    "a:b": (True, [("a", "b")]),
    "a:b:a": (True, [("a", "b")]),
    "a:(b + c)": (True, [("a", "b"), ("a", "c")]),
    "(a + b):c": (True, [("a", "c"), ("b", "c")]),
    "a:(b - c)": (True, [("a", "b")]),
    "c + a:c + a:(b - c)": (True, ["c", ("a", "c"), ("a", "b")]),
    "(a - b):c": (True, [("a", "c")]),
    "b + b:c + (a - b):c": (True, ["b", ("b", "c"), ("a", "c")]),

    "a:b - a:b": (True, []),
    "a:b - b:a": (True, []),

    "1 - (a + b)": (True, []),
    "a + b - (a + b)": (True, []),

    "a * b": (True, ["a", "b", ("a", "b")]),
    "a * b * a": (True, ["a", "b", ("a", "b")]),
    "a * (b + c)": (True, ["a", "b", "c", ("a", "b"), ("a", "c")]),
    "(a + b) * c": (True, ["a", "b", "c", ("a", "c"), ("b", "c")]),
    "a * (b - c)": (True, ["a", "b", ("a", "b")]),
    "c + a:c + a * (b - c)": (True, ["c", ("a", "c"), "a", "b", ("a", "b")]),
    "(a - b) * c": (True, ["a", "c", ("a", "c")]),
    "b + b:c + (a - b) * c": (True, ["b", ("b", "c"), "a", "c", ("a", "c")]),

    "a/b": (True, ["a", ("a", "b")]),
    "(a + b)/c": (True, ["a", "b", ("a", "b", "c")]),
    "b + b:c + (a - b)/c": (True, ["b", ("b", "c"), "a", ("a", "c")]),
    "a/(b + c)": (True, ["a", ("a", "b"), ("a", "c")]),

    "a ** 2": (True, ["a"]),
    "(a + b + c + d) ** 2": (True, ["a", "b", "c", "d",
                                    ("a", "b"), ("a", "c"), ("a", "d"),
                                    ("b", "c"), ("b", "d"), ("c", "d")]),
    "(a + b + c + d) ** 3": (True, ["a", "b", "c", "d",
                                    ("a", "b"), ("a", "c"), ("a", "d"),
                                    ("b", "c"), ("b", "d"), ("c", "d"),
                                    ("a", "b", "c"), ("a", "b", "d"),
                                    ("a", "c", "d"), ("b", "c", "d")]),

    "a + +a": (True, ["a"]),

    "~ a + b": (True, ["a", "b"]),
    "~ a*b": (True, ["a", "b", ("a", "b")]),
    "~ a*b + 0": (False, ["a", "b", ("a", "b")]),
    "~ -1": (False, []),

    "0 ~ a + b": (True, ["a", "b"]),
    "1 ~ a + b": (True, [], True, ["a", "b"]),
    "y ~ a + b": (False, ["y"], True, ["a", "b"]),
    "0 + y ~ a + b": (False, ["y"], True, ["a", "b"]),
    "0 + y * z ~ a + b": (False, ["y", "z", ("y", "z")], True, ["a", "b"]),
    "-1 ~ 1": (False, [], True, []),
    "1 + y ~ a + b": (True, ["y"], True, ["a", "b"]),

    # Check precedence:
    "a + b * c": (True, ["a", "b", "c", ("b", "c")]),
    "a * b + c": (True, ["a", "b", ("a", "b"), "c"]),
    "a * b - a": (True, ["b", ("a", "b")]),
    "a + b / c": (True, ["a", "b", ("b", "c")]),
    "a / b + c": (True, ["a", ("a", "b"), "c"]),
    "a*b:c": (True, ["a", ("b", "c"), ("a", "b", "c")]),
    "a:b*c": (True, [("a", "b"), "c", ("a", "b", "c")]),

    # Intercept handling:
    "~ 1 + 1 + 0 + 1": (True, []),
    "~ 0 + 1 + 0": (False, []),
    "~ 0 - 1 - 1 + 0 + 1": (True, []),
    "~ 1 - 1": (False, []),
    "~ 0 + a + 1": (True, ["a"]),
    "~ 1 + (a + 0)": (True, ["a"]), # This is correct, but perhaps surprising!
    "~ 0 + (a + 1)": (True, ["a"]), # Also correct!
    "~ 1 - (a + 1)": (False, []),
}

# <> mark off where the error should be reported:
_eval_error_tests = [
    "a <+>",
    "a + <(>",

    "b + <(-a)>",

    "a:<1>",
    "(a + <1>)*b",

    "a + <2>",
    "a + <1.0>",
    # eh, catching this is a hassle, we'll just leave the user some rope if
    # they really want it:
    #"a + <0x1>",

    "a ** <b>",
    "a ** <(1 + 1)>",
    "a ** <1.5>",

    "a + b <# asdf>",

    "<)>",
    "a + <)>",
    "<*> a",
    "a + <*>",

    "a + <foo[bar>",
    "a + <foo{bar>",
    "a + <foo(bar>",

    "a + <[bar>",
    "a + <{bar>",

    "a + <{bar[]>",

    "a + foo<]>bar",
    "a + foo[]<]>bar",
    "a + foo{}<}>bar",
    "a + foo<)>bar",

    "a + b<)>",
    "(a) <.>",

    "<(>a + b",

    "<y ~ a> ~ b",
    "y ~ <(a ~ b)>",
    "<~ a> ~ b",
    "~ <(a ~ b)>",

    "1 + <-(a + b)>",

    "<- a>",
    "a + <-a**2>",
]

def _assert_terms_match(terms, expected_intercept, expecteds, eval_env): # pragma: no cover
    if expected_intercept:
        expecteds = [()] + expecteds
    assert len(terms) == len(expecteds)
    for term, expected in zip(terms, expecteds):
        if isinstance(term, Term):
            if isinstance(expected, str):
                expected = (expected,)
            assert term.factors == tuple([EvalFactor(s, eval_env)
                                          for s in expected])
        else:
            assert term == expected

def _do_eval_formula_tests(tests): # pragma: no cover
    for code, result in tests.iteritems():
        if len(result) == 2:
            result = (False, []) + result
        eval_env = EvalEnvironment.capture(0)
        model_desc = ModelDesc.from_formula(code, eval_env)
        print repr(code)
        print result
        print model_desc
        lhs_intercept, lhs_termlist, rhs_intercept, rhs_termlist = result
        _assert_terms_match(model_desc.lhs_termlist,
                            lhs_intercept, lhs_termlist,
                            eval_env)
        _assert_terms_match(model_desc.rhs_termlist,
                            rhs_intercept, rhs_termlist,
                            eval_env)

def test_eval_formula():
    _do_eval_formula_tests(_eval_tests)

def test_eval_formula_error_reporting():
    from patsy.parse_formula import _parsing_error_test
    parse_fn = lambda formula: ModelDesc.from_formula(formula,
                                                      EvalEnvironment.capture(0))
    _parsing_error_test(parse_fn, _eval_error_tests)

def test_formula_factor_origin():
    from patsy.origin import Origin
    desc = ModelDesc.from_formula("a + b", EvalEnvironment.capture(0))
    assert (desc.rhs_termlist[1].factors[0].origin
            == Origin("a + b", 0, 1))
    assert (desc.rhs_termlist[2].factors[0].origin
            == Origin("a + b", 4, 5))
    

########NEW FILE########
__FILENAME__ = design_info
# This file is part of Patsy
# Copyright (C) 2011-2012 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# This file defines the main class for storing metadata about a model
# design. It also defines a 'value-added' design matrix type -- a subclass of
# ndarray that represents a design matrix and holds metadata about its
# columns.  The intent is that these are useful and usable data structures
# even if you're not using *any* of the rest of patsy to actually build
# your matrices.

# These are made available in the patsy.* namespace
__all__ = ["DesignInfo", "DesignMatrix"]

import numpy as np
from patsy import PatsyError
from patsy.util import atleast_2d_column_default
from patsy.compat import OrderedDict
from patsy.util import repr_pretty_delegate, repr_pretty_impl
from patsy.constraint import linear_constraint

class DesignInfo(object):
    """A DesignInfo object holds metadata about a design matrix.

    This is the main object that Patsy uses to pass information to
    statistical libraries. Usually encountered as the `.design_info` attribute
    on design matrices.
    """
    def __init__(self, column_names,
                 term_slices=None, term_name_slices=None,
                 builder=None):
        self.column_name_indexes = OrderedDict(zip(column_names,
                                                   range(len(column_names))))
        if term_slices is not None:
            #: An OrderedDict mapping :class:`Term` objects to Python
            #: func:`slice` objects. May be None, for design matrices which
            #: were constructed directly rather than by using the patsy
            #: machinery. If it is not None, then it
            #: is guaranteed to list the terms in order, and the slices are
            #: guaranteed to exactly cover all columns with no overlap or
            #: gaps.
            self.term_slices = OrderedDict(term_slices)
            if term_name_slices is not None:
                raise ValueError("specify only one of term_slices and "
                                 "term_name_slices")
            term_names = [term.name() for term in self.term_slices]
            #: And OrderedDict mapping term names (as strings) to Python
            #: :func:`slice` objects. Guaranteed never to be None. Guaranteed
            #: to list the terms in order, and the slices are
            #: guaranteed to exactly cover all columns with no overlap or
            #: gaps. Name overlap is allowed between term names and column
            #: names, but it is guaranteed that if it occurs, then they refer
            #: to exactly the same column.
            self.term_name_slices = OrderedDict(zip(term_names,
                                                    self.term_slices.values()))
        else: # term_slices is None
            self.term_slices = None
            if term_name_slices is None:
                # Make up one term per column
                term_names = column_names
                slices = [slice(i, i + 1) for i in xrange(len(column_names))]
                term_name_slices = zip(term_names, slices)
            self.term_name_slices = OrderedDict(term_name_slices)

        self.builder = builder

        # Guarantees:
        #   term_name_slices is never None
        #   The slices in term_name_slices are in order and exactly cover the
        #     whole range of columns.
        #   term_slices may be None
        #   If term_slices is not None, then its slices match the ones in
        #     term_name_slices.
        #   If there is any name overlap between terms and columns, they refer
        #     to the same columns.
        assert self.term_name_slices is not None
        if self.term_slices is not None:
            assert self.term_slices.values() == self.term_name_slices.values()
        covered = 0
        for slice_ in self.term_name_slices.itervalues():
            start, stop, step = slice_.indices(len(column_names))
            if start != covered:
                raise ValueError, "bad term slices"
            if step != 1:
                raise ValueError, "bad term slices"
            covered = stop
        if covered != len(column_names):
            raise ValueError, "bad term indices"
        for column_name, index in self.column_name_indexes.iteritems():
            if column_name in self.term_name_slices:
                slice_ = self.term_name_slices[column_name]
                if slice_ != slice(index, index + 1):
                    raise ValueError, "term/column name collision"

    __repr__ = repr_pretty_delegate
    def _repr_pretty_(self, p, cycle):
        assert not cycle
        if self.term_slices is None:
            kwargs = [("term_name_slices", self.term_name_slices)]
        else:
            kwargs = [("term_slices", self.term_slices)]
        if self.builder is not None:
            kwargs.append(("builder", self.builder))
        repr_pretty_impl(p, self, [self.column_names], kwargs)

    @property
    def column_names(self):
        "A list of the column names, in order."
        return self.column_name_indexes.keys()

    @property
    def terms(self):
        "A list of :class:`Terms`, in order, or else None."
        if self.term_slices is None:
            return None
        return self.term_slices.keys()

    @property
    def term_names(self):
        "A list of terms, in order."
        return self.term_name_slices.keys()

    def slice(self, columns_specifier):
        """Locate a subset of design matrix columns, specified symbolically.

        A patsy design matrix has two levels of structure: the individual
        columns (which are named), and the :ref:`terms <formulas>` in
        the formula that generated those columns. This is a one-to-many
        relationship: a single term may span several columns. This method
        provides a user-friendly API for locating those columns.

        (While we talk about columns here, this is probably most useful for
        indexing into other arrays that are derived from the design matrix,
        such as regression coefficients or covariance matrices.)

        The `columns_specifier` argument can take a number of forms:

        * A term name
        * A column name
        * A :class:`Term` object
        * An integer giving a raw index
        * A raw slice object

        In all cases, a Python :func:`slice` object is returned, which can be
        used directly for indexing.

        Example::

          y, X = dmatrices("y ~ a", demo_data("y", "a", nlevels=3))
          betas = np.linalg.lstsq(X, y)[0]
          a_betas = betas[X.design_info.slice("a")]

        (If you want to look up a single individual column by name, use
        ``design_info.column_name_indexes[name]``.)
        """
        if isinstance(columns_specifier, slice):
            return columns_specifier
        if np.issubsctype(type(columns_specifier), np.integer):
            return slice(columns_specifier, columns_specifier + 1)
        if (self.term_slices is not None
            and columns_specifier in self.term_slices):
            return self.term_slices[columns_specifier]
        if columns_specifier in self.term_name_slices:
            return self.term_name_slices[columns_specifier]
        if columns_specifier in self.column_name_indexes:
            idx = self.column_name_indexes[columns_specifier]
            return slice(idx, idx + 1)
        raise PatsyError("unknown column specified '%s'"
                            % (columns_specifier,))

    def linear_constraint(self, constraint_likes):
        """Construct a linear constraint in matrix form from a (possibly
        symbolic) description.

        Possible inputs:

        * A dictionary which is taken as a set of equality constraint. Keys
          can be either string column names, or integer column indexes.
        * A string giving a arithmetic expression referring to the matrix
          columns by name.
        * A list of such strings which are ANDed together.
        * A tuple (A, b) where A and b are array_likes, and the constraint is
          Ax = b. If necessary, these will be coerced to the proper
          dimensionality by appending dimensions with size 1.

        The string-based language has the standard arithmetic operators, / * +
        - and parentheses, plus "=" is used for equality and "," is used to
        AND together multiple constraint equations within a string. You can
        If no = appears in some expression, then that expression is assumed to
        be equal to zero. Division is always float-based, even if
        ``__future__.true_division`` isn't in effect.

        Returns a :class:`LinearConstraint` object.

        Examples::

          di = DesignInfo(["x1", "x2", "x3"])

          # Equivalent ways to write x1 == 0:
          di.linear_constraint({"x1": 0})  # by name
          di.linear_constraint({0: 0})  # by index
          di.linear_constraint("x1 = 0")  # string based
          di.linear_constraint("x1")  # can leave out "= 0"
          di.linear_constraint("2 * x1 = (x1 + 2 * x1) / 3")
          di.linear_constraint(([1, 0, 0], 0))  # constraint matrices

          # Equivalent ways to write x1 == 0 and x3 == 10
          di.linear_constraint({"x1": 0, "x3": 10})
          di.linear_constraint({0: 0, 2: 10})
          di.linear_constraint({0: 0, "x3": 10})
          di.linear_constraint("x1 = 0, x3 = 10")
          di.linear_constraint("x1, x3 = 10")
          di.linear_constraint(["x1", "x3 = 0"])  # list of strings
          di.linear_constraint("x1 = 0, x3 - 10 = x1")
          di.linear_constraint([[1, 0, 0], [0, 0, 1]], [0, 10])

          # You can also chain together equalities, just like Python:
          di.linear_constraint("x1 = x2 = 3")
        """
        return linear_constraint(constraint_likes, self.column_names)

    def describe(self):
        """Returns a human-readable string describing this design info.

        Example:

        .. ipython::

          In [1]: y, X = dmatrices("y ~ x1 + x2", demo_data("y", "x1", "x2"))

          In [2]: y.design_info.describe()
          Out[2]: 'y'

          In [3]: X.design_info.describe()
          Out[3]: '1 + x1 + x2'

        .. warning::

           There is no guarantee that the strings returned by this
           function can be parsed as formulas. They are best-effort descriptions
           intended for human users.
        """

        names = []
        for name in self.term_names:
            if name == "Intercept":
                names.append("1")
            else:
                names.append(name)
        return " + ".join(names)

    @classmethod
    def from_array(cls, array_like, default_column_prefix="column"):
        """Find or construct a DesignInfo appropriate for a given array_like.

        If the input `array_like` already has a ``.design_info``
        attribute, then it will be returned. Otherwise, a new DesignInfo
        object will be constructed, using names either taken from the
        `array_like` (e.g., for a pandas DataFrame with named columns), or
        constructed using `default_column_prefix`.

        This is how :func:`dmatrix` (for example) creates a DesignInfo object
        if an arbitrary matrix is passed in.

        :arg array_like: An ndarray or pandas container.
        :arg default_column_prefix: If it's necessary to invent column names,
          then this will be used to construct them.
        :returns: a DesignInfo object
        """
        if hasattr(array_like, "design_info") and isinstance(array_like.design_info, cls):
            return array_like.design_info
        arr = atleast_2d_column_default(array_like, preserve_pandas=True)
        if arr.ndim > 2:
            raise ValueError, "design matrix can't have >2 dimensions"
        columns = getattr(arr, "columns", xrange(arr.shape[1]))
        if (isinstance(columns, np.ndarray)
            and not np.issubdtype(columns.dtype, np.integer)):
            column_names = [str(obj) for obj in columns]
        else:
            column_names = ["%s%s" % (default_column_prefix, i)
                            for i in columns]
        return DesignInfo(column_names)

def test_DesignInfo():
    from nose.tools import assert_raises
    class _MockTerm(object):
        def __init__(self, name):
            self._name = name

        def name(self):
            return self._name
    t_a = _MockTerm("a")
    t_b = _MockTerm("b")
    di = DesignInfo(["a1", "a2", "a3", "b"],
                    [(t_a, slice(0, 3)), (t_b, slice(3, 4))],
                    builder="asdf")
    assert di.column_names == ["a1", "a2", "a3", "b"]
    assert di.term_names == ["a", "b"]
    assert di.terms == [t_a, t_b]
    assert di.column_name_indexes == {"a1": 0, "a2": 1, "a3": 2, "b": 3}
    assert di.term_name_slices == {"a": slice(0, 3), "b": slice(3, 4)}
    assert di.term_slices == {t_a: slice(0, 3), t_b: slice(3, 4)}
    assert di.describe() == "a + b"
    assert di.builder == "asdf"

    assert di.slice(1) == slice(1, 2)
    assert di.slice("a1") == slice(0, 1)
    assert di.slice("a2") == slice(1, 2)
    assert di.slice("a3") == slice(2, 3)
    assert di.slice("a") == slice(0, 3)
    assert di.slice(t_a) == slice(0, 3)
    assert di.slice("b") == slice(3, 4)
    assert di.slice(t_b) == slice(3, 4)
    assert di.slice(slice(2, 4)) == slice(2, 4)
    assert_raises(PatsyError, di.slice, "asdf")

    # smoke test
    repr(di)

    # One without term objects
    di = DesignInfo(["a1", "a2", "a3", "b"],
                    term_name_slices=[("a", slice(0, 3)),
                                      ("b", slice(3, 4))])
    assert di.column_names == ["a1", "a2", "a3", "b"]
    assert di.term_names == ["a", "b"]
    assert di.terms is None
    assert di.column_name_indexes == {"a1": 0, "a2": 1, "a3": 2, "b": 3}
    assert di.term_name_slices == {"a": slice(0, 3), "b": slice(3, 4)}
    assert di.term_slices is None
    assert di.describe() == "a + b"

    assert di.slice(1) == slice(1, 2)
    assert di.slice("a") == slice(0, 3)
    assert di.slice("a1") == slice(0, 1)
    assert di.slice("a2") == slice(1, 2)
    assert di.slice("a3") == slice(2, 3)
    assert di.slice("b") == slice(3, 4)

    # smoke test
    repr(di)

    # One without term objects *or* names
    di = DesignInfo(["a1", "a2", "a3", "b"])
    assert di.column_names == ["a1", "a2", "a3", "b"]
    assert di.term_names == ["a1", "a2", "a3", "b"]
    assert di.terms is None
    assert di.column_name_indexes == {"a1": 0, "a2": 1, "a3": 2, "b": 3}
    assert di.term_name_slices == {"a1": slice(0, 1),
                                   "a2": slice(1, 2),
                                   "a3": slice(2, 3),
                                   "b": slice(3, 4)}
    assert di.term_slices is None
    assert di.describe() == "a1 + a2 + a3 + b"

    assert di.slice(1) == slice(1, 2)
    assert di.slice("a1") == slice(0, 1)
    assert di.slice("a2") == slice(1, 2)
    assert di.slice("a3") == slice(2, 3)
    assert di.slice("b") == slice(3, 4)

    # Check intercept handling in describe()
    assert DesignInfo(["Intercept", "a", "b"]).describe() == "1 + a + b"

    # Can't specify both term_slices and term_name_slices
    assert_raises(ValueError,
                  DesignInfo,
                  ["a1", "a2"],
                  term_slices=[(t_a, slice(0, 2))],
                  term_name_slices=[("a", slice(0, 2))])
    # out-of-order slices are bad
    assert_raises(ValueError, DesignInfo, ["a1", "a2", "a3", "a4"],
                  term_slices=[(t_a, slice(3, 4)), (t_b, slice(0, 3))])
    # gaps in slices are bad
    assert_raises(ValueError, DesignInfo, ["a1", "a2", "a3", "a4"],
                  term_slices=[(t_a, slice(0, 2)), (t_b, slice(3, 4))])
    assert_raises(ValueError, DesignInfo, ["a1", "a2", "a3", "a4"],
                  term_slices=[(t_a, slice(1, 3)), (t_b, slice(3, 4))])
    assert_raises(ValueError, DesignInfo, ["a1", "a2", "a3", "a4"],
                  term_slices=[(t_a, slice(0, 2)), (t_b, slice(2, 3))])
    # overlapping slices ditto
    assert_raises(ValueError, DesignInfo, ["a1", "a2", "a3", "a4"],
                  term_slices=[(t_a, slice(0, 3)), (t_b, slice(2, 4))])
    # no step arguments
    assert_raises(ValueError, DesignInfo, ["a1", "a2", "a3", "a4"],
                  term_slices=[(t_a, slice(0, 4, 2))])
    # no term names that mismatch column names
    assert_raises(ValueError, DesignInfo, ["a1", "a2", "a3", "a4"],
                  term_name_slices=[("a1", slice(0, 3)), ("b", slice(3, 4))])

def test_DesignInfo_from_array():
    di = DesignInfo.from_array([1, 2, 3])
    assert di.column_names == ["column0"]
    di2 = DesignInfo.from_array([[1, 2], [2, 3], [3, 4]])
    assert di2.column_names == ["column0", "column1"]
    di3 = DesignInfo.from_array([1, 2, 3], default_column_prefix="x")
    assert di3.column_names == ["x0"]
    di4 = DesignInfo.from_array([[1, 2], [2, 3], [3, 4]],
                                default_column_prefix="x")
    assert di4.column_names == ["x0", "x1"]
    m = DesignMatrix([1, 2, 3], di3)
    assert DesignInfo.from_array(m) is di3
    # But weird objects are ignored
    m.design_info = "asdf"
    di_weird = DesignInfo.from_array(m)
    assert di_weird.column_names == ["column0"]

    from patsy.util import have_pandas
    if have_pandas:
        import pandas
        # with named columns
        di5 = DesignInfo.from_array(pandas.DataFrame([[1, 2]],
                                                     columns=["a", "b"]))
        assert di5.column_names == ["a", "b"]
        # with irregularly numbered columns
        di6 = DesignInfo.from_array(pandas.DataFrame([[1, 2]],
                                                     columns=[0, 10]))
        assert di6.column_names == ["column0", "column10"]
        # with .design_info attr
        df = pandas.DataFrame([[1, 2]])
        df.design_info = di6
        assert DesignInfo.from_array(df) is di6

def test_lincon():
    di = DesignInfo(["a1", "a2", "a3", "b"],
                    term_name_slices=[("a", slice(0, 3)),
                                      ("b", slice(3, 4))])
    con = di.linear_constraint(["2 * a1 = b + 1", "a3"])
    assert con.variable_names == ["a1", "a2", "a3", "b"]
    assert np.all(con.coefs == [[2, 0, 0, -1], [0, 0, 1, 0]])
    assert np.all(con.constants == [[1], [0]])

# Idea: format with a reasonable amount of precision, then if that turns out
# to be higher than necessary, remove as many zeros as we can. But only do
# this while we can do it to *all* the ordinarily-formatted numbers, to keep
# decimal points aligned.
def _format_float_column(precision, col):
    format_str = "%." + str(precision) + "f"
    assert col.ndim == 1
    # We don't want to look at numbers like "1e-5" or "nan" when stripping.
    simple_float_chars = set("+-0123456789.")
    col_strs = np.array([format_str % (x,) for x in col], dtype=object)
    # Really every item should have a decimal, but just in case, we don't want
    # to strip zeros off the end of "10" or something like that.
    mask = np.array([simple_float_chars.issuperset(col_str) and "." in col_str
                     for col_str in col_strs])
    mask_idxes = np.nonzero(mask)[0]
    strip_char = "0"
    if np.any(mask):
        while True:
            if np.all([s.endswith(strip_char) for s in col_strs[mask]]):
                for idx in mask_idxes:
                    col_strs[idx] = col_strs[idx][:-1]
            else:
                if strip_char == "0":
                    strip_char = "."
                else:
                    break
    return col_strs
            
def test__format_float_column():
    def t(precision, numbers, expected):
        got = _format_float_column(precision, np.asarray(numbers))
        print got, expected
        assert np.array_equal(got, expected)
    # This acts weird on old python versions (e.g. it can be "-nan"), so don't
    # hardcode it:
    nan_string = "%.3f" % (np.nan,)
    t(3, [1, 2.1234, 2.1239, np.nan], ["1.000", "2.123", "2.124", nan_string])
    t(3, [1, 2, 3, np.nan], ["1", "2", "3", nan_string])
    t(3, [1.0001, 2, 3, np.nan], ["1", "2", "3", nan_string])
    t(4, [1.0001, 2, 3, np.nan], ["1.0001", "2.0000", "3.0000", nan_string])

# http://docs.scipy.org/doc/numpy/user/basics.subclassing.html#slightly-more-realistic-example-attribute-added-to-existing-array
class DesignMatrix(np.ndarray):
    """A simple numpy array subclass that carries design matrix metadata.

    .. attribute:: design_info

       A :class:`DesignInfo` object containing metadata about this design
       matrix.

    This class also defines a fancy __repr__ method with labeled
    columns. Otherwise it is identical to a regular numpy ndarray.

    .. warning::

       You should never check for this class using
       :func:`isinstance`. Limitations of the numpy API mean that it is
       impossible to prevent the creation of numpy arrays that have type
       DesignMatrix, but that are not actually design matrices (and such
       objects will behave like regular ndarrays in every way). Instead, check
       for the presence of a ``.design_info`` attribute -- this will be
       present only on "real" DesignMatrix objects.
    """

    def __new__(cls, input_array, design_info=None,
                default_column_prefix="column"):
        """Create a DesignMatrix, or cast an existing matrix to a DesignMatrix.

        A call like::

          DesignMatrix(my_array)

        will convert an arbitrary array_like object into a DesignMatrix.

        The return from this function is guaranteed to be a two-dimensional
        ndarray with a real-valued floating point dtype, and a
        ``.design_info`` attribute which matches its shape. If the
        `design_info` argument is not given, then one is created via
        :meth:`DesignInfo.from_array` using the given
        `default_column_prefix`.

        Depending on the input array, it is possible this will pass through
        its input unchanged, or create a view.
        """
        # Pass through existing DesignMatrixes. The design_info check is
        # necessary because numpy is sort of annoying and cannot be stopped
        # from turning non-design-matrix arrays into DesignMatrix
        # instances. (E.g., my_dm.diagonal() will return a DesignMatrix
        # object, but one without a design_info attribute.)
        if (isinstance(input_array, DesignMatrix)
            and hasattr(input_array, "design_info")):
            return input_array
        self = atleast_2d_column_default(input_array).view(cls)
        # Upcast integer to floating point
        if np.issubdtype(self.dtype, np.integer):
            self = np.asarray(self, dtype=float).view(cls)
        if self.ndim > 2:
            raise ValueError, "DesignMatrix must be 2d"
        assert self.ndim == 2
        if design_info is None:
            design_info = DesignInfo.from_array(self, default_column_prefix)
        if len(design_info.column_names) != self.shape[1]:
            raise ValueError("wrong number of column names for design matrix "
                             "(got %s, wanted %s)"
                             % (len(design_info.column_names), self.shape[1]))
        self.design_info = design_info
        if not np.issubdtype(self.dtype, np.floating):
            raise ValueError, "design matrix must be real-valued floating point"
        return self

    __repr__ = repr_pretty_delegate
    def _repr_pretty_(self, p, cycle):
        if not hasattr(self, "design_info"):
            # Not a real DesignMatrix
            p.pretty(np.asarray(self))
            return
        assert not cycle

        # XX: could try calculating width of the current terminal window:
        #   http://stackoverflow.com/questions/566746/how-to-get-console-window-width-in-python
        # sadly it looks like ipython does not actually pass this information
        # in, even if we use _repr_pretty_ -- the pretty-printer object has a
        # fixed width it always uses. (As of IPython 0.12.)
        MAX_TOTAL_WIDTH = 78
        SEP = 2
        INDENT = 2
        MAX_ROWS = 30
        PRECISION = 5

        names = self.design_info.column_names
        column_name_widths = [len(name) for name in names]
        min_total_width = (INDENT + SEP * (self.shape[1] - 1)
                           + np.sum(column_name_widths))
        if min_total_width <= MAX_TOTAL_WIDTH:
            printable_part = np.asarray(self)[:MAX_ROWS, :]
            formatted_cols = [_format_float_column(PRECISION,
                                                   printable_part[:, i])
                              for i in xrange(self.shape[1])]
            column_num_widths = [max([len(s) for s in col])
                                 for col in formatted_cols]
            column_widths = [max(name_width, num_width)
                             for (name_width, num_width)
                             in zip(column_name_widths, column_num_widths)]
            total_width = (INDENT + SEP * (self.shape[1] - 1)
                           + np.sum(column_widths))
            print_numbers = (total_width < MAX_TOTAL_WIDTH)
        else:
            print_numbers = False   

        p.begin_group(INDENT, "DesignMatrix with shape %s" % (self.shape,))
        p.breakable("\n" + " " * p.indentation)
        if print_numbers:
            # We can fit the numbers on the screen
            sep = " " * SEP
            # list() is for Py3 compatibility
            for row in [names] + list(zip(*formatted_cols)):
                cells = [cell.rjust(width)
                         for (width, cell) in zip(column_widths, row)]
                p.text(sep.join(cells))
                p.text("\n" + " " * p.indentation)
            if MAX_ROWS < self.shape[0]:
                p.text("[%s rows omitted]" % (self.shape[0] - MAX_ROWS,))
                p.text("\n" + " " * p.indentation)
        else:
            p.begin_group(2, "Columns:")
            p.breakable("\n" + " " * p.indentation)
            p.pretty(names)
            p.end_group(2, "")
            p.breakable("\n" + " " * p.indentation)

        p.begin_group(2, "Terms:")
        p.breakable("\n" + " " * p.indentation)
        for term_name, span in self.design_info.term_name_slices.iteritems():
            if span.start != 0:
                p.breakable(", ")
            p.pretty(term_name)
            if span.stop - span.start == 1:
                coltext = "column %s" % (span.start,)
            else:
                coltext = "columns %s:%s" % (span.start, span.stop)
            p.text(" (%s)" % (coltext,))
        p.end_group(2, "")

        if not print_numbers or self.shape[0] > MAX_ROWS:
            # some data was not shown
            p.breakable("\n" + " " * p.indentation)
            p.text("(to view full data, use np.asarray(this_obj))")

        p.end_group(INDENT, "")

    # No __array_finalize__ method, because we don't want slices of this
    # object to keep the design_info (they may have different columns!), or
    # anything fancy like that.

def test_design_matrix():
    from nose.tools import assert_raises

    di = DesignInfo(["a1", "a2", "a3", "b"],
                    term_name_slices=[("a", slice(0, 3)),
                                      ("b", slice(3, 4))])
    mm = DesignMatrix([[12, 14, 16, 18]], di)
    assert mm.design_info.column_names == ["a1", "a2", "a3", "b"]

    bad_di = DesignInfo(["a1"])
    assert_raises(ValueError, DesignMatrix, [[12, 14, 16, 18]], bad_di)

    mm2 = DesignMatrix([[12, 14, 16, 18]])
    assert mm2.design_info.column_names == ["column0", "column1", "column2",
                                            "column3"]

    mm3 = DesignMatrix([12, 14, 16, 18])
    assert mm3.shape == (4, 1)

    # DesignMatrix always has exactly 2 dimensions
    assert_raises(ValueError, DesignMatrix, [[[1]]])

    # DesignMatrix constructor passes through existing DesignMatrixes
    mm4 = DesignMatrix(mm)
    assert mm4 is mm
    # But not if they are really slices:
    mm5 = DesignMatrix(mm.diagonal())
    assert mm5 is not mm

    mm6 = DesignMatrix([[12, 14, 16, 18]], default_column_prefix="x")
    assert mm6.design_info.column_names == ["x0", "x1", "x2", "x3"]

    # Only real-valued matrices can be DesignMatrixs
    assert_raises(ValueError, DesignMatrix, [1, 2, 3j])
    assert_raises(ValueError, DesignMatrix, ["a", "b", "c"])
    assert_raises(ValueError, DesignMatrix, [1, 2, object()])

    # Just smoke tests
    repr(mm)
    repr(DesignMatrix(np.arange(100)))
    repr(DesignMatrix(np.arange(100) * 2.0))
    repr(mm[1:, :])
    repr(DesignMatrix(np.arange(100).reshape((1, 100))))
    repr(DesignMatrix([np.nan, np.inf]))
    repr(DesignMatrix([np.nan, 0, 1e20, 20.5]))

########NEW FILE########
__FILENAME__ = eval
# This file is part of Patsy
# Copyright (C) 2011 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# Utilities that require an over-intimate knowledge of Python's execution
# environment.

# These are made available in the patsy.* namespace
__all__ = ["EvalEnvironment", "EvalFactor"]

import sys
import __future__
import inspect
import tokenize
from patsy import PatsyError
from patsy.util import PushbackAdapter
from patsy.tokens import (pretty_untokenize, normalize_token_spacing,
                             python_tokenize)
from patsy.compat import call_and_wrap_exc

def _all_future_flags():
    flags = 0
    for feature_name in __future__.all_feature_names:
        feature = getattr(__future__, feature_name)
        if feature.getMandatoryRelease() > sys.version_info:
            flags |= feature.compiler_flag
    return flags

_ALL_FUTURE_FLAGS = _all_future_flags()

# This is just a minimal dict-like object that does lookup in a 'stack' of
# dicts -- first it checks the first, then the second, etc. Assignments go
# into an internal, zeroth dict.
class VarLookupDict(object):
    def __init__(self, dicts):
        self._dicts = [{}] + list(dicts)

    def __getitem__(self, key):
        for d in self._dicts:
            try:
                return d[key]
            except KeyError:
                pass
        raise KeyError, key

    def __setitem__(self, key, value):
        self._dicts[0][key] = value

    def __contains__(self, key):
        try:
            self[key]
        except KeyError:
            return False
        else:
            return True

    def get(self, key, default=None):
        try:
            return self[key]
        except KeyError:
            return default

    def __repr__(self):
        return "%s(%r)" % (self.__class__.__name__, self._dicts)

def test_VarLookupDict():
    d1 = {"a": 1}
    d2 = {"a": 2, "b": 3}
    ds = VarLookupDict([d1, d2])
    assert ds["a"] == 1
    assert ds["b"] == 3
    assert "a" in ds
    assert "c" not in ds
    from nose.tools import assert_raises
    assert_raises(KeyError, ds.__getitem__, "c")
    ds["a"] = 10
    assert ds["a"] == 10
    assert d1["a"] == 1

class EvalEnvironment(object):
    """Represents a Python execution environment.

    Encapsulates a namespace for variable lookup and set of __future__
    flags."""
    def __init__(self, namespaces, flags=0):
        assert not flags & ~_ALL_FUTURE_FLAGS
        self._namespaces = list(namespaces)
        self.flags = flags

    @property
    def namespace(self):
        """A dict-like object that can be used to look up variables accessible
        from the encapsulated environment."""
        return VarLookupDict(self._namespaces)

    def add_outer_namespace(self, namespace):
        """Expose the contents of a dict-like object to the encapsulated
        environment.

        The given namespace will be checked last, after all existing namespace
        lookups have failed.
        """
        # ModelDesc.from_formula unconditionally calls
        #   eval_env.add_outer_namespace(builtins)
        # which means that if someone uses the same environment for a bunch of
        # formulas, our namespace chain will grow without bound, which would
        # suck.
        if id(namespace) not in self._namespace_ids():
            self._namespaces.append(namespace)

    def eval(self, expr, source_name="<string>", inner_namespace={}):
        """Evaluate some Python code in the encapsulated environment.

        :arg expr: A string containing a Python expression.
        :arg source_name: A name for this string, for use in tracebacks.
        :arg inner_namespace: A dict-like object that will be checked first
          when `expr` attempts to access any variables.
        :returns: The value of `expr`.
        """
        code = compile(expr, source_name, "eval", self.flags, False)
        return eval(code, {}, VarLookupDict([inner_namespace]
                                            + self._namespaces))

    @classmethod
    def capture(cls, eval_env=0, reference=0):
        """Capture an execution environment from the stack.

        If `eval_env` is already an :class:`EvalEnvironment`, it is returned
        unchanged. Otherwise, we walk up the stack by ``eval_env + reference``
        steps and capture that function's evaluation environment.

        For ``eval_env=0`` and ``reference=0``, the default, this captures the
        stack frame of the function that calls :meth:`capture`. If ``eval_env
        + reference`` is 1, then we capture that function's caller, etc.

        This somewhat complicated calling convention is designed to be
        convenient for functions which want to capture their caller's
        environment by default, but also allow explicit environments to be
        specified. See the second example.

        Example::

          x = 1
          this_env = EvalEnvironment.capture()
          assert this_env["x"] == 1
          def child_func():
              return EvalEnvironment.capture(1)
          this_env_from_child = child_func()
          assert this_env_from_child["x"] == 1

        Example::

          # This function can be used like:
          #   my_model(formula_like, data)
          #     -> evaluates formula_like in caller's environment
          #   my_model(formula_like, data, eval_env=1)
          #     -> evaluates formula_like in caller's caller's environment
          #   my_model(formula_like, data, eval_env=my_env)
          #     -> evaluates formula_like in environment 'my_env'
          def my_model(formula_like, data, eval_env=0):
              eval_env = EvalEnvironment.capture(eval_env, reference=1)
              return model_setup_helper(formula_like, data, eval_env)

        This is how :func:`dmatrix` works.

        .. versionadded: 0.2.0
           The ``reference`` argument.
        """
        if isinstance(eval_env, cls):
            return eval_env
        else:
            depth = eval_env + reference
        frame = inspect.currentframe()
        try:
            for i in xrange(depth + 1):
                if frame is None:
                    raise ValueError, "call-stack is not that deep!"
                frame = frame.f_back
            return cls([frame.f_locals, frame.f_globals],
                       frame.f_code.co_flags & _ALL_FUTURE_FLAGS)
        # The try/finally is important to avoid a potential reference cycle --
        # any exception traceback will carry a reference to *our* frame, which
        # contains a reference to our local variables, which would otherwise
        # carry a reference to some parent frame, where the exception was
        # caught...:
        finally:
            del frame

    def _namespace_ids(self):
        return [id(n) for n in self._namespaces]

    def __eq__(self, other):
        return (isinstance(other, EvalEnvironment)
                and self.flags == other.flags
                and self._namespace_ids() == other._namespace_ids())

    def __ne__(self, other):
        return not self == other

    def __hash__(self):
        return hash((EvalEnvironment,
                     self.flags,
                     tuple(self._namespace_ids())))

def _a(): # pragma: no cover
    _a = 1
    return _b()

def _b(): # pragma: no cover
    _b = 1
    return _c()

def _c(): # pragma: no cover
    _c = 1
    return [EvalEnvironment.capture(),
            EvalEnvironment.capture(0),
            EvalEnvironment.capture(1),
            EvalEnvironment.capture(0, reference=1),
            EvalEnvironment.capture(2),
            EvalEnvironment.capture(0, 2),
            ]

def test_EvalEnvironment_capture_namespace():
    c0, c, b1, b2, a1, a2 = _a()
    assert "test_EvalEnvironment_capture_namespace" in c0.namespace
    assert "test_EvalEnvironment_capture_namespace" in c.namespace
    assert "test_EvalEnvironment_capture_namespace" in b1.namespace
    assert "test_EvalEnvironment_capture_namespace" in b2.namespace
    assert "test_EvalEnvironment_capture_namespace" in a1.namespace
    assert "test_EvalEnvironment_capture_namespace" in a2.namespace
    assert c0.namespace["_c"] == 1
    assert c.namespace["_c"] == 1
    assert b1.namespace["_b"] == 1
    assert b2.namespace["_b"] == 1
    assert a1.namespace["_a"] == 1
    assert a2.namespace["_a"] == 1
    assert b1.namespace["_c"] is _c
    assert b2.namespace["_c"] is _c
    from nose.tools import assert_raises
    assert_raises(ValueError, EvalEnvironment.capture, 10 ** 6)

    assert EvalEnvironment.capture(b1) is b1

def test_EvalEnvironment_capture_flags():
    if sys.version_info >= (3,):
        # This is the only __future__ feature currently usable in Python
        # 3... fortunately it is probably not going anywhere.
        TEST_FEATURE = "barry_as_FLUFL"
    else:
        TEST_FEATURE = "division"
    test_flag = getattr(__future__, TEST_FEATURE).compiler_flag
    assert test_flag & _ALL_FUTURE_FLAGS
    source = ("def f():\n"
              "    in_f = 'hi from f'\n"
              "    global RETURN_INNER, RETURN_OUTER, RETURN_INNER_FROM_OUTER\n"
              "    RETURN_INNER = EvalEnvironment.capture(0)\n"
              "    RETURN_OUTER = call_capture_0()\n"
              "    RETURN_INNER_FROM_OUTER = call_capture_1()\n"
              "f()\n")
    code = compile(source, "<test string>", "exec", 0, 1)
    env = {"EvalEnvironment": EvalEnvironment,
           "call_capture_0": lambda: EvalEnvironment.capture(0),
           "call_capture_1": lambda: EvalEnvironment.capture(1),
           }
    env2 = dict(env)
    exec code in env
    assert env["RETURN_INNER"].namespace["in_f"] == "hi from f"
    assert env["RETURN_INNER_FROM_OUTER"].namespace["in_f"] == "hi from f"
    assert "in_f" not in env["RETURN_OUTER"].namespace
    assert env["RETURN_INNER"].flags & _ALL_FUTURE_FLAGS == 0
    assert env["RETURN_OUTER"].flags & _ALL_FUTURE_FLAGS == 0
    assert env["RETURN_INNER_FROM_OUTER"].flags & _ALL_FUTURE_FLAGS == 0

    code2 = compile(("from __future__ import %s\n" % (TEST_FEATURE,))
                    + source,
                    "<test string 2>", "exec", 0, 1)
    exec code2 in env2
    assert env2["RETURN_INNER"].namespace["in_f"] == "hi from f"
    assert env2["RETURN_INNER_FROM_OUTER"].namespace["in_f"] == "hi from f"
    assert "in_f" not in env2["RETURN_OUTER"].namespace
    assert env2["RETURN_INNER"].flags & _ALL_FUTURE_FLAGS == test_flag
    assert env2["RETURN_OUTER"].flags & _ALL_FUTURE_FLAGS == 0
    assert env2["RETURN_INNER_FROM_OUTER"].flags & _ALL_FUTURE_FLAGS == test_flag

def test_EvalEnvironment_eval_namespace():
    env = EvalEnvironment([{"a": 1}])
    assert env.eval("2 * a") == 2
    assert env.eval("2 * a", inner_namespace={"a": 2}) == 4
    from nose.tools import assert_raises
    assert_raises(NameError, env.eval, "2 * b")
    a = 3
    env2 = EvalEnvironment.capture(0)
    assert env2.eval("2 * a") == 6

def test_EvalEnvironment_eval_flags():
    from nose.tools import assert_raises
    if sys.version_info >= (3,):
        # This joke __future__ statement replaces "!=" with "<>":
        #   http://www.python.org/dev/peps/pep-0401/
        test_flag = __future__.barry_as_FLUFL.compiler_flag
        assert test_flag & _ALL_FUTURE_FLAGS
        env = EvalEnvironment([{"a": 11}], flags=0)
        assert env.eval("a != 0") == True
        assert_raises(SyntaxError, env.eval, "a <> 0")
        env2 = EvalEnvironment([{"a": 11}], flags=test_flag)
        assert env2.eval("a <> 0") == True
        assert_raises(SyntaxError, env2.eval, "a != 0")
    else:
        test_flag = __future__.division.compiler_flag
        assert test_flag & _ALL_FUTURE_FLAGS
        env = EvalEnvironment([{"a": 11}], flags=0)
        assert env.eval("a / 2") == 11 // 2 == 5
        env2 = EvalEnvironment([{"a": 11}], flags=test_flag)
        assert env2.eval("a / 2") == 11 * 1. / 2 != 5

def test_EvalEnvironment_eq():
    # Two environments are eq only if they refer to exactly the same
    # global/local dicts
    env1 = EvalEnvironment.capture(0)
    env2 = EvalEnvironment.capture(0)
    assert env1 == env2
    assert hash(env1) == hash(env2)
    capture_local_env = lambda: EvalEnvironment.capture(0)
    env3 = capture_local_env()
    env4 = capture_local_env()
    assert env3 != env4

def test_EvalEnvironment_add_outer_namespace():
    a = 1
    env = EvalEnvironment.capture(0)
    env2 = EvalEnvironment.capture(0)
    assert env.namespace["a"] == 1
    assert "b" not in env.namespace
    assert env == env2
    env.add_outer_namespace({"a": 10, "b": 2})
    assert env.namespace["a"] == 1
    assert env.namespace["b"] == 2
    assert env != env2

class EvalFactor(object):
    def __init__(self, code, eval_env, origin=None):
        """A factor class that executes arbitrary Python code and supports
        stateful transforms.

        :arg code: A string containing a Python expression, that will be
          evaluated to produce this factor's value.
        :arg eval_env: The :class:`EvalEnvironment` where `code` will be
          evaluated.

        This is the standard factor class that is used when parsing formula
        strings and implements the standard stateful transform processing. See
        :ref:`stateful-transforms` and :ref:`expert-model-specification`.

        Two EvalFactor's are considered equal (e.g., for purposes of
        redundancy detection) if they use the same evaluation environment and
        they contain the same token stream. Basically this means that the
        source code must be identical except for whitespace::

          env = EvalEnvironment.capture()
          assert EvalFactor("a + b", env) == EvalFactor("a+b", env)
          assert EvalFactor("a + b", env) != EvalFactor("b + a", env)
        """
        # For parsed formulas, the code will already have been normalized by
        # the parser. But let's normalize anyway, so we can be sure of having
        # consistent semantics for __eq__ and __hash__.
        self.code = normalize_token_spacing(code)
        self._eval_env = eval_env
        self.origin = origin

    def name(self):
        return self.code

    def __repr__(self):
        return "%s(%r)" % (self.__class__.__name__, self.code)

    def __eq__(self, other):
        return (isinstance(other, EvalFactor)
                and self.code == other.code
                and self._eval_env == other._eval_env)

    def __ne__(self, other):
        return not self == other

    def __hash__(self):
        return hash((EvalFactor, self.code, self._eval_env))

    def memorize_passes_needed(self, state):
        # 'state' is just an empty dict which we can do whatever we want with,
        # and that will be passed back to later memorize functions
        state["transforms"] = {}

        # example code: == "2 * center(x)"
        i = [0]
        def new_name_maker(token):
            value = self._eval_env.namespace.get(token)
            if hasattr(value, "__patsy_stateful_transform__"):
                obj_name = "_patsy_stobj%s__%s__" % (i[0], token)
                i[0] += 1
                obj = value.__patsy_stateful_transform__()
                state["transforms"][obj_name] = obj
                return obj_name + ".transform"
            else:
                return token
        # example eval_code: == "2 * _patsy_stobj0__center__.transform(x)"
        eval_code = replace_bare_funcalls(self.code, new_name_maker)
        state["eval_code"] = eval_code
        # paranoia: verify that none of our new names appeared anywhere in the
        # original code
        if has_bare_variable_reference(state["transforms"], self.code):
            raise PatsyError("names of this form are reserved for "
                                "internal use (%s)" % (token,), token.origin)
        # Pull out all the '_patsy_stobj0__center__.transform(x)' pieces
        # to make '_patsy_stobj0__center__.memorize_chunk(x)' pieces
        state["memorize_code"] = {}
        for obj_name in state["transforms"]:
            transform_calls = capture_obj_method_calls(obj_name, eval_code)
            assert len(transform_calls) == 1
            transform_call = transform_calls[0]
            transform_call_name, transform_call_code = transform_call
            assert transform_call_name == obj_name + ".transform"
            assert transform_call_code.startswith(transform_call_name + "(")
            memorize_code = (obj_name
                             + ".memorize_chunk"
                             + transform_call_code[len(transform_call_name):])
            state["memorize_code"][obj_name] = memorize_code
        # Then sort the codes into bins, so that every item in bin number i
        # depends only on items in bin (i-1) or less. (By 'depends', we mean
        # that in something like:
        #   spline(center(x))
        # we have to first run:
        #    center.memorize_chunk(x)
        # then
        #    center.memorize_finish(x)
        # and only then can we run:
        #    spline.memorize_chunk(center.transform(x))
        # Since all of our objects have unique names, figuring out who
        # depends on who is pretty easy -- we just check whether the
        # memorization code for spline:
        #    spline.memorize_chunk(center.transform(x))
        # mentions the variable 'center' (which in the example, of course, it
        # does).
        pass_bins = []
        unsorted = set(state["transforms"])
        while unsorted:
            pass_bin = set()
            for obj_name in unsorted:
                other_objs = unsorted.difference([obj_name])
                memorize_code = state["memorize_code"][obj_name]
                if not has_bare_variable_reference(other_objs, memorize_code):
                    pass_bin.add(obj_name)
            assert pass_bin
            unsorted.difference_update(pass_bin)
            pass_bins.append(pass_bin)
        state["pass_bins"] = pass_bins

        return len(pass_bins)

    def _eval(self, code, memorize_state, data):
        inner_namespace = VarLookupDict([data, memorize_state["transforms"]])
        return call_and_wrap_exc("Error evaluating factor",
                                 self,
                                 self._eval_env.eval,
                                 code, inner_namespace=inner_namespace)

    def memorize_chunk(self, state, which_pass, data):
        for obj_name in state["pass_bins"][which_pass]:
            self._eval(state["memorize_code"][obj_name], state, data)

    def memorize_finish(self, state, which_pass):
        for obj_name in state["pass_bins"][which_pass]:
            state["transforms"][obj_name].memorize_finish()

    # XX FIXME: consider doing something cleverer with exceptions raised here,
    # to make it clearer what's really going on. The new exception chaining
    # stuff doesn't appear to be present in any 2.x version of Python, so we
    # can't use that, but some other options:
    #    http://blog.ianbicking.org/2007/09/12/re-raising-exceptions/
    #    http://nedbatchelder.com/blog/200711/rethrowing_exceptions_in_python.html
    def eval(self, memorize_state, data):
        return self._eval(memorize_state["eval_code"], memorize_state, data)

def test_EvalFactor_basics():
    e = EvalFactor("a+b", EvalEnvironment.capture(0))
    assert e.code == "a + b"
    assert e.name() == "a + b"
    e2 = EvalFactor("a    +b", EvalEnvironment.capture(0), origin="asdf")
    assert e == e2
    assert hash(e) == hash(e2)
    assert e.origin is None
    assert e2.origin == "asdf"

def test_EvalFactor_memorize_passes_needed():
    from patsy.state import stateful_transform
    foo = stateful_transform(lambda: "FOO-OBJ")
    bar = stateful_transform(lambda: "BAR-OBJ")
    quux = stateful_transform(lambda: "QUUX-OBJ")
    e = EvalFactor("foo(x) + bar(foo(y)) + quux(z, w)",
                   EvalEnvironment.capture(0))
    state = {}
    passes = e.memorize_passes_needed(state)
    print passes
    print state
    assert passes == 2
    assert state["transforms"] == {"_patsy_stobj0__foo__": "FOO-OBJ",
                                   "_patsy_stobj1__bar__": "BAR-OBJ",
                                   "_patsy_stobj2__foo__": "FOO-OBJ",
                                   "_patsy_stobj3__quux__": "QUUX-OBJ"}
    assert (state["eval_code"]
            == "_patsy_stobj0__foo__.transform(x)"
               " + _patsy_stobj1__bar__.transform("
               "_patsy_stobj2__foo__.transform(y))"
               " + _patsy_stobj3__quux__.transform(z, w)")

    assert (state["memorize_code"]
            == {"_patsy_stobj0__foo__":
                    "_patsy_stobj0__foo__.memorize_chunk(x)",
                "_patsy_stobj1__bar__":
                    "_patsy_stobj1__bar__.memorize_chunk(_patsy_stobj2__foo__.transform(y))",
                "_patsy_stobj2__foo__":
                    "_patsy_stobj2__foo__.memorize_chunk(y)",
                "_patsy_stobj3__quux__":
                    "_patsy_stobj3__quux__.memorize_chunk(z, w)",
                })
    assert state["pass_bins"] == [set(["_patsy_stobj0__foo__",
                                       "_patsy_stobj2__foo__",
                                       "_patsy_stobj3__quux__"]),
                                  set(["_patsy_stobj1__bar__"])]

class _MockTransform(object):
    # Adds up all memorized data, then subtracts that sum from each datum
    def __init__(self):
        self._sum = 0
        self._memorize_chunk_called = 0
        self._memorize_finish_called = 0

    def memorize_chunk(self, data):
        self._memorize_chunk_called += 1
        import numpy as np
        self._sum += np.sum(data)

    def memorize_finish(self):
        self._memorize_finish_called += 1

    def transform(self, data):
        return data - self._sum

def test_EvalFactor_end_to_end():
    from patsy.state import stateful_transform
    foo = stateful_transform(_MockTransform)
    e = EvalFactor("foo(x) + foo(foo(y))", EvalEnvironment.capture(0))
    state = {}
    passes = e.memorize_passes_needed(state)
    print passes
    print state
    assert passes == 2
    import numpy as np
    e.memorize_chunk(state, 0,
                     {"x": np.array([1, 2]),
                      "y": np.array([10, 11])})
    assert state["transforms"]["_patsy_stobj0__foo__"]._memorize_chunk_called == 1
    assert state["transforms"]["_patsy_stobj2__foo__"]._memorize_chunk_called == 1
    e.memorize_chunk(state, 0, {"x": np.array([12, -10]),
                                "y": np.array([100, 3])})
    assert state["transforms"]["_patsy_stobj0__foo__"]._memorize_chunk_called == 2
    assert state["transforms"]["_patsy_stobj2__foo__"]._memorize_chunk_called == 2
    assert state["transforms"]["_patsy_stobj0__foo__"]._memorize_finish_called == 0
    assert state["transforms"]["_patsy_stobj2__foo__"]._memorize_finish_called == 0
    e.memorize_finish(state, 0)
    assert state["transforms"]["_patsy_stobj0__foo__"]._memorize_finish_called == 1
    assert state["transforms"]["_patsy_stobj2__foo__"]._memorize_finish_called == 1
    assert state["transforms"]["_patsy_stobj1__foo__"]._memorize_chunk_called == 0
    assert state["transforms"]["_patsy_stobj1__foo__"]._memorize_finish_called == 0
    e.memorize_chunk(state, 1, {"x": np.array([1, 2]),
                                "y": np.array([10, 11])})
    e.memorize_chunk(state, 1, {"x": np.array([12, -10]),
                                "y": np.array([100, 3])})
    e.memorize_finish(state, 1)
    for transform in state["transforms"].itervalues():
        assert transform._memorize_chunk_called == 2
        assert transform._memorize_finish_called == 1
    # sums:
    # 0: 1 + 2 + 12 + -10 == 5
    # 2: 10 + 11 + 100 + 3 == 124
    # 1: (10 - 124) + (11 - 124) + (100 - 124) + (3 - 124) == -372
    # results:
    # 0: -4, -3, 7, -15
    # 2: -114, -113, -24, -121
    # 1: 258, 259, 348, 251
    # 0 + 1: 254, 256, 355, 236
    assert np.all(e.eval(state,
                         {"x": np.array([1, 2, 12, -10]),
                          "y": np.array([10, 11, 100, 3])})
                  == [254, 256, 355, 236])

def annotated_tokens(code):
    prev_was_dot = False
    it = PushbackAdapter(python_tokenize(code))
    for (token_type, token, origin) in it:
        props = {}
        props["bare_ref"] = (not prev_was_dot and token_type == tokenize.NAME)
        props["bare_funcall"] = (props["bare_ref"]
                                 and it.has_more() and it.peek()[1] == "(")
        yield (token_type, token, origin, props)
        prev_was_dot = (token == ".")

def test_annotated_tokens():
    tokens_without_origins = [(token_type, token, props)
                              for (token_type, token, origin, props)
                              in (annotated_tokens("a(b) + c.d"))]
    assert (tokens_without_origins
            == [(tokenize.NAME, "a", {"bare_ref": True, "bare_funcall": True}),
                (tokenize.OP, "(", {"bare_ref": False, "bare_funcall": False}),
                (tokenize.NAME, "b", {"bare_ref": True, "bare_funcall": False}),
                (tokenize.OP, ")", {"bare_ref": False, "bare_funcall": False}),
                (tokenize.OP, "+", {"bare_ref": False, "bare_funcall": False}),
                (tokenize.NAME, "c", {"bare_ref": True, "bare_funcall": False}),
                (tokenize.OP, ".", {"bare_ref": False, "bare_funcall": False}),
                (tokenize.NAME, "d",
                    {"bare_ref": False, "bare_funcall": False}),
                ])

    # This was a bug:
    assert len(list(annotated_tokens("x"))) == 1

def has_bare_variable_reference(names, code):
    for (_, token, _, props) in annotated_tokens(code):
        if props["bare_ref"] and token in names:
            return True
    return False

def replace_bare_funcalls(code, replacer):
    tokens = []
    for (token_type, token, origin, props) in annotated_tokens(code):
        if props["bare_ref"] and props["bare_funcall"]:
            token = replacer(token)
        tokens.append((token_type, token))
    return pretty_untokenize(tokens)

def test_replace_bare_funcalls():
    def replacer1(token):
        return {"a": "b", "foo": "_internal.foo.process"}.get(token, token)
    def t1(code, expected):
        replaced = replace_bare_funcalls(code, replacer1)
        print "%r -> %r" % (code, replaced)
        print "(wanted %r)" % (expected,)
        assert replaced == expected
    t1("foobar()", "foobar()")
    t1("a()", "b()")
    t1("foobar.a()", "foobar.a()")
    t1("foo()", "_internal.foo.process()")
    t1("a + 1", "a + 1")
    t1("b() + a() * x[foo(2 ** 3)]",
       "b() + b() * x[_internal.foo.process(2 ** 3)]")

class _FuncallCapturer(object):
    # captures the next funcall
    def __init__(self, start_token_type, start_token):
        self.func = [start_token]
        self.tokens = [(start_token_type, start_token)]
        self.paren_depth = 0
        self.started = False
        self.done = False

    def add_token(self, token_type, token):
        if self.done:
            return
        self.tokens.append((token_type, token))
        if token in ["(", "{", "["]:
            self.paren_depth += 1
        if token in [")", "}", "]"]:
            self.paren_depth -= 1
        assert self.paren_depth >= 0
        if not self.started:
            if token == "(":
                self.started = True
            else:
                assert token_type == tokenize.NAME or token == "."
                self.func.append(token)
        if self.started and self.paren_depth == 0:
            self.done = True

# This is not a very general function -- it assumes that all references to the
# given object are of the form '<obj_name>.something(method call)'.
def capture_obj_method_calls(obj_name, code):
    capturers = []
    for (token_type, token, origin, props) in annotated_tokens(code):
        for capturer in capturers:
            capturer.add_token(token_type, token)
        if props["bare_ref"] and token == obj_name:
            capturers.append(_FuncallCapturer(token_type, token))
    return [("".join(capturer.func), pretty_untokenize(capturer.tokens))
            for capturer in capturers]

def test_capture_obj_method_calls():
    assert (capture_obj_method_calls("foo", "a + foo.baz(bar) + b.c(d)")
            == [("foo.baz", "foo.baz(bar)")])
    assert (capture_obj_method_calls("b", "a + foo.baz(bar) + b.c(d)")
            == [("b.c", "b.c(d)")])
    assert (capture_obj_method_calls("foo", "foo.bar(foo.baz(quux))")
            == [("foo.bar", "foo.bar(foo.baz(quux))"),
                ("foo.baz", "foo.baz(quux)")])
    assert (capture_obj_method_calls("bar", "foo[bar.baz(x(z[asdf])) ** 2]")
            == [("bar.baz", "bar.baz(x(z[asdf]))")])

########NEW FILE########
__FILENAME__ = highlevel
# This file is part of Patsy
# Copyright (C) 2011-2013 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# These are made available in the patsy.* namespace:
__all__ = ["dmatrix", "dmatrices",
           "incr_dbuilder", "incr_dbuilders"]

# problems:
#   statsmodels reluctant to pass around separate eval environment, suggesting
#     that design_and_matrices-equivalent should return a formula_like
#   is ModelDesc really the high-level thing?
#   ModelDesign doesn't work -- need to work with the builder set
#   want to be able to return either a matrix or a pandas dataframe

import numpy as np
from patsy import PatsyError
from patsy.design_info import DesignMatrix, DesignInfo
from patsy.eval import EvalEnvironment
from patsy.desc import ModelDesc
from patsy.build import (design_matrix_builders,
                         build_design_matrices,
                         DesignMatrixBuilder)
from patsy.util import (have_pandas, asarray_or_pandas,
                        atleast_2d_column_default)

if have_pandas:
    import pandas

# Tries to build a (lhs, rhs) design given a formula_like and an incremental
# data source. If formula_like is not capable of doing this, then returns
# None.
def _try_incr_builders(formula_like, data_iter_maker, eval_env,
                       NA_action):
    if isinstance(formula_like, DesignMatrixBuilder):
        return (design_matrix_builders([[]], data_iter_maker, NA_action)[0],
                formula_like)
    if (isinstance(formula_like, tuple)
        and len(formula_like) == 2
        and isinstance(formula_like[0], DesignMatrixBuilder)
        and isinstance(formula_like[1], DesignMatrixBuilder)):
        return formula_like
    if hasattr(formula_like, "__patsy_get_model_desc__"):
        formula_like = formula_like.__patsy_get_model_desc__(eval_env)
        if not isinstance(formula_like, ModelDesc):
            raise PatsyError("bad value from %r.__patsy_get_model_desc__"
                                % (formula_like,))
        # fallthrough
    if isinstance(formula_like, basestring):
        assert isinstance(eval_env, EvalEnvironment)
        formula_like = ModelDesc.from_formula(formula_like, eval_env)
        # fallthrough
    if isinstance(formula_like, ModelDesc):
        return design_matrix_builders([formula_like.lhs_termlist,
                                       formula_like.rhs_termlist],
                                      data_iter_maker,
                                      NA_action)
    else:
        return None

def incr_dbuilder(formula_like, data_iter_maker, eval_env=0, NA_action="drop"):
    """Construct a design matrix builder incrementally from a large data set.

    :arg formula_like: Similar to :func:`dmatrix`, except that explicit
      matrices are not allowed. Must be a formula string, a
      :class:`ModelDesc`, a :class:`DesignMatrixBuilder`, or an object with a
      ``__patsy_get_model_desc__`` method.
    :arg data_iter_maker: A zero-argument callable which returns an iterator
      over dict-like data objects. This must be a callable rather than a
      simple iterator because sufficiently complex formulas may require
      multiple passes over the data (e.g. if there are nested stateful
      transforms).
    :arg eval_env: Either a :class:`EvalEnvironment` which will be used to
      look up any variables referenced in `formula_like` that cannot be
      found in `data`, or else a depth represented as an
      integer which will be passed to :meth:`EvalEnvironment.capture`.
      ``eval_env=0`` means to use the context of the function calling
      :func:`incr_dbuilder` for lookups. If calling this function from a
      library, you probably want ``eval_env=1``, which means that variables
      should be resolved in *your* caller's namespace.
    :arg NA_action: An :class:`NAAction` object or string, used to determine
      what values count as 'missing' for purposes of determining the levels of
      categorical factors.
    :returns: A :class:`DesignMatrixBuilder`

    Tip: for `data_iter_maker`, write a generator like::

      def iter_maker():
          for data_chunk in my_data_store:
              yield data_chunk

    and pass `iter_maker` (*not* `iter_maker()`).

    .. versionadded:: 0.2.0
       The ``NA_action`` argument.
    """
    eval_env = EvalEnvironment.capture(eval_env, reference=1)
    builders = _try_incr_builders(formula_like, data_iter_maker, eval_env,
                                  NA_action)
    if builders is None:
        raise PatsyError("bad formula-like object")
    if len(builders[0].design_info.column_names) > 0:
        raise PatsyError("encountered outcome variables for a model "
                            "that does not expect them")
    return builders[1]

def incr_dbuilders(formula_like, data_iter_maker, eval_env=0,
                   NA_action="drop"):
    """Construct two design matrix builders incrementally from a large data
    set.

    :func:`incr_dbuilders` is to :func:`incr_dbuilder` as :func:`dmatrices` is
    to :func:`dmatrix`. See :func:`incr_dbuilder` for details.
    """
    eval_env = EvalEnvironment.capture(eval_env, reference=1)
    builders = _try_incr_builders(formula_like, data_iter_maker, eval_env,
                                  NA_action)
    if builders is None:
        raise PatsyError("bad formula-like object")
    if len(builders[0].design_info.column_names) == 0:
        raise PatsyError("model is missing required outcome variables")
    return builders

# This always returns a length-two tuple,
#   response, predictors
# where
#   response is a DesignMatrix (possibly with 0 columns)
#   predictors is a DesignMatrix
# The input 'formula_like' could be like:
#   (np.ndarray, np.ndarray)
#   (DesignMatrix, DesignMatrix)
#   (None, DesignMatrix)
#   np.ndarray  # for predictor-only models
#   DesignMatrix
#   (None, np.ndarray)
#   "y ~ x"
#   ModelDesc(...)
#   DesignMatrixBuilder
#   (DesignMatrixBuilder, DesignMatrixBuilder)
#   any object with a special method __patsy_get_model_desc__
def _do_highlevel_design(formula_like, data, eval_env,
                         NA_action, return_type):
    if return_type == "dataframe" and not have_pandas:
        raise PatsyError("pandas.DataFrame was requested, but pandas "
                            "is not installed")
    if return_type not in ("matrix", "dataframe"):
        raise PatsyError("unrecognized output type %r, should be "
                            "'matrix' or 'dataframe'" % (return_type,))
    def data_iter_maker():
        return iter([data])
    builders = _try_incr_builders(formula_like, data_iter_maker, eval_env,
                                  NA_action)
    if builders is not None:
        return build_design_matrices(builders, data,
                                     NA_action=NA_action,
                                     return_type=return_type)
    else:
        # No builders, but maybe we can still get matrices
        if isinstance(formula_like, tuple):
            if len(formula_like) != 2:
                raise PatsyError("don't know what to do with a length %s "
                                    "matrices tuple"
                                    % (len(formula_like),))
            (lhs, rhs) = formula_like
        else:
            # subok=True is necessary here to allow DesignMatrixes to pass
            # through
            (lhs, rhs) = (None, asarray_or_pandas(formula_like, subok=True))
        # some sort of explicit matrix or matrices were given. Currently we
        # have them in one of these forms:
        #   -- an ndarray or subclass
        #   -- a DesignMatrix
        #   -- a pandas.Series
        #   -- a pandas.DataFrame
        # and we have to produce a standard output format.
        def _regularize_matrix(m, default_column_prefix):
            di = DesignInfo.from_array(m, default_column_prefix)
            if have_pandas and isinstance(m, (pandas.Series, pandas.DataFrame)):
                orig_index = m.index
            else:
                orig_index = None
            if return_type == "dataframe":
                m = atleast_2d_column_default(m, preserve_pandas=True)
                m = pandas.DataFrame(m)
                m.columns = di.column_names
                m.design_info = di
                return (m, orig_index)
            else:
                return (DesignMatrix(m, di), orig_index)
        rhs, rhs_orig_index = _regularize_matrix(rhs, "x")
        if lhs is None:
            lhs = np.zeros((rhs.shape[0], 0), dtype=float)
        lhs, lhs_orig_index = _regularize_matrix(lhs, "y")

        assert isinstance(getattr(lhs, "design_info", None), DesignInfo)
        assert isinstance(getattr(rhs, "design_info", None), DesignInfo)
        if lhs.shape[0] != rhs.shape[0]:
            raise PatsyError("shape mismatch: outcome matrix has %s rows, "
                                "predictor matrix has %s rows"
                                % (lhs.shape[0], rhs.shape[0]))
        if rhs_orig_index is not None and lhs_orig_index is not None:
            if not rhs_orig_index.equals(lhs_orig_index):
                raise PatsyError("index mismatch: outcome and "
                                    "predictor have incompatible indexes")
        if return_type == "dataframe":
            if rhs_orig_index is not None and lhs_orig_index is None:
                lhs.index = rhs.index
            if rhs_orig_index is None and lhs_orig_index is not None:
                rhs.index = lhs.index
        return (lhs, rhs)

def dmatrix(formula_like, data={}, eval_env=0,
            NA_action="drop", return_type="matrix"):
    """Construct a single design matrix given a formula_like and data.

    :arg formula_like: An object that can be used to construct a design
      matrix. See below.
    :arg data: A dict-like object that can be used to look up variables
      referenced in `formula_like`.
    :arg eval_env: Either a :class:`EvalEnvironment` which will be used to
      look up any variables referenced in `formula_like` that cannot be
      found in `data`, or else a depth represented as an
      integer which will be passed to :meth:`EvalEnvironment.capture`.
      ``eval_env=0`` means to use the context of the function calling
      :func:`dmatrix` for lookups. If calling this function from a library,
      you probably want ``eval_env=1``, which means that variables should be
      resolved in *your* caller's namespace.
    :arg NA_action: What to do with rows that contain missing values. You can
      ``"drop"`` them, ``"raise"`` an error, or for customization, pass an
      :class:`NAAction` object. See :class:`NAAction` for details on what
      values count as 'missing' (and how to alter this).
    :arg return_type: Either ``"matrix"`` or ``"dataframe"``. See below.

    The `formula_like` can take a variety of forms. You can use any of the
    following:

    * (The most common option) A formula string like ``"x1 + x2"`` (for
      :func:`dmatrix`) or ``"y ~ x1 + x2"`` (for :func:`dmatrices`). For
      details see :ref:`formulas`.
    * A :class:`ModelDesc`, which is a Python object representation of a
      formula. See :ref:`formulas` and :ref:`expert-model-specification` for
      details.
    * A :class:`DesignMatrixBuilder`.
    * An object that has a method called :meth:`__patsy_get_model_desc__`.
      For details see :ref:`expert-model-specification`.
    * A numpy array_like (for :func:`dmatrix`) or a tuple
      (array_like, array_like) (for :func:`dmatrices`). These will have
      metadata added, representation normalized, and then be returned
      directly. In this case `data` and `eval_env` are
      ignored. There is special handling for two cases:

      * :class:`DesignMatrix` objects will have their :class:`DesignInfo`
        preserved. This allows you to set up custom column names and term
        information even if you aren't using the rest of the patsy
        machinery.
      * :class:`pandas.DataFrame` or :class:`pandas.Series` objects will have
        their (row) indexes checked. If two are passed in, their indexes must
        be aligned. If ``return_type="dataframe"``, then their indexes will be
        preserved on the output.

    Regardless of the input, the return type is always either:

    * A :class:`DesignMatrix`, if ``return_type="matrix"`` (the default)
    * A :class:`pandas.DataFrame`, if ``return_type="dataframe"``.

    The actual contents of the design matrix is identical in both cases, and
    in both cases a :class:`DesignInfo` object will be available in a
    ``.design_info`` attribute on the return value. However, for
    ``return_type="dataframe"``, any pandas indexes on the input (either in
    `data` or directly passed through `formula_like`) will be preserved, which
    may be useful for e.g. time-series models.

    .. versionadded:: 0.2.0
       The ``NA_action`` argument.
    """
    eval_env = EvalEnvironment.capture(eval_env, reference=1)
    (lhs, rhs) = _do_highlevel_design(formula_like, data, eval_env,
                                      NA_action, return_type)
    if lhs.shape[1] != 0:
        raise PatsyError("encountered outcome variables for a model "
                            "that does not expect them")
    return rhs

def dmatrices(formula_like, data={}, eval_env=0,
              NA_action="drop", return_type="matrix"):
    """Construct two design matrices given a formula_like and data.

    This function is identical to :func:`dmatrix`, except that it requires
    (and returns) two matrices instead of one. By convention, the first matrix
    is the "outcome" or "y" data, and the second is the "predictor" or "x"
    data.

    See :func:`dmatrix` for details.
    """
    eval_env = EvalEnvironment.capture(eval_env, reference=1)
    (lhs, rhs) = _do_highlevel_design(formula_like, data, eval_env,
                                      NA_action, return_type)
    if lhs.shape[1] == 0:
        raise PatsyError("model is missing required outcome variables")
    return (lhs, rhs)

########NEW FILE########
__FILENAME__ = infix_parser
# This file is part of Patsy
# Copyright (C) 2011 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# This file implements a simple "shunting yard algorithm" parser for infix
# languages with parentheses. It is used as the core of our parser for
# formulas, but is generic enough to be used for other purposes as well
# (e.g. parsing linear constraints). It just builds a parse tree; semantics
# are somebody else's problem.
# 
# Plus it spends energy on tracking where each item in the parse tree comes
# from, to allow high-quality error reporting.
#
# You are expected to provide an collection of Operators, a collection of
# atomic types, and an iterator that provides Tokens. Each Operator should
# have a unique token_type (which is an arbitrary Python object), and each
# Token should have a matching token_type, or one of the special types
# Token.LPAREN, Token.RPAREN. Each Token is required to have a valid Origin
# attached, for error reporting.

# XX: still seriously consider putting the magic intercept handling into the
# tokenizer. we'd still need separate term-sets that get pasted together by ~
# to create the modeldesc, though... heck maybe we should just have a
# modeldesc be 1-or-more termsets, with the convention that if it's 1, then
# it's a rhs, and if it's 2, it's (lhs, rhs), and otherwise you're on your
# own. Test: would this be useful for multiple-group log-linear models,
# maybe? Answer: Perhaps. outcome ~ x1 + x2 ~ group. But lots of other
# plausible, maybe better ways to write this -- (outcome | group) ~ x1 + x2?
# "outcome ~ x1 + x2", group="group"? etc.

__all__ = ["Token", "ParseNode", "Operator", "parse"]

from patsy import PatsyError
from patsy.origin import Origin
from patsy.util import repr_pretty_delegate, repr_pretty_impl

class _UniqueValue(object):
    def __init__(self, print_as):
        self._print_as = print_as

    def __repr__(self):
        return "%s(%r)" % (self.__class__.__name__, self._print_as)


class Token(object):
    """A token with possible payload.

    .. attribute:: type

       An arbitrary object indicating the type of this token. Should be
      :term:`hashable`, but otherwise it can be whatever you like.
    """
    LPAREN = _UniqueValue("LPAREN")
    RPAREN = _UniqueValue("RPAREN")

    def __init__(self, type, origin, extra=None):
        self.type = type
        self.origin = origin
        self.extra = extra

    __repr__ = repr_pretty_delegate
    def _repr_pretty_(self, p, cycle):
        assert not cycle
        kwargs = []
        if self.extra is not None:
            kwargs = [("extra", self.extra)]
        return repr_pretty_impl(p, self, [self.type, self.origin], kwargs)

class ParseNode(object):
    def __init__(self, type, token, args, origin):
        self.type = type
        self.token = token
        self.args = args
        self.origin = origin

    __repr__ = repr_pretty_delegate
    def _repr_pretty_(self, p, cycle):
        return repr_pretty_impl(p, self, [self.type, self.token, self.args])

class Operator(object):
    def __init__(self, token_type, arity, precedence):
        self.token_type = token_type
        self.arity = arity
        self.precedence = precedence

    def __repr__(self):
        return "%s(%r, %r, %r)" % (self.__class__.__name__,
                                   self.token_type, self.arity, self.precedence)

class _StackOperator(object):
    def __init__(self, op, token):
        self.op = op
        self.token = token

_open_paren = Operator(Token.LPAREN, -1, -9999999)

class _ParseContext(object):
    def __init__(self, unary_ops, binary_ops, atomic_types, trace):
        self.op_stack = []
        self.noun_stack = []
        self.unary_ops = unary_ops
        self.binary_ops = binary_ops
        self.atomic_types = atomic_types
        self.trace = trace

def _read_noun_context(token, c):
    if token.type == Token.LPAREN:
        if c.trace:
            print "Pushing open-paren"
        c.op_stack.append(_StackOperator(_open_paren, token))
        return True
    elif token.type in c.unary_ops:
        if c.trace:
            print "Pushing unary op %r" % (token.type,)
        c.op_stack.append(_StackOperator(c.unary_ops[token.type], token))
        return True
    elif token.type in c.atomic_types:
        if c.trace:
            print "Pushing noun %r (%r)" % (token.type, token.extra)
        c.noun_stack.append(ParseNode(token.type, token, [],
                                      token.origin))
        return False
    else:
        raise PatsyError("expected a noun, not '%s'"
                            % (token.origin.relevant_code(),),
                            token)

def _run_op(c):
    assert c.op_stack
    stackop = c.op_stack.pop()
    args = []
    for i in xrange(stackop.op.arity):
        args.append(c.noun_stack.pop())
    args.reverse()
    if c.trace:
        print "Reducing %r (%r)" % (stackop.op.token_type, args)
    node = ParseNode(stackop.op.token_type, stackop.token, args,
                     Origin.combine([stackop.token] + args))
    c.noun_stack.append(node)

def _read_op_context(token, c):
    if token.type == Token.RPAREN:
        if c.trace:
            print "Found close-paren"
        while c.op_stack and c.op_stack[-1].op.token_type != Token.LPAREN:
            _run_op(c)
        if not c.op_stack:
            raise PatsyError("missing '(' or extra ')'", token)
        assert c.op_stack[-1].op.token_type == Token.LPAREN
        # Expand the origin of the item on top of the noun stack to include
        # the open and close parens:
        combined = Origin.combine([c.op_stack[-1].token,
                                   c.noun_stack[-1].token,
                                   token])
        c.noun_stack[-1].origin = combined
        # Pop the open-paren
        c.op_stack.pop()
        return False
    elif token.type in c.binary_ops:
        if c.trace:
            print "Found binary operator %r" % (token.type)
        stackop = _StackOperator(c.binary_ops[token.type], token)
        while (c.op_stack
               and stackop.op.precedence <= c.op_stack[-1].op.precedence):
            _run_op(c)
        if c.trace:
            print "Pushing binary operator %r" % (token.type)
        c.op_stack.append(stackop)
        return True
    else:
        raise PatsyError("expected an operator, not '%s'"
                            % (token.origin.relevant_code(),),
                            token)

def infix_parse(tokens, operators, atomic_types, trace=False):
    token_source = iter(tokens)

    unary_ops = {}
    binary_ops = {}
    for op in operators:
        assert op.precedence > _open_paren.precedence
        if op.arity == 1:
            unary_ops[op.token_type] = op
        elif op.arity == 2:
            binary_ops[op.token_type] = op
        else:
            raise ValueError, "operators must be unary or binary"

    c = _ParseContext(unary_ops, binary_ops, atomic_types, trace)

    # This is an implementation of Dijkstra's shunting yard algorithm:
    #   http://en.wikipedia.org/wiki/Shunting_yard_algorithm
    #   http://www.engr.mun.ca/~theo/Misc/exp_parsing.htm

    want_noun = True
    for token in token_source:
        if c.trace:
            print "Reading next token (want_noun=%r)" % (want_noun,)
        if want_noun:
            want_noun = _read_noun_context(token, c)
        else:
            want_noun = _read_op_context(token, c)
    if c.trace:
        print "End of token stream"
        
    if want_noun:
        raise PatsyError("expected a noun, but instead the expression ended",
                            c.op_stack[-1].token.origin)

    while c.op_stack:
        if c.op_stack[-1].op.token_type == Token.LPAREN:
            raise PatsyError("Unmatched '('", c.op_stack[-1].token)
        _run_op(c)

    assert len(c.noun_stack) == 1
    return c.noun_stack.pop()

# Much more thorough tests in parse_formula.py, this is just a smoke test:
def test_infix_parse():
    ops = [Operator("+", 2, 10),
           Operator("*", 2, 20),
           Operator("-", 1, 30)]
    atomic = ["ATOM1", "ATOM2"]
    # a + -b * (c + d)
    mock_origin = Origin("asdf", 2, 3)
    tokens = [Token("ATOM1", mock_origin, "a"),
              Token("+", mock_origin, "+"),
              Token("-", mock_origin, "-"),
              Token("ATOM2", mock_origin, "b"),
              Token("*", mock_origin, "*"),
              Token(Token.LPAREN, mock_origin, "("),
              Token("ATOM1", mock_origin, "c"),
              Token("+", mock_origin, "+"),
              Token("ATOM2", mock_origin, "d"),
              Token(Token.RPAREN, mock_origin, ")")]
    tree = infix_parse(tokens, ops, atomic)
    def te(tree, type, extra):
        assert tree.type == type
        assert tree.token.extra == extra
    te(tree, "+", "+")
    te(tree.args[0], "ATOM1", "a")
    assert tree.args[0].args == []
    te(tree.args[1], "*", "*")
    te(tree.args[1].args[0], "-", "-")
    assert len(tree.args[1].args[0].args) == 1
    te(tree.args[1].args[0].args[0], "ATOM2", "b")
    te(tree.args[1].args[1], "+", "+")
    te(tree.args[1].args[1].args[0], "ATOM1", "c")
    te(tree.args[1].args[1].args[1], "ATOM2", "d")

    from nose.tools import assert_raises
    # No ternary ops
    assert_raises(ValueError,
                  infix_parse, [], [Operator("+", 3, 10)], ["ATOMIC"])

    # smoke test just to make sure there are no egregious bugs in 'trace'
    infix_parse(tokens, ops, atomic, trace=True)

########NEW FILE########
__FILENAME__ = missing
# This file is part of Patsy
# Copyright (C) 2013 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# Missing data detection/handling

# First, how do we represent missing data? (i.e., which values count as
# "missing"?) In the long run, we want to use numpy's NA support... but that
# doesn't exist yet. Until then, people use various sorts of ad-hoc
# things. Some things that might be considered NA:
#   NA (eventually)
#   NaN  (in float or object arrays)
#   None (in object arrays)
#   np.ma.masked (in numpy.ma masked arrays)
# Pandas compatibility considerations:
#   For numeric arrays, None is unconditionally converted to NaN.
#   For object arrays (including string arrays!), None and NaN are preserved,
#     but pandas.isnull() returns True for both.
# np.ma compatibility considerations:
#   Preserving array subtypes is a huge pain, because it means that we can't
#   just call 'asarray' and be done... we already jump through tons of hoops
#   to write code that can handle both ndarray's and pandas objects, and
#   just thinking about adding another item to this list makes me tired. So
#   for now we don't support np.ma missing values. Use pandas!

# Next, what should be done once we find missing data? R's options:
#   -- throw away those rows (from all aligned matrices)
#      -- with or without preserving information on which rows were discarded
#   -- error out
#   -- carry on
# The 'carry on' option requires that we have some way to represent NA in our
# output array. To avoid further solidifying the use of NaN for this purpose,
# we'll leave this option out for now, until real NA support is
# available. Also, we always preserve information on which rows were
# discarded, using the pandas index functionality (currently this is only
# returned to the original caller if they used return_type="dataframe",
# though).

import numpy as np
from patsy import PatsyError
from patsy.util import safe_isnan, safe_scalar_isnan

# These are made available in the patsy.* namespace
__all__ = ["NAAction"]

_valid_NA_types = ["None", "NaN"]
_valid_NA_responses = ["raise", "drop"]
def _desc_options(options):
    return ", ".join([repr(opt) for opt in options])

class NAAction(object):
    """An :class:`NAAction` object defines a strategy for handling missing
    data.

    "NA" is short for "Not Available", and is used to refer to any value which
    is somehow unmeasured or unavailable. In the long run, it is devoutly
    hoped that numpy will gain first-class missing value support. Until then,
    we work around this lack as best we're able.

    There are two parts to this: First, we have to determine what counts as
    missing data. For numerical data, the default is to treat NaN values
    (e.g., ``numpy.nan``) as missing. For categorical data, the default is to
    treat NaN values, and also the Python object None, as missing. (This is
    consistent with how pandas does things, so if you're already using
    None/NaN to mark missing data in your pandas DataFrames, you're good to
    go.)

    Second, we have to decide what to do with any missing data when we
    encounter it. One option is to simply discard any rows which contain
    missing data from our design matrices (``drop``). Another option is to
    raise an error (``raise``). A third option would be to simply let the
    missing values pass through into the returned design matrices. However,
    this last option is not yet implemented, because of the lack of any
    standard way to represent missing values in arbitrary numpy matrices;
    we're hoping numpy will get this sorted out before we standardize on
    anything ourselves.

    You can control how patsy handles missing data through the ``NA_action=``
    argument to functions like :func:`build_design_matrices` and
    :func:`dmatrix`. If all you want to do is to choose between ``drop`` and
    ``raise`` behaviour, you can pass one of those strings as the
    ``NA_action=`` argument directly. If you want more fine-grained control
    over how missing values are detected and handled, then you can create an
    instance of this class, or your own object that implements the same
    interface, and pass that as the ``NA_action=`` argument instead.
    """
    def __init__(self, on_NA="drop", NA_types=["None", "NaN"]):
        """The :class:`NAAction` constructor takes the following arguments:

        :arg on_NA: How to handle missing values. The default is ``"drop"``,
          which removes all rows from all matrices which contain any missing
          values. Also available is ``"raise"``, which raises an exception
          when any missing values are encountered.
        :arg NA_types: Which rules are used to identify missing values, as a
          list of strings. Allowed values are:

          * ``"None"``: treat the ``None`` object as missing in categorical
            data.
          * ``"NaN"``: treat floating point NaN values as missing in
            categorical and numerical data.

        .. versionadded:: 0.2.0
        """
        self.on_NA = on_NA
        if self.on_NA not in _valid_NA_responses:
            raise ValueError("invalid on_NA action %r "
                             "(should be one of %s)"
                             % (on_NA, _desc_options(_valid_NA_responses)))
        if isinstance(NA_types, basestring):
            raise ValueError("NA_types should be a list of strings")
        self.NA_types = tuple(NA_types)
        for NA_type in self.NA_types:
            if NA_type not in _valid_NA_types:
                raise ValueError("invalid NA_type %r "
                                 "(should be one of %s)"
                                 % (NA_type, _desc_options(_valid_NA_types)))

    def is_categorical_NA(self, obj):
        """Return True if `obj` is a categorical NA value.

        Note that here `obj` is a single scalar value."""
        if "NaN" in self.NA_types and safe_scalar_isnan(obj):
            return True
        if "None" in self.NA_types and obj is None:
            return True
        return False

    def is_numerical_NA(self, arr):
        """Returns a 1-d mask array indicating which rows in an array of
        numerical values contain at least one NA value.

        Note that here `arr` is a numpy array or pandas DataFrame."""
        mask = np.zeros(arr.shape, dtype=bool)
        if "NaN" in self.NA_types:
            mask |= np.isnan(arr)
        if mask.ndim > 1:
            mask = np.any(mask, axis=1)
        return mask

    def handle_NA(self, values, is_NAs, origins):
        """Takes a set of factor values that may have NAs, and handles them
        appropriately.

        :arg values: A list of `ndarray` objects representing the data.
          These may be 1- or 2-dimensional, and may be of varying dtype. All
          will have the same number of rows (or entries, for 1-d arrays).
        :arg is_NAs: A list with the same number of entries as `values`,
          containing boolean `ndarray` objects that indicate which rows
          contain NAs in the corresponding entry in `values`.
        :arg origins: A list with the same number of entries as
          `values`, containing information on the origin of each
          value. If we encounter a problem with some particular value, we use
          the corresponding entry in `origins` as the origin argument when
          raising a :class:`PatsyError`.
        :returns: A list of new values (which may have a differing number of
          rows.)
        """
        assert len(values) == len(is_NAs) == len(origins)
        if len(values) == 0:
            return values
        if self.on_NA == "raise":
            return self._handle_NA_raise(values, is_NAs, origins)
        elif self.on_NA == "drop":
            return self._handle_NA_drop(values, is_NAs, origins)
        else: # pragma: no cover
            assert False

    def _handle_NA_raise(self, values, is_NAs, origins):
        for is_NA, origin in zip(is_NAs, origins):
            if np.any(is_NA):
                raise PatsyError("factor contains missing values", origin)
        return values

    def _handle_NA_drop(self, values, is_NAs, origins):
        total_mask = np.zeros(is_NAs[0].shape[0], dtype=bool)
        for is_NA in is_NAs:
            total_mask |= is_NA
        good_mask = ~total_mask
        # "..." to handle 1- versus 2-dim indexing
        return [v[good_mask, ...] for v in values]

def test_NAAction_basic():
    from nose.tools import assert_raises
    assert_raises(ValueError, NAAction, on_NA="pord")
    assert_raises(ValueError, NAAction, NA_types=("NaN", "asdf"))
    assert_raises(ValueError, NAAction, NA_types="NaN")

def test_NAAction_NA_types_numerical():
    for NA_types in [[], ["NaN"], ["None"], ["NaN", "None"]]:
        action = NAAction(NA_types=NA_types)
        for extra_shape in [(), (1,), (2,)]:
            arr = np.ones((4,) + extra_shape, dtype=float)
            nan_rows = [0, 2]
            if arr.ndim > 1 and arr.shape[1] > 1:
                arr[nan_rows, [0, 1]] = np.nan
            else:
                arr[nan_rows] = np.nan
            exp_NA_mask = np.zeros(4, dtype=bool)
            if "NaN" in NA_types:
                exp_NA_mask[nan_rows] = True
            got_NA_mask = action.is_numerical_NA(arr)
            assert np.array_equal(got_NA_mask, exp_NA_mask)

def test_NAAction_NA_types_categorical():
    for NA_types in [[], ["NaN"], ["None"], ["NaN", "None"]]:
        action = NAAction(NA_types=NA_types)
        assert not action.is_categorical_NA("a")
        assert not action.is_categorical_NA(1)
        assert action.is_categorical_NA(None) == ("None" in NA_types)
        assert action.is_categorical_NA(np.nan) == ("NaN" in NA_types)

def test_NAAction_drop():
    action = NAAction("drop")
    in_values = [np.asarray([-1, 2, -1, 4, 5]),
                 np.asarray([10.0, 20.0, 30.0, 40.0, 50.0]),
                 np.asarray([[1.0, np.nan],
                             [3.0, 4.0],
                             [10.0, 5.0],
                             [6.0, 7.0],
                             [8.0, np.nan]]),
                 ]
    is_NAs = [np.asarray([True, False, True, False, False]),
              np.zeros(5, dtype=bool),
              np.asarray([True, False, False, False, True]),
              ]
    out_values = action.handle_NA(in_values, is_NAs, [None] * 3)
    assert len(out_values) == 3
    assert np.array_equal(out_values[0], [2, 4])
    assert np.array_equal(out_values[1], [20.0, 40.0])
    assert np.array_equal(out_values[2], [[3.0, 4.0], [6.0, 7.0]])
    
def test_NAAction_raise():
    action = NAAction(on_NA="raise")

    # no-NA just passes through:
    in_arrs = [np.asarray([1.1, 1.2]),
               np.asarray([1, 2])]
    is_NAs = [np.asarray([False, False])] * 2
    got_arrs = action.handle_NA(in_arrs, is_NAs, [None, None])
    assert np.array_equal(got_arrs[0], in_arrs[0])
    assert np.array_equal(got_arrs[1], in_arrs[1])

    from patsy.origin import Origin
    o1 = Origin("asdf", 0, 1)
    o2 = Origin("asdf", 2, 3)

    # NA raises an error with a correct origin
    in_idx = np.arange(2)
    in_arrs = [np.asarray([1.1, 1.2]),
               np.asarray([1.0, np.nan])]
    is_NAs = [np.asarray([False, False]),
              np.asarray([False, True])]
    try:
        action.handle_NA(in_arrs, is_NAs, [o1, o2])
        assert False
    except PatsyError, e:
        assert e.origin is o2

########NEW FILE########
__FILENAME__ = origin
# This file is part of Patsy
# Copyright (C) 2011-2012 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# The core 'origin' tracking system. This point of this is to have machinery
# so if some object is ultimately derived from some portion of a string (e.g.,
# a formula), then we can keep track of that, and use it to give proper error
# messages.

# These are made available in the patsy.* namespace
__all__ = ["Origin"]

class Origin(object):
    """This represents the origin of some object in some string.

    For example, if we have an object ``x1_obj`` that was produced by parsing
    the ``x1`` in the formula ``"y ~ x1:x2"``, then we conventionally keep
    track of that relationship by doing::

      x1_obj.origin = Origin("y ~ x1:x2", 4, 6)

    Then later if we run into a problem, we can do::

      raise PatsyError("invalid factor", x1_obj)

    and we'll produce a nice error message like::

      PatsyError: invalid factor
          y ~ x1:x2
              ^^

    Origins are compared by value, and hashable.
    """

    def __init__(self, code, start, end):
        self.code = code
        self.start = start
        self.end = end

    @classmethod
    def combine(cls, origin_objs):
        """Class method for combining a set of Origins into one large Origin
        that spans them.

        Example usage: if we wanted to represent the origin of the "x1:x2"
        term, we could do ``Origin.combine([x1_obj, x2_obj])``.

        Single argument is an iterable, and each element in the iterable
        should be either:

        * An Origin object
        * ``None``
        * An object that has a ``.origin`` attribute which fulfills the above
          criteria.
          
        Returns either an Origin object, or None.
        """
        origins = []
        for obj in origin_objs:
            if obj is not None and not isinstance(obj, Origin):
                obj = obj.origin
            if obj is None:
                continue
            origins.append(obj)
        if not origins:
            return None
        codes = set([o.code for o in origins])
        assert len(codes) == 1
        start = min([o.start for o in origins])
        end = max([o.end for o in origins])
        return cls(codes.pop(), start, end)

    def relevant_code(self):
        """Extracts and returns the span of the original code represented by
        this Origin. Example: ``x1``."""
        return self.code[self.start:self.end]

    def __eq__(self, other):
        return (isinstance(other, Origin)
                and self.code == other.code
                and self.start == other.start
                and self.end == other.end)

    def __ne__(self, other):
        return not self == other

    def __hash__(self):
        return hash((Origin, self.code, self.start, self.end))

    def caretize(self, indent=0):
        """Produces a user-readable two line string indicating the origin of
        some code. Example::

          y ~ x1:x2
              ^^

        If optional argument 'indent' is given, then both lines will be
        indented by this much. The returned string does not have a trailing
        newline.
        """
        return ("%s%s\n%s%s%s" 
                % (" " * indent,
                   self.code,
                   " " * indent,
                   " " * self.start,
                   "^" * (self.end - self.start)))

    def __repr__(self):
        return "<Origin %s->%s<-%s (%s-%s)>" % (
            self.code[:self.start],
            self.code[self.start:self.end],
            self.code[self.end:],
            self.start, self.end)

def test_Origin():
    o1 = Origin("012345", 2, 4)
    o2 = Origin("012345", 4, 5)
    assert o1.caretize() == "012345\n  ^^"
    assert o2.caretize() == "012345\n    ^"
    o3 = Origin.combine([o1, o2])
    assert o3.code == "012345"
    assert o3.start == 2
    assert o3.end == 5
    assert o3.caretize(indent=2) == "  012345\n    ^^^"
    assert o3 == Origin("012345", 2, 5)

    class ObjWithOrigin(object):
        def __init__(self, origin=None):
            self.origin = origin
    o4 = Origin.combine([ObjWithOrigin(o1), ObjWithOrigin(), None])
    assert o4 == o1
    o5 = Origin.combine([ObjWithOrigin(o1), o2])
    assert o5 == o3

    assert Origin.combine([ObjWithOrigin(), ObjWithOrigin()]) is None

########NEW FILE########
__FILENAME__ = parse_formula
 # This file is part of Patsy
# Copyright (C) 2011 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# This file defines a parser for a simple language based on S/R "formulas"
# (which are described in sections 2.3 and 2.4 in Chambers & Hastie, 1992). It
# uses the machinery in patsy.parse_core to do the heavy-lifting -- its
# biggest job is to handle tokenization.

__all__ = ["parse_formula"]

from cStringIO import StringIO
# The Python tokenizer
import tokenize

from patsy import PatsyError
from patsy.origin import Origin
from patsy.infix_parser import Token, Operator, infix_parse, ParseNode
from patsy.tokens import python_tokenize, pretty_untokenize
from patsy.util import PushbackAdapter

_atomic_token_types = ["PYTHON_EXPR", "ZERO", "ONE", "NUMBER"]

def _is_a(f, v):
    try:
        f(v)
    except ValueError:
        return False
    else:
        return True

# Helper function for _tokenize_formula:
def _read_python_expr(it, end_tokens):
    # Read out a full python expression, stopping when we hit an
    # unnested end token.
    pytypes = []
    token_strings = []
    origins = []
    bracket_level = 0
    for pytype, token_string, origin in it:
        assert bracket_level >= 0
        if bracket_level == 0 and token_string in end_tokens:
            it.push_back((pytype, token_string, origin))
            break
        if token_string in ("(", "[", "{"):
            bracket_level += 1
        if token_string in (")", "]", "}"):
            bracket_level -= 1
        if bracket_level < 0:
            raise PatsyError("unmatched close bracket", origin)
        pytypes.append(pytype)
        token_strings.append(token_string)
        origins.append(origin)
    # Either we found an end_token, or we hit the end of the string
    if bracket_level == 0:
        expr_text = pretty_untokenize(zip(pytypes, token_strings))
        if expr_text == "0":
            token_type = "ZERO"
        elif expr_text == "1":
            token_type = "ONE"
        elif _is_a(int, expr_text) or _is_a(float, expr_text):
            token_type = "NUMBER"
        else:
            token_type = "PYTHON_EXPR"
        return Token(token_type, Origin.combine(origins), extra=expr_text)
    else:
        raise PatsyError("unclosed bracket in embedded Python "
                            "expression",
                            Origin.combine(origins))

def _tokenize_formula(code, operator_strings):
    assert "(" not in operator_strings
    assert ")" not in operator_strings
    magic_token_types = {"(": Token.LPAREN,
                         ")": Token.RPAREN,
                         }
    for operator_string in operator_strings:
        magic_token_types[operator_string] = operator_string
    # Once we enter a Python expression, a ( does not end it, but any other
    # "magic" token does:
    end_tokens = set(magic_token_types)
    end_tokens.remove("(")
    
    it = PushbackAdapter(python_tokenize(code))
    for pytype, token_string, origin in it:
        if token_string in magic_token_types:
            yield Token(magic_token_types[token_string], origin)
        else:
            it.push_back((pytype, token_string, origin))
            yield _read_python_expr(it, end_tokens)
                    
def test__tokenize_formula():
    code = "y ~ a + (foo(b,c +   2)) + -1 + 0 + 10"
    tokens = list(_tokenize_formula(code, ["+", "-", "~"]))
    expecteds = [("PYTHON_EXPR", Origin(code, 0, 1), "y"),
                 ("~", Origin(code, 2, 3), None),
                 ("PYTHON_EXPR", Origin(code, 4, 5), "a"),
                 ("+", Origin(code, 6, 7), None),
                 (Token.LPAREN, Origin(code, 8, 9), None),
                 ("PYTHON_EXPR", Origin(code, 9, 23), "foo(b, c + 2)"),
                 (Token.RPAREN, Origin(code, 23, 24), None),
                 ("+", Origin(code, 25, 26), None),
                 ("-", Origin(code, 27, 28), None),
                 ("ONE", Origin(code, 28, 29), "1"),
                 ("+", Origin(code, 30, 31), None),
                 ("ZERO", Origin(code, 32, 33), "0"),
                 ("+", Origin(code, 34, 35), None),
                 ("NUMBER", Origin(code, 36, 38), "10"),
                 ]
    for got, expected in zip(tokens, expecteds):
        assert isinstance(got, Token)
        assert got.type == expected[0]
        assert got.origin == expected[1]
        assert got.extra == expected[2]

_unary_tilde = Operator("~", 1, -100)
_default_ops = [
    _unary_tilde,
    Operator("~", 2, -100),

    Operator("+", 2, 100),
    Operator("-", 2, 100),
    Operator("*", 2, 200),
    Operator("/", 2, 200),
    Operator(":", 2, 300),
    Operator("**", 2, 500),

    Operator("+", 1, 100),
    Operator("-", 1, 100),
]

def parse_formula(code, extra_operators=[]):
    if not code.strip():
        code = "~ 1"

    for op in extra_operators:
        if op.precedence < 0:
            raise ValueError, "all operators must have precedence >= 0"

    operators = _default_ops + extra_operators
    operator_strings = [op.token_type for op in operators]
    tree = infix_parse(_tokenize_formula(code, operator_strings),
                       operators,
                       _atomic_token_types)
    if not isinstance(tree, ParseNode) or tree.type != "~":
        tree = ParseNode("~", None, [tree], tree.origin)
    return tree

#############

_parser_tests = {
    "": ["~", "1"],
    " ": ["~", "1"],
    " \n ": ["~", "1"],

    "1": ["~", "1"],
    "a": ["~", "a"],
    "a ~ b": ["~", "a", "b"],

    "(a ~ b)": ["~", "a", "b"],
    "a ~ ((((b))))": ["~", "a", "b"],
    "a ~ ((((+b))))": ["~", "a", ["+", "b"]],

    "a + b + c": ["~", ["+", ["+", "a", "b"], "c"]],
    "a + (b ~ c) + d": ["~", ["+", ["+", "a", ["~", "b", "c"]], "d"]],

    "a + np.log(a, base=10)": ["~", ["+", "a", "np.log(a, base=10)"]],
    # Note different spacing:
    "a + np . log(a , base = 10)": ["~", ["+", "a", "np.log(a, base=10)"]],

    # Check precedence
    "a + b ~ c * d": ["~", ["+", "a", "b"], ["*", "c", "d"]],
    "a + b * c": ["~", ["+", "a", ["*", "b", "c"]]],
    "-a**2": ["~", ["-", ["**", "a", "2"]]],
    "-a:b": ["~", ["-", [":", "a", "b"]]],
    "a + b:c": ["~", ["+", "a", [":", "b", "c"]]],
    "(a + b):c": ["~", [":", ["+", "a", "b"], "c"]],
    "a*b:c": ["~", ["*", "a", [":", "b", "c"]]],

    "a+b / c": ["~", ["+", "a", ["/", "b", "c"]]],
    "~ a": ["~", "a"],

    "-1": ["~", ["-", "1"]],
    }

def _compare_trees(got, expected):
    assert isinstance(got, ParseNode)
    if got.args:
        assert got.type == expected[0]
        for arg, expected_arg in zip(got.args, expected[1:]):
            _compare_trees(arg, expected_arg)
    else:
        assert got.type in _atomic_token_types
        assert got.token.extra == expected

def _do_parse_test(test_cases, extra_operators):
    for code, expected in test_cases.iteritems():
        actual = parse_formula(code, extra_operators=extra_operators)
        print repr(code), repr(expected)
        print actual
        _compare_trees(actual, expected)

def test_parse_formula():
    _do_parse_test(_parser_tests, [])

def test_parse_origin():
    tree = parse_formula("a ~ b + c")
    assert tree.origin == Origin("a ~ b + c", 0, 9)
    assert tree.token.origin == Origin("a ~ b + c", 2, 3)
    assert tree.args[0].origin == Origin("a ~ b + c", 0, 1)
    assert tree.args[1].origin == Origin("a ~ b + c", 4, 9)
    assert tree.args[1].token.origin == Origin("a ~ b + c", 6, 7)
    assert tree.args[1].args[0].origin == Origin("a ~ b + c", 4, 5)
    assert tree.args[1].args[1].origin == Origin("a ~ b + c", 8, 9)

# <> mark off where the error should be reported:
_parser_error_tests = [
    "a <+>",
    "a + <(>",

    "a + b <# asdf>",

    "<)>",
    "a + <)>",
    "<*> a",
    "a + <*>",

    "a + <foo[bar>",
    "a + <foo{bar>",
    "a + <foo(bar>",

    "a + <[bar>",
    "a + <{bar>",

    "a + <{bar[]>",

    "a + foo<]>bar",
    "a + foo[]<]>bar",
    "a + foo{}<}>bar",
    "a + foo<)>bar",

    "a + b<)>",
    "(a) <.>",

    "<(>a + b",

    "a +< >'foo", # Not the best placement for the error
]

# Split out so it can also be used by tests of the evaluator (which also
# raises PatsyError's)
def _parsing_error_test(parse_fn, error_descs): # pragma: no cover
    for error_desc in error_descs:
        letters = []
        start = None
        end = None
        for letter in error_desc:
            if letter == "<":
                start = len(letters)
            elif letter == ">":
                end = len(letters)
            else:
                letters.append(letter)
        bad_code = "".join(letters)
        assert start is not None and end is not None
        print error_desc
        print repr(bad_code), start, end
        try:
            parse_fn(bad_code)
        except PatsyError, e:
            print e
            assert e.origin.code == bad_code
            assert e.origin.start == start
            assert e.origin.end == end
        else:
            assert False, "parser failed to report an error!"

def test_parse_errors(extra_operators=[]):
    def parse_fn(code):
        return parse_formula(code, extra_operators=extra_operators)
    _parsing_error_test(parse_fn, _parser_error_tests)

_extra_op_parser_tests = {
    "a | b": ["~", ["|", "a", "b"]],
    "a * b|c": ["~", ["*", "a", ["|", "b", "c"]]],
    }

def test_parse_extra_op():
    extra_operators = [Operator("|", 2, 250)]
    _do_parse_test(_parser_tests,
                   extra_operators=extra_operators)
    _do_parse_test(_extra_op_parser_tests,
                   extra_operators=extra_operators)
    test_parse_errors(extra_operators=extra_operators)

########NEW FILE########
__FILENAME__ = redundancy
# This file is part of Patsy
# Copyright (C) 2011 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# This file has the code that figures out how each factor in some given Term
# should be coded. This is complicated by dealing with models with categorical
# factors like:
#   1 + a + a:b
# then technically 'a' (which represents the space of vectors that can be
# produced as linear combinations of the dummy coding of the levels of the
# factor a) is collinear with the intercept, and 'a:b' (which represents the
# space of vectors that can be produced as linear combinations of the dummy
# coding *of a new factor whose levels are the cartesian product of a and b)
# is collinear with both 'a' and the intercept.
#
# In such a case, the rule is that we find some way to code each term so that
# the full space of vectors that it represents *is present in the model* BUT
# there is no collinearity between the different terms. In effect, we have to
# choose a set of vectors that spans everything that that term wants to span,
# *except* that part of the vector space which was already spanned by earlier
# terms.

# How? We replace each term with the set of "subterms" that it covers, like
# so:
#   1 -> ()
#   a -> (), a-
#   a:b -> (), a-, b-, a-:b-
# where "-" means "coded so as not to span the intercept". So that example
# above expands to
#   [()] + [() + a-] + [() + a- + b- + a-:b-]
# so we go through from left to right, and for each term we:
#   1) toss out all the subterms that have already been used (this is a simple
#      equality test, no magic)
#   2) simplify the terms that are left, according to rules like
#        () + a- = a+
#      (here + means, "coded to span the intercept")
#   3) use the resulting subterm list as our coding for this term!
# So in the above, we go:
#   (): stays the same, coded as intercept
#   () + a-: reduced to just a-, which is what we code
#   () + a- + b- + a-:b-: reduced to b- + a-:b-, which is simplified to a+:b-.

# This should really be a named tuple, but those don't exist until Python
# 2.6...
class _ExpandedFactor(object):
    """A factor, with an additional annotation for whether it is coded
    full-rank (includes_intercept=True) or not.

    These objects are treated as immutable."""
    def __init__(self, includes_intercept, factor):
        self.includes_intercept = includes_intercept
        self.factor = factor

    def __hash__(self):
        return hash((_ExpandedFactor, self.includes_intercept, self.factor))

    def __eq__(self, other):
        return (isinstance(other, _ExpandedFactor)
                and other.includes_intercept == self.includes_intercept
                and other.factor == self.factor)

    def __ne__(self, other):
        return not self == other

    def __repr__(self):
        if self.includes_intercept:
            suffix = "+"
        else:
            suffix = "-"
        return "%r%s" % (self.factor, suffix)

class _Subterm(object):
    "Also immutable."
    def __init__(self, efactors):
        self.efactors = frozenset(efactors)

    def can_absorb(self, other):
        # returns True if 'self' is like a-:b-, and 'other' is like a-
        return (len(self.efactors) - len(other.efactors) == 1
                and self.efactors.issuperset(other.efactors))

    def absorb(self, other):
        diff = self.efactors.difference(other.efactors)
        assert len(diff) == 1
        efactor = list(diff)[0]
        assert not efactor.includes_intercept
        new_factors = set(other.efactors)
        new_factors.add(_ExpandedFactor(True, efactor.factor))
        return _Subterm(new_factors)

    def __hash__(self):
        return hash((_Subterm, self.efactors))

    def __eq__(self, other):
        return (isinstance(other, _Subterm)
                and self.efactors == self.efactors)

    def __ne__(self, other):
        return not self == other

    def __repr__(self):
        return "%s(%r)" % (self.__class__.__name__, list(self.efactors))

# For testing: takes a shorthand description of a list of subterms like
#   [(), ("a-",), ("a-", "b+")]
# and expands it into a list of _Subterm and _ExpandedFactor objects.
def _expand_test_abbrevs(short_subterms):
    subterms = []
    for subterm in short_subterms:
        factors = []
        for factor_name in subterm:
            assert factor_name[-1] in ("+", "-")
            factors.append(_ExpandedFactor(factor_name[-1] == "+",
                                           factor_name[:-1]))
        subterms.append(_Subterm(factors))
    return subterms

def test__Subterm():
    s_ab = _expand_test_abbrevs([["a-", "b-"]])[0]
    s_abc = _expand_test_abbrevs([["a-", "b-", "c-"]])[0]
    s_null = _expand_test_abbrevs([[]])[0]
    s_cd = _expand_test_abbrevs([["c-", "d-"]])[0]
    s_a = _expand_test_abbrevs([["a-"]])[0]
    s_ap = _expand_test_abbrevs([["a+"]])[0]
    s_abp = _expand_test_abbrevs([["a-", "b+"]])[0]
    for bad in s_abc, s_null, s_cd, s_ap, s_abp:
        assert not s_ab.can_absorb(bad)
    assert s_ab.can_absorb(s_a)
    assert s_ab.absorb(s_a) == s_abp

# Importantly, this preserves the order of the input. Both the items inside
# each subset are in the order they were in the original tuple, and the tuples
# are emitted so that they're sorted with respect to their elements position
# in the original tuple.
def _subsets_sorted(tupl):
    def helper(seq):
        if not seq:
            yield ()
        else:
            obj = seq[0]
            for subset in _subsets_sorted(seq[1:]):
                yield subset
                yield (obj,) + subset
    # Transform each obj -> (idx, obj) tuple, so that we can later sort them
    # by their position in the original list.
    expanded = list(enumerate(tupl))
    expanded_subsets = list(helper(expanded))
    # This exploits Python's stable sort: we want short before long, and ties
    # broken by natural ordering on the (idx, obj) entries in each subset. So
    # we sort by the latter first, then by the former.
    expanded_subsets.sort()
    expanded_subsets.sort(key=len)
    # And finally, we strip off the idx's:
    for subset in expanded_subsets:
        yield tuple([obj for (idx, obj) in subset])
    
def test__subsets_sorted():
    assert list(_subsets_sorted((1, 2))) == [(), (1,), (2,), (1, 2)]
    assert (list(_subsets_sorted((1, 2, 3)))
            == [(), (1,), (2,), (3,), (1, 2), (1, 3), (2, 3), (1, 2, 3)])
    assert len(list(_subsets_sorted(range(5)))) == 2 ** 5

def _simplify_one_subterm(subterms):
    # We simplify greedily from left to right.
    # Returns True if succeeded, False otherwise
    for short_i, short_subterm in enumerate(subterms):
        for long_i, long_subterm in enumerate(subterms[short_i + 1:]):
            if long_subterm.can_absorb(short_subterm):
                new_subterm = long_subterm.absorb(short_subterm)
                subterms[short_i + 1 + long_i] = new_subterm
                subterms.pop(short_i)
                return True
    return False
            
def _simplify_subterms(subterms):
    while _simplify_one_subterm(subterms):
        pass

def test__simplify_subterms():
    def t(given, expected):
        given = _expand_test_abbrevs(given)
        expected = _expand_test_abbrevs(expected)
        print "testing if:", given, "->", expected
        _simplify_subterms(given)
        assert given == expected
    t([("a-",)], [("a-",)])
    t([(), ("a-",)], [("a+",)])
    t([(), ("a-",), ("b-",), ("a-", "b-")], [("a+", "b+")])
    t([(), ("a-",), ("a-", "b-")], [("a+",), ("a-", "b-")])
    t([("a-",), ("b-",), ("a-", "b-")], [("b-",), ("a-", "b+")])

# 'term' is a Term
# 'numeric_factors' is any set-like object which lists the
#   numeric/non-categorical factors in this term. Such factors are just
#   ignored by this routine.
# 'used_subterms' is a set which records which subterms have previously been
#   used. E.g., a:b has subterms (), a, b, a:b, and if we're processing
#    y ~ a + a:b
#   then by the time we reach a:b, the () and a subterms will have already
#   been used. This is an in/out argument, and should be treated as opaque by
#   callers -- really it is a way for multiple invocations of this routine to
#   talk to each other. Each time it is called, this routine adds the subterms
#   of each factor to this set in place. So the first time this routine is
#   called, pass in an empty set, and then just keep passing the same set to
#   any future calls.
# Returns: a list of dicts. Each dict maps from factors to booleans. The
# coding for the given term should use a full-rank contrast for those factors
# which map to True, a (n-1)-rank contrast for those factors which map to
# False, and any factors which are not mentioned are numeric and should be
# added back in. These dicts should add columns to the design matrix from left
# to right.
def pick_contrasts_for_term(term, numeric_factors, used_subterms):
    categorical_factors = [f for f in term.factors if f not in numeric_factors]
    # Converts a term into an expanded list of subterms like:
    #   a:b  ->  1 + a- + b- + a-:b-
    # and discards the ones that have already been used.
    subterms = []
    for subset in _subsets_sorted(categorical_factors):
        subterm = _Subterm([_ExpandedFactor(False, f) for f in subset])
        if subterm not in used_subterms:
            subterms.append(subterm)
    used_subterms.update(subterms)
    _simplify_subterms(subterms)
    factor_codings = []
    for subterm in subterms:
        factor_coding = {}
        for expanded in subterm.efactors:
            factor_coding[expanded.factor] = expanded.includes_intercept
        factor_codings.append(factor_coding)
    return factor_codings

def test_pick_contrasts_for_term():
    from patsy.desc import Term
    used = set()
    codings = pick_contrasts_for_term(Term([]), set(), used)
    assert codings == [{}]
    codings = pick_contrasts_for_term(Term(["a", "x"]), set(["x"]), used)
    assert codings == [{"a": False}]
    codings = pick_contrasts_for_term(Term(["a", "b"]), set(), used)
    assert codings == [{"a": True, "b": False}]
    used_snapshot = set(used)
    codings = pick_contrasts_for_term(Term(["c", "d"]), set(), used)
    assert codings == [{"d": False}, {"c": False, "d": True}]
    # Do it again backwards, to make sure we're deterministic with respect to
    # order:
    codings = pick_contrasts_for_term(Term(["d", "c"]), set(), used_snapshot)
    assert codings == [{"c": False}, {"c": True, "d": False}]

########NEW FILE########
__FILENAME__ = splines
# This file is part of Patsy
# Copyright (C) 2012-2013 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# R-compatible spline basis functions

# These are made available in the patsy.* namespace
__all__ = ["bs"]

import numpy as np

from patsy.util import have_pandas
from patsy.state import stateful_transform

if have_pandas:
    import pandas

def _eval_bspline_basis(x, knots, degree):
    try:
        from scipy.interpolate import splev
    except ImportError: # pragma: no cover
        raise ImportError("spline functionality requires scipy")
    # 'knots' are assumed to be already pre-processed. E.g. usually you
    # want to include duplicate copies of boundary knots; you should do
    # that *before* calling this constructor.
    knots = np.atleast_1d(np.asarray(knots, dtype=float))
    assert knots.ndim == 1
    knots.sort()
    degree = int(degree)
    x = np.atleast_1d(x)
    if x.ndim == 2 and x.shape[1] == 1:
        x = x[:, 0]
    assert x.ndim == 1
    # XX FIXME: when points fall outside of the boundaries, splev and R seem
    # to handle them differently. I don't know why yet. So until we understand
    # this and decide what to do with it, I'm going to play it safe and
    # disallow such points.
    if np.min(x) < np.min(knots) or np.max(x) > np.max(knots):
        raise NotImplementedError("some data points fall outside the "
                                  "outermost knots, and I'm not sure how "
                                  "to handle them. (Patches accepted!)")
    # Thanks to Charles Harris for explaining splev. It's not well
    # documented, but basically it computes an arbitrary b-spline basis
    # given knots and degree on some specificed points (or derivatives
    # thereof, but we don't use that functionality), and then returns some
    # linear combination of these basis functions. To get out the basis
    # functions themselves, we use linear combinations like [1, 0, 0], [0,
    # 1, 0], [0, 0, 1].
    # NB: This probably makes it rather inefficient (though I haven't checked
    # to be sure -- maybe the fortran code actually skips computing the basis
    # function for coefficients that are zero).
    # Note: the order of a spline is the same as its degree + 1.
    # Note: there are (len(knots) - order) basis functions.
    n_bases = len(knots) - (degree + 1)
    basis = np.empty((x.shape[0], n_bases), dtype=float)
    for i in xrange(n_bases):
        coefs = np.zeros((n_bases,))
        coefs[i] = 1
        basis[:, i] = splev(x, (knots, coefs, degree))
    return basis

def _R_compat_quantile(x, probs):
    #return np.percentile(x, 100 * np.asarray(probs))
    probs = np.asarray(probs)
    quantiles = np.asarray([np.percentile(x, 100 * prob)
                            for prob in probs.ravel(order="C")])
    return quantiles.reshape(probs.shape, order="C")

def test__R_compat_quantile():
    def t(x, prob, expected):
        assert np.allclose(_R_compat_quantile(x, prob), expected)
    t([10, 20], 0.5, 15)
    t([10, 20], 0.3, 13)
    t([10, 20], [0.3, 0.7], [13, 17])
    t(range(10), [0.3, 0.7], [2.7, 6.3])

class BS(object):
    """bs(x, df=None, knots=None, degree=3, include_intercept=False, lower_bound=None, upper_bound=None)

    Generates a B-spline basis for ``x``, allowing non-linear fits. The usual
    usage is something like::

      y ~ 1 + bs(x, 4)

    to fit ``y`` as a smooth function of ``x``, with 4 degrees of freedom
    given to the smooth.

    :arg df: The number of degrees of freedom to use for this spline. The
      return value will have this many columns. You must specify at least one
      of ``df`` and ``knots``.
    :arg knots: The interior knots to use for the spline. If unspecified, then
      equally spaced quantiles of the input data are used. You must specify at
      least one of ``df`` and ``knots``.
    :arg degree: The degree of the spline to use.
    :arg include_intercept: If ``True``, then the resulting
      spline basis will span the intercept term (i.e., the constant
      function). If ``False`` (the default) then this will not be the case,
      which is useful for avoiding overspecification in models that include
      multiple spline terms and/or an intercept term.
    :arg lower_bound: The lower exterior knot location.
    :arg upper_bound: The upper exterior knot location.

    A spline with ``degree=0`` is piecewise constant with breakpoints at each
    knot, and the default knot positions are quantiles of the input. So if you
    find yourself in the situation of wanting to quantize a continuous
    variable into ``num_bins`` equal-sized bins with a constant effect across
    each bin, you can use ``bs(x, num_bins - 1, degree=0)``. (The ``- 1`` is
    because one degree of freedom will be taken by the intercept;
    alternatively, you could leave the intercept term out of your model and
    use ``bs(x, num_bins, degree=0, include_intercept=True)``.

    A spline with ``degree=1`` is piecewise linear with breakpoints at each
    knot.

    The default is ``degree=3``, which gives a cubic b-spline.

    This is a stateful transform (for details see
    :ref:`stateful-transforms`). If ``knots``, ``lower_bound``, or
    ``upper_bound`` are not specified, they will be calculated from the data
    and then the chosen values will be remembered and re-used for prediction
    from the fitted model.

    Using this function requires scipy be installed.

    .. note:: This function is very similar to the R function of the same
      name. In cases where both return output at all (e.g., R's ``bs`` will
      raise an error if ``degree=0``, while patsy's will not), they should
      produce identical output given identical input and parameter settings.

    .. warning:: I'm not sure on what the proper handling of points outside
      the lower/upper bounds is, so for now attempting to evaluate a spline
      basis at such points produces an error. Patches gratefully accepted.

    .. versionadded:: 0.2.0
    """
    def __init__(self):
        self._tmp = {}
        self._degree = None
        self._all_knots = None

    def memorize_chunk(self, x, df=None, knots=None, degree=3,
                       include_intercept=False,
                       lower_bound=None, upper_bound=None):
        args = {"df": df,
                "knots": knots,
                "degree": degree,
                "include_intercept": include_intercept,
                "lower_bound": lower_bound,
                "upper_bound": upper_bound,
                }
        self._tmp["args"] = args
        # XX: check whether we need x values before saving them
        x = np.atleast_1d(x)
        if x.ndim == 2 and x.shape[1] == 1:
            x = x[:, 0]
        if x.ndim > 1:
            raise ValueError("input to 'bs' must be 1-d, "
                             "or a 2-d column vector")
        # There's no better way to compute exact quantiles than memorizing
        # all data.
        self._tmp.setdefault("xs", []).append(x)

    def memorize_finish(self):
        tmp = self._tmp
        args = tmp["args"]
        del self._tmp

        if args["degree"] < 0:
            raise ValueError("degree must be greater than 0 (not %r)"
                             % (args["degree"],))
        if int(args["degree"]) != args["degree"]:
            raise ValueError("degree must be an integer (not %r)"
                             % (self._degree,))

        # These are guaranteed to all be 1d vectors by the code above
        x = np.concatenate(tmp["xs"])
        if args["df"] is None and args["knots"] is None:
            raise ValueError("must specify either df or knots")
        order = args["degree"] + 1
        if args["df"] is not None:
            n_inner_knots = args["df"] - order
            if not args["include_intercept"]:
                n_inner_knots += 1
            if n_inner_knots < 0:
                raise ValueError("df=%r is too small for degree=%r and "
                                 "include_intercept=%r; must be >= %s"
                                 % (args["df"], args["degree"],
                                    args["include_intercept"],
                                    # We know that n_inner_knots is negative;
                                    # if df were that much larger, it would
                                    # have been zero, and things would work.
                                    args["df"] - n_inner_knots))
            if args["knots"] is not None:
                if len(args["knots"]) != n_inner_knots:
                    raise ValueError("df=%s with degree=%r implies %s knots, "
                                     "but %s knots were provided"
                                     % (args["df"], args["degree"],
                                        n_inner_knots, len(args["knots"])))
            else:
                # Need to compute inner knots
                knot_quantiles = np.linspace(0, 1, n_inner_knots + 2)[1:-1]
                inner_knots = _R_compat_quantile(x, knot_quantiles)
        if args["knots"] is not None:
            inner_knots = args["knots"]
        if args["lower_bound"] is not None:
            lower_bound = args["lower_bound"]
        else:
            lower_bound = np.min(x)
        if args["upper_bound"] is not None:
            upper_bound = args["upper_bound"]
        else:
            upper_bound = np.max(x)
        if lower_bound > upper_bound:
            raise ValueError("lower_bound > upper_bound (%r > %r)"
                             % (lower_bound, upper_bound))
        inner_knots = np.asarray(inner_knots)
        if inner_knots.ndim > 1:
            raise ValueError("knots must be 1 dimensional")
        if np.any(inner_knots < lower_bound):
            raise ValueError("some knot values (%s) fall below lower bound "
                             "(%r)"
                             % (inner_knots[inner_knots < lower_bound],
                                lower_bound))
        if np.any(inner_knots > upper_bound):
            raise ValueError("some knot values (%s) fall above upper bound "
                             "(%r)"
                             % (inner_knots[inner_knots > upper_bound],
                                upper_bound))
        all_knots = np.concatenate(([lower_bound, upper_bound] * order,
                                    inner_knots))
        all_knots.sort()

        self._degree = args["degree"]
        self._all_knots = all_knots

    def transform(self, x, df=None, knots=None, degree=3,
                  include_intercept=False,
                  lower_bound=None, upper_bound=None):
        basis = _eval_bspline_basis(x, self._all_knots, self._degree)
        if not include_intercept:
            basis = basis[:, 1:]
        if have_pandas:
            if isinstance(x, (pandas.Series, pandas.DataFrame)):
                basis = pandas.DataFrame(basis)
                basis.index = x.index
        return basis

bs = stateful_transform(BS)

def test_bs_compat():
    from patsy.test_state import check_stateful
    from patsy.test_splines_bs_data import (R_bs_test_x,
                                            R_bs_test_data,
                                            R_bs_num_tests)
    lines = R_bs_test_data.split("\n")
    tests_ran = 0
    start_idx = lines.index("--BEGIN TEST CASE--")
    while True:
        if not lines[start_idx] == "--BEGIN TEST CASE--":
            break
        start_idx += 1
        stop_idx = lines.index("--END TEST CASE--", start_idx)
        block = lines[start_idx:stop_idx]
        test_data = {}
        for line in block:
            key, value = line.split("=", 1)
            test_data[key] = value
        # Translate the R output into Python calling conventions
        kwargs = {
            "degree": int(test_data["degree"]),
            # integer, or None
            "df": eval(test_data["df"]),
            # np.array() call, or None
            "knots": eval(test_data["knots"]),
            }
        if test_data["Boundary.knots"] != "None":
            lower, upper = eval(test_data["Boundary.knots"])
            kwargs["lower_bound"] = lower
            kwargs["upper_bound"] = upper
        kwargs["include_intercept"] = (test_data["intercept"] == "TRUE")
        # Special case: in R, setting intercept=TRUE increases the effective
        # dof by 1. Adjust our arguments to match.
        # if kwargs["df"] is not None and kwargs["include_intercept"]:
        #     kwargs["df"] += 1
        output = np.asarray(eval(test_data["output"]))
        if kwargs["df"] is not None:
            assert output.shape[1] == kwargs["df"]
        # Do the actual test
        check_stateful(BS, False, R_bs_test_x, output, **kwargs)
        tests_ran += 1
        # Set up for the next one
        start_idx = stop_idx + 1
    assert tests_ran == R_bs_num_tests

test_bs_compat.slow = 1

# This isn't checked by the above, because R doesn't have zero degree
# b-splines.
def test_bs_0degree():
    x = np.logspace(-1, 1, 10)
    result = bs(x, knots=[1, 4], degree=0, include_intercept=True)
    assert result.shape[1] == 3
    expected_0 = np.zeros(10)
    expected_0[x < 1] = 1
    assert np.array_equal(result[:, 0], expected_0)
    expected_1 = np.zeros(10)
    expected_1[(x >= 1) & (x < 4)] = 1
    assert np.array_equal(result[:, 1], expected_1)
    expected_2 = np.zeros(10)
    expected_2[x >= 4] = 1
    assert np.array_equal(result[:, 2], expected_2)
    # Check handling of points that exactly fall on knots. They arbitrarily
    # get included into the larger region, not the smaller. This is consistent
    # with Python's half-open interval convention -- each basis function is
    # constant on [knot[i], knot[i + 1]).
    assert np.array_equal(bs([0, 1, 2], degree=0, knots=[1],
                             include_intercept=True),
                          [[1, 0],
                           [0, 1],
                           [0, 1]])

    result_int = bs(x, knots=[1, 4], degree=0, include_intercept=True)
    result_no_int = bs(x, knots=[1, 4], degree=0, include_intercept=False)
    assert np.array_equal(result_int[:, 1:], result_no_int)

def test_bs_errors():
    from nose.tools import assert_raises
    x = np.linspace(-10, 10, 20)
    # error checks:
    # out of bounds
    assert_raises(NotImplementedError, bs, x, 3, lower_bound=0)
    assert_raises(NotImplementedError, bs, x, 3, upper_bound=0)
    # must specify df or knots
    assert_raises(ValueError, bs, x)
    # df/knots match/mismatch (with and without intercept)
    #   match:
    bs(x, df=10, include_intercept=False, knots=[0] * 7)
    bs(x, df=10, include_intercept=True, knots=[0] * 6)
    bs(x, df=10, include_intercept=False, knots=[0] * 9, degree=1)
    bs(x, df=10, include_intercept=True, knots=[0] * 8, degree=1)
    #   too many knots:
    assert_raises(ValueError,
                  bs, x, df=10, include_intercept=False, knots=[0] * 8)
    assert_raises(ValueError,
                  bs, x, df=10, include_intercept=True, knots=[0] * 7)
    assert_raises(ValueError,
                  bs, x, df=10, include_intercept=False, knots=[0] * 10,
                  degree=1)
    assert_raises(ValueError,
                  bs, x, df=10, include_intercept=True, knots=[0] * 9,
                  degree=1)
    #   too few knots:
    assert_raises(ValueError,
                  bs, x, df=10, include_intercept=False, knots=[0] * 6)
    assert_raises(ValueError,
                  bs, x, df=10, include_intercept=True, knots=[0] * 5)
    assert_raises(ValueError,
                  bs, x, df=10, include_intercept=False, knots=[0] * 8,
                  degree=1)
    assert_raises(ValueError,
                  bs, x, df=10, include_intercept=True, knots=[0] * 7,
                  degree=1)
    # df too small
    assert_raises(ValueError,
                  bs, x, df=1, degree=3)
    assert_raises(ValueError,
                  bs, x, df=3, degree=5)
    # bad degree
    assert_raises(ValueError,
                  bs, x, df=10, degree=-1)
    assert_raises(ValueError,
                  bs, x, df=10, degree=1.5)
    # upper_bound < lower_bound
    assert_raises(ValueError,
                  bs, x, 3, lower_bound=1, upper_bound=-1)
    # multidimensional input
    assert_raises(ValueError,
                  bs, np.column_stack((x, x)), 3)
    # unsorted knots are okay, and get sorted
    assert np.array_equal(bs(x, knots=[1, 4]), bs(x, knots=[4, 1]))
    # 2d knots
    assert_raises(ValueError,
                  bs, x, knots=[[0], [20]])
    # knots > upper_bound
    assert_raises(ValueError,
                  bs, x, knots=[0, 20])
    assert_raises(ValueError,
                  bs, x, knots=[0, 4], upper_bound=3)
    # knots < lower_bound
    assert_raises(ValueError,
                  bs, x, knots=[-20, 0])
    assert_raises(ValueError,
                  bs, x, knots=[-4, 0], lower_bound=-3)



# differences between bs and ns (since the R code is a pile of copy-paste):
# - degree is always 3
# - different number of interior knots given df (b/c fewer dof used at edges I
#   guess)
# - boundary knots always repeated exactly 4 times (same as bs with degree=3)
# - complications at the end to handle boundary conditions
# the 'rcs' function uses slightly different conventions -- in particular it
# picks boundary knots that are not quite at the edges of the data, which
# makes sense for a natural spline.

########NEW FILE########
__FILENAME__ = state
# This file is part of Patsy
# Copyright (C) 2011 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# Stateful transform protocol:
#   def __init__(self):
#       pass
#   def memorize_chunk(self, input_data):
#       return None
#   def memorize_finish(self):
#       return None
#   def transform(self, input_data):
#       return output_data

# BETTER WAY: always run the first row of data through the builder alone, and
# check that it gives the same output row as when running the whole block of
# data through at once. This gives us the same information, but it's robust
# against people writing their own centering functions.

# QUESTION: right now we refuse to even fit a model that contains a
# my_transform(x)-style function. Maybe we should allow it to be fit (with a
# warning), and only disallow making predictions with it? Need to revisit this
# question once it's clearer what exactly our public API will look like,
# because right now I'm not sure how to tell whether we are being called for
# fitting versus being called for prediction.

import numpy as np
from patsy.util import (atleast_2d_column_default,
                        asarray_or_pandas, pandas_friendly_reshape,
                        wide_dtype_for)
from patsy.compat import wraps

# These are made available in the patsy.* namespace
__all__ = ["stateful_transform",
           "center", "standardize", "scale",
           ]

def stateful_transform(class_):
    """Create a stateful transform callable object from a class that fulfills
    the :ref:`stateful transform protocol <stateful-transform-protocol>`.
    """
    @wraps(class_)
    def stateful_transform_wrapper(*args, **kwargs):
        transform = class_()
        transform.memorize_chunk(*args, **kwargs)
        transform.memorize_finish()
        return transform.transform(*args, **kwargs)
    stateful_transform_wrapper.__patsy_stateful_transform__ = class_
    return stateful_transform_wrapper

# class NonIncrementalStatefulTransform(object):
#     def __init__(self):
#         self._data = []
#
#     def memorize_chunk(self, input_data, *args, **kwargs):
#         self._data.append(input_data)
#         self._args = _args
#         self._kwargs = kwargs
#
#     def memorize_finish(self):
#         all_data = np.row_stack(self._data)
#         args = self._args
#         kwargs = self._kwargs
#         del self._data
#         del self._args
#         del self._kwargs
#         self.memorize_all(all_data, *args, **kwargs)
#
#     def memorize_all(self, input_data, *args, **kwargs):
#         raise NotImplementedError
#
#     def transform(self, input_data, *args, **kwargs):
#         raise NotImplementedError
#
# class QuantileEstimatingTransform(NonIncrementalStatefulTransform):
#     def memorize_all(self, input_data, *args, **kwargs):

class Center(object):
    """center(x)

    A stateful transform that centers input data, i.e., subtracts the mean.

    If input has multiple columns, centers each column separately.

    Equivalent to ``standardize(x, rescale=False)``
    """
    def __init__(self):
        self._sum = None
        self._count = 0

    def memorize_chunk(self, x):
        x = atleast_2d_column_default(x)
        self._count += x.shape[0]
        this_total = np.sum(x, 0, dtype=wide_dtype_for(x))
        # This is to handle potentially multi-column x's:
        if self._sum is None:
            self._sum = this_total
        else:
            self._sum += this_total

    def memorize_finish(self):
        pass

    def transform(self, x):
        x = asarray_or_pandas(x)
        # This doesn't copy data unless our input is a DataFrame that has
        # heterogenous types. And in that case we're going to be munging the
        # types anyway, so copying isn't a big deal.
        x_arr = np.asarray(x)
        if np.issubdtype(x_arr.dtype, np.integer):
            dt = float
        else:
            dt = x_arr.dtype
        mean_val = np.asarray(self._sum / self._count, dtype=dt)
        centered = atleast_2d_column_default(x, preserve_pandas=True) - mean_val
        return pandas_friendly_reshape(centered, x.shape)

center = stateful_transform(Center)

# See:
#   http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#On-line_algorithm
# or page 232 of Knuth vol. 3 (3rd ed.).
class Standardize(object):
    """standardize(x, center=True, rescale=True, ddof=0)

    A stateful transform that standardizes input data, i.e. it subtracts the
    mean and divides by the sample standard deviation.

    Either centering or rescaling or both can be disabled by use of keyword
    arguments. The `ddof` argument controls the delta degrees of freedom when
    computing the standard deviation (cf. :func:`numpy.std`). The default of
    ``ddof=0`` produces the maximum likelihood estimate; use ``ddof=1`` if you
    prefer the square root of the unbiased estimate of the variance.

    If input has multiple columns, standardizes each column separately.

    .. note:: This function computes the mean and standard deviation using a
       memory-efficient online algorithm, making it suitable for use with
       large incrementally processed data-sets.
    """
    def __init__(self):
        self.current_n = 0
        self.current_mean = None
        self.current_M2 = None

    def memorize_chunk(self, x, center=True, rescale=True, ddof=0):
        x = atleast_2d_column_default(x)
        if self.current_mean is None:
            self.current_mean = np.zeros(x.shape[1], dtype=wide_dtype_for(x))
            self.current_M2 = np.zeros(x.shape[1], dtype=wide_dtype_for(x))
        # XX this can surely be vectorized but I am feeling lazy:
        for i in xrange(x.shape[0]):
            self.current_n += 1
            delta = x[i, :] - self.current_mean
            self.current_mean += delta / self.current_n
            self.current_M2 += delta * (x[i, :] - self.current_mean)

    def memorize_finish(self):
        pass

    def transform(self, x, center=True, rescale=True, ddof=0):
        # XX: this forces all inputs to double-precision real, even if the
        # input is single- or extended-precision or complex. But I got all
        # tangled up in knots trying to do that without breaking something
        # else (e.g. by requiring an extra copy).
        x = asarray_or_pandas(x, copy=True, dtype=float)
        x_2d = atleast_2d_column_default(x, preserve_pandas=True)
        if center:
            x_2d -= self.current_mean
        if rescale:
            x_2d /= np.sqrt(self.current_M2 / (self.current_n - ddof))
        return pandas_friendly_reshape(x_2d, x.shape)

standardize = stateful_transform(Standardize)
# R compatibility:
scale = standardize

########NEW FILE########
__FILENAME__ = test_build
# This file is part of Patsy
# Copyright (C) 2012-2013 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# There are a number of unit tests in build.py, but this file contains more
# thorough tests of the overall design matrix building system. (These are
# still not exhaustive end-to-end tests, though -- for that see
# test_highlevel.py.)

import numpy as np
from nose.tools import assert_raises
from patsy import PatsyError
from patsy.util import (atleast_2d_column_default,
                        have_pandas, have_pandas_categorical)
from patsy.compat import itertools_product
from patsy.desc import Term, INTERCEPT
from patsy.build import *
from patsy.categorical import C
from patsy.user_util import balanced, LookupFactor
from patsy.design_info import DesignMatrix

if have_pandas:
    import pandas

def assert_full_rank(m):
    m = atleast_2d_column_default(m)
    if m.shape[1] == 0:
        return True
    u, s, v = np.linalg.svd(m)
    rank = np.sum(s > 1e-10)
    assert rank == m.shape[1]
    
def test_assert_full_rank():
    assert_full_rank(np.eye(10))
    assert_full_rank([[1, 0], [1, 0], [1, 0], [1, 1]])
    assert_raises(AssertionError,
                  assert_full_rank, [[1, 0], [2, 0]])
    assert_raises(AssertionError,
                  assert_full_rank, [[1, 2], [2, 4]])
    assert_raises(AssertionError,
                  assert_full_rank, [[1, 2, 3], [1, 10, 100]])
    # col1 + col2 = col3
    assert_raises(AssertionError,
                  assert_full_rank, [[1, 2, 3], [1, 5, 6], [1, 6, 7]])
    
def make_termlist(*entries):
    terms = []
    for entry in entries:
        terms.append(Term([LookupFactor(name) for name in entry]))
    return terms

def check_design_matrix(mm, expected_rank, termlist, column_names=None):
    assert_full_rank(mm)
    assert set(mm.design_info.terms) == set(termlist)
    if column_names is not None:
        assert mm.design_info.column_names == column_names
    assert mm.ndim == 2
    assert mm.shape[1] == expected_rank

def make_matrix(data, expected_rank, entries, column_names=None):
    termlist = make_termlist(*entries)
    def iter_maker():
        yield data
    builders = design_matrix_builders([termlist], iter_maker)
    matrices = build_design_matrices(builders, data)
    matrix = matrices[0]
    assert (builders[0].design_info.term_slices
            == matrix.design_info.term_slices)
    assert (builders[0].design_info.column_names
            == matrix.design_info.column_names)
    assert matrix.design_info.builder is builders[0]
    check_design_matrix(matrix, expected_rank, termlist,
                        column_names=column_names)
    return matrix

def test_simple():
    data = balanced(a=2, b=2)
    x1 = data["x1"] = np.linspace(0, 1, len(data["a"]))
    x2 = data["x2"] = data["x1"] ** 2

    m = make_matrix(data, 2, [["a"]], column_names=["a[a1]", "a[a2]"])
    assert np.allclose(m, [[1, 0], [1, 0], [0, 1], [0, 1]])

    m = make_matrix(data, 2, [[], ["a"]], column_names=["Intercept", "a[T.a2]"])
    assert np.allclose(m, [[1, 0], [1, 0], [1, 1], [1, 1]])

    m = make_matrix(data, 4, [["a", "b"]],
                    column_names=["a[a1]:b[b1]", "a[a2]:b[b1]",
                                  "a[a1]:b[b2]", "a[a2]:b[b2]"])
    assert np.allclose(m, [[1, 0, 0, 0],
                           [0, 0, 1, 0],
                           [0, 1, 0, 0],
                           [0, 0, 0, 1]])

    m = make_matrix(data, 4, [[], ["a"], ["b"], ["a", "b"]],
                    column_names=["Intercept", "a[T.a2]",
                                  "b[T.b2]", "a[T.a2]:b[T.b2]"])
    assert np.allclose(m, [[1, 0, 0, 0],
                           [1, 0, 1, 0],
                           [1, 1, 0, 0],
                           [1, 1, 1, 1]])

    m = make_matrix(data, 4, [[], ["b"], ["a"], ["b", "a"]],
                    column_names=["Intercept", "b[T.b2]",
                                  "a[T.a2]", "b[T.b2]:a[T.a2]"])
    assert np.allclose(m, [[1, 0, 0, 0],
                           [1, 1, 0, 0],
                           [1, 0, 1, 0],
                           [1, 1, 1, 1]])

    m = make_matrix(data, 4, [["a"], ["x1"], ["a", "x1"]],
                    column_names=["a[a1]", "a[a2]", "x1", "a[T.a2]:x1"])
    assert np.allclose(m, [[1, 0, x1[0], 0],
                           [1, 0, x1[1], 0],
                           [0, 1, x1[2], x1[2]],
                           [0, 1, x1[3], x1[3]]])
    
    m = make_matrix(data, 3, [["x1"], ["x2"], ["x2", "x1"]],
                    column_names=["x1", "x2", "x2:x1"])
    assert np.allclose(m, np.column_stack((x1, x2, x1 * x2)))
    
def test_R_bugs():
    data = balanced(a=2, b=2, c=2)
    data["x"] = np.linspace(0, 1, len(data["a"]))
    # For "1 + a:b", R produces a design matrix with too many columns (5
    # instead of 4), because it can't tell that there is a redundancy between
    # the two terms.
    make_matrix(data, 4, [[], ["a", "b"]])
    # For "0 + a:x + a:b", R produces a design matrix with too few columns (4
    # instead of 6), because it thinks that there is a redundancy which
    # doesn't exist.
    make_matrix(data, 6, [["a", "x"], ["a", "b"]])
    # This can be compared with "0 + a:c + a:b", where the redundancy does
    # exist. Confusingly, adding another categorical factor increases the
    # baseline dimensionality to 8, and then the redundancy reduces it to 6
    # again, so the result is the same as before but for different reasons. (R
    # does get this one right, but we might as well test it.)
    make_matrix(data, 6, [["a", "c"], ["a", "b"]])

def test_redundancy_thoroughly():
    # To make sure there aren't any lurking bugs analogous to the ones that R
    # has (see above), we check that we get the correct matrix rank for every
    # possible combination of 2 categorical and 2 numerical factors.
    data = balanced(a=2, b=2, repeat=5)
    data["x1"] = np.linspace(0, 1, len(data["a"]))
    data["x2"] = data["x1"] ** 2

    def all_subsets(l):
        if not l:
            yield tuple()
        else:
            obj = l[0]
            for subset in all_subsets(l[1:]):
                yield tuple(sorted(subset))
                yield tuple(sorted((obj,) + subset))

    all_terms = list(all_subsets(("a", "b", "x1", "x2")))
    all_termlist_templates = list(all_subsets(all_terms))
    print len(all_termlist_templates)
    # eliminate some of the symmetric versions to speed things up
    redundant = [[("b",), ("a",)],
                 [("x2",), ("x1",)],
                 [("b", "x2"), ("a", "x1")],
                 [("a", "b", "x2"), ("a", "b", "x1")],
                 [("b", "x1", "x2"), ("a", "x1", "x2")]]
    count = 0
    for termlist_template in all_termlist_templates:
        termlist_set = set(termlist_template)
        for dispreferred, preferred in redundant:
            if dispreferred in termlist_set and preferred not in termlist_set:
                break
        else:
            expanded_terms = set()
            for term_template in termlist_template:
                numeric = tuple([t for t in term_template if t.startswith("x")])
                rest = [t for t in term_template if not t.startswith("x")]
                for subset_rest in all_subsets(rest):
                    expanded_terms.add(frozenset(subset_rest + numeric))
            # Because our categorical variables have 2 levels, each expanded
            # term corresponds to 1 unique dimension of variation
            expected_rank = len(expanded_terms)
            if termlist_template in [(), ((),)]:
                # No data dependence, should fail
                assert_raises(PatsyError,
                              make_matrix,
                              data, expected_rank, termlist_template)
            else:
                make_matrix(data, expected_rank, termlist_template)
            count += 1
    print count

test_redundancy_thoroughly.slow = 1

def test_data_types():
    basic_dict = {"a": ["a1", "a2", "a1", "a2"],
                  "x": [1, 2, 3, 4]}
    # On Python 2, this is identical to basic_dict:
    basic_dict_bytes = dict(basic_dict)
    basic_dict_bytes["a"] = [str(s) for s in basic_dict_bytes["a"]]
    # On Python 3, this is identical to basic_dict:
    basic_dict_unicode = {"a": ["a1", "a2", "a1", "a2"],
                          "x": [1, 2, 3, 4]}
    basic_dict_unicode = dict(basic_dict)
    basic_dict_unicode["a"] = [unicode(s) for s in basic_dict_unicode["a"]]

    structured_array_bytes = np.array(zip(basic_dict["a"], basic_dict["x"]),
                                      dtype=[("a", "S2"), ("x", int)])
    structured_array_unicode = np.array(zip(basic_dict["a"], basic_dict["x"]),
                                        dtype=[("a", "U2"), ("x", int)])
    recarray_bytes = structured_array_bytes.view(np.recarray)
    recarray_unicode = structured_array_unicode.view(np.recarray)
    datas = [basic_dict, structured_array_bytes, structured_array_unicode,
             recarray_bytes, recarray_unicode]
    if have_pandas:
        df_bytes = pandas.DataFrame(basic_dict_bytes)
        datas.append(df_bytes)
        df_unicode = pandas.DataFrame(basic_dict_unicode)
        datas.append(df_unicode)
    for data in datas:
        m = make_matrix(data, 4, [["a"], ["a", "x"]],
                        column_names=["a[a1]", "a[a2]", "a[a1]:x", "a[a2]:x"])
        assert np.allclose(m, [[1, 0, 1, 0],
                               [0, 1, 0, 2],
                               [1, 0, 3, 0],
                               [0, 1, 0, 4]])

def test_build_design_matrices_dtype():
    data = {"x": [1, 2, 3]}
    def iter_maker():
        yield data
    builder = design_matrix_builders([make_termlist("x")], iter_maker)[0]

    mat = build_design_matrices([builder], data)[0]
    assert mat.dtype == np.dtype(np.float64)

    mat = build_design_matrices([builder], data, dtype=np.float32)[0]
    assert mat.dtype == np.dtype(np.float32)

    if hasattr(np, "float128"):
        mat = build_design_matrices([builder], data, dtype=np.float128)[0]
        assert mat.dtype == np.dtype(np.float128)

def test_return_type():
    data = {"x": [1, 2, 3]}
    def iter_maker():
        yield data
    builder = design_matrix_builders([make_termlist("x")], iter_maker)[0]
    
    # Check explicitly passing return_type="matrix" works
    mat = build_design_matrices([builder], data, return_type="matrix")[0]
    assert isinstance(mat, DesignMatrix)

    # Check that nonsense is detected
    assert_raises(PatsyError,
                  build_design_matrices, [builder], data,
                  return_type="asdfsadf")

def test_NA_action():
    initial_data = {"x": [1, 2, 3], "c": ["c1", "c2", "c1"]}
    def iter_maker():
        yield initial_data
    builder = design_matrix_builders([make_termlist("x", "c")], iter_maker)[0]

    # By default drops rows containing either NaN or None
    mat = build_design_matrices([builder],
                                {"x": [10.0, np.nan, 20.0],
                                 "c": np.asarray(["c1", "c2", None],
                                                 dtype=object)})[0]
    assert mat.shape == (1, 3)
    assert np.array_equal(mat, [[1.0, 0.0, 10.0]])

    # NA_action="a string" also accepted:
    mat = build_design_matrices([builder],
                                {"x": [10.0, np.nan, 20.0],
                                 "c": np.asarray(["c1", "c2", None],
                                                 dtype=object)},
                                NA_action="drop")[0]
    assert mat.shape == (1, 3)
    assert np.array_equal(mat, [[1.0, 0.0, 10.0]])

    # And objects
    from patsy.missing import NAAction
    # allows NaN's to pass through
    NA_action = NAAction(NA_types=[])
    mat = build_design_matrices([builder],
                                {"x": [10.0, np.nan],
                                 "c": np.asarray(["c1", "c2"],
                                                 dtype=object)},
                                NA_action=NA_action)[0]
    assert mat.shape == (2, 3)
    # According to this (and only this) function, NaN == NaN.
    np.testing.assert_array_equal(mat, [[1.0, 0.0, 10.0], [0.0, 1.0, np.nan]])
    
    # NA_action="raise"
    assert_raises(PatsyError,
                  build_design_matrices,
                  [builder],
                  {"x": [10.0, np.nan, 20.0],
                   "c": np.asarray(["c1", "c2", None],
                                   dtype=object)},
                  NA_action="raise")

def test_NA_drop_preserves_levels():
    # Even if all instances of some level are dropped, we still include it in
    # the output matrix (as an all-zeros column)
    data = {"x": [1.0, np.nan, 3.0], "c": ["c1", "c2", "c3"]}
    def iter_maker():
        yield data
    builder = design_matrix_builders([make_termlist("x", "c")], iter_maker)[0]

    assert builder.design_info.column_names == ["c[c1]", "c[c2]", "c[c3]", "x"]

    mat, = build_design_matrices([builder], data)

    assert mat.shape == (2, 4)
    assert np.array_equal(mat, [[1.0, 0.0, 0.0, 1.0],
                                [0.0, 0.0, 1.0, 3.0]])

def test_return_type_pandas():
    if not have_pandas:
        return

    data = pandas.DataFrame({"x": [1, 2, 3],
                             "y": [4, 5, 6],
                             "a": ["a1", "a2", "a1"]},
                            index=[10, 20, 30])
    def iter_maker():
        yield data
    int_builder, = design_matrix_builders([make_termlist([])], iter_maker)
    (y_builder, x_builder) = design_matrix_builders([make_termlist("y"),
                                                     make_termlist("x")],
                                                    iter_maker)
    (x_a_builder,) = design_matrix_builders([make_termlist("x", "a")],
                                            iter_maker)
    (x_y_builder,) = design_matrix_builders([make_termlist("x", "y")],
                                            iter_maker)
    # Index compatibility is always checked for pandas input, regardless of
    # whether we're producing pandas output
    assert_raises(PatsyError,
                  build_design_matrices,
                  [x_a_builder], {"x": data["x"], "a": data["a"][::-1]})
    assert_raises(PatsyError,
                  build_design_matrices,
                  [y_builder, x_builder],
                  {"x": data["x"], "y": data["y"][::-1]})
    # And we also check consistency between data.index and value indexes
    # Creating a mismatch between these is a bit tricky. We want a data object
    # such that isinstance(data, DataFrame), but data["x"].index !=
    # data.index.
    class CheatingDataFrame(pandas.DataFrame):
        def __getitem__(self, key):
            if key == "x":
                return pandas.DataFrame.__getitem__(self, key)[::-1]
            else:
                return pandas.DataFrame.__getitem__(self, key)
    assert_raises(PatsyError,
                  build_design_matrices,
                  [x_builder],
                  CheatingDataFrame(data))

    # A mix of pandas input and unindexed input is fine
    (mat,) = build_design_matrices([x_y_builder],
                                   {"x": data["x"], "y": [40, 50, 60]})
    assert np.allclose(mat, [[1, 40], [2, 50], [3, 60]])

    # with return_type="dataframe", we get out DataFrames with nice indices
    # and nice column names and design_info
    y_df, x_df = build_design_matrices([y_builder, x_builder], data,
                                       return_type="dataframe")
    assert isinstance(y_df, pandas.DataFrame)
    assert isinstance(x_df, pandas.DataFrame)
    assert np.array_equal(y_df, [[4], [5], [6]])
    assert np.array_equal(x_df, [[1], [2], [3]])
    assert np.array_equal(y_df.index, [10, 20, 30])
    assert np.array_equal(x_df.index, [10, 20, 30])
    assert np.array_equal(y_df.columns, ["y"])
    assert np.array_equal(x_df.columns, ["x"])
    assert y_df.design_info.column_names == ["y"]
    assert x_df.design_info.column_names == ["x"]
    assert y_df.design_info.term_names == ["y"]
    assert x_df.design_info.term_names == ["x"]
    # Same with mix of pandas and unindexed info, even if in different
    # matrices
    y_df, x_df = build_design_matrices([y_builder, x_builder],
                                       {"y": [7, 8, 9], "x": data["x"]},
                                       return_type="dataframe")
    assert isinstance(y_df, pandas.DataFrame)
    assert isinstance(x_df, pandas.DataFrame)
    assert np.array_equal(y_df, [[7], [8], [9]])
    assert np.array_equal(x_df, [[1], [2], [3]])
    assert np.array_equal(y_df.index, [10, 20, 30])
    assert np.array_equal(x_df.index, [10, 20, 30])
    assert np.array_equal(y_df.columns, ["y"])
    assert np.array_equal(x_df.columns, ["x"])
    assert y_df.design_info.column_names == ["y"]
    assert x_df.design_info.column_names == ["x"]
    assert y_df.design_info.term_names == ["y"]
    assert x_df.design_info.term_names == ["x"]
    # Check categorical works for carrying index too
    (x_a_df,) = build_design_matrices([x_a_builder],
                                      {"x": [-1, -2, -3], "a": data["a"]},
                                      return_type="dataframe")
    assert isinstance(x_a_df, pandas.DataFrame)
    assert np.array_equal(x_a_df, [[1, 0, -1], [0, 1, -2], [1, 0, -3]])
    assert np.array_equal(x_a_df.index, [10, 20, 30])
    # And if we have no indexed input, then we let pandas make up an index as
    # per its usual rules:
    (x_y_df,) = build_design_matrices([x_y_builder],
                                      {"y": [7, 8, 9], "x": [10, 11, 12]},
                                      return_type="dataframe")
    assert isinstance(x_y_df, pandas.DataFrame)
    assert np.array_equal(x_y_df, [[10, 7], [11, 8], [12, 9]])
    assert np.array_equal(x_y_df.index, [0, 1, 2])

    # If 'data' is a DataFrame, then that suffices, even if no factors are
    # available.
    (int_df,) = build_design_matrices([int_builder], data,
                                      return_type="dataframe")
    assert isinstance(int_df, pandas.DataFrame)
    assert np.array_equal(int_df, [[1], [1], [1]])
    assert int_df.index.equals([10, 20, 30])

    import patsy.build
    had_pandas = patsy.build.have_pandas
    try:
        patsy.build.have_pandas = False
        # return_type="dataframe" gives a nice error if pandas is not available
        assert_raises(PatsyError,
                      build_design_matrices,
                      [x_builder], {"x": [1, 2, 3]}, return_type="dataframe")
    finally:
        patsy.build.have_pandas = had_pandas

    x_df, = build_design_matrices([x_a_builder],
                                  {"x": [1.0, np.nan, 3.0],
                                   "a": np.asarray([None, "a2", "a1"],
                                                   dtype=object)},
                                  NA_action="drop",
                                  return_type="dataframe")
    assert x_df.index.equals([2])

def test_data_mismatch():
    test_cases_twoway = [
        # Data type mismatch
        ([1, 2, 3], [True, False, True]),
        (C(["a", "b", "c"], levels=["c", "b", "a"]),
         C(["a", "b", "c"], levels=["a", "b", "c"])),
        # column number mismatches
        ([[1], [2], [3]], [[1, 1], [2, 2], [3, 3]]),
        ([[1, 1, 1], [2, 2, 2], [3, 3, 3]], [[1, 1], [2, 2], [3, 3]]),
        ]
    test_cases_oneway = [
        ([1, 2, 3], ["a", "b", "c"]),
        ([1, 2, 3], C(["a", "b", "c"])),
        ([True, False, True], C(["a", "b", "c"])),
        ([True, False, True], ["a", "b", "c"]),
        ]
    setup_predict_only = [
        # This is not an error if both are fed in during make_builders, but it
        # is an error to pass one to make_builders and the other to
        # make_matrices.
        (["a", "b", "c"], ["a", "b", "d"]),
        ]
    termlist = make_termlist(["x"])
    def t_incremental(data1, data2):
        def iter_maker():
            yield {"x": data1}
            yield {"x": data2}
        try:
            builders = design_matrix_builders([termlist], iter_maker)
            build_design_matrices(builders, {"x": data1})
            build_design_matrices(builders, {"x": data2})
        except PatsyError:
            pass
        else:
            raise AssertionError
    def t_setup_predict(data1, data2):
        def iter_maker():
            yield {"x": data1}
        builders = design_matrix_builders([termlist], iter_maker)
        assert_raises(PatsyError,
                      build_design_matrices, builders, {"x": data2})
    for (a, b) in test_cases_twoway:
        t_incremental(a, b)
        t_incremental(b, a)
        t_setup_predict(a, b)
        t_setup_predict(b, a)
    for (a, b) in test_cases_oneway:
        t_incremental(a, b)
        t_setup_predict(a, b)
    for (a, b) in setup_predict_only:
        t_setup_predict(a, b)
        t_setup_predict(b, a)

    assert_raises(PatsyError,
                  make_matrix, {"x": [1, 2, 3], "y": [1, 2, 3, 4]},
                  2, [["x"], ["y"]])

def test_data_independent_builder():
    data = {"x": [1, 2, 3]}
    def iter_maker():
        yield data

    # Trying to build a matrix that doesn't depend on the data at all is an
    # error, if:
    # - the index argument is not given
    # - the data is not a DataFrame
    # - there are no other matrices
    null_builder = design_matrix_builders([make_termlist()], iter_maker)[0]
    assert_raises(PatsyError, build_design_matrices, [null_builder], data)

    intercept_builder = design_matrix_builders([make_termlist([])],
                                               iter_maker)[0]
    assert_raises(PatsyError, build_design_matrices, [intercept_builder], data)

    assert_raises(PatsyError,
                  build_design_matrices,
                  [null_builder, intercept_builder], data)

    # If data is a DataFrame, it sets the number of rows.
    if have_pandas:
        int_m, null_m = build_design_matrices([intercept_builder,
                                               null_builder],
                                              pandas.DataFrame(data))
        assert np.allclose(int_m, [[1], [1], [1]])
        assert null_m.shape == (3, 0)

    # If there are other matrices that do depend on the data, we make the
    # data-independent matrices have the same number of rows.
    x_termlist = make_termlist(["x"])

    builders = design_matrix_builders([x_termlist, make_termlist()],
                                      iter_maker)
    x_m, null_m = build_design_matrices(builders, data)
    assert np.allclose(x_m, [[1], [2], [3]])
    assert null_m.shape == (3, 0)

    builders = design_matrix_builders([x_termlist, make_termlist([])],
                                      iter_maker)
    x_m, null_m = build_design_matrices(builders, data)
    x_m, intercept_m = build_design_matrices(builders, data)
    assert np.allclose(x_m, [[1], [2], [3]])
    assert np.allclose(intercept_m, [[1], [1], [1]])

def test_same_factor_in_two_matrices():
    data = {"x": [1, 2, 3], "a": ["a1", "a2", "a1"]}
    def iter_maker():
        yield data
    t1 = make_termlist(["x"])
    t2 = make_termlist(["x", "a"])
    builders = design_matrix_builders([t1, t2], iter_maker)
    m1, m2 = build_design_matrices(builders, data)
    check_design_matrix(m1, 1, t1, column_names=["x"])
    assert np.allclose(m1, [[1], [2], [3]])
    check_design_matrix(m2, 2, t2, column_names=["x:a[a1]", "x:a[a2]"])
    assert np.allclose(m2, [[1, 0], [0, 2], [3, 0]])

def test_categorical():
    data_strings = {"a": ["a1", "a2", "a1"]}
    data_categ = {"a": C(["a2", "a1", "a2"])}
    datas = [data_strings, data_categ]
    if have_pandas_categorical:
        data_pandas = {"a": pandas.Categorical.from_array(["a1", "a2", "a2"])}
        datas.append(data_pandas)
    def t(data1, data2):
        def iter_maker():
            yield data1
        builders = design_matrix_builders([make_termlist(["a"])],
                                          iter_maker)
        build_design_matrices(builders, data2)
    for data1 in datas:
        for data2 in datas:
            t(data1, data2)

def test_contrast():
    from patsy.contrasts import ContrastMatrix, Sum
    values = ["a1", "a3", "a1", "a2"]
    
    # No intercept in model, full-rank coding of 'a'
    m = make_matrix({"a": C(values)}, 3, [["a"]],
                    column_names=["a[a1]", "a[a2]", "a[a3]"])

    assert np.allclose(m, [[1, 0, 0],
                           [0, 0, 1],
                           [1, 0, 0],
                           [0, 1, 0]])
    
    for s in (Sum, Sum()):
        m = make_matrix({"a": C(values, s)}, 3, [["a"]],
                        column_names=["a[mean]", "a[S.a1]", "a[S.a2]"])
        # Output from R
        assert np.allclose(m, [[1, 1, 0],
                               [1,-1, -1],
                               [1, 1, 0],
                               [1, 0, 1]])
    
    m = make_matrix({"a": C(values, Sum(omit=0))}, 3, [["a"]],
                    column_names=["a[mean]", "a[S.a2]", "a[S.a3]"])
    # Output from R
    assert np.allclose(m, [[1, -1, -1],
                           [1,  0,  1],
                           [1, -1, -1],
                           [1,  1,  0]])

    # Intercept in model, non-full-rank coding of 'a'
    m = make_matrix({"a": C(values)}, 3, [[], ["a"]],
                    column_names=["Intercept", "a[T.a2]", "a[T.a3]"])

    assert np.allclose(m, [[1, 0, 0],
                           [1, 0, 1],
                           [1, 0, 0],
                           [1, 1, 0]])
    
    for s in (Sum, Sum()):
        m = make_matrix({"a": C(values, s)}, 3, [[], ["a"]],
                        column_names=["Intercept", "a[S.a1]", "a[S.a2]"])
        # Output from R
        assert np.allclose(m, [[1, 1, 0],
                               [1,-1, -1],
                               [1, 1, 0],
                               [1, 0, 1]])
    
    m = make_matrix({"a": C(values, Sum(omit=0))}, 3, [[], ["a"]],
                    column_names=["Intercept", "a[S.a2]", "a[S.a3]"])
    # Output from R
    assert np.allclose(m, [[1, -1, -1],
                           [1,  0,  1],
                           [1, -1, -1],
                           [1,  1,  0]])

    # Weird ad hoc less-than-full-rank coding of 'a'
    m = make_matrix({"a": C(values, [[7, 12],
                                     [2, 13],
                                     [8, -1]])},
                    2, [["a"]],
                    column_names=["a[custom0]", "a[custom1]"])
    assert np.allclose(m, [[7, 12],
                           [8, -1],
                           [7, 12],
                           [2, 13]])

    m = make_matrix({"a": C(values, ContrastMatrix([[7, 12],
                                                    [2, 13],
                                                    [8, -1]],
                                                   ["[foo]", "[bar]"]))},
                    2, [["a"]],
                    column_names=["a[foo]", "a[bar]"])
    assert np.allclose(m, [[7, 12],
                           [8, -1],
                           [7, 12],
                           [2, 13]])

def test_DesignMatrixBuilder_subset():
    # For each combination of:
    #   formula, term names, term objects, mixed term name and term objects
    # check that results match subset of full build
    # and that removed variables don't hurt
    all_data = {"x": [1, 2],
                "y": [[3.1, 3.2],
                      [4.1, 4.2]],
                "z": [5, 6]}
    all_terms = make_termlist("x", "y", "z")
    def iter_maker():
        yield all_data
    all_builder = design_matrix_builders([all_terms], iter_maker)[0]
    full_matrix = build_design_matrices([all_builder], all_data)[0]

    def t(which_terms, variables, columns):
        sub_builder = all_builder.subset(which_terms)
        sub_data = {}
        for variable in variables:
            sub_data[variable] = all_data[variable]
        sub_matrix = build_design_matrices([sub_builder], sub_data)[0]
        sub_full_matrix = full_matrix[:, columns]
        if not isinstance(which_terms, basestring):
            assert len(which_terms) == len(sub_builder.design_info.terms)
        assert np.array_equal(sub_matrix, sub_full_matrix)

    t("~ 0 + x + y + z", ["x", "y", "z"], slice(None))
    t(["x", "y", "z"], ["x", "y", "z"], slice(None))
    t([unicode("x"), unicode("y"), unicode("z")],
      ["x", "y", "z"], slice(None))
    t(all_terms, ["x", "y", "z"], slice(None))
    t([all_terms[0], "y", all_terms[2]], ["x", "y", "z"], slice(None))

    t("~ 0 + x + z", ["x", "z"], [0, 3])
    t(["x", "z"], ["x", "z"], [0, 3])
    t([unicode("x"), unicode("z")], ["x", "z"], [0, 3])
    t([all_terms[0], all_terms[2]], ["x", "z"], [0, 3])
    t([all_terms[0], "z"], ["x", "z"], [0, 3])

    t("~ 0 + z + x", ["x", "z"], [3, 0])
    t(["z", "x"], ["x", "z"], [3, 0])
    t([unicode("z"), unicode("x")], ["x", "z"], [3, 0])
    t([all_terms[2], all_terms[0]], ["x", "z"], [3, 0])
    t([all_terms[2], "x"], ["x", "z"], [3, 0])

    t("~ 0 + y", ["y"], [1, 2])
    t(["y"], ["y"], [1, 2])
    t([unicode("y")], ["y"], [1, 2])
    t([all_terms[1]], ["y"], [1, 2])

    # Formula can't have a LHS
    assert_raises(PatsyError, all_builder.subset, "a ~ a")
    # Term must exist
    assert_raises(PatsyError, all_builder.subset, "~ asdf")
    assert_raises(PatsyError, all_builder.subset, ["asdf"])
    assert_raises(PatsyError,
                  all_builder.subset, [Term(["asdf"])])

########NEW FILE########
__FILENAME__ = test_highlevel
# This file is part of Patsy
# Copyright (C) 2012-2013 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# Exhaustive end-to-end tests of the top-level API.

import sys
import __future__
import numpy as np
from nose.tools import assert_raises
from patsy import PatsyError
from patsy.design_info import DesignMatrix
from patsy.eval import EvalEnvironment
from patsy.desc import ModelDesc, Term, INTERCEPT
from patsy.categorical import C
from patsy.contrasts import Helmert
from patsy.user_util import balanced, LookupFactor
from patsy.build import (design_matrix_builders,
                         build_design_matrices,
                         DesignMatrixBuilder)
from patsy.highlevel import *
from patsy.util import have_pandas
from patsy.origin import Origin

if have_pandas:
    import pandas

def check_result(expect_builders, lhs, rhs, data,
                 expected_rhs_values, expected_rhs_names,
                 expected_lhs_values, expected_lhs_names): # pragma: no cover
    assert np.allclose(rhs, expected_rhs_values)
    assert rhs.design_info.column_names == expected_rhs_names
    if lhs is not None:
        assert np.allclose(lhs, expected_lhs_values)
        assert lhs.design_info.column_names == expected_lhs_names
    else:
        assert expected_lhs_values is None
        assert expected_lhs_names is None
    
    if expect_builders:
        if lhs is None:
            new_rhs, = build_design_matrices([rhs.design_info.builder], data)
        else:
            new_lhs, new_rhs = build_design_matrices([lhs.design_info.builder,
                                                      rhs.design_info.builder],
                                                     data)
            assert np.allclose(new_lhs, lhs)
            assert new_lhs.design_info.column_names == expected_lhs_names
        assert np.allclose(new_rhs, rhs)
        assert new_rhs.design_info.column_names == expected_rhs_names
    else:
        assert rhs.design_info.builder is None
        assert lhs is None or lhs.design_info.builder is None

def dmatrix_pandas(formula_like, data={}, depth=0, return_type="matrix"):
    return_type = "dataframe"
    if isinstance(depth, int):
        depth += 1
    return dmatrix(formula_like, data, depth, return_type=return_type)

def dmatrices_pandas(formula_like, data={}, depth=0, return_type="matrix"):
    return_type = "dataframe"
    if isinstance(depth, int):
        depth += 1
    return dmatrices(formula_like, data, depth, return_type=return_type)

def t(formula_like, data, depth,
      expect_builders,
      expected_rhs_values, expected_rhs_names,
      expected_lhs_values=None, expected_lhs_names=None): # pragma: no cover
    if isinstance(depth, int):
        depth += 1
    def data_iter_maker():
        return iter([data])
    if (isinstance(formula_like, (basestring, ModelDesc, DesignMatrixBuilder))
        or (isinstance(formula_like, tuple)
            and isinstance(formula_like[0], DesignMatrixBuilder))
        or hasattr(formula_like, "__patsy_get_model_desc__")):
        if expected_lhs_values is None:
            builder = incr_dbuilder(formula_like, data_iter_maker, depth)
            lhs = None
            (rhs,) = build_design_matrices([builder], data)
        else:
            builders = incr_dbuilders(formula_like, data_iter_maker, depth)
            lhs, rhs = build_design_matrices(builders, data)
        check_result(expect_builders, lhs, rhs, data,
                     expected_rhs_values, expected_rhs_names,
                     expected_lhs_values, expected_lhs_names)
    else:
        assert_raises(PatsyError, incr_dbuilders,
                      formula_like, data_iter_maker)
        assert_raises(PatsyError, incr_dbuilder,
                      formula_like, data_iter_maker)
    one_mat_fs = [dmatrix]
    two_mat_fs = [dmatrices]
    if have_pandas:
        one_mat_fs.append(dmatrix_pandas)
        two_mat_fs.append(dmatrices_pandas)
    if expected_lhs_values is None:
        for f in one_mat_fs:
            rhs = f(formula_like, data, depth)
            check_result(expect_builders, None, rhs, data,
                         expected_rhs_values, expected_rhs_names,
                         expected_lhs_values, expected_lhs_names)

        # We inline assert_raises here to avoid complications with the
        # depth argument.
        for f in two_mat_fs:
            try:
                f(formula_like, data, depth)
            except PatsyError:
                pass
            else:
                raise AssertionError
    else:
        for f in one_mat_fs:
            try:
                f(formula_like, data, depth)
            except PatsyError:
                pass
            else:
                raise AssertionError

        for f in two_mat_fs:
            (lhs, rhs) = f(formula_like, data, depth)
            check_result(expect_builders, lhs, rhs, data,
                         expected_rhs_values, expected_rhs_names,
                         expected_lhs_values, expected_lhs_names)

def t_invalid(formula_like, data, depth, exc=PatsyError): # pragma: no cover
    if isinstance(depth, int):
        depth += 1
    fs = [dmatrix, dmatrices]
    if have_pandas:
        fs += [dmatrix_pandas, dmatrices_pandas]
    for f in fs:
        try:
            f(formula_like, data, depth)
        except exc:
            pass
        else:
            raise AssertionError

# Exercise all the different calling conventions for the high-level API
def test_formula_likes():
    # Plain array-like, rhs only
    t([[1, 2, 3], [4, 5, 6]], {}, 0,
      False,
      [[1, 2, 3], [4, 5, 6]], ["x0", "x1", "x2"])
    t((None, [[1, 2, 3], [4, 5, 6]]), {}, 0,
      False,
      [[1, 2, 3], [4, 5, 6]], ["x0", "x1", "x2"])
    t(np.asarray([[1, 2, 3], [4, 5, 6]]), {}, 0,
      False,
      [[1, 2, 3], [4, 5, 6]], ["x0", "x1", "x2"])
    t((None, np.asarray([[1, 2, 3], [4, 5, 6]])), {}, 0,
      False,
      [[1, 2, 3], [4, 5, 6]], ["x0", "x1", "x2"])
    dm = DesignMatrix([[1, 2, 3], [4, 5, 6]], default_column_prefix="foo")
    t(dm, {}, 0,
      False,
      [[1, 2, 3], [4, 5, 6]], ["foo0", "foo1", "foo2"])
    t((None, dm), {}, 0,
      False,
      [[1, 2, 3], [4, 5, 6]], ["foo0", "foo1", "foo2"])
      
    # Plain array-likes, lhs and rhs
    t(([1, 2], [[1, 2, 3], [4, 5, 6]]), {}, 0,
      False,
      [[1, 2, 3], [4, 5, 6]], ["x0", "x1", "x2"],
      [[1], [2]], ["y0"])
    t(([[1], [2]], [[1, 2, 3], [4, 5, 6]]), {}, 0,
      False,
      [[1, 2, 3], [4, 5, 6]], ["x0", "x1", "x2"],
      [[1], [2]], ["y0"])
    t((np.asarray([1, 2]), np.asarray([[1, 2, 3], [4, 5, 6]])), {}, 0,
      False,
      [[1, 2, 3], [4, 5, 6]], ["x0", "x1", "x2"],
      [[1], [2]], ["y0"])
    t((np.asarray([[1], [2]]), np.asarray([[1, 2, 3], [4, 5, 6]])), {}, 0,
      False,
      [[1, 2, 3], [4, 5, 6]], ["x0", "x1", "x2"],
      [[1], [2]], ["y0"])
    x_dm = DesignMatrix([[1, 2, 3], [4, 5, 6]], default_column_prefix="foo")
    y_dm = DesignMatrix([1, 2], default_column_prefix="bar")
    t((y_dm, x_dm), {}, 0,
      False,
      [[1, 2, 3], [4, 5, 6]], ["foo0", "foo1", "foo2"],
      [[1], [2]], ["bar0"])
    # number of rows must match
    t_invalid(([1, 2, 3], [[1, 2, 3], [4, 5, 6]]), {}, 0)

    # tuples must have the right size
    t_invalid(([[1, 2, 3]],), {}, 0)
    t_invalid(([[1, 2, 3]], [[1, 2, 3]], [[1, 2, 3]]), {}, 0)

    # plain Series and DataFrames
    if have_pandas:
        # Names are extracted
        t(pandas.DataFrame({"x": [1, 2, 3]}), {}, 0,
          False,
          [[1], [2], [3]], ["x"])
        t(pandas.Series([1, 2, 3], name="asdf"), {}, 0,
          False,
          [[1], [2], [3]], ["asdf"])
        t((pandas.DataFrame({"y": [4, 5, 6]}),
           pandas.DataFrame({"x": [1, 2, 3]})), {}, 0,
          False,
          [[1], [2], [3]], ["x"],
          [[4], [5], [6]], ["y"])
        t((pandas.Series([4, 5, 6], name="y"),
           pandas.Series([1, 2, 3], name="x")), {}, 0,
          False,
          [[1], [2], [3]], ["x"],
          [[4], [5], [6]], ["y"])
        # Or invented
        t((pandas.DataFrame([[4, 5, 6]]),
           pandas.DataFrame([[1, 2, 3]], columns=[7, 8, 9])), {}, 0,
          False,
          [[1, 2, 3]], ["x7", "x8", "x9"],
          [[4, 5, 6]], ["y0", "y1", "y2"])
        t(pandas.Series([1, 2, 3]), {}, 0,
          False,
          [[1], [2], [3]], ["x0"])
        # indices must match
        t_invalid((pandas.DataFrame([[1]], index=[1]),
                   pandas.DataFrame([[1]], index=[2])),
                  {}, 0)

    # Foreign ModelDesc factories
    class ForeignModelSource(object):
        def __patsy_get_model_desc__(self, data):
            return ModelDesc([Term([LookupFactor("Y")])],
                             [Term([LookupFactor("X")])])
    foreign_model = ForeignModelSource()
    t(foreign_model,
      {"Y": [1, 2],
       "X": [[1, 2], [3, 4]]},
      0,
      True,
      [[1, 2], [3, 4]], ["X[0]", "X[1]"],
      [[1], [2]], ["Y"])
    class BadForeignModelSource(object):
        def __patsy_get_model_desc__(self, data):
            return data
    t_invalid(BadForeignModelSource(), {}, 0)

    # string formulas
    t("y ~ x", {"y": [1, 2], "x": [3, 4]}, 0,
      True,
      [[1, 3], [1, 4]], ["Intercept", "x"],
      [[1], [2]], ["y"])
    t("~ x", {"y": [1, 2], "x": [3, 4]}, 0,
      True,
      [[1, 3], [1, 4]], ["Intercept", "x"])
    t("x + y", {"y": [1, 2], "x": [3, 4]}, 0,
      True,
      [[1, 3, 1], [1, 4, 2]], ["Intercept", "x", "y"])
    
    # ModelDesc
    desc = ModelDesc([], [Term([LookupFactor("x")])])
    t(desc, {"x": [1.5, 2.5, 3.5]}, 0,
      True,
      [[1.5], [2.5], [3.5]], ["x"])
    desc = ModelDesc([], [Term([]), Term([LookupFactor("x")])])
    t(desc, {"x": [1.5, 2.5, 3.5]}, 0,
      True,
      [[1, 1.5], [1, 2.5], [1, 3.5]], ["Intercept", "x"])
    desc = ModelDesc([Term([LookupFactor("y")])],
                     [Term([]), Term([LookupFactor("x")])])
    t(desc, {"x": [1.5, 2.5, 3.5], "y": [10, 20, 30]}, 0,
      True,
      [[1, 1.5], [1, 2.5], [1, 3.5]], ["Intercept", "x"],
      [[10], [20], [30]], ["y"])

    # builders
    termlists = ([],
                 [Term([LookupFactor("x")])],
                 [Term([]), Term([LookupFactor("x")])],
                 )
    builders = design_matrix_builders(termlists,
                                      lambda: iter([{"x": [1, 2, 3]}]))
    # twople but with no LHS
    t((builders[0], builders[2]), {"x": [10, 20, 30]}, 0,
      True,
      [[1, 10], [1, 20], [1, 30]], ["Intercept", "x"])
    # single DesignMatrixBuilder
    t(builders[2], {"x": [10, 20, 30]}, 0,
      True,
      [[1, 10], [1, 20], [1, 30]], ["Intercept", "x"])
    # twople with LHS
    t((builders[1], builders[2]), {"x": [10, 20, 30]}, 0,
      True,
      [[1, 10], [1, 20], [1, 30]], ["Intercept", "x"],
      [[10], [20], [30]], ["x"])
    
    # check depth arguments
    x_in_env = [1, 2, 3]
    t("~ x_in_env", {}, 0,
      True,
      [[1, 1], [1, 2], [1, 3]], ["Intercept", "x_in_env"])
    t("~ x_in_env", {"x_in_env": [10, 20, 30]}, 0,
      True,
      [[1, 10], [1, 20], [1, 30]], ["Intercept", "x_in_env"])
    # Trying to pull x_in_env out of our *caller* shouldn't work.
    t_invalid("~ x_in_env", {}, 1, exc=(NameError, PatsyError))
    # But then again it should, if called from one down on the stack:
    def check_nested_call():
        x_in_env = "asdf"
        t("~ x_in_env", {}, 1,
          True,
          [[1, 1], [1, 2], [1, 3]], ["Intercept", "x_in_env"])
    check_nested_call()
    # passing in an explicit EvalEnvironment also works:
    e = EvalEnvironment.capture(1)
    t_invalid("~ x_in_env", {}, e, exc=(NameError, PatsyError))
    e = EvalEnvironment.capture(0)
    def check_nested_call_2():
        x_in_env = "asdf"
        t("~ x_in_env", {}, e,
          True,
          [[1, 1], [1, 2], [1, 3]], ["Intercept", "x_in_env"])
    check_nested_call_2()

def test_return_pandas():
    if not have_pandas:
        return
    # basic check of pulling a Series out of the environment
    s1 = pandas.Series([1, 2, 3], name="AA", index=[10, 20, 30])
    s2 = pandas.Series([4, 5, 6], name="BB", index=[10, 20, 30])
    df1 = dmatrix("s1", return_type="dataframe")
    assert np.allclose(df1, [[1, 1], [1, 2], [1, 3]])
    assert np.array_equal(df1.columns, ["Intercept", "s1"])
    assert df1.design_info.column_names == ["Intercept", "s1"]
    assert np.array_equal(df1.index, [10, 20, 30])
    df2, df3 = dmatrices("s2 ~ s1", return_type="dataframe")
    assert np.allclose(df2, [[4], [5], [6]])
    assert np.array_equal(df2.columns, ["s2"])
    assert df2.design_info.column_names == ["s2"]
    assert np.array_equal(df2.index, [10, 20, 30])
    assert np.allclose(df3, [[1, 1], [1, 2], [1, 3]])
    assert np.array_equal(df3.columns, ["Intercept", "s1"])
    assert df3.design_info.column_names == ["Intercept", "s1"]
    assert np.array_equal(df3.index, [10, 20, 30])
    # indices are preserved if pandas is passed in directly
    df4 = dmatrix(s1, return_type="dataframe")
    assert np.allclose(df4, [[1], [2], [3]])
    assert np.array_equal(df4.columns, ["AA"])
    assert df4.design_info.column_names == ["AA"]
    assert np.array_equal(df4.index, [10, 20, 30])
    df5, df6 = dmatrices((s2, s1), return_type="dataframe")
    assert np.allclose(df5, [[4], [5], [6]])
    assert np.array_equal(df5.columns, ["BB"])
    assert df5.design_info.column_names == ["BB"]
    assert np.array_equal(df5.index, [10, 20, 30])
    assert np.allclose(df6, [[1], [2], [3]])
    assert np.array_equal(df6.columns, ["AA"])
    assert df6.design_info.column_names == ["AA"]
    assert np.array_equal(df6.index, [10, 20, 30])
    # Both combinations of with-index and without-index
    df7, df8 = dmatrices((s1, [10, 11, 12]), return_type="dataframe")
    assert np.array_equal(df7.index, s1.index)
    assert np.array_equal(df8.index, s1.index)
    df9, df10 = dmatrices(([10, 11, 12], s1), return_type="dataframe")
    assert np.array_equal(df9.index, s1.index)
    assert np.array_equal(df10.index, s1.index)
    # pandas must be available
    import patsy.highlevel
    had_pandas = patsy.highlevel.have_pandas
    try:
        patsy.highlevel.have_pandas = False
        assert_raises(PatsyError,
                      dmatrix, "x", {"x": [1]}, 0, return_type="dataframe")
        assert_raises(PatsyError,
                      dmatrices, "y ~ x", {"x": [1], "y": [2]}, 0,
                      return_type="dataframe")
    finally:
        patsy.highlevel.have_pandas = had_pandas

def test_term_info():
    data = balanced(a=2, b=2)
    rhs = dmatrix("a:b", data)
    assert rhs.design_info.column_names == ["Intercept", "b[T.b2]",
                                            "a[T.a2]:b[b1]", "a[T.a2]:b[b2]"]
    assert rhs.design_info.term_names == ["Intercept", "a:b"]
    assert len(rhs.design_info.terms) == 2
    assert rhs.design_info.terms[0] == INTERCEPT
    
def test_data_types():
    data = {"a": [1, 2, 3],
            "b": [1.0, 2.0, 3.0],
            "c": np.asarray([1, 2, 3], dtype=np.float32),
            "d": [True, False, True],
            "e": ["foo", "bar", "baz"],
            "f": C([1, 2, 3]),
            "g": C(["foo", "bar", "baz"]),
            "h": np.array(["foo", 1, (1, "hi")], dtype=object),
            }
    t("~ 0 + a", data, 0, True,
      [[1], [2], [3]], ["a"])
    t("~ 0 + b", data, 0, True,
      [[1], [2], [3]], ["b"])
    t("~ 0 + c", data, 0, True,
      [[1], [2], [3]], ["c"])
    t("~ 0 + d", data, 0, True,
      [[0, 1], [1, 0], [0, 1]], ["d[False]", "d[True]"])
    t("~ 0 + e", data, 0, True,
      [[0, 0, 1], [1, 0, 0], [0, 1, 0]], ["e[bar]", "e[baz]", "e[foo]"])
    t("~ 0 + f", data, 0, True,
      [[1, 0, 0], [0, 1, 0], [0, 0, 1]], ["f[1]", "f[2]", "f[3]"])
    t("~ 0 + g", data, 0, True,
      [[0, 0, 1], [1, 0, 0], [0, 1, 0]], ["g[bar]", "g[baz]", "g[foo]"])
    # This depends on Python's sorting behavior:
    t("~ 0 + h", data, 0, True,
      [[0, 1, 0], [1, 0, 0], [0, 0, 1]],
      ["h[1]", "h[foo]", "h[(1, 'hi')]"])
    
def test_categorical():
    data = balanced(a=2, b=2)
    # There are more exhaustive tests for all the different coding options in
    # test_build; let's just make sure that C() and stuff works.
    t("~ C(a)", data, 0,
      True,
      [[1, 0], [1, 0], [1, 1], [1, 1]], ["Intercept", "C(a)[T.a2]"])
    t("~ C(a, levels=['a2', 'a1'])", data, 0,
      True,
      [[1, 1], [1, 1], [1, 0], [1, 0]],
      ["Intercept", "C(a, levels=['a2', 'a1'])[T.a1]"])
    t("~ C(a, Treatment(reference=-1))", data, 0,
      True,
      [[1, 1], [1, 1], [1, 0], [1, 0]],
      ["Intercept", "C(a, Treatment(reference=-1))[T.a1]"])

    # Different interactions
    t("a*b", data, 0,
      True,
      [[1, 0, 0, 0],
       [1, 0, 1, 0],
       [1, 1, 0, 0],
       [1, 1, 1, 1]],
      ["Intercept", "a[T.a2]", "b[T.b2]", "a[T.a2]:b[T.b2]"])
    t("0 + a:b", data, 0,
      True,
      [[1, 0, 0, 0],
       [0, 0, 1, 0],
       [0, 1, 0, 0],
       [0, 0, 0, 1]],
      ["a[a1]:b[b1]", "a[a2]:b[b1]", "a[a1]:b[b2]", "a[a2]:b[b2]"])
    t("1 + a + a:b", data, 0,
      True,
      [[1, 0, 0, 0],
       [1, 0, 1, 0],
       [1, 1, 0, 0],
       [1, 1, 0, 1]],
      ["Intercept", "a[T.a2]", "a[a1]:b[T.b2]", "a[a2]:b[T.b2]"])

    # Changing contrast with C()
    data["a"] = C(data["a"], Helmert)
    t("a", data, 0,
      True,
      [[1, -1], [1, -1], [1, 1], [1, 1]], ["Intercept", "a[H.a2]"])
    t("C(a, Treatment)", data, 0,
      True,
      [[1, 0], [1, 0], [1, 1], [1, 1]], ["Intercept", "C(a, Treatment)[T.a2]"])
    # That didn't affect the original object
    t("a", data, 0,
      True,
      [[1, -1], [1, -1], [1, 1], [1, 1]], ["Intercept", "a[H.a2]"])

def test_builtins():
    data = {"x": [1, 2, 3],
            "y": [4, 5, 6],
            "a b c": [10, 20, 30]}
    t("0 + I(x + y)", data, 0,
      True,
      [[1], [2], [3], [4], [5], [6]], ["I(x + y)"])
    t("Q('a b c')", data, 0,
      True,
      [[1, 10], [1, 20], [1, 30]], ["Intercept", "Q('a b c')"])
    t("center(x)", data, 0,
      True,
      [[1, -1], [1, 0], [1, 1]], ["Intercept", "center(x)"])

def test_incremental():
    # incr_dbuilder(s)
    # stateful transformations
    datas = [
        {"a": ["a2", "a2", "a2"],
         "x": [1, 2, 3]},
        {"a": ["a2", "a2", "a1"],
         "x": [4, 5, 6]},
        ]
    x = np.asarray([1, 2, 3, 4, 5, 6])
    sin_center_x = np.sin(x - np.mean(x))
    x_col = sin_center_x - np.mean(sin_center_x)
    def data_iter_maker():
        return iter(datas)
    builders = incr_dbuilders("1 ~ a + center(np.sin(center(x)))",
                              data_iter_maker)
    lhs, rhs = build_design_matrices(builders, datas[1])
    assert lhs.design_info.column_names == ["Intercept"]
    assert rhs.design_info.column_names == ["Intercept",
                                            "a[T.a2]",
                                            "center(np.sin(center(x)))"]
    assert np.allclose(lhs, [[1], [1], [1]])
    assert np.allclose(rhs, np.column_stack(([1, 1, 1],
                                             [1, 1, 0],
                                             x_col[3:])))

    builder = incr_dbuilder("~ a + center(np.sin(center(x)))",
                            data_iter_maker)
    (rhs,) = build_design_matrices([builder], datas[1])
    assert rhs.design_info.column_names == ["Intercept",
                                            "a[T.a2]",
                                            "center(np.sin(center(x)))"]
    assert np.allclose(lhs, [[1], [1], [1]])
    assert np.allclose(rhs, np.column_stack(([1, 1, 1],
                                             [1, 1, 0],
                                             x_col[3:])))

    assert_raises(PatsyError, incr_dbuilder, "x ~ x", data_iter_maker)
    assert_raises(PatsyError, incr_dbuilders, "x", data_iter_maker)

def test_env_transform():
    t("~ np.sin(x)", {"x": [1, 2, 3]}, 0,
      True,
      [[1, np.sin(1)], [1, np.sin(2)], [1, np.sin(3)]],
      ["Intercept", "np.sin(x)"])

# Term ordering:
#   1) all 0-order no-numeric
#   2) all 1st-order no-numeric
#   3) all 2nd-order no-numeric
#   4) ...
#   5) all 0-order with the first numeric interaction encountered
#   6) all 1st-order with the first numeric interaction encountered
#   7) ...
#   8) all 0-order with the second numeric interaction encountered
#   9) ...
def test_term_order():
    data = balanced(a=2, b=2)
    data["x1"] = np.linspace(0, 1, 4)
    data["x2"] = data["x1"] ** 2

    def t_terms(formula, order):
        m = dmatrix(formula, data)
        assert m.design_info.term_names == order

    t_terms("a + b + x1 + x2", ["Intercept", "a", "b", "x1", "x2"])
    t_terms("b + a + x2 + x1", ["Intercept", "b", "a", "x2", "x1"])
    t_terms("0 + x1 + a + x2 + b + 1", ["Intercept", "a", "b", "x1", "x2"])
    t_terms("0 + a:b + a + b + 1", ["Intercept", "a", "b", "a:b"])
    t_terms("a + a:x1 + x2 + x1 + b",
            ["Intercept", "a", "b", "x1", "a:x1", "x2"])
    t_terms("0 + a:x1:x2 + a + x2:x1:b + x2 + x1 + a:x1 + x1:x2 + x1:a:x2:a:b",
            ["a",
             "x1:x2", "a:x1:x2", "x2:x1:b", "x1:a:x2:b",
             "x2",
             "x1",
             "a:x1"])

def _check_division(expect_true_division): # pragma: no cover
    # We evaluate the formula "I(x / y)" in our *caller's* scope, so the
    # result depends on whether our caller has done 'from __future__ import
    # division'.
    data = {"x": 5, "y": 2}
    m = dmatrix("0 + I(x / y)", data, 1)
    if expect_true_division:
        assert np.allclose(m, [[2.5]])
    else:
        assert np.allclose(m, [[2]])

def test_future():
    if __future__.division.getMandatoryRelease() < sys.version_info:
        # This is Python 3, where division is already default
        return
    # no __future__.division in this module's scope
    _check_division(False)
    # create an execution context where __future__.division is in effect
    exec ("from __future__ import division\n"
          "_check_division(True)\n")

def test_multicolumn():
    data = {
        "a": ["a1", "a2"],
        "X": [[1, 2], [3, 4]],
        "Y": [[1, 3], [2, 4]],
        }
    t("X*Y", data, 0,
      True,
      [[1, 1, 2, 1, 3, 1 * 1, 2 * 1, 1 * 3, 2 * 3],
       [1, 3, 4, 2, 4, 3 * 2, 4 * 2, 3 * 4, 4 * 4]],
      ["Intercept", "X[0]", "X[1]", "Y[0]", "Y[1]",
       "X[0]:Y[0]", "X[1]:Y[0]", "X[0]:Y[1]", "X[1]:Y[1]"])
    t("a:X + Y", data, 0,
      True,
      [[1, 1, 0, 2, 0, 1, 3],
       [1, 0, 3, 0, 4, 2, 4]],
      ["Intercept",
       "a[a1]:X[0]", "a[a2]:X[0]", "a[a1]:X[1]", "a[a2]:X[1]",
       "Y[0]", "Y[1]"])

def test_dmatrix_dmatrices_no_data():
    x = [1, 2, 3]
    y = [4, 5, 6]
    assert np.allclose(dmatrix("x"), [[1, 1], [1, 2], [1, 3]])
    lhs, rhs = dmatrices("y ~ x")
    assert np.allclose(lhs, [[4], [5], [6]])
    assert np.allclose(rhs, [[1, 1], [1, 2], [1, 3]])

def test_designinfo_describe():
    lhs, rhs = dmatrices("y ~ x + a", {"y": [1, 2, 3],
                                       "x": [4, 5, 6],
                                       "a": ["a1", "a2", "a3"]})
    assert lhs.design_info.describe() == "y"
    assert rhs.design_info.describe() == "1 + a + x"

def test_evalfactor_reraise():
    # This will produce a PatsyError, but buried inside the factor evaluation,
    # so the original code has no way to give it an appropriate origin=
    # attribute. EvalFactor should notice this, and add a useful origin:
    def raise_patsy_error(x):
        raise PatsyError("WHEEEEEE")
    formula = "raise_patsy_error(X) + Y"
    try:
        dmatrix(formula, {"X": [1, 2, 3], "Y": [4, 5, 6]})
    except PatsyError, e:
        assert e.origin == Origin(formula, 0, formula.index(" "))
    else:
        assert False
    # This will produce a KeyError, which on Python 3 we can do wrap without
    # destroying the traceback, so we do so. On Python 2 we let the original
    # exception escape.
    try:
        dmatrix("1 + x[1]", {"x": {}})
    except Exception, e:
        if sys.version_info[0] >= 3:
            assert isinstance(e, PatsyError)
            assert e.origin == Origin("1 + x[1]", 4, 8)
        else:
            assert isinstance(e, KeyError)
    else:
        assert False

def test_dmatrix_NA_action():
    data = {"x": [1, 2, 3, np.nan], "y": [np.nan, 20, 30, 40]}

    return_types = ["matrix"]
    if have_pandas:
        return_types.append("dataframe")

    for return_type in return_types:
        mat = dmatrix("x + y", data=data, return_type=return_type)
        assert np.array_equal(mat, [[1, 2, 20],
                                    [1, 3, 30]])
        if return_type == "dataframe":
            assert mat.index.equals([1, 2])
        assert_raises(PatsyError, dmatrix, "x + y", data=data,
                      return_type=return_type,
                      NA_action="raise")

        lmat, rmat = dmatrices("y ~ x", data=data, return_type=return_type)
        assert np.array_equal(lmat, [[20], [30]])
        assert np.array_equal(rmat, [[1, 2], [1, 3]])
        if return_type == "dataframe":
            assert lmat.index.equals([1, 2])
            assert rmat.index.equals([1, 2])
        assert_raises(PatsyError,
                      dmatrices, "y ~ x", data=data, return_type=return_type,
                      NA_action="raise")

        # Initial release for the NA handling code had problems with
        # non-data-dependent matrices like "~ 1".
        lmat, rmat = dmatrices("y ~ 1", data=data, return_type=return_type)
        assert np.array_equal(lmat, [[20], [30], [40]])
        assert np.array_equal(rmat, [[1], [1], [1]])
        if return_type == "dataframe":
            assert lmat.index.equals([1, 2, 3])
            assert rmat.index.equals([1, 2, 3])
        assert_raises(PatsyError,
                      dmatrices, "y ~ 1", data=data, return_type=return_type,
                      NA_action="raise")

########NEW FILE########
__FILENAME__ = test_regressions
# This file is part of Patsy
# Copyright (C) 2013 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# Regression tests for fixed bugs (when not otherwise better covered somewhere
# else)

from patsy import (EvalEnvironment, dmatrix, build_design_matrices,
                   PatsyError, Origin)

def test_issue_11():
    # Give a sensible error message for level mismatches
    # (At some points we've failed to put an origin= on these errors)
    env = EvalEnvironment.capture()
    data = {"X" : [0,1,2,3], "Y" : [1,2,3,4]}
    formula = "C(X) + Y"
    new_data = {"X" : [0,0,1,2,3,3,4], "Y" : [1,2,3,4,5,6,7]}
    info = dmatrix(formula, data)
    try:
        build_design_matrices([info.design_info.builder], new_data)
    except PatsyError, e:
        assert e.origin == Origin(formula, 0, 4)
    else:
        assert False

########NEW FILE########
__FILENAME__ = test_splines_bs_data
# This file auto-generated by tools/get-R-bs-test-vectors.R
# Using: R version 2.15.1 (2012-06-22)
import numpy as np
R_bs_test_x = np.array([1, 1.5, 2.25, 3.375, 5.0625, 7.59375, 11.390625, 17.0859375, 25.62890625, 38.443359375, 57.6650390625, 86.49755859375, 129.746337890625, 194.6195068359375, 291.92926025390625, 437.893890380859375, 656.8408355712890625, 985.26125335693359375, 1477.8918800354003906, 2216.8378200531005859, ])
R_bs_test_data = """
--BEGIN TEST CASE--
degree=1
df=3
intercept=TRUE
Boundary.knots=None
knots=None
output=np.array([1, 0.98937395581474985029, 0.97343488953687462573, 0.94952629012006184439, 0.91366339099484261688, 0.85986904230701377561, 0.77917751927527056921, 0.65814023472765570411, 0.47658430790623346196, 0.20425041767410004323, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.010626044185250137566, 0.026565110463125343049, 0.050473709879938155609, 0.086336609005157369245, 0.14013095769298619664, 0.22082248072472943079, 0.34185976527234429589, 0.52341569209376659355, 0.79574958232589998453, 0.99556855753085560234, 0.98227423012342263142, 0.96233273901227300851, 0.93242050234554862964, 0.88755214734546206135, 0.82024961484533231992, 0.71929581609513748575, 0.56786511796984540101, 0.34071907078190727391, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0044314424691443508181, 0.017725769876577403272, 0.037667260987726984556, 0.067579497654451342603, 0.11244785265453789702, 0.17975038515466773559, 0.28070418390486245874, 0.43213488203015459899, 0.6592809292180927816, 1, ]).reshape((20, 3, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=5
intercept=TRUE
Boundary.knots=None
knots=None
output=np.array([1, 0.9161205766710354137, 0.79030144167758842322, 0.60157273918741804852, 0.31847968545216254199, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.083879423328964614059, 0.20969855832241152127, 0.39842726081258189597, 0.68152031454783745801, 0.9846005774783446185, 0.89220404234841199642, 0.7536092396535130078, 0.54571703561116458037, 0.23387872954764196698, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.015399422521655438748, 0.10779595765158807297, 0.24639076034648701996, 0.45428296438883541963, 0.76612127045235811629, 0.96572040707016604255, 0.86288162828066417021, 0.7086234600964114172, 0.47723620782003217666, 0.13015532940546331586, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.034279592929833957449, 0.13711837171933582979, 0.29137653990358863831, 0.52276379217996793436, 0.86984467059453673965, 0.94202898550724645244, 0.82608695652173913526, 0.65217391304347827052, 0.39130434782608697342, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.057971014492753623892, 0.17391304347826089249, 0.34782608695652178499, 0.60869565217391308209, 1, ]).reshape((20, 5, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=12
intercept=TRUE
Boundary.knots=None
knots=None
output=np.array([1, 0.52173913043478281626, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.47826086956521718374, 0.90243902439024414885, 0.36585365853658557977, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.097560975609755906657, 0.63414634146341442023, 0.77777777777777779011, 0.16666666666666651864, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.22222222222222218213, 0.83333333333333348136, 0.625, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.37499999999999994449, 0.96992481203007530066, 0.47368421052631592971, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.030075187969924695869, 0.52631578947368407029, 0.86440677966101697738, 0.30508474576271171763, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.13559322033898302262, 0.69491525423728828237, 0.72815533980582491935, 0.087378640776698143777, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.27184466019417513616, 0.91262135922330189786, 0.57446808510638269762, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.42553191489361730238, 0.9375, 0.4218750000000004996, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.062499999999999958367, 0.57812499999999944489, 0.82300884955752262595, 0.23893805309734547637, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.17699115044247745732, 0.76106194690265460689, 0.67346938775510212238, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.32653061224489787762, 1, ]).reshape((20, 12, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=None
intercept=TRUE
Boundary.knots=None
knots=np.array([])
output=np.array([1, 0.99977435171677508929, 0.9994358792919375567, 0.99892817065468131332, 0.99816660769879694826, 0.99702426326497040066, 0.99531074661423057925, 0.99274047163812084715, 0.98888505917395630451, 0.98310194047770937953, 0.97442726243333910308, 0.96141524536678357737, 0.94189721976695039984, 0.91262018136720068906, 0.86870462376757595635, 0.8028312873681389128, 0.70402128276898334747, 0.55580627587024999947, 0.33348376552215003299, 0, 0, 0.00022564828322499611425, 0.00056412070806249024497, 0.001071829345318731563, 0.0018333923012030933775, 0.0029757367350296362075, 0.0046892533857694502358, 0.0072595283618791719288, 0.011114940826043754468, 0.01689805952229062741, 0.025572737566660935088, 0.038584754633216401809, 0.058102780233049600156, 0.087379818632799394207, 0.13129537623242407141, 0.19716871263186111496, 0.29597871723101670804, 0.44419372412975000053, 0.66651623447785002252, 1, ]).reshape((20, 2, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=None
intercept=TRUE
Boundary.knots=None
knots=np.array([100, ])
output=np.array([1, 0.994949494949495028, 0.98737373737373745897, 0.97601010101010110542, 0.9589646464646465196, 0.93339646464646475188, 0.89504419191919204479, 0.83751578282828287314, 0.75122316919191922668, 0.62178424873737381251, 0.42762586805555558023, 0.13638829703282828731, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0050505050505050509344, 0.01262626262626262777, 0.023989898989898991721, 0.041035353535353535914, 0.066603535353535359143, 0.10495580808080809398, 0.16248421717171718237, 0.24877683080808082883, 0.37821575126262629851, 0.5723741319444445308, 0.86361170296717182371, 0.98594774828339049044, 0.95530148510216805757, 0.90933209033033446378, 0.84037799817258396207, 0.7369468599359582095, 0.58180015258101969167, 0.3490800915486118039, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.014052251716609454046, 0.044698514897831886916, 0.090667909669665536221, 0.15962200182741601018, 0.26305314006404173499, 0.41819984741898030833, 0.65091990845138814059, 1, ]).reshape((20, 3, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=None
intercept=TRUE
Boundary.knots=None
knots=np.array([1000, ])
output=np.array([1, 0.99949949949949945527, 0.99874874874874874919, 0.99762262262262257906, 0.99593343343343343488, 0.9933996496496496631, 0.98959897397397400542, 0.98389796046046040789, 0.97534644019019023364, 0.96251915978478475022, 0.94327823917667663611, 0.91441685826451446495, 0.87112478689627126371, 0.80618667984390635084, 0.70877951926535909255, 0.5626687783975381496, 0.34350266709580673519, 0.014753500143209615295, 0, 0, 0, 0.00050050050050050049616, 0.0012512512512512512404, 0.0023773773773773775736, 0.0040665665665665668566, 0.0066003503503503499136, 0.0104010260260260258, 0.016102039539539540064, 0.02465355980980980799, 0.037480840215215215083, 0.056721760823323322254, 0.08558314173548547954, 0.12887521310372873629, 0.19381332015609359365, 0.29122048073464090745, 0.4373312216024618504, 0.6564973329041932093, 0.98524649985679035868, 0.60726740066762052717, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.39273259933237941732, 1, ]).reshape((20, 3, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=None
intercept=TRUE
Boundary.knots=None
knots=np.array([10, 100, 1000, ])
output=np.array([1, 0.94444444444444441977, 0.86111111111111104943, 0.73611111111111104943, 0.54861111111111104943, 0.26736111111111110494, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.055555555555555552472, 0.13888888888888889506, 0.26388888888888889506, 0.45138888888888883955, 0.73263888888888883955, 0.98454861111111113825, 0.92126736111111118266, 0.82634548611111113825, 0.68396267361111118266, 0.47038845486111113825, 0.15002712673611112715, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.015451388888888889506, 0.07873263888888888673, 0.17365451388888888951, 0.31603732638888892836, 0.52961154513888886175, 0.84997287326388892836, 0.96694851345486110272, 0.89486721462673612937, 0.78674526638454855831, 0.62456234402126731275, 0.38128796047634549993, 0.016376385158962673133, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.033051486545138890338, 0.10513278537326388451, 0.21325473361545138618, 0.37543765597873263173, 0.61871203952365450007, 0.98362361484103733034, 0.60726740066762052717, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.39273259933237941732, 1, ]).reshape((20, 5, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=3
intercept=TRUE
Boundary.knots=np.array([0, 3000, ])
knots=None
output=np.array([0.97919016410100090386, 0.9687852461515014113, 0.95317786922725200593, 0.92976680384087795339, 0.89465020576131693009, 0.84197530864197533962, 0.76296296296296306494, 0.64444444444444448639, 0.46666666666666667407, 0.2000000000000000111, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.020809835898999137771, 0.031214753848498706656, 0.046822130772748063454, 0.070233196159122088242, 0.1053497942386831393, 0.15802469135802471589, 0.23703703703703704608, 0.35555555555555556912, 0.53333333333333332593, 0.80000000000000004441, 0.99674423566950098863, 0.98697694267800384349, 0.9723260031907582368, 0.95034959395988971576, 0.9173849801135870452, 0.86793805934413292835, 0.7937676781899518641, 0.68251210645868021221, 0.51562874886177267886, 0.26530371246641143435, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0032557643304990334897, 0.013023057321996133959, 0.027673996809241787481, 0.049650406040110263428, 0.082615019886412982553, 0.13206194065586704389, 0.20623232181004816366, 0.3174878935413198433, 0.48437125113822732114, 0.73469628753358862117, ]).reshape((20, 3, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=5
intercept=TRUE
Boundary.knots=np.array([0, 3000, ])
knots=None
output=np.array([0.85634118967452310667, 0.78451178451178460449, 0.67676767676767679571, 0.51515151515151524908, 0.2727272727272727626, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.14365881032547700435, 0.21548821548821550653, 0.3232323232323232598, 0.48484848484848486194, 0.72727272727272729291, 0.9846005774783446185, 0.89220404234841199642, 0.7536092396535130078, 0.54571703561116458037, 0.23387872954764196698, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.015399422521655438748, 0.10779595765158807297, 0.24639076034648701996, 0.45428296438883541963, 0.76612127045235811629, 0.96572040707016604255, 0.86288162828066417021, 0.7086234600964114172, 0.47723620782003217666, 0.13015532940546331586, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.034279592929833957449, 0.13711837171933582979, 0.29137653990358863831, 0.52276379217996793436, 0.86984467059453673965, 0.9590229415870602514, 0.8770688247611807542, 0.7541376495223615084, 0.56974088666413258419, 0.29314574237678914237, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.040977058412939762477, 0.12293117523881928743, 0.24586235047763857486, 0.43025911333586747132, 0.70685425762321085763, ]).reshape((20, 5, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=12
intercept=TRUE
Boundary.knots=np.array([0, 3000, ])
knots=None
output=np.array([0.51111111111111118266, 0.2666666666666668295, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.48888888888888881734, 0.7333333333333331705, 0.90243902439024414885, 0.36585365853658557977, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.097560975609755906657, 0.63414634146341442023, 0.77777777777777779011, 0.16666666666666651864, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.22222222222222218213, 0.83333333333333348136, 0.625, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.37499999999999994449, 0.96992481203007530066, 0.47368421052631592971, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.030075187969924695869, 0.52631578947368407029, 0.86440677966101697738, 0.30508474576271171763, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.13559322033898302262, 0.69491525423728828237, 0.72815533980582491935, 0.087378640776698143777, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.27184466019417513616, 0.91262135922330189786, 0.57446808510638269762, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.42553191489361730238, 0.9375, 0.4218750000000004996, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.062499999999999958367, 0.57812499999999944489, 0.82300884955752262595, 0.23893805309734547637, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.17699115044247745732, 0.76106194690265460689, 0.80946623645948467818, 0.41649034915717175753, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.19053376354051526631, 0.58350965084282824247, ]).reshape((20, 12, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=None
intercept=TRUE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([])
output=np.array([0.99966666666666659236, 0.99949999999999994404, 0.99924999999999997158, 0.99887499999999995737, 0.99831249999999993605, 0.9974687500000000151, 0.99620312499999996714, 0.99430468750000000622, 0.99145703124999995381, 0.98718554687499993072, 0.98077832031250000711, 0.97116748046875001066, 0.95675122070312501599, 0.93512683105468752398, 0.90269024658203123046, 0.85403536987304684569, 0.78105305480957032405, 0.67157958221435543056, 0.50736937332153320135, 0.26105405998229980202, 0.0003333333333333333222, 0.00050000000000000001041, 0.00075000000000000001561, 0.001124999999999999915, 0.0016874999999999999809, 0.0025312500000000000798, 0.0037968749999999999029, 0.0056953124999999998543, 0.0085429687499999993477, 0.012814453124999999889, 0.019221679687499999833, 0.02883251953124999975, 0.043248779296874997891, 0.064873168945312503775, 0.097309753417968741784, 0.14596463012695312655, 0.21894694519042967595, 0.32842041778564451393, 0.49263062667846679865, 0.73894594001770019798, ]).reshape((20, 2, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=None
intercept=TRUE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([100, ])
output=np.array([0.98999999999999999112, 0.98499999999999998668, 0.97750000000000003553, 0.96625000000000005329, 0.94937499999999996891, 0.92406250000000000888, 0.88609375000000001332, 0.82914062499999996447, 0.74371093750000005773, 0.61556640625000003109, 0.42334960937499999112, 0.13502441406250001443, 0, 0, 0, 0, 0, 0, 0, 0, 0.010000000000000000208, 0.014999999999999999445, 0.022499999999999999167, 0.03375000000000000222, 0.050625000000000003331, 0.075937500000000004996, 0.11390625000000000056, 0.17085937500000000777, 0.25628906249999999778, 0.38443359375000002442, 0.57665039062500000888, 0.86497558593750001332, 0.98974264210668094766, 0.96737258384967661495, 0.93381749646417022692, 0.88348486538591053385, 0.80798591876852099425, 0.69473749884243662933, 0.52486486895331019298, 0.27005592411962048294, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.010257357893318963873, 0.032627416150323274024, 0.066182503535829731445, 0.11651513461408942451, 0.192014081231478978, 0.30526250115756325965, 0.47513513104668975151, 0.72994407588037946155, ]).reshape((20, 3, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=None
intercept=TRUE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([1000, ])
output=np.array([0.99899999999999999911, 0.99850000000000005418, 0.99775000000000002576, 0.99662499999999998312, 0.9949375000000000302, 0.9924062500000000453, 0.98860937500000001243, 0.98291406250000001865, 0.97437109374999997247, 0.96155664062500001421, 0.94233496093750002132, 0.91350244140625003197, 0.87025366210937504796, 0.80538049316406257194, 0.7080707397460938024, 0.56210610961914064809, 0.34315916442871097214, 0.014738746643066406167, 0, 0, 0.0010000000000000000208, 0.0015000000000000000312, 0.0022500000000000002637, 0.0033749999999999999618, 0.0050625000000000001596, 0.0075937499999999998057, 0.011390624999999999709, 0.017085937499999998695, 0.025628906249999999778, 0.038443359374999999667, 0.0576650390624999995, 0.086497558593749995781, 0.12974633789062500755, 0.19461950683593751132, 0.29192926025390625311, 0.43789389038085940742, 0.65684083557128902786, 0.9852612533569335973, 0.76105405998229980202, 0.39158108997344970303, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.23894594001770019798, 0.60841891002655035248, ]).reshape((20, 3, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=None
intercept=TRUE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([10, 100, 1000, ])
output=np.array([0.9000000000000000222, 0.85000000000000008882, 0.7750000000000000222, 0.66250000000000008882, 0.4937500000000000222, 0.24062500000000000555, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.10000000000000000555, 0.1500000000000000222, 0.22500000000000000555, 0.3375000000000000222, 0.5062499999999999778, 0.7593750000000000222, 0.98454861111111113825, 0.92126736111111118266, 0.82634548611111113825, 0.68396267361111118266, 0.47038845486111113825, 0.15002712673611112715, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.015451388888888889506, 0.07873263888888888673, 0.17365451388888888951, 0.31603732638888892836, 0.52961154513888886175, 0.84997287326388892836, 0.96694851345486110272, 0.89486721462673612937, 0.78674526638454855831, 0.62456234402126731275, 0.38128796047634549993, 0.016376385158962673133, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.033051486545138890338, 0.10513278537326388451, 0.21325473361545138618, 0.37543765597873263173, 0.61871203952365450007, 0.98362361484103733034, 0.76105405998229980202, 0.39158108997344970303, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.23894594001770019798, 0.60841891002655035248, ]).reshape((20, 5, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=3
intercept=FALSE
Boundary.knots=None
knots=None
output=np.array([0, 0.040686586141131603211, 0.10171646535282900803, 0.19326128417037510832, 0.33057851239669427956, 0.53655435473617296704, 0.84551811824539113704, 0.97622585438335818253, 0.92273402674591387118, 0.84249628528974740416, 0.72213967310549775913, 0.54160475482912329159, 0.27080237741456153477, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.023774145616641918083, 0.077265973254086212085, 0.1575037147102526236, 0.27786032689450229638, 0.45839524517087676392, 0.72919762258543852074, 0.98941973879980160689, 0.9418085633989089489, 0.87039180029756990642, 0.76326665564556128718, 0.60257893866754841383, 0.3615473632005290483, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.010580261200198394847, 0.058191436601091106606, 0.12960819970243014909, 0.23673334435443876833, 0.39742106133245164168, 0.63845263679947106272, 1, ]).reshape((20, 3, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=5
intercept=FALSE
Boundary.knots=None
knots=None
output=np.array([0, 0.1342281879194630323, 0.33557046979865756686, 0.63758389261744941034, 0.98069963811821481148, 0.83594692400482528694, 0.61881785283474100012, 0.2931242460796145699, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.019300361881785188523, 0.16405307599517471306, 0.38118214716525899988, 0.70687575392038548561, 0.9581151832460734763, 0.80104712041884851281, 0.56544502617801106759, 0.21204188481675489975, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.041884816753926495947, 0.19895287958115145943, 0.4345549738219888769, 0.78795811518324510025, 0.93133047210300479168, 0.75965665236051571618, 0.50214592274678215844, 0.11587982832618169693, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.068669527896995374849, 0.2403433476394844226, 0.49785407725321800809, 0.88412017167381840022, 0.89905362776025266047, 0.70977917981072580211, 0.42586750788643545906, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.10094637223974732565, 0.29022082018927419789, 0.57413249211356454094, 1, ]).reshape((20, 5, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=12
intercept=FALSE
Boundary.knots=None
knots=None
output=np.array([0, 0.53333333333333343695, 0.81818181818181801024, 0.16363636363636296922, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.18181818181818207303, 0.83636363636363708629, 0.57446808510638280865, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.42553191489361724686, 0.9000000000000000222, 0.29999999999999982236, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.10000000000000003331, 0.70000000000000017764, 0.67346938775510178932, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.32653061224489815517, 0.96923076923076900702, 0.41538461538461529665, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.030769230769230916656, 0.58461538461538464784, 0.7714285714285711304, 0.085714285714284479956, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.22857142857142889736, 0.91428571428571558943, 0.5217391304347820391, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.47826086956521790539, 0.86086956521739110837, 0.23478260869565212299, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.13913043478260883612, 0.76521739130434784926, 0.62499999999999966693, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.37500000000000027756, 0.93599999999999949907, 0.35999999999999854339, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.064000000000000500933, 0.64000000000000145661, 0.7199999999999991962, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2800000000000008038, 1, ]).reshape((20, 12, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=None
intercept=FALSE
Boundary.knots=None
knots=np.array([])
output=np.array([0, 0.00022564828322499611425, 0.00056412070806249024497, 0.001071829345318731563, 0.0018333923012030933775, 0.0029757367350296362075, 0.0046892533857694502358, 0.0072595283618791719288, 0.011114940826043754468, 0.01689805952229062741, 0.025572737566660935088, 0.038584754633216401809, 0.058102780233049600156, 0.087379818632799394207, 0.13129537623242407141, 0.19716871263186111496, 0.29597871723101670804, 0.44419372412975000053, 0.66651623447785002252, 1, ]).reshape((20, 1, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=None
intercept=FALSE
Boundary.knots=None
knots=np.array([100, ])
output=np.array([0, 0.0050505050505050509344, 0.01262626262626262777, 0.023989898989898991721, 0.041035353535353535914, 0.066603535353535359143, 0.10495580808080809398, 0.16248421717171718237, 0.24877683080808082883, 0.37821575126262629851, 0.5723741319444445308, 0.86361170296717182371, 0.98594774828339049044, 0.95530148510216805757, 0.90933209033033446378, 0.84037799817258396207, 0.7369468599359582095, 0.58180015258101969167, 0.3490800915486118039, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.014052251716609454046, 0.044698514897831886916, 0.090667909669665536221, 0.15962200182741601018, 0.26305314006404173499, 0.41819984741898030833, 0.65091990845138814059, 1, ]).reshape((20, 2, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=None
intercept=FALSE
Boundary.knots=None
knots=np.array([1000, ])
output=np.array([0, 0.00050050050050050049616, 0.0012512512512512512404, 0.0023773773773773775736, 0.0040665665665665668566, 0.0066003503503503499136, 0.0104010260260260258, 0.016102039539539540064, 0.02465355980980980799, 0.037480840215215215083, 0.056721760823323322254, 0.08558314173548547954, 0.12887521310372873629, 0.19381332015609359365, 0.29122048073464090745, 0.4373312216024618504, 0.6564973329041932093, 0.98524649985679035868, 0.60726740066762052717, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.39273259933237941732, 1, ]).reshape((20, 2, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=None
intercept=FALSE
Boundary.knots=None
knots=np.array([10, 100, 1000, ])
output=np.array([0, 0.055555555555555552472, 0.13888888888888889506, 0.26388888888888889506, 0.45138888888888883955, 0.73263888888888883955, 0.98454861111111113825, 0.92126736111111118266, 0.82634548611111113825, 0.68396267361111118266, 0.47038845486111113825, 0.15002712673611112715, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.015451388888888889506, 0.07873263888888888673, 0.17365451388888888951, 0.31603732638888892836, 0.52961154513888886175, 0.84997287326388892836, 0.96694851345486110272, 0.89486721462673612937, 0.78674526638454855831, 0.62456234402126731275, 0.38128796047634549993, 0.016376385158962673133, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.033051486545138890338, 0.10513278537326388451, 0.21325473361545138618, 0.37543765597873263173, 0.61871203952365450007, 0.98362361484103733034, 0.60726740066762052717, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.39273259933237941732, 1, ]).reshape((20, 4, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=3
intercept=FALSE
Boundary.knots=np.array([0, 3000, ])
knots=None
output=np.array([0.075249853027630819735, 0.1128747795414462296, 0.16931216931216935828, 0.25396825396825400967, 0.38095238095238104226, 0.57142857142857150787, 0.85714285714285731732, 0.97622585438335818253, 0.92273402674591387118, 0.84249628528974740416, 0.72213967310549775913, 0.54160475482912329159, 0.27080237741456153477, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.023774145616641918083, 0.077265973254086212085, 0.1575037147102526236, 0.27786032689450229638, 0.45839524517087676392, 0.72919762258543852074, 0.99235077739698696053, 0.95792927568342844946, 0.90629702311309068286, 0.82884864425758408846, 0.71267607597432414135, 0.53841722354943410966, 0.27702894491209917316, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0076492226030130125794, 0.042070724316571522783, 0.093702976886909289389, 0.1711513557424159393, 0.28732392402567591416, 0.46158277645056589034, 0.72297105508790082684, ]).reshape((20, 3, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=5
intercept=FALSE
Boundary.knots=np.array([0, 3000, ])
knots=None
output=np.array([0.21164021164021157295, 0.31746031746031733167, 0.47619047619047605302, 0.71428571428571407953, 0.98069963811821481148, 0.83594692400482528694, 0.61881785283474100012, 0.2931242460796145699, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.019300361881785188523, 0.16405307599517471306, 0.38118214716525899988, 0.70687575392038548561, 0.9581151832460734763, 0.80104712041884851281, 0.56544502617801106759, 0.21204188481675489975, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.041884816753926495947, 0.19895287958115145943, 0.4345549738219888769, 0.78795811518324510025, 0.93133047210300479168, 0.75965665236051571618, 0.50214592274678215844, 0.11587982832618169693, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.068669527896995374849, 0.2403433476394844226, 0.49785407725321800809, 0.88412017167381840022, 0.93044657380826634174, 0.80003389969876526067, 0.60441488853451352803, 0.31098637178813609561, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.069553426191733672135, 0.19996610030123476709, 0.39558511146548641646, 0.68901362821186384888, ]).reshape((20, 5, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=12
intercept=FALSE
Boundary.knots=np.array([0, 3000, ])
knots=None
output=np.array([0.51612903225806461283, 0.77419354838709697475, 0.81818181818181801024, 0.16363636363636296922, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.18181818181818207303, 0.83636363636363708629, 0.57446808510638280865, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.42553191489361724686, 0.9000000000000000222, 0.29999999999999982236, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.10000000000000003331, 0.70000000000000017764, 0.67346938775510178932, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.32653061224489815517, 0.96923076923076900702, 0.41538461538461529665, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.030769230769230916656, 0.58461538461538464784, 0.7714285714285711304, 0.085714285714284479956, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.22857142857142889736, 0.91428571428571558943, 0.5217391304347820391, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.47826086956521790539, 0.86086956521739110837, 0.23478260869565212299, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.13913043478260883612, 0.76521739130434784926, 0.62499999999999966693, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.37500000000000027756, 0.93599999999999949907, 0.35999999999999854339, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.064000000000000500933, 0.64000000000000145661, 0.84118724544512846197, 0.43281159087546033915, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.15881275455487159354, 0.56718840912453971637, ]).reshape((20, 12, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=None
intercept=FALSE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([])
output=np.array([0.0003333333333333333222, 0.00050000000000000001041, 0.00075000000000000001561, 0.001124999999999999915, 0.0016874999999999999809, 0.0025312500000000000798, 0.0037968749999999999029, 0.0056953124999999998543, 0.0085429687499999993477, 0.012814453124999999889, 0.019221679687499999833, 0.02883251953124999975, 0.043248779296874997891, 0.064873168945312503775, 0.097309753417968741784, 0.14596463012695312655, 0.21894694519042967595, 0.32842041778564451393, 0.49263062667846679865, 0.73894594001770019798, ]).reshape((20, 1, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=None
intercept=FALSE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([100, ])
output=np.array([0.010000000000000000208, 0.014999999999999999445, 0.022499999999999999167, 0.03375000000000000222, 0.050625000000000003331, 0.075937500000000004996, 0.11390625000000000056, 0.17085937500000000777, 0.25628906249999999778, 0.38443359375000002442, 0.57665039062500000888, 0.86497558593750001332, 0.98974264210668094766, 0.96737258384967661495, 0.93381749646417022692, 0.88348486538591053385, 0.80798591876852099425, 0.69473749884243662933, 0.52486486895331019298, 0.27005592411962048294, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.010257357893318963873, 0.032627416150323274024, 0.066182503535829731445, 0.11651513461408942451, 0.192014081231478978, 0.30526250115756325965, 0.47513513104668975151, 0.72994407588037946155, ]).reshape((20, 2, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=None
intercept=FALSE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([1000, ])
output=np.array([0.0010000000000000000208, 0.0015000000000000000312, 0.0022500000000000002637, 0.0033749999999999999618, 0.0050625000000000001596, 0.0075937499999999998057, 0.011390624999999999709, 0.017085937499999998695, 0.025628906249999999778, 0.038443359374999999667, 0.0576650390624999995, 0.086497558593749995781, 0.12974633789062500755, 0.19461950683593751132, 0.29192926025390625311, 0.43789389038085940742, 0.65684083557128902786, 0.9852612533569335973, 0.76105405998229980202, 0.39158108997344970303, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.23894594001770019798, 0.60841891002655035248, ]).reshape((20, 2, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=1
df=None
intercept=FALSE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([10, 100, 1000, ])
output=np.array([0.10000000000000000555, 0.1500000000000000222, 0.22500000000000000555, 0.3375000000000000222, 0.5062499999999999778, 0.7593750000000000222, 0.98454861111111113825, 0.92126736111111118266, 0.82634548611111113825, 0.68396267361111118266, 0.47038845486111113825, 0.15002712673611112715, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.015451388888888889506, 0.07873263888888888673, 0.17365451388888888951, 0.31603732638888892836, 0.52961154513888886175, 0.84997287326388892836, 0.96694851345486110272, 0.89486721462673612937, 0.78674526638454855831, 0.62456234402126731275, 0.38128796047634549993, 0.016376385158962673133, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.033051486545138890338, 0.10513278537326388451, 0.21325473361545138618, 0.37543765597873263173, 0.61871203952365450007, 0.98362361484103733034, 0.76105405998229980202, 0.39158108997344970303, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.23894594001770019798, 0.60841891002655035248, ]).reshape((20, 4, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=5
intercept=TRUE
Boundary.knots=None
knots=None
output=np.array([1, 0.96845940607276881362, 0.92240303675860391142, 0.85609306993676004272, 0.76270864919645953162, 0.63576547531534310931, 0.47305239057526615731, 0.28507250058887945166, 0.10824783418564158655, 0.0085209665393945269174, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.031533426700978403612, 0.077552412011353655252, 0.14374742102560256196, 0.23683044051100490823, 0.36304322063581223601, 0.52407464745263665495, 0.70834870097260460575, 0.8774087197542927985, 0.96206652916722557034, 0.94530079752693674244, 0.90793500327984399956, 0.85375309463923021447, 0.77659028109030259213, 0.66978920735324132263, 0.52868210857343611586, 0.35651607003382906891, 0.175425242216674937, 0.037891852318801787225, 0, 0, 7.1666852050360488124e-06, 4.4542776169713856592e-05, 0.00015945105252372870864, 0.00046062008693220732759, 0.0011900631879368495612, 0.002868106285078954841, 0.0065607821717948554968, 0.014278782285967288324, 0.029185282443915855355, 0.053915675965905400513, 0.089616579303442189808, 0.13947317683937743293, 0.20621746127156848072, 0.28916251068209719577, 0.37804948423631462573, 0.44191546230800060613, 0.4167872147195200716, 0.22637517283088642861, 0, 0, 5.4104786148500394417e-10, 8.4538728357031864466e-09, 5.7985113780088147304e-08, 2.9020560343812349123e-07, 1.2408609078339073411e-06, 4.8556870182056249453e-06, 1.8016266721020254751e-05, 6.4663774098311141273e-05, 0.00022722184946421446292, 0.0007834394838982231728, 0.0024428479280998137424, 0.0067202853620942429314, 0.016883622849287124174, 0.039626432891546964354, 0.087460636122680179838, 0.179450426719582945, 0.32709043481102628714, 0.44917563313558994675, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8.7023259593866311309e-08, 5.5694886140074439238e-06, 5.3443159298083143627e-05, 0.00030863478884180254799, 0.001421849073114332283, 0.0058077710675691540318, 0.022118040938587192612, 0.080697108252778662618, 0.28655734171472191374, 1, ]).reshape((20, 5, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=12
intercept=TRUE
Boundary.knots=None
knots=None
output=np.array([1, 0.25770097670924113631, 0.00075131480090157780165, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.64290814257309680801, 0.55191727963673464785, 0.16384783952351522629, 0.0025601224925549254108, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.097791408043809202599, 0.4223396450334132024, 0.68523715148898289851, 0.53751029745031897455, 0.17971670556949506659, 0.0066561742803516550301, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0015994726738528394164, 0.024991760528950617698, 0.14994115610379393777, 0.44102623014868380658, 0.69378612010084461659, 0.57911832988268308053, 0.21512754628498662046, 0.013768162962239142988, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00097385288370810028057, 0.018903349908442362848, 0.12592791140580222864, 0.39992732893636862013, 0.67912381051853998315, 0.61416869876328705757, 0.2513505357806551932, 0.023605422218318485722, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00056926292385829722204, 0.014298166900596829057, 0.10543398177170558438, 0.36126140155236396989, 0.66048745076429016265, 0.64290852334776726895, 0.28801011480255123143, 0.036001264350318869234, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00031466142476788003723, 0.010801736722109872915, 0.088001307706519454888, 0.32534582615335716493, 0.63869155767138829916, 0.66560703209160065885, 0.32480624491998255632, 0.050709395542809954094, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00016070574853527342459, 0.0081402282805572035579, 0.073225347399450108066, 0.29228033065100789134, 0.61441387376474365656, 0.68269380491744569017, 0.36150149884261828515, 0.067452174711159121334, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7.2980126610408391891e-05, 0.006111372907072773128, 0.06075244688059205922, 0.26203239546958334572, 0.58821589703491272694, 0.69467005250274826977, 0.39790980589205254825, 0.08594851807268334698, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2.7434434681658694525e-05, 0.004564404070160951038, 0.05026345997459336773, 0.22896647107718723357, 0.49280704180866574671, 0.39171522957479332216, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.9144147875867501007e-05, 0.0089113017089054759323, 0.10925932166656721067, 0.44968558969337457665, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2.383063271446895644e-05, 0.072650662659148546041, 1, ]).reshape((20, 12, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=None
intercept=TRUE
Boundary.knots=None
knots=np.array([])
output=np.array([1, 0.9993232078902789528, 0.99830859239281100059, 0.99678795718714330309, 0.99450990091574942298, 0.99109932847208381812, 0.98599810402219045802, 0.97837913458786918142, 0.96702443008955374371, 0.95015762953345339614, 0.92522695834704149487, 0.88865464163271656872, 0.83562330741598667139, 0.7600990769655836532, 0.65556596659430688145, 0.51745533329454529436, 0.34894530919915062173, 0.17170001728344266856, 0.037087203733223626789, 0, 0, 0.00067663938125675493745, 0.0016904532697119580269, 0.0032085988210942785123, 0.0054800274275235618532, 0.0088741592009194299184, 0.013936134910759763808, 0.021463528321228261819, 0.032607690503775640933, 0.048995387512752772152, 0.072844594278869884141, 0.10699389717286836299, 0.15464119556617272888, 0.21832955541935578081, 0.29724527028257624606, 0.38124822804791375086, 0.44010197217760271826, 0.41166179703994132399, 0.22237265440082615298, 0, 0, 1.5271697506625337419e-07, 9.5415795571804482003e-07, 3.4427604254568415078e-06, 1.0065494095385778498e-05, 2.6485976820866183047e-05, 6.5657954600734260006e-05, 0.00015695450829847096968, 0.00036650624565625982386, 0.00084215780726247437687, 0.0019117237011529813161, 0.0042940168564642817658, 0.0095393459205458924072, 0.020904202366139650049, 0.044925430954654600735, 0.093631406124953353576, 0.18502397635429304601, 0.32899518168053926148, 0.4444443765651269751, 0, 0, 1.1489366970270387462e-11, 1.7952135891047479178e-10, 1.2313370007669466487e-09, 6.1626316488486427104e-09, 2.6350175916113057325e-08, 1.0311244903883365944e-07, 3.825826041044410135e-07, 1.3731610143675871737e-06, 4.8251465313854754769e-06, 1.6723672935651049261e-05, 5.7444337950753896285e-05, 0.0001961510972947611009, 0.00066716524892103249485, 0.0022633321684622557078, 0.0076650325325876428328, 0.025928742268953787475, 0.087643003996076676576, 0.29609576530082332146, 1, ]).reshape((20, 4, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=None
intercept=TRUE
Boundary.knots=None
knots=np.array([100, ])
output=np.array([1, 0.98492487882601165161, 0.96259746673448087773, 0.92974304223814019377, 0.88187654067159670923, 0.81320203136184721071, 0.71702357673990413378, 0.58746094552406258327, 0.42394246616286163087, 0.24039152271518871018, 0.078197326716827678106, 0.0025370634003338671768, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.015071708273510188089, 0.037381259008501491192, 0.070180462942842899987, 0.11790098280764790828, 0.18621717511273339074, 0.28155389351842002865, 0.40920060085307918829, 0.56848075164647782209, 0.74296035718046105067, 0.88664340493727211712, 0.92755941161894106539, 0.87470363121335126255, 0.79564729322794469635, 0.68622539174158070363, 0.5416556180357140482, 0.36526473871966497198, 0.17973006169687555378, 0.038821693326525116841, 0, 0, 3.4126433211387775282e-06, 2.1270238934697356366e-05, 7.6467258985853174243e-05, 0.00022233858750032365649, 0.00058020375050652503793, 0.0014202218581983020763, 0.003329890582419185209, 0.0075460478259266952628, 0.0165401227086783148, 0.03478495575013232366, 0.06861779430820802439, 0.12096540781627201921, 0.191329633412285538, 0.27905349779077742722, 0.37374631578125083742, 0.44360195032526716918, 0.42250875072786614473, 0.23095694223535656597, 0, 0, 2.5715731172923263021e-10, 4.0180829957692598212e-09, 2.7560031267981358852e-08, 1.3793325533914163604e-07, 5.8977491272702679876e-07, 2.3078834777630869564e-06, 8.5630404390810414166e-06, 3.0734364733920997454e-05, 0.00010799739567214114531, 0.00037431259576781883034, 0.0012857306725170922646, 0.0043281861315545133023, 0.012933767638577380737, 0.033975759516814023342, 0.080531027913042049771, 0.17293083479161802662, 0.32462175106248042367, 0.45442872954598834134, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2.7748388221855075202e-06, 8.9305721192384863918e-05, 0.00074535095082784331034, 0.0040670382699930507364, 0.018202476163449766294, 0.07313943651277800273, 0.27579263489213001748, 1, ]).reshape((20, 5, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=None
intercept=TRUE
Boundary.knots=None
knots=np.array([1000, ])
output=np.array([1, 0.9984992498753757495, 0.99625094117633150592, 0.99288481020069696559, 0.98784984394255781481, 0.98032935528140474624, 0.96912034075214636974, 0.95246753354617186282, 0.92784773021983435459, 0.89171926591812056273, 0.83930429776320658597, 0.76459714573223325207, 0.66106035705829924165, 0.52397052211349615103, 0.35606843795797349372, 0.17813877131971986301, 0.040531281972235988498, 3.2113319168708665608e-06, 0, 0, 0, 0.0015004113953972979083, 0.0037469425348356701772, 0.0071075541598605747329, 0.012127833209738946019, 0.019611910086607075437, 0.030734076107229148234, 0.04718451762439003494, 0.071340002591639736784, 0.10641511473995685089, 0.15646348082190494888, 0.22590626848315542574, 0.31787571113275914225, 0.42998547021895328069, 0.54537912943555955092, 0.61788880868397033641, 0.56161589860385985329, 0.31265651834613988891, 0.067535071081621433908, 0, 0, 3.3870374289331948997e-07, 2.1158906445184196164e-06, 7.6329082682329570748e-06, 2.2309178641915861703e-05, 5.8676185825556269023e-05, 0.00014535443145101360611, 0.00034710023984489360048, 0.00080922144066932952776, 0.0018549168973065219106, 0.0041951273738663188637, 0.0093691710332807192491, 0.020628857715017115404, 0.044564197866965707395, 0.093532235391030102423, 0.18697094954821646962, 0.34034142027880176506, 0.49294319032437394767, 0.3494912754584255099, 0, 0, 2.5484057919113140944e-11, 3.9818840498614283815e-10, 2.7311742697999542126e-09, 1.3669061339914938246e-08, 5.844616252249814789e-08, 2.2870917347201761224e-07, 8.4858959306208503663e-07, 3.0457478565146930827e-06, 1.0702444616158128632e-05, 3.7094041022036097191e-05, 0.00012741475133052262828, 0.00043507409392457165433, 0.0014798098005849495782, 0.0050201972154368997708, 0.017001470448093195659, 0.057511399145102344577, 0.1943970799975693331, 0.52239901147403788872, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.060574641985915035625, 1, ]).reshape((20, 5, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=None
intercept=TRUE
Boundary.knots=None
knots=np.array([10, 100, 1000, ])
output=np.array([1, 0.84242112482853226396, 0.63852451989026048906, 0.39886884859396426473, 0.16511776352451987271, 0.019111497248478225702, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1567541293972270211, 0.35648024152864221659, 0.58396161300859317222, 0.78843465486178421209, 0.87349958752470602263, 0.78872593441389438063, 0.64620704007646878608, 0.46633671277914778841, 0.26443067498670752569, 0.086017059388510438978, 0.0027907697403672538511, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00082460534200152601353, 0.0049930443273597038822, 0.017154488011057174995, 0.046372256997114502663, 0.10706684228609344989, 0.21001709629214043717, 0.349560411044427366, 0.52164005645501687614, 0.7052885939808604121, 0.84461817796425076033, 0.85332055763972392004, 0.74044769630139117833, 0.58689461845094414993, 0.39882902037092649028, 0.19953161831184262898, 0.045398720470901786361, 3.5969836861605402089e-06, 0, 0, 0, 1.4043223919767127475e-07, 2.1942537374636142735e-06, 1.5050386385262928333e-05, 7.5324616581368114944e-05, 0.0003220729407222703883, 0.001256955617244540557, 0.0042307394405898370374, 0.012003815755055342179, 0.030163701749655900952, 0.068814016157702301291, 0.14161203135855129909, 0.25097331049689530769, 0.38604331096967514636, 0.52843933204950088722, 0.62702364005086641541, 0.58518234575989880319, 0.32861311390225123041, 0.070981847410833104339, 0, 0, 0, 0, 0, 0, 0, 1.3676720629513371727e-08, 1.8094385139768081762e-06, 1.9415010780026244647e-05, 0.00011702928277625300924, 0.00055074648953661010323, 0.0022766412613576141911, 0.0085724666641985424603, 0.026852019770306118779, 0.070978550821878275134, 0.16387889672004127273, 0.32660594492319316995, 0.49935626095003893266, 0.36224174733681202554, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6.5265375148376491226e-06, 0.00021005080907461989208, 0.0017530967576943525671, 0.0095658449172495805396, 0.042812988846006136412, 0.17202702816402348773, 0.50620176326643973042, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.060574641985915035625, 1, ]).reshape((20, 7, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=5
intercept=TRUE
Boundary.knots=np.array([0, 3000, ])
knots=None
output=np.array([0.93886062842918460714, 0.90924840659363448392, 0.86600789475617090396, 0.80375207763469636024, 0.71607712169296244831, 0.5968951736881422665, 0.4441302646954225497, 0.26764334705075454313, 0.1016296296296296392, 0.0080000000000000019013, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.061118708396877156896, 0.090705266259710190524, 0.13388842602098646739, 0.1960165235067286571, 0.28340857382601336578, 0.40196904868840105385, 0.55338648582541094534, 0.72701318595705810566, 0.88717054427173769326, 0.96957793066711428498, 0.95879430886579075644, 0.93088336665495841071, 0.89004087164963097134, 0.83104478735029430059, 0.74753085865141255528, 0.63305354842556382788, 0.48423310979820943789, 0.30782602005074449769, 0.13273505055352541326, 0.018080241660177655966, 2.0660861734307157164e-05, 4.6319342966912629609e-05, 0.00010365288539398855667, 0.00023130996968619285063, 0.00051400448102423420497, 0.0011347651234567903995, 0.0024798322916666664696, 0.005331933984375000421, 0.011160902197265624297, 0.022290701165771482917, 0.040764443570473873901, 0.067755947834115426431, 0.10621083986053703185, 0.15942942065206799906, 0.22958192424619683347, 0.31428339352859285816, 0.39934193575081916583, 0.44659448090264364239, 0.38447678365411958046, 0.15324078379324712618, 2.3122039887776819585e-09, 7.8036884621246768168e-09, 2.6337448559670789633e-08, 8.888888888888889516e-08, 3.0000000000000003936e-07, 1.0125000000000002586e-06, 3.4171875000000000522e-06, 1.1533007812500000494e-05, 3.892390136718749976e-05, 0.0001313681671142578227, 0.00044121305262908775248, 0.0013584768001177710377, 0.0037270943566249025922, 0.0094033956626723494421, 0.022323349639243747489, 0.050359850764330853223, 0.10765352865786298464, 0.21357717575211032646, 0.36914715819588089785, 0.43210561538410807714, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.451110638240695161e-08, 2.208710808474044903e-06, 2.1194133207095667231e-05, 0.00012239633496539237658, 0.00056386746314699115034, 0.0023032072815124867565, 0.0087714257931084897019, 0.032002323294501686113, 0.11364100759647420558, 0.3965733591624671095, ]).reshape((20, 5, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=12
intercept=TRUE
Boundary.knots=np.array([0, 3000, ])
knots=None
output=np.array([0.19405161102201490264, 0.050007289692374980172, 0.00014579384749963551142, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.62621171786550611227, 0.59601302425037649968, 0.37359381834086596852, 0.11076923076923075873, 0.0017307692307692306051, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.17410393357590786545, 0.33496919687132054033, 0.56209998680912809377, 0.6845598495992195609, 0.49785783114523274318, 0.16619304988878053075, 0.0061552981440288964676, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0056327375365711959687, 0.019010489185927782058, 0.064160401002506278756, 0.20369706674784149314, 0.48150804971555583034, 0.70730977578155906915, 0.57961920601900573935, 0.21512754628498662046, 0.013768162962239142988, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00097385288370810028057, 0.018903349908442362848, 0.12592791140580222864, 0.39992732893636862013, 0.67912381051853998315, 0.61416869876328705757, 0.2513505357806551932, 0.023605422218318485722, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00056926292385829722204, 0.014298166900596829057, 0.10543398177170558438, 0.36126140155236396989, 0.66048745076429016265, 0.64290852334776726895, 0.28801011480255123143, 0.036001264350318869234, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00031466142476788003723, 0.010801736722109872915, 0.088001307706519454888, 0.32534582615335716493, 0.63869155767138829916, 0.66560703209160065885, 0.32480624491998255632, 0.050709395542809954094, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00016070574853527342459, 0.0081402282805572035579, 0.073225347399450108066, 0.29228033065100789134, 0.61441387376474365656, 0.68269380491744569017, 0.36150149884261828515, 0.067452174711159121334, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7.2980126610408391891e-05, 0.006111372907072773128, 0.06076004701308617556, 0.2632968675132925096, 0.60214193117458769677, 0.75884556552586945877, 0.54356876952344934661, 0.23438768460872402843, 0.031926653602217504313, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.9834302187538887846e-05, 0.0032999320264517715412, 0.03634723886267338111, 0.16935876914548009253, 0.40315927025443520915, 0.49317292913076199445, 0.23502476599459576345, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9.3311201208237671141e-06, 0.004343490617491435786, 0.053266330072947364049, 0.25527517275326377932, 0.49679177449105649256, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5.6301491679855540114e-06, 0.017164213507250107582, 0.23625680591213024662, ]).reshape((20, 12, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=None
intercept=TRUE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([])
output=np.array([0.99900033329629622791, 0.99850074987500003765, 0.9977516870781251157, 0.9966287954511718894, 0.99494603816333004875, 0.99242545546139537826, 0.98865256904256049175, 0.98301118751693283837, 0.97458941720955349908, 0.96204716698765402327, 0.94343627795644391387, 0.91597241507140558792, 0.87578413786327058421, 0.81773305675994945041, 0.73555685971681306068, 0.62291325464817093316, 0.47647663168935677769, 0.30289524241999998821, 0.13060889169932207721, 0.017790631148623885227, 0.00099933344444444439057, 0.0014985003750000000684, 0.0022466262656250001253, 0.003367410521484375148, 0.0050454284787597655781, 0.0075553552955017096171, 0.011304291651615143086, 0.016891872202619076515, 0.02519288281652980882, 0.037464410913716578166, 0.055469506915694993809, 0.081581580145842852447, 0.11876628136094780075, 0.1701874001952980997, 0.23787846467254591953, 0.31938947503806314199, 0.40070172821401484065, 0.44437167848162473227, 0.38044436785608970464, 0.1510757732538548781, 3.3322222222222224585e-07, 7.4962500000000004377e-07, 1.6862343750000001263e-06, 3.7926035156249994121e-06, 8.5285524902343742213e-06, 1.9173024810791016186e-05, 4.3084569087982178912e-05, 9.675554396295546727e-05, 0.00021707648827975990682, 0.00048631783460495621712, 0.0010871132367784521085, 0.0024220359003474063384, 0.0053686857976677918383, 0.011806522493618208658, 0.025643231250562635581, 0.054587395598501557703, 0.11232581293261123534, 0.217309662419816918, 0.36939270915445387988, 0.42763873999331791786, 3.7037037037037035514e-11, 1.2500000000000000779e-10, 4.2187500000000000366e-10, 1.4238281249999998508e-09, 4.8054199218749994447e-09, 1.6218292236328128606e-08, 5.4736736297607425773e-08, 1.8473648500442501235e-07, 6.234856368899344564e-07, 2.1042640245035291742e-06, 7.1018910826994105393e-06, 2.3968882404110509299e-05, 8.0894978113872978626e-05, 0.00027302055113432129778, 0.00092144436007833422416, 0.003109874715264377993, 0.010495827164017276431, 0.035423416678558306003, 0.11955403129013431052, 0.40349485560420322861, ]).reshape((20, 4, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=None
intercept=TRUE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([100, ])
output=np.array([0.97029899999999991156, 0.95567162499999991354, 0.93400735937500012351, 0.9021287441406251606, 0.8556839255371094799, 0.78904911782836917311, 0.69572725948715208322, 0.57001276798105227073, 0.41135095097535856468, 0.23325165409902481883, 0.075874787916011168787, 0.0024617100802805510704, 0, 0, 0, 0, 0, 0, 0, 0, 0.029691034444444444618, 0.044305991250000002768, 0.06594240796875000532, 0.097758673769531262421, 0.1440642544409179715, 0.21038931479278566439, 0.30302618229869843214, 0.4272397443474625911, 0.58266048231123623857, 0.75392639264340943761, 0.89747740349010274308, 0.94501107412874996161, 0.90598359089303848179, 0.84593074837236126307, 0.76092088936222046502, 0.64439302204983184286, 0.49290686036830000383, 0.31333990595172406257, 0.13511264658550559137, 0.018404101188231606484, 9.9644444444444446617e-06, 2.2379999999999999178e-05, 5.0219999999999996902e-05, 0.00011253937500000000703, 0.00025167585937500001371, 0.0005610808300781249848, 0.0012449161120605468141, 0.002741945576934814565, 0.0059698621442985533997, 0.012758825336830616898, 0.02643475186140507513, 0.051808149318846051512, 0.091620856894323976505, 0.14688590543057178373, 0.21984251899359169569, 0.30818245617558825966, 0.39752224089834975462, 0.44889001546541440479, 0.38890408238266155339, 0.15565065849749706861, 1.1111111111111110654e-09, 3.7499999999999996649e-09, 1.2656249999999997938e-08, 4.2714843750000004622e-08, 1.4416259765625001146e-07, 4.8654876708984372583e-07, 1.6421020889282224026e-06, 5.5420945501327518529e-06, 1.8704569106698035598e-05, 6.3127920735105882001e-05, 0.00021305673248098232295, 0.00071906647212331535352, 0.002394473001231371169, 0.0071486127371715351211, 0.018946704087009914874, 0.045842738337222695144, 0.10249145334758576198, 0.20932413300445151805, 0.3687199031810673433, 0.43701763935524273741, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0792114061605856464e-06, 3.473345989527011488e-05, 0.00028988755717800383316, 0.0015817834373569547059, 0.0070794453857644188549, 0.028445945578409920912, 0.10726336785076537317, 0.38892760095902845219, ]).reshape((20, 5, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=None
intercept=TRUE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([1000, ])
output=np.array([0.99700299900000011188, 0.99550674662500016066, 0.99326517610937503644, 0.98990913343164044225, 0.9848892569724120305, 0.97739130722329725653, 0.96621588612179176714, 0.94961298739566668559, 0.92506696964451773368, 0.88904678238644452293, 0.83678890194350608045, 0.76230564732187677812, 0.65907915850713516548, 0.52240018193475146191, 0.35500130049334505111, 0.17760488924393591503, 0.040409809679633923452, 3.2017075519046728555e-06, 0, 0, 0.0029960014444444446613, 0.0044910048749999993689, 0.0067297664531250009357, 0.010079493029296874088, 0.015085171786376951053, 0.02255122235714721729, 0.033655024381153107738, 0.050097300181899548366, 0.074283671347553537068, 0.10950057690181402847, 0.15997106401940666687, 0.23050015162429349225, 0.3250574690342031281, 0.44299931223779698275, 0.57083333883520204211, 0.6679625481063525827, 0.65410023301458430911, 0.45433806106867219432, 0.19591333754898310193, 0.026685946722935827841, 9.9944444444444452216e-07, 2.2481249999999997573e-06, 5.0561718750000005462e-06, 1.1369267578125000521e-05, 2.5556824951171875081e-05, 5.7421764678955089446e-05, 0.00012892528684616086799, 0.00028915821297883983077, 0.00064748855101794003473, 0.0014463279196678473747, 0.0032187283638391594488, 0.0071222944066175568334, 0.015620687524320137074, 0.033781444174048706752, 0.071401027591217913759, 0.14510293850391842163, 0.2740024758137300509, 0.43938848718810108451, 0.4727098830096430615, 0.21327068651931446741, 1.1111111111111111947e-10, 3.750000000000000492e-10, 1.2656250000000001661e-09, 4.2714843749999999659e-09, 1.4416259765625000816e-08, 4.8654876708984385818e-08, 1.6421020889282225085e-07, 5.5420945501327514294e-07, 1.8704569106698034751e-06, 6.3127920735105875225e-06, 2.1305673248098235006e-05, 7.190664721233152451e-05, 0.00024268493434161889522, 0.00081906165340296394756, 0.0027643330802350029977, 0.0093296241457931361474, 0.031487481492051827558, 0.10627025003567493189, 0.31773412222685926132, 0.53482276673031980962, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.013642657214514538819, 0.22522060002743005125, ]).reshape((20, 5, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=None
intercept=TRUE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([10, 100, 1000, ])
output=np.array([0.72899999999999987033, 0.61412500000000003197, 0.46548437500000000577, 0.29077539062500001865, 0.12037084960937501077, 0.013932281494140625472, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.26810999999999995946, 0.37949624999999997943, 0.52058109375000005681, 0.6792815039062500837, 0.81701452880859382066, 0.86124092926025386241, 0.77303028831905795659, 0.63334751997894722653, 0.45705661219484272628, 0.2591685045544720456, 0.084305319906679110353, 0.002735233422533945441, 0, 0, 0, 0, 0, 0, 0, 0, 0.0028890000000000000749, 0.0063753749999999990983, 0.013923140625000001921, 0.029904662109375004103, 0.062484875244140626604, 0.12438889535522459906, 0.22549483803134068305, 0.36206390866050391919, 0.53039385361573909705, 0.70984283151236515774, 0.8454844390975239099, 0.85252274549222006872, 0.73970724860508996201, 0.58630772383249318835, 0.39843019135055551816, 0.19933208669353075226, 0.045353321750430879156, 3.5933867024743799741e-06, 0, 0, 9.9999999999999995475e-07, 3.3749999999999998737e-06, 1.1390625000000002433e-05, 3.8443359375000005034e-05, 0.00012974633789062501275, 0.00043789389038085933673, 0.0014748635551852823811, 0.00458723586310897085, 0.012535204497088376849, 0.030902287796947387061, 0.069803749964903766267, 0.14306169398214990673, 0.2539595816136605011, 0.39367833655080086697, 0.54755337589389629915, 0.67082414394774525501, 0.67380308481828354861, 0.47157640817504958841, 0.20334679251999171479, 0.027698480049178006435, 0, 0, 0, 0, 0, 0, 1.009441616706038712e-08, 1.3354974400350171813e-06, 1.4329692329799520979e-05, 8.6376136215493001325e-05, 0.00040649103089326774291, 0.0016803271030961133663, 0.0063296923222741757059, 0.019902020690376800299, 0.053082350626863387955, 0.12474691161612924684, 0.25803204718826688868, 0.43676084046337160238, 0.48134381599312991984, 0.21958058358824134038, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.4774589754063323979e-06, 0.00011191892632920368619, 0.00093408212868467904808, 0.0050968577425946322637, 0.022811546243018676616, 0.091659157974876420694, 0.30166673427236384564, 0.52750033633515069909, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.013642657214514538819, 0.22522060002743005125, ]).reshape((20, 7, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=5
intercept=FALSE
Boundary.knots=None
knots=None
output=np.array([0, 0.11681123728381428983, 0.2730259078882545376, 0.46749900909245512004, 0.67935641673089897097, 0.85057627482867270707, 0.88934886597560480759, 0.80216081429413377268, 0.67739211125208298458, 0.51560190205519351725, 0.32469390916595280983, 0.13698024292938629221, 0.017122530366173265709, 0, 0, 0, 0, 0, 0, 0, 0, 0.0003496520800200754106, 0.0021367360720303471555, 0.0074505054668944550172, 0.02064467788236522966, 0.049822794327671247883, 0.10672495197833482827, 0.1969631195446764349, 0.31985257188474064405, 0.47686971233712693863, 0.65666375567302859295, 0.82030294962505467815, 0.89243231477003071017, 0.82865930832017831165, 0.7146974084023058893, 0.56412932414843675044, 0.38041985226043306678, 0.187187199501233692, 0.04043243509226646798, 0, 0, 2.6689299275350298406e-08, 4.1702030117734841467e-07, 2.86034224577543261e-06, 1.4315525026353663709e-05, 6.1210311482175452555e-05, 0.00023952573005394856335, 0.00087599557622739540694, 0.0027528938137588820738, 0.0075078612376215788857, 0.018529647319498028513, 0.042210845625950982329, 0.088408426246782453872, 0.16366728062036642322, 0.25992689327612594763, 0.36501727743515482993, 0.44566378087513086603, 0.43199704724253940036, 0.23880246889964723556, 0, 0, 0, 0, 0, 0, 0, 0, 7.058496255109760934e-08, 2.4230494175743949962e-06, 2.0524370058046372483e-05, 0.00011268784152075793045, 0.00050596181960801099326, 0.0020367286170134709343, 0.0076722266846277449928, 0.025178647959627011715, 0.068676202883725015469, 0.1606491966704423624, 0.31804567985939574681, 0.46051790249468016469, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.1843748275951593331e-06, 0.00019705036194114395882, 0.0021771955326834735445, 0.013267170193993682234, 0.062770073396831202461, 0.26024719351340613871, 1, ]).reshape((20, 5, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=12
intercept=FALSE
Boundary.knots=None
knots=None
output=np.array([0, 0.67103518571234044288, 0.42848228882111855098, 0.069535818471346141911, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.13629558602559715963, 0.52330141887680614587, 0.67103578567914412556, 0.33835295499291540011, 0.045464904980911415022, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0030869584374568800715, 0.048215269973619155619, 0.25524093849303608472, 0.60346220790852211913, 0.66476640009174547963, 0.3023720875414114273, 0.032137228723967029009, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0223284561703245952e-06, 0.0041874573564736911752, 0.058175656367661768287, 0.28413057856289342107, 0.62749346465948352414, 0.64458417746071206, 0.26642650281928137446, 0.020972348618427394396, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9.1807309007930243297e-06, 0.0056381163644495775958, 0.070099504751495456123, 0.31573089553161143295, 0.64918292160807222757, 0.61885031117979483195, 0.23077208437307619726, 0.012169621636861570682, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.494304760976545528e-05, 0.0075476982837094025794, 0.084296696676371143941, 0.3501161628650154567, 0.66789290688121760731, 0.58719180485958433202, 0.19572881144127046715, 0.0058504368174458563218, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9.3878896275372805593e-05, 0.010061177336762315224, 0.10112604997332759471, 0.38726521207131708868, 0.68285613364861541541, 0.54940228374944388712, 0.16169301614535666611, 0.001987124003607085819, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00020895877237870428959, 0.013373361432237074534, 0.12100109812882933746, 0.42699888243551975542, 0.69315383369930994029, 0.50558220978145163027, 0.12915009496447266146, 0.00028659920802505926699, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00041395678128490346389, 0.017748396997590374508, 0.14439460468269080251, 0.46888258402048205165, 0.69769409523153691488, 0.45636616933565676835, 0.098685730485585029803, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00075854547264270536476, 0.023548082194459314664, 0.16992724106945886198, 0.46662458342302287617, 0.42010939261246582621, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0032285687345316718341, 0.076722648033295162695, 0.42847050190194924113, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.052734374999999840405, 1, ]).reshape((20, 12, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=None
intercept=FALSE
Boundary.knots=None
knots=np.array([])
output=np.array([0, 0.00067663938125675493745, 0.0016904532697119580269, 0.0032085988210942785123, 0.0054800274275235618532, 0.0088741592009194299184, 0.013936134910759763808, 0.021463528321228261819, 0.032607690503775640933, 0.048995387512752772152, 0.072844594278869884141, 0.10699389717286836299, 0.15464119556617272888, 0.21832955541935578081, 0.29724527028257624606, 0.38124822804791375086, 0.44010197217760271826, 0.41166179703994132399, 0.22237265440082615298, 0, 0, 1.5271697506625337419e-07, 9.5415795571804482003e-07, 3.4427604254568415078e-06, 1.0065494095385778498e-05, 2.6485976820866183047e-05, 6.5657954600734260006e-05, 0.00015695450829847096968, 0.00036650624565625982386, 0.00084215780726247437687, 0.0019117237011529813161, 0.0042940168564642817658, 0.0095393459205458924072, 0.020904202366139650049, 0.044925430954654600735, 0.093631406124953353576, 0.18502397635429304601, 0.32899518168053926148, 0.4444443765651269751, 0, 0, 1.1489366970270387462e-11, 1.7952135891047479178e-10, 1.2313370007669466487e-09, 6.1626316488486427104e-09, 2.6350175916113057325e-08, 1.0311244903883365944e-07, 3.825826041044410135e-07, 1.3731610143675871737e-06, 4.8251465313854754769e-06, 1.6723672935651049261e-05, 5.7444337950753896285e-05, 0.0001961510972947611009, 0.00066716524892103249485, 0.0022633321684622557078, 0.0076650325325876428328, 0.025928742268953787475, 0.087643003996076676576, 0.29609576530082332146, 1, ]).reshape((20, 3, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=None
intercept=FALSE
Boundary.knots=None
knots=np.array([100, ])
output=np.array([0, 0.015071708273510188089, 0.037381259008501491192, 0.070180462942842899987, 0.11790098280764790828, 0.18621717511273339074, 0.28155389351842002865, 0.40920060085307918829, 0.56848075164647782209, 0.74296035718046105067, 0.88664340493727211712, 0.92755941161894106539, 0.87470363121335126255, 0.79564729322794469635, 0.68622539174158070363, 0.5416556180357140482, 0.36526473871966497198, 0.17973006169687555378, 0.038821693326525116841, 0, 0, 3.4126433211387775282e-06, 2.1270238934697356366e-05, 7.6467258985853174243e-05, 0.00022233858750032365649, 0.00058020375050652503793, 0.0014202218581983020763, 0.003329890582419185209, 0.0075460478259266952628, 0.0165401227086783148, 0.03478495575013232366, 0.06861779430820802439, 0.12096540781627201921, 0.191329633412285538, 0.27905349779077742722, 0.37374631578125083742, 0.44360195032526716918, 0.42250875072786614473, 0.23095694223535656597, 0, 0, 2.5715731172923263021e-10, 4.0180829957692598212e-09, 2.7560031267981358852e-08, 1.3793325533914163604e-07, 5.8977491272702679876e-07, 2.3078834777630869564e-06, 8.5630404390810414166e-06, 3.0734364733920997454e-05, 0.00010799739567214114531, 0.00037431259576781883034, 0.0012857306725170922646, 0.0043281861315545133023, 0.012933767638577380737, 0.033975759516814023342, 0.080531027913042049771, 0.17293083479161802662, 0.32462175106248042367, 0.45442872954598834134, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2.7748388221855075202e-06, 8.9305721192384863918e-05, 0.00074535095082784331034, 0.0040670382699930507364, 0.018202476163449766294, 0.07313943651277800273, 0.27579263489213001748, 1, ]).reshape((20, 4, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=None
intercept=FALSE
Boundary.knots=None
knots=np.array([1000, ])
output=np.array([0, 0.0015004113953972979083, 0.0037469425348356701772, 0.0071075541598605747329, 0.012127833209738946019, 0.019611910086607075437, 0.030734076107229148234, 0.04718451762439003494, 0.071340002591639736784, 0.10641511473995685089, 0.15646348082190494888, 0.22590626848315542574, 0.31787571113275914225, 0.42998547021895328069, 0.54537912943555955092, 0.61788880868397033641, 0.56161589860385985329, 0.31265651834613988891, 0.067535071081621433908, 0, 0, 3.3870374289331948997e-07, 2.1158906445184196164e-06, 7.6329082682329570748e-06, 2.2309178641915861703e-05, 5.8676185825556269023e-05, 0.00014535443145101360611, 0.00034710023984489360048, 0.00080922144066932952776, 0.0018549168973065219106, 0.0041951273738663188637, 0.0093691710332807192491, 0.020628857715017115404, 0.044564197866965707395, 0.093532235391030102423, 0.18697094954821646962, 0.34034142027880176506, 0.49294319032437394767, 0.3494912754584255099, 0, 0, 2.5484057919113140944e-11, 3.9818840498614283815e-10, 2.7311742697999542126e-09, 1.3669061339914938246e-08, 5.844616252249814789e-08, 2.2870917347201761224e-07, 8.4858959306208503663e-07, 3.0457478565146930827e-06, 1.0702444616158128632e-05, 3.7094041022036097191e-05, 0.00012741475133052262828, 0.00043507409392457165433, 0.0014798098005849495782, 0.0050201972154368997708, 0.017001470448093195659, 0.057511399145102344577, 0.1943970799975693331, 0.52239901147403788872, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.060574641985915035625, 1, ]).reshape((20, 4, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=None
intercept=FALSE
Boundary.knots=None
knots=np.array([10, 100, 1000, ])
output=np.array([0, 0.1567541293972270211, 0.35648024152864221659, 0.58396161300859317222, 0.78843465486178421209, 0.87349958752470602263, 0.78872593441389438063, 0.64620704007646878608, 0.46633671277914778841, 0.26443067498670752569, 0.086017059388510438978, 0.0027907697403672538511, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00082460534200152601353, 0.0049930443273597038822, 0.017154488011057174995, 0.046372256997114502663, 0.10706684228609344989, 0.21001709629214043717, 0.349560411044427366, 0.52164005645501687614, 0.7052885939808604121, 0.84461817796425076033, 0.85332055763972392004, 0.74044769630139117833, 0.58689461845094414993, 0.39882902037092649028, 0.19953161831184262898, 0.045398720470901786361, 3.5969836861605402089e-06, 0, 0, 0, 1.4043223919767127475e-07, 2.1942537374636142735e-06, 1.5050386385262928333e-05, 7.5324616581368114944e-05, 0.0003220729407222703883, 0.001256955617244540557, 0.0042307394405898370374, 0.012003815755055342179, 0.030163701749655900952, 0.068814016157702301291, 0.14161203135855129909, 0.25097331049689530769, 0.38604331096967514636, 0.52843933204950088722, 0.62702364005086641541, 0.58518234575989880319, 0.32861311390225123041, 0.070981847410833104339, 0, 0, 0, 0, 0, 0, 0, 1.3676720629513371727e-08, 1.8094385139768081762e-06, 1.9415010780026244647e-05, 0.00011702928277625300924, 0.00055074648953661010323, 0.0022766412613576141911, 0.0085724666641985424603, 0.026852019770306118779, 0.070978550821878275134, 0.16387889672004127273, 0.32660594492319316995, 0.49935626095003893266, 0.36224174733681202554, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6.5265375148376491226e-06, 0.00021005080907461989208, 0.0017530967576943525671, 0.0095658449172495805396, 0.042812988846006136412, 0.17202702816402348773, 0.50620176326643973042, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.060574641985915035625, 1, ]).reshape((20, 6, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=5
intercept=FALSE
Boundary.knots=np.array([0, 3000, ])
knots=None
output=np.array([0.20791834248678603414, 0.29902312242189532654, 0.42058546782444744538, 0.57127689681172544311, 0.73389736255977222612, 0.86146730066325027941, 0.87986332643537079612, 0.79291382076038952054, 0.66958340212426392668, 0.50965824665676062732, 0.32095096582466564605, 0.13540118870728076739, 0.016925148588410078576, 0, 0, 0, 0, 0, 0, 0, 0.0012695557913002517882, 0.0028166847710175994465, 0.0062031625467877079053, 0.013503589345765960872, 0.028852424480264381862, 0.059752006107076843788, 0.11700693595529969293, 0.20637160010579103098, 0.32827217823508614281, 0.48461478028531612683, 0.66503213888472001436, 0.83266225254172265835, 0.91555098377957688793, 0.87163430177910428132, 0.78404142346700400612, 0.66397286412208100792, 0.507883804798330174, 0.32286071959112722096, 0.1392180359854287286, 0.018963308663199267973, 1.4499417527118528165e-07, 4.893553415402503421e-07, 1.6515742776983449575e-06, 5.5740631872319126435e-06, 1.8812463256907710572e-05, 6.3492063492063516403e-05, 0.000214285714285714356, 0.0007145407126011744112, 0.0021431007122621795712, 0.0057158011131347528228, 0.013955556387630695808, 0.03166115066306447734, 0.066415223634956249699, 0.12418853438999710725, 0.20209136094044644061, 0.29685688827833944803, 0.39377505733963352741, 0.45246903544196670488, 0.39638281649040951748, 0.1597891894713379668, 0, 0, 0, 0, 0, 0, 0, 3.8421218399486777976e-08, 1.3189283878698809637e-06, 1.117194478839918966e-05, 6.133890298378388427e-05, 0.00027540808793216300541, 0.0011086439970568826713, 0.0041767162702452207551, 0.013792752688869525796, 0.038347512235655537016, 0.093327637726685397368, 0.20095020769675242533, 0.36605493991801690834, 0.4433598240831516657, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4.4756065322375969297e-07, 7.4462903680102773569e-05, 0.0008227353639237797861, 0.0050135001353509671407, 0.023720037270153770254, 0.09834420760614477619, 0.37788767778231108219, ]).reshape((20, 5, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=12
intercept=FALSE
Boundary.knots=np.array([0, 3000, ])
knots=None
output=np.array([0.61574942641905272556, 0.5337910278271761344, 0.26630652260904380535, 0.04321728691476599965, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.21709357831448763965, 0.40428360185244682778, 0.62558529044677246844, 0.63046184489143586305, 0.30534794185536534572, 0.041029980550522253402, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0094918061936686368846, 0.032034845903631647968, 0.10810716461572758562, 0.32213341083732455195, 0.63646722104607222903, 0.66920132452213465513, 0.3023720875414114273, 0.032137228723967029009, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0223284561703245952e-06, 0.0041874573564736911752, 0.058175656367661768287, 0.28413057856289342107, 0.62749346465948352414, 0.64458417746071206, 0.26642650281928137446, 0.020972348618427394396, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9.1807309007930243297e-06, 0.0056381163644495775958, 0.070099504751495456123, 0.31573089553161143295, 0.64918292160807222757, 0.61885031117979483195, 0.23077208437307619726, 0.012169621636861570682, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.494304760976545528e-05, 0.0075476982837094025794, 0.084296696676371143941, 0.3501161628650154567, 0.66789290688121760731, 0.58719180485958433202, 0.19572881144127046715, 0.0058504368174458563218, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9.3878896275372805593e-05, 0.010061177336762315224, 0.10112604997332759471, 0.38726521207131708868, 0.68285613364861541541, 0.54940228374944388712, 0.16169301614535666611, 0.001987124003607085819, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00020895877237870428959, 0.013373361432237074534, 0.12100109812882933746, 0.42699888243551975542, 0.69315383369930994029, 0.50558220978145163027, 0.12915009496447266146, 0.00028659920802505926699, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00041395678128490346389, 0.017748396997590374508, 0.14460859026960704021, 0.4755254951851585199, 0.74591378938143138022, 0.59473172632230031365, 0.2566313326020329133, 0.03495652799821041129, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00054455988572647427617, 0.016905171029782832537, 0.12340338564379622899, 0.3685583814319431939, 0.50219016290765505772, 0.24887848966988834754, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0015327300102998579149, 0.03642329303773143151, 0.22969990963832651043, 0.49849681328683803638, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.011478594851985721506, 0.21766816904506322561, ]).reshape((20, 12, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=None
intercept=FALSE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([])
output=np.array([0.00099933344444444439057, 0.0014985003750000000684, 0.0022466262656250001253, 0.003367410521484375148, 0.0050454284787597655781, 0.0075553552955017096171, 0.011304291651615143086, 0.016891872202619076515, 0.02519288281652980882, 0.037464410913716578166, 0.055469506915694993809, 0.081581580145842852447, 0.11876628136094780075, 0.1701874001952980997, 0.23787846467254591953, 0.31938947503806314199, 0.40070172821401484065, 0.44437167848162473227, 0.38044436785608970464, 0.1510757732538548781, 3.3322222222222224585e-07, 7.4962500000000004377e-07, 1.6862343750000001263e-06, 3.7926035156249994121e-06, 8.5285524902343742213e-06, 1.9173024810791016186e-05, 4.3084569087982178912e-05, 9.675554396295546727e-05, 0.00021707648827975990682, 0.00048631783460495621712, 0.0010871132367784521085, 0.0024220359003474063384, 0.0053686857976677918383, 0.011806522493618208658, 0.025643231250562635581, 0.054587395598501557703, 0.11232581293261123534, 0.217309662419816918, 0.36939270915445387988, 0.42763873999331791786, 3.7037037037037035514e-11, 1.2500000000000000779e-10, 4.2187500000000000366e-10, 1.4238281249999998508e-09, 4.8054199218749994447e-09, 1.6218292236328128606e-08, 5.4736736297607425773e-08, 1.8473648500442501235e-07, 6.234856368899344564e-07, 2.1042640245035291742e-06, 7.1018910826994105393e-06, 2.3968882404110509299e-05, 8.0894978113872978626e-05, 0.00027302055113432129778, 0.00092144436007833422416, 0.003109874715264377993, 0.010495827164017276431, 0.035423416678558306003, 0.11955403129013431052, 0.40349485560420322861, ]).reshape((20, 3, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=None
intercept=FALSE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([100, ])
output=np.array([0.029691034444444444618, 0.044305991250000002768, 0.06594240796875000532, 0.097758673769531262421, 0.1440642544409179715, 0.21038931479278566439, 0.30302618229869843214, 0.4272397443474625911, 0.58266048231123623857, 0.75392639264340943761, 0.89747740349010274308, 0.94501107412874996161, 0.90598359089303848179, 0.84593074837236126307, 0.76092088936222046502, 0.64439302204983184286, 0.49290686036830000383, 0.31333990595172406257, 0.13511264658550559137, 0.018404101188231606484, 9.9644444444444446617e-06, 2.2379999999999999178e-05, 5.0219999999999996902e-05, 0.00011253937500000000703, 0.00025167585937500001371, 0.0005610808300781249848, 0.0012449161120605468141, 0.002741945576934814565, 0.0059698621442985533997, 0.012758825336830616898, 0.02643475186140507513, 0.051808149318846051512, 0.091620856894323976505, 0.14688590543057178373, 0.21984251899359169569, 0.30818245617558825966, 0.39752224089834975462, 0.44889001546541440479, 0.38890408238266155339, 0.15565065849749706861, 1.1111111111111110654e-09, 3.7499999999999996649e-09, 1.2656249999999997938e-08, 4.2714843750000004622e-08, 1.4416259765625001146e-07, 4.8654876708984372583e-07, 1.6421020889282224026e-06, 5.5420945501327518529e-06, 1.8704569106698035598e-05, 6.3127920735105882001e-05, 0.00021305673248098232295, 0.00071906647212331535352, 0.002394473001231371169, 0.0071486127371715351211, 0.018946704087009914874, 0.045842738337222695144, 0.10249145334758576198, 0.20932413300445151805, 0.3687199031810673433, 0.43701763935524273741, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0792114061605856464e-06, 3.473345989527011488e-05, 0.00028988755717800383316, 0.0015817834373569547059, 0.0070794453857644188549, 0.028445945578409920912, 0.10726336785076537317, 0.38892760095902845219, ]).reshape((20, 4, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=None
intercept=FALSE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([1000, ])
output=np.array([0.0029960014444444446613, 0.0044910048749999993689, 0.0067297664531250009357, 0.010079493029296874088, 0.015085171786376951053, 0.02255122235714721729, 0.033655024381153107738, 0.050097300181899548366, 0.074283671347553537068, 0.10950057690181402847, 0.15997106401940666687, 0.23050015162429349225, 0.3250574690342031281, 0.44299931223779698275, 0.57083333883520204211, 0.6679625481063525827, 0.65410023301458430911, 0.45433806106867219432, 0.19591333754898310193, 0.026685946722935827841, 9.9944444444444452216e-07, 2.2481249999999997573e-06, 5.0561718750000005462e-06, 1.1369267578125000521e-05, 2.5556824951171875081e-05, 5.7421764678955089446e-05, 0.00012892528684616086799, 0.00028915821297883983077, 0.00064748855101794003473, 0.0014463279196678473747, 0.0032187283638391594488, 0.0071222944066175568334, 0.015620687524320137074, 0.033781444174048706752, 0.071401027591217913759, 0.14510293850391842163, 0.2740024758137300509, 0.43938848718810108451, 0.4727098830096430615, 0.21327068651931446741, 1.1111111111111111947e-10, 3.750000000000000492e-10, 1.2656250000000001661e-09, 4.2714843749999999659e-09, 1.4416259765625000816e-08, 4.8654876708984385818e-08, 1.6421020889282225085e-07, 5.5420945501327514294e-07, 1.8704569106698034751e-06, 6.3127920735105875225e-06, 2.1305673248098235006e-05, 7.190664721233152451e-05, 0.00024268493434161889522, 0.00081906165340296394756, 0.0027643330802350029977, 0.0093296241457931361474, 0.031487481492051827558, 0.10627025003567493189, 0.31773412222685926132, 0.53482276673031980962, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.013642657214514538819, 0.22522060002743005125, ]).reshape((20, 4, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=3
df=None
intercept=FALSE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([10, 100, 1000, ])
output=np.array([0.26810999999999995946, 0.37949624999999997943, 0.52058109375000005681, 0.6792815039062500837, 0.81701452880859382066, 0.86124092926025386241, 0.77303028831905795659, 0.63334751997894722653, 0.45705661219484272628, 0.2591685045544720456, 0.084305319906679110353, 0.002735233422533945441, 0, 0, 0, 0, 0, 0, 0, 0, 0.0028890000000000000749, 0.0063753749999999990983, 0.013923140625000001921, 0.029904662109375004103, 0.062484875244140626604, 0.12438889535522459906, 0.22549483803134068305, 0.36206390866050391919, 0.53039385361573909705, 0.70984283151236515774, 0.8454844390975239099, 0.85252274549222006872, 0.73970724860508996201, 0.58630772383249318835, 0.39843019135055551816, 0.19933208669353075226, 0.045353321750430879156, 3.5933867024743799741e-06, 0, 0, 9.9999999999999995475e-07, 3.3749999999999998737e-06, 1.1390625000000002433e-05, 3.8443359375000005034e-05, 0.00012974633789062501275, 0.00043789389038085933673, 0.0014748635551852823811, 0.00458723586310897085, 0.012535204497088376849, 0.030902287796947387061, 0.069803749964903766267, 0.14306169398214990673, 0.2539595816136605011, 0.39367833655080086697, 0.54755337589389629915, 0.67082414394774525501, 0.67380308481828354861, 0.47157640817504958841, 0.20334679251999171479, 0.027698480049178006435, 0, 0, 0, 0, 0, 0, 1.009441616706038712e-08, 1.3354974400350171813e-06, 1.4329692329799520979e-05, 8.6376136215493001325e-05, 0.00040649103089326774291, 0.0016803271030961133663, 0.0063296923222741757059, 0.019902020690376800299, 0.053082350626863387955, 0.12474691161612924684, 0.25803204718826688868, 0.43676084046337160238, 0.48134381599312991984, 0.21958058358824134038, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.4774589754063323979e-06, 0.00011191892632920368619, 0.00093408212868467904808, 0.0050968577425946322637, 0.022811546243018676616, 0.091659157974876420694, 0.30166673427236384564, 0.52750033633515069909, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.013642657214514538819, 0.22522060002743005125, ]).reshape((20, 6, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=5
df=12
intercept=TRUE
Boundary.knots=None
knots=None
output=np.array([1, 0.24780328615799376846, 0.0091743090758214639741, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.64369896942547122354, 0.57211971408849160436, 0.24237606578683060232, 0.044144491052264721309, 0.00040499520490831021667, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.10612816657504857421, 0.39035389287158045457, 0.63115822592338399755, 0.61942792972003923868, 0.3746409686363950664, 0.1299270153679348283, 0.01456850160548600788, 2.4933115730197022697e-06, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0023615475345642161081, 0.028071577965966144214, 0.12345862383415923125, 0.3183634697319876472, 0.55248624494736087165, 0.66157907323456133231, 0.54063561857561015511, 0.27886806629206489783, 0.077562585629719432712, 0.0043678218067639239253, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8.026857613403984366e-06, 0.00028016915158263086755, 0.0029987443674941152358, 0.017947370921348515527, 0.071410333320899577192, 0.20176799613356366514, 0.41383987857855386583, 0.6154467549253210823, 0.65231295225138286042, 0.47044608242977714596, 0.21195012900662815736, 0.045727490940629646199, 0.00095767216067723433693, 0, 0, 0, 0, 0, 0, 0, 3.4493087511455702552e-09, 3.3684655772905958113e-07, 8.3400865772486489595e-06, 0.00011672374464387017944, 0.001056583561129178974, 0.0067076933166381781659, 0.030729882600253097258, 0.10383387100350169319, 0.2594160581947105304, 0.48008955662192837055, 0.64645432100283450882, 0.61990700734803594329, 0.39505102004440167951, 0.15152949559297854143, 0.022327498493473717928, 5.4255821339140527268e-05, 0, 0, 0, 0, 0, 0, 1.5550084505986725427e-12, 1.4829716211306275165e-08, 8.7432930709587782537e-07, 1.8221855962693547294e-05, 0.00022606146610042746276, 0.0018465551075490084856, 0.010668371345813800616, 0.044660809207147045274, 0.13839012473302086947, 0.31749140013695392737, 0.53873603574199457888, 0.65962064818889976081, 0.56521993555148342114, 0.310506638922921685, 0.095251921793756827439, 0.0074067894386825334357, 0, 0, 0, 0, 0, 0, 0, 9.1339471742565674011e-11, 5.7173996413972389293e-08, 2.2593599905185296879e-06, 4.0028808856421427135e-05, 0.00043513707847336695594, 0.0031885139595084076476, 0.016618589891805510966, 0.062812605113451408512, 0.1727378974109962384, 0.33696595086343833492, 0.43637044859132123609, 0.31354081400102012944, 0.068636818178064884499, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.7695168114557225724e-09, 5.9285591015796308443e-07, 1.6911296921410870622e-05, 0.00025529307410796154469, 0.0024263360322963763681, 0.015741731604329466804, 0.070644470707028478307, 0.21311252564833091383, 0.3762972750744253414, 0.24384934178453185338, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0868704401501409189e-12, 2.1860846716771802376e-07, 1.6330907178590211063e-05, 0.00037021035039809161273, 0.0048110469704648085865, 0.038471273482950811562, 0.18655513337806783891, 0.39552144655799137407, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.6852398034939565224e-08, 3.1097414111410212796e-05, 0.0014848575331362907741, 0.028249998152729726558, 0.25456576340175340878, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00010485760000000057303, 0.030019840638976030833, 1, ]).reshape((20, 12, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=5
df=None
intercept=TRUE
Boundary.knots=None
knots=np.array([])
output=np.array([1, 0.99887226764047176708, 0.99718257698671308731, 0.99465232914808876519, 0.99086659019744105503, 0.98520960330607720845, 0.9767725953351565904, 0.96422555375161622671, 0.94564705949867622348, 0.91831730137969458383, 0.87851085191767241955, 0.82140061321171964348, 0.74134024099742168445, 0.63306797132011838336, 0.49472136419952228437, 0.33351966492579832035, 0.17295333506564730675, 0.053041675157364839843, 0.0041245206602366912829, 0, 0, 0.0011272234177996938269, 0.0028142442804627715303, 0.0053362072774053633667, 0.0090999196124958610377, 0.014702372431044495246, 0.023009568696928141274, 0.035255048800207078319, 0.053144756365466998271, 0.078922539871719557536, 0.11527760116980134697, 0.16482753559964863355, 0.22865514515079901625, 0.30306893078627178406, 0.37385911082806172478, 0.40954833229285447782, 0.36355794566479110452, 0.2119513600790149388, 0.041217298467029310494, 0, 0, 5.0882687398560041373e-07, 3.1769391294621280289e-06, 1.1451280924188917779e-05, 3.3428733100138665235e-05, 8.7761935886847988286e-05, 0.00021681208262686383053, 0.00051561316170019592855, 0.0011946804469067555735, 0.0027131220507094727483, 0.0060506596144855023106, 0.013230141811374926397, 0.028210083582453096551, 0.058035333309565624582, 0.11300958063559926603, 0.20116335451605230067, 0.30568795867586873172, 0.33877798094471850421, 0.16475763686175207146, 0, 0, 1.1484182443411814437e-10, 1.7931887260774516498e-09, 1.2286988490864389756e-08, 6.1400553206305291637e-08, 2.6193586874264870438e-07, 1.0214767558696939837e-06, 3.7704802796487805225e-06, 1.3428054504627923681e-05, 4.6634530984498030184e-05, 0.00015879269432593164958, 0.00053096908751411363639, 0.00174019442073816092, 0.0055566565395111476577, 0.017080184681265514479, 0.049404053208565550104, 0.12851476524402274948, 0.27074730808567093465, 0.32929231067852438031, 0, 0, 1.2959854631941128047e-14, 5.06072933143619642e-13, 6.5918427455453850986e-12, 5.6389033989814727996e-11, 3.9088927700060016774e-10, 2.406265255469648607e-09, 1.378603437152614992e-08, 7.5464802427343835863e-08, 4.0078909822273000137e-07, 2.0836670196190539855e-06, 1.0654767572225721294e-05, 5.3673655611779830157e-05, 0.00026601408260545672708, 0.0012907432586925205419, 0.0060666130749972404027, 0.027014549341864656923, 0.10818900424286860551, 0.32906949849914446382, 0, 0, 5.8500579526198340832e-19, 5.7129472193553077444e-17, 1.4145822896898457119e-15, 2.0714622186680568112e-14, 2.3333104795078233906e-13, 2.2673496764008742317e-12, 2.0162390952550372537e-11, 1.6964293385718098169e-10, 1.3777936465799164788e-09, 1.0936695210263550545e-08, 8.5522170452256458058e-08, 6.6219297623306537514e-07, 5.0939619277349284707e-06, 3.9016396858695978102e-05, 0.00029798198173219142288, 0.00227144600780565184, 0.017292671490362138825, 0.13153873483331313121, 1, ]).reshape((20, 6, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=5
df=None
intercept=TRUE
Boundary.knots=None
knots=np.array([100, ])
output=np.array([1, 0.97500126574733914087, 0.93844290960000364965, 0.88566924020111725824, 0.81098530154789516544, 0.70848516022771668155, 0.57441053105822748037, 0.41206430922482584212, 0.2392461015407435776, 0.092939125284492898893, 0.014299466806025581608, 4.7193863677059842883e-05, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.024987397851780435848, 0.061486796626462968118, 0.1140799960900419352, 0.1882939537142028219, 0.28966625638367565765, 0.42117968175620157378, 0.57798465087862005429, 0.73943782746930653005, 0.86397935690336380432, 0.90462871246453147034, 0.85976637084822715718, 0.77601114642224255924, 0.66267521310641597232, 0.51785842958629058064, 0.34911766989097126057, 0.18104199448456753663, 0.055522321426452851678, 0.0043174157141209739547, 0, 0, 1.1333836374491041766e-05, 7.0253868335539153452e-05, 0.00025049169236306427153, 0.00071939598049462782393, 0.0018428967293926601032, 0.0043879998552115674279, 0.0098727402942039785283, 0.021048289930652556295, 0.042207103219050953746, 0.078361329513138483494, 0.1323267251748408424, 0.20305646979998087653, 0.28625091024051202426, 0.36712456913477936604, 0.41237454577086030127, 0.37209382828918063923, 0.21926721332006066101, 0.042943027456719351509, 0, 0, 2.5642163564224008165e-09, 3.9893896948733327568e-08, 2.7186958324120860093e-07, 1.3475047468249140318e-06, 5.6780166980285093331e-06, 2.1734507210374597486e-05, 7.8000238621073664019e-05, 0.00026616938183067457386, 0.00086607259863206911138, 0.0026688434677071591963, 0.0076602480569788727188, 0.020032890189762748295, 0.047362176442216685768, 0.10112516340575794516, 0.19128545657973258787, 0.30258229747242648688, 0.34436724521558481626, 0.17045464690987927048, 0, 0, 2.8979008849349871451e-13, 1.1299684644739179399e-11, 1.4686295227238142049e-10, 1.2521970298985838129e-09, 8.6372943219355255736e-09, 5.2772400925513216102e-08, 2.9891245041530529781e-07, 1.607880484564848822e-06, 8.3111564072833673416e-06, 4.1402961295506068027e-05, 0.00019754788193526286806, 0.00088468396831814228969, 0.0036015013400145573161, 0.013149579882492583999, 0.042768561910798960635, 0.12037399719492403172, 0.26730426029673903798, 0.33672081019045274619, 0, 0, 1.309371682920953332e-17, 1.2786832841024935257e-15, 3.1661464010829005262e-14, 4.6363882090260461786e-13, 5.2224622287067364928e-12, 5.0748274386406234902e-11, 4.5127867086219768747e-10, 3.7969821085404869e-09, 3.0838053235562167103e-08, 2.44787301750460405e-07, 1.9141743408195724709e-06, 1.4809071760067401229e-05, 0.00011002044182894339316, 0.0007361307065949982114, 0.0043501410333869746858, 0.022648326644076191561, 0.10074752233050462968, 0.32871166293410319925, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5.4793570769921117731e-10, 1.7842901170031562027e-07, 6.1272840845677704696e-06, 0.00010362481425005589904, 0.0012595559148250148798, 0.012791437410658173385, 0.11685243679472454015, 1, ]).reshape((20, 7, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=5
df=None
intercept=TRUE
Boundary.knots=None
knots=np.array([1000, ])
output=np.array([1, 0.9975000012515636838, 0.99375938046297329631, 0.9881694981371595965, 0.97983186568317970355, 0.96743102855471574397, 0.94906548971852167096, 0.92204114544049331492, 0.88266217520230549898, 0.82612719043156179755, 0.74679100501527517775, 0.63932416597329111418, 0.50165120112244221406, 0.34054780971703357828, 0.17887753393501265586, 0.056398049910775530091, 0.0047824514208348522723, 6.9899702309353164834e-10, 0, 0, 0, 0.0024988701975066367374, 0.0062335737743966241481, 0.011805108206123089004, 0.020094017057672219906, 0.032374436158623724757, 0.050454096263104174225, 0.076816980711751164934, 0.11469423978741608017, 0.16787638509201657788, 0.23985925947403505254, 0.33155764168655321722, 0.4364690436594241274, 0.53267348082579712987, 0.57514542430970139186, 0.50463302930351572329, 0.30623588293439429897, 0.096587849561305616497, 0.0075106712808647091081, 0, 0, 1.1282962103271197581e-06, 7.0417854820895302466e-06, 2.536640667453833115e-05, 7.3981096505374880712e-05, 0.00019395448667024030114, 0.00047814948330325205252, 0.0011335173229715391711, 0.0026138368581788955919, 0.0058931764589364287604, 0.012998501527337540801, 0.027945386445582046098, 0.058044007669012950834, 0.1145680954987259581, 0.2086069927114643785, 0.33148565972609811414, 0.41061823564260202524, 0.3066625410662744966, 0.068889778730265194273, 0, 0, 2.5469057156714312774e-10, 3.976025713192789869e-09, 2.7235422387831816846e-08, 1.3603757883811768205e-07, 5.7993308882550318022e-07, 2.2591989366454093128e-06, 8.3259563410340815473e-06, 2.9580849000601624262e-05, 0.00010235971125482330811, 0.00034661758311907198181, 0.001149214388416119545, 0.0037170166582407206807, 0.011623043605661884797, 0.034525979049628140183, 0.094171296281739258482, 0.21954225794050497012, 0.36514412761590397949, 0.24346326935859208263, 0, 0, 2.8744101198050118059e-14, 1.1223436953991581798e-12, 1.4617253735361878953e-11, 1.2501806315302350017e-10, 8.6638386346496238246e-10, 5.3311050540562872207e-09, 3.0523721489433262118e-08, 1.6692682142232341016e-07, 8.8525020719146049419e-07, 4.5921420319874677743e-06, 2.3401813204582281116e-05, 0.0001172621098580812533, 0.00057627166059595505247, 0.0027575294458836020588, 0.012651024092369157759, 0.053782977897194939043, 0.19324936944781076487, 0.3997562713358867037, 0, 0, 1.2975755416333770318e-18, 1.2671636148763449243e-16, 3.1376225596317031531e-15, 4.5946189458816680778e-14, 5.1754130194391066624e-13, 5.0291082725267428713e-12, 4.4721309725082660758e-11, 3.762775062517599803e-10, 3.0560232936142684245e-09, 2.4258201074369941878e-08, 1.8969295269383146249e-07, 1.4687810219326831175e-06, 1.1298692185571222767e-05, 8.6540548310009559815e-05, 0.00066094068550206401882, 0.0050381941644688631871, 0.038356111609708157251, 0.27103704347500323646, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0093429658193878890871, 1, ]).reshape((20, 7, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=5
df=None
intercept=TRUE
Boundary.knots=None
knots=np.array([10, 100, 1000, ])
output=np.array([1, 0.75141884282545001739, 0.47347381451739223301, 0.21613090194838843749, 0.049696178730542467372, 0.0013661273532290147976, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.24594066521407775827, 0.51146600459087232515, 0.73649217207800155016, 0.8374180350990879651, 0.77783093616193677011, 0.63185158416405007298, 0.45327074014730833751, 0.26317071169481792703, 0.10223303781294215686, 0.015729413486628138208, 5.1913250044765826493e-05, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0026391300640233733565, 0.015039867450995366219, 0.047247102051768179176, 0.1123025082166889399, 0.21868817428784292911, 0.36168436874353226962, 0.52963938325215309533, 0.69654238826126158024, 0.81185888378284087885, 0.81901416673831617388, 0.71604356256126133751, 0.5618949499117826818, 0.38144450577487010179, 0.20035909960120826256, 0.06317094299551503922, 0.0053567803596460189172, 7.8294021104867289565e-10, 0, 0, 0, 1.3617399508643758347e-06, 2.0307441416267721758e-05, 0.00012974796415461092966, 0.00058265627417901637731, 0.0021107453507099383297, 0.0064420295146436082692, 0.01698600253976614155, 0.039859183304507203593, 0.084333549165298365979, 0.1600060050847842974, 0.26809274258076365438, 0.39566435336691235802, 0.51703688602850017553, 0.58200587206136744634, 0.52329606670761874554, 0.32126381870611603331, 0.10151738768556310688, 0.0078939921772932797328, 0, 0, 1.5649080494945798507e-10, 5.9986253804949622248e-09, 7.5940397022685528773e-08, 6.2142631098330448954e-07, 4.0139943226430085122e-06, 2.1989865743143280786e-05, 0.00010363219420155694861, 0.00042588199885955281988, 0.0015624557228769457226, 0.0051797373818038370402, 0.015437236122329209276, 0.040650688803865277221, 0.093964606272201794956, 0.18957613958735397564, 0.32176934741298385267, 0.41523761089925581569, 0.31715186300322989466, 0.072004316292680670131, 0, 0, 7.1504090682135632783e-15, 6.9828213556773098054e-13, 1.7290156975971226543e-11, 2.531906923450678459e-10, 2.8519588261786051741e-09, 2.7712025453436415246e-08, 2.4184791568371559048e-07, 1.8337667890099757592e-06, 1.2054075181315261422e-05, 7.0420380839424076883e-05, 0.00037180990877366712185, 0.0017643270358445378374, 0.0073398980933030329166, 0.026537861233464601213, 0.082516974679995302999, 0.20957251000715282352, 0.36763462897045218192, 0.25222621956325630421, 0, 0, 0, 0, 0, 0, 0, 5.430767431219823664e-15, 1.8655102027007631066e-11, 9.7376457669355941836e-10, 1.9440860511832082848e-08, 2.5692762827202482626e-07, 2.7355768273926624709e-06, 2.5679592827130744701e-05, 0.00021368415859150435886, 0.0015066158869563137275, 0.0090029382862195743431, 0.045606751586747915073, 0.18361012124256628764, 0.40566571262016642985, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.2887678100169456754e-09, 4.196725335132502196e-07, 1.441162964935832764e-05, 0.00024372991766721753461, 0.0029625284410813028553, 0.030085998315248142776, 0.25286679352721525005, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0093429658193878890871, 1, ]).reshape((20, 9, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=5
df=12
intercept=TRUE
Boundary.knots=np.array([0, 3000, ])
knots=None
output=np.array([0.13756522087537645382, 0.03408911379396852015, 0.0012620658543943507734, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.63650820114425754603, 0.56442389527296787932, 0.36731014666049194295, 0.15313548110822144954, 0.027890905208072903215, 0.00025587978478329849409, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.21582542819545830204, 0.37161327360106727324, 0.54958413949399687048, 0.64871936385648787393, 0.56949455728677522703, 0.3353237146654387546, 0.11624293381549648252, 0.013034128145880629485, 2.2307127685738320293e-06, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.010030255096273907323, 0.029534067226298671427, 0.080270381333806692381, 0.19129075916133608803, 0.37605173703394079165, 0.57813180094222393901, 0.65912921049582962052, 0.52910502202296083585, 0.27213902033160508598, 0.075690939564343556745, 0.0042624228385304534922, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7.0831944077479075004e-05, 0.00033917363922361448356, 0.0015696484900234396707, 0.006826920796842389158, 0.026357789549328776435, 0.084918297819850163677, 0.21706326994359459448, 0.42522472470932526356, 0.61970296224817356112, 0.65157393079529135616, 0.46867117785961504017, 0.21110309109651023696, 0.045544745506395555024, 0.00095384491778118137267, 0, 0, 0, 0, 0, 0, 6.2744556242273010119e-08, 4.764664739647606222e-07, 3.6181672866699010542e-06, 2.7475075557388156146e-05, 0.00020499609216608624608, 0.0013694324583970120431, 0.0075463637977772560911, 0.032410006481736260142, 0.10630697223991346367, 0.26202672571617779962, 0.48196986016032394851, 0.64730135891295237371, 0.6200897527822700761, 0.39505484728729772792, 0.15152949559297854143, 0.022327498493473717928, 5.4255821339140527268e-05, 0, 0, 0, 0, 0, 0, 1.5550084505986725427e-12, 1.4829716211306275165e-08, 8.7432930709587782537e-07, 1.8221879880749471527e-05, 0.00022607643762169348975, 0.0018471467411283852555, 0.010678853506751016453, 0.044774794636566098149, 0.13922623229659780719, 0.32186074315609991547, 0.55535177436803673245, 0.70594649934249964485, 0.65842055649705455433, 0.44019721934864292079, 0.20692042038665983683, 0.050925719374514837046, 0.0018364038370179928007, 0, 0, 0, 0, 0, 0, 6.7421415818675005971e-11, 4.2202475147942958715e-08, 1.6677264111419149931e-06, 2.9548372537568077557e-05, 0.0003214228907650466577, 0.0023601436034748866921, 0.012366085974335782813, 0.047309807190604125093, 0.13367877749039860924, 0.27692812785680914756, 0.41099272250632723491, 0.40841144311061738925, 0.22284961609469711163, 0.025059128845635693372, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2.0448984495923750749e-09, 3.2161419941923913368e-07, 9.1740900392921949651e-06, 0.00013858698681402402734, 0.0013233320235020520645, 0.0087002635234140028586, 0.040418548886275447451, 0.13266384308046400009, 0.2919610966000151242, 0.36977070091947494834, 0.13378226109816304668, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4.2555387649176344133e-13, 8.5594084815015888324e-08, 6.3942127780347038856e-06, 0.00014495964162350503772, 0.001897132263692037548, 0.015703476623777332805, 0.085263653984348167225, 0.27486427411742569982, 0.34006034727733425171, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4.4090854418790210764e-09, 8.1360026955471610933e-06, 0.00038848261944930331932, 0.0074310477767932352905, 0.07805738431434718072, 0.38159617170452009294, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.2338141566056994185e-05, 0.0035323051795402063933, 0.11766568723732878654, ]).reshape((20, 12, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=5
df=None
intercept=TRUE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([])
output=np.array([0.9983344440741356296, 0.99750249875031249402, 0.99625562078283180778, 0.99438764201972595913, 0.99159092854883290613, 0.98740766028786153274, 0.98115924126864673127, 0.97184596123454425332, 0.95800877113085403103, 0.93754892830310510021, 0.90751599227918267054, 0.86391429083558635149, 0.801669064575886825, 0.71507663977921398502, 0.59936827269939108032, 0.45433823528182060159, 0.29067165046987419874, 0.13661155030676189193, 0.033621821670569602969, 0.0012124176758172098417, 0.0016644455553086624083, 0.002495003748750156472, 0.0037387626499230611904, 0.0055997301828166279672, 0.0083806908754831561242, 0.012528591196985617434, 0.018697687754151010031, 0.027833351892417868001, 0.041273795716988054272, 0.060850651795732749183, 0.088929278684076601413, 0.12824165844093327049, 0.18119239198615591513, 0.2480374111869024234, 0.32305865186741777872, 0.38825858275226748928, 0.40740939128236214328, 0.33403355026188807919, 0.1632256493056772062, 0.017159494075893101661, 1.1100003703292180217e-06, 2.4962518746875000594e-06, 5.6123532398144536609e-06, 1.2613583192428893271e-05, 2.8332643039885458129e-05, 6.3586947395334136954e-05, 0.00014252672253270057788, 0.00031885525421459126778, 0.00071127791904317637373, 0.0015797796625579053637, 0.0034857420363066235133, 0.0076146085944587566657, 0.016381164929213501424, 0.034414525059745272595, 0.069651262704549549154, 0.13271586265443335861, 0.22841224706537738287, 0.32670272008482859061, 0.31696810306481593145, 0.097144158424494309045, 3.7012349794238681896e-10, 1.2487503125000001559e-09, 4.2124242480468747114e-09, 1.4206263137512205132e-08, 4.7892153138227467571e-08, 1.6136291046154529314e-07, 5.4321868305356652586e-07, 1.8263821320052253243e-06, 6.1287830369107454558e-06, 2.0506795807293100125e-05, 6.8314944883560930534e-05, 0.00022606641535874693596, 0.00074049070575379688161, 0.0023874614910316710632, 0.0075083864312185822493, 0.022682692646808663012, 0.064029150684570476648, 0.15976638757864897178, 0.30776038811264622153, 0.27497860584540806395, 6.1707818930041155992e-14, 3.1234375000000000385e-13, 1.5808447265625000092e-12, 8.0000230407714837838e-12, 4.0477309670448300476e-11, 2.0474318975195293786e-10, 1.0351972331039468262e-09, 5.2306989582536832363e-09, 2.6404574434177521861e-08, 1.3309725534797475592e-07, 6.6943159388078163953e-07, 3.3557879908853077946e-06, 1.6736492419104248054e-05, 8.2813468459374991711e-05, 0.00040470096744437702128, 0.0019383686901439161829, 0.0089744140037595811904, 0.039065023078631577746, 0.14941007561236896439, 0.3891805482645412928, 4.1152263374485595055e-18, 3.1249999999999996265e-17, 2.3730468749999997491e-16, 1.802032470703124905e-15, 1.3684184074401855573e-14, 1.0391427281498912374e-13, 7.8909900918882334723e-13, 5.9922206010276265389e-12, 4.550342518905353542e-11, 3.4554163502937537769e-10, 2.6239567910043193503e-09, 1.9925671881689045749e-08, 1.5131057085157624606e-07, 1.1490146474041569126e-06, 8.7253299787253137094e-06, 6.625797452594535961e-05, 0.00050314649405639756945, 0.0038207686892407699206, 0.02901396223392209775, 0.22032477571384589954, ]).reshape((20, 6, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=5
df=None
intercept=TRUE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([100, ])
output=np.array([0.95099004990000002291, 0.92721650236562502823, 0.89244986942880866199, 0.84226263493375552738, 0.77123895238719930578, 0.67376233787836592448, 0.54625869959414907751, 0.39186905799172611076, 0.22752066204261220395, 0.088384183391962217735, 0.013598650651405662371, 4.4880894773220947292e-05, 0, 0, 0, 0, 0, 0, 0, 0, 0.048976959490485305615, 0.072709651432435309926, 0.10738526002140330595, 0.15737069698548675212, 0.22795032016720712109, 0.32446067835465053353, 0.44989711207706667428, 0.59997610680291524332, 0.75567735422921566979, 0.87844628783911327119, 0.92474207754597603781, 0.89365801028359981295, 0.82931282542333117913, 0.73973445494401424138, 0.62003614417178387619, 0.47000507098119359561, 0.30069481083090426887, 0.14132229342078814205, 0.034781194831623722663, 0.0012542251818798723622, 3.2979557543950620822e-05, 7.3809001036874996929e-05, 0.00016474549918236330078, 0.00036624856893145208737, 0.00080932434818232947688, 0.0017723123294799305975, 0.003828742087843573557, 0.008104291378262778317, 0.016639190251049165714, 0.032657698828719619599, 0.060108147688838667322, 0.10184799113601375464, 0.15884341152280503917, 0.23108234071251923525, 0.31281804868450863166, 0.38543973833058026157, 0.4110892044013776947, 0.34067876601502938838, 0.16765476842547211156, 0.017707951623962527032, 1.1050122962962964101e-08, 3.7191558750000007188e-08, 1.250033797265625178e-07, 4.1927333935913091575e-07, 1.401894586697731242e-06, 4.665382495865281394e-06, 1.5415847866808392633e-05, 5.0391939592239989756e-05, 0.00016203956276710770662, 0.00050812727751784608765, 0.0015332452896675867124, 0.004365181610267205789, 0.011468673667365519159, 0.027632876244132381638, 0.061266201119033726619, 0.12400124625180757032, 0.22211304163999795458, 0.32622078746654581405, 0.32211683874203467237, 0.099883337969340235674, 1.8476543209876543964e-12, 9.3431250000000011463e-12, 4.7218886718750000078e-11, 2.3843292297363282786e-10, 1.2024140499687195818e-09, 6.0518902752095459008e-09, 3.0369400855124124217e-08, 1.5170773682471650721e-07, 7.5254925311084902029e-07, 3.6922964379636862066e-06, 1.7800105408249686863e-05, 8.3338305189489776193e-05, 0.00037055336224994447902, 0.0015169299478213021069, 0.0056546686833628886926, 0.019188949419050076173, 0.058577982030935032975, 0.15402658068596292162, 0.30726533809094308536, 0.28101637370320353693, 1.2345679012345678054e-16, 9.3749999999999994959e-16, 7.1191406249999982613e-15, 5.4060974121093752673e-14, 4.1052552223205572084e-13, 3.1174281844496722735e-12, 2.3672970275664694964e-11, 1.7976661803082883009e-10, 1.3651027556716062823e-09, 1.0366249050881264123e-08, 7.8718703730129575545e-08, 5.9777015645067147174e-07, 4.5359107007994105463e-06, 3.3361176067584402466e-05, 0.00022366759792994563949, 0.0013435210788023243395, 0.0072639461407535297829, 0.035100831436999477275, 0.14396679069931464512, 0.39291034738734598175, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.1354749159583518215e-10, 3.6975445151343504115e-08, 1.2697433810604620809e-06, 2.1473938565867883887e-05, 0.00026101495603127997146, 0.0026507409746741200131, 0.024215069210611606804, 0.20722776413426768904, ]).reshape((20, 7, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=5
df=None
intercept=TRUE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([1000, ])
output=np.array([0.9950099900049993451, 0.99252246627530493761, 0.98880051122183698631, 0.98323852246469989336, 0.9749424948799998436, 0.96260353805275378214, 0.94432964343891578896, 0.91744015090894337483, 0.87825768212583654737, 0.82200480749416604542, 0.74306451043607213425, 0.63613393199503898146, 0.4991479566148370739, 0.33884847274276985729, 0.17798493325279582389, 0.056116623078022197235, 0.0047585869404442801556, 6.9550902096181903072e-10, 0, 0, 0.0049866811037051986949, 0.0074700487125113432946, 0.011182664341492391802, 0.016723679332538900905, 0.024972650503249212151, 0.037206183352661716113, 0.055244396744596628579, 0.081608715488401706306, 0.11962663350752600344, 0.17331618121340841565, 0.2466772227646655824, 0.34167053826082144363, 0.45378166194157465441, 0.56434225055466613608, 0.6320750091698927875, 0.59733241830569760999, 0.42886959529414492298, 0.2049173244168793484, 0.050432732505854400984, 0.0018186265137258147626, 3.3277811103950621097e-06, 7.4812668695624991679e-06, 1.6811804138396487754e-05, 3.7755607955489316522e-05, 8.4711061600122608767e-05, 0.00018979511914756760698, 0.00042433325892820140772, 0.00094567009442594711366, 0.002097376821719070146, 0.0046178870868948916628, 0.010055306643782090104, 0.021527218530989256778, 0.04489775700844654549, 0.089884991503020608694, 0.16855047321618027434, 0.28372166497555245668, 0.39667928927647067017, 0.39859166318439254173, 0.21962210770558857065, 0.024829927856976751616, 1.1100002962962965174e-09, 3.7443772500000006212e-09, 1.2627790523437500951e-08, 4.2570810898681638324e-08, 1.4343375976686856963e-07, 4.8286151921739802212e-07, 1.6234543349501860428e-06, 5.4478341089133651746e-06, 1.8228467705229433379e-05, 6.0725950389411787269e-05, 0.000200959732568890164, 0.00065830362619350942834, 0.0021228688895969845957, 0.0066792918381076149537, 0.020201657448734186562, 0.057212961493873837338, 0.14427872595983071147, 0.29075824853504678158, 0.36564110074442957021, 0.13330127370825312072, 1.8509876543209877828e-13, 9.3684375000000001865e-13, 4.7411103515625013439e-12, 2.3989256927490236273e-11, 1.2134982390689849712e-10, 6.1360608361896876588e-10, 3.1008571052567070342e-09, 1.5656143551154886378e-08, 7.8940702751398238619e-08, 3.9721851623374797589e-07, 1.9925510408963188817e-06, 9.9478099413657914188e-06, 4.930161383220328208e-05, 0.00024154631749370003959, 0.0011617509224607792256, 0.0054175582232760775836, 0.023904363046940359239, 0.094270457100450164023, 0.27882003179675446392, 0.34581727191398570209, 1.2345679012345680828e-17, 9.3750000000000019611e-17, 7.1191406250000002335e-16, 5.4060974121093743207e-15, 4.1052552223205574609e-14, 3.1174281844496730813e-13, 2.3672970275664701427e-12, 1.7976661803082880424e-11, 1.3651027556716061272e-10, 1.0366249050881260814e-09, 7.8718703730129572237e-09, 5.9777015645067147174e-08, 4.5393171255472860584e-07, 3.4470439422124707377e-06, 2.6175989936175949598e-05, 0.00019877392357783613304, 0.0015094394821691928168, 0.011462306067722308894, 0.084705097520176186876, 0.41086218643981919918, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00077892972719669731786, 0.083370713567239546071, ]).reshape((20, 7, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=5
df=None
intercept=TRUE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([10, 100, 1000, ])
output=np.array([0.59048999999999984833, 0.44370531250000000423, 0.27958155273437496069, 0.12762313629150390248, 0.029345096578598028197, 0.00080668454080820085113, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.40055561099999992258, 0.53723465540624992798, 0.68096479632714856933, 0.79404388738027953387, 0.82432650645400162848, 0.74772850370839760714, 0.60695411066016569102, 0.43541006443525137604, 0.25280073560290239332, 0.098204648213291390046, 0.015109611834895182725, 4.9867660859134372328e-05, 0, 0, 0, 0, 0, 0, 0, 0, 0.0089449327889999999397, 0.01902900664771874778, 0.039353322248692389207, 0.078015642723590794549, 0.14535820079903546964, 0.24864829948443537, 0.38546024005089984943, 0.54588573189455047441, 0.70480915137930777448, 0.81344804238272472308, 0.81717844525199323513, 0.71389962178539645432, 0.56021095018500233209, 0.38030131620961826755, 0.19975862317934431345, 0.062981619616186521049, 0.0053407260835513798575, 7.8059373845321992395e-10, 0, 0, 9.4545814444444459148e-06, 3.1017290343749996553e-05, 0.00010028811495410157556, 0.00031713361145809936415, 0.00096922483761113851879, 0.0028119072707210087531, 0.0075647083349296134064, 0.01861553313384527869, 0.042046394339120052308, 0.087124325583113268467, 0.16371060089498334911, 0.27411331174625735985, 0.40781324509205862938, 0.54286075051770132927, 0.63352683070691229172, 0.61289306201898841042, 0.44453962951650222157, 0.21269241367644950436, 0.052346279846925344859, 0.0018876298723944570646, 1.6294444444444447802e-09, 8.1548437499999982349e-09, 4.0568422851562507446e-08, 1.9994451278686527012e-07, 9.7096128099918361425e-07, 4.602189952521174856e-06, 2.0919648984453888767e-05, 8.8510989072612499115e-05, 0.0003426071500521665281, 0.0012159913347531664096, 0.0039613824662355322237, 0.011728070771626651625, 0.030981021316488138728, 0.072628957837475555115, 0.15091202659488145432, 0.27128094978421118944, 0.39491276471324787689, 0.40566968711467132902, 0.22597501897826155481, 0.025700635234072819607, 1.1111111111111111028e-13, 8.4374999999999991361e-13, 6.4072265625000016033e-12, 4.8654876708984379097e-11, 3.694729700088501736e-10, 2.8056853660047051398e-09, 2.1305018189503776537e-08, 1.5953977978988917004e-07, 1.1111370996233737328e-06, 6.9846696024166045466e-06, 3.9856249948076781855e-05, 0.00020802815256623063945, 0.00098445759075916142167, 0.0041227618062229087065, 0.015186511992247679614, 0.04905127538388008579, 0.1347633037922978394, 0.28642046903795781443, 0.37096041450757122337, 0.1373866644432741313, 0, 0, 0, 0, 0, 0, 2.1835286481805527197e-15, 7.5005881261891668233e-12, 3.9151793493690686266e-10, 7.8165151445884371071e-09, 1.0330194469679949402e-07, 1.0998832941312460716e-06, 1.0325449816590390154e-05, 8.6094485881001237893e-05, 0.00061191613127527461416, 0.0037238993946882535668, 0.01960252770274439893, 0.08667615291637777164, 0.27442290749348607903, 0.35255941616703945218, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.6587525069769108936e-10, 1.1914310104321792976e-07, 4.09139533897260065e-06, 6.9193802045574309841e-05, 0.00084104819165634655664, 0.0085412764739499439509, 0.075516449446559150149, 0.39909494071597967357, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00077892972719669731786, 0.083370713567239546071, ]).reshape((20, 9, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=5
df=12
intercept=FALSE
Boundary.knots=None
knots=None
output=np.array([0, 0.66169375447141842717, 0.42704385876993139481, 0.10949465799552411671, 0.0045536628406788173459, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.16320829536765205092, 0.50699791406824767925, 0.63821768315482607647, 0.43263056231744068114, 0.15013545271065373288, 0.016177354520514077713, 1.5876171140511428811e-06, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0058674236731072158813, 0.063738396787633516682, 0.24043272488013067711, 0.50054000504763473955, 0.64199088129854386953, 0.51113227122624471654, 0.23070045750246975791, 0.045936684143309251815, 0.00066416458897031876996, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.5532476724270158346e-05, 0.0011936972661650952464, 0.011779052740953399256, 0.061287867695344819263, 0.20002288232936923928, 0.43156920172035251326, 0.62168249332663905182, 0.58254996098081013312, 0.32664310035477628347, 0.092349295039142950681, 0.0054390297948912662632, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.1496907292528123154e-08, 3.0758698527859495562e-06, 7.5879789673663857776e-05, 0.00098724833216487398006, 0.0078266015648352053879, 0.040722255850196634186, 0.14383543945950985621, 0.34844598770889789741, 0.57762867947116447453, 0.6345732251821071257, 0.43704729558185051452, 0.16650826117204259313, 0.023068804299786735412, 3.4807107343935620814e-05, 0, 0, 0, 0, 0, 0, 0, 0, 1.4388921206346279741e-09, 6.5376673591293467664e-07, 2.4182089760364024493e-05, 0.00039880176232783617072, 0.0037728522997899590664, 0.022917112728778440273, 0.09334968530661211239, 0.26084062164632215719, 0.49935247038577756928, 0.64262810933455249973, 0.53564397016856835076, 0.2604583835482605636, 0.059387695355091404958, 0.0015752311746957685552, 0, 0, 0, 0, 0, 0, 0, 0, 6.8376488468020759222e-12, 1.1492036416820246223e-07, 7.1697944771843335622e-06, 0.00015024167017123804383, 0.0017124892221411901899, 0.012184251164569406822, 0.05741986723560143363, 0.1846190190189749003, 0.40690335279758527154, 0.61033896958284106216, 0.60106063282500987732, 0.35935886624758728303, 0.11099693368003141214, 0.0086311215629592420023, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.2768033036904166236e-08, 1.8810563357318595251e-06, 5.2605966584901652059e-05, 0.00074031169779739149084, 0.0062001835283552559838, 0.03359211253079281978, 0.12126275319647328299, 0.28972724543117178708, 0.43422014732068858756, 0.34126033432252839139, 0.077899048558433395262, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.0012735173756063062e-09, 1.0253040817926208576e-06, 4.44269427274169616e-05, 0.0007905173401505205099, 0.0078092535818893079463, 0.047648780279836128182, 0.17901740350372433164, 0.37155904838956399505, 0.26547319923364387506, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.3473917264272251468e-12, 1.2428631162803442157e-06, 9.5832983192035734268e-05, 0.0021733294132454694138, 0.025265394711489429919, 0.1581848201187764924, 0.40157681487049512459, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2.3166956453278957354e-06, 0.00056295704181467877848, 0.017992654275869164604, 0.22701602442886983924, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6.2092132305915514147e-06, 0.019403791345598601914, 1, ]).reshape((20, 12, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=5
df=None
intercept=FALSE
Boundary.knots=None
knots=np.array([])
output=np.array([0, 0.0011272234177996938269, 0.0028142442804627715303, 0.0053362072774053633667, 0.0090999196124958610377, 0.014702372431044495246, 0.023009568696928141274, 0.035255048800207078319, 0.053144756365466998271, 0.078922539871719557536, 0.11527760116980134697, 0.16482753559964863355, 0.22865514515079901625, 0.30306893078627178406, 0.37385911082806172478, 0.40954833229285447782, 0.36355794566479110452, 0.2119513600790149388, 0.041217298467029310494, 0, 0, 5.0882687398560041373e-07, 3.1769391294621280289e-06, 1.1451280924188917779e-05, 3.3428733100138665235e-05, 8.7761935886847988286e-05, 0.00021681208262686383053, 0.00051561316170019592855, 0.0011946804469067555735, 0.0027131220507094727483, 0.0060506596144855023106, 0.013230141811374926397, 0.028210083582453096551, 0.058035333309565624582, 0.11300958063559926603, 0.20116335451605230067, 0.30568795867586873172, 0.33877798094471850421, 0.16475763686175207146, 0, 0, 1.1484182443411814437e-10, 1.7931887260774516498e-09, 1.2286988490864389756e-08, 6.1400553206305291637e-08, 2.6193586874264870438e-07, 1.0214767558696939837e-06, 3.7704802796487805225e-06, 1.3428054504627923681e-05, 4.6634530984498030184e-05, 0.00015879269432593164958, 0.00053096908751411363639, 0.00174019442073816092, 0.0055566565395111476577, 0.017080184681265514479, 0.049404053208565550104, 0.12851476524402274948, 0.27074730808567093465, 0.32929231067852438031, 0, 0, 1.2959854631941128047e-14, 5.06072933143619642e-13, 6.5918427455453850986e-12, 5.6389033989814727996e-11, 3.9088927700060016774e-10, 2.406265255469648607e-09, 1.378603437152614992e-08, 7.5464802427343835863e-08, 4.0078909822273000137e-07, 2.0836670196190539855e-06, 1.0654767572225721294e-05, 5.3673655611779830157e-05, 0.00026601408260545672708, 0.0012907432586925205419, 0.0060666130749972404027, 0.027014549341864656923, 0.10818900424286860551, 0.32906949849914446382, 0, 0, 5.8500579526198340832e-19, 5.7129472193553077444e-17, 1.4145822896898457119e-15, 2.0714622186680568112e-14, 2.3333104795078233906e-13, 2.2673496764008742317e-12, 2.0162390952550372537e-11, 1.6964293385718098169e-10, 1.3777936465799164788e-09, 1.0936695210263550545e-08, 8.5522170452256458058e-08, 6.6219297623306537514e-07, 5.0939619277349284707e-06, 3.9016396858695978102e-05, 0.00029798198173219142288, 0.00227144600780565184, 0.017292671490362138825, 0.13153873483331313121, 1, ]).reshape((20, 5, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=5
df=None
intercept=FALSE
Boundary.knots=None
knots=np.array([100, ])
output=np.array([0, 0.024987397851780435848, 0.061486796626462968118, 0.1140799960900419352, 0.1882939537142028219, 0.28966625638367565765, 0.42117968175620157378, 0.57798465087862005429, 0.73943782746930653005, 0.86397935690336380432, 0.90462871246453147034, 0.85976637084822715718, 0.77601114642224255924, 0.66267521310641597232, 0.51785842958629058064, 0.34911766989097126057, 0.18104199448456753663, 0.055522321426452851678, 0.0043174157141209739547, 0, 0, 1.1333836374491041766e-05, 7.0253868335539153452e-05, 0.00025049169236306427153, 0.00071939598049462782393, 0.0018428967293926601032, 0.0043879998552115674279, 0.0098727402942039785283, 0.021048289930652556295, 0.042207103219050953746, 0.078361329513138483494, 0.1323267251748408424, 0.20305646979998087653, 0.28625091024051202426, 0.36712456913477936604, 0.41237454577086030127, 0.37209382828918063923, 0.21926721332006066101, 0.042943027456719351509, 0, 0, 2.5642163564224008165e-09, 3.9893896948733327568e-08, 2.7186958324120860093e-07, 1.3475047468249140318e-06, 5.6780166980285093331e-06, 2.1734507210374597486e-05, 7.8000238621073664019e-05, 0.00026616938183067457386, 0.00086607259863206911138, 0.0026688434677071591963, 0.0076602480569788727188, 0.020032890189762748295, 0.047362176442216685768, 0.10112516340575794516, 0.19128545657973258787, 0.30258229747242648688, 0.34436724521558481626, 0.17045464690987927048, 0, 0, 2.8979008849349871451e-13, 1.1299684644739179399e-11, 1.4686295227238142049e-10, 1.2521970298985838129e-09, 8.6372943219355255736e-09, 5.2772400925513216102e-08, 2.9891245041530529781e-07, 1.607880484564848822e-06, 8.3111564072833673416e-06, 4.1402961295506068027e-05, 0.00019754788193526286806, 0.00088468396831814228969, 0.0036015013400145573161, 0.013149579882492583999, 0.042768561910798960635, 0.12037399719492403172, 0.26730426029673903798, 0.33672081019045274619, 0, 0, 1.309371682920953332e-17, 1.2786832841024935257e-15, 3.1661464010829005262e-14, 4.6363882090260461786e-13, 5.2224622287067364928e-12, 5.0748274386406234902e-11, 4.5127867086219768747e-10, 3.7969821085404869e-09, 3.0838053235562167103e-08, 2.44787301750460405e-07, 1.9141743408195724709e-06, 1.4809071760067401229e-05, 0.00011002044182894339316, 0.0007361307065949982114, 0.0043501410333869746858, 0.022648326644076191561, 0.10074752233050462968, 0.32871166293410319925, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5.4793570769921117731e-10, 1.7842901170031562027e-07, 6.1272840845677704696e-06, 0.00010362481425005589904, 0.0012595559148250148798, 0.012791437410658173385, 0.11685243679472454015, 1, ]).reshape((20, 6, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=5
df=None
intercept=FALSE
Boundary.knots=None
knots=np.array([1000, ])
output=np.array([0, 0.0024988701975066367374, 0.0062335737743966241481, 0.011805108206123089004, 0.020094017057672219906, 0.032374436158623724757, 0.050454096263104174225, 0.076816980711751164934, 0.11469423978741608017, 0.16787638509201657788, 0.23985925947403505254, 0.33155764168655321722, 0.4364690436594241274, 0.53267348082579712987, 0.57514542430970139186, 0.50463302930351572329, 0.30623588293439429897, 0.096587849561305616497, 0.0075106712808647091081, 0, 0, 1.1282962103271197581e-06, 7.0417854820895302466e-06, 2.536640667453833115e-05, 7.3981096505374880712e-05, 0.00019395448667024030114, 0.00047814948330325205252, 0.0011335173229715391711, 0.0026138368581788955919, 0.0058931764589364287604, 0.012998501527337540801, 0.027945386445582046098, 0.058044007669012950834, 0.1145680954987259581, 0.2086069927114643785, 0.33148565972609811414, 0.41061823564260202524, 0.3066625410662744966, 0.068889778730265194273, 0, 0, 2.5469057156714312774e-10, 3.976025713192789869e-09, 2.7235422387831816846e-08, 1.3603757883811768205e-07, 5.7993308882550318022e-07, 2.2591989366454093128e-06, 8.3259563410340815473e-06, 2.9580849000601624262e-05, 0.00010235971125482330811, 0.00034661758311907198181, 0.001149214388416119545, 0.0037170166582407206807, 0.011623043605661884797, 0.034525979049628140183, 0.094171296281739258482, 0.21954225794050497012, 0.36514412761590397949, 0.24346326935859208263, 0, 0, 2.8744101198050118059e-14, 1.1223436953991581798e-12, 1.4617253735361878953e-11, 1.2501806315302350017e-10, 8.6638386346496238246e-10, 5.3311050540562872207e-09, 3.0523721489433262118e-08, 1.6692682142232341016e-07, 8.8525020719146049419e-07, 4.5921420319874677743e-06, 2.3401813204582281116e-05, 0.0001172621098580812533, 0.00057627166059595505247, 0.0027575294458836020588, 0.012651024092369157759, 0.053782977897194939043, 0.19324936944781076487, 0.3997562713358867037, 0, 0, 1.2975755416333770318e-18, 1.2671636148763449243e-16, 3.1376225596317031531e-15, 4.5946189458816680778e-14, 5.1754130194391066624e-13, 5.0291082725267428713e-12, 4.4721309725082660758e-11, 3.762775062517599803e-10, 3.0560232936142684245e-09, 2.4258201074369941878e-08, 1.8969295269383146249e-07, 1.4687810219326831175e-06, 1.1298692185571222767e-05, 8.6540548310009559815e-05, 0.00066094068550206401882, 0.0050381941644688631871, 0.038356111609708157251, 0.27103704347500323646, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0093429658193878890871, 1, ]).reshape((20, 6, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=5
df=None
intercept=FALSE
Boundary.knots=None
knots=np.array([10, 100, 1000, ])
output=np.array([0, 0.24594066521407775827, 0.51146600459087232515, 0.73649217207800155016, 0.8374180350990879651, 0.77783093616193677011, 0.63185158416405007298, 0.45327074014730833751, 0.26317071169481792703, 0.10223303781294215686, 0.015729413486628138208, 5.1913250044765826493e-05, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0026391300640233733565, 0.015039867450995366219, 0.047247102051768179176, 0.1123025082166889399, 0.21868817428784292911, 0.36168436874353226962, 0.52963938325215309533, 0.69654238826126158024, 0.81185888378284087885, 0.81901416673831617388, 0.71604356256126133751, 0.5618949499117826818, 0.38144450577487010179, 0.20035909960120826256, 0.06317094299551503922, 0.0053567803596460189172, 7.8294021104867289565e-10, 0, 0, 0, 1.3617399508643758347e-06, 2.0307441416267721758e-05, 0.00012974796415461092966, 0.00058265627417901637731, 0.0021107453507099383297, 0.0064420295146436082692, 0.01698600253976614155, 0.039859183304507203593, 0.084333549165298365979, 0.1600060050847842974, 0.26809274258076365438, 0.39566435336691235802, 0.51703688602850017553, 0.58200587206136744634, 0.52329606670761874554, 0.32126381870611603331, 0.10151738768556310688, 0.0078939921772932797328, 0, 0, 1.5649080494945798507e-10, 5.9986253804949622248e-09, 7.5940397022685528773e-08, 6.2142631098330448954e-07, 4.0139943226430085122e-06, 2.1989865743143280786e-05, 0.00010363219420155694861, 0.00042588199885955281988, 0.0015624557228769457226, 0.0051797373818038370402, 0.015437236122329209276, 0.040650688803865277221, 0.093964606272201794956, 0.18957613958735397564, 0.32176934741298385267, 0.41523761089925581569, 0.31715186300322989466, 0.072004316292680670131, 0, 0, 7.1504090682135632783e-15, 6.9828213556773098054e-13, 1.7290156975971226543e-11, 2.531906923450678459e-10, 2.8519588261786051741e-09, 2.7712025453436415246e-08, 2.4184791568371559048e-07, 1.8337667890099757592e-06, 1.2054075181315261422e-05, 7.0420380839424076883e-05, 0.00037180990877366712185, 0.0017643270358445378374, 0.0073398980933030329166, 0.026537861233464601213, 0.082516974679995302999, 0.20957251000715282352, 0.36763462897045218192, 0.25222621956325630421, 0, 0, 0, 0, 0, 0, 0, 5.430767431219823664e-15, 1.8655102027007631066e-11, 9.7376457669355941836e-10, 1.9440860511832082848e-08, 2.5692762827202482626e-07, 2.7355768273926624709e-06, 2.5679592827130744701e-05, 0.00021368415859150435886, 0.0015066158869563137275, 0.0090029382862195743431, 0.045606751586747915073, 0.18361012124256628764, 0.40566571262016642985, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.2887678100169456754e-09, 4.196725335132502196e-07, 1.441162964935832764e-05, 0.00024372991766721753461, 0.0029625284410813028553, 0.030085998315248142776, 0.25286679352721525005, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.0093429658193878890871, 1, ]).reshape((20, 8, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=5
df=12
intercept=FALSE
Boundary.knots=np.array([0, 3000, ])
knots=None
output=np.array([0.59169275909036811445, 0.45594291761992611356, 0.23025182795481774489, 0.058881690084655928519, 0.00244877119161214735, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2905201731274964505, 0.4652923293619554701, 0.60774274744642686752, 0.58452788522997400911, 0.36691850516129698168, 0.12666647389296178949, 0.013648531489621140711, 1.3394428704094372146e-06, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.021589057498422929704, 0.061252310080865124409, 0.15611979121414976124, 0.33262545668279458466, 0.54628842278228206819, 0.6385151816856337037, 0.49255505710881447579, 0.221198927694986891, 0.044044707273317475205, 0.00063680989274803034839, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00027107258187882916, 0.00127840807191650788, 0.0057589114783163615832, 0.023748086219301876854, 0.082775728340449605813, 0.22525005798130448564, 0.44911867244320990977, 0.62614581126197998984, 0.57974912164267822234, 0.32404333353141845375, 0.091606580575315788018, 0.0053952866770240375319, 0, 0, 0, 0, 0, 0, 0, 0, 4.9568448953450556364e-07, 3.7641040924026516272e-06, 2.8583665451682636297e-05, 0.00021688034438166792803, 0.0015679187576233750197, 0.0095441043435020560953, 0.04427882227566239115, 0.1488738995058955239, 0.35313880391702157091, 0.58025580099074469675, 0.63531593964593435775, 0.4370910386997177155, 0.16650826117204259313, 0.023068804299786735412, 3.4807107343935620814e-05, 0, 0, 0, 0, 0, 0, 0, 0, 1.4388921206346279741e-09, 6.5376673591293467664e-07, 2.4182089760364024493e-05, 0.00039880176232783617072, 0.0037728522997899590664, 0.022917112728778440273, 0.09334968530661211239, 0.26084062164632215719, 0.49935247038577756928, 0.64262810933455249973, 0.53564397016856835076, 0.2604583835482605636, 0.059387695355091404958, 0.0015752311746957685552, 0, 0, 0, 0, 0, 0, 0, 0, 6.8376488468020759222e-12, 1.1492036416820246223e-07, 7.1697944771843335622e-06, 0.00015024502361066615209, 0.0017129832691462772674, 0.012198067843520864206, 0.057614376649993519208, 0.18625055189529041155, 0.41578120601587292837, 0.64273379913532080465, 0.68051701302793721204, 0.48636048533160558538, 0.22961899064651025704, 0.056512123157649481187, 0.0020378520142550187594, 0, 0, 0, 0, 0, 0, 0, 0, 9.4145936087959968535e-09, 1.3870093306447404973e-06, 3.87897484409095467e-05, 0.00054627415024945797078, 0.0045890968808638254312, 0.025078293403640077724, 0.092478958938429417502, 0.23258667138901650828, 0.39413899790146145197, 0.42298881551441958049, 0.23945155313138169473, 0.027468762021704565962, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5.4046605238618360691e-10, 5.5343723764345201347e-07, 2.3980715961026268597e-05, 0.00042724725529509507927, 0.004257128234965919765, 0.026669687498425763417, 0.10780673176995581031, 0.27416234388481963702, 0.37835275864003403701, 0.14369344182750692918, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.2897006858339270209e-12, 4.7885683674533647596e-07, 3.6923035679543052192e-05, 0.00083835438251815758755, 0.0099780154986416052382, 0.068735165020702704286, 0.25981214905110344704, 0.35274625004443893594, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5.7834701086365511565e-07, 0.00014053832363986818919, 0.0044940505180239382829, 0.063888867507214189279, 0.37188043953033556033, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6.344155240374716805e-07, 0.0019825485126170991519, 0.10217325456175885279, ]).reshape((20, 12, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=5
df=None
intercept=FALSE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([])
output=np.array([0.0016644455553086624083, 0.002495003748750156472, 0.0037387626499230611904, 0.0055997301828166279672, 0.0083806908754831561242, 0.012528591196985617434, 0.018697687754151010031, 0.027833351892417868001, 0.041273795716988054272, 0.060850651795732749183, 0.088929278684076601413, 0.12824165844093327049, 0.18119239198615591513, 0.2480374111869024234, 0.32305865186741777872, 0.38825858275226748928, 0.40740939128236214328, 0.33403355026188807919, 0.1632256493056772062, 0.017159494075893101661, 1.1100003703292180217e-06, 2.4962518746875000594e-06, 5.6123532398144536609e-06, 1.2613583192428893271e-05, 2.8332643039885458129e-05, 6.3586947395334136954e-05, 0.00014252672253270057788, 0.00031885525421459126778, 0.00071127791904317637373, 0.0015797796625579053637, 0.0034857420363066235133, 0.0076146085944587566657, 0.016381164929213501424, 0.034414525059745272595, 0.069651262704549549154, 0.13271586265443335861, 0.22841224706537738287, 0.32670272008482859061, 0.31696810306481593145, 0.097144158424494309045, 3.7012349794238681896e-10, 1.2487503125000001559e-09, 4.2124242480468747114e-09, 1.4206263137512205132e-08, 4.7892153138227467571e-08, 1.6136291046154529314e-07, 5.4321868305356652586e-07, 1.8263821320052253243e-06, 6.1287830369107454558e-06, 2.0506795807293100125e-05, 6.8314944883560930534e-05, 0.00022606641535874693596, 0.00074049070575379688161, 0.0023874614910316710632, 0.0075083864312185822493, 0.022682692646808663012, 0.064029150684570476648, 0.15976638757864897178, 0.30776038811264622153, 0.27497860584540806395, 6.1707818930041155992e-14, 3.1234375000000000385e-13, 1.5808447265625000092e-12, 8.0000230407714837838e-12, 4.0477309670448300476e-11, 2.0474318975195293786e-10, 1.0351972331039468262e-09, 5.2306989582536832363e-09, 2.6404574434177521861e-08, 1.3309725534797475592e-07, 6.6943159388078163953e-07, 3.3557879908853077946e-06, 1.6736492419104248054e-05, 8.2813468459374991711e-05, 0.00040470096744437702128, 0.0019383686901439161829, 0.0089744140037595811904, 0.039065023078631577746, 0.14941007561236896439, 0.3891805482645412928, 4.1152263374485595055e-18, 3.1249999999999996265e-17, 2.3730468749999997491e-16, 1.802032470703124905e-15, 1.3684184074401855573e-14, 1.0391427281498912374e-13, 7.8909900918882334723e-13, 5.9922206010276265389e-12, 4.550342518905353542e-11, 3.4554163502937537769e-10, 2.6239567910043193503e-09, 1.9925671881689045749e-08, 1.5131057085157624606e-07, 1.1490146474041569126e-06, 8.7253299787253137094e-06, 6.625797452594535961e-05, 0.00050314649405639756945, 0.0038207686892407699206, 0.02901396223392209775, 0.22032477571384589954, ]).reshape((20, 5, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=5
df=None
intercept=FALSE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([100, ])
output=np.array([0.048976959490485305615, 0.072709651432435309926, 0.10738526002140330595, 0.15737069698548675212, 0.22795032016720712109, 0.32446067835465053353, 0.44989711207706667428, 0.59997610680291524332, 0.75567735422921566979, 0.87844628783911327119, 0.92474207754597603781, 0.89365801028359981295, 0.82931282542333117913, 0.73973445494401424138, 0.62003614417178387619, 0.47000507098119359561, 0.30069481083090426887, 0.14132229342078814205, 0.034781194831623722663, 0.0012542251818798723622, 3.2979557543950620822e-05, 7.3809001036874996929e-05, 0.00016474549918236330078, 0.00036624856893145208737, 0.00080932434818232947688, 0.0017723123294799305975, 0.003828742087843573557, 0.008104291378262778317, 0.016639190251049165714, 0.032657698828719619599, 0.060108147688838667322, 0.10184799113601375464, 0.15884341152280503917, 0.23108234071251923525, 0.31281804868450863166, 0.38543973833058026157, 0.4110892044013776947, 0.34067876601502938838, 0.16765476842547211156, 0.017707951623962527032, 1.1050122962962964101e-08, 3.7191558750000007188e-08, 1.250033797265625178e-07, 4.1927333935913091575e-07, 1.401894586697731242e-06, 4.665382495865281394e-06, 1.5415847866808392633e-05, 5.0391939592239989756e-05, 0.00016203956276710770662, 0.00050812727751784608765, 0.0015332452896675867124, 0.004365181610267205789, 0.011468673667365519159, 0.027632876244132381638, 0.061266201119033726619, 0.12400124625180757032, 0.22211304163999795458, 0.32622078746654581405, 0.32211683874203467237, 0.099883337969340235674, 1.8476543209876543964e-12, 9.3431250000000011463e-12, 4.7218886718750000078e-11, 2.3843292297363282786e-10, 1.2024140499687195818e-09, 6.0518902752095459008e-09, 3.0369400855124124217e-08, 1.5170773682471650721e-07, 7.5254925311084902029e-07, 3.6922964379636862066e-06, 1.7800105408249686863e-05, 8.3338305189489776193e-05, 0.00037055336224994447902, 0.0015169299478213021069, 0.0056546686833628886926, 0.019188949419050076173, 0.058577982030935032975, 0.15402658068596292162, 0.30726533809094308536, 0.28101637370320353693, 1.2345679012345678054e-16, 9.3749999999999994959e-16, 7.1191406249999982613e-15, 5.4060974121093752673e-14, 4.1052552223205572084e-13, 3.1174281844496722735e-12, 2.3672970275664694964e-11, 1.7976661803082883009e-10, 1.3651027556716062823e-09, 1.0366249050881264123e-08, 7.8718703730129575545e-08, 5.9777015645067147174e-07, 4.5359107007994105463e-06, 3.3361176067584402466e-05, 0.00022366759792994563949, 0.0013435210788023243395, 0.0072639461407535297829, 0.035100831436999477275, 0.14396679069931464512, 0.39291034738734598175, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1.1354749159583518215e-10, 3.6975445151343504115e-08, 1.2697433810604620809e-06, 2.1473938565867883887e-05, 0.00026101495603127997146, 0.0026507409746741200131, 0.024215069210611606804, 0.20722776413426768904, ]).reshape((20, 6, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=5
df=None
intercept=FALSE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([1000, ])
output=np.array([0.0049866811037051986949, 0.0074700487125113432946, 0.011182664341492391802, 0.016723679332538900905, 0.024972650503249212151, 0.037206183352661716113, 0.055244396744596628579, 0.081608715488401706306, 0.11962663350752600344, 0.17331618121340841565, 0.2466772227646655824, 0.34167053826082144363, 0.45378166194157465441, 0.56434225055466613608, 0.6320750091698927875, 0.59733241830569760999, 0.42886959529414492298, 0.2049173244168793484, 0.050432732505854400984, 0.0018186265137258147626, 3.3277811103950621097e-06, 7.4812668695624991679e-06, 1.6811804138396487754e-05, 3.7755607955489316522e-05, 8.4711061600122608767e-05, 0.00018979511914756760698, 0.00042433325892820140772, 0.00094567009442594711366, 0.002097376821719070146, 0.0046178870868948916628, 0.010055306643782090104, 0.021527218530989256778, 0.04489775700844654549, 0.089884991503020608694, 0.16855047321618027434, 0.28372166497555245668, 0.39667928927647067017, 0.39859166318439254173, 0.21962210770558857065, 0.024829927856976751616, 1.1100002962962965174e-09, 3.7443772500000006212e-09, 1.2627790523437500951e-08, 4.2570810898681638324e-08, 1.4343375976686856963e-07, 4.8286151921739802212e-07, 1.6234543349501860428e-06, 5.4478341089133651746e-06, 1.8228467705229433379e-05, 6.0725950389411787269e-05, 0.000200959732568890164, 0.00065830362619350942834, 0.0021228688895969845957, 0.0066792918381076149537, 0.020201657448734186562, 0.057212961493873837338, 0.14427872595983071147, 0.29075824853504678158, 0.36564110074442957021, 0.13330127370825312072, 1.8509876543209877828e-13, 9.3684375000000001865e-13, 4.7411103515625013439e-12, 2.3989256927490236273e-11, 1.2134982390689849712e-10, 6.1360608361896876588e-10, 3.1008571052567070342e-09, 1.5656143551154886378e-08, 7.8940702751398238619e-08, 3.9721851623374797589e-07, 1.9925510408963188817e-06, 9.9478099413657914188e-06, 4.930161383220328208e-05, 0.00024154631749370003959, 0.0011617509224607792256, 0.0054175582232760775836, 0.023904363046940359239, 0.094270457100450164023, 0.27882003179675446392, 0.34581727191398570209, 1.2345679012345680828e-17, 9.3750000000000019611e-17, 7.1191406250000002335e-16, 5.4060974121093743207e-15, 4.1052552223205574609e-14, 3.1174281844496730813e-13, 2.3672970275664701427e-12, 1.7976661803082880424e-11, 1.3651027556716061272e-10, 1.0366249050881260814e-09, 7.8718703730129572237e-09, 5.9777015645067147174e-08, 4.5393171255472860584e-07, 3.4470439422124707377e-06, 2.6175989936175949598e-05, 0.00019877392357783613304, 0.0015094394821691928168, 0.011462306067722308894, 0.084705097520176186876, 0.41086218643981919918, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00077892972719669731786, 0.083370713567239546071, ]).reshape((20, 6, ), order="F")
--END TEST CASE--
--BEGIN TEST CASE--
degree=5
df=None
intercept=FALSE
Boundary.knots=np.array([0, 3000, ])
knots=np.array([10, 100, 1000, ])
output=np.array([0.40055561099999992258, 0.53723465540624992798, 0.68096479632714856933, 0.79404388738027953387, 0.82432650645400162848, 0.74772850370839760714, 0.60695411066016569102, 0.43541006443525137604, 0.25280073560290239332, 0.098204648213291390046, 0.015109611834895182725, 4.9867660859134372328e-05, 0, 0, 0, 0, 0, 0, 0, 0, 0.0089449327889999999397, 0.01902900664771874778, 0.039353322248692389207, 0.078015642723590794549, 0.14535820079903546964, 0.24864829948443537, 0.38546024005089984943, 0.54588573189455047441, 0.70480915137930777448, 0.81344804238272472308, 0.81717844525199323513, 0.71389962178539645432, 0.56021095018500233209, 0.38030131620961826755, 0.19975862317934431345, 0.062981619616186521049, 0.0053407260835513798575, 7.8059373845321992395e-10, 0, 0, 9.4545814444444459148e-06, 3.1017290343749996553e-05, 0.00010028811495410157556, 0.00031713361145809936415, 0.00096922483761113851879, 0.0028119072707210087531, 0.0075647083349296134064, 0.01861553313384527869, 0.042046394339120052308, 0.087124325583113268467, 0.16371060089498334911, 0.27411331174625735985, 0.40781324509205862938, 0.54286075051770132927, 0.63352683070691229172, 0.61289306201898841042, 0.44453962951650222157, 0.21269241367644950436, 0.052346279846925344859, 0.0018876298723944570646, 1.6294444444444447802e-09, 8.1548437499999982349e-09, 4.0568422851562507446e-08, 1.9994451278686527012e-07, 9.7096128099918361425e-07, 4.602189952521174856e-06, 2.0919648984453888767e-05, 8.8510989072612499115e-05, 0.0003426071500521665281, 0.0012159913347531664096, 0.0039613824662355322237, 0.011728070771626651625, 0.030981021316488138728, 0.072628957837475555115, 0.15091202659488145432, 0.27128094978421118944, 0.39491276471324787689, 0.40566968711467132902, 0.22597501897826155481, 0.025700635234072819607, 1.1111111111111111028e-13, 8.4374999999999991361e-13, 6.4072265625000016033e-12, 4.8654876708984379097e-11, 3.694729700088501736e-10, 2.8056853660047051398e-09, 2.1305018189503776537e-08, 1.5953977978988917004e-07, 1.1111370996233737328e-06, 6.9846696024166045466e-06, 3.9856249948076781855e-05, 0.00020802815256623063945, 0.00098445759075916142167, 0.0041227618062229087065, 0.015186511992247679614, 0.04905127538388008579, 0.1347633037922978394, 0.28642046903795781443, 0.37096041450757122337, 0.1373866644432741313, 0, 0, 0, 0, 0, 0, 2.1835286481805527197e-15, 7.5005881261891668233e-12, 3.9151793493690686266e-10, 7.8165151445884371071e-09, 1.0330194469679949402e-07, 1.0998832941312460716e-06, 1.0325449816590390154e-05, 8.6094485881001237893e-05, 0.00061191613127527461416, 0.0037238993946882535668, 0.01960252770274439893, 0.08667615291637777164, 0.27442290749348607903, 0.35255941616703945218, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3.6587525069769108936e-10, 1.1914310104321792976e-07, 4.09139533897260065e-06, 6.9193802045574309841e-05, 0.00084104819165634655664, 0.0085412764739499439509, 0.075516449446559150149, 0.39909494071597967357, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.00077892972719669731786, 0.083370713567239546071, ]).reshape((20, 8, ), order="F")
--END TEST CASE--
"""
R_bs_num_tests = 72

########NEW FILE########
__FILENAME__ = test_state
import numpy as np
from patsy.state import Center, Standardize, center
from patsy.util import atleast_2d_column_default

def check_stateful(cls, accepts_multicolumn, input, output, *args, **kwargs):
    input = np.asarray(input)
    output = np.asarray(output)
    test_cases = [
        # List input, one chunk
        ([input], output),
        # Scalar input, many chunks
        (input, output),
        # List input, many chunks:
        ([[n] for n in input], output),
        # 0-d array input, many chunks:
        ([np.array(n) for n in input], output),
        # 1-d array input, one chunk:
        ([np.array(input)], output),
        # 1-d array input, many chunks:
        ([np.array([n]) for n in input], output),
        # 2-d but 1 column input, one chunk:
        ([np.array(input)[:, None]], atleast_2d_column_default(output)),
        # 2-d but 1 column input, many chunks:
        ([np.array([[n]]) for n in input], atleast_2d_column_default(output)),
        ]
    if accepts_multicolumn:
        # 2-d array input, one chunk:
        test_cases += [
            ([np.column_stack((input, input[::-1]))],
             np.column_stack((output, output[::-1]))),
            # 2-d array input, many chunks:
                ([np.array([[input[i], input[-i-1]]]) for i in xrange(len(input))],
                 np.column_stack((output, output[::-1]))),
            ]
    from patsy.util import have_pandas
    if have_pandas:
        import pandas
        pandas_type = (pandas.Series, pandas.DataFrame)
        pandas_index = np.linspace(0, 1, num=len(input))
        # 1d and 2d here refer to the dimensionality of the input
        if output.ndim == 1:
            output_1d = pandas.Series(output, index=pandas_index)
        else:
            output_1d = pandas.DataFrame(output, index=pandas_index)
        test_cases += [
            # Series input, one chunk
            ([pandas.Series(input, index=pandas_index)], output_1d),
            # Series input, many chunks
            ([pandas.Series([x], index=[idx])
              for (x, idx) in zip(input, pandas_index)],
             output_1d),
            ]
        if accepts_multicolumn:
            input_2d_2col = np.column_stack((input, input[::-1]))
            output_2d_2col = np.column_stack((output, output[::-1]))
            output_2col_dataframe = pandas.DataFrame(output_2d_2col,
                                                     index=pandas_index)
            test_cases += [
                # DataFrame input, one chunk
                ([pandas.DataFrame(input_2d_2col, index=pandas_index)],
                 output_2col_dataframe),
                # DataFrame input, many chunks
                ([pandas.DataFrame([input_2d_2col[i, :]],
                                   index=[pandas_index[i]])
                  for i in xrange(len(input))],
                 output_2col_dataframe),
            ]
    for input_obj, output_obj in test_cases:
        print input_obj
        t = cls()
        for input_chunk in input_obj:
            t.memorize_chunk(input_chunk, *args, **kwargs)
        t.memorize_finish()
        all_outputs = []
        for input_chunk in input_obj:
            output_chunk = t.transform(input_chunk, *args, **kwargs)
            if input.ndim == output.ndim:
                assert output_chunk.ndim == np.asarray(input_chunk).ndim
            all_outputs.append(output_chunk)
        if have_pandas and isinstance(all_outputs[0], pandas_type):
            all_output1 = pandas.concat(all_outputs)
            assert np.array_equal(all_output1.index, pandas_index)
        elif all_outputs[0].ndim == 0:
            all_output1 = np.array(all_outputs)
        elif all_outputs[0].ndim == 1:
            all_output1 = np.concatenate(all_outputs)
        else:
            all_output1 = np.row_stack(all_outputs)
        assert all_output1.shape[0] == len(input)
        # output_obj_reshaped = np.asarray(output_obj).reshape(all_output1.shape)
        # assert np.allclose(all_output1, output_obj_reshaped)
        assert np.allclose(all_output1, output_obj)
        if np.asarray(input_obj[0]).ndim == 0:
            all_input = np.array(input_obj)
        elif have_pandas and isinstance(input_obj[0], pandas_type):
            # handles both Series and DataFrames
            all_input = pandas.concat(input_obj)
        elif np.asarray(input_obj[0]).ndim == 1:
            # Don't use row_stack, because that would turn this into a 1xn
            # matrix:
            all_input = np.concatenate(input_obj)
        else:
            all_input = np.row_stack(input_obj)
        all_output2 = t.transform(all_input, *args, **kwargs)
        if have_pandas and isinstance(input_obj[0], pandas_type):
            assert np.array_equal(all_output2.index, pandas_index)
        if input.ndim == output.ndim:
            assert all_output2.ndim == all_input.ndim
        assert np.allclose(all_output2, output_obj)

def test_Center():
    check_stateful(Center, True, [1, 2, 3], [-1, 0, 1])
    check_stateful(Center, True, [1, 2, 1, 2], [-0.5, 0.5, -0.5, 0.5])
    check_stateful(Center, True,
                   [1.3, -10.1, 7.0, 12.0],
                   [-1.25, -12.65, 4.45, 9.45])

def test_stateful_transform_wrapper():
    assert np.allclose(center([1, 2, 3]), [-1, 0, 1])
    assert np.allclose(center([1, 2, 1, 2]), [-0.5, 0.5, -0.5, 0.5])
    assert center([1.0, 2.0, 3.0]).dtype == np.dtype(float)
    assert (center(np.array([1.0, 2.0, 3.0], dtype=np.float32)).dtype
            == np.dtype(np.float32))
    assert center([1, 2, 3]).dtype == np.dtype(float)

    from patsy.util import have_pandas
    if have_pandas:
        import pandas
        s = pandas.Series([1, 2, 3], index=["a", "b", "c"])
        df = pandas.DataFrame([[1, 2], [2, 4], [3, 6]],
                              columns=["x1", "x2"],
                              index=[10, 20, 30])
        s_c = center(s)
        assert isinstance(s_c, pandas.Series)
        assert np.array_equal(s_c.index, ["a", "b", "c"])
        assert np.allclose(s_c, [-1, 0, 1])
        df_c = center(df)
        assert isinstance(df_c, pandas.DataFrame)
        assert np.array_equal(df_c.index, [10, 20, 30])
        assert np.array_equal(df_c.columns, ["x1", "x2"])
        assert np.allclose(df_c, [[-1, -2], [0, 0], [1, 2]])

def test_Standardize():
    check_stateful(Standardize, True, [1, -1], [1, -1])
    check_stateful(Standardize, True, [12, 10], [1, -1])
    check_stateful(Standardize, True,
                   [12, 11, 10],
                   [np.sqrt(3./2), 0, -np.sqrt(3./2)])

    check_stateful(Standardize, True,
                   [12.0, 11.0, 10.0],
                   [np.sqrt(3./2), 0, -np.sqrt(3./2)])

    # XX: see the comment in Standardize.transform about why this doesn't
    # work:
    # check_stateful(Standardize,
    #               [12.0+0j, 11.0+0j, 10.0],
    #               [np.sqrt(3./2)+0j, 0, -np.sqrt(3./2)])

    check_stateful(Standardize, True, [1, -1], [np.sqrt(2)/2, -np.sqrt(2)/2],
                   ddof=1)

    check_stateful(Standardize, True,
                   range(20),
                   list((np.arange(20) - 9.5) / 5.7662812973353983),
                   ddof=0)
    check_stateful(Standardize, True,
                   range(20),
                   list((np.arange(20) - 9.5) / 5.9160797830996161),
                   ddof=1)
    check_stateful(Standardize, True,
                   range(20),
                   list((np.arange(20) - 9.5)),
                   rescale=False, ddof=1)
    check_stateful(Standardize, True,
                   range(20),
                   list(np.arange(20) / 5.9160797830996161),
                   center=False, ddof=1)
    check_stateful(Standardize, True,
                   range(20),
                   range(20),
                   center=False, rescale=False, ddof=1)

########NEW FILE########
__FILENAME__ = tokens
# This file is part of Patsy
# Copyright (C) 2011 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# Utilities for dealing with Python code at the token level.
#
# Includes:
#   a "pretty printer" that converts a sequence of tokens back into a
#       readable, white-space normalized string.
#   a utility function to replace calls to global functions with calls to
#       other functions

import tokenize
from cStringIO import StringIO

from patsy import PatsyError
from patsy.origin import Origin

__all__ = ["python_tokenize", "pretty_untokenize",
           "normalize_token_spacing"]

# A convenience wrapper around tokenize.generate_tokens. yields tuples
#   (tokenize type, token string, origin object)
def python_tokenize(code):
    # Since formulas can only contain Python expressions, and Python
    # expressions cannot meaningfully contain newlines, we'll just remove all
    # the newlines up front to avoid any complications:
    code = code.replace("\n", " ").strip()
    it = tokenize.generate_tokens(StringIO(code).readline)
    try:
        for (pytype, string, (_, start), (_, end), code) in it:
            if pytype == tokenize.ENDMARKER:
                break
            origin = Origin(code, start, end)
            assert pytype not in (tokenize.NL, tokenize.NEWLINE)
            if pytype == tokenize.ERRORTOKEN:
                raise PatsyError("error tokenizing input "
                                 "(maybe an unclosed string?)",
                                 origin)
            if pytype == tokenize.COMMENT:
                raise PatsyError("comments are not allowed", origin)
            yield (pytype, string, origin)
        else: # pragma: no cover
            raise ValueError, "stream ended without ENDMARKER?!?"
    except tokenize.TokenError, e:
        # TokenError is raised iff the tokenizer thinks that there is
        # some sort of multi-line construct in progress (e.g., an
        # unclosed parentheses, which in Python lets a virtual line
        # continue past the end of the physical line), and it hits the
        # end of the source text. We have our own error handling for
        # such cases, so just treat this as an end-of-stream.
        # 
        # Just in case someone adds some other error case:
        assert e.args[0].startswith("EOF in multi-line")
        return

def test_python_tokenize():
    code = "a + (foo * -1)"
    tokens = list(python_tokenize(code))
    expected = [(tokenize.NAME, "a", Origin(code, 0, 1)),
                (tokenize.OP, "+", Origin(code, 2, 3)),
                (tokenize.OP, "(", Origin(code, 4, 5)),
                (tokenize.NAME, "foo", Origin(code, 5, 8)),
                (tokenize.OP, "*", Origin(code, 9, 10)),
                (tokenize.OP, "-", Origin(code, 11, 12)),
                (tokenize.NUMBER, "1", Origin(code, 12, 13)),
                (tokenize.OP, ")", Origin(code, 13, 14))]
    assert tokens == expected

    code2 = "a + (b"
    tokens2 = list(python_tokenize(code2))
    expected2 = [(tokenize.NAME, "a", Origin(code2, 0, 1)),
                 (tokenize.OP, "+", Origin(code2, 2, 3)),
                 (tokenize.OP, "(", Origin(code2, 4, 5)),
                 (tokenize.NAME, "b", Origin(code2, 5, 6))]
    assert tokens2 == expected2

    from nose.tools import assert_raises
    assert_raises(PatsyError, list, python_tokenize("a b # c"))

    from nose.tools import assert_raises
    assert_raises(PatsyError, list, python_tokenize("a b \"c"))

_python_space_both = (list("+-*/%&^|<>")
                      + ["==", "<>", "!=", "<=", ">=",
                         "<<", ">>", "**", "//"])
_python_space_before = (_python_space_both
                        + ["!", "~"])
_python_space_after = (_python_space_both
                       + [",", ":"])

def pretty_untokenize(typed_tokens):
    text = []
    prev_was_space_delim = False
    prev_wants_space = False
    prev_was_open_paren_or_comma = False
    prev_was_object_like = False
    brackets = []
    for token_type, token in typed_tokens:
        assert token_type not in (tokenize.INDENT, tokenize.DEDENT,
                                  tokenize.NEWLINE, tokenize.NL)
        if token_type == tokenize.ENDMARKER:
            continue
        if token_type in (tokenize.NAME, tokenize.NUMBER, tokenize.STRING):
            if prev_wants_space or prev_was_space_delim:
                text.append(" ")
            text.append(token)
            prev_wants_space = False
            prev_was_space_delim = True
        else:
            if token in ("(", "[", "{"):
                brackets.append(token)
            elif brackets and token in (")", "]", "}"):
                brackets.pop()
            this_wants_space_before = (token in _python_space_before)
            this_wants_space_after = (token in _python_space_after)
            # Special case for slice syntax: foo[:10]
            # Otherwise ":" is spaced after, like: "{1: ...}", "if a: ..."
            if token == ":" and brackets and brackets[-1] == "[":
                this_wants_space_after = False
            # Special case for foo(*args), foo(a, *args):
            if token in ("*", "**") and prev_was_open_paren_or_comma:
                this_wants_space_before = False
                this_wants_space_after = False
            # Special case for "a = foo(b=1)":
            if token == "=" and not brackets:
                this_wants_space_before = True
                this_wants_space_after = True
            # Special case for unary -, +. Our heuristic is that if we see the
            # + or - after something that looks like an object (a NAME,
            # NUMBER, STRING, or close paren) then it is probably binary,
            # otherwise it is probably unary.
            if token in ("+", "-") and not prev_was_object_like:
                this_wants_space_before = False
                this_wants_space_after = False
            if prev_wants_space or this_wants_space_before:
                text.append(" ")
            text.append(token)
            prev_wants_space = this_wants_space_after
            prev_was_space_delim = False
        if (token_type in (tokenize.NAME, tokenize.NUMBER, tokenize.STRING)
            or token == ")"):
            prev_was_object_like = True
        else:
            prev_was_object_like = False
        prev_was_open_paren_or_comma = token in ("(", ",")
    return "".join(text)

def normalize_token_spacing(code):
    tokens = [(t[0], t[1])
              for t in tokenize.generate_tokens(StringIO(code).readline)]
    return pretty_untokenize(tokens)

def test_pretty_untokenize_and_normalize_token_spacing():
    assert normalize_token_spacing("1 + 1") == "1 + 1"
    assert normalize_token_spacing("1+1") == "1 + 1"
    assert normalize_token_spacing("1*(2+3**2)") == "1 * (2 + 3 ** 2)"
    assert normalize_token_spacing("a and b") == "a and b"
    assert normalize_token_spacing("foo(a=bar.baz[1:])") == "foo(a=bar.baz[1:])"
    assert normalize_token_spacing("""{"hi":foo[:]}""") == """{"hi": foo[:]}"""
    assert normalize_token_spacing("""'a' "b" 'c'""") == """'a' "b" 'c'"""
    assert normalize_token_spacing('"""a""" is 1 or 2==3') == '"""a""" is 1 or 2 == 3'
    assert normalize_token_spacing("foo ( * args )") == "foo(*args)"
    assert normalize_token_spacing("foo ( a * args )") == "foo(a * args)"
    assert normalize_token_spacing("foo ( ** args )") == "foo(**args)"
    assert normalize_token_spacing("foo ( a ** args )") == "foo(a ** args)"
    assert normalize_token_spacing("foo (1, * args )") == "foo(1, *args)"
    assert normalize_token_spacing("foo (1, a * args )") == "foo(1, a * args)"
    assert normalize_token_spacing("foo (1, ** args )") == "foo(1, **args)"
    assert normalize_token_spacing("foo (1, a ** args )") == "foo(1, a ** args)"

    assert normalize_token_spacing("a=foo(b = 1)") == "a = foo(b=1)"

    assert normalize_token_spacing("foo(+ 10, bar = - 1)") == "foo(+10, bar=-1)"
    assert normalize_token_spacing("1 + +10 + -1 - 5") == "1 + +10 + -1 - 5"

########NEW FILE########
__FILENAME__ = user_util
# This file is part of Patsy
# Copyright (C) 2012 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# Miscellaneous utilities that are useful to users (as compared to
# patsy.util, which is misc. utilities useful for implementing patsy).

# These are made available in the patsy.* namespace
__all__ = ["balanced", "demo_data", "LookupFactor"]

import numpy as np
from patsy import PatsyError
from patsy.compat import itertools_product
from patsy.categorical import C

def balanced(**kwargs):
    """balanced(factor_name=num_levels, [factor_name=num_levels, ..., repeat=1])

    Create simple balanced factorial designs for testing.

    Given some factor names and the number of desired levels for each,
    generates a balanced factorial design in the form of a data
    dictionary. For example:

    .. ipython::

       In [1]: balanced(a=2, b=3)
       Out[1]:
       {'a': ['a1', 'a1', 'a1', 'a2', 'a2', 'a2'],
        'b': ['b1', 'b2', 'b3', 'b1', 'b2', 'b3']}

    By default it produces exactly one instance of each combination of levels,
    but if you want multiple replicates this can be accomplished via the
    `repeat` argument:

    .. ipython::

       In [2]: balanced(a=2, b=2, repeat=2)
       Out[2]:
       {'a': ['a1', 'a1', 'a2', 'a2', 'a1', 'a1', 'a2', 'a2'],
        'b': ['b1', 'b2', 'b1', 'b2', 'b1', 'b2', 'b1', 'b2']}
    """
    repeat = kwargs.pop("repeat", 1)
    levels = []
    names = sorted(kwargs)
    for name in names:
        level_count = kwargs[name]
        levels.append(["%s%s" % (name, i) for i in xrange(1, level_count + 1)])
    # zip(*...) does an "unzip"
    values = zip(*itertools_product(*levels))
    data = {}
    for name, value in zip(names, values):
        data[name] = list(value) * repeat
    return data

def test_balanced():
    data = balanced(a=2, b=3)
    assert data["a"] == ["a1", "a1", "a1", "a2", "a2", "a2"]
    assert data["b"] == ["b1", "b2", "b3", "b1", "b2", "b3"]
    data = balanced(a=2, b=3, repeat=2)
    assert data["a"] == ["a1", "a1", "a1", "a2", "a2", "a2",
                         "a1", "a1", "a1", "a2", "a2", "a2"]
    assert data["b"] == ["b1", "b2", "b3", "b1", "b2", "b3",
                         "b1", "b2", "b3", "b1", "b2", "b3"]

def demo_data(*names, **kwargs):
    """demo_data(*names, nlevels=2, min_rows=5)

    Create simple categorical/numerical demo data.

    Pass in a set of variable names, and this function will return a simple
    data set using those variable names.

    Names whose first letter falls in the range "a" through "m" will be made
    categorical (with `nlevels` levels). Those that start with a "p" through
    "z" are numerical.

    We attempt to produce a balanced design on the categorical variables,
    repeating as necessary to generate at least `min_rows` data
    points. Categorical variables are returned as a list of strings.

    Numerical data is generated by sampling from a normal distribution. A
    fixed random seed is used, so that identical calls to demo_data() will
    produce identical results. Numerical data is returned in a numpy array.

    Example:

    .. ipython:

       In [1]: patsy.demo_data("a", "b", "x", "y")
       Out[1]: 
       {'a': ['a1', 'a1', 'a2', 'a2', 'a1', 'a1', 'a2', 'a2'],
        'b': ['b1', 'b2', 'b1', 'b2', 'b1', 'b2', 'b1', 'b2'],
        'x': array([ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,
                     1.86755799, -0.97727788,  0.95008842, -0.15135721]),
        'y': array([-0.10321885,  0.4105985 ,  0.14404357,  1.45427351,
                     0.76103773,  0.12167502,  0.44386323,  0.33367433])}
    """
    nlevels = kwargs.pop("nlevels", 2)
    min_rows = kwargs.pop("min_rows", 5)
    if kwargs:
        raise TypeError, "unexpected keyword arguments %r" % (kwargs)
    numerical = set()
    categorical = {}
    for name in names:
        if name[0] in "abcdefghijklmn":
            categorical[name] = nlevels
        elif name[0] in "pqrstuvwxyz":
            numerical.add(name)
        else:
            raise PatsyError, "bad name %r" % (name,)
    balanced_design_size = np.prod(categorical.values(), dtype=int)
    repeat = int(np.ceil(min_rows * 1.0 / balanced_design_size))
    num_rows = repeat * balanced_design_size
    data = balanced(repeat=repeat, **categorical)
    r = np.random.RandomState(0)
    for name in sorted(numerical):
        data[name] = r.normal(size=num_rows)
    return data
    
def test_demo_data():
    d1 = demo_data("a", "b", "x")
    assert sorted(d1.keys()) == ["a", "b", "x"]
    assert d1["a"] == ["a1", "a1", "a2", "a2", "a1", "a1", "a2", "a2"]
    assert d1["b"] == ["b1", "b2", "b1", "b2", "b1", "b2", "b1", "b2"]
    assert d1["x"].dtype == np.dtype(float)
    assert d1["x"].shape == (8,)

    d2 = demo_data("x", "y")
    assert sorted(d2.keys()) == ["x", "y"]
    assert len(d2["x"]) == len(d2["y"]) == 5

    assert len(demo_data("x", min_rows=10)["x"]) == 10
    assert len(demo_data("a", "b", "x", min_rows=10)["x"]) == 12
    assert len(demo_data("a", "b", "x", min_rows=10, nlevels=3)["x"]) == 18

    from nose.tools import assert_raises
    assert_raises(PatsyError, demo_data, "a", "b", "__123")
    assert_raises(TypeError, demo_data, "a", "b", asdfasdf=123)

class LookupFactor(object):
    """A simple factor class that simply looks up a named entry in the given
    data.

    Useful for programatically constructing formulas, and as a simple example
    of the factor protocol.  For details see
    :ref:`expert-model-specification`.

    Example::

      dmatrix(ModelDesc([], [Term([LookupFactor("x")])]), {"x": [1, 2, 3]})

    :arg varname: The name of this variable; used as a lookup key in the
      passed in data dictionary/DataFrame/whatever.
    :arg force_categorical: If True, then treat this factor as
      categorical. (Equivalent to using :func:`C` in a regular formula, but
      of course you can't do that with a :class:`LookupFactor`.
    :arg contrast: If given, the contrast to use; see :func:`C`. (Requires
      ``force_categorical=True``.)
    :arg levels: If given, the categorical levels; see :func:`C`. (Requires
      ``force_categorical=True``.)
    :arg origin: Either ``None``, or the :class:`Origin` of this factor for use
      in error reporting.

    .. versionadded:: 0.2.0
       The ``force_categorical`` and related arguments.
    """
    def __init__(self, varname,
                 force_categorical=False, contrast=None, levels=None,
                 origin=None):
        self._varname = varname
        self._force_categorical = force_categorical
        self._contrast = contrast
        self._levels = levels
        self.origin = origin
        if not self._force_categorical:
            if contrast is not None:
                raise ValueError("contrast= requires force_categorical=True")
            if levels is not None:
                raise ValueError("levels= requires force_categorical=True")

    def name(self):
        return self._varname

    def __repr__(self):
        return "%s(%r)" % (self.__class__.__name__, self._varname)
       
    def __eq__(self, other):
        return (isinstance(other, LookupFactor)
                and self._varname == other._varname
                and self._force_categorical == other._force_categorical
                and self._contrast == other._contrast
                and self._levels == other._levels)

    def __ne__(self, other):
        return not self == other

    def __hash__(self):
        return hash((LookupFactor, self._varname,
                     self._force_categorical, self._contrast, self._levels))

    def memorize_passes_needed(self, state):
        return 0

    def memorize_chunk(self, state, which_pass, env): # pragma: no cover
        assert False

    def memorize_finish(self, state, which_pass): # pragma: no cover
        assert False

    def eval(self, memorize_state, data):
        value = data[self._varname]
        if self._force_categorical:
            value = C(value, contrast=self._contrast, levels=self._levels)
        return value

def test_LookupFactor():
    l_a = LookupFactor("a")
    assert l_a.name() == "a"
    assert l_a == LookupFactor("a")
    assert l_a != LookupFactor("b")
    assert hash(l_a) == hash(LookupFactor("a"))
    assert hash(l_a) != hash(LookupFactor("b"))
    assert l_a.eval({}, {"a": 1}) == 1
    assert l_a.eval({}, {"a": 2}) == 2
    assert repr(l_a) == "LookupFactor('a')"
    assert l_a.origin is None
    l_with_origin = LookupFactor("b", origin="asdf")
    assert l_with_origin.origin == "asdf"

    l_c = LookupFactor("c", force_categorical=True,
                       contrast="CONTRAST", levels=(1, 2))
    box = l_c.eval({}, {"c": [1, 1, 2]})
    assert box.data == [1, 1, 2]
    assert box.contrast == "CONTRAST"
    assert box.levels == (1, 2)

    from nose.tools import assert_raises
    assert_raises(ValueError, LookupFactor, "nc", contrast="CONTRAST")
    assert_raises(ValueError, LookupFactor, "nc", levels=(1, 2))

########NEW FILE########
__FILENAME__ = util
# This file is part of Patsy
# Copyright (C) 2011-2013 Nathaniel Smith <njs@pobox.com>
# See file LICENSE.txt for license information.

# Some generic utilities.

__all__ = ["atleast_2d_column_default", "uniqueify_list",
           "widest_float", "widest_complex", "wide_dtype_for", "widen",
           "repr_pretty_delegate", "repr_pretty_impl",
           "SortAnythingKey", "safe_scalar_isnan", "safe_isnan",
           "iterable",
           ]

import sys
import numpy as np
from cStringIO import StringIO
from compat import optional_dep_ok

try:
    import pandas
except ImportError:
    have_pandas = False
else:
    have_pandas = True

# Pandas versions < 0.9.0 don't have Categorical
# Can drop this guard whenever we drop support for such older versions of
# pandas.
have_pandas_categorical = (have_pandas and hasattr(pandas, "Categorical"))

# Passes through Series and DataFrames, call np.asarray() on everything else
def asarray_or_pandas(a, copy=False, dtype=None, subok=False):
    if have_pandas:
        if isinstance(a, (pandas.Series, pandas.DataFrame)):
            # The .name attribute on Series is discarded when passing through
            # the constructor:
            #   https://github.com/pydata/pandas/issues/1578
            extra_args = {}
            if hasattr(a, "name"):
                extra_args["name"] = a.name
            return a.__class__(a, copy=copy, dtype=dtype, **extra_args)
    return np.array(a, copy=copy, dtype=dtype, subok=subok)

def test_asarray_or_pandas():
    assert type(asarray_or_pandas([1, 2, 3])) is np.ndarray
    assert type(asarray_or_pandas(np.matrix([[1, 2, 3]]))) is np.ndarray
    assert type(asarray_or_pandas(np.matrix([[1, 2, 3]]), subok=True)) is np.matrix
    a = np.array([1, 2, 3])
    assert asarray_or_pandas(a) is a
    a_copy = asarray_or_pandas(a, copy=True)
    assert np.array_equal(a, a_copy)
    a_copy[0] = 100
    assert not np.array_equal(a, a_copy)
    assert np.allclose(asarray_or_pandas([1, 2, 3], dtype=float),
                       [1.0, 2.0, 3.0])
    assert asarray_or_pandas([1, 2, 3], dtype=float).dtype == np.dtype(float)
    a_view = asarray_or_pandas(a, dtype=a.dtype)
    a_view[0] = 99
    assert a[0] == 99
    global have_pandas
    if have_pandas:
        s = pandas.Series([1, 2, 3], name="A", index=[10, 20, 30])
        s_view1 = asarray_or_pandas(s)
        assert s_view1.name == "A"
        assert np.array_equal(s_view1.index, [10, 20, 30])
        s_view1[10] = 101
        assert s[10] == 101
        s_copy = asarray_or_pandas(s, copy=True)
        assert s_copy.name == "A"
        assert np.array_equal(s_copy.index, [10, 20, 30])
        assert np.array_equal(s_copy, s)
        s_copy[10] = 100
        assert not np.array_equal(s_copy, s)
        assert asarray_or_pandas(s, dtype=float).dtype == np.dtype(float)
        s_view2 = asarray_or_pandas(s, dtype=s.dtype)
        assert s_view2.name == "A"
        assert np.array_equal(s_view2.index, [10, 20, 30])
        s_view2[10] = 99
        assert s[10] == 99

        df = pandas.DataFrame([[1, 2, 3]],
                              columns=["A", "B", "C"],
                              index=[10])
        df_view1 = asarray_or_pandas(df)
        df_view1.ix[10, "A"] = 101
        assert np.array_equal(df_view1.columns, ["A", "B", "C"])
        assert np.array_equal(df_view1.index, [10])
        assert df.ix[10, "A"] == 101
        df_copy = asarray_or_pandas(df, copy=True)
        assert np.array_equal(df_copy, df)
        assert np.array_equal(df_copy.columns, ["A", "B", "C"])
        assert np.array_equal(df_copy.index, [10])
        df_copy.ix[10, "A"] = 100
        assert not np.array_equal(df_copy, df)
        df_converted = asarray_or_pandas(df, dtype=float)
        assert df_converted["A"].dtype == np.dtype(float)
        assert np.allclose(df_converted, df)
        assert np.array_equal(df_converted.columns, ["A", "B", "C"])
        assert np.array_equal(df_converted.index, [10])
        df_view2 = asarray_or_pandas(df, dtype=df["A"].dtype)
        assert np.array_equal(df_view2.columns, ["A", "B", "C"])
        assert np.array_equal(df_view2.index, [10])
        # This actually makes a copy, not a view, because of a pandas bug:
        #   https://github.com/pydata/pandas/issues/1572
        assert np.array_equal(df, df_view2)
        # df_view2[0][0] = 99
        # assert df[0][0] == 99

        had_pandas = have_pandas
        try:
            have_pandas = False
            assert (type(asarray_or_pandas(pandas.Series([1, 2, 3])))
                    is np.ndarray)
            assert (type(asarray_or_pandas(pandas.DataFrame([[1, 2, 3]])))
                    is np.ndarray)
        finally:
            have_pandas = had_pandas

# Like np.atleast_2d, but this converts lower-dimensional arrays into columns,
# instead of rows. It also converts ndarray subclasses into basic ndarrays,
# which makes it easier to guarantee correctness. However, there are many
# places in the code where we want to preserve pandas indexing information if
# present, so there is also an option 
def atleast_2d_column_default(a, preserve_pandas=False):
    if preserve_pandas and have_pandas:
        if isinstance(a, pandas.Series):
            return pandas.DataFrame(a)
        elif isinstance(a, pandas.DataFrame):
            return a
        # fall through
    a = np.asarray(a)
    a = np.atleast_1d(a)
    if a.ndim <= 1:
        a = a.reshape((-1, 1))
    assert a.ndim >= 2
    return a

def test_atleast_2d_column_default():
    assert np.all(atleast_2d_column_default([1, 2, 3]) == [[1], [2], [3]])

    assert atleast_2d_column_default(1).shape == (1, 1)
    assert atleast_2d_column_default([1]).shape == (1, 1)
    assert atleast_2d_column_default([[1]]).shape == (1, 1)
    assert atleast_2d_column_default([[[1]]]).shape == (1, 1, 1)

    assert atleast_2d_column_default([1, 2, 3]).shape == (3, 1)
    assert atleast_2d_column_default([[1], [2], [3]]).shape == (3, 1)

    assert type(atleast_2d_column_default(np.matrix(1))) == np.ndarray

    global have_pandas
    if have_pandas:
        assert (type(atleast_2d_column_default(pandas.Series([1, 2])))
                == np.ndarray)
        assert (type(atleast_2d_column_default(pandas.DataFrame([[1], [2]])))
                == np.ndarray)
        assert (type(atleast_2d_column_default(pandas.Series([1, 2]),
                                               preserve_pandas=True))
                == pandas.DataFrame)
        assert (type(atleast_2d_column_default(pandas.DataFrame([[1], [2]]),
                                               preserve_pandas=True))
                == pandas.DataFrame)
        s = pandas.Series([10, 11,12], name="hi", index=["a", "b", "c"])
        df = atleast_2d_column_default(s, preserve_pandas=True)
        assert isinstance(df, pandas.DataFrame)
        assert np.all(df.columns == ["hi"])
        assert np.all(df.index == ["a", "b", "c"])
    assert (type(atleast_2d_column_default(np.matrix(1),
                                           preserve_pandas=True))
            == np.ndarray)
    assert (type(atleast_2d_column_default([1, 2, 3],
                                           preserve_pandas=True))
            == np.ndarray)
        
    if have_pandas:
        had_pandas = have_pandas
        try:
            have_pandas = False
            assert (type(atleast_2d_column_default(pandas.Series([1, 2]),
                                                   preserve_pandas=True))
                    == np.ndarray)
            assert (type(atleast_2d_column_default(pandas.DataFrame([[1], [2]]),
                                                   preserve_pandas=True))
                    == np.ndarray)
        finally:
            have_pandas = had_pandas

# A version of .reshape() that knows how to down-convert a 1-column
# pandas.DataFrame into a pandas.Series. Useful for code that wants to be
# agnostic between 1d and 2d data, with the pattern:
#   new_a = atleast_2d_column_default(a, preserve_pandas=True)
#   # do stuff to new_a, which can assume it's always 2 dimensional
#   return pandas_friendly_reshape(new_a, a.shape)
def pandas_friendly_reshape(a, new_shape):
    if not have_pandas:
        return a.reshape(new_shape)
    if not isinstance(a, pandas.DataFrame):
        return a.reshape(new_shape)
    # we have a DataFrame. Only supported reshapes are no-op, and
    # single-column DataFrame -> Series.
    if new_shape == a.shape:
        return a
    if len(new_shape) == 1 and a.shape[1] == 1:
        if new_shape[0] != a.shape[0]:
            raise ValueError, "arrays have incompatible sizes"
        return a[a.columns[0]]
    raise ValueError("cannot reshape a DataFrame with shape %s to shape %s"
                     % (a.shape, new_shape))

def test_pandas_friendly_reshape():
    from nose.tools import assert_raises
    global have_pandas
    assert np.allclose(pandas_friendly_reshape(np.arange(10).reshape(5, 2),
                                               (2, 5)),
                       np.arange(10).reshape(2, 5))
    if have_pandas:
        df = pandas.DataFrame({"x": [1, 2, 3]}, index=["a", "b", "c"])
        noop = pandas_friendly_reshape(df, (3, 1))
        assert isinstance(noop, pandas.DataFrame)
        assert np.array_equal(noop.index, ["a", "b", "c"])
        assert np.array_equal(noop.columns, ["x"])
        squozen = pandas_friendly_reshape(df, (3,))
        assert isinstance(squozen, pandas.Series)
        assert np.array_equal(squozen.index, ["a", "b", "c"])
        assert squozen.name == "x"

        assert_raises(ValueError, pandas_friendly_reshape, df, (4,))
        assert_raises(ValueError, pandas_friendly_reshape, df, (1, 3))
        assert_raises(ValueError, pandas_friendly_reshape, df, (3, 3))

        had_pandas = have_pandas
        try:
            have_pandas = False
            # this will try to do a reshape directly, and DataFrames *have* no
            # reshape method
            assert_raises(AttributeError, pandas_friendly_reshape, df, (3,))
        finally:
            have_pandas = had_pandas

def uniqueify_list(seq):
    seq_new = []
    seen = set()
    for obj in seq:
        if obj not in seen:
            seq_new.append(obj)
            seen.add(obj)
    return seq_new

def test_to_uniqueify_list():
    assert uniqueify_list([1, 2, 3]) == [1, 2, 3]
    assert uniqueify_list([1, 3, 3, 2, 3, 1]) == [1, 3, 2]
    assert uniqueify_list([3, 2, 1, 4, 1, 2, 3]) == [3, 2, 1, 4]

for float_type in ("float128", "float96", "float64"):
    if hasattr(np, float_type):
        widest_float = getattr(np, float_type)
        break
else: # pragma: no cover
    assert False
for complex_type in ("complex256", "complex196", "complex128"):
    if hasattr(np, complex_type):
        widest_complex = getattr(np, complex_type)
        break
else: # pragma: no cover
    assert False

def wide_dtype_for(arr):
    arr = np.asarray(arr)
    if (np.issubdtype(arr.dtype, np.integer)
        or np.issubdtype(arr.dtype, np.floating)):
        return widest_float
    elif np.issubdtype(arr.dtype, np.complexfloating):
        return widest_complex
    raise ValueError, "cannot widen a non-numeric type %r" % (arr.dtype,)

def widen(arr):
    return np.asarray(arr, dtype=wide_dtype_for(arr))

def test_wide_dtype_for_and_widen():
    assert np.allclose(widen([1, 2, 3]), [1, 2, 3])
    assert widen([1, 2, 3]).dtype == widest_float
    assert np.allclose(widen([1.0, 2.0, 3.0]), [1, 2, 3])
    assert widen([1.0, 2.0, 3.0]).dtype == widest_float
    assert np.allclose(widen([1+0j, 2, 3]), [1, 2, 3])
    assert widen([1+0j, 2, 3]).dtype == widest_complex
    from nose.tools import assert_raises
    assert_raises(ValueError, widen, ["hi"])

class PushbackAdapter(object):
    def __init__(self, it):
        self._it = it
        self._pushed = []

    def __iter__(self):
        return self

    def push_back(self, obj):
        self._pushed.append(obj)

    def next(self):
        if self._pushed:
            return self._pushed.pop()
        else:
            # May raise StopIteration
            return self._it.next()

    def peek(self):
        try:
            obj = self.next()
        except StopIteration:
            raise ValueError, "no more data"
        self.push_back(obj)
        return obj

    def has_more(self):
        try:
            self.peek()
        except ValueError:
            return False
        else:
            return True

def test_PushbackAdapter():
    it = PushbackAdapter(iter([1, 2, 3, 4]))
    assert it.has_more()
    assert it.next() == 1
    it.push_back(0)
    assert it.next() == 0
    assert it.next() == 2
    assert it.peek() == 3
    it.push_back(10)
    assert it.peek() == 10
    it.push_back(20)
    assert it.peek() == 20
    assert it.has_more()
    assert list(it) == [20, 10, 3, 4]
    assert not it.has_more()

# The IPython pretty-printer gives very nice output that is difficult to get
# otherwise, e.g., look how much more readable this is than if it were all
# smooshed onto one line:
# 
#    ModelDesc(input_code='y ~ x*asdf',
#              lhs_terms=[Term([EvalFactor('y')])],
#              rhs_terms=[Term([]),
#                         Term([EvalFactor('x')]),
#                         Term([EvalFactor('asdf')]),
#                         Term([EvalFactor('x'), EvalFactor('asdf')])],
#              )
#              
# But, we don't want to assume it always exists; nor do we want to be
# re-writing every repr function twice, once for regular repr and once for
# the pretty printer. So, here's an ugly fallback implementation that can be
# used unconditionally to implement __repr__ in terms of _pretty_repr_.
#
# Pretty printer docs:
#   http://ipython.org/ipython-doc/dev/api/generated/IPython.lib.pretty.html

from cStringIO import StringIO
class _MiniPPrinter(object):
    def __init__(self):
        self._out = StringIO()
        self.indentation = 0

    def text(self, text):
        self._out.write(text)

    def breakable(self, sep=" "):
        self._out.write(sep)

    def begin_group(self, _, text):
        self.text(text)

    def end_group(self, _, text):
        self.text(text)

    def pretty(self, obj):
        if hasattr(obj, "_repr_pretty_"):
            obj._repr_pretty_(self, False)
        else:
            self.text(repr(obj))

    def getvalue(self):
        return self._out.getvalue()

def _mini_pretty(obj):
   printer = _MiniPPrinter()
   printer.pretty(obj)
   return printer.getvalue()

def repr_pretty_delegate(obj):
    # If IPython is already loaded, then might as well use it. (Most commonly
    # this will occur if we are in an IPython session, but somehow someone has
    # called repr() directly. This can happen for example if printing an
    # container like a namedtuple that IPython lacks special code for
    # pretty-printing.)  But, if IPython is not already imported, we do not
    # attempt to import it. This makes patsy itself faster to import (as of
    # Nov. 2012 I measured the extra overhead from loading IPython as ~4
    # seconds on a cold cache), it prevents IPython from automatically
    # spawning a bunch of child processes (!) which may not be what you want
    # if you are not otherwise using IPython, and it avoids annoying the
    # pandas people who have some hack to tell whether you are using IPython
    # in their test suite (see patsy bug #12).
    if optional_dep_ok and "IPython" in sys.modules:
        from IPython.lib.pretty import pretty
        return pretty(obj)
    else:
        return _mini_pretty(obj)

def repr_pretty_impl(p, obj, args, kwargs=[]):
    name = obj.__class__.__name__
    p.begin_group(len(name) + 1, "%s(" % (name,))
    started = [False]
    def new_item():
        if started[0]:
            p.text(",")
            p.breakable()
        started[0] = True
    for arg in args:
        new_item()
        p.pretty(arg)
    for label, value in kwargs:
        new_item()
        p.begin_group(len(label) + 1, "%s=" % (label,))
        p.pretty(value)
        p.end_group(len(label) + 1, "")
    p.end_group(len(name) + 1, ")")

def test_repr_pretty():
    assert repr_pretty_delegate("asdf") == "'asdf'"
    printer = _MiniPPrinter()
    class MyClass(object):
        pass
    repr_pretty_impl(printer, MyClass(),
                     ["a", 1], [("foo", "bar"), ("asdf", "asdf")])
    assert printer.getvalue() == "MyClass('a', 1, foo='bar', asdf='asdf')"

# In Python 3, objects of different types are not generally comparable, so a
# list of heterogenous types cannot be sorted. This implements a Python 2
# style comparison for arbitrary types. (It works on Python 2 too, but just
# gives you the built-in ordering.) To understand why this is tricky, consider
# this example:
#   a = 1    # type 'int'
#   b = 1.5  # type 'float'
#   class gggg:
#       pass
#   c = gggg()
#   sorted([a, b, c])
# The fallback ordering sorts by class name, so according to the fallback
# ordering, we have b < c < a. But, of course, a and b are comparable (even
# though they're of different types), so we also have a < b. This is
# inconsistent. There is no general solution to this problem (which I guess is
# why Python 3 stopped trying), but the worst offender is all the different
# "numeric" classes (int, float, complex, decimal, rational...), so as a
# special-case, we sort all numeric objects to the start of the list.
# (In Python 2, there is also a similar special case for str and unicode, but
# we don't have to worry about that for Python 3.)
class SortAnythingKey(object):
    def __init__(self, obj):
        self.obj = obj

    def _python_lt(self, other_obj):
        # On Py2, < never raises an error, so this is just <. (Actually it
        # does raise a TypeError for comparing complex to numeric, but not for
        # comparisons of complex to other types. Sigh. Whatever.)
        # On Py3, this returns a bool if available, and otherwise returns
        # NotImplemented
        try:
            return self.obj < other_obj
        except TypeError:
            return NotImplemented

    def __lt__(self, other):
        assert isinstance(other, SortAnythingKey)
        result = self._python_lt(other.obj)
        if result is not NotImplemented:
            return result
        # Okay, that didn't work, time to fall back.
        # If one of these is a number, then it is smaller.
        if self._python_lt(0) is not NotImplemented:
            return True
        if other._python_lt(0) is not NotImplemented:
            return False
        # Also check ==, since it may well be defined for otherwise
        # unorderable objects, and if so then we should be consistent with
        # it:
        if self.obj == other.obj:
            return False
        # Otherwise, we break ties based on class name and memory position
        return ((self.obj.__class__.__name__, id(self.obj))
                < (other.obj.__class__.__name__, id(other.obj)))

def test_SortAnythingKey():
    assert sorted([20, 10, 0, 15], key=SortAnythingKey) == [0, 10, 15, 20]
    assert sorted([10, -1.5], key=SortAnythingKey) == [-1.5, 10]
    assert sorted([10, "a", 20.5, "b"], key=SortAnythingKey) == [10, 20.5, "a", "b"]
    class a(object):
        pass
    class b(object):
        pass
    class z(object):
        pass
    a_obj = a()
    b_obj = b()
    z_obj = z()
    o_obj = object()
    assert (sorted([z_obj, a_obj, 1, b_obj, o_obj], key=SortAnythingKey)
            == [1, a_obj, b_obj, o_obj, z_obj])

# NaN checking functions that work on arbitrary objects, on old Python
# versions (math.isnan is only in 2.6+), etc.
def safe_scalar_isnan(x):
    try:
        return np.isnan(float(x))
    except (TypeError, ValueError, NotImplementedError):
        return False
safe_isnan = np.vectorize(safe_scalar_isnan, otypes=[bool])

def test_safe_scalar_isnan():
    assert not safe_scalar_isnan(True)
    assert not safe_scalar_isnan(None)
    assert not safe_scalar_isnan("sadf")
    assert not safe_scalar_isnan((1, 2, 3))
    assert not safe_scalar_isnan(np.asarray([1, 2, 3]))
    assert not safe_scalar_isnan([np.nan])
    assert safe_scalar_isnan(np.nan)
    assert safe_scalar_isnan(np.float32(np.nan))
    assert safe_scalar_isnan(float(np.nan))

def test_safe_isnan():
    assert np.array_equal(safe_isnan([1, True, None, np.nan, "asdf"]),
                          [False, False, False, True, False])
    assert safe_isnan(np.nan).ndim == 0
    assert safe_isnan(np.nan)
    assert not safe_isnan(None)
    # raw isnan raises a *different* error for strings than for objects:
    assert not safe_isnan("asdf")
    
def iterable(obj):
    try:
        iter(obj)
    except Exception:
        return False
    return True

def test_iterable():
    assert iterable("asdf")
    assert iterable([])
    assert iterable({"a": 1})
    assert not iterable(1)
    assert not iterable(iterable)

########NEW FILE########
__FILENAME__ = check-API-refs
#!/usr/bin/env python

# NB: this currently works on both Py2 and Py3, and should be kept that way.

import sys
import re
from os.path import dirname, abspath

root = dirname(dirname(abspath(__file__)))
patsy_ref = root + "/doc/API-reference.rst"

doc_re = re.compile("^\.\. (.*):: ([^\(]*)")
def _documented(rst_path):
    documented = set()
    for line in open(rst_path):
        match = doc_re.match(line.rstrip())
        if match:
            directive = match.group(1)
            symbol = match.group(2)
            if directive not in ["module", "ipython"]:
                documented.add(symbol)
    return documented

try:
    import patsy
except ImportError:
    sys.path.append(root)
    import patsy

documented = set(_documented(patsy_ref))
#print(documented)
exported = set(patsy.__all__)
missed = exported.difference(documented)
extra = documented.difference(exported)
if missed:
    print("DOCS MISSING FROM %s:" % (patsy_ref,))
    for m in sorted(missed):
        print("  %s" % (m,))
if extra:
    print("EXTRA DOCS IN %s:" % (patsy_ref,))
    for m in sorted(extra):
        print("  %s" % (m,))

if missed or extra:
    sys.exit(1)
else:
    print("Reference docs look good.")
    sys.exit(0)

########NEW FILE########
