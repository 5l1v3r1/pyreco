__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# PyMC documentation build configuration file, created by
# sphinx-quickstart on Mon Apr 29 09:34:37 2013.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.todo', 'sphinx.ext.coverage', 'sphinx.ext.pngmath', 'sphinx.ext.mathjax', 'sphinx.ext.ifconfig', 'sphinx.ext.viewcode']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'PyMC'
copyright = u'2013, John Salvatier and Christopher Fonnesbeck'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '3.0'
# The full version, including alpha/beta/rc tags.
release = '3.0a1'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'PyMCdoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'PyMC.tex', u'PyMC Documentation',
   u'John Salvatier and Christopher Fonnesbeck', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'pymc', u'PyMC Documentation',
     [u'John Salvatier and Christopher Fonnesbeck'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'PyMC', u'PyMC Documentation',
   u'John Salvatier and Christopher Fonnesbeck', 'PyMC', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'


# -- Options for Epub output ---------------------------------------------------

# Bibliographic Dublin Core info.
epub_title = u'PyMC'
epub_author = u'John Salvatier and Christopher Fonnesbeck'
epub_publisher = u'John Salvatier and Christopher Fonnesbeck'
epub_copyright = u'2013, John Salvatier and Christopher Fonnesbeck'

# The language of the text. It defaults to the language option
# or en if the language is not set.
#epub_language = ''

# The scheme of the identifier. Typical schemes are ISBN or URL.
#epub_scheme = ''

# The unique identifier of the text. This can be a ISBN number
# or the project homepage.
#epub_identifier = ''

# A unique identification for the text.
#epub_uid = ''

# A tuple containing the cover image and cover page html template filenames.
#epub_cover = ()

# HTML files that should be inserted before the pages created by sphinx.
# The format is a list of tuples containing the path and title.
#epub_pre_files = []

# HTML files shat should be inserted after the pages created by sphinx.
# The format is a list of tuples containing the path and title.
#epub_post_files = []

# A list of files that should not be packed into the epub file.
#epub_exclude_files = []

# The depth of the table of contents in toc.ncx.
#epub_tocdepth = 3

# Allow duplicate toc entries.
#epub_tocdup = True

########NEW FILE########
__FILENAME__ = base
"""Base backend for traces

See the docstring for pymc.backends for more information (includng
creating custom backends).
"""
import numpy as np
from pymc.model import modelcontext


class BaseTrace(object):
    """Base trace object

    Parameters
    ----------
    name : str
        Name of backend
    model : Model
        If None, the model is taken from the `with` context.
    vars : list of variables
        Sampling values will be stored for these variables. If None,
        `model.unobserved_RVs` is used.
    """
    def __init__(self, name, model=None, vars=None):
        self.name = name

        model = modelcontext(model)
        self.model = model
        if vars is None:
            vars = model.unobserved_RVs
        self.vars = vars
        self.varnames = [str(var) for var in vars]
        self.fn = model.fastfn(vars)

        ## Get variable shapes. Most backends will need this
        ## information.
        var_values = zip(self.varnames, self.fn(model.test_point))
        self.var_shapes = {var: value.shape
                           for var, value in var_values}
        self.chain = None

    ## Sampling methods

    def setup(self, draws, chain):
        """Perform chain-specific setup.

        Parameters
        ----------
        draws : int
            Expected number of draws
        chain : int
            Chain number
        """
        pass

    def record(self, point):
        """Record results of a sampling iteration.

        Parameters
        ----------
        point : dict
            Values mapped to variable names
        """
        raise NotImplementedError

    def close(self):
        """Close the database backend.

        This is called after sampling has finished.
        """
        pass

    ## Selection methods

    def __getitem__(self, idx):
        if isinstance(idx, slice):
            return self._slice(idx)

        try:
            return self.point(idx)
        except ValueError:
            pass
        except TypeError:
            pass
        return self.get_values(idx)

    def __len__(self):
        raise NotImplementedError

    def get_values(self, varname, burn=0, thin=1):
        """Get values from trace.

        Parameters
        ----------
        varname : str
        burn : int
        thin : int

        Returns
        -------
        A NumPy array
        """
        raise NotImplementedError

    def _slice(self, idx):
        """Slice trace object."""
        raise NotImplementedError

    def point(self, idx, chain=None):
        """Return dictionary of point values at `idx` for current chain
        with variables names as keys.
        """
        raise NotImplementedError


class MultiTrace(object):
    """Main interface for accessing values from MCMC results

    The core method to select values is `get_values`. Values can also be
    accessed by indexing the MultiTrace object. Indexing can behave in
    three ways:

    1. Indexing with a variable or variable name (str) returns all
       values for that variable.
    2. Indexing with an integer returns a dictionary with values for
       each variable at the given index (corresponding to a single
       sampling iteration).
    3. Slicing with a range returns a new trace with the number of draws
       corresponding to the range.

    For any methods that require a single trace (e.g., taking the length
    of the MultiTrace instance, which returns the number of draws), the
    trace with the highest chain number is always used.

    Parameters
    ----------
    traces : list of traces
        Each object must have a unique `chain` attribute.
    """
    def __init__(self, traces):
        self._traces = {}
        for trace in traces:
            if trace.chain in self._traces:
                raise ValueError("Chains are not unique.")
            self._traces[trace.chain] = trace

    @property
    def nchains(self):
        return len(self._traces)

    @property
    def chains(self):
        return list(sorted(self._traces.keys()))

    def __getitem__(self, idx):
        if isinstance(idx, slice):
            return self._slice(idx)

        try:
            return self.point(idx)
        except ValueError:
            pass
        except TypeError:
            pass
        return self.get_values(idx)

    def __len__(self):
        chain = self.chains[-1]
        return len(self._traces[chain])

    @property
    def varnames(self):
        chain = self.chains[-1]
        return self._traces[chain].varnames

    def get_values(self, varname, burn=0, thin=1, combine=False, chains=None,
                   squeeze=True):
        """Get values from traces.

        Parameters
        ----------
        varname : str
        burn : int
        thin : int
        combine : bool
            If True, results from `chains` will be concatenated.
        chains : int or list of ints
            Chains to retrieve. If None, all chains are used. A single
            values can also accepted.
        squeeze : bool
            Return a single array element if the resulting list of
            values only has one element. If False, the result will
            always be a list of arrays, even if `combine` is True.

        Returns
        -------
        A list of NumPy arrays or a single NumPy array (depending on
        `squeeze`).
        """
        if chains is None:
            chains = self.chains
        varname = str(varname)
        try:
            results = [self._traces[chain].get_values(varname, burn, thin)
                       for chain in chains]
        except TypeError:  # Single chain passed.
            results = [self._traces[chains].get_values(varname, burn, thin)]
        return _squeeze_cat(results, combine, squeeze)

    def _slice(self, idx):
        """Return a new MultiTrace object sliced according to `idx`."""
        new_traces = [trace._slice(idx) for trace in self._traces.values()]
        return MultiTrace(new_traces)

    def point(self, idx, chain=None):
        """Return a dictionary of point values at `idx`.

        Parameters
        ----------
        idx : int
        chain : int
            If a chain is not given, the highest chain number is used.
        """
        if chain is None:
            chain = self.chains[-1]
        return self._traces[chain].point(idx)


def merge_traces(mtraces):
    """Merge MultiTrace objects.

    Parameters
    ----------
    mtraces : list of MultiTraces
        Each instance should have unique chain numbers.

    Raises
    ------
    A ValueError is raised if any traces have overlapping chain numbers.

    Returns
    -------
    A MultiTrace instance with merged chains
    """
    base_mtrace = mtraces[0]
    for new_mtrace in mtraces[1:]:
        for new_chain, trace in new_mtrace._traces.items():
            if new_chain in base_mtrace._traces:
                raise ValueError("Chains are not unique.")
            base_mtrace._traces[new_chain] = trace
    return base_mtrace


def _squeeze_cat(results, combine, squeeze):
    """Squeeze and concatenate the results depending on values of
    `combine` and `squeeze`."""
    if combine:
        results = np.concatenate(results)
        if not squeeze:
            results = [results]
    else:
        if squeeze and len(results) == 1:
            results = results[0]
    return results

########NEW FILE########
__FILENAME__ = ndarray
"""NumPy array trace backend

Store sampling values in memory as a NumPy array.
"""
import numpy as np
from pymc.backends import base


class NDArray(base.BaseTrace):
    """NDArray trace object

    Parameters
    ----------
    name : str
        Name of backend. This has no meaning for the NDArray backend.
    model : Model
        If None, the model is taken from the `with` context.
    vars : list of variables
        Sampling values will be stored for these variables. If None,
        `model.unobserved_RVs` is used.
    """
    def __init__(self, name=None, model=None, vars=None):
        super(NDArray, self).__init__(name, model, vars)
        self.draw_idx = 0
        self.draws = None
        self.samples = {}

    ## Sampling methods

    def setup(self, draws, chain):
        """Perform chain-specific setup.

        Parameters
        ----------
        draws : int
            Expected number of draws
        chain : int
            Chain number
        """
        self.chain = chain
        if self.samples:  # Concatenate new array if chain is already present.
            old_draws = len(self)
            self.draws = old_draws + draws
            self.draws_idx = old_draws
            for varname, shape in self.var_shapes.items():
                old_trace = self.samples[varname]
                new_trace = np.zeros((draws, ) + shape)
                self.samples[varname] = np.concatenate((old_trace, new_trace),
                                                       axis=0)
        else:  # Otherwise, make array of zeros for each variable.
            self.draws = draws
            for varname, shape in self.var_shapes.items():
                self.samples[varname] = np.zeros((draws, ) + shape)

    def record(self, point):
        """Record results of a sampling iteration.

        Parameters
        ----------
        point : dict
            Values mapped to variable names
        """
        for varname, value in zip(self.varnames, self.fn(point)):
            self.samples[varname][self.draw_idx] = value
        self.draw_idx += 1

    def close(self):
        if self.draw_idx == self.draws:
            return
        ## Remove trailing zeros if interrupted before completed all
        ## draws.
        self.samples = {var: trace[:self.draw_idx]
                        for var, trace in self.samples.items()}

    ## Selection methods

    def __len__(self):
        if not self.samples:  # `setup` has not been called.
            return 0
        varname = self.varnames[0]
        return self.samples[varname].shape[0]

    def get_values(self, varname, burn=0, thin=1):
        """Get values from trace.

        Parameters
        ----------
        varname : str
        burn : int
        thin : int

        Returns
        -------
        A NumPy array
        """
        return self.samples[varname][burn::thin]

    def _slice(self, idx):
        sliced = NDArray(model=self.model, vars=self.vars)
        sliced.chain = self.chain
        sliced.samples = {varname: values[idx]
                          for varname, values in self.samples.items()}
        return sliced

    def point(self, idx):
        """Return dictionary of point values at `idx` for current chain
        with variables names as keys.
        """
        idx = int(idx)
        return {varname: values[idx]
                for varname, values in self.samples.items()}

########NEW FILE########
__FILENAME__ = sqlite
"""SQLite trace backend

Store and retrieve sampling values in SQLite database file.

Database format
---------------
For each variable, a table is created with the following format:

 recid (INT), draw (INT), chain (INT),  v1 (FLOAT), v2 (FLOAT), v3 (FLOAT) ...

The variable column names are extended to reflect addition dimensions.
For example, a variable with the shape (2, 2) would be stored as

 key (INT), draw (INT), chain (INT),  v1_1 (FLOAT), v1_2 (FLOAT), v2_1 (FLOAT) ...

The key is autoincremented each time a new row is added to the table.
The chain column denotes the chain index and starts at 0.
"""
import numpy as np
import sqlite3
import warnings

from pymc.backends import base

TEMPLATES = {
    'table':            ('CREATE TABLE IF NOT EXISTS [{table}] '
                         '(recid INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT, '
                         'draw INTEGER, chain INT(5), '
                         '{value_cols})'),
    'insert':           ('INSERT INTO [{table}] '
                         '(recid, draw, chain, {value_cols}) '
                         'VALUES (NULL, ?, ?, {values})'),
    'max_draw':         ('SELECT MAX(draw) FROM [{table}] '
                         'WHERE chain = ?'),
    'draw_count':       ('SELECT COUNT(*) FROM [{table}] '
                         'WHERE chain = ?'),
    ## Named placeholders are used in the selection templates because
    ## some values occur more than once in the same template.
    'select':           ('SELECT * FROM [{table}] '
                         'WHERE (chain = :chain)'),
    'select_burn':      ('SELECT * FROM [{table}] '
                         'WHERE (chain = :chain) AND (draw > :burn)'),
    'select_thin':      ('SELECT * FROM [{table}] '
                         'WHERE (chain = :chain) AND '
                         '(draw - (SELECT draw FROM [{table}] '
                         'WHERE chain = :chain '
                         'ORDER BY draw LIMIT 1)) % :thin = 0'),
    'select_burn_thin': ('SELECT * FROM [{table}] '
                         'WHERE (chain = :chain) AND (draw > :burn) '
                         'AND (draw - (SELECT draw FROM [{table}] '
                         'WHERE (chain = :chain) AND (draw > :burn) '
                         'ORDER BY draw LIMIT 1)) % :thin = 0'),
    'select_point':     ('SELECT * FROM [{table}] '
                         'WHERE (chain = :chain) AND (draw = :draw)'),
}


class SQLite(base.BaseTrace):
    """SQLite trace object

    Parameters
    ----------
    name : str
        Name of database file
    model : Model
        If None, the model is taken from the `with` context.
    vars : list of variables
        Sampling values will be stored for these variables. If None,
        `model.unobserved_RVs` is used.
    """
    def __init__(self, name, model=None, vars=None):
        super(SQLite, self).__init__(name, model, vars)
        self._var_cols = {}
        self.var_inserts = {}  # varname -> insert statement
        self.draw_idx = 0
        self._is_setup = False
        self._len = None

        self.db = _SQLiteDB(name)
        ## Inserting sampling information is queued to avoid locks
        ## caused by hitting the database with transactions each
        ## iteration.
        self._queue = {varname: [] for varname in self.varnames}
        self._queue_limit = 5000

    ## Sampling methods

    def setup(self, draws, chain):
        """Perform chain-specific setup.

        Parameters
        ----------
        draws : int
            Expected number of draws
        chain : int
            Chain number
        """
        self.db.connect()
        self.chain = chain

        if self._is_setup:
            self.draw_idx = self._get_max_draw(chain) + 1
            self._len = None
        else:  # Table has not been created.
            self._var_cols = {varname: _create_colnames(shape)
                              for varname, shape in self.var_shapes.items()}
            self._create_table()
            self._is_setup = True
        self._create_insert_queries(chain)

    def _create_table(self):
        template = TEMPLATES['table']
        with self.db.con:
            for varname, var_cols in self._var_cols.items():
                var_float = ', '.join([v + ' FLOAT' for v in var_cols])
                statement = template.format(table=varname,
                                            value_cols=var_float)
                self.db.cursor.execute(statement)

    def _create_insert_queries(self, chain):
        template = TEMPLATES['insert']
        for varname, var_cols in self._var_cols.items():
            ## Create insert statement for each variable.
            var_str = ', '.join(var_cols)
            placeholders = ', '.join(['?'] * len(var_cols))
            statement = template.format(table=varname,
                                        value_cols=var_str,
                                        values=placeholders)
            self.var_inserts[varname] = statement

    def record(self, point):
        """Record results of a sampling iteration.

        Parameters
        ----------
        point : dict
            Values mapped to variable names
        """
        for varname, value in zip(self.varnames, self.fn(point)):
            values = (self.draw_idx, self.chain) + tuple(np.ravel(value))
            self._queue[varname].append(values)

        if len(self._queue[varname]) > self._queue_limit:
            self._execute_queue()
        self.draw_idx += 1

    def _execute_queue(self):
        with self.db.con:
            for varname in self.varnames:
                if not self._queue[varname]:
                    continue
                self.db.cursor.executemany(self.var_inserts[varname],
                                           self._queue[varname])
                self._queue[varname] = []

    def close(self):
        self._execute_queue()
        self.db.close()

    ## Selection methods

    def __len__(self):
        if not self._is_setup:
            return 0
        if self._len is None:
            self._len = self._get_number_draws()
        return self._len

    def _get_number_draws(self):
        self.db.connect()
        statement = TEMPLATES['draw_count'].format(table=self.varnames[0])
        self.db.cursor.execute(statement, (self.chain,))
        return self.db.cursor.fetchall()[0][0]

    def _get_max_draw(self, chain):
        self.db.connect()
        statement = TEMPLATES['max_draw'].format(table=self.varnames[0])
        self.db.cursor.execute(statement, (chain, ))
        return self.db.cursor.fetchall()[0][0]

    def get_values(self, varname, burn=0, thin=1):
        """Get values from trace.

        Parameters
        ----------
        varname : str
        burn : int
        thin : int

        Returns
        -------
        A NumPy array
        """
        if burn < 0:
            raise ValueError('Negative burn values not supported '
                             'in SQLite backend.')
        if thin < 1:
            raise ValueError('Only positive thin values are supported '
                             'in SQLite backend.')
        varname = str(varname)

        statement_args = {'chain': self.chain}
        if burn == 0 and thin == 1:
            action = 'select'
        elif thin == 1:
            action = 'select_burn'
            statement_args['burn'] = burn - 1
        elif burn == 0:
            action = 'select_thin'
            statement_args['thin'] = thin
        else:
            action = 'select_burn_thin'
            statement_args['burn'] = burn - 1
            statement_args['thin'] = thin

        self.db.connect()
        shape = (-1,) + self.var_shapes[varname]
        statement = TEMPLATES[action].format(table=varname)
        self.db.cursor.execute(statement, statement_args)
        values = _rows_to_ndarray(self.db.cursor)
        return values.reshape(shape)

    def _slice(self, idx):
        warnings.warn('Slice for SQLite backend has no effect.')

    def point(self, idx):
        """Return dictionary of point values at `idx` for current chain
        with variables names as keys.
        """
        idx = int(idx)
        if idx < 0:
            idx = self._get_max_draw(self.chain) - idx - 1
        statement = TEMPLATES['select_point']
        self.db.connect()
        var_values = {}
        statement_args = {'chain': self.chain, 'draw': idx}
        for varname in self.varnames:
            self.db.cursor.execute(statement.format(table=varname),
                                   statement_args)
            values = _rows_to_ndarray(self.db.cursor)
            var_values[varname] = values.reshape(self.var_shapes[varname])
        return var_values


class _SQLiteDB(object):
    def __init__(self, name):
        self.name = name
        self.con = None
        self.cursor = None
        self.connected = False

    def connect(self):
        if self.connected:
            return
        self.con = sqlite3.connect(self.name)
        self.connected = True
        self.cursor = self.con.cursor()

    def close(self):
        if not self.connected:
            return
        self.con.commit()
        self.cursor.close()
        self.con.close()
        self.connected = False


def _create_colnames(shape):
    """Return column names based on `shape`.

    Examples
    --------
    >>> create_colnames((5,))
    ['v1', 'v2', 'v3', 'v4', 'v5']

    >>> create_colnames((2,2))
    ['v1_1', 'v1_2', 'v2_1', 'v2_2']
    """
    if not shape:
        return ['v1']

    size = np.prod(shape)
    indices = (np.indices(shape) + 1).reshape(-1, size)
    return ['v' + '_'.join(map(str, i)) for i in zip(*indices)]


def _create_shape(colnames):
    """Reverse `_create_colnames`.
    """
    if len(colnames) == 1:
        return ()
    last_col = colnames[-1]
    shape = tuple(int(i) for i in last_col[1:].split('_'))
    return shape


def load(name, model=None):
    """Load SQLite database.

    Parameters
    ----------
    name : str
        Path to SQLite database file
    model : Model
        If None, the model is taken from the `with` context.

    Returns
    -------
    A MultiTrace instance
    """
    db = _SQLiteDB(name)
    db.connect()
    varnames = _get_table_list(db.cursor)
    chains = _get_chain_list(db.cursor, varnames[0])

    traces = []
    for chain in chains:
        trace = SQLite(name, model=model)
        trace.varnames = varnames
        trace.chain = chain
        trace._is_setup = True
        trace.db = db  # Share the db with all traces.
        traces.append(trace)
    return base.MultiTrace(traces)


def _get_table_list(cursor):
    """Return a list of table names in the current database."""
    ## Modified from Django. Skips the sqlite_sequence system table used
    ## for autoincrement key generation.
    cursor.execute("SELECT name FROM sqlite_master "
                   "WHERE type='table' AND NOT name='sqlite_sequence' "
                   "ORDER BY name")
    return [row[0] for row in cursor.fetchall()]


def _get_var_strs(cursor, varname):
    cursor.execute('SELECT * FROM [{}]'.format(varname))
    col_names = (col_descr[0] for col_descr in cursor.description)
    return [name for name in col_names if name.startswith('v')]


def _get_chain_list(cursor, varname):
    """Return a list of sorted chains for `varname`."""
    cursor.execute('SELECT DISTINCT chain FROM [{}]'.format(varname))
    chains = [chain[0] for chain in cursor.fetchall()]
    chains.sort()
    return chains


def _rows_to_ndarray(cursor):
    """Convert SQL row to NDArray."""
    return np.squeeze(np.array([row[3:] for row in cursor.fetchall()]))

########NEW FILE########
__FILENAME__ = text
"""Text file trace backend

After sampling with NDArray backend, save results as text files.

As this other backends, this can be used by passing the backend instance
to `sample`.

    >>> import pymc as pm
    >>> db = pm.backends.Text('test')
    >>> trace = pm.sample(..., trace=db)

Or sampling can be performed with the default NDArray backend and then
dumped to text files after.

    >>> from pymc.backends import text
    >>> trace = pm.sample(...)
    >>> text.dump('test', trace)

Database format
---------------

For each chain, a directory named `chain-N` is created. In this
directory, one file per variable is created containing the values of the
object. To deal with multidimensional variables, the array is reshaped
to one dimension before saving with `numpy.savetxt`. The shape
information is saved in a json file in the same directory and is used to
load the database back again using `numpy.loadtxt`.
"""
import os
import glob
import json
import numpy as np

from pymc.backends import base
from pymc.backends.ndarray import NDArray


class Text(NDArray):
    """Text storage

    Parameters
    ----------
    name : str
        Name of directory to store text files
    model : Model
        If None, the model is taken from the `with` context.
    vars : list of variables
        Sampling values will be stored for these variables. If None,
        `model.unobserved_RVs` is used.
    """
    def __init__(self, name, model=None, vars=None):
        if not os.path.exists(name):
            os.mkdir(name)
        super(Text, self).__init__(name, model, vars)

    def close(self):
        super(Text, self).close()
        _dump_trace(self.name, self)


def dump(name, trace, chains=None):
    """Store NDArray trace as text database.

    Parameters
    ----------
    name : str
        Name of directory to store text files
    trace : MultiTrace of NDArray traces
        Result of MCMC run with default NDArray backend
    chains : list
        Chains to dump. If None, all chains are dumped.
    """
    if not os.path.exists(name):
        os.mkdir(name)
    if chains is None:
        chains = trace.chains
    for chain in chains:
        _dump_trace(name, trace._traces[chain])


def _dump_trace(name, trace):
    """Dump a single-chain trace.
    """
    chain_name = 'chain-{}'.format(trace.chain)
    chain_dir = os.path.join(name, chain_name)
    os.mkdir(chain_dir)

    shapes = {}
    for varname in trace.varnames:
        data = trace.get_values(varname)
        var_file = os.path.join(chain_dir, varname + '.txt')
        np.savetxt(var_file, data.reshape(-1, data.size))
        shapes[varname] = data.shape
    ## Store shape information for reloading.
    shape_file = os.path.join(chain_dir, 'shapes.json')
    with open(shape_file, 'w') as sfh:
        json.dump(shapes, sfh)


def load(name, chains=None, model=None):
    """Load text database.

    Parameters
    ----------
    name : str
        Path to root directory for text database
    chains : list
        Chains to load. If None, all chains are loaded.
    model : Model
        If None, the model is taken from the `with` context.

    Returns
    -------
    ndarray.Trace instance
    """
    chain_dirs = _get_chain_dirs(name)
    if chains is None:
        chains = list(chain_dirs.keys())

    traces = []
    for chain in chains:
        chain_dir = chain_dirs[chain]
        shape_file = os.path.join(chain_dir, 'shapes.json')
        with open(shape_file, 'r') as sfh:
            shapes = json.load(sfh)
        samples = {}
        for varname, shape in shapes.items():
            var_file = os.path.join(chain_dir, varname + '.txt')
            samples[varname] = np.loadtxt(var_file).reshape(shape)
        trace = NDArray(model=model)
        trace.samples = samples
        trace.chain = chain
        traces.append(trace)
    return base.MultiTrace(traces)


def _get_chain_dirs(name):
    """Return mapping of chain number to directory."""
    return {_chain_dir_to_chain(chain_dir): chain_dir
            for chain_dir in glob.glob(os.path.join(name, 'chain-*'))}


def _chain_dir_to_chain(chain_dir):
    return int(os.path.basename(chain_dir).split('-')[1])

########NEW FILE########
__FILENAME__ = blocking
"""
pymc.blocking

Classes for working with subsets of parameters.
"""
import numpy as np
import collections

__all__ = ['ArrayOrdering', 'DictToArrayBijection', 'DictToVarBijection']

VarMap = collections.namedtuple('VarMap', 'var, slc, shp')

# TODO Classes and methods need to be fully documented.


class ArrayOrdering(object):
    """
    An ordering for an array space
    """
    def __init__(self, vars):
        self.vmap = []
        dim = 0

        for var in vars:
            slc = slice(dim, dim + var.dsize)
            self.vmap.append(VarMap(str(var), slc, var.dshape))
            dim += var.dsize

        self.dimensions = dim


class DictToArrayBijection(object):
    """
    A mapping between a dict space and an array space
    """
    def __init__(self, ordering, dpoint):
        self.ordering = ordering
        self.dpt = dpoint

    def map(self, dpt):
        """
        Maps value from dict space to array space

        Parameters
        ----------
        dpt : dict
        """
        apt = np.empty(self.ordering.dimensions)
        for var, slc, _ in self.ordering.vmap:
                apt[slc] = np.ravel(dpt[var])
        return apt

    def rmap(self, apt):
        """
        Maps value from array space to dict space

        Parameters
        ----------
        apt : array
        """
        dpt = self.dpt.copy()

        for var, slc, shp in self.ordering.vmap:
            dpt[var] = np.reshape(apt[slc], shp)

        return dpt

    def mapf(self, f):
        """
         function f : DictSpace -> T to ArraySpace -> T

        Parameters
        ----------

        f : dict -> T

        Returns
        -------
        f : array -> T
        """
        return Compose(f, self.rmap)


class DictToVarBijection(object):
    """
    A mapping between a dict space and the array space for one element within the dict space
    """
    def __init__(self, var, idx, dpoint):
        self.var = str(var)
        self.idx = idx
        self.dpt = dpoint

    def map(self, dpt):
        return dpt[self.var][self.idx]

    def rmap(self, apt):
        dpt = self.dpt.copy()

        dvar = dpt[self.var].copy()
        dvar[self.idx] = apt

        dpt[self.var] = dvar

        return dpt

    def mapf(self, f):
        return Compose(f, self.rmap)


class Compose(object):
    """
    Compose two functions in a pickleable way
    """
    def __init__(self, fa, fb):
        self.fa = fa
        self.fb = fb

    def __call__(self, x):
        return self.fa(self.fb(x))

########NEW FILE########
__FILENAME__ = core
from .vartypes import *
from .point import *
from .model import *
from .theanof import *
from .blocking import *
import numpy as np

########NEW FILE########
__FILENAME__ = data
import pkgutil
import io

__all__ = ['get_data_file']

def get_data_file(pkg, path):
    """Returns a file object for a package data file.
    
    Parameters
    ----------
    pkg : str
        dotted package hierarchy. e.g. "pymc.examples"
    path : str 
        file path within package. e.g. "data/wells.dat"
    Returns 
    -------
    BytesIO of the data
    """

    return io.BytesIO(pkgutil.get_data(pkg, path))


########NEW FILE########
__FILENAME__ = debug
import numpy as np
from .blocking import *

# TODO I could not locate this function used anywhere in the code base
# do we need it?


def eval_univariate(f, var, idx, point, x):
    """
    Evaluate a function as a at a specific point and only varying values at one index.

    Useful for debugging misspecified likelihoods.

    Parameters
    ----------

    f : function : dict -> val
    var : variable
    idx : index into variable
    point : point at which to center
    x : array points at which to evaluate x

    """
    bij = DictToVarBijection(var, idx, point)
    return list(map(bij.mapf(f), x))

########NEW FILE########
__FILENAME__ = diagnostics
"""Convergence diagnostics and model validation"""

import numpy as np
from .stats import autocorr, autocov, statfunc
from copy import copy

__all__ = ['geweke', 'gelman_rubin', 'trace_to_dataframe']


@statfunc
def geweke(x, first=.1, last=.5, intervals=20):
    """Return z-scores for convergence diagnostics.

    Compare the mean of the first % of series with the mean of the last % of
    series. x is divided into a number of segments for which this difference is
    computed. If the series is converged, this score should oscillate between
    -1 and 1.

    Parameters
    ----------
    x : array-like
      The trace of some stochastic parameter.
    first : float
      The fraction of series at the beginning of the trace.
    last : float
      The fraction of series at the end to be compared with the section
      at the beginning.
    intervals : int
      The number of segments.

    Returns
    -------
    scores : list [[]]
      Return a list of [i, score], where i is the starting index for each
      interval and score the Geweke score on the interval.

    Notes
    -----

    The Geweke score on some series x is computed by:

      .. math:: \frac{E[x_s] - E[x_e]}{\sqrt{V[x_s] + V[x_e]}}

    where :math:`E` stands for the mean, :math:`V` the variance,
    :math:`x_s` a section at the start of the series and
    :math:`x_e` a section at the end of the series.

    References
    ----------
    Geweke (1992)
    """

    if np.rank(x) > 1:
        return [geweke(y, first, last, intervals) for y in np.transpose(x)]

    # Filter out invalid intervals
    if first + last >= 1:
        raise ValueError(
            "Invalid intervals for Geweke convergence analysis",
            (first,
             last))

    # Initialize list of z-scores
    zscores = []

    # Last index value
    end = len(x) - 1

    # Calculate starting indices
    sindices = np.arange(0, end // 2, step=int((end / 2) / (intervals - 1)))

    # Loop over start indices
    for start in sindices:

        # Calculate slices
        first_slice = x[start: start + int(first * (end - start))]
        last_slice = x[int(end - last * (end - start)):]

        z = (first_slice.mean() - last_slice.mean())
        z /= np.sqrt(first_slice.std() ** 2 + last_slice.std() ** 2)

        zscores.append([start, z])

    if intervals is None:
        return np.array(zscores[0])
    else:
        return np.array(zscores)


def gelman_rubin(mtrace):
    """ Returns estimate of R for a set of traces.

    The Gelman-Rubin diagnostic tests for lack of convergence by comparing
    the variance between multiple chains to the variance within each chain.
    If convergence has been achieved, the between-chain and within-chain
    variances should be identical. To be most effective in detecting evidence
    for nonconvergence, each chain should have been initialized to starting
    values that are dispersed relative to the target distribution.

    Parameters
    ----------
    mtrace : MultiTrace
      A MultiTrace object containing parallel traces (minimum 2)
      of one or more stochastic parameters.

    Returns
    -------
    Rhat : dict
      Returns dictionary of the potential scale reduction factors, :math:`\hat{R}`

    Notes
    -----

    The diagnostic is computed by:

      .. math:: \hat{R} = \frac{\hat{V}}{W}

    where :math:`W` is the within-chain variance and :math:`\hat{V}` is
    the posterior variance estimate for the pooled traces. This is the
    potential scale reduction factor, which converges to unity when each
    of the traces is a sample from the target posterior. Values greater
    than one indicate that one or more chains have not yet converged.

    References
    ----------
    Brooks and Gelman (1998)
    Gelman and Rubin (1992)"""

    if mtrace.nchains < 2:
        raise ValueError(
            'Gelman-Rubin diagnostic requires multiple chains of the same length.')

    def calc_rhat(x):

        try:
            # When the variable is multidimensional, this assignment will fail, triggering
            # a ValueError that will handle the multidimensional case
            m, n = x.shape

            # Calculate between-chain variance
            B = n * np.var(np.mean(x, axis=1), ddof=1)

            # Calculate within-chain variance
            W = np.mean(np.var(x, axis=1, ddof=1))

            # Estimate of marginal posterior variance
            Vhat = W*(n - 1)/n + B/n

            return np.sqrt(Vhat/W)

        except ValueError:

            # Tricky transpose here, shifting the last dimension to the first
            rotated_indices = np.roll(np.arange(x.ndim), 1)
            # Now iterate over the dimension of the variable
            return np.squeeze([calc_rhat(xi) for xi in x.transpose(rotated_indices)])

    Rhat = {}
    for var in mtrace.varnames:

        # Get all traces for var
        x = np.array(mtrace.get_values(var))

        try:
            Rhat[var] = calc_rhat(x)
        except ValueError:
            Rhat[var] = [calc_rhat(y.transpose()) for y in x.transpose()]

    return Rhat


def trace_to_dataframe(trace):
    """Convert a PyMC trace consisting of 1-D variables to a pandas DataFrame
    """
    import pandas as pd
    return pd.DataFrame(
        {varname: np.squeeze(trace.get_values(varname, combine=True))
         for varname in trace.varnames})

########NEW FILE########
__FILENAME__ = continuous
"""
pymc.distributions

A collection of common probability distributions for stochastic
nodes in PyMC.

"""
from __future__ import division

from .dist_math import *
from numpy.random import uniform as runiform, normal as rnormal

__all__ = ['Uniform', 'Flat', 'Normal', 'Beta', 'Exponential', 'Laplace',
           'T', 'Cauchy', 'HalfCauchy', 'Gamma', 'Weibull','Bound',
           'Tpos', 'Lognormal', 'ChiSquared', 'HalfNormal', 'Wald',
           'Pareto', 'InverseGamma']

def get_tau(tau=None, sd=None):
    if tau is None:
        if sd is None:
            return 1.
        else:
            return sd ** -2

    else:
        if sd is not None:
            raise ValueError("Can't pass both tau and sd")
        else:
            return tau

class Uniform(Continuous):
    """
    Continuous uniform log-likelihood.

    .. math::
        f(x \mid lower, upper) = \frac{1}{upper-lower}

    Parameters
    ----------
    lower : float
        Lower limit (defaults to 0)
    upper : float
        Upper limit (defaults to 1)
    """
    def __init__(self, lower=0, upper=1, *args, **kwargs):
        super(Uniform, self).__init__(*args, **kwargs)
        self.lower = lower
        self.upper = upper
        self.mean = (upper + lower) / 2.
        self.median = self.mean

    def logp(self, value):
        lower = self.lower
        upper = self.upper

        return bound(
            -log(upper - lower),
            lower <= value, value <= upper)

    def random(self, size=None):
        return runiform(self.upper, self.lower, size)


class Flat(Continuous):
    """
    Uninformative log-likelihood that returns 0 regardless of
    the passed value.
    """
    def __init__(self, *args, **kwargs):
        super(Flat, self).__init__(*args, **kwargs)
        self.median = 0

    def logp(self, value):
        return zeros_like(value)


class Normal(Continuous):
    """
    Normal log-likelihood.

    .. math::
        f(x \mid \mu, \tau) = \sqrt{\frac{\tau}{2\pi}} \exp\left\{ -\frac{\tau}{2} (x-\mu)^2 \right\}

    Parameters
    ----------
    mu : float
        Mean of the distribution.
    tau : float
        Precision of the distribution, which corresponds to
        :math:`1/\sigma^2` (tau > 0).
    sd : float
        Standard deviation of the distribution. Alternative parameterization.

    .. note::
    - :math:`E(X) = \mu`
    - :math:`Var(X) = 1/\tau`

    """
    def __init__(self, mu=0.0, tau=None, sd=None, *args, **kwargs):
        super(Normal, self).__init__(*args, **kwargs)
        self.mean = self.median = self.mode = self.mu = mu
        self.tau = get_tau(tau=tau, sd=sd)
        self.variance = 1. / self.tau

    def logp(self, value):
        tau = self.tau
        mu = self.mu

        return bound(
            (-tau * (value - mu) ** 2 + log(tau / pi / 2.)) / 2.,
            tau > 0)


class HalfNormal(Continuous):
    """
    Half-normal log-likelihood, a normal distribution with mean 0 limited
    to the domain :math:`x \in [0, \infty)`.

    .. math::
        f(x \mid \tau) = \sqrt{\frac{2\tau}{\pi}}\exp\left\{ {\frac{-x^2 \tau}{2}}\right\}

    :Parameters:
      - `x` : :math:`x \ge 0`
      - `tau` : tau > 0

    """
    def __init__(self, tau=None, sd=None, *args, **kwargs):
        super(HalfNormal, self).__init__(*args, **kwargs)
        self.tau = get_tau(tau=tau, sd=sd)
        self.mean = sqrt(2 / (pi * self.tau))
        self.variance = (1. - 2/pi) / self.tau

    def logp(self, value):
        tau = self.tau

        return bound(
            -0.5 * tau * value**2 + 0.5 * log(tau * 2. / pi),
            tau > 0)


class Wald(Continuous):
    """
    Wald (inverse Gaussian) log likelihood.

    .. math::
        f(x \mid \mu) = \sqrt{\frac{1}{2\pi x^3}} \exp\left\{ \frac{-(x-\mu)^2}{2 \mu^2 x} \right\}

    Parameters
    ----------
    mu : float
        Mean of the distribution.

    .. note::
    - :math:`E(X) = \mu`
    - :math:`Var(X) = \mu^3`
    """
    def __init__(self, mu, *args, **kwargs):
        super(Wald, self).__init__(*args, **kwargs)
        self.mu = mu
        self.mean = mu
        self.variance = mu**3

    def logp(self, value):
        mu = self.mu

        return bound(
            ((- (value - mu) ** 2) /
                (2. * value * (mu ** 2))) + logpow(2. * pi * value **3, -0.5),
            mu > 0)



class Beta(Continuous):
    """
    Beta log-likelihood. The conjugate prior for the parameter
    :math:`p` of the binomial distribution.

    .. math::
        f(x \mid \alpha, \beta) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} x^{\alpha - 1} (1 - x)^{\beta - 1}

    Parameters
    ----------
    alpha : float
        alpha > 0
    beta : float
        beta > 0

    .. note::
    - :math:`E(X)=\frac{\alpha}{\alpha+\beta}`
    - :math:`Var(X)=\frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)}`

    """
    def __init__(self, alpha, beta, *args, **kwargs):
        super(Beta, self).__init__(*args, **kwargs)
        self.alpha = alpha
        self.beta = beta
        self.mean = alpha / (alpha + beta)
        self.variance = alpha * beta / (
            (alpha + beta) ** 2 * (alpha + beta + 1))

    def logp(self, value):
        alpha = self.alpha
        beta = self.beta

        return bound(
            gammaln(alpha + beta) - gammaln(alpha) - gammaln(beta) +
            logpow(
                value, alpha - 1) + logpow(1 - value, beta - 1),
            0 <= value, value <= 1,
            alpha > 0,
            beta > 0)


class Exponential(Continuous):
    """
    Exponential distribution

    Parameters
    ----------
    lam : float
        lam > 0
        rate or inverse scale
    """
    def __init__(self, lam, *args, **kwargs):
        super(Exponential, self).__init__(*args, **kwargs)
        self.lam = lam
        self.mean = 1. / lam
        self.median = self.mean * log(2)
        self.mode = 0

        self.variance = lam ** -2

    def logp(self, value):
        lam = self.lam
        return bound(log(lam) - lam * value,
                     value > 0,
                     lam > 0)


class Laplace(Continuous):
    """
    Laplace distribution

    Parameters
    ----------
    mu : float
        mean
    b : float
        scale
    """

    def __init__(self, mu, b, *args, **kwargs):
        super(Laplace, self).__init__(*args, **kwargs)
        self.b = b
        self.mean = self.median = self.mode = self.mu = mu

        self.variance = 2 * b ** 2

    def logp(self, value):
        mu = self.mu
        b = self.b

        return -log(2 * b) - abs(value - mu) / b


class Lognormal(Continuous):
    """
    Log-normal log-likelihood.

    Distribution of any random variable whose logarithm is normally
    distributed. A variable might be modeled as log-normal if it can
    be thought of as the multiplicative product of many small
    independent factors.

    .. math::
        f(x \mid \mu, \tau) = \sqrt{\frac{\tau}{2\pi}}\frac{
        \exp\left\{ -\frac{\tau}{2} (\ln(x)-\mu)^2 \right\}}{x}

    :Parameters:
      - `x` : x > 0
      - `mu` : Location parameter.
      - `tau` : Scale parameter (tau > 0).

    .. note::

       :math:`E(X)=e^{\mu+\frac{1}{2\tau}}`
       :math:`Var(X)=(e^{1/\tau}-1)e^{2\mu+\frac{1}{\tau}}`

    """
    def __init__(self, mu=0, tau=1, *args, **kwargs):
        super(Lognormal, self).__init__(*args, **kwargs)
        self.mu = mu
        self.tau = tau
        self.mean = exp(mu + 1./(2*tau))
        self.median = exp(mu)
        self.mode = exp(mu - 1./tau)

        self.variance = (exp(1./tau) - 1) * exp(2*mu + 1./tau)

    def logp(self, value):
        mu = self.mu
        tau = self.tau

        return bound(
            -0.5*tau*(log(value) - mu)**2 + 0.5*log(tau/(2.*pi)) - log(value),
            tau > 0)


class T(Continuous):
    """
    Non-central Student's T log-likelihood.

    Describes a normal variable whose precision is gamma distributed. If
    only nu parameter is passed, this specifies a standard (central)
    Student's T.

    .. math::
        f(x|\mu,\lambda,\nu) = \frac{\Gamma(\frac{\nu +
        1}{2})}{\Gamma(\frac{\nu}{2})}
        \left(\frac{\lambda}{\pi\nu}\right)^{\frac{1}{2}}
        \left[1+\frac{\lambda(x-\mu)^2}{\nu}\right]^{-\frac{\nu+1}{2}}

    Parameters
    ----------
    nu : int
        Degrees of freedom
    mu : float
        Location parameter (defaults to 0)
    lam : float
        Scale parameter (defaults to 1)
    """
    def __init__(self, nu, mu=0, lam=1, *args, **kwargs):
        super(T, self).__init__(*args, **kwargs)
        self.nu = nu
        self.lam = lam
        self.mean = self.median = self.mode = self.mu = mu

        self.variance = switch((nu > 2) * 1, nu / (nu - 2) / lam, inf)

    def logp(self, value):
        nu = self.nu
        mu = self.mu
        lam = self.lam

        return bound(
            gammaln((nu + 1.0) / 2.0) + .5 * log(lam / (nu * pi)) - gammaln(nu / 2.0) - (nu + 1.0) / 2.0 * log(1.0 + lam * (value - mu) ** 2 / nu),
            lam > 0,
            nu > 0)


class Pareto(Continuous):
    """
    Pareto log-likelihood. The Pareto is a continuous, positive
    probability distribution with two parameters. It is often used
    to characterize wealth distribution, or other examples of the
    80/20 rule.

    .. math::
        f(x \mid \alpha, m) = \frac{\alpha m^{\alpha}}{x^{\alpha+1}}

    Parameters
    ----------
    alpha : float
        Shape parameter (alpha>0)
    m : float
        Scale parameter (m>0)

    .. note::
       - :math:`E(x)=\frac{\alpha m}{\alpha-1} if \alpha > 1`
       - :math:`Var(x)=\frac{m^2 \alpha}{(\alpha-1)^2(\alpha-2)} if \alpha > 2`

    """
    def __init__(self, alpha, m, *args, **kwargs):
        super(Pareto, self).__init__(*args, **kwargs)
        self.alpha = alpha
        self.m = m
        self.mean = switch(gt(alpha,1), alpha * m / (alpha - 1.), inf)
        self.variance = switch(gt(alpha,2), (alpha * m**2) / ((alpha - 2.) * (alpha - 1.)**2), inf)

    def logp(self, value):
        alpha = self.alpha
        m = self.m
        return bound(
            log(alpha) + logpow(m, alpha) - logpow(value, alpha+1),

            alpha > 0,
            m > 0,
            value >= m)


class Cauchy(Continuous):
    """
    Cauchy log-likelihood. The Cauchy distribution is also known as the
    Lorentz or the Breit-Wigner distribution.

    .. math::
        f(x \mid \alpha, \beta) = \frac{1}{\pi \beta [1 + (\frac{x-\alpha}{\beta})^2]}

    Parameters
    ----------
    alpha : float
        Location parameter
    beta : float
        Scale parameter > 0

    .. note::
    Mode and median are at alpha.

    """

    def __init__(self, alpha, beta, *args, **kwargs):
        super(Cauchy, self).__init__(*args, **kwargs)
        self.median = self.mode = self.alpha = alpha
        self.beta = beta

    def logp(self, value):
        alpha = self.alpha
        beta = self.beta
        return bound(
            -log(pi) - log(beta) - log(1 + ((
                                            value - alpha) / beta) ** 2),
            beta > 0)

class HalfCauchy(Continuous):
    """
    Half-Cauchy log-likelihood. Simply the absolute value of Cauchy.

    .. math::
        f(x \mid \beta) = \frac{2}{\pi \beta [1 + (\frac{x}{\beta})^2]}

    :Parameters:
      - `beta` : Scale parameter (beta > 0).

    .. note::
      - x must be non-negative.
    """

    def __init__(self, beta, *args, **kwargs):
        super(HalfCauchy, self).__init__(*args, **kwargs)
        self.mode = 0
        self.beta = beta

    def logp(self, value):
        beta = self.beta
        return bound(
            log(2) - log(pi) - log(beta) - log(1 + (value / beta) ** 2),
            beta > 0,
            value >= 0)


class Gamma(Continuous):
    """
    Gamma log-likelihood.

    Represents the sum of alpha exponentially distributed random variables, each
    of which has mean beta.

    .. math::
        f(x \mid \alpha, \beta) = \frac{\beta^{\alpha}x^{\alpha-1}e^{-\beta x}}{\Gamma(\alpha)}

    Parameters
    ----------
    x : float
        math:`x \ge 0`
    alpha : float
        Shape parameter (alpha > 0).
    beta : float
        Rate parameter (beta > 0).

    .. note::
    - :math:`E(X) = \frac{\alpha}{\beta}`
    - :math:`Var(X) = \frac{\alpha}{\beta^2}`

    """
    def __init__(self, alpha, beta, *args, **kwargs):
        super(Gamma, self).__init__(*args, **kwargs)
        self.alpha = alpha
        self.beta = beta
        self.mean = alpha / beta
        self.median = maximum((alpha - 1) / beta, 0)
        self.variance = alpha / beta ** 2

    def logp(self, value):
        alpha = self.alpha
        beta = self.beta
        return bound(
            -gammaln(alpha) + logpow(
                beta, alpha) - beta * value + logpow(value, alpha - 1),

            value >= 0,
            alpha > 0,
            beta > 0)


class InverseGamma(Continuous):
    """
    Inverse gamma log-likelihood, the reciprocal of the gamma distribution.

    .. math::
        f(x \mid \alpha, \beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)} x^{-\alpha - 1} \exp\left(\frac{-\beta}{x}\right)

    Parameters
    ----------
      alpha : float
          Shape parameter (alpha > 0).
      beta : float
          Scale parameter (beta > 0).

    .. note::

       :math:`E(X)=\frac{\beta}{\alpha-1}`  for :math:`\alpha > 1`
       :math:`Var(X)=\frac{\beta^2}{(\alpha-1)^2(\alpha)}`  for :math:`\alpha > 2`

    """
    def __init__(self, alpha, beta=1, *args, **kwargs):
        super(InverseGamma, self).__init__(*args, **kwargs)
        self.alpha = alpha
        self.beta = beta
        self.mean = (alpha > 1) * beta / (alpha - 1.) or inf
        self.mode = beta / (alpha + 1.)
        self.variance = switch(gt(alpha, 2), (beta ** 2) / (alpha * (alpha - 1.)**2), inf)

    def logp(self, value):
        alpha = self.alpha
        beta = self.beta
        return bound(
            logpow(beta, alpha) - gammaln(alpha) - beta / value + logpow(value, -alpha-1),

            value > 0,
            alpha > 0,
            beta > 0)


class ChiSquared(Gamma):
    """
    Chi-squared :math:`\chi^2` log-likelihood.

    .. math::
        f(x \mid \nu) = \frac{x^{(\nu-2)/2}e^{-x/2}}{2^{\nu/2}\Gamma(\nu/2)}

    :Parameters:
      - `x` : > 0
      - `nu` : [int] Degrees of freedom ( nu > 0 )

    .. note::
      - :math:`E(X)=\nu`
      - :math:`Var(X)=2\nu`
    """
    def __init__(self, nu, *args, **kwargs):
        self.nu = nu
        super(ChiSquared, self).__init__(alpha=nu/2., beta=0.5, *args, **kwargs)


class Weibull(Continuous):
    """
    Weibull log-likelihood

    .. math::
        f(x \mid \alpha, \beta) = \frac{\alpha x^{\alpha - 1}
        \exp(-(\frac{x}{\beta})^{\alpha})}{\beta^\alpha}

    :Parameters:
      - `x` : :math:`x \ge 0`
      - `alpha` : alpha > 0
      - `beta` : beta > 0

    .. note::
      - :math:`E(x)=\beta \Gamma(1+\frac{1}{\alpha})`
      - :math:`median(x)=\Gamma(\log(2))^{1/\alpha}`
      - :math:`Var(x)=\beta^2 \Gamma(1+\frac{2}{\alpha} - \mu^2)`

    """
    def __init__(self, alpha, beta, *args, **kwargs):
        super(Weibull, self).__init__(*args, **kwargs)
        self.alpha = alpha
        self.beta = beta
        self.mean = beta * exp(gammaln(1 + 1./alpha))
        self.median = beta * exp(gammaln(log(2)))**(1./alpha)
        self.variance = (beta**2) * exp(gammaln(1 + 2./alpha - self.mean**2))

    def logp(self, value):
        alpha = self.alpha
        beta = self.beta
        return bound(
            (log(alpha) - log(beta) + (alpha - 1)*log(value/beta)
            - (value/beta)**alpha),
            value >= 0,
            alpha > 0,
            beta > 0)


class Bounded(Continuous):
    """A bounded distribution."""
    def __init__(self, distribution, lower, upper, *args, **kwargs):
        self.dist = distribution.dist(*args, **kwargs)

        self.__dict__.update(self.dist.__dict__)
        self.__dict__.update(locals())

        if hasattr(self.dist, 'mode'):
            self.mode = self.dist.mode

    def logp(self, value):
        return bound(
            self.dist.logp(value),

            self.lower <= value, value <= self.upper)



class Bound(object):
    """Creates a new bounded distribution"""
    def __init__(self, distribution, lower=-inf, upper=inf):
        self.distribution = distribution
        self.lower = lower
        self.upper = upper

    def __call__(self, *args, **kwargs):
        first, args = args[0], args[1:]

        return Bounded(first, self.distribution, self.lower, self.upper, *args, **kwargs)

    def dist(*args, **kwargs):
        return Bounded.dist(self.distribution, self.lower, self.upper, *args, **kwargs)


Tpos = Bound(T, 0)

########NEW FILE########
__FILENAME__ = discrete
from .dist_math import *

__all__ = ['Binomial',  'BetaBin',  'Bernoulli',  'Poisson', 'NegativeBinomial',
'ConstantDist', 'ZeroInflatedPoisson', 'DiscreteUniform', 'Geometric',
'Categorical']


class Binomial(Discrete):
    """
    Binomial log-likelihood.  The discrete probability distribution
    of the number of successes in a sequence of n independent yes/no
    experiments, each of which yields success with probability p.

    .. math::
        f(x \mid n, p) = \frac{n!}{x!(n-x)!} p^x (1-p)^{n-x}

    Parameters
    ----------
    n : int
        Number of Bernoulli trials, n > x
    p : float
        Probability of success in each trial, :math:`p \in [0,1]`

    .. note::
    - :math:`E(X)=np`
    - :math:`Var(X)=np(1-p)`

    """
    def __init__(self, n, p, *args, **kwargs):
        super(Binomial, self).__init__(*args, **kwargs)
        self.n = n
        self.p = p
        self.mode = cast(round(n * p), 'int8')

    def logp(self, value):
        n = self.n
        p = self.p

        return bound(

            logpow(p, value) + logpow(
                1 - p, n - value) + factln(
                    n) - factln(value) - factln(n - value),

            0 <= value, value <= n,
            0 <= p, p <= 1)


class BetaBin(Discrete):
    """
    Beta-binomial log-likelihood. Equivalent to binomial random
    variables with probabilities drawn from a
    :math:`\texttt{Beta}(\alpha,\beta)` distribution.

    .. math::
        f(x \mid \alpha, \beta, n) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)} \frac{\Gamma(n+1)}{\Gamma(x+1)\Gamma(n-x+1)} \frac{\Gamma(\alpha + x)\Gamma(n+\beta-x)}{\Gamma(\alpha+\beta+n)}

    Parameters
    ----------
    alpha : float
        alpha > 0
    beta : float
        beta > 0
    n : int
        n=x,x+1,...

    .. note::
    - :math:`E(X)=n\frac{\alpha}{\alpha+\beta}`
    - :math:`Var(X)=n\frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)}`

    """
    def __init__(self, alpha, beta, n, *args, **kwargs):
        super(BetaBin, self).__init__(*args, **kwargs)
        self.alpha = alpha
        self.beta = beta
        self.n = n
        self.mode = cast(round(alpha / (alpha + beta)), 'int8')

    def logp(self, value):
        alpha = self.alpha
        beta = self.beta
        n = self.n

        return bound(gammaln(alpha + beta) - gammaln(alpha) - gammaln(beta) +
                     gammaln(n + 1) - gammaln(value + 1) - gammaln(n - value + 1) +
                     gammaln(alpha + value) + gammaln(n + beta - value) - gammaln(beta + alpha + n),

                     0 <= value, value <= n,
                     alpha > 0,
                     beta > 0)


class Bernoulli(Discrete):
    """Bernoulli log-likelihood

    The Bernoulli distribution describes the probability of successes (x=1) and
    failures (x=0).

    .. math::  f(x \mid p) = p^{x} (1-p)^{1-x}

    Parameters
    ----------
    p : float
        Probability of success. :math:`0 < p < 1`.

    .. note::
    - :math:`E(x)= p`
    - :math:`Var(x)= p(1-p)`

    """
    def __init__(self, p, *args, **kwargs):
        super(Bernoulli, self).__init__(*args, **kwargs)
        self.p = p
        self.mode = cast(round(p), 'int8')

    def logp(self, value):
        p = self.p
        return bound(
            switch(value, log(p), log(1 - p)),
            0 <= p, p <= 1)


class Poisson(Discrete):
    """
    Poisson log-likelihood.

    The Poisson is a discrete probability
    distribution.  It is often used to model the number of events
    occurring in a fixed period of time when the times at which events
    occur are independent. The Poisson distribution can be derived as
    a limiting case of the binomial distribution.

    .. math::
        f(x \mid \mu) = \frac{e^{-\mu}\mu^x}{x!}

    Parameters
    ----------
    mu : float
        Expected number of occurrences during the given interval, :math:`\mu \geq 0`.

    .. note::
       - :math:`E(x)=\mu`
       - :math:`Var(x)=\mu`

    """
    def __init__(self, mu, *args, **kwargs):
        super(Poisson, self).__init__(*args, **kwargs)
        self.mu = mu
        self.mode = floor(mu).astype('int32')

    def logp(self, value):
        mu = self.mu

        return bound(
            logpow(mu, value) - factln(value) - mu,

            mu > 0, value >= 0)


class NegativeBinomial(Discrete):
    """
    Negative binomial log-likelihood.

     The negative binomial distribution  describes a Poisson random variable
     whose rate parameter is gamma distributed. PyMC's chosen parameterization
     is based on this mixture interpretation.

    .. math::
        f(x \mid \mu, \alpha) = \frac{\Gamma(x+\alpha)}{x! \Gamma(\alpha)} (\alpha/(\mu+\alpha))^\alpha (\mu/(\mu+\alpha))^x

    Parameters
    ----------
      mu : float
          mu > 0
      alpha : float
          alpha > 0

    .. note::
      - :math:`E[x]=\mu`

    """
    def __init__(self, mu, alpha, *args, **kwargs):
        super(NegativeBinomial, self).__init__(*args, **kwargs)
        self.mu = mu
        self.alpha = alpha
        self.mode = floor(mu).astype('int32')

    def logp(self, value):
        mu = self.mu
        alpha = self.alpha

        # Return Poisson when alpha gets very large
        pois = bound(logpow(mu, value) - factln(value) - mu,
                     mu > 0,
                     value >= 0)
        negbinom = bound(gammaln(value + alpha) - factln(value) - gammaln(alpha) +
                     logpow(mu / (mu + alpha), value) + logpow(alpha / (mu + alpha), alpha),
                      mu > 0, alpha > 0, value >= 0)

        return switch(alpha > 1e10,
                      pois,
                      negbinom)


class Geometric(Discrete):
    """
    Geometric log-likelihood. The probability that the first success in a
    sequence of Bernoulli trials occurs on the x'th trial.

    .. math::
        f(x \mid p) = p(1-p)^{x-1}

    Parameters
    ----------
    p : float
        Probability of success on an individual trial, :math:`p \in [0,1]`

    .. note::
      - :math:`E(X)=1/p`
      - :math:`Var(X)=\frac{1-p}{p^2}`

    """
    def __init__(self, p, *args, **kwargs):
        super(Geometric, self).__init__(*args, **kwargs)
        self.p = p
        self.mode = 1

    def logp(self, value):
        p = self.p
        return bound(log(p) + logpow(1 - p, value - 1),
                     0 <= p, p <= 1, value >= 1)


class DiscreteUniform(Discrete):
    """
    Discrete uniform distribution.

    .. math::
        f(x \mid lower, upper) = \frac{1}{upper-lower}

    Parameters
    ----------
    lower : int
        Lower limit.
    upper : int
        Upper limit (upper > lower).

    """
    def __init__(self, lower, upper, *args, **kwargs):
        super(DiscreteUniform, self).__init__(*args, **kwargs)
        self.lower, self.upper = floor(lower).astype('int32'), floor(upper).astype('int32')
        self.mode = floor((upper - lower) / 2.).astype('int32')

    def logp(self, value):
        upper = self.upper
        lower = self.lower

        return bound(
            -log(upper - lower + 1),

            lower <= value, value <= upper)


class Categorical(Discrete):
    """
    Categorical log-likelihood. The most general discrete distribution.

    .. math::  f(x=i \mid p) = p_i

    for :math:`i \in 0 \ldots k-1`.

    Parameters
    ----------
    p : float
        :math:`p > 0`, :math:`\sum p = 1`

    """
    def __init__(self, p, *args, **kwargs):
        super(Categorical, self).__init__(*args, **kwargs)
        self.k = p.shape[0]
        self.p = p
        self.mode = argmax(p)

    def logp(self, value):
        p = self.p
        k = self.k

        return bound(log(p[value]),
            value >= 0,
            value <= (k - 1),
            le(abs(sum(p) - 1), 1e-5))


class ConstantDist(Discrete):
    """
    Constant log-likelihood with parameter c={0}.

    Parameters
    ----------
    value : float or int
        Data value(s)
    """

    def __init__(self, c, *args, **kwargs):
        super(ConstantDist, self).__init__(*args, **kwargs)
        self.mean = self.median = self.mode = self.c = c

    def logp(self, value):
        c = self.c
        return bound(0, eq(value, c))


class ZeroInflatedPoisson(Discrete):
    def __init__(self, theta, z, *args, **kwargs):
        super(ZeroInflatedPoisson, self).__init__(*args, **kwargs)
        self.theta = theta
        self.z = z
        self.pois = Poisson.dist(theta)
        self.const = ConstantDist.dist(0)
        self.mode = self.pois.mode

    def logp(self, value):
        z = self.z
        return switch(z,
                      self.pois.logp(value),
                      self.const.logp(value))

########NEW FILE########
__FILENAME__ = distribution
import theano.tensor as t
import numpy as np
from ..model import Model

__all__ = ['DensityDist', 'Distribution', 'Continuous', 'Discrete']


class Distribution(object):
    """Statistical distribution"""
    def __new__(cls, name, *args, **kwargs):
        try:
            model = Model.get_context()
        except TypeError:
            raise TypeError("No model on context stack, which is needed to use the Normal('x', 0,1) syntax. Add a 'with model:' block")

        if isinstance(name, str):  
            data = kwargs.pop('observed', None)
            dist = cls.dist(*args, **kwargs)
            return model.Var(name, dist, data)
        elif name is None:
            return object.__new__(cls) #for pickle
        else: 
            raise TypeError("needed name or None but got: " + name)

    def __getnewargs__(self):
        return None, 

    @classmethod
    def dist(cls, *args, **kwargs):
        dist = object.__new__(cls)
        dist.__init__(*args, **kwargs)
        return dist

    def __init__(self, shape, dtype, testval=None, defaults=[]):
        self.shape = np.atleast_1d(shape)
        self.dtype = dtype
        self.type = TensorType(self.dtype, self.shape)
        self.testval = testval
        self.defaults = defaults

    def default(self):
        return self.get_test_val(self.testval, self.defaults)

    def get_test_val(self, val, defaults):
        if val is None:
            for v in defaults:
                if hasattr(self, v):
                    val = getattr(self, v)
                    break

        if val is None:
            raise AttributeError(str(self) + " has no default value to use, checked for: " +
                         str(defaults) + " pass testval argument or provide one of these.")

        if isinstance(val, str):
            val = getattr(self, val)

        if isinstance(val, t.TensorVariable):
            return val.tag.test_value

        return val


def TensorType(dtype, shape):
    return t.TensorType(str(dtype), np.atleast_1d(shape) == 1)

class Discrete(Distribution): 
    """Base class for discrete distributions"""
    def __init__(self, shape=(), dtype='int64', *args, **kwargs):
        super(Discrete, self).__init__(shape, dtype, defaults=['mode'], *args, **kwargs)

class Continuous(Distribution): 
    """Base class for continuous distributions"""
    def __init__(self, shape=(), dtype='float64', *args, **kwargs):
        super(Continuous, self).__init__(shape, dtype, defaults=['median', 'mean', 'mode'], *args, **kwargs)

class DensityDist(Distribution):
    """Distribution based on a given log density function."""
    def __init__(self, logp, shape=(), dtype='float64',testval=0, *args, **kwargs):
        super(DensityDist, self).__init__(shape, dtype, testval, *args, **kwargs)
        self.logp = logp

########NEW FILE########
__FILENAME__ = dist_math
'''
Created on Mar 7, 2011

@author: johnsalvatier
'''
from __future__ import division
import theano.tensor as t
from theano.tensor import (
    sum, switch, log, exp, sqrt,
    eq, neq, lt, gt, le, ge, all, any,
    cast, round, arange, max, min,
    maximum, minimum, floor, ceil,
    zeros_like, ones, ones_like,
    concatenate, constant, argmax)


from numpy import pi, inf, nan
from .special import gammaln, multigammaln

from theano.printing import Print
from .distribution import *


def bound(logp, *conditions):
    """
    Bounds a log probability density with several conditions

    Parameters
    ----------
    logp : float
    *conditionss : booleans

    Returns
    -------
    logp if all conditions are true
    -inf if some are false
    """

    return switch(alltrue(conditions), logp, -inf)


def alltrue(vals):
    ret = 1
    for c in vals:
        ret = ret * (1 * c)
    return ret


def logpow(x, m):
    """
    Calculates log(x**m) since m*log(x) will fail when m, x = 0.
    """
    return switch(eq(x, 0) & eq(m, 0), 0, m * log(x))


def factln(n):
    return gammaln(n + 1)


def idfn(x):
    return x

########NEW FILE########
__FILENAME__ = meta
# TODO Is this still relevant? If so, please document.
from .dist_math import *


def Normal_Summary(u, tau):
    def like(mean, sd, n):
        return switch(gt(tau, 0),
                      -tau / 2 * (sd ** 2 * (n - 1) + n * (u - mean)
                                  ** 2) + n / 2 * log(.5 * tau / pi),
                      -inf)
    return like

########NEW FILE########
__FILENAME__ = multivariate
from .dist_math import *

from theano.sandbox.linalg import det, solve, matrix_inverse, trace
from theano.tensor import dot, cast
from theano.printing import Print

__all__ = ['MvNormal', 'Dirichlet', 'Multinomial', 'Wishart']

class MvNormal(Continuous):
    """
    Multivariate normal

    :Parameters:
        mu : vector of means
        tau : precision matrix

    .. math::
        f(x \mid \pi, T) = \frac{|T|^{1/2}}{(2\pi)^{1/2}} \exp\left\{ -\frac{1}{2} (x-\mu)^{\prime}T(x-\mu) \right\}

    :Support:
        2 array of floats
    """
    def __init__(self, mu, tau, *args, **kwargs):
        super(MvNormal, self).__init__(*args, **kwargs)
        self.mean = self.median = self.mode = self.mu = mu
        self.tau = tau

    def logp(self, value):
        mu = self.mu
        tau = self.tau

        delta = value - mu
        k = tau.shape[0]

        return 1/2. * (-k * log(2*pi) + log(det(tau)) - dot(delta.T, dot(tau, delta)))


class Dirichlet(Continuous):
    """
    Dirichlet

    This is a multivariate continuous distribution.

    .. math::
        f(\mathbf{x}) = \frac{\Gamma(\sum_{i=1}^k \theta_i)}{\prod \Gamma(\theta_i)}\prod_{i=1}^{k-1} x_i^{\theta_i - 1}
        \cdot\left(1-\sum_{i=1}^{k-1}x_i\right)^\theta_k

    :Parameters:
        a : float tensor
            a > 0
            concentration parameters
            last index is the k index

    :Support:
        x : vector
            sum(x) == 1 and x > 0

    .. note::
        Only the first `k-1` elements of `x` are expected. Can be used
        as a parent of Multinomial and Categorical nevertheless.
    """
    def __init__(self, a, *args, **kwargs):
        super(Dirichlet, self).__init__(*args, **kwargs)
        self.a = a
        self.k = a.shape[0]
        self.mean = a / sum(a)

        self.mode = switch(all(a > 1),
                           (a - 1) / sum(a - 1),
                           nan)

    def logp(self, value):
        k = self.k
        a = self.a

        # only defined for sum(value) == 1
        return bound(
            sum(logpow(
                value, a - 1) - gammaln(a), axis=0) + gammaln(sum(a)),

            k > 1,
            all(a > 0))


class Multinomial(Discrete):
    """
    Generalization of the binomial
    distribution, but instead of each trial resulting in "success" or
    "failure", each one results in exactly one of some fixed finite number k
    of possible outcomes over n independent trials. 'x[i]' indicates the number
    of times outcome number i was observed over the n trials.

    .. math::
        f(x \mid n, p) = \frac{n!}{\prod_{i=1}^k x_i!} \prod_{i=1}^k p_i^{x_i}

    :Parameters:
        n : int
            Number of trials.
        p : (k,)
            Probability of each one of the different outcomes.
            :math:`\sum_{i=1}^k p_i = 1)`, :math:`p_i \ge 0`.

    :Support:
        x : (ns, k) int
            Random variable indicating the number of time outcome i is
            observed. :math:`\sum_{i=1}^k x_i=n`, :math:`x_i \ge 0`.

    .. note::
        - :math:`E(X_i)=n p_i`
        - :math:`Var(X_i)=n p_i(1-p_i)`
        - :math:`Cov(X_i,X_j) = -n p_i p_j`
    """
    def __init__(self, n, p, *args, **kwargs):
        super(Multinomial, self).__init__(*args, **kwargs)
        self.n = n
        self.p = p
        self.mean = n * p
        self.mode = cast(round(n * p), 'int32')

    def logp(self, x):
        n = self.n
        p = self.p
        # only defined for sum(p) == 1
        return bound(
            factln(n) + sum(x * log(p) - factln(x)),
            n > 0,
            eq(sum(x), n),
            all(0 <= x), all(x <= n))


class Wishart(Continuous):
    """
    The Wishart distribution is the probability
    distribution of the maximum-likelihood estimator (MLE) of the precision
    matrix of a multivariate normal distribution. If V=1, the distribution
    is identical to the chi-square distribution with n degrees of freedom.

    For an alternative parameterization based on :math:`C=T{-1}` (Not yet implemented)

    .. math::
        f(X \mid n, T) = \frac{{\mid T \mid}^{n/2}{\mid X \mid}^{(n-k-1)/2}}{2^{nk/2}
        \Gamma_p(n/2)} \exp\left\{ -\frac{1}{2} Tr(TX) \right\}

    where :math:`k` is the rank of X.

    :Parameters:
      n : int
        Degrees of freedom, > 0.
      V : ndarray
        p x p positive definite matrix


    :Support:
      X : matrix
        Symmetric, positive definite.
    """
    def __init__(self, n, V, *args, **kwargs):
        super(Wishart, self).__init__(*args, **kwargs)
        self.n = n
        self.p = p = V.shape[0]
        self.V = V
        self.mean = n * V
        self.mode = switch(1*(n >= p + 1),
                     (n - p - 1) * V,
                      nan)

    def logp(self, X):
        n = self.n
        p = self.p
        V = self.V

        IVI = det(V)

        return bound(
            ((n - p - 1) * log(IVI) - trace(matrix_inverse(V).dot(X)) -
             n * p * log(
             2) - n * log(IVI) - 2 * multigammaln(p, n / 2)) / 2,

             n > (p - 1))

########NEW FILE########
__FILENAME__ = special
'''
Created on Mar 17, 2011

@author: jsalvatier
'''
from theano import scalar, tensor
import numpy
from scipy import special, misc
from .dist_math import *

__all__ = ['gammaln', 'multigammaln', 'psi', 'trigamma', 'factln']


class GammaLn(scalar.UnaryScalarOp):
    """
    Compute gammaln(x)
    """
    @staticmethod
    def st_impl(x):
        return special.gammaln(x)

    def impl(self, x):
        return GammaLn.st_impl(x)

    def grad(self, inp, grads):
        x, = inp
        gz, = grads
        return [gz * scalar_psi(x)]

    def c_code(self, node, name, inp, out, sub):
        x, = inp
        z, = out
        if node.inputs[0].type in [scalar.float32, scalar.float64]:
            return """%(z)s =
                lgamma(%(x)s);""" % locals()
        raise NotImplementedError('only floatingpoint is implemented')

    def __eq__(self, other):
        return type(self) == type(other)

    def __hash__(self):
        return hash(type(self))

scalar_gammaln = GammaLn(scalar.upgrade_to_float, name='scalar_gammaln')
gammaln = tensor.Elemwise(scalar_gammaln, name='gammaln')


def multigammaln(a, p):
    """Multivariate Log Gamma

    :Parameters:
        a : tensor like
        p : int degrees of freedom
            p > 0
    """
    a = t.shape_padright(a)
    i = arange(p)

    return p * (p - 1) * log(pi) / 4. + t.sum(gammaln(a + i / 2.), axis=a.ndim - 1)


cpsifunc = """
#ifndef _PSIFUNCDEFINED
#define _PSIFUNCDEFINED
double _psi(double x){

    /*taken from
    Bernardo, J. M. (1976). Algorithm AS 103: Psi (Digamma) Function. Applied Statistics. 25 (3), 315-317.
    http://www.uv.es/~bernardo/1976AppStatist.pdf */

    double y, R, psi_ = 0;
    double S  = 1.0e-5;
    double C = 8.5;
    double S3 = 8.333333333e-2;
    double S4 = 8.333333333e-3;
    double S5 = 3.968253968e-3;
    double D1 = -0.5772156649  ;

    y = x;

    if (y <= 0.0)
        return psi_;

    if (y <= S )
        return D1 - 1.0/y;

    while (y < C){
        psi_ = psi_ - 1.0 / y;
        y = y + 1;}

    R = 1.0 / y;
    psi_ = psi_ + log(y) - .5 * R ;
    R= R*R;
    psi_ = psi_ - R * (S3 - R * (S4 - R * S5));

    return psi_;}
    #endif
        """


class Psi(scalar.UnaryScalarOp):
    """
    Compute derivative of gammaln(x)
    """
    @staticmethod
    def st_impl(x):
        return special.psi(x)

    def impl(self, x):
        return Psi.st_impl(x)

    def grad(self, inp, grads):
        x, = inp
        gz, = grads
        return [gz * scalar_trigamma(x)]

    def c_support_code(self):
        return cpsifunc

    def c_code(self, node, name, inp, out, sub):
        x, = inp
        z, = out
        if node.inputs[0].type in [scalar.float32, scalar.float64]:
            return """%(z)s =
                _psi(%(x)s);""" % locals()
        raise NotImplementedError('only floatingpoint is implemented')

    def __eq__(self, other):
        return type(self) == type(other)

    def __hash__(self):
        return hash(type(self))

scalar_psi = Psi(scalar.upgrade_to_float, name='scalar_psi')
psi = tensor.Elemwise(scalar_psi, name='psi')


class Trigamma(scalar.UnaryScalarOp):
    """
    Compute 2nd derivative of gammaln(x)
    """
    @staticmethod
    def st_impl(x):
        return special.polygamma(1, x)

    def impl(self, x):
        return Psi.st_impl(x)

    # def grad()  no gradient now

    def __eq__(self, other):
        return type(self) == type(other)

    def __hash__(self):
        return hash(type(self))

scalar_trigamma = Trigamma(scalar.upgrade_to_float, name='scalar_trigamma')
trigamma = tensor.Elemwise(scalar_trigamma, name='trigamma')

########NEW FILE########
__FILENAME__ = timeseries
from .dist_math import *
from .continuous import *

__all__ = ['AR1', 'GaussianRandomWalk']



class AR1(Continuous):
    """
    Autoregressive process with 1 lag.

    Parameters
    ----------
    k : tensor
       effect of lagged value on current value
    tau_e : tensor
       precision for innovations
    """
    def __init__(self, k, tau_e, *args, **kwargs):
        super(AR1, self).__init__(*args, **kwargs)
        self.k = k
        self.tau_e = tau_e
        self.tau = tau_e * (1 - k ** 2)
        self.mode = 0.

    def logp(self, x):
        k = self.k
        tau_e = self.tau_e

        x_im1 = x[:-1]
        x_i = x[1:]
        boundary = Normal.dist(0, tau).logp

        innov_like = Normal.dist(k * x_im1, tau_e).logp(x_i)
        return boundary(x[0]) + sum(innov_like) + boundary(x[-1])


class GaussianRandomWalk(Continuous):
    """
    Random Walk with Normal innovations

    Parameters
    ----------
    tau : tensor
        tau > 0, innovation precision
    init : distribution
        distribution for initial value (Defaults to Flat())
    """
    def __init__(self, tau=None, init=Flat.dist(), sd=None, *args, **kwargs):
        super(GaussianRandomWalk, self).__init__(*args, **kwargs)
        self.tau = tau
        self.sd = sd
        self.init = init
        self.mean = 0.

    def logp(self, x):
        tau = self.tau
        sd = self.sd
        init = self.init

        x_im1 = x[:-1]
        x_i = x[1:]

        innov_like = Normal.dist(x_im1, tau, sd=sd).logp(x_i)
        return init.logp(x[0]) + sum(innov_like)

########NEW FILE########
__FILENAME__ = transforms
from .dist_math import *
from ..model import FreeRV

__all__ = ['transform', 'logtransform', 'simplextransform']

class Transform(object):
    """A transformation of a random variable from one space into another."""
    def __init__(self, name, forward, backward, jacobian_det):
        """
        Parameters
        ----------
        name : str
        forward : function 
            forward transformation
        backwards : function 
            backwards transformation
        jacobian_det : function 
            jacobian determinant of the transformation"""
        self.__dict__.update(locals())

    def apply(self, dist):
        return TransformedDistribution.dist(dist, self)

    def __str__(self):
        return name + " transform"

class TransformedDistribution(Distribution):
    """A distribution that has been transformed from one space into another."""
    def __init__(self, dist, transform, *args, **kwargs):
        """
        Parameters
        ----------
        dist : Distribution 
        transform : Transform
        args, kwargs
            arguments to Distribution"""
        forward = transform.forward 
        try:
            testval = forward(dist.testval)
        except TypeError:
            testval = dist.testval
        
        if hasattr(dist, "mode"):
            self.mode = forward(dist.mode)
        if hasattr(dist, "median"):
            self.mode = forward(dist.median)
        
        self.dist = dist
        self.transform = transform
        v = forward(FreeRV(name='v', distribution=dist))
        self.type = v.type

        super(TransformedDistribution, self).__init__(v.shape.tag.test_value,
                v.dtype, 
                testval, dist.defaults, 
                *args, **kwargs)


    def logp(self, x):
        return self.dist.logp(self.transform.backward(x)) + self.transform.jacobian_det(x)

transform = Transform

logtransform = transform("log", log, exp, idfn)

simplextransform = transform("simplex",
                             lambda p: p[:-1],
                             lambda p: concatenate(
                             [p, 1 - sum(p, keepdims=True)]),
                             lambda p: constant([0]))

########NEW FILE########
__FILENAME__ = arbitrary_stochastic
from pymc import *
import numpy as np
with Model() as model:
    lam = Exponential('lam', 1)

    failure = np.array([0, 1])
    value = np.array([1, 0])

    def logp(failure, value):
        return sum(failure * log(lam) - lam * value)

    x = DensityDist('x', logp, observed=(failure, value))


def run (n=3000):
    if n == "short":
        n = 50
    with model:

        start = model.test_point
        h = find_hessian(start)
        step = Metropolis(model.vars, h)
        trace = sample(n, step, start)

if __name__ == "__main__":
    run()


########NEW FILE########
__FILENAME__ = ARM12_6
import numpy as np
from pymc import *
import pandas as pd

data = pd.read_csv(get_data_file('pymc.examples', 'data/srrs2.dat'))

cty_data = pd.read_csv(get_data_file('pymc.examples', 'data/cty.dat'))

data = data[data.state == 'MN']

data['fips'] = data.stfips * 1000 + data.cntyfips
cty_data['fips'] = cty_data.stfips * 1000 + cty_data.ctfips


data['lradon'] = np.log(np.where(data.activity == 0, .1, data.activity))


data = data.merge(cty_data, 'inner', on='fips')

unique = data[['fips']].drop_duplicates()
unique['group'] = np.arange(len(unique))
unique.set_index('fips')
data = data.merge(unique, 'inner', on='fips')

obs_means = data.groupby('fips').lradon.mean()
n = len(obs_means)

lradon = np.array(data.lradon)
floor = np.array(data.floor)
group = np.array(data.group)


model = Model()
with model:
    groupmean = Normal('groupmean', 0, 10. ** -2.)
    # as recommended by "Prior distributions for variance parameters in
    # hierarchical models"
    groupsd = Uniform('groupsd', 0, 10.)

    sd = Uniform('sd', 0, 10.)

    floor_m = Normal('floor_m', 0, 5. ** -2.)
    means = Normal('means', groupmean, groupsd ** -2., shape=n)

    lr = Normal(
        'lr', floor * floor_m + means[group], sd ** -2., observed=lradon)

def run(n=3000):
    if n == "short":
        n = 50
    with model:
        start = {'groupmean': obs_means.mean(),
                 'groupsd': obs_means.std(),
                 'sd': data.groupby('group').lradon.std().mean(),
                 'means': np.array(obs_means),
                 'floor_m': 0.,
                 }

        start = find_MAP(start, [groupmean, sd, floor_m])
        H = model.fastd2logp()
        h = np.diag(H(start))

        step = HamiltonianMC(model.vars, h)

        trace = sample(n, step, start)
        
if __name__ == '__main__':
    run()

########NEW FILE########
__FILENAME__ = ARM12_6uranium
import numpy as np
from pymc import *
import pandas as pd

data = pd.read_csv(get_data_file('pymc.examples', 'data/srrs2.dat'))

cty_data = pd.read_csv(get_data_file('pymc.examples', 'data/cty.dat'))

data = data[data.state == 'MN']

data['fips'] = data.stfips * 1000 + data.cntyfips
cty_data['fips'] = cty_data.stfips * 1000 + cty_data.ctfips


data['lradon'] = np.log(np.where(data.activity == 0, .1, data.activity))


data = data.merge(cty_data, 'inner', on='fips')

unique = data[['fips']].drop_duplicates()
unique['group'] = np.arange(len(unique))
unique.set_index('fips')
data = data.merge(unique, 'inner', on='fips')

obs_means = data.groupby('fips').lradon.mean()
n = len(obs_means)

lradon = np.array(data.lradon)
floor = np.array(data.floor)
group = np.array(data.group)
ufull = np.array(data.Uppm)


model = Model()
with model:
    groupmean = Normal('groupmean', 0, 10. ** -2.)
    # as recommended by "Prior distributions for variance parameters in
    # hierarchical models"
    groupsd = Uniform('groupsd', 0, 10.)

    sd = Uniform('sd', 0, 10.)

    floor_m = Normal('floor_m', 0, 5. ** -2.)
    u_m = Normal('u_m', 0, 5. ** -2)
    means = Normal('means', groupmean, groupsd ** -2., shape=n)

    lr = Normal('lr', floor * floor_m + means[group] + ufull * u_m, sd ** -
                2., observed=lradon)


def run(n=3000):
    if n == "short":
        n = 50
    with model:

        start = {'groupmean': obs_means.mean(),
                 'groupsd': obs_means.std(),
                 'sd': data.groupby('group').lradon.std().mean(),
                 'means': np.array(obs_means),
                 'u_m': np.array([.72]),
                 'floor_m': 0.,
                 }

        start = find_MAP(start, model.vars[:-1])
        H = model.fastd2logp()
        h = np.diag(H(start))

        step = HamiltonianMC(model.vars, h)

        trace = sample(n, step, start)
if __name__ == '__main__':
    run()      

########NEW FILE########
__FILENAME__ = ARM5_4
'''
Created on May 18, 2012

@author: jsalvatier
'''
import numpy as np
from pymc import *
import theano.tensor as t
import pandas as pd

wells = get_data_file('pymc.examples', 'data/wells.dat')

data = pd.read_csv(wells, delimiter=u' ', index_col=u'id',
                   dtype={u'switch': np.int8})

data.dist /= 100
data.educ /= 4

col = data.columns

P = data[col[1:]]

P = P - P.mean()
P['1'] = 1

Pa = np.array(P)

with Model() as model:
    effects = Normal(
        'effects', mu=0, tau=100. ** -2, shape=len(P.columns))
    p = sigmoid(dot(Pa, effects))

    s = Bernoulli('s', p, observed=np.array(data.switch))

def run(n=3000):
    if n == "short":
        n = 50
    with model:
        # move the chain to the MAP which should be a good starting point
        start = find_MAP()
        H = model.fastd2logp()  # find a good orientation using the hessian at the MAP
        h = H(start)

        step = HamiltonianMC(model.vars, h)

        trace = sample(n, step, start)
if __name__ == '__main__':
    run()

########NEW FILE########
__FILENAME__ = banana
'''
Created on May 10, 2012

@author: jsalvatier
'''
from pymc import *
from numpy.random import normal
import numpy as np
import pylab as pl
from itertools import product


"""
This model is U shaped because of the non identifiability. I think this is the same as the Rosenbrock function.
As n increases, the walls become steeper but the distribution does not shrink towards the mode.
As n increases this distribution gets harder and harder for HMC to sample.

Low Flip HMC seems to do a bit better.

This example comes from
Discussion of Riemann manifold Langevin and
Hamiltonian Monte Carlo methods by M.
Girolami and B. Calderhead

http://arxiv.org/abs/1011.0057
"""
N = 200
with Model() as model:

    x = Normal('x', 0, 1)
    y = Normal('y', 0, 1)
    N = 200
    d = Normal('d', x + y ** 2, 1., observed=np.zeros(N))

    start = model.test_point
    h = np.ones(2) * np.diag(find_hessian(start))[0]

    step = HamiltonianMC(model.vars, h, path_length=4.)


def run(n = 3000):
    if n == "short":
        n = 50
    with model:
        trace = sample(n, step, start)

        pl.figure()
        pl.hexbin(trace['x'], trace['y'])


        # lets plot the samples vs. the actual distribution
        from theano import function
        xn = 1500
        yn = 1000

        xs = np.linspace(-3, .25, xn)[np.newaxis, :]
        ys = np.linspace(-1.5, 1.5, yn)[:, np.newaxis]

        like = (xs + ys ** 2) ** 2 * N
        post = np.exp(-.5 * (xs ** 2 + ys ** 2 + like))
        post = post

        pl.figure()
        extent = np.min(xs), np.max(xs), np.min(ys), np.max(ys)
        pl.imshow(post, extent=extent)



########NEW FILE########
__FILENAME__ = dirichlet
import numpy as np
from pymc import *

model = Model()
with model:

    k = 5
    a = constant(np.array([2, 3., 4, 2, 2]))

    p, p_m1 = model.TransformedVar(
        'p', Dirichlet.dist(a, shape=k),
        simplextransform)

    c = Categorical('c', p, observed=np.random.randint(0, k, 5))

def run(n=3000):
    if n == "short":
        n = 50
    with model:
        step = Slice()
        trace = sample(n, step)

if __name__ == '__main__':
    run()



########NEW FILE########
__FILENAME__ = disaster_model
"""
A model for coal mining disasters data with a changepoint

switchpoint ~ U(0, 110)
early_mean ~ Exp(1.)
late_mean ~ Exp(1.)
disasters[t] ~ Po(early_mean if t <= switchpoint, late_mean otherwise)

"""

from pymc import *

import theano.tensor as t
from numpy import arange, array, ones, concatenate
from numpy.random import randint

__all__ = ['disasters_data', 'switchpoint', 'early_mean', 'late_mean', 'rate',
             'disasters']

# Time series of recorded coal mining disasters in the UK from 1851 to 1962
disasters_data = array([4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6,
                            3, 3, 5, 4, 5, 3, 1, 4, 4, 1, 5, 5, 3, 4, 2, 5,
                            2, 2, 3, 4, 2, 1, 3, 2, 2, 1, 1, 1, 1, 3, 0, 0,
                            1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1,
                            0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2,
                            3, 3, 1, 1, 2, 1, 1, 1, 1, 2, 4, 2, 0, 0, 1, 4,
                            0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])
years = len(disasters_data)

with Model() as model:

    # Prior for distribution of switchpoint location
    switchpoint = DiscreteUniform('switchpoint', lower=0, upper=years)
    # Priors for pre- and post-switch mean number of disasters
    early_mean = Exponential('early_mean', lam=1.)
    late_mean = Exponential('late_mean', lam=1.)

    # Allocate appropriate Poisson rates to years before and after current
    # switchpoint location
    idx = arange(years)
    rate = switch(switchpoint >= idx, early_mean, late_mean)

    # Data likelihood
    disasters = Poisson('disasters', rate, observed=disasters_data)


def run(n=1000):
    if n == "short":
        n = 50
    with model:

        # Initial values for stochastic nodes
        start = {'early_mean': 2., 'late_mean': 3.}

        # Use slice sampler for means
        step1 = Slice([early_mean, late_mean])
        # Use Metropolis for switchpoint, since it accomodates discrete variables
        step2 = Metropolis([switchpoint])

        tr = sample(n, tune=500, start=start, step=[step1, step2])

if __name__ == '__main__':
    run()



########NEW FILE########
__FILENAME__ = discrete_find_MAP
# Using `find_MAP` on models with discrete variables

# Maximum a posterior(MAP) estimation, can be difficult in models which have
# discrete stochastic variables. Here we demonstrate the problem with a simple
# model, and present a few possible work arounds.

import pymc as mc

# We define a simple model of a survey with one data point. We use a $Beta$
# distribution for the $p$ parameter in a binomial. We would like to know both
# the posterior distribution for p, as well as the predictive posterior
# distribution over the survey parameter.

alpha = 4
beta = 4
n = 20
yes = 15

with mc.Model() as model:
    p = mc.Beta('p', alpha, beta)
    surv_sim = mc.Binomial('surv_sim', n=n, p=p)
    surv = mc.Binomial('surv', n=n, p=p, observed=yes)

# First let's try and use `find_MAP`.

with model:
    print(mc.find_MAP())

# `find_map` defaults to find the MAP for only the continuous variables we have
# to specify if we would like to use the discrete variables.

with model:
    print(mc.find_MAP(vars=model.vars, disp=True))

# We set the `disp` variable to display a warning that we are using a
# non-gradient minimization technique, as discrete variables do not give much
# gradient information. To demonstrate this, if we use a gradient based
# minimization, `fmin_bfgs`, with various starting points we see that the map
# does not converge.

with model:
    for i in range(n+1):
        s = {'p': 0.5, 'surv_sim': i}
        map_est = mc.find_MAP(start=s, vars=model.vars,
                              fmin=mc.starting.optimize.fmin_bfgs)
        print('surv_sim: %i->%i, p: %f->%f, LogP:%f'%(s['surv_sim'],
                                                      map_est['surv_sim'],
                                                      s['p'],
                                                      map_est['p'],
                                                      model.logp(map_est)))

# Once again because the gradient of `surv_sim` provides no information to the
# `fmin` routine and it is only changed in a few cases, most of which are not
# correct. Manually, looking at the log proability we can see that the maximum
# is somewhere around `surv_sim`$=14$ and `p`$=0.7$. If we employ a
# non-gradient minimization, such as `fmin_powell` (the default when discrete
# variables are detected), we might be able to get a better estimate.

with model:
    for i in range(n+1):
        s = {'p': 0.5, 'surv_sim': i}
        map_est = mc.find_MAP(start=s, vars=model.vars)
        print('surv_sim: %i->%i, p: %f->%f, LogP:%f'%(s['surv_sim'],
                                                      map_est['surv_sim'],
                                                      s['p'],
                                                      map_est['p'],
                                                      model.logp(map_est)))

# For most starting values this converges to the maximum log likelihood of
# $\approx -3.15$, but for particularly low starting values of `surv_sim`, or
# values near `surv_sim`$=14$ there is still some noise. The scipy optimize
# package contains some more general 'global' minimization functions that we
# can utilize. The `basinhopping` algorithm restarts the optimization at places
# near found minimums. Because it has a slightly different interface to other
# minimization schemes we have to define a wrapper function.


def bh(*args, **kwargs):
    result = mc.starting.optimize.basinhopping(*args, **kwargs)
    # A `Result` object is returned, the argmin value can be in `x`
    return result['x']

with model:
    for i in range(n+1):
        s = {'p': 0.5, 'surv_sim': i}
        map_est = mc.find_MAP(start=s, vars=model.vars, fmin=bh)
        print('surv_sim: %i->%i, p: %f->%f, LogP:%f'%(s['surv_sim'],
                                                      map_est['surv_sim'],
                                                      s['p'],
                                                      map_est['p'],
                                                      model.logp(map_est)))

# By default `basinhopping` uses a gradient minimization technique,
# `fmin_bfgs`, resulting in inaccurate predictions many times. If we force
# `basinhoping` to use a non-gradient technique we get much better results

with model:
    for i in range(n+1):
        s = {'p': 0.5, 'surv_sim': i}
        map_est = mc.find_MAP(start=s, vars=model.vars,
                              fmin=bh, minimizer_kwargs={"method": "Powell"})
        print('surv_sim: %i->%i, p: %f->%f, LogP:%f'%(s['surv_sim'],
                                                      map_est['surv_sim'],
                                                      s['p'],
                                                      map_est['p'],
                                                      model.logp(map_est)))

# Confident in our MAP estimate we can sample from the posterior, making sure
# we use the `Metropolis` method for our discrete variables.

with model:
    step1 = mc.step_methods.HamiltonianMC(vars=[p])
    step2 = mc.step_methods.Metropolis(vars=[surv_sim])

with model:
    trace = mc.sample(25000, [step1, step2], start=map_est)

mc.traceplot(trace);

########NEW FILE########
__FILENAME__ = factor_potential
from pymc import *

with Model() as model:
    x = Normal('x', 1, 1)
    x2 = Potential('x2', -x ** 2)

    start = model.test_point
    h = find_hessian(start)
    step = Metropolis(model.vars, h)

def run(n = 3000):
    if n == "short":
        n = 50
    with model:
        trace = sample(n, step, start)
if __name__ == '__main__':
    run()



########NEW FILE########
__FILENAME__ = gelman_bioassay
from pymc import *
from numpy import ones, array
import theano.tensor as t

# Samples for each dose level
n = 5 * ones(4, dtype=int)
# Log-dose
dose = array([-.86, -.3, -.05, .73])

def tinvlogit(x):
    return t.exp(x) / (1 + t.exp(x))

with Model() as model:

    # Logit-linear model parameters
    alpha = Normal('alpha', 0, 0.01)
    beta = Normal('beta', 0, 0.01)

    # Calculate probabilities of death
    theta = Deterministic('theta', tinvlogit(alpha + beta * dose))

    # Data likelihood
    deaths = Binomial('deaths', n=n, p=theta, observed=[0, 1, 3, 5])

    step = NUTS()

def run(n=1000): 
    if n == "short":
        n = 50
    with model:
        trace = sample(n, step)

if __name__ == '__main__':
    run()



########NEW FILE########
__FILENAME__ = GHME_2013
# -*- coding: utf-8 -*-
# <nbformat>3.0</nbformat>

# <codecell>
from pylab import *
import pandas as pd
from pymc import *
from pymc.distributions.timeseries import *

# <markdowncell>

# Data
# ----

# <codecell>

data = pd.read_csv(get_data_file('pymc.examples', 'data/pancreatitis/input_data.csv'))
countries = ['CYP', 'DNK', 'ESP', 'FIN','GBR', 'ISL']
data = data[data.area.isin(countries)]

age = data['age'] = np.array(data.age_start + data.age_end)/2
rate = data.value = data.value*1000
group, countries = pd.factorize(data.area, order=countries)


ncountries = len(countries)

# <codecell>

for i, country in enumerate(countries):
    subplot(2,3,i+1)
    title(country)
    d = data[data.area == country]
    plot(d.age, d.value, '.')

    ylim(0,rate.max())

# <markdowncell>

# Model Specification
# -------------------

# <codecell>

nknots = 10
knots = np.linspace(data.age_start.min(),data.age_end.max(), nknots)


def interpolate(x0,y0, x, group):
    x = np.array(x)
    group = np.array(group)

    idx = np.searchsorted(x0, x)
    dl = np.array(x - x0[idx - 1])
    dr = np.array(x0[idx] - x)
    d=dl+dr
    wl = dr/d

    return wl*y0[idx-1, group] + (1-wl)*y0[idx, group]


with Model() as model:
    coeff_sd = T('coeff_sd', 10, 1, 5**-2)

    y = GaussianRandomWalk('y', sd=coeff_sd, shape = (nknots, ncountries))

    p = interpolate(knots, y, age, group)

    sd = T('sd', 10, 2, 5**-2)

    vals = Normal('vals', p, sd=sd, observed = rate)

# <markdowncell>

# Model Fitting
# -------------

# <codecell>

with model:
    s = find_MAP( vars=[sd, y])

    step = NUTS(scaling = s)
    trace = sample(100, step, s)

    s = trace[-1]

    step = NUTS(scaling = s)
    
def run(n=3000):
    if n == "short":
        n = 150
    with model:
        trace = sample(n, step, s)


    # <codecell>

    for i, country in enumerate(countries):
        subplot(2,3,i+1)
        title(country)

        d = data[data.area == country]
        plot(d.age, d.value, '.')
        plot(knots, trace[y][::5,:,i].T, color ='r', alpha =.01);

        ylim(0,rate.max())

    # <codecell>

    traceplot(trace[100:], vars = [coeff_sd,sd ]);

    # <codecell>

    autocorrplot(trace, vars = [coeff_sd,sd ])
    
if __name__ == '__main__':
    run()


########NEW FILE########
__FILENAME__ = glm_linear
import numpy as np

try:
    import statsmodels.api as sm
except ImportError:
    print("Example requires statsmodels")
    sys.exit(0)

from pymc import *

# Generate data
size = 50
true_intercept = 1
true_slope = 2

x = np.linspace(0, 1, size)
y = true_intercept + x*true_slope + np.random.normal(scale=.5, size=size)

data = dict(x=x, y=y)

with Model() as model:
    glm.glm('y ~ x', data)


def run(n=2000):
    if n == "short":
        n = 50
    import matplotlib.pyplot as plt

    with model:
        trace = sample(n, Slice(model.vars))

    plt.plot(x, y, 'x')
    glm.plot_posterior_predictive(trace)
    # plt.show()

if __name__ == '__main__':
    run()



########NEW FILE########
__FILENAME__ = glm_robust
import numpy as np

try:
    import statsmodels.api as sm
except ImportError:
    sys.exit(0)

from pymc import *

# Generate data
size = 50
true_intercept = 1
true_slope = 2

x = np.linspace(0, 1, size)
y = true_intercept + x*true_slope + np.random.normal(scale=.5, size=size)

# Add outliers
x = np.append(x, [.1, .15, .2])
y = np.append(y, [8, 6, 9])

data_outlier = dict(x=x, y=y)

with Model() as model:
    family = glm.families.T(link=glm.links.Identity,
			                priors={'nu': 1.5,
				                    'lam': ('sigma', Uniform.dist(0, 20))})
    glm.glm('y ~ x', data_outlier, family=family)

def run(n=2000):
    if n == "short":
        n = 50
    import matplotlib.pyplot as plt

    with model:
        trace = sample(n, Slice(model.vars))

    plt.plot(x, y, 'x')
    glm.plot_posterior_predictive(trace)
    plt.show()

if __name__ == '__main__':
    run()



########NEW FILE########
__FILENAME__ = hierarchical
from pymc import *
import theano.tensor as T
from numpy import random, sum as nsum, ones, concatenate, newaxis, dot, arange
import numpy as np

random.seed(1)

n_groups = 10
no_pergroup = 30
n_observed = no_pergroup * n_groups
n_group_predictors = 1
n_predictors = 3

group = concatenate([[i] * no_pergroup for i in range(n_groups)])
group_predictors = random.normal(size=(n_groups, n_group_predictors))  # random.normal(size = (n_groups, n_group_predictors))
predictors = random.normal(size=(n_observed, n_predictors))

group_effects_a = random.normal(size=(n_group_predictors, n_predictors))
effects_a = random.normal(
    size=(n_groups, n_predictors)) + dot(group_predictors, group_effects_a)

y = nsum(
    effects_a[group, :] * predictors, 1) + random.normal(size=(n_observed))


model = Model()
with model:

    # m_g ~ N(0, .1)
    group_effects = Normal(
        "group_effects", 0, .1, shape=(1, n_group_predictors, n_predictors))

    # sg ~ Uniform(.05, 10)
    sg = Uniform("sg", .05, 10, testval=2.)


    # m ~ N(mg * pg, sg)
    effects = Normal("effects",
                     sum(group_predictors[:, :, newaxis] *
                     group_effects, 1), sg ** -2,
                     shape=(n_groups, n_predictors))

    s = Uniform("s", .01, 10, shape=n_groups)

    g = T.constant(group)

    # y ~ Normal(m[g] * p, s)
    yd = Normal('y', sum(effects[g] * predictors, 1), s[g] ** -2, observed=y)

    start = find_MAP()
    #h = find_hessian(start)

    step = NUTS(model.vars, scaling=start)

def run(n=3000):
    if n == "short":
        n = 50
    with model:
        trace = sample(n, step, start)
        
if __name__ == '__main__':
    run()

########NEW FILE########
__FILENAME__ = hierarchical_sqlite
if __name__ == '__main__':  # Avoid loading during tests.

    import pymc as pm
    import pymc.examples.hierarchical as hier

    with hier.model:
        trace = pm.sample(3000, hier.step, hier.start, trace='sqlite')

########NEW FILE########
__FILENAME__ = lasso_block_update
# -*- coding: utf-8 -*-
# <nbformat>3.0</nbformat>

# <markdowncell>

# Sometimes, it is very useful to update a set of parameters together. For example, variables that are highly correlated are often good to update together. In PyMC 3 block updating is simple, as example will demonstrate.
#
# Here we have a LASSO regression model where the two coefficients are strongly correlated. Normally, we would define the coefficient parameters as a single random variable, but here we define them separately to show how to do block updates.
#
# First we generate some fake data.

# <codecell>
from matplotlib.pylab import *
from pymc import *
import numpy as np

d = np.random.normal(size=(3, 30))
d1 = d[0] + 4
d2 = d[1] + 4
yd = .2 * d1 + .3 * d2 + d[2]

# <markdowncell>

# Then define the random variables.

# <codecell>

with Model() as model:
    s = Exponential('s', 1)
    m1 = Laplace('m1', 0, 100)
    m2 = Laplace('m2', 0, 100)

    p = d1 * m1 + d2 * m2

    y = Normal('y', p, s ** -2, observed=yd)

# <markdowncell>

# For most samplers, including Metropolis and HamiltonianMC, simply pass a
# list of variables to sample as a block. This works with both scalar and
# array parameters.

# <codecell>

with model:
    step1 = Metropolis([m1, m2])

    step2 = Metropolis([s], proposal_dist=LaplaceProposal)

def run(n=5000):
    if n == "short":
        n = 300
    with model:
        start = find_MAP()
        trace = sample(n, [step1, step2], start)

        dh = fn(hessian_diag(model.logpt))

    # <codecell>

    traceplot(trace)

    # <codecell>

    hexbin(trace[m1], trace[m2], gridsize=50)

# <codecell>
if __name__ == '__main__':
    run()



########NEW FILE########
__FILENAME__ = latent_occupancy
"""
From the PyMC example list
latent_occupancy.py

Simple model demonstrating the estimation of occupancy, using latent variables. Suppose
a population of n sites, with some proportion pi being occupied. Each site is surveyed,
yielding an array of counts, y:

y = [3, 0, 0, 2, 1, 0, 1, 0, ..., ]

This is a classic zero-inflated count problem, where more zeros appear in the data than would
be predicted by a simple Poisson model. We have, in fact, a mixture of models; one, conditional
on occupancy, with a poisson mean of theta, and another, conditional on absence, with mean zero.
One way to tackle the problem is to model the latent state of 'occupancy' as a Bernoulli variable
at each site, with some unknown probability:

z_i ~ Bern(pi)

These latent variables can then be used to generate an array of Poisson parameters:

t_i = theta (if z_i=1) or 0 (if z_i=0)

Hence, the likelihood is just:

y_i = Poisson(t_i)

(Note in this elementary model, we are ignoring the issue of imperfect detection.)

Created by Chris Fonnesbeck on 2008-07-28.
Copyright (c) 2008 University of Otago. All rights reserved.
"""

# Import statements
from pymc import *
from numpy import random, array, arange, ones
import theano.tensor as t
# Sample size
n = 100
# True mean count, given occupancy
theta = 2.1
# True occupancy
pi = 0.4

# Simulate some data data
y = array([(random.random() < pi) * random.poisson(theta) for i in range(n)])

model = Model()
with model:
    # Estimated occupancy

    p = Beta('p', 1, 1)

    # Latent variable for occupancy
    z = Bernoulli('z', p, y.shape)

    # Estimated mean count
    theta = Uniform('theta', 0, 100)

    # Poisson likelihood
    yd = ZeroInflatedPoisson('y', theta, z, observed=y)


point = model.test_point

_pymc3_logp = model.logp


def pymc3_logp():
    _pymc3_logp(point)

_pymc3_dlogp = model.dlogp()


def pymc3_dlogp():
    _pymc3_dlogp(point)

def run(n=5000):
    if n == "short":
        n = 50
    with model:
        start = {'p': 0.5, 'z': (y > 0).astype(int), 'theta': 5}

        step1 = Metropolis([theta, p])

        step2 = BinaryMetropolis([z])

        trace = sample(n, [step1, step2], start)
        
if __name__ == '__main__':
    run()

########NEW FILE########
__FILENAME__ = logistic
from pymc import *

import theano.tensor as t
import numpy as np


def invlogit(x):
    import numpy as np
    return np.exp(x) / (1 + np.exp(x))

npred = 4
n = 4000

effects_a = np.random.normal(size=npred)
predictors = np.random.normal(size=(n, npred))


outcomes = np.random.binomial(
    1, invlogit(np.sum(effects_a[None, :] * predictors, 1)))


def tinvlogit(x):
    import theano.tensor as t
    return t.exp(x) / (1 + t.exp(x))

model = Model()

with model:
    effects = Normal('effects', mu=0, tau=2. ** -2, shape=(1, npred))
    p = tinvlogit(sum(effects * predictors, 1))

    o = Bernoulli('o', p, observed=outcomes)

def run(n=3000):
    if n == "short":
        n = 50
    with model:
        # move the chain to the MAP which should be a good starting point
        start = find_MAP()
        step = NUTS(scaling=start)

        trace = sample(n, step, start)

if __name__ == '__main__':
    run()

########NEW FILE########
__FILENAME__ = normal
from pymc import *
import numpy as np
import theano

# import pydevd
# pydevd.set_pm_excepthook()
np.seterr(invalid='raise')

data = np.random.normal(size=(3, 20))
n = 1

model = Model()
with model:
    x = Normal('x', 0, 1., shape=n)

    # start sampling at the MAP
    start = find_MAP()

    step = NUTS(scaling=start)

def run(n=3000):
    if n == "short":
        n = 50
    with model:
        trace = sample(n, step, start=start)
        
if __name__ == '__main__':
    run()

########NEW FILE########
__FILENAME__ = nutstest
# -*- coding: utf-8 -*-
# <nbformat>3.0</nbformat>

# <codecell>

from matplotlib.pylab import *
from pymc import *
import numpy as np 
from numpy.random import normal, beta
import theano 



xtrue = normal(scale = 2., size = 1)

with Model() as model:
    x = Normal('x', mu = 0., tau = 1)

    step = NUTS()

def run(n=5000):
    if n == "short":
        n = 50
    with model:
        trace = sample(n, step)
        
 

    # <markdowncell>

    # To use more than one sampler, use a CompoundStep which takes a list of step methods. 
    # 
    # The trace object can be indexed by the variables returning an array with the first index being the sample index
    # and the other indexes the shape of the parameter. Thus the shape of trace[x].shape == (ndraw, 2,1).
    # 
    # Pymc3 provides `traceplot` a simple plot for a trace.

    # <codecell>

    plot(trace[x])

    # <codecell>

    trace[x]

    # <codecell>

    traceplot(trace)
    
if __name__ == '__main__':
    run()


########NEW FILE########
__FILENAME__ = python_vs_c
from pymc import *
import numpy as np
import theano.tensor as t

# import pydevd
# pydevd.set_pm_excepthook()


def invlogit(x):
    import numpy as np
    return np.exp(x) / (1 + np.exp(x))

npred = 4
n = 4000

effects_a = np.random.normal(size=npred)
predictors = np.random.normal(size=(n, npred))


outcomes = np.random.binomial(
    1, invlogit(np.sum(effects_a[None, :] * predictors, 1)))


def tinvlogit(x):
    import theano.tensor as t
    return t.exp(x) / (1 + t.exp(x))

model = Model()
with model:

    effects = Normal('effects', mu=0, tau=2. ** -2, shape=(1, npred))
    p = tinvlogit(sum(effects * predictors, 1))
    o = Bernoulli('o', p, observed=outcomes)


start = model.test_point

from theano import ProfileMode

def run(n=1):
    if n == "short":
        n = 50
    for mode in [ProfileMode(linker='py'),
                 ProfileMode(linker='c|py')]:

        print(mode)
        logp = model.logpt
        f = model.fn([logp, gradient(logp)], mode)
        print(f(start))
        
        #mode.print_summary()
    
if __name__ == '__main__':
    run()

########NEW FILE########
__FILENAME__ = simpletest
import matplotlib.pyplot as plt
from pymc import *
import numpy as np
import theano

# import pydevd
# pydevd.set_pm_excepthook()
np.seterr(invalid='raise')

data = np.random.normal(size=(2, 20))


model = Model()

with model:
    x = Normal('x', mu=.5, tau=2. ** -2, shape=(2, 1))

    z = Beta('z', alpha=10, beta=5.5)

    d = Normal('data', mu=x, tau=.75 ** -2, observed=data)

    step = NUTS()

def run(n=1000):
    if n == "short":
        n = 50
    with model:
        trace = sample(n, step)

    plt.subplot(2, 2, 1)
    plt.plot(trace[x][:, 0, 0])
    plt.subplot(2, 2, 2)
    plt.hist(trace[x][:, 0, 0])

    plt.subplot(2, 2, 3)
    plt.plot(trace[x][:, 1, 0])
    plt.subplot(2, 2, 4)
    plt.hist(trace[x][:, 1, 0])
    plt.show()

if __name__ == '__main__':
    run()

########NEW FILE########
__FILENAME__ = speedtest_pymc3
"""
From the PyMC example list
latent_occupancy.py

Simple model demonstrating the estimation of occupancy, using latent variables. Suppose
a population of n sites, with some proportion pi being occupied. Each site is surveyed,
yielding an array of counts, y:

y = [3, 0, 0, 2, 1, 0, 1, 0, ..., ]

This is a classic zero-inflated count problem, where more zeros appear in the data than would
be predicted by a simple Poisson model. We have, in fact, a mixture of models; one, conditional
on occupancy, with a poisson mean of theta, and another, conditional on absence, with mean zero.
One way to tackle the problem is to model the latent state of 'occupancy' as a Bernoulli variable
at each site, with some unknown probability:

z_i ~ Bern(pi)

These latent variables can then be used to generate an array of Poisson parameters:

t_i = theta (if z_i=1) or 0 (if z_i=0)

Hence, the likelihood is just:

y_i = Poisson(t_i)

(Note in this elementary model, we are ignoring the issue of imperfect detection.)

Created by Chris Fonnesbeck on 2008-07-28.
Copyright (c) 2008 University of Otago. All rights reserved.
"""

# Import statements
from pymc import *
from numpy import random, array, arange, ones
import theano.tensor as t
# Sample size
n = 100000
# True mean count, given occupancy
theta = 2.1
# True occupancy
pi = 0.4

# Simulate some data data
y = array([(random.random() < pi) * random.poisson(theta) for i in range(n)])

model = Model()
with model:
    # Estimated occupancy
    p = Beta('b', 1, 1)

    # Latent variable for occupancy
    z = Bernoulli('z', p, shape=y.shape)

    # Estimated mean count
    theta = Uniform('theta', 0, 100)

    # Poisson likelihood
    z = ZeroInflatedPoisson('z', theta, z)


point = model.test_point

_pymc3_logp = model.logp


def pymc3_logp():
    _pymc3_logp(point)

_pymc3_dlogp = model.dlogp()


def pymc3_dlogp():
    _pymc3_dlogp(point)
    
def run(n=1):
    pass
    
if __name__ == '__main__':
    run()

########NEW FILE########
__FILENAME__ = stochastic_volatility
# -*- coding: utf-8 -*-
# <nbformat>3.0</nbformat>

# <codecell>

from matplotlib.pylab import *
import numpy as np
from pymc import *
from pymc.distributions.timeseries import *

from scipy.sparse import csc_matrix
from scipy import optimize

# <markdowncell>

# Asset prices have time-varying volatility (variance of day over day `returns`). In some periods, returns are highly variable, while in others very stable. Stochastic volatility models model this with a latent volatility variable, modeled as a stochastic process. The following model is similar to the one described in the No-U-Turn Sampler paper, Hoffman (2011) p21.
#
# $$ \sigma \sim Exponential(50) $$
#
# $$ \nu \sim Exponential(.1) $$
#
# $$ s_i \sim Normal(s_{i-1}, \sigma^{-2}) $$
#
# $$ log(\frac{y_i}{y_{i-1}}) \sim t(\nu, 0, exp(-2 s_i)) $$
#
# Here, $y$ is the daily return series and $s$ is the latent log
# volatility process.

# <markdowncell>

# ## Build Model

# <markdowncell>

# First we load some daily returns of the S&P 500.

# <codecell>

n = 400

returns = np.genfromtxt(get_data_file('pymc.examples', "data/SP500.csv"))[-n:]
returns[:5]

# <markdowncell>

# Specifying the model in pymc mirrors its statistical specification.
#
# However, it is easier to sample the scale of the log volatility process innovations, $\sigma$, on a log scale, so we create it using `TransformedVar` and use `logtransform`. `TransformedVar` creates one variable in the transformed space and one in the normal space. The one in the transformed space (here $\text{log}(\sigma) $) is the one over which sampling will occur, and the one in the normal space is the one to use throughout the rest of the model.
#
# It takes a variable name, a distribution and a transformation to use.

# <codecell>

model = Model()
with model:
    sigma, log_sigma = model.TransformedVar(
        'sigma', Exponential.dist(1. / .02, testval=.1),
        logtransform)

    nu = Exponential('nu', 1. / 10)

    s = GaussianRandomWalk('s', sigma ** -2, shape=n)

    r = T('r', nu, lam=exp(-2 * s), observed=returns)

# <markdowncell>

# ## Fit Model
#
# To get a decent scaling matrix for the Hamiltonian sampler, we find the Hessian at a point. The method `Model.d2logpc` gives us a `Theano` compiled function that returns the matrix of 2nd derivatives.
#
# However, the 2nd derivatives for the degrees of freedom parameter, `nu`, are negative and thus not very informative and make the matrix non-positive definite, so we replace that entry with a reasonable guess at the scale. The interactions between `log_sigma`/`nu` and `s` are also not very useful, so we set them to zero.
#
# The Hessian matrix is also sparse, so we can get faster sampling by
# using a sparse scaling matrix. If you have `scikits.sparse` installed,
# convert the Hessian to a csc matrixs by uncommenting the appropriate
# line below.

# <codecell>

H = model.fastd2logp()


def hessian(point, nusd):
    h = H(Point(point))
    h[1, 1] = nusd ** -2
    h[:2, 2:] = h[2:, :2] = 0

    # h = csc_matrix(h)
    return h

# <markdowncell>

# For this model, the full maximum a posteriori (MAP) point is degenerate and has infinite density. However, if we fix `log_sigma` and `nu` it is no longer degenerate, so we find the MAP with respect to the volatility process, 's', keeping `log_sigma` and `nu` constant at their default values.
#
# We use L-BFGS because it is more efficient for high dimensional
# functions (`s` has n elements).

# <codecell>

with model:
    start = find_MAP(vars=[s], fmin=optimize.fmin_l_bfgs_b)

# <markdowncell>

# We do a short initial run to get near the right area, then start again
# using a new Hessian at the new starting point to get faster sampling due
# to better scaling. We do a short run since this is an interactive
# example.

# <codecell>

with model:
    step = NUTS(model.vars, hessian(start, 6))
    
    
    
def run(n=2000):
    if n == "short":
        n = 50
    with model:
        trace = sample(5, step, start, trace=model.vars + [sigma])

        # Start next run at the last sampled position.
        start2 = trace.point(-1)
        step2 = HamiltonianMC(model.vars, hessian(start2, 6), path_length=4.)
        trace = sample(n, step2, trace=trace)

    # <codecell>

    # figsize(12,6)
    title(str(s))
    plot(trace[s][::10].T, 'b', alpha=.03)
    xlabel('time')
    ylabel('log volatility')

    # figsize(12,6)
    traceplot(trace, model.vars[:-1])
    
if __name__ == '__main__':
    run()

# <markdowncell>

# ## References
#
# 1. Hoffman & Gelman. (2011). [The No-U-Turn Sampler: Adaptively Setting
# Path Lengths in Hamiltonian Monte
# Carlo](http://arxiv.org/abs/1111.4246).

########NEW FILE########
__FILENAME__ = tutorial

# In[1]:

import pymc as pm
import numpy as np


# Model
# -----
# Consider the following true generative model:
#
# $$ x_{true} \sim \textrm{Normal}(2,1) $$
# $$ y_{true} \sim \textrm{Normal}(\textrm{exp}(x_{true}),1)$$
# $$ z_{data} \sim \textrm{Normal}(x_{true} + y_{true},0.75)$$
#
# Where $x_{true}$ is a scalar, $y_{true}$ is a vector of length 2, and $z_{data}$ is a $2\times 20$ matrix.
#
# We can simulate this using Numpy:

# In[2]:

ndims = 2
nobs = 20

xtrue = np.random.normal(scale=2., size=1)
ytrue = np.random.normal(loc=np.exp(xtrue), scale=1, size=(ndims, 1))
zdata = np.random.normal(loc=xtrue + ytrue, scale=.75, size=(ndims, nobs))


# `zdata` is observed but `xtrue` and `ytrue` are not. Thus x and y are unknown, and we want to come up with posterior distributions for them.

# Build Model
# -----------
#
# Now we want to do inference assuming the following model:
#
# $$ x \sim \textrm{Normal}(0,1) $$
# $$ y \sim \textrm{Normal}(\textrm{exp}(x),2)$$
# $$ z \sim \textrm{Normal}(x + y,0.75)$$
#
# The aim here is to get posteriors over $x$ and $y$ given the data we have about $z$ (`zdata`).
#
# We create a new `Model` objects, and do operations within its context. The `with` lets PyMC know this model is the current model of interest.
#
# We construct new random variables with the constructor for its prior distribution such as `Normal` while within a model context (inside the `with`). When you make a random variable it is automatically added to the model. The constructor returns a Theano variable.
#
# Using the constructor may specify the name of the random variable, the parameters of a random variable's prior distribution, as well as the shape of the random variable. We can specify that a random variable is observed by specifying the data that was observed.

# In[3]:

with pm.Model() as model:
    x = pm.Normal('x', mu=0., sd=1)
    y = pm.Normal('y', mu=pm.exp(x), sd=2., shape=(ndims, 1)) # here, shape is telling us it's a vector rather than a scalar.
    z = pm.Normal('z', mu=x + y, sd=.75, observed=zdata) # shape is inferred from zdata


# A parenthetical note on the parameters for the normal. Variance is encoded as `tau`, indicating precision, which is simply inverse variance (so $\tau=\sigma^{-2}$ ). This is used because the gamma function is the conjugate prior for precision, and must be inverted to get variance. Encoding in terms of precision saves the inversion step in cases where variance is actually modeled using gamma as a prior.

# Fit Model
# ---------
# We need a starting point for our sampling. The `find_MAP` function finds the maximum a posteriori point (MAP), which is often a good choice for starting point. `find_MAP` uses an optimization algorithm (`scipy.optimize.fmin_l_bfgs_b`, or [BFGS](http://en.wikipedia.org/wiki/BFGS_method), by default) to find the local maximum of the log posterior.
#
# Note that this `with` construction is used again. Functions like `find_MAP` and `HamiltonianMC` need to have a model in their context. `with` activates the context of a particular model within its block.

# In[4]:

with model:
    start = pm.find_MAP()


# Points in parameter space are represented by dictionaries with parameter names as they keys and the value of the parameters as the values.

# In[5]:

print("MAP found:")
print("x:", start['x'])
print("y:", start['y'])

print("Compare with true values:")
print("ytrue", ytrue)
print("xtrue", xtrue)


# Out[5]:

#     MAP found:
#     x: 2.08679716543
#     y: [[ 7.93662462]
#      [ 9.10291792]]
#     Compare with true values:
#     ytrue [[ 8.09247902]
#      [ 9.01023203]]
#     xtrue [ 2.03525495]
#

# We will use NUTS to sample from the posterior as implemented by the `NUTS` step method class.

# In[6]:

with model:
    step = pm.NUTS()


# The `sample` function takes a number of steps to sample, a step method, a starting point. It returns a trace object which contains our samples.

# In[7]:

with model:
    trace = pm.sample(3000, step, start)


# Out[7]:

# To use more than one sampler, pass a list of step methods to `sample`.
#
# The trace object can be indexed by the variables in the model, returning an array with the first index being the sample index
# and the other indexes the shape of the parameter. Thus for this example:

# In[8]:

trace[y].shape


# Out[8]:

#     (3000, 2, 1)

# `traceplot` is a summary plotting function for a trace.

# In[9]:

pm.traceplot(trace);


# Out[9]:

# image file:

# ## PyMC Internals
#
# ### Model
#
# The `Model` class has very simple internals: just a list of unobserved variables (`Model.vars`) and a list of factors which go into computing the posterior density (`Model.factors`) (see model.py for more).
#
# A Python "`with model:`" block has `model` as the current model. Many functions, like `find_MAP` and `sample`, must be in such a block to work correctly by default. They look in the current context for a model to use. You may also explicitly specify the model for them to use. This allows us to treat the current model as an implicit parameter to these functions.
#
# ### Distribution Classes
#
# `Normal` and other distributions are actually `Distribution` subclasses. The constructors have different behavior depending on whether they are called with a name argument or not (string argument in 1st slot). This allows PyMC to have intuitive model specification syntax and still distinguish between random variables and distributions.
#
# When a `Distribution` constructor is called:
#
# * Without a name argument, it simply constructs a distribution object and returns it. It won't construct a random variable. This object has properties like `logp` (density function) and `expectation`.
# * With a name argument, it constructs a random variable using the distrubtion object as the prior distribution and inserts this random variable into the current model. Then the constructor returns the random variable.
#

# In[10]:

help(model)


# Out[10]:

#     Help on Model in module pymc.model object:
#
#     class Model(Context)
#      |  Base class for encapsulation of the variables and
#      |  likelihood factors of a model.
#      |
#      |  Method resolution order:
#      |      Model
#      |      Context
#      |      __builtin__.object
#      |
#      |  Methods defined here:
#      |
#      |  AddNamed(model, var)
#      |
#      |  Data(model, data, dist)
#      |
#      |  TransformedVar(model, name, dist, trans)
#      |
#      |  Var(model, name, dist)
#      |
#      |  __getitem__(self, key)
#      |
#      |  __init__(self)
#      |
#      |  d2logpc(model, vars=None)
#      |      Compiled log probability density hessian function
#      |
#      |  dlogpc(model, vars=None)
#      |      Compiled log probability density gradient function
#      |
#      |  ----------------------------------------------------------------------
#      |  Data descriptors defined here:
#      |
#      |  cont_vars
#      |      All the continuous variables in the model
#      |
#      |  logp
#      |      log-probability of the model
#      |
#      |      Parameters
#      |      ----------
#      |
#      |      model : Model
#      |
#      |      Returns
#      |      -------
#      |
#      |      logp : Theano scalar
#      |
#      |  logpc
#      |      Compiled log probability density function
#      |
#      |  test_point
#      |      Test point used to check that the model doesn't generate errors
#      |
#      |  ----------------------------------------------------------------------
#      |  Data and other attributes defined here:
#      |
#      |  contexts = []
#      |
#      |  ----------------------------------------------------------------------
#      |  Methods inherited from Context:
#      |
#      |  __enter__(self)
#      |
#      |  __exit__(self, typ, value, traceback)
#      |
#      |  ----------------------------------------------------------------------
#      |  Class methods inherited from Context:
#      |
#      |  get_context(cls) from __builtin__.type
#      |
#      |  get_contexts(cls) from __builtin__.type
#      |
#      |  ----------------------------------------------------------------------
#      |  Data descriptors inherited from Context:
#      |
#      |  __dict__
#      |      dictionary for instance variables (if defined)
#      |
#      |  __weakref__
#      |      list of weak references to the object (if defined)
#
#

# In[10]:




########NEW FILE########
__FILENAME__ = families
import numbers
from copy import copy

try:
    from statsmodels.genmod.families.family import (Gaussian, Binomial, Poisson)
except ImportError:
    Gaussian = None
    Binomial = None
    Poisson = None

from .links import *
import pymc

__all__ = ['Normal', 'T', 'Binomial', 'Poisson']

class Family(object):
    """Base class for Family of likelihood distribution and link functions.
    """
    priors = {}
    link = Identity

    def __init__(self, **kwargs):
        # Overwrite defaults
        for key, val in kwargs.items():
            if key == 'priors':
                self.priors = copy(self.priors)
                self.priors.update(val)
            else:
                setattr(self, key, val)

        # Instantiate link function
        self.link_func = self.link()

    def _get_priors(self, model=None):
        """Return prior distributions of the likelihood.

        Returns
        -------
        dict : mapping name -> pymc distribution
        """
        model = pymc.modelcontext(model)
        priors = {}
        for key, val in self.priors.items():
            if isinstance(val, numbers.Number):
                priors[key] = val
            else:
                priors[key] = model.Var(val[0], val[1])

        return priors

    def create_likelihood(self, y_est, y_data, model=None):
        """Create likelihood distribution of observed data.

        Parameters
        ----------
        y_est : theano.tensor
            Estimate of dependent variable
        y_data : array
            Observed dependent variable
        """
        priors = self._get_priors(model=model)
        # Wrap y_est in link function
        priors[self.parent] = self.link_func.theano(y_est)
        return self.likelihood('y', observed=y_data, **priors)

    def create_statsmodel_family(self):
        """Instantiate and return statsmodel family object.
        """
        if self.sm_family is None:
            return None
        else:
            return self.sm_family(self.link.sm)

    def __repr__(self):
        return """Family {klass}:
    Likelihood   : {likelihood}({parent})
    Priors       : {priors}
    Link function: {link}.""".format(klass=self.__class__, likelihood=self.likelihood.__name__, parent=self.parent, priors=self.priors, link=self.link)


class Normal(Family):
    sm_family = Gaussian
    link = Identity
    likelihood = pymc.Normal
    parent = 'mu'
    priors = {'sd': ('sigma', pymc.Uniform.dist(0, 100))}


class T(Family):
    sm_family = Gaussian
    link = Identity
    likelihood = pymc.T
    parent = 'mu'
    priors = {'lam': ('sigma', pymc.Uniform.dist(0, 100)),
              'nu': 1}


class Binomial(Family):
    link = Logit
    sm_family = Binomial
    likelihood = pymc.Bernoulli
    parent = 'p'


class Poisson(Family):
    link = Log
    sm_family = Poisson
    likelihood = pymc.Poisson
    parent = ''

########NEW FILE########
__FILENAME__ = glm
import numpy as np
from pymc import *
import patsy
import theano
import pandas as pd
from collections import defaultdict
from statsmodels.formula.api import glm as glm_sm
import statsmodels.api as sm
from pandas.tools.plotting import scatter_matrix

from . import links
from . import families

def linear_component(formula, data, priors=None,
                     intercept_prior=None,
                     regressor_prior=None,
                     init=True, init_vals=None, family=None,
                     model=None):
    """Create linear model according to patsy specification.

    Parameters
    ----------
    formula : str
        Patsy linear model descriptor.
    data : array
        Labeled array (e.g. pandas DataFrame, recarray).
    priors : dict
        Mapping prior name to prior distribution.
        E.g. {'Intercept': Normal.dist(mu=0, sd=1)}
    intercept_prior : pymc distribution
        Prior to use for the intercept.
        Default: Normal.dist(mu=0, tau=1.0E-12)
    regressor_prior : pymc distribution
        Prior to use for all regressor(s).
        Default: Normal.dist(mu=0, tau=1.0E-12)
    init : bool
        Whether to set the starting values via statsmodels
        Default: True
    init_vals : dict
        Set starting values externally: parameter -> value
        Default: None
    family : statsmodels.family
        Link function to pass to statsmodels (init has to be True).
    See `statsmodels.api.families`
        Default: identity

    Output
    ------
    (y_est, coeffs) : Estimate for y, list of coefficients

    Example
    -------
    # Logistic regression
    y_est, coeffs = glm('male ~ height + weight',
                        htwt_data,
                        family=glm.families.Binomial(links=glm.link.Logit))
    y_data = Bernoulli('y', y_est, observed=data.male)
    """
    if intercept_prior is None:
        intercept_prior = Normal.dist(mu=0, tau=1.0E-12)
    if regressor_prior is None:
        regressor_prior = Normal.dist(mu=0, tau=1.0E-12)

    if priors is None:
        priors = defaultdict(None)

    # Build patsy design matrix and get regressor names.
    _, dmatrix = patsy.dmatrices(formula, data)
    reg_names = dmatrix.design_info.column_names

    if init_vals is None and init:
        init_vals = glm_sm(formula, data, family=family).fit().params
    else:
        init_vals = defaultdict(lambda: None)

    # Create individual coefficients
    model = modelcontext(model)
    coeffs = []

    if reg_names[0] == 'Intercept':
        prior = priors.get('Intercept', intercept_prior)
        coeff = model.Var(reg_names.pop(0), prior)
        coeff.tag.test_value = init_vals['Intercept']
        coeffs.append(coeff)

    for reg_name in reg_names:
        prior = priors.get(reg_name, regressor_prior)
        coeff = model.Var(reg_name, prior)
        coeff.tag.test_value = init_vals[reg_name]
        coeffs.append(coeff)

    y_est = theano.dot(np.asarray(dmatrix), theano.tensor.stack(*coeffs)).reshape((1, -1))

    return y_est, coeffs


def glm(*args, **kwargs):
    """Create GLM after Patsy model specification string.

    Parameters
    ----------
    formula : str
        Patsy linear model descriptor.
    data : array
        Labeled array (e.g. pandas DataFrame, recarray).
    priors : dict
        Mapping prior name to prior distribution.
        E.g. {'Intercept': Normal.dist(mu=0, sd=1)}
    intercept_prior : pymc distribution
        Prior to use for the intercept.
        Default: Normal.dist(mu=0, tau=1.0E-12)
    regressor_prior : pymc distribution
        Prior to use for all regressor(s).
        Default: Normal.dist(mu=0, tau=1.0E-12)
    init : bool
        Whether initialize test values via statsmodels
        Default: True
    init_vals : dict
        Set starting values externally: parameter -> value
        Default: None
    find_MAP : bool
        Whether to call find_MAP on non-initialized nodes.
    family : statsmodels.family
        Distribution of likelihood, see pymc.glm.families
        (init has to be True).

    Output
    ------
    vars : List of created random variables (y_est, coefficients etc)

    Example
    -------
    # Logistic regression
    vars = glm('male ~ height + weight',
               data,
               family=glm.families.Binomial(link=glm.links.Logit))
    """

    model = modelcontext(kwargs.get('model'))

    family = kwargs.pop('family', families.Normal())

    call_find_map = kwargs.pop('find_MAP', True)
    formula = args[0]
    data = args[1]
    y_data = np.asarray(patsy.dmatrices(formula, data)[0]).T

    # Create GLM
    kwargs['family'] = family.create_statsmodel_family()

    y_est, coeffs = linear_component(*args, **kwargs)
    family.create_likelihood(y_est, y_data)

    # Find vars we have not initialized yet
    non_init_vars = set(model.vars).difference(set(coeffs))
    if len(non_init_vars) != 0 and call_find_map:
        start = find_MAP(vars=non_init_vars)

        for var in non_init_vars:
            var.tag.test_value = start[var.name]

    return [y_est] + coeffs + list(non_init_vars)


def plot_posterior_predictive(trace, eval=None, lm=None, samples=30, **kwargs):
    """Plot posterior predictive of a linear model.

    :Arguments:
        trace : <array>
            Array of posterior samples with columns
        eval : <array>
            Array over which to evaluate lm
        lm : function <default: linear function>
            Function mapping parameters at different points
            to their respective outputs.
            input: point, sample
            output: estimated value
        samples : int <default=30>
            How many posterior samples to draw.

    Additional keyword arguments are passed to pylab.plot().

    """
    import matplotlib.pyplot as plt

    if lm is None:
        lm = lambda x, sample: sample['Intercept'] + sample['x'] * x

    if eval is None:
        eval = np.linspace(0, 1, 100)

    # Set default plotting arguments
    if 'lw' not in kwargs and 'linewidth' not in kwargs:
        kwargs['lw'] = .2
    if 'c' not in kwargs and 'color' not in kwargs:
        kwargs['c'] = 'k'

    for rand_loc in np.random.randint(0, len(trace), samples):
        rand_sample = trace[rand_loc]
        plt.plot(eval, lm(eval, rand_sample), **kwargs)
        # Make sure to not plot label multiple times
        kwargs.pop('label', None)

    plt.title('Posterior predictive')

########NEW FILE########
__FILENAME__ = links
import theano.tensor

try:
    # Statsmodels is optional
    from statsmodels.genmod.families.links import (identity, logit, inverse_power, log)
except:
    identity, logit, inverse_power, log = [None] * 4

__all__ = ['Identity', 'Logit', 'Inverse', 'Log']

class LinkFunction(object):
    """Base class to define link functions.

    If initialization via statsmodels is desired, define sm.
    """

    def __init__(self, theano_link=None, sm_link=None):
        if theano_link is not None:
            self.theano = theano_link
        if sm_link is not None:
            self.sm = sm_link

class Identity(LinkFunction):
    theano = lambda self, x: x
    sm = identity

class Logit(LinkFunction):
    theano = theano.tensor.nnet.sigmoid
    sm = logit

class Inverse(LinkFunction):
    theano = theano.tensor.inv
    sm = inverse_power

class Log(LinkFunction):
    theano = theano.tensor.log
    sm = log

########NEW FILE########
__FILENAME__ = math
from __future__ import division
from theano.tensor import constant, flatten, zeros_like, ones_like, stack, concatenate, sum, prod, lt, gt, le, ge, eq, neq, switch, clip, where, and_, or_, abs_

from theano.tensor import exp, log, cos, sin, tan, cosh, sinh, tanh, sqr, sqrt, erf, erfinv, dot
from theano.tensor import maximum, minimum, sgn, ceil, floor

from theano.sandbox.linalg.ops import det, matrix_inverse, extract_diag, matrix_dot, trace
from theano.tensor.nnet import sigmoid
import theano

########NEW FILE########
__FILENAME__ = memoize
import functools

def memoize(obj):
    """
    An expensive memoizer that works with unhashables
    """
    cache = obj.cache = {}

    @functools.wraps(obj)
    def memoizer(*args, **kwargs):
        key = (hashable(args), hashable(kwargs))

        if key not in cache:
            cache[key] = obj(*args, **kwargs)

        return cache[key]
    return memoizer

def hashable(a):
    """
    Turn some unhashable objects into hashable ones.
    """
    if isinstance(a, dict):
        return hashable(a.items())
    try:
        return tuple(map(hashable, a))
    except:
        return a

########NEW FILE########
__FILENAME__ = model
from .point import *
from .vartypes import *

from theano import theano, tensor as t, function
from theano.tensor.var import TensorVariable

import numpy as np
from functools import wraps
from .theanof import *
from inspect import getargspec

from .memoize import memoize

__all__ = ['Model', 'Factor', 'compilef', 'fn', 'fastfn', 'modelcontext', 'Point', 'Deterministic', 'Potential']


class Context(object):
    """Functionality for objects that put themselves in a context using the `with` statement."""
    def __enter__(self):
        type(self).get_contexts().append(self)
        return self

    def __exit__(self, typ, value, traceback):
        type(self).get_contexts().pop()

    @classmethod
    def get_contexts(cls):
        if not hasattr(cls, "contexts"):
            cls.contexts = []

        return cls.contexts

    @classmethod
    def get_context(cls):
        """Return the deepest context on the stack."""
        try:
            return cls.get_contexts()[-1]
        except IndexError:
            raise TypeError("No context on context stack")


def modelcontext(model):
    """return the given model or try to find it in the context if there was none supplied."""
    if model is None:
        return Model.get_context()
    return model

class Factor(object):
    """Common functionality for objects with a log probability density associated with them."""
    @property
    def logp(self):
        """Compiled log probability density function"""
        return self.model.fn(self.logpt)

    @property
    def logp_elemwise(self):
        return self.model.fn(self.logp_elemwiset)

    def dlogp(self, vars=None):
        """Compiled log probability density gradient function"""
        return self.model.fn(gradient(self.logpt, vars))

    def d2logp(self, vars=None):
        """Compiled log probability density hessian function"""
        return self.model.fn(hessian(self.logpt, vars))

    @property
    def fastlogp(self):
        """Compiled log probability density function"""
        return self.model.fastfn(self.logpt)

    def fastdlogp(self, vars=None):
        """Compiled log probability density gradient function"""
        return self.model.fastfn(gradient(self.logpt, vars))

    def fastd2logp(self, vars=None):
        """Compiled log probability density hessian function"""
        return self.model.fastfn(hessian(self.logpt, vars))

    @property
    def logpt(self):
        """Theano scalar of log-probability of the model"""
        return t.sum(self.logp_elemwiset)

class Model(Context, Factor):
    """Encapsulates the variables and likelihood factors of a model."""

    def __init__(self):
        self.named_vars = {}
        self.free_RVs = []
        self.observed_RVs = []
        self.deterministics = []
        self.potentials = []
        self.model = self

    @property
    @memoize
    def logpt(self):
        """Theano scalar of log-probability of the model"""
        factors = [var.logpt for var in self.basic_RVs] + self.potentials
        return t.add(*map(t.sum, factors))

    @property
    def vars(self):
        """List of unobserved random variables the model is defined in terms of (which excludes deterministics)."""
        return self.free_RVs

    @property
    def basic_RVs(self):
        """List of random variables the model is defined in terms of (which excludes deterministics)."""
        return (self.free_RVs + self.observed_RVs)

    @property
    def unobserved_RVs(self):
        """List of all random variable, including deterministic ones."""
        return self.free_RVs + self.deterministics


    @property
    def test_point(self):
        """Test point used to check that the model doesn't generate errors"""
        return Point(((var, var.tag.test_value) for var in self.vars),
                     model=self)

    @property
    def cont_vars(self):
        """All the continuous variables in the model"""
        return list(typefilter(self.vars, continuous_types))

    def Var(self, name, dist, data=None):
        """Create and add (un)observed random variable to the model with an appropriate prior distribution.

        Parameters
        ----------
            name : str
            dist : distribution for the random variable
            data : arraylike (optional)
               if data is provided, the variable is observed. If None, the variable is unobserved.
        Returns
        -------
            FreeRV or ObservedRV
        """
        if data is None:
            var = FreeRV(name=name, distribution=dist, model=self)
            self.free_RVs.append(var)
        else:
            var = ObservedRV(name=name, data=data, distribution=dist, model=self)
            self.observed_RVs.append(var)
        self.add_random_variable(var)
        return var

    def TransformedVar(self, name, dist, trans):
        """Create random variable that after being transformed has the given prior.

        Parameters
        ----------
        name : str
        dist : Distribution
        trans : Transform

        Returns
        -------
        Random Variable
        """
        tvar = self.Var(trans.name + '_' + name, trans.apply(dist))

        return Deterministic(name, trans.backward(tvar)), tvar

    def add_random_variable(self, var):
        """Add a random variable to the named variables of the model."""
        self.named_vars[var.name] = var
        if not hasattr(self, var.name):
            setattr(self, var.name, var)

    def __getitem__(self, key):
        return self.named_vars[key]

    @memoize
    def makefn(self, outs, mode=None):
        """Compiles a Theano function which returns `outs` and takes the variable
        ancestors of `outs` as inputs.

        Parameters
        ----------
        outs : Theano variable or iterable of Theano variables
        mode : Theano compilation mode

        Returns
        -------
        Compiled Theano function"""
        return function(self.vars, outs,
                     allow_input_downcast=True,
                     on_unused_input='ignore',
                     mode=mode)

    def fn(self, outs, mode=None):
        """Compiles a Theano function which returns the values of `outs` and takes values of model
        vars as arguments.

        Parameters
        ----------
        outs : Theano variable or iterable of Theano variables
        mode : Theano compilation mode

        Returns
        -------
        Compiled Theano function"""
        return LoosePointFunc(self.makefn(outs, mode), self)

    def fastfn(self, outs, mode=None):
        """Compiles a Theano function which returns `outs` and takes values of model
        vars as a dict as an argument.

        Parameters
        ----------
        outs : Theano variable or iterable of Theano variables
        mode : Theano compilation mode

        Returns
        -------
        Compiled Theano function as point function."""
        return FastPointFunc(self.makefn(outs, mode))

def fn(outs, mode=None, model=None):
    """Compiles a Theano function which returns the values of `outs` and takes values of model
    vars as arguments.

    Parameters
    ----------
    outs : Theano variable or iterable of Theano variables
    mode : Theano compilation mode

    Returns
    -------
    Compiled Theano function"""
    model = modelcontext(model)
    return model.fn(outs,mode)

def fastfn(outs, mode=None, model=None):
    """Compiles a Theano function which returns `outs` and takes values of model
    vars as a dict as an argument.

    Parameters
    ----------
    outs : Theano variable or iterable of Theano variables
    mode : Theano compilation mode

    Returns
    -------
    Compiled Theano function as point function."""
    model = modelcontext(model)
    return model.fastfn(outs,mode)


def Point(*args, **kwargs):
    """Build a point. Uses same args as dict() does.
    Filters out variables not in the model. All keys are strings.

    Parameters
    ----------
        *args, **kwargs
            arguments to build a dict"""
    model = modelcontext(kwargs.pop('model', None))

    args = [a for a in args]
    try:
        d = dict(*args, **kwargs)
    except Exception as e:
        raise TypeError(
            "can't turn " + str(args) + " and " + str(kwargs) +
            " into a dict. " + str(e))

    varnames = list(map(str, model.vars))
    return dict((str(k), np.array(v))
                for (k, v) in d.items()
                if str(k) in varnames)

class FastPointFunc(object):
    """Wraps so a function so it takes a dict of arguments instead of arguments."""
    def __init__(self, f):
        self.f = f

    def __call__(self, state):
        return self.f(**state)

class LoosePointFunc(object):
    """Wraps so a function so it takes a dict of arguments instead of arguments
    but can still take arguments."""
    def __init__(self, f, model):
        self.f = f
        self.model = model

    def __call__(self, *args, **kwargs):
        point = Point(model=self.model, *args, **kwargs)
        return self.f(**point)

compilef = fastfn


class FreeRV(Factor, TensorVariable):
    """Unobserved random variable that a model is specified in terms of."""
    def __init__(self, type=None, owner=None, index=None, name=None, distribution=None, model=None):
        """
        Parameters
        ----------

        type : theano type (optional)
        owner : theano owner (optional)

        name : str
        distribution : Distribution
        model : Model"""
        if type is None:
            type = distribution.type
        super(FreeRV, self).__init__(type, owner, index, name)

        if distribution is not None:
            self.dshape = tuple(distribution.shape)
            self.dsize = int(np.prod(distribution.shape))
            self.distribution = distribution
            self.tag.test_value = np.ones(
                distribution.shape, distribution.dtype) * distribution.default()
            self.logp_elemwiset = distribution.logp(self)
            self.model = model

class ObservedRV(Factor):
    """Observed random variable that a model is specified in terms of."""
    def __init__(self, name, data, distribution, model):
        """
        Parameters
        ----------

        type : theano type (optional)
        owner : theano owner (optional)

        name : str
        distribution : Distribution
        model : Model
        """
        self.name = name
        data = getattr(data, 'values', data) #handle pandas
        args = as_iterargs(data)

        if len(args) > 1:
            params = getargspec(distribution.logp).args
            args = [t.constant(d, name=name + "_" + param)
                    for d,param in zip(args,params) ]
        else:
            args = [t.constant(args[0], name=name)]

        self.logp_elemwiset = distribution.logp(*args)
        self.model = model

def Deterministic(name, var, model=None):
    """Create a named deterministic variable

    Parameters
    ----------
        name : str
        var : theano variables
    Returns
    -------
        n : var but with name name"""
    var.name = name
    modelcontext(model).deterministics.append(var)
    modelcontext(model).add_random_variable(var)
    return var

def Potential(name, var, model=None):
    """Add an arbitrary factor potential to the model likelihood

    Parameters
    ----------
        name : str
        var : theano variables
    Returns
    -------
        var : var, with name attribute
    """

    var.name = name
    modelcontext(model).potentials.append(var)
    return var

def as_iterargs(data):
    if isinstance(data, tuple):
        return data
    if hasattr(data, 'columns'):  # data frames
        return [np.asarray(data[c]) for c in data.columns]
    else:
        return [data]

# theano stuff
theano.config.warn.sum_div_dimshuffle_bug = False
theano.config.compute_test_value = 'raise'

########NEW FILE########
__FILENAME__ = plots
import numpy as np
from scipy.stats import kde
from .stats import *

__all__ = ['traceplot', 'kdeplot', 'kde2plot', 'forestplot', 'autocorrplot']


def traceplot(trace, vars=None, figsize=None,
              lines=None, combined=False, grid=True):
    """Plot samples histograms and values

    Parameters
    ----------

    trace : result of MCMC run
    vars : list of variable names
        Variables to be plotted, if None all variable are plotted
    figsize : figure size tuple
        If None, size is (12, num of variables * 2) inch
    lines : dict
        Dictionary of variable name / value  to be overplotted as vertical
        lines to the posteriors and horizontal lines on sample values
        e.g. mean of posteriors, true values of a simulation
    combined : bool
        Flag for combining multiple chains into a single chain. If False
        (default), chains will be plotted separately.
    grid : bool
        Flag for adding gridlines to histogram. Defaults to True.

    Returns
    -------

    fig : figure object

    """
    import matplotlib.pyplot as plt
    if vars is None:
        vars = trace.varnames

    n = len(vars)

    if figsize is None:
        figsize = (12, n*2)

    fig, ax = plt.subplots(n, 2, squeeze=False, figsize=figsize)

    for i, v in enumerate(vars):
        for d in trace.get_values(v, combine=combined, squeeze=False):
            d = np.squeeze(d)
            d = make_2d(d)
            if d.dtype.kind == 'i':
                histplot_op(ax[i, 0], d)
            else:
                kdeplot_op(ax[i, 0], d)
            ax[i, 0].set_title(str(v))
            ax[i, 0].grid(grid)
            ax[i, 1].set_title(str(v))
            ax[i, 1].plot(d, alpha=.35)

            ax[i, 0].set_ylabel("Frequency")
            ax[i, 1].set_ylabel("Sample value")

            if lines:
                try:
                    ax[i, 0].axvline(x=lines[v], color="r", lw=1.5)
                    ax[i, 1].axhline(y=lines[v], color="r", lw=1.5, alpha=.35)
                except KeyError:
                    pass

    plt.tight_layout()
    return fig

def histplot_op(ax, data):
    for i in range(data.shape[1]):
        d = data[:, i]

        mind = np.min(d)
        maxd = np.max(d)
        ax.hist(d, bins=range(mind, maxd + 2), align='left')
        ax.set_xlim(mind - .5, maxd + .5)

def kdeplot_op(ax, data):
    for i in range(data.shape[1]):
        d = data[:, i]
        density = kde.gaussian_kde(d)
        l = np.min(d)
        u = np.max(d)
        x = np.linspace(0, 1, 100) * (u - l) + l

        ax.plot(x, density(x))

def make_2d(a): 
    """Ravel the dimensions after the first.
    """
    a = np.atleast_2d(a.T).T
    #flatten out dimensions beyond the first
    n = a.shape[0]
    newshape = np.product(a.shape[1:]).astype(int)
    a = a.reshape((n, newshape), order='F')
    return a


def kde2plot_op(ax, x, y, grid=200):
    xmin = x.min()
    xmax = x.max()
    ymin = y.min()
    ymax = y.max()

    grid = grid * 1j
    X, Y = np.mgrid[xmin:xmax:grid, ymin:ymax:grid]
    positions = np.vstack([X.ravel(), Y.ravel()])
    values = np.vstack([x, y])
    kernel = kde.gaussian_kde(values)
    Z = np.reshape(kernel(positions).T, X.shape)

    ax.imshow(np.rot90(Z), cmap=plt.cm.gist_earth_r,
              extent=[xmin, xmax, ymin, ymax])


def kdeplot(data):
    f, ax = subplots(1, 1, squeeze=True)
    kdeplot_op(ax, data)
    return f


def kde2plot(x, y, grid=200):
    f, ax = subplots(1, 1, squeeze=True)
    kde2plot_op(ax, x, y, grid)
    return f


def autocorrplot(trace, vars=None, fontmap=None, max_lag=100):
    """Bar plot of the autocorrelation function for a trace"""
    import matplotlib.pyplot as plt
    if fontmap is None:
        fontmap = {1: 10, 2: 8, 3: 6, 4: 5, 5: 4}

    if vars is None:
        vars = trace.varnames
    else:
        vars = [str(var) for var in vars]

    chains = trace.nchains

    f, ax = plt.subplots(len(vars), chains, squeeze=False)

    max_lag = min(len(trace) - 1, max_lag)

    for i, v in enumerate(vars):
        for j in range(chains):
            d = np.squeeze(trace.get_values(v, chains=[j]))

            ax[i, j].acorr(d, detrend=plt.mlab.detrend_mean, maxlags=max_lag)

            if not j:
                ax[i, j].set_ylabel("correlation")
            ax[i, j].set_xlabel("lag")

            if chains > 1:
                ax[i, j].set_title("chain {0}".format(j+1))

    # Smaller tick labels
    tlabels = plt.gca().get_xticklabels()
    plt.setp(tlabels, 'fontsize', fontmap[1])

    tlabels = plt.gca().get_yticklabels()
    plt.setp(tlabels, 'fontsize', fontmap[1])


def var_str(name, shape):
    """Return a sequence of strings naming the element of the tallyable object.
    This is a support function for forestplot.

    :Example:
    >>> var_str('theta', (4,))
    ['theta[1]', 'theta[2]', 'theta[3]', 'theta[4]']

    """

    size = np.prod(shape)
    ind = (np.indices(shape) + 1).reshape(-1, size)
    names = ['[' + ','.join(map(str, i)) + ']' for i in zip(*ind)]
    # if len(name)>12:
    #     name = '\n'.join(name.split('_'))
    #     name += '\n'
    names[0] = '%s %s' % (name, names[0])
    return names


def forestplot(trace_obj, vars=None, alpha=0.05, quartiles=True, rhat=True,
               main=None, xtitle=None, xrange=None, ylabels=None,
               chain_spacing=0.05, vline=0):
    """ Forest plot (model summary plot)

    Generates a "forest plot" of 100*(1-alpha)% credible intervals for either
    the set of variables in a given model, or a specified set of nodes.

    :Arguments:
        trace_obj: NpTrace or MultiTrace object
            Trace(s) from an MCMC sample.

        vars: list
            List of variables to plot (defaults to None, which results in all
            variables plotted).

        alpha (optional): float
            Alpha value for (1-alpha)*100% credible intervals (defaults to
            0.05).

        quartiles (optional): bool
            Flag for plotting the interquartile range, in addition to the
            (1-alpha)*100% intervals (defaults to True).

        rhat (optional): bool
            Flag for plotting Gelman-Rubin statistics. Requires 2 or more
            chains (defaults to True).

        main (optional): string
            Title for main plot. Passing False results in titles being
            suppressed; passing None (default) results in default titles.

        xtitle (optional): string
            Label for x-axis. Defaults to no label

        xrange (optional): list or tuple
            Range for x-axis. Defaults to matplotlib's best guess.

        ylabels (optional): list
            User-defined labels for each variable. If not provided, the node
            __name__ attributes are used.

        chain_spacing (optional): float
            Plot spacing between chains (defaults to 0.05).

        vline (optional): numeric
            Location of vertical reference line (defaults to 0).

    """
    import matplotlib.pyplot as plt
    try:
        import matplotlib.gridspec as gridspec
    except ImportError:
        gridspec = None

    if not gridspec:
        print_('\nYour installation of matplotlib is not recent enough to ' +
               'support summary_plot; this function is disabled until ' +
               'matplotlib is updated.')
        return

    # Quantiles to be calculated
    qlist = [100 * alpha / 2, 50, 100 * (1 - alpha / 2)]
    if quartiles:
        qlist = [100 * alpha / 2, 25, 50, 75, 100 * (1 - alpha / 2)]

    # Range for x-axis
    plotrange = None

    # Number of chains
    chains = None

    # Gridspec
    gs = None

    # Subplots
    interval_plot = None
    rhat_plot = None

    nchains = trace_obj.nchains
    if nchains > 1:
        from .diagnostics import gelman_rubin

        R = gelman_rubin(trace_obj)
        if vars is not None:
            R = {v: R[v] for v in vars}
    else:
        # Can't calculate Gelman-Rubin with a single trace
        rhat = False

    if vars is None:
        vars = trace_obj.varnames

    # Empty list for y-axis labels
    labels = []

    if gs is None:
        # Initialize plot
        if rhat and nchains > 1:
            gs = gridspec.GridSpec(1, 2, width_ratios=[3, 1])

        else:

            gs = gridspec.GridSpec(1, 1)

        # Subplot for confidence intervals
        interval_plot = plt.subplot(gs[0])


    trace_quantiles = quantiles(trace_obj, qlist, squeeze=False)
    hpd_intervals = hpd(trace_obj, alpha, squeeze=False)

    for j, chain in enumerate(trace_obj.chains):
        # Counter for current variable
        var = 1
        for varname in vars:
            var_quantiles = trace_quantiles[chain][varname]

            quants = list(var_quantiles.values())
            var_hpd = hpd_intervals[chain][varname].T

            # Substitute HPD interval for quantile
            quants[0] = var_hpd[0].T
            quants[-1] = var_hpd[1].T

            # Ensure x-axis contains range of current interval
            if plotrange:
                plotrange = [min(
                             plotrange[0],
                             np.min(quants)),
                             max(plotrange[1],
                                 np.max(quants))]
            else:
                plotrange = [np.min(quants), np.max(quants)]

            # Number of elements in current variable
            value = trace_obj.get_values(varname, chains=[chain])[0]
            k = np.size(value)

            # Append variable name(s) to list
            if not j:
                if k > 1:
                    names = var_str(varname, np.shape(value))
                    labels += names
                else:
                    labels.append(varname)
                    # labels.append('\n'.join(varname.split('_')))

            # Add spacing for each chain, if more than one
            e = [0] + [(chain_spacing * ((i + 2) / 2)) *
                       (-1) ** i for i in range(nchains - 1)]

            # Deal with multivariate nodes
            if k > 1:

                for i, q in enumerate(np.transpose(quants).squeeze()):

                    # Y coordinate with jitter
                    y = -(var + i) + e[j]

                    if quartiles:
                        # Plot median
                        plt.plot(q[2], y, 'bo', markersize=4)
                        # Plot quartile interval
                        plt.errorbar(
                            x=(q[1],
                                q[3]),
                            y=(y,
                                y),
                            linewidth=2,
                            color='b')

                    else:
                        # Plot median
                        plt.plot(q[1], y, 'bo', markersize=4)

                    # Plot outer interval
                    plt.errorbar(
                        x=(q[0],
                            q[-1]),
                        y=(y,
                            y),
                        linewidth=1,
                        color='b')

            else:

                # Y coordinate with jitter
                y = -var + e[j]

                if quartiles:
                    # Plot median
                    plt.plot(quants[2], y, 'bo', markersize=4)
                    # Plot quartile interval
                    plt.errorbar(
                        x=(quants[1],
                            quants[3]),
                        y=(y,
                            y),
                        linewidth=2,
                        color='b')
                else:
                    # Plot median
                    plt.plot(quants[1], y, 'bo', markersize=4)

                # Plot outer interval
                plt.errorbar(
                    x=(quants[0],
                        quants[-1]),
                    y=(y,
                        y),
                    linewidth=1,
                    color='b')

            # Increment index
            var += k

    labels = ylabels or labels

    # Update margins
    left_margin = np.max([len(x) for x in labels]) * 0.015
    gs.update(left=left_margin, right=0.95, top=0.9, bottom=0.05)

    # Define range of y-axis
    plt.ylim(-var + 0.5, -0.5)

    datarange = plotrange[1] - plotrange[0]
    plt.xlim(plotrange[0] - 0.05 * datarange, plotrange[1] + 0.05 * datarange)

    # Add variable labels
    plt.yticks([-(l + 1) for l in range(len(labels))], labels)

    # Add title
    if main is not False:
        plot_title = main or str(int((
            1 - alpha) * 100)) + "% Credible Intervals"
        plt.title(plot_title)

    # Add x-axis label
    if xtitle is not None:
        plt.xlabel(xtitle)

    # Constrain to specified range
    if xrange is not None:
        plt.xlim(*xrange)

    # Remove ticklines on y-axes
    for ticks in interval_plot.yaxis.get_major_ticks():
        ticks.tick1On = False
        ticks.tick2On = False

    for loc, spine in interval_plot.spines.items():
        if loc in ['bottom', 'top']:
            pass
            # spine.set_position(('outward',10)) # outward by 10 points
        elif loc in ['left', 'right']:
            spine.set_color('none')  # don't draw spine

    # Reference line
    plt.axvline(vline, color='k', linestyle='--')

    # Genenerate Gelman-Rubin plot
    if rhat and nchains > 1:

        # If there are multiple chains, calculate R-hat
        rhat_plot = plt.subplot(gs[1])

        if main is not False:
            plt.title("R-hat")

        # Set x range
        plt.xlim(0.9, 2.1)

        # X axis labels
        plt.xticks((1.0, 1.5, 2.0), ("1", "1.5", "2+"))
        plt.yticks([-(l + 1) for l in range(len(labels))], "")

        i = 1
        for varname in vars:

            chain = trace_obj.chains[0]
            value = trace_obj.get_values(varname, chains=[chain])[0]
            k = np.size(value)

            if k > 1:
                plt.plot([min(r, 2) for r in R[varname]], [-(j + i)
                     for j in range(k)], 'bo', markersize=4)
            else:
                plt.plot(min(R[varname], 2), -i, 'bo', markersize=4)

            i += k

        # Define range of y-axis
        plt.ylim(-i + 0.5, -0.5)

        # Remove ticklines on y-axes
        for ticks in rhat_plot.yaxis.get_major_ticks():
            ticks.tick1On = False
            ticks.tick2On = False

        for loc, spine in rhat_plot.spines.items():
            if loc in ['bottom', 'top']:
                pass
                # spine.set_position(('outward',10)) # outward by 10 points
            elif loc in ['left', 'right']:
                spine.set_color('none')  # don't draw spine

    return gs

########NEW FILE########
__FILENAME__ = point

########NEW FILE########
__FILENAME__ = progressbar
"""
A simple progress bar to monitor MCMC sampling progress.
Modified from original code by Corey Goldberg (2010)
"""

from __future__ import print_function

import sys
import time
import uuid
try:
    from IPython.core.display import HTML, Javascript, display
except ImportError:
    pass

__all__ = ['progress_bar']


class ProgressBar(object):
    def __init__(self, iterations, animation_interval=.5):
        self.iterations = iterations
        self.start = time.time()
        self.last = 0
        self.animation_interval = animation_interval

    def percentage(self, i):
        return 100 * i / float(self.iterations)

    def update(self, i):
        elapsed = time.time() - self.start
        i += 1

        if elapsed - self.last > self.animation_interval:
            self.animate(i + 1, elapsed)
            self.last = elapsed
        elif i == self.iterations:
            self.animate(i, elapsed)


class TextProgressBar(ProgressBar):
    def __init__(self, iterations, printer):
        self.fill_char = '-'
        self.width = 40
        self.printer = printer

        super(TextProgressBar, self).__init__(iterations)
        self.update(0)

    def animate(self, i, elapsed):
        self.printer(self.progbar(i, elapsed))

    def progbar(self, i, elapsed):
        bar = self.bar(self.percentage(i))
        return "[%s] %i of %i complete in %.1f sec" % (bar, i, self.iterations, round(elapsed, 1))

    def bar(self, percent):
        all_full = self.width - 2
        num_hashes = int(percent / 100 * all_full)

        bar = self.fill_char * num_hashes + ' ' * (all_full - num_hashes)

        info = '%d%%' % percent
        loc = (len(bar) - len(info)) // 2
        return replace_at(bar, info, loc, loc + len(info))


def replace_at(str, new, start, stop):
    return str[:start] + new + str[stop:]


def consoleprint(s):
    if sys.platform.lower().startswith('win'):
        print(s, '\r', end='')
    else:
        print(s)


def ipythonprint(s):
    print('\r', s, end='')
    sys.stdout.flush()


class IPythonNotebookPB(ProgressBar):
    def __init__(self, iterations):
        self.divid = str(uuid.uuid4())
        self.sec_id = str(uuid.uuid4())

        pb = HTML(
            """
            <div style="float: left; border: 1px solid black; width:500px">
              <div id="%s" style="background-color:blue; width:0%%">&nbsp;</div>
            </div>
            <label id="%s" style="padding-left: 10px;" text = ""/>
            """ % (self.divid, self.sec_id))
        display(pb)

        super(IPythonNotebookPB, self).__init__(iterations)

    def animate(self, i, elapsed):
        percentage = int(self.fraction(i))

        display(Javascript(
            "$('div#%s').width('%i%%')" % (self.divid, percentage)))
        display(Javascript("$('label#%s').text('%i%% in %.1f sec')" %
                (self.sec_id, fraction, round(elapsed, 1))))


def run_from_ipython():
        try:
            __IPYTHON__
            return True
        except NameError:
            return False


def progress_bar(iters):
    if run_from_ipython():
        if None:
            return NotebookProgressBar(iters)
        else:
            return TextProgressBar(iters, ipythonprint)
    else:
        return TextProgressBar(iters, consoleprint)

########NEW FILE########
__FILENAME__ = sampling
from .point import *
from pymc import backends
from pymc.backends.base import merge_traces, BaseTrace, MultiTrace
from pymc.backends.ndarray import NDArray
import multiprocessing as mp
from time import time
from .core import *
from . import step_methods
from .progressbar import progress_bar
from numpy.random import seed

__all__ = ['sample', 'iter_sample']


def sample(draws, step, start=None, trace=None, chain=0, njobs=1, tune=None,
           progressbar=True, model=None, random_seed=None):
    """
    Draw a number of samples using the given step method.
    Multiple step methods supported via compound step method
    returns the amount of time taken.

    Parameters
    ----------

    draws : int
        The number of samples to draw
    step : function
        A step function
    start : dict
        Starting point in parameter space (or partial point)
        Defaults to trace.point(-1)) if there is a trace provided and
        model.test_point if not (defaults to empty dict)
    trace : backend, list, or MultiTrace
        This should be a backend instance, a list of variables to track,
        or a MultiTrace object with past values. If a MultiTrace object
        is given, it must contain samples for the chain number `chain`.
        If None or a list of variables, the NDArray backend is used.
        Passing either "text" or "sqlite" is taken as a shortcut to set
        up the corresponding backend (with "mcmc" used as the base
        name).
    chain : int
        Chain number used to store sample in backend. If `njobs` is
        greater than one, chain numbers will start here.
    njobs : int
        Number of parallel jobs to start. If None, set to number of cpus
        in the system - 2.
    tune : int
        Number of iterations to tune, if applicable (defaults to None)
    progressbar : bool
        Flag for progress bar
    model : Model (optional if in `with` context)
    random_seed : int or list of ints
        A list is accepted if more if `njobs` is greater than one.

    Returns
    -------
    MultiTrace object with access to sampling values
    """
    if njobs is None:
        njobs = max(mp.cpu_count() - 2, 1)
    if njobs > 1:
        try:
            if not len(random_seed) == njobs:
                random_seeds = [random_seed] * njobs
            else:
                random_seeds = random_seed
        except TypeError:  # None, int
            random_seeds = [random_seed] * njobs

        chains = list(range(chain, chain + njobs))

        pbars = [progressbar] + [False] * (njobs - 1)

        argset = zip([draws] * njobs,
                     [step] * njobs,
                     [start] * njobs,
                     [trace] * njobs,
                     chains,
                     [tune] * njobs,
                     pbars,
                     [model] * njobs,
                     random_seeds)
        sample_func = _mp_sample
        sample_args = [njobs, argset]
    else:
        sample_func = _sample
        sample_args = [draws, step, start, trace, chain,
                       tune, progressbar, model, random_seed]
    return sample_func(*sample_args)


def _sample(draws, step, start=None, trace=None, chain=0, tune=None,
            progressbar=True, model=None, random_seed=None):
    sampling = _iter_sample(draws, step, start, trace, chain,
                            tune, model, random_seed)
    progress = progress_bar(draws)
    try:
        for i, trace in enumerate(sampling):
            if progressbar:
                progress.update(i)
    except KeyboardInterrupt:
        trace.close()
    return MultiTrace([trace])


def iter_sample(draws, step, start=None, trace=None, chain=0, tune=None,
                model=None, random_seed=None):
    """
    Generator that returns a trace on each iteration using the given
    step method.  Multiple step methods supported via compound step
    method returns the amount of time taken.


    Parameters
    ----------

    draws : int
        The number of samples to draw
    step : function
        A step function
    start : dict
        Starting point in parameter space (or partial point)
        Defaults to trace.point(-1)) if there is a trace provided and
        model.test_point if not (defaults to empty dict)
    trace : backend, list, or MultiTrace
        This should be a backend instance, a list of variables to track,
        or a MultiTrace object with past values. If a MultiTrace object
        is given, it must contain samples for the chain number `chain`.
        If None or a list of variables, the NDArray backend is used.
    chain : int
        Chain number used to store sample in backend. If `njobs` is
        greater than one, chain numbers will start here.
    tune : int
        Number of iterations to tune, if applicable (defaults to None)
    model : Model (optional if in `with` context)
    random_seed : int or list of ints
        A list is accepted if more if `njobs` is greater than one.

    Example
    -------

    for trace in iter_sample(500, step):
        ...
    """
    sampling = _iter_sample(draws, step, start, trace, chain, tune,
                            model, random_seed)
    for i, trace in enumerate(sampling):
        yield trace[:i + 1]


def _iter_sample(draws, step, start=None, trace=None, chain=0, tune=None,
                 model=None, random_seed=None):
    model = modelcontext(model)
    draws = int(draws)
    seed(random_seed)
    if draws < 1:
        raise ValueError('Argument `draws` should be above 0.')

    if start is None:
        start = {}

    trace = _choose_backend(trace, chain, model=model)

    if len(trace) > 0:
        _soft_update(start, trace.point(-1))
    else:
        _soft_update(start, model.test_point)

    try:
        step = step_methods.CompoundStep(step)
    except TypeError:
        pass

    point = Point(start, model=model)

    trace.setup(draws, chain)
    for i in range(draws):
        if i == tune:
            step = stop_tuning(step)
        point = step.step(point)
        trace.record(point)
        yield trace
    else:
        trace.close()


def _choose_backend(trace, chain, shortcuts=None, **kwds):
    if isinstance(trace, BaseTrace):
        return trace
    if isinstance(trace, MultiTrace):
        return trace._traces[chain]
    if trace is None:
        return NDArray(**kwds)

    if shortcuts is None:
        shortcuts = backends._shortcuts

    try:
        backend = shortcuts[trace]['backend']
        name = shortcuts[trace]['name']
        return backend(name, **kwds)
    except TypeError:
        return NDArray(vars=trace, **kwds)
    except KeyError:
        raise ValueError('Argument `trace` is invalid.')


def _mp_sample(njobs, args):
    p = mp.Pool(njobs)
    traces = p.map(argsample, args)
    p.close()
    return merge_traces(traces)


def stop_tuning(step):
    """ stop tuning the current step method """

    if hasattr(step, 'tune'):
        step.tune = False

    elif hasattr(step, 'methods'):
        step.methods = [stop_tuning(s) for s in step.methods]

    return step


def argsample(args):
    """ defined at top level so it can be pickled"""
    return _sample(*args)


def _soft_update(a, b):
    """As opposed to dict.update, don't overwrite keys if present.
    """
    a.update({k: v for k, v in b.items() if k not in a})

########NEW FILE########
__FILENAME__ = stats
"""Utility functions for PyMC"""

import numpy as np
import itertools

__all__ = ['autocorr', 'autocov', 'hpd', 'quantiles', 'mc_error', 'summary']

def statfunc(f):
    """
    Decorator for statistical utility function to automatically
    extract the trace array from whatever object is passed.
    """

    def wrapped_f(pymc_obj, *args, **kwargs):
        try:
            vars = kwargs.pop('vars',  pymc_obj.varnames)
            chains = kwargs.pop('chains', pymc_obj.chains)
        except AttributeError:
            # If fails, assume that raw data was passed.
            return f(pymc_obj, *args, **kwargs)

        burn = kwargs.pop('burn', 0)
        thin = kwargs.pop('thin', 1)
        combine = kwargs.pop('combine', False)
        ## Remove outer level chain keys if only one chain)
        squeeze = kwargs.pop('squeeze', True)

        results = {chain: {} for chain in chains}
        for var in vars:
            samples = pymc_obj.get_values(var, chains=chains, burn=burn,
                                          thin=thin, combine=combine,
                                          squeeze=False)
            for chain, data in zip(chains, samples):
                results[chain][var] = f(np.squeeze(data), *args, **kwargs)

        if squeeze and (len(chains) == 1 or combine):
            results = results[chains[0]]
        return results

    wrapped_f.__doc__ = f.__doc__
    wrapped_f.__name__ = f.__name__

    return wrapped_f

@statfunc
def autocorr(x, lag=1):
    """Sample autocorrelation at specified lag.
    The autocorrelation is the correlation of x_i with x_{i+lag}.
    """

    S = autocov(x, lag)
    return S[0, 1]/np.sqrt(np.prod(np.diag(S)))

@statfunc
def autocov(x, lag=1):
    """
    Sample autocovariance at specified lag.
    The autocovariance is a 2x2 matrix with the variances of
    x[:-lag] and x[lag:] in the diagonal and the autocovariance
    on the off-diagonal.
    """
    x = np.asarray(x)

    if not lag: return 1
    if lag < 0:
        raise ValueError("Autocovariance lag must be a positive integer")
    return np.cov(x[:-lag], x[lag:], bias=1)

def make_indices(dimensions):
    # Generates complete set of indices for given dimensions

    level = len(dimensions)

    if level == 1: return list(range(dimensions[0]))

    indices = [[]]

    while level:

        _indices = []

        for j in range(dimensions[level-1]):

            _indices += [[j]+i for i in indices]

        indices = _indices

        level -= 1

    try:
        return [tuple(i) for i in indices]
    except TypeError:
        return indices

def calc_min_interval(x, alpha):
    """Internal method to determine the minimum interval of
    a given width

    Assumes that x is sorted numpy array.
    """

    n = len(x)
    cred_mass = 1.0-alpha

    interval_idx_inc = int(np.floor(cred_mass*n))
    n_intervals = n - interval_idx_inc
    interval_width = x[interval_idx_inc:] - x[:n_intervals]

    if len(interval_width) == 0:
        raise ValueError('Too few elements for interval calculation')

    min_idx = np.argmin(interval_width)
    hdi_min = x[min_idx]
    hdi_max = x[min_idx+interval_idx_inc]
    return hdi_min, hdi_max

@statfunc
def hpd(x, alpha=0.05):
    """Calculate highest posterior density (HPD) of array for given alpha. The HPD is the
    minimum width Bayesian credible interval (BCI).

    :Arguments:
      x : Numpy array
          An array containing MCMC samples
      alpha : float
          Desired probability of type I error (defaults to 0.05)

    """

    # Make a copy of trace
    x = x.copy()

    # For multivariate node
    if x.ndim > 1:

        # Transpose first, then sort
        tx = np.transpose(x, list(range(x.ndim))[1:]+[0])
        dims = np.shape(tx)

        # Container list for intervals
        intervals = np.resize(0.0, dims[:-1]+(2,))

        for index in make_indices(dims[:-1]):

            try:
                index = tuple(index)
            except TypeError:
                pass

            # Sort trace
            sx = np.sort(tx[index])

            # Append to list
            intervals[index] = calc_min_interval(sx, alpha)

        # Transpose back before returning
        return np.array(intervals)

    else:
        # Sort univariate node
        sx = np.sort(x)

        return np.array(calc_min_interval(sx, alpha))

@statfunc
def mc_error(x, batches=5):
    """
    Calculates the simulation standard error, accounting for non-independent
    samples. The trace is divided into batches, and the standard deviation of
    the batch means is calculated.

    :Arguments:
      x : Numpy array
          An array containing MCMC samples
      batches : integer
          Number of batchas
    """

    if x.ndim > 1:

        dims = np.shape(x)
        #ttrace = np.transpose(np.reshape(trace, (dims[0], sum(dims[1:]))))
        trace = np.transpose([t.ravel() for t in x])

        return np.reshape([mc_error(t, batches) for t in trace], dims[1:])

    else:
        if batches == 1: return np.std(x)/np.sqrt(len(x))

        try:
            batched_traces = np.resize(x, (batches, len(x)/batches))
        except ValueError:
            # If batches do not divide evenly, trim excess samples
            resid = len(x) % batches
            batched_traces = np.resize(x[:-resid], (batches, len(x)/batches))

        means = np.mean(batched_traces, 1)

        return np.std(means)/np.sqrt(batches)

@statfunc
def quantiles(x, qlist=(2.5, 25, 50, 75, 97.5)):
    """Returns a dictionary of requested quantiles from array

    :Arguments:
      x : Numpy array
          An array containing MCMC samples
      qlist : tuple or list
          A list of desired quantiles (defaults to (2.5, 25, 50, 75, 97.5))

    """

    # Make a copy of trace
    x = x.copy()

    # For multivariate node
    if x.ndim > 1:
        # Transpose first, then sort, then transpose back
        sx = np.sort(x.T).T
    else:
        # Sort univariate node
        sx = np.sort(x)

    try:
        # Generate specified quantiles
        quants = [sx[int(len(sx)*q/100.0)] for q in qlist]

        return dict(zip(qlist, quants))

    except IndexError:
        print("Too few elements for quantile calculation")


def summary(trace, vars=None, alpha=0.05, start=0, batches=100, roundto=3):
    """
    Generate a pretty-printed summary of the node.

    :Parameters:
    trace : Trace object
      Trace containing MCMC sample

    vars : list of strings
      List of variables to summarize. Defaults to None, which results
      in all variables summarized.

    alpha : float
      The alpha level for generating posterior intervals. Defaults to
      0.05.

    start : int
      The starting index from which to summarize (each) chain. Defaults
      to zero.

    batches : int
      Batch size for calculating standard deviation for non-independent
      samples. Defaults to 100.

    roundto : int
      The number of digits to round posterior statistics.

    """
    if vars is None:
        vars = trace.varnames

    stat_summ = _StatSummary(roundto, batches, alpha)
    pq_summ = _PosteriorQuantileSummary(roundto, alpha)

    for var in vars:
        # Extract sampled values
        sample = trace.get_values(var, burn=start, combine=True)

        print('\n%s:' % var)
        print(' ')

        stat_summ.print_output(sample)
        pq_summ.print_output(sample)


class _Summary(object):
    """Base class for summary output"""
    def __init__(self, roundto):
        self.roundto = roundto
        self.header_lines = None
        self.leader = '  '
        self.spaces = None
        self.width = None

    def print_output(self, sample):
        print('\n'.join(list(self._get_lines(sample))) + '\n')

    def _get_lines(self, sample):
        for line in self.header_lines:
            yield self.leader + line
        summary_lines = self._calculate_values(sample)
        for line in self._create_value_output(summary_lines):
            yield self.leader + line

    def _create_value_output(self, lines):
        for values in lines:
            try:
                self._format_values(values)
                yield self.value_line.format(pad=self.spaces, **values).strip()
            except AttributeError:
            # This is a key for the leading indices, not a normal row.
            # `values` will be an empty tuple unless it is 2d or above.
                if values:
                    leading_idxs = [str(v) for v in values]
                    numpy_idx = '[{}, :]'.format(', '.join(leading_idxs))
                    yield self._create_idx_row(numpy_idx)
                else:
                    yield ''

    def _calculate_values(self, sample):
        raise NotImplementedError

    def _format_values(self, summary_values):
        for key, val in summary_values.items():
            summary_values[key] = '{:.{ndec}f}'.format(
                float(val), ndec=self.roundto)

    def _create_idx_row(self, value):
        return '{:.^{}}'.format(value, self.width)


class _StatSummary(_Summary):
    def __init__(self, roundto, batches, alpha):
        super(_StatSummary, self).__init__(roundto)
        spaces = 17
        hpd_name = '{}% HPD interval'.format(int(100 * (1 - alpha)))
        value_line = '{mean:<{pad}}{sd:<{pad}}{mce:<{pad}}{hpd:<{pad}}'
        header = value_line.format(mean='Mean', sd='SD', mce='MC Error',
                                  hpd=hpd_name, pad=spaces).strip()
        self.width = len(header)
        hline = '-' * self.width

        self.header_lines = [header, hline]
        self.spaces = spaces
        self.value_line = value_line
        self.batches = batches
        self.alpha = alpha

    def _calculate_values(self, sample):
        return _calculate_stats(sample, self.batches, self.alpha)

    def _format_values(self, summary_values):
        roundto = self.roundto
        for key, val in summary_values.items():
            if key == 'hpd':
                summary_values[key] = '[{:.{ndec}f}, {:.{ndec}f}]'.format(
                    *val, ndec=roundto)
            else:
                summary_values[key] = '{:.{ndec}f}'.format(
                    float(val), ndec=roundto)


class _PosteriorQuantileSummary(_Summary):
    def __init__(self, roundto, alpha):
        super(_PosteriorQuantileSummary, self).__init__(roundto)
        spaces = 15
        title = 'Posterior quantiles:'
        value_line = '{lo:<{pad}}{q25:<{pad}}{q50:<{pad}}{q75:<{pad}}{hi:<{pad}}'
        lo, hi = 100 * alpha / 2, 100 * (1. - alpha / 2)
        qlist = (lo, 25, 50, 75, hi)
        header = value_line.format(lo=lo, q25=25, q50=50, q75=75, hi=hi,
                                   pad=spaces).strip()
        self.width = len(header)
        hline = '|{thin}|{thick}|{thick}|{thin}|'.format(
            thin='-' * (spaces - 1), thick='=' * (spaces - 1))

        self.header_lines = [title, header, hline]
        self.spaces = spaces
        self.lo, self.hi = lo, hi
        self.qlist = qlist
        self.value_line = value_line

    def _calculate_values(self, sample):
        return _calculate_posterior_quantiles(sample, self.qlist)


def _calculate_stats(sample, batches, alpha):
    means = sample.mean(0)
    sds = sample.std(0)
    mces = mc_error(sample, batches)
    intervals = hpd(sample, alpha)
    for key, idxs in _groupby_leading_idxs(sample.shape[1:]):
        yield key
        for idx in idxs:
            mean, sd, mce = [stat[idx] for stat in (means, sds, mces)]
            interval = intervals[idx].squeeze().tolist()
            yield {'mean': mean, 'sd': sd, 'mce': mce, 'hpd': interval}


def _calculate_posterior_quantiles(sample, qlist):
    var_quantiles = quantiles(sample, qlist=qlist)
    ## Replace ends of qlist with 'lo' and 'hi'
    qends = {qlist[0]: 'lo', qlist[-1]: 'hi'}
    qkeys = {q: qends[q] if q in qends else 'q{}'.format(q) for q in qlist}
    for key, idxs in _groupby_leading_idxs(sample.shape[1:]):
        yield key
        for idx in idxs:
            yield {qkeys[q]: var_quantiles[q][idx] for q in qlist}


def _groupby_leading_idxs(shape):
    """Group the indices for `shape` by the leading indices of `shape`.

    All dimensions except for the rightmost dimension are used to create
    groups.

    A 3d shape will be grouped by the indices for the two leading
    dimensions.

        >>> for key, idxs in _groupby_leading_idxs((3, 2, 2)):
        ...     print('key: {}'.format(key))
        ...     print(list(idxs))
        key: (0, 0)
        [(0, 0, 0), (0, 0, 1)]
        key: (0, 1)
        [(0, 1, 0), (0, 1, 1)]
        key: (1, 0)
        [(1, 0, 0), (1, 0, 1)]
        key: (1, 1)
        [(1, 1, 0), (1, 1, 1)]
        key: (2, 0)
        [(2, 0, 0), (2, 0, 1)]
        key: (2, 1)
        [(2, 1, 0), (2, 1, 1)]

    A 1d shape will only have one group.

        >>> for key, idxs in _groupby_leading_idxs((2,)):
        ...     print('key: {}'.format(key))
        ...     print(list(idxs))
        key: ()
        [(0,), (1,)]
    """
    idxs = itertools.product(*[range(s) for s in shape])
    return itertools.groupby(idxs, lambda x: x[:-1])

########NEW FILE########
__FILENAME__ = arraystep
from ..core import *
import numpy as np
from numpy.random import uniform
from numpy import log, isfinite

__all__ = ['ArrayStep', 'metrop_select', 'SamplerHist']

# TODO Add docstrings to ArrayStep


class ArrayStep(object):
    def __init__(self, vars, fs, allvars=False):
        self.ordering = ArrayOrdering(vars)
        self.fs = fs
        self.allvars = allvars

    def step(self, point):
        bij = DictToArrayBijection(self.ordering, point)

        inputs = list(map(bij.mapf, self.fs))
        if self.allvars:
            inputs += [point]

        apoint = self.astep(bij.map(point), *inputs)
        return bij.rmap(apoint)


def metrop_select(mr, q, q0):
    # Perform rejection/acceptance step for Metropolis class samplers

    # Compare acceptance ratio to uniform random number
    if isfinite(mr) and log(uniform()) < mr:
        # Accept proposed value
        return q
    else:
        # Reject proposed value
        return q0


class SamplerHist(object):
    def __init__(self):
        self.metrops = []

    def acceptr(self):
        return np.minimum(np.exp(self.metrops), 1)

########NEW FILE########
__FILENAME__ = compound
'''
Created on Mar 7, 2011

@author: johnsalvatier
'''

class CompoundStep(object):
    """Step method composed of a list of several other step methods applied in sequence."""
    def __init__(self, methods):
        self.methods = list(methods)

    def step(self, point):
        for method in self.methods:
            point = method.step(point)
        return point

########NEW FILE########
__FILENAME__ = gibbs
'''
Created on May 12, 2012

@author: john
'''
from ..core import *
from .arraystep import *
from numpy import array, max, exp, cumsum, nested_iters, empty, searchsorted, ones
from numpy.random import uniform

from theano.gof.graph import inputs
from theano.tensor import add 

__all__ = ['ElemwiseCategoricalStep']


class ElemwiseCategoricalStep(ArrayStep):
    """
    Gibbs sampling for categorical variables that only have only have elementwise effects
    the variable can't be indexed into or transposed or anything otherwise that will mess things up

    """
    # TODO: It would be great to come up with a way to make
    # ElemwiseCategoricalStep  more general (handling more complex elementwise
    # variables)
    def __init__(self, var, values, model=None):
        model = modelcontext(model)
        self.sh = ones(var.dshape, var.dtype)
        self.values = values
        self.var = var

        super(ElemwiseCategoricalStep, self).__init__([var], [elemwise_logp(model, var)])

    def astep(self, q, logp):
        p = array([logp(v * self.sh) for v in self.values])
        return categorical(p, self.var.dshape)


def elemwise_logp(model, var):
    terms = [v.logp_elemwiset for v in model.basic_RVs if var in inputs([v.logpt])]
    return model.fn(add(*terms))


def categorical(prob, shape):
    out = empty([1] + list(shape))

    n = len(shape)
    it0, it1 = nested_iters([prob, out], [list(range(1, n + 1)), [0]],
                            op_flags=[['readonly'], ['readwrite']],
                            flags=['reduce_ok'])

    for i in it0:
        p, o = it1.itviews
        p = cumsum(exp(p - max(p, axis=0)))
        r = uniform() * p[-1]

        o[0] = searchsorted(p, r)

    return out[0, ...]

########NEW FILE########
__FILENAME__ = hmc
'''
Created on Mar 7, 2011

@author: johnsalvatier
'''
from numpy import floor
from .quadpotential import *
from .arraystep import *
from ..core import *
from ..tuning import guess_scaling

import numpy as np
from scipy.sparse import issparse

from collections import namedtuple

__all__ = ['HamiltonianMC']

# TODO:
# add constraint handling via page 37 of Radford's
# http://www.cs.utoronto.ca/~radford/ham-mcmc.abstract.html


def unif(step_size, elow=.85, ehigh=1.15):
    return np.random.uniform(elow, ehigh) * step_size


class HamiltonianMC(ArrayStep):
    def __init__(self, vars=None, scaling=None, step_scale=.25, path_length=2., is_cov=False, step_rand=unif, state=None, model=None):
        """
        Parameters
        ----------
            vars : list of theano variables
            scaling : array_like, ndim = {1,2}
                Scaling for momentum distribution. 1d arrays interpreted matrix diagonal.
            step_scale : float, default=.25
                Size of steps to take, automatically scaled down by 1/n**(1/4) (defaults to .25)
            path_length : float, default=2
                total length to travel
            is_cov : bool, default=False
                Treat scaling as a covariance matrix/vector if True, else treat it as a precision matrix/vector
            step_rand : function float -> float, default=unif
                A function which takes the step size and returns an new one used to randomize the step size at each iteration.
            state
                State object
            model : Model
        """
        model = modelcontext(model)

        if vars is None:
            vars = model.cont_vars

        if scaling is None:
            scaling = model.test_point

        if isinstance(scaling, dict):
            scaling = guess_scaling(Point(scaling, model=model), model=model)

        n = scaling.shape[0]

        self.step_size = step_scale / n ** (1 / 4.)

        self.potential = quad_potential(scaling, is_cov, as_cov=False)

        self.path_length = path_length
        self.step_rand = step_rand

        if state is None:
            state = SamplerHist()
        self.state = state

        super(HamiltonianMC, self).__init__(vars, [model.fastlogp, model.fastdlogp(vars)])

    def astep(self, q0, logp, dlogp):
        H = Hamiltonian(logp, dlogp, self.potential)

        e = self.step_rand(self.step_size)
        nstep = int(self.path_length / e)

        p0 = H.pot.random()

        q, p = leapfrog(H, q0, p0, nstep, e)
        p = -p

        mr = energy(H, q0, p0) - energy(H, q, p)

        self.state.metrops.append(mr)

        return metrop_select(mr, q, q0)



def bern(p):
    return np.random.uniform() < p

Hamiltonian = namedtuple("Hamiltonian", "logp, dlogp, pot")

def energy(H, q, p):
        return -(H.logp(q) - H.pot.energy(p))

def leapfrog(H, q, p, n, e):
    _, dlogp, pot = H

    p = p - (e/2) * -dlogp(q)  # half momentum update

    for i in range(n):
        #alternate full variable and momentum updates
        q = q + e * pot.velocity(p)

        if i != n - 1:
            p = p - e * -dlogp(q)

    p = p - (e/2) * -dlogp(q)  # do a half step momentum update to finish off
    return q, p

########NEW FILE########
__FILENAME__ = metropolis
from numpy.linalg import cholesky

from ..core import *
from .quadpotential import quad_potential

from .arraystep import *
from numpy.random import normal, standard_cauchy, standard_exponential, poisson, random
from numpy import round, exp, copy, where


__all__ = ['Metropolis', 'BinaryMetropolis', 'NormalProposal', 'CauchyProposal', 'LaplaceProposal', 'PoissonProposal', 'MultivariateNormalProposal']

# Available proposal distributions for Metropolis


class Proposal(object):
    def __init__(self, s):
        self.s = s


class NormalProposal(Proposal):
    def __call__(self):
        return normal(scale=self.s)


class CauchyProposal(Proposal):
    def __call__(self):
        return standard_cauchy(size=np.size(self.s)) * self.s


class LaplaceProposal(Proposal):
    def __call__(self):
        size = np.size(self.s)
        return (standard_exponential(size=size) - standard_exponential(size=size)) * self.s


class PoissonProposal(Proposal):
    def __call__(self):
        return poisson(lam=self.s, size=np.size(self.s)) - self.s


class MultivariateNormalProposal(Proposal):
    def __call__(self):
        return np.random.multivariate_normal(mean=np.zeros(self.s.shape[0]), cov=self.s)


class Metropolis(ArrayStep):
    """
    Metropolis-Hastings sampling step

    Parameters
    ----------
    vars : list
        List of variables for sampler
    S : standard deviation or covariance matrix
        Some measure of variance to parameterize proposal distribution
    proposal_dist : function
        Function that returns zero-mean deviates when parameterized with
        S (and n). Defaults to quad_potential.
    scaling : scalar or array
        Initial scale factor for proposal. Defaults to 1.
    tune : bool
        Flag for tuning. Defaults to True.
    model : PyMC Model
        Optional model for sampling step. Defaults to None (taken from context).

    """
    def __init__(self, vars=None, S=None, proposal_dist=NormalProposal, scaling=1.,
                 tune=True, tune_interval=100, model=None):

        model = modelcontext(model)

        if vars is None:
            vars = model.vars
        if S is None:
            S = np.ones(sum(v.dsize for v in vars))
        self.proposal_dist = proposal_dist(S)
        self.scaling = scaling
        self.tune = tune
        self.tune_interval = tune_interval
        self.steps_until_tune = tune_interval
        self.accepted = 0

        # Determine type of variables
        self.discrete = np.array([v.dtype in discrete_types for v in vars])

        super(Metropolis, self).__init__(vars, [model.fastlogp])

    def astep(self, q0, logp):

        if self.tune and not self.steps_until_tune:
            # Tune scaling parameter
            self.scaling = tune(
                self.scaling, self.accepted / float(self.tune_interval))
            # Reset counter
            self.steps_until_tune = self.tune_interval
            self.accepted = 0

        delta = np.atleast_1d(self.proposal_dist() * self.scaling)
        if np.any(self.discrete):
            delta[self.discrete] = round(delta[self.discrete], 0).astype(int)

        q = q0 + delta

        q_new = metrop_select(logp(q) - logp(q0), q, q0)

        if (any(q_new != q0) or all(q0 == q)):
            self.accepted += 1

        self.steps_until_tune -= 1

        return q_new


def tune(scale, acc_rate):
    """
    Tunes the scaling parameter for the proposal distribution
    according to the acceptance rate over the last tune_interval:

    Rate    Variance adaptation
    ----    -------------------
    <0.001        x 0.1
    <0.05         x 0.5
    <0.2          x 0.9
    >0.5          x 1.1
    >0.75         x 2
    >0.95         x 10

    """

    # Switch statement
    if acc_rate < 0.001:
        # reduce by 90 percent
        scale *= 0.1
    elif acc_rate < 0.05:
        # reduce by 50 percent
        scale *= 0.5
    elif acc_rate < 0.2:
        # reduce by ten percent
        scale *= 0.9
    elif acc_rate > 0.95:
        # increase by factor of ten
        scale *= 10.0
    elif acc_rate > 0.75:
        # increase by double
        scale *= 2.0
    elif acc_rate > 0.5:
        # increase by ten percent
        scale *= 1.1

    return scale


class BinaryMetropolis(ArrayStep):
    """Metropolis-Hastings optimized for binary variables"""
    def __init__(self, vars, scaling=1., tune=True, tune_interval=100, model=None):

        model = modelcontext(model)

        self.scaling = scaling
        self.tune = tune
        self.tune_interval = tune_interval
        self.steps_until_tune = tune_interval
        self.accepted = 0

        if not all([v.dtype in discrete_types for v in vars]):
            raise ValueError(
                'All variables must be Bernoulli for BinaryMetropolis')

        super(BinaryMetropolis, self).__init__(vars, [model.fastlogp])

    def astep(self, q0, logp):

        # Convert adaptive_scale_factor to a jump probability
        p_jump = 1. - .5 ** self.scaling

        rand_array = random(q0.shape)
        q = copy(q0)
        # Locations where switches occur, according to p_jump
        switch_locs = where(rand_array < p_jump)
        q[switch_locs] = True - q[switch_locs]

        q_new = metrop_select(logp(q) - logp(q0), q, q0)

        return q_new

########NEW FILE########
__FILENAME__ = nuts
from .quadpotential import *
from .arraystep import *
from ..core import *
from numpy import exp, log
from numpy.random import uniform
from .hmc import leapfrog, Hamiltonian, bern, energy
from ..tuning import guess_scaling

__all__ = ['NUTS']

class NUTS(ArrayStep):
    """
    Automatically tunes step size and adjust number of steps for good performance.

    Implements "Algorithm 6: Efficient No-U-Turn Sampler with Dual Averaging" in:

    Hoffman, Matthew D., & Gelman, Andrew. (2011).
    The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.
    """
    def __init__(self, vars=None, scaling=None, step_scale=0.25, is_cov=False, state=None,
                 Emax=1000,
                 target_accept=0.65,
                 gamma=0.05,
                 k=0.75,
                 t0=10,
                 model=None):
        """
        Parameters
        ----------
            vars : list of Theano variables, default continuous vars
            scaling : array_like, ndim = {1,2} or point
                Scaling for momentum distribution. 1d arrays interpreted matrix diagonal.
            step_scale : float, default=.25
                Size of steps to take, automatically scaled down by 1/n**(1/4)
            is_cov : bool, default=False
                Treat C as a covariance matrix/vector if True, else treat it as a precision matrix/vector
            state
                state to start from
            Emax : float, default 1000
                maximum energy
            target_accept : float (0,1) default .65
                target for avg accept probability between final branch and initial position
            gamma : float, default .05
            k : float (.5,1) default .75
                scaling of speed of adaptation
            t0 : int, default 10
                slows inital adapatation
            model : Model
        """
        model = modelcontext(model)

        if vars is None:
            vars = model.cont_vars

        if scaling is None:
            scaling = model.test_point

        if isinstance(scaling, dict):
            scaling = guess_scaling(Point(scaling, model=model), model=model, vars = vars)



        n = scaling.shape[0]

        self.step_size = step_scale / n**(1/4.)


        self.potential = quad_potential(scaling, is_cov, as_cov=False)

        if state is None:
            state = SamplerHist()
        self.state = state
        self.Emax = Emax

        self.target_accept = target_accept
        self.gamma = gamma
        self.t0 = t0
        self.k = k

        self.Hbar = 0
        self.u = log(self.step_size*10)
        self.m = 0



        super(NUTS, self).__init__(vars, [model.fastlogp, model.fastdlogp(vars)])

    def astep(self, q0, logp, dlogp):
        H = Hamiltonian(logp, dlogp, self.potential)
        Emax = self.Emax
        e = self.step_size

        p0 = H.pot.random()
        u = uniform()
        q = qn = qp = q0
        p = pn = pp = p0

        n, s, j = 1, 1, 0

        while s == 1:
            v = bern(.5) * 2 - 1

            if v == -1:
                qn, pn, _, _, q1, n1, s1, a, na = buildtree(H, qn, pn, u, v, j, e, Emax, q0, p0)
            else:
                _, _, qp, pp, q1, n1, s1, a, na = buildtree(H, qp, pp, u, v, j, e, Emax, q0, p0)

            if s1 == 1 and bern(min(1, n1*1./n)):
                q = q1

            n = n + n1

            span = qp - qn
            s = s1 * (span.dot(pn) >= 0) * (span.dot(pp) >= 0)
            j = j + 1

        p = -p

        w = 1./(self.m+self.t0)
        self.Hbar = (1 - w) * self.Hbar + w*(self.target_accept - a*1./na)

        self.step_size = exp(self.u - (self.m**.5/self.gamma)*self.Hbar)
        self.m += 1

        return q


def buildtree(H, q, p, u, v, j, e, Emax, q0, p0):
    if j == 0:
        q1, p1 = leapfrog(H, q, p, 1, v*e)
        E = energy(H, q1, p1)
        E0 = energy(H, q0, p0)

        dE = E - E0

        n1 = int(log(u) + dE <= 0)
        s1 = int(log(u) + dE < Emax)
        return q1, p1, q1, p1, q1, n1, s1, min(1, exp(-dE)), 1
    else:
        qn, pn, qp, pp, q1, n1, s1, a1, na1 = buildtree(H, q, p, u, v, j - 1, e, Emax, q0, p0)
        if s1 == 1:
            if v == -1:
                qn, pn, _, _, q11, n11, s11, a11, na11 = buildtree(H, qn, pn, u, v, j - 1, e, Emax, q0, p0)
            else:
                _, _, qp, pp, q11, n11, s11, a11, na11 = buildtree(H, qp, pp, u, v, j - 1, e, Emax, q0, p0)

            if bern(n11*1./(max(n1 + n11, 1))):
                q1 = q11

            a1 = a1 + a11
            na1 = na1 + na11

            span = qp - qn
            s1 = s11 * (span.dot(pn) >= 0) * (span.dot(pp) >= 0)
            n1 = n1 + n11
        return qn, pn, qp, pp, q1, n1, s1, a1, na1
    return

########NEW FILE########
__FILENAME__ = quadpotential
from numpy import dot
from numpy.random import normal
from numpy.linalg import solve
from scipy.linalg import cholesky, cho_solve
from scipy.sparse import issparse

import numpy as np

__all__ = ['quad_potential', 'ElemWiseQuadPotential', 'QuadPotential',
           'QuadPotential_Inv', 'isquadpotential']

def quad_potential(C, is_cov, as_cov):
    """
    Parameters
    ----------
        C : arraylike, 0 <= ndim <= 2
            scaling matrix for the potential
            vector treated as diagonal matrix
        is_cov : Boolean
            whether C is provided as a covariance matrix or hessian
        as_cov : Boolean
            whether the random draws should come from the normal dist
            using the covariance matrix above or the inverse

    Returns
    -------
        q : Quadpotential
    """

    if issparse(C) and is_cov != as_cov:
        if not chol_available:
            raise ImportError("Requires scikits.sparse")
        return QuadPotential_SparseInv(C)

    partial_check_positive_definite(C)
    if C.ndim == 1:
        if is_cov != as_cov:
            return ElemWiseQuadPotential(C)
        else:
            return ElemWiseQuadPotential(1. / C)
    else:
        if is_cov != as_cov:
            return QuadPotential(C)
        else:
            return QuadPotential_Inv(C)


def partial_check_positive_definite(C):
    """Simple but partial check for Positive Definiteness"""
    if C.ndim == 1:
        d = C
    else:
        d = np.diag(C)
    i, = np.nonzero(np.logical_or(np.isnan(d), d <= 0))

    if len(i):
        raise PositiveDefiniteError(
            "Simple check failed. Diagonal contains negatives", i)


class PositiveDefiniteError(ValueError):
    def __init__(self, msg, idx):
        self.idx = idx
        self.msg = msg

    def __str__(self):
        return "Scaling is not positive definite. " + self.msg + ". Check indexes " + str(self.idx)


def isquadpotential(o):
    return all(hasattr(o, attr) for attr in ["velocity", "random", "energy"])


class ElemWiseQuadPotential(object):
    def __init__(self, v):
        s = v ** .5

        self.s = s
        self.inv_s = 1. / s
        self.v = v

    def velocity(self, x):
        return self.v * x

    def random(self):
        return normal(size=self.s.shape) * self.inv_s

    def energy(self, x):
        return .5 * dot(x, self.v * x)


class QuadPotential_Inv(object):
    def __init__(self, A):
        self.L = cholesky(A, lower=True)

    def velocity(self, x):
        return cho_solve((self.L, True), x)

    def random(self):
        n = normal(size=self.L.shape[0])
        return dot(self.L, n)

    def energy(self, x):
        L1x = solve(self.L, x)
        return .5 * dot(L1x.T, L1x)


class QuadPotential(object):
    def __init__(self, A):
        self.A = A
        self.L = cholesky(A, lower=True)

    def velocity(self, x):
        return dot(self.A, x)

    def random(self):
        n = normal(size=self.L.shape[0])
        return solve(self.L.T, n)

    def energy(self, x):
        return .5 * dot(x, dot(self.A, x))

    __call__ = random

try:
    import scikits.sparse.cholmod as cholmod
    chol_available = True
except ImportError:
    chol_available = False

if chol_available:
    __all__ += ['QuadPotential_SparseInv']

    class QuadPotential_SparseInv(object):
        def __init__(self, A):
            self.n = A.shape[0]
            self.factor = factor = cholmod.cholesky(A)
            self.L = factor.L()
            self.p = np.argsort(factor.P())

        def velocity(self, x):
            x = np.ones((x.shape[0], 2)) * x[:, np.newaxis]
            return self.factor(x)[:, 0]

        def Ldot(self, x):
            return (self.L * x)[self.p]

        def random(self):
            return self.Ldot(normal(size=self.n))

        def energy(self, x):
            return .5 * dot(x, self.velocity(x))

########NEW FILE########
__FILENAME__ = slicer
# Modified from original implementation by Dominik Wabersich (2013)

from ..core import *
from .arraystep import *
from numpy import floor, abs, atleast_1d, empty, isfinite, sum
from numpy.random import standard_exponential, random, uniform

__all__ = ['Slice']


class Slice(ArrayStep):

    """Slice sampler"""
    def __init__(self, vars=None, w=1, tune=True, model=None):

        model = modelcontext(model)

        if vars is None:
            vars = model.cont_vars

        self.w = w
        self.tune = tune
        self.w_tune = []
        self.model = model

        super(Slice, self).__init__(vars, [model.fastlogp])

    def astep(self, q0, logp):

        q = q0.copy()
        self.w = np.resize(self.w, len(q))

        y = logp(q0) - standard_exponential()

        # Stepping out procedure
        ql = q0.copy()
        ql -= uniform(0, self.w)
        qr = q0.copy()
        qr = ql + self.w

        yl = logp(ql)
        yr = logp(qr)

        while((y < yl).all()):
            ql -= self.w
            yl = logp(ql)

        while((y < yr).all()):
            qr += self.w
            yr = logp(qr)

        q_next = q0.copy()
        while True:

            # Sample uniformly from slice
            qi = uniform(ql, qr)

            yi = logp(qi)

            if yi > y:
                q = qi
                break
            elif (qi > q).all():
                qr = qi
            elif (qi < q).all():
                ql = qi

        if self.tune:
            # Tune sampler parameters
            self.w_tune.append(abs(q0 - q))
            self.w = 2 * sum(self.w_tune, 0) / len(self.w_tune)

        return q

########NEW FILE########
__FILENAME__ = backend_fixtures
import unittest
import numpy as np
import numpy.testing as npt
import os
import shutil

from pymc.tests import models
from pymc.backends import base


class ModelBackendSetupTestCase(unittest.TestCase):
    """Set up a backend trace.

    Provides the attributes
    - test_point
    - model
    - trace
    - draws

    Children must define
    - backend
    - name
    - shape
    """
    def setUp(self):
        self.test_point, self.model, _ = models.non_normal(self.shape)
        with self.model:
            self.trace = self.backend(self.name)
        self.draws, self.chain = 3, 0
        self.trace.setup(self.draws, self.chain)

    def tearDown(self):
        if self.name is not None:
            remove_file_or_directory(self.name)


class ModelBackendSampledTestCase(unittest.TestCase):
    """Setup and sample a backend trace.

    Provides the attributes
    - test_point
    - model
    - mtrace (MultiTrace object)
    - draws
    - expected
        Expected values mapped to chain number and variable name.

    Children must define
    - backend
    - name
    - shape
    """
    @classmethod
    def setUpClass(cls):
        cls.test_point, cls.model, _ = models.non_normal(cls.shape)
        with cls.model:
            trace0 = cls.backend(cls.name)
            trace1 = cls.backend(cls.name)

        cls.draws = 5
        trace0.setup(cls.draws, chain=0)
        trace1.setup(cls.draws, chain=1)

        varnames = list(cls.test_point.keys())
        shapes = {varname: value.shape
                  for varname, value in cls.test_point.items()}

        cls.expected = {0: {}, 1: {}}
        for varname in varnames:
            mcmc_shape = (cls.draws,) + shapes[varname]
            values = np.arange(cls.draws * np.prod(shapes[varname]))
            cls.expected[0][varname] = values.reshape(mcmc_shape)
            cls.expected[1][varname] = values.reshape(mcmc_shape) * 100

        for idx in range(cls.draws):
            point0 = {varname: cls.expected[0][varname][idx, ...]
                      for varname in varnames}
            point1 = {varname: cls.expected[1][varname][idx, ...]
                      for varname in varnames}
            trace0.record(point=point0)
            trace1.record(point=point1)
        trace0.close()
        trace1.close()
        cls.mtrace = base.MultiTrace([trace0, trace1])

    @classmethod
    def tearDownClass(cls):
        if cls.name is not None:
            remove_file_or_directory(cls.name)


class SamplingTestCase(ModelBackendSetupTestCase):
    """Test backend sampling.

    Children must define
    - backend
    - name
    - shape
    """

    def test_standard_close(self):
        for idx in range(self.draws):
            point = {varname: np.tile(idx, value.shape)
                     for varname, value in self.test_point.items()}
            self.trace.record(point=point)
        self.trace.close()

        for varname in self.trace.varnames:
            npt.assert_equal(self.trace[varname][0, ...],
                             np.zeros(self.trace.var_shapes[varname]))
            last_idx = self.draws - 1
            npt.assert_equal(self.trace[varname][last_idx, ...],
                             np.tile(last_idx, self.trace.var_shapes[varname]))

    def test_clean_interrupt(self):
        self.trace.record(point=self.test_point)
        self.trace.close()
        for varname in self.trace.varnames:
            self.assertEqual(self.trace[varname].shape[0], 1)


class SelectionTestCase(ModelBackendSampledTestCase):
    """Test backend selection.

    Children must define
    - backend
    - name
    - shape
    """
    def test_get_values_default(self):
        for varname in self.mtrace.varnames:
            expected = [self.expected[0][varname], self.expected[1][varname]]
            result = self.mtrace.get_values(varname)
            npt.assert_equal(result, expected)

    def test_get_values_burn_keyword(self):
        burn = 2
        for varname in self.mtrace.varnames:
            expected = [self.expected[0][varname][burn:],
                        self.expected[1][varname][burn:]]
            result = self.mtrace.get_values(varname, burn=burn)
            npt.assert_equal(result, expected)

    def test_len(self):
        self.assertEqual(len(self.mtrace), self.draws)

    def test_get_values_thin_keyword(self):
        thin = 2
        for varname in self.mtrace.varnames:
            expected = [self.expected[0][varname][::thin],
                        self.expected[1][varname][::thin]]
            result = self.mtrace.get_values(varname, thin=thin)
            npt.assert_equal(result, expected)

    def test_get_point(self):
        idx = 2
        result = self.mtrace.point(idx)
        for varname in self.mtrace.varnames:
            expected = self.expected[1][varname][idx]
            npt.assert_equal(result[varname], expected)

    def test_get_slice(self):
        expected = []
        for chain in [0, 1]:
            expected.append({varname: self.expected[chain][varname][:2]
                             for varname in self.mtrace.varnames})
        result = self.mtrace[:2]
        for chain in [0, 1]:
            for varname in self.mtrace.varnames:
                npt.assert_equal(result.get_values(varname, chains=[chain]),
                                 expected[chain][varname])

    def test_get_values_one_chain(self):
        for varname in self.mtrace.varnames:
            expected = self.expected[0][varname]
            result = self.mtrace.get_values(varname, chains=[0])
            npt.assert_equal(result, expected)

    def test_get_values_chains_reversed(self):
        for varname in self.mtrace.varnames:
            expected = [self.expected[1][varname], self.expected[0][varname]]
            result = self.mtrace.get_values(varname, chains=[1, 0])
            npt.assert_equal(result, expected)

    def test_nchains(self):
        self.mtrace.nchains == 2

    def test_get_values_one_chain_int_arg(self):
        varname = self.mtrace.varnames[0]
        npt.assert_equal(self.mtrace.get_values(varname, chains=[0]),
                         self.mtrace.get_values(varname, chains=0))

    def test_get_values_combine(self):
        varname = self.mtrace.varnames[0]
        expected = np.concatenate([self.expected[chain][varname]
                                   for chain in [0, 1]])
        result = self.mtrace.get_values('x', combine=True)
        npt.assert_equal(result, expected)

    def test_get_values_combine_burn_arg(self):
        varname = self.mtrace.varnames[0]
        burn = 2
        expected = np.concatenate([self.expected[chain][varname][burn:]
                                   for chain in [0, 1]])
        result = self.mtrace.get_values('x', combine=True, burn=burn)
        npt.assert_equal(result, expected)

    def test_get_values_combine_thin_arg(self):
        varname = self.mtrace.varnames[0]
        thin = 2
        expected = np.concatenate([self.expected[chain][varname][::thin]
                                   for chain in [0, 1]])
        result = self.mtrace.get_values('x', combine=True, thin=thin)
        npt.assert_equal(result, expected)


class SelectionNoSliceTestCase(SelectionTestCase):
    def test_get_slice(self):
        pass


class DumpLoadTestCase(ModelBackendSampledTestCase):
    """Test equality of a dumped and loaded trace with original.

    Children must define
    - backend
    - load_func
        Function to load dumped backend
    - name
    - shape
    """
    @classmethod
    def setUpClass(cls):
        super(DumpLoadTestCase, cls).setUpClass()
        try:
            with cls.model:
                cls.dumped = cls.load_func(cls.name)
        except:
            remove_file_or_directory(cls.name)
            raise

    @classmethod
    def tearDownClass(cls):
        remove_file_or_directory(cls.name)

    def test_nchains(self):
        self.assertEqual(self.mtrace.nchains, self.dumped.nchains)

    def test_varnames(self):
        trace_names = list(sorted(self.mtrace.varnames))
        dumped_names = list(sorted(self.dumped.varnames))
        self.assertEqual(trace_names, dumped_names)

    def test_values(self):
        trace = self.mtrace
        dumped = self.dumped
        for chain in trace.chains:
            for varname in trace.varnames:
                data = trace.get_values(varname, chains=[chain])
                dumped_data = dumped.get_values(varname, chains=[chain])
                npt.assert_equal(data, dumped_data)


class BackendEqualityTestCase(ModelBackendSampledTestCase):
    """Test equality of attirbutes from two backends.

    Children must define
    - backend0
    - backend1
    - name0
    - name1
    - shape
    """
    @classmethod
    def setUpClass(cls):
        cls.backend = cls.backend0
        cls.name = cls.name0
        super(BackendEqualityTestCase, cls).setUpClass()
        cls.mtrace0 = cls.mtrace

        cls.backend = cls.backend1
        cls.name = cls.name1
        super(BackendEqualityTestCase, cls).setUpClass()
        cls.mtrace1 = cls.mtrace

    @classmethod
    def tearDownClass(cls):
        for name in [cls.name0, cls.name1]:
            if name is not None:
                remove_file_or_directory(name)

    def test_chain_length(self):
        assert self.mtrace0.nchains == self.mtrace1.nchains
        assert len(self.mtrace0) == len(self.mtrace1)

    def test_number_of_draws(self):
        values0 = self.mtrace0.get_values('x', squeeze=False)
        values1 = self.mtrace1.get_values('x', squeeze=False)
        assert values0[0].shape[0] == self.draws
        assert values1[0].shape[0] == self.draws

    def test_get_item(self):
        npt.assert_equal(self.mtrace0['x'], self.mtrace1['x'])

    def test_get_values(self):
        for cf in [False, True]:
            npt.assert_equal(self.mtrace0.get_values('x', combine=cf),
                             self.mtrace1.get_values('x', combine=cf))

    def test_get_values_no_squeeze(self):
        npt.assert_equal(self.mtrace0.get_values('x', combine=False,
                                                 squeeze=False),
                         self.mtrace1.get_values('x', combine=False,
                                                 squeeze=False))

    def test_get_values_combine_and_no_squeeze(self):
        npt.assert_equal(self.mtrace0.get_values('x', combine=True,
                                                 squeeze=False),
                         self.mtrace1.get_values('x', combine=True,
                                                 squeeze=False))

    def test_get_values_with_burn(self):
        for cf in [False, True]:
            npt.assert_equal(self.mtrace0.get_values('x', combine=cf, burn=3),
                             self.mtrace1.get_values('x', combine=cf, burn=3))
            ## Burn to one value.
            npt.assert_equal(self.mtrace0.get_values('x', combine=cf,
                                                     burn=self.draws - 1),
                             self.mtrace1.get_values('x', combine=cf,
                                                     burn=self.draws - 1))

    def test_get_values_with_thin(self):
        for cf in [False, True]:
            npt.assert_equal(self.mtrace0.get_values('x', combine=cf, thin=2),
                             self.mtrace1.get_values('x', combine=cf, thin=2))

    def test_get_values_with_burn_and_thin(self):
        for cf in [False, True]:
            npt.assert_equal(self.mtrace0.get_values('x', combine=cf,
                                                     burn=2, thin=2),
                             self.mtrace1.get_values('x', combine=cf,
                                                     burn=2, thin=2))

    def test_get_values_with_chains_arg(self):
        for cf in [False, True]:
            npt.assert_equal(self.mtrace0.get_values('x', chains=[0]),
                             self.mtrace1.get_values('x', chains=[0]))

    def test_get_point(self):
        npoint, spoint = self.mtrace0[4], self.mtrace1[4]
        npt.assert_equal(npoint['x'], spoint['x'])

    def test_point_with_chain_arg(self):
        npoint = self.mtrace0.point(4, chain=0)
        spoint = self.mtrace1.point(4, chain=0)
        npt.assert_equal(npoint['x'], spoint['x'])


def remove_file_or_directory(name):
    try:
        os.remove(name)
    except OSError:
        shutil.rmtree(name, ignore_errors=True)

########NEW FILE########
__FILENAME__ = checks
import pymc as pm
import numpy as np

from pymc import sample

from numpy.testing import assert_almost_equal


def close_to(x, v, bound, name="value"):
    assert np.all(np.logical_or(
        np.abs(x - v) < bound,
        x == v)), name + " out of bounds : " + repr(x) + ", " + repr(v) + ", " + repr(bound)

########NEW FILE########
__FILENAME__ = knownfailure
import functools
import nose

def knownfailure(msg):
    def decorator(test):
        @functools.wraps(test)
        def inner(*args, **kwargs):
            try:
                test(*args, **kwargs)
            except Exception:
                raise nose.SkipTest
            else:
                raise AssertionError('Failure expected: ' + msg)
        return inner
    return decorator

########NEW FILE########
__FILENAME__ = models
from pymc import Model, Normal, Metropolis, MvNormal
import numpy as np
import pymc as pm
from itertools import product
from theano.tensor import log

def simple_model():
    mu = -2.1
    tau = 1.3
    with Model() as model:
        x = Normal('x', mu, tau, shape=2, testval=[.1]*2)

    return model.test_point, model, (mu, tau ** -1)

def multidimensional_model():
    mu = -2.1
    tau = 1.3
    with Model() as model:
        x = Normal('x', mu, tau, shape=(3,2), testval=.1*np.ones((3,2)) )

    return model.test_point, model, (mu, tau ** -1)

def simple_init():
    start, model, moments = simple_model()

    step = Metropolis(model.vars, np.diag([1.]), model=model)
    return model, start, step, moments


def simple_2model():
    mu = -2.1
    tau = 1.3
    p = .4
    with Model() as model:
        x = pm.Normal('x', mu, tau, testval=.1)
        logx = pm.Deterministic('logx', log(x))
        y = pm.Bernoulli('y', p)

    return model.test_point, model


def mv_simple():
    mu = np.array([-.1, .5, 1.1])
    p = np.array([
        [2., 0, 0],
        [.05, .1, 0],
        [1., -0.05, 5.5]])

    tau = np.dot(p, p.T)

    with pm.Model() as model:
        x = pm.MvNormal('x', pm.constant(mu), pm.constant(
            tau), shape=3, testval=np.array([.1, 1., .8]))

    H = tau
    C = np.linalg.inv(H)

    return model.test_point, model, (mu, C)

def mv_simple_discrete():
    d= 2
    n = 5
    p = np.array([.15,.85])

    with pm.Model() as model:
        x = pm.Multinomial('x', n, pm.constant(p), shape=d, testval=np.array([1,4]))
        mu = n * p

        #covariance matrix
        C = np.zeros((d,d))
        for (i, j) in product(range(d), range(d)):
            if i == j:
                C[i,i] = n * p[i]*(1-p[i])
            else:
                C[i,j] = -n*p[i]*p[j]

    return model.test_point, model, (mu, C)

def non_normal(n=2):
    with pm.Model() as model:
        x = pm.Beta('x', 3, 3, shape=n)

    return model.test_point, model, (np.tile([.5], n), None)

def exponential_beta(n=2):
    with pm.Model() as model:
        x = pm.Beta('x', 3, 1, shape=n)
        y = pm.Exponential('y', 1, shape=n)

    return model.test_point, model, None

########NEW FILE########
__FILENAME__ = test_diagnostics
from pymc import *

from pymc.examples import disaster_model as dm

def test_gelman_rubin(n=1000):

    with dm.model:
        # Run sampler
        step1 = Slice([dm.early_mean, dm.late_mean])
        step2 = Metropolis([dm.switchpoint])
        start = {'early_mean': 2., 'late_mean': 3., 'switchpoint': 50}
        ptrace = sample(n, [step1, step2], start, njobs=2,
                        random_seed=[1, 3])

    rhat = gelman_rubin(ptrace)

    assert np.all([r < 1.5 for r in rhat.values()])


def test_geweke(n=3000):

    with dm.model:
        # Run sampler
        step1 = Slice([dm.early_mean, dm.late_mean])
        step2 = Metropolis([dm.switchpoint])
        trace = sample(n, [step1, step2], progressbar=False,
                       random_seed=1)

    z_switch = geweke(trace['switchpoint'], last=.5, intervals=20)

    # Ensure `intervals` argument is honored
    assert len(z_switch) == 20

    # Ensure `last` argument is honored
    assert z_switch[-1, 0] < (n / 2)

    # These should all be z-scores
    print(max(abs(z_switch[:, 1])))
    assert max(abs(z_switch[:, 1])) < 1

########NEW FILE########
__FILENAME__ = test_distributions
from __future__ import division
import unittest

import itertools
from .checks import *
from pymc import *
from numpy import array, inf
import numpy
from numpy.linalg import inv

from scipy import integrate
import scipy.stats.distributions  as sp
import scipy.stats

from .knownfailure import *


class Domain(object):
    def __init__(self, vals, dtype=None, edges=None, shape=None):
        avals = array(vals)

        if edges is None:
            edges = array(vals[0]), array(vals[-1])
            vals = vals[1:-1]
        if shape is None:
            shape = avals[0].shape

        self.vals = vals
        self.shape = shape

        self.lower, self.upper = edges
        self.dtype = avals.dtype

    def __neg__(self):
        return Domain([-v for v in self.vals], self.dtype, (-self.lower, -self.upper), self.shape)


def product(domains):
    names = [name for (name, domain) in domains.items()]
    domains = [domain for (name, domain) in domains.items()]

    for val in itertools.product(*[d.vals for d in domains]):
        yield zip(names, val)

R = Domain([-inf, -2.1, -1, -.01, .0, .01, 1, 2.1, inf])
Rplus = Domain([0, .01, .1, .9, .99, 1, 1.5, 2, 100, inf])
Rplusbig = Domain([0, .5, .9, .99, 1, 1.5, 2, 20, inf])
Unit = Domain([0, .001, .1, .5, .75, .99, 1])

Runif = Domain([-1, -.4, 0, .4, 1])
Rdunif = Domain([-10, 0, 10.])
Rplusunif = Domain([0, .5, inf])
Rplusdunif = Domain([2, 10, 100], 'int64')

I = Domain([-1000, -3, -2, -1, 0, 1, 2, 3, 1000], 'int64')

NatSmall = Domain([0, 3, 4, 5, 1000], 'int64')
Nat = Domain([0, 1, 2, 3, 2000], 'int64')
NatBig = Domain([0, 1, 2, 3, 5000, 50000], 'int64')

Bool = Domain([0, 0, 1, 1], 'int64')





class ProductDomain(object):
    def __init__(self, domains):
        self.vals = list(itertools.product(*[d.vals for d in domains]))

        self.shape = (len(domains),) + domains[0].shape

        self.lower = [d.lower for d in domains]
        self.upper = [d.upper for d in domains]

        self.dtype = domains[0].dtype

def Vector(D, n):
    return ProductDomain([D] *n)

def simplex_values(n):
    if n == 1:
        yield array([1.0])
    else:
        for v in Unit.vals:
            for vals in simplex_values(n-1):
                yield np.concatenate([[v], (1-v) * vals])

class Simplex(object):
    def __init__(self, n):
        self.vals = list(simplex_values(n))

        self.shape = (n,)
        self.dtype = Unit.dtype
        return

def PdMatrix(n):
    if n == 2:
        return PdMatrix2
    elif n == 3:
        return PdMatrix3
    else:
        raise ValueError("n out of bounds")

PdMatrix2 = Domain([
    np.eye(2),
    [[.5, .05],
     [.05, 4.5]]
    ],
    edges = (None,None))

PdMatrix3 = Domain([
    np.eye(3),
    [[.5, .1,0],
     [.1, 1, 0],
     [0, 0, 2.5]]
    ],
    edges = (None,None))

def test_uniform():
    pymc_matches_scipy(
            Uniform, Runif, {'lower': -Rplusunif, 'upper': Rplusunif},
            lambda value, lower, upper: sp.uniform.logpdf(value, lower, upper - lower)
            )


def test_discrete_unif():
    pymc_matches_scipy(
            DiscreteUniform, Rdunif,
            {'lower': -Rplusdunif, 'upper': Rplusdunif},
            lambda value, lower, upper: sp.randint.logpmf(value, lower, upper+1)
            )

def test_flat():
    pymc_matches_scipy(
            Flat, Runif, {},
            lambda value: 0
            )


def test_normal():
    pymc_matches_scipy(
            Normal, R, {'mu': R, 'sd': Rplus},
            lambda value, mu, sd: sp.norm.logpdf(value, mu, sd)
            )


def test_half_normal():
    pymc_matches_scipy(
            HalfNormal, Rplus, {'sd': Rplus},
            lambda value, sd: sp.halfnorm.logpdf(value, scale=sd)
            )

def test_chi_squared():
    pymc_matches_scipy(
            ChiSquared, Rplus, {'nu': Rplusdunif},
            lambda value, nu: sp.chi2.logpdf(value, df=nu)
            )

def test_wald():
    pymc_matches_scipy(
            Wald, Rplus, {'mu': Rplus},
            lambda value, mu: sp.invgauss.logpdf(value, mu)
            )

def test_beta():
    pymc_matches_scipy(
            Beta, Unit, {'alpha': Rplus, 'beta': Rplus},
            lambda value, alpha, beta: sp.beta.logpdf(value, alpha, beta)
            )


def test_exponential():
    pymc_matches_scipy(
            Exponential, Rplus, {'lam': Rplus},
            lambda value, lam: sp.expon.logpdf(value, 0, 1/lam)
            )

def test_geometric():
    pymc_matches_scipy(
            Geometric, Nat, {'p': Unit},
            lambda value, p: np.log(sp.geom.pmf(value, p))
            )


def test_negative_binomial():
    pymc_matches_scipy(
            NegativeBinomial, Nat, {'mu': Rplus, 'alpha': Rplus},
            lambda value, mu, alpha: sp.nbinom.logpmf(value, alpha, 1 - mu/(mu + alpha))
            )


def test_laplace():
    pymc_matches_scipy(
            Laplace, R, {'mu': R, 'b': Rplus},
            lambda value, mu, b: sp.laplace.logpdf(value, mu, b)
            )


def test_lognormal():
    pymc_matches_scipy(
            Lognormal, Rplus, {'mu': R, 'tau': Rplusbig},
            lambda value, mu, tau: sp.lognorm.logpdf(value, tau**-.5, 0, np.exp(mu))
            )

def test_t():
    pymc_matches_scipy(
            T, R, {'nu': Rplus, 'mu': R, 'lam': Rplus},
            lambda value, nu, mu, lam: sp.t.logpdf(value, nu, mu, lam**-.5)
            )


def test_cauchy():
    pymc_matches_scipy(
            Cauchy, R, {'alpha': R, 'beta': Rplusbig},
            lambda value, alpha, beta: sp.cauchy.logpdf(value, alpha, beta)
            )

def test_half_cauchy():
    pymc_matches_scipy(
            HalfCauchy, Rplus, {'beta': Rplusbig},
            lambda value, beta: sp.halfcauchy.logpdf(value, scale=beta)
            )

def test_gamma():
    pymc_matches_scipy(
            Gamma, Rplus, {'alpha': Rplusbig, 'beta': Rplusbig},
            lambda value, alpha, beta: sp.gamma.logpdf(value, alpha, scale=1.0/beta)
            )

def test_inverse_gamma():
    pymc_matches_scipy(
            InverseGamma, Rplus, {'alpha': Rplus, 'beta': Rplus},
            lambda value, alpha, beta: sp.invgamma.logpdf(value, alpha, scale=beta)
            )

def test_pareto():
    pymc_matches_scipy(
            Pareto, Rplus, {'alpha': Rplusbig, 'm': Rplusbig},
            lambda value, alpha, m: sp.pareto.logpdf(value, alpha, scale=m)
            )

def scipy_exponweib_sucks(value, alpha, beta):
    """
    This function is required because SciPy's implementation of
    the Weibull PDF fails for some valid combinations of parameters, while the
    log-PDF fails for others.
    """
    pdf = numpy.log(sp.exponweib.pdf(value, 1, alpha, scale=beta))
    logpdf = sp.exponweib.logpdf(value, 1, alpha, scale=beta)

    if np.isinf(pdf):
        return logpdf
    return pdf

def test_weibull():
    pymc_matches_scipy(
            Weibull, Rplus, {'alpha': Rplusbig, 'beta': Rplusbig},
            scipy_exponweib_sucks
            )

def test_tpos():
    #TODO: this actually shouldn't pass
    pymc_matches_scipy(
            Tpos, Rplus, {'nu': Rplus, 'mu': R, 'lam': Rplus},
            lambda value, nu, mu, lam: sp.t.logpdf(value, nu, mu, lam**-.5)
            )


def test_binomial():
    pymc_matches_scipy(
            Binomial, Nat, {'n': NatSmall, 'p': Unit},
            lambda value, n, p: sp.binom.logpmf(value, n, p)
            )

def test_betabin():
    checkd(BetaBin, Nat, {'alpha': Rplus, 'beta': Rplus, 'n': NatSmall})


def test_bernoulli():
    pymc_matches_scipy(
            Bernoulli, Bool, {'p': Unit},
            lambda value, p: sp.bernoulli.logpmf(value, p)
            )

def test_poisson():
    pymc_matches_scipy(
            Poisson, Nat, {'mu': Rplus},
            lambda value, mu: sp.poisson.logpmf(value, mu)
            )

def test_constantdist():
    pymc_matches_scipy(
            ConstantDist, I, {'c': I},
            lambda value, c: np.log(c == value)
            )

def test_zeroinflatedpoisson():
    checkd(ZeroInflatedPoisson, I, {'theta': Rplus, 'z': Bool})

def test_mvnormal():
    for n in [2,3]:
        yield check_mvnormal, n

def check_mvnormal(n):
    pymc_matches_scipy(
            MvNormal, Vector(R,n), {'mu': Vector(R,n), 'tau': PdMatrix(n)},
            normal_logpdf
            )

def normal_logpdf(value, mu, tau):
    (k,) = value.shape
    return  (-k/2)* np.log(2*np.pi) + .5 * np.log(np.linalg.det(tau)) - .5*(value-mu).dot(tau).dot(value -mu)

def test_wishart():
    for n in [2,3]:
        yield check_wishart,n

def check_wishart(n):
    checkd(Wishart, PdMatrix(n), {'n': Domain([2, 3, 4, 2000]) , 'V': PdMatrix(n) }, checks = [check_dlogp])

def betafn(a):
    return scipy.special.gammaln(a).sum() - scipy.special.gammaln(a.sum())

def logpow(v, p):
    return np.choose(v==0, [p * np.log(v), 0])

def dirichlet_logpdf(value, a):
    return -betafn(a) + logpow(value, a-1).sum()

def test_dirichlet():
    for n in [2,3]:
        yield check_dirichlet, n

def check_dirichlet(n):
        pymc_matches_scipy(
                Dirichlet, Simplex(n), {'a': Vector(Rplus, n) },
                dirichlet_logpdf
                )

def multinomial_logpdf(value, n, p):
    if value.sum() == n and all(value >= 0) and all(value <= n) :
        return scipy.special.gammaln(n+1) - scipy.special.gammaln(value+1).sum() + logpow(p, value).sum()
    else:
        return -inf

def test_multinomial():
    for n in [2,3]:
        yield check_multinomial, n

def check_multinomial(n):
        pymc_matches_scipy(
                Multinomial, Vector(Nat, n), {'p': Simplex(n), 'n' : Nat },
                multinomial_logpdf)

def categorical_logpdf(value, p):
    if value >= 0 and value <= len(p):
        return np.log(p[value])
    else:
        return -inf

def test_categorical():
    for n in [2,3,4]:
        yield check_categorical, n

def check_categorical(n):
    pymc_matches_scipy(
        Categorical, Domain(range(n), 'int64'), {'p': Simplex(n)},
        lambda value, p: categorical_logpdf(value, p)
        )

def test_densitydist():
    def logp(x):
        return -log(2 * .5) - abs(x - .5) / .5

    checkd(DensityDist, R, {}, extra_args={'logp': logp})


def test_addpotential():
    with Model() as model:
        value = Normal('value', 1, 1)
        value_squared = Potential('value_squared', -value ** 2)

        check_dlogp(model, value, R, {})



def pymc_matches_scipy(pymc_dist, domain, paramdomains, scipy_dist, extra_args={}):
    model= build_model(pymc_dist, domain, paramdomains, extra_args)
    value = model.named_vars['value']

    def logp(args):
        return scipy_dist(**args)

    check_logp(model, value, domain, paramdomains, logp)



def test_bound():
    with Model() as model:
        PositiveNormal = Bound(Normal, -.2)
        value = PositiveNormal('value', 1, 1)

        Rplus2 = Domain([-.2, -.19, -.1, 0, .5, 1, inf])

        check_dlogp(model, value, Rplus2, {})

def check_int_to_1(model, value, domain, paramdomains):
    pdf = model.fastfn(exp(model.logpt))

    for pt in product(paramdomains):
        pt = Point(pt, value=value.tag.test_value, model=model)

        bij = DictToVarBijection(value, (), pt)
        pdfx = bij.mapf(pdf)

        area = integrate_nd(pdfx, domain, value.dshape, value.dtype)

        assert_almost_equal(area, 1, err_msg=str(pt))

def integrate_nd(f, domain, shape, dtype):

    if shape == () or shape == (1,):
        if dtype in continuous_types:
            return integrate.quad(f, domain.lower, domain.upper, epsabs=1e-8)[0]
        else:
            return np.sum(list(map(f, np.arange(domain.lower, domain.upper + 1))))
    elif shape == (2,):
        def f2(a, b):
            return f([a, b])

        return integrate.dblquad(f2,
                                 domain.lower[0],
                                 domain.upper[0],
                                 lambda a: domain.lower[1],
                                 lambda a: domain.upper[1])[0]

    elif shape == (3,):
        def f3(a, b, c):
            return f([a, b, c])

        return integrate.tplquad(f3,
                                 domain.lower[0], domain.upper[0],
                                 lambda a: domain.lower[1],
                                 lambda a: domain.upper[1],
                                 lambda a, b: domain.lower[2],
                                 lambda a, b: domain.upper[2])[0]
    else:
        raise ValueError("Dont know how to integrate shape: " + str(shape))


def check_dlogp(model, value, domain, paramdomains):
    try:
        from numdifftools import Gradient
    except ImportError:
        return

    domains = paramdomains.copy()
    domains['value'] = domain

    bij = DictToArrayBijection(
        ArrayOrdering(model.cont_vars), model.test_point)

    if not model.cont_vars:
        return

    dlogp = bij.mapf(model.fastdlogp(model.cont_vars))
    logp = bij.mapf(model.fastlogp)


    def wrapped_logp(x):
        try :
            return logp(x)
        except:
            return np.nan

    ndlogp = Gradient(wrapped_logp)

    for pt in product(domains):
        pt = Point(pt, model=model)

        pt = bij.map(pt)

        assert_almost_equal(dlogp(pt), ndlogp(pt),
                            decimal=6, err_msg=str(pt))

def check_logp(model, value, domain, paramdomains, logp_reference):
    domains = paramdomains.copy()
    domains['value'] = domain


    logp = model.fastlogp

    for pt in product(domains):
        pt = Point(pt, model=model)

        assert_almost_equal(logp(pt), logp_reference(pt),
                            decimal=6, err_msg=str(pt))


def build_model(distfam, valuedomain, vardomains, extra_args={}):
    with Model() as m:
        vars = dict((v, Flat(
            v, dtype=dom.dtype, shape=dom.shape, testval=dom.vals[0])) for v, dom in vardomains.items())
        vars.update(extra_args)

        value = distfam(
            'value', shape=valuedomain.shape, **vars)
    return m


def checkd(distfam, valuedomain, vardomains,
           checks=(check_int_to_1, check_dlogp), extra_args={}):

        m = build_model(distfam, valuedomain, vardomains, extra_args=extra_args)


        for check in checks:
            check(m, m.named_vars['value'], valuedomain, vardomains)

########NEW FILE########
__FILENAME__ = test_examples
import matplotlib, pkgutil, itertools
from pymc import examples 

matplotlib.use('Agg', warn=False)

def get_examples():
    prefix = examples.__name__ + "."
    for _, example, _ in pkgutil.iter_modules(examples.__path__):
        yield check_example, prefix + example

        
def check_example(example_name):
    example = __import__(example_name, fromlist='dummy')
    if hasattr(example, 'run'):
        example.run("short")


def test_examples0():
    for t in itertools.islice(get_examples(), 0, 10):
        yield t

def test_examples1():
    for t in itertools.islice(get_examples(), 10, 20):
        yield t

def test_examples2():
    for t in itertools.islice(get_examples(), 20, None):
        yield t

########NEW FILE########
__FILENAME__ = test_glm
import unittest
from nose import SkipTest

from pymc import *
import sys
try:
    import statsmodels.api as sm
except ImportError:
    sys.exit(0)

from pymc.examples import glm_linear, glm_robust


np.random.seed(1)
# Generate data
true_intercept = 0
true_slope = 3

def generate_data(size=700):
    x = np.linspace(-1, 1, size)
    y = true_intercept + x*true_slope
    return x, y

true_sd = .05
x_linear, y_linear = generate_data(size=1000)
y_linear += np.random.normal(size=1000, scale=true_sd)
data_linear = dict(x=x_linear, y=y_linear)

x_logistic, y_logistic = generate_data(size=3000)
y_logistic = 1 / (1+np.exp(-y_logistic))
bern_trials = [np.random.binomial(1, i) for i in y_logistic]
data_logistic = dict(x=x_logistic, y=bern_trials)

class TestGLM(unittest.TestCase):
    @unittest.skip("Fails only on travis. Investigate")
    def test_linear_component(self):
        with Model() as model:
            y_est, coeffs = glm.linear_component('y ~ x', data_linear)
            for coeff, true_val in zip(coeffs, [true_intercept, true_slope]):
                self.assertAlmostEqual(coeff.tag.test_value, true_val, 1)
            sigma = Uniform('sigma', 0, 20)
            y_obs = Normal('y_obs', mu=y_est, sd=sigma, observed=y_linear)
            start = find_MAP(vars=[sigma])
            step = Slice(model.vars)
            trace = sample(2000, step, start, progressbar=False)

            self.assertAlmostEqual(np.mean(trace['Intercept']), true_intercept, 1)
            self.assertAlmostEqual(np.mean(trace['x']), true_slope, 1)
            self.assertAlmostEqual(np.mean(trace['sigma']), true_sd, 1)

    @unittest.skip("Fails only on travis. Investigate")
    def test_glm(self):
        with Model() as model:
            vars = glm.glm('y ~ x', data_linear)
            for coeff, true_val in zip(vars[1:], [true_intercept, true_slope, true_sd]):
                self.assertAlmostEqual(coeff.tag.test_value, true_val, 1)
            step = Slice(model.vars)
            trace = sample(2000, step, progressbar=False)

            self.assertAlmostEqual(np.mean(trace['Intercept']), true_intercept, 1)
            self.assertAlmostEqual(np.mean(trace['x']), true_slope, 1)
            self.assertAlmostEqual(np.mean(trace['sigma']), true_sd, 1)

    def test_glm_link_func(self):
        with Model() as model:
            vars = glm.glm('y ~ x', data_logistic,
                           family=glm.families.Binomial(link=glm.links.Logit))

            for coeff, true_val in zip(vars[1:], [true_intercept, true_slope]):
                self.assertAlmostEqual(coeff.tag.test_value, true_val, 0)
            step = Slice(model.vars)
            trace = sample(2000, step, progressbar=False)

            self.assertAlmostEqual(np.mean(trace['Intercept']), true_intercept, 1)
            self.assertAlmostEqual(np.mean(trace['x']), true_slope, 0)

########NEW FILE########
__FILENAME__ = test_hmc
import pymc as pm

from . import models
from pymc.step_methods.hmc import leapfrog, Hamiltonian
from .checks import *


def test_leapfrog_reversible():
    n = 3
    start, model, _ = models.non_normal(n)

    with model:
        h = pm.find_hessian(start, model=model)
        step = pm.HamiltonianMC(model.vars, h, model=model)

    bij = pm.DictToArrayBijection(step.ordering, start)

    logp, dlogp = list(map(bij.mapf, step.fs))
    H = Hamiltonian(logp, dlogp, step.potential)

    q0 = bij.map(start)
    p0 = np.ones(n)*.05
    for e in [.01, .1, 1.2]:
        for L in [1, 2, 3, 4, 20]:

            q, p = q0, p0
            q, p = leapfrog(H, q, p, L, e)
            q, p = leapfrog(H, q, -p, L, e)

            close_to(q, q0, 1e-8, str((L, e)))
            close_to(-p, p0, 1e-8, str((L, e)))

########NEW FILE########
__FILENAME__ = test_memo
from pymc.memoize import memoize

def getmemo():
    @memoize
    def f(a, b=['a']):
        return str(a) + str(b)
    return f


def test_memo():
    f = getmemo()

    assert f('x', ['y', 'z']) == "x['y', 'z']"
    assert f('x', ['a', 'z']) == "x['a', 'z']"
    assert f('x', ['y', 'z']) == "x['y', 'z']"

########NEW FILE########
__FILENAME__ = test_model_func
import pymc as pm
from .models import *
from .checks import *


def test_lop():
    start, model, _ = simple_model()

    lp = model.fastlogp

    lp(start)


def test_dlogp():
    start, model, (mu, sig) = simple_model()
    dlogp = model.fastdlogp()

    close_to(dlogp(start), -(start['x'] - mu) / sig, 1. / sig / 100.)


def test_dlogp2():
    start, model, (mu, sig) = mv_simple()
    H = np.linalg.inv(sig)

    d2logp = model.fastd2logp()

    close_to(d2logp(start), H, np.abs(H / 100.))


def test_deterministic():
    with pm.Model() as model:
        x = Normal('x', 0, 1)
        y = pm.Deterministic('y', x**2)

    assert model.y == y
    assert model['y'] == y

########NEW FILE########
__FILENAME__ = test_ndarray_backend
import unittest
import numpy as np
import numpy.testing as npt
from pymc.tests import backend_fixtures as bf
from pymc.backends import base, ndarray


class TestNDArray0dSampling(bf.SamplingTestCase):
    backend = ndarray.NDArray
    name = None
    shape = ()


class TestNDArray1dSampling(bf.SamplingTestCase):
    backend = ndarray.NDArray
    name = None
    shape = 2


class TestNDArray2dSampling(bf.SamplingTestCase):
    backend = ndarray.NDArray
    name = None
    shape = (2, 3)


class TestNDArray0dSelection(bf.SelectionTestCase):
    backend = ndarray.NDArray
    name = None
    shape = ()


class TestNDArray1dSelection(bf.SelectionTestCase):
    backend = ndarray.NDArray
    name = None
    shape = 2


class TestNDArray2dSelection(bf.SelectionTestCase):
    backend = ndarray.NDArray
    name = None
    shape = (2, 3)


class TestMultiTrace(bf.ModelBackendSetupTestCase):
    name = None
    backend = ndarray.NDArray
    shape = ()

    def setUp(self):
        self.chain = 0
        super(TestMultiTrace, self).setUp()
        self.trace0 = self.trace

        self.chain = 1
        super(TestMultiTrace, self).setUp()
        self.trace1 = self.trace

    def test_multitrace_nonunique(self):
        self.assertRaises(ValueError,
                          base.MultiTrace, [self.trace0, self.trace1])

    def test_merge_traces_nonunique(self):
        mtrace0 = base.MultiTrace([self.trace0])
        mtrace1 = base.MultiTrace([self.trace1])

        self.assertRaises(ValueError,
                          base.merge_traces, [mtrace0, mtrace1])



class TestSqueezeCat(unittest.TestCase):

    def setUp(self):
        self.x = np.arange(10)
        self.y = np.arange(10, 20)

    def test_combine_false_squeeze_false(self):
        expected = [self.x, self.y]
        result = base._squeeze_cat([self.x, self.y], False, False)
        npt.assert_equal(result, expected)

    def test_combine_true_squeeze_false(self):
        expected = [np.concatenate([self.x, self.y])]
        result = base._squeeze_cat([self.x, self.y], True, False)
        npt.assert_equal(result, expected)

    def test_combine_false_squeeze_true_more_than_one_item(self):
        expected = [self.x, self.y]
        result = base._squeeze_cat([self.x, self.y], False, True)
        npt.assert_equal(result, expected)

    def test_combine_false_squeeze_true_one_item(self):
        expected = self.x
        result = base._squeeze_cat([self.x], False, True)
        npt.assert_equal(result, expected)

    def test_combine_true_squeeze_true(self):
        expected = np.concatenate([self.x, self.y])
        result = base._squeeze_cat([self.x, self.y], True, True)
        npt.assert_equal(result, expected)

########NEW FILE########
__FILENAME__ = test_plots
import matplotlib
matplotlib.use('Agg', warn=False)

import numpy as np 
from .checks import close_to

import pymc.plots
from pymc.plots import *
from pymc import Slice, Metropolis, find_hessian, sample


def test_plots():

    # Test single trace
    from pymc.examples import arbitrary_stochastic as asmod

    with asmod.model as model:

        start = model.test_point
        h = find_hessian(start)
        step = Metropolis(model.vars, h)
        trace = sample(3000, step, start)

        traceplot(trace)
        forestplot(trace)

        autocorrplot(trace)

def test_plots_multidimensional():

    # Test single trace
    from .models import multidimensional_model


    start, model, _ = multidimensional_model()
    with model as model:
        h = np.diag(find_hessian(start))
        step = Metropolis(model.vars, h)
        trace = sample(3000, step, start)

        traceplot(trace)
        #forestplot(trace)
        #autocorrplot(trace)


def test_multichain_plots():

    from pymc.examples import disaster_model as dm

    with dm.model as model:
        # Run sampler
        step1 = Slice([dm.early_mean, dm.late_mean])
        step2 = Metropolis([dm.switchpoint])
        start = {'early_mean': 2., 'late_mean': 3., 'switchpoint': 50}
        ptrace = sample(1000, [step1, step2], start, njobs=2)

    forestplot(ptrace, vars=['early_mean', 'late_mean'])

    autocorrplot(ptrace, vars=['switchpoint'])

def test_make_2d(): 

    a = np.arange(4)
    close_to(pymc.plots.make_2d(a), a[:,None], 0)

    n = 7
    a = np.arange(n*4*5).reshape((n,4,5))
    res = pymc.plots.make_2d(a)

    assert res.shape == (n,20)
    close_to(a[:,0,0], res[:,0], 0)
    close_to(a[:,3,2], res[:,2*4+3], 0)

########NEW FILE########
__FILENAME__ = test_sampling
import numpy as np
import numpy.testing as npt
try:
    import unittest.mock as mock  # py3
except ImportError:
    import mock
import unittest

import pymc
from pymc import sampling
from pymc.sampling import sample
from .models import simple_init

# Test if multiprocessing is available
import multiprocessing
try:
    multiprocessing.Pool(2)
    test_parallel = False
except:
    test_parallel = False


def test_sample():

    model, start, step, _ = simple_init()

    test_njobs = [1]

    if test_parallel:
        test_samplers.append(psample)

    with model:
        for njobs in test_njobs:
            for n in [1, 10, 300]:
                yield sample, n, step, {}, None, njobs


def test_iter_sample():
    model, start, step, _ = simple_init()
    samps = sampling.iter_sample(5, step, start, model=model)
    for i, trace in enumerate(samps):
        assert i == len(trace) - 1, "Trace does not have correct length."


def test_soft_update_all_present():
    start = {'a': 1, 'b': 2}
    test_point = {'a': 3, 'b': 4}
    sampling._soft_update(start, test_point)
    assert start == {'a': 1, 'b': 2}


def test_soft_update_one_missing():
    start = {'a': 1, }
    test_point = {'a': 3, 'b': 4}
    sampling._soft_update(start, test_point)
    assert start == {'a': 1, 'b': 4}


def test_soft_update_empty():
    start = {}
    test_point = {'a': 3, 'b': 4}
    sampling._soft_update(start, test_point)
    assert start == test_point


class TestChooseBackend(unittest.TestCase):

    def test_choose_backend_none(self):
        with mock.patch('pymc.sampling.NDArray') as nd:
            sampling._choose_backend(None, 'chain')
        self.assertTrue(nd.called)

    def test_choose_backend_list_of_variables(self):
        with mock.patch('pymc.sampling.NDArray') as nd:
            sampling._choose_backend(['var1', 'var2'], 'chain')
        nd.assert_called_with(vars=['var1', 'var2'])

    def test_choose_backend_invalid(self):
        self.assertRaises(ValueError,
                          sampling._choose_backend,
                          'invalid', 'chain')

    def test_choose_backend_shortcut(self):
        backend = mock.Mock()
        shortcuts = {'test_backend': {'backend': backend,
                                      'name': None}}
        sampling._choose_backend('test_backend', 'chain', shortcuts=shortcuts)
        self.assertTrue(backend.called)

########NEW FILE########
__FILENAME__ = test_special_functions
from theano import function
import theano.tensor as t

import pymc.distributions.special as ps
import scipy.special as ss
import numpy as np

from .checks import close_to


def test_functions():
    xvals = list(map(np.atleast_1d, [.01, .1, 2, 100, 10000]))

    x = t.dvector('x')
    x.tag.test_value = xvals[0]

    p = t.iscalar('p')
    p.tag.test_value = 1

    gammaln = function([x], ps.gammaln(x))
    psi = function([x], ps.psi(x))
    multigammaln = function([x, p], ps.multigammaln(x, p))

    for x in xvals:
        yield check_vals, gammaln, ss.gammaln, x
    for x in xvals[1:]:
        yield check_vals, psi, ss.psi, x

"""
scipy.special.multigammaln gives bad values if you pass a non scalar to a
In [14]:

    import scipy.special
    scipy.special.multigammaln([2.1], 3)
    Out[14]:
        array([ 1.76253257,  1.60450306,  1.66722239])
"""


def t_multigamma():
    xvals = list(map(np.atleast_1d, [0, .1, 2, 100]))

    x = t.dvector('x')
    x.tag.test_value = xvals[0]

    p = t.iscalar('p')
    p.tag.test_value = 1

    multigammaln = function([x, p], ps.multigammaln(x, p))

    def ssmultigammaln(a, b):
        return ss.multigammaln(a[0], b)

    for p in [0, 1, 2, 3, 4, 100]:
        for x in xvals:
            yield check_vals, multigammaln, ssmultigammaln, x, p


def check_vals(fn1, fn2, *args):
    v = fn1(*args)
    close_to(v, fn2(*args), 1e-6)

########NEW FILE########
__FILENAME__ = test_sqlite_backend
import numpy.testing as npt
from pymc.tests import backend_fixtures as bf
from pymc.backends import ndarray, sqlite


class TestSQlite0dSampling(bf.SamplingTestCase):
    backend = sqlite.SQLite
    name = 'test.db'
    shape = ()


class TestSQlite1dSampling(bf.SamplingTestCase):
    backend = sqlite.SQLite
    name = 'test.db'
    shape = 2


class TestSQlite2dSampling(bf.SamplingTestCase):
    backend = sqlite.SQLite
    name = 'test.db'
    shape = (2, 3)


class TestSQLite0dSelection(bf.SelectionNoSliceTestCase):
    backend = sqlite.SQLite
    name = 'test.db'
    shape = ()


class TestSQLite1dSelection(bf.SelectionNoSliceTestCase):
    backend = sqlite.SQLite
    name = 'test.db'
    shape = 2


class TestSQLite2dSelection(bf.SelectionNoSliceTestCase):
    backend = sqlite.SQLite
    name = 'test.db'
    shape = (2, 3)


class TestSQLiteDumpLoad(bf.DumpLoadTestCase):
    backend = sqlite.SQLite
    load_func = staticmethod(sqlite.load)
    name = 'test.db'
    shape = (2, 3)


class TestNDArraySqliteEquality(bf.BackendEqualityTestCase):
    backend0 = ndarray.NDArray
    name0 = None
    backend1 = sqlite.SQLite
    name1 = 'test.db'
    shape = (2, 3)


def test_create_column_0d():
    shape = ()
    result = sqlite._create_colnames(shape)
    expected = ['v1']
    assert result == expected
    assert sqlite._create_shape(result) == shape


def test_create_column_1d():
    shape = 2,
    result = sqlite._create_colnames(shape)
    expected = ['v1', 'v2']
    assert result == expected
    assert sqlite._create_shape(result) == shape


def test_create_column_2d():
    shape = 2, 3
    result = sqlite._create_colnames(shape)
    expected = ['v1_1', 'v1_2', 'v1_3',
                'v2_1', 'v2_2', 'v2_3']
    assert result == expected
    assert sqlite._create_shape(result) == shape


def test_create_column_3d():
    shape = 2, 3, 4
    assert sqlite._create_shape(sqlite._create_colnames(shape)) == shape

########NEW FILE########
__FILENAME__ = test_starting
from .checks import *
from numpy import inf
from pymc.tuning import starting
from pymc import find_MAP, Point, Model
from pymc import Model, Uniform, Normal, Beta, Binomial
from .models import simple_init, simple_model, non_normal, exponential_beta

def test_accuracy_normal():
    _, model, (mu, _) = simple_model()

    with model: 
        newstart = find_MAP(Point(x = [-10.5, 100.5]))
        close_to(newstart['x'], [mu, mu], 1e-5)


def test_accuracy_non_normal():
    _, model, (mu, _) = non_normal(4)

    with model: 
        newstart = find_MAP(Point(x = [.5, .01, .95, .99]))
        close_to(newstart['x'], mu, 1e-5)

def test_errors():
    _, model, _ = exponential_beta(2)

    with model: 
        try : 
            newstart = find_MAP(Point(x = [-.5, .01], y = [.5, 4.4]))
        except ValueError as e:
            msg = str(e) 
            assert "x.logp" in msg, msg
            assert "x.value" not in msg, msg
        else:
            assert False, newstart

def test_find_MAP_discrete():
    tol = 2.0**-11
    alpha = 4
    beta = 4
    n = 20
    yes = 15

    with Model() as model:
        p = Beta('p', alpha, beta)
        ss = Binomial('ss', n=n, p=p)
        s = Binomial('s', n=n, p=p, observed=yes)

        map_est1 = starting.find_MAP()
        map_est2 = starting.find_MAP(vars=model.vars)

    close_to(map_est1['p'], 0.6086956533498806, tol)

    close_to(map_est2['p'], 0.695642178810167, tol)
    assert map_est2['ss'] == 14


def test_find_MAP():
    tol = 2.0**-11  # 16 bit machine epsilon, a low bar
    data = np.random.randn(100)
    # data should be roughly mean 0, std 1, but let's
    # normalize anyway to get it really close
    data = (data-np.mean(data))/np.std(data)

    with Model() as model:
        mu = Uniform('mu', -1, 1)
        sigma = Uniform('sigma', .5, 1.5)
        y = Normal('y', mu=mu, tau=sigma**-2, observed=data)

        # Test gradient minimization
        map_est1 = starting.find_MAP()
        # Test non-gradient minimization
        map_est2 = starting.find_MAP(fmin=starting.optimize.fmin_powell)

    close_to(map_est1['mu'], 0, tol)
    close_to(map_est1['sigma'], 1, tol)

    close_to(map_est2['mu'], 0, tol)
    close_to(map_est2['sigma'], 1, tol)

########NEW FILE########
__FILENAME__ = test_stats
from ..stats import *
from .models import Model, Normal, Metropolis
import numpy as np
import pymc as pm
from numpy.random import random, normal, seed
from numpy.testing import assert_equal, assert_almost_equal, assert_array_almost_equal

seed(111)
normal_sample = normal(0, 1, 1000000)

def test_autocorr():
    """Test autocorrelation and autocovariance functions"""

    assert_almost_equal(autocorr(normal_sample), 0, 2)

    y = [(normal_sample[i-1] + normal_sample[i])/2 for i in range(1, len(normal_sample))]
    assert_almost_equal(autocorr(y), 0.5, 2)

def test_hpd():
    """Test HPD calculation"""

    interval = hpd(normal_sample)

    assert_array_almost_equal(interval, [-1.96, 1.96], 2)

def test_make_indices():
    """Test make_indices function"""

    from ..stats import make_indices

    ind = [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2)]

    assert_equal(ind, make_indices((2, 3)))

def test_mc_error():
    """Test batch standard deviation function"""

    x = random(100000)

    assert(mc_error(x) < 0.0025)

def test_quantiles():
    """Test quantiles function"""

    q = quantiles(normal_sample)

    assert_array_almost_equal(sorted(q.values()), [-1.96, -0.67, 0, 0.67, 1.96], 2)


## For all the summary tests, the number of dimensions refer to the
## original variable dimensions, not the MCMC trace dimensions.


def test_summary_0d_variable_model():
    mu = -2.1
    tau = 1.3
    with Model() as model:
        x = Normal('x', mu, tau, testval=.1)
        step = Metropolis(model.vars, np.diag([1.]))
        trace = pm.sample(100, step=step)
    pm.summary(trace)


def test_summary_1d_variable_model():
    mu = -2.1
    tau = 1.3
    with Model() as model:
        x = Normal('x', mu, tau, shape=2, testval=[.1, .1])
        step = Metropolis(model.vars, np.diag([1.]))
        trace = pm.sample(100, step=step)
    pm.summary(trace)


def test_summary_2d_variable_model():
    mu = -2.1
    tau = 1.3
    with Model() as model:
        x = Normal('x', mu, tau, shape=(2, 2),
                   testval=np.tile(.1, (2, 2)))
        step = Metropolis(model.vars, np.diag([1.]))
        trace = pm.sample(100, step=step)
    pm.summary(trace)


def test_summary_format_values():
    roundto = 2
    summ = pm.stats._Summary(roundto)
    d = {'nodec': 1, 'onedec': 1.0, 'twodec': 1.00, 'threedec': 1.000}
    summ._format_values(d)
    for val in d.values():
        assert val == '1.00'


def test_stat_summary_format_hpd_values():
    roundto = 2
    summ = pm.stats._StatSummary(roundto, None, 0.05)
    d = {'nodec': 1, 'hpd': [1, 1]}
    summ._format_values(d)
    for key, val in d.items():
        if key == 'hpd':
            assert val == '[1.00, 1.00]'
        else:
            assert val == '1.00'


def test_calculate_stats_0d_variable():
    sample = np.arange(10)
    result = list(pm.stats._calculate_stats(sample, 5, 0.05))
    assert result[0] == ()
    assert len(result) == 2


def test_calculate_stats_variable_1d_variable():
    sample = np.arange(10).reshape(5, 2)
    result= list(pm.stats._calculate_stats(sample, 5, 0.05))
    assert result[0] == ()
    assert len(result) == 3

def test_calculate_pquantiles_0d_variable():
    sample = np.arange(10)[:, None]
    qlist = (0.25, 25, 50, 75, 0.98)
    result = list(pm.stats._calculate_posterior_quantiles(sample, qlist))
    assert result[0] == ()
    assert len(result) == 2


def test_stats_value_line():
    roundto = 1
    summ = pm.stats._StatSummary(roundto, None, 0.05)
    values = [{'mean': 0, 'sd': 1, 'mce': 2, 'hpd': [4, 4]},
              {'mean': 5, 'sd': 6, 'mce': 7, 'hpd': [8, 8]},]

    expected = ['0.0              1.0              2.0              [4.0, 4.0]',
                '5.0              6.0              7.0              [8.0, 8.0]']
    result = list(summ._create_value_output(values))
    assert result == expected


def test_post_quantile_value_line():
    roundto = 1
    summ = pm.stats._PosteriorQuantileSummary(roundto, 0.05)
    values = [{'lo': 0, 'q25': 1, 'q50': 2, 'q75': 4, 'hi': 5},
              {'lo': 6, 'q25': 7, 'q50': 8, 'q75': 9, 'hi': 10},]

    expected = ['0.0            1.0            2.0            4.0            5.0',
                '6.0            7.0            8.0            9.0            10.0']
    result = list(summ._create_value_output(values))
    assert result == expected


def test_stats_output_lines_0d_variable():
    roundto = 1
    x = np.arange(5)

    summ = pm.stats._StatSummary(roundto, 5, 0.05)

    expected = ['  Mean             SD               MC Error         95% HPD interval',
                '  -------------------------------------------------------------------',
                '  ',
                '  2.0              1.4              0.6              [0.0, 4.0]',]

    result = list(summ._get_lines(x))
    assert result == expected


def test_stats_output_lines_1d_variable():
    roundto = 1
    x = np.arange(10).reshape(5, 2)

    summ = pm.stats._StatSummary(roundto, 5, 0.05)

    expected = ['  Mean             SD               MC Error         95% HPD interval',
                '  -------------------------------------------------------------------',
                '  ',
                '  4.0              2.8              1.3              [0.0, 8.0]',
                '  5.0              2.8              1.3              [1.0, 9.0]',]
    result = list(summ._get_lines(x))
    assert result == expected


def test_stats_output_lines_2d_variable():
    roundto = 1
    x = np.arange(20).reshape(5, 2, 2)

    summ = pm.stats._StatSummary(roundto, 5, 0.05)

    expected = ['  Mean             SD               MC Error         95% HPD interval',
                '  -------------------------------------------------------------------',
                '  ..............................[0, :]...............................',
                '  8.0              5.7              2.5              [0.0, 16.0]',
                '  9.0              5.7              2.5              [1.0, 17.0]',
                '  ..............................[1, :]...............................',
                '  10.0             5.7              2.5              [2.0, 18.0]',
                '  11.0             5.7              2.5              [3.0, 19.0]',]
    result = list(summ._get_lines(x))
    assert result == expected


def test_posterior_quantiles_output_lines_0d_variable():
    roundto = 1
    x = np.arange(5)

    summ = pm.stats._PosteriorQuantileSummary(roundto, 0.05)

    expected = ['  Posterior quantiles:',
                '  2.5            25             50             75             97.5',
                '  |--------------|==============|==============|--------------|',
                '  ',
                '  0.0            1.0            2.0            3.0            4.0',]

    result = list(summ._get_lines(x))
    assert result == expected


def test_posterior_quantiles_output_lines_1d_variable():
    roundto = 1
    x = np.arange(10).reshape(5, 2)

    summ = pm.stats._PosteriorQuantileSummary(roundto, 0.05)

    expected = ['  Posterior quantiles:',
                '  2.5            25             50             75             97.5',
                '  |--------------|==============|==============|--------------|',
                '  ',
                '  0.0            2.0            4.0            6.0            8.0',
                '  1.0            3.0            5.0            7.0            9.0']

    result = list(summ._get_lines(x))
    assert result == expected


def test_posterior_quantiles_output_lines_2d_variable():
    roundto = 1
    x = np.arange(20).reshape(5, 2, 2)

    summ = pm.stats._PosteriorQuantileSummary(roundto, 0.05)

    expected = ['  Posterior quantiles:',
                '  2.5            25             50             75             97.5',
                '  |--------------|==============|==============|--------------|',
                '  .............................[0, :].............................',
                '  0.0            4.0            8.0            12.0           16.0',
                '  1.0            5.0            9.0            13.0           17.0',
                '  .............................[1, :].............................',
                '  2.0            6.0            10.0           14.0           18.0',
                '  3.0            7.0            11.0           15.0           19.0',]

    result = list(summ._get_lines(x))
    assert result == expected


def test_groupby_leading_idxs_0d_variable():
    result = {k: list(v) for k, v in pm.stats._groupby_leading_idxs(())}
    assert list(result.keys()) == [()]
    assert result[()] == [()]


def test_groupby_leading_idxs_1d_variable():
    result = {k: list(v) for k, v in pm.stats._groupby_leading_idxs((2,))}
    assert list(result.keys()) == [()]
    assert result[()] == [(0,), (1,)]


def test_groupby_leading_idxs_2d_variable():
    result = {k: list(v) for k, v in pm.stats._groupby_leading_idxs((2, 3))}

    expected_keys = [(0,), (1,)]
    keys = list(result.keys())
    assert len(keys) == len(expected_keys)
    for key in keys:
        assert result[key] == [key + (0,), key + (1,), key + (2,)]


def test_groupby_leading_idxs_3d_variable():
    result = {k: list(v) for k, v in pm.stats._groupby_leading_idxs((2, 3, 2))}

    expected_keys = [(0, 0), (0, 1), (0, 2),
                     (1, 0), (1, 1), (1, 2)]
    keys = list(result.keys())
    assert len(keys) == len(expected_keys)
    for key in keys:
        assert result[key] == [key + (0,), key + (1,)]

########NEW FILE########
__FILENAME__ = test_step
from .checks import *
from .models import simple_model, mv_simple, mv_simple_discrete
from theano.tensor import constant
from scipy.stats.mstats import moment


def check_stat(name, trace, var, stat, value, bound):
    s = stat(trace[var][2000:], axis=0)
    close_to(s, value, bound)


def test_step_continuous():
    start, model, (mu, C) = mv_simple()

    with model:
        hmc = pm.HamiltonianMC(scaling=C, is_cov=True)
        mh = pm.Metropolis(S=C,
                           proposal_dist=pm.MultivariateNormalProposal)
        slicer = pm.Slice()
        nuts = pm.NUTS(scaling=C, is_cov=True)
        compound = pm.CompoundStep([hmc, mh])


    steps = [mh, hmc, slicer, nuts, compound]

    unc = np.diag(C) ** .5
    check = [('x', np.mean, mu, unc / 10.),
             ('x', np.std, unc, unc / 10.)]

    for st in steps:
        h = sample(8000, st, start, model=model, random_seed=1)
        for (var, stat, val, bound) in check:
            yield check_stat, repr(st), h, var, stat, val, bound

def test_step_discrete():
    start, model, (mu, C) = mv_simple_discrete()

    with model:
        mh = pm.Metropolis(S=C,
                           proposal_dist=pm.MultivariateNormalProposal)
        slicer = pm.Slice()


    steps = [mh]

    unc = np.diag(C) ** .5
    check = [('x', np.mean, mu, unc / 10.),
             ('x', np.std, unc, unc / 10.)]

    for st in steps:
        h = sample(20000, st, start, model=model, random_seed=1)

        for (var, stat, val, bound) in check:
            yield check_stat, repr(st), h, var, stat, val, bound

########NEW FILE########
__FILENAME__ = test_text_backend
import numpy.testing as npt
from pymc.tests import backend_fixtures as bf
from pymc.backends import ndarray, text


class TestTextDumpLoad(bf.DumpLoadTestCase):
    backend = text.Text
    load_func = staticmethod(text.load)
    name = 'text-db'
    shape = (2, 3)


class TestTextDumpFunction(bf.BackendEqualityTestCase):
    backend0 = backend1 = ndarray.NDArray
    name0 = None
    name1 = 'text-db'
    shape = (2, 3)

    @classmethod
    def setUpClass(cls):
        super(TestTextDumpFunction, cls).setUpClass()
        text.dump(cls.name1, cls.mtrace1)
        with cls.model:
            cls.mtrace1 = text.load(cls.name1)


def test__chain_dir_to_chain():
    assert text._chain_dir_to_chain('/path/to/chain-0') == 0
    assert text._chain_dir_to_chain('chain-0') == 0

########NEW FILE########
__FILENAME__ = test_tuning
import numpy as np
from numpy import inf
from pymc.tuning import scaling
from pymc.tests.checks import close_to
from . import models

a = np.array([-10, -.01, 0, 10, 1e300, -inf, inf])


def test_adjust_precision():
    a1 = scaling.adjust_precision(a)

    assert all((a1 > 0) & (a1 < 1e200))


def test_guess_scaling():

    start, model, _ = models.non_normal(n=5)
    a1 = scaling.guess_scaling(start, model=model)

    assert all((a1 > 0) & (a1 < 1e200))

########NEW FILE########
__FILENAME__ = theanof
from .vartypes import typefilter, continuous_types
from theano import theano, tensor as t
from theano.gof.graph import inputs
from .memoize import memoize

__all__ = ['gradient', 'hessian', 'hessian_diag', 'inputvars', 'cont_inputs']

def inputvars(a):
    """
    Get the inputs into a theano variables

    Parameters
    ----------
        a : theano variable

    Returns
    -------
        r : list of tensor variables that are inputs
    """
    return [v for v in inputs(makeiter(a)) if isinstance(v, t.TensorVariable)]

def cont_inputs(f):
    """
    Get the continuous inputs into a theano variables

    Parameters
    ----------
        a : theano variable

    Returns
    -------
        r : list of tensor variables that are continuous inputs
    """
    return typefilter(inputvars(f), continuous_types)



"""
Theano derivative functions
"""
def gradient1(f, v):
    """flat gradient of f wrt v"""
    return t.flatten(t.grad(f, v, disconnected_inputs='warn'))


@memoize
def gradient(f, vars=None):
    if vars is None:
        vars = cont_inputs(f)

    return t.concatenate([gradient1(f, v) for v in vars], axis=0)


def jacobian1(f, v):
    """jacobian of f wrt v"""
    f = t.flatten(f)
    idx = t.arange(f.shape[0])

    def grad_i(i):
        return gradient1(f[i], v)

    return theano.map(grad_i, idx)[0]


@memoize
def jacobian(f, vars=None):
    if vars is None:
        vars = cont_inputs(f)

    return t.concatenate([jacobian1(f, v) for v in vars], axis=1)


@memoize
def hessian(f, vars=None):
    return -jacobian(gradient(f, vars), vars)


def hessian_diag1(f, v):

    g = gradient1(f, v)
    idx = t.arange(g.shape[0])

    def hess_ii(i):
        return gradient1(g[i], v)[i]

    return theano.map(hess_ii, idx)[0]


@memoize
def hessian_diag(f, vars=None):
    if vars is None:
        vars = cont_inputs(f)

    return -t.concatenate([hessian_diag1(f, v) for v in vars], axis=0)


def makeiter(a):
    if isinstance(a, (tuple, list)):
        return a
    else:
        return [a]

########NEW FILE########
__FILENAME__ = scaling
'''
Created on Mar 12, 2011

from __future__ import division
@author: johnsalvatier
'''
import numpy as np
from numpy import exp, log, sqrt
from ..core import *

__all__ = ['approx_hessian', 'find_hessian', 'trace_cov', 'guess_scaling']


def approx_hessian(point, vars=None, model=None):
    """
    Returns an approximation of the Hessian at the current chain location.

    Parameters
    ----------
    model : Model (optional if in `with` context)
    point : dict
    vars : list
        Variables for which Hessian is to be calculated.
    """
    from numdifftools import Jacobian

    model = modelcontext(model)
    if vars is None:
        vars = model.cont_vars

    point = Point(point, model=model)

    bij = DictToArrayBijection(ArrayOrdering(vars), point)
    dlogp = bij.mapf(model.fastdlogp(vars))

    def grad_logp(point):
        return np.nan_to_num(dlogp(point))

    '''
    Find the jacobian of the gradient function at the current position
    this should be the Hessian; invert it to find the approximate
    covariance matrix.
    '''
    return -Jacobian(grad_logp)(bij.map(point))


def find_hessian(point, vars=None, model=None):
    """
    Returns Hessian of logp at the point passed.

    Parameters
    ----------
    model : Model (optional if in `with` context)
    point : dict
    vars : list
        Variables for which Hessian is to be calculated.
    """
    model = modelcontext(model)
    H = model.fastd2logp(vars)
    return H(Point(point, model=model))

def find_hessian_diag(point, vars=None, model=None):
    """
    Returns Hessian of logp at the point passed.

    Parameters
    ----------
    model : Model (optional if in `with` context)
    point : dict
    vars : list
        Variables for which Hessian is to be calculated.
    """
    model = modelcontext(model)
    H = model.fastfn(hessian_diag(model.logpt, vars))
    return H(Point(point, model=model))

def guess_scaling(point, vars=None, model=None):
    model = modelcontext(model)
    h = find_hessian_diag(point, vars, model=model)
    return adjust_scaling(h)

def adjust_scaling(s):
    if s.ndim < 2:
        return adjust_precision(s)
    else:
        val, vec = np.linalg.eigh(s)
        val = adjust_precision(val)
        return eig_recompose(val, vec)

def adjust_precision(tau):
    mag = sqrt(abs(tau))

    bounded = bound(log(mag), log(1e-10), log(1e10))
    return exp(bounded)**2


def bound(a, l, u):
    return np.maximum(np.minimum(a, u), l)

def eig_recompose(val, vec):
    return vec.dot(np.diag(val)).dot(vec.T)


def trace_cov(trace, vars=None):
    """
    Calculate the flattened covariance matrix using a sample trace

    Useful if you want to base your covariance matrix for further sampling on some initial samples.

    Parameters
    ----------
    trace : Trace
    vars : list
        variables for which to calculate covariance matrix

    Returns
    -------
    r : array (n,n)
        covariance matrix
    """

    if vars is None:
        vars = trace.samples.keys

    def flat_t(var):
        x = trace[str(var)]
        return x.reshape((x.shape[0], np.prod(x.shape[1:])))

    return np.cov(np.concatenate(list(map(flat_t, vars)), 1).T)

########NEW FILE########
__FILENAME__ = starting
'''
Created on Mar 12, 2011

@author: johnsalvatier
'''
from scipy import optimize
import numpy as np
from numpy import isfinite, nan_to_num, logical_not
from ..core import *
from ..vartypes import discrete_types, typefilter

from inspect import getargspec

__all__ = ['find_MAP', 'scipyminimize']


def find_MAP(start=None, vars=None, fmin=None, return_raw=False,
             disp=False, model=None, *args, **kwargs):
    """
    Sets state to the local maximum a posteriori point given a model.
    Current default of fmin_Hessian does not deal well with optimizing close
    to sharp edges, especially if they are the minimum.

    Parameters
    ----------
    start : `dict` of parameter values (Defaults to `model.test_point`)
    vars : list
        List of variables to set to MAP point (Defaults to all continuous).
    fmin : function
        Optimization algorithm (Defaults to `scipy.optimize.fmin_bfgs` unless
        discrete variables are specified in `vars`, then
        `scipy.optimize.fmin_powell` which will perform better).
    return_raw : Bool
        Whether to return extra value returned by fmin (Defaults to `False`)
    disp : Bool
        Display helpful warnings, and verbose output of `fmin` (Defaults to
        `False`)
    model : Model (optional if in `with` context)
    *args, **kwargs
        Extra args passed to fmin
    """
    model = modelcontext(model)
    if start is None:
        start = model.test_point

    if vars is None:
        vars = model.cont_vars

    disc_vars = list(typefilter(vars, discrete_types))

    if disc_vars and disp:
        print("Warning: vars contains discrete variables. MAP " +
              "estimates may not be accurate for the default " +
              "parameters. Defaulting to non-gradient minimization " +
              "fmin_powell.")

    if fmin is None:
        if disc_vars:
            fmin = optimize.fmin_powell
        else:
            fmin = optimize.fmin_bfgs

    allinmodel(vars, model)

    start = Point(start, model=model)
    bij = DictToArrayBijection(ArrayOrdering(vars), start)

    logp = bij.mapf(model.fastlogp)
    dlogp = bij.mapf(model.fastdlogp(vars))

    def logp_o(point):
        return nan_to_high(-logp(point))

    def grad_logp_o(point):
        return nan_to_num(-dlogp(point))

    # Check to see if minimization function actually uses the gradient
    if 'fprime' in getargspec(fmin).args:
        r = fmin(logp_o, bij.map(
            start), fprime=grad_logp_o, disp=disp, *args, **kwargs)
    else:
        r = fmin(logp_o, bij.map(start), disp=disp, *args, **kwargs)

    if isinstance(r, tuple):
        mx0 = r[0]
    else:
        mx0 = r

    mx = bij.rmap(mx0)

    if (not allfinite(mx0) or
        not allfinite(model.logp(mx)) or
        not allfinite(model.dlogp()(mx))):


        messages = []
        for var in vars:

            vals = { 
                "value"   : mx[var.name],
                "logp"    : var.logp(mx),
                "dlogp"   : var.dlogp()(mx) }

            def message(name, values):
                if np.size(values) < 10:
                    return name + " bad: " + str(values)
                else:
                    idx = np.nonzero(logical_not(isfinite(values)))
                    return name + " bad at idx: " + str(idx) + " with values: " + str(values[idx])

            messages += [ 
                message(var.name + "." + k, v)
                for k,v in vals.items()
                if not allfinite(v)]

        specific_errors = '\n'.join(messages)
        raise ValueError("Optimization error: max, logp or dlogp at " +
                         "max have non-finite values. Some values may be " +
                         "outside of distribution support. max: " +
                         repr(mx) + " logp: " + repr(model.logp(mx)) +
                         " dlogp: " + repr(model.dlogp()(mx)) + "Check that " +
                         "1) you don't have hierarchical parameters, " +
                         "these will lead to points with infinite " +
                         "density. 2) your distribution logp's are " +
                         "properly specified. Specific issues: \n" + 
                         specific_errors)
    mx = {v.name: np.floor(mx[v.name]) if v.dtype in discrete_types else
          mx[v.name] for v in model.vars}
    if return_raw:
        return mx, r
    else:
        return mx


def allfinite(x):
    return np.all(isfinite(x))


def nan_to_high(x):
    return np.where(isfinite(x), x, 1.0e100)


def scipyminimize(f, x0, fprime, *args, **kwargs):
    r = scipy.optimize.minimize(f, x0, jac=fprime, *args, **kwargs)
    return r.x, r


def allinmodel(vars, model):
    notin = [v for v in vars if v not in model.vars]
    if notin:
        raise ValueError("Some variables not in the model: " + str(notin))

########NEW FILE########
__FILENAME__ = vartypes

bool_types = set(['int8'])

int_types = set(['int8',
                 'int16',
                 'int32',
                 'int64',
                 'uint8',
                 'uint16',
                 'uint32',
                 'uint64'])
float_types = set(['float32',
                   'float64'])
complex_types = set(['complex64',
                     'complex128'])
continuous_types = float_types | complex_types
discrete_types = bool_types | int_types

default_type = {'discrete': 'int64',
                'continuous': 'float64'}


def typefilter(vars, types):
    # Returns variables of type `types` from `vars`
    return filter(lambda v: v.dtype in types, vars)

########NEW FILE########
