__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# pysal documentation build configuration file, created by
# sphinx-quickstart on Wed Aug 26 19:58:20 2009.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys 
import os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.append(os.path.abspath('sphinxext'))
#sys.path.append(os.path.abspath('../../'))
sys.path.append(os.path.abspath('~/anaconda/lib/python2.7/site-packages/'))

# -- General configuration -----------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc',
'sphinx.ext.doctest','sphinx.ext.graphviz', 'sphinx.ext.intersphinx',
'sphinx.ext.pngmath',
'sphinx.ext.autosummary','sphinx.ext.viewcode', 'sphinxcontrib.napoleon']

#doctest extension config values
doctest_path = '/Users/stephens/code/pysal/doc/source/users/tutorials/' 
doctest_test_doctest_blocks = ''
#doctest_global_setup = 

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'
#source_suffix = '.txt'

# The encoding of source files.
#source_encoding = 'utf-8'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'pysal'
copyright = u'2009-14, Sergio Rey'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '1.8.0dev'
# The full version, including alpha/beta/rc tags.
release = '1.8.0dev'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of documents that shouldn't be included in the build.
#unused_docs = []

# List of directories, relative to source directory, that shouldn't be searched
# for source files.
exclude_trees = []


# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
add_function_parentheses = False

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------
#custom strikethrough setup
#html_style = 'mydoc.css'

# The theme to use for HTML and HTML Help pages.  Major themes that come with
# Sphinx are currently 'default' and 'sphinxdoc'.
html_theme = 'default'
#html_theme = 'sphinxdoc'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
html_theme_options = {
        "rightsidebar": "true",
        "relbarbgcolor": "CornflowerBlue",
        "sidebartextcolor": "black",
        "sidebarlinkcolor": "#355f7c",
        "sidebarbgcolor": "#F2F2F2",
        "codebgcolor": "AliceBlue",
        "footerbgcolor": "Black",
        "externalrefs": "false",
        "bodyfont": "Optima",
        "headfont": "Optima "
        }

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = "%s v%s Reference Guide" % (project, version)
html_title = "Python Spatial Analysis Library" 

# A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = "PySAL"

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
html_favicon = 'favicon.png'

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_use_modindex = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'pysaldoc'


# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'pysal.tex', u'pysal Documentation',
   u'Serge Rey', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_use_modindex = True


# Example configuration for intersphinx: refer to the Python standard library.
intersphinx_mapping = {'http://docs.python.org/': None}

#numpydoc option
numpydoc_show_class_members = True
numpydoc_class_members_toctree = False

########NEW FILE########
__FILENAME__ = collection
"""
Computational geometry code for PySAL: Python Spatial Analysis Library.

Authors:
Sergio Rey <srey@asu.edu>
Xinyue Ye <xinyue.ye@gmail.com>
Charles Schmidt <schmidtc@gmail.com>
Andrew Winslow <Andrew.Winslow@asu.edu>

Not to be used without permission of the authors.

Style Guide, Follow:
http://www.python.org/dev/peps/pep-0008/


Class comment format:

    Brief class description.

    Attributes:
    attr 1 -- type -- description of attr 1
    attr 2 -- type -- description of attr 2

    Extras (notes, references, examples, doctest, etc.)


Function comment format:

    Brief function description.

    function(arg 1 type, arg 2 type, keyword=keyword arg 3 type) -> return type

    Argument:
    arg 1 -- description of arg 1
    arg 2 -- description of arg 2

    Keyword Arguments:
    arg 3 -- description of arg 3

    Extras (notes, references, examples, doctest, etc.)
"""

__author__ = "Sergio J. Rey, Xinyue Ye, Charles Schmidt, Andrew Winslow"
__credits__ = "Copyright (c) 2005-2009 Sergio J. Rey"

import doctest


class GeometryCollection:
    """
    Represents a collection of geometric objects. Useful for performing various
    queries on the collection as a whole (see attributes/methods).
    """

    def __init__(self, objs):
        """
        Creates a GeometryCollection of a set of objects.

        __init__(list of x) -> GeometryCollection

        objs -- a list of geometric objects (from pysal.cg.shapes).

        Examples
        --------
        >>> import pysal.cg.shapes as shapes
        >>> g = GeometryCollection([shapes.Point((0, 0)), shapes.LineSegment(Point((1, 4)), Point((7, 3)))])
        """
        pass

    @property
    def centroid(self):
        """
        Returns the centroid of the collection.

        centroid -> number n-tuple, n > 1

        Examples
        --------
        >>> import pysal.cg.shapes as shapes
        >>> g = GeometryCollection([shapes.Point((0, 0)), shapes.Point((1, 1))])
        >>> g.centroid
        (0.5, 0.5)
        """
        pass

    @property
    def _get_bounding_box(self):
        """
        Returns the bounding box of the collection.

        bounding_box -> number 4-tuple

        Examples
        --------
        >>> import pysal.cg.shapes as shapes
        >>> g = GeometryCollection([shapes.Point((0, 1)), shapes.Point((4, 5))])
        >>> g.bounding_box
        (0, 1, 4, 5)
        """
        pass

    def add(self, obj):
        """
        Adds the object _obj_ to the to geometry collection.

        add(Shape obj) -> bool

        Examples
        --------
        >>> import pysal.cg.shapes as shapes
        >>> g = GeometryCollection()
        >>> g.add(shapes.Point((0, 1)))
        True
        >>> g.add(shapes.Point((4, 5)))
        True
        """
        pass

    def remove(self, obj):
        """
        Removes the object _obj_ from the geometry collection.

        remove(Shape obj) -> bool

        Examples
        --------
        >>> import pysal.cg.shapes as shapes
        >>> g = GeometryCollection([shapes.Point((0, 1)), shapes.Point((4, 5))])
        >>> g.remove(shapes.Point((0, 1)))
        True
        >>> g.remove(shapes.Point((0, 1)))
        False
        """
        pass


def _test():
    import doctest
    doctest.testmod()

if __name__ == '__main__':
    _test()

########NEW FILE########
__FILENAME__ = kdtree
"""
KDTree for PySAL: Python Spatial Analysis Library.

Adds support for Arc Distance to scipy.spatial.KDTree.
"""
import sys
import math
import scipy.spatial
import numpy
from scipy import inf
import sphere

__author__ = "Charles R Schmidt <schmidtc@gmail.com>"
DISTANCE_METRICS = ['Euclidean', 'Arc']
FLOAT_EPS = numpy.finfo(float).eps


class Arc_KDTree(scipy.spatial.KDTree):
    def __init__(self, data, leafsize=10, radius=1.0):
        """
        KDTree using Arc Distance instead of Euclidean Distance.

        Returned distances are based on radius.
        For Example, pass in the the radius of earth in miles to get back miles.
        Assumes data are Lng/Lat, does not account for geoids.

        For more information see docs for scipy.spatial.KDTree

        Examples
        --------
        >>> pts = [(0,90), (0,0), (180,0), (0,-90)]
        >>> kd = Arc_KDTree(pts, radius = sphere.RADIUS_EARTH_KM)
        >>> d,i = kd.query((90,0), k=4)
        >>> d
        array([ 10007.54339801,  10007.54339801,  10007.54339801,  10007.54339801])
        >>> circumference = 2*math.pi*sphere.RADIUS_EARTH_KM
        >>> round(d[0],5) == round(circumference/4.0,5)
        True
        """
        self.radius = radius
        self.circumference = 2 * math.pi * radius
        scipy.spatial.KDTree.__init__(self, map(sphere.toXYZ, data), leafsize)

    def _toXYZ(self, x):
        if not issubclass(type(x), numpy.ndarray):
            x = numpy.array(x)
        if len(x.shape) == 2 and x.shape[1] == 3:  # assume point is already in XYZ
            return x
        if len(x.shape) == 1 and x.shape[0] == 3:  # assume point is already in XYZ
            return x
        elif len(x.shape) == 1:
            x = numpy.array(sphere.toXYZ(x))
        else:
            x = map(sphere.toXYZ, x)
        return x

    def count_neighbors(self, other, r, p=2):
        """
        See scipy.spatial.KDTree.count_neighbors

        Parameters
        ----------
        p: ignored, kept to maintain compatibility with scipy.spatial.KDTree

        Examples
        --------
        >>> pts = [(0,90), (0,0), (180,0), (0,-90)]
        >>> kd = Arc_KDTree(pts, radius = sphere.RADIUS_EARTH_KM)
        >>> kd.count_neighbors(kd,0)
        4
        >>> circumference = 2.0*math.pi*sphere.RADIUS_EARTH_KM
        >>> kd.count_neighbors(kd,circumference/2.0)
        16
        """
        if r > 0.5 * self.circumference:
            raise ValueError("r, must not exceed 1/2 circumference of the sphere (%f)." % self.circumference * 0.5)
        r = sphere.arcdist2linear(r, self.radius)
        return scipy.spatial.KDTree.count_neighbors(self, other, r)

    def query(self, x, k=1, eps=0, p=2, distance_upper_bound=inf):
        """
        See scipy.spatial.KDTree.query

        Parameters
        ----------
        x : array-like, last dimension self.m
            query points are lng/lat.
        p: ignored, kept to maintain compatibility with scipy.spatial.KDTree

        Examples
        --------
        >>> pts = [(0,90), (0,0), (180,0), (0,-90)]
        >>> kd = Arc_KDTree(pts, radius = sphere.RADIUS_EARTH_KM)
        >>> d,i = kd.query((90,0), k=4)
        >>> d
        array([ 10007.54339801,  10007.54339801,  10007.54339801,  10007.54339801])
        >>> circumference = 2*math.pi*sphere.RADIUS_EARTH_KM
        >>> round(d[0],5) == round(circumference/4.0,5)
        True
        >>> d,i = kd.query(kd.data, k=3)
        >>> d2,i2 = kd.query(pts, k=3)
        >>> (d == d2).all()
        True
        >>> (i == i2).all()
        True
        """
        eps = sphere.arcdist2linear(eps, self.radius)
        if distance_upper_bound != inf:
            distance_upper_bound = sphere.arcdist2linear(
                distance_upper_bound, self.radius)
        d, i = scipy.spatial.KDTree.query(self, self._toXYZ(x), k,
                                          eps=eps, distance_upper_bound=distance_upper_bound)
        dims = len(d.shape)
        r = self.radius
        if dims == 0:
            return sphere.linear2arcdist(d, r), i
        if dims == 1:
            #TODO: implement linear2arcdist on numpy arrays
            d = [sphere.linear2arcdist(x, r) for x in d]
        elif dims == 2:
            d = [[sphere.linear2arcdist(x, r) for x in row] for row in d]
        return numpy.array(d), i

    def query_ball_point(self, x, r, p=2, eps=0):
        """
        See scipy.spatial.KDTree.query_ball_point

        Parameters
        ----------
        p: ignored, kept to maintain compatibility with scipy.spatial.KDTree

        Examples
        --------
        >>> pts = [(0,90), (0,0), (180,0), (0,-90)]
        >>> kd = Arc_KDTree(pts, radius = sphere.RADIUS_EARTH_KM)
        >>> circumference = 2*math.pi*sphere.RADIUS_EARTH_KM
        >>> kd.query_ball_point(pts, circumference/4.)
        array([[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]], dtype=object)
        >>> kd.query_ball_point(pts, circumference/2.)
        array([[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]], dtype=object)
        """
        eps = sphere.arcdist2linear(eps, self.radius)
        #scipy.sphere.KDTree.query_ball_point appears to ignore the eps argument.
        # we have some floating point errors moving back and forth between cordinate systems,
        # so we'll account for that be adding some to our radius, 3*float's eps value.
        if r > 0.5 * self.circumference:
            raise ValueError("r, must not exceed 1/2 circumference of the sphere (%f)." % self.circumference * 0.5)
        r = sphere.arcdist2linear(r, self.radius) + FLOAT_EPS * 3
        return scipy.spatial.KDTree.query_ball_point(self, self._toXYZ(x), r, eps=eps)

    def query_ball_tree(self, other, r, p=2, eps=0):
        """
        See scipy.spatial.KDTree.query_ball_tree

        Parameters
        ----------
        p: ignored, kept to maintain compatibility with scipy.spatial.KDTree

        Examples
        --------
        >>> pts = [(0,90), (0,0), (180,0), (0,-90)]
        >>> kd = Arc_KDTree(pts, radius = sphere.RADIUS_EARTH_KM)
        >>> kd.query_ball_tree(kd, kd.circumference/4.)
        [[0, 1, 2], [0, 1, 3], [0, 2, 3], [1, 2, 3]]
        >>> kd.query_ball_tree(kd, kd.circumference/2.)
        [[0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3], [0, 1, 2, 3]]
        """
        eps = sphere.arcdist2linear(eps, self.radius)
        #scipy.sphere.KDTree.query_ball_point appears to ignore the eps argument.
        # we have some floating point errors moving back and forth between cordinate systems,
        # so we'll account for that be adding some to our radius, 3*float's eps value.
        if self.radius != other.radius:
            raise ValueError("Both trees must have the same radius.")
        if r > 0.5 * self.circumference:
            raise ValueError("r, must not exceed 1/2 circumference of the sphere (%f)." % self.circumference * 0.5)
        r = sphere.arcdist2linear(r, self.radius) + FLOAT_EPS * 3
        return scipy.spatial.KDTree.query_ball_tree(self, other, r, eps=eps)

    def query_pairs(self, r, p=2, eps=0):
        """
        See scipy.spatial.KDTree.query_pairs

        Parameters
        ----------
        p: ignored, kept to maintain compatibility with scipy.spatial.KDTree

        Examples
        --------
        >>> pts = [(0,90), (0,0), (180,0), (0,-90)]
        >>> kd = Arc_KDTree(pts, radius = sphere.RADIUS_EARTH_KM)
        >>> kd.query_pairs(kd.circumference/4.)
        set([(0, 1), (1, 3), (2, 3), (0, 2)])
        >>> kd.query_pairs(kd.circumference/2.)
        set([(0, 1), (1, 2), (1, 3), (2, 3), (0, 3), (0, 2)])
        """
        if r > 0.5 * self.circumference:
            raise ValueError("r, must not exceed 1/2 circumference of the sphere (%f)." % self.circumference * 0.5)
        r = sphere.arcdist2linear(r, self.radius) + FLOAT_EPS * 3
        return scipy.spatial.KDTree.query_pairs(self, r, eps=eps)

    def sparse_distance_matrix(self, other, max_distance, p=2):
        """
        See scipy.spatial.KDTree.sparse_distance_matrix

        Parameters
        ----------
        p: ignored, kept to maintain compatibility with scipy.spatial.KDTree

        Examples
        --------
        >>> pts = [(0,90), (0,0), (180,0), (0,-90)]
        >>> kd = Arc_KDTree(pts, radius = sphere.RADIUS_EARTH_KM)
        >>> kd.sparse_distance_matrix(kd, kd.circumference/4.).todense()
        matrix([[     0.        ,  10007.54339801,  10007.54339801,      0.        ],
                [ 10007.54339801,      0.        ,      0.        ,  10007.54339801],
                [ 10007.54339801,      0.        ,      0.        ,  10007.54339801],
                [     0.        ,  10007.54339801,  10007.54339801,      0.        ]])
        >>> kd.sparse_distance_matrix(kd, kd.circumference/2.).todense()
        matrix([[     0.        ,  10007.54339801,  10007.54339801,  20015.08679602],
                [ 10007.54339801,      0.        ,  20015.08679602,  10007.54339801],
                [ 10007.54339801,  20015.08679602,      0.        ,  10007.54339801],
                [ 20015.08679602,  10007.54339801,  10007.54339801,      0.        ]])
        """
        if self.radius != other.radius:
            raise ValueError("Both trees must have the same radius.")
        if max_distance > 0.5 * self.circumference:
            raise ValueError("max_distance, must not exceed 1/2 circumference of the sphere (%f)." % self.circumference * 0.5)
        max_distance = sphere.arcdist2linear(
            max_distance, self.radius) + FLOAT_EPS * 3
        D = scipy.spatial.KDTree.sparse_distance_matrix(
            self, other, max_distance)
        D = D.tocoo()
        #print D.data
        a2l = lambda x: sphere.linear2arcdist(x, self.radius)
        #print map(a2l,D.data)
        return scipy.sparse.coo_matrix((map(a2l, D.data), (D.row, D.col))).todok()


def KDTree(data, leafsize=10, distance_metric='Euclidean', radius=1.0):
    if distance_metric == 'Euclidean':
        return scipy.spatial.KDTree(data, leafsize)
    elif distance_metric == 'Arc':
        return Arc_KDTree(data, leafsize, radius)

########NEW FILE########
__FILENAME__ = locators
"""
Computational geometry code for PySAL: Python Spatial Analysis Library.
"""

__author__ = "Sergio J. Rey, Xinyue Ye, Charles Schmidt, Andrew Winslow"
__credits__ = "Copyright (c) 2005-2011 Sergio J. Rey"

import math
import copy
import doctest
from rtree import *
from standalone import *
from shapes import *

__all__ = ["IntervalTree", "Grid", "BruteForcePointLocator",
           "PointLocator", "PolygonLocator"]


class IntervalTree:
    """
    Representation of an interval tree. An interval tree is a data structure which is used to
    quickly determine which intervals in a set contain a value or overlap with a query interval.

    References
    ----------

    de Berg, van Kreveld, Overmars, Schwarzkopf. Computational Geometry: Algorithms and Application.
    212-217. Springer-Verlag, Berlin, 2000.
    """

    class _Node:
        """
        Private class representing a node in an interval tree.
        """

        def __init__(self, val, left_list, right_list, left_node, right_node):
            self.val = val
            self.left_list = left_list
            self.right_list = right_list
            self.left_node = left_node
            self.right_node = right_node

        def query(self, q):
            i = 0
            if q < self.val:
                while i < len(self.left_list) and self.left_list[i][0] <= q:
                    i += 1
                return [rec[2] for rec in self.left_list[0:i]]
            else:
                while i < len(self.right_list) and self.right_list[i][1] >= q:
                    i += 1
                return [rec[2] for rec in self.right_list[0:i]]

        def add(self, i):
            """
            Adds an interval to the IntervalTree node.
            """
            if not i[0] <= self.val <= i[1]:
                raise Exception('Attempt to add an interval to an inappropriate IntervalTree node')
            index = 0
            while index < len(self.left_list) and self.left_list[index] < i[0]:
                index = index + 1
            self.left_list.insert(index, i)
            index = 0
            while index < len(self.right_list) and self.right_list[index] > i[1]:
                index = index + 1
            self.right_list.insert(index, i)

        def remove(self, i):
            """
            Removes an interval from the IntervalTree node.
            """
            l = 0
            r = len(self.left_list)
            while l < r:
                m = (l + r) / 2
                if self.left_list[m] < i[0]:
                    l = m + 1
                elif self.left_list[m] > i[0]:
                    r = m
                else:
                    if self.left_list[m] == i:
                        self.left_list.pop(m)
                    else:
                        raise Exception('Attempt to remove an unknown interval')
            l = 0
            r = len(self.right_list)
            while l < r:
                m = (l + r) / 2
                if self.right_list[m] > i[1]:
                    l = m + 1
                elif self.right_left[m] < i[1]:
                    r = m
                else:
                    if self.right_list[m] == i:
                        self.right_list.pop(m)
                    else:
                        raise Exception('Attempt to remove an unknown interval')

    def __init__(self, intervals):
        """
        __init__((number, number, x) list) -> IntervalTree
        Returns an interval tree containing specified intervals.

        Parameters
        ----------
        intervals : a list of (lower, upper, item) elements to build the interval tree

        Examples
        --------

        >>> intervals = [(-1, 2, 'A'), (5, 9, 'B'), (3, 6, 'C')]
        >>> it = IntervalTree(intervals)
        >>> isinstance(it, IntervalTree)
        True
        """
        self._build(intervals)

    def _build(self, intervals):
        """
        Build an interval tree containing _intervals_.
        Each interval should be of the form (start, end, object).

        build((number, number, x) list) -> None

        Test tag: <tc>#is#IntervalTree.build</tc>
        """
        bad_is = filter(lambda i: i[0] > i[1], intervals)
        if bad_is != []:
            raise Exception('Attempt to build IntervalTree with invalid intervals: ' + str(bad_is))
        eps = list(set([i[0] for i in intervals] + [i[1] for i in intervals]))
        eps.sort()
        self.root = self._recursive_build(copy.copy(intervals), eps)

    def query(self, q):
        """
        Returns the intervals intersected by a value or interval.

        query((number, number) or number) -> x list

        Parameters
        ----------

        q : a value or interval to find intervals intersecting

        Examples
        --------

        >>> intervals = [(-1, 2, 'A'), (5, 9, 'B'), (3, 6, 'C')]
        >>> it = IntervalTree(intervals)
        >>> it.query((7, 14))
        ['B']
        >>> it.query(1)
        ['A']
        """
        if isinstance(q, tuple):
            return self._query_range(q, self.root)
        else:
            return self._query_points(q)

    def _query_range(self, q, root):
        if root is None:
            return []
        if root.val < q[0]:
            return self._query_range(q, root.right_node) + root.query(q[0])
        elif root.val > q[1]:
            return self._query_range(q, root.left_node) + root.query(q[1])
        else:
            return root.query(root.val) + self._query_range(q, root.left_node) + self._query_range(q, root.right_node)

    def _query_points(self, q):
        found = []
        cur = self.root
        while cur is not None:
            found.extend(cur.query(q))
            if q < cur.val:
                cur = cur.left_node
            else:
                cur = cur.right_node
        return found

    def _recursive_build(self, intervals, eps):
        def sign(x):
            if x < 0:
                return -1
            elif x > 0:
                return 1
            else:
                return 0

        def binary_search(list, q):
            l = 0
            r = len(list)
            while l < r:
                m = (l + r) / 2
                if list[m] < q:
                    l = m + 1
                else:
                    r = m
            return l

        if eps == []:
            return None
        median = eps[len(eps) / 2]
        hit_is = []
        rem_is = []
        for i in intervals:
            if i[0] <= median <= i[1]:
                hit_is.append(i)
            else:
                rem_is.append(i)
        left_list = copy.copy(hit_is)
        left_list.sort(lambda a, b: sign(a[0] - b[0]))
        right_list = copy.copy(hit_is)
        right_list.sort(lambda a, b: sign(b[1] - a[1]))
        eps = list(set([i[0] for i in intervals] + [i[1] for i in intervals]))
        eps.sort()
        bp = binary_search(eps, median)
        left_eps = eps[:bp]
        right_eps = eps[bp:]
        node = (IntervalTree._Node(median, left_list, right_list,
                                   self._recursive_build(rem_is, left_eps),
                                   self._recursive_build(rem_is, right_eps)))
        return node


class Grid:
    """
    Representation of a binning data structure.
    """

    def __init__(self, bounds, resolution):
        """
        Returns a grid with specified properties.

        __init__(Rectangle, number) -> Grid

        Parameters
        ----------
        bounds      : the area for the grid to encompass
        resolution  : the diameter of each bin

        Examples
        --------
        TODO: complete this doctest
        >>> g = Grid(Rectangle(0, 0, 10, 10), 1)
        """
        if resolution == 0:
            raise Exception('Cannot create grid with resolution 0')
        self.res = resolution
        self.hash = {}
        self.x_range = (bounds.left, bounds.right)
        self.y_range = (bounds.lower, bounds.upper)
        try:
            self.i_range = int(math.ceil(
                (self.x_range[1] - self.x_range[0]) / self.res))
            self.j_range = int(math.ceil(
                (self.y_range[1] - self.y_range[0]) / self.res))
        except Exception:
            raise Exception('Invalid arguments for Grid(): (' +
                            str(x_range) + ', ' + str(y_range) + ', ' + str(res) + ')')

    def in_grid(self, loc):
        """
        Returns whether a 2-tuple location _loc_ lies inside the grid bounds.

        Test tag: <tc>#is#Grid.in_grid</tc>
        """
        return (self.x_range[0] <= loc[0] <= self.x_range[1] and
                self.y_range[0] <= loc[1] <= self.y_range[1])

    def __grid_loc(self, loc):
        i = min(self.i_range, max(int((loc[0] - self.x_range[0]) /
                                      self.res), 0))
        j = min(self.j_range, max(int((loc[1] - self.y_range[0]) /
                                      self.res), 0))
        return (i, j)

    def add(self, item, pt):
        """
        Adds an item to the grid at a specified location.

        add(x, Point) -> x

        Parameters
        ----------
        item  : the item to insert into the grid
        pt : the location to insert the item at

        Examples
        --------

        >>> g = Grid(Rectangle(0, 0, 10, 10), 1)
        >>> g.add('A', Point((4.2, 8.7)))
        'A'
        """
        if not self.in_grid(pt):
            raise Exception('Attempt to insert item at location outside grid bounds: ' + str(pt))
        grid_loc = self.__grid_loc(pt)
        if grid_loc in self.hash:
            self.hash[grid_loc].append((pt, item))
        else:
            self.hash[grid_loc] = [(pt, item)]
        return item

    def remove(self, item, pt):
        """
        Removes an item from the grid at a specified location.

        remove(x, Point) -> x

        Parameters
        ----------
        item : the item to remove from the grid
        pt : the location the item was added at

        Examples
        --------

        >>> g = Grid(Rectangle(0, 0, 10, 10), 1)
        >>> g.add('A', Point((4.2, 8.7)))
        'A'
        >>> g.remove('A', Point((4.2, 8.7)))
        'A'
        """
        if not self.in_grid(pt):
            raise Exception('Attempt to remove item at location outside grid bounds: ' + str(pt))
        grid_loc = self.__grid_loc(pt)
        self.hash[grid_loc].remove((pt, item))
        if self.hash[grid_loc] == []:
            del self.hash[grid_loc]
        return item

    def bounds(self, bounds):
        """
        Returns a list of items found in the grid within the bounds specified.

        bounds(Rectangle) -> x list

        Parameters
        ----------
        item     : the item to remove from the grid
        pt       : the location the item was added at

        Examples
        --------

        >>> g = Grid(Rectangle(0, 0, 10, 10), 1)
        >>> g.add('A', Point((1.0, 1.0)))
        'A'
        >>> g.add('B', Point((4.0, 4.0)))
        'B'
        >>> g.bounds(Rectangle(0, 0, 3, 3))
        ['A']
        >>> g.bounds(Rectangle(2, 2, 5, 5))
        ['B']
        >>> sorted(g.bounds(Rectangle(0, 0, 5, 5)))
        ['A', 'B']
        """
        x_range = (bounds.left, bounds.right)
        y_range = (bounds.lower, bounds.upper)
        items = []
        lower_left = self.__grid_loc((x_range[0], y_range[0]))
        upper_right = self.__grid_loc((x_range[1], y_range[1]))
        for i in xrange(lower_left[0], upper_right[0] + 1):
            for j in xrange(lower_left[1], upper_right[1] + 1):
                if (i, j) in self.hash:
                    items.extend(map(lambda item: item[1], filter(lambda item: x_range[0] <= item[0][0] <= x_range[1] and y_range[0] <= item[0][1] <= y_range[1], self.hash[(i, j)])))
        return items

    def proximity(self, pt, r):
        """
        Returns a list of items found in the grid within a specified distance of a point.

        proximity(Point, number) -> x list

        Parameters
        ----------
        pt : the location to search around
        r  : the distance to search around the point

        Examples
        --------
        >>> g = Grid(Rectangle(0, 0, 10, 10), 1)
        >>> g.add('A', Point((1.0, 1.0)))
        'A'
        >>> g.add('B', Point((4.0, 4.0)))
        'B'
        >>> g.proximity(Point((2.0, 1.0)), 2)
        ['A']
        >>> g.proximity(Point((6.0, 5.0)), 3.0)
        ['B']
        >>> sorted(g.proximity(Point((4.0, 1.0)), 4.0))
        ['A', 'B']
        """
        items = []
        lower_left = self.__grid_loc((pt[0] - r, pt[1] - r))
        upper_right = self.__grid_loc((pt[0] + r, pt[1] + r))
        for i in xrange(lower_left[0], upper_right[0] + 1):
            for j in xrange(lower_left[1], upper_right[1] + 1):
                if (i, j) in self.hash:
                    items.extend(map(lambda item: item[1], filter(lambda item: get_points_dist(pt, item[0]) <= r, self.hash[(i, j)])))
        return items

    def nearest(self, pt):
        """
        Returns the nearest item to a point.

        nearest(Point) -> x

        Parameters
        ----------
        pt : the location to search near

        Examples
        --------
        >>> g = Grid(Rectangle(0, 0, 10, 10), 1)
        >>> g.add('A', Point((1.0, 1.0)))
        'A'
        >>> g.add('B', Point((4.0, 4.0)))
        'B'
        >>> g.nearest(Point((2.0, 1.0)))
        'A'
        >>> g.nearest(Point((7.0, 5.0)))
        'B'
        """
        search_size = self.res
        while (self.proximity(pt, search_size) == [] and
               (get_points_dist((self.x_range[0], self.y_range[0]), pt) > search_size or
                get_points_dist((self.x_range[1], self.y_range[0]), pt) > search_size or
                get_points_dist((self.x_range[0], self.y_range[1]), pt) > search_size or
                get_points_dist((self.x_range[1], self.y_range[1]), pt) > search_size)):
            search_size = 2 * search_size
        items = []
        lower_left = self.__grid_loc(
            (pt[0] - search_size, pt[1] - search_size))
        upper_right = self.__grid_loc(
            (pt[0] + search_size, pt[1] + search_size))
        for i in xrange(lower_left[0], upper_right[0] + 1):
            for j in xrange(lower_left[1], upper_right[1] + 1):
                if (i, j) in self.hash:
                    items.extend(map(lambda item: (get_points_dist(pt, item[
                        0]), item[1]), self.hash[(i, j)]))
        if items == []:
            return None
        return min(items)[1]


class BruteForcePointLocator:
    """
    A class which does naive linear search on a set of Point objects.
    """
    def __init__(self, points):
        """
        Creates a naive index of the points specified.

        __init__(Point list) -> BruteForcePointLocator

        Parameters
        ----------
        points : a list of points to index (Point list)

        Examples
        --------
        >>> pl = BruteForcePointLocator([Point((0, 0)), Point((5, 0)), Point((0, 10))])
        """
        self._points = points

    def nearest(self, query_point):
        """
        Returns the nearest point indexed to a query point.

        nearest(Point) -> Point

        Parameters
        ----------
        query_point : a point to find the nearest indexed point to

        Examples
        --------
        >>> points = [Point((0, 0)), Point((1, 6)), Point((5.4, 1.4))]
        >>> pl = BruteForcePointLocator(points)
        >>> n = pl.nearest(Point((1, 1)))
        >>> str(n)
        '(0.0, 0.0)'
        """
        return min(self._points, key=lambda p: get_points_dist(p, query_point))

    def region(self, region_rect):
        """
        Returns the indexed points located inside a rectangular query region.

        region(Rectangle) -> Point list

        Parameters
        ----------
        region_rect : the rectangular range to find indexed points in

        Examples
        --------
        >>> points = [Point((0, 0)), Point((1, 6)), Point((5.4, 1.4))]
        >>> pl = BruteForcePointLocator(points)
        >>> pts = pl.region(Rectangle(-1, -1, 10, 10))
        >>> len(pts)
        3
        """
        return filter(lambda p: get_rectangle_point_intersect(region_rect, p) is not None, self._points)

    def proximity(self, origin, r):
        """
        Returns the indexed points located within some distance of an origin point.

        proximity(Point, number) -> Point list

        Parameters
        ----------
        origin  : the point to find indexed points near
        r       : the maximum distance to find indexed point from the origin point

        Examples
        --------
        >>> points = [Point((0, 0)), Point((1, 6)), Point((5.4, 1.4))]
        >>> pl = BruteForcePointLocator(points)
        >>> neighs = pl.proximity(Point((1, 0)), 2)
        >>> len(neighs)
        1
        >>> p = neighs[0]
        >>> isinstance(p, Point)
        True
        >>> str(p)
        '(0.0, 0.0)'
        """
        return filter(lambda p: get_points_dist(p, origin) <= r, self._points)


class PointLocator:
    """
    An abstract representation of a point indexing data structure.
    """

    def __init__(self, points):
        """
        Returns a point locator object.

        __init__(Point list) -> PointLocator

        Parameters
        ----------
        points : a list of points to index

        Examples
        --------
        >>> points = [Point((0, 0)), Point((1, 6)), Point((5.4, 1.4))]
        >>> pl = PointLocator(points)
        """
        self._locator = BruteForcePointLocator(points)

    def nearest(self, query_point):
        """
        Returns the nearest point indexed to a query point.

        nearest(Point) -> Point

        Parameters
        ----------
        query_point : a point to find the nearest indexed point to

        Examples
        --------
        >>> points = [Point((0, 0)), Point((1, 6)), Point((5.4, 1.4))]
        >>> pl = PointLocator(points)
        >>> n = pl.nearest(Point((1, 1)))
        >>> str(n)
        '(0.0, 0.0)'
        """
        return self._locator.nearest(query_point)

    def region(self, region_rect):
        """
        Returns the indexed points located inside a rectangular query region.

        region(Rectangle) -> Point list

        Parameters
        ----------
        region_rect : the rectangular range to find indexed points in

        Examples
        --------
        >>> points = [Point((0, 0)), Point((1, 6)), Point((5.4, 1.4))]
        >>> pl = PointLocator(points)
        >>> pts = pl.region(Rectangle(-1, -1, 10, 10))
        >>> len(pts)
        3
        """
        return self._locator.region(region_rect)
    overlapping = region

    def polygon(self, polygon):
        """
        Returns the indexed points located inside a polygon
        """

        # get points in polygon bounding box

        # for points in bounding box, check for inclusion in polygon

    def proximity(self, origin, r):
        """
        Returns the indexed points located within some distance of an origin point.

        proximity(Point, number) -> Point list

        Parameters
        ----------
        origin  : the point to find indexed points near
        r       : the maximum distance to find indexed point from the origin point

        Examples
        --------
        >>> points = [Point((0, 0)), Point((1, 6)), Point((5.4, 1.4))]
        >>> pl = PointLocator(points)
        >>> len(pl.proximity(Point((1, 0)), 2))
        1
        """
        return self._locator.proximity(origin, r)


class PolygonLocator:
    """
    An abstract representation of a polygon indexing data structure.
    """

    def __init__(self, polygons):
        """
        Returns a polygon locator object.

        __init__(Polygon list) -> PolygonLocator

        Parameters
        ----------
        polygons : a list of polygons to index

        Examples
        --------
        >>> p1 = Polygon([Point((0, 1)), Point((4, 5)), Point((5, 1))])
        >>> p2 = Polygon([Point((3, 9)), Point((6, 7)), Point((1, 1))])
        >>> pl = PolygonLocator([p1, p2])
        >>> isinstance(pl, PolygonLocator)
        True
        """

        self._locator = polygons
        # create and rtree
        self._rtree = RTree()
        for polygon in polygons:
            x = polygon.bounding_box.left
            y = polygon.bounding_box.lower
            X = polygon.bounding_box.right
            Y = polygon.bounding_box.upper
            self._rtree.insert(polygon, Rect(x, y, X, Y))

    def inside(self, query_rectangle):
        """
        Returns polygons that are inside query_rectangle

        Examples
        --------

        >>> p1 = Polygon([Point((0, 1)), Point((4, 5)), Point((5, 1))])
        >>> p2 = Polygon([Point((3, 9)), Point((6, 7)), Point((1, 1))])
        >>> p3 = Polygon([Point((7, 1)), Point((8, 7)), Point((9, 1))])
        >>> pl = PolygonLocator([p1, p2, p3])
        >>> qr = Rectangle(0, 0, 5, 5)
        >>> res = pl.inside( qr )
        >>> len(res)
        1
        >>> qr = Rectangle(3, 7, 5, 8)
        >>> res = pl.inside( qr )
        >>> len(res)
        0
        >>> qr = Rectangle(10, 10, 12, 12)
        >>> res = pl.inside( qr )
        >>> len(res)
        0
        >>> qr = Rectangle(0, 0, 12, 12)
        >>> res = pl.inside( qr )
        >>> len(res)
        3

        Notes
        -----

        inside means the intersection of the query rectangle and a
        polygon is not empty and is equal to the area of the polygon
        """
        left = query_rectangle.left
        right = query_rectangle.right
        upper = query_rectangle.upper
        lower = query_rectangle.lower

        # rtree rect
        qr = Rect(left, lower, right, upper)
        # bb overlaps
        res = [r.leaf_obj() for r in self._rtree.query_rect(qr)
               if r.is_leaf()]

        qp = Polygon([Point((left, lower)), Point((right, lower)),
                      Point((right, upper)), Point((left, upper))])
        ip = []
        GPPI = get_polygon_point_intersect
        for poly in res:
            flag = True
            lower = poly.bounding_box.lower
            right = poly.bounding_box.right
            upper = poly.bounding_box.upper
            left = poly.bounding_box.left
            p1 = Point((left, lower))
            p2 = Point((right, upper))
            if GPPI(qp, p1) and GPPI(qp, p2):
                ip.append(poly)
        return ip

    def overlapping(self, query_rectangle):
        """
        Returns list of polygons that overlap query_rectangle

        Examples
        --------

        >>> p1 = Polygon([Point((0, 1)), Point((4, 5)), Point((5, 1))])
        >>> p2 = Polygon([Point((3, 9)), Point((6, 7)), Point((1, 1))])
        >>> p3 = Polygon([Point((7, 1)), Point((8, 7)), Point((9, 1))])
        >>> pl = PolygonLocator([p1, p2, p3])
        >>> qr = Rectangle(0, 0, 5, 5)
        >>> res = pl.overlapping( qr )
        >>> len(res)
        2
        >>> qr = Rectangle(3, 7, 5, 8)
        >>> res = pl.overlapping( qr )
        >>> len(res)
        1
        >>> qr = Rectangle(10, 10, 12, 12)
        >>> res = pl.overlapping( qr )
        >>> len(res)
        0
        >>> qr = Rectangle(0, 0, 12, 12)
        >>> res = pl.overlapping( qr )
        >>> len(res)
        3
        >>> qr = Rectangle(8, 3, 9, 4)
        >>> p1 = Polygon([Point((2, 1)), Point((2, 3)), Point((4, 3)), Point((4,1))])
        >>> p2 = Polygon([Point((7, 1)), Point((7, 5)), Point((10, 5)), Point((10, 1))])
        >>> pl = PolygonLocator([p1, p2])
        >>> res = pl.overlapping(qr)
        >>> len(res)
        1

        Notes
        -----
        overlapping means the intersection of the query rectangle and a
        polygon is not empty and is no larger than the area of the polygon
        """
        left = query_rectangle.left
        right = query_rectangle.right
        upper = query_rectangle.upper
        lower = query_rectangle.lower

        # rtree rect
        qr = Rect(left, lower, right, upper)

        # bb overlaps
        res = [r.leaf_obj() for r in self._rtree.query_rect(qr)
               if r.is_leaf()]
        # have to check for polygon overlap using segment intersection

        # add polys whose bb contains at least one of the corners of the query
        # rectangle

        sw = (left, lower)
        se = (right, lower)
        ne = (right, upper)
        nw = (left, upper)
        pnts = [sw, se, ne, nw]
        cs = []
        for pnt in pnts:
            c = [r.leaf_obj() for r in self._rtree.query_point(
                pnt) if r.is_leaf()]
            cs.extend(c)

        cs = list(set(cs))

        overlapping = []

        # first find polygons with at least one vertex inside query rectangle
        remaining = copy.copy(res)
        for polygon in res:
            vertices = polygon.vertices
            for vertex in vertices:
                xb = vertex[0] >= left
                xb *= vertex[0] < right
                yb = vertex[1] >= lower
                yb *= vertex[1] < upper
                if xb * yb:
                    overlapping.append(polygon)
                    remaining.remove(polygon)
                    break

        # for remaining polys in bb overlap check if vertex chains intersect
        # segments of the query rectangle
        left_edge = LineSegment(Point((left, lower)), Point((left,
                                                             upper)))
        right_edge = LineSegment(Point((right, lower)), Point((right,
                                                               upper)))
        lower_edge = LineSegment(Point((left, lower)), Point((right,
                                                              lower)))
        upper_edge = LineSegment(Point((left, upper)), Point((right,
                                                              upper)))
        for polygon in remaining:
            vertices = copy.copy(polygon.vertices)
            if vertices[-1] != vertices[0]:
                vertices.append(vertices[0])  # put on closed cartographic form
            nv = len(vertices)
            for i in range(nv - 1):
                head = vertices[i]
                tail = vertices[i + 1]
                edge = LineSegment(head, tail)
                li = get_segments_intersect(edge, left_edge)
                if li:
                    overlapping.append(polygon)
                    break
                elif get_segments_intersect(edge, right_edge):
                    overlapping.append(polygon)
                    break
                elif get_segments_intersect(edge, lower_edge):
                    overlapping.append(polygon)
                    break
                elif get_segments_intersect(edge, upper_edge):
                    overlapping.append(polygon)
                    break
        # check remaining for explicit containment of the bounding rectangle
        # cs has candidates for this check
        sw = Point(sw)
        se = Point(se)
        ne = Point(ne)
        nw = Point(nw)
        for polygon in cs:
            if get_polygon_point_intersect(polygon, sw):
                overlapping.append(polygon)
                break
            elif get_polygon_point_intersect(polygon, se):
                overlapping.append(polygon)
                break
            elif get_polygon_point_intersect(polygon, ne):
                overlapping.append(polygon)
                break
            elif get_polygon_point_intersect(polygon, nw):
                overlapping.append(polygon)
                break
        return list(set(overlapping))

    def nearest(self, query_point, rule='vertex'):
        """
        Returns the nearest polygon indexed to a query point based on
        various rules.

        nearest(Polygon) -> Polygon

        Parameters
        ----------
        query_point  : a point to find the nearest indexed polygon to

        rule         : representative point for polygon in nearest query.
                 vertex -- measures distance between vertices and query_point
                 centroid -- measures distance between centroid and
                 query_point
                 edge   -- measures the distance between edges and query_point

        Examples
        --------
        >>> p1 = Polygon([Point((0, 1)), Point((4, 5)), Point((5, 1))])
        >>> p2 = Polygon([Point((3, 9)), Point((6, 7)), Point((1, 1))])
        >>> pl = PolygonLocator([p1, p2])
        >>> try: n = pl.nearest(Point((-1, 1)))
        ... except NotImplementedError: print "future test: str(min(n.vertices())) == (0.0, 1.0)"
        future test: str(min(n.vertices())) == (0.0, 1.0)
        """
        raise NotImplementedError

    def region(self, region_rect):
        """
        Returns the indexed polygons located inside a rectangular query region.

        region(Rectangle) -> Polygon list

        Parameters
        ----------
        region_rect  : the rectangular range to find indexed polygons in

        Examples
        --------
        >>> p1 = Polygon([Point((0, 1)), Point((4, 5)), Point((5, 1))])
        >>> p2 = Polygon([Point((3, 9)), Point((6, 7)), Point((1, 1))])
        >>> pl = PolygonLocator([p1, p2])
        >>> n = pl.region(Rectangle(0, 0, 4, 10))
        >>> len(n)
        2
        """
        n = self._locator
        for polygon in n:
            points = polygon.vertices
            pl = BruteForcePointLocator(points)
            pts = pl.region(region_rect)
            if len(pts) == 0:
                n.remove(polygon)
        return n

    def contains_point(self, point):
        """
        Returns polygons that contain point


        Parameters
        ----------
        point: point (x,y)

        Returns
        -------
        list of polygons containing point

        Examples
        --------
        >>> p1 = Polygon([Point((0,0)), Point((6,0)), Point((4,4))])
        >>> p2 = Polygon([Point((1,2)), Point((4,0)), Point((4,4))])
        >>> p1.contains_point((2,2))
        1
        >>> p2.contains_point((2,2))
        1
        >>> pl = PolygonLocator([p1, p2])
        >>> len(pl.contains_point((2,2)))
        2
        >>> p2.contains_point((1,1))
        0
        >>> p1.contains_point((1,1))
        1
        >>> len(pl.contains_point((1,1)))
        1
        >>> p1.centroid
        (3.3333333333333335, 1.3333333333333333)
        >>> pl.contains_point((1,1))[0].centroid
        (3.3333333333333335, 1.3333333333333333)

        """
        # bbounding box containment
        res = [r.leaf_obj() for r in self._rtree.query_point(point)
               if r.is_leaf()]
        # explicit containment check for candidate polygons needed
        return [poly for poly in res if poly.contains_point(point)]

    def proximity(self, origin, r, rule='vertex'):
        """
        Returns the indexed polygons located within some distance of an
        origin point based on various rules.

        proximity(Polygon, number) -> Polygon list

        Parameters
        ----------
        origin  : the point to find indexed polygons near
        r       : the maximum distance to find indexed polygon from the origin point

        rule    : representative point for polygon in nearest query.
                vertex -- measures distance between vertices and query_point
                centroid -- measures distance between centroid and
                query_point
                edge   -- measures the distance between edges and query_point

        Examples
        --------
        >>> p1 = Polygon([Point((0, 1)), Point((4, 5)), Point((5, 1))])
        >>> p2 = Polygon([Point((3, 9)), Point((6, 7)), Point((1, 1))])
        >>> pl = PolygonLocator([p1, p2])
        >>> try:
        ...     len(pl.proximity(Point((0, 0)), 2))
        ... except NotImplementedError:
        ...     print "future test: len(pl.proximity(Point((0, 0)), 2)) == 2"
        future test: len(pl.proximity(Point((0, 0)), 2)) == 2
        """
        raise NotImplementedError


########NEW FILE########
__FILENAME__ = rtree
#pylint: disable-msg=C0103, C0301
"""
Pure Python implementation of RTree spatial index



Adaptation of
http://code.google.com/p/pyrtree/

R-tree.
see doc/ref/r-tree-clustering-split-algo.pdf
"""

__author__ = "Sergio J. Rey"

__all__ = ['RTree', 'Rect', 'Rtree']

MAXCHILDREN = 10
MAX_KMEANS = 5
BUFFER = 0.0000001
import math
import random
import time
import array


class Rect(object):
    """
    A rectangle class that stores: an axis aligned rectangle, and: two
     flags (swapped_x and swapped_y).  (The flags are stored
     implicitly via swaps in the order of minx/y and maxx/y.)
    """

    __slots__ = ("x", "y", "xx", "yy", "swapped_x", "swapped_y")

    def __getstate__(self):
        return (self.x, self.y, self.xx, self.yy, self.swapped_x, self.swapped_y)

    def __setstate__(self, state):
        self.x, self.y, self.xx, self.yy, self.swapped_x, self.swapped_y = state

    def __init__(self, minx, miny, maxx, maxy):
        self.swapped_x = (maxx < minx)
        self.swapped_y = (maxy < miny)
        self.x = minx
        self.y = miny
        self.xx = maxx
        self.yy = maxy

        if self.swapped_x:
            self.x, self.xx = maxx, minx
        if self.swapped_y:
            self.y, self.yy = maxy, miny

    def coords(self):
        return self.x, self.y, self.xx, self.yy

    def overlap(self, orect):
        return self.intersect(orect).area()

    def write_raw_coords(self, toarray, idx):
        toarray[idx] = self.x
        toarray[idx + 1] = self.y
        toarray[idx + 2] = self.xx
        toarray[idx + 3] = self.yy
        if (self.swapped_x):
            toarray[idx] = self.xx
            toarray[idx + 2] = self.x
        if (self.swapped_y):
            toarray[idx + 1] = self.yy
            toarray[idx + 3] = self.y

    def area(self):
        w = self.xx - self.x
        h = self.yy - self.y
        return w * h

    def extent(self):
        x = self.x
        y = self.y
        return (x, y, self.xx - x, self.yy - y)

    def grow(self, amt):
        a = amt * 0.5
        return Rect(self.x - a, self.y - a, self.xx + a, self.yy + a)

    def intersect(self, o):
        if self is NullRect:
            return NullRect
        if o is NullRect:
            return NullRect

        nx, ny = max(self.x, o.x), max(self.y, o.y)
        nx2, ny2 = min(self.xx, o.xx), min(self.yy, o.yy)
        w, h = nx2 - nx, ny2 - ny

        if w <= 0 or h <= 0:
            return NullRect

        return Rect(nx, ny, nx2, ny2)

    def does_contain(self, o):
        return self.does_containpoint((o.x, o.y)) and self.does_containpoint((o.xx, o.yy))

    def does_intersect(self, o):
        return (self.intersect(o).area() > 0)

    def does_containpoint(self, p):
        x, y = p
        return (x >= self.x and x <= self.xx and y >= self.y and y <= self.yy)

    def union(self, o):
        if o is NullRect:
            return Rect(self.x, self.y, self.xx, self.yy)
        if self is NullRect:
            return Rect(o.x, o.y, o.xx, o.yy)

        x = self.x
        y = self.y
        xx = self.xx
        yy = self.yy
        ox = o.x
        oy = o.y
        oxx = o.xx
        oyy = o.yy

        nx = x if x < ox else ox
        ny = y if y < oy else oy
        nx2 = xx if xx > oxx else oxx
        ny2 = yy if yy > oyy else oyy

        res = Rect(nx, ny, nx2, ny2)

        return res

    def union_point(self, o):
        x, y = o
        return self.union(Rect(x, y, x, y))

    def diagonal_sq(self):
        if self is NullRect:
            return 0
        w = self.xx - self.x
        h = self.yy - self.y
        return w * w + h * h

    def diagonal(self):
        return math.sqrt(self.diagonal_sq())

NullRect = Rect(0.0, 0.0, 0.0, 0.0)
NullRect.swapped_x = False
NullRect.swapped_y = False


def union_all(kids):
    cur = NullRect
    for k in kids:
        cur = cur.union(k.rect)
    assert(False == cur.swapped_x)
    return cur


def Rtree():
    return RTree()


class RTree(object):
    def __init__(self):
        self.count = 0
        self.stats = {
            "overflow_f": 0,
            "avg_overflow_t_f": 0.0,
            "longest_overflow": 0.0,
            "longest_kmeans": 0.0,
            "sum_kmeans_iter_f": 0,
            "count_kmeans_iter_f": 0,
            "avg_kmeans_iter_f": 0.0
        }

        # This round: not using objects directly -- they
        #   take up too much memory, and efficiency goes down the toilet
        #   (obviously) if things start to page.
        #  Less obviously: using object graph directly leads to really long GC
        #   pause times, too.
        # Instead, it uses pools of arrays:
        self.count = 0
        self.leaf_count = 0
        self.rect_pool = array.array('d')
        self.node_pool = array.array('L')
        self.leaf_pool = []  # leaf objects.

        self.cursor = _NodeCursor.create(self, NullRect)

    def _ensure_pool(self, idx):
        if len(self.rect_pool) < (4 * idx):
            self.rect_pool.extend([0, 0, 0, 0] * idx)
            self.node_pool.extend([0, 0] * idx)

    def insert(self, o, orect):
        self.cursor.insert(o, orect)
        assert(self.cursor.index == 0)

    def query_rect(self, r):
        for x in self.cursor.query_rect(r):
            yield x

    def query_point(self, p):
        for x in self.cursor.query_point(p):
            yield x

    def walk(self, pred):
        return self.cursor.walk(pred)

    def intersection(self, boundingbox):
        """
        replicate c rtree method

        Returns
        -------

        ids : list
              list of object ids whose bounding boxes intersect with query
              bounding box

        """
        # grow the bounding box slightly to handle coincident edges

        bb = boundingbox[:]
        bb[0] = bb[0] - BUFFER
        bb[1] = bb[1] - BUFFER
        bb[2] = bb[2] + BUFFER
        bb[3] = bb[3] + BUFFER

        qr = Rect(bb[0], bb[1], bb[2], bb[3])
        return [r.leaf_obj() for r in self.query_rect(qr) if r.is_leaf()]

    def add(self, id, boundingbox):
        """
        replicate c rtree method

        Arguments
        ---------

        id: object id

        boundingbox: list
                   bounding box [minx, miny, maxx, maxy]
        """
        bb = boundingbox
        self.cursor.insert(id, Rect(bb[0], bb[1], bb[2], bb[3]))


class _NodeCursor(object):
    @classmethod
    def create(cls, rooto, rect):
        idx = rooto.count
        rooto.count += 1

        rooto._ensure_pool(idx + 1)
        #rooto.node_pool.extend([0,0])
        #rooto.rect_pool.extend([0,0,0,0])

        retv = _NodeCursor(rooto, idx, rect, 0, 0)

        retv._save_back()
        return retv

    @classmethod
    def create_with_children(cls, children, rooto):
        rect = union_all([c for c in children])
        nr = Rect(rect.x, rect.y, rect.xx, rect.yy)
        assert(not rect.swapped_x)
        nc = _NodeCursor.create(rooto, rect)
        nc._set_children(children)
        assert(not nc.is_leaf())
        return nc

    @classmethod
    def create_leaf(cls, rooto, leaf_obj, leaf_rect):
        rect = Rect(leaf_rect.x, leaf_rect.y, leaf_rect.xx, leaf_rect.yy)
        rect.swapped_x = True  # Mark as leaf by setting the xswap flag.
        res = _NodeCursor.create(rooto, rect)
        idx = res.index
        res.first_child = rooto.leaf_count
        rooto.leaf_count += 1
        res.next_sibling = 0
        rooto.leaf_pool.append(leaf_obj)
        res._save_back()
        res._become(idx)
        assert(res.is_leaf())
        return res

    __slots__ = ("root", "npool", "rpool", "index", "rect",
                 "next_sibling", "first_child")

    def __getstate__(self):
        return (self.root, self.npool, self.rpool, self.index, self.rect, self.next_sibling, self.first_child)

    def __setstate__(self, state):
        self.root, self.npool, self.rpool, self.index, self.rect, self.next_sibling, self.first_child = state

    def __init__(self, rooto, index, rect, first_child, next_sibling):
        self.root = rooto
        self.rpool = rooto.rect_pool
        self.npool = rooto.node_pool

        self.index = index
        self.rect = rect
        self.next_sibling = next_sibling
        self.first_child = first_child

    def walk(self, predicate):
        if (predicate(self, self.leaf_obj())):
            yield self
            if not self.is_leaf():
                for c in self.children():
                    for cr in c.walk(predicate):
                        yield cr

    def query_rect(self, r):
        """ Return things that intersect with 'r'. """
        def p(o, x):
            return r.does_intersect(o.rect)
        for rr in self.walk(p):
            yield rr

    def query_point(self, point):
        """ Query by a point """
        def p(o, x):
            return o.rect.does_containpoint(point)

        for rr in self.walk(p):
            yield rr

    def lift(self):
        return _NodeCursor(self.root,
                           self.index,
                           self.rect,
                           self.first_child,
                           self.next_sibling)

    def _become(self, index):
        recti = index * 4
        nodei = index * 2
        rp = self.rpool
        x = rp[recti]
        y = rp[recti + 1]
        xx = rp[recti + 2]
        yy = rp[recti + 3]

        if (x == 0.0 and y == 0.0 and xx == 0.0 and yy == 0.0):
            self.rect = NullRect
        else:
            self.rect = Rect(x, y, xx, yy)

        self.next_sibling = self.npool[nodei]
        self.first_child = self.npool[nodei + 1]
        self.index = index

    def is_leaf(self):
        return self.rect.swapped_x

    def has_children(self):
        return not self.is_leaf() and 0 != self.first_child

    def holds_leaves(self):
        if 0 == self.first_child:
            return True
        else:
            return self.has_children() and self.get_first_child().is_leaf()

    def get_first_child(self):
        fc = self.first_child
        c = _NodeCursor(self.root, 0, NullRect, 0, 0)
        c._become(self.first_child)
        return c

    def leaf_obj(self):
        if self.is_leaf():
            return self.root.leaf_pool[self.first_child]
        else:
            return None

    def _save_back(self):
        rp = self.rpool
        recti = self.index * 4
        nodei = self.index * 2

        if self.rect is not NullRect:
            self.rect.write_raw_coords(rp, recti)
        else:
            rp[recti] = 0
            rp[recti + 1] = 0
            rp[recti + 2] = 0
            rp[recti + 3] = 0

        self.npool[nodei] = self.next_sibling
        self.npool[nodei + 1] = self.first_child

    def nchildren(self):
        i = self.index
        c = 0
        for x in self.children():
            c += 1
        return c

    def insert(self, leafo, leafrect):
        index = self.index

        # tail recursion, made into loop:
        while True:
            if self.holds_leaves():
                self.rect = self.rect.union(leafrect)
                self._insert_child(_NodeCursor.create_leaf(
                    self.root, leafo, leafrect))

                self._balance()

                # done: become the original again
                self._become(index)
                return
            else:
                # Not holding leaves, move down a level in the tree:

                # Micro-optimization:
                #  inlining union() calls -- logic is:
                # ignored,child = min([ ((c.rect.union(leafrect)).area() - c.rect.area(),c.index) for c in self.children() ])
                child = None
                minarea = -1.0
                for c in self.children():
                    x, y, xx, yy = c.rect.coords()
                    lx, ly, lxx, lyy = leafrect.coords()
                    nx = x if x < lx else lx
                    nxx = xx if xx > lxx else lxx
                    ny = y if y < ly else ly
                    nyy = yy if yy > lyy else lyy
                    a = (nxx - nx) * (nyy - ny)
                    if minarea < 0 or a < minarea:
                        minarea = a
                        child = c.index
                # End micro-optimization

                self.rect = self.rect.union(leafrect)
                self._save_back()
                self._become(child)  # recurse.

    def _balance(self):
        if (self.nchildren() <= MAXCHILDREN):
            return

        t = time.clock()

        cur_score = -10

        s_children = [c.lift() for c in self.children()]

        memo = {}

        clusterings = [k_means_cluster(
            self.root, k, s_children) for k in range(2, MAX_KMEANS)]
        score, bestcluster = max(
            [(silhouette_coeff(c, memo), c) for c in clusterings])

        nodes = [_NodeCursor.create_with_children(
            c, self.root) for c in bestcluster if len(c) > 0]

        self._set_children(nodes)

        dur = (time.clock() - t)
        c = float(self.root.stats["overflow_f"])
        oa = self.root.stats["avg_overflow_t_f"]
        self.root.stats["avg_overflow_t_f"] = (
            dur / (c + 1.0)) + (c * oa / (c + 1.0))
        self.root.stats["overflow_f"] += 1
        self.root.stats["longest_overflow"] = max(
            self.root.stats["longest_overflow"], dur)

    def _set_children(self, cs):
        self.first_child = 0

        if 0 == len(cs):
            return

        pred = None
        for c in cs:
            if pred is not None:
                pred.next_sibling = c.index
                pred._save_back()
            if 0 == self.first_child:
                self.first_child = c.index
            pred = c
        pred.next_sibling = 0
        pred._save_back()
        self._save_back()

    def _insert_child(self, c):
        c.next_sibling = self.first_child
        self.first_child = c.index
        c._save_back()
        self._save_back()

    def children(self):
        if (0 == self.first_child):
            return

        idx = self.index
        fc = self.first_child
        ns = self.next_sibling
        r = self.rect

        self._become(self.first_child)
        while True:
            yield self
            if 0 == self.next_sibling:
                break
            else:
                self._become(self.next_sibling)

        # Go back to becoming the same node we were.
        #self._become(idx)
        self.index = idx
        self.first_child = fc
        self.next_sibling = ns
        self.rect = r


def avg_diagonals(node, onodes, memo_tab):
    nidx = node.index
    sv = 0.0
    diag = 0.0
    for onode in onodes:
        k1 = (nidx, onode.index)
        k2 = (onode.index, nidx)
        if k1 in memo_tab:
            diag = memo_tab[k1]
        elif k2 in memo_tab:
            diag = memo_tab[k2]
        else:
            diag = node.rect.union(onode.rect).diagonal()
            memo_tab[k1] = diag

        sv += diag

    return sv / len(onodes)


def silhouette_w(node, cluster, next_closest_cluster, memo):
    ndist = avg_diagonals(node, cluster, memo)
    sdist = avg_diagonals(node, next_closest_cluster, memo)
    return (sdist - ndist) / max(sdist, ndist)


def silhouette_coeff(clustering, memo_tab):
    # special case for a clustering of 1.0
    if (len(clustering) == 1):
        return 1.0

    coeffs = []
    for cluster in clustering:
        others = [c for c in clustering if c is not cluster]
        others_cntr = [center_of_gravity(c) for c in others]
        ws = [silhouette_w(node, cluster, others[closest(
            others_cntr, node)], memo_tab) for node in cluster]
        cluster_coeff = sum(ws) / len(ws)
        coeffs.append(cluster_coeff)
    return sum(coeffs) / len(coeffs)


def center_of_gravity(nodes):
    totarea = 0.0
    xs, ys = 0, 0
    for n in nodes:
        if n.rect is not NullRect:
            x, y, w, h = n.rect.extent()
            a = w * h
            xs = xs + (a * (x + (0.5 * w)))
            ys = ys + (a * (y + (0.5 * h)))
            totarea = totarea + a
    return (xs / totarea), (ys / totarea)


def closest(centroids, node):
    x, y = center_of_gravity([node])
    dist = -1
    ridx = -1

    for (i, (xx, yy)) in enumerate(centroids):
        dsq = ((xx - x) ** 2) + ((yy - y) ** 2)
        if -1 == dist or dsq < dist:
            dist = dsq
            ridx = i
    return ridx


def k_means_cluster(root, k, nodes):
    t = time.clock()
    if len(nodes) <= k:
        return [[n] for n in nodes]

    ns = list(nodes)
    root.stats["count_kmeans_iter_f"] += 1

    # Initialize: take n random nodes.
    #random.shuffle(ns)

    cluster_starts = ns[:k]
    cluster_centers = [center_of_gravity([n]) for n in ns[:k]]

    # Loop until stable:
    while True:
        root.stats["sum_kmeans_iter_f"] += 1
        clusters = [[] for c in cluster_centers]

        for n in ns:
            idx = closest(cluster_centers, n)
            clusters[idx].append(n)

        #FIXME HACK TODO: is it okay for there to be empty clusters?
        clusters = [c for c in clusters if len(c) > 0]

        for c in clusters:
            if (len(c) == 0):
                print("Errorrr....")
                print("Nodes: %d, centers: %s" % (len(ns),
                                                  repr(cluster_centers)))

            assert(len(c) > 0)

        rest = ns
        first = False

        new_cluster_centers = [center_of_gravity(c) for c in clusters]
        if new_cluster_centers == cluster_centers:
            root.stats["avg_kmeans_iter_f"] = float(root.stats["sum_kmeans_iter_f"] / root.stats["count_kmeans_iter_f"])
            root.stats["longest_kmeans"] = max(
                root.stats["longest_kmeans"], (time.clock() - t))
            return clusters
        else:
            cluster_centers = new_cluster_centers

########NEW FILE########
__FILENAME__ = segmentLocator
import math
import scipy
import numpy
from pysal.cg.shapes import Rectangle, Point, LineSegment
from pysal.cg.standalone import get_segment_point_dist, get_bounding_box
import random
import time

__all__ = ["SegmentGrid", "SegmentLocator",
           "Polyline_Shapefile_SegmentLocator"]
DEBUG = False


class BruteSegmentLocator(object):
    def __init__(self, segments):
        self.data = segments
        self.n = len(segments)

    def nearest(self, pt):
        d = self.data
        distances = [get_segment_point_dist(
            d[i], pt)[0] for i in xrange(self.n)]
        return numpy.argmin(distances)


class SegmentLocator(object):
    def __init__(self, segments, nbins=500):
        self.data = segments
        if hasattr(segments, 'bounding_box'):
            bbox = segment.bounding_box
        else:
            bbox = get_bounding_box(segments)
        self.bbox = bbox
        res = max((bbox.right - bbox.left), (bbox.upper -
                                             bbox.lower)) / float(nbins)
        self.grid = SegmentGrid(bbox, res)
        for i, seg in enumerate(segments):
            self.grid.add(seg, i)

    def nearest(self, pt):
        d = self.data
        possibles = self.grid.nearest(pt)
        distances = [get_segment_point_dist(d[i], pt)[0] for i in possibles]
        #print "possibles",possibles
        #print "distances",distances
        #print "argmin", numpy.argmin(distances)
        return possibles[numpy.argmin(distances)]


class Polyline_Shapefile_SegmentLocator(object):
    def __init__(self, shpfile, nbins=500):
        self.data = shpfile
        bbox = Rectangle(*shpfile.bbox)
        res = max((bbox.right - bbox.left), (bbox.upper -
                                             bbox.lower)) / float(nbins)
        self.grid = SegmentGrid(bbox, res)
        for i, polyline in enumerate(shpfile):
            for p, part in enumerate(polyline.segments):
                for j, seg in enumerate(part):
                    self.grid.add(seg, (i, p, j))

    def nearest(self, pt):
        d = self.data
        possibles = self.grid.nearest(pt)
        distances = [get_segment_point_dist(
            d[i].segments[p][j], pt)[0] for (i, p, j) in possibles]
        #print "possibles",possibles
        #print "distances",distances
        #print "argmin", numpy.argmin(distances)
        return possibles[numpy.argmin(distances)]


class SegmentGrid(object):
    """
    Notes:
        SegmentGrid is a low level Grid class.
        This class does not maintain a copy of the geometry in the grid.
        It returns only approx. Solutions.
        This Grid should be wrapped by a locator.
    """
    def __init__(self, bounds, resolution):
        """
        Returns a grid with specified properties.

        __init__(Rectangle, number) -> SegmentGrid

        Parameters
        ----------
        bounds      : the area for the grid to encompass
        resolution  : the diameter of each bin

        Examples
        --------
        TODO: complete this doctest
        >>> g = SegmentGrid(Rectangle(0, 0, 10, 10), 1)
        """
        if resolution == 0:
            raise Exception('Cannot create grid with resolution 0')
        self.res = resolution
        self.hash = {}
        self._kd = None
        self._kd2 = None
        self._hashKeys = None
        self.x_range = (bounds.left, bounds.right)
        self.y_range = (bounds.lower, bounds.upper)
        try:
            self.i_range = int(math.ceil((self.x_range[1] -
                                          self.x_range[0]) / self.res)) + 1
            self.j_range = int(math.ceil((self.y_range[1] -
                                          self.y_range[0]) / self.res)) + 1
            self.mask = numpy.zeros((self.i_range, self.j_range), bool)
            self.endMask = numpy.zeros((self.i_range, self.j_range), bool)
        except Exception:
            raise Exception('Invalid arguments for SegmentGrid(): (' + str(self.x_range) + ', ' + str(self.y_range) + ', ' + str(self.res) + ')')
    @property
    def hashKeys(self):
        if self._hashKeys == None:
            self._hashKeys = numpy.array(self.hash.keys(),dtype=float)
        return self._hashKeys

    @property
    def kd(self):
        if self._kd == None:
            self._kd = scipy.spatial.cKDTree(self.hashKeys)
        return self._kd

    @property
    def kd2(self):
        if self._kd2 == None:
            self._kd2 = scipy.spatial.KDTree(self.hashKeys)
        return self._kd2

    def in_grid(self, loc):
        """
        Returns whether a 2-tuple location _loc_ lies inside the grid bounds.
        """
        return (self.x_range[0] <= loc[0] <= self.x_range[1] and
                self.y_range[0] <= loc[1] <= self.y_range[1])

    def _grid_loc(self, loc):
        i = int((loc[0] - self.x_range[0]) / self.res)  # floored
        j = int((loc[1] - self.y_range[0]) / self.res)  # floored
        #i = min(self.i_range-1, max(int((loc[0] - self.x_range[0])/self.res), 0))
        #j = min(self.j_range-1, max(int((loc[1] - self.y_range[0])/self.res), 0))
        #print "bin:", loc, " -> ", (i,j)
        return (i, j)

    def _real_loc(self, grid_loc):
        x = (grid_loc[0] * self.res) + self.x_range[0]
        y = (grid_loc[1] * self.res) + self.y_range[0]
        return x, y

    def bin_loc(self, loc, id):
        grid_loc = self._grid_loc(loc)
        if grid_loc not in self.hash:
            self.hash[grid_loc] = set()
            self.mask[grid_loc] = True
        self.hash[grid_loc].add(id)
        return grid_loc

    def add(self, segment, id):
        """
        Adds segment to the grid.

        add(segment, id) -> bool

        Parameters
        ----------
        id -- id to be stored int he grid.
        segment -- the segment which identifies where to store 'id' in the grid.

        Examples
        --------
        >>> g = SegmentGrid(Rectangle(0, 0, 10, 10), 1)
        >>> g.add(LineSegment(Point((0.2, 0.7)), Point((4.2, 8.7))), 0)
        True
        """
        if not (self.in_grid(segment.p1) and self.in_grid(segment.p2)):
            raise Exception('Attempt to insert item at location outside grid bounds: ' + str(segment))
        i, j = self.bin_loc(segment.p1, id)
        I, J = self.bin_loc(segment.p2, id)
        self.endMask[i, j] = True
        self.endMask[I, J] = True

        bbox = segment.bounding_box
        left = bbox.left
        lower = bbox.lower
        res = self.res
        line = segment.line
        tiny = res / 1000.
        for i in xrange(1 + min(i, I), max(i, I)):
            #print 'i',i
            x = self.x_range[0] + (i * res)
            y = line.y(x)
            self.bin_loc((x - tiny, y), id)
            self.bin_loc((x + tiny, y), id)
        for j in xrange(1 + min(j, J), max(j, J)):
            #print 'j',j
            y = self.y_range[0] + (j * res)
            x = line.x(y)
            self.bin_loc((x, y - tiny), id)
            self.bin_loc((x, y + tiny), id)
        self._kd = None
        self._kd2 = None
        return True

    def remove(self, segment):
        self._kd = None
        self._kd2 = None
        pass

    def nearest(self, pt):
        """
        Return a set of ids.

        The ids identify line segments within a radius of the query point.
        The true nearest segment is guaranteed to be within the set.

        Filtering possibles is the responsibility of the locator not the grid.
        This means the Grid doesn't need to keep a reference to the underlying segments,
        which in turn means the Locator can keep the segments on disk.

        Locators can be customized to different data stores (shape files, SQL, etc.)
        """
        grid_loc = numpy.array(self._grid_loc(pt))
        possibles = set()

        if DEBUG:
            print "in_grid:", self.in_grid(pt)
            i = pylab.matshow(self.mask, origin='lower',
                              extent=self.x_range + self.y_range, fignum=1)
        # Use KD tree to search out the nearest filled bin.
        # it may be faster to not use kdtree, or at least check grid_loc first
        # The KD tree is build on the keys of self.hash, a dictionary of stored bins.
        dist, i = self.kd.query(grid_loc, 1)

        ### Find non-empty bins within a radius of the query point.
        # Location of Q point
        row, col = grid_loc
        # distance to nearest filled cell +2.
        # +1 returns inconsistent results (compared to BruteSegmentLocator)
        # +2 seems to do the trick.
        radius = int(math.ceil(dist)) + 2
        if radius < 30:
            a, b = numpy.ogrid[-radius:radius + 1, -radius:radius +
                               1]   # build square index arrays centered at 0,0
            index = a ** 2 + b ** 2 <= radius ** 2                        # create a boolean mask to filter indicies outside radius
            a, b = index.nonzero()
                # grad the (i,j)'s of the elements within radius.
            rows, cols = row + a - radius, col + b - radius                   # recenter the (i,j)'s over the Q point
            #### Filter indicies by bounds of the grid.
            ### filters must be applied one at a time
            ### I havn't figure out a way to group these
            filter = rows >= 0
            rows = rows[filter]
            cols = cols[filter]  # i >= 0
            filter = rows < self.i_range
            rows = rows[filter]
            cols = cols[filter]  # i < i_range
            filter = cols >= 0
            rows = rows[
                filter]
            cols = cols[filter]  # j >= 0
            filter = cols < self.j_range
            rows = rows[
                filter]
            cols = cols[filter]  # j < j_range
            if DEBUG:
                maskCopy = self.mask.copy().astype(float)
                maskCopy += self.endMask.astype(float)
                maskCopy[rows, cols] += 1
                maskCopy[row, col] += 3
                i = pylab.matshow(maskCopy, origin='lower', extent=self.x_range + self.y_range, fignum=1)
                #raw_input('pause')
            ### All that was just setup for this one line...
            idx = self.mask[rows, cols].nonzero()[0] # Filter out empty bins.
            rows, cols = rows[idx], cols[idx]        # (i,j)'s of the filled grid cells within radius.

            for t in zip(rows, cols):
                possibles.update(self.hash[t])

            if DEBUG:
                print "possibles", possibles
        else:
        ### The old way...
        ### previously I was using kd.query_ball_point on, but the performance was terrible.
            I = self.kd2.query_ball_point(grid_loc, radius)
            for i in I:
                t = tuple(self.kd.data[i])
                possibles.update(self.hash[t])
        return list(possibles)


def random_segments(n):
    segs = []
    for i in xrange(n):
        a, b, c, d = [random.random() for x in [1, 2, 3, 4]]
        seg = LineSegment(Point((a, b)), Point((c, d)))
        segs.append(seg)
    return segs


def random_points(n):
    return [Point((random.random(), random.random())) for x in xrange(n)]


def test_combo(bins, segments, qpoints):
    G = SegmentLocator(segments, bins)
    G2 = BruteSegmentLocator(segs)
    for pt in qpoints:
        a = G.nearest(pt)
        b = G2.nearest(pt)
        if a != b:
            print a, b, a == b
            global DEBUG
            DEBUG = True
            a = G.nearest(pt)
            print a
            a = segments[a]
            b = segments[b]
            print "pt to a (grid)", get_segment_point_dist(a, pt)
            print "pt to b (brut)", get_segment_point_dist(b, pt)
            raw_input()
            pylab.clf()
            DEBUG = False


def test_brute(segments, qpoints):
    t0 = time.time()
    G2 = BruteSegmentLocator(segs)
    t1 = time.time()
    print "Created Brute in %0.4f seconds" % (t1 - t0)
    t2 = time.time()
    q = map(G2.nearest, qpoints)
    t3 = time.time()
    print "Brute Found %d matches in %0.4f seconds" % (len(qpoints), t3 - t2)
    print "Total Brute Time:", t3 - t0
    print
    return q


def test_grid(bins, segments, qpoints, visualize=False):
    t0 = time.time()
    G = SegmentLocator(segments, bins)
    t1 = time.time()
    G.grid.kd
    t2 = time.time()
    print "Created Grid in %0.4f seconds" % (t1 - t0)
    print "Created KDTree in %0.4f seconds" % (t2 - t1)
    if visualize:
        i = pylab.matshow(G.grid.mask, origin='lower',
                          extent=G.grid.x_range + G.grid.y_range)

    t2 = time.time()
    q = map(G.nearest, qpoints)
    t3 = time.time()
    print "Grid Found %d matches in %0.4f seconds" % (len(qpoints), t3 - t2)
    print "Total Grid Time:", t3 - t0
    qps = len(qpoints) / (t3 - t2)
    print "q/s:", qps
    #print
    return qps


def binSizeTest():
    q = 100
    minN = 1000
    maxN = 10000
    stepN = 1000
    minB = 250
    maxB = 2000
    stepB = 250
    sizes = range(minN, maxN, stepN)
    binSizes = range(minB, maxB, stepB)
    results = numpy.zeros((len(sizes), len(binSizes)))
    for row, n in enumerate(sizes):
        segs = random_segments(n)
        qpts = random_points(q)
        for col, bins in enumerate(binSizes):
            print "N, Bins:", n, bins
            qps = test_grid(bins, segs, qpts)
            results[row, col] = qps
    return results

if __name__ == '__main__':
    import pylab
    pylab.ion()

    n = 100
    q = 1000

    t0 = time.time()
    segs = random_segments(n)
    t1 = time.time()
    qpts = random_points(q)
    t2 = time.time()
    print "segments:", t1 - t0
    print "points:", t2 - t1
    #test_brute(segs,qpts)
    #test_grid(50, segs, qpts)

    SG = SegmentLocator(segs)
    grid = SG.grid

########NEW FILE########
__FILENAME__ = shapes
"""
Computational geometry code for PySAL: Python Spatial Analysis Library.

"""

__author__ = "Sergio J. Rey, Xinyue Ye, Charles Schmidt, Andrew Winslow"
__credits__ = "Copyright (c) 2005-2009 Sergio J. Rey"

import doctest
import math
from warnings import warn
from sphere import arcdist

__all__ = ['Point', 'LineSegment', 'Line', 'Ray', 'Chain', 'Polygon',
           'Rectangle', 'asShape']


def asShape(obj):
    """
    Returns a pysal shape object from obj.
    obj must support the __geo_interface__.
    """
    if hasattr(obj, '__geo_interface__'):
        geo = obj.__geo_interface__
    else:
        geo = obj
    if hasattr(geo, 'type'):
        raise TypeError('%r does not appear to be a shape object' % (obj))
    geo_type = geo['type'].lower()
    #if geo_type.startswith('multi'):
    #    raise NotImplementedError, "%s are not supported at this time."%geo_type
    if geo_type in _geoJSON_type_to_Pysal_type:
        return _geoJSON_type_to_Pysal_type[geo_type].__from_geo_interface__(geo)
    else:
        raise NotImplementedError(
            "%s is not supported at this time." % geo_type)


class Point(object):
    """
    Geometric class for point objects.

    Attributes
    ----------
    None
    """
    def __init__(self, loc):
        """
        Returns an instance of a Point object.

        __init__((number, number)) -> Point

        Test tag: <tc>#is#Point.__init__</tc>
        Test tag: <tc>#tests#Point.__init__</tc>

        Parameters
        ----------
        loc : tuple location (number x-tuple, x > 1)

        Attributes
        ----------

        Examples
        --------
        >>> p = Point((1, 3))
        """
        self.__loc = tuple(map(float, loc))

    @classmethod
    def __from_geo_interface__(cls, geo):
        return cls(geo['coordinates'])

    @property
    def __geo_interface__(self):
        return {'type': 'Point', 'coordinates': self.__loc}

    def __lt__(self, other):
        """
        Tests if the Point is < another object.

        __ne__(x) -> bool

        Parameters
        ----------
        other : an object to test equality against

        Attributes
        ----------

        Examples
        --------
        >>> Point((0,1)) < Point((0,1))
        False
        >>> Point((0,1)) < Point((1,1))
        True
        """
        return (self.__loc) < (other.__loc)

    def __le__(self, other):
        """
        Tests if the Point is <= another object.

        __ne__(x) -> bool

        Parameters
        ----------
        other : an object to test equality against

        Attributes
        ----------

        Examples
        --------
        >>> Point((0,1)) <= Point((0,1))
        True
        >>> Point((0,1)) <= Point((1,1))
        True
        """
        return (self.__loc) <= (other.__loc)

    def __eq__(self, other):
        """
        Tests if the Point is equal to another object.

        __eq__(x) -> bool

        Parameters
        ----------
        other : an object to test equality against

        Attributes
        ----------

        Examples
        --------
        >>> Point((0,1)) == Point((0,1))
        True
        >>> Point((0,1)) == Point((1,1))
        False
        """
        try:
            return (self.__loc) == (other.__loc)
        except AttributeError:
            return False

    def __ne__(self, other):
        """
        Tests if the Point is not equal to another object.

        __ne__(x) -> bool

        Parameters
        ----------
        other : an object to test equality against

        Attributes
        ----------

        Examples
        --------
        >>> Point((0,1)) != Point((0,1))
        False
        >>> Point((0,1)) != Point((1,1))
        True
        """
        try:
            return (self.__loc) != (other.__loc)
        except AttributeError:
            return True

    def __gt__(self, other):
        """
        Tests if the Point is > another object.

        __ne__(x) -> bool

        Parameters
        ----------
        other : an object to test equality against

        Attributes
        ----------

        Examples
        --------
        >>> Point((0,1)) > Point((0,1))
        False
        >>> Point((0,1)) > Point((1,1))
        False
        """
        return (self.__loc) > (other.__loc)

    def __ge__(self, other):
        """
        Tests if the Point is >= another object.

        __ne__(x) -> bool

        Parameters
        ----------
        other : an object to test equality against

        Attributes
        ----------

        Examples
        --------
        >>> Point((0,1)) >= Point((0,1))
        True
        >>> Point((0,1)) >= Point((1,1))
        False
        """
        return (self.__loc) >= (other.__loc)

    def __hash__(self):
        """
        Returns the hash of the Point's location.

        x.__hash__() -> hash(x)

        Parameters
        ----------
        None

        Attributes
        ----------

        Examples
        --------
        >>> hash(Point((0,1))) == hash(Point((0,1)))
        True
        >>> hash(Point((0,1))) == hash(Point((1,1)))
        False
        """
        return hash(self.__loc)

    def __getitem__(self, *args):
        """
        Return the coordinate for the given dimension.

        x.__getitem__(i) -> x[i]

        Parameters
        ----------
        i : index of the desired dimension.

        Attributes
        ----------

        Examples
        --------
        >>> p = Point((5.5,4.3))
        >>> p[0] == 5.5
        True
        >>> p[1] == 4.3
        True
        """
        return self.__loc.__getitem__(*args)

    def __getslice__(self, *args):
        """
        Return the coordinate for the given dimensions.

        x.__getitem__(i,j) -> x[i:j]

        Parameters
        ----------
        i : index to start slice
        j : index to end slice (excluded).

        Attributes
        ----------

        Examples
        --------
        >>> p = Point((3,6,2))
        >>> p[:2] == (3,6)
        True
        >>> p[1:2] == (6,)
        True
        """
        return self.__loc.__getslice__(*args)

    def __len__(self):
        """
        Returns the number of dimension in the point.

        __len__() -> int

        Parameters
        ----------
        None

        Attributes
        ----------

        Examples
        --------
        >>> len(Point((1,2)))
        2
        """
        return len(self.__loc)

    def __repr__(self):
        """
        Returns the string representation of the Point

        __repr__() -> string

        Parameters
        ----------
        None

        Attributes
        ----------

        Examples
        --------
        >>> Point((0,1))
        (0.0, 1.0)
        """
        return self.__loc.__repr__()

    def __str__(self):
        """
        Returns a string representation of a Point object.

        __str__() -> string

        Test tag: <tc>#is#Point.__str__</tc>
        Test tag: <tc>#tests#Point.__str__</tc>

        Attributes
        ----------

        Examples
        --------
        >>> p = Point((1, 3))
        >>> str(p)
        '(1.0, 3.0)'
        """
        return str(self.__loc)


class LineSegment(object):
    """
    Geometric representation of line segment objects.

    Parameters
    ----------

    start_pt     : Point
                   Point where segment begins
    end_pt       : Point
                   Point where segment ends

    Attributes
    ----------

    p1              : Point
                      Starting point
    p2              : Point
                      Ending point
    bounding_box    : tuple
                      The bounding box of the segment (number 4-tuple)
    len             : float
                      The length of the segment
    line            : Line
                      The line on which the segment lies

    """

    def __init__(self, start_pt, end_pt):
        """
        Creates a LineSegment object.

        __init__(Point, Point) -> LineSegment

        Test tag: <tc>#is#LineSegment.__init__</tc>
        Test tag: <tc>#tests#LineSegment.__init__</tc>


        Attributes
        ----------
        None

        Examples
        --------
        >>> ls = LineSegment(Point((1, 2)), Point((5, 6)))
        """
        self._p1 = start_pt
        self._p2 = end_pt
        self._reset_props()

    def __str__(self):
        return "LineSegment(" + str(self._p1) + ", " + str(self._p2) + ")"

    def __eq__(self, other):
        """
        Returns true if self and other are the same line segment

        Examples
        --------
        >>> l1 = LineSegment(Point((1, 2)), Point((5, 6)))
        >>> l2 = LineSegment(Point((5, 6)), Point((1, 2)))
        >>> l1 == l2
        True
        >>> l2 == l1
        True
        """
        if not isinstance(other, self.__class__):
            return False
        if (other.p1 == self._p1 and other.p2 == self._p2):
            return True
        elif (other.p2 == self._p1 and other.p1 == self._p2):
            return True
        return False

    def intersect(self, other):
        """
        Test whether segment intersects with other segment

        Handles endpoints of segments being on other segment

        Examples
        --------

        >>> ls = LineSegment(Point((5,0)), Point((10,0)))
        >>> ls1 = LineSegment(Point((5,0)), Point((10,1)))
        >>> ls.intersect(ls1)
        True
        >>> ls2 = LineSegment(Point((5,1)), Point((10,1)))
        >>> ls.intersect(ls2)
        False
        >>> ls2 = LineSegment(Point((7,-1)), Point((7,2)))
        >>> ls.intersect(ls2)
        True
        >>>
        """
        ccw1 = self.sw_ccw(other.p2)
        ccw2 = self.sw_ccw(other.p1)
        ccw3 = other.sw_ccw(self.p1)
        ccw4 = other.sw_ccw(self.p2)

        return ccw1*ccw2 <= 0 and ccw3*ccw4 <=0



    def _reset_props(self):
        """
        HELPER METHOD. DO NOT CALL.

        Resets attributes which are functions of other attributes. The getters for these attributes (implemented as
        properties) then recompute their values if they have been reset since the last call to the getter.

        _reset_props() -> None

        Attributes
        ----------

        Examples
        --------
        >>> ls = LineSegment(Point((1, 2)), Point((5, 6)))
        >>> ls._reset_props()
        """
        self._bounding_box = None
        self._len = None
        self._line = False

    def _get_p1(self):
        """
        HELPER METHOD. DO NOT CALL.

        Returns the p1 attribute of the line segment.

        _get_p1() -> Point

        Attributes
        ----------

        Examples
        --------
        >>> ls = LineSegment(Point((1, 2)), Point((5, 6)))
        >>> r = ls._get_p1()
        >>> r == Point((1, 2))
        True
        """
        return self._p1

    def _set_p1(self, p1):
        """
        HELPER METHOD. DO NOT CALL.

        Sets the p1 attribute of the line segment.

        _set_p1(Point) -> Point

        Attributes
        ----------

        Examples
        --------
        >>> ls = LineSegment(Point((1, 2)), Point((5, 6)))
        >>> r = ls._set_p1(Point((3, -1)))
        >>> r == Point((3.0, -1.0))
        True
        """
        self._p1 = p1
        self._reset_props()
        return self._p1

    p1 = property(_get_p1, _set_p1)

    def _get_p2(self):
        """
        HELPER METHOD. DO NOT CALL.

        Returns the p2 attribute of the line segment.

        _get_p2() -> Point

        Attributes
        ----------

        Examples
        --------
        >>> ls = LineSegment(Point((1, 2)), Point((5, 6)))
        >>> r = ls._get_p2()
        >>> r == Point((5, 6))
        True
        """
        return self._p2

    def _set_p2(self, p2):
        """
        HELPER METHOD. DO NOT CALL.

        Sets the p2 attribute of the line segment.

        _set_p2(Point) -> Point

        Attributes
        ----------

        Examples
        --------
        >>> ls = LineSegment(Point((1, 2)), Point((5, 6)))
        >>> r = ls._set_p2(Point((3, -1)))
        >>> r == Point((3.0, -1.0))
        True
        """
        self._p2 = p2
        self._reset_props()
        return self._p2

    p2 = property(_get_p2, _set_p2)

    def is_ccw(self, pt):
        """
        Returns whether a point is counterclockwise of the segment. Exclusive.

        is_ccw(Point) -> bool

        Test tag: <tc>#is#LineSegment.is_ccw</tc>
        Test tag: <tc>#tests#LineSegment.is_ccw</tc>

        Parameters
        ----------
        pt : point lying ccw or cw of a segment

        Attributes
        ----------

        Examples
        --------
        >>> ls = LineSegment(Point((0, 0)), Point((5, 0)))
        >>> ls.is_ccw(Point((2, 2)))
        True
        >>> ls.is_ccw(Point((2, -2)))
        False
        """
        v1 = (self._p2[0] - self._p1[0], self._p2[1] - self._p1[1])
        v2 = (pt[0] - self._p1[0], pt[1] - self._p1[1])

        return v1[0] * v2[1] - v1[1] * v2[0] > 0

    def is_cw(self, pt):
        """
        Returns whether a point is clockwise of the segment. Exclusive.

        is_cw(Point) -> bool

        Test tag: <tc>#is#LineSegment.is_cw</tc>
        Test tag: <tc>#tests#LineSegment.is_cw</tc>

        Parameters
        ----------
        pt : point lying ccw or cw of a segment

        Attributes
        ----------

        Examples
        --------
        >>> ls = LineSegment(Point((0, 0)), Point((5, 0)))
        >>> ls.is_cw(Point((2, 2)))
        False
        >>> ls.is_cw(Point((2, -2)))
        True
        """
        v1 = (self._p2[0] - self._p1[0], self._p2[1] - self._p1[1])
        v2 = (pt[0] - self._p1[0], pt[1] - self._p1[1])
        return v1[0] * v2[1] - v1[1] * v2[0] < 0

    def sw_ccw(self, pt):
        """
        Sedgewick test for pt being ccw of segment

        Returns
        -------

        1 if turn from self.p1 to self.p2 to pt is ccw
        -1 if turn from self.p1 to self.p2 to pt is cw
        -1 if the points are collinear and self.p1 is in the middle
        1 if the points are collinear and self.p2 is in the middle
        0 if the points are collinear and pt is in the middle
        
        """

        p0 = self.p1
        p1 = self.p2
        p2 = pt

        dx1 = p1[0] - p0[0]
        dy1 = p1[1] - p0[1]
        dx2 = p2[0] - p0[0]
        dy2 = p2[1] - p0[1]

        if dy1*dx2 < dy2*dx1:
            return 1
        if dy1*dx2 > dy2*dx1:
            return -1
        if (dx1*dx2 < 0 or dy1*dy2 <0):
                return -1
        if dx1*dx1 + dy1*dy1 >= dx2*dx2 + dy2*dy2:
            return 0
        else:
            return 1




    def get_swap(self):
        """
        Returns a LineSegment object which has its endpoints swapped.

        get_swap() -> LineSegment

        Test tag: <tc>#is#LineSegment.get_swap</tc>
        Test tag: <tc>#tests#LineSegment.get_swap</tc>

        Attributes
        ----------

        Examples
        --------
        >>> ls = LineSegment(Point((1, 2)), Point((5, 6)))
        >>> swap = ls.get_swap()
        >>> swap.p1[0]
        5.0
        >>> swap.p1[1]
        6.0
        >>> swap.p2[0]
        1.0
        >>> swap.p2[1]
        2.0
        """
        return LineSegment(self._p2, self._p1)

    @property
    def bounding_box(self):
        """
        Returns the minimum bounding box of a LineSegment object.

        Test tag: <tc>#is#LineSegment.bounding_box</tc>
        Test tag: <tc>#tests#LineSegment.bounding_box</tc>

        bounding_box -> Rectangle

        Attributes
        ----------

        Examples
        --------
        >>> ls = LineSegment(Point((1, 2)), Point((5, 6)))
        >>> ls.bounding_box.left
        1.0
        >>> ls.bounding_box.lower
        2.0
        >>> ls.bounding_box.right
        5.0
        >>> ls.bounding_box.upper
        6.0
        """
        if self._bounding_box is None:  # If LineSegment attributes p1, p2 changed, recompute
            self._bounding_box = Rectangle(
                min([self._p1[0], self._p2[0]]), min([
                    self._p1[1], self._p2[1]]),
                max([self._p1[0], self._p2[0]]), max([self._p1[1], self._p2[1]]))
        return Rectangle(
            self._bounding_box.left, self._bounding_box.lower, self._bounding_box.right,
            self._bounding_box.upper)

    @property
    def len(self):
        """
        Returns the length of a LineSegment object.

        Test tag: <tc>#is#LineSegment.len</tc>
        Test tag: <tc>#tests#LineSegment.len</tc>

        len() -> number

        Attributes
        ----------

        Examples
        --------
        >>> ls = LineSegment(Point((2, 2)), Point((5, 2)))
        >>> ls.len
        3.0
        """
        if self._len is None:  # If LineSegment attributes p1, p2 changed, recompute
            self._len = math.hypot(self._p1[0] - self._p2[0],
                                   self._p1[1] - self._p2[1])
        return self._len

    @property
    def line(self):
        """
        Returns a Line object of the line which the segment lies on.

        Test tag: <tc>#is#LineSegment.line</tc>
        Test tag: <tc>#tests#LineSegment.line</tc>

        line() -> Line

        Attributes
        ----------

        Examples
        --------
        >>> ls = LineSegment(Point((2, 2)), Point((3, 3)))
        >>> l = ls.line
        >>> l.m
        1.0
        >>> l.b
        0.0
        """
        if self._line == False:
            dx = self._p1[0] - self._p2[0]
            dy = self._p1[1] - self._p2[1]
            if dx == 0 and dy == 0:
                self._line = None
            elif dx == 0:
                self._line = VerticalLine(self._p1[0])
            else:
                m = dy / float(dx)
                b = self._p1[1] - m * self._p1[0]  # y - mx
                self._line = Line(m, b)
        return self._line


class VerticalLine:
    """
    Geometric representation of verticle line objects.

    Attributes
    ----------
    x       : float
              x-intercept
    """
    def __init__(self, x):
        """
        Returns a VerticalLine object.

        __init__(number) -> VerticalLine

        Parameters
        ----------
        x : the x-intercept of the line

        Attributes
        ----------

        Examples
        --------
        >>> ls = VerticalLine(0)
        >>> ls.m
        inf
        >>> ls.b
        nan
        """
        self._x = float(x)
        self.m = float('inf')
        self.b = float('nan')

    def x(self, y):
        """
        Returns the x-value of the line at a particular y-value.

        x(number) -> number

        Parameters
        ----------
        y : the y-value to compute x at

        Attributes
        ----------

        Examples
        --------
        >>> l = VerticalLine(0)
        >>> l.x(0.25)
        0.0
        """
        return self._x

    def y(self, x):
        """
        Returns the y-value of the line at a particular x-value.

        y(number) -> number

        Parameters
        ----------
        x : the x-value to compute y at

        Attributes
        ----------

        Examples
        --------
        >>> l = VerticalLine(1)
        >>> l.y(1)
        nan
        """
        return float('nan')


class Line:
    """
    Geometric representation of line objects.

    Attributes
    ----------
    m       : float
              slope
    b       : float
              y-intercept

    """

    def __init__(self, m, b):
        """
        Returns a Line object.

        __init__(number, number) -> Line

        Test tag: <tc>#is#Line.__init__</tc>
        Test tag: <tc>#tests#Line.__init__</tc>

        Parameters
        ----------
        m : the slope of the line
        b : the y-intercept of the line

        Attributes
        ----------

        Examples
        --------
        >>> ls = Line(1, 0)
        >>> ls.m
        1.0
        >>> ls.b
        0.0
        """
        if m == float('inf') or m == float('inf'):
            raise ArithmeticError('Slope cannot be infinite.')
        self.m = float(m)
        self.b = float(b)

    def x(self, y):
        """
        Returns the x-value of the line at a particular y-value.

        x(number) -> number

        Parameters
        ----------
        y : the y-value to compute x at

        Attributes
        ----------

        Examples
        --------
        >>> l = Line(0.5, 0)
        >>> l.x(0.25)
        0.5
        """
        if self.m == 0:
            raise ArithmeticError('Cannot solve for X when slope is zero.')
        return (y - self.b) / self.m

    def y(self, x):
        """
        Returns the y-value of the line at a particular x-value.

        y(number) -> number

        Parameters
        ----------
        x : the x-value to compute y at

        Attributes
        ----------

        Examples
        --------
        >>> l = Line(1, 0)
        >>> l.y(1)
        1.0
        """
        if self.m == 0:
            return self.b
        return self.m * x + self.b


class Ray:
    """
    Geometric representation of ray objects.

    Attributes
    ----------

    o       : Point
              Origin (point where ray originates)
    p       : Point
              Second point on the ray (not point where ray originates)
    """

    def __init__(self, origin, second_p):
        """
        Returns a ray with the values specified.

        __init__(Point, Point) -> Ray

        Parameters
        ----------
        origin   : the point where the ray originates
        second_p : the second point specifying the ray (not the origin)

        Attributes
        ----------

        Examples
        --------
        >>> l = Ray(Point((0, 0)), Point((1, 0)))
        >>> str(l.o)
        '(0.0, 0.0)'
        >>> str(l.p)
        '(1.0, 0.0)'
        """
        self.o = origin
        self.p = second_p


class Chain(object):
    """
    Geometric representation of a chain, also known as a polyline.

    Attributes
    ----------

    vertices    : list
                  List of Points of the vertices of the chain in order.
    len         : float
                  The geometric length of the chain.

    """

    def __init__(self, vertices):
        """
        Returns a chain created from the points specified.

        __init__(Point list or list of Point lists) -> Chain

        Parameters
        ----------
        vertices : list -- Point list or list of Point lists.

        Attributes
        ----------

        Examples
        --------
        >>> c = Chain([Point((0, 0)), Point((1, 0)), Point((1, 1)), Point((2, 1))])
        """
        if isinstance(vertices[0], list):
            self._vertices = [part for part in vertices]
        else:
            self._vertices = [vertices]
        self._reset_props()

    @classmethod
    def __from_geo_interface__(cls, geo):
        verts = [Point(pt) for pt in geo['coordinates']]
        return cls(verts)

    @property
    def __geo_interface__(self):
        return {'type': 'LineString', 'coordinates': self.vertices}

    def _reset_props(self):
        """
        HELPER METHOD. DO NOT CALL.

        Resets attributes which are functions of other attributes. The getters for these attributes (implemented as
        properties) then recompute their values if they have been reset since the last call to the getter.

        _reset_props() -> None

        Attributes
        ----------

        Examples
        --------
        >>> ls = Chain([Point((1, 2)), Point((5, 6))])
        >>> ls._reset_props()
        """
        self._len = None
        self._arclen = None
        self._bounding_box = None

    @property
    def vertices(self):
        """
        Returns the vertices of the chain in clockwise order.

        vertices -> Point list

        Attributes
        ----------

        Examples
        --------
        >>> c = Chain([Point((0, 0)), Point((1, 0)), Point((1, 1)), Point((2, 1))])
        >>> verts = c.vertices
        >>> len(verts)
        4
        """
        return sum([part for part in self._vertices], [])

    @property
    def parts(self):
        """
        Returns the parts of the chain.

        parts -> Point list

        Attributes
        ----------

        Examples
        --------
        >>> c = Chain([[Point((0, 0)), Point((1, 0)), Point((1, 1)), Point((0, 1))],[Point((2,1)),Point((2,2)),Point((1,2)),Point((1,1))]])
        >>> len(c.parts)
        2
        """
        return [[v for v in part] for part in self._vertices]

    @property
    def bounding_box(self):
        """
        Returns the bounding box of the chain.

        bounding_box -> Rectangle

        Attributes
        ----------

        Examples
        --------
        >>> c = Chain([Point((0, 0)), Point((2, 0)), Point((2, 1)), Point((0, 1))])
        >>> c.bounding_box.left
        0.0
        >>> c.bounding_box.lower
        0.0
        >>> c.bounding_box.right
        2.0
        >>> c.bounding_box.upper
        1.0
        """
        if self._bounding_box is None:
            vertices = self.vertices
            self._bounding_box = Rectangle(
                min([v[0] for v in vertices]), min([v[1] for v in vertices]),
                max([v[0] for v in vertices]), max([v[1] for v in vertices]))
        return self._bounding_box

    @property
    def len(self):
        """
        Returns the geometric length of the chain.

        len -> number

        Attributes
        ----------

        Examples
        --------
        >>> c = Chain([Point((0, 0)), Point((1, 0)), Point((1, 1)), Point((2, 1))])
        >>> c.len
        3.0
        >>> c = Chain([[Point((0, 0)), Point((1, 0)), Point((1, 1))],[Point((10,10)),Point((11,10)),Point((11,11))]])
        >>> c.len
        4.0
        """
        def dist(v1, v2):
            return math.hypot(v1[0] - v2[0], v1[1] - v2[1])

        def part_perimeter(part):
            return sum([dist(part[i], part[i + 1]) for i in xrange(len(part) - 1)])

        if self._len is None:
            self._len = sum([part_perimeter(part) for part in self._vertices])
        return self._len

    @property
    def arclen(self):
        """
        Returns the geometric length of the chain computed using arcdistance (meters).

        len -> number

        Attributes
        ----------

        Examples
        --------
        """
        def part_perimeter(part):
            return sum([arcdist(part[i], part[i + 1]) * 1000. for i in xrange(len(part) - 1)])
        if self._arclen is None:
            self._arclen = sum(
                [part_perimeter(part) for part in self._vertices])
        return self._arclen

    @property
    def segments(self):
        """
        Returns the segments that compose the Chain
        """
        return [[LineSegment(a, b) for (a, b) in zip(part[:-1], part[1:])] for part in self._vertices]


class Ring(object):
    """
    Geometric representation of a Linear Ring

    Linear Rings must be closed, the first and last point must be the same. Open rings will be closed.

    This class exists primarily as a geometric primitive to form complex polygons with multiple rings and holes.

    The ordering of the vertices is ignored and will not be altered.

    Parameters
    ----------
    vertices : list -- a list of vertices

    Attributes
    __________
    vertices        : list
                      List of Points with the vertices of the ring
    len             : int
                      Number of vertices
    perimeter       : float
                      Geometric length of the perimeter of the ring
    bounding_box    : Rectangle
                      Bounding box of the ring
    area            : float
                      area enclosed by the ring
    centroid        : tuple
                      The centroid of the ring defined by the 'center of gravity' or 'center or mass'
    """
    def __init__(self, vertices):
        if vertices[0] != vertices[-1]:
            vertices = vertices[:] + vertices[0:1]
            #raise ValueError, "Supplied vertices do not form a closed ring, the first and last vertices are not the same"
        self.vertices = tuple(vertices)
        self._perimeter = None
        self._bounding_box = None
        self._area = None
        self._centroid = None

    def __len__(self):
        return len(self.vertices)

    @property
    def len(self):
        return len(self)

    @staticmethod
    def dist(v1, v2):
        return math.hypot(v1[0] - v2[0], v1[1] - v2[1])

    @property
    def perimeter(self):
        if self._perimeter is None:
            dist = self.dist
            v = self.vertices
            self._perimeter = sum([dist(v[i], v[i + 1])
                                   for i in xrange(-1, len(self) - 1)])
        return self._perimeter

    @property
    def bounding_box(self):
        """
        Returns the bounding box of the ring

        bounding_box -> Rectangle

        Examples
        --------
        >>> r = Ring([Point((0, 0)), Point((2, 0)), Point((2, 1)), Point((0, 1)), Point((0,0))])
        >>> r.bounding_box.left
        0.0
        >>> r.bounding_box.lower
        0.0
        >>> r.bounding_box.right
        2.0
        >>> r.bounding_box.upper
        1.0
        """
        if self._bounding_box is None:
            vertices = self.vertices
            x = [v[0] for v in vertices]
            y = [v[1] for v in vertices]
            self._bounding_box = Rectangle(min(x), min(y), max(x), max(y))
        return self._bounding_box

    @property
    def area(self):
        """
        Returns the area of the ring.

        area -> number

        Examples
        --------
        >>> r = Ring([Point((0, 0)), Point((2, 0)), Point((2, 1)), Point((0, 1)), Point((0,0))])
        >>> r.area
        2.0
        """
        return abs(self.signed_area)

    @property
    def signed_area(self):
        if self._area is None:
            vertices = self.vertices
            x = [v[0] for v in vertices]
            y = [v[1] for v in vertices]
            N = len(self)

            A = 0.0
            for i in xrange(N - 1):
                A += (x[i] * y[i + 1] - x[i + 1] * y[i])
            A = A / 2.0
            self._area = A
        return self._area

    @property
    def centroid(self):
        """
        Returns the centroid of the ring.

        centroid -> Point

        Notes
        -----
        The centroid returned by this method is the geometric centroid.
        Also known as the 'center of gravity' or 'center of mass'.


        Examples
        --------
        >>> r = Ring([Point((0, 0)), Point((2, 0)), Point((2, 1)), Point((0, 1)), Point((0,0))])
        >>> str(r.centroid)
        '(1.0, 0.5)'
        """
        if self._centroid is None:
            vertices = self.vertices
            x = [v[0] for v in vertices]
            y = [v[1] for v in vertices]
            A = self.signed_area
            N = len(self)
            cx = 0
            cy = 0
            for i in xrange(N - 1):
                f = (x[i] * y[i + 1] - x[i + 1] * y[i])
                cx += (x[i] + x[i + 1]) * f
                cy += (y[i] + y[i + 1]) * f
            cx = 1.0 / (6 * A) * cx
            cy = 1.0 / (6 * A) * cy
            self._centroid = Point((cx, cy))
        return self._centroid


class Polygon(object):
    """
    Geometric representation of polygon objects.

    Attributes
    ----------
    vertices        : list
                      List of Points with the vertices of the Polygon in
                      clockwise order
    len             : int
                      Number of vertices including holes
    perimeter       : float
                      Geometric length of the perimeter of the Polygon
    bounding_box    : Rectangle
                      Bounding box of the polygon
    bbox            : List
                      [left, lower, right, upper]
    area            : float
                      Area enclosed by the polygon
    centroid        : tuple
                      The 'center of gravity', i.e. the mean point of the polygon.
    """

    def __init__(self, vertices, holes=None):
        """
        Returns a polygon created from the objects specified.

        __init__(Point list or list of Point lists, holes list ) -> Polygon

        Parameters
        ----------
        vertices : list -- a list of vertices or a list of lists of vertices.
        holes    : list -- a list of sub-polygons to be considered as holes.

        Attributes
        ----------

        Examples
        --------
        >>> p1 = Polygon([Point((0, 0)), Point((1, 0)), Point((1, 1)), Point((0, 1))])
        """
        self._part_rings = []
        self._hole_rings = []

        def clockwise(part):
            if standalone.is_clockwise(part):
                return part[:]
            else:
                return part[::-1]

        if isinstance(vertices[0], list):
            self._part_rings = map(Ring, vertices)
            self._vertices = [clockwise(part) for part in vertices]
        else:
            self._part_rings = [Ring(vertices)]
            self._vertices = [clockwise(vertices)]
        if holes is not None and holes != []:
            if isinstance(holes[0], list):
                self._hole_rings = map(Ring, holes)
                self._holes = [clockwise(hole) for hole in holes]
            else:
                self._hole_rings = [Ring(holes)]
                self._holes = [clockwise(holes)]
        else:
            self._holes = [[]]
        self._reset_props()

    @classmethod
    def __from_geo_interface__(cls, geo):
        """
        While pysal does not differentiate polygons and multipolygons GEOS,Shapely and geoJSON do.
        In GEOS, etc, polygons may only have a single exterior ring, all other parts are holes.
        MultiPolygons are simply a list of polygons.
        """
        geo_type = geo['type'].lower()
        if geo_type == 'multipolygon':
            parts = []
            holes = []
            for polygon in geo['coordinates']:
                verts = [[Point(pt) for pt in part] for part in polygon]
                parts += verts[0:1]
                holes += verts[1:]
            if not holes:
                holes = None
            return cls(parts, holes)
        else:
            verts = [[Point(pt) for pt in part] for part in geo['coordinates']]
            return cls(verts[0:1], verts[1:])

    @property
    def __geo_interface__(self):
        if len(self.parts) > 1:
            geo = {'type': 'MultiPolygon', 'coordinates': [[
                part] for part in self.parts]}
            if self._holes[0]:
                geo['coordinates'][0] += self._holes
            return geo
        if self._holes[0]:
            return {'type': 'Polygon', 'coordinates': self._vertices + self._holes}
        else:
            return {'type': 'Polygon', 'coordinates': self._vertices}

    def _reset_props(self):
        self._perimeter = None
        self._bounding_box = None
        self._bbox = None
        self._area = None
        self._centroid = None
        self._len = None

    def __len__(self):
        return self.len

    @property
    def len(self):
        """
        Returns the number of vertices in the polygon.

        len -> int

        Attributes
        ----------

        Examples
        --------
        >>> p1 = Polygon([Point((0, 0)), Point((0, 1)), Point((1, 1)), Point((1, 0))])
        >>> p1.len
        4
        >>> len(p1)
        4
        """
        if self._len is None:
            self._len = len(self.vertices)
        return self._len

    @property
    def vertices(self):
        """
        Returns the vertices of the polygon in clockwise order.

        vertices -> Point list

        Attributes
        ----------

        Examples
        --------
        >>> p1 = Polygon([Point((0, 0)), Point((0, 1)), Point((1, 1)), Point((1, 0))])
        >>> len(p1.vertices)
        4
        """
        return sum([part for part in self._vertices], []) + sum([part for part in self._holes], [])

    @property
    def holes(self):
        """
        Returns the holes of the polygon in clockwise order.

        holes -> Point list

        Attributes
        ----------

        Examples
        --------
        >>> p = Polygon([Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))], [Point((1, 2)), Point((2, 2)), Point((2, 1)), Point((1, 1))])
        >>> len(p.holes)
        1
        """
        return [[v for v in part] for part in self._holes]

    @property
    def parts(self):
        """
        Returns the parts of the polygon in clockwise order.

        parts -> Point list

        Attributes
        ----------

        Attributes
        ----------

        Examples
        --------
        >>> p = Polygon([[Point((0, 0)), Point((1, 0)), Point((1, 1)), Point((0, 1))], [Point((2,1)),Point((2,2)),Point((1,2)),Point((1,1))]])
        >>> len(p.parts)
        2
        """
        return [[v for v in part] for part in self._vertices]

    @property
    def perimeter(self):
        """
        Returns the perimeter of the polygon.

        perimeter() -> number

        Attributes
        ----------

        Examples
        --------
        >>> p = Polygon([Point((0, 0)), Point((1, 0)), Point((1, 1)), Point((0, 1))])
        >>> p.perimeter
        4.0
        """
        def dist(v1, v2):
            return math.hypot(v1[0] - v2[0], v1[1] - v2[1])

        def part_perimeter(part):
            return sum([dist(part[i], part[i + 1]) for i in xrange(-1, len(part) - 1)])

        if self._perimeter is None:
            self._perimeter = (sum([part_perimeter(part) for part in self._vertices]) +
                               sum([part_perimeter(hole) for hole in self._holes]))
        return self._perimeter

    @property
    def bbox(self):
        """
        Returns the bounding box of the polygon as a list

        See also bounding_box
        """
        if self._bbox is None:
            self._bbox = [ self.bounding_box.left,
                    self.bounding_box.lower,
                    self.bounding_box.right,
                    self.bounding_box.upper]
        return self._bbox


    @property
    def bounding_box(self):
        """
        Returns the bounding box of the polygon.

        bounding_box -> Rectangle

        Attributes
        ----------

        Examples
        --------
        >>> p = Polygon([Point((0, 0)), Point((2, 0)), Point((2, 1)), Point((0, 1))])
        >>> p.bounding_box.left
        0.0
        >>> p.bounding_box.lower
        0.0
        >>> p.bounding_box.right
        2.0
        >>> p.bounding_box.upper
        1.0
        """
        if self._bounding_box is None:
            vertices = self.vertices
            self._bounding_box = Rectangle(
                min([v[0] for v in vertices]), min([v[1] for v in vertices]),
                max([v[0] for v in vertices]), max([v[1] for v in vertices]))
        return self._bounding_box

    @property
    def area(self):
        """
        Returns the area of the polygon.

        area -> number

        Attributes
        ----------

        Examples
        --------
        >>> p = Polygon([Point((0, 0)), Point((1, 0)), Point((1, 1)), Point((0, 1))])
        >>> p.area
        1.0
        >>> p = Polygon([Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))],[Point((2,1)),Point((2,2)),Point((1,2)),Point((1,1))])
        >>> p.area
        99.0
        """
        def part_area(part_verts):
            area = 0
            for i in xrange(-1, len(part_verts) - 1):
                area += (part_verts[i][0] + part_verts[i + 1][0]) * \
                    (part_verts[i][1] - part_verts[i + 1][1])
            area = area * 0.5
            if area < 0:
                area = -area
            return area

        return (sum([part_area(part) for part in self._vertices]) -
                sum([part_area(hole) for hole in self._holes]))

    @property
    def centroid(self):
        """
        Returns the centroid of the polygon

        centroid -> Point

        Notes
        -----
        The centroid returned by this method is the geometric centroid and respects multipart polygons with holes.
        Also known as the 'center of gravity' or 'center of mass'.


        Examples
        --------
        >>> p = Polygon([Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))], [Point((1, 1)), Point((1, 2)), Point((2, 2)), Point((2, 1))])
        >>> p.centroid
        (5.0353535353535355, 5.0353535353535355)
        """
        CP = [ring.centroid for ring in self._part_rings]
        AP = [ring.area for ring in self._part_rings]
        CH = [ring.centroid for ring in self._hole_rings]
        AH = [-ring.area for ring in self._hole_rings]

        A = AP + AH
        cx = sum([pt[0] * area for pt, area in zip(CP + CH, A)]) / sum(A)
        cy = sum([pt[1] * area for pt, area in zip(CP + CH, A)]) / sum(A)
        return cx, cy

    def contains_point(self, point):
        """
        Test if polygon contains point

        Examples
        --------
        >>> p = Polygon([Point((0,0)), Point((4,0)), Point((4,5)), Point((2,3)), Point((0,5))])
        >>> p.contains_point((3,3))
        1
        >>> p.contains_point((0,5))
        0
        >>> p.contains_point((2,3))
        0
        >>> p.contains_point((4,5))
        0
        >>> p.contains_point((4,0))
        1
        >>>

        Handles holes

        >>> p = Polygon([Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))], [Point((1, 2)), Point((2, 2)), Point((2, 1)), Point((1, 1))])
        >>> p.contains_point((1.0,1.0))
        0
        >>> p.contains_point((2.0,2.0))
        1
        >>> p.contains_point((10,10))
        0
        >>>


        Notes
        -----
        Points falling exactly on polygon edges may yield unpredictable
        results
        """

        # ray from point to just outside left edge of bb
        left = self.bounding_box.left - 0.000001
        y = point[1]
        right = point[0]
        cn = 0
        verts = self.vertices
        c = Point((left, y))
        d = Point((right, y))
        ray = LineSegment(c, d)
        for i in xrange(-1, len(self.vertices) - 1):
            a = verts[i]
            b = verts[i + 1]
            ab = LineSegment(a, b)
            ac = LineSegment(a, c)
            bc = LineSegment(b, c)
            if ac.is_ccw(d) == bc.is_ccw(d):
                pass
            elif ab.is_ccw(c) == ab.is_ccw(d):
                pass
            else:
                cn += 1
        return cn % 2


class Rectangle:
    """
    Geometric representation of rectangle objects.

    Attributes
    ----------

    left    : float
              Minimum x-value of the rectangle
    lower   : float
              Minimum y-value of the rectangle
    right   : float
              Maximum x-value of the rectangle
    upper   : float
              Maximum y-value of the rectangle
    """

    def __init__(self, left, lower, right, upper):
        """
        Returns a Rectangle object.

        __init__(number, number, number, number) -> Rectangle

        Parameters
        ----------
        left  : the minimum x-value of the rectangle
        lower : the minimum y-value of the rectangle
        right : the maximum x-value of the rectangle
        upper : the maximum y-value of the rectangle

        Attributes
        ----------

        Examples
        --------
        >>> r = Rectangle(-4, 3, 10, 17)
        >>> r.left #minx
        -4.0
        >>> r.lower #miny
        3.0
        >>> r.right #maxx
        10.0
        >>> r.upper #maxy
        17.0
        """
        if right < left or upper < lower:
            raise ArithmeticError('Rectangle must have positive area.')
        self.left = float(left)
        self.lower = float(lower)
        self.right = float(right)
        self.upper = float(upper)

    def __nonzero__(self):
        """
        ___nonzero__ is used "to implement truth value testing and the built-in operation bool()" -- http://docs.python.org/reference/datamodel.html

        Rectangles will evaluate to Flase if they have Zero Area.
        >>> r = Rectangle(0,0,0,0)
        >>> bool(r)
        False
        >>> r = Rectangle(0,0,1,1)
        >>> bool(r)
        True
        """
        return bool(self.area)

    def __eq__(self, other):
        if other:
            return self[:] == other[:]
        return False

    def __add__(self, other):
        x, y, X, Y = self[:]
        x1, y2, X1, Y1 = other[:]
        return Rectangle(min(self.left, other.left), min(self.lower, other.lower), max(self.right, other.right), max(self.upper, other.upper))

    def __getitem__(self, key):
        """
        >>> r = Rectangle(-4, 3, 10, 17)
        >>> r[:]
        [-4.0, 3.0, 10.0, 17.0]
        """
        l = [self.left, self.lower, self.right, self.upper]
        return l.__getitem__(key)

    def set_centroid(self, new_center):
        """
        Moves the rectangle center to a new specified point.

        set_centroid(Point) -> Point

        Parameters
        ----------
        new_center : the new location of the centroid of the polygon

        Attributes
        ----------

        Examples
        --------
        >>> r = Rectangle(0, 0, 4, 4)
        >>> r.set_centroid(Point((4, 4)))
        >>> r.left
        2.0
        >>> r.right
        6.0
        >>> r.lower
        2.0
        >>> r.upper
        6.0
        """
        shift = (new_center[0] - (self.left + self.right) / 2,
                 new_center[1] - (self.lower + self.upper) / 2)
        self.left = self.left + shift[0]
        self.right = self.right + shift[0]
        self.lower = self.lower + shift[1]
        self.upper = self.upper + shift[1]

    def set_scale(self, scale):
        """
        Rescales the rectangle around its center.

        set_scale(number) -> number

        Parameters
        ----------
        scale : the ratio of the new scale to the old scale (e.g. 1.0 is current size)

        Attributes
        ----------

        Examples
        --------
        >>> r = Rectangle(0, 0, 4, 4)
        >>> r.set_scale(2)
        >>> r.left
        -2.0
        >>> r.right
        6.0
        >>> r.lower
        -2.0
        >>> r.upper
        6.0
        """
        center = ((self.left + self.right) / 2, (self.lower + self.upper) / 2)
        self.left = center[0] + scale * (self.left - center[0])
        self.right = center[0] + scale * (self.right - center[0])
        self.lower = center[1] + scale * (self.lower - center[1])
        self.upper = center[1] + scale * (self.upper - center[1])

    @property
    def area(self):
        """
        Returns the area of the Rectangle.

        area -> number

        Attributes
        ----------

        Examples
        --------
        >>> r = Rectangle(0, 0, 4, 4)
        >>> r.area
        16.0
        """
        return (self.right - self.left) * (self.upper - self.lower)

    @property
    def width(self):
        """
        Returns the width of the Rectangle.

        width -> number

        Attributes
        ----------

        Examples
        --------
        >>> r = Rectangle(0, 0, 4, 4)
        >>> r.width
        4.0
        """
        return self.right - self.left

    @property
    def height(self):
        """
        Returns the height of the Rectangle.

        height -> number

        Examples
        --------
        >>> r = Rectangle(0, 0, 4, 4)
        >>> r.height
        4.0
        """
        return self.upper - self.lower


_geoJSON_type_to_Pysal_type = {'point': Point, 'linestring': Chain,
                               'polygon': Polygon, 'multipolygon': Polygon}
import standalone  # moving this to top breaks unit tests !



########NEW FILE########
__FILENAME__ = sphere
"""
sphere: Tools for working with spherical distances.

Author: Charles R Schmidt <schmidtc@gmail.com>
"""

__author__ = "Charles R Schmidt <schmidtc@gmail.com>"
import math
import random
import numpy
import scipy.spatial
import scipy.constants
from scipy.spatial.distance import euclidean
from math import pi, cos, sin, asin

__all__ = ['RADIUS_EARTH_KM', 'RADIUS_EARTH_MILES', 'arcdist', 'arcdist2linear', 'brute_knn', 'fast_knn', 'fast_threshold', 'linear2arcdist', 'toLngLat', 'toXYZ']


RADIUS_EARTH_KM = 6371.0  
RADIUS_EARTH_MILES = (
    RADIUS_EARTH_KM * scipy.constants.kilo) / scipy.constants.mile


def arcdist(pt0, pt1, radius=RADIUS_EARTH_KM):
    """
    Parameters
    ----------
    pt0 : point
        assumed to be in form (lng,lat)
    pt1 : point
        assumed to be in form (lng,lat)
    radius : radius of the sphere
        defaults to Earth's radius

        Source: http://nssdc.gsfc.nasa.gov/planetary/factsheet/earthfact.html

    Returns 
    -------
    The arc distance between pt0 and pt1 using supplied radius

    Examples
    --------
    >>> pt0 = (0,0)
    >>> pt1 = (180,0)
    >>> d = arcdist(pt0,pt1,RADIUS_EARTH_MILES)
    >>> d == math.pi*RADIUS_EARTH_MILES
    True
    """
    return linear2arcdist(euclidean(toXYZ(pt0), toXYZ(pt1)), radius)


def arcdist2linear(arc_dist, radius=RADIUS_EARTH_KM):
    """
    Convert an arc distance (spherical earth) to a linear distance (R3) in the unit sphere.

    Examples
    --------
    >>> pt0 = (0,0)
    >>> pt1 = (180,0)
    >>> d = arcdist(pt0,pt1,RADIUS_EARTH_MILES)
    >>> d == math.pi*RADIUS_EARTH_MILES
    True
    >>> arcdist2linear(d,RADIUS_EARTH_MILES)
    2.0
    """
    c = 2 * math.pi * radius
    d = (2 - (2 * math.cos(math.radians((arc_dist * 360.0) / c)))) ** (0.5)
    return d


def linear2arcdist(linear_dist, radius=RADIUS_EARTH_KM):
    """
    Convert a linear distance in the unit sphere (R3) to an arc distance based on supplied radius

    Examples
    --------
    >>> pt0 = (0,0)
    >>> pt1 = (180,0)
    >>> d = arcdist(pt0,pt1,RADIUS_EARTH_MILES)
    >>> d == linear2arcdist(2.0, radius = RADIUS_EARTH_MILES)
    True
    """
    if linear_dist == float('inf'):
        return float('inf')
    elif linear_dist > 2.0:
        raise ValueError("linear_dist, must not exceed the diameter of the unit sphere, 2.0")
    c = 2 * math.pi * radius
    a2 = linear_dist ** 2
    theta = math.degrees(math.acos((2 - a2) / (2.)))
    d = (theta * c) / 360.0
    return d


def toXYZ(pt):
    """
    Parameters
    ----------
    pt0 : point
        assumed to be in form (lng,lat)
    pt1 : point
        assumed to be in form (lng,lat)

    Returns
    -------
    x, y, z
    """
    phi, theta = map(math.radians, pt)
    phi, theta = phi + pi, theta + (pi / 2)
    x = 1 * sin(theta) * cos(phi)
    y = 1 * sin(theta) * sin(phi)
    z = 1 * cos(theta)
    return x, y, z


def toLngLat(xyz):
    x, y, z = xyz
    if z == -1 or z == 1:
        phi = 0
    else:
        phi = math.atan2(y, x)
        if phi > 0:
            phi = phi - math.pi
        elif phi < 0:
            phi = phi + math.pi
    theta = math.acos(z) - (math.pi / 2)
    return phi, theta


def brute_knn(pts, k, mode='arc'):
    """
    valid modes are ['arc','xrz']
    """
    n = len(pts)
    full = numpy.zeros((n, n))
    for i in xrange(n):
        for j in xrange(i + 1, n):
            if mode == 'arc':
                lng0, lat0 = pts[i]
                lng1, lat1 = pts[j]
                dist = arcdist(pts[i], pts[j], radius=RADIUS_EARTH_KM)
            elif mode == 'xyz':
                dist = euclidean(pts[i], pts[j])
            full[i, j] = dist
            full[j, i] = dist
    w = {}
    for i in xrange(n):
        w[i] = full[i].argsort()[1:k + 1].tolist()
    return w


def fast_knn(pts, k, return_dist=False):
    """
    Computes k nearest neighbors on a sphere.

    Parameters
    ----------
    pts :  list of x,y pairs
    k   :  int
        Number of points to query
    return_dist : bool
        Return distances in the 'wd' container object

    Returns
    -------
    wn  :  list
        list of neighbors
    wd  : list
        list of neighbor distances (optional)

    """
    pts = numpy.array(pts)
    kd = scipy.spatial.KDTree(pts)
    d, w = kd.query(pts, k + 1)
    w = w[:, 1:]
    wn = {}
    for i in xrange(len(pts)):
        wn[i] = w[i].tolist()
    if return_dist:
        d = d[:, 1:]
        wd = {}
        for i in xrange(len(pts)):
            wd[i] = [linear2arcdist(x,
                                    radius=RADIUS_EARTH_MILES) for x in d[i].tolist()]
        return wn, wd
    return wn


def fast_threshold(pts, dist, radius=RADIUS_EARTH_KM):
    d = arcdist2linear(dist, radius)
    kd = scipy.spatial.KDTree(pts)
    r = kd.query_ball_tree(kd, d)
    wd = {}
    for i in xrange(len(pts)):
        l = r[i]
        l.remove(i)
        wd[i] = l
    return wd


if __name__ == '__main__':
    def random_ll():
        long = (random.random() * 360) - 180
        lat = (random.random() * 180) - 90
        return long, lat

    for i in range(1):
        n = 99
        # generate random surface points.
        pts = [random_ll() for i in xrange(n)]
        # convert to unit sphere points.
        pts2 = map(toXYZ, pts)

        w = brute_knn(pts, 4, 'arc')
        w2 = brute_knn(pts2, 4, 'xyz')
        w3 = fast_knn(pts2, 4)
        assert w == w2 == w3

    ### Make knn1
    import pysal
    f = pysal.open('/Users/charlie/Documents/data/stl_hom/stl_hom.shp', 'r')
    shapes = f.read()
    pts = [shape.centroid for shape in shapes]
    w0 = brute_knn(pts, 4, 'xyz')
    w1 = brute_knn(pts, 4, 'arc')
    pts = map(toXYZ, pts)
    w2 = brute_knn(pts, 4, 'xyz')
    w3 = fast_knn(pts, 4)

    wn, wd = fast_knn(pts, 4, True)
    ids = range(1, len(pts) + 1)

########NEW FILE########
__FILENAME__ = standalone
"""
Helper functions for computational geometry in PySAL

"""

__author__ = "Sergio J. Rey, Xinyue Ye, Charles Schmidt, Andrew Winslow"
__credits__ = "Copyright (c) 2005-2009 Sergio J. Rey"

import doctest
import math
import copy
from shapes import *
from itertools import islice
import scipy.spatial
from pysal.common import *

EPSILON_SCALER = 3


__all__ = ['bbcommon', 'get_bounding_box', 'get_angle_between', 'is_collinear', 'get_segments_intersect', 'get_segment_point_intersect', 'get_polygon_point_intersect', 'get_rectangle_point_intersect', 'get_ray_segment_intersect', 'get_rectangle_rectangle_intersection', 'get_polygon_point_dist', 'get_points_dist', 'get_segment_point_dist', 'get_point_at_angle_and_dist', 'convex_hull', 'is_clockwise', 'point_touches_rectangle', 'get_shared_segments', 'distance_matrix']


def bbcommon(bb, bbother):
    """
    Old Stars method for bounding box overlap testing
    Also defined in pysal.weights._cont_binning

    Examples
    --------

    >>> b0 = [0,0,10,10]
    >>> b1 = [10,0,20,10]
    >>> bbcommon(b0,b1)
    1
    """
    chflag = 0
    if not ((bbother[2] < bb[0]) or (bbother[0] > bb[2])):
        if not ((bbother[3] < bb[1]) or (bbother[1] > bb[3])):
            chflag = 1
    return chflag


def get_bounding_box(items):
    """

    Examples
    --------
    >>> bb = get_bounding_box([Point((-1, 5)), Rectangle(0, 6, 11, 12)])
    >>> bb.left
    -1.0
    >>> bb.lower
    5.0
    >>> bb.right
    11.0
    >>> bb.upper
    12.0
    """

    def left(o):
        if hasattr(o, 'bounding_box'):  # Polygon, Ellipse
            return o.bounding_box.left
        elif hasattr(o, 'left'):  # Rectangle
            return o.left
        else:  # Point
            return o[0]

    def right(o):
        if hasattr(o, 'bounding_box'):  # Polygon, Ellipse
            return o.bounding_box.right
        elif hasattr(o, 'right'):  # Rectangle
            return o.right
        else:  # Point
            return o[0]

    def lower(o):
        if hasattr(o, 'bounding_box'):  # Polygon, Ellipse
            return o.bounding_box.lower
        elif hasattr(o, 'lower'):  # Rectangle
            return o.lower
        else:  # Point
            return o[1]

    def upper(o):
        if hasattr(o, 'bounding_box'):  # Polygon, Ellipse
            return o.bounding_box.upper
        elif hasattr(o, 'upper'):  # Rectangle
            return o.upper
        else:  # Point
            return o[1]

    return Rectangle(min(map(left, items)), min(map(lower, items)), max(map(right, items)), max(map(upper, items)))


def get_angle_between(ray1, ray2):
    """
    Returns the angle formed between a pair of rays which share an origin
    get_angle_between(Ray, Ray) -> number

    Parameters
    ----------
    ray1   : a ray forming the beginning of the angle measured
    ray2   : a ray forming the end of the angle measured

    Examples
    --------
    >>> get_angle_between(Ray(Point((0, 0)), Point((1, 0))), Ray(Point((0, 0)), Point((1, 0))))
    0.0
    """

    if ray1.o != ray2.o:
        raise ValueError('Rays must have the same origin.')
    vec1 = (ray1.p[0] - ray1.o[0], ray1.p[1] - ray1.o[1])
    vec2 = (ray2.p[0] - ray2.o[0], ray2.p[1] - ray2.o[1])
    rot_theta = -math.atan2(vec1[1], vec1[0])
    rot_matrix = [[math.cos(rot_theta), -math.sin(rot_theta)], [
        math.sin(rot_theta), math.cos(rot_theta)]]
    rot_vec2 = (rot_matrix[0][0] * vec2[0] + rot_matrix[0][1] * vec2[1],
                rot_matrix[1][0] * vec2[0] + rot_matrix[1][1] * vec2[1])
    return math.atan2(rot_vec2[1], rot_vec2[0])


def is_collinear(p1, p2, p3):
    """
    Returns whether a triplet of points is collinear.

    is_collinear(Point, Point, Point) -> bool

    Parameters
    ----------
    p1 : a point (Point)
    p2 : another point (Point)
    p3 : yet another point (Point)

    Attributes
    ----------

    Examples
    --------
    >>> is_collinear(Point((0, 0)), Point((1, 1)), Point((5, 5)))
    True
    >>> is_collinear(Point((0, 0)), Point((1, 1)), Point((5, 0)))
    False
    """
    eps = np.finfo(type(p1[0])).eps

    return (abs((p2[0] - p1[0]) * (p3[1] - p1[1]) - (p2[1] - p1[1]) * (p3[0] - p1[0])) < EPSILON_SCALER * eps)


def get_segments_intersect(seg1, seg2):
    """
    Returns the intersection of two segments.

    get_segments_intersect(LineSegment, LineSegment) -> Point or LineSegment

    Parameters
    ----------
    seg1 : a segment to check intersection for
    seg2 : a segment to check intersection for

    Attributes
    ----------

    Examples
    --------
    >>> seg1 = LineSegment(Point((0, 0)), Point((0, 10)))
    >>> seg2 = LineSegment(Point((-5, 5)), Point((5, 5)))
    >>> i = get_segments_intersect(seg1, seg2)
    >>> isinstance(i, Point)
    True
    >>> str(i)
    '(0.0, 5.0)'
    >>> seg3 = LineSegment(Point((100, 100)), Point((100, 101)))
    >>> i = get_segments_intersect(seg2, seg3)
    """

    p1 = seg1.p1
    p2 = seg1.p2
    p3 = seg2.p1
    p4 = seg2.p2
    a = p2[0] - p1[0]
    b = p3[0] - p4[0]
    c = p2[1] - p1[1]
    d = p3[1] - p4[1]
    det = float(a * d - b * c)
    if det == 0:
        if seg1 == seg2:
            return LineSegment(seg1.p1, seg1.p2)
        else:
            a = get_segment_point_intersect(seg2, seg1.p1)
            b = get_segment_point_intersect(seg2, seg1.p2)
            c = get_segment_point_intersect(seg1, seg2.p1)
            d = get_segment_point_intersect(seg1, seg2.p2)

            if a and b:  # seg1 in seg2
                return LineSegment(seg1.p1, seg1.p2)
            if c and d:  # seg2 in seg1
                return LineSegment(seg2.p1, seg2.p2)
            if (a or b) and (c or d):
                p1 = a if a else b
                p2 = c if c else d
                return LineSegment(p1, p2)

        return None
    a_inv = d / det
    b_inv = -b / det
    c_inv = -c / det
    d_inv = a / det
    m = p3[0] - p1[0]
    n = p3[1] - p1[1]
    x = a_inv * m + b_inv * n
    y = c_inv * m + d_inv * n
    intersect_exists = 0 <= x <= 1 and 0 <= y <= 1
    if not intersect_exists:
        return None
    return Point((p1[0] + x * (p2[0] - p1[0]), p1[1] + x * (p2[1] - p1[1])))


def get_segment_point_intersect(seg, pt):
    """
    Returns the intersection of a segment and point.

    get_segment_point_intersect(LineSegment, Point) -> Point

    Parameters
    ----------
    seg : a segment to check intersection for
    pt  : a point to check intersection for

    Attributes
    ----------

    Examples
    --------
    >>> seg = LineSegment(Point((0, 0)), Point((0, 10)))
    >>> pt = Point((0, 5))
    >>> i = get_segment_point_intersect(seg, pt)
    >>> str(i)
    '(0.0, 5.0)'
    >>> pt2 = Point((5, 5))
    >>> get_segment_point_intersect(seg, pt2)
    """
    eps = np.finfo(type(pt[0])).eps

    if is_collinear(pt, seg.p1, seg.p2):
        if get_segment_point_dist(seg, pt)[0] < EPSILON_SCALER * eps:
            return pt
        else:
            return None

    vec1 = (pt[0] - seg.p1[0], pt[1] - seg.p1[1])
    vec2 = (seg.p2[0] - seg.p1[0], seg.p2[1] - seg.p1[1])
    if abs(vec1[0] * vec2[1] - vec1[1] * vec2[0]) < eps:
        return pt
    return None


def get_polygon_point_intersect(poly, pt):
    """
    Returns the intersection of a polygon and point.

    get_polygon_point_intersect(Polygon, Point) -> Point

    Parameters
    ----------
    poly : a polygon to check intersection for
    pt   : a point to check intersection for

    Attributes
    ----------

    Examples
    --------
    >>> poly = Polygon([Point((0, 0)), Point((1, 0)), Point((1, 1)), Point((0, 1))])
    >>> pt = Point((0.5, 0.5))
    >>> i = get_polygon_point_intersect(poly, pt)
    >>> str(i)
    '(0.5, 0.5)'
    >>> pt2 = Point((2, 2))
    >>> get_polygon_point_intersect(poly, pt2)
    """
    def pt_lies_on_part_boundary(pt, vertices):
        return filter(
            lambda i: get_segment_point_dist(LineSegment(
                vertices[i], vertices[i + 1]), pt)[0] == 0,
            xrange(-1, len(vertices) - 1)) != []

    ret = None
    if get_rectangle_point_intersect(poly.bounding_box, pt) is None:  # Weed out points that aren't even close
        return None
    elif filter(lambda verts: pt_lies_on_part_boundary(pt, verts), poly._vertices) != []:
        ret = pt
    elif filter(lambda verts: _point_in_vertices(pt, verts), poly._vertices) != []:
        ret = pt
    if poly._holes != [[]]:
        if filter(lambda verts: pt_lies_on_part_boundary(pt, verts), poly.holes) != []:
            # pt lies on boundary of hole.
            pass
        if filter(lambda verts: _point_in_vertices(pt, verts), poly.holes) != []:
            # pt lines inside a hole.
            ret = None
        #raise NotImplementedError, 'Cannot compute containment for polygon with holes'
    return ret


def get_rectangle_point_intersect(rect, pt):
    """
    Returns the intersection of a rectangle and point.

    get_rectangle_point_intersect(Rectangle, Point) -> Point

    Parameters
    ----------
    rect : a rectangle to check intersection for
    pt   : a point to check intersection for

    Attributes
    ----------

    Examples
    --------
    >>> rect = Rectangle(0, 0, 5, 5)
    >>> pt = Point((1, 1))
    >>> i = get_rectangle_point_intersect(rect, pt)
    >>> str(i)
    '(1.0, 1.0)'
    >>> pt2 = Point((10, 10))
    >>> get_rectangle_point_intersect(rect, pt2)
    """
    if rect.left <= pt[0] <= rect.right and rect.lower <= pt[1] <= rect.upper:
        return pt
    return None


def get_ray_segment_intersect(ray, seg):
    """
    Returns the intersection of a ray and line segment.

    get_ray_segment_intersect(Ray, Point) -> Point or LineSegment

    Parameters
    ----------

    ray : a ray to check intersection for
    seg : a line segment to check intersection for

    Attributes
    ----------

    Examples
    --------
    >>> ray = Ray(Point((0, 0)), Point((0, 1)))
    >>> seg = LineSegment(Point((-1, 10)), Point((1, 10)))
    >>> i = get_ray_segment_intersect(ray, seg)
    >>> isinstance(i, Point)
    True
    >>> str(i)
    '(0.0, 10.0)'
    >>> seg2 = LineSegment(Point((10, 10)), Point((10, 11)))
    >>> get_ray_segment_intersect(ray, seg2)
    """
    d = max(math.hypot(seg.p1[0] - ray.o[0], seg.p1[1] - ray.o[1]),
            math.hypot(seg.p2[0] - ray.o[0], seg.p2[1] - ray.o[1])) + 1  # Upper bound on origin to segment dist (+1)
    ratio = d / math.hypot(ray.o[0] - ray.p[0], ray.o[1] - ray.p[1])
    ray_seg = LineSegment(
        ray.o, Point((ray.o[0] + ratio * (ray.p[0] - ray.o[0]),
                      ray.o[1] + ratio * (ray.p[1] - ray.o[1]))))
    return get_segments_intersect(seg, ray_seg)


def get_rectangle_rectangle_intersection(r0, r1, checkOverlap=True):
    """
    Returns the intersection between two rectangles.

    Note: Algorithm assumes the rectangles overlap.
          checkOverlap=False should be used with extreme caution.

    get_rectangle_rectangle_intersection(r0, r1) -> Rectangle, Segment, Point or None

    Parameters
    ----------
    r0   : a Rectangle
    r1   : a Rectangle

    Attributes
    ----------

    Examples
    --------
    >>> r0 = Rectangle(0,4,6,9)
    >>> r1 = Rectangle(4,0,9,7)
    >>> ri = get_rectangle_rectangle_intersection(r0,r1)
    >>> ri[:]
    [4.0, 4.0, 6.0, 7.0]
    >>> r0 = Rectangle(0,0,4,4)
    >>> r1 = Rectangle(2,1,6,3)
    >>> ri = get_rectangle_rectangle_intersection(r0,r1)
    >>> ri[:]
    [2.0, 1.0, 4.0, 3.0]
    >>> r0 = Rectangle(0,0,4,4)
    >>> r1 = Rectangle(2,1,3,2)
    >>> ri = get_rectangle_rectangle_intersection(r0,r1)
    >>> ri[:] == r1[:]
    True
    """
    if checkOverlap:
        if not bbcommon(r0, r1):
            #raise ValueError, "Rectangles do not intersect"
            return None
    left = max(r0.left, r1.left)
    lower = max(r0.lower, r1.lower)
    right = min(r0.right, r1.right)
    upper = min(r0.upper, r1.upper)

    if upper == lower and left == right:
        return Point((left, lower))
    elif upper == lower:
        return LineSegment(Point((left, lower)), Point((right, lower)))
    elif left == right:
        return LineSegment(Point((left, lower)), Point((left, upper)))

    return Rectangle(left, lower, right, upper)


def get_polygon_point_dist(poly, pt):
    """
    Returns the distance between a polygon and point.

    get_polygon_point_dist(Polygon, Point) -> number

    Parameters
    ----------
    poly : a polygon to compute distance from
    pt   : a point to compute distance from

    Attributes
    ----------

    Examples
    --------
    >>> poly = Polygon([Point((0, 0)), Point((1, 0)), Point((1, 1)), Point((0, 1))])
    >>> pt = Point((2, 0.5))
    >>> get_polygon_point_dist(poly, pt)
    1.0
    >>> pt2 = Point((0.5, 0.5))
    >>> get_polygon_point_dist(poly, pt2)
    0.0
    """
    if get_polygon_point_intersect(poly, pt) is not None:
        return 0.0
    part_prox = []
    for vertices in poly._vertices:
        part_prox.append(min([get_segment_point_dist(LineSegment(vertices[i], vertices[i + 1]), pt)[0]
                              for i in xrange(-1, len(vertices) - 1)]))
    return min(part_prox)


def get_points_dist(pt1, pt2):
    """
    Returns the distance between a pair of points.

    get_points_dist(Point, Point) -> number

    Parameters
    ----------
    pt1 : a point
    pt2 : the other point

    Attributes
    ----------

    Examples
    --------
    >>> get_points_dist(Point((4, 4)), Point((4, 8)))
    4.0
    >>> get_points_dist(Point((0, 0)), Point((0, 0)))
    0.0
    """
    return math.hypot(pt1[0] - pt2[0], pt1[1] - pt2[1])


def get_segment_point_dist(seg, pt):
    """
    Returns the distance between a line segment and point and distance along the segment of the closest
    point on the segment to the point as a ratio of the length of the segment.

    get_segment_point_dist(LineSegment, Point) -> (number, number)

    Parameters
    ----------
    seg  : a line segment to compute distance from
    pt   : a point to compute distance from

    Attributes
    ----------

    Examples
    --------
    >>> seg = LineSegment(Point((0, 0)), Point((10, 0)))
    >>> pt = Point((5, 5))
    >>> get_segment_point_dist(seg, pt)
    (5.0, 0.5)
    >>> pt2 = Point((0, 0))
    >>> get_segment_point_dist(seg, pt2)
    (0.0, 0.0)
    """
    src_p = seg.p1
    dest_p = seg.p2

    # Shift line to go through origin
    points_0 = pt[0] - src_p[0]
    points_1 = pt[1] - src_p[1]
    points_2 = 0
    points_3 = 0
    points_4 = dest_p[0] - src_p[0]
    points_5 = dest_p[1] - src_p[1]

    segment_length = get_points_dist(src_p, dest_p)

    # Meh, robustness...maybe should incorporate this into a more general
    # approach later
    if segment_length == 0:
        return (get_points_dist(pt, src_p), 0)

    u_x = points_4 / segment_length
    u_y = points_5 / segment_length

    inter_x = u_x * u_x * points_0 + u_x * u_y * points_1
    inter_y = u_x * u_y * points_0 + u_y * u_y * points_1

    src_proj_dist = get_points_dist((0, 0), (inter_x, inter_y))
    dest_proj_dist = get_points_dist((inter_x, inter_y), (points_4, points_5))

    if src_proj_dist > segment_length or dest_proj_dist > segment_length:
        src_pt_dist = get_points_dist(
            (points_2, points_3), (points_0, points_1))
        dest_pt_dist = get_points_dist(
            (points_4, points_5), (points_0, points_1))
        if src_pt_dist < dest_pt_dist:
            return (src_pt_dist, 0)
        else:
            return (dest_pt_dist, 1)
    else:
        return (get_points_dist((inter_x, inter_y), (points_0, points_1)), src_proj_dist / segment_length)


def get_point_at_angle_and_dist(ray, angle, dist):
    """
    Returns the point at a distance and angle relative to the origin of a ray.

    get_point_at_angle_and_dist(Ray, number, number) -> Point

    Parameters
    ----------
    ray   : the ray which the angle and distance are relative to
    angle : the angle relative to the ray at which the point is located
    dist  : the distance from the ray origin at which the point is located

    Attributes
    ----------

    Examples
    --------
    >>> ray = Ray(Point((0, 0)), Point((1, 0)))
    >>> pt = get_point_at_angle_and_dist(ray, math.pi, 1.0)
    >>> isinstance(pt, Point)
    True
    >>> round(pt[0], 8)
    -1.0
    >>> round(pt[1], 8)
    0.0
    """
    v = (ray.p[0] - ray.o[0], ray.p[1] - ray.o[1])
    cur_angle = math.atan2(v[1], v[0])
    dest_angle = cur_angle + angle
    return Point((ray.o[0] + dist * math.cos(dest_angle), ray.o[1] + dist * math.sin(dest_angle)))


def convex_hull(points):
    """
    Returns the convex hull of a set of points.

    convex_hull(Point list) -> Polygon

    Parameters
    ----------
    points : a list of points to compute the convex hull for

    Attributes
    ----------

    Examples
    --------
    >>> points = [Point((0, 0)), Point((4, 4)), Point((4, 0)), Point((3, 1))]
    >>> convex_hull(points)
    [(0.0, 0.0), (4.0, 0.0), (4.0, 4.0)]
    """
    points = copy.copy(points)
    lowest = min(points, key=lambda p: (p[1], p[0]))

    points.remove(lowest)
    points.sort(key=lambda p: math.atan2(p[1] - lowest[1], p[0] - lowest[0]))

    stack = [lowest]

    def right_turn(p1, p2, p3):
        # Returns if p1 -> p2 -> p3 forms a 'right turn'
        vec1 = (p2[0] - p1[0], p2[1] - p1[1])
        vec2 = (p3[0] - p2[0], p3[1] - p2[1])
        return vec2[0] * vec1[1] - vec2[1] * vec1[0] >= 0

    for p in points:
        stack.append(p)
        while len(stack) > 3 and right_turn(stack[-3], stack[-2], stack[-1]):
            stack.pop(-2)

    return stack


def is_clockwise(vertices):
    """
    Returns whether a list of points describing a polygon are clockwise or counterclockwise.

    is_clockwise(Point list) -> bool

    Parameters
    ----------
    vertices : a list of points that form a single ring

    Examples
    --------
    >>> is_clockwise([Point((0, 0)), Point((10, 0)), Point((0, 10))])
    False
    >>> is_clockwise([Point((0, 0)), Point((0, 10)), Point((10, 0))])
    True
    >>> v = [(-106.57798, 35.174143999999998), (-106.583412, 35.174141999999996), (-106.58417999999999, 35.174143000000001), (-106.58377999999999, 35.175542999999998), (-106.58287999999999, 35.180543), (-106.58263099999999, 35.181455), (-106.58257999999999, 35.181643000000001), (-106.58198299999999, 35.184615000000001), (-106.58148, 35.187242999999995), (-106.58127999999999, 35.188243), (-106.58138, 35.188243), (-106.58108, 35.189442999999997), (-106.58104, 35.189644000000001), (-106.58028, 35.193442999999995), (-106.580029, 35.194541000000001), (-106.57974399999999, 35.195785999999998), (-106.579475, 35.196961999999999), (-106.57922699999999, 35.198042999999998), (-106.578397, 35.201665999999996), (-106.57827999999999, 35.201642999999997), (-106.57737999999999, 35.201642999999997), (-106.57697999999999, 35.201543000000001), (-106.56436599999999, 35.200311999999997), (-106.56058, 35.199942999999998), (-106.56048, 35.197342999999996), (-106.56048, 35.195842999999996), (-106.56048, 35.194342999999996), (-106.56048, 35.193142999999999), (-106.56048, 35.191873999999999), (-106.56048, 35.191742999999995), (-106.56048, 35.190242999999995), (-106.56037999999999, 35.188642999999999), (-106.56037999999999, 35.187242999999995), (-106.56037999999999, 35.186842999999996), (-106.56037999999999, 35.186552999999996), (-106.56037999999999, 35.185842999999998), (-106.56037999999999, 35.184443000000002), (-106.56037999999999, 35.182943000000002), (-106.56037999999999, 35.181342999999998), (-106.56037999999999, 35.180433000000001), (-106.56037999999999, 35.179943000000002), (-106.56037999999999, 35.178542999999998), (-106.56037999999999, 35.177790999999999), (-106.56037999999999, 35.177143999999998), (-106.56037999999999, 35.175643999999998), (-106.56037999999999, 35.174444000000001), (-106.56037999999999, 35.174043999999995), (-106.560526, 35.174043999999995), (-106.56478, 35.174043999999995), (-106.56627999999999, 35.174143999999998), (-106.566541, 35.174144999999996), (-106.569023, 35.174157000000001), (-106.56917199999999, 35.174157999999998), (-106.56938, 35.174143999999998), (-106.57061499999999, 35.174143999999998), (-106.57097999999999, 35.174143999999998), (-106.57679999999999, 35.174143999999998), (-106.57798, 35.174143999999998)]
    >>> is_clockwise(v)
    True
    """
    if len(vertices) < 3:
        return True
    area = 0.0
    ax, ay = vertices[0]
    for bx, by in vertices[1:]:
        area += ax * by - ay * bx
        ax, ay = bx, by
    bx, by = vertices[0]
    area += ax * by - ay * bx
    return area < 0.0


def ccw(vertices):
    """
    Returns whether a list of points is counterclockwise

    >>> ccw([Point((0, 0)), Point((10, 0)), Point((0, 10))])
    True
    >>> ccw([Point((0, 0)), Point((0, 10)), Point((10, 0))])
    False
    """

    if is_clockwise(vertices):
        return False
    else:
        return True


def seg_intersect(a, b, c, d):
    """
    Tests if two segments (a,b) (c,d) intersect

    >>> a = Point((0,1))
    >>> b = Point((0,10))
    >>> c = Point((-2,5))
    >>> d = Point((2,5))
    >>> e = Point((-3,5))
    >>> seg_intersect(a, b, c, d)
    True
    >>> seg_intersect(a, b, c, e)
    False
    """
    if ccw([a, c, d]) == ccw([b, c, d]):
        return False
    elif ccw([a, b, c]) == ccw([a, b, d]):
        return False
    else:
        return True


def _point_in_vertices(pt, vertices):
    """
    HELPER METHOD. DO NOT CALL.

    Returns whether a point is contained in a polygon specified by a sequence of vertices.

    _point_in_vertices(Point, Point list) -> bool

    Parameters
    ----------

    Attributes
    ----------

    Examples
    --------
    >>> _point_in_vertices(Point((1, 1)), [Point((0, 0)), Point((10, 0)), Point((0, 10))])
    True
    """

    def neg_ray_intersect(p1, p2, p3):
        # Returns whether a ray in the negative-x direction from p3 intersects the segment between
        if not min(p1[1], p2[1]) <= p3[1] <= max(p1[1], p2[1]):
            return False
        if p1[1] > p2[1]:
            vec1 = (p2[0] - p1[0], p2[1] - p1[1])
        else:
            vec1 = (p1[0] - p2[0], p1[1] - p2[1])
        vec2 = (p3[0] - p1[0], p3[1] - p1[1])
        return vec1[0] * vec2[1] - vec2[0] * vec1[1] >= 0

    vert_y_set = set([v[1] for v in vertices])
    while pt[1] in vert_y_set:
        pt = (pt[0], pt[1] + -1e-14 + random.random(
        ) * 2e-14)  # Perturb the location very slightly
    inters = 0
    for i in xrange(-1, len(vertices) - 1):
        v1 = vertices[i]
        v2 = vertices[i + 1]
        if neg_ray_intersect(v1, v2, pt):
            inters += 1

    return inters % 2 == 1


def point_touches_rectangle(point, rect):
    """
    Returns True if the point is in the rectangle or touches it's boundary.

    point_touches_rectangle(point, rect) -> bool

    Parameters
    ----------
    point : Point or Tuple
    rect  : Rectangle

    Examples
    --------
    >>> rect = Rectangle(0,0,10,10)
    >>> a = Point((5,5))
    >>> b = Point((10,5))
    >>> c = Point((11,11))
    >>> point_touches_rectangle(a,rect)
    1
    >>> point_touches_rectangle(b,rect)
    1
    >>> point_touches_rectangle(c,rect)
    0
    """
    chflag = 0
    if point[0] >= rect.left and point[0] <= rect.right:
        if point[1] >= rect.lower and point[1] <= rect.upper:
            chflag = 1
    return chflag


def get_shared_segments(poly1, poly2, bool_ret=False):
    """
    Returns the line segments in common to both polygons.

    get_shared_segments(poly1, poly2) -> list

    Parameters
    ----------
    poly1   : a Polygon
    poly2   : a Polygon

    Attributes
    ----------

    Examples
    --------
    >>> x = [0, 0, 1, 1]
    >>> y = [0, 1, 1, 0]
    >>> poly1 = Polygon( map(Point,zip(x,y)) )
    >>> x = [a+1 for a in x]
    >>> poly2 = Polygon( map(Point,zip(x,y)) )
    >>> get_shared_segments(poly1, poly2, bool_ret=True)
    True

    """
    #get_rectangle_rectangle_intersection inlined for speed.
    r0 = poly1.bounding_box
    r1 = poly2.bounding_box
    wLeft = max(r0.left, r1.left)
    wLower = max(r0.lower, r1.lower)
    wRight = min(r0.right, r1.right)
    wUpper = min(r0.upper, r1.upper)

    segmentsA = set()
    common = list()
    partsA = poly1.parts
    for part in poly1.parts + [p for p in poly1.holes if p]:
        if part[0] != part[-1]:  # not closed
            part = part[:] + part[0:1]
        a = part[0]
        for b in islice(part, 1, None):
            # inlining point_touches_rectangle for speed
            x, y = a
            # check if point a is in the bounding box intersection
            if x >= wLeft and x <= wRight and y >= wLower and y <= wUpper:
                x, y = b
                # check if point b is in the bounding box intersection
                if x >= wLeft and x <= wRight and y >= wLower and y <= wUpper:
                    if a > b:
                        segmentsA.add((b, a))
                    else:
                        segmentsA.add((a, b))
            a = b
    for part in poly2.parts + [p for p in poly2.holes if p]:
        if part[0] != part[-1]:  # not closed
            part = part[:] + part[0:1]
        a = part[0]
        for b in islice(part, 1, None):
            # inlining point_touches_rectangle for speed
            x, y = a
            if x >= wLeft and x <= wRight and y >= wLower and y <= wUpper:
                x, y = b
                if x >= wLeft and x <= wRight and y >= wLower and y <= wUpper:
                    if a > b:
                        seg = (b, a)
                    else:
                        seg = (a, b)
                    if seg in segmentsA:
                        common.append(LineSegment(*seg))
                        if bool_ret:
                            return True
            a = b
    if bool_ret:
        if len(common) > 0:
            return True
        else:
            return False
    return common


def distance_matrix(X, p=2.0, threshold=5e7):
    """
    Distance Matrices

    XXX Needs optimization/integration with other weights in pysal

    Parameters
    ----------
    X          : An, n by k numpy.ndarray
                    Where n is number of observations
                    k is number of dimmensions (2 for x,y)
    p          : float
                    Minkowski p-norm distance metric parameter:
                    1<=p<=infinity
                    2: Euclidean distance
                    1: Manhattan distance
    threshold  : positive integer
                    If (n**2)*32 > threshold use scipy.spatial.distance_matrix instead
                    of working in ram, this is roughly the ammount of ram (in bytes) that will be used.

    Examples
    --------
    >>> x,y=[r.flatten() for r in np.indices((3,3))]
    >>> data = np.array([x,y]).T
    >>> d=distance_matrix(data)
    >>> np.array(d)
    array([[ 0.        ,  1.        ,  2.        ,  1.        ,  1.41421356,
             2.23606798,  2.        ,  2.23606798,  2.82842712],
           [ 1.        ,  0.        ,  1.        ,  1.41421356,  1.        ,
             1.41421356,  2.23606798,  2.        ,  2.23606798],
           [ 2.        ,  1.        ,  0.        ,  2.23606798,  1.41421356,
             1.        ,  2.82842712,  2.23606798,  2.        ],
           [ 1.        ,  1.41421356,  2.23606798,  0.        ,  1.        ,
             2.        ,  1.        ,  1.41421356,  2.23606798],
           [ 1.41421356,  1.        ,  1.41421356,  1.        ,  0.        ,
             1.        ,  1.41421356,  1.        ,  1.41421356],
           [ 2.23606798,  1.41421356,  1.        ,  2.        ,  1.        ,
             0.        ,  2.23606798,  1.41421356,  1.        ],
           [ 2.        ,  2.23606798,  2.82842712,  1.        ,  1.41421356,
             2.23606798,  0.        ,  1.        ,  2.        ],
           [ 2.23606798,  2.        ,  2.23606798,  1.41421356,  1.        ,
             1.41421356,  1.        ,  0.        ,  1.        ],
           [ 2.82842712,  2.23606798,  2.        ,  2.23606798,  1.41421356,
             1.        ,  2.        ,  1.        ,  0.        ]])
    >>>
    """
    if X.ndim == 1:
        X.shape = (X.shape[0], 1)
    if X.ndim > 2:
        raise TypeError("wtf?")
    n, k = X.shape

    if (n ** 2) * 32 > threshold:
        return scipy.spatial.distance_matrix(X, X, p)
    else:
        M = np.ones((n, n))
        D = np.zeros((n, n))
        for col in range(k):
            x = X[:, col]
            xM = x * M
            dx = xM - xM.T
            if p % 2 != 0:
                dx = np.abs(dx)
            dx2 = dx ** p
            D += dx2
        D = D ** (1.0 / p)
        return D



########NEW FILE########
__FILENAME__ = test_geoJSON
import pysal
import doctest
import unittest


class test_MultiPloygon(unittest.TestCase):

    def test___init__1(self):
        """
        Tests conversion of polygons with multiple shells to 
        geoJSON multipolygons. and back.
        """
        shp = pysal.open(pysal.examples.get_path("NAT.SHP"),'r')
        multipolygons = [p for p in shp if len(p.parts) > 1]
        geoJSON = [p.__geo_interface__ for p in multipolygons]
        for poly in multipolygons:
            json = poly.__geo_interface__
            shape = pysal.cg.asShape(json)
            self.assertEquals(json['type'],'MultiPolygon')
            self.assertEquals(str(shape.holes), str(poly.holes))
            self.assertEquals(str(shape.parts), str(poly.parts))

if __name__ == '__main__':
    unittest.main()
    #runner = unittest.TextTestRunner()
    #runner.run(suite)

########NEW FILE########
__FILENAME__ = test_locators

"""locators Unittest."""
from pysal.cg import *
import unittest


class PolygonLocator_Tester(unittest.TestCase):
    """setup class for unit tests."""
    def setUp(self):
        p1 = Polygon([Point((0, 1)), Point((4, 5)), Point((5, 1))])
        p2 = Polygon([Point((3, 9)), Point((6, 7)), Point((1, 1))])
        p3 = Polygon([Point((7, 1)), Point((8, 7)), Point((9, 1))])
        self.polygons = [p1, p2, p3]
        self.pl = PolygonLocator(self.polygons)

        pt = Point
        pg = Polygon
        polys = []
        for i in range(5):
            l = i * 10
            r = l + 10
            b = 10
            t = 20
            sw = pt((l, b))
            se = pt((r, b))
            ne = pt((r, t))
            nw = pt((l, t))
            polys.append(pg([sw, se, ne, nw]))
        self.pl2 = PolygonLocator(polys)

    def test_PolygonLocator(self):
        qr = Rectangle(3, 7, 5, 8)
        res = self.pl.inside(qr)
        self.assertEqual(len(res), 0)

    def test_inside(self):
        qr = Rectangle(3, 3, 5, 5)
        res = self.pl.inside(qr)
        self.assertEqual(len(res), 0)
        qr = Rectangle(0, 0, 5, 5)
        res = self.pl.inside(qr)
        self.assertEqual(len(res), 1)

    def test_overlapping(self):

        qr = Rectangle(3, 3, 5, 5)
        res = self.pl.overlapping(qr)
        self.assertEqual(len(res), 2)
        qr = Rectangle(8, 3, 10, 10)
        res = self.pl.overlapping(qr)
        self.assertEqual(len(res), 1)

        qr = Rectangle(2, 12, 35, 15)
        res = self.pl2.overlapping(qr)
        self.assertEqual(len(res), 4)

suite = unittest.TestSuite()
test_classes = [PolygonLocator_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_rtree

"""pyrtree Unittest."""
from pysal.cg import RTree, Rect
import unittest


class Pyrtree_Tester(unittest.TestCase):
    """setup class for unit tests."""
    def setUp(self):
        k = 10
        w = 20
        objects = {}
        id = 0
        for i in range(k):
            mn_y = i * w
            mx_y = mn_y + w
            for j in range(k):
                mn_x = j * w
                mx_x = mn_x + w
                objects[id] = Rect(mn_x, mn_y, mx_x, mx_y)
                id += 1
        self.objects = objects

    def test_rtree(self):
        t = RTree()
        for object in self.objects:
            t.insert(object, self.objects[object])
        self.assertEqual(len(self.objects), 100)

        qr = Rect(5, 5, 25, 25)

        # find objects with mbrs intersecting with qr
        res = [r.leaf_obj() for r in t.query_rect(qr) if r.is_leaf()]
        self.assertEqual(len(res), 4)
        res.sort()
        self.assertEqual(res, [0, 1, 10, 11])

        # vertices are shared by all coincident rectangles
        res = [r.leaf_obj(
        ) for r in t.query_point((20.0, 20.0)) if r.is_leaf()]
        self.assertEqual(len(res), 4)

        res = [r.leaf_obj() for r in t.query_point((21, 20)) if r.is_leaf()]
        self.assertEqual(len(res), 2)

        # single internal point
        res = [r.leaf_obj() for r in t.query_point((21, 21)) if r.is_leaf()]
        self.assertEqual(len(res), 1)

        # single external point
        res = [r.leaf_obj() for r in t.query_point((-12, 21)) if r.is_leaf()]
        self.assertEqual(len(res), 0)

        qr = Rect(5, 6, 65, 7)

        res = [r.leaf_obj() for r in t.query_rect((qr)) if r.is_leaf()]
        self.assertEqual(len(res), 4)


suite = unittest.TestSuite()
test_classes = [Pyrtree_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_segmentLocator
"""Segment Locator Unittest."""
from pysal.cg import *
from pysal.cg.segmentLocator import *
import unittest


class SegmentGrid_Tester(unittest.TestCase):
    """setup class for unit tests."""
    def setUp(self):
        # 10x10 grid with four line segments, one for each edge of the grid.
        self.grid = SegmentGrid(Rectangle(0, 0, 10, 10), 1)
        self.grid.add(LineSegment(Point((0.0, 0.0)), Point((0.0, 10.0))), 0)
        self.grid.add(LineSegment(Point((0.0, 10.0)), Point((10.0, 10.0))), 1)
        self.grid.add(LineSegment(Point((10.0, 10.0)), Point((10.0, 0.0))), 2)
        self.grid.add(LineSegment(Point((10.0, 0.0)), Point((0.0, 0.0))), 3)

    def test_nearest_1(self):
        self.assertEquals([0, 1, 2, 3], self.grid.nearest(Point((
            5.0, 5.0))))  # Center
        self.assertEquals(
            [0], self.grid.nearest(Point((0.0, 5.0))))  # Left Edge
        self.assertEquals(
            [1], self.grid.nearest(Point((5.0, 10.0))))  # Top Edge
        self.assertEquals(
            [2], self.grid.nearest(Point((10.0, 5.0))))  # Right Edge
        self.assertEquals(
            [3], self.grid.nearest(Point((5.0, 0.0))))  # Bottom Edge

    def test_nearest_2(self):
        self.assertEquals([0, 1, 3], self.grid.nearest(Point((-
                                                              100000.0, 5.0))))  # Left Edge
        self.assertEquals([1, 2, 3], self.grid.nearest(Point((
            100000.0, 5.0))))  # Right Edge
        self.assertEquals([0, 2, 3], self.grid.nearest(Point((5.0,
                                                              -100000.0))))  # Bottom Edge
        self.assertEquals([0, 1, 2], self.grid.nearest(Point((5.0,
                                                              100000.0))))  # Top Edge


suite = unittest.TestSuite()
test_classes = [SegmentGrid_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_shapes
from pysal.cg import Point, LineSegment, Line, Ray, Chain, Rectangle, Polygon
import doctest
import unittest


class test_Point(unittest.TestCase):

    def test___init__1(self):
        """
        Tests whether points are created without issue.

        Test tag: <tc>#tests#Point.__init__</tc>
        """
        for l in [(-5.0, 10.0), (0.0, -6.0), (float(1e300), float(-1e300))]:
            p = Point(l)

    def test___str__1(self):
        """
        Tests whether the string produced is valid for corner cases.

        Test tag: <tc>#tests#Point__str__</tc>
        """
        for l in [(-5, 10), (0, -6.0), (float(1e300), -1e300)]:
            p = Point(l)
            self.assertEquals(str(p), str((float(l[0]), float(
                l[1]))))  # Recast to floats like point does


class test_LineSegment(unittest.TestCase):

    def test_is_ccw1(self):
        """
        Test corner cases for horizontal segment starting at origin.

        Test tag: <tc>#tests#LineSegment.is_ccw</tc>
        """
        ls = LineSegment(Point((0, 0)), Point((5, 0)))
        self.assertFalse(ls.is_ccw(
            Point((10, 0))))  # At positive boundary beyond segment
        self.assertFalse(ls.is_ccw(Point((3, 0))))  # On segment
        self.assertFalse(ls.is_ccw(
            Point((-10, 0))))  # At negative boundary beyond segment
        self.assertFalse(ls.is_ccw(Point((0, 0))))  # Endpoint of segment
        self.assertFalse(ls.is_ccw(Point((5, 0))))  # Endpoint of segment

    def test_is_ccw2(self):
        """
        Test corner cases for vertical segment ending at origin.

        Test tag: <tc>#tests#LineSegment.is_ccw</tc>
        """
        ls = LineSegment(Point((0, -5)), Point((0, 0)))
        self.assertFalse(ls.is_ccw(
            Point((0, 10))))  # At positive boundary beyond segment
        self.assertFalse(ls.is_ccw(Point((0, -3))))  # On segment
        self.assertFalse(ls.is_ccw(
            Point((0, -10))))  # At negative boundary beyond segment
        self.assertFalse(ls.is_ccw(Point((0, -5))))  # Endpoint of segment
        self.assertFalse(ls.is_ccw(Point((0, 0))))  # Endpoint of segment

    def test_is_ccw3(self):
        """
        Test corner cases for non-axis-aligned segment not through origin.

        Test tag: <tc>#tests#LineSegment.is_ccw</tc>
        """
        ls = LineSegment(Point((0, 1)), Point((5, 6)))
        self.assertFalse(ls.is_ccw(
            Point((10, 11))))  # At positive boundary beyond segment
        self.assertFalse(ls.is_ccw(Point((3, 4))))  # On segment
        self.assertFalse(ls.is_ccw(
            Point((-10, -9))))  # At negative boundary beyond segment
        self.assertFalse(ls.is_ccw(Point((0, 1))))  # Endpoint of segment
        self.assertFalse(ls.is_ccw(Point((5, 6))))  # Endpoint of segment

    def test_is_cw1(self):
        """
        Test corner cases for horizontal segment starting at origin.

        Test tag: <tc>#tests#LineSegment.is_cw</tc>
        """
        ls = LineSegment(Point((0, 0)), Point((5, 0)))
        self.assertFalse(ls.is_cw(
            Point((10, 0))))  # At positive boundary beyond segment
        self.assertFalse(ls.is_cw(Point((3, 0))))  # On segment
        self.assertFalse(ls.is_cw(
            Point((-10, 0))))  # At negative boundary beyond segment
        self.assertFalse(ls.is_cw(Point((0, 0))))  # Endpoint of segment
        self.assertFalse(ls.is_cw(Point((5, 0))))  # Endpoint of segment

    def test_is_cw2(self):
        """
        Test corner cases for vertical segment ending at origin.

        Test tag: <tc>#tests#LineSegment.is_cw</tc>
        """
        ls = LineSegment(Point((0, -5)), Point((0, 0)))
        self.assertFalse(ls.is_cw(
            Point((0, 10))))  # At positive boundary beyond segment
        self.assertFalse(ls.is_cw(Point((0, -3))))  # On segment
        self.assertFalse(ls.is_cw(
            Point((0, -10))))  # At negative boundary beyond segment
        self.assertFalse(ls.is_cw(Point((0, -5))))  # Endpoint of segment
        self.assertFalse(ls.is_cw(Point((0, 0))))  # Endpoint of segment

    def test_is_cw3(self):
        """
        Test corner cases for non-axis-aligned segment not through origin.

        Test tag: <tc>#tests#LineSegment.is_cw</tc>
        """
        ls = LineSegment(Point((0, 1)), Point((5, 6)))
        self.assertFalse(ls.is_cw(
            Point((10, 11))))  # At positive boundary beyond segment
        self.assertFalse(ls.is_cw(Point((3, 4))))  # On segment
        self.assertFalse(ls.is_cw(
            Point((-10, -9))))  # At negative boundary beyond segment
        self.assertFalse(ls.is_cw(Point((0, 1))))  # Endpoint of segment
        self.assertFalse(ls.is_cw(Point((5, 6))))  # Endpoint of segment

    def test_get_swap1(self):
        """
        Tests corner cases.

        Test tag: <tc>#tests#LineSegment.get_swap</tc>
        """
        ls = LineSegment(Point((0, 0)), Point((10, 0)))
        swap = ls.get_swap()
        self.assertEquals(ls.p1, swap.p2)
        self.assertEquals(ls.p2, swap.p1)

        ls = LineSegment(Point((-5, 0)), Point((5, 0)))
        swap = ls.get_swap()
        self.assertEquals(ls.p1, swap.p2)
        self.assertEquals(ls.p2, swap.p1)

        ls = LineSegment(Point((0, 0)), Point((0, 0)))
        swap = ls.get_swap()
        self.assertEquals(ls.p1, swap.p2)
        self.assertEquals(ls.p2, swap.p1)

        ls = LineSegment(Point((5, 5)), Point((5, 5)))
        swap = ls.get_swap()
        self.assertEquals(ls.p1, swap.p2)
        self.assertEquals(ls.p2, swap.p1)

    def test_bounding_box(self):
        """
        Tests corner cases.

        Test tag: <tc>#tests#LineSegment.bounding_box</tc>
        """
        ls = LineSegment(Point((0, 0)), Point((0, 10)))
        self.assertEquals(ls.bounding_box.left, 0)
        self.assertEquals(ls.bounding_box.lower, 0)
        self.assertEquals(ls.bounding_box.right, 0)
        self.assertEquals(ls.bounding_box.upper, 10)

        ls = LineSegment(Point((0, 0)), Point((-3, -4)))
        self.assertEquals(ls.bounding_box.left, -3)
        self.assertEquals(ls.bounding_box.lower, -4)
        self.assertEquals(ls.bounding_box.right, 0)
        self.assertEquals(ls.bounding_box.upper, 0)

        ls = LineSegment(Point((-5, 0)), Point((3, 0)))
        self.assertEquals(ls.bounding_box.left, -5)
        self.assertEquals(ls.bounding_box.lower, 0)
        self.assertEquals(ls.bounding_box.right, 3)
        self.assertEquals(ls.bounding_box.upper, 0)

    def test_len1(self):
        """
        Tests corner cases.

        Test tag: <tc>#tests#LineSegment.len</tc>
        """
        ls = LineSegment(Point((0, 0)), Point((0, 0)))
        self.assertEquals(ls.len, 0)

        ls = LineSegment(Point((0, 0)), Point((-3, 0)))
        self.assertEquals(ls.len, 3)

    def test_line1(self):
        """
        Tests corner cases.

        Test tag: <tc>#tests#LineSegment.line</tc>
        """
        import math
        ls = LineSegment(Point((0, 0)), Point((1, 0)))
        self.assertEquals(ls.line.m, 0)
        self.assertEquals(ls.line.b, 0)

        ls = LineSegment(Point((0, 0)), Point((0, 1)))
        self.assertEquals(ls.line.m, float('inf'))
        self.assertTrue(math.isnan(ls.line.b))

        ls = LineSegment(Point((0, 0)), Point((0, -1)))
        self.assertEquals(ls.line.m, float('inf'))
        self.assertTrue(math.isnan(ls.line.b))

        ls = LineSegment(Point((0, 0)), Point((0, 0)))
        self.assertEquals(ls.line, None)

        ls = LineSegment(Point((5,0)), Point((10,0)))
        ls1 = LineSegment(Point((5,0)), Point((10,1)))
        self.assertTrue(ls.intersect(ls1))
        ls2 = LineSegment(Point((5,1)), Point((10,1)))
        self.assertFalse(ls.intersect(ls2))
        ls2 = LineSegment(Point((7,-1)), Point((7,2)))
        self.assertTrue(ls.intersect(ls2))
        




class test_Line(unittest.TestCase):

    def test___init__1(self):
        """
        Tests a variety of generic cases.

        Test tag: <tc>#tests#Line.__init__</tc>
        """
        for m, b in [(4, 0.0), (-140, 5), (0, 0)]:
            l = Line(m, b)

    def test_y1(self):
        """
        Tests a variety of generic and special cases (+-infinity).

        Test tag: <tc>#tests#Line.y</tc>
        """
        l = Line(0, 0)
        self.assertEquals(l.y(0), 0)
        self.assertEquals(l.y(-1e600), 0)
        self.assertEquals(l.y(1e600), 0)

        l = Line(1, 1)
        self.assertEquals(l.y(2), 3)
        self.assertEquals(l.y(-1e600), -1e600)
        self.assertEquals(l.y(1e600), 1e600)

        l = Line(-1, 1)
        self.assertEquals(l.y(2), -1)
        self.assertEquals(l.y(-1e600), 1e600)
        self.assertEquals(l.y(1e600), -1e600)

    def test_x1(self):
        """
        Tests a variety of generic and special cases (+-infinity).

        Test tag: <tc>#tests#Line.x</tc>
        """
        l = Line(0, 0)
        #self.assertEquals(l.x(0), 0)
        with self.assertRaises(ArithmeticError):
            l.x(0)
        with self.assertRaises(ArithmeticError):
            l.x(-1e600)
        with self.assertRaises(ArithmeticError):
            l.x(1e600)

        l = Line(1, 1)
        self.assertEquals(l.x(3), 2)
        self.assertEquals(l.x(-1e600), -1e600)
        self.assertEquals(l.x(1e600), 1e600)

        l = Line(-1, 1)
        self.assertEquals(l.x(2), -1)
        self.assertEquals(l.x(-1e600), 1e600)
        self.assertEquals(l.x(1e600), -1e600)


class test_Ray(unittest.TestCase):

    def test___init__1(self):
        """
        Tests generic cases.

        <tc>#tests#Ray.__init__</tc>
        """
        r = Ray(Point((0, 0)), Point((1, 1)))
        r = Ray(Point((8, -3)), Point((-5, 9)))


class test_Chain(unittest.TestCase):

    def test___init__1(self):
        """
        Generic testing that no exception is thrown.

        Test tag: <tc>#tests#Chain.__init__</tc>
        """
        c = Chain([Point((0, 0))])
        c = Chain([[Point((0, 0)), Point((1, 1))], [Point((2, 5))]])

    def test_vertices1(self):
        """
        Testing for repeated vertices and multiple parts.

        Test tag: <tc>#tests#Chain.vertices</tc>
        """
        vertices = [Point((0, 0)), Point((1, 1)), Point((2, 5)),
                    Point((0, 0)), Point((1, 1)), Point((2, 5))]
        self.assertEquals(Chain(vertices).vertices, vertices)

        vertices = [[Point((0, 0)), Point((1, 1)), Point((2, 5))],
                    [Point((0, 0)), Point((1, 1)), Point((2, 5))]]
        self.assertEquals(Chain(vertices).vertices, vertices[0] + vertices[1])

    def test_parts1(self):
        """
        Generic testing of parts functionality.

        Test tag: <tc>#tests#Chain.parts</tc>
        """
        vertices = [Point((0, 0)), Point((1, 1)), Point((2, 5)),
                    Point((0, 0)), Point((1, 1)), Point((2, 5))]
        self.assertEquals(Chain(vertices).parts, [vertices])

        vertices = [[Point((0, 0)), Point((1, 1)), Point((2, 5))],
                    [Point((0, 0)), Point((1, 1)), Point((2, 5))]]
        self.assertEquals(Chain(vertices).parts, vertices)

    def test_bounding_box1(self):
        """
        Test correctness with multiple parts.

        Test tag: <tc>#tests#Chain.bounding_box</tc>
        """
        vertices = [[Point((0, 0)), Point((1, 1)), Point((2, 6))],
                    [Point((-5, -5)), Point((0, 0)), Point((2, 5))]]
        bb = Chain(vertices).bounding_box
        self.assertEquals(bb.left, -5)
        self.assertEquals(bb.lower, -5)
        self.assertEquals(bb.right, 2)
        self.assertEquals(bb.upper, 6)

    def test_len1(self):
        """
        Test correctness with multiple parts and zero-length point-to-point distances.

        Test tag: <tc>#tests#Chain.len</tc>
        """
        vertices = [[Point((0, 0)), Point((1, 0)), Point((1, 5))],
                    [Point((-5, -5)), Point((-5, 0)), Point((0, 0)), Point((0, 0))]]
        self.assertEquals(Chain(vertices).len, 6 + 10)


class test_Polygon(unittest.TestCase):

    def test___init__1(self):
        """
        Test various input configurations (list vs. lists of lists, holes)

        <tc>#tests#Polygon.__init__</tc>
        """
        # Input configurations tested (in order of test):
        # one part, no holes
        # multi parts, no holes
        # one part, one hole
        # multi part, one hole
        # one part, multi holes
        # multi part, multi holes
        p = Polygon([Point(
            (0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))])
        p = Polygon(
            [[Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))],
                     [Point((30, 30)), Point((40, 30)), Point((40, 40)), Point((30, 40))]])
        p = Polygon(
            [Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))],
                    holes=[Point((2, 2)), Point((4, 2)), Point((4, 4)), Point((2, 4))])
        p = Polygon(
            [[Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))],
                     [Point(
                         (
                             30, 30)), Point(
                                 (40, 30)), Point((40, 40)), Point((30, 40))]],
                    holes=[Point((2, 2)), Point((4, 2)), Point((4, 4)), Point((2, 4))])
        p = Polygon(
            [Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))],
                    holes=[[Point(
                        (2, 2)), Point((4, 2)), Point((4, 4)), Point((2, 4))],
                        [Point((6, 6)), Point((6, 8)), Point((8, 8)), Point((8, 6))]])
        p = Polygon(
            [[Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))],
                     [Point(
                         (
                             30, 30)), Point(
                                 (40, 30)), Point((40, 40)), Point((30, 40))]],
                    holes=[[Point(
                        (2, 2)), Point((4, 2)), Point((4, 4)), Point((2, 4))],
                        [Point((6, 6)), Point((6, 8)), Point((8, 8)), Point((8, 6))]])

    def test_area1(self):
        """
        Test multiple parts.

        Test tag: <tc>#tests#Polygon.area</tc>
        """
        p = Polygon(
            [[Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))],
                     [Point((30, 30)), Point((40, 30)), Point((40, 40)), Point((30, 40))]])
        self.assertEquals(p.area, 200)

    def test_area2(self):
        """
        Test holes.

        Test tag: <tc>#tests#Polygon.area</tc>
        """
        p = Polygon(
            [Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))],
                    holes=[Point((2, 2)), Point((4, 2)), Point((4, 4)), Point((2, 4))])
        self.assertEquals(p.area, 100 - 4)

        p = Polygon(
            [Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))],
                    holes=[[Point(
                        (2, 2)), Point((4, 2)), Point((4, 4)), Point((2, 4))],
                        [Point((6, 6)), Point((6, 8)), Point((8, 8)), Point((8, 6))]])
        self.assertEquals(p.area, 100 - (4 + 4))

        p = Polygon(
            [[Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))],
                     [Point(
                         (
                             30, 30)), Point(
                                 (40, 30)), Point((40, 40)), Point((30, 40))]],
                    holes=[[Point(
                        (2, 2)), Point((4, 2)), Point((4, 4)), Point((2, 4))],
                        [Point((36, 36)), Point((36, 38)), Point((38, 38)), Point((38, 36))]])
        self.assertEquals(p.area, 200 - (4 + 4))

    def test_area4(self):
        """
        Test polygons with vertices in both orders (cw, ccw).

        Test tag: <tc>#tests#Polygon.area</tc>
        """
        p = Polygon([Point(
            (0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))])
        self.assertEquals(p.area, 100)

        p = Polygon([Point(
            (0, 0)), Point((0, 10)), Point((10, 10)), Point((10, 0))])
        self.assertEquals(p.area, 100)

    def test_bounding_box1(self):
        """
        Test polygons with multiple parts.

        Test tag: <tc>#tests#Polygon.bounding_box</tc>
        """
        p = Polygon(
            [[Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))],
                     [Point((30, 30)), Point((40, 30)), Point((40, 40)), Point((30, 40))]])
        bb = p.bounding_box
        self.assertEquals(bb.left, 0)
        self.assertEquals(bb.lower, 0)
        self.assertEquals(bb.right, 40)
        self.assertEquals(bb.upper, 40)

    def test_centroid1(self):
        """
        Test polygons with multiple parts of the same size.

        Test tag: <tc>#tests#Polygon.centroid</tc>
        """
        p = Polygon(
            [[Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))],
                     [Point((30, 30)), Point((40, 30)), Point((40, 40)), Point((30, 40))]])
        c = p.centroid
        self.assertEquals(c[0], 20)
        self.assertEquals(c[1], 20)

    def test_centroid2(self):
        """
        Test polygons with multiple parts of different size.

        Test tag: <tc>#tests#Polygon.centroid</tc>
        """
        p = Polygon(
            [[Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))],
                     [Point((30, 30)), Point((35, 30)), Point((35, 35)), Point((30, 35))]])
        c = p.centroid
        self.assertEquals(c[0], 10.5)
        self.assertEquals(c[1], 10.5)

    def test_holes1(self):
        """
        Test for correct vertex values/order.

        Test tag: <tc>#tests#Polygon.holes</tc>
        """
        p = Polygon(
            [Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))],
                    holes=[Point((2, 2)), Point((4, 2)), Point((4, 4)), Point((2, 4))])
        self.assertEquals(len(p.holes), 1)
        e_holes = [Point((2, 2)), Point((2, 4)), Point((4, 4)), Point((4, 2))]
        self.assertTrue(p.holes[0] in [e_holes, [e_holes[-1]] + e_holes[:3],
                                       e_holes[-2:] + e_holes[:2], e_holes[-3:] + [e_holes[0]]])

    def test_holes2(self):
        """
        Test for multiple holes.

        Test tag: <tc>#tests#Polygon.holes</tc>
        """
        p = Polygon(
            [Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))],
                    holes=[[Point(
                        (2, 2)), Point((4, 2)), Point((4, 4)), Point((2, 4))],
                        [Point((6, 6)), Point((6, 8)), Point((8, 8)), Point((8, 6))]])
        holes = p.holes
        self.assertEquals(len(holes), 2)

    def test_parts1(self):
        """
        Test for correct vertex values/order.

        Test tag: <tc>#tests#Polygon.parts</tc>
        """
        p = Polygon(
            [[Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))],
                     [Point((30, 30)), Point((40, 30)), Point((30, 40))]])
        self.assertEquals(len(p.parts), 2)

        part1 = [Point(
            (0, 0)), Point((0, 10)), Point((10, 10)), Point((10, 0))]
        part2 = [Point((30, 30)), Point((30, 40)), Point((40, 30))]
        if len(p.parts[0]) == 4:
            self.assertTrue(p.parts[0] in [part1, part1[-1:] + part1[:3],
                                           part1[-2:] + part1[:2], part1[-3:] + part1[:1]])
            self.assertTrue(p.parts[1] in [part2, part2[-1:] +
                                           part2[:2], part2[-2:] + part2[:1]])
        elif len(p.parts[0]) == 3:
            self.assertTrue(p.parts[0] in [part2, part2[-1:] +
                                           part2[:2], part2[-2:] + part2[:1]])
            self.assertTrue(p.parts[1] in [part1, part1[-1:] + part1[:3],
                                           part1[-2:] + part1[:2], part1[-3:] + part1[:1]])
        else:
            self.fail()

    def test_perimeter1(self):
        """
        Test with multiple parts.

        Test tag: <tc>#tests#Polygon.perimeter</tc>
        """
        p = Polygon(
            [[Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))],
                     [Point((30, 30)), Point((40, 30)), Point((40, 40)), Point((30, 40))]])
        self.assertEquals(p.perimeter, 80)

    def test_perimeter2(self):
        """
        Test with holes.

        Test tag: <tc>#tests#Polygon.perimeter</tc>
        """
        p = Polygon(
            [[Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))],
                     [Point(
                         (
                             30, 30)), Point(
                                 (40, 30)), Point((40, 40)), Point((30, 40))]],
                    holes=[[Point(
                        (2, 2)), Point((4, 2)), Point((4, 4)), Point((2, 4))],
                        [Point((6, 6)), Point((6, 8)), Point((8, 8)), Point((8, 6))]])
        self.assertEquals(p.perimeter, 80 + 16)

    def test_vertices1(self):
        """
        Test for correct values/order of vertices.

        Test tag: <tc>#tests#Polygon.vertices</tc>
        """
        p = Polygon([Point(
            (0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))])
        self.assertEquals(len(p.vertices), 4)
        e_verts = [Point(
            (0, 0)), Point((0, 10)), Point((10, 10)), Point((10, 0))]
        self.assertTrue(p.vertices in [e_verts, e_verts[-1:] + e_verts[:3],
                                       e_verts[-2:] + e_verts[:2], e_verts[-3:] + e_verts[:1]])

    def test_vertices2(self):
        """
        Test for multiple parts.

        Test tag: <tc>#tests#Polygon.vertices</tc>
        """
        p = Polygon(
            [[Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))],
                     [Point((30, 30)), Point((40, 30)), Point((40, 40)), Point((30, 40))]])
        self.assertEquals(len(p.vertices), 8)

    def test_contains_point(self):
        p = Polygon([Point((0, 0)), Point((10, 0)), Point((10, 10)), Point((0, 10))], [Point((1, 2)), Point((2, 2)), Point((2, 1)), Point((1, 1))])
        self.assertEquals(p.contains_point((0, 0)), 1)
        self.assertEquals(p.contains_point((1, 1)), 0)
        self.assertEquals(p.contains_point((2, 2)), 1)
        self.assertEquals(p.contains_point((5, 5)), 1)
        self.assertEquals(p.contains_point((10, 10)), 0)


class test_Rectangle(unittest.TestCase):

    def test___init__1(self):
        """
        Test exceptions are thrown correctly.

        Test tag: <tc>#tests#Rectangle.__init__</tc>
        """
        try:
            r = Rectangle(1, 1, -1, 5)  # right < left
        except ArithmeticError:
            pass
        else:
            self.fail()

        try:
            r = Rectangle(1, 1, 5, -1)  # upper < lower
        except ArithmeticError:
            pass
        else:
            self.fail()

    def test_set_centroid1(self):
        """
        Test with rectangles of zero width or height.

        Test tag: <tc>#tests#Rectangle.set_centroid</tc>
        """
        r = Rectangle(5, 5, 5, 10)  # Zero width
        r.set_centroid(Point((0, 0)))
        self.assertEquals(r.left, 0)
        self.assertEquals(r.lower, -2.5)
        self.assertEquals(r.right, 0)
        self.assertEquals(r.upper, 2.5)

        r = Rectangle(10, 5, 20, 5)  # Zero height
        r.set_centroid(Point((40, 40)))
        self.assertEquals(r.left, 35)
        self.assertEquals(r.lower, 40)
        self.assertEquals(r.right, 45)
        self.assertEquals(r.upper, 40)

        r = Rectangle(0, 0, 0, 0)  # Zero width and height
        r.set_centroid(Point((-4, -4)))
        self.assertEquals(r.left, -4)
        self.assertEquals(r.lower, -4)
        self.assertEquals(r.right, -4)
        self.assertEquals(r.upper, -4)

    def test_set_scale1(self):
        """
        Test repeated scaling.

        Test tag: <tc>#tests#Rectangle.set_scale</tc>
        """
        r = Rectangle(2, 2, 4, 4)

        r.set_scale(0.5)
        self.assertEquals(r.left, 2.5)
        self.assertEquals(r.lower, 2.5)
        self.assertEquals(r.right, 3.5)
        self.assertEquals(r.upper, 3.5)

        r.set_scale(2)
        self.assertEquals(r.left, 2)
        self.assertEquals(r.lower, 2)
        self.assertEquals(r.right, 4)
        self.assertEquals(r.upper, 4)

    def test_set_scale2(self):
        """
        Test scaling of rectangles with zero width/height..

        Test tag: <tc>#tests#Rectangle.set_scale</tc>
        """
        r = Rectangle(5, 5, 5, 10)  # Zero width
        r.set_scale(2)
        self.assertEquals(r.left, 5)
        self.assertEquals(r.lower, 2.5)
        self.assertEquals(r.right, 5)
        self.assertEquals(r.upper, 12.5)

        r = Rectangle(10, 5, 20, 5)  # Zero height
        r.set_scale(2)
        self.assertEquals(r.left, 5)
        self.assertEquals(r.lower, 5)
        self.assertEquals(r.right, 25)
        self.assertEquals(r.upper, 5)

        r = Rectangle(0, 0, 0, 0)  # Zero width and height
        r.set_scale(100)
        self.assertEquals(r.left, 0)
        self.assertEquals(r.lower, 0)
        self.assertEquals(r.right, 0)
        self.assertEquals(r.upper, 0)

        r = Rectangle(0, 0, 0, 0)  # Zero width and height
        r.set_scale(0.01)
        self.assertEquals(r.left, 0)
        self.assertEquals(r.lower, 0)
        self.assertEquals(r.right, 0)
        self.assertEquals(r.upper, 0)

    def test_area1(self):
        """
        Test rectangles with zero width/height

        Test tag: <tc>#tests#Rectangle.area</tc>
        """
        r = Rectangle(5, 5, 5, 10)  # Zero width
        self.assertEquals(r.area, 0)

        r = Rectangle(10, 5, 20, 5)  # Zero height
        self.assertEquals(r.area, 0)

        r = Rectangle(0, 0, 0, 0)  # Zero width and height
        self.assertEquals(r.area, 0)

    def test_height1(self):
        """
        Test rectangles with zero height.

        Test tag: <tc>#tests#Rectangle.height</tc>
        """
        r = Rectangle(10, 5, 20, 5)  # Zero height
        self.assertEquals(r.height, 0)

    def test_width1(self):
        """
        Test rectangles with zero width.

        Test tag: <tc>#tests#Rectangle.width</tc>
        """
        r = Rectangle(5, 5, 5, 10)  # Zero width
        self.assertEquals(r.width, 0)


#suite = unittest.TestSuite()
#suite.addTest(doctest.DocTestSuite('pysal.cg.shapes'))
#A = unittest.TestLoader().loadTestsFromTestCase(_TestPoint)
#B = unittest.TestLoader().loadTestsFromTestCase(_TestLineSegment)
#C = unittest.TestLoader().loadTestsFromTestCase(_TestLine)
#D = unittest.TestLoader().loadTestsFromTestCase(_TestRay)
#E = unittest.TestLoader().loadTestsFromTestCase(_TestChain)
#F = unittest.TestLoader().loadTestsFromTestCase(_TestPolygon)
#G = unittest.TestLoader().loadTestsFromTestCase(_TestRectangle)
#suite.addTests([A,B,C,D,E,D,G])
if __name__ == '__main__':
    unittest.main()
    #runner = unittest.TextTestRunner()
    #runner.run(suite)

########NEW FILE########
__FILENAME__ = test_standalone
import unittest
import numpy as np
import math

from pysal.cg.shapes import *
from pysal.cg.standalone import *


class TestBbcommon(unittest.TestCase):
    def test_bbcommon(self):
        b0 = [0, 0, 10, 10]
        b1 = [5, 5, 15, 15]
        self.assertEqual(1, bbcommon(b0, b1))

    def test_bbcommon_same(self):
        b0 = [0, 0, 10, 10]
        b1 = [0, 0, 10, 10]
        self.assertEqual(1, bbcommon(b0, b1))

    def test_bbcommon_nested(self):
        b0 = [0, 0, 10, 10]
        b1 = [1, 1, 9, 9]
        self.assertEqual(1, bbcommon(b0, b1))

    def test_bbcommon_top(self):
        b0 = [0, 0, 10, 10]
        b1 = [3, 5, 6, 15]
        self.assertEqual(1, bbcommon(b0, b1))

    def test_bbcommon_shared_edge(self):
        b0 = [0, 0, 10, 10]
        b1 = [0, 10, 10, 20]
        self.assertEqual(1, bbcommon(b0, b1))

    def test_bbcommon_shared_corner(self):
        b0 = [0, 0, 10, 10]
        b1 = [10, 10, 20, 20]
        self.assertEqual(1, bbcommon(b0, b1))

    def test_bbcommon_floats(self):
        b0 = [0.0, 0.0, 0.1, 0.1]
        b1 = [0.05, 0.05, 0.15, 0.15]
        self.assertEqual(1, bbcommon(b0, b1))


class TestGetBoundingBox(unittest.TestCase):
    def test_get_bounding_box(self):
        items = [Point((-1, 5)), Rectangle(0, 6, 11, 12)]
        expected = [-1, 5, 11, 12]
        self.assertEqual(expected, get_bounding_box(items)[:])


class TestGetAngleBetween(unittest.TestCase):
    def test_get_angle_between(self):
        ray1 = Ray(Point((0, 0)), Point((1, 0)))
        ray2 = Ray(Point((0, 0)), Point((1, 0)))
        self.assertEqual(0.0, get_angle_between(ray1, ray2))

    def test_get_angle_between_expect45(self):
        ray1 = Ray(Point((0, 0)), Point((1, 0)))
        ray2 = Ray(Point((0, 0)), Point((1, 1)))
        self.assertEqual(45.0, math.degrees(get_angle_between(ray1, ray2)))

    def test_get_angle_between_expect90(self):
        ray1 = Ray(Point((0, 0)), Point((1, 0)))
        ray2 = Ray(Point((0, 0)), Point((0, 1)))
        self.assertEqual(90.0, math.degrees(get_angle_between(ray1, ray2)))


class TestIsCollinear(unittest.TestCase):
    def test_is_collinear(self):
        self.assertEqual(True, is_collinear(Point((0, 0)), Point((
            1, 1)), Point((5, 5))))

    def test_is_collinear_expectFalse(self):
        self.assertEqual(False, is_collinear(Point((0, 0)), Point((
            1, 1)), Point((5, 0))))

    def test_is_collinear_AlongX(self):
        self.assertEqual(True, is_collinear(Point((0, 0)), Point((
            1, 0)), Point((5, 0))))

    def test_is_collinear_AlongY(self):
        self.assertEqual(True, is_collinear(
            Point((0, 0)), Point((0, 1)), Point((0, -1))))

    def test_is_collinear_smallFloat(self):
        """
        Given: p1 = (0.1, 0.2), p2 = (0.2, 0.3), p3 = (0.3, 0.4)

        Line(p1,p2):  y = mx + b
            m = (0.3-0.2) / (0.2-0.1) = .1/.1 = 1
            y - mx = b
            b = 0.3 - 1*0.2 = 0.1
            b = 0.2 - 1*0.1 = 0.1

            y = 1*x + 0.1

        Line(p2,p3): y = mx + b
            m = (0.4-0.3) / (0.3-0.2) = .1/.1 = 1
            y - mx = b
            b = 0.4 - 1*0.3 = 0.1
            b = 0.4 - 1*0.2 = 0.1

            y = 1*x + 0.1

        Line(p1,p2) == Line(p2,p3)
        Therefore p1,p2,p3 are collinear.

        Due to floating point rounding areas the standard test,
            ((p2[0]-p1[0])*(p3[1]-p1[1]) - (p2[1]-p1[1])*(p3[0]-p1[0])) == 0
        will fail.  To get around this we use an epsilon.  numpy.finfo function
        return an smallest epsilon for the given data types such that,
            (numpy.finfo(float).eps + 1.0) != 1.0

        Therefore if
            abs((p2[0]-p1[0])*(p3[1]-p1[1]) - (p2[1]-p1[1])*(
                p3[0]-p1[0])) < numpy.finfo(p1[0]).eps
        The points are collinear.
        """
        self.assertEqual(True, is_collinear(
            Point((0.1, 0.2)), Point((0.2, 0.3)), Point((0.3, 0.4))))

    def test_is_collinear_random(self):
        for i in range(10):
            a, b, c = np.random.random(3) * 10 ** (i)
            self.assertEqual(True, is_collinear(
                Point((a, a)), Point((b, b)), Point((c, c))))

    def test_is_collinear_random2(self):
        for i in range(1000):
            a, b, c = np.random.random(3)
            self.assertEqual(True, is_collinear(
                Point((a, a)), Point((b, b)), Point((c, c))))


class TestGetSegmentsIntersect(unittest.TestCase):
    def test_get_segments_intersect(self):
        seg1 = LineSegment(Point((0, 0)), Point((0, 10)))
        seg2 = LineSegment(Point((-5, 5)), Point((5, 5)))
        self.assertEqual((0.0, 5.0), get_segments_intersect(seg1, seg2)[:])

    def test_get_segments_intersect_shared_vert(self):
        seg1 = LineSegment(Point((0, 0)), Point((0, 10)))
        seg2 = LineSegment(Point((-5, 5)), Point((0, 10)))
        self.assertEqual((0.0, 10.0), get_segments_intersect(seg1, seg2)[:])

    def test_get_segments_intersect_floats(self):
        seg1 = LineSegment(Point((0, 0)), Point((0, .10)))
        seg2 = LineSegment(Point((-.5, .05)), Point((.5, .05)))
        self.assertEqual((0.0, .05), get_segments_intersect(seg1, seg2)[:])

    def test_get_segments_intersect_angles(self):
        seg1 = LineSegment(Point((0, 0)), Point((1, 1)))
        seg2 = LineSegment(Point((1, 0)), Point((0, 1)))
        self.assertEqual((0.5, 0.5), get_segments_intersect(seg1, seg2)[:])

    def test_get_segments_intersect_no_intersect(self):
        seg1 = LineSegment(Point((-5, 5)), Point((5, 5)))
        seg2 = LineSegment(Point((100, 100)), Point((100, 101)))
        self.assertEqual(None, get_segments_intersect(seg1, seg2))

    def test_get_segments_intersect_overlap(self):
        seg1 = LineSegment(Point((0.1, 0.1)), Point((0.6, 0.6)))
        seg2 = LineSegment(Point((0.3, 0.3)), Point((0.9, 0.9)))
        expected = LineSegment(Point((0.3, 0.3)), Point((0.6, 0.6)))
        self.assertEqual(expected, get_segments_intersect(seg1, seg2))

    def test_get_segments_intersect_same(self):
        seg1 = LineSegment(Point((-5, 5)), Point((5, 5)))
        self.assertEqual(seg1, get_segments_intersect(seg1, seg1))

    def test_get_segments_intersect_nested(self):
        seg1 = LineSegment(Point((0.1, 0.1)), Point((0.9, 0.9)))
        seg2 = LineSegment(Point((0.3, 0.3)), Point((0.6, 0.6)))
        self.assertEqual(seg2, get_segments_intersect(seg1, seg2))


class TestGetSegmentPointIntersect(unittest.TestCase):
    def test_get_segment_point_intersect(self):
        seg = LineSegment(Point((0, 0)), Point((0, 10)))
        pt = Point((0, 5))
        self.assertEqual(pt, get_segment_point_intersect(seg, pt))

    def test_get_segment_point_intersect_left_end(self):
        seg = LineSegment(Point((0, 0)), Point((0, 10)))
        pt = seg.p1
        self.assertEqual(pt, get_segment_point_intersect(seg, pt))

    def test_get_segment_point_intersect_right_end(self):
        seg = LineSegment(Point((0, 0)), Point((0, 10)))
        pt = seg.p2
        self.assertEqual(pt, get_segment_point_intersect(seg, pt))

    def test_get_segment_point_intersect_angle(self):
        seg = LineSegment(Point((0, 0)), Point((1, 1)))
        pt = Point((.1, .1))
        self.assertEqual(pt, get_segment_point_intersect(seg, pt))

    def test_get_segment_point_intersect_no_intersect(self):
        seg = LineSegment(Point((0, 0)), Point((0, 10)))
        pt = Point((5, 5))
        self.assertEqual(None, get_segment_point_intersect(seg, pt))

    def test_get_segment_point_intersect_no_intersect_collinear(self):
        seg = LineSegment(Point((0, 0)), Point((0, 10)))
        pt = Point((0, 20))
        self.assertEqual(None, get_segment_point_intersect(seg, pt))

    def test_get_segment_point_intersect_floats(self):
        seg = LineSegment(Point((0.3, 0.3)), Point((.9, .9)))
        pt = Point((.5, .5))
        self.assertEqual(pt, get_segment_point_intersect(seg, pt))

    def test_get_segment_point_intersect_floats(self):
        seg = LineSegment(Point((0.0, 0.0)), Point((
            2.7071067811865475, 2.7071067811865475)))
        pt = Point((1.0, 1.0))
        self.assertEqual(pt, get_segment_point_intersect(seg, pt))

    def test_get_segment_point_intersect_floats_no_intersect(self):
        seg = LineSegment(Point((0.3, 0.3)), Point((.9, .9)))
        pt = Point((.1, .1))
        self.assertEqual(None, get_segment_point_intersect(seg, pt))


class TestGetPolygonPointIntersect(unittest.TestCase):
    def test_get_polygon_point_intersect(self):
        poly = Polygon([Point(
            (0, 0)), Point((1, 0)), Point((1, 1)), Point((0, 1))])
        pt = Point((0.5, 0.5))
        self.assertEqual(pt, get_polygon_point_intersect(poly, pt))

    def test_get_polygon_point_intersect_on_edge(self):
        poly = Polygon([Point(
            (0, 0)), Point((1, 0)), Point((1, 1)), Point((0, 1))])
        pt = Point((1.0, 0.5))
        self.assertEqual(pt, get_polygon_point_intersect(poly, pt))

    def test_get_polygon_point_intersect_on_vertex(self):
        poly = Polygon([Point(
            (0, 0)), Point((1, 0)), Point((1, 1)), Point((0, 1))])
        pt = Point((1.0, 1.0))
        self.assertEqual(pt, get_polygon_point_intersect(poly, pt))

    def test_get_polygon_point_intersect_outside(self):
        poly = Polygon([Point(
            (0, 0)), Point((1, 0)), Point((1, 1)), Point((0, 1))])
        pt = Point((2.0, 2.0))
        self.assertEqual(None, get_polygon_point_intersect(poly, pt))


class TestGetRectanglePointIntersect(unittest.TestCase):
    def test_get_rectangle_point_intersect(self):
        rect = Rectangle(0, 0, 5, 5)
        pt = Point((1, 1))
        self.assertEqual(pt, get_rectangle_point_intersect(rect, pt))

    def test_get_rectangle_point_intersect_on_edge(self):
        rect = Rectangle(0, 0, 5, 5)
        pt = Point((2.5, 5))
        self.assertEqual(pt, get_rectangle_point_intersect(rect, pt))

    def test_get_rectangle_point_intersect_on_vertex(self):
        rect = Rectangle(0, 0, 5, 5)
        pt = Point((5, 5))
        self.assertEqual(pt, get_rectangle_point_intersect(rect, pt))

    def test_get_rectangle_point_intersect_outside(self):
        rect = Rectangle(0, 0, 5, 5)
        pt = Point((10, 10))
        self.assertEqual(None, get_rectangle_point_intersect(rect, pt))


class TestGetRaySegmentIntersect(unittest.TestCase):
    def test_get_ray_segment_intersect(self):
        ray = Ray(Point((0, 0)), Point((0, 1)))
        seg = LineSegment(Point((-1, 10)), Point((1, 10)))
        self.assertEqual((0.0, 10.), get_ray_segment_intersect(ray, seg)[:])

    def test_get_ray_segment_intersect_orgin(self):
        ray = Ray(Point((0, 0)), Point((0, 1)))
        seg = LineSegment(Point((-1, 0)), Point((1, 0)))
        self.assertEqual((0.0, 0.0), get_ray_segment_intersect(ray, seg)[:])

    def test_get_ray_segment_intersect_edge(self):
        ray = Ray(Point((0, 0)), Point((0, 1)))
        seg = LineSegment(Point((0, 2)), Point((2, 2)))
        self.assertEqual((0.0, 2.0), get_ray_segment_intersect(ray, seg)[:])

    def test_get_ray_segment_intersect_no_intersect(self):
        ray = Ray(Point((0, 0)), Point((0, 1)))
        seg = LineSegment(Point((10, 10)), Point((10, 11)))
        self.assertEqual(None, get_ray_segment_intersect(ray, seg))

    def test_get_ray_segment_intersect_segment(self):
        ray = Ray(Point((0, 0)), Point((5, 5)))
        seg = LineSegment(Point((1, 1)), Point((2, 2)))
        self.assertEqual(seg, get_ray_segment_intersect(ray, seg))


class TestGetRectangleRectangleIntersection(unittest.TestCase):
    def test_get_rectangle_rectangle_intersection_leftright(self):
        r0 = Rectangle(0, 4, 6, 9)
        r1 = Rectangle(4, 0, 9, 7)
        expected = [4.0, 4.0, 6.0, 7.0]
        self.assertEqual(
            expected, get_rectangle_rectangle_intersection(r0, r1)[:])

    def test_get_rectangle_rectangle_intersection_topbottom(self):
        r0 = Rectangle(0, 0, 4, 4)
        r1 = Rectangle(2, 1, 6, 3)
        expected = [2.0, 1.0, 4.0, 3.0]
        self.assertEqual(
            expected, get_rectangle_rectangle_intersection(r0, r1)[:])

    def test_get_rectangle_rectangle_intersection_nested(self):
        r0 = Rectangle(0, 0, 4, 4)
        r1 = Rectangle(2, 1, 3, 2)
        self.assertEqual(r1, get_rectangle_rectangle_intersection(r0, r1))

    def test_get_rectangle_rectangle_intersection_shared_corner(self):
        r0 = Rectangle(0, 0, 4, 4)
        r1 = Rectangle(4, 4, 8, 8)
        self.assertEqual(Point(
            (4, 4)), get_rectangle_rectangle_intersection(r0, r1))

    def test_get_rectangle_rectangle_intersection_shared_edge(self):
        r0 = Rectangle(0, 0, 4, 4)
        r1 = Rectangle(0, 4, 4, 8)
        self.assertEqual(LineSegment(Point((0, 4)), Point(
            (4, 4))), get_rectangle_rectangle_intersection(r0, r1))

    def test_get_rectangle_rectangle_intersection_shifted_edge(self):
        r0 = Rectangle(0, 0, 4, 4)
        r1 = Rectangle(2, 4, 6, 8)
        self.assertEqual(LineSegment(Point((2, 4)), Point(
            (4, 4))), get_rectangle_rectangle_intersection(r0, r1))

    def test_get_rectangle_rectangle_intersection_no_intersect(self):
        r0 = Rectangle(0, 0, 4, 4)
        r1 = Rectangle(5, 5, 8, 8)
        self.assertEqual(None, get_rectangle_rectangle_intersection(r0, r1))


class TestGetPolygonPointDist(unittest.TestCase):
    def test_get_polygon_point_dist(self):
        poly = Polygon([Point(
            (0, 0)), Point((1, 0)), Point((1, 1)), Point((0, 1))])
        pt = Point((2, 0.5))
        expected = 1.0
        self.assertEqual(expected, get_polygon_point_dist(poly, pt))

    def test_get_polygon_point_dist_inside(self):
        poly = Polygon([Point(
            (0, 0)), Point((1, 0)), Point((1, 1)), Point((0, 1))])
        pt = Point((0.5, 0.5))
        expected = 0.0
        self.assertEqual(expected, get_polygon_point_dist(poly, pt))

    def test_get_polygon_point_dist_on_vertex(self):
        poly = Polygon([Point(
            (0, 0)), Point((1, 0)), Point((1, 1)), Point((0, 1))])
        pt = Point((1.0, 1.0))
        expected = 0.0
        self.assertEqual(expected, get_polygon_point_dist(poly, pt))

    def test_get_polygon_point_dist_on_edge(self):
        poly = Polygon([Point(
            (0, 0)), Point((1, 0)), Point((1, 1)), Point((0, 1))])
        pt = Point((0.5, 1.0))
        expected = 0.0
        self.assertEqual(expected, get_polygon_point_dist(poly, pt))


class TestGetPointsDist(unittest.TestCase):
    def test_get_points_dist(self):
        pt1 = Point((0.5, 0.5))
        pt2 = Point((0.5, 0.5))
        self.assertEqual(0, get_points_dist(pt1, pt2))

    def test_get_points_dist_diag(self):
        pt1 = Point((0, 0))
        pt2 = Point((1, 1))
        self.assertEqual(2 ** (0.5), get_points_dist(pt1, pt2))

    def test_get_points_dist_alongX(self):
        pt1 = Point((-1000, 1 / 3.0))
        pt2 = Point((1000, 1 / 3.0))
        self.assertEqual(2000, get_points_dist(pt1, pt2))

    def test_get_points_dist_alongY(self):
        pt1 = Point((1 / 3.0, -500))
        pt2 = Point((1 / 3.0, 500))
        self.assertEqual(1000, get_points_dist(pt1, pt2))


class TestGetSegmentPointDist(unittest.TestCase):
    def test_get_segment_point_dist(self):
        seg = LineSegment(Point((0, 0)), Point((10, 0)))
        pt = Point((5, 5))
        self.assertEqual((5.0, 0.5), get_segment_point_dist(seg, pt))

    def test_get_segment_point_dist_on_endPoint(self):
        seg = LineSegment(Point((0, 0)), Point((10, 0)))
        pt = Point((0, 0))
        self.assertEqual((0.0, 0.0), get_segment_point_dist(seg, pt))

    def test_get_segment_point_dist_on_middle(self):
        seg = LineSegment(Point((0, 0)), Point((10, 0)))
        pt = Point((5, 0))
        self.assertEqual((0.0, 0.5), get_segment_point_dist(seg, pt))

    def test_get_segment_point_diag(self):
        seg = LineSegment(Point((0, 0)), Point((10, 10)))
        pt = Point((5, 5))
        self.assertAlmostEqual(0.0, get_segment_point_dist(seg, pt)[0])
        self.assertAlmostEqual(0.5, get_segment_point_dist(seg, pt)[1])

    def test_get_segment_point_diag_with_dist(self):
        seg = LineSegment(Point((0, 0)), Point((10, 10)))
        pt = Point((0, 10))
        self.assertAlmostEqual(50 ** (0.5), get_segment_point_dist(seg, pt)[0])
        self.assertAlmostEqual(0.5, get_segment_point_dist(seg, pt)[1])


class TestGetPointAtAngleAndDist(unittest.TestCase):
    def test_get_point_at_angle_and_dist(self):
        ray = Ray(Point((0, 0)), Point((1, 0)))
        pt = get_point_at_angle_and_dist(ray, math.pi, 1.0)
        self.assertAlmostEqual(-1.0, pt[0])
        self.assertAlmostEqual(0.0, pt[1])

    def test_get_point_at_angle_and_dist_diag(self):
        ray = Ray(Point((0, 0)), Point((1, 1)))
        pt = get_point_at_angle_and_dist(ray, math.pi, 2 ** (0.5))
        self.assertAlmostEqual(-1.0, pt[0])
        self.assertAlmostEqual(-1.0, pt[1])

    def test_get_point_at_angle_and_dist_diag_90(self):
        ray = Ray(Point((0, 0)), Point((1, 1)))
        pt = get_point_at_angle_and_dist(ray, -math.pi / 2.0, 2 ** (0.5))
        self.assertAlmostEqual(1.0, pt[0])
        self.assertAlmostEqual(-1.0, pt[1])

    def test_get_point_at_angle_and_dist_diag_45(self):
        ray = Ray(Point((0, 0)), Point((1, 1)))
        pt = get_point_at_angle_and_dist(ray, -math.pi / 4.0, 1)
        self.assertAlmostEqual(1.0, pt[0])
        self.assertAlmostEqual(0.0, pt[1])


class TestConvexHull(unittest.TestCase):
    def test_convex_hull(self):
        points = [Point((0, 0)), Point((4, 4)), Point((4, 0)), Point((3, 1))]
        self.assertEqual([Point((0.0, 0.0)), Point(
            (4.0, 0.0)), Point((4.0, 4.0))], convex_hull(points))


class TestIsClockwise(unittest.TestCase):
    def test_is_clockwise(self):
        vertices = [Point((0, 0)), Point((0, 10)), Point((10, 0))]
        self.assertEqual(True, is_clockwise(vertices))

    def test_is_clockwise_expect_false(self):
        vertices = [Point((0, 0)), Point((10, 0)), Point((0, 10))]
        self.assertEqual(False, is_clockwise(vertices))

    def test_is_clockwise_big(self):
        vertices = [(
            -106.57798, 35.174143999999998), (-106.583412, 35.174141999999996),
                    (-106.58417999999999, 35.174143000000001), (-106.58377999999999, 35.175542999999998),
                    (-106.58287999999999, 35.180543), (
                        -106.58263099999999, 35.181455),
                    (-106.58257999999999, 35.181643000000001), (-106.58198299999999, 35.184615000000001),
                    (-106.58148, 35.187242999999995), (
                        -106.58127999999999, 35.188243),
                    (-106.58138, 35.188243), (-106.58108, 35.189442999999997),
                    (-106.58104, 35.189644000000001), (
                        -106.58028, 35.193442999999995),
                    (-106.580029, 35.194541000000001), (-106.57974399999999,
                                                        35.195785999999998),
                    (-106.579475, 35.196961999999999), (-106.57922699999999,
                                                        35.198042999999998),
                    (-106.578397, 35.201665999999996), (-106.57827999999999,
                                                        35.201642999999997),
                    (-106.57737999999999, 35.201642999999997), (-106.57697999999999, 35.201543000000001),
                    (-106.56436599999999, 35.200311999999997), (
                        -106.56058, 35.199942999999998),
                    (-106.56048, 35.197342999999996), (
                        -106.56048, 35.195842999999996),
                    (-106.56048, 35.194342999999996), (
                        -106.56048, 35.193142999999999),
                    (-106.56048, 35.191873999999999), (
                        -106.56048, 35.191742999999995),
                    (-106.56048, 35.190242999999995), (-106.56037999999999,
                                                       35.188642999999999),
                    (-106.56037999999999, 35.187242999999995), (-106.56037999999999, 35.186842999999996),
                    (-106.56037999999999, 35.186552999999996), (-106.56037999999999, 35.185842999999998),
                    (-106.56037999999999, 35.184443000000002), (-106.56037999999999, 35.182943000000002),
                    (-106.56037999999999, 35.181342999999998), (-106.56037999999999, 35.180433000000001),
                    (-106.56037999999999, 35.179943000000002), (-106.56037999999999, 35.178542999999998),
                    (-106.56037999999999, 35.177790999999999), (-106.56037999999999, 35.177143999999998),
                    (-106.56037999999999, 35.175643999999998), (-106.56037999999999, 35.174444000000001),
                    (-106.56037999999999, 35.174043999999995), (
                        -106.560526, 35.174043999999995),
                    (-106.56478, 35.174043999999995), (-106.56627999999999,
                                                       35.174143999999998),
                    (-106.566541, 35.174144999999996), (
                        -106.569023, 35.174157000000001),
                    (-106.56917199999999, 35.174157999999998), (
                        -106.56938, 35.174143999999998),
                    (-106.57061499999999, 35.174143999999998), (-106.57097999999999, 35.174143999999998),
                    (-106.57679999999999, 35.174143999999998), (-106.57798, 35.174143999999998)]
        self.assertEqual(True, is_clockwise(vertices))


class TestPointTouchesRectangle(unittest.TestCase):
    def test_point_touches_rectangle_inside(self):
        rect = Rectangle(0, 0, 10, 10)
        point = Point((5, 5))
        self.assertEqual(True, point_touches_rectangle(point, rect))

    def test_point_touches_rectangle_on_edge(self):
        rect = Rectangle(0, 0, 10, 10)
        point = Point((10, 5))
        self.assertEqual(True, point_touches_rectangle(point, rect))

    def test_point_touches_rectangle_on_corner(self):
        rect = Rectangle(0, 0, 10, 10)
        point = Point((10, 10))
        self.assertEqual(True, point_touches_rectangle(point, rect))

    def test_point_touches_rectangle_outside(self):
        rect = Rectangle(0, 0, 10, 10)
        point = Point((11, 11))
        self.assertEqual(False, point_touches_rectangle(point, rect))


class TestGetSharedSegments(unittest.TestCase):
    def test_get_shared_segments(self):
        poly1 = Polygon([Point(
            (0, 0)), Point((0, 1)), Point((1, 1)), Point((1, 0))])
        poly2 = Polygon([Point(
            (1, 0)), Point((1, 1)), Point((2, 1)), Point((2, 0))])
        poly3 = Polygon([Point(
            (0, 1)), Point((0, 2)), Point((1, 2)), Point((1, 1))])
        poly4 = Polygon([Point(
            (1, 1)), Point((1, 2)), Point((2, 2)), Point((2, 1))])
        self.assertEqual(
            True, get_shared_segments(poly1, poly2, bool_ret=True))
        self.assertEqual(
            True, get_shared_segments(poly1, poly3, bool_ret=True))
        self.assertEqual(
            True, get_shared_segments(poly3, poly4, bool_ret=True))
        self.assertEqual(
            True, get_shared_segments(poly4, poly2, bool_ret=True))

        self.assertEqual(
            False, get_shared_segments(poly1, poly4, bool_ret=True))
        self.assertEqual(
            False, get_shared_segments(poly3, poly2, bool_ret=True))

    def test_get_shared_segments_non_bool(self):
        poly1 = Polygon([Point(
            (0, 0)), Point((0, 1)), Point((1, 1)), Point((1, 0))])
        poly2 = Polygon([Point(
            (1, 0)), Point((1, 1)), Point((2, 1)), Point((2, 0))])
        poly3 = Polygon([Point(
            (0, 1)), Point((0, 2)), Point((1, 2)), Point((1, 1))])
        poly4 = Polygon([Point(
            (1, 1)), Point((1, 2)), Point((2, 2)), Point((2, 1))])
        self.assertEqual(LineSegment(Point((1, 0)), Point((1, 1))),
                         get_shared_segments(poly1, poly2)[0])
        self.assertEqual(LineSegment(Point((0, 1)), Point((1, 1))),
                         get_shared_segments(poly1, poly3)[0])
        self.assertEqual(LineSegment(Point((1, 2)), Point((1, 1))),
                         get_shared_segments(poly3, poly4)[0])
        self.assertEqual(LineSegment(Point((2, 1)), Point((1, 1))),
                         get_shared_segments(poly4, poly2)[0])
        #expected =  [LineSegment(Point((1, 1)), Point((1, 0)))]
        #assert expected == get_shared_segments(poly1, poly3)
        #expected =  [LineSegment(Point((1, 1)), Point((1, 0)))]
        #assert expected == get_shared_segments(poly3, poly4)
        #expected =  [LineSegment(Point((1, 1)), Point((1, 0)))]
        #assert expected == get_shared_segments(poly4, poly2)


class TestDistanceMatrix(unittest.TestCase):
    def test_distance_matrix(self):
        points = [(10, 10), (20, 10), (40, 10), (15, 20), (30, 20), (30, 30)]
        dist = distance_matrix(np.array(points), 2)
        for i in range(0, len(points)):
            for j in range(i, len(points)):
                x, y = points[i]
                X, Y = points[j]
                d = ((x - X) ** 2 + (y - Y) ** 2) ** (0.5)
                self.assertEqual(dist[i, j], d)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = common

# external imports

try:
    import numpy as np
    import numpy.linalg as la
except:
    print 'numpy 1.3 is required'
    raise
try:
    import scipy as sp
    import scipy.stats as stats
    from cg.kdtree import KDTree
    from scipy.spatial.distance import pdist, cdist
except:
    print 'scipy 0.7+ is required'
    raise


import copy
import math
import random
import sys
import time
import unittest

########NEW FILE########
__FILENAME__ = PGDump
#!/usr/bin/python
__author__ = "Philip Stephens <philip.stephens@asu.edu "

__all__ = ['db2shape', 'db_table2gal']

from django.contrib.gis.gdal.geomtype import OGRGeomType
from osgeo import ogr
import pysal
import os

def db2shape(connstring, input, output):
    """
    dumps a postgis database table to shapefile 

    Arguments
    ---------
    connstring = A connection string with 4 parameters. Example is 
    "PG: host='localhost' dbname='pysaldb' user='myusername'
    password='my_password'"

    input : the db table

    output : a filename

    Examples
    --------
    TBD 
    TODO: make pysal db tables read-only and available on network

    Note
    ----

    If a file exists with the same name as 'output', it will be deleted
    before being overwritten.
    """
    conn = ogr.Open(connstring)
    layer = conn.GetLayerByName(input)
    type = layer.GetGeomType()  # returns an int
    geom_type = OGRGeomType._types[type]  # map int to string description

    # Schema definition of SHP file
    out_driver = ogr.GetDriverByName( 'ESRI Shapefile' )
    if os.path.exists(output):
        out_driver.DeleteDataSource(output)
    out_ds = out_driver.CreateDataSource(output)
    out_srs = None
    out_layer = out_ds.CreateLayer(geom_type, out_srs, type)
    fd = ogr.FieldDefn('name',ogr.OFTString)
    out_layer.CreateField(fd)

    #layer = conn.ExecuteSQL(sql)

    feat = layer.GetNextFeature()
    while feat is not None:
        featDef = ogr.Feature(out_layer.GetLayerDefn())
        featDef.SetGeometry(feat.GetGeometryRef())
        #featDef.SetField('name',feat.TITLE)
        out_layer.CreateFeature(featDef)
        feat.Destroy()
        feat = layer.GetNextFeature()
    conn.Destroy()
    out_ds.Destroy()

def db_table2gal(connstring, input, weights_type=Contiguity):
    """
    generates a GAL file from a postgis database table 

    Arguments
    ---------
    connstring = A connection string with 4 parameters. Example is 
    "PG: host='localhost' dbname='pysaldb' user='myusername'
    password='my_password'"

    input : the db table

    weights_type : type of spatial weights calculation, defaults to
    Contiguity

    Examples
    --------
    TBD 

    Note
    ----
    TBD

    pseudo
    ------
    connect to db
    simplify
    create topo
    query topogeom
    return aswkt or asgeojson
    convert returned to pysal w 
    write w to GAL


    """
    pass


if __name__ == '__main__':
    import nose
    nose.runmodule()

########NEW FILE########
__FILENAME__ = access
"""
A library of spatial network accessibility functions.
Not to be used without permission.

Contact: 

Andrew Winslow
GeoDa Center for Geospatial Analysis
Arizona State University
Tempe, AZ
Andrew.Winslow@asu.edu
"""

import math
import unittest

import test


def coverage(dists, bandwidth):
    """
    Takes a list of numeric distances and a numeric bandwidth and returns the 
    number of distances less than or equal to the bandwidth.
    """
    return len(filter(lambda d: d <= bandwidth, dists))

def equity(dists):
    """
    Takes a list of numeric distances and returns the smallest of them.
    """
    return min(dists)

def potential_entropy(dists, power=1):
    """
    Takes a list of numeric distances and returns the sum of the values
    of a function of a distances. The function is e^(-power*distance).  
    """
    return sum([math.e**(-power*d) for d in dists])

def potential_gravity(dists, power=2):
    """
    Takes a list of numeric distances and returns the sum of the values
    of a function of a distances. The function is 1/(d^power).
    """
    return sum([1.0/(d**power) for d in filter(lambda d: d > 0, dists)])

def travel_cost(dists):
    """
    Takes a list of distances and compute the sum. 
    """
    return sum(dists)
   
    




########NEW FILE########
__FILENAME__ = kernel
"""
A library of spatial network kernel density functions.
Not to be used without permission.

Contact: 

Andrew Winslow
GeoDa Center for Geospatial Analysis
Arizona State University
Tempe, AZ
Andrew.Winslow@asu.edu
"""

import operator
import unittest
import test
import priordict as priordict
import network as pynet
from math import exp, sqrt, pi
import time

def triangular(z):
    return 1 - abs(z)

def uniform(z):
    return abs(z)

def quadratic(z):
    return 0.75*(1 - z*z)

def quartic(z):
    return (3.0/pi)*(1-z*z)*(1-z*z)
    #return (15*1.0/16)*(1-z*z)*(1-z*z)

def gaussian(z):
    return sqrt(2*pi)*exp(-0.5*z*z)

def dijkstras_w_prev(G, start, r=1e600):
    D = {}  # dictionary of final distances
    P = {}  # dictionary of previous nodes
    Q = priordict.PriorityDictionary()   # est.dist. of non-final vert.
    Q[start] = 0
    P[start] = None
    for v in Q:
        D[v] = Q[v]
        if v == None or D[v] > r:
            break
        for w in G[v]:
            vwLength = D[v] + G[v][w]
            if w in D:
                pass
            elif w not in Q or vwLength < Q[w]:
                Q[w] = vwLength
                P[w] = v
    return (D, P)

def kernel_density(network, events, bandwidth, orig_nodes, kernel='quadratic'):
    """
    This function estimates Kernel densities on a planar undirected network. 
    It implements the equal-split discontinuous Kernel function developed by Okabe et al. (2009).
    Particularly, it computes Kernel densities by using equation 19 and 20 
    in the paper of Okabe et al. (2009). 

    Parameters
    ----------
    network: A dictionary of dictionaries like {n1:{n2:d12,...},...}
             A planar undirected network
             It is assumed that this network is divided by a certain cell size 
             and is restructured to incorporate the new nodes resulting from the division as well as 
             events. Therefore, nodes in the network can be classified into three groups:
             i) original nodes, 2) event points, and 3) cell points.  
    events: a list of tuples
            a tuple is the network-projected coordinate of an event
            that takes the form of (x,y)
    bandwidth: a float
            Kernel bandwidth
    orig_nodes: a list of tuples
            a tuple is the coordinate of a node that is part of the original base network
            each tuple takes the form of (x,y)
    kernel: string
            the type of Kernel function
            allowed values: 'quadratic', 'gaussian', 'quartic', 'uniform', 'triangular'

    Returns
    -------   
    A dictioinary where keys are node and values are their densities
    Example: {n1:d1,n2:d2,...}

    <tc>#is#kernel_density</tc>
    """

    # beginning of step i
    density = {}
    for n in network:
        density[n] = []
    # end of step i

    # beginning of step ii
    def compute_split_multiplier(prev_D, n):
        '''
        computes the demoninator of the formula 19

        Parameters
        ----------
        prev_D: a dictionary storing pathes from n to other nodes in the network
                its form is like: {n1:prev_node_of_n1(=n2), n2:prev_node_of_n2(=n3),...}
        n: a tuple containing the geographic coordinate of a starting point 
           its form is like: (x,y)

        Returns
        -------
        An integer

        '''
        split_multiplier = 1 
        p = prev_D[n] 
        while p != None:
            if len(network[p]) > 1:
                split_multiplier *= (len(network[p]) - 1)
            if p not in prev_D: 
                p = None
            else:
                p = prev_D[p]
        return split_multiplier
    # end of step ii

    kernel_funcs = {'triangular':triangular, 'uniform': uniform, 
                    'quadratic': quadratic, 'quartic':quartic, 'gaussian':gaussian}
    #t1 = time.time()
    # beginning of step iii
    kernel_func = kernel_funcs[kernel]
    for e in events:
        # beginning of step a
        src_D = pynet.dijkstras(network, e, bandwidth, True)
        # end of step a
        # beginning of step b
        density[e].append(kernel_func(0))
        # end of step b
        # beginning of step c
        for n in src_D[0]: # src_D[0] - a dictionary of nodes whose distance from e is smaller than e 
            if src_D[0][n] == 0: continue
            # src_D[1] - a dictionary from which a path from e to n can be traced
            d = src_D[0][n]
            if d <= bandwidth:
                n_degree = 2.0
                if n in events and n in orig_nodes and len(network[n]) > 0:
                    n_degree = len(network[n])
                unsplit_density = kernel_func(d*1.0/bandwidth*1.0) 
                # src_D[1] - a dictionary from which a path from e to n can be traced
                split_multiplier = compute_split_multiplier(src_D[1], n) 
                density[n].append((1.0/split_multiplier)*(2.0/n_degree)*unsplit_density)
                #if str(n[0]) == '724900.335127' and str(n[1]) == '872127.948935':
                #    print 'event', e
                #    print 'distance', d
                #    print 'unsplit_density', unsplit_density
                #    print 'n_degree', n_degree
                #    print 'split_multiplier', split_multiplier
                #    print 'density', (1.0/split_multiplier)*(2.0/n_degree)*unsplit_density
        # end of step c

    # beginning of step iv 
    #t1 = time.time()
    no_events = len(events)
    for node in density:
        if len(density[node]) > 0:
            #if str(node[0]) == '724900.335127' and str(node[1]) == '872127.948935':
            #    print density[node]
            density[node] = sum(density[node])/no_events
            #density[node] = sum(density[node])*1.0/len(density[node])
        else:
            density[node] = 0.0
    # end of step iv

    #for node in events:
    #    del density[node]

    #print 'normalizing density: %s' % (str(time.time() - t1))

    return density

    




########NEW FILE########
__FILENAME__ = kfuncs
"""
A library of spatial network k-function functions.
Not to be used without permission.

Contact: 

Andrew Winslow
GeoDa Center for Geospatial Analysis
Arizona State University
Tempe, AZ
Andrew.Winslow@asu.edu
"""

import unittest
import test 

def _fxrange(start, end, incr):
    """
    A float version of the xrange() built-in function.

    _fxrange(number, number, number) -> iterator

    Arguments:
    start -- the lower end of the range (inclusive)
    end -- the upper end of the range (exclusive)
    incr -- the step size. must be positive.
    """
    i = 0
    while True:
        t = start + i*incr
        if t >= end:
            break
        yield t
        i += 1    

def _binary_search(list, q):
    """
    Returns the index in a list where an item should be found.
     
    Arguments:
    list -- a list of items
    q -- a value to be searched for
    """
    l = 0
    r = len(list)
    while l < r:
        m = (l + r)/2
        if list[m] > q:
            r = m
        else:
            l = m + 1
    return l

def kt_values(t_specs, distances, scaling_const):
    """
    Returns a dictionary of t numerics to k(t) numerics.

    kt_values(number list, number list, number) -> number to number dictionary

    Arguments:
    t_specs -- a 3-tuple of (t_min, t_max, t_delta) specifying the t-values to compute k(t) for
    distances -- a list of distances to compute k(t) from
    scaling_const -- a constant to multiple k(t) by for each t 
    """
    ks = {}
    distances.sort()
    if type(t_specs) == tuple:
        t_specs = [t for t in _fxrange(t_specs[0], t_specs[1], t_specs[2])]
        
    for t in t_specs:
        ks[t] = scaling_const*_binary_search(distances, t)
    return ks     




########NEW FILE########
__FILENAME__ = klincs
#!/usr/env python

"""
A library for computing local K function for network-constrained data

Author:
Andrew Winslow Andrew.Winslow@asu.edu
Myunghwa Hwang mhwang4@gmail.com

"""
import unittest
import numpy as np
import geodanet.network as pynet
import geodanet.kfuncs as pykfuncs
import geodanet.simulator as pysim
import time
import random
import platform                                                                                                                 
try:
    if platform.system() == 'Darwin':
        import multiprocessing
    else:
        multiprocessing = None
except ImportError:
    multiprocessing = None 

class WeightedRandomSampleGenerator(object):
    """
    A generator for randomly sampling n elements from 
    a population group with consideration to a given set of weights
    """

    def __init__(self, weights, population, n):
	"""
	weights: an iterable with m numeric elements
	population: a numpy array with m elements
	n: an integer representing sample size
	"""
        self.totals = np.cumsum(weights)
        self.population = population
        self.n = n
        self.norm = self.totals[-1]

    def next(self):
        sample = []
        for i in xrange(self.n):
            throw = np.random.rand()*self.norm
            sample.append(self.population[np.searchsorted(self.totals, throw)])
        return sample

    def __call__(self):
        return self.next()

class RandomSampleGenerator(object):
    """
    A generator for randomly sampling n elements 
    from a population group
    """
    def __init__(self, population, n):
	"""
	population: a numpy array with m elements
	n: an integer representing sample size
	"""
        self.population = population
        self.n = n

    def next(self):
        return random.sample(self.population, self.n)

    def __call__(self):
        return self.next()

def local_k(network, events, refs, scale_set, cache=None):
    """
    Computes local K function

    network: an undirected network data to which reference points are injected
    refs: a set of reference points on the given network
          points unprojected into the network
    events: a set of event points on the given network
            points projected into the network
    scale_set: a tuple defining spatial scales to be examined
               (min, max, interval)
    """

    node2localK = {}
    net_distances = {}
    if cache: net_distances = cache
    for node in refs:
        node = node[1][0]
        a_dest = network[node].keys()[0]
        node_proj = (node, a_dest, 0, network[node][a_dest])
        if node not in net_distances:
            net_distances[node] = pynet.dijkstras(network, node, scale_set[1])
        if a_dest not in net_distances:
            net_distances[a_dest] = pynet.dijkstras(network, node, scale_set[1])
        distances = pynet.proj_distances_undirected(network, node_proj, events, scale_set[1], cache=net_distances).values()
        node2localK[node] = pykfuncs.kt_values(scale_set, distances, 1)
    return node2localK, net_distances

def cluster_type(obs, lower, upper):
    if obs < lower: return -1
    if obs > upper: return 1
    return 0

def simulate_local_k_01(args):
    sims = args[0]
    n = args[1]
    net_file = args[2]
    network = args[3]
    events = args[4]
    refs = args[5]
    scale_set = args[6]
    cache = args[7]

    #print 'simulated_local_k_01'
    simulator = pysim.Simulation(net_file)
    sims_outcomes = []
    for sim in xrange(sims):
        points = simulator.getRandomPoints(n, projected=True)
        sim_events = []
        for edge in points:
            for point in points[edge]:
                sim_events.append(point)
        res, dists = local_k(network, sim_events, refs, scale_set, cache=cache)
        sims_outcomes.append(res)

    return sims_outcomes

def simulate_local_k_02(args):
    sims = args[0]
    n = args[1]
    refs = args[2]
    scale_set = args[3]
    cache = args[4]

    #print 'simulated_local_k_02'
    sims_outcomes = []
    sampler = RandomSampleGenerator(refs, n).next
    for sim in xrange(sims):
        sim_events = sampler()
        sim_localk = {}
        for node in refs:
            all_distances = cache[node[1][0]]
            distances = []
            for event in sim_events:
                event = event[1][0]
                if event in all_distances:
                    distances.append(all_distances[event]) 
            sim_localk[node[1][0]] = pykfuncs.kt_values(scale_set, distances, 1)
        sims_outcomes.append(sim_localk)
    
    return sims_outcomes

def k_cluster(network, events, refs, scale_set, sims, sig=0.1, sim_network=None, cpus=1):

    """
    Parameters:
    network: a network to which reference points are injected
    events: a set of event points projected into the network
    refs: a set of reference points unprojected into the network
    scale_set: tuple same as (min, max, resolution)
    sims: integer; the number of simulations
    sig: float; siginificance level
    sim_network: the source shape file containing the network data
                 this is used to simualte point patterns for inference 
    cpus: integer: the number of cpus
          multiprocessing can be used for inference
    """

    """
    1. For an observed set of n events on the network, calculate local K function 
    values for all m reference points
    """
    node2localK, net_dists = local_k(network, events, refs, scale_set)
    """
    When n < m (simulator == None):
    2. Select n out of m reference points randomly and 
    calculate local K function values for these randomly sampled points
    When n >= m (simulator != None):
    2. Randomly simulate n points on network edges and
    calculate local K function values for these randomly simulated points
    3. Repeat 2 as many as the number of simulations
    Note: on Darwin systems, simulation will be parallelized
    """
    n = len(events)
    sims_outcomes = []
    if not multiprocessing or cpus == 1:
        if sim_network:
            sims_outcomes = simulate_local_k_01((sims, n, sim_network, network, events, refs, scale_set, net_dists))
        else:
            sims_outcomes = simulate_local_k_02((sims, n, refs, scale_set, net_dists))
    elif multiprocessing and cpus >= 2:
        pool = multiprocessing.Pool(cpus)
        sims_list = range(sims)
        sims_list = map(len, [sims_list[i::cpus] for i in xrange(cpus)])
        partial_outcomes = None
        if sim_network:
             partial_outcomes = pool.map(simulate_local_k_01, 
                         [(sim, n, sim_network, network, events, refs, scale_set, net_dists) for sim in sims_list])
        else:
             partial_outcomes = pool.map(simulate_local_k_02, 
                         [(sim, n, refs, scale_set, net_dists) for sim in sims_list])
        sims_outcomes = partial_outcomes[0]
        for partial in partial_outcomes[1:]:
             sims_outcomes.extend(partial)

    """
    4. Determine lower and upper envelopes for the observed K function values 
       as well as the type of cluster (dispersion or clustering)
    """
    # 4. P-value evaluation
    lower_envelope = {}
    upper_envelope = {}
    lower_p = int(sims*sig/2)
    upper_p = int(sims*(1-sig/2))
    localKs = {}
    for node in refs:
        node = node[1][0]
        lower_envelope[node] = {}
        upper_envelope[node] = {}
        localKs[node] = {}
        for scale in node2localK[node].keys():
            local_outcomes = [sim[node][scale] for sim in sims_outcomes]
            local_outcomes.sort()
            obs = node2localK[node][scale]
            lower = local_outcomes[lower_p]
            upper = local_outcomes[upper_p]
            cluster = cluster_type(obs, lower, upper)
            localKs[node][scale] = [obs, lower, upper, cluster]

    return localKs


########NEW FILE########
__FILENAME__ = lincs
#!/usr/bin/env python

"""
A library for computing local indicators of network-constrained clusters

Author:
Myunghwa Hwang mhwang4@gmail.com

"""
import unittest
import numpy as np
import scipy.stats as stats
import geodanet.network as pynet
import pysal, copy
import time

def unconditional_sim(event, base, s): 
    """ 
    Parameters:
        event: n*1 numpy array with integer values
              observed values for an event variable
        base: n*1 numpy array with integer values
              observed values for a population variable
        s: integer
              the number of simulations

    Returns:
            : n*s numpy array
    """
    mean_risk = event.sum()*1.0/base.sum()
    if base.dtype != int:
        base = np.array([int(v) for v in base])
    base_zeros = (base == 0.0)
    base[base_zeros] += 1.0
    sims = np.random.binomial(base, mean_risk, (s, len(event))).transpose()
    sims[base_zeros, :] = 0.0
    return sims

def unconditional_sim_poisson(event, base, s): 
    """ 
    Parameters:
        event: n*1 numpy array with integer values
              observed values for an event variable
        base: n*1 numpy array with integer values
              observed values for a population variable
        s: integer
              the number of simulations

    Returns:
            : n*s numpy array
    """
    mean_risk = event.sum()*1.0/base.sum()
    E = base*mean_risk
    return np.random.poisson(E, (s, len(event))).transpose()

def conditional_multinomial(event, base, s): 
    """ 
    Parameters:
        event: n*1 numpy array with integer values
              observed values for an event variable
        base: n*1 numpy array with integer values
              observed values for a population variable
        s: integer
              the number of simulations

    Returns:
            : n*s numpy array
    """
    m = int(event.sum())
    props = base*1.0/base.sum()
    return np.random.multinomial(m, props, s).transpose()

def pseudo_pvalues(obs, sims):
    """
    Get pseudo p-values from a set of observed indices and their simulated ones.

    Parameters:
        obs: n*1 numpy array for observed values
        sims: n*sims numpy array; sims is the number of simulations

    Returns:
        p_sim : n*1 numpy array for pseudo p-values
        E_sim : mean of p_sim
        SE_sim: standard deviation of p_sim
        V_sim: variance of p_sim
        z_sim: standardarized observed values
        p_z_sim: p-value of z_sim based on normal distribution   
    """

    sims = np.transpose(sims)
    permutations = sims.shape[0]
    above = sims >= obs
    larger = sum(above)
    low_extreme = (permutations - larger) < larger
    larger[low_extreme] = permutations - larger[low_extreme]
    p_sim = (larger + 1.0)/(permutations + 1.0)
    E_sim = sims.mean()
    SE_sim = sims.std()
    V_sim = SE_sim*SE_sim
    z_sim = (obs - E_sim)/SE_sim
    p_z_sim = 1 - stats.norm.cdf(np.abs(z_sim))
    return p_sim, E_sim, SE_sim, V_sim, z_sim, p_z_sim 

def node_weights(network, attribute=False):
    """
    Obtains a spatial weights matrix of edges in a network
    if two edges share a node, they are neighbors

    Parameters:
        network: a network with/without attributes
        attribute: boolean
                   if true, attributes of edges are added to a dictionary of edges,
                   which is a return value

    Returns:
        w: a spatial weights instance
        id2link: an associative dictionary that connects a sequential id to a unique 
                 edge on the network
                 if attribute is true, each item in the dictionary includes the attributes

    """
    link2id, id2link = {}, {}
    counter = 0 
    neighbors, weights = {},{}
    for n1 in network:
        for n2 in network[n1]:
            if (n1,n2) not in link2id or link2id[(n1,n2)] not in neighbors:
                if (n1,n2) not in link2id:
                    link2id[(n1,n2)] = counter
                    link2id[(n2,n1)] = counter
                    if not attribute:
                        id2link[counter] = (n1, n2) 
                    else:
                        id2link[counter] = tuple([(n1,n2)] + list(network[n1][n2][1:]))
                    counter += 1
                neighbors_from_n1 = [(n1, n) for n in network[n1] if n != n2] 
                neighbors_from_n2 = [(n2, n) for n in network[n2] if n != n1] 
                neighbors_all = neighbors_from_n1 + neighbors_from_n2
                neighbor_ids = []
                for edge in neighbors_all:
                    if edge not in link2id:
                        link2id[edge] = counter
                        link2id[(edge[-1], edge[0])] = counter
                        if not attribute:
                            id2link[counter] = edge
                        else:
                            id2link[counter] = tuple([edge] + list(network[edge[0]][edge[1]][1:]))
                        neighbor_ids.append(counter)    
                        counter += 1
                    else:
                        neighbor_ids.append(link2id[edge])
                neighbors[link2id[(n1,n2)]] = neighbor_ids
                weights[link2id[(n1,n2)]] = [1.0]*(len(neighbors_from_n1) + len(neighbors_from_n2))
    return pysal.weights.W(neighbors, weights), id2link 

def edgepoints_from_network(network, attribute=False):
    """
    Obtains a list of projected points which are midpoints of edges
    
    Parameters:
        network: a network with/without attributes
        attribute: boolean
                   if true, one of return values includes attributes for each edge

    Returns:
        id2linkpoints: a dictionary that associates a sequential id to a projected, midpoint of each edge
        id2attr: a dictionary that associates a sequential id to the attributes of each edge
        link2id: a dictionary that associates each edge to its id
    """
    link2id, id2linkpoints, id2attr = {}, {}, {}
    counter = 0
    for n1 in network:
        for n2 in network[n1]:
            if (n1,n2) not in link2id or (n2,n1) not in link2id:
                link2id[(n1,n2)] = counter
                link2id[(n2,n1)] = counter
                if type(network[n1][n2]) != list:
                    half_dist = network[n1][n2]/2 
                else:
                    half_dist = network[n1][n2][0]/2 
                if n1[0] < n2[0] or (n1[0] == n2[0] and n1[1] < n2[1]):
                    id2linkpoints[counter] = (n1,n2,half_dist,half_dist)
                else:
                    id2linkpoints[counter] = (n2,n1,half_dist,half_dist)
                if attribute:
                    id2attr[counter] = network[n1][n2][1:]
                counter += 1
    return id2linkpoints, id2attr, link2id

def dist_weights(network, id2linkpoints, link2id, bandwidth):
    """
    Obtains a distance-based spatial weights matrix using network distance

    Parameters:
        network: an undirected network without additional attributes 
        id2linkpoints: a dictionary that includes a list of network-projected, midpoints of edges in the network
        link2id: a dictionary that associates each edge to a unique id
        bandwidth: a threshold distance for creating a spatial weights matrix

    Returns:
        w : a distance-based, binary spatial weights matrix
        id2link: a dictionary that associates a unique id to each edge of the network
    """
    linkpoints = id2linkpoints.values()
    neighbors, id2link = {}, {}
    net_distances = {}
    for linkpoint in id2linkpoints:
        if linkpoints[linkpoint] not in net_distances:
            net_distances[linkpoints[linkpoint][0]] = pynet.dijkstras(network, linkpoints[linkpoint][0], r=bandwidth)
            net_distances[linkpoints[linkpoint][1]] = pynet.dijkstras(network, linkpoints[linkpoint][1], r=bandwidth)
        ngh = pynet.proj_distances_undirected(network, linkpoints[linkpoint], linkpoints, r=bandwidth, cache=net_distances)
        #ngh = pynet.proj_distances_undirected(network, linkpoints[linkpoint], linkpoints, r=bandwidth)
        if linkpoints[linkpoint] in ngh:
            del ngh[linkpoints[linkpoint]]
        if linkpoint not in neighbors:
            neighbors[linkpoint] = []
        for k in ngh.keys():
            neighbor = link2id[k[:2]]
            if neighbor not in neighbors[linkpoint]:
                neighbors[linkpoint].append(neighbor)
            if neighbor not in neighbors:
                neighbors[neighbor] = []
            if linkpoint not in neighbors[neighbor]:
                neighbors[neighbor].append(linkpoint)
        id2link[linkpoint] = id2linkpoints[linkpoint][:2]
    weights = copy.copy(neighbors)
    for ngh in weights:
        weights[ngh] = [1.0]*len(weights[ngh])
    return pysal.weights.W(neighbors, weights), id2link


def lincs(network, event, base, weight, dist=None, lisa_func='moran', sim_method="permutations", sim_num=99):
    """
    Compute local Moran's I for edges in the network

    Parameters:
        network: a clean network where each edge has up to three attributes:
                 Its length, an event variable, and a base variable
        event: integer
               an index for the event variable 
        base: integer 
              an index for the base variable
        weight: string
                type of binary spatial weights
                two options are allowed: Node-based, Distance-based
        dist: float
              threshold distance value for the distance-based weight
        lisa_func: string
                   type of LISA functions
                   three options allowed: moran, g, and g_star
        sim_method: string
                    type of simulation methods
                    four options allowed: permutations, binomial (unconditional),
                    poisson (unconditional), multinomial (conditional)
        sim_num: integer
                 the number of simulations

    Returns:
               : a dictionary of edges
                 an edge and its moran's I are the key and item
               : a Weights object
                 PySAL spatial weights object

    """
    if lisa_func in ['g', 'g_star'] and weight == 'Node-based':
        print 'Local G statistics can work only with distance-based weights matrix'
        raise 

    if lisa_func == 'moran':
        lisa_func = pysal.esda.moran.Moran_Local
    else:
        lisa_func = pysal.esda.getisord.G_Local

    star = False
    if lisa_func == 'g_star':
        star = True    

    if base:
        def getBase(edges, edge, base):
            return edges[edge][base]
    else:
        def getBase(edges, edge, base):
            return 1.0
    w, edges, e, b, edges_geom = None, None, None, None, []
    if weight == 'Node-based':
        w, edges = node_weights(network, attribute=True)
	n = len(edges)
	e, b = np.zeros(n), np.zeros(n)
	for edge in edges:
            edges_geom.append(edges[edge][0])
	    e[edge] = edges[edge][event]
            b[edge] = getBase(edges, edge, base)
        w.id_order = edges.keys()
    elif dist is not None:
        id2edgepoints, id2attr, edge2id = edgepoints_from_network(network, attribute=True)
        for n1 in network:
            for n2 in network[n1]:
                network[n1][n2] = network[n1][n2][0]
        w, edges = dist_weights(network, id2edgepoints, edge2id, dist)
        n = len(id2attr)
	e, b = np.zeros(n), np.zeros(n)
        if base:
            base -= 1
	for edge in id2attr:
            edges_geom.append(edges[edge])
	    e[edge] = id2attr[edge][event - 1]
            b[edge] = getBase(id2attr, edge, base)
        w.id_order = id2attr.keys()

    Is, p_sim, Zs = None,None, None
    if sim_method == 'permutation':
        if lisa_func == pysal.esda.moran.Moran_Local:
	    lisa_i = lisa_func(e*1.0/b,w,transformation="r",permutations=sim_num)
            Is = lisa_i.Is
            Zs = lisa_i.q
        else:
	    lisa_i = lisa_func(e*1.0/b,w,transform="R",permutations=sim_num,star=star)
            Is = lisa_i.Gs
            Zs = lisa_i.Zs
        p_sim = lisa_i.p_sim
    else:
	sims = None
        if lisa_func == pysal.esda.moran.Moran_Local:
	    lisa_i = lisa_func(e*1.0/b,w,transformation="r",permutations=0)
            Is = lisa_i.Is
            Zs = lisa_i.q
        else:
	    lisa_i = lisa_func(e*1.0/b,w,transform="R",permutations=0,star=star)
	    Is = lisa_i.Gs
	    Zs = lisa_i.Zs
	if sim_method == 'binomial':
	    sims = unconditional_sim(e, b, sim_num)
	elif sim_method == 'poisson':
	    sims = unconditional_sim_poisson(e, b, sim_num)
	else:
	    sims = conditional_multinomial(e, b, sim_num)
        if lisa_func == pysal.esda.moran.Moran_Local:
	    for i in range(sim_num):
		sims[:,i] = lisa_func(sims[:,i]*1.0/b,w,transformation="r",permutations=0).Is
        else:
	    for i in range(sim_num):
		sims[:,i] = lisa_func(sims[:,i]*1.0/b,w,permutations=0,star=star).Gs
	sim_res = pseudo_pvalues(Is, sims)
	p_sim = sim_res[0]

    w.transform = 'O'
    return zip(edges_geom, e, b, Is, Zs, p_sim), w

        

########NEW FILE########
__FILENAME__ = network
import math
import pysal
from pysal.cg.shapes import Point, Chain, LineSegment, Rectangle
from pysal.cg.locators import Grid
import random, copy
from heapq import heappush, heappop
import time

def no_nodes(G):
    """
    returns the number of nodes in a undirected network
    """
    return len(G)

def no_edges(G):
    """
    returns the number of edges in a undirected network
    """
    e = 0.0
    for n in G:
        e += len(G[n])
    return e/2.0 

def tot_net_length(G):
    """
    returns the total length of a undirected network
    """
    l = 0.0
    done = set()
    for n in G:
        for m in G[n]:
            if m in done: continue
            l += G[n][m]
        done.add(n)
    return l/2.0

def walk(G, s, S=set()):
    """ 
    Returns a traversal path from s on G
    source: Python Algorithms Mastering Basic Algorithms in the Python Language, 2010, p.104
    """
    P, Q, SG = dict(), set(), dict()
    P[s] = None
    SG[s] = G[s]
    Q.add(s)
    while Q:
        u = Q.pop()
        for v in set(G[u].keys()).difference(P, S):
            Q.add(v)
            P[v] = u
            SG[v] = G[v]
    return SG

def components(G):
    """ 
    Returns connected components of G
    source: Python Algorithms Mastering Basic Algorithms in the Python Language, 2010, p.105
    Complexity: O(E+V) where E is the number of edges and V is the number of nodes in a graph
    """
    comp, seen = [], set()
    for u in G:
        if u in seen: continue
        C = walk(G, u)
        seen.update(set(C.keys()))
        comp.append(C)
    return comp

def no_components(G):
    return len(components(G))

def net_global_stats(G, boundary=None, detour=True):
    v = no_nodes(G)
    e = no_edges(G)
    L = tot_net_length(G)
    p = no_components(G)
    u = e - v + p # cyclomatic number
    alpha = u*1.0/(2*v - 5)
    beta = e*1.0/v
    emax = 3*(v-2)
    gamma = e*1.0/emax
    eta = L*1.0/e
    net_den = None
    if boundary:
        s = pysal.open(boundary)
        if s.type != pysal.cg.shapes.Polygon: 
            raise ValueError, 'File is not of type POLYGON'
        net_den = s.next().area
    net_dist, eucl_dist = 0.0, 0.0
    det = None
    if detour:
        nodes = G.keys()
        for n in nodes:
            net_D = dijkstras(G, n)
            net_dist += sum(net_D.values())
            eucl_D = [ math.sqrt((n[0] - m[0])**2 + (n[1] - m[1])**2) for m in nodes]
            eucl_dist += sum(eucl_D)
        net_dist /= 2.0
        eucl_dist /= 2.0
        if net_dist > 0.0:
            det = eucl_dist*1.0/net_dist
    return v, e, L, p, u, alpha, beta, emax, gamma, eta, net_den, det


def random_projs(G, n):
    """
    Returns a list of random locations on the network as projections
    with the form (src, dest, dist_from, dist_from_src, dist_from_dest)
    """

    def binary_search(list, q):
        l = 0
        r = len(list)
        while l < r:
            m = (l + r)/2
            if list[m][0] > q:
                r = m
            else:
                l = m + 1
        return list[l][1]

    total_net_len = 0
    for src in G:
        for dest in G[src]:
            total_net_len += G[src][dest]

    lengthogram = [(0, (None, None))]
    for src in G:
        for dest in G[src]:
            lengthogram.append((lengthogram[-1][0] + G[src][dest], (src, dest)))

    projs = []
    for i in xrange(n):
        e = binary_search(lengthogram, random.random() * total_net_len)
        wgt = G[e[0]][e[1]]
        along = wgt * random.random()
        # (src, dest, dist_from_src, dist_from_dest)
        projs.append((e[0], e[1], along, wgt - along))

    return projs

def proj_distances_undirected(G, src, dests, r=1e600, cache=None):
    if cache and src[0] in cache:
        SND = cache[src[0]]
    else:
        SND = dijkstras(G, src[0], r) # Distance from edge start node to other nodes
    if cache and src[1] in cache:
        DND = cache[src[1]]
    else:
        DND = dijkstras(G, src[1], r) # Distance from edge end node to other nodes
    D = {}
    for d in dests:
        # If the dest lies on the same edge as the src (or its inverse)
        if (d[0] == src[0] and d[1] == src[1]) or (d[0] == src[1] and d[1] == src[0]):
                dist = abs(src[2] - d[2])
        else:
            # get the four path distances
            # src edge start to dest edge start
            src2src, src2dest, dest2src, dest2dest = 1e600, 1e600, 1e600, 1e600
            if d[0] in SND: 
                src2src = src[2] + SND[d[0]] + d[2] 
            # src edge start to dest edge end
            if d[1] in SND:
                src2dest = src[2] + SND[d[1]] + d[3] 
            # src edge end to dest edge start
            if d[0] in DND:
                dest2src = src[3] + DND[d[0]] + d[2]
             # src edge end to dest edge end
            if d[1] in DND:
                dest2dest = src[3] + DND[d[1]] + d[3]
            dist = min(src2src, src2dest, dest2src, dest2dest)

        if dist <= r:
            D[d] = dist

    return D

def proj_distances_directed(G, src, dests, r=1e600):
    ND = dijkstras(G, src[1], r) # Distance from edge destination node to other nodes
    D = {}
    for d in dests:
        if d[0] in ND:
            if d[0] == src[0] and d[1] == src[1]: # Same edge and dest further along
                dist = abs(src[2] - d[2])
                if dist <= r:
                    D[d] = dist
            else:
                # dist from edge proj to end of src edge + 
                # dist from src edge dest node to dest edge src node +
                # dist from start of dest edge to edge proj
                dist = src[3] + ND[d[0]] + d[2] 
                #print dist
                if dist <= r:
                    D[d] = dist
    return D

def relax(G, u, v, D, P, r=1e600): 
    """ 
    Update the distance to v 
    if the route to v through u is shorter than the existing route to v
    Code from Hetland 2010 
    Python Algorithms Mastering Basic Algorithms in the Python Language, p.200
    """
    d = D.get(u, r) + G[u][v]
    if d <= D.get(v, r):
        D[v], P[v] = d, u
        return True

def dijkstras(G, start, r=1e600, p=False): 
    """ 
    Find a shortest path from s to all nodes in the network G
    Code from Hetland 2010 
    Python Algorithms Mastering Basic Algorithms in the Python Language, p.205
    Complexity: O(M*lgN) where M is the number of edges and N is the number of nodes
    """
    D, P, Q, S = {start:0}, {}, [(0,start)], set()  # Distance estimates, tree (path), queue, visited
    while Q:
        _, u = heappop(Q)
        if u in S: 
            continue 
        S.add(u)
        for v in G[u]:
            relaxed = relax(G, u, v, D, P, r=r)
            if relaxed: 
                heappush(Q, (D[v], v)) 
    if p: 
        return D, P
    return D

class Snapper:
    """
    Snaps points to their nearest location on the network.

    Uses a novel algorithm which relies on two properties of the input network:
    1.  Most of the edges are very short relative to the total area 
        encompassed by the network.
    2.  The edges have a relatively constant density throughout this area.

    The algorithm works by creating a binning of the midpoints of all the edges.
    When a query point is given, all the edges in a region around the query
    point (located by their binned midpoints) are compared to the query point. 
    If none are found, the neighborhood is enlarged. When a closest edge is found, 
    the neighborhood is enlarged slightly and checked again. The enlargement is 
    such that if the closest edge found remains the closest edge, then it will
    always be the closest edge.
    """

    def __init__(self, network):
        """
        Test tag <tc>#is#Snapper.__init__</tc>
        """

        """
        Generate a list of the edge lengths and pick a maximum length
        allowed based on the median length. This maximum length will be
        used to use multiple midpoints to represent edges which are 
        exceptionally long, lest they ruin the efficiency of the algorithm.
        """

        # Generate list of lengths
        self.network = network
        edge_lens = []
        for n in network:
            for m in network[n]:
                if n != m: 
                    edge_lens.append(pysal.cg.get_points_dist(Point(n), Point(m))) # it can be optional
        if edge_lens == []:
            raise ValueError, 'Network has no positive-length edges'
        edge_lens.sort()
        max_allowed_edge_len = 5 * edge_lens[len(edge_lens)/2]

        """ 
        Create a bin structures with proper range to hold all of the edges.
        The size of the bin is on the order of the length of the longest edge (and
        of the neighborhoods searched around each query point.
        """
        endpoints = network.keys()
        endpoints_start, endpoints_end = [ep[0] for ep in endpoints], [ep[1] for ep in endpoints]
        bounds = Rectangle(min(endpoints_start),min(endpoints_end),max(endpoints_start),max(endpoints_end))
        self.grid = Grid(bounds, max_allowed_edge_len)

        """
        Insert the midpoint of each edge into the grid. If an edge is too long, 
        break it into edges of length less than the maximum allowed length and
        add the midpoint of each.
        """
        self.search_rad = max_allowed_edge_len*0.55
        for n in network:
            for m in network[n]:
                edge_len = pysal.cg.get_points_dist(Point(n), Point(m)) # it can be a direct extraction
                if edge_len > max_allowed_edge_len:
                    mid_edge = []
                    num_parts = int(math.ceil(edge_len/max_allowed_edge_len))
                    part_step = 1.0/num_parts
                    dx = m[0] - n[0]
                    dy = m[1] - n[1]
                    midpoint = (n[0] + dx*part_step/2, n[1] + dy*part_step/2)
                    for r in [part_step*t for t in xrange(num_parts)]:
                        mid_edge.append(((n, m), midpoint))
                        midpoint = (midpoint[0] + dx*part_step, midpoint[1] + dy*part_step)
                    for me in mid_edge:
                        self.grid.add(me[0], Point(me[1]))
                else:
                    self.grid.add((n, m), Point(((n[0] + m[0])/2, (n[1] + m[1])/2)))

        """
        During the snapping of a query point we will initialize the closest point on the network
        to be a dummy location known to be invalid. This must be done in case the neighborhood
        search does not find any edge midpoints and it must be grown repeatedly. In this case
        we want to make sure we don't give up having not found a valid closest edge.
        """
        self.dummy_proj = (None, None, 0, 0) # Src, dest, dist_from_src, dist_from_dest)

    def snap(self, p):
        """
        Test tag <tc>#is#Snapper.snap</tc>
        """

        """
        Initialize the closest location found so far to be infinitely far away.
        Then begin with a neighborhood on the order of the maximum edge allowed and
        repeatedly growing it. When a closest edge is found, grow once more and check again.
        """
        
        cur_s_rad = self.search_rad
        found_something = False
        # Whle neighborhood is empty, enlarge and check again    
        while not found_something: 
            if self.grid.proximity(Point(p), cur_s_rad) != []:
                found_something = True
            cur_s_rad *= 2
        # Expand to include any edges whose endpoints might lie just outside
        # the search radius
        cur_s_rad += self.search_rad
        # Now find closest in this neighborhood
        best_seg_dist = 1e600
        for e in self.grid.proximity(Point(p), cur_s_rad):
            seg = LineSegment(Point(e[0]), Point(e[1]))
            p2seg = pysal.cg.get_segment_point_dist(seg, Point(p))
            dist = p2seg[0]
            if p2seg[0] < best_seg_dist:
                # (src, dest, dist_from_src, dist_from_dest)
                best_proj = (e[0], e[1], dist*p2seg[1], dist*(1-p2seg[1]))
                best_seg_dist = p2seg[0]
        return best_proj

def network_from_endnodes(s, d, wgt, undirected=True):
    G = {}
    for g, r in zip(s,d):
        start = g.vertices[0]
        end = g.vertices[-1]
        G.setdefault(start, {})
        G.setdefault(end, {})
        r_w = wgt(g,r)
        G[start][end] = r_w
        if undirected:
            G[end][start] = r_w
    s.close()
    d.close()
    return G

def network_from_allvertices(s, d):
    G = {}
    for g, r in zip(s, d):
        vertices = g.vertices
        for i, vertex in enumerate(vertices[:-1]):
            n1, n2 = vertex, vertices[i+1]
            dist = pysal.cg.get_points_dist(Point(n1), Point(n2)) 
            G.setdefault(n1, {}) 
            G.setdefault(n2, {}) 
            G[n1][n2] = dist 
            G[n2][n1] = dist 
    s.close()
    d.close()
    return G

def read_hierarchical_network(s, d):
    G, Gj, G_to_Gj = {}, {}, {}
    for g, r in zip(s, d):
        vertices = g.vertices
        Gj.setdefault(vertices[0], {}) 
        Gj.setdefault(vertices[-1], {}) 
        d_total = 0.0 
        for i, vertex in enumerate(vertices[:-1]):
            n1, n2 = vertex, vertices[i+1]
            dist = pysal.cg.get_points_dist(Point(n1), Point(n2)) 
            G.setdefault(n1, {}) 
            G.setdefault(n2, {}) 
            G[n1][n2] = dist 
            G[n2][n1] = dist
            G_to_Gj[(n1,n2)] = [(vertices[0], vertices[-1]), d_total] # info for the opposite direction 
            d_total += dist
        Gj[vertices[0]][vertices[-1]] = d_total
        Gj[vertices[-1]][vertices[0]] = d_total
    s.close()
    d.close()
    return G, Gj, G_to_Gj

def read_network(filename, wgt_field=None, undirected=True, endnodes=False, hierarchical=False, attrs=None):
    s = pysal.open(filename)
    dbf = pysal.open(filename[:-3] + 'dbf')
    if s.type != pysal.cg.shapes.Chain:
        raise ValueError, 'File is not of type ARC'
    if not endnodes and not undirected:
        raise ValueError, 'Network using all vertices should be undirected'
    if hierarchical and not (undirected and not endnodes):
        raise ValueError, 'Hierarchial network should be undirected and use all vertices'
    if endnodes:
        if wgt_field and attrs == None:
            w = dbf.header.index(wgt_field)
            def wgt(g, r):
                return r[w]
        elif wgt_field and attrs:
            attrs = [wgt_field] + attrs
            w_indices = [dbf.header.index(field) for field in attrs]
            def wgt(g, r):
                return [r[w] for w in w_indices]
        elif wgt_field is None and attrs:
            w_indices = [dbf.header.index(field) for field in attrs]
            def wgt(g, r):
                d = pysal.cg.get_points_dist(Point(g.vertices[0]), Point(g.vertices[-1]))
                return [d] + [r[w] for w in w_indices] 
        else:
            def wgt(g, r):
                return pysal.cg.get_points_dist(Point(g.vertices[0]), Point(g.vertices[-1]))
        return network_from_endnodes(s, dbf, wgt, undirected)
    if not endnodes and not hierarchical:
        return network_from_allvertices(s, dbf)
    if hierarchical:
        return read_hierarchial_netowrk(s, dbf)

def proj_pnt_coor(proj_pnt):
    n1, n2 = proj_pnt[0], proj_pnt[1]
    dist_n12 = pysal.cg.get_points_dist(Point(n1), Point(n2))
    len_ratio = proj_pnt[2]*1.0/dist_n12
    xrange, yrange = n2[0] - n1[0], n2[1] - n1[1]
    x = n1[0] + xrange*len_ratio
    y = n1[1] + yrange*len_ratio
    return (x,y)

def inject_points(network, proj_pnts):

    pnts_by_seg = {}
    proj_pnt_coors = []
    for pnt in proj_pnts:
        target_edge = None
        if (pnt[0], pnt[1]) not in pnts_by_seg and (pnt[1], pnt[0]) not in pnts_by_seg:
            target_edge = (pnt[0], pnt[1])
            pnts_by_seg[target_edge] = set()
        elif (pnt[0], pnt[1]) in pnts_by_seg:
            target_edge = (pnt[0], pnt[1])
        elif (pnt[1], pnt[0]) in pnts_by_seg:
            target_edge = (pnt[1], pnt[0])
        coor = proj_pnt_coor(pnt)
        pnts_by_seg[target_edge].add(coor)
        proj_pnt_coors.append(coor)

    new_network = copy.deepcopy(network)
    
    for seg in pnts_by_seg:
        proj_nodes = set(list(pnts_by_seg[seg]) + [seg[0], seg[1]])
        proj_nodes = list(proj_nodes)
        if seg[0][0] == seg[1][0]:
            proj_nodes.sort(key=lambda coords: coords[1])
        else:
            proj_nodes.sort()
        proj_nodes_len = len(proj_nodes)
        prev_seg_d, next_seg_d = 0.0, 0.0
        for i in range(proj_nodes_len - 1):
            start, end = proj_nodes[i], proj_nodes[i+1]
            if start not in new_network:
                new_network[start] = {}
            if end not in new_network:
                new_network[end] = {}
            d = pysal.cg.get_points_dist(Point(start), Point(end))
            new_network[start][end] = d
            new_network[end][start] = d
        if new_network.has_key(seg[0]) and new_network[seg[0]].has_key(seg[1]):
            del new_network[seg[0]][seg[1]]
            del new_network[seg[1]][seg[0]]
        else:
            print seg, network.has_key(seg[0]), network[seg[0]], network.has_key(seg[1]), network[seg[1]]

    return new_network, proj_pnt_coors

def mesh_network(network, cellwidth, at_center=False):
    mesh_net = {}
    done = {}
    #done = set()
    for n1 in network:
        for n2 in network[n1]:
            #if n2 in done: continue
            if (n1,n2) in done or (n2,n1) in done:
                continue
            len_ratio = cellwidth*1.0/network[n1][n2]
            start, end = n1, n2
            # The order for reading a network edge is slightly different from SANET. 
            # SANET does not seem to have a set of consistent rules. 
            if n1[0] < n2[0] or (n1[0] == n2[0] and n1[1] < n2[1]):
                start, end = n2, n1
            xrange, yrange = end[0] - start[0], end[1] - start[1]
            dx, dy = xrange*len_ratio, yrange*len_ratio
            no_segments = int(math.floor(1.0/len_ratio))
            if at_center:
                xs = [start[0], start[0] + dx/2.0]
                ys = [start[1], start[1] + dy/2.0]
                xs = xs + [xs[-1] + i*dx for i in range(1, no_segments + 1)]
                ys = ys + [ys[-1] + i*dy for i in range(1, no_segments + 1)] 
            else:
                xs = [start[0] + i*dx for i in range(no_segments + 1)]
                ys = [start[1] + i*dy for i in range(no_segments + 1)]
            if xs[-1] != end[0] or ys[-1] != end[1]:
                xs.append(end[0])
                ys.append(end[1])
            new_nodes = zip(xs, ys)
            for i in range(len(new_nodes) - 1):
                n, m = new_nodes[i], new_nodes[i+1]
                d = pysal.cg.get_points_dist(Point(n), Point(m))
                if n not in mesh_net: mesh_net[n] = {}
                if m not in mesh_net: mesh_net[m] = {}
                mesh_net[n][m] = d
                mesh_net[m][n] = d
            done[(n1,n2)] = True
        #done.add(n1)
    return mesh_net

def write_network_to_pysalshp(network, filename, header=None, field_spec=None):

    if not filename.endswith('shp') and not filename.endswith('SHP'):
        print 'filename would end with shp or SHP'
        return

    shp = pysal.open(filename, 'w')
    dbf = pysal.open(filename[:-3] + 'dbf', 'w')
    if not header:
        dbf.header = ['ID', 'VALUE']
    else:
        dbf.header = ['ID'] + header
    if not field_spec:
        dbf.field_spec = [('N', 9, 0), ('N', 15, 8)]
        def getValue(G, n, m):
            return [G[n][m]]
    else:
        dbf.field_spec = [('N', 9, 0)] + field_spec
        v = network[network.keys()[0]] 
        if type(v) == dict:
            v = v.values()[0]
        if type(v) == list:
            wrap_func = list
        else:
            def wrap_func(value):
                return [value]
        def getValue(G, n, m):
            return wrap_func(G[n][m])
             
    used, counter = set(), 0
    for n1 in network:
        for n2 in network[n1]:
            if n2 in used: continue
            shp.write(Chain([Point(n1), Point(n2)]))                           
            dbf.write([counter] + getValue(network,n1,n2))
            counter += 1
        used.add(n1)

    shp.close()
    dbf.close()

def write_valued_network_to_shp(filename, fields, types, net, values, valFunc):
    oShp = pysal.open(filename, 'w')        
    oDbf = pysal.open(filename[:-3] + 'dbf', 'w')
    oDbf.header = fields                    
    oDbf.field_spec = types                 
    #for n in net:                           
    #    for m in net:                       
    #        oShp.write(Chain([Point(n), Point(m)]))
    #        oDbf.write([valFunc(values, n), valFunc(values, m)])
    used, counter = set(), 0
    for n in net:                           
        for m in net[n]:
            if m in used: continue                       
            oShp.write(Chain([Point(n), Point(m)]))
            oDbf.write([valFunc(values, n), valFunc(values, m)])
            counter += 1
        used.add(n)
    oShp.close()                            
    oDbf.close()

def write_list_network_to_shp(filename, fields, types, net):
    oShp = pysal.open(filename, 'w')        
    oDbf = pysal.open(filename[:-3] + 'dbf', 'w')
    oDbf.header = ['ID'] + fields                    
    oDbf.field_spec = [('N',9,0)] + types                 
    for i, rec in enumerate(net):
        geom = rec[0]
        table_data = list(rec[1:])
        oShp.write(Chain([Point(geom[0]), Point(geom[1])]))
        oDbf.write([i] + table_data)
    oShp.close()
    oDbf.close()


########NEW FILE########
__FILENAME__ = priordict
"""
Code adapted from source from:
David Eppstein, UC Irvine, 8 Mar 2002
"""

from __future__ import generators

class PriorityDictionary(dict):

    def __init__(self):
        """
        Initialize PriorityDictionary by creating binary heap
        of pairs (value,key).  Note that changing or removing a dict entry will
        not remove the old pair from the heap until it is found by smallest() or
        until the heap is rebuilt.
        """
        self.__heap = []
        dict.__init__(self)

    def smallest(self):
        """
        Find smallest item after removing deleted items from heap.
        """
        if len(self) == 0:
            raise IndexError, "smallest of empty PriorityDictionary"
        heap = self.__heap
        while heap[0][1] not in self or self[heap[0][1]] != heap[0][0]:
            lastItem = heap.pop()
            insertionPoint = 0
            while 1:
                smallChild = 2*insertionPoint+1
                if smallChild+1 < len(heap) and \
                        heap[smallChild] > heap[smallChild+1]:
                    smallChild += 1
                if smallChild >= len(heap) or lastItem <= heap[smallChild]:
                    heap[insertionPoint] = lastItem
                    break
                heap[insertionPoint] = heap[smallChild]
                insertionPoint = smallChild
        return heap[0][1]
	
    def __iter__(self):
        """
        Create destructive sorted iterator of PriorityDictionary.
        """
        def iterfn():
            while len(self) > 0:
                x = self.smallest()
                yield x
                del self[x]
        return iterfn()
	
    def __setitem__(self,key,val):
        """
        Change value stored in dictionary and add corresponding
        pair to heap.  Rebuilds the heap if the number of deleted items grows
        too large, to avoid memory leakage.
        """
        dict.__setitem__(self,key,val)
        heap = self.__heap
        if len(heap) > 2 * len(self):
            self.__heap = [(v,k) for k,v in self.iteritems()]
            self.__heap.sort()  # builtin sort likely faster than O(n) heapify
        else:
            newPair = (val,key)
            insertionPoint = len(heap)
            heap.append(None)
            while insertionPoint > 0 and \
                    newPair < heap[(insertionPoint-1)//2]:
                heap[insertionPoint] = heap[(insertionPoint-1)//2]
                insertionPoint = (insertionPoint-1)//2
            heap[insertionPoint] = newPair
	
    def setdefault(self,key,val):
        """
        Reimplement setdefault to call our customized __setitem__.
        """
        if key not in self:
            self[key] = val
        return self[key]

########NEW FILE########
__FILENAME__ = simulator
#!/usr/bin/env python

"""
Author: Ran Wei, Myunghwa Hwang
"""

import pysal
import numpy as np

class Simulation(object):
   
    def __init__(self, src_filename):
        "create a bidirectional network with its total length and total number of links"
        self.nw = pysal.open(src_filename, 'r')
        self.G = {} # {edge_index:(d12,(n1,n2))}
        self.GNet = {} # {n1:{n2:d12}}
        self.total_length = 0.0
        self.nwNum = 0
        for line in self.nw:
            vertices = line.vertices
            for i, vertex in enumerate(vertices[:-1]):
                n1, n2 = vertex, vertices[i+1]
                self.G.setdefault(self.nwNum, ())
                self.GNet.setdefault(n1, {})
                self.GNet.setdefault(n2, {})
                d = pysal.cg.get_points_dist(pysal.cg.Point(n1), pysal.cg.Point(n2))
                self.G[self.nwNum] = (d, (n1, n2))
                self.GNet[n1][n2] = self.nwNum
                self.GNet[n2][n1] = self.nwNum
                self.total_length += d
                self.nwNum += 1
        self.nw.close()
        self.imaginaryLineGenerated = False
            
    def generateImaginaryLine(self):
        '''
           Create an imaginary line that starts from 0 and ends at 1
           and mark the locations of end points of each link
        '''
        self.nwCumPro = [0.0]
        for e in self.G.keys():
            self.nwCumPro.append(self.nwCumPro[-1] + (self.G[e][0]/self.total_length))
                                                                        
        # self.nwCumProDict --> {edge_index:right_side_end_point_of_the_link_on_the_imaginary_line}
        self.nwCumProDict = dict(zip(self.G.keys(), self.nwCumPro[1:])) 
        self.imaginaryLineGenerated = True
            
    def getRandomPoints(self, n, projected=False, toShp=False):   
        '''Create a random point pattern data set on the given network'''

        if not self.imaginaryLineGenerated:
            self.generateImaginaryLine()

        ## generate n unique random numbers between 0 and 1
        #randSet = set()
        #while len(randSet) < n: 
        #    randSet = set(np.random.random_sample(n))
        #randSet = np.array(list(randSet))        

        # generate n random numbers between 0 and 1
        randSet = np.random.random_sample(n)

        # Assign the random numbers to the links on the network
        # Think nwCumPro as bins; get bin numbers for all random numbers
        randSet_to_bins=np.digitize(randSet,self.nwCumPro)
        randSet_to_bins=zip(randSet_to_bins,randSet)
        randSet_to_bins.sort()
        # Determine geographic coordinates for each random number
        nwPtDict = {}
        for bin_id, rand_number in randSet_to_bins:
            bid = bin_id - 1
            n1, n2 = self.G[bid][1] # n1 and n2 are geographic (real) coordinates for the end points of a link
            origin = 0 if bid <= 0 else self.nwCumProDict[bid-1]
            length = self.nwCumProDict[bid] - origin
            # get prop to determine the geographic coordinate of a random number on the link (n1, n2)
            # length is the length of the link (n1, n2) on the imaginary line
            # (self.nwCumProDict[bin_id] - rand_number) is the distance between a random point and n2 
            # on the imaginary line
            prop = (self.nwCumProDict[bid] - rand_number)*1.0/length
            nwPtDict.setdefault(bid, [])
            if not projected:
                x = n2[0] - (n2[0] - n1[0])*prop # n2[0]: the geographic coordinate of n2 on the X axis
                y = n2[1] - (n2[1] - n1[1])*prop # n2[1]: the geographic coordinate of n2 on the Y axis
                nwPtDict[bid].append((x,y))
            else:
                dist = self.G[bid][0]
                proj_pnt = (n1, n2, dist*(1-prop), dist*prop)
                if toShp:
                    x = n2[0] - (n2[0] - n1[0])*prop
                    y = n2[1] - (n2[1] - n1[1])*prop
                    proj_pnt = tuple(list(proj_pnt) + [x, y])                        
                nwPtDict[bid].append(proj_pnt)
            
        return nwPtDict

    def countPointsOnNetwork(self, points, defaultBase=True):
        G = {}
        for k in self.G:
            n1, n2 = self.G[k][-1]
            G.setdefault(n1, {})
            G.setdefault(n2, {})
            if n2 not in G[n1]:
                attr = G[n1].setdefault(n2, [self.G[n1][n2], 0])
            if n1 not in G[n2]:
                attr = G[n2].setdefault(n1, [self.G[n2][n1], 0])
            if k in points:
                attr[-1] += len(points[k])
            if defaultBase:
                attr += [1.0]
            G[n1][n2] = attr
            G[n2][n1] = attr
        return G

    def createProjRandomPointsShp(self, n, out_filename):
        points = nwPtDict = self.getRandomPoints(n, projected=True, toShp=True)
        shp = pysal.open(out_filename, 'w')
        dbf = pysal.open(out_filename[:-3] + 'dbf', 'w')
        dbf.header = ['ID', 'FROM_P1', 'FROM_P2', 'TO_P1', 'TO_P2', 'D_FROM', 'D_TO']
        dbf.field_spec = [('N',9,0)] + [('N',18,7)]*6
        counter = 0
        for k in points:
            for p in points[k]:
                p = list(p)
                shp.write(pysal.cg.Point(tuple(p[-2:])))
                dbf.write([counter, p[0][0], p[0][1], p[1][0], p[1][1], p[2], p[3]])
                counter += 1
        shp.close()
        dbf.close()
            
    def createRandomPointsShp(self, n, out_filename):
        nwPtDict = self.getRandomPoints(n)
        self.writePoints(nwPtDict, out_filename)

    def writePoints(self, points, out_filename):
        shp = pysal.open(out_filename, 'w')
        dbf = pysal.open(out_filename[:-3] + 'dbf', 'w')
        dbf.header = ['ID']
        dbf.field_spec = [('N',9,0)]
        counter = 0
        for k in points:
            for p in points[k]:
                shp.write(pysal.cg.Point(tuple(p)))
                dbf.write([counter])
                counter += 1
        shp.close()
        dbf.close()

    def getClusteredPoints(self, centerNum, ptNum, percent, clusterMeta=False):

        # split network into center- and non-center network
        centerIDs = np.random.randint(0, self.nwNum, centerNum)
        centers = set(centerIDs)
        centerG, centerG_length = {}, 0
        counter, counter2ID = 0, {}
        for center in centerIDs:
            centerG[counter] = self.G[center]
            centerG_length += self.G[center][0]
            counter2ID[counter] = center
            counter += 1
            n1, n2 = self.G[center][1]
            for neighbor in self.GNet[n1]:
                if neighbor != n2:
                    nghLink = self.GNet[n1][neighbor]
                    centerG[counter] = self.G[nghLink]
                    centerG_length += self.G[nghLink][0]
                    counter2ID[counter] = nghLink
                    counter += 1
                    centers.add(nghLink)
            for neighbor in self.GNet[n2]:
                if neighbor != n1:
                    nghLink = self.GNet[n2][neighbor]
                    centerG[counter] = self.G[nghLink]
                    centerG_length += self.G[nghLink][0]
                    counter2ID[counter] = nghLink
                    counter += 1
                    centers.add(nghLink)
        nonCenterIDs = set(self.G.keys()).difference(centers)
        nonCenterG, nonCenterG_length = {}, 0
        for i, nonCenter in enumerate(nonCenterIDs):
             nonCenterG[i] = self.G[nonCenter]
             nonCenterG_length += self.G[nonCenter][0]

        self.oldG, self.old_total_length = self.G, self.total_length

        self.G, self.total_length = centerG, centerG_length
        n_centerPoints = int(percent*ptNum*1.0)
        self.imaginaryLineGenerated = False
        pointsInCenter = self.getRandomPoints(n_centerPoints) 
        meta = {}
        if clusterMeta:
            for cluster in pointsInCenter:
                num_points = len(pointsInCenter[cluster])
                centerLink = self.oldG[cluster][1]
                centerID = counter2ID[cluster]
                centerLink = self.oldG[centerID][1]
                meta[centerLink] = [self.oldG[centerID][0], num_points, centerID in centerIDs]
                #meta[centerID] = [num_points] + list(self.oldG[centerID])
       
        self.G, self.total_length = nonCenterG, nonCenterG_length 
        self.imaginaryLineGenerated = False
        pointsInNonCenter = self.getRandomPoints(ptNum - n_centerPoints)
        centers_no = len(centers)
        for k in pointsInNonCenter:
            pointsInCenter[k + centers_no] = pointsInNonCenter[k]
        pointsInCenter.update(pointsInNonCenter)

        self.G, self.total_length = self.oldG, self.old_total_length

        self.imaginaryLineGenerated = False

        return pointsInCenter, meta

    def writeMeta(self, metaData, out_file):
        shp = pysal.open(out_file, 'w')
        dbf = pysal.open(out_file[:-3] + 'dbf', 'w')
        dbf.header = ['LENGTH', 'NO_PNTS', 'INITIAL_CENTER']
        dbf.field_spec = [('N',9,0)]*2 + [('L',1,0)]
        for link in metaData:
            vertices = list(link)
            vertices = [pysal.cg.Point(v) for v in vertices]
            shp.write(pysal.cg.Chain(vertices))
            dbf.write(metaData[link])
        shp.close()
        dbf.close()

    def createClusteredPointsShp(self, centerNum, n, percent, out_filename, clusterMetaFile=None):
        nwPtDict, meta = self.getClusteredPoints(centerNum, n, percent, clusterMetaFile!=None)
        self.writePoints(nwPtDict, out_filename)
        if clusterMetaFile:
            self.writeMeta(meta, clusterMetaFile)

if __name__ == '__main__':
    sim=Simulation("streets.shp")
    sim.createProjRandomPointsShp(100, "random_100.shp")
    sim.createClusteredPointsShp(2, 100, 0.1, "clustered_100_10p.shp", "clustered_100_10p_meta.shp") 

########NEW FILE########
__FILENAME__ = test_access
"""access unittest"""
import unittest
import access as pyacc

class Access_Tester(unittest.TestCase):

    def setUp(self):
        self.distances = {1:[1,2,3,4],2:[1,1,2,3],3:[2,1,1,2],
                          4:[3,2,1,1],5:[4,3,2,1]}

    def test_coverage(self):
        coverage = []
        for d in self.distances.values():
            coverage.append(pyacc.coverage(d, 2.5))
        self.assertEqual(coverage, [2,3,4,3,2])

    def test_equity(self):
        equity = []
        for d in self.distances.values():
            equity.append(pyacc.equity(d))
        self.assertEqual(equity, [1,1,1,1,1])

    def test_potential_entropy(self):
        entropy = []
        for d in self.distances.values():
            entropy.append(pyacc.potential_entropy(d))
        entropy_values = [0.57131743166465321, 0.92088123394736132, 
                1.0064294488161101, 0.92088123394736132, 0.57131743166465321]
        self.assertEqual(entropy, entropy_values)

    def test_potential_gravity(self):
        gravity = []
        for d in self.distances.values():
            gravity.append(pyacc.potential_gravity(d))
        gravity_values = [1.4236111111111112, 2.3611111111111112, 2.5, 
                          2.3611111111111112, 1.4236111111111112]
        self.assertEqual(gravity, gravity_values)

    def test_travel_cost(self):
        cost = []
        for d in self.distances.values():
            cost.append(pyacc.travel_cost(d))
        self.assertEqual(cost, [10, 7, 6, 7, 10])

suite = unittest.TestSuite()
test_classes = [Access_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_kernel
"""network unittest"""
import unittest
import network as pynet
import kernel as pykernel

class Kernel_Tester(unittest.TestCase):

    def setUp(self):
        self.G = {(1,1): {(2,2): 0.125, (3,3): 0.75}, (2,2): {(1,1): 0.125, (4,4): 1.2},
                   (3,3): {(1,1): 0.75, (4,4): 0.375},
                   (4,4): {(2,2): 1.2, (3,3): 0.375, (5,5): 0.5},
                   (5,5): {(4,4): 0.5}, (6,6):{(7,7):1.0}, (7,7):{(6,6):1.0}}
        self.G_meshed = pynet.mesh_network(self.G, 0.1)
        self.points = [((3.6666666666666665, 3.6666666666666665), (3.5, 3.5), 
                         1.8011569244041523e-18, 8.0119209423694433e-18), 
                       ((4.0, 4.0), (3.8333333333333335, 3.8333333333333335), 
                         6.4354219496947843e-18, 1.3190733783852405e-17), 
                       ((6.5999999999999996, 6.5999999999999996), (6.5, 6.5), 
                         8.6525456558003033e-19, 4.0412843678067672e-18)]
        self.proj_points = []
        for p in self.points:
            self.proj_points.append(pynet.proj_pnt_coor(p))

    def test_dijkstras_w_prev(self):
        distances, previous_nodes = pykernel.dijkstras_w_prev(self.G, (1,1))
        distances_values = {(5, 5): 1.625, (3, 3): 0.75, (4, 4): 1.125, (1, 1): 0, (2, 2): 0.125}
        prev_nodes = {(5, 5): (4, 4), (2, 2): (1, 1), (1, 1): None, (4, 4): (3, 3), (3, 3): (1, 1)}
        self.assertEqual(distances, distances_values)
        self.assertEqual(previous_nodes, prev_nodes)

    def test_kernel_density(self):
        density = pykernel.kernel_density(self.G_meshed, self.proj_points, 0.3, self.G.keys())
        self.assertEqual(density[(4.0, 4.0)], 0.25)


suite = unittest.TestSuite()
test_classes = [Kernel_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_kfuncs
"""network unittest"""
import unittest
import network as pynet
import kfuncs

class Kfuncs_Tester(unittest.TestCase):

    def setUp(self):
        self.distances = {1:[1,2,3,4],2:[1,1,2,3],3:[2,1,1,2],
                          4:[3,2,1,1],5:[4,3,2,1]}

    def test__fxrange(self):
        values = kfuncs._fxrange(0.0,1.0,0.2)
        for v1, v2 in zip(values, [0.0,0.2,0.4,0.6,0.8,1.0]):
            self.assertAlmostEqual(v1, v2)

    def test__binary_search(self):
        v = kfuncs._binary_search([0.0,0.2,0.4,0.6,0.8,1.0],0.9)
        self.assertEqual(v, 5)

    def test_kt_values(self):
        expected_values = {1: {0.5: 0, 1.5: 10, 2.5: 20}, 
                           2: {0.5: 0, 1.5: 20, 2.5: 30}, 
                           3: {0.5: 0, 1.5: 20, 2.5: 40}, 
                           4: {0.5: 0, 1.5: 20, 2.5: 30}, 
                           5: {0.5: 0, 1.5: 10, 2.5: 20}}
        kfunc_values = {}
        for k, v in self.distances.items():
            kfunc_values[k] = kfuncs.kt_values((0.5,3.5,1.0),v,10)
        self.assertEqual(kfunc_values, expected_values)

suite = unittest.TestSuite()
test_classes = [Kfuncs_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_klincs
"""network unittest"""
import unittest
import network as pynet
import klincs 
import random
random.seed(10)
import pysal
import numpy as np

class KLINCS_Tester(unittest.TestCase):

    def setUp(self):
        self.population = range(5)
        self.weights = [0.1, 0.25, 0.1, 0.2, 0.35]
        np.random.seed(10)
        self.network_file = 'streets.shp'
        self.G = pynet.read_network(self.network_file)
        self.references = [[i, [n]] for i, n in enumerate(self.G.keys())]
        self.scale_set = (0, 1500, 500) 
        self.network_distances_cache = {}
        search_radius = self.scale_set[1]
        for i, node in self.references:
            n = node[0]
            self.network_distances_cache[n] = pynet.dijkstras(self.G, n, search_radius) 
        self.snapper = pynet.Snapper(self.G)
        self.events_file = 'crimes.shp' 
        points = self.get_points_from_shapefile(self.events_file)
        self.events = []
        for p in points:
            self.events.append(self.snapper.snap(p[0]))
        self.test_node = (724587.78057580709, 877802.4281426128)

    def get_points_from_shapefile(self, src_filename, src_uid=None):
        src_file = pysal.open(src_filename)
        dbf = pysal.open(src_filename[:-3] + 'dbf')
        src_uid_index = dbf.header.index(src_uid) if src_uid else None 
        if src_uid_index != None:
            def get_index(index, record):
                return record[src_uid_index]
        else:
            def get_index(index, record):
                return index
        if src_file.type == pysal.cg.shapes.Polygon:
            def get_geom(g):
                return g.centroid
        elif src_file.type == pysal.cg.shapes.Point:
            def get_geom(g):
                return (g[0], g[1])
        srcs = [] 
        for i, rec in enumerate(src_file):
            srcs.append([get_geom(rec), get_index(i, dbf.next())]) 
        src_file.close()
        return srcs 

    def test_WeightedRandomSampleGenerator(self):
        generator = klincs.WeightedRandomSampleGenerator(self.weights, self.population, 3)
        sample = generator.next()
        self.assertEqual(sample,[4, 0, 3])

    def test_RandomSampleGenerator(self):
        generator = klincs.RandomSampleGenerator(self.population, 3)
        sample = generator.next()
        self.assertEqual(sample,[2, 1, 3])

    def test_local_k(self):
        network = self.G
        references = self.references
        events = self.events
        scale_set = self.scale_set 
        cache = self.network_distances_cache
        node2localK, net_distances = klincs.local_k(network, events, references, scale_set, cache)
        # node2localK
        # for each reference node,
        # local_k returns the number of events within a distance 
        # the distance is determined by scale_set
        # example: (724587.78057580709, 877802.4281426128): {0: 0, 1000: 22, 500: 9}
        # 
        # net_distances - a dictionary containing network distances 
        # between nodes in the input network
        test_node = self.test_node
        self.assertEqual(node2localK[test_node][500], 9)

    def test_cluster_type(self):
        cluster1 = klincs.cluster_type(0.25, 0.01, 0.33)
        cluster2 = klincs.cluster_type(0.45, 0.01, 0.33)
        self.assertEqual(cluster1, 0)
        self.assertEqual(cluster2, 1)

    def test_simulate_local_k_01(self):
        sims = 1
        n = len(self.events)
        net_file = self.network_file
        network = self.G
        events = self.events
        refs = self.references
        scale_set = self.scale_set
        cache = self.network_distances_cache
        args = (sims, n, net_file, network, events, refs, scale_set, cache)
        sim_outcomes = klincs.simulate_local_k_01(args)
        self.assertEqual(sim_outcomes[0][self.test_node], {0: 0, 1000: 9, 500: 4})

    def test_simulate_local_k_02(self):
        sims = 1
        n = 50
        refs = self.references
        scale_set = self.scale_set
        cache = self.network_distances_cache
        args = (sims, n, refs, scale_set, cache)
        sim_outcomes = klincs.simulate_local_k_02(args)
        self.assertEqual(sim_outcomes[0][self.test_node], {0: 1, 1000: 2, 500: 2})
          
    def test_k_cluster(self):
        network = self.G
        events = self.events
        refs = self.references
        scale_set = self.scale_set
        sims = 1
        sim_network = self.network_file
        localKs = klincs.k_cluster(network, events, refs, scale_set, sims, sim_network=sim_network)
        test_node = self.test_node
        # in [9,4,4,1]
        # 9 - the number of events observed at the search radius of 500
        # 4 - the lowest number of simulated points observed at the search radius of 500
        # 4 - the highest number of simulated points observed at the search radius of 500
        # 1 - the type of cluster
        self.assertEqual(localKs[test_node][500],[9,4,4,1])

suite = unittest.TestSuite()
test_classes = [KLINCS_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_lincs
"""network unittest"""
import unittest
import network as pynet
import lincs 
import random
random.seed(10)
import pysal
import numpy as np
import copy

class LINCS_Tester(unittest.TestCase):

    def setUp(self):
        self.base = np.array([100, 80, 50, 120, 90])
        self.events = np.array([20, 20, 5, 10, 25])
        np.random.seed(10)
        self.observed = np.array([0.1,0.15,0.2])
        self.simulated = np.array([[0.05,0.10,0.25],[0.12,0.11,0.3],[0.11,0.09,0.27]])
        self.network_file = 'streets.shp'
        self.G = pynet.read_network(self.network_file)
        self.test_link = ((724432.38723173144, 877800.08747069736), 
                          (724587.78057580709, 877802.4281426128))
        self.G2 = copy.deepcopy(self.G)
        done = set()
        for n1 in self.G2:
            for n2 in self.G2[n1]:
                if (n1, n2) in done:
                    continue
                dist = self.G2[n1][n2]
                base = int(random.random()*1000)
                event = int(random.random()*base)
                self.G2[n1][n2] = [dist, base, event]
                self.G2[n2][n1] = [dist, base, event]
                done.add((n1,n2))
                done.add((n2,n1))

    def test_unconditional_sim(self):
        simulated_events = lincs.unconditional_sim(self.events, self.base, 2)
        self.assertEqual(list(simulated_events[0]),[21,15])

    def test_unconditional_sim_poisson(self):
        simulated_events = lincs.unconditional_sim_poisson(self.events, self.base, 2)
        self.assertEqual(list(simulated_events[0]),[22,21])

    def test_conditional_multinomial(self):
        simulated_events = lincs.conditional_multinomial(self.events, self.base, 2)
        self.assertEqual(list(simulated_events[0]),[21,18])

    def test_pseudo_pvalues(self):
        pseudo_pvalues = lincs.pseudo_pvalues(self.observed, self.simulated)
        self.assertEqual(list(pseudo_pvalues[0]),[ 0.5,  0.5,  0.5])

    def test_node_weights(self):
        w, id2link = lincs.node_weights(self.G)
        self.assertEqual(w.neighbors[0],[1, 2, 3, 4])

    def test_edgepoints_from_network(self):
        id2linkpoints, id2attr, link2id = lincs.edgepoints_from_network(self.G)
        link = id2linkpoints[0]
        self.assertEqual(link[:2], self.test_link)
        self.assertEqual(link2id[link[:2]], 0)

    def test_dist_weights(self):
        id2linkpoints, id2attr, link2id = lincs.edgepoints_from_network(self.G)
        w, id2link = lincs.dist_weights(self.G, id2linkpoints, link2id, 500)
        self.assertEqual(w.neighbors[0],[1,154,153,155])
        self.assertEqual(id2link[0], self.test_link)

    def test_lincs(self):
        network = self.G2
        event_index = 2
        base_index = 1
        weight = 'Distance-based'
        dist = 500
        lisa_func = 'moran'
        sim_method = 'permutations'
        sim_num = 2
        lisa, w = lincs.lincs(network, event_index, base_index, weight, dist, lisa_func, sim_method, sim_num)
        self.assertEqual(lisa[0][3], -0.64342055427251854)

suite = unittest.TestSuite()
test_classes = [LINCS_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_network
"""network unittest"""
import unittest
import network as pynet
import random
random.seed(10)
import pysal

class Network_Tester(unittest.TestCase):

    def setUp(self):
        self.net = 'streets.shp'
        self.G = pynet.read_network(self.net)
        self.G2 = {(1,1): {(2,2): 0.125, (3,3): 0.75}, (2,2): {(1,1): 0.125, (4,4): 1.2},
                   (3,3): {(1,1): 0.75, (4,4): 0.375},
                   (4,4): {(2,2): 1.2, (3,3): 0.375, (5,5): 0.5},
                   (5,5): {(4,4): 0.5}, (6,6):{(7,7):1.0}, (7,7):{(6,6):1.0}}
        self.GDirected = {(1,1): {(2,2): 0.125, (3,3): 0.75}, (2,2): {(1,1): 0.125},
                   (3,3): {(1,1): 0.75, (4,4): 0.375},
                   (4,4): {(2,2): 1.2, (3,3): 0.375, (5,5): 0.5},
                   (5,5): {(4,4): 0.5}, (6,6):{(7,7):1.0}, (7,7):{(6,6):1.0}}
        self.points = [((4,4), (2,2), 0.51466686561013752, 0.68533313438986243), 
                       ((4,4), (3,3), 0.077286837052313151, 0.29771316294768685), 
                       ((6,6), (7,7), 0.82358887253344548, 0.17641112746655452)]

    def test_no_nodes(self):
        self.assertEqual(pynet.no_nodes(self.G), 230)

    def test_no_edges(self):
        self.assertEqual(pynet.no_edges(self.G), 303)

    def test_tot_net_length(self):
        self.assertAlmostEqual(pynet.tot_net_length(self.G), 52207.04600797734, places=1)

    def test_walk(self):
        correct_path = {(5, 5): {(4, 4): 0.5}, (2, 2): {(4, 4): 1.2, (1, 1): 0.125}, 
                        (1, 1): {(3, 3): 0.75, (2, 2): 0.125}, 
                        (4, 4): {(5, 5): 0.5, (3, 3): 0.375, (2, 2): 1.2}, 
                        (3, 3): {(4, 4): 0.375, (1, 1): 0.75}}
        traversal_path = pynet.walk(self.G2, (1,1))
        self.assertEqual(traversal_path, correct_path)        

    def test_components(self):
        components = pynet.components(self.G)
        self.assertEqual(pynet.no_nodes(components[0]), 230)        
        components = pynet.components(self.G2)
        self.assertEqual(len(components), 2)        

    def test_no_components(self):
        no_components = pynet.no_components(self.G2)
        self.assertEqual(no_components, 2)        

    def test_net_global_stats(self):
        stats = ['v', 'e', 'L', 'p', 'u', 'alpha', 'beta', 'emax', 'gamma', 'eta', 
                 'net_den', 'detour']
        values = pynet.net_global_stats(self.G, detour=True)
        values = dict(zip(stats, values))
        self.assertEqual(values['v'], 230)
        self.assertEqual(values['e'], 303)
        self.assertAlmostEqual(values['L'], 52207.04600797734, places=1)
        self.assertAlmostEqual(values['eta'], 172.30048187451268)
        self.assertAlmostEqual(values['beta'], 1.317391304347826, places=2)
        self.assertAlmostEqual(values['emax'], 684)
        self.assertAlmostEqual(values['gamma'], 0.44298245614035087, places=2)
        self.assertAlmostEqual(values['detour'], 0.78002937059822186, places=4)
        for k in values:
            print k, values[k]

    def test_random_projs(self):
        random.seed(10)
        random_points = pynet.random_projs(self.G2, 3)
        points = [((7, 7), (6, 6), 0.42888905467511462, 0.57111094532488538), 
                  ((7, 7), (6, 6), 0.20609823213950174, 0.79390176786049826), 
                  ((1, 1), (3, 3), 0.61769165440008411, 0.13230834559991589)] 
        self.assertEqual(random_points, points)
        
    def test_proj_distances_undirected(self):
        source = self.points[0]
        destinations = self.points[1:]
        distances = pynet.proj_distances_undirected(self.G2, source, destinations, r=1.0)
        self.assertAlmostEqual(distances.values()[0], 0.59195370266245062)

    def test_proj_distances_directed(self):
        source = self.points[0]
        destinations = self.points
        distances = pynet.proj_distances_directed(self.G2, source, destinations)
        distance_values = [0.0, 1.9626199714421757]
        self.assertEqual(distances.values(), distance_values)

    def test_dijkstras(self):
        distances = pynet.dijkstras(self.G2, (1,1))
        distance_values = {(5, 5): 1.625, (2, 2): 0.125, (1, 1): 0, (4, 4): 1.125, (3, 3): 0.75}
        self.assertEqual(distances, distance_values)

    def test_snap(self):
        snapper = pynet.Snapper(self.G2)
        projected_point = snapper.snap((2.5,2.5))
        self.assertEqual(projected_point, ((2, 2),(4, 4),3.9252311467094367e-17,1.1775693440128314e-16))

    def test_network_from_endnodes(self):
        shape = pysal.open(self.net)
        dbf = pysal.open(self.net[:-3] + 'dbf')
        def weight(geo_object, record):
            return 1
        graph = pynet.network_from_endnodes(shape, dbf, weight)
        neighbors = {(724432.38723173144, 877800.08747069736): 1, 
                     (725247.70571468933, 877812.36851842562): 1}
        start_point = (724587.78057580709, 877802.4281426128)
        self.assertEqual(graph[start_point], neighbors)

    def test_network_from_allvertices(self):
        shape = pysal.open(self.net)
        dbf = pysal.open(self.net[:-3] + 'dbf')
        graph = pynet.network_from_allvertices(shape, dbf)
        neighbors = {(724432.38723173144, 877800.08747069736): 155.41097171058956, 
                     (725247.70571468933, 877812.36851842562): 660.00000000003809}
        start_point = (724587.78057580709, 877802.4281426128)
        self.assertEqual(graph[start_point], neighbors)

    def test_read_hierarchical_network(self):
        shape = pysal.open(self.net)
        dbf = pysal.open(self.net[:-3] + 'dbf')
        graph_detail, graph_endnode, link = pynet.read_hierarchical_network(shape, dbf)
        neighbors = {(725220.77363919443, 880985.09708712087): 659.99999999994725, 
                     (724400.64597190416, 880984.45593258401): 160.12791790928244}
        vertex1 = (724560.77384088072, 880984.58111639845)
        vertex2 = (724400.64597190416, 880984.45593258401)
        self.assertEqual(graph_detail[vertex1], neighbors)
        self.assertEqual(graph_endnode[vertex1], neighbors)
        self.assertEqual(link[(vertex1, vertex2)][0], (vertex1, vertex2))

    def test_read_network(self):
        graph = pynet.read_network(self.net)
        self.assertEqual(graph, self.G) 

    def test_proj_pnt_coor(self):
        point = pynet.proj_pnt_coor(self.points[0])
        point_value = (3.6360755692750462, 3.6360755692750462)
        self.assertEqual(point, point_value)

    def test_inject_points(self):
        graph, point_coors = pynet.inject_points(self.G2, self.points)
        self.assertEqual(len(graph), len(self.G2) + 3)

    def test_mesh_network(self):
        graph = pynet.mesh_network(self.G2, 0.1)
        self.assertEqual(len(graph), 41)

    def test_write_network_to_pysalshp(self):
        out = 'output_network.shp'
        pynet.write_network_to_pysalshp(self.G, out)
        graph = pynet.read_network(out)
        self.assertEqual(graph, self.G)    

    def test_write_valued_network_to_shp(self):
        out = 'output_network.shp'
        fields = ['WEIGHT1','WEIGHT2']
        types = [('N',7,3),('N',7,3)]
        values = {(1,1):1,(2,2):2,(3,3):3,(4,4):4,(5,5):5,(6,6):6,(7,7):7}
        def doubleX(values, node):
            return values[node]*2.0
        pynet.write_valued_network_to_shp(out,fields,types,self.G2,values,doubleX)
        graph = pynet.read_network(out)
        self.assertEqual(len(graph), len(self.G2))    

    def test_list_network_to_shp(self):
        out = 'output_network.shp'
        fields = ['WEIGHT']
        types = [('N',7,3)]
        list_network = []
        for node1 in self.G2:
            for node2 in self.G2[node1]:
                list_network.append(((node1, node2),random.random()))
        pynet.write_list_network_to_shp(out,fields,types,list_network)
        graph = pynet.read_network(out)
        self.assertEqual(len(graph), len(self.G2))    


suite = unittest.TestSuite()
test_classes = [Network_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_weights
"""network unittest"""
import unittest
import network as pynet
import numpy as np
import weights 

class Weights_Tester(unittest.TestCase):

    def test_dist_weights(self):
        ids = np.array(map(str,range(1,9)))
        w = weights.dist_weights('distances.csv','knn',ids,3)
        self.assertEqual(w.neighbors['1'],['6','8','7'])

suite = unittest.TestSuite()
test_classes = [Weights_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = weights
"""
A library of spatial network functions.
Not to be used without permission.

Contact: 

Andrew Winslow
GeoDa Center for Geospatial Analysis
Arizona State University
Tempe, AZ
Andrew.Winslow@asu.edu
"""

import csv
import numpy as np
from pysal import W
import unittest
import test

def dist_weights(distfile, weight_type, ids, cutoff, inverse=False):
    """
    Returns a distance-based weights object using user-defined options
    
    Parameters
    ----------
    distfile: string, a path to distance csv file
    weighttype: string, either 'threshold' or 'knn'
    ids: a numpy array of id values
    cutoff: float or integer; float for 'threshold' weight type and integer for knn type
    inverse: boolean; true if inversed weights required

    """
    try:
        data_csv = csv.reader(open(distfile))        
        if csv.Sniffer().has_header(distfile):
            data_csv.next()
    except:        
        data_csv = None
    
    if weight_type == 'threshold':
        def neighbor_func(dists, threshold):
            dists = filter(lambda x: x[0] <= threshold, dists)
            return dists
    else:
        def neighbor_func(dists, k):
            dists.sort()
            return dists[:k]

    if inverse:
        def weight_func(dists, alpha=-1.0):
            return list((np.array(dists)**alpha).round(decimals=6))
    else:
        def weight_func(dists, binary=False):
            return [1]*len(dists)

    dist_src = {}
    for row in data_csv:
        des = dist_src.setdefault(row[0], {})
        if row[0] != row[1]:
            des[row[1]] = float(row[2])

    neighbors, weights = {}, {}
    for id_val in ids:
        if id_val not in dist_src:
            raise ValueError, 'An ID value doest not exist in distance file'
        else:
            dists = zip(dist_src[id_val].values(), dist_src[id_val].keys())
        ngh, wgt = [], []
        if len(dists) > 0:
            nghs = neighbor_func(dists, cutoff)
            for d, i in nghs:
                ngh.append(i)
                wgt.append(d)
        neighbors[id_val] = ngh
        weights[id_val] = weight_func(wgt)
    w = W(neighbors, weights)
    w.id_order = ids
    return w



########NEW FILE########
__FILENAME__ = shapely_ext
import shapely
import shapely.geometry
import shapely.ops
_basegeom = shapely.geometry.base.BaseGeometry
import pysal
__all__ = ["to_wkb", "to_wkt", "area", "distance", "length", "boundary", "bounds", "centroid", "representative_point", "convex_hull", "envelope", "buffer", "simplify", "difference", "intersection", "symmetric_difference", "union", "unary_union", "cascaded_union", "has_z", "is_empty", "is_ring", "is_simple", "is_valid", "relate", "contains", "crosses", "disjoint", "equals", "intersects", "overlaps", "touches", "within", "equals_exact", "almost_equals", "project", "interpolate"]


def to_wkb(shape):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    return o.to_wkb()

def to_wkt(shape):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    return o.to_wkt()

# Real-valued properties and methods
# ----------------------------------
def area(shape):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    return o.area

def distance(shape, other):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    if not hasattr(other,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    o2 = shapely.geometry.asShape(other)
    return o.distance(o2)

def length(shape):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    return o.length

# Topological properties
# ----------------------
def boundary(shape):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    res = o.bondary
    return pysal.cg.shapes.asShape(res)

def bounds(shape):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    return o.bounds

def centroid(shape):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    res = o.centroid
    return pysal.cg.shapes.asShape(res)

def representative_point(shape):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    res = o.representative_point
    return pysal.cg.shapes.asShape(res)

def convex_hull(shape):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    res = o.convex_hull
    return pysal.cg.shapes.asShape(res)

def envelope(shape):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    res = o.envelope
    return pysal.cg.shapes.asShape(res)

def buffer(shape, radius, resolution=16):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    res = o.buffer(radius, resolution)
    return pysal.cg.shapes.asShape(res)

def simplify(shape, tolerance, preserve_topology=True):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    res = o.simplify(tolerance, preserve_topology)
    return pysal.cg.shapes.asShape(res)
    
# Binary operations
# -----------------
def difference(shape, other):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    if not hasattr(other,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    o2 = shapely.geometry.asShape(other)
    res = o.difference(o2)
    return pysal.cg.shapes.asShape(res)

def intersection(shape, other):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    if not hasattr(other,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    o2 = shapely.geometry.asShape(other)
    res = o.intersection(o2)
    return pysal.cg.shapes.asShape(res)

def symmetric_difference(shape, other):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    if not hasattr(other,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    o2 = shapely.geometry.asShape(other)
    res = o.symmetric_difference(o2)
    return pysal.cg.shapes.asShape(res)

def union(shape, other):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    if not hasattr(other,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    o2 = shapely.geometry.asShape(other)
    res = o.union(o2)
    return pysal.cg.shapes.asShape(res)

def cascaded_union(shapes):
    o = []
    for shape in shapes:
        if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
        o.append(shapely.geometry.asShape(shape))
    res = shapely.ops.cascaded_union(o)
    return pysal.cg.shapes.asShape(res)

def unary_union(shapes):
    # seems to be the same as cascade_union except that it handles multipart polygons
    if shapely.__version__ < '1.2.16':
        raise Exception, "shapely 1.2.16 or higher needed for unary_union; upgrade shapely or try cascade_union instead"
    o = []
    for shape in shapes:
        if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
        o.append(shapely.geometry.asShape(shape))
    res = shapely.ops.unary_union(o)
    return pysal.cg.shapes.asShape(res)

# Unary predicates
# ----------------
def has_z(shape):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    return o.has_z

def is_empty(shape):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    return o.is_empty

def is_ring(shape):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    return o.is_ring

def is_simple(shape):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    return o.is_simple

def is_valid(shape):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    return o.is_valid

# Binary predicates
# -----------------
def relate(shape, other):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    if not hasattr(other,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    o2 = shapely.geometry.asShape(other)
    return o.relate(o2)

def contains(shape, other):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    if not hasattr(other,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    o2 = shapely.geometry.asShape(other)
    return o.contains(o2)

def crosses(shape, other):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    if not hasattr(other,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    o2 = shapely.geometry.asShape(other)
    return o.crosses(o2)

def disjoint(shape, other):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    if not hasattr(other,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    o2 = shapely.geometry.asShape(other)
    return o.disjoint(o2)

def equals(shape, other):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    if not hasattr(other,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    o2 = shapely.geometry.asShape(other)
    return o.equals(o2)

def intersects(shape, other):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    if not hasattr(other,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    o2 = shapely.geometry.asShape(other)
    return o.intersects(o2)

def overlaps(shape, other):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    if not hasattr(other,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    o2 = shapely.geometry.asShape(other)
    return o.overlaps(o2)

def touches(shape, other):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    if not hasattr(other,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    o2 = shapely.geometry.asShape(other)
    return o.touches(o2)

def within(shape, other):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    if not hasattr(other,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    o2 = shapely.geometry.asShape(other)
    return o.within(o2)

def equals_exact(shape, other, tolerance):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    if not hasattr(other,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    o2 = shapely.geometry.asShape(other)
    return o.equals_exact(o2, tolerance)

def almost_equals(shape, other, decimal=6):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    if not hasattr(other,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    o2 = shapely.geometry.asShape(other)
    return o.almost_equals(o2, decimal)

# Linear referencing
# ------------------

def project(shape, other, normalized=False):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    if not hasattr(other,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    o2 = shapely.geometry.asShape(other)
    return o.project(o2, normalized)

def interpolate(shape, distance, normalized=False):
    if not hasattr(shape,'__geo_interface__'): raise TypeError, "%r does not appear to be a shape"%shape
    o = shapely.geometry.asShape(shape)
    res = o.interpolate(distance, normalized)
    return pysal.cg.shapes.asShape(res)
    

# Copy doc strings from shapely
for method in __all__:
    if hasattr(_basegeom, method):
        locals()[method].__doc__ = getattr(_basegeom,method).__doc__

if __name__=='__main__':
    #step 0, create 2 points
    pt1 = pysal.cg.shapes.Point((0,0))
    pt2 = pysal.cg.shapes.Point((10,10))
    o = pysal.open('step0.shp','w')
    o.write(pt1)
    o.write(pt2)
    o.close()

    #step 1, buffer 2 points
    b1 = buffer(pt1,10)
    b2 = buffer(pt2,10)
    o = pysal.open('step1.shp','w')
    o.write(b1)
    o.write(b2)
    o.close()

    #step 2, intersect 2 buffers
    i = intersection(b1,b2)
    o = pysal.open('step2.shp','w')
    o.write(i)
    o.close()
    
    #step 3, union 2 buffers
    u = union(b1, b2)
    o = pysal.open('step3.shp','w')
    o.write(u)
    o.close()
    
    #step 4, find convex_hull of union
    c = convex_hull(u)
    o = pysal.open('step4.shp','w')
    o.write(c)
    o.close()

########NEW FILE########
__FILENAME__ = shared_perimeter_weights
"""
shared_perimeter_weights -- calculate shared perimeters weights....

wij = l_ij/P_i
wji = l_ij/P_j

l_ij = length of shared border i and j
P_j = perimeter of j

"""
__author__ = "Charles R Schmidt <schmidtc@gmail.com>"
__all__ = ["spw_from_shapefile"]


import pysal
import shapely.geometry

def spw_from_shapefile(shapefile, idVariable=None):
    polygons = pysal.open(shapefile,'r').read()
    polygons = map(shapely.geometry.asShape,polygons)
    perimeters = [p.length for p in polygons]
    Wsrc = pysal.rook_from_shapefile(shapefile)
    new_weights = {}
    for i in Wsrc.neighbors:
        a = polygons[i]
        p = perimeters[i]
        new_weights[i] = [a.intersection(polygons[j]).length/p for j in Wsrc.neighbors[i]]
    return pysal.W(Wsrc.neighbors,new_weights)

if __name__=='__main__':
    fname = pysal.examples.get_path('stl_hom.shp')
    W = spw_from_shapefile(fname)


########NEW FILE########
__FILENAME__ = cleanNetShp
"""
cleanNetShp -- Tools to clean spatial Network Shapefiles.
"""
import pysal
import numpy

__author__ = "Charles R. Schmidt <schmidtc@gmail.com>"
__all__ = ['snap_verts', 'find_nodes', 'split_at_nodes']


def snap_verts(shp,tolerance=0.001,arc=True):
    """
    snap_verts -- Snap verts that are within tolerance meters of each other.

    Description -- Snapping should be performed with a very small tolerance.
                   The goal is not to change the network, but to ensure rounding
                   errors don't prevent edges from being split at proper intersections.
                   The default of 1mm should be adequate if the input is of decent quality.
                   Higher snapping values can be used to correct digitizing errors, but care
                   should be taken.

    Arguments
    ---------
    tolerance -- float -- snapping tolerance in meters
    arc -- bool -- If true, Ard Distance will be used instead of Euclidean

    Returns
    -------
    generator -- each element is a new pysal.cg.Chain with corrected vertices.
    """
    kmtol = tolerance/1000.

    data = numpy.concatenate([rec.vertices for rec in shp])
    
    if arc:
        kd = pysal.cg.KDTree(data,distance_metric="Arc",radius = pysal.cg.sphere.RADIUS_EARTH_KM)
    else:
        kd = pysal.cg.KDTree(data)
    q = kd.query_ball_tree(kd,kmtol)
    ### Next three lines assert that snappings are mutual... if 1 snaps to 8, 8 must snap to 1.
    for r,a in enumerate(q):
        for o in a:
            assert a==q[o]
    ### non-mutual snapping can happen.
    ### consider the three points, A (-1,0), B (0,0), C (1,0) and a snapping tolerance of 1.
    ### A-> B
    ### B-> A,C
    ### C-> B
    ### For now, try lowering adjusting the tolerance to avoid this.

    data2 = numpy.empty_like(data)
    for i,r in enumerate(q):
        data2[i] = data[r].mean(0)
    pos=0
    for rec in shp:
        vrts = rec.vertices
        n = len(vrts)
        nrec = pysal.cg.Chain(map(tuple,data2[pos:pos+n]))
        pos+=n
        yield nrec
    
def find_nodes(shp):
    """
    find_nodes -- Finds vertices in a line type shapefile that appear more than once and/or are end points of a line

    Arguments
    ---------
    shp -- Shapefile Object -- Should be of type Line.

    Returns
    -------
    set
    """
    node_count = {}
    for road in shp:
        vrts = road.vertices
        for node in vrts:
            if node not in node_count:
                node_count[node] = 0
            node_count[node] += 1
        node_count[vrts[0]] += 1
        node_count[vrts[-1]] += 1
    return set([node for node,c in node_count.iteritems() if c > 1])

def split_at_nodes(shp):
    """
    split_at_nodes -- Split line features at nodes

    Arguments
    ---------
    shp -- list or shapefile -- Chain features to be split at common nodes.

    Returns
    -------
    generator -- yields pysal.cg.Chain objects
    """
    nodes = find_nodes(shp)
    nodeIds = list(nodes)
    nodeIds.sort()
    nodeIds = dict([(node,i) for i,node in enumerate(nodeIds)])
    
    for road in shp:
        vrts = road.vertices
        midVrts = set(road.vertices[1:-1]) #we know end points are nodes
        midNodes = midVrts.intersection(nodes) # find any nodes in the middle of the feature.
        midIdx = [vrts.index(node) for node in midNodes] # Get their indices
        midIdx.sort()
        if midIdx:
            #print vrts
            starts = [0]+midIdx
            stops = [x+1 for x in midIdx]+[None]
            for start,stop in zip(starts,stops):
                feat = pysal.cg.Chain(vrts[start:stop])
                rec = (nodeIds[feat.vertices[0]],nodeIds[feat.vertices[-1]],False)
                yield feat,rec
        else:
            rec = (nodeIds[road.vertices[0]],nodeIds[road.vertices[-1]],False)
            yield road,rec


def createSpatialNetworkShapefile(inshp,outshp):
    assert inshp.lower().endswith('.shp')
    assert outshp.lower().endswith('.shp')
    shp = pysal.open(inshp,'r')
    snapped = list(snap_verts(shp,.001))
    o = pysal.open(outshp,'w')
    odb = pysal.open(outshp[:-4]+'.dbf','w')
    odb.header = ["FNODE","TNODE","ONEWAY"]
    odb.field_spec = [('N',20,0),('N',20,0),('L',1,0)]

    new = list(split_at_nodes(snapped))
    for feat,rec in new:
        o.write(feat)
        odb.write(rec)
    o.close()
    odb.close()
    print "Split %d roads in %d network edges"%(len(shp),len(new))

if __name__=='__main__':
    createSpatialNetworkShapefile('beth_roads.shp','beth_network.shp')


########NEW FILE########
__FILENAME__ = spatialnet
import pysal
from pysal.cg.segmentLocator import Polyline_Shapefile_SegmentLocator
import networkx

EUCLIDEAN_DISTANCE = "Euclidean"
ARC_DISTANCE = "Arc"

class SpatialNetwork(object):
    """
    SpatialNetwork -- Represents a Spatial Network. 

    A Spatial Network in PySAL is a graph who's nodes and edges are represented by geographic features 
    such as Points for nodes and Lines for edges.
    An example of a spatial network is a road network.

    Arguments
    ---------
    shapefile -- Shapefile contains the geographic represention of the network.
                 The shapefile must be a polyline type and the associated DBF MUST contain the following fields:
                    FNODE -- source node -- ID of the source node, the first vertex in the polyline feature.
                    TNODE -- destination node -- ID of the destination node, the last vertex in the polyline feature.
                    ONEWAY -- bool -- If True, the edge will be marked oneway starting at FNODE and ending at TNODE
    distance_metric -- EUCLIDEAN_DISTANCE or ARC_DISTANCE
    
    """
    def __init__(self,shapefile,distance_metric=EUCLIDEAN_DISTANCE):
        if issubclass(type(shapefile),basestring): #Path
            self.shp = shp = pysal.open(shapefile,'r')
        else:
            raise TypeError,"Expecting a string, shapefile should the path to shapefile"
        if shp.type != pysal.cg.shapes.Chain:
            raise ValueError,"Shapefile must contain polyline features"
        self.dbf = dbf = pysal.open(shapefile[:-4]+'.dbf','r')
        header = dbf.header
        if (('FNODE' not in header) or ('TNODE' not in header) or ('ONEWAY' not in header)):
            raise ValueError,"DBF must contain: FNODE,TNODE,ONEWAY"
        
        oneway = [{'F':False,'T':True}[x] for x in dbf.by_col('ONEWAY')]
        fnode = dbf.by_col('FNODE')
        tnode = dbf.by_col('TNODE')
        if distance_metric == EUCLIDEAN_DISTANCE:
            lengths = [x.len for x in shp]
        elif distance_metric == ARC_DISTANCE:
            lengths = [x.arclen for x in shp]
        else:
            raise ValueError,"distance_metric must be either EUCLIDEAN_DISTANCE or ARC_DISTANCE"
        self.lengths = lengths
        if any(oneway):
            G = networkx.MultiDiGraph()
            #def isoneway(x):
            #    return x[-1]
            #def isnotoneway(x):
            #    if(x[-1]):
            #        return False
            #    return True
            #    
            #A = filter(isoneway,zip(fnode,tnode,oneway))
            #B = filter(isnotoneway,zip(fnode,tnode,oneway))
            #C = filter(isnotoneway,zip(tnode,fnode,oneway))
            #G.add_edges_from(A)
            #G.add_edges_from(B)
            #G.add_edges_from(C)
        else:
            G = networkx.MultiGraph()
        #zip(fnode,tnode))
        self.G = G
        shp.seek(0)
        self._locator = Polyline_Shapefile_SegmentLocator(shp)
    def snap(self,pt):
        i,p,j = self._locator.nearest(pt) #shpID,partID,segmentID
        segment = self.shp[i].segments[p][j] #grab segment
        d,pct = pysal.cg.get_segment_point_dist(segment,pt) #find pct along segment
        x0,x1 = segment.p1[0],segment.p2[0]
        x2 = x0 + (x1-x0)*pct # find x location of snap
        y2 = segment.line.y(x2) # find y location of snap

        #dbf = self.dbf
        #rec = dict(zip(dbf.header,dbf[i][0]))
        #edge = (rec['FNODE'],rec['TNODE'])
        #TODO: Calculate location along edge and distance to edge"
        #return edge
        return x2,y2
        
    
if __name__=='__main__':
    import random
    net = SpatialNetwork('beth_network.shp',ARC_DISTANCE)

    n = 1000
    minX,minY,maxX,maxY = net.shp.bbox
    xRange = maxX-minX
    yRange = maxY-minY
    qpts = [(random.random(), random.random()) for i in xrange(n)]
    qpts = [pysal.cg.Point((minX+(xRange*x),minY+(yRange*y))) for x,y in qpts]
    o = pysal.open('random_qpts.shp','w')
    for p in qpts:
        o.write(p)
    o.close()
    o = pysal.open('random_qpts_snapped.shp','w')
    for qpt in qpts:
        spt = net.snap(qpt)
        o.write(pysal.cg.Chain([qpt,spt]))
    o.close()
    




########NEW FILE########
__FILENAME__ = util
"""
Utility module for network contrib 
"""

import pysal as ps
import networkx as nx
import numpy as np

__author__ = "Serge Rey <sjsrey@gmail.com>"

def w2dg(w):
    """
    Return a networkx directed graph from a PySAL W object


    Parameters
    ----------

    w: Weights 


    Returns
    -------
    G: A networkx directed graph


    Example
    ------

    >>> import networkx as nx
    >>> import pysal as ps
    >>> w = ps.lat2W()
    >>> guw = w2dg(w)
    >>> guw.in_degree()
    {0: 2, 1: 3, 2: 3, 3: 3, 4: 2, 5: 3, 6: 4, 7: 4, 8: 4, 9: 3, 10: 3, 11: 4, 12: 4, 13: 4, 14: 3, 15: 3, 16: 4, 17: 4, 18: 4, 19: 3, 20: 2, 21: 3, 22: 3, 23: 3, 24: 2}
    >>> dict([(k,len(w.neighbors[k])) for k in w.neighbors])
    {0: 2, 1: 3, 2: 3, 3: 3, 4: 2, 5: 3, 6: 4, 7: 4, 8: 4, 9: 3, 10: 3, 11: 4, 12: 4, 13: 4, 14: 3, 15: 3, 16: 4, 17: 4, 18: 4, 19: 3, 20: 2, 21: 3, 22: 3, 23: 3, 24: 2}
    >>> 
    """

    w_l = [(i,j) for i in w.neighbors for j in w[i]]
    G = nx.DiGraph()
    G.add_edges_from(w_l)
    return G

def w2dwg(w):
    """
    Return a directed, weighted graph from a PySAL W object


    Parameters
    ----------

    w: Weights 


    Returns
    -------
    G: A networkx directed, weighted graph


    Example
    -------
    >>> import networkx as nx
    >>> import pysal as ps
    >>> w = ps.lat2W()
    >>> w.transform = 'r'
    >>> gw = w2dwg(w)
    >>> gw.get_edge_data(0,1)
    {'weight': 0.5}
    >>> gw.get_edge_data(1,0)
    {'weight': 0.33333333333333331}
    """

    w_l = [(i,j,w[i][j]) for i in w.neighbors for j in w[i]]
    G = nx.DiGraph() # allow for asymmetries in weights
    G.add_weighted_edges_from(w_l)
    return G


def dwg2w(g, weight_name = 'weight'):
    """
    Returns a PySAL W object from a directed-weighted graph

    Parameters
    ----------

    g: networkx digraph

    weight_name: name of weight attribute of g

    Returns
    -------
    w: PySAL W 

    Example
    -------
    >>> w = ps.lat2W()
    >>> w.transform = 'r'
    >>> g = w2dwg(w)
    >>> w1 = dwg2w(g)
    >>> w1.n
    25
    >>> w1.neighbors[0]
    [1, 5]
    >>> w1.neighbors[1]
    [0, 2, 6]
    >>> w1.weights[0]
    [0.5, 0.5]
    >>> w1.weights[1]
    [0.33333333333333331, 0.33333333333333331, 0.33333333333333331]
    """

    neighbors = {}
    weights = {}
    for node in g.nodes_iter():
        neighbors[node] = []
        weights[node] = []
        for neighbor in g.neighbors_iter(node):
            neighbors[node].append(neighbor)
            weight = g.get_edge_data(node,neighbor)
            if weight:
                weights[node].append(weight[weight_name])
            else:
                weights[node].append(1)
    return ps.W(neighbors=neighbors, weights=weights)




def edge2w(edgelist, nodetype=str):
    """
    Create a PySAL W object from an edgelist

    Parameters
    ----------

    edge_file: file with edgelist

    nodetype: type for node (str, int, float)


    Returns
    -------
    W: PySAL W


    Example
    -------
    >>> lines = ["1 2", "2 3", "3 4", "4 5"]
    >>> w = edge2w(lines)
    >>> w.n
    5
    >>> w.neighbors["2"]
    ['1', '3']

    >>> w = edge2w(lines, nodetype=int)
    >>> w.neighbors[2]
    [1, 3]
    >>> lines = ["1 2 {'weight':1.0}", "2 3 {'weight':0.5}", "3 4 {'weight':3.0}"] 
    >>> w = edge2w(lines, nodetype=int)
    >>> w.neighbors[2]
    [1, 3]
    >>> w.weights[2]
    [1.0, 0.5]

    """
    G = nx.parse_edgelist(edgelist, nodetype=nodetype)
    return dwg2w(G)

    

def adjl2w(adjacency_list, nodetype=str):
    """
    Create a PySAL W object from an adjacency list file

    Parameters
    ----------

    adjacency_list: list of adjacencies
                    for directed graphs list only outgoing adjacencies

    nodetype: type for node (str, int, float)


    Returns
    -------
    W: PySAL W


    Example
    -------
    >>> al = [[1], [0,2], [1,3], [2]]
    >>> w = adjl2w(al)
    >>> w.n
    4
    >>> w.neighbors['0']
    ['1']
    >>> w = adjl2w(al, nodetype=int)
    >>> w.n
    4
    >>> w.neighbors[0]
    [1]


    """

    adjacency_list = [ map(nodetype, neighs) for neighs in adjacency_list]
    return ps.W(dict([(nodetype(i),neighs) for i,neighs in enumerate(adjacency_list)]))

                       

if __name__ == '__main__':
    import doctest
    doctest.testmod()







########NEW FILE########
__FILENAME__ = mapping
"""
Choropleth mapping using PySAL and Matplotlib

ToDo:
    * map_line_shp, map_point_shp should take a shp object not a shp_link
    * Same for map_poly_shp(_lonlat)

"""

__author__ = "Sergio Rey <sjsrey@gmail.com>", "Dani Arribas-Bel <daniel.arribas.bel@gmail.com"


import pandas as pd
import pysal as ps
import numpy as np
import  matplotlib.pyplot as plt
from matplotlib import colors as clrs
import matplotlib as mpl
from matplotlib.pyplot import fill, text
from matplotlib import cm
from matplotlib.patches import Polygon
from matplotlib.path import Path
from matplotlib.collections import LineCollection, PathCollection, PolyCollection, PathCollection, PatchCollection

def map_point_shp(shp, which='all'):
    '''
    Create a map object from a point shape
    ...

    Arguments
    ---------

    shp             : iterable
                      PySAL point iterable with the attribute `bbox` (e.g.
                      shape object from `ps.open` a poly shapefile)
    which           : str/list

    Returns
    -------

    map             : PatchCollection
                      Map object with the points from the shape

    '''
    pts = []
    if which == 'all':
        for pt in shp:
                pts.append(pt)
    else:
        for inwhich, pt in zip(which, shp):
            if inwhich:
                    pts.append(pt)
    pts = np.array(pts)
    sc = plt.scatter(pts[:, 0], pts[:, 1])
    _ = _add_axes2col(sc, shp.bbox)
    return sc

def map_line_shp(shp, which='all'):
    '''
    Create a map object from a line shape
    ...

    Arguments
    ---------

    shp             : iterable
                      PySAL line iterable with the attribute `bbox` (e.g.
                      shape object from `ps.open` a poly shapefile)
    which           : str/list

    Returns
    -------

    map             : PatchCollection
                      Map object with the lines from the shape
                      This includes the attribute `shp2dbf_row` with the
                      cardinality of every line to its row in the dbf
                      (zero-offset)

    '''
    patches = []
    rows = []
    i = 0
    if which == 'all':
        for shape in shp:
            for xy in shape.parts:
                patches.append(xy)
                rows.append(i)
            i += 1
    else:
        for inwhich, shape in zip(which, shp):
            if inwhich:
                for xy in shape.parts:
                    patches.append(xy)
                    rows.append(i)
                i += 1
    lc = LineCollection(patches)
    _ = _add_axes2col(lc, shp.bbox)
    lc.shp2dbf_row = rows
    return lc

def map_poly_shp(shp, which='all'):
    '''
    Create a map object from a polygon shape
    ...

    Arguments
    ---------

    shp             : iterable
                      PySAL polygon iterable with the attribute `bbox` (e.g.
                      shape object from `ps.open` a poly shapefile)
    which           : str/list
                      List of booleans for which polygons of the shapefile to
                      be included (True) or excluded (False)

    Returns
    -------

    map             : PatchCollection
                      Map object with the polygons from the shape
                      This includes the attribute `shp2dbf_row` with the
                      cardinality of every polygon to its row in the dbf
                      (zero-offset)

    '''
    patches = []
    rows = []
    i = 0
    if which == 'all':
        for shape in shp:
            for ring in shape.parts:
                xy = np.array(ring)
                patches.append(xy)
                rows.append(i)
            i += 1
    else:
        for inwhich, shape in zip(which, shp):
            if inwhich:
                for ring in shape.parts:
                    xy = np.array(ring)
                    patches.append(xy)
                    rows.append(i)
                i += 1
    pc = PolyCollection(patches)
    _ = _add_axes2col(pc, shp.bbox)
    pc.shp2dbf_row = rows
    return pc

def setup_ax(polyCos_list, ax=None):
    '''
    Generate an Axes object for a list of collections
    ...

    Arguments
    ---------
    polyCos_list: list
                  List of Matplotlib collections (e.g. an object from
                  map_poly_shp)
    ax          : AxesSubplot
                  (Optional) Pre-existing axes to which append the collections
                  and setup

    Returns
    -------
    ax          : AxesSubplot
                  Rescaled axes object with the collection and without frame
                  or X/Yaxis
    '''
    if not ax:
        ax = plt.axes()
    # Determine bboxes of new axes
    xlim = [np.inf, -np.inf]
    ylim = [np.inf, -np.inf]
    for polyCo in polyCos_list:
        axs = polyCo.get_axes()
        xmin, xmax = axs.get_xlim()
        ymin, ymax = axs.get_ylim()
        if xmin < xlim[0]:
            xlim[0] = xmin
        if xmax > xlim[1]:
            xlim[1] = xmax
        if ymin < ylim[0]:
            ylim[0] = ymin
        if ymax > ylim[1]:
            ylim[1] = ymax
    ax.set_xlim(xlim)
    ax.set_ylim(ylim)
    # Resize bbox of each coll and add it to axes
    for polyCo in polyCos_list:
        polyCo.get_axes().set_xlim(ax.get_xlim())
        polyCo.get_axes().set_ylim(ax.get_ylim())
        ax.add_collection(polyCo)
    ax.set_frame_on(False)
    ax.axes.get_yaxis().set_visible(False)
    ax.axes.get_xaxis().set_visible(False)
    return ax

def _add_axes2col(col, bbox):
    """
    Adds (inplace) axes with proper limits to a poly/line collection. This is
    still pretty much a hack! Ideally, you don't have to setup a new figure
    for this
    ...

    Arguments
    ---------
    col     : Collection
    bbox    : list
              Bounding box as [xmin, ymin, xmax, ymax]
    """
    tf = plt.figure()
    ax = plt.axes()
    minx, miny, maxx, maxy = bbox
    ax.set_xlim((minx, maxx))
    ax.set_ylim((miny, maxy))
    col.set_axes(ax)
    plt.close(tf)
    return None

def plot_poly_lines(shp_link,  savein=None, poly_col='none'):
    '''
    Quick plotting of shapefiles
    ...

    Arguments
    ---------
    shp_link        : str
                      Path to shapefile
    savein          : str
                      Path to png file where to dump the plot. Optional,
                      defaults to None
    poly_col        : str
                      Face color of polygons
    '''
    fig = plt.figure()
    ax = fig.add_subplot(111)
    shp = ps.open(shp_link)
    patchco = map_poly_shp(shp)
    patchco.set_facecolor('none')
    patchco.set_edgecolor('0.8')
    ax = setup_ax([patchco], ax)
    if savein:
        plt.savefig(savein)
    else:
        plt.show()
    return None

def plot_choropleth(shp_link, values, type, k=5, cmap=None, \
        shp_type='poly', sample_fisher=True, title='', \
        savein=None, figsize=None, dpi=300):
    '''
    Wrapper to quickly create and plot from a lat/lon shapefile
    ...

    Arguments
    ---------

    shp_link        : str
                      Path to shapefile
    values          : array
                      Numpy array with values to map
    type            : str
                      Type of choropleth. Supported methods:
                        * 'classless'
                        * 'unique_values'
                        * 'quantiles' (default)
                        * 'fisher_jenks'
                        * 'equal_interval'
    k               : int
                      Number of bins to classify values in and assign a color
                      to (defaults to 5)
    cmap            : str
                      Matplotlib coloring scheme. If None (default), uses:
                        * 'classless': 'Greys'
                        * 'unique_values': 'hot_r'
                        * 'quantiles': 'hot_r'
                        * 'fisher_jenks': 'hot_r'
                        * 'equal_interval': 'hot_r'
    shp_type        : str
                      'poly' (default) or 'line', for the kind of shapefile
                      passed
    sample_fisher   : Boolean
                      Defaults to True, controls whether Fisher-Jenks
                      classification uses a sample (faster) or the entire
                      array of values. Ignored if 'classification'!='fisher_jenks'
    title           : str
                      Optional string for the title
    savein          : str
                      Path to png file where to dump the plot. Optional,
                      defaults to None
    figsize         : tuple
                      Figure dimensions
    dpi             : int
                      resolution of graphic file

    Returns
    -------

    map             : PatchCollection
                      Map object with the polygons from the shapefile and
                      unique value coloring

    '''
    shp = ps.open(shp_link)
    if shp_type == 'poly':
        map_obj = map_poly_shp(shp)
    if shp_type == 'line':
        map_obj = map_line_shp(shp)

    if type == 'classless':
        if not cmap:
            cmap = 'Greys'
        map_obj = base_choropleth_classless(map_obj, values, cmap=cmap)
    if type == 'unique_values':
        if not cmap:
            cmap = 'hot_r'
        map_obj = base_choropleth_unique(map_obj, values, cmap=cmap)
    if type == 'quantiles':
        if not cmap:
            cmap = 'hot_r'
        map_obj = base_choropleth_classif(map_obj, values, k=k, \
                classification='quantiles', cmap=cmap)
    if type == 'fisher_jenks':
        if not cmap:
            cmap = 'hot_r'
        map_obj = base_choropleth_classif(map_obj, values, k=k, \
                classification='fisher_jenks', cmap=cmap, \
                sample_fisher=sample_fisher)
    if type == 'equal_interval':
        if not cmap:
            cmap = 'hot_r'
        map_obj = base_choropleth_classif(map_obj, values, k=k, \
                classification='equal_interval', cmap=cmap)

    fig = plt.figure(figsize=figsize)
    ax = fig.add_subplot(111)
    ax = setup_ax([map_obj], ax)
    if title:
        ax.set_title(title)
    if type=='quantiles' or type=='fisher_jenks' or type=='equal_interval':
        cmap = map_obj.get_cmap()
        norm = map_obj.norm
        boundaries = np.round(map_obj.norm.boundaries, decimals=3)
        cbar = plt.colorbar(map_obj, cmap=cmap, norm=norm, boundaries=boundaries, \
                ticks=boundaries, orientation='horizontal', shrink=0.5)
    if savein:
        plt.savefig(savein, dpi=dpi)
    else:
        plt.show()
    return None


def base_choropleth_classless(map_obj, values, cmap='Greys' ):
    '''
    Set classless coloring from a map object
    ...

    Arguments
    ---------

    map_obj         : Poly/Line collection
                      Output from map_X_shp
    values          : array
                      Numpy array with values to map
    cmap            : str
                      Matplotlib coloring scheme

    Returns
    -------

    map             : PatchCollection
                      Map object with the polygons from the shapefile and
                      classless coloring

    '''
    cmap = cm.get_cmap(cmap)
    map_obj.set_cmap(cmap)
    if isinstance(map_obj, mpl.collections.PolyCollection):
        pvalues = _expand_values(values, map_obj.shp2dbf_row)
        map_obj.set_array(pvalues)
        map_obj.set_edgecolor('k')
    elif isinstance(map_obj, mpl.collections.LineCollection):
        pvalues = _expand_values(values, map_obj.shp2dbf_row)
        map_obj.set_array(pvalues)
    elif isinstance(map_obj, mpl.collections.PathCollection):
        if not hasattr(map_obj, 'shp2dbf_row'):
            map_obj.shp2dbf_row = np.arange(values.shape[0])
        map_obj.set_array(values)
    return map_obj

def base_choropleth_unique(map_obj, values,  cmap='hot_r'):
    '''
    Set coloring based on unique values from a map object
    ...

    Arguments
    ---------

    map_obj         : Poly/Line collection
                      Output from map_X_shp
    values          : array
                      Numpy array with values to map
    cmap            : str
                      Matplotlib coloring scheme

    Returns
    -------

    map             : PatchCollection
                      Map object with the polygons from the shapefile and
                      unique value coloring

    '''
    uvals = np.unique(values)
    colormap = getattr(plt.cm, cmap)
    colors = [colormap(i) for i in np.linspace(0, 0.9, len(uvals))]
    colors = np.random.permutation(colors)
    colormatch = {val: col for val, col in zip(uvals, colors)}

    if isinstance(map_obj, mpl.collections.PolyCollection):
        pvalues = _expand_values(values, map_obj.shp2dbf_row)
        map_obj.set_color([colormatch[i] for i in pvalues])
        map_obj.set_edgecolor('k')
    elif isinstance(map_obj, mpl.collections.LineCollection):
        pvalues = _expand_values(values, map_obj.shp2dbf_row)
        map_obj.set_color([colormatch[i] for i in pvalues])
    elif isinstance(map_obj, mpl.collections.PathCollection):
        if not hasattr(map_obj, 'shp2dbf_row'):
            map_obj.shp2dbf_row = np.arange(values.shape[0])
        map_obj.set_array(values)
    return map_obj

def base_choropleth_classif(map_obj, values, classification='quantiles', \
        k=5, cmap='hot_r', sample_fisher=True):
    '''
    Set coloring based based on different classification
    methods
    ...

    Arguments
    ---------

    map_obj         : Poly/Line collection
                      Output from map_X_shp
    values          : array
                      Numpy array with values to map
    classification  : str
                      Classificatio method to use. Options supported:
                        * 'quantiles' (default)
                        * 'fisher_jenks'
                        * 'equal_interval'

    k               : int
                      Number of bins to classify values in and assign a color
                      to
    cmap            : str
                      Matplotlib coloring scheme
    sample_fisher   : Boolean
                      Defaults to True, controls whether Fisher-Jenks
                      classification uses a sample (faster) or the entire
                      array of values. Ignored if 'classification'!='fisher_jenks'

    Returns
    -------

    map             : PatchCollection
                      Map object with the polygons from the shapefile and
                      unique value coloring

    '''
    if classification == 'quantiles':
        classification = ps.Quantiles(values, k)
        boundaries = classification.bins.tolist()

    if classification == 'equal_interval':
        classification = ps.Equal_Interval(values, k)
        boundaries = classification.bins.tolist()

    if classification == 'fisher_jenks':
        if sample_fisher:
            classification = ps.esda.mapclassify.Fisher_Jenks_Sampled(values,k)
        else:
            classification = ps.Fisher_Jenks(values,k)
        boundaries = classification.bins[:]

    map_obj.set_alpha(0.4)

    cmap = cm.get_cmap(cmap, k+1)
    map_obj.set_cmap(cmap)

    boundaries.insert(0,0)
    norm = clrs.BoundaryNorm(boundaries, cmap.N)
    map_obj.set_norm(norm)

    if isinstance(map_obj, mpl.collections.PolyCollection):
        pvalues = _expand_values(values, map_obj.shp2dbf_row)
        map_obj.set_array(pvalues)
        map_obj.set_edgecolor('k')
    elif isinstance(map_obj, mpl.collections.LineCollection):
        pvalues = _expand_values(values, map_obj.shp2dbf_row)
        map_obj.set_array(pvalues)
    elif isinstance(map_obj, mpl.collections.PathCollection):
        if not hasattr(map_obj, 'shp2dbf_row'):
            map_obj.shp2dbf_row = np.arange(values.shape[0])
        map_obj.set_array(values)
    return map_obj

def _expand_values(values, shp2dbf_row):
    '''
    Expand series of values based on dbf order to polygons (to allow plotting
    of multi-part polygons).
    ...

    NOTE: this is done externally so it's easy to drop dependency on Pandas
    when neccesary/time is available.

    Arguments
    ---------
    values          : ndarray
                      Values aligned with dbf rows to be plotted (e.d.
                      choropleth)
    shp2dbf_row    : list/sequence
                      Cardinality list of polygon to dbf row as provided by
                      map_poly_shp

    Returns
    -------
    pvalues         : ndarray
                      Values repeated enough times in the right order to be
                      passed from dbf to polygons
    '''
    pvalues = pd.Series(values, index=np.arange(values.shape[0]))\
            .reindex(shp2dbf_row)#Expand values to every poly
    return pvalues.values

    

if __name__ == '__main__':

    data = 'none'
    if data == 'poly':
        shp_link = ps.examples.get_path("sids2.shp")
        shp_link = ps.examples.get_path("Polygon.shp")
        dbf = ps.open(shp_link.replace('.shp', '.dbf'))
        '''
        values = np.array(dbf.by_col("SIDR74"))
        #values[: values.shape[0]/2] = 1
        #values[values.shape[0]/2: ] = 0
        '''
        patchco = map_poly_shp(ps.open(shp_link))
        #patchco = base_choropleth_classif(shp_link, np.random.random(3))
        #patchco = plot_choropleth(shp_link, np.random.random(3), 'quantiles')

    if data == 'point':
        shp_link = ps.examples.get_path("burkitt.shp")
        dbf = ps.open(shp_link.replace('.shp', '.dbf'))
        patchco = map_point_shp(ps.open(shp_link))

    if data == 'line':
        shp_link = ps.examples.get_path("eberly_net.shp")
        dbf = ps.open(shp_link.replace('.shp', '.dbf'))
        values = np.array(dbf.by_col('TNODE'))
        mobj = map_line_shp(ps.open(shp_link))
        patchco = base_choropleth_unique(mobj, values)

    '''
    which = values > 1.

    for shp_link in [shp_link]:

        fig = plt.figure()
        patchco = map_poly_shp(shp_link)
        patchcoB = map_poly_shp(shp_link, which=which)
        patchco.set_facecolor('none')
        ax = setup_ax([patchco, patchcoB])
        fig.add_axes(ax)
        plt.show()
        break
    '''

    xy = (((0, 0), (0, 0)), ((2, 1), (2, 1)), ((3, 1), (3, 1)), ((2, 5), (2, 5)))
    xy = np.array([[10, 30], [20, 20]])
    markerobj = mpl.markers.MarkerStyle('o')
    path = markerobj.get_path().transformed(
            markerobj.get_transform())
    scales = np.array([2, 2])
    fig = plt.figure()
    ax = fig.add_subplot(111)
    pc = PathCollection((path,), scales, offsets=xy, \
            facecolors='r', transOffset=mpl.transforms.IdentityTransform())
    #pc.set_transform(mpl.transforms.IdentityTransform())
    #_ = _add_axes2col(pc, [0, 0, 5, 5])
    ax.add_collection(pc)
    fig.add_axes(ax)
    #ax = setup_ax([pc], ax)
    plt.show()


########NEW FILE########
__FILENAME__ = transforms
import pysal

__author__ = "Charles R Schmidt <schmidtc@gmail.com>"

class WorldToViewTransform(object):
    """
    An abstract class modeling a View window.
    Supports Panning, Zooming, Resizing.

    Is observable.

    Parameters:
    worldExtent -- Extent,List -- Extent of the world, left,lower,right,upper in world coords
    pixel_width -- int -- intial width of the view in pixels
    pixel_height -- int -- intial height of the view in pixels

    Notes:
    World coordinates are expected to increase the X and Y direction.
    Pixel coordinates are inverted in the Y direction.

    This class helps tranform world coordinates to screen coordinates.
    To transform a GraphicsMatrix,
    matrix.Scale(1.0/model.scale,-1.0/model.scale)
    matrix.Translate(*model.offset)
    
    The transforms will be applied in reverse order,
    The coordinates will first be translated to the origin (of the current view).
    The coordinates will then be scaled.

    Eg.
    >>> view = WorldToViewTransform([-180,-90,180,90],500,500)
    """
    def __init__(self,worldExtent,pixel_width,pixel_height):
        """ Intialize the view to the extent of the world """
        self.__pixel_width = float(pixel_width)
        self.__pixel_height = float(pixel_height)
        self.__world = worldExtent
        self.extent = worldExtent
        # In World Coords
    def __copy__(self):
        return WorldToViewTransform(self.extent,self.__pixel_width,self.__pixel_height)
    copy = __copy__
    def __get_offset(self):
        """ 
        Returns the offset of the top left corner of the current view in world coords.
        Move the world this many units to aling it with the view.
        """
        return self.__offset
    def __set_offset(self,value):
        """
        Set the Offset of the top left corner in world coords.
        """
        assert len(value) == 2
        self.__offset = value
    offset = property(fget=__get_offset,fset=__set_offset)
    def __get_scale(self):
        """ Returns the current scale in units/pixel """
        return self.__scale
    def __set_scale(self,value):
        """ Sets the current scale in units/pixel """
        self.__scale = value
    scale = property(fget=__get_scale,fset=__set_scale)
    def __get_extent(self):
        """Returns the extent of the current view in World Coordinates."""
        left,upper = self.pixel_to_world(0,0)
        right,lower = self.pixel_to_world(self.__pixel_width,self.__pixel_height)
        return pysal.cg.Rectangle(left,lower,right,upper)
    def __set_extent(self,value):
        """ Set the extent of the current view in World Coordinates.
            Preserve fixed scale, take the max of (sx,sy).

            Use this to zoom to a sepcific region when you know the region's 
            bbox in world coords.
        """
        left,lower,right,upper = value
        width = abs(right-left)
        height = abs(upper-lower)
        sx = width/self.__pixel_width
        sy = height/self.__pixel_height
        self.__scale = max(sx,sy)

        #The offset translate the world to the origin.
        #The X offset + world.left == 0
        #The Y offset + world.upper == 0

        # Move the offset a little, so that the center of the extent is in the center of the view.
        oleft = (left+(width/2.0)) - (self.__pixel_width*self.__scale/2.0)
        oupper = (upper-height/2.0) + (self.__pixel_height*self.__scale/2.0)

        #self.__offset = (-left,-upper) # in world coords
        self.__offset = (-oleft,-oupper) # in world coords
    extent = property(fget=__get_extent,fset=__set_extent)
    def __get_width(self):
        """ Returns the width of the current view in world coords """
        return self.__pixel_width*self.scale
    def __set_width(self, value):
        """
        Sets the width of the current view, value in pixels
        
        Eg.
        >>> view = WorldToViewTransform([0,0,100,100],500,500)
        >>> view.extent[:]
        [0.0, 0.0, 100.0, 100.0]
        >>> view.width = 250
        >>> view.extent[:]
        [0.0, 0.0, 50.0, 100.0]
        """
        if self.__pixel_width != value:
            self.__pixel_width = value
    width = property(fget=__get_width,fset=__set_width)
    def __get_height(self):
        """ Returns the height of the current view in world coords """
        return self.__pixel_height*self.scale
    def __set_height(self, value):
        """
        Sets the height of the current view, value in pixels
        
        Eg.
        >>> view = WorldToViewTransform([0,0,100,100],500,500)
        >>> view.extent[:]
        [0.0, 0.0, 100.0, 100.0]
        >>> view.height = 250
        >>> view.extent[:]
        [0.0, 50.0, 100.0, 100.0]
        """
        if self.__pixel_height != value:
            self.__pixel_height = value
    height = property(fget=__get_height,fset=__set_height)
    def __get_pixel_size(self):
        """
        Set and Return the current size of the view in pixels.
        """
        return self.__pixel_width,self.__pixel_height
    def __set_pixel_size(self,value):
        w,h = value
        if self.__pixel_width != w:
            self.__pixel_width = w
        if self.__pixel_height != h:
            self.__pixel_height = h
    pixel_size = property(fget=__get_pixel_size,fset=__set_pixel_size)

    def pan(self,dpx,dpy):
        """ 
        Pan the view by (dpx,dpy) pixel coordinates.
        
        Positive deltas move the world right and down.
        Negative deltas move the world left and up.

        Eg.
        >>> view = WorldToViewTransform([0,0,100,100],500,500)
        >>> view.pan(500,0)
        >>> view.extent[:]
        [-100.0, 0.0, 0.0, 100.0]
        >>> view.pan(-500,500)
        >>> view.extent[:]
        [0.0, 100.0, 100.0, 200.0]
        >>> view.pan(0,-500)
        >>> view.extent[:]
        [0.0, 0.0, 100.0, 100.0]
        >>> view.pan(490,490)
        >>> view.extent[:]
        [-98.0, 98.0, 2.0, 198.0]
        >>> view.pan(-490,-490)
        >>> view.extent[:]
        [0.0, 0.0, 100.0, 100.0]
        """
        ogx,ogy = self.__offset
        s = self.scale
        self.__offset = ogx+(dpx*s),ogy-(dpy*s)
    def pan_to(self,extent):
        initScale = self.scale
        self.extent = extent
        self.scale = initScale
    def pixel_to_world(self,px,py):
        """
        Returns the world coordinates of the Pixel (px,py).

        Eg.
        >>> view = WorldToViewTransform([0,0,100,100],500,500)
        >>> view.pixel_to_world(0,0)
        (0.0, 100.0)
        >>> view.pixel_to_world(500,500)
        (100.0, 0.0)
        """
        sx = self.scale
        sy = -sx
        ogx,ogy = self.__offset
        return px*sx - ogx, py*sy - ogy
    def world_to_pixel(self,x,y):
        """
        Returns the pixel of the world coordinate (x,y).

        Eg.
        >>> view = WorldToViewTransform([0,0,100,100],500,500)
        >>> view.world_to_pixel(0,0)
        (0.0, 500.0)
        >>> view.world_to_pixel(100,100)
        (500.0, -0.0)
        """
        sx = self.scale
        sy = -sx
        ogx,ogy = self.__offset
        return (x+ogx)/sx, (y+ogy)/sy

if __name__=="__main__":
    import doctest
    doctest.testmod()
    view = WorldToViewTransform([0,0,100,100],500,500)

########NEW FILE########
__FILENAME__ = weights_viewer
import wx
import pysal
from transforms import WorldToViewTransform

__author__ = "Charles R Schmidt <schmidtc@gmail.com>"

POINT_RADIUS = 5
BORDER_COLOR = wx.Colour(0,0,0,255)
SELECTION_COLOR = wx.Colour(255,128,0,255)
NEIGHBORS_COLOR = wx.Colour(128,255,0,255)
BACKGROUND_COLOR = wx.Colour(0,0,0,0)

class WeightsMapFrame(wx.Frame):
    def __init__(self,parent=None,size=(600,600), style=wx.DEFAULT_FRAME_STYLE, geo=None, w=None):
        wx.Frame.__init__(self,parent,size=size,style=style)
        self.Bind
        self.SetTitle("Weights Inspector")
        if issubclass(type(geo),basestring):
            geo = pysal.open(geo,'r')
        self.geo = geo
        if issubclass(type(w),basestring):
            w = pysal.open(w,'r').read()
        self.w = w
        self.wm = WeightsMap(self,self.geo,self.w)

class WeightsMap(wx.Panel):
    """ Display a Weights Inspection Map """
    def __init__(self, parent, geo, w_obj):
        wx.Panel.__init__(self,parent,size=(600,600))
        self.status = parent.CreateStatusBar(3)
        self.status.SetStatusWidths([-1,-2,-2])
        self.status.SetStatusText('No Selection',0)
        self.Bind(wx.EVT_SIZE, self.onSize)
        self.Bind(wx.EVT_PAINT, self.onPaint)
        self.Bind(wx.EVT_MOUSE_EVENTS, self.onMouse)
        self.trns = 0
        self.background = (255,255,255,255)
        w,h = self.GetSize()
        self.buffer = wx.EmptyBitmapRGBA(w,h,alpha=self.trns)
        self.geo = geo
        if geo.type == pysal.cg.shapes.Polygon:
            self.drawfunc = self.drawpoly
            self.locator = pysal.cg.PolygonLocator(geo)
        elif geo.type == pysal.cg.shapes.Point:
            self.drawfunc = self.drawpt
            self.locator = pysal.cg.PointLocator(geo)
        else:
            raise TypeError, "Unsupported Type: %r"%(geo.type)
        self.w = w_obj
        self._ids = range(self.w.n)
        self.transform = WorldToViewTransform(geo.bbox,w,h)
        self.selection = None
    def onMouse(self, evt):
        x,y = evt.X,evt.Y
        X,Y = self.transform.pixel_to_world(x,y)
        if self.geo.type == pysal.cg.shapes.Polygon:
            rs = self.locator.contains_point((X,Y))
            if rs:
                selection = rs[0].id-1
                self.set_selection(selection)
            else:
                self.set_selection(None)
        else:
            print self.locator.nearest((X,Y))
    def onSize(self, evt):
        w,h = self.GetSize()
        self.buffer = wx.EmptyBitmapRGBA(w,h,alpha=self.trns)
        self.transform = WorldToViewTransform(self.geo.bbox,w,h)
        self.draw()
    def onPaint(self, evt):
        pdc = wx.PaintDC(self)
        pdc.Clear()
        self.draw()
    def draw(self):
        dc = wx.MemoryDC()
        dc.SelectObject(self.buffer)
        dc.SetBackground(wx.Brush(wx.Colour(*self.background)))
        dc.Clear()
        dc.SelectObject(wx.NullBitmap)
        self.draw_shps(self.buffer)
        cdc = wx.ClientDC(self)
        cdc.DrawBitmap(self.buffer,0,0)
    def drawpoly(self,gc,matrix,fill=False,ids=None):
        geo = self.geo
        pth = gc.CreatePath()
        if not ids:
            ids = xrange(len(geo))
        for i in ids:
            poly = geo.get(i)
            parts = poly.parts
            if poly.holes[0]:
                parts = parts+poly.holes
            for part in parts:
                x,y = part[0]
                pth.MoveToPoint(x,y)
                for x,y in part[1:]:
                    pth.AddLineToPoint(x,y)
                pth.CloseSubpath()
        pth.Transform(matrix)
        if fill:
            gc.FillPath(pth)
        gc.StrokePath(pth)
        return gc
    def drawpt(self,gc,matrix,fill=False,ids=None):
        r = POINT_RADIUS
        radius = r/2.0
        geo = self.geo
        for pt in geo:
            x,y = matrix.TransformPoint(*pt)
            gc.DrawEllipse(x-radius,y-radius,r,r)
        return gc
    def draw_shps(self, buff, fill_color=None, ids=None, fill_style=wx.SOLID):
        transform = self.transform
        dc = wx.MemoryDC()
        dc.SelectObject(buff)
        gc = wx.GraphicsContext.Create(dc)
        gc.SetPen( gc.CreatePen(wx.Pen(BORDER_COLOR,1)) )
        if fill_color != None:
            gc.SetBrush( gc.CreateBrush(wx.Brush(fill_color,fill_style)) )
            fill = True
        else:
            fill = False
        matrix = gc.CreateMatrix()
        matrix.Scale(1./transform.scale,1./-transform.scale)
        matrix.Translate(*transform.offset)
        self.drawfunc(gc,matrix,fill,ids)
    def set_selection(self,sel):
        if self.selection == sel:
            return
        self.selection = sel

        cdc = wx.ClientDC(self)
        if sel != None:
            w,h = self.transform.pixel_size
            buff = self.buffer.GetSubBitmap((0,0,w,h))
            id = self.w.id_order[sel]
            neighbors = map(self.w.id_order.index,self.w.neighbors[id])
            self.draw_shps(buff, NEIGHBORS_COLOR, neighbors)
            if sel in neighbors:
                self.draw_shps(buff, SELECTION_COLOR, [sel], wx.CROSSDIAG_HATCH)
            else:
                self.draw_shps(buff, SELECTION_COLOR, [sel])
            print sel,":",neighbors
            cdc.DrawBitmap(buff,0,0)
            stat0 = "Selection:%s"%id
            stat1 = "Neighbors:%s"%(','.join(map(str,self.w.neighbors[id])))
            stat2 = "Weights:%s"%(','.join(map(str,self.w.weights[id])))
            self.status.SetStatusText(stat0,0)
            self.status.SetStatusText(stat1,1)
            self.status.SetStatusText(stat2,2)
        else:
            self.status.SetStatusText('No Selection',0)
            self.status.SetStatusText('',1)
            self.status.SetStatusText('',2)
            cdc.DrawBitmap(self.buffer,0,0)

class WeightsMapApp(wx.App):
    def __init__(self, geo=None, w=None, redirect=False):
        self.geo = geo
        self.w = w
        wx.App.__init__(self, redirect)
    def OnInit(self):
        self.SetAppName("Weights Inspector")
        self.frame = WeightsMapFrame(None,size=(600,600),geo=self.geo,w=self.w)
        self.SetTopWindow(self.frame)
        self.frame.Show()
        return True


if __name__=='__main__':
    #shp = pysal.examples.get_path('sids2.shp')
    #w = pysal.examples.get_path('sids2.gal')
    #app = WeightsMapApp(shp,w)
    #app.MainLoop()

    shp = pysal.examples.get_path('columbus.shp')
    w = pysal.queen_from_shapefile(shp)
    app = WeightsMapApp(shp,w)
    app.MainLoop()

########NEW FILE########
__FILENAME__ = FileIO
"""
FileIO: Module for reading and writing various file types in a Pythonic way.
This module should not be used directly, instead...
import pysal.core.FileIO as FileIO
Readers and Writers will mimic python file objects.
.seek(n) seeks to the n'th object
.read(n) reads n objects, default == all
.next() reads the next object
"""

__author__ = "Charles R Schmidt <schmidtc@gmail.com>"

__all__ = ['FileIO']
import os.path
import struct
from warnings import warn
import pysal


class FileIO_MetaCls(type):
    """
    This Meta Class is instantiated when the class is first defined.
    All subclasses of FileIO also inherit this meta class, which registers their abilities with the FileIO registry.
    Subclasses must contain FORMATS and MODES (both are type(list))
    """
    def __new__(mcs, name, bases, dict):
        cls = type.__new__(mcs, name, bases, dict)
        if name != 'FileIO' and name != 'DataTable':
            if "FORMATS" in dict and "MODES" in dict:
                #print "Registering %s with FileIO.\n\tFormats: %r\n\tModes: %r"%(name,dict['FORMATS'],dict['MODES'])
                FileIO._register(cls, dict['FORMATS'], dict['MODES'])
            else:
                raise TypeError("FileIO subclasses must have FORMATS and MODES defined")
        return cls


class FileIO(object):  # should be a type?
    """
    How this works:
    FileIO.open(\*args) == FileIO(\*args)
    When creating a new instance of FileIO the .__new__ method intercepts
    .__new__ parses the filename to determine the fileType
    next, .__registry and checked for that type.
    Each type supports one or more modes ['r','w','a',etc]
    If we support the type and mode, an instance of the appropriate handler
    is created and returned.
    All handlers must inherit from this class, and by doing so are automatically
    added to the .__registry and are forced to conform to the prescribed API.
    The metaclass takes cares of the registration by parsing the class definition.
    It doesn't make much sense to treat weights in the same way as shapefiles and dbfs,
    ....for now we'll just return an instance of W on mode='r'
    .... on mode='w', .write will expect an instance of W
    """
    __metaclass__ = FileIO_MetaCls
    __registry = {}  # {'shp':{'r':[OGRshpReader,pysalShpReader]}}

    def __new__(cls, dataPath='', mode='r', dataFormat=None):
        """
        Intercepts the instantiation of FileIO and dispatches to the correct handler
        If no suitable handler is found a python file object is returned.
        """
        if cls is FileIO:
            try:
                newCls = object.__new__(cls.__registry[cls.getType(dataPath,
                                                                   mode, dataFormat)][mode][0])
            except KeyError:
                return open(dataPath, mode)
            return newCls
        else:
            return object.__new__(cls)

    @staticmethod
    def getType(dataPath, mode, dataFormat=None):
        """Parse the dataPath and return the data type"""
        if dataFormat:
            ext = dataFormat
        else:
            ext = os.path.splitext(dataPath)[1]
            ext = ext.replace('.', '')
            ext = ext.lower()
        if ext == 'txt':
            f = open(dataPath, 'r')
            l1 = f.readline()
            l2 = f.readline()
            if ext == 'txt':
                try:
                    n, k = l1.split(',')
                    n, k = int(n), int(k)
                    fields = l2.split(',')
                    assert len(fields) == k
                    return 'geoda_txt'
                except:
                    return ext
        return ext

    @classmethod
    def _register(cls, parser, formats, modes):
        """ This method is called automatically via the MetaClass of FileIO subclasses
        This should be private, but that hides it from the MetaClass
        """
        assert cls is FileIO
        for format in formats:
            if not format in cls.__registry:
                cls.__registry[format] = {}
            for mode in modes:
                if not mode in cls.__registry[format]:
                    cls.__registry[format][mode] = []
                cls.__registry[format][mode].append(parser)
        #cls.check()

    @classmethod
    def check(cls):
        """ Prints the contents of the registry """
        print "PySAL File I/O understands the following file extensions:"
        for key, val in cls.__registry.iteritems():
            print "Ext: '.%s', Modes: %r" % (key, val.keys())

    @classmethod
    def open(cls, *args, **kwargs):
        """ Alias for FileIO() """
        return cls(*args, **kwargs)

    class _By_Row:
        def __init__(self, parent):
            self.p = parent

        def __repr__(self):
            if not self.p.ids:
                return "keys: range(0,n)"
            else:
                return "keys: " + self.p.ids.keys().__repr__()

        def __getitem__(self, key):
            if type(key) == list:
                r = []
                if self.p.ids:
                    for k in key:
                        r.append(self.p.get(self.p.ids[k]))
                else:
                    for k in key:
                        r.append(self.p.get(k))
                return r
            if self.p.ids:
                return self.p.get(self.p.ids[key])
            else:
                return self.p.get(key)
        __call__ = __getitem__

    def __init__(self, dataPath='', mode='r', dataFormat=None):
        self.dataPath = dataPath
        self.dataObj = ''
        self.mode = mode
        #pos Should ALWAYS be in the range 0,...,n
        #for custom IDs set the ids property.
        self.pos = 0
        self.__ids = None  # {'id':n}
        self.__rIds = None
        self.closed = False
        self._spec = []
        self.header = []

    def __getitem__(self, key):
        return self.by_row.__getitem__(key)

    @property
    def by_row(self):
        return self._By_Row(self)

    def __getIds(self):
        return self.__ids

    def __setIds(self, ids):
        """ Property Method for .ids
        Takes a list of ids and maps then to a 0 based index
        Need to provide a method to set ID's based on a fieldName
        preferably without reading the whole file.
        """
        if isinstance(ids, list):
            try:
                assert len(ids) == len(set(ids))
            except AssertionError:
                raise KeyError("IDs must be unique")
            # keys: ID values: i
            self.__ids = {}
            # keys: i values: ID
            self.__rIds = {}
            for i, id in enumerate(ids):
                self.__ids[id] = i
                self.__rIds[i] = id
        elif isinstance(ids, dict):
            self.__ids = ids
            self.__rIds = {}
            for id, n in ids.iteritems():
                self.__rIds[n] = id
        elif not ids:
            self.__ids = None
            self.__rIds = None
    ids = property(fget=__getIds, fset=__setIds)

    @property
    def rIds(self):
        return self.__rIds

    def __iter__(self):
        self.seek(0)
        return self

    @staticmethod
    def _complain_ifclosed(closed):
        """ from StringIO """
        if closed:
            raise ValueError("I/O operation on closed file")

    def cast(self, key, typ):
        """cast key as typ"""
        if key in self.header:
            if not self._spec:
                self._spec = [lambda x:x for key in self.header]
            if typ is None:
                self._spec[self.header.index(key)] = lambda x: x
            else:
                try:
                    assert hasattr(typ, '__call__')
                    self._spec[self.header.index(key)] = typ
                except AssertionError:
                    raise TypeError('Cast Objects must be callable')
        else:
            raise KeyError("%s" % key)

    def _cast(self, row):
        if self._spec and row:
            try:
                return [f(v) for f, v in zip(self._spec, row)]
            except ValueError:
                r = []
                for f, v in zip(self._spec, row):
                    try:
                        if not v and f != str:
                            raise ValueError
                        r.append(f(v))
                    except ValueError:
                        warn("Value '%r' could not be cast to %s, value set to pysal.MISSINGVALUE" % (v, str(f)), RuntimeWarning)
                        r.append(pysal.MISSINGVALUE)
                return r

        else:
            return row

    def next(self):
        """A FileIO object is its own iterator, see StringIO"""
        self._complain_ifclosed(self.closed)
        r = self.__read()
        if r is None:
            raise StopIteration
        return r

    def close(self):
        """ subclasses should clean themselves up and then call this method """
        if not self.closed:
            self.closed = True
            del self.dataObj, self.pos

    def get(self, n):
        """ Seeks the file to n and returns n
        If .ids is set n should be an id,
        else, n should be an offset
        """
        prevPos = self.tell()
        self.seek(n)
        obj = self.__read()
        self.seek(prevPos)
        return obj

    def seek(self, n):
        """ Seek the FileObj to the beginning of the n'th record,
            if ids are set, seeks to the beginning of the record at id, n"""
        self._complain_ifclosed(self.closed)
        self.pos = n

    def tell(self):
        """ Return id (or offset) of next object """
        self._complain_ifclosed(self.closed)
        return self.pos

    def read(self, n=-1):
        """ Read at most n objects, less if read hits EOF
        if size is negative or omitted read all objects until EOF
        returns None if EOF is reached before any objects.
        """
        self._complain_ifclosed(self.closed)
        if n < 0:
            #return list(self)
            result = []
            while 1:
                try:
                    result.append(self.__read())
                except StopIteration:
                    break
            return result
        elif n == 0:
            return None
        else:
            result = []
            for i in range(0, n):
                try:
                    result.append(self.__read())
                except StopIteration:
                    break
            return result

    def __read(self):
        """ Gets one row from the file handler, and if necessary casts it's objects """
        row = self._read()
        if row is None:
            raise StopIteration
        row = self._cast(row)
        return row

    def _read(self):
        """ Must be implemented by subclasses that support 'r'
        subclasses should increment .pos
        and redefine this doc string
        """
        self._complain_ifclosed(self.closed)
        raise NotImplementedError

    def truncate(self, size=None):
        """ Should be implemented by subclasses
        and redefine this doc string
        """
        self._complain_ifclosed(self.closed)
        raise NotImplementedError

    def write(self, obj):
        """ Must be implemented by subclasses that support 'w'
        subclasses should increment .pos
        subclasses should also check if obj is an instance of type(list)
        and redefine this doc string
        """
        self._complain_ifclosed(self.closed)
        "Write obj to dataObj"
        raise NotImplementedError

    def flush(self):
        self._complain_ifclosed(self.closed)
        raise NotImplementedError

########NEW FILE########
__FILENAME__ = arcgis_dbf
import pysal
import os.path
import pysal.core.FileIO as FileIO
from pysal.weights import W
from pysal.weights.util import remap_ids
from warnings import warn

__author__ = "Myunghwa Hwang <mhwang4@gmail.com>"
__all__ = ["ArcGISDbfIO"]


class ArcGISDbfIO(FileIO.FileIO):
    """
    Opens, reads, and writes weights file objects in ArcGIS dbf format.

    Spatial weights objects in the ArcGIS dbf format are used in
    ArcGIS Spatial Statistics tools.
    This format is the same as the general dbf format,
    but the structure of the weights dbf file is fixed unlike other dbf files.
    This dbf format can be used with the "Generate Spatial Weights Matrix" tool,
    but not with the tools under the "Mapping Clusters" category.

    The ArcGIS dbf file is assumed to have three or four data columns.
    When the file has four columns,
    the first column is meaningless and will be ignored in PySAL
    during both file reading and file writing.
    The next three columns hold origin IDs, destinations IDs, and weight values.
    When the file has three columns,
    it is assumed that only these data columns exist in the stated order.
    The name for the orgin IDs column should be the name of
    ID variable in the original source data table.
    The names for the destination IDs and weight values columns are NID
    and WEIGHT, respectively.
    ArcGIS Spatial Statistics tools support only unique integer IDs.
    Therefore, the values for origin and destination ID columns should
    be integer.
    For the case where the IDs of a weights object are not integers,
    ArcGISDbfIO allows users to use internal id values corresponding to
    record numbers, instead of original ids.

    An exemplary structure of an ArcGIS dbf file is as follows:
    [Line 1]    Field1    RECORD_ID    NID    WEIGHT
    [Line 2]    0         72           76     1
    [Line 3]    0         72           79     1
    [Line 4]    0         72           78     1
    ...

    Unlike the ArcGIS text format, this format does not seem to include self-neighbors.

    References
    ----------
    http://webhelp.esri.com/arcgisdesktop/9.3/index.cfm?TopicName=Convert_Spatial_Weights_Matrix_to_Table_(Spatial_Statistics)

    """

    FORMATS = ['arcgis_dbf']
    MODES = ['r', 'w']

    def __init__(self, *args, **kwargs):
        self._varName = 'Unknown'
        args = args[:2]
        FileIO.FileIO.__init__(self, *args, **kwargs)
        self.file = pysal.open(self.dataPath, self.mode)

    def _set_varName(self, val):
        if issubclass(type(val), basestring):
            self._varName = val

    def _get_varName(self):
        return self._varName
    varName = property(fget=_get_varName, fset=_set_varName)

    def read(self, n=-1):
        self._complain_ifclosed(self.closed)
        return self._read()

    def seek(self, pos):
        self.file.seek(pos)
        self.pos = self.file.pos

    def _read(self):
        """Reads ArcGIS dbf file
        Returns a pysal.weights.weights.W object

        Examples
        --------

        Type 'dir(w)' at the interpreter to see what methods are supported.
        Open an ArcGIS dbf file and read it into a pysal weights object

        >>> w = pysal.open(pysal.examples.get_path('arcgis_ohio.dbf'),'r','arcgis_dbf').read()

        Get the number of observations from the header

        >>> w.n
        88

        Get the mean number of neighbors

        >>> w.mean_neighbors
        5.25

        Get neighbor distances for a single observation

        >>> w[1]
        {2: 1.0, 11: 1.0, 6: 1.0, 7: 1.0}

        """
        if self.pos > 0:
            raise StopIteration

        id_var = self.file.header[1]
        startPos = len(self.file.header)

        if startPos == 3:
            startPos = 0
        elif startPos == 4:
            startPos = 1
        else:
            raise ValueError("Wrong structure, a weights dbf file requires at least three data columns")

        self.varName = id_var
        id_type = int
        id_spec = self.file.field_spec[startPos]
        if id_spec[0] != 'N':
            raise TypeError('The data type for ids should be integer.')
        self.id_var = id_var

        weights = {}
        neighbors = {}
        for row in self.file:
            i, j, w = tuple(row)[startPos:]
            i = id_type(i)
            j = id_type(j)
            w = float(w)
            if i not in weights:
                weights[i] = []
                neighbors[i] = []
            weights[i].append(w)
            neighbors[i].append(j)
            self.pos = self.file.pos

        return W(neighbors, weights)

    def write(self, obj, useIdIndex=False):
        """

        Parameters
        ----------
        .write(weightsObject)
        accepts a weights object

        Returns
        ------

        an ArcGIS dbf file
        write a weights object to the opened dbf file.

        Examples
        --------

        >>> import tempfile, pysal, os
        >>> testfile = pysal.open(pysal.examples.get_path('arcgis_ohio.dbf'),'r','arcgis_dbf')
        >>> w = testfile.read()

        Create a temporary file for this example

        >>> f = tempfile.NamedTemporaryFile(suffix='.dbf')

        Reassign to new var

        >>> fname = f.name

        Close the temporary named file

        >>> f.close()

        Open the new file in write mode

        >>> o = pysal.open(fname,'w','arcgis_dbf')

        Write the Weights object into the open file

        >>> o.write(w)
        >>> o.close()

        Read in the newly created text file

        >>> wnew =  pysal.open(fname,'r','arcgis_dbf').read()

        Compare values from old to new

        >>> wnew.pct_nonzero == w.pct_nonzero
        True

        Clean up temporary file created for this example

        >>> os.remove(fname)

        """
        self._complain_ifclosed(self.closed)
        if issubclass(type(obj), W):
            self.file.header = [self.varName, 'NID', 'WEIGHT']

            id_type = type(obj.id_order[0])
            if id_type is not int and not useIdIndex:
                raise TypeError("ArcGIS DBF weight files support only integer IDs")
            if useIdIndex:
                id2i = obj.id2i
                obj = remap_ids(obj, id2i)

            id_spec = ('N', len(str(max(obj.id_order))), 0)
            self.file.field_spec = [id_spec, id_spec, ('N', 13, 6)]

            for id in obj.id_order:
                neighbors = zip(obj.neighbors[id], obj.weights[id])
                for neighbor, weight in neighbors:
                    self.file.write([id, neighbor, weight])
                    self.pos = self.file.pos

        else:
            raise TypeError("Expected a pysal weights object, got: %s" % (
                type(obj)))

    def flush(self):
        self._complain_ifclosed(self.closed)
        self.file.flush()

    def close(self):
        self.file.close()


########NEW FILE########
__FILENAME__ = arcgis_swm
import pysal
import os.path
import numpy as np
from struct import pack, unpack
import pysal.core.FileIO as FileIO
from pysal.weights import W
from pysal.weights.util import remap_ids
from warnings import warn

__author__ = "Myunghwa Hwang <mhwang4@gmail.com>"
__all__ = ["ArcGISSwmIO"]


class ArcGISSwmIO(FileIO.FileIO):
    """
    Opens, reads, and writes weights file objects in ArcGIS swm format.

    Spatial weights objects in the ArcGIS swm format are used in
    ArcGIS Spatial Statistics tools.
    Particularly, this format can be directly used with the tools under
    the category of Mapping Clusters.

    The values for [ORG_i] and [DST_i] should be integers,
    as ArcGIS Spatial Statistics tools support only unique integer IDs.
    For the case where a weights object uses non-integer IDs,
    ArcGISSwmIO allows users to use internal ids corresponding to record numbers,
    instead of original ids.

    The specifics of each part of the above structure is as follows.

  .. table:: ArcGIS SWM Components

    ============ ============ ==================================== ================================
        Part      Data type           Description                   Length                        
    ============ ============ ==================================== ================================
     ID_VAR_NAME  ASCII TEXT  ID variable name                     Flexible (Up to the 1st ;)     
     ESRI_SRS     ASCII TEXT  ESRI spatial reference system        Flexible (Btw the 1st ; and \\n)  
     NO_OBS       l.e. int    Number of observations               4                         
     ROW_STD      l.e. int    Whether or not row-standardized      4                         
     WGT_i                                                                                   
     ORG_i        l.e. int    ID of observaiton i                  4                         
     NO_NGH_i     l.e. int    Number of neighbors for obs. i (m)   4                         
     NGHS_i                                                                                  
     DSTS_i       l.e. int    IDs of all neighbors of obs. i       4*m                       
     WS_i         l.e. float  Weights for obs. i and its neighbors 8*m                       
     W_SUM_i      l.e. float  Sum of weights for "                 8                         
    ============ ============ ==================================== ================================

    """

    FORMATS = ['swm']
    MODES = ['r', 'w']

    def __init__(self, *args, **kwargs):
        self._varName = 'Unknown'
        FileIO.FileIO.__init__(self, *args, **kwargs)
        self.file = open(self.dataPath, self.mode + 'b')

    def _set_varName(self, val):
        if issubclass(type(val), basestring):
            self._varName = val

    def _get_varName(self):
        return self._varName
    varName = property(fget=_get_varName, fset=_set_varName)

    def read(self, n=-1):
        self._complain_ifclosed(self.closed)
        return self._read()

    def seek(self, pos):
        if pos == 0:
            self.file.seek(0)
            self.pos = 0

    def _read(self):
        """
        Reads ArcGIS swm file.
        Returns a pysal.weights.weights.W object

        Examples
        --------

        Type 'dir(w)' at the interpreter to see what methods are supported.
        Open an ArcGIS swm file and read it into a pysal weights object

        >>> w = pysal.open(pysal.examples.get_path('ohio.swm'),'r').read()

        Get the number of observations from the header

        >>> w.n
        88

        Get the mean number of neighbors

        >>> w.mean_neighbors
        5.25

        Get neighbor distances for a single observation

        >>> w[1]
        {2: 1.0, 11: 1.0, 6: 1.0, 7: 1.0}

        """

        if self.pos > 0:
            raise StopIteration

        header01 = self.file.readline()
        id_var, srs = header01[:-1].split(';')
        self.varName = id_var
        self.header_len = len(header01) + 8
        no_obs, row_std = tuple(unpack('<2l', self.file.read(8)))

        neighbors = {}
        weights = {}
        for i in xrange(no_obs):
            origin, no_nghs = tuple(unpack('<2l', self.file.read(8)))
            neighbors[origin] = []
            weights[origin] = []
            if no_nghs > 0:
                neighbors[origin] = list(unpack('<%il' %
                                                no_nghs, self.file.read(4 * no_nghs)))
                weights[origin] = list(unpack('<%id' %
                                              no_nghs, self.file.read(8 * no_nghs)))
                w_sum = list(unpack('<d', self.file.read(8)))[0]

        self.pos += 1
        return W(neighbors, weights)

    def write(self, obj, useIdIndex=False):
        """
        Writes a spatial weights matrix data file in swm format.

        Parameters
        ----------
        .write(weightsObject)
        accepts a weights object

        Returns
        -------

        an ArcGIS swm file
        write a weights object to the opened swm file.

        Examples
        --------

        >>> import tempfile, pysal, os
        >>> testfile = pysal.open(pysal.examples.get_path('ohio.swm'),'r')
        >>> w = testfile.read()

        Create a temporary file for this example

        >>> f = tempfile.NamedTemporaryFile(suffix='.swm')

        Reassign to new var

        >>> fname = f.name

        Close the temporary named file

        >>> f.close()

        Open the new file in write mode

        >>> o = pysal.open(fname,'w')

        Write the Weights object into the open file

        >>> o.write(w)
        >>> o.close()

        Read in the newly created text file

        >>> wnew = pysal.open(fname,'r').read()

        Compare values from old to new

        >>> wnew.pct_nonzero == w.pct_nonzero
        True

        Clean up temporary file created for this example

        >>> os.remove(fname) """

        self._complain_ifclosed(self.closed)
        if issubclass(type(obj), W):
            if not (type(obj.id_order[0]) in (np.int32, np.int64, int)) and not useIdIndex:
                raise TypeError("ArcGIS SWM files support only integer IDs")
            if useIdIndex:
                id2i = obj.id2i
                obj = remap_ids(obj, id2i)
            self.file.write('%s;Unknown\n' % self.varName)
            self.file.write(pack('<l', obj.n))
            self.file.write(pack('<l', obj.transform.upper() == 'R'))
            for obs in obj.weights:
                self.file.write(pack('<l', obs))
                no_nghs = len(obj.weights[obs])
                self.file.write(pack('<l', no_nghs))
                self.file.write(pack('<%il' % no_nghs, *obj.neighbors[obs]))
                self.file.write(pack('<%id' % no_nghs, *obj.weights[obs]))
                self.file.write(pack('<d', sum(obj.weights[obs])))
            self.pos += 1

        else:
            raise TypeError("Expected a pysal weights object, got: %s" % (
                type(obj)))

    def close(self):
        self.file.close()
        FileIO.FileIO.close(self)

########NEW FILE########
__FILENAME__ = arcgis_txt
import pysal
import os.path
import gwt
from pysal.weights import W
from pysal.weights.util import remap_ids
from warnings import warn

__author__ = "Myunghwa Hwang <mhwang4@gmail.com>"
__all__ = ["ArcGISTextIO"]


class ArcGISTextIO(gwt.GwtIO):
    """
    Opens, reads, and writes weights file objects in ArcGIS ASCII text format.

    Spatial weights objects in the ArcGIS text format are used in
    ArcGIS Spatial Statistics tools.
    This format is a simple text file with ASCII encoding.
    This format can be directly used with the tools under
    the category of "Mapping Clusters." But, it cannot be used with
    the "Generate Spatial Weights Matrix" tool.

    The first line of the ArcGIS text file is a header including the name of
    a data column that holded the ID variable in the original source data table.
    After this header line, it includes three data columns
    for origin id, destination id, and weight values.
    ArcGIS Spatial Statistics tools support only unique integer ids.
    Thus, the values in the first two columns should be integers.
    For the case where a weights object uses non-integer IDs,
    ArcGISTextIO allows users to use internal ids corresponding to record numbers,
    instead of original ids.

    An exemplary structure of an ArcGIS text file is as follows:
    [Line 1]    StationID
    [Line 2]    1    1    0.0
    [Line 3]    1    2    0.1
    [Line 4]    1    3    0.14286
    [Line 5]    2    1    0.1
    [Line 6]    2    3    0.05
    [Line 7]    3    1    0.16667
    [Line 8]    3    2    0.06667
    [Line 9]    3    3    0.0
    ...

    As shown in the above example, this file format allows explicit specification
    of weights for self-neighbors.
    When no entry is available for self-neighbors,
    ArcGIS spatial statistics tools consider they have zero weights.
    PySAL ArcGISTextIO class ignores self-neighbors if their weights are zero.

    References
    ----------
    http://webhelp.esri.com/arcgisdesktop/9.3/index.cfm?TopicName=Modeling_spatial_relationships

    Notes
    -----
    When there are an dbf file whose name is identical to the name of the source text file,
    ArcGISTextIO checks the data type of the ID data column and uses it for reading and
    writing the text file. Otherwise, it considers IDs are strings.

    """

    FORMATS = ['arcgis_text']
    MODES = ['r', 'w']

    def __init__(self, *args, **kwargs):
        args = args[:2]
        gwt.GwtIO.__init__(self, *args, **kwargs)

    def _read(self):
        """Reads ArcGIS Text file
        Returns a pysal.weights.weights.W object

        Examples
        --------

        Type 'dir(w)' at the interpreter to see what methods are supported.
        Open a text file and read it into a pysal weights object

        >>> w = pysal.open(pysal.examples.get_path('arcgis_txt.txt'),'r','arcgis_text').read()

        Get the number of observations from the header

        >>> w.n
        3

        Get the mean number of neighbors

        >>> w.mean_neighbors
        2.0

        Get neighbor distances for a single observation

        >>> w[1]
        {2: 0.1, 3: 0.14286}

        """
        if self.pos > 0:
            raise StopIteration

        id_var = self.file.readline().strip()
        self.varName = id_var
        id_order = None
        id_type = int
        try:
            dbf = os.path.join(self.dataPath + '.dbf')
            if os.path.exists(dbf):
                db = pysal.open(dbf, 'r')
                if id_var in db.header:
                    id_order = db.by_col(id_var)
                    id_type = type(id_order[0])
                else:
                    warn("ID_VAR:'%s' was in in the DBF header, proceeding with unordered string ids." % (id_var), RuntimeWarning)
            else:
                warn("DBF relating to ArcGIS TEXT was not found, proceeding with unordered string ids.", RuntimeWarning)
        except:
            warn("Exception occurred will reading DBF, proceeding with unordered string ids.", RuntimeWarning)

        if (id_type is not int) or (id_order and type(id_order)[0] is not int):
            raise TypeError("The data type for ids should be integer.")

        if id_order:
            self.n = len(id_order)
            self.shp = os.path.split(self.dataPath)[1].split('.')[0]
        self.id_var = id_var

        weights, neighbors = self._readlines(id_type)
        for k in neighbors:
            if k in neighbors[k]:
                k_index = neighbors[k].index(k)
                if weights[k][k_index] == 0.0:
                    del neighbors[k][k_index]
                    del weights[k][k_index]

        self.pos += 1
        return W(neighbors, weights)

    def write(self, obj, useIdIndex=False):
        """

        Parameters
        ----------
        .write(weightsObject)
        accepts a weights object

        Returns
        ------

        an ArcGIS text file
        write a weights object to the opened text file.

        Examples
        --------

        >>> import tempfile, pysal, os
        >>> testfile = pysal.open(pysal.examples.get_path('arcgis_txt.txt'),'r','arcgis_text')
        >>> w = testfile.read()

        Create a temporary file for this example

        >>> f = tempfile.NamedTemporaryFile(suffix='.txt')

        Reassign to new var

        >>> fname = f.name

        Close the temporary named file

        >>> f.close()

        Open the new file in write mode

        >>> o = pysal.open(fname,'w','arcgis_text')

        Write the Weights object into the open file

        >>> o.write(w)
        >>> o.close()

        Read in the newly created text file

        >>> wnew =  pysal.open(fname,'r','arcgis_text').read()

        Compare values from old to new

        >>> wnew.pct_nonzero == w.pct_nonzero
        True

        Clean up temporary file created for this example

        >>> os.remove(fname)
        """
        self._complain_ifclosed(self.closed)
        if issubclass(type(obj), W):
            id_type = type(obj.id_order[0])
            if id_type is not int and not useIdIndex:
                raise TypeError("ArcGIS TEXT weight files support only integer IDs")
            if useIdIndex:
                id2i = obj.id2i
                obj = remap_ids(obj, id2i)

            header = '%s\n' % self.varName
            self.file.write(header)
            self._writelines(obj)
        else:
            raise TypeError("Expected a pysal weights object, got: %s" % (
                type(obj)))


########NEW FILE########
__FILENAME__ = csvWrapper
import pysal.core.Tables as Tables
import csv

__author__ = "Charles R Schmidt <schmidtc@gmail.com>"
__all__ = ['csvWrapper']


class csvWrapper(Tables.DataTable):

    __doc__ = Tables.DataTable.__doc__

    FORMATS = ['csv']
    READ_MODES = ['r','Ur','rU','U']
    MODES = READ_MODES[:]

    def __init__(self, *args, **kwargs):
        """

        Examples
        --------
        >>> import pysal
        >>> file_name = pysal.examples.get_path('stl_hom.csv')
        >>> f = pysal.open(file_name,'r')
        >>> y = f.read()
        >>> f.header
        ['WKT', 'NAME', 'STATE_NAME', 'STATE_FIPS', 'CNTY_FIPS', 'FIPS', 'FIPSNO', 'HR7984', 'HR8488', 'HR8893', 'HC7984', 'HC8488', 'HC8893', 'PO7984', 'PO8488', 'PO8893', 'PE77', 'PE82', 'PE87', 'RDAC80', 'RDAC85', 'RDAC90']
        >>> f._spec
        [<type 'str'>, <type 'str'>, <type 'str'>, <type 'int'>, <type 'int'>, <type 'int'>, <type 'int'>, <type 'float'>, <type 'float'>, <type 'float'>, <type 'int'>, <type 'int'>, <type 'int'>, <type 'int'>, <type 'int'>, <type 'int'>, <type 'float'>, <type 'float'>, <type 'float'>, <type 'float'>, <type 'float'>, <type 'float'>]


        """
        Tables.DataTable.__init__(self, *args, **kwargs)
        self.__idx = {}
        self.__len = None
        self._open()

    def __len__(self):
        return self.__len

    def _open(self):
        self.fileObj = open(self.dataPath, self.mode)
        if self.mode in self.READ_MODES:
            self.dataObj = csv.reader(self.fileObj)
            data = list(self.dataObj)
            if self._determineHeader(data):
                self.header = data.pop(0)
            else:
                self.header = ['field_%d' % i for i in range(len(data[0]))]
            self._spec = self._determineSpec(data)
            self.data = data
            self.fileObj.close()
            self.__len = len(data)

    def _determineHeader(self, data):
        #head = [val.strip().replace('-','').replace('.','').isdigit() for val in data[0]]
        #if True in head: #no numbers in header!
        #    HEADER = False
        #    return HEADER
        headSpec = self._determineSpec([data[0]])
        restSpec = self._determineSpec(data[1:])
        if headSpec == restSpec:
            HEADER = False
            return HEADER
        return True

    @staticmethod
    def _determineSpec(data):
        cols = len(data[0])
        spec = []
        for j in range(cols):
            isInt = True
            isFloat = True
            for row in data:
                val = row[j]
                if not val.strip().replace('-', '').replace('.', '').isdigit():
                    isInt = False
                    isFloat = False
                    break
                else:
                    if isInt and '.' in val:
                        isInt = False
            if isInt:
                spec.append(int)
            elif isFloat:
                spec.append(float)
            else:
                spec.append(str)
        return spec

    def _read(self):
        if self.pos < len(self):
            row = self.data[self.pos]
            self.pos += 1
            return row
        else:
            return None


########NEW FILE########
__FILENAME__ = dat
import pysal
import os.path
import gwt
from pysal.weights import W
from warnings import warn

__author__ = "Myunghwa Hwang <mhwang4@gmail.com>"
__all__ = ["DatIO"]


class DatIO(gwt.GwtIO):
    """
    Opens, reads, and writes file objects in DAT format.

    Spatial weights objects in DAT format are used in
    Dr. LeSage's MatLab Econ library.
    This DAT format is a simple text file with DAT or dat extension.
    Without header line, it includes three data columns
    for origin id, destination id, and weight values as follows:

    [Line 1]    2    1    0.25
    [Line 2]    5    1    0.50
    ...

    Origin/destination IDs in this file format are simply record
    numbers starting with 1. IDs are not necessarily integers.
    Data values for all columns should be numeric.

    """

    FORMATS = ['dat']
    MODES = ['r', 'w']

    def _read(self):
        """Reads .dat file
        Returns a pysal.weights.weights.W object

        Examples
        --------

        Type 'dir(w)' at the interpreter to see what methods are supported.
        Open .dat file and read it into a pysal weights object

        >>> w = pysal.open(pysal.examples.get_path('wmat.dat'),'r').read()

        Get the number of observations from the header

        >>> w.n
        49

        Get the mean number of neighbors

        >>> w.mean_neighbors
        4.7346938775510203

        Get neighbor distances for a single observation

        >>> w[1]
        {2.0: 0.3333, 5.0: 0.3333, 6.0: 0.3333}

        """
        if self.pos > 0:
            raise StopIteration

        id_type = float
        weights, neighbors = self._readlines(id_type)

        self.pos += 1
        return W(neighbors, weights)

    def write(self, obj):
        """

        Parameters
        ----------
        .write(weightsObject)
        accepts a weights object

        Returns
        ------

        a DAT file
        write a weights object to the opened DAT file.

        Examples
        --------

        >>> import tempfile, pysal, os
        >>> testfile = pysal.open(pysal.examples.get_path('wmat.dat'),'r')
        >>> w = testfile.read()

        Create a temporary file for this example

        >>> f = tempfile.NamedTemporaryFile(suffix='.dat')

        Reassign to new var

        >>> fname = f.name

        Close the temporary named file

        >>> f.close()

        Open the new file in write mode

        >>> o = pysal.open(fname,'w')

        Write the Weights object into the open file

        >>> o.write(w)
        >>> o.close()

        Read in the newly created dat file

        >>> wnew =  pysal.open(fname,'r').read()

        Compare values from old to new

        >>> wnew.pct_nonzero == w.pct_nonzero
        True

        Clean up temporary file created for this example

        >>> os.remove(fname)
        """
        self._complain_ifclosed(self.closed)
        if issubclass(type(obj), W):
            self._writelines(obj)
        else:
            raise TypeError("Expected a pysal weights object, got: %s" % (
                type(obj)))


########NEW FILE########
__FILENAME__ = gal
import pysal.core.FileIO as FileIO
from pysal.weights import W, WSP
from scipy import sparse
import numpy as np

__author__ = 'Charles R Schmidt <schmidtc@gmail.com>'
__all__ = ['GalIO']


class GalIO(FileIO.FileIO):
    """
    Opens, reads, and writes file objects in GAL format.


    """
    FORMATS = ['gal']
    MODES = ['r', 'w']

    def __init__(self, *args, **kwargs):
        self._typ = str
        FileIO.FileIO.__init__(self, *args, **kwargs)
        self.file = open(self.dataPath, self.mode)

    def read(self, n=-1, sparse=False):
        """

        sparse: boolean
               If true return scipy sparse object
               If false return pysal w object
        """
        self._sparse = sparse
        self._complain_ifclosed(self.closed)
        return self._read()

    def seek(self, pos):
        if pos == 0:
            self.file.seek(0)
            self.pos = 0

    def _get_data_type(self):
        return self._typ

    def _set_data_type(self, typ):
        if callable(typ):
            self._typ = typ
        else:
            raise TypeError("Expecting a callable")
    data_type = property(fset=_set_data_type, fget=_get_data_type)

    def _read(self):
        """
        Parameters
        ----------
        reads in a GalIO object

        Returns
        -------
        returns a W object

        Examples
        --------

        >>> import tempfile, pysal, os

        Read in a file GAL file

        >>> testfile = pysal.open(pysal.examples.get_path('sids2.gal'),'r')

        Return a W object

        >>> w = testfile.read()
        >>> w.n == 100
        True
        >>> w.sd == 1.5151237573214935
        True
        >>> testfile = pysal.open(pysal.examples.get_path('sids2.gal'),'r')

        Return a sparse matrix for the w information

        >>> wsp = testfile.read(sparse=True)
        >>> wsp.sparse.nnz
        462

        """
        if self._sparse:
            if self.pos > 0:
                raise StopIteration

            header = self.file.readline().strip().split()
            header_n = len(header)
            n = int(header[0])
            if header_n > 1:
                n = int(header[1])
            ids = []
            idsappend = ids.append
            row = []
            extend = row.extend    # avoid dot in loops
            col = []
            append = col.append
            counter = 0
            typ = self.data_type
            for i in xrange(n):
                id, n_neighbors = self.file.readline().strip().split()
                id = typ(id)
                n_neighbors = int(n_neighbors)
                neighbors_i = map(typ, self.file.readline().strip().split())
                nn = len(neighbors_i)
                extend([id] * nn)
                counter += nn
                for id_neigh in neighbors_i:
                    append(id_neigh)
                idsappend(id)
            self.pos += 1
            row = np.array(row)
            col = np.array(col)
            data = np.ones(counter)
            ids = np.unique(row)
            row = np.array([np.where(ids == j)[0] for j in row]).flatten()
            col = np.array([np.where(ids == j)[0] for j in col]).flatten()
            spmat = sparse.csr_matrix((data, (row, col)), shape=(n, n))
            return WSP(spmat)

        else:
            if self.pos > 0:
                raise StopIteration
            neighbors = {}
            ids = []
            # handle case where more than n is specified in first line
            header = self.file.readline().strip().split()
            header_n = len(header)
            n = int(header[0])
            if header_n > 1:
                n = int(header[1])
            w = {}
            typ = self.data_type
            for i in range(n):
                id, n_neighbors = self.file.readline().strip().split()
                id = typ(id)
                n_neighbors = int(n_neighbors)
                neighbors_i = map(typ, self.file.readline().strip().split())
                neighbors[id] = neighbors_i
                ids.append(id)
            self.pos += 1
            return W(neighbors, id_order=ids)

    def write(self, obj):
        """

        Parameters
        ----------
        .write(weightsObject)
        accepts a weights object

        Returns
        ------

        a GAL file
        write a weights object to the opened GAL file.

        Examples
        --------

        >>> import tempfile, pysal, os
        >>> testfile = pysal.open(pysal.examples.get_path('sids2.gal'),'r')
        >>> w = testfile.read()

        Create a temporary file for this example

        >>> f = tempfile.NamedTemporaryFile(suffix='.gal')

        Reassign to new var

        >>> fname = f.name

        Close the temporary named file

        >>> f.close()

        Open the new file in write mode

        >>> o = pysal.open(fname,'w')

        Write the Weights object into the open file

        >>> o.write(w)
        >>> o.close()

        Read in the newly created gal file

        >>> wnew =  pysal.open(fname,'r').read()

        Compare values from old to new

        >>> wnew.pct_nonzero == w.pct_nonzero
        True

        Clean up temporary file created for this example

        >>> os.remove(fname)
        """
        self._complain_ifclosed(self.closed)
        if issubclass(type(obj), W):
            IDS = obj.id_order
            self.file.write('%d\n' % (obj.n))
            for id in IDS:
                neighbors = obj.neighbors[id]
                self.file.write('%s %d\n' % (str(id), len(neighbors)))
                self.file.write(' '.join(map(str, neighbors)) + '\n')
            self.pos += 1
        else:
            raise TypeError("Expected a pysal weights object, got: %s" %
                            (type(obj)))

    def close(self):
        self.file.close()
        FileIO.FileIO.close(self)



########NEW FILE########
__FILENAME__ = geobugs_txt
import pysal
import os.path
import pysal.core.FileIO as FileIO
from pysal.weights import W
from warnings import warn

__author__ = "Myunghwa Hwang <mhwang4@gmail.com>"
__all__ = ["GeoBUGSTextIO"]


class GeoBUGSTextIO(FileIO.FileIO):
    """
    Opens, reads, and writes weights file objects in the text format
    used in GeoBUGS. GeoBUGS generates a spatial weights matrix
    as an R object and writes it out as an ASCII text representation of
    the R object.

    An exemplary GeoBUGS text file is as follows.
    list([CARD],[ADJ],[WGT],[SUMNUMNEIGH])
    where [CARD] and [ADJ] are required but the others are optional.
    PySAL assumes [CARD] and [ADJ] always exist in an input text file.
    It can read a GeoBUGS text file, even when its content is not written
    in the order of [CARD], [ADJ], [WGT], and [SUMNUMNEIGH].
    It always writes all of [CARD], [ADJ], [WGT], and [SUMNUMNEIGH].
    PySAL does not apply text wrapping during file writing.

    In the above example,

    [CARD]:
        num=c([a list of comma-splitted neighbor cardinalities])

    [ADJ]:
        adj=c([a list of comma-splitted neighbor IDs])
        if caridnality is zero, neighbor IDs are skipped.
        The ordering of observations is the same in both [CARD] and
        [ADJ].
        Neighbor IDs are record numbers starting from one.

    [WGT]:
        weights=c([a list of comma-splitted weights])
        The restrictions for [ADJ] also apply to [WGT].

    [SUMNUMNEIGH]: 
        sumNumNeigh=[The total number of neighbor pairs]
        the total number of neighbor pairs  is an integer
        value and the same as the sum of neighbor cardinalities.

    Notes
    -----
    For the files generated from R spdep nb2WB and dput function,
    it is assumed that the value for the control parameter of dput function
    is NULL. Please refer to R spdep nb2WB function help file.

    References
    ----------
    Thomas, A., Best, N., Lunn, D., Arnold, R., and Spiegelhalter, D.

    (2004) GeoBUGS User Manual.

    R spdep nb2WB function help file.

    """

    FORMATS = ['geobugs_text']
    MODES = ['r', 'w']

    def __init__(self, *args, **kwargs):
        args = args[:2]
        FileIO.FileIO.__init__(self, *args, **kwargs)
        self.file = open(self.dataPath, self.mode)

    def read(self, n=-1):
        """
        Reads GeoBUGS text file

        Returns
        -------
        a pysal.weights.weights.W object

        Examples
        --------

        Type 'dir(w)' at the interpreter to see what methods are supported.
        Open a GeoBUGS text file and read it into a pysal weights object

        >>> w = pysal.open(pysal.examples.get_path('geobugs_scot'),'r','geobugs_text').read()
        WARNING: there are 3 disconnected observations
        Island ids:  [6, 8, 11]

        Get the number of observations from the header

        >>> w.n
        56

        Get the mean number of neighbors

        >>> w.mean_neighbors
        4.1785714285714288

        Get neighbor distances for a single observation

        >>> w[1]
        {9: 1.0, 19: 1.0, 5: 1.0}

        """
        self._complain_ifclosed(self.closed)
        return self._read()

    def seek(self, pos):
        if pos == 0:
            self.file.seek(0)
            self.pos = 0

    def _read(self):
        if self.pos > 0:
            raise StopIteration

        fbody = self.file.read()
        body_structure = {}
        for i in ['num', 'adj', 'weights', 'sumNumNeigh']:
            i_loc = fbody.find(i)
            if i_loc != -1:
                body_structure[i] = (i_loc, i)
        body_sequence = sorted(body_structure.values())
        body_sequence.append((-1, 'eof'))

        for i in range(len(body_sequence) - 1):
            part, next_part = body_sequence[i], body_sequence[i + 1]
            start, end = part[0], next_part[0]
            part_text = fbody[start:end]

            part_length, start, end = len(part_text), 0, -1
            for c in xrange(part_length):
                if part_text[c].isdigit():
                    start = c
                    break

            for c in xrange(part_length - 1, 0, -1):
                if part_text[c].isdigit():
                    end = c + 1
                    break
            part_text = part_text[start: end]
            part_text = part_text.replace('\n', '')
            value_type = int
            if part[1] == 'weights':
                value_type = float
            body_structure[part[1]] = [value_type(v)
                                       for v in part_text.split(',')]

        cardinalities = body_structure['num']
        adjacency = body_structure['adj']
        raw_weights = [1.0] * int(sum(cardinalities))
        if 'weights' in body_structure and isinstance(body_structure['weights'], list):
            raw_weights = body_structure['weights']

        no_obs = len(cardinalities)
        neighbors = {}
        weights = {}
        pos = 0
        for i in xrange(no_obs):
            neighbors[i + 1] = []
            weights[i + 1] = []
            no_nghs = cardinalities[i]
            if no_nghs > 0:
                neighbors[i + 1] = adjacency[pos: pos + no_nghs]
                weights[i + 1] = raw_weights[pos: pos + no_nghs]
            pos += no_nghs

        self.pos += 1
        return W(neighbors, weights)

    def write(self, obj):
        """
        Writes a weights object to the opened text file.

        Parameters
        ----------
        .write(weightsObject)
        accepts a weights object

        Returns
        ------

        a GeoBUGS text file

        Examples
        --------

        >>> import tempfile, pysal, os
        >>> testfile = pysal.open(pysal.examples.get_path('geobugs_scot'),'r','geobugs_text')
        >>> w = testfile.read()
        WARNING: there are 3 disconnected observations
        Island ids:  [6, 8, 11]

        Create a temporary file for this example

        >>> f = tempfile.NamedTemporaryFile(suffix='')

        Reassign to new var

        >>> fname = f.name

        Close the temporary named file

        >>> f.close()

        Open the new file in write mode

        >>> o = pysal.open(fname,'w','geobugs_text')

        Write the Weights object into the open file

        >>> o.write(w)
        >>> o.close()

        Read in the newly created text file

        >>> wnew =  pysal.open(fname,'r','geobugs_text').read()
        WARNING: there are 3 disconnected observations
        Island ids:  [6, 8, 11]

        Compare values from old to new

        >>> wnew.pct_nonzero == w.pct_nonzero
        True

        Clean up temporary file created for this example

        >>> os.remove(fname)

        """
        self._complain_ifclosed(self.closed)
        if issubclass(type(obj), W):

            cardinalities, neighbors, weights = [], [], []
            for i in obj.id_order:
                cardinalities.append(obj.cardinalities[i])
                neighbors.extend(obj.neighbors[i])
                weights.extend(obj.weights[i])

            self.file.write('list(')
            self.file.write('num=c(%s),' % ','.join(map(str, cardinalities)))
            self.file.write('adj=c(%s),' % ','.join(map(str, neighbors)))
            self.file.write('sumNumNeigh=%i)' % sum(cardinalities))
            self.pos += 1

        else:
            raise TypeError("Expected a pysal weights object, got: %s" % (
                type(obj)))

    def close(self):
        self.file.close()
        FileIO.FileIO.close(self)


########NEW FILE########
__FILENAME__ = geoda_txt
import pysal.core.Tables as Tables

__author__ = "Charles R Schmidt <schmidtc@gmail.com>"
__all__ = ['GeoDaTxtReader']


class GeoDaTxtReader(Tables.DataTable):
    """GeoDa Text File Export Format
    """
    __doc__ = Tables.DataTable.__doc__
    FORMATS = ['geoda_txt']
    MODES = ['r']

    def __init__(self, *args, **kwargs):
        """
        Examples
        --------
        >>> import pysal
        >>> f = pysal.open(pysal.examples.get_path('stl_hom.txt'),'r')
        >>> f.header
        ['FIPSNO', 'HR8488', 'HR8893', 'HC8488']
        >>> len(f)
        78
        >>> f.dat[0]
        ['17107', '1.290722', '1.624458', '2']
        >>> f.dat[-1]
        ['29223', '0', '8.451537', '0']
        >>> f._spec
        [<type 'int'>, <type 'float'>, <type 'float'>, <type 'int'>]

        """
        Tables.DataTable.__init__(self, *args, **kwargs)
        self.__idx = {}
        self.__len = None
        self.pos = 0
        self._open()

    def _open(self):
        if self.mode == 'r':
            self.fileObj = open(self.dataPath, 'r')
            n, k = self.fileObj.readline().strip().split(',')
            n, k = int(n), int(k)
            header = self.fileObj.readline().strip().split(',')
            self.header = [f.replace('"', '') for f in header]
            try:
                assert len(self.header) == k
            except AssertionError:
                raise TypeError("This is not a valid geoda_txt file.")
            dat = self.fileObj.readlines()
            self.dat = [line.strip().split(',') for line in dat]
            self._spec = self._determineSpec(self.dat)
            self.__len = len(dat)

    def __len__(self):
        return self.__len

    def _read(self):
        if self.pos < len(self):
            row = self.dat[self.pos]
            self.pos += 1
            return row
        else:
            raise None

    def close(self):
        self.fileObj.close()
        Tables.DataTable.close(self)

    @staticmethod
    def _determineSpec(data):
        cols = len(data[0])
        spec = []
        for j in range(cols):
            isInt = True
            isFloat = True
            for row in data:
                val = row[j]
                if not val.strip().replace('-', '').replace('.', '').isdigit():
                    isInt = False
                    isFloat = False
                    break
                else:
                    if isInt and '.' in val:
                        isInt = False
            if isInt:
                spec.append(int)
            elif isFloat:
                spec.append(float)
            else:
                spec.append(str)
        return spec

########NEW FILE########
__FILENAME__ = gwt
import pysal
import os.path
import pysal.core.FileIO as FileIO
from pysal.weights import W
from warnings import warn

__author__ = "Charles R Schmidt <schmidtc@gmail.com>"
__all__ = ["GwtIO"]


class unique_filter(object):
    """
    Util function:
    When a new instance is passed as an arugment to the builtin filter
    it will remove duplicate entries without changing the order of the list.

    Besure to ceate a new instance everytime, unless you want a global filter.

    Example:
    >>> l = ['a','a','b','a','c','v','d','a','v','d']
    >>> filter(unique_filter(),l)
    ['a', 'b', 'c', 'v', 'd']
    """
    def __init__(self):
        self.exclude = set()

    def __call__(self, x):
        if x in self.exclude:
            return False
        else:
            self.exclude.add(x)
            return True


class GwtIO(FileIO.FileIO):

    FORMATS = ['kwt','gwt']
    MODES = ['r', 'w']

    def __init__(self, *args, **kwargs):
        self._varName = 'Unknown'
        self._shpName = 'Unknown'
        FileIO.FileIO.__init__(self, *args, **kwargs)
        self.file = open(self.dataPath, self.mode)

    def _set_varName(self, val):
        if issubclass(type(val), basestring):
            self._varName = val

    def _get_varName(self):
        return self._varName
    varName = property(fget=_get_varName, fset=_set_varName)

    def _set_shpName(self, val):
        if issubclass(type(val), basestring):
            self._shpName = val

    def _get_shpName(self):
        return self._shpName
    shpName = property(fget=_get_shpName, fset=_set_shpName)

    def read(self, n=-1):
        self._complain_ifclosed(self.closed)
        return self._read()

    def seek(self, pos):
        if pos == 0:
            self.file.seek(0)
            self.pos = 0

    def _readlines(self, id_type, ret_ids=False):
        """
        Reads the main body of gwt-like weights files
        into two dictionaries containing weights and neighbors.
        This code part is repeatedly used for many weight file formats.
        Header lines, however, are different from format to format.
        So, for code reusability, this part is separated out from
        _read function by Myunghwa Hwang.
        """
        data = [row.strip().split() for row in self.file.readlines()]
        ids = filter(unique_filter(), [x[0] for x in data])
        ids = map(id_type, ids)
        WN = {}
        for id in ids:  # note: fromkeys is no good here, all keys end up sharing the say dict value
            WN[id] = {}
        for i, j, v in data:
            i = id_type(i)
            j = id_type(j)
            WN[i][j] = float(v)
        weights = {}
        neighbors = {}
        for i in WN:
            weights[i] = WN[i].values()
            neighbors[i] = WN[i].keys()
        if ret_ids:
            return weights, neighbors, ids
        else:
            return weights, neighbors

    def _read(self):
        """Reads .gwt file
        Returns a pysal.weights.weights.W object

        Examples
        --------

        Type 'dir(f)' at the interpreter to see what methods are supported.
        Open .gwt file and read it into a pysal weights object

        >>> f = pysal.open(pysal.examples.get_path('juvenile.gwt'),'r').read()

        Get the number of observations from the header

        >>> f.n
        168

        Get the mean number of neighbors

        >>> f.mean_neighbors
        16.678571428571427

        Get neighbor distances for a single observation

        >>> f[1]
        {2: 14.1421356}


        """
        if self.pos > 0:
            raise StopIteration

        flag, n, shp, id_var = self.file.readline().strip().split()
        self.shpName = shp
        self.varName = id_var
        id_order = None
        id_type = str
        try:
            base = os.path.split(self.dataPath)[0]
            dbf = os.path.join(base, self.shpName.replace('.shp', '') + '.dbf')
            if os.path.exists(dbf):
                db = pysal.open(dbf, 'r')
                if id_var in db.header:
                    id_order = db.by_col(id_var)
                    id_type = type(id_order[0])
                else:
                    warn("ID_VAR:'%s' was not in the DBF header, proceeding with unordered string ids." % (id_var), RuntimeWarning)
            else:
                warn("DBF relating to GWT was not found, proceeding with unordered string ids.", RuntimeWarning)
        except:
            warn("Exception occurred will reading DBF, proceeding with unordered string ids.", RuntimeWarning)
        self.flag = flag
        self.n = n
        self.shp = shp
        self.id_var = id_var
        if id_order is None:
            weights, neighbors, id_order = self._readlines(id_type, True)
        else:
            weights, neighbors = self._readlines(id_type)

        self.pos += 1
        w = W(neighbors, weights, id_order)
        #w.transform = 'b'
        #set meta data
        w._shpName = self.shpName
        w._varName = self.varName
        #warn("Weights have been converted to binary. To retrieve original values use w.transform='o'", RuntimeWarning)
        return w

    def _writelines(self, obj):
        """
        Writes  the main body of gwt-like weights files.
        This code part is repeatedly used for many weight file formats.
        Header lines, however, are different from format to format.
        So, for code reusability, this part is separated out from
        write function by Myunghwa Hwang.
        """
        for id in obj.id_order:
            neighbors = zip(obj.neighbors[id], obj.weights[id])
            str_id = "_".join(str(id).split())
            for neighbor, weight in neighbors:
                neighbor = "_".join(str(neighbor).split())

                self.file.write('%s %s %6G\n' % (str_id,
                                                 neighbor, weight))
                self.pos += 1

    def write(self, obj):
        """

        Parameters
        ----------
        .write(weightsObject)
        accepts a weights object

        Returns
        ------

        a GWT file
        write a weights object to the opened GWT file.

        Examples
        --------

        >>> import tempfile, pysal, os
        >>> testfile = pysal.open(pysal.examples.get_path('juvenile.gwt'),'r')
        >>> w = testfile.read()

        Create a temporary file for this example

        >>> f = tempfile.NamedTemporaryFile(suffix='.gwt')

        Reassign to new var

        >>> fname = f.name

        Close the temporary named file

        >>> f.close()

        Open the new file in write mode

        >>> o = pysal.open(fname,'w')

        Write the Weights object into the open file

        >>> o.write(w)
        >>> o.close()

        Read in the newly created gwt file

        >>> wnew =  pysal.open(fname,'r').read()

        Compare values from old to new

        >>> wnew.pct_nonzero == w.pct_nonzero
        True

        Clean up temporary file created for this example

        >>> os.remove(fname)
        """
        self._complain_ifclosed(self.closed)
        if issubclass(type(obj), W):
            #transform = obj.transform
            #obj.transform = 'o'
            if hasattr(obj, '_shpName'):
                self.shpName = obj._shpName
            if hasattr(obj, '_varName'):
                self.varName = obj._varName
            header = '%s %i %s %s\n' % ('0', obj.n, self.shpName, self.varName)
            self.file.write(header)
            self._writelines(obj)
            #obj.transform = transform

        else:
            raise TypeError("Expected a pysal weights object, got: %s" % (
                type(obj)))

    def close(self):
        self.file.close()
        FileIO.FileIO.close(self)

    @staticmethod
    def __zero_offset(neighbors, weights, original_ids=None):
        if not original_ids:
            original_ids = neighbors.keys()
        old_weights = weights
        new_weights = {}
        new_ids = {}
        old_ids = {}
        new_neighbors = {}
        for i in original_ids:
            new_i = original_ids.index(i)
            new_ids[new_i] = i
            old_ids[i] = new_i
            neighbors_i = neighbors[i]
            new_neighbors_i = [original_ids.index(j) for j in neighbors_i]
            new_neighbors[new_i] = new_neighbors_i
            new_weights[new_i] = weights[i]
        info = {}
        info['new_ids'] = new_ids
        info['old_ids'] = old_ids
        info['new_neighbors'] = new_neighbors
        info['new_weights'] = new_weights
        return info


########NEW FILE########
__FILENAME__ = mat
import pysal
import os.path
import scipy.io as sio
import pysal.core.FileIO as FileIO
from pysal.weights import W
from pysal.weights.util import full, full2W
from warnings import warn

__author__ = "Myunghwa Hwang <mhwang4@gmail.com>"
__all__ = ["MatIO"]


class MatIO(FileIO.FileIO):
    """
    Opens, reads, and writes weights file objects in MATLAB Level 4-5 MAT format.

    MAT files are used in Dr. LeSage's MATLAB Econometrics library.
    The MAT file format can handle both full and sparse matrices,
    and it allows for a matrix dimension greater than 256.
    In PySAL, row and column headers of a MATLAB array are ignored.

    PySAL uses matlab io tools in scipy.
    Thus, it is subject to all limits that loadmat and savemat in scipy have.

    Notes
    -----
    If a given weights object contains too many observations to
    write it out as a full matrix,
    PySAL writes out the object as a sparse matrix.

    References
    ----------
    MathWorks (2011) "MATLAB 7 MAT-File Format" at
    http://www.mathworks.com/help/pdf_doc/matlab/matfile_format.pdf.

    scipy matlab io
    http://docs.scipy.org/doc/scipy/reference/tutorial/io.html

    """

    FORMATS = ['mat']
    MODES = ['r', 'w']

    def __init__(self, *args, **kwargs):
        self._varName = 'Unknown'
        FileIO.FileIO.__init__(self, *args, **kwargs)
        self.file = open(self.dataPath, self.mode + 'b')

    def _set_varName(self, val):
        if issubclass(type(val), basestring):
            self._varName = val

    def _get_varName(self):
        return self._varName
    varName = property(fget=_get_varName, fset=_set_varName)

    def read(self, n=-1):
        self._complain_ifclosed(self.closed)
        return self._read()

    def seek(self, pos):
        if pos == 0:
            self.file.seek(0)
            self.pos = 0

    def _read(self):
        """Reads MATLAB mat file
        Returns a pysal.weights.weights.W object

        Examples
        --------

        Type 'dir(w)' at the interpreter to see what methods are supported.
        Open a MATLAB mat file and read it into a pysal weights object

        >>> w = pysal.open(pysal.examples.get_path('spat-sym-us.mat'),'r').read()

        Get the number of observations from the header

        >>> w.n
        46

        Get the mean number of neighbors

        >>> w.mean_neighbors
        4.0869565217391308

        Get neighbor distances for a single observation

        >>> w[1]
        {25: 1, 3: 1, 28: 1, 39: 1}

        """
        if self.pos > 0:
            raise StopIteration

        mat = sio.loadmat(self.file)
        mat_keys = [k for k in mat if not k.startswith("_")]
        full_w = mat[mat_keys[0]]

        self.pos += 1
        return full2W(full_w)

    def write(self, obj):
        """

        Parameters
        ----------
        .write(weightsObject)
        accepts a weights object

        Returns
        ------

        a MATLAB mat file
        write a weights object to the opened mat file.

        Examples
        --------

        >>> import tempfile, pysal, os
        >>> testfile = pysal.open(pysal.examples.get_path('spat-sym-us.mat'),'r')
        >>> w = testfile.read()

        Create a temporary file for this example

        >>> f = tempfile.NamedTemporaryFile(suffix='.mat')

        Reassign to new var

        >>> fname = f.name

        Close the temporary named file

        >>> f.close()

        Open the new file in write mode

        >>> o = pysal.open(fname,'w')

        Write the Weights object into the open file

        >>> o.write(w)
        >>> o.close()

        Read in the newly created mat file

        >>> wnew =  pysal.open(fname,'r').read()

        Compare values from old to new

        >>> wnew.pct_nonzero == w.pct_nonzero
        True

        Clean up temporary file created for this example

        >>> os.remove(fname)

        """
        self._complain_ifclosed(self.closed)
        if issubclass(type(obj), W):
            try:
                w = full(obj)[0]
            except ValueError:
                w = obj.sparse
            sio.savemat(self.file, {'WEIGHT': w})
            self.pos += 1
        else:
            raise TypeError("Expected a pysal weights object, got: %s" % (
                type(obj)))

    def close(self):
        self.file.close()
        FileIO.FileIO.close(self)


########NEW FILE########
__FILENAME__ = mtx
import pysal
import os.path
import scipy.io as sio
import pysal.core.FileIO as FileIO
from pysal.weights import W, WSP
from pysal.weights.util import full, full2W
from warnings import warn

__author__ = "Myunghwa Hwang <mhwang4@gmail.com>"
__all__ = ["MtxIO"]


class MtxIO(FileIO.FileIO):
    """
    Opens, reads, and writes weights file objects in Matrix Market MTX format.

    The Matrix Market MTX format is used to facilitate the exchange of matrix data.
    In PySAL, it is being tested as a new file format for delivering
    the weights information of a spatial weights matrix.
    Although the MTX format supports both full and sparse matrices with different
    data types, it is assumed that spatial weights files in the mtx format always
    use the sparse (or coordinate) format with real data values.
    For now, no additional assumption (e.g., symmetry) is made of the structure
    of a weights matrix.

    With the above assumptions,
    the structure of a MTX file containing a spatial weights matrix
    can be defined as follows:
    %%MatrixMarket matrix coordinate real general <--- header 1 (constant)
    % Comments starts                             <---
    % ....                                           | 0 or more comment lines
    % Comments ends                               <---
    M    N    L                                   <--- header 2, rows, columns, entries
    I1   J1   A(I1,J1)                            <---
    ...                                              | L entry lines
    IL   JL   A(IL,JL)                            <---

    In the MTX foramt, the index for rows or columns starts with 1.

    PySAL uses mtx io tools in scipy.
    Thus, it is subject to all limits that scipy currently has.
    Reengineering might be required, since scipy currently reads in
    the entire entry into memory.

    References
    ----------
    MTX format specification
    http://math.nist.gov/MatrixMarket/formats.html

    scipy matlab io
    http://docs.scipy.org/doc/scipy/reference/tutorial/io.html

    """

    FORMATS = ['mtx']
    MODES = ['r', 'w']

    def __init__(self, *args, **kwargs):
        FileIO.FileIO.__init__(self, *args, **kwargs)
        self.file = open(self.dataPath, self.mode + 'b')

    def read(self, n=-1, sparse=False):
        """
        sparse: boolean
                if true, return pysal WSP object
                if false, return pysal W object
        """
        self._sparse = sparse
        self._complain_ifclosed(self.closed)
        return self._read()

    def seek(self, pos):
        if pos == 0:
            self.file.seek(0)
            self.pos = 0

    def _read(self):
        """Reads MatrixMarket mtx file
        Returns a pysal.weights.weights.W or pysal.weights.weights.WSP object

        Examples
        --------

        Type 'dir(w)' at the interpreter to see what methods are supported.
        Open a MatrixMarket mtx file and read it into a pysal weights object

        >>> f = pysal.open(pysal.examples.get_path('wmat.mtx'),'r')

        >>> w = f.read()

        Get the number of observations from the header

        >>> w.n
        49

        Get the mean number of neighbors

        >>> w.mean_neighbors
        4.7346938775510203

        Get neighbor weights for a single observation

        >>> w[1]
        {2: 0.33329999999999999, 5: 0.33329999999999999, 6: 0.33329999999999999}

        >>> f.close()

        >>> f = pysal.open(pysal.examples.get_path('wmat.mtx'),'r')

        >>> wsp = f.read(sparse=True)

        Get the number of observations from the header

        >>> wsp.n
        49

        Get row from the weights matrix. Note that the first row in the sparse
        matrix (the 0th row) corresponds to ID 1 from the original mtx file
        read in.

        >>> print wsp.sparse[0].todense()
        [[ 0.      0.3333  0.      0.      0.3333  0.3333  0.      0.      0.      0.
           0.      0.      0.      0.      0.      0.      0.      0.      0.      0.
           0.      0.      0.      0.      0.      0.      0.      0.      0.      0.
           0.      0.      0.      0.      0.      0.      0.      0.      0.      0.
           0.      0.      0.      0.      0.      0.      0.      0.      0.    ]]

        """
        if self.pos > 0:
            raise StopIteration
        mtx = sio.mmread(self.file)
        ids = range(1, mtx.shape[0] + 1)  # matrix market indexes start at one
        wsp = WSP(mtx, ids)
        if self._sparse:
            w = wsp
        else:
            w = pysal.weights.WSP2W(wsp)
        self.pos += 1
        return w

    def write(self, obj):
        """

        Parameters
        ----------
        .write(weightsObject)
        accepts a weights object

        Returns
        ------

        a MatrixMarket mtx file
        write a weights object to the opened mtx file.

        Examples
        --------

        >>> import tempfile, pysal, os
        >>> testfile = pysal.open(pysal.examples.get_path('wmat.mtx'),'r')
        >>> w = testfile.read()

        Create a temporary file for this example

        >>> f = tempfile.NamedTemporaryFile(suffix='.mtx')

        Reassign to new var

        >>> fname = f.name

        Close the temporary named file

        >>> f.close()

        Open the new file in write mode

        >>> o = pysal.open(fname,'w')

        Write the Weights object into the open file

        >>> o.write(w)
        >>> o.close()

        Read in the newly created mtx file

        >>> wnew =  pysal.open(fname,'r').read()

        Compare values from old to new

        >>> wnew.pct_nonzero == w.pct_nonzero
        True

        Clean up temporary file created for this example

        >>> os.remove(fname)

        Go to the beginning of the test file

        >>> testfile.seek(0)

        Create a sparse weights instance from the test file

        >>> wsp = testfile.read(sparse=True)

        Open the new file in write mode

        >>> o = pysal.open(fname,'w')

        Write the sparse weights object into the open file

        >>> o.write(wsp)
        >>> o.close()

        Read in the newly created mtx file

        >>> wsp_new =  pysal.open(fname,'r').read(sparse=True)

        Compare values from old to new

        >>> wsp_new.s0 == wsp.s0
        True

        Clean up temporary file created for this example

        >>> os.remove(fname)

        """
        self._complain_ifclosed(self.closed)
        if issubclass(type(obj), W) or issubclass(type(obj), WSP):
            w = obj.sparse
            sio.mmwrite(self.file, w, comment='Generated by PySAL',
                        field='real', precision=7)
            self.pos += 1
        else:
            raise TypeError("Expected a pysal weights object, got: %s" % (
                type(obj)))

    def close(self):
        self.file.close()
        FileIO.FileIO.close(self)



########NEW FILE########
__FILENAME__ = pyDbfIO
import pysal.core.Tables
import datetime
import struct
import itertools
from warnings import warn
import pysal

__author__ = "Charles R Schmidt <schmidtc@gmail.com>"
__all__ = ['DBF']


class DBF(pysal.core.Tables.DataTable):
    """
    PySAL DBF Reader/Writer

    This DBF handler implements the PySAL DataTable interface.

    Attributes
    ----------

    header      : list
                  A list of field names. The header is a python list of
                  strings.  Each string is a field name and field name must
                  not be longer than 10 characters.
    field_spec  : list
                  A list describing the data types of each field. It is
                  comprised of a list of tuples, each tuple describing a
                  field. The format for the tuples is ("Type",len,precision).
                  Valid Types are 'C' for characters, 'L' for bool, 'D' for
                  data, 'N' or 'F' for number.

    Examples
    --------

    >>> import pysal
    >>> dbf = pysal.open(pysal.examples.get_path('juvenile.dbf'), 'r')
    >>> dbf.header
    ['ID', 'X', 'Y']
    >>> dbf.field_spec
    [('N', 9, 0), ('N', 9, 0), ('N', 9, 0)]

    """
    FORMATS = ['dbf']
    MODES = ['r', 'w']

    def __init__(self, *args, **kwargs):
        """
        Initializes an instance of the pysal's DBF handler.

        Arguments:
        dataPath -- str -- Path to file, including file.
        mode -- str -- 'r' or 'w'
        """
        pysal.core.Tables.DataTable.__init__(self, *args, **kwargs)
        if self.mode == 'r':
            self.f = f = open(self.dataPath, 'rb')
            numrec, lenheader = struct.unpack('<xxxxLH22x', f.read(32))
            numfields = (lenheader - 33) // 32
            self.n_records = numrec
            self.n_fields = numfields
            self.field_info = [('DeletionFlag', 'C', 1, 0)]
            record_size = 1
            fmt = 's'
            self._col_index = {}
            idx = 0
            for fieldno in xrange(numfields):
                name, typ, size, deci = struct.unpack(
                    '<11sc4xBB14x', f.read(32))
                name = name.replace('\0', '')
                    # eliminate NULs from string
                self._col_index[name] = (idx, record_size)
                idx += 1
                fmt += '%ds' % size
                record_size += size
                self.field_info.append((name, typ, size, deci))
            terminator = f.read(1)
            assert terminator == '\r'
            self.header_size = self.f.tell()
            self.record_size = record_size
            self.record_fmt = fmt
            self.pos = 0
            self.header = [fInfo[0] for fInfo in self.field_info[1:]]
            field_spec = []
            for fname, ftype, flen, fpre in self.field_info[1:]:
                field_spec.append((ftype, flen, fpre))
            self.field_spec = field_spec

            #self.spec = [types[fInfo[0]] for fInfo in self.field_info]
        elif self.mode == 'w':
            self.f = open(self.dataPath, 'wb')
            self.header = None
            self.field_spec = None
            self.numrec = 0
            self.FIRST_WRITE = True

    def __len__(self):
        if self.mode != 'r':
            raise IOError("Invalid operation, Cannot read from a file opened in 'w' mode.")
        return self.n_records

    def seek(self, i):
        self.f.seek(self.header_size + (self.record_size * i))
        self.pos = i

    def _get_col(self, key):
        """return the column vector"""
        if key not in self._col_index:
            raise AttributeError('Field: % s does not exist in header' % key)
        prevPos = self.tell()
        idx, offset = self._col_index[key]
        typ, size, deci = self.field_spec[idx]
        gap = (self.record_size - size)
        f = self.f
        f.seek(self.header_size + offset)
        col = [0] * self.n_records
        for i in xrange(self.n_records):
            value = f.read(size)
            f.seek(gap, 1)
            if typ == 'N':
                value = value.replace('\0', '').lstrip()
                if value == '':
                    value = pysal.MISSINGVALUE
                elif deci:
                    try:
                        value = float(value)
                    except ValueError:
                        value = pysal.MISSINGVALUE
                else:
                    try:
                        value = int(value)
                    except ValueError:
                        value = pysal.MISSINGVALUE
            elif typ == 'D':
                try:
                    y, m, d = int(value[:4]), int(value[4:6]), int(value[6:8])
                    value = datetime.date(y, m, d)
                except ValueError:
                    value = pysal.MISSINGVALUE
            elif typ == 'L':
                value = (value in 'YyTt' and 'T') or (
                    value in 'NnFf' and 'F') or '?'
            elif typ == 'F':
                value = value.replace('\0', '').lstrip()
                if value == '':
                    value = pysal.MISSINGVALUE
                else:
                    value = float(value)
            if isinstance(value, str):
                value = value.rstrip()
            col[i] = value
        self.seek(prevPos)
        return col

    def read_record(self, i):
        self.seek(i)
        rec = list(struct.unpack(
            self.record_fmt, self.f.read(self.record_size)))
        if rec[0] != ' ':
            return self.read_record(i + 1)
        result = []
        for (name, typ, size, deci), value in itertools.izip(self.field_info, rec):
            if name == 'DeletionFlag':
                continue
            if typ == 'N':
                value = value.replace('\0', '').lstrip()
                if value == '':
                    value = pysal.MISSINGVALUE
                elif deci:
                    try:
                        value = float(value)
                    except ValueError:
                        value = pysal.MISSINGVALUE
                else:
                    try:
                        value = int(value)
                    except ValueError:
                        value = pysal.MISSINGVALUE
            elif typ == 'D':
                try:
                    y, m, d = int(value[:4]), int(value[4:6]), int(value[6:8])
                    value = datetime.date(y, m, d)
                except ValueError:
                    #value = datetime.date.min#NULL Date: See issue 114
                    value = pysal.MISSINGVALUE
            elif typ == 'L':
                value = (value in 'YyTt' and 'T') or (
                    value in 'NnFf' and 'F') or '?'
            elif typ == 'F':
                value = value.replace('\0', '').lstrip()
                if value == '':
                    value = pysal.MISSINGVALUE
                else:
                    value = float(value)
            if isinstance(value, str):
                value = value.rstrip()
            result.append(value)
        return result

    def _read(self):
        if self.mode != 'r':
            raise IOError("Invalid operation, Cannot read from a file opened in 'w' mode.")
        if self.pos < len(self):
            rec = self.read_record(self.pos)
            self.pos += 1
            return rec
        else:
            return None

    def write(self, obj):
        self._complain_ifclosed(self.closed)
        if self.mode != 'w':
            raise IOError("Invalid operation, Cannot write to a file opened in 'r' mode.")
        if self.FIRST_WRITE:
            self._firstWrite(obj)
        if len(obj) != len(self.header):
            raise TypeError("Rows must contains %d fields" % len(self.header))
        self.numrec += 1
        self.f.write(' ')                        # deletion flag
        for (typ, size, deci), value in itertools.izip(self.field_spec, obj):
            if value is None:
                if typ == 'C':
                    value = ' ' * size
                else:
                    value = '\0' * size
            elif typ == "N" or typ == "F":
                v = str(value).rjust(size, ' ')
                #if len(v) == size:
                #    value = v
                #else:
                value = (("%" + "%d.%d" % (size, deci) + "f") % (value))[:size]
            elif typ == 'D':
                value = value.strftime('%Y%m%d')
            elif typ == 'L':
                value = str(value)[0].upper()
            else:
                value = str(value)[:size].ljust(size, ' ')
            try:
                assert len(value) == size
            except:
                print value, len(value), size
                raise
            self.f.write(value)
            self.pos += 1

    def flush(self):
        self._complain_ifclosed(self.closed)
        self._writeHeader()
        self.f.flush()

    def close(self):
        if self.mode == 'w':
            self.flush()
            # End of file
            self.f.write('\x1A')
        self.f.close()
        pysal.core.Tables.DataTable.close(self)

    def _firstWrite(self, obj):
        if not self.header:
            raise IOError("No header, DBF files require a header.")
        if not self.field_spec:
            raise IOError("No field_spec, DBF files require a specification.")
        self._writeHeader()
        self.FIRST_WRITE = False

    def _writeHeader(self):
        """ Modified from: http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/362715 """
        POS = self.f.tell()
        self.f.seek(0)
        ver = 3
        now = datetime.datetime.now()
        yr, mon, day = now.year - 1900, now.month, now.day
        numrec = self.numrec
        numfields = len(self.header)
        lenheader = numfields * 32 + 33
        lenrecord = sum(field[1] for field in self.field_spec) + 1
        hdr = struct.pack('<BBBBLHH20x', ver, yr, mon, day, numrec,
                          lenheader, lenrecord)
        self.f.write(hdr)
        # field specs
        for name, (typ, size, deci) in itertools.izip(self.header, self.field_spec):
            name = name.ljust(11, '\x00')
            fld = struct.pack('<11sc4xBB14x', name, typ, size, deci)
            self.f.write(fld)
        # terminator
        self.f.write('\r')
        if self.f.tell() != POS and not self.FIRST_WRITE:
            self.f.seek(POS)

if __name__ == '__main__':
    import pysal
    file_name = pysal.examples.get_path("10740.dbf")
    f = pysal.open(file_name, 'r')
    newDB = pysal.open('copy.dbf', 'w')
    newDB.header = f.header
    newDB.field_spec = f.field_spec
    print f.header
    for row in f:
        print row
        newDB.write(row)
    newDB.close()
    copy = pysal.open('copy.dbf', 'r')
    f.seek(0)
    print "HEADER: ", copy.header == f.header
    print "SPEC: ", copy.field_spec == f.field_spec
    print "DATA: ", list(copy) == list(f)

########NEW FILE########
__FILENAME__ = pyShpIO
"""
PySAL ShapeFile Reader and Writer based on pure python shapefile module.

"""

__author__ = "Charles R Schmidt <schmidtc@gmail.com>"
__credits__ = "Copyright (c) 2009 Charles R. Schmidt"
__all__ = ['PurePyShpWrapper']

#import pysal
import pysal.core.FileIO  # as FileIO
from pysal.core.util import shp_file
import pysal.cg as cg
from warnings import warn
import unittest

STRING_TO_TYPE = {'POLYGON': cg.Polygon, 'POINT': cg.Point, 'POINTM':
                  cg.Point, 'POINTZ': cg.Point, 'ARC': cg.Chain, 'POLYGONZ': cg.Polygon}
TYPE_TO_STRING = {cg.Polygon: 'POLYGON', cg.Point: 'POINT', cg.Chain:
                  'ARC'}  # build the reverse map
#for key,value in STRING_TO_TYPE.iteritems():
#    TYPE_TO_STRING[value] = key


class PurePyShpWrapper(pysal.core.FileIO.FileIO):
    """
    FileIO handeler for ESRI ShapeFiles.

    Notes
    -----
    This class wraps _pyShpIO's shp_file class with the PySAL FileIO API.
    shp_file can be used without PySAL.

    Attributes
    ----------

    Formats     : list
                  A list of support file extensions
    Modes       : list
                  A list of support file modes

    Examples
    --------

    >>> import tempfile
    >>> f = tempfile.NamedTemporaryFile(suffix='.shp'); fname = f.name; f.close()
    >>> import pysal
    >>> i = pysal.open(pysal.examples.get_path('10740.shp'),'r')
    >>> o = pysal.open(fname,'w')
    >>> for shp in i:
    ...     o.write(shp)
    >>> o.close()
    >>> open(pysal.examples.get_path('10740.shp'),'rb').read() == open(fname,'rb').read()
    True
    >>> open(pysal.examples.get_path('10740.shx'),'rb').read() == open(fname[:-1]+'x','rb').read()
    True
    >>> import os
    >>> os.remove(fname); os.remove(fname.replace('.shp','.shx'))

    """
    FORMATS = ['shp', 'shx']
    MODES = ['w', 'r', 'wb', 'rb']

    def __init__(self, *args, **kwargs):
        pysal.core.FileIO.FileIO.__init__(self, *args, **kwargs)
        self.dataObj = None
        if self.mode == 'r' or self.mode == 'rb':
            self.__open()
        elif self.mode == 'w' or self.mode == 'wb':
            self.__create()

    def __len__(self):
        if self.dataObj:
            return len(self.dataObj)
        else:
            return 0

    def __open(self):
        self.dataObj = shp_file(self.dataPath)
        self.header = self.dataObj.header
        self.bbox = self.dataObj.bbox
        try:
            self.type = STRING_TO_TYPE[self.dataObj.type()]
        except KeyError:
            raise TypeError('%s does not support shapes of type: %s'
                            % (self.__class__.__name__, self.dataObj.type()))

    def __create(self):
        self.write = self.__firstWrite

    def __firstWrite(self, shape):
        self.type = TYPE_TO_STRING[type(shape)]
        if self.type == 'POINT':
            if len(shape) == 3:
                self.type = 'POINTM'
            if len(shape) == 4:
                self.type = 'POINTZ'
        self.dataObj = shp_file(self.dataPath, 'w', self.type)
        self.write = self.__writer
        self.write(shape)

    def __writer(self, shape):
        if TYPE_TO_STRING[type(shape)] != self.type:
            raise TypeError("This file only supports %s type shapes" %
                            self.type)
        rec = {}
        rec['Shape Type'] = shp_file.SHAPE_TYPES[self.type]
        if self.type == 'POINT':
            rec['X'] = shape[0]
            rec['Y'] = shape[1]
            if len(shape) > 2:
                rec['M'] = shape[2]
            if len(shape) > 3:
                rec['Z'] = shape[3]
            shape = rec
        else:
            rec['BBOX Xmin'] = shape.bounding_box.left
            rec['BBOX Ymin'] = shape.bounding_box.lower
            rec['BBOX Xmax'] = shape.bounding_box.right
            rec['BBOX Ymax'] = shape.bounding_box.upper
            if self.type == 'POLYGON':
                holes = [hole[::-1] for hole in shape.holes if hole]
                    #holes should be in CCW order
                rec['NumParts'] = len(shape.parts) + len(holes)
                all_parts = shape.parts + holes
            else:
                rec['NumParts'] = len(shape.parts)
                all_parts = shape.parts
            partsIndex = [0]
            for l in map(len, all_parts)[:-1]:
                partsIndex.append(partsIndex[-1] + l)
            rec['Parts Index'] = partsIndex
            verts = sum(all_parts, [])
            verts = [(x, y) for x, y in verts]
            rec['NumPoints'] = len(verts)
            rec['Vertices'] = verts
        self.dataObj.add_shape(rec)
        self.pos += 1

    def _read(self):
        try:
            rec = self.dataObj.get_shape(self.pos)
        except IndexError:
            return None
        self.pos += 1
        if self.dataObj.type() == 'POINT':
            shp = self.type((rec['X'], rec['Y']))
        elif self.dataObj.type() == 'POINTZ':
            shp = self.type((rec['X'], rec['Y']))
            shp.Z = rec['Z']
            shp.M = rec['M']
        else:
            if rec['NumParts'] > 1:
                partsIndex = list(rec['Parts Index'])
                partsIndex.append(None)
                parts = [rec['Vertices'][partsIndex[i]:partsIndex[
                    i + 1]] for i in xrange(rec['NumParts'])]
                if self.dataObj.type() == 'POLYGON':
                    is_cw = map(pysal.cg.is_clockwise, parts)
                    vertices = [part for part, cw in zip(parts, is_cw) if cw]
                    holes = [part for part, cw in zip(parts, is_cw) if not cw]
                    if not holes:
                        holes = None
                    shp = self.type(vertices, holes)
                else:
                    vertices = parts
                    shp = self.type(vertices)
            elif rec['NumParts'] == 1:
                vertices = rec['Vertices']
                if self.dataObj.type() == 'POLYGON' and not pysal.cg.is_clockwise(vertices):
                    ### SHAPEFILE WARNING: Polygon %d topology has been fixed. (ccw -> cw)
                    warn("SHAPEFILE WARNING: Polygon %d topology has been fixed. (ccw -> cw)" % (self.pos), RuntimeWarning)
                    print "SHAPEFILE WARNING: Polygon %d topology has been fixed. (ccw -> cw)" % (self.pos)

                shp = self.type(vertices)
            else:
                warn("Polygon %d has zero parts" % self.pos, RuntimeWarning)
                shp = self.type([[]])
                #raise ValueError, "Polygon %d has zero parts"%self.pos
        if self.ids:
            shp.id = self.rIds[self.pos - 1]  # shp IDs start at 1.
        else:
            shp.id = self.pos  # shp IDs start at 1.
        return shp

    def close(self):
        self.dataObj.close()
        pysal.core.FileIO.FileIO.close(self)


########NEW FILE########
__FILENAME__ = stata_txt
import pysal
import os.path
import pysal.core.FileIO as FileIO
from pysal.weights import W
from warnings import warn

__author__ = "Myunghwa Hwang <mhwang4@gmail.com>"
__all__ = ["StataTextIO"]


class StataTextIO(FileIO.FileIO):
    """
    Opens, reads, and writes weights file objects in STATA text format.

    Spatial weights objects in the STATA text format are used in
    STATA sppack library through the spmat command.
    This format is a simple text file delimited by a whitespace.
    The spmat command does not specify which file extension to use.
    But, txt seems the default file extension, which is assumed in PySAL.

    The first line of the STATA text file  is
    a header including the number of observations.
    After this header line, it includes at least one data column that contains
    unique ids or record numbers of observations.
    When an id variable is not specified for the original spatial weights
    matrix in STATA, record numbers are used to identify individual observations,
    and the record numbers start with one.
    The spmat command seems to allow only integer IDs,
    which is also assumed in PySAL.

    A STATA text file can have one of the following structures according to
    its export options in STATA.

    Structure 1: encoding using the list of neighbor ids
    [Line 1]    [Number_of_Observations]
    [Line 2]    [ID_of_Obs_1] [ID_of_Neighbor_1_of_Obs_1] [ID_of_Neighbor_2_of_Obs_1] .... [ID_of_Neighbor_m_of_Obs_1]
    [Line 3]    [ID_of_Obs_2]
    [Line 4]    [ID_of_Obs_3] [ID_of_Neighbor_1_of_Obs_3] [ID_of_Neighbor_2_of_Obs_3]
    ...
    Note that for island observations their IDs are still recorded.

    Structure 2: encoding using a full matrix format
    [Line 1]    [Number_of_Observations]
    [Line 2]    [ID_of_Obs_1] [w_11] [w_12] ... [w_1n]
    [Line 3]    [ID_of_Obs_2] [w_21] [w_22] ... [w_2n]
    [Line 4]    [ID_of_Obs_3] [w_31] [w_32] ... [w_3n]
    ...
    [Line n+1]  [ID_of_Obs_n] [w_n1] [w_n2] ... [w_nn]
    where w_ij can be a form of general weight.
    That is, w_ij can be both a binary value or a general numeric value.
    If an observation is an island, all of its w columns contains 0.

    References
    ----------
    Drukker D.M., Peng H., Prucha I.R., and Raciborski R. (2011)
    "Creating and managing spatial-weighting matrices using the spmat command"

    Notes
    -----
    The spmat command allows users to add any note to a spatial weights matrix object in STATA.
    However, all those notes are lost when the matrix is exported.
    PySAL also does not take care of those notes.

    """

    FORMATS = ['stata_text']
    MODES = ['r', 'w']

    def __init__(self, *args, **kwargs):
        args = args[:2]
        FileIO.FileIO.__init__(self, *args, **kwargs)
        self.file = open(self.dataPath, self.mode)

    def read(self, n=-1):
        self._complain_ifclosed(self.closed)
        return self._read()

    def seek(self, pos):
        if pos == 0:
            self.file.seek(0)
            self.pos = 0

    def _read(self):
        """Reads STATA Text file
        Returns a pysal.weights.weights.W object

        Examples
        --------

        Type 'dir(w)' at the interpreter to see what methods are supported.
        Open a text file and read it into a pysal weights object

        >>> w = pysal.open(pysal.examples.get_path('stata_sparse.txt'),'r','stata_text').read()
        WARNING: there are 7 disconnected observations
        Island ids:  [5, 9, 10, 11, 12, 14, 15]

        Get the number of observations from the header

        >>> w.n
        56

        Get the mean number of neighbors

        >>> w.mean_neighbors
        4.0

        Get neighbor distances for a single observation

        >>> w[1]
        {53: 1.0, 51: 1.0, 45: 1.0, 54: 1.0, 7: 1.0}

        """
        if self.pos > 0:
            raise StopIteration

        n = int(self.file.readline().strip())
        line1 = self.file.readline().strip()
        obs_01 = line1.split(' ')
        matrix_form = False
        if len(obs_01) == 1 or float(obs_01[1]) != 0.0:
            def line2wgt(line):
                row = [int(i) for i in line.strip().split(' ')]
                return row[0], row[1:], [1.0] * len(row[1:])
        else:
            matrix_form = True

            def line2wgt(line):
                row = line.strip().split(' ')
                obs = int(float(row[0]))
                ngh, wgt = [], []
                for i in range(n):
                    w = float(row[i + 1])
                    if w > 0:
                        ngh.append(i)
                        wgt.append(w)
                return obs, ngh, wgt

        id_order = []
        weights, neighbors = {}, {}
        l = line1
        for i in range(n):
            obs, ngh, wgt = line2wgt(l)
            id_order.append(obs)
            neighbors[obs] = ngh
            weights[obs] = wgt
            l = self.file.readline()
        if matrix_form:
            for obs in neighbors:
                neighbors[obs] = [id_order[ngh] for ngh in neighbors[obs]]

        self.pos += 1
        return W(neighbors, weights)

    def write(self, obj, matrix_form=False):
        """

        Parameters
        ----------
        .write(weightsObject)
        accepts a weights object

        Returns
        ------

        a STATA text file
        write a weights object to the opened text file.

        Examples
        --------

        >>> import tempfile, pysal, os
        >>> testfile = pysal.open(pysal.examples.get_path('stata_sparse.txt'),'r','stata_text')
        >>> w = testfile.read()
        WARNING: there are 7 disconnected observations
        Island ids:  [5, 9, 10, 11, 12, 14, 15]

        Create a temporary file for this example

        >>> f = tempfile.NamedTemporaryFile(suffix='.txt')

        Reassign to new var

        >>> fname = f.name

        Close the temporary named file

        >>> f.close()

        Open the new file in write mode

        >>> o = pysal.open(fname,'w','stata_text')

        Write the Weights object into the open file

        >>> o.write(w)
        >>> o.close()

        Read in the newly created text file

        >>> wnew =  pysal.open(fname,'r','stata_text').read()
        WARNING: there are 7 disconnected observations
        Island ids:  [5, 9, 10, 11, 12, 14, 15]

        Compare values from old to new

        >>> wnew.pct_nonzero == w.pct_nonzero
        True

        Clean up temporary file created for this example

        >>> os.remove(fname)
        """
        self._complain_ifclosed(self.closed)
        if issubclass(type(obj), W):
            header = '%s\n' % obj.n
            self.file.write(header)
            if matrix_form:
                def wgt2line(obs_id, neighbor, weight):
                    w = ['0.0'] * obj.n
                    for ngh, wgt in zip(neighbor, weight):
                        w[obj.id2i[ngh]] = str(wgt)
                    return [str(obs_id)] + w
            else:
                def wgt2line(obs_id, neighbor, weight):
                    return [str(obs_id)] + [str(ngh) for ngh in neighbor]
            for id in obj.id_order:
                line = wgt2line(id, obj.neighbors[id], obj.weights[id])
                self.file.write('%s\n' % ' '.join(line))
        else:
            raise TypeError("Expected a pysal weights object, got: %s" % (
                type(obj)))

    def close(self):
        self.file.close()
        FileIO.FileIO.close(self)


########NEW FILE########
__FILENAME__ = template
""" Example Reader and Writer

    These are working readers/writers that parse '.foo' and '.bar' files

"""

import pysal
from pysal.core.FileIO import FileIO
__author__ = "Charles R Schmidt <schmidtc@gmail.com>"
__all__ = ['TemplateWriter', 'TemplateReaderWriter']


# Always subclass FileIO
class TemplateWriter(FileIO):
    #REQUIRED, List the formats this class supports.
    FORMATS = ['foo']
    #REQUIRED, List the modes supported by this class.
    # One class can support both reading and writing.
    # For simplicity this class will only support one.
    # You could support custom modes, but these could be hard to document.
    MODES = ['w']

    # use .__init__ to open any need file handlers
    def __init__(self, *args, **kwargs):
        # initialize the parent class...
        FileIO.__init__(self, *args, **kwargs)
        # this gives you:
        # self.dataPath == the connection string or path to file
        # self.mode == the mode the file should be opened in

        self.fileObj = open(self.dataPath, self.mode)

    #writers must subclass .write
    def write(self, obj):
        """ .write method of the 'foobar' template, receives an obj """

        # GOOD TO HAVE, this will prevent invalid operations on closed files.
        self._complain_ifclosed(self.closed)

        # it's up to the writer to understand the object, you should check that object is of the type you expect and raise a TypeError is its now.
        # we will support writing string objects in this example, all string are derived from basestring...
        if issubclass(type(obj), basestring):

            #Non-essential ...
            def foobar(c):
                if c in 'foobar':
                    return True
                else:
                    return False
            result = filter(foobar, obj)  # e.g.   'foobara' == filter(foobar,'my little foobar example')

            #do the actual writing...
            self.fileObj.write(result + '\n')
            #REQUIRED, increment the internal pos pointer.
            self.pos += 1

        else:
            raise TypeError("Expected a string, got: %s" % (type(obj)))

    #default is to raise "NotImplementedError"
    def flush(self):
        self._complain_ifclosed(self.closed)
        self.fileObj.flush()

    #REQUIRED
    def close(self):
        self.fileObj.close()
        #clean up the parent class too....
        FileIO.close(self)


class TemplateReaderWriter(FileIO):
    FORMATS = ['bar']
    MODES = ['r', 'w']

    def __init__(self, *args, **kwargs):
        FileIO.__init__(self, *args, **kwargs)
        self.fileObj = open(self.dataPath, self.mode)
    #Notice reading is a bit different

    def _filter(self, st):
        def foobar(c):
            if c in 'foobar':
                return True
            else:
                return False
        return filter(foobar, st)  # e.g.   'foobara' == filter(foobar,'my little foobar example')

    def _read(self):
        """ the _read method should return only ONE object and raise StopIteration at the EOF."""
        line = self.fileObj.readline()
        obj = self._filter(line)
        self.pos += 1  # REQUIRED
        if line:
            return obj + '\n'
        else:
            raise StopIteration  # REQUIRED

    def write(self, obj):
        """ .write method of the 'foobar' template, receives an obj """
        self._complain_ifclosed(self.closed)
        if issubclass(type(obj), basestring):
            result = self._filter(obj)
            self.fileObj.write(result + '\n')
            self.pos += 1
        else:
            raise TypeError("Expected a string, got: %s" % (type(obj)))

    def flush(self):
        self._complain_ifclosed(self.closed)
        self.fileObj.flush()

    def close(self):
        self.fileObj.close()
        FileIO.close(self)


if __name__ == '__main__':
    "note, by running OR importing this module it's automatically added to the pysal fileIO registry."
    pysal.open.check()

    lines = ['This is an example of template FileIO classes',
             'Each call to write expects a string object',
             'that string is filtered and only letters "f,o,b,a,r" are kept',
             'these kept letters are written to the file and a new line char is appends to each line',
             'likewise the reader filters each line from a file']
    f = pysal.open('test.foo', 'w')
    for line in lines:
        f.write(line)
    f.close()

    f = pysal.open('test.bar', 'w')
    for line in lines:
        f.write(line)
    f.close()

    f = pysal.open('test.bar', 'r')
    s = ''.join(f.read())
    f.close()
    print s

    f = open('test.foo', 'r')
    s2 = f.read()
    f.close()
    print s == s2

########NEW FILE########
__FILENAME__ = test_arcgis_dbf
import unittest
import pysal
from pysal.core.IOHandlers.arcgis_dbf import ArcGISDbfIO
import tempfile
import os
import warnings


class test_ArcGISDbfIO(unittest.TestCase):
    def setUp(self):
        self.test_file = test_file = pysal.examples.get_path('arcgis_ohio.dbf')
        self.obj = ArcGISDbfIO(test_file, 'r')

    def test_close(self):
        f = self.obj
        f.close()
        self.failUnlessRaises(ValueError, f.read)

    def test_read(self):
        with warnings.catch_warnings(record=True) as warn:
            warnings.simplefilter("always")
            w = self.obj.read()
            if len(warn) > 0:
                assert issubclass(warn[0].category, RuntimeWarning)
                assert "Missing Value Found, setting value to pysal.MISSINGVALUE" in str(warn[0].message)
        self.assertEqual(88, w.n)
        self.assertEqual(5.25, w.mean_neighbors)
        self.assertEqual([1.0, 1.0, 1.0, 1.0], w[1].values())

    def test_seek(self):
        self.test_read()
        self.failUnlessRaises(StopIteration, self.obj.read)
        self.obj.seek(0)
        self.test_read()

    def test_write(self):
        with warnings.catch_warnings(record=True) as warn:
            warnings.simplefilter("always")
            w = self.obj.read()
            if len(warn) > 0:
                assert issubclass(warn[0].category, RuntimeWarning)
                assert "Missing Value Found, setting value to pysal.MISSINGVALUE" in str(warn[0].message)
        f = tempfile.NamedTemporaryFile(
            suffix='.dbf', dir=pysal.examples.get_path(''))
        fname = f.name
        f.close()
        o = pysal.open(fname, 'w', 'arcgis_dbf')
        o.write(w)
        o.close()
        f = pysal.open(fname, 'r', 'arcgis_dbf')
        wnew = f.read()
        f.close()
        self.assertEqual(wnew.pct_nonzero, w.pct_nonzero)
        os.remove(fname)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_arcgis_swm
import unittest
import pysal
from pysal.core.IOHandlers.arcgis_swm import ArcGISSwmIO
import tempfile
import os


class test_ArcGISSwmIO(unittest.TestCase):
    def setUp(self):
        self.test_file = test_file = pysal.examples.get_path('ohio.swm')
        self.obj = ArcGISSwmIO(test_file, 'r')

    def test_close(self):
        f = self.obj
        f.close()
        self.failUnlessRaises(ValueError, f.read)

    def test_read(self):
        w = self.obj.read()
        self.assertEqual(88, w.n)
        self.assertEqual(5.25, w.mean_neighbors)
        self.assertEqual([1.0, 1.0, 1.0, 1.0], w[1].values())

    def test_seek(self):
        self.test_read()
        self.failUnlessRaises(StopIteration, self.obj.read)
        self.obj.seek(0)
        self.test_read()

    def test_write(self):
        w = self.obj.read()
        f = tempfile.NamedTemporaryFile(
            suffix='.swm', dir=pysal.examples.get_path(''))
        fname = f.name
        f.close()
        o = pysal.open(fname, 'w')
        o.write(w)
        o.close()
        wnew = pysal.open(fname, 'r').read()
        self.assertEqual(wnew.pct_nonzero, w.pct_nonzero)
        os.remove(fname)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_arcgis_txt
import unittest
import pysal
from pysal.core.IOHandlers.arcgis_txt import ArcGISTextIO
import tempfile
import os
import warnings


class test_ArcGISTextIO(unittest.TestCase):
    def setUp(self):
        self.test_file = test_file = pysal.examples.get_path('arcgis_txt.txt')
        self.obj = ArcGISTextIO(test_file, 'r')

    def test_close(self):
        f = self.obj
        f.close()
        self.failUnlessRaises(ValueError, f.read)

    def test_read(self):
        with warnings.catch_warnings(record=True) as warn:
            warnings.simplefilter("always")
            w = self.obj.read()
            if len(warn) > 0:
                assert issubclass(warn[0].category, RuntimeWarning)
                assert "DBF relating to ArcGIS TEXT was not found, proceeding with unordered string ids." in str(warn[0].message)
        self.assertEqual(3, w.n)
        self.assertEqual(2.0, w.mean_neighbors)
        self.assertEqual([0.1, 0.05], w[2].values())

    def test_seek(self):
        self.test_read()
        self.failUnlessRaises(StopIteration, self.obj.read)
        self.obj.seek(0)
        self.test_read()

    def test_write(self):
        with warnings.catch_warnings(record=True) as warn:
            warnings.simplefilter("always")
            w = self.obj.read()
            if len(warn) > 0:
                assert issubclass(warn[0].category, RuntimeWarning)
                assert "DBF relating to ArcGIS TEXT was not found, proceeding with unordered string ids." in str(warn[0].message)
        f = tempfile.NamedTemporaryFile(
            suffix='.txt', dir=pysal.examples.get_path(''))
        fname = f.name
        f.close()
        o = pysal.open(fname, 'w', 'arcgis_text')
        o.write(w)
        o.close()
        with warnings.catch_warnings(record=True) as warn:
            warnings.simplefilter("always")
            wnew = pysal.open(fname, 'r', 'arcgis_text').read()
            if len(warn) > 0:
                assert issubclass(warn[0].category, RuntimeWarning)
                assert "DBF relating to ArcGIS TEXT was not found, proceeding with unordered string ids." in str(warn[0].message)
        self.assertEqual(wnew.pct_nonzero, w.pct_nonzero)
        os.remove(fname)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_csvWrapper
import unittest
import pysal
import tempfile
import os


class test_csvWrapper(unittest.TestCase):
    def setUp(self):
        self.test_file = test_file = pysal.examples.get_path('stl_hom.csv')
        self.obj = pysal.core.IOHandlers.csvWrapper.csvWrapper(test_file, 'r')

    def test_len(self):
        self.assertEquals(len(self.obj), 78)

    def test_tell(self):
        self.assertEquals(self.obj.tell(), 0)
        self.obj.read(1)
        self.assertEquals(self.obj.tell(), 1)
        self.obj.read(50)
        self.assertEquals(self.obj.tell(), 51)
        self.obj.read()
        self.assertEquals(self.obj.tell(), 78)

    def test_seek(self):
        self.obj.seek(0)
        self.assertEquals(self.obj.tell(), 0)
        self.obj.seek(55)
        self.assertEquals(self.obj.tell(), 55)
        self.obj.read(1)
        self.assertEquals(self.obj.tell(), 56)

    def test_read(self):
        self.obj.seek(0)
        objs = self.obj.read()
        self.assertEquals(len(objs), 78)
        self.obj.seek(0)
        objsB = list(self.obj)
        self.assertEquals(len(objsB), 78)
        for rowA, rowB in zip(objs, objsB):
            self.assertEquals(rowA, rowB)

    def test_casting(self):
        self.obj.cast('WKT', pysal.core.util.WKTParser())
        verts = [(-89.585220336914062, 39.978794097900391), (-89.581146240234375, 40.094867706298828), (-89.603988647460938, 40.095306396484375), (-89.60589599609375, 40.136119842529297), (-89.6103515625, 40.3251953125), (-89.269027709960938, 40.329566955566406), (-89.268562316894531, 40.285579681396484), (-89.154655456542969, 40.285774230957031), (-89.152763366699219, 40.054969787597656), (-89.151618957519531, 39.919403076171875), (-89.224777221679688, 39.918678283691406), (-89.411857604980469, 39.918041229248047), (-89.412437438964844, 39.931644439697266), (-89.495201110839844, 39.933486938476562), (-89.4927978515625, 39.980186462402344), (-89.585220336914062, 39.978794097900391)]
        for i, pt in enumerate(self.obj.next()[0].vertices):
            self.assertEquals(pt[:], verts[i])

    def test_by_col(self):
        for field in self.obj.header:
            self.assertEquals(len(self.obj.by_col[field]), 78)

    def test_slicing(self):
        chunk = self.obj[50:55, 1:3]
        self.assertEquals(chunk[0], ['Jefferson', 'Missouri'])
        self.assertEquals(chunk[1], ['Jefferson', 'Illinois'])
        self.assertEquals(chunk[2], ['Miller', 'Missouri'])
        self.assertEquals(chunk[3], ['Maries', 'Missouri'])
        self.assertEquals(chunk[4], ['White', 'Illinois'])

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_dat
import unittest
import pysal
from pysal.core.IOHandlers.dat import DatIO
import tempfile
import os


class test_DatIO(unittest.TestCase):
    def setUp(self):
        self.test_file = test_file = pysal.examples.get_path('wmat.dat')
        self.obj = DatIO(test_file, 'r')

    def test_close(self):
        f = self.obj
        f.close()
        self.failUnlessRaises(ValueError, f.read)

    def test_read(self):
        w = self.obj.read()
        self.assertEqual(49, w.n)
        self.assertEqual(4.7346938775510203, w.mean_neighbors)
        self.assertEqual([0.5, 0.5], w[5.0].values())

    def test_seek(self):
        self.test_read()
        self.failUnlessRaises(StopIteration, self.obj.read)
        self.obj.seek(0)
        self.test_read()

    def test_write(self):
        w = self.obj.read()
        f = tempfile.NamedTemporaryFile(
            suffix='.dat', dir=pysal.examples.get_path(''))
        fname = f.name
        f.close()
        o = pysal.open(fname, 'w')
        o.write(w)
        o.close()
        wnew = pysal.open(fname, 'r').read()
        self.assertEqual(wnew.pct_nonzero, w.pct_nonzero)
        os.remove(fname)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_gal
"""Unit tests for gal.py"""
import unittest
import pysal
import tempfile
import os
from pysal.core.IOHandlers.gal import GalIO


class test_GalIO(unittest.TestCase):
    def setUp(self):
        self.test_file = test_file = pysal.examples.get_path('sids2.gal')
        self.obj = GalIO(test_file, 'r')

    def test___init__(self):
        self.assertEqual(self.obj._typ, str)

    def test_close(self):
        f = self.obj
        f.close()
        self.failUnlessRaises(ValueError, f.read)

    def test_read(self):
        # reading a GAL returns a W
        w = self.obj.read()
        self.assertEqual(w.n, 100)
        self.assertAlmostEqual(w.sd, 1.5151237573214935)
        self.assertEqual(w.s0, 462.0)
        self.assertEqual(w.s1, 924.0)

    def test_seek(self):
        self.test_read()
        self.failUnlessRaises(StopIteration, self.obj.read)
        self.obj.seek(0)
        self.test_read()

    def test_write(self):
        w = self.obj.read()
        f = tempfile.NamedTemporaryFile(suffix='.gal')
        fname = f.name
        f.close()
        o = pysal.open(fname, 'w')
        o.write(w)
        o.close()
        wnew = pysal.open(fname, 'r').read()
        self.assertEqual(wnew.pct_nonzero, w.pct_nonzero)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_geobugs_txt
import unittest
import pysal
from pysal.core.IOHandlers.geobugs_txt import GeoBUGSTextIO
import tempfile
import os


class test_GeoBUGSTextIO(unittest.TestCase):
    def setUp(self):
        self.test_file_scot = test_file_scot = pysal.examples.get_path(
            'geobugs_scot')
        self.test_file_col = test_file_col = pysal.examples.get_path(
            'spdep_listw2WB_columbus')
        self.obj_scot = GeoBUGSTextIO(test_file_scot, 'r')
        self.obj_col = GeoBUGSTextIO(test_file_col, 'r')

    def test_close(self):
        for obj in [self.obj_scot, self.obj_col]:
            f = obj
            f.close()
            self.failUnlessRaises(ValueError, f.read)

    def test_read(self):
        w_scot = self.obj_scot.read()
        self.assertEqual(56, w_scot.n)
        self.assertEqual(4.1785714285714288, w_scot.mean_neighbors)
        self.assertEqual([1.0, 1.0, 1.0], w_scot[1].values())

        w_col = self.obj_col.read()
        self.assertEqual(49, w_col.n)
        self.assertEqual(4.6938775510204085, w_col.mean_neighbors)
        self.assertEqual([0.5, 0.5], w_col[1].values())

    def test_seek(self):
        self.test_read()
        self.failUnlessRaises(StopIteration, self.obj_scot.read)
        self.failUnlessRaises(StopIteration, self.obj_col.read)
        self.obj_scot.seek(0)
        self.obj_col.seek(0)
        self.test_read()

    def test_write(self):
        for obj in [self.obj_scot, self.obj_col]:
            w = obj.read()
            f = tempfile.NamedTemporaryFile(
                suffix='', dir=pysal.examples.get_path(''))
            fname = f.name
            f.close()
            o = pysal.open(fname, 'w', 'geobugs_text')
            o.write(w)
            o.close()
            wnew = pysal.open(fname, 'r', 'geobugs_text').read()
            self.assertEqual(wnew.pct_nonzero, w.pct_nonzero)
            os.remove(fname)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_geoda_txt
'''GeoDa Text File Reader Unit Tests'''
import unittest
import pysal
from pysal.core.IOHandlers.geoda_txt import GeoDaTxtReader as GTR


class test_GeoDaTxtReader(unittest.TestCase):
    def setUp(self):
        test_file = pysal.examples.get_path('stl_hom.txt')
        self.obj = GTR(test_file, 'r')

    def test___init__(self):
        self.assertEqual(
            self.obj.header, ['FIPSNO', 'HR8488', 'HR8893', 'HC8488'])

    def test___len__(self):
        expected = 78
        self.assertEqual(expected, len(self.obj))

    def test_close(self):
        f = self.obj
        f.close()
        self.failUnlessRaises(ValueError, f.read)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_gwt
import unittest
import pysal
from pysal.core.IOHandlers.gwt import GwtIO
import tempfile
import os
import warnings


class test_GwtIO(unittest.TestCase):
    def setUp(self):
        self.test_file = test_file = pysal.examples.get_path('juvenile.gwt')
        self.obj = GwtIO(test_file, 'r')

    def test_close(self):
        f = self.obj
        f.close()
        self.failUnlessRaises(ValueError, f.read)

    def test_read(self):
        w = self.obj.read()
        self.assertEqual(168, w.n)
        self.assertEqual(16.678571428571427, w.mean_neighbors)
        w.transform = 'B'
        self.assertEqual([1.0], w[1].values())

    def test_seek(self):
        self.test_read()
        self.failUnlessRaises(StopIteration, self.obj.read)
        self.obj.seek(0)
        self.test_read()

    # Commented out by CRS, GWT 'w' mode removed until we can find a good solution for retaining distances.
    # see issue #153.
    # Added back by CRS,
    def test_write(self):
        w = self.obj.read()
        f = tempfile.NamedTemporaryFile(
            suffix='.gwt', dir=pysal.examples.get_path(''))
        fname = f.name
        f.close()
        o = pysal.open(fname, 'w')
        #copy the shapefile and ID variable names from the old gwt.
        # this is only available after the read() method has been called.
        #o.shpName = self.obj.shpName
        #o.varName = self.obj.varName
        o.write(w)
        o.close()
        wnew = pysal.open(fname, 'r').read()
        self.assertEqual(wnew.pct_nonzero, w.pct_nonzero)
        os.remove(fname)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_mat
import unittest
import pysal
from pysal.core.IOHandlers.mat import MatIO
import tempfile
import os
import warnings


class test_MatIO(unittest.TestCase):
    def setUp(self):
        self.test_file = test_file = pysal.examples.get_path('spat-sym-us.mat')
        self.obj = MatIO(test_file, 'r')

    def test_close(self):
        f = self.obj
        f.close()
        self.failUnlessRaises(ValueError, f.read)

    def test_read(self):
        w = self.obj.read()
        self.assertEqual(46, w.n)
        self.assertEqual(4.0869565217391308, w.mean_neighbors)
        self.assertEqual([1.0, 1.0, 1.0, 1.0], w[1].values())

    def test_seek(self):
        self.test_read()
        self.failUnlessRaises(StopIteration, self.obj.read)
        self.obj.seek(0)
        self.test_read()

    def test_write(self):
        w = self.obj.read()
        f = tempfile.NamedTemporaryFile(
            suffix='.mat', dir=pysal.examples.get_path(''))
        fname = f.name
        f.close()
        o = pysal.open(fname, 'w')
        with warnings.catch_warnings(record=True) as warn:
            warnings.simplefilter("always")
            o.write(w)
            if len(warn) > 0:
                assert issubclass(warn[0].category, FutureWarning)
        o.close()
        wnew = pysal.open(fname, 'r').read()
        self.assertEqual(wnew.pct_nonzero, w.pct_nonzero)
        os.remove(fname)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_mtx
import unittest
import pysal
from pysal.core.IOHandlers.mtx import MtxIO
import tempfile
import os
import warnings
import scipy.sparse as SP


class test_MtxIO(unittest.TestCase):
    def setUp(self):
        self.test_file = test_file = pysal.examples.get_path('wmat.mtx')
        self.obj = MtxIO(test_file, 'r')

    def test_close(self):
        f = self.obj
        f.close()
        self.failUnlessRaises(ValueError, f.read)

    def test_read(self):
        w = self.obj.read()
        self.assertEqual(49, w.n)
        self.assertEqual(4.7346938775510203, w.mean_neighbors)
        self.assertEqual([0.33329999999999999, 0.33329999999999999,
                          0.33329999999999999], w[1].values())
        s0 = w.s0
        self.obj.seek(0)
        wsp = self.obj.read(sparse=True)
        self.assertEqual(49, wsp.n)
        self.assertEqual(s0, wsp.s0)

    def test_seek(self):
        self.test_read()
        self.failUnlessRaises(StopIteration, self.obj.read)
        self.obj.seek(0)
        self.test_read()

    def test_write(self):
        for i in [False, True]:
            self.obj.seek(0)
            w = self.obj.read(sparse=i)
            f = tempfile.NamedTemporaryFile(
                suffix='.mtx', dir=pysal.examples.get_path(''))
            fname = f.name
            f.close()
            o = pysal.open(fname, 'w')
            o.write(w)
            o.close()
            wnew = pysal.open(fname, 'r').read(sparse=i)
            if i:
                self.assertEqual(wnew.s0, w.s0)
            else:
                self.assertEqual(wnew.pct_nonzero, w.pct_nonzero)
            os.remove(fname)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_pyDbfIO
import unittest
import pysal
import tempfile
import os


class test_DBF(unittest.TestCase):
    def setUp(self):
        self.test_file = test_file = pysal.examples.get_path('10740.dbf')
        self.dbObj = pysal.core.IOHandlers.pyDbfIO.DBF(test_file, 'r')

    def test_len(self):
        self.assertEquals(len(self.dbObj), 195)

    def test_tell(self):
        self.assertEquals(self.dbObj.tell(), 0)
        self.dbObj.read(1)
        self.assertEquals(self.dbObj.tell(), 1)
        self.dbObj.read(50)
        self.assertEquals(self.dbObj.tell(), 51)
        self.dbObj.read()
        self.assertEquals(self.dbObj.tell(), 195)

    def test_seek(self):
        self.dbObj.seek(0)
        self.assertEquals(self.dbObj.tell(), 0)
        self.dbObj.seek(55)
        self.assertEquals(self.dbObj.tell(), 55)
        self.dbObj.read(1)
        self.assertEquals(self.dbObj.tell(), 56)

    def test_read(self):
        self.dbObj.seek(0)
        objs = self.dbObj.read()
        self.assertEquals(len(objs), 195)
        self.dbObj.seek(0)
        objsB = list(self.dbObj)
        self.assertEquals(len(objsB), 195)
        for rowA, rowB in zip(objs, objsB):
            self.assertEquals(rowA, rowB)

    def test_random_access(self):
        self.dbObj.seek(0)
        db0 = self.dbObj.read(1)[0]
        self.assertEquals(db0, [1, '35001', '000107', '35001000107', '1.07'])
        self.dbObj.seek(57)
        db57 = self.dbObj.read(1)[0]
        self.assertEquals(db57, [58, '35001', '001900', '35001001900', '19'])
        self.dbObj.seek(32)
        db32 = self.dbObj.read(1)[0]
        self.assertEquals(db32, [33, '35001', '000500', '35001000500', '5'])
        self.dbObj.seek(0)
        self.assertEquals(self.dbObj.next(), db0)
        self.dbObj.seek(57)
        self.assertEquals(self.dbObj.next(), db57)
        self.dbObj.seek(32)
        self.assertEquals(self.dbObj.next(), db32)

    def test_write(self):
        f = tempfile.NamedTemporaryFile(suffix='.dbf')
        fname = f.name
        f.close()
        self.dbfcopy = fname
        self.out = pysal.core.IOHandlers.pyDbfIO.DBF(fname, 'w')
        self.dbObj.seek(0)
        self.out.header = self.dbObj.header
        self.out.field_spec = self.dbObj.field_spec
        for row in self.dbObj:
            self.out.write(row)
        self.out.close()

        orig = open(self.test_file, 'rb')
        copy = open(self.dbfcopy, 'rb')
        orig.seek(32)  # self.dbObj.header_size) #skip the header, file date has changed
        copy.seek(32)  # self.dbObj.header_size) #skip the header, file date has changed

        #PySAL writes proper DBF files with a terminator at the end, not everyone does.
        n = self.dbObj.record_size * self.dbObj.n_records  # bytes to read.
        self.assertEquals(orig.read(n), copy.read(n))
        #self.assertEquals(orig.read(1), copy.read(1)) # last byte may fail
        orig.close()
        copy.close()
        os.remove(self.dbfcopy)

    def test_writeNones(self):
        import datetime
        import time
        f = tempfile.NamedTemporaryFile(
            suffix='.dbf')
        fname = f.name
        f.close()
        db = pysal.core.IOHandlers.pyDbfIO.DBF(fname, 'w')
        db.header = ["recID", "date", "strID", "aFloat"]
        db.field_spec = [('N', 10, 0), ('D', 8, 0), ('C', 10, 0), ('N', 5, 5)]
        records = []
        for i in range(10):
            d = datetime.date(*time.localtime()[:3])
            rec = [i + 1, d, str(i + 1), (i + 1) / 2.0]
            records.append(rec)
        records.append([None, None, '', None])
        records.append(rec)
        for rec in records:
            db.write(rec)
        db.close()
        db2 = pysal.core.IOHandlers.pyDbfIO.DBF(fname, 'r')
        self.assertEquals(records, db2.read())

        os.remove(fname)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_pyShpIO
import unittest
import pysal
import tempfile
import os


class test_PurePyShpWrapper(unittest.TestCase):
    def setUp(self):
        test_file = pysal.examples.get_path('10740.shp')
        self.test_file = test_file
        self.shpObj = pysal.core.IOHandlers.pyShpIO.PurePyShpWrapper(
            test_file, 'r')
        f = tempfile.NamedTemporaryFile(suffix='.shp')
        shpcopy = f.name
        f.close()
        self.shpcopy = shpcopy
        self.shxcopy = shpcopy.replace('.shp', '.shx')

    def test_len(self):
        self.assertEquals(len(self.shpObj), 195)

    def test_tell(self):
        self.assertEquals(self.shpObj.tell(), 0)
        self.shpObj.read(1)
        self.assertEquals(self.shpObj.tell(), 1)
        self.shpObj.read(50)
        self.assertEquals(self.shpObj.tell(), 51)
        self.shpObj.read()
        self.assertEquals(self.shpObj.tell(), 195)

    def test_seek(self):
        self.shpObj.seek(0)
        self.assertEquals(self.shpObj.tell(), 0)
        self.shpObj.seek(55)
        self.assertEquals(self.shpObj.tell(), 55)
        self.shpObj.read(1)
        self.assertEquals(self.shpObj.tell(), 56)

    def test_read(self):
        self.shpObj.seek(0)
        objs = self.shpObj.read()
        self.assertEquals(len(objs), 195)

        self.shpObj.seek(0)
        objsB = list(self.shpObj)
        self.assertEquals(len(objsB), 195)

        for shpA, shpB in zip(objs, objsB):
            self.assertEquals(shpA.vertices, shpB.vertices)

    def test_random_access(self):
        self.shpObj.seek(57)
        shp57 = self.shpObj.read(1)[0]
        self.shpObj.seek(32)
        shp32 = self.shpObj.read(1)[0]

        self.shpObj.seek(57)
        self.assertEquals(self.shpObj.read(1)[0].vertices, shp57.vertices)
        self.shpObj.seek(32)
        self.assertEquals(self.shpObj.read(1)[0].vertices, shp32.vertices)

    def test_write(self):
        out = pysal.core.IOHandlers.pyShpIO.PurePyShpWrapper(self.shpcopy, 'w')
        self.shpObj.seek(0)
        for shp in self.shpObj:
            out.write(shp)
        out.close()

        orig = open(self.test_file, 'rb')
        copy = open(self.shpcopy, 'rb')
        self.assertEquals(orig.read(), copy.read())
        orig.close()
        copy.close()

        oshx = open(self.test_file.replace('.shp', '.shx'), 'rb')
        cshx = open(self.shxcopy, 'rb')
        self.assertEquals(oshx.read(), cshx.read())
        oshx.close()
        cshx.close()

        os.remove(self.shpcopy)
        os.remove(self.shxcopy)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_stata_txt
import unittest
import pysal
from pysal.core.IOHandlers.stata_txt import StataTextIO
import tempfile
import os


class test_StataTextIO(unittest.TestCase):
    def setUp(self):
        self.test_file_sparse = test_file_sparse = pysal.examples.get_path(
            'stata_sparse.txt')
        self.test_file_full = test_file_full = pysal.examples.get_path(
            'stata_full.txt')
        self.obj_sparse = StataTextIO(test_file_sparse, 'r')
        self.obj_full = StataTextIO(test_file_full, 'r')

    def test_close(self):
        for obj in [self.obj_sparse, self.obj_full]:
            f = obj
            f.close()
            self.failUnlessRaises(ValueError, f.read)

    def test_read(self):
        w_sparse = self.obj_sparse.read()
        self.assertEqual(56, w_sparse.n)
        self.assertEqual(4.0, w_sparse.mean_neighbors)
        self.assertEqual([1.0, 1.0, 1.0, 1.0, 1.0], w_sparse[1].values())

        w_full = self.obj_full.read()
        self.assertEqual(56, w_full.n)
        self.assertEqual(4.0, w_full.mean_neighbors)
        self.assertEqual(
            [0.125, 0.125, 0.125, 0.125, 0.125], w_full[1].values())

    def test_seek(self):
        self.test_read()
        self.failUnlessRaises(StopIteration, self.obj_sparse.read)
        self.failUnlessRaises(StopIteration, self.obj_full.read)
        self.obj_sparse.seek(0)
        self.obj_full.seek(0)
        self.test_read()

    def test_write(self):
        for obj in [self.obj_sparse, self.obj_full]:
            w = obj.read()
            f = tempfile.NamedTemporaryFile(
                suffix='.txt', dir=pysal.examples.get_path(''))
            fname = f.name
            f.close()
            o = pysal.open(fname, 'w', 'stata_text')
            if obj == self.obj_sparse:
                o.write(w)
            else:
                o.write(w, matrix_form=True)
            o.close()
            wnew = pysal.open(fname, 'r', 'stata_text').read()
            self.assertEqual(wnew.pct_nonzero, w.pct_nonzero)
            os.remove(fname)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_wk1
import unittest
import pysal
from pysal.core.IOHandlers.wk1 import Wk1IO
import tempfile
import os


class test_Wk1IO(unittest.TestCase):
    def setUp(self):
        self.test_file = test_file = pysal.examples.get_path('spat-sym-us.wk1')
        self.obj = Wk1IO(test_file, 'r')

    def test_close(self):
        f = self.obj
        f.close()
        self.failUnlessRaises(ValueError, f.read)

    def test_read(self):
        w = self.obj.read()
        self.assertEqual(46, w.n)
        self.assertEqual(4.0869565217391308, w.mean_neighbors)
        self.assertEqual([1.0, 1.0, 1.0, 1.0], w[1].values())

    def test_seek(self):
        self.test_read()
        self.failUnlessRaises(StopIteration, self.obj.read)
        self.obj.seek(0)
        self.test_read()

    def test_write(self):
        w = self.obj.read()
        f = tempfile.NamedTemporaryFile(
            suffix='.wk1', dir=pysal.examples.get_path(''))
        fname = f.name
        f.close()
        o = pysal.open(fname, 'w')
        o.write(w)
        o.close()
        wnew = pysal.open(fname, 'r').read()
        self.assertEqual(wnew.pct_nonzero, w.pct_nonzero)
        os.remove(fname)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_wkt
import unittest
import pysal
from pysal.core.IOHandlers.wkt import WKTReader


class test_WKTReader(unittest.TestCase):
    def setUp(self):
        self.test_file = test_file = pysal.examples.get_path('stl_hom.wkt')
        self.obj = WKTReader(test_file, 'r')

    def test_close(self):
        f = self.obj
        f.close()
        self.failUnlessRaises(ValueError, f.read)
        # w_kt_reader = WKTReader(*args, **kwargs)
        # self.assertEqual(expected, w_kt_reader.close())

    def test_open(self):
        f = self.obj
        expected = ['wkt']
        self.assertEqual(expected, f.FORMATS)

    def test__read(self):
        polys = self.obj.read()
        self.assertEqual(78, len(polys))
        self.assertEqual((-91.195784694307383, 39.990883050220845),
                         polys[1].centroid)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = wk1
import pysal
import os.path
import struct
import pysal.core.FileIO as FileIO
from pysal.weights import W
from warnings import warn

__author__ = "Myunghwa Hwang <mhwang4@gmail.com>"
__all__ = ["Wk1IO"]


class Wk1IO(FileIO.FileIO):
    """
    MATLAB wk1read.m and wk1write.m that were written by Brian M. Bourgault in 10/22/93

    Opens, reads, and writes weights file objects in Lotus Wk1 format.

    Lotus Wk1 file is used in Dr. LeSage's MATLAB Econometrics library.

    A Wk1 file holds a spatial weights object in a full matrix form
    without any row and column headers.
    The maximum number of columns supported in a Wk1 file is 256.
    Wk1 starts the row (column) number from 0 and
    uses little endian binary endcoding.
    In PySAL, when the number of observations is n,
    it is assumed that each cell of a n\*n(=m) matrix either is a blank or
    have a number.

    The internal structure of a Wk1 file written by PySAL is as follows:
    [BOF][DIM][CPI][CAL][CMODE][CORD][SPLIT][SYNC][CURS][WIN]
    [HCOL][MRG][LBL][CELL_1]...[CELL_m][EOF]
    where [CELL_k] equals to [DTYPE][DLEN][DFORMAT][CINDEX][CVALUE].
    The parts between [BOF] and [CELL_1] are variable according to the software
    program used to write a wk1 file. While reading a wk1 file,
    PySAL ignores them.
    Each part of this structure is detailed below.

 .. table:: Lotus WK1 fields

   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |Part         |Description          |Data Type                |Length |Value                        |
   +=============+=====================+=========================+=======+=============================+
   |[BOF]        |Begining of field    |unsigned character       |6      |0,0,2,0,6,4                  |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |[DIM]        |Matrix dimension                                                                     |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |  [DIMDTYPE] |Type of dim. rec     |unsigned short           |2      |6                            |
   |  [DIMLEN]   |Length of dim. rec   |unsigned short           |2      |8                            |
   |  [DIMVAL]   |Value of dim. rec    |unsigned short           |8      |0,0,n,n                      |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |[CPI]        |CPI                                                                                  |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |  [CPITYPE]  |Type of cpi rec      |unsigned short           |2      |150                          |
   |  [CPILEN]   |Length of cpi rec    |unsigned short           |2      |6                            |
   |  [CPIVAL]   |Value of cpi rec     |unsigned char            |6      |0,0,0,0,0,0                  |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |[CAL]        |calcount                                                                             |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |  [CALTYPE]  |Type of calcount rec |unsigned short           |2      |47                           |
   |  [CALLEN]   |Length calcount rec  |unsigned short           |2      |1                            |
   |  [CALVAL]   |Value of calcount rec|unsigned char            |1      |0                            |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |[CMODE]      |calmode                                                                              |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |  [CMODETYP] |Type of calmode rec  |unsigned short           |2      |2                            |
   |  [CMODELEN] |Length of calmode rec|unsigned short           |2      |1                            |
   |  [CMODEVAL] |Value of calmode rec |signed char              |1      |0                            |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |[CORD]       |calorder                                                                             |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |  [CORDTYPE] |Type of calorder rec |unsigned short           |2      |3                            |
   |  [CORDLEN]  |Length calorder rec  |unsigned short           |2      |1                            |
   |  [CORDVAL]  |Value of calorder rec|signed char              |1      |0                            |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |[SPLIT]      |split                                                                                |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |  [SPLTYPE]  |Type of split rec    |unsigned short           |2      |4                            |
   |  [SPLLEN]   |Length of split rec  |unsigned short           |2      |1                            |
   |  [SPLVAL]   |Value of split rec   |signed char              |1      |0                            |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |[SYNC]       |sync                                                                                 |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |  [SYNCTYP]  |Type of sync rec     |unsigned short           |2      |5                            |
   |  [SYNCLEN]  |Length of sync rec   |unsigned short           |2      |1                            |
   |  [SYNCVAL]  |Value of sync rec    |singed char              |1      |0                            |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |[CURS]       |cursor                                                                               |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |  [CURSTYP]  |Type of cursor rec   |unsigned short           |2      |49                           |
   |  [CURSLEN]  |Length of cursor rec |unsigned short           |2      |1                            |
   |  [CURSVAL]  |Value of cursor rec  |signed char              |1      |1                            |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |[WIN]        |window                                                                               |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |  [WINTYPE]  |Type of window rec   |unsigned short           |2      |7                            |
   |  [WINLEN]   |Length of window rec |unsigned short           |2      |32                           |
   |  [WINVAL1]  |Value 1 of window rec|unsigned short           |4      |0,0                          |
   |  [WINVAL2]  |Value 2 of window rec|signed char              |2      |113,0                        |
   |  [WINVAL3]  |Value 3 of window rec|unsigned short           |26     |10,n,n,0,0,0,0,0,0,0,0,72,0  |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |[HCOL]       |hidcol                                                                               |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |  [HCOLTYP]  |Type of hidcol rec   |unsigned short           |2      |100                          |
   |  [HCOLLEN]  |Length of hidcol rec |unsigned short           |2      |32                           |
   |  [HCOLVAL]  |Value of hidcol rec  |signed char              |32     |0*32                         |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |[MRG]        |margins                                                                              |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |  [MRGTYPE]  |Type of margins rec  |unsigned short           |2      |40                           |
   |  [MRGLEN]   |Length of margins rec|unsigned short           |2      |10                           |
   |  [MRGVAL]   |Value of margins rec |unsigned short           |10     |4,76,66,2,2                  |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |[LBL]        |labels                                                                               |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |  [LBLTYPE]  |Type of labels rec   |unsigned short           |2      |41                           |
   |  [LBLLEN]   |Length of labels rec |unsigned short           |2      |1                            |
   |  [LBLVAL]   |Value of labels rec  |char                     |1      |'                            |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |[CELL_k]                                                                                           |
   +-------------+---------------------+-------------------------+-------+-----------------------------+
   |  [DTYPE]    |Type of cell data    |unsigned short           |2      |[DTYPE][0]==0: end of file   |
   |             |                     |                         |       |          ==14: number       |
   |             |                     |                         |       |          ==16: formula      |
   |             |                     |                         |       |          ==13: integer      |
   |             |                     |                         |       |          ==11: nrange       |
   |             |                     |                         |       |          ==else: unknown    |
   |  [DLEN]     |Length of cell data  |unsigned short           |2      |                             |
   |  [DFORMAT]  |Format of cell data  |not sure                 |1      |                             |
   |  [CINDEX]   |Row, column of cell  |unsigned short           |4      |                             |
   |  [CVALUE]   |Value of cell        |double, [DTYPE][0]==14   |8      |                             |
   |             |                     |formula,[DTYPE][0]==16   |8 +    |[DTYPE][1] - 13              |
   |             |                     |integer,[DTYPE][0]==13   |2      |                             |
   |             |                     |nrange, [DTYPE][0]==11   |24     |                             |
   |             |                     |else,   [DTYPE][0]==else |       |[DTYPE][1]                   |
   |  [EOF]      |End of file          |unsigned short           |4      |1,0,0,0                      |
   +-------------+---------------------+-------------------------+-------+-----------------------------+


    """

    FORMATS = ['wk1']
    MODES = ['r', 'w']

    def __init__(self, *args, **kwargs):
        self._varName = 'Unknown'
        FileIO.FileIO.__init__(self, *args, **kwargs)
        self.file = open(self.dataPath, self.mode + 'b')

    def _set_varName(self, val):
        if issubclass(type(val), basestring):
            self._varName = val

    def _get_varName(self):
        return self._varName
    varName = property(fget=_get_varName, fset=_set_varName)

    def read(self, n=-1):
        self._complain_ifclosed(self.closed)
        return self._read()

    def seek(self, pos):
        if pos == 0:
            self.file.seek(0)
            self.pos = 0

    def _read(self):
        """
        Reads Lotus Wk1 file

        Returns
        -------
        A pysal.weights.weights.W object

        Examples
        --------

        Type 'dir(w)' at the interpreter to see what methods are supported.
        Open a Lotus Wk1 file and read it into a pysal weights object

        >>> w = pysal.open(pysal.examples.get_path('spat-sym-us.wk1'),'r').read()

        Get the number of observations from the header

        >>> w.n
        46

        Get the mean number of neighbors

        >>> w.mean_neighbors
        4.0869565217391308

        Get neighbor distances for a single observation

        >>> w[1]
        {25: 1.0, 3: 1.0, 28: 1.0, 39: 1.0}

        """
        if self.pos > 0:
            raise StopIteration

        bof = struct.unpack('<6B', self.file.read(6))
        if bof != (0, 0, 2, 0, 6, 4):
            raise ValueError('The header of your file is wrong!')

        neighbors = {}
        weights = {}
        dtype, dlen = struct.unpack('<2H', self.file.read(4))
        while(dtype != 1):
            if dtype in [13, 14, 16]:
                self.file.read(1)
                row, column = struct.unpack('2H', self.file.read(4))
                format, length = '<d', 8
                if dtype == 13:
                    format, length = '<h', 2
                value = float(struct.unpack(format, self.file.read(length))[0])
                if value > 0:
                    ngh = neighbors.setdefault(row, [])
                    ngh.append(column)
                    wgt = weights.setdefault(row, [])
                    wgt.append(value)
                if dtype == 16:
                    self.file.read(dlen - 13)
            elif dtype == 11:
                self.file.read(24)
            else:
                self.file.read(dlen)
            dtype, dlen = struct.unpack('<2H', self.file.read(4))

        self.pos += 1
        return W(neighbors, weights)

    def write(self, obj):
        """

        Parameters
        ----------
        .write(weightsObject)
        accepts a weights object

        Returns
        ------

        a Lotus wk1 file
        write a weights object to the opened wk1 file.

        Examples
        --------

        >>> import tempfile, pysal, os
        >>> testfile = pysal.open(pysal.examples.get_path('spat-sym-us.wk1'),'r')
        >>> w = testfile.read()

        Create a temporary file for this example

        >>> f = tempfile.NamedTemporaryFile(suffix='.wk1')

        Reassign to new var

        >>> fname = f.name

        Close the temporary named file

        >>> f.close()

        Open the new file in write mode

        >>> o = pysal.open(fname,'w')

        Write the Weights object into the open file

        >>> o.write(w)
        >>> o.close()

        Read in the newly created text file

        >>> wnew =  pysal.open(fname,'r').read()

        Compare values from old to new

        >>> wnew.pct_nonzero == w.pct_nonzero
        True

        Clean up temporary file created for this example

        >>> os.remove(fname)

        """
        self._complain_ifclosed(self.closed)
        if issubclass(type(obj), W):
            f = self.file
            n = obj.n
            if n > 256:
                raise ValueError('WK1 file format supports only up to 256 observations.')
            pack = struct.pack
            f.write(pack('<6B', 0, 0, 2, 0, 6, 4))
            f.write(pack('<6H', 6, 8, 0, 0, n, n))
            f.write(pack('<2H6B', 150, 6, 0, 0, 0, 0, 0, 0))
            f.write(pack('<2H1B', 47, 1, 0))
            f.write(pack('<2H1b', 2, 1, 0))
            f.write(pack('<2H1b', 3, 1, 0))
            f.write(pack('<2H1b', 4, 1, 0))
            f.write(pack('<2H1b', 5, 1, 0))
            f.write(pack('<2H1b', 49, 1, 1))
            f.write(pack('<4H2b13H', 7, 32, 0, 0, 113, 0, 10,
                         n, n, 0, 0, 0, 0, 0, 0, 0, 0, 72, 0))
            hidcol = tuple(['<2H32b', 100, 32] + [0] * 32)
            f.write(pack(*hidcol))
            f.write(pack('<7H', 40, 10, 4, 76, 66, 2, 2))
            f.write(pack('<2H1c', 41, 1, "'"))

            id2i = obj.id2i
            for i, w_i in enumerate(obj):
                row = [0.0] * n
                for k in w_i:
                    row[id2i[k]] = w_i[k]
                for c, v in enumerate(row):
                    cell = tuple(['<2H1b2H1d', 14, 13, 113, i, c, v])
                    f.write(pack(*cell))
            f.write(pack('<4B', 1, 0, 0, 0))
            self.pos += 1

        else:
            raise TypeError("Expected a pysal weights object, got: %s" % (
                type(obj)))

    def close(self):
        self.file.close()
        FileIO.FileIO.close(self)



########NEW FILE########
__FILENAME__ = wkt
import pysal.core.FileIO as FileIO
from pysal.core.util import WKTParser
from pysal import cg
import re

__author__ = "Charles R Schmidt <schmidtc@gmail.com>"
__all__ = ['WKTReader']
#####################################################################
## ToDo: Add Well-Known-Binary support...
##       * WKB spec:
##  http://webhelp.esri.com/arcgisserver/9.3/dotNet/index.htm#geodatabases/the_ogc_103951442.htm
##
##
#####################################################################


class WKTReader(FileIO.FileIO):
    """
    Parameters
    ----------
    Reads Well-Known Text
    Returns a list of PySAL Polygon objects

    Examples
    --------
    Read in WKT-formatted file

    >>> import pysal
    >>> f = pysal.open(pysal.examples.get_path('stl_hom.wkt'), 'r')

    Convert wkt to pysal polygons

    >>> polys = f.read()

    Check length

    >>> len(polys)
    78

    Return centroid of polygon at index 1

    >>> polys[1].centroid
    (-91.19578469430738, 39.990883050220845)

    Type dir(polys[1]) at the python interpreter to get a list of supported methods

    """
    MODES = ['r']
    FORMATS = ['wkt']

    def __init__(self, *args, **kwargs):
        FileIO.FileIO.__init__(self, *args, **kwargs)
        self.__idx = {}
        self.__pos = 0
        self.__open()

    def open(self):
        self.__open()

    def __open(self):
        self.dataObj = open(self.dataPath, self.mode)
        self.wkt = WKTParser()

    def _read(self):
        FileIO.FileIO._complain_ifclosed(self.closed)
        if self.__pos not in self.__idx:
            self.__idx[self.__pos] = self.dataObj.tell()
        line = self.dataObj.readline()
        if line:
            shape = self.wkt.fromWKT(line)
            shape.id = self.pos
            self.__pos += 1
            self.pos += 1
            return shape
        else:
            self.seek(0)
            return None

    def seek(self, n):
        FileIO.FileIO.seek(self, n)
        pos = self.pos
        if pos in self.__idx:
            self.dataObj.seek(self.__idx[pos])
            self.__pos = pos
        else:
            while pos not in self.__idx:
                s = self._read()
                if not s:
                    raise IndexError("%d not in range(0,%d)" % (
                        pos, max(self.__idx.keys())))
            self.pos = pos
            self.__pos = pos
            self.dataObj.seek(self.__idx[pos])

    def close(self):
        self.dataObj.close()
        FileIO.FileIO.close(self)


########NEW FILE########
__FILENAME__ = Tables
__all__ = ['DataTable']
import FileIO

__author__ = "Charles R Schmidt <schmidtc@gmail.com>"


class DataTable(FileIO.FileIO):
    """ DataTable provides additional functionality to FileIO for data table file tables
        FileIO Handlers that provide data tables should subclass this instead of FileIO """
    class _By_Col:
        def __init__(self, parent):
            self.p = parent

        def __repr__(self):
            return "keys: " + self.p.header.__repr__()

        def __getitem__(self, key):
            return self.p._get_col(key)

        def __setitem__(self, key, val):
            self.p.cast(key, val)

        def __call__(self, key):
            return self.p._get_col(key)

    def __init__(self, *args, **kwargs):
        FileIO.FileIO.__init__(self, *args, **kwargs)

    def __repr__(self):
        return 'DataTable: % s' % self.dataPath

    def __len__(self):
        """ __len__ should be implemented by DataTable Subclasses """
        raise NotImplementedError

    @property
    def by_col(self):
        return self._By_Col(self)

    def _get_col(self, key):
        """ returns the column vector
        """
        if not self.header:
            raise AttributeError('Please set the header')
        if key in self.header:
            return self[:, self.header.index(key)]
        else:
            raise AttributeError('Field: % s does not exist in header' % key)

    def __getitem__(self, key):
        """ DataTables fully support slicing in 2D,
            To provide slicing,  handlers must provide __len__
            Slicing accepts up to two arguments.
            Syntax,
            table[row]
            table[row, col]
            table[row_start:row_stop]
            table[row_start:row_stop:row_step]
            table[:, col]
            table[:, col_start:col_stop]
            etc.

            ALL indices are Zero-Offsets,
            i.e.
            #>>> assert index in range(0, len(table))
        """
        prevPos = self.tell()
        if issubclass(type(key), basestring):
            raise TypeError("index should be int or slice")
        if issubclass(type(key), int) or isinstance(key, slice):
            rows = key
            cols = None
        elif len(key) > 2:
            raise TypeError("DataTables support two dimmensional slicing,  % d slices provided" % len(key))
        elif len(key) == 2:
            rows, cols = key
        else:
            raise TypeError("Key: % r,  is confusing me.  I don't know what to do" % key)
        if isinstance(rows, slice):
            row_start, row_stop, row_step = rows.indices(len(self))
            self.seek(row_start)
            data = [self.next() for i in range(row_start, row_stop, row_step)]
        else:
            self.seek(slice(rows).indices(len(self))[1])
            data = [self.next()]
        if cols is not None:
            if isinstance(cols, slice):
                col_start, col_stop, col_step = cols.indices(len(data[0]))
                data = [r[col_start:col_stop:col_step] for r in data]
            else:
                #col_start, col_stop, col_step = cols, cols+1, 1
                data = [r[cols] for r in data]
        self.seek(prevPos)
        return data


def _test():
    import doctest
    doctest.testmod(verbose=True)

if __name__ == '__main__':
    _test()

########NEW FILE########
__FILENAME__ = test_FileIO

########NEW FILE########
__FILENAME__ = shapefile
"""
A Pure Python ShapeFile Reader and Writer
This module is selfcontained and does not require pysal.
This module returns and expects dictionary based data strucutres.
This module should be wrapped into your native data strcutures.

Contact:
Charles Schmidt
GeoDa Center
Arizona State University
Tempe, AZ
http://geodacenter.asu.edu
"""

__author__ = "Charles R Schmidt <schmidtc@gmail.com>"

from struct import calcsize, unpack, pack
from cStringIO import StringIO
from itertools import izip, islice
import array
import sys
if sys.byteorder == 'little':
    SYS_BYTE_ORDER = '<'
else:
    SYS_BYTE_ORDER = '>'
STRUCT_ITEMSIZE = {}
STRUCT_ITEMSIZE['i'] = calcsize('i')
STRUCT_ITEMSIZE['d'] = calcsize('d')

__all__ = ['shp_file', 'shx_file']

#SHAPEFILE Globals


def struct2arrayinfo(struct):
    struct = list(struct)
    names = [x[0] for x in struct]
    types = [x[1] for x in struct]
    orders = [x[2] for x in struct]
    lname, ltype, lorder = struct.pop(0)
    groups = {}
    g = 0
    groups[g] = {'names': [lname], 'size': STRUCT_ITEMSIZE[ltype],
                 'fmt': ltype, 'order': lorder}
    while struct:
        name, type, order = struct.pop(0)
        if order == lorder:
            groups[g]['names'].append(name)
            groups[g]['size'] += STRUCT_ITEMSIZE[type]
            groups[g]['fmt'] += type
        else:
            g += 1
            groups[g] = {'names': [name], 'size': STRUCT_ITEMSIZE[
                type], 'fmt': type, 'order': order}
        lname, ltype, lorder = name, type, order
    return [groups[x] for x in range(g + 1)]

HEADERSTRUCT = (
    ('File Code', 'i', '>'),
    ('Unused0', 'i', '>'),
    ('Unused1', 'i', '>'),
    ('Unused2', 'i', '>'),
    ('Unused3', 'i', '>'),
    ('Unused4', 'i', '>'),
    ('File Length', 'i', '>'),
    ('Version', 'i', '<'),
    ('Shape Type', 'i', '<'),
    ('BBOX Xmin', 'd', '<'),
    ('BBOX Ymin', 'd', '<'),
    ('BBOX Xmax', 'd', '<'),
    ('BBOX Ymax', 'd', '<'),
    ('BBOX Zmin', 'd', '<'),
    ('BBOX Zmax', 'd', '<'),
    ('BBOX Mmin', 'd', '<'),
    ('BBOX Mmax', 'd', '<'))
UHEADERSTRUCT = struct2arrayinfo(HEADERSTRUCT)
RHEADERSTRUCT = (
    ('Record Number', 'i', '>'),
    ('Content Length', 'i', '>'))
URHEADERSTRUCT = struct2arrayinfo(RHEADERSTRUCT)


def noneMax(a, b):
    if a is None:
        return b
    if b is None:
        return a
    return max(a, b)


def noneMin(a, b):
    if a is None:
        return b
    if b is None:
        return a
    return min(a, b)


def _unpackDict(structure, fileObj):
    """Utility Function, Requires a Tuple of tuples that desribes the element structure...

    _unpackDict(structure tuple, fileObj file) -> dict

    Arguments:
        structure -- tuple of tuples -- (('FieldName 1','type','byteOrder'),('FieldName 2','type','byteOrder'))
        fileObj -- file -- an open file at the correct position!
    Returns:
        {'FieldName 1': value, 'FieldName 2': value}
    Side Effects:
        #file at new position

    Example:
    >>> import pysal
    >>> _unpackDict(UHEADERSTRUCT,open(pysal.examples.get_path('10740.shx'),'rb')) == {'BBOX Xmax': -105.29012, 'BBOX Ymax': 36.219799000000002, 'BBOX Mmax': 0.0, 'BBOX Zmin': 0.0, 'BBOX Mmin': 0.0, 'File Code': 9994, 'BBOX Ymin': 34.259672000000002, 'BBOX Xmin': -107.62651, 'Unused0': 0, 'Unused1': 0, 'Unused2': 0, 'Unused3': 0, 'Unused4': 0, 'Version': 1000, 'BBOX Zmax': 0.0, 'Shape Type': 5, 'File Length': 830}
    True
    """
    d = {}
    for struct in structure:
        items = unpack(struct['order'] + struct['fmt'],
                       fileObj.read(struct['size']))
        for i, name in enumerate(struct['names']):
            d[name] = items[i]
    return d


def _unpackDict2(d, structure, fileObj):
    """Utility Function, used arrays instead from struct

    Arguments:
        d -- dict -- Dictionary to be updated.
        structure -- tuple of tuples -- (('FieldName 1',('type',n),'byteOrder'),('FieldName 2',('type',n,'byteOrder'))
    """
    for name, dtype, order in structure:
        dtype, n = dtype
        result = array.array(dtype)
        result.fromstring(fileObj.read(result.itemsize * n))
        if order != SYS_BYTE_ORDER:
            result.byteswap()
        d[name] = result.tolist()
    return d


def _packDict(structure, d):
    """Utility Function

    _packDict(structure tuple, d dict) -> str

    Arguments:
        structure -- tuple of tuples -- (('FieldName 1','type','byteOrder'),('FieldName 2','type','byteOrder'))
        d -- dict -- {'FieldName 1': value, 'FieldName 2': value}

    Example:
    >>> s = _packDict( (('FieldName 1','i','<'),('FieldName 2','i','<')), {'FieldName 1': 1, 'FieldName 2': 2} )
    >>> s==pack('<ii',1,2)
    True
    >>> unpack('<ii',s)
    (1, 2)
    """
    string = b''
    for name, dtype, order in structure:
        if len(dtype) > 1:
            string += pack(order + dtype, *d[name])
        else:
            string += pack(order + dtype, d[name])
    return string


class shp_file:
    """
    Reads and Writes the SHP compenent of a ShapeFile

    Attributes:
    header -- dict -- Contents of the SHP header. #For contents see: HEADERSTRUCT
    shape -- int -- ShapeType.

    Notes: The header of both the SHP and SHX files are indentical.

    """
    SHAPE_TYPES = {'POINT': 1, 'ARC': 3, 'POLYGON': 5, 'MULTIPOINT': 8, 'POINTZ': 11, 'ARCZ': 13, 'POLYGONZ': 15, 'MULTIPOINTZ': 18, 'POINTM': 21, 'ARCM': 23, 'POLYGONM': 25, 'MULTIPOINTM': 28, 'MULTIPATCH': 31}

    def __iswritable(self):
        try:
            assert self.__mode == 'w'
        except AssertionError:
            raise IOError("[Errno 9] Bad file descriptor")
        return True

    def __isreadable(self):
        try:
            assert self.__mode == 'r'
        except AssertionError:
            raise IOError("[Errno 9] Bad file descriptor")
        return True

    def __init__(self, fileName, mode='r', shape_type=None):
        self.__mode = mode
        if fileName.lower().endswith('.shp') or fileName.lower().endswith('.shx') or fileName.lower().endswith('.dbf'):
            fileName = fileName[:-4]
        self.fileName = fileName

        if mode == 'r':
            self._open_shp_file()
        elif mode == 'w':
            if shape_type not in self.SHAPE_TYPES:
                raise Exception('Attempt to create shp/shx file of invalid type')
            self._create_shp_file(shape_type)
        else:
            raise Exception('Only "w" and "r" modes are supported')

    def _open_shp_file(self):
        """
        Opens a shp/shx file.

        shp_file(fileName string, 'r') -> Shpfile

        Arguments:
        filename -- the name of the file to create
        mode -- string -- 'r'
        shape_type -- None

        Example:
        >>> import pysal
        >>> shp = shp_file(pysal.examples.get_path('10740.shp'))
        >>> shp.header == {'BBOX Xmax': -105.29012, 'BBOX Ymax': 36.219799000000002, 'BBOX Mmax': 0.0, 'BBOX Zmin': 0.0, 'BBOX Mmin': 0.0, 'File Code': 9994, 'BBOX Ymin': 34.259672000000002, 'BBOX Xmin': -107.62651, 'Unused0': 0, 'Unused1': 0, 'Unused2': 0, 'Unused3': 0, 'Unused4': 0, 'Version': 1000, 'BBOX Zmax': 0.0, 'Shape Type': 5, 'File Length': 260534}
        True
        >>> len(shp)
        195
        """
        self.__isreadable()
        fileName = self.fileName
        self.fileObj = open(fileName + '.shp', 'rb')
        self._shx = shx_file(fileName)
        self.header = _unpackDict(UHEADERSTRUCT, self.fileObj)
        self.shape = TYPE_DISPATCH[self.header['Shape Type']]
        self.__lastShape = 0
        # localizing for convenience
        self.__numRecords = self._shx.numRecords
        # constructing bounding box from header
        h = self.header
        self.bbox = [h['BBOX Xmin'], h['BBOX Ymin'],
                     h['BBOX Xmax'], h['BBOX Ymax']]
        self.shapeType = self.header['Shape Type']

    def _create_shp_file(self, shape_type):
        """
        Creates a shp/shx file.

        shp_file(fileName string, 'w', shape_type string) -> Shpfile

        Arguments:
        filename -- the name of the file to create
        mode -- string -- must be 'w'
        shape_type -- string -- the type of shp/shx file to create. must be one of
                the following: 'POINT', 'POINTZ', 'POINTM',
                'ARC', 'ARCZ', 'ARCM', 'POLYGON', 'POLYGONZ', 'POLYGONM',
                'MULTIPOINT', 'MULTIPOINTZ', 'MULTIPOINTM', 'MULTIPATCH'

        Example:
        >>> import pysal,os
        >>> shp = shp_file('test','w','POINT')
        >>> p = shp_file(pysal.examples.get_path('Point.shp'))
        >>> for pt in p:
        ...   shp.add_shape(pt)
        ...
        >>> shp.close()
        >>> open('test.shp','rb').read() == open(pysal.examples.get_path('Point.shp'),'rb').read()
        True
        >>> open('test.shx','rb').read() == open(pysal.examples.get_path('Point.shx'),'rb').read()
        True
        >>> os.remove('test.shx')
        >>> os.remove('test.shp')
        """
        self.__iswritable()
        fileName = self.fileName
        self.fileObj = open(fileName + '.shp', 'wb')
        self._shx = shx_file(fileName, 'w')
        self.header = {}
        self.header['Shape Type'] = self.SHAPE_TYPES[shape_type]
        self.header['Version'] = 1000
        self.header['Unused0'] = 0
        self.header['Unused1'] = 0
        self.header['Unused2'] = 0
        self.header['Unused3'] = 0
        self.header['Unused4'] = 0
        self.header['File Code'] = 9994
        self.__file_Length = 100
        self.header['File Length'] = 0
        self.header['BBOX Xmax'] = None
        self.header['BBOX Ymax'] = None
        self.header['BBOX Mmax'] = None
        self.header['BBOX Zmax'] = None
        self.header['BBOX Xmin'] = None
        self.header['BBOX Ymin'] = None
        self.header['BBOX Mmin'] = None
        self.header['BBOX Zmin'] = None
        self.shape = TYPE_DISPATCH[self.header['Shape Type']]
        #self.__numRecords = self._shx.numRecords

    def __len__(self):
        return self.__numRecords

    def __iter__(self):
        return self

    def type(self):
        return self.shape.String_Type

    def next(self):
        """returns the next Shape in the shapeFile

        Example:
        >>> import pysal
        >>> list(shp_file(pysal.examples.get_path('Point.shp'))) == [{'Y': -0.25904661905760773, 'X': -0.00068176617532103578, 'Shape Type': 1}, {'Y': -0.25630328607387354, 'X': 0.11697145363360706, 'Shape Type': 1}, {'Y': -0.33930131004366804, 'X': 0.05043668122270728, 'Shape Type': 1}, {'Y': -0.41266375545851519, 'X': -0.041266375545851552, 'Shape Type': 1}, {'Y': -0.44017467248908293, 'X': -0.011462882096069604, 'Shape Type': 1}, {'Y': -0.46080786026200882, 'X': 0.027510917030567628, 'Shape Type': 1}, {'Y': -0.45851528384279472, 'X': 0.075655021834060809, 'Shape Type': 1}, {'Y': -0.43558951965065495, 'X': 0.11233624454148461, 'Shape Type': 1}, {'Y': -0.40578602620087334, 'X': 0.13984716157205224, 'Shape Type': 1}]
        True
        """
        self.__isreadable()
        nextShape = self.__lastShape
        if nextShape == self._shx.numRecords:
            self.__lastShape = 0
            raise StopIteration
        else:
            self.__lastShape = nextShape + 1
            return self.get_shape(nextShape)

    def __seek(self, pos):
        if pos != self.fileObj.tell():
            self.fileObj.seek(pos)

    def __read(self, pos, size):
        self.__isreadable()
        if pos != self.fileObj.tell():
            self.fileObj.seek(pos)
        return self.fileObj.read(size)

    def get_shape(self, shpId):
        self.__isreadable()
        if shpId + 1 > self.__numRecords:
            raise IndexError
        fPosition, bytes = self._shx.index[shpId]
        self.__seek(fPosition)
        #the index does not include the 2 byte record header (which contains, Record ID and Content Length)
        rec_id, con_len = _unpackDict(URHEADERSTRUCT, self.fileObj)
        return self.shape.unpack(StringIO(self.fileObj.read(bytes)))
        #return self.shape.unpack(self.fileObj.read(bytes))

    def __update_bbox(self, s):
        h = self.header
        if s.get('Shape Type') == 1:
            h['BBOX Xmax'] = noneMax(h['BBOX Xmax'], s.get('X'))
            h['BBOX Ymax'] = noneMax(h['BBOX Ymax'], s.get('Y'))
            h['BBOX Mmax'] = noneMax(h['BBOX Mmax'], s.get('M'))
            h['BBOX Zmax'] = noneMax(h['BBOX Zmax'], s.get('Z'))
            h['BBOX Xmin'] = noneMin(h['BBOX Xmin'], s.get('X'))
            h['BBOX Ymin'] = noneMin(h['BBOX Ymin'], s.get('Y'))
            h['BBOX Mmin'] = noneMin(h['BBOX Mmin'], s.get('M'))
            h['BBOX Zmin'] = noneMin(h['BBOX Zmin'], s.get('Z'))
        else:
            h['BBOX Xmax'] = noneMax(h['BBOX Xmax'], s.get('BBOX Xmax'))
            h['BBOX Ymax'] = noneMax(h['BBOX Ymax'], s.get('BBOX Ymax'))
            h['BBOX Mmax'] = noneMax(h['BBOX Mmax'], s.get('BBOX Mmax'))
            h['BBOX Zmax'] = noneMax(h['BBOX Zmax'], s.get('BBOX Zmax'))
            h['BBOX Xmin'] = noneMin(h['BBOX Xmin'], s.get('BBOX Xmin'))
            h['BBOX Ymin'] = noneMin(h['BBOX Ymin'], s.get('BBOX Ymin'))
            h['BBOX Mmin'] = noneMin(h['BBOX Mmin'], s.get('BBOX Mmin'))
            h['BBOX Zmin'] = noneMin(h['BBOX Zmin'], s.get('BBOX Zmin'))
        if not self.shape.HASM:
            self.header['BBOX Mmax'] = 0.0
            self.header['BBOX Mmin'] = 0.0
        if not self.shape.HASZ:
            self.header['BBOX Zmax'] = 0.0
            self.header['BBOX Zmin'] = 0.0

    def add_shape(self, s):
        self.__iswritable()
        self.__update_bbox(s)
        rec = self.shape.pack(s)
        con_len = len(rec)
        self.__file_Length += con_len + 8
        rec_id, pos = self._shx.add_record(con_len)
        self.__seek(pos)
        self.fileObj.write(pack('>ii', rec_id, con_len / 2))
        self.fileObj.write(rec)

    def close(self):
        self._shx.close(self.header)
        if self.__mode == 'w':
            self.header['File Length'] = self.__file_Length / 2
            self.__seek(0)
            self.fileObj.write(_packDict(HEADERSTRUCT, self.header))
        self.fileObj.close()


class shx_file:
    """
    Reads and Writes the SHX compenent of a ShapeFile

    Attributes:
    index -- list -- Contains the file offset and len of each recond in the SHP component
    numRecords -- int -- Number of records

    """
    def __iswritable(self):
        try:
            assert self.__mode == 'w'
        except AssertionError:
            raise IOError("[Errno 9] Bad file descriptor")
        return True

    def __isreadable(self):
        try:
            assert self.__mode == 'r'
        except AssertionError:
            raise IOError("[Errno 9] Bad file descriptor")
        return True

    def __init__(self, fileName=None, mode='r'):
        self.__mode = mode
        if fileName.endswith('.shp') or fileName.endswith('.shx') or fileName.endswith('.dbf'):
            fileName = fileName[:-4]
        self.fileName = fileName

        if mode == 'r':
            self._open_shx_file()
        elif mode == 'w':
            self._create_shx_file()

    def _open_shx_file(self):
        """ Opens the SHX file.

        shx_file(filename,'r') --> shx_file

        Arguments:
        filename -- string -- extension is optional, will remove '.dbf','.shx','.shp' and append '.shx'
        mode -- string -- Must be 'r'

        Example:
        >>> import pysal
        >>> shx = shx_file(pysal.examples.get_path('10740'))
        >>> shx._header == {'BBOX Xmax': -105.29012, 'BBOX Ymax': 36.219799000000002, 'BBOX Mmax': 0.0, 'BBOX Zmin': 0.0, 'BBOX Mmin': 0.0, 'File Code': 9994, 'BBOX Ymin': 34.259672000000002, 'BBOX Xmin': -107.62651, 'Unused0': 0, 'Unused1': 0, 'Unused2': 0, 'Unused3': 0, 'Unused4': 0, 'Version': 1000, 'BBOX Zmax': 0.0, 'Shape Type': 5, 'File Length': 830}
        True
        >>> len(shx.index)
        195
        """
        self.__isreadable()
        self.fileObj = open(self.fileName + '.shx', 'rb')
        self._header = _unpackDict(UHEADERSTRUCT, self.fileObj)
        self.numRecords = numRecords = (self._header['File Length'] - 50) / 4
        index = {}
        fmt = '>%di' % (2 * numRecords)
        size = calcsize(fmt)
        dat = unpack(fmt, self.fileObj.read(size))
        self.index = [(dat[i] * 2, dat[i + 1] * 2) for i in xrange(
            0, len(dat), 2)]

    def _create_shx_file(self):
        """ Creates the SHX file.

        shx_file(filename,'w') --> shx_file

        Arguments:
        filename -- string -- extension is optional, will remove '.dbf','.shx','.shp' and append '.shx'
        mode -- string -- Must be 'w'

        Example:
        >>> import pysal
        >>> shx = shx_file(pysal.examples.get_path('Point'))
        >>> isinstance(shx,shx_file)
        True
        """
        self.__iswritable()
        self.fileObj = open(self.fileName + '.shx', 'wb')
        self.numRecords = 0
        self.index = []
        self.__offset = 100  # length of header
        self.__next_rid = 1  # record IDs start at 1

    def add_record(self, size):
        """ Add a record to the shx index.

        add_record(size int) --> RecordID int

        Arguments:
        size -- int -- the length of the record in bytes NOT including the 8byte record header

        Returns:
        rec_id -- int -- the sequential record ID, 1-based.

        Note: the SHX records contain (Offset, Length) in 16-bit words.

        Example:
        >>> import pysal,os
        >>> shx = shx_file(pysal.examples.get_path('Point'))
        >>> shx.index
        [(100, 20), (128, 20), (156, 20), (184, 20), (212, 20), (240, 20), (268, 20), (296, 20), (324, 20)]
        >>> shx2 = shx_file('test','w')
        >>> [shx2.add_record(rec[1]) for rec in shx.index]
        [(1, 100), (2, 128), (3, 156), (4, 184), (5, 212), (6, 240), (7, 268), (8, 296), (9, 324)]
        >>> shx2.index == shx.index
        True
        >>> shx2.close(shx._header)
        >>> open('test.shx','rb').read() == open(pysal.examples.get_path('Point.shx'),'rb').read()
        True
        >>> os.remove('test.shx')
        """
        self.__iswritable()
        pos = self.__offset
        rec_id = self.__next_rid
        self.index.append((self.__offset, size))
        self.__offset += size + 8  # the 8byte record Header.
        self.numRecords += 1
        self.__next_rid += 1
        return rec_id, pos

    def close(self, header):
        if self.__mode == 'w':
            self.__iswritable()
            header['File Length'] = (
                self.numRecords * calcsize('>ii') + 100) / 2
            self.fileObj.seek(0)
            self.fileObj.write(_packDict(HEADERSTRUCT, header))
            fmt = '>%di' % (2 * self.numRecords)
            values = []
            for off, size in self.index:
                values.extend([off / 2, size / 2])
            self.fileObj.write(pack(fmt, *values))
        self.fileObj.close()


class NullShape:
    Shape_Type = 0
    STRUCT = (('Shape Type', 'i', '<'))

    def unpack(self):
        return None

    def pack(self, x=None):
        return pack('<i', 0)


class Point(object):
    """ Packs and Unpacks a ShapeFile Point Type
    Example:
    >>> import pysal
    >>> shp = shp_file(pysal.examples.get_path('Point.shp'))
    >>> rec = shp.get_shape(0)
    >>> rec == {'Y': -0.25904661905760773, 'X': -0.00068176617532103578, 'Shape Type': 1}
    True
    >>> pos = shp.fileObj.seek(shp._shx.index[0][0]+8) #+8 byte record header
    >>> dat = shp.fileObj.read(shp._shx.index[0][1])
    >>> dat == Point.pack(rec)
    True
    """
    Shape_Type = 1
    String_Type = 'POINT'
    HASZ = False
    HASM = False
    STRUCT = (('Shape Type', 'i', '<'),
              ('X', 'd', '<'),
              ('Y', 'd', '<'))
    USTRUCT = [{'fmt': 'idd', 'order': '<', 'names': ['Shape Type',
                                                      'X', 'Y'], 'size': 20}]

    @classmethod
    def unpack(cls, dat):
        return _unpackDict(cls.USTRUCT, dat)

    @classmethod
    def pack(cls, record):
        rheader = _packDict(cls.STRUCT, record)
        return rheader


class PointZ(Point):
    Shape_Type = 11
    String_Type = "POINTZ"
    HASZ = True
    HASM = True
    STRUCT = (('Shape Type', 'i', '<'),
              ('X', 'd', '<'),
              ('Y', 'd', '<'),
              ('Z', 'd', '<'),
              ('M', 'd', '<'))
    USTRUCT = [{'fmt': 'idddd', 'order': '<', 'names': ['Shape Type',
                                                        'X', 'Y', 'Z', 'M'], 'size': 36}]


class PolyLine:
    """ Packs and Unpacks a ShapeFile PolyLine Type
    Example:
    >>> import pysal
    >>> shp = shp_file(pysal.examples.get_path('Line.shp'))
    >>> rec = shp.get_shape(0)
    >>> rec == {'BBOX Ymax': -0.25832280562918325, 'NumPoints': 3, 'BBOX Ymin': -0.25895877033237352, 'NumParts': 1, 'Vertices': [(-0.0090539248870159517, -0.25832280562918325), (0.0074811573959305822, -0.25895877033237352), (0.0074811573959305822, -0.25895877033237352)], 'BBOX Xmax': 0.0074811573959305822, 'BBOX Xmin': -0.0090539248870159517, 'Shape Type': 3, 'Parts Index': [0]}
    True
    >>> pos = shp.fileObj.seek(shp._shx.index[0][0]+8) #+8 byte record header
    >>> dat = shp.fileObj.read(shp._shx.index[0][1])
    >>> dat == PolyLine.pack(rec)
    True
    """
    HASZ = False
    HASM = False
    String_Type = 'ARC'
    STRUCT = (('Shape Type', 'i', '<'),
              ('BBOX Xmin', 'd', '<'),
              ('BBOX Ymin', 'd', '<'),
              ('BBOX Xmax', 'd', '<'),
              ('BBOX Ymax', 'd', '<'),
              ('NumParts', 'i', '<'),
              ('NumPoints', 'i', '<'))
    USTRUCT = [{'fmt': 'iddddii', 'order': '<', 'names': ['Shape Type', 'BBOX Xmin', 'BBOX Ymin', 'BBOX Xmax', 'BBOX Ymax', 'NumParts', 'NumPoints'], 'size': 44}]

    @classmethod
    def unpack(cls, dat):
        record = _unpackDict(cls.USTRUCT, dat)
        contentStruct = (('Parts Index', ('i', record['NumParts']), '<'),
                         ('Vertices', ('d', 2 * record['NumPoints']), '<'))
        _unpackDict2(record, contentStruct, dat)
        #record['Vertices'] = [(record['Vertices'][i],record['Vertices'][i+1]) for i in xrange(0,record['NumPoints']*2,2)]
        verts = record['Vertices']
        #Next line is equivalent to: zip(verts[::2],verts[1::2])
        record['Vertices'] = list(izip(
            islice(verts, 0, None, 2), islice(verts, 1, None, 2)))
        if not record['Parts Index']:
            record['Parts Index'] = [0]
        return record
        #partsIndex = list(partsIndex)
        #partsIndex.append(None)
        #parts = [vertices[partsIndex[i]:partsIndex[i+1]] for i in xrange(header['NumParts'])]

    @classmethod
    def pack(cls, record):
        rheader = _packDict(cls.STRUCT, record)
        contentStruct = (('Parts Index', '%di' % record['NumParts'], '<'),
                         ('Vertices', '%dd' % (2 * record['NumPoints']), '<'))
        content = {}
        content['Parts Index'] = record['Parts Index']
        verts = []
        [verts.extend(vert) for vert in record['Vertices']]
        content['Vertices'] = verts
        content = _packDict(contentStruct, content)
        return rheader + content


class PolyLineZ(object):
    HASZ = True
    HASM = True
    String_Type = 'ARC'
    STRUCT = (('Shape Type', 'i', '<'),
              ('BBOX Xmin', 'd', '<'),
              ('BBOX Ymin', 'd', '<'),
              ('BBOX Xmax', 'd', '<'),
              ('BBOX Ymax', 'd', '<'),
              ('NumParts', 'i', '<'),
              ('NumPoints', 'i', '<'))
    USTRUCT = [{'fmt': 'iddddii', 'order': '<', 'names': ['Shape Type', 'BBOX Xmin', 'BBOX Ymin', 'BBOX Xmax', 'BBOX Ymax', 'NumParts', 'NumPoints'], 'size': 44}]

    @classmethod
    def unpack(cls, dat):
        record = _unpackDict(cls.USTRUCT, dat)
        contentStruct = (('Parts Index', ('i', record['NumParts']), '<'),
                         ('Vertices', ('d', 2 * record['NumPoints']), '<'),
                         ('Zmin', ('d', 1), '<'),
                         ('Zmax', ('d', 1), '<'),
                         ('Zarray', ('d', record['NumPoints']), '<'),
                         ('Mmin', ('d', 1), '<'),
                         ('Mmax', ('d', 1), '<'),
                         ('Marray', ('d', record['NumPoints']), '<'),)
        _unpackDict2(record, contentStruct, dat)
        verts = record['Vertices']
        record['Vertices'] = list(izip(
            islice(verts, 0, None, 2), islice(verts, 1, None, 2)))
        if not record['Parts Index']:
            record['Parts Index'] = [0]
        record['Zmin'] = record['Zmin'][0]
        record['Zmax'] = record['Zmax'][0]
        record['Mmin'] = record['Mmin'][0]
        record['Mmax'] = record['Mmax'][0]
        return record

    @classmethod
    def pack(cls, record):
        rheader = _packDict(cls.STRUCT, record)
        contentStruct = (('Parts Index', '%di' % record['NumParts'], '<'),
                         ('Vertices', '%dd' % (2 * record['NumPoints']), '<'),
                         ('Zmin', 'd', '<'),
                         ('Zmax', 'd', '<'),
                         ('Zarray', '%dd' % (record['NumPoints']), '<'),
                         ('Mmin', 'd', '<'),
                         ('Mmax', 'd', '<'),
                         ('Marray', '%dd' % (record['NumPoints']), '<'))
        content = {}
        content.update(record)
        content['Parts Index'] = record['Parts Index']
        verts = []
        [verts.extend(vert) for vert in record['Vertices']]
        content['Vertices'] = verts
        content = _packDict(contentStruct, content)
        return rheader + content


class Polygon(PolyLine):
    """ Packs and Unpacks a ShapeFile Polygon Type
    Indentical to PolyLine.

    Example:
    >>> import pysal
    >>> shp = shp_file(pysal.examples.get_path('Polygon.shp'))
    >>> rec = shp.get_shape(1)
    >>> rec == {'BBOX Ymax': -0.3126531125455273, 'NumPoints': 7, 'BBOX Ymin': -0.35957259110238166, 'NumParts': 1, 'Vertices': [(0.05396439570183631, -0.3126531125455273), (0.051473095955454629, -0.35251390848763364), (0.059777428443393454, -0.34254870950210703), (0.063099161438568974, -0.34462479262409174), (0.048981796209073003, -0.35957259110238166), (0.046905713087088297, -0.3126531125455273), (0.05396439570183631, -0.3126531125455273)], 'BBOX Xmax': 0.063099161438568974, 'BBOX Xmin': 0.046905713087088297, 'Shape Type': 5, 'Parts Index': [0]}
    True
    >>> pos = shp.fileObj.seek(shp._shx.index[1][0]+8) #+8 byte record header
    >>> dat = shp.fileObj.read(shp._shx.index[1][1])
    >>> dat == Polygon.pack(rec)
    True
    """
    String_Type = 'POLYGON'


class MultiPoint:
    def __init__(self):
        raise NotImplementedError("No MultiPoint Support at this time.")


class PolygonZ(PolyLineZ):
    String_Type = 'POLYGONZ'


class MultiPointZ:
    def __init__(self):
        raise NotImplementedError("No MultiPointZ Support at this time.")


class PointM:
    def __init__(self):
        raise NotImplementedError("No PointM Support at this time.")


class PolyLineM:
    def __init__(self):
        raise NotImplementedError("No PolyLineM Support at this time.")


class PolygonM:
    def __init__(self):
        raise NotImplementedError("No PolygonM Support at this time.")


class MultiPointM:
    def __init__(self):
        raise NotImplementedError("No MultiPointM Support at this time.")


class MultiPatch:
    def __init__(self):
        raise NotImplementedError("No MultiPatch Support at this time.")

TYPE_DISPATCH = {0: NullShape, 1: Point, 3: PolyLine, 5: Polygon, 8: MultiPoint, 11: PointZ, 13: PolyLineZ, 15: PolygonZ, 18: MultiPointZ, 21: PointM, 23: PolyLineM, 25: PolygonM, 28: MultiPointM, 31: MultiPatch, 'POINT': Point, 'POINTZ': PointZ, 'POINTM': PointM, 'ARC': PolyLine, 'ARCZ': PolyLineZ, 'ARCM': PolyLineM, 'POLYGON': Polygon, 'POLYGONZ': PolygonZ, 'POLYGONM': PolygonM, 'MULTIPOINT': MultiPoint, 'MULTIPOINTZ': MultiPointZ, 'MULTIPOINTM': MultiPointM, 'MULTIPATCH': MultiPatch}


########NEW FILE########
__FILENAME__ = test_shapefile
import unittest
from cStringIO import StringIO
from pysal.core.util.shapefile import noneMax, noneMin, shp_file, shx_file, NullShape, Point, PolyLine, MultiPoint, PointZ, PolyLineZ, PolygonZ, MultiPointZ, PointM, PolyLineM, PolygonM, MultiPointM, MultiPatch
import os
import pysal


class TestNoneMax(unittest.TestCase):
    def test_none_max(self):
        self.assertEqual(5, noneMax(5, None))
        self.assertEqual(1, noneMax(None, 1))
        self.assertEqual(None, noneMax(None, None))


class TestNoneMin(unittest.TestCase):
    def test_none_min(self):
        self.assertEqual(5, noneMin(5, None))
        self.assertEqual(1, noneMin(None, 1))
        self.assertEqual(None, noneMin(None, None))


class test_shp_file(unittest.TestCase):
    def test___init__(self):
        shp = shp_file(pysal.examples.get_path('10740.shp'))
        assert shp.header == {'BBOX Xmax': -105.29012, 'BBOX Ymax': 36.219799000000002, 'BBOX Mmax': 0.0, 'BBOX Zmin': 0.0, 'BBOX Mmin': 0.0, 'File Code': 9994, 'BBOX Ymin': 34.259672000000002, 'BBOX Xmin': -107.62651, 'Unused0': 0, 'Unused1': 0, 'Unused2': 0, 'Unused3': 0, 'Unused4': 0, 'Version': 1000, 'BBOX Zmax': 0.0, 'Shape Type': 5, 'File Length': 260534}

    def test___iter__(self):
        shp = shp_file(pysal.examples.get_path('Point.shp'))
        points = [pt for pt in shp]
        expected = [{'Y': -0.25904661905760773, 'X': -0.00068176617532103578, 'Shape Type': 1},
                    {'Y': -0.25630328607387354, 'X': 0.11697145363360706,
                        'Shape Type': 1},
                    {'Y': -0.33930131004366804, 'X': 0.05043668122270728,
                        'Shape Type': 1},
                    {'Y': -0.41266375545851519, 'X': -0.041266375545851552,
                        'Shape Type': 1},
                    {'Y': -0.44017467248908293, 'X': -0.011462882096069604,
                        'Shape Type': 1},
                    {'Y': -0.46080786026200882, 'X': 0.027510917030567628,
                        'Shape Type': 1},
                    {'Y': -0.45851528384279472, 'X': 0.075655021834060809,
                        'Shape Type': 1},
                    {'Y': -0.43558951965065495, 'X': 0.11233624454148461,
                        'Shape Type': 1},
                    {'Y': -0.40578602620087334, 'X': 0.13984716157205224, 'Shape Type': 1}]
        assert points == expected

    def test___len__(self):
        shp = shp_file(pysal.examples.get_path('10740.shp'))
        assert len(shp) == 195

    def test_add_shape(self):
        shp = shp_file('test_point', 'w', 'POINT')
        points = [{'Shape Type': 1, 'X': 0, 'Y': 0},
                  {'Shape Type': 1, 'X': 1, 'Y': 1},
                  {'Shape Type': 1, 'X': 2, 'Y': 2},
                  {'Shape Type': 1, 'X': 3, 'Y': 3},
                  {'Shape Type': 1, 'X': 4, 'Y': 4}]
        for pt in points:
            shp.add_shape(pt)
        shp.close()

        for a, b in zip(points, shp_file('test_point')):
            self.assertEquals(a, b)
        os.remove('test_point.shp')
        os.remove('test_point.shx')

    def test_close(self):
        shp = shp_file(pysal.examples.get_path('10740.shp'))
        shp.close()
        self.assertEqual(shp.fileObj.closed, True)

    def test_get_shape(self):
        shp = shp_file(pysal.examples.get_path('Line.shp'))
        rec = shp.get_shape(0)
        expected = {'BBOX Ymax': -0.25832280562918325,
                    'NumPoints': 3,
                    'BBOX Ymin': -0.25895877033237352,
                    'NumParts': 1,
                    'Vertices': [(-0.0090539248870159517, -0.25832280562918325),
                                 (0.0074811573959305822, -0.25895877033237352),
                                 (
                                     0.0074811573959305822, -0.25895877033237352)],
                    'BBOX Xmax': 0.0074811573959305822,
                    'BBOX Xmin': -0.0090539248870159517,
                    'Shape Type': 3,
                    'Parts Index': [0]}
        self.assertEqual(expected, shp.get_shape(0))

    def test_next(self):
        shp = shp_file(pysal.examples.get_path('Point.shp'))
        points = [pt for pt in shp]
        expected = {'Y': -0.25904661905760773, 'X': -
                    0.00068176617532103578, 'Shape Type': 1}
        self.assertEqual(expected, shp.next())
        expected = {'Y': -0.25630328607387354, 'X':
                    0.11697145363360706, 'Shape Type': 1}
        self.assertEqual(expected, shp.next())

    def test_type(self):
        shp = shp_file(pysal.examples.get_path('Point.shp'))
        self.assertEqual("POINT", shp.type())
        shp = shp_file(pysal.examples.get_path('Polygon.shp'))
        self.assertEqual("POLYGON", shp.type())
        shp = shp_file(pysal.examples.get_path('Line.shp'))
        self.assertEqual("ARC", shp.type())


class test_shx_file(unittest.TestCase):
    def test___init__(self):
        shx = shx_file(pysal.examples.get_path('Point'))
        assert isinstance(shx, shx_file)

    def test_add_record(self):
        shx = shx_file(pysal.examples.get_path('Point'))
        expectedIndex = [(100, 20), (128, 20), (156, 20),
                         (184, 20), (212, 20), (240, 20),
                         (268, 20), (296, 20), (324, 20)]
        assert shx.index == expectedIndex
        shx2 = shx_file('test', 'w')
        for i, rec in enumerate(shx.index):
            id, location = shx2.add_record(rec[1])
            assert id == (i + 1)
            assert location == rec[0]
        assert shx2.index == shx.index
        shx2.close(shx._header)
        new_shx = open('test.shx', 'rb').read()
        expected_shx = open(pysal.examples.get_path('Point.shx'), 'rb').read()
        assert new_shx == expected_shx
        os.remove('test.shx')

    def test_close(self):
        shx = shx_file(pysal.examples.get_path('Point'))
        shx.close(None)
        self.assertEqual(shx.fileObj.closed, True)


class TestNullShape(unittest.TestCase):
    def test_pack(self):
        null_shape = NullShape()
        self.assertEqual(b'\x00' * 4, null_shape.pack())

    def test_unpack(self):
        null_shape = NullShape()
        self.assertEqual(None, null_shape.unpack())


class TestPoint(unittest.TestCase):
    def test_pack(self):
        record = {"X": 5, "Y": 5, "Shape Type": 1}
        expected = b"\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x14\x40\x00\x00\x00\x00\x00\x00\x14\x40"
        self.assertEqual(expected, Point.pack(record))

    def test_unpack(self):
        dat = StringIO(b"\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x14\x40\x00\x00\x00\x00\x00\x00\x14\x40")
        expected = {"X": 5, "Y": 5, "Shape Type": 1}
        self.assertEqual(expected, Point.unpack(dat))


class TestPolyLine(unittest.TestCase):
    def test_pack(self):
        record = {'BBOX Ymax': -0.25832280562918325, 'NumPoints': 3, 'BBOX Ymin': -0.25895877033237352, 'NumParts': 1, 'Vertices': [(-0.0090539248870159517, -0.25832280562918325), (0.0074811573959305822, -0.25895877033237352), (0.0074811573959305822, -0.25895877033237352)], 'BBOX Xmax': 0.0074811573959305822, 'BBOX Xmin': -0.0090539248870159517, 'Shape Type': 3, 'Parts Index': [0]}
        expected = b"""\x03\x00\x00\x00\xc0\x46\x52\x3a\xdd\x8a\x82\
\xbf\x3d\xc1\x65\xce\xc7\x92\xd0\xbf\x00\xc5\
\xa0\xe5\x8f\xa4\x7e\x3f\x6b\x40\x7f\x60\x5c\
\x88\xd0\xbf\x01\x00\x00\x00\x03\x00\x00\x00\
\x00\x00\x00\x00\xc0\x46\x52\x3a\xdd\x8a\x82\
\xbf\x6b\x40\x7f\x60\x5c\x88\xd0\xbf\x00\xc5\
\xa0\xe5\x8f\xa4\x7e\x3f\x3d\xc1\x65\xce\xc7\
\x92\xd0\xbf\x00\xc5\xa0\xe5\x8f\xa4\x7e\x3f\
\x3d\xc1\x65\xce\xc7\x92\xd0\xbf"""
        self.assertEqual(expected, PolyLine.pack(record))

    def test_unpack(self):
        dat = StringIO(b"""\x03\x00\x00\x00\xc0\x46\x52\x3a\xdd\x8a\x82\
\xbf\x3d\xc1\x65\xce\xc7\x92\xd0\xbf\x00\xc5\
\xa0\xe5\x8f\xa4\x7e\x3f\x6b\x40\x7f\x60\x5c\
\x88\xd0\xbf\x01\x00\x00\x00\x03\x00\x00\x00\
\x00\x00\x00\x00\xc0\x46\x52\x3a\xdd\x8a\x82\
\xbf\x6b\x40\x7f\x60\x5c\x88\xd0\xbf\x00\xc5\
\xa0\xe5\x8f\xa4\x7e\x3f\x3d\xc1\x65\xce\xc7\
\x92\xd0\xbf\x00\xc5\xa0\xe5\x8f\xa4\x7e\x3f\
\x3d\xc1\x65\xce\xc7\x92\xd0\xbf""")
        expected = {'BBOX Ymax': -0.25832280562918325, 'NumPoints': 3, 'BBOX Ymin': -0.25895877033237352, 'NumParts': 1, 'Vertices': [(-0.0090539248870159517, -0.25832280562918325), (0.0074811573959305822, -0.25895877033237352), (0.0074811573959305822, -0.25895877033237352)], 'BBOX Xmax': 0.0074811573959305822, 'BBOX Xmin': -0.0090539248870159517, 'Shape Type': 3, 'Parts Index': [0]}
        self.assertEqual(expected, PolyLine.unpack(dat))


class TestMultiPoint(unittest.TestCase):
    def test___init__(self):
        self.failUnlessRaises(NotImplementedError, MultiPoint)


class TestPointZ(unittest.TestCase):
    def test_pack(self):
        record = {"X": 5, "Y": 5, "Z": 5, "M": 5, "Shape Type": 11}
        expected = b"\x0b\x00\x00\x00\x00\x00\x00\x00\x00\x00\x14@\x00\x00\x00\x00\x00\x00\x14@\x00\x00\x00\x00\x00\x00\x14@\x00\x00\x00\x00\x00\x00\x14@"
        self.assertEqual(expected, PointZ.pack(record))

    def test_unpack(self):
        dat = StringIO(b"\x0b\x00\x00\x00\x00\x00\x00\x00\x00\x00\x14@\x00\x00\x00\x00\x00\x00\x14@\x00\x00\x00\x00\x00\x00\x14@\x00\x00\x00\x00\x00\x00\x14@")
        expected = {"X": 5, "Y": 5, "Z": 5, "M": 5, "Shape Type": 11}
        self.assertEqual(expected, PointZ.unpack(dat))


class TestPolyLineZ(unittest.TestCase):
    def test___init__(self):
        self.failUnlessRaises(NotImplementedError, PolyLineZ)


class TestPolyLineZ(unittest.TestCase):
    def test_pack(self):
        record = {'BBOX Ymax': -0.25832280562918325, 'NumPoints': 3, 'BBOX Ymin': -0.25895877033237352, 'NumParts': 1, 'Vertices': [(-0.0090539248870159517, -0.25832280562918325), (0.0074811573959305822, -0.25895877033237352), (0.0074811573959305822, -0.25895877033237352)], 'BBOX Xmax': 0.0074811573959305822, 'BBOX Xmin': -0.0090539248870159517, 'Shape Type': 13, 'Parts Index': [0], 'Zmin': 0, 'Zmax': 10, 'Zarray': [0, 5, 10], 'Mmin': 2, 'Mmax': 4, 'Marray': [2, 3, 4]}
        expected = b"""\r\x00\x00\x00\xc0FR:\xdd\x8a\x82\xbf=\xc1e\xce\xc7\x92\xd0\xbf\x00\xc5\xa0\xe5\x8f\xa4~?k@\x7f`\\\x88\xd0\xbf\x01\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00\xc0FR:\xdd\x8a\x82\xbfk@\x7f`\\\x88\xd0\xbf\x00\xc5\xa0\xe5\x8f\xa4~?=\xc1e\xce\xc7\x92\xd0\xbf\x00\xc5\xa0\xe5\x8f\xa4~?=\xc1e\xce\xc7\x92\xd0\xbf\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00$@\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x14@\x00\x00\x00\x00\x00\x00$@\x00\x00\x00\x00\x00\x00\x00@\x00\x00\x00\x00\x00\x00\x10@\x00\x00\x00\x00\x00\x00\x00@\x00\x00\x00\x00\x00\x00\x08@\x00\x00\x00\x00\x00\x00\x10@"""
        self.assertEqual(expected, PolyLineZ.pack(record))

    def test_unpack(self):
        dat = StringIO(b"""\r\x00\x00\x00\xc0FR:\xdd\x8a\x82\xbf=\xc1e\xce\xc7\x92\xd0\xbf\x00\xc5\xa0\xe5\x8f\xa4~?k@\x7f`\\\x88\xd0\xbf\x01\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00\xc0FR:\xdd\x8a\x82\xbfk@\x7f`\\\x88\xd0\xbf\x00\xc5\xa0\xe5\x8f\xa4~?=\xc1e\xce\xc7\x92\xd0\xbf\x00\xc5\xa0\xe5\x8f\xa4~?=\xc1e\xce\xc7\x92\xd0\xbf\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00$@\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x14@\x00\x00\x00\x00\x00\x00$@\x00\x00\x00\x00\x00\x00\x00@\x00\x00\x00\x00\x00\x00\x10@\x00\x00\x00\x00\x00\x00\x00@\x00\x00\x00\x00\x00\x00\x08@\x00\x00\x00\x00\x00\x00\x10@""")
        expected = {'BBOX Ymax': -0.25832280562918325, 'NumPoints': 3, 'BBOX Ymin': -0.25895877033237352, 'NumParts': 1, 'Vertices': [(-0.0090539248870159517, -0.25832280562918325), (0.0074811573959305822, -0.25895877033237352), (0.0074811573959305822, -0.25895877033237352)], 'BBOX Xmax': 0.0074811573959305822, 'BBOX Xmin': -0.0090539248870159517, 'Shape Type': 13, 'Parts Index': [0], 'Zmin': 0, 'Zmax': 10, 'Zarray': [0, 5, 10], 'Mmin': 2, 'Mmax': 4, 'Marray': [2, 3, 4]}
        self.assertEqual(expected, PolyLineZ.unpack(dat))


class TestPolygonZ(unittest.TestCase):
    def test_pack(self):
        record = {
            'BBOX Xmin': 0.0,
            'BBOX Xmax': 10.0,
            'BBOX Ymin': 0.0,
            'BBOX Ymax': 10.0,
            'NumPoints': 4,
            'NumParts': 1,
            'Vertices': [(0.0, 0.0),
                         (10.0, 10.0),
                         (10.0, 0.0),
                         (0.0, 0.0)],
            'Shape Type': 15,
            'Parts Index': [0],
            'Zmin': 0,
            'Zmax': 10,
            'Zarray': [0, 10, 0, 0],
            'Mmin': 2,
            'Mmax': 4,
            'Marray': [2, 4, 2, 2]
        }
        dat = StringIO(PolygonZ.pack(record))
        self.assertEqual(record, PolygonZ.unpack(dat))


class TestMultiPointZ(unittest.TestCase):
    def test___init__(self):
        self.failUnlessRaises(NotImplementedError, MultiPointZ)
        # multi_point_z = MultiPointZ()


class TestPointM(unittest.TestCase):
    def test___init__(self):
        self.failUnlessRaises(NotImplementedError, PointM)
        # point_m = PointM()


class TestPolyLineM(unittest.TestCase):
    def test___init__(self):
        self.failUnlessRaises(NotImplementedError, PolyLineM)
        # poly_line_m = PolyLineM()


class TestPolygonM(unittest.TestCase):
    def test___init__(self):
        self.failUnlessRaises(NotImplementedError, PolygonM)
        # polygon_m = PolygonM()


class TestMultiPointM(unittest.TestCase):
    def test___init__(self):
        self.failUnlessRaises(NotImplementedError, MultiPointM)
        # multi_point_m = MultiPointM()


class TestMultiPatch(unittest.TestCase):
    def test___init__(self):
        self.failUnlessRaises(NotImplementedError, MultiPatch)
        # multi_patch = MultiPatch()


class _TestPoints(unittest.TestCase):
    def test1(self):
        """ Test creating and reading Point Shape Files """
        shp = shp_file('test_point', 'w', 'POINT')
        points = [{'Shape Type': 1, 'X': 0, 'Y': 0}, {'Shape Type': 1, 'X': 1, 'Y': 1}, {'Shape Type': 1, 'X': 2, 'Y': 2}, {'Shape Type': 1, 'X': 3, 'Y': 3}, {'Shape Type': 1, 'X': 4, 'Y': 4}]
        for pt in points:
            shp.add_shape(pt)
        shp.close()

        shp = list(shp_file('test_point'))
        for a, b in zip(points, shp):
            self.assertEquals(a, b)
        os.remove('test_point.shp')
        os.remove('test_point.shx')


class _TestPolyLines(unittest.TestCase):
    def test1(self):
        """ Test creating and reading PolyLine Shape Files """
        lines = [[(0, 0), (4, 4)], [(1, 0), (5, 4)], [(2, 0), (6, 4)]]
        shapes = []
        for line in lines:
            x = [v[0] for v in line]
            y = [v[1] for v in line]
            rec = {}
            rec['BBOX Xmin'] = min(x)
            rec['BBOX Ymin'] = min(y)
            rec['BBOX Xmax'] = max(x)
            rec['BBOX Ymax'] = max(y)
            rec['NumPoints'] = len(line)
            rec['NumParts'] = 1
            rec['Vertices'] = line
            rec['Shape Type'] = 3
            rec['Parts Index'] = [0]
            shapes.append(rec)
        shp = shp_file('test_line', 'w', 'ARC')
        for line in shapes:
            shp.add_shape(line)
        shp.close()
        shp = list(shp_file('test_line'))
        for a, b in zip(shapes, shp):
            self.assertEquals(a, b)
        os.remove('test_line.shp')
        os.remove('test_line.shx')


class _TestPolygons(unittest.TestCase):
    def test1(self):
        """ Test creating and reading PolyLine Shape Files """
        lines = [[(0, 0), (4, 4), (5, 4), (
            1, 0), (0, 0)], [(1, 0), (5, 4), (6, 4), (2, 0), (1, 0)]]
        shapes = []
        for line in lines:
            x = [v[0] for v in line]
            y = [v[1] for v in line]
            rec = {}
            rec['BBOX Xmin'] = min(x)
            rec['BBOX Ymin'] = min(y)
            rec['BBOX Xmax'] = max(x)
            rec['BBOX Ymax'] = max(y)
            rec['NumPoints'] = len(line)
            rec['NumParts'] = 1
            rec['Vertices'] = line
            rec['Shape Type'] = 5
            rec['Parts Index'] = [0]
            shapes.append(rec)
        shp = shp_file('test_poly', 'w', 'POLYGON')
        for line in shapes:
            shp.add_shape(line)
        shp.close()
        shp = list(shp_file('test_poly'))
        for a, b in zip(shapes, shp):
            self.assertEquals(a, b)
        os.remove('test_poly.shp')
        os.remove('test_poly.shx')


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_weight_converter
import unittest
import pysal
from pysal.core.util.weight_converter import WeightConverter
from pysal.core.util.weight_converter import weight_convert
import tempfile
import os
import warnings


class test_WeightConverter(unittest.TestCase):
    def setUp(self):
        self.base_dir = pysal.examples.get_path('')
        self.test_files = ['arcgis_ohio.dbf', 'arcgis_txt.txt', 'ohio.swm',
                           'wmat.dat', 'wmat.mtx', 'sids2.gal', 'juvenile.gwt',
                           'geobugs_scot', 'stata_full.txt', 'stata_sparse.txt',
                           'spat-sym-us.mat', 'spat-sym-us.wk1']
        dataformats = ['arcgis_dbf', 'arcgis_text', None, None, None, None, None,
                       'geobugs_text', 'stata_text', 'stata_text', None, None]
        ns = [88, 3, 88, 49, 49, 100, 168, 56, 56, 56, 46, 46]
        self.dataformats = dict(zip(self.test_files, dataformats))
        self.ns = dict(zip(self.test_files, ns))
        self.fileformats = [('dbf', 'arcgis_dbf'), ('txt', 'arcgis_text'), ('swm', None),
                            ('dat', None), ('mtx', None), ('gal', None), ('',
                                                                          'geobugs_text'),
                            ('gwt', None), ('txt', 'stata_text'), ('mat', None), ('wk1', None)]

    def test__setW(self):
        for f in self.test_files:
            with warnings.catch_warnings(record=True) as warn:
                # note: we are just suppressing the warnings here; individual warnings
                #       are tested in their specific readers
                warnings.simplefilter("always")
                wc = WeightConverter(self.base_dir + f,
                                     dataFormat=self.dataformats[f])
            self.assertEqual(wc.w_set(), True)
            self.assertEqual(wc.w.n, self.ns[f])

    def test_write(self):
        for f in self.test_files:
            with warnings.catch_warnings(record=True) as warn:
                # note: we are just suppressing the warnings here; individual warnings
                #       are tested in their specific readers
                warnings.simplefilter("always")
                wc = WeightConverter(self.base_dir + f,
                                     dataFormat=self.dataformats[f])

            for ext, dataformat in self.fileformats:
                if f.lower().endswith(ext):
                    continue
                temp_f = tempfile.NamedTemporaryFile(
                    suffix='.%s' % ext, dir=self.base_dir)
                temp_fname = temp_f.name
                temp_f.close()

                with warnings.catch_warnings(record=True) as warn:
                    # note: we are just suppressing the warnings here; individual warnings
                    #       are tested in their specific readers
                    warnings.simplefilter("always")
                    if ext == 'swm':
                        wc.write(temp_fname, useIdIndex=True)
                    elif dataformat is None:
                        wc.write(temp_fname)
                    elif dataformat in ['arcgis_dbf', 'arcgis_text']:
                        wc.write(temp_fname, dataFormat=dataformat,
                                 useIdIndex=True)
                    elif dataformat == 'stata_text':
                        wc.write(temp_fname, dataFormat=dataformat,
                                 matrix_form=True)
                    else:
                        wc.write(temp_fname, dataFormat=dataformat)

                with warnings.catch_warnings(record=True) as warn:
                    # note: we are just suppressing the warnings here; individual warnings
                    #       are tested in their specific readers
                    warnings.simplefilter("always")
                    if dataformat is None:
                        wnew = pysal.open(temp_fname, 'r').read()
                    else:
                        wnew = pysal.open(temp_fname, 'r', dataformat).read()

                if (ext in ['dbf', 'swm', 'dat', 'wk1', 'gwt'] or dataformat == 'arcgis_text'):
                    self.assertEqual(wnew.n, wc.w.n - len(wc.w.islands))
                else:
                    self.assertEqual(wnew.n, wc.w.n)
                os.remove(temp_fname)

    def test_weight_convert(self):
        for f in self.test_files:
            inFile = self.base_dir + f
            inDataFormat = self.dataformats[f]
            with warnings.catch_warnings(record=True) as warn:
                # note: we are just suppressing the warnings here; individual warnings
                #       are tested in their specific readers
                warnings.simplefilter("always")
                if inDataFormat is None:
                    in_file = pysal.open(inFile, 'r')
                else:
                    in_file = pysal.open(inFile, 'r', inDataFormat)
                wold = in_file.read()
                in_file.close()

            for ext, dataformat in self.fileformats:
                if f.lower().endswith(ext):
                    continue
                temp_f = tempfile.NamedTemporaryFile(
                    suffix='.%s' % ext, dir=self.base_dir)
                outFile = temp_f.name
                temp_f.close()
                outDataFormat, useIdIndex, matrix_form = dataformat, False, False
                if ext == 'swm' or dataformat in ['arcgis_dbf', 'arcgis_text']:
                    useIdIndex = True
                elif dataformat == 'stata_text':
                    matrix_form = True

                with warnings.catch_warnings(record=True) as warn:
                    # note: we are just suppressing the warnings here; individual warnings
                    #       are tested in their specific readers
                    warnings.simplefilter("always")
                    weight_convert(inFile, outFile, inDataFormat, outDataFormat, useIdIndex, matrix_form)

                with warnings.catch_warnings(record=True) as warn:
                    # note: we are just suppressing the warnings here; individual warnings
                    #       are tested in their specific readers
                    warnings.simplefilter("always")
                    if dataformat is None:
                        wnew = pysal.open(outFile, 'r').read()
                    else:
                        wnew = pysal.open(outFile, 'r', dataformat).read()

                if (ext in ['dbf', 'swm', 'dat', 'wk1', 'gwt'] or dataformat == 'arcgis_text'):
                    self.assertEqual(wnew.n, wold.n - len(wold.islands))
                else:
                    self.assertEqual(wnew.n, wold.n)
                os.remove(outFile)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_wkt
import unittest
import pysal


class test_WKTParser(unittest.TestCase):
    def setUp(self):
        #Create some Well-Known Text objects
        self.wktPOINT = 'POINT(6 10)'
        self.wktLINESTRING = 'LINESTRING(3 4,10 50,20 25)'
        self.wktPOLYGON = 'POLYGON((1 1,5 1,5 5,1 5,1 1),(2 2, 3 2, 3 3, 2 3,2 2))'
        self.unsupported = ['MULTIPOINT(3.5 5.6,4.8 10.5)',
                            'MULTILINESTRING((3 4,10 50,20 25),(-5 -8,-10 -8,-15 -4))',
                            'MULTIPOLYGON(((1 1,5 1,5 5,1 5,1 1),(2 2, 3 2, 3 3, 2 3,2 2)),((3 3,6 2,6 4,3 3)))',
                            'GEOMETRYCOLLECTION(POINT(4 6),LINESTRING(4 6,7 10))',
                            'POINT ZM (1 1 5 60)',
                            'POINT M (1 1 80)']
        self.empty = ['POINT EMPTY', 'MULTIPOLYGON EMPTY']
        self.parser = pysal.core.util.WKTParser()

    def test_Point(self):
        pt = self.parser(self.wktPOINT)
        self.assert_(issubclass(type(pt), pysal.cg.Point))
        self.assertEquals(pt[:], (6.0, 10.0))

    def test_LineString(self):
        line = self.parser(self.wktLINESTRING)
        self.assert_(issubclass(type(line), pysal.cg.Chain))
        parts = [[pt[:] for pt in part] for part in line.parts]
        self.assertEquals(parts, [[(3.0, 4.0), (10.0, 50.0), (20.0, 25.0)]])
        self.assertEquals(line.len, 73.455384532199886)

    def test_Polygon(self):
        poly = self.parser(self.wktPOLYGON)
        self.assert_(issubclass(type(poly), pysal.cg.Polygon))
        parts = [[pt[:] for pt in part] for part in poly.parts]
        self.assertEquals(parts, [[(1.0, 1.0), (1.0, 5.0), (5.0, 5.0), (5.0,
                                                                        1.0), (1.0, 1.0)], [(2.0, 2.0), (2.0, 3.0), (3.0, 3.0), (3.0, 2.0),
                                                                                            (2.0, 2.0)]])
        self.assertEquals(
            poly.centroid, (2.9705882352941178, 2.9705882352941178))
        self.assertEquals(poly.area, 17.0)

    def test_fromWKT(self):
        for wkt in self.unsupported:
            self.failUnlessRaises(
                NotImplementedError, self.parser.fromWKT, wkt)
        for wkt in self.empty:
            self.assertEquals(self.parser.fromWKT(wkt), None)
        self.assertEquals(self.parser.__call__, self.parser.fromWKT)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = weight_converter
import os
import pysal

__author__ = "Myunghwa Hwang <mhwang4@gmail.com>"
__all__ = ["weight_convert"]


class WeightConverter(object):

    """
    Open and reads a weights file in a format.
    Then, writes the file in other formats.

    WeightConverter can read a weights file in the following formats:
    GAL, GWT, ArcGIS DBF/SWM/Text, DAT, MAT, MTX, WK1, GeoBUGS Text, and STATA Text.
    It can convert the input file into all of the formats listed above, except GWT.
    Currently, PySAL does not support writing a weights object in the GWT format.

    When an input weight file includes multiple islands and
    the format of an output weight file is ArcGIS DBF/SWM/TEXT, DAT, or WK1,
    the number of observations in the new weights file will be
    the original number of observations substracted by the number of islands.
    This is because ArcGIS DBF/SWM/TEXT, DAT, WK1 formats ignore islands.

    """

    def __init__(self, inputPath, dataFormat=None):
        self.inputPath = inputPath
        self.inputDataFormat = dataFormat
        self._setW()

    def _setW(self):
        """
        Reads a weights file and sets a pysal.weights.weights.W object as an attribute

        Examples
        --------

        Create a WeightConvert object

        >>> wc = WeightConverter(pysal.examples.get_path('arcgis_ohio.dbf'),dataFormat='arcgis_dbf')

        Check whether or not the W object is set as an attribute

        >>> wc.w_set()
        True

        Get the number of observations included in the W object

        >>> wc.w.n
        88

        """
        try:
            if self.inputDataFormat:
                f = pysal.open(self.inputPath, 'r', self.inputDataFormat)
            else:
                f = pysal.open(self.inputPath, 'r')
        except:
            raise IOError('A problem occurred while reading the input file.')
        else:
            try:
                self.w = f.read()
            except:
                raise RuntimeError('A problem occurred while creating a weights object.')
            finally:
                f.close()

    def w_set(self):
        """
        Checks if a source w object is set
        """
        return hasattr(self, 'w')

    def write(self, outputPath, dataFormat=None, useIdIndex=True, matrix_form=True):
        """
        Parameters
        ----------
        outputPath: string
                    path to the output weights file
        dataFormat: string
                    'arcgis_dbf' for ArcGIS DBF format
                    'arcgis_text' for ArcGIS Text format
                    'geobugs_text' for GeoBUGS Text format
                    'stata_text' for STATA Text format
        useIdIndex: boolean
                    True or False
                    Applies only to ArcGIS DBF/SWM/Text formats
        matrix_form: boolean
                     True or False
                     STATA Text format

        Returns
        -------
        A weights file is created

        Examples
        --------
        >>> import tempfile, os, pysal

        Create a WeightConverter object

        >>> wc = WeightConverter(pysal.examples.get_path('sids2.gal'))

        Check whether or not the W object is set as an attribute

        >>> wc.w_set()
        True

        Create a temporary file for this example

        >>> f = tempfile.NamedTemporaryFile(suffix='.dbf')

        Reassign to new variable

        >>> fname = f.name

        Close the temporary named file

        >>> f.close()

        Write the input gal file in the ArcGIS dbf format

        >>> wc.write(fname, dataFormat='arcgis_dbf', useIdIndex=True)

        Create a new weights object from the converted dbf file

        >>> wnew = pysal.open(fname, 'r', 'arcgis_dbf').read()

        Compare the number of observations in two W objects

        >>> wc.w.n == wnew.n
        True

        Clean up the temporary file

        >>> os.remove(fname)

        """
        ext = os.path.splitext(outputPath)[1]
        ext = ext.replace('.', '')
        #if ext.lower() == 'gwt':
        #    raise TypeError, 'Currently, PySAL does not support writing a weights object into a gwt file.'

        if not self.w_set():
            raise RuntimeError('There is no weights object to write out.')

        try:
            if dataFormat:
                o = pysal.open(outputPath, 'w', dataFormat)
            else:
                o = pysal.open(outputPath, 'w')
        except:
            raise IOError('A problem occurred while creating the output file.')
        else:
            try:
                if dataFormat in ['arcgis_text', 'arcgis_dbf'] or ext == 'swm':
                    o.write(self.w, useIdIndex=useIdIndex)
                elif dataFormat == 'stata_text':
                    o.write(self.w, matrix_form=matrix_form)
                else:
                    o.write(self.w)
            except:
                raise RuntimeError('A problem occurred while writing out the weights object')
            finally:
                o.close()


def weight_convert(inPath, outPath, inDataFormat=None, outDataFormat=None, useIdIndex=True, matrix_form=True):
    """
    A utility function for directly converting a given weight
    file into the format specified in outPath

    Parameters
    ----------
    inPath: string
            path to the input weights file
    outPath: string
             path to the output weights file
    indataFormat: string
                  'arcgis_dbf' for ArcGIS DBF format
                  'arcgis_text' for ArcGIS Text format
                  'geobugs_text' for GeoBUGS Text format
                  'stata_text' for STATA Text format
    outdataFormat: string
                   'arcgis_dbf' for ArcGIS DBF format
                   'arcgis_text' for ArcGIS Text format
                   'geobugs_text' for GeoBUGS Text format
                   'stata_text' for STATA Text format
    useIdIndex: boolean
                True or False
                Applies only to ArcGIS DBF/SWM/Text formats
    matrix_form: boolean
                 True or False
                 STATA Text format

    Returns
    -------
    A weights file is created

    Examples
    --------
    >>> import tempfile, os, pysal

    Create a temporary file for this example

    >>> f = tempfile.NamedTemporaryFile(suffix='.dbf')

    Reassign to new variable

    >>> fname = f.name

    Close the temporary named file

    >>> f.close()

    Create a WeightConverter object

    >>> weight_convert(pysal.examples.get_path('sids2.gal'), fname, outDataFormat='arcgis_dbf', useIdIndex=True)

    Create a new weights object from the gal file

    >>> wold = pysal.open(pysal.examples.get_path('sids2.gal'), 'r').read()

    Create a new weights object from the converted dbf file

    >>> wnew = pysal.open(fname, 'r', 'arcgis_dbf').read()

    Compare the number of observations in two W objects

    >>> wold.n == wnew.n
    True

    Clean up the temporary file

    >>> os.remove(fname)

    """

    converter = WeightConverter(inPath, dataFormat=inDataFormat)
    converter.write(outPath, dataFormat=outDataFormat,
                    useIdIndex=useIdIndex, matrix_form=matrix_form)


########NEW FILE########
__FILENAME__ = wkt
from pysal import cg
import re

__author__ = "Charles R Schmidt <schmidtc@gmail.com>"
__all__ = ['WKTParser']
#####################################################################
## ToDo: Add Well-Known-Binary support...
##       * WKB spec:
##  http://webhelp.esri.com/arcgisserver/9.3/dotNet/index.htm#geodatabases/the_ogc_103951442.htm
##
##
#####################################################################


class WKTParser:
    """ Class to represent OGC WKT, supports reading and writing
        Modified from...
        # URL: http://dev.openlayers.org/releases/OpenLayers-2.7/lib/OpenLayers/Format/WKT.js
        #Reg Ex Strings copied from OpenLayers.Format.WKT

    Example
    -------
    >>> from pysal.core.IOHandlers import wkt
    >>> import pysal

    Create some Well-Known Text objects

    >>> p = 'POLYGON((1 1,5 1,5 5,1 5,1 1),(2 2, 3 2, 3 3, 2 3,2 2))'
    >>> pt = 'POINT(6 10)'
    >>> l = 'LINESTRING(3 4,10 50,20 25)'

    Instantiate the parser

    >>> wkt = WKTParser()

    Inspect our WKT polygon

    >>> wkt(p).parts
    [[(1.0, 1.0), (1.0, 5.0), (5.0, 5.0), (5.0, 1.0), (1.0, 1.0)], [(2.0, 2.0), (2.0, 3.0), (3.0, 3.0), (3.0, 2.0), (2.0, 2.0)]]
    >>> wkt(p).centroid
    (2.9705882352941178, 2.9705882352941178)
    >>> wkt(p).area
    17.0

    Inspect pt, our WKT point object

    >>> wkt(pt)
    (6.0, 10.0)

    Inspect our WKT linestring

    >>> wkt(l).len
    73.45538453219989
    >>> wkt(l).parts
    [[(3.0, 4.0), (10.0, 50.0), (20.0, 25.0)]]

    Read in WKT from a file

    >>> f = pysal.open(pysal.examples.get_path('stl_hom.wkt'))
    >>> f.mode
    'r'
    >>> f.header
    []

    See local doctest output for the items not tested...

    """
    regExes = {'typeStr': re.compile('^\s*([\w\s]+)\s*\(\s*(.*)\s*\)\s*$'),
               'spaces': re.compile('\s+'),
               'parenComma': re.compile('\)\s*,\s*\('),
               'doubleParenComma': re.compile('\)\s*\)\s*,\s*\(\s*\('),  # can't use {2} here
               'trimParens': re.compile('^\s*\(?(.*?)\)?\s*$')}

    def __init__(self):
        self.parsers = p = {}
        p['point'] = self.Point
        p['linestring'] = self.LineString
        p['polygon'] = self.Polygon

    def Point(self, geoStr):
        coords = self.regExes['spaces'].split(geoStr.strip())
        return cg.Point((coords[0], coords[1]))

    def LineString(self, geoStr):
        points = geoStr.strip().split(',')
        points = map(self.Point, points)
        return cg.Chain(points)

    def Polygon(self, geoStr):
        rings = self.regExes['parenComma'].split(geoStr.strip())
        for i, ring in enumerate(rings):
            ring = self.regExes['trimParens'].match(ring).groups()[0]
            ring = self.LineString(ring).vertices
            rings[i] = ring
        return cg.Polygon(rings)

    def fromWKT(self, wkt):
        matches = self.regExes['typeStr'].match(wkt)
        if matches:
            geoType, geoStr = matches.groups()
            geoType = geoType.lower().strip()
            try:
                return self.parsers[geoType](geoStr)
            except KeyError:
                raise NotImplementedError("Unsupported WKT Type: %s" % geoType)
        else:
            return None
    __call__ = fromWKT
if __name__ == '__main__':
    p = 'POLYGON((1 1,5 1,5 5,1 5,1 1),(2 2, 3 2, 3 3, 2 3,2 2))'
    pt = 'POINT(6 10)'
    l = 'LINESTRING(3 4,10 50,20 25)'
    wktExamples = ['POINT(6 10)',
                   'LINESTRING(3 4,10 50,20 25)',
                   'POLYGON((1 1,5 1,5 5,1 5,1 1),(2 2, 3 2, 3 3, 2 3,2 2))',
                   'MULTIPOINT(3.5 5.6,4.8 10.5)',
                   'MULTILINESTRING((3 4,10 50,20 25),(-5 -8,-10 -8,-15 -4))',
                   'MULTIPOLYGON(((1 1,5 1,5 5,1 5,1 1),(2 2, 3 2, 3 3, 2 3,2 2)),((3 3,6 2,6 4,3 3)))',
                   'GEOMETRYCOLLECTION(POINT(4 6),LINESTRING(4 6,7 10))',
                   'POINT ZM (1 1 5 60)',
                   'POINT M (1 1 80)',
                   'POINT EMPTY',
                   'MULTIPOLYGON EMPTY']
    wkt = WKTParser()


########NEW FILE########
__FILENAME__ = gamma
"""
Gamma index for spatial autocorrelation


"""
__author__ = "Luc Anselin <luc.anselin@asu.edu>"

import pysal
import numpy as np

__all__ = ['Gamma']

PERMUTATIONS = 999


class Gamma:
    """Gamma index for spatial autocorrelation


    Parameters
    ----------

    y               : array
                      variable measured across n spatial units
    w               : W
                      spatial weights instance
                      can be binary or row-standardized
    operation       : attribute similarity function
                      'c' cross product (default)
                      's' squared difference
                      'a' absolute difference
    standardize     : standardize variables first
                      'no' keep as is (default)
                      'yes' or 'y' standardize to mean zero and variance one
    permutations    : int
                      number of random permutations for calculation of pseudo-p_values

    Attributes
    ----------
    y            : array
                   original variable
    w            : W
                   original w object
    op           : attribute similarity function
    stand        : standardization
    permutations : int
                   number of permutations
    gamma        : float
                   value of Gamma index
    sim_g        : array (if permutations>0)
                   vector of Gamma index values for permuted samples
    p_sim_g      : array (if permutations>0)
                   p-value based on permutations (one-sided)
                   null: spatial randomness
                   alternative: the observed Gamma is more extreme than under randomness
                   implemented as a two-sided test
    mean_g       : average of permuted Gamma values
    min_g        : minimum of permuted Gamma values
    max_g        : maximum of permuted Gamma values


    Examples
    --------

    use same example as for join counts to show similarity

    >>> import numpy as np
    >>> w=pysal.lat2W(4,4)
    >>> y=np.ones(16)
    >>> y[0:8]=0
    >>> np.random.seed(12345)
    >>> g = pysal.Gamma(y,w)
    >>> g.g
    20.0
    >>> g.g_z
    3.1879280354548638
    >>> g.p_sim_g
    0.0030000000000000001
    >>> g.min_g
    0.0
    >>> g.max_g
    20.0
    >>> g.mean_g
    11.093093093093094
    >>> np.random.seed(12345)
    >>> g1 = pysal.Gamma(y,w,operation='s')
    >>> g1.g
    8.0
    >>> g1.g_z
    -3.7057554345954791
    >>> g1.p_sim_g
    0.001
    >>> g1.min_g
    14.0
    >>> g1.max_g
    48.0
    >>> g1.mean_g
    25.623623623623622
    >>> np.random.seed(12345)
    >>> g2 = pysal.Gamma(y,w,operation='a')
    >>> g2.g
    8.0
    >>> g2.g_z
    -3.7057554345954791
    >>> g2.p_sim_g
    0.001
    >>> g2.min_g
    14.0
    >>> g2.max_g
    48.0
    >>> g2.mean_g
    25.623623623623622
    >>> np.random.seed(12345)
    >>> g3 = pysal.Gamma(y,w,standardize='y')
    >>> g3.g
    32.0
    >>> g3.g_z
    3.7057554345954791
    >>> g3.p_sim_g
    0.001
    >>> g3.min_g
    -48.0
    >>> g3.max_g
    20.0
    >>> g3.mean_g
    -3.2472472472472473
    >>> np.random.seed(12345)
    >>> def func(z,i,j):
    ...     q = z[i]*z[j]
    ...     return q
    ...
    >>> g4 = pysal.Gamma(y,w,operation=func)
    >>> g4.g
    20.0
    >>> g4.g_z
    3.1879280354548638
    >>> g4.p_sim_g
    0.0030000000000000001

    """
    def __init__(self, y, w, operation='c', standardize='no', permutations=PERMUTATIONS):
        self.w = w
        self.y = y
        self.op = operation
        self.stand = standardize.lower()
        self.permutations = permutations
        if self.stand == 'yes' or self.stand == 'y':
            ym = np.mean(self.y)
            ysd = np.std(self.y)
            ys = (self.y - ym) / ysd
            self.y = ys
        self.g = self.__calc(self.y, self.op)

        if permutations:
            sim = [self.__calc(np.random.permutation(self.y), self.op)
                   for i in xrange(permutations)]
            self.sim_g = np.array(sim)
            self.min_g = np.min(self.sim_g)
            self.mean_g = np.mean(self.sim_g)
            self.max_g = np.max(self.sim_g)
            p_sim_g = self.__pseudop(self.sim_g, self.g)
            self.p_sim_g = p_sim_g
            self.g_z = (self.g - self.mean_g) / np.std(self.sim_g)

    def __calc(self, z, op):
        if op == 'c':     # cross-product
            zl = pysal.lag_spatial(self.w, z)
            g = (z * zl).sum()
        elif op == 's':   # squared difference
            zs = np.zeros(z.shape)
            z2 = z ** 2
            for i, i0 in enumerate(self.w.id_order):
                neighbors = self.w.neighbor_offsets[i0]
                wijs = self.w.weights[i0]
                zw = zip(neighbors, wijs)
                zs[i] = sum([wij * (z2[i] - 2.0 * z[i] * z[
                    j] + z2[j]) for j, wij in zw])
            g = zs.sum()
        elif op == 'a':    # absolute difference
            zs = np.zeros(z.shape)
            for i, i0 in enumerate(self.w.id_order):
                neighbors = self.w.neighbor_offsets[i0]
                wijs = self.w.weights[i0]
                zw = zip(neighbors, wijs)
                zs[i] = sum([wij * abs(z[i] - z[j]) for j, wij in zw])
            g = zs.sum()
        else:              # any previously defined function op
            zs = np.zeros(z.shape)
            for i, i0 in enumerate(self.w.id_order):
                neighbors = self.w.neighbor_offsets[i0]
                wijs = self.w.weights[i0]
                zw = zip(neighbors, wijs)
                zs[i] = sum([wij * op(z, i, j) for j, wij in zw])
            g = zs.sum()
        return g

    def __pseudop(self, sim, g):
        above = sim >= g
        larger = above.sum()
        psim = (larger + 1.) / (self.permutations + 1.)
        if psim > 0.5:
            psim = (self.permutations - larger + 1.) / (self.permutations + 1.)
        return psim


########NEW FILE########
__FILENAME__ = geary
"""
Geary's C statistic for spatial autocorrelation
"""
__author__ = "Sergio J. Rey <srey@asu.edu> "

import numpy as np
import scipy.stats as stats

__all__ = ['Geary']


class Geary:
    """
    Global Geary C Autocorrelation statistic

    Parameters
    ----------
    y              : array
    w              : W
                     spatial weights
    transformation : string
                     weights transformation, default is binary.
                     Other options include "R": row-standardized, "D":
                     doubly-standardized, "U": untransformed (general
                     weights), "V": variance-stabilizing.
    permutations   : int
                     number of random permutations for calculation of
                     pseudo-p_values

    Attributes
    ----------
    y              : array
                     original variable
    w              : W
                     spatial weights
    permutations   : int
                     number of permutations
    C              : float
                     value of statistic
    EC             : float
                     expected value
    VC             : float
                     variance of G under normality assumption
    z_norm         : float
                     z-statistic for C under normality assumption
    z_rand         : float
                     z-statistic for C under randomization assumption
    p_norm         : float
                     p-value under normality assumption (one-tailed)
    p_rand         : float
                     p-value under randomization assumption (one-tailed)
    sim            : array (if permutations!=0)
                     vector of I values for permutated samples
    p_sim          : float (if permutations!=0)
                     p-value based on permutations (one-tailed)
                     null: sptial randomness
                     alternative: the observed C is extreme
                     it is either extremely high or extremely low
    EC_sim         : float (if permutations!=0)
                     average value of C from permutations
    VC_sim         : float (if permutations!=0)
                     variance of C from permutations
    seC_sim        : float (if permutations!=0)
                     standard deviation of C under permutations.
    z_sim          : float (if permutations!=0)
                     standardized C based on permutations
    p_z_sim        : float (if permutations!=0)
                     p-value based on standard normal approximation from
                     permutations (one-tailed)

    Examples
    --------
    >>> import pysal
    >>> w = pysal.open(pysal.examples.get_path("book.gal")).read()
    >>> f = pysal.open(pysal.examples.get_path("book.txt"))
    >>> y = np.array(f.by_col['y'])
    >>> c = Geary(y,w,permutations=0)
    >>> print round(c.C,7)
    0.3330108
    >>> print round(c.p_norm,7)
    9.2e-05
    >>>
    """
    def __init__(self, y, w, transformation="r", permutations=999):
        self.n = len(y)
        self.y = y
        w.transform = transformation
        self.w = w
        self.permutations = permutations
        self.__moments()
        xn = xrange(len(y))
        self.xn = xn
        self.y2 = y * y
        yd = y - y.mean()
        yss = sum(yd * yd)
        self.den = yss * self.w.s0 * 2.0
        self.C = self.__calc(y)
        de = self.C - 1.0
        self.EC = 1.0
        self.z_norm = de / self.seC_norm
        self.z_rand = de / self.seC_rand
        if de > 0:
            self.p_norm = 1 - stats.norm.cdf(self.z_norm)
            self.p_rand = 1 - stats.norm.cdf(self.z_rand)
        else:
            self.p_norm = stats.norm.cdf(self.z_norm)
            self.p_rand = stats.norm.cdf(self.z_rand)


        if permutations:
            sim = [self.__calc(np.random.permutation(self.y))
                   for i in xrange(permutations)]
            self.sim = sim = np.array(sim)
            above = sim >= self.C
            larger = sum(above)
            if (permutations - larger) < larger:
                larger = permutations - larger
            self.p_sim = (larger + 1.) / (permutations + 1.)
            self.EC_sim = sum(sim) / permutations
            self.seC_sim = np.array(sim).std()
            self.VC_sim = self.seC_sim ** 2
            self.z_sim = (self.C - self.EC_sim) / self.seC_sim
            self.p_z_sim = 1 - stats.norm.cdf(np.abs(self.z_sim))

    def __moments(self):
        y = self.y
        n = self.n
        w = self.w
        s0 = w.s0
        s1 = w.s1
        s2 = w.s2
        s02 = s0 * s0

        yd = y - y.mean()
        k = (1 / (sum(yd ** 4)) * ((sum(yd ** 2)) ** 2))
        vc_rand = (1 / (n * ((n - 2) ** 2) * s02)) * \
            ((((n - 1) * s1) * (n * n - 3 * n + 3 - (n - 1) * k))
             - ((.25 * (n - 1) * s2) * (n * n + 3 * n - 6 -
                (n * n - n + 2) * k))
                + (s02 * (n * n - 3 - ((n - 1) ** 2) * k)))
        vc_norm = ((1 / (2 * (n + 1) * s02)) *
                   ((2 * s1 + s2) * (n - 1) - 4 * s02))

        self.VC_rand = vc_rand
        self.VC_norm = vc_norm
        self.seC_rand = vc_rand ** (0.5)
        self.seC_norm = vc_norm ** (0.5)

    def __calc(self, y):
        ys = np.zeros(y.shape)
        y2 = y ** 2
        for i, i0 in enumerate(self.w.id_order):
            neighbors = self.w.neighbor_offsets[i0]
            wijs = self.w.weights[i0]
            z = zip(neighbors, wijs)
            ys[i] = sum([wij * (y2[i] - 2 * y[i] * y[j] + y2[j])
                         for j, wij in z])
        a = (self.n - 1) * sum(ys)
        return a / self.den



########NEW FILE########
__FILENAME__ = getisord
"""
Getis and Ord G statistic for spatial autocorrelation
"""
__author__ = "Sergio J. Rey <srey@asu.edu>, Myunghwa Hwang <mhwang4@gmail.com> "
__all__ = ['G', 'G_Local']

from pysal.common import np, stats, math
from pysal.weights.spatial_lag import lag_spatial as slag

PERMUTATIONS = 999


class G:
    """
    Global G Autocorrelation Statistic

    Parameters:
    -----------
    y: array
    w: DistanceBand W
       spatial weights based on distance band
    permutations: int
                  the number of random permutations for calculating
                  pseudo p_values

    Attributes:
    -----------
    y: array
       original variable
    w: DistanceBand W
       spatial weights based on distance band
    permutation: int
                 the number of permutations
    G: float
       the value of statistic
    EG: float
        the expected value of statistic
    VG: float
        the variance of G under normality assumption
    z_norm: float
         standard normal test statistic
    p_norm: float
            p-value under normality assumption (one-sided)
    sim: array (if permutations > 0)
         vector of G values for permutated samples
    p_sim: float
           p-value based on permutations (one-sided)
           null: spatial randomness
           alternative: the observed G is extreme
                        it is either extremely high or extremely low
    EG_sim: float
            average value of G from permutations
    VG_sim: float
            variance of G from permutations
    seG_sim: float
             standard deviation of G under permutations.
    z_sim: float
           standardized G based on permutations
    p_z_sim: float
             p-value based on standard normal approximation from
             permutations (one-sided)

    Notes
    -----
    Moments are based on normality assumption.

    Examples
    --------
    >>> from pysal.weights.Distance import DistanceBand
    >>> import numpy
    >>> numpy.random.seed(10)

    Preparing a point data set

    >>> points = [(10, 10), (20, 10), (40, 10), (15, 20), (30, 20), (30, 30)]

    Creating a weights object from points

    >>> w = DistanceBand(points,threshold=15)
    >>> w.transform = "B"

    Prepareing a variable

    >>> y = numpy.array([2, 3, 3.2, 5, 8, 7])

    Applying Getis and Ord G test
    >>> g = G(y,w)

    Examining the results
    >>> print "%.8f" % g.G
    0.55709779

    >>> print "%.4f" % g.p_norm
    0.1729

    """
    def __init__(self, y, w, permutations=PERMUTATIONS):
        self.n = len(y)
        self.y = y
        w.transform = "B"
        self.w = w
        self.permutations = permutations
        self.__moments()
        self.y2 = y * y
        y = y.reshape(len(y), 1)  # Ensure that y is an n by 1 vector, otherwise y*y.T == y*y
        self.den_sum = (y * y.T).sum() - (y * y).sum()
        self.G = self.__calc(self.y)
        self.z_norm = (self.G - self.EG) / math.sqrt(self.VG)
        self.p_norm = 1.0 - stats.norm.cdf(np.abs(self.z_norm))

        if permutations:
            sim = [self.__calc(np.random.permutation(self.y))
                   for i in xrange(permutations)]
            self.sim = sim = np.array(sim)
            above = sim >= self.G
            larger = sum(above)
            if (self.permutations - larger) < larger:
                larger = self.permutations - larger
            self.p_sim = (larger + 1.0) / (permutations + 1.)
            self.EG_sim = sum(sim) / permutations
            self.seG_sim = sim.std()
            self.VG_sim = self.seG_sim ** 2
            self.z_sim = (self.G - self.EG_sim) / self.seG_sim
            self.p_z_sim = 1. - stats.norm.cdf(np.abs(self.z_sim))

    def __moments(self):
        y = self.y
        n = self.n
        w = self.w
        n2 = n * n
        s0 = w.s0
        self.EG = s0 / (n * (n - 1))
        s02 = s0 * s0
        s1 = w.s1
        s2 = w.s2
        b0 = (n2 - 3 * n + 3) * s1 - n * s2 + 3 * s02
        b1 = (-1.) * ((n2 - n) * s1 - 2 * n * s2 + 6 * s02)
        b2 = (-1.) * (2 * n * s1 - (n + 3) * s2 + 6 * s02)
        b3 = 4 * (n - 1) * s1 - 2 * (n + 1) * s2 + 8 * s02
        b4 = s1 - s2 + s02
        self.b0 = b0
        self.b1 = b1
        self.b2 = b2
        self.b3 = b3
        self.b4 = b4
        y2 = y * y
        y3 = y * y2
        y4 = y2 * y2
        EG2 = (b0 * (sum(
            y2) ** 2) + b1 * sum(y4) + b2 * (sum(y) ** 2) * sum(y2))
        EG2 += b3 * sum(y) * sum(y3) + b4 * (sum(y) ** 4)
        EG2NUM = EG2
        EG2DEN = (((sum(y) ** 2 - sum(y2)) ** 2) * n * (n - 1) * (
            n - 2) * (n - 3))
        self.EG2 = EG2NUM / EG2DEN
        self.VG = self.EG2 - self.EG ** 2

    def __calc(self, y):
        yl = slag(self.w, y)
        self.num = y * yl
        return self.num.sum() / self.den_sum


class G_Local:
    """
    Generalized Local G Autocorrelation Statistic

    Parameters:
    -----------
    y: array
       variable
    w: DistanceBand W
       weights instance that is based on threshold distance
       and is assumed to be aligned with y
    transform: string
       the type of w, either 'B' (binary) or 'R' (row-standardized)
    permutations: int
                  the number of random permutations for calculating
                  pseudo p values
    star: boolean
          whether or not to include focal observation in sums
          default is False

    Attributes:
    -----------
    y: array
       original variable
    w: DistanceBand W
       original weights object
    permutations: int
                 the number of permutations
    Gs: array of floats
        the value of the orginal G statistic in Getis & Ord (1992)
    EGs: float
         expected value of Gs under normality assumption
         the values is scalar, since the expectation is identical
         across all observations
    VGs: array of floats
         variance values of Gs under normality assumption
    Zs: array of floats
        standardized Gs
    p_norm: array of floats
            p-value under normality assumption (one-sided)
            for two-sided tests, this value should be multiplied by 2
    sim: array of arrays of floats (if permutations>0)
         vector of I values for permutated samples
    p_sim: array of floats
           p-value based on permutations (one-sided)
           null: spatial randomness
           alternative: the observed G is extreme
                        it is either extremely high or extremely low
    EG_sim: array of floats
            average value of G from permutations
    VG_sim: array of floats
            variance of G from permutations
    seG_sim: array of floats
             standard deviation of G under permutations.
    z_sim: array of floats
           standardized G based on permutations
    p_z_sim: array of floats
             p-value based on standard normal approximation from
             permutations (one-sided)

    Notes
    -----
    To compute moments of Gs under normality assumption,
    PySAL considers w is either binary or row-standardized.
    For binary weights object, the weight value for self is 1
    For row-standardized weights object, the weight value for self is
    1/(the number of its neighbors + 1).

    References
    ----------
    Getis, A. and Ord., J.K. (1992) The analysis of spatial association by use of
    distance statistics. Geographical Analysis, 24(3):189-206
    Ord, J.K. and Getis, A. (1995) Local spatial autocorrelation statistics:
    distributional issues and an application. Geographical Analysis, 27(4):286-306
    Getis, A. and Ord, J. K. (1996) Local spatial statistics: an overview,
    in Spatial Analysis: Modelling in a GIS Environment, edited by Longley, P.
    and Batty, M.

    Examples
    --------
    >>> from pysal.weights.Distance import DistanceBand
    >>> import numpy
    >>> numpy.random.seed(10)

    Preparing a point data set

    >>> points = [(10, 10), (20, 10), (40, 10), (15, 20), (30, 20), (30, 30)]

    Creating a weights object from points

    >>> w = DistanceBand(points,threshold=15)

    Prepareing a variable

    >>> y = numpy.array([2, 3, 3.2, 5, 8, 7])

    Applying Getis and Ord local G test using a binary weights object
    >>> lg = G_Local(y,w,transform='B')

    Examining the results
    >>> lg.Zs
    array([-1.0136729 , -0.04361589,  1.31558703, -0.31412676,  1.15373986,
            1.77833941])
    >>> lg.p_sim[0]
    0.10100000000000001

    >>> numpy.random.seed(10)

    Applying Getis and Ord local G* test using a binary weights object
    >>> lg_star = G_Local(y,w,transform='B',star=True)

    Examining the results
    >>> lg_star.Zs
    array([-1.39727626, -0.28917762,  0.65064964, -0.28917762,  1.23452088,
            2.02424331])
    >>> lg_star.p_sim[0]
    0.10100000000000001

    >>> numpy.random.seed(10)

    Applying Getis and Ord local G test using a row-standardized weights object
    >>> lg = G_Local(y,w,transform='R')

    Examining the results
    >>> lg.Zs
    array([-0.62074534, -0.01780611,  1.31558703, -0.12824171,  0.28843496,
            1.77833941])
    >>> lg.p_sim[0]
    0.10100000000000001

    >>> numpy.random.seed(10)

    Applying Getis and Ord local G* test using a row-standardized weights object
    >>> lg_star = G_Local(y,w,transform='R',star=True)

    Examining the results
    >>> lg_star.Zs
    array([-0.62488094, -0.09144599,  0.41150696, -0.09144599,  0.24690418,
            1.28024388])
    >>> lg_star.p_sim[0]
    0.10100000000000001

    """
    def __init__(self, y, w, transform='R', permutations=PERMUTATIONS, star=False):
        self.n = len(y)
        self.y = y
        self.w = w
        self.w_original = w.transform
        self.w.transform = self.w_transform = transform.lower()
        self.permutations = permutations
        self.star = star
        self.calc()
        self.p_norm = np.array(
            [1 - stats.norm.cdf(np.abs(i)) for i in self.Zs])
        if permutations:
            self.__crand()
            sim = np.transpose(self.rGs)
            above = sim >= self.Gs
            larger = sum(above)
            low_extreme = (self.permutations - larger) < larger
            larger[low_extreme] = self.permutations - larger[low_extreme]
            self.p_sim = (larger + 1.0) / (permutations + 1)
            self.sim = sim
            self.EG_sim = sim.mean()
            self.seG_sim = sim.std()
            self.VG_sim = self.seG_sim * self.seG_sim
            self.z_sim = (self.Gs - self.EG_sim) / self.seG_sim
            self.p_z_sim = 1 - stats.norm.cdf(np.abs(self.z_sim))

    def __crand(self):
        y = self.y
        rGs = np.zeros((self.n, self.permutations))
        n_1 = self.n - 1
        rid = range(n_1)
        prange = range(self.permutations)
        k = self.w.max_neighbors + 1
        rids = np.array([np.random.permutation(rid)[0:k] for i in prange])
        ids = np.arange(self.w.n)
        ido = self.w.id_order
        wc = self.__getCardinalities()
        if self.w_transform == 'r':
            den = np.array(wc) + self.star
        else:
            den = np.ones(self.w.n)
        for i in range(self.w.n):
            idsi = ids[ids != i]
            np.random.shuffle(idsi)
            yi_star = y[i] * self.star
            wci = wc[i]
            rGs[i] = (y[idsi[rids[:, 0:wci]]]).sum(1) + yi_star
            rGs[i] = (np.array(rGs[i]) / den[i]) / (
                self.y_sum - (1 - self.star) * y[i])
        self.rGs = rGs

    def __getCardinalities(self):
        ido = self.w.id_order
        self.wc = np.array(
            [self.w.cardinalities[ido[i]] for i in range(self.n)])
        return self.wc

    def calc(self):
        y = self.y
        y2 = y * y
        self.y_sum = y_sum = sum(y)
        y2_sum = sum(y2)

        if not self.star:
            yl = 1.0 * slag(self.w, y)
            ydi = y_sum - y
            self.Gs = yl / ydi
            N = self.n - 1
            yl_mean = ydi / N
            s2 = (y2_sum - y2) / N - (yl_mean) ** 2
        else:
            self.w.transform = 'B'
            yl = 1.0 * slag(self.w, y)
            yl += y
            if self.w_transform == 'r':
                yl = yl / (self.__getCardinalities() + 1.0)
            self.Gs = yl / y_sum
            N = self.n
            yl_mean = y.mean()
            s2 = y.var()

        EGs_num, VGs_num = 1.0, 1.0
        if self.w_transform == 'b':
            W = self.__getCardinalities()
            W += self.star
            EGs_num = W * 1.0
            VGs_num = (W * (1.0 * N - W)) / (1.0 * N - 1)

        self.EGs = (EGs_num * 1.0) / N
        self.VGs = (VGs_num) * (1.0 / (N ** 2)) * ((s2 * 1.0) / (yl_mean ** 2))
        self.Zs = (self.Gs - self.EGs) / np.sqrt(self.VGs)

        self.w.transform = self.w_original


########NEW FILE########
__FILENAME__ = join_counts
"""
Spatial autocorrelation for binary attributes

"""
__author__ = "Sergio J. Rey <srey@asu.edu> , Luc Anselin <luc.anselin@asu.edu>"

import pysal
import numpy as np

__all__ = ['Join_Counts']

PERMUTATIONS = 999


class Join_Counts:
    """Binary Join Counts


    Parameters
    ----------

    y               : array
                      binary variable measured across n spatial units
    w               : W
                      spatial weights instance
    permutations    : int
                      number of random permutations for calculation of pseudo-p_values

    Attributes
    ----------
    y            : array
                   original variable
    w            : W
                   original w object
    permutations : int
                   number of permutations
    bb           : float
                   number of black-black joins
    ww           : float
                   number of white-white joins
    bw           : float
                   number of black-white joins
    J            : float
                   number of joins
    sim_bb       : array (if permutations>0)
                   vector of bb values for permuted samples
    p_sim_bb     : array (if permutations>0)
                   p-value based on permutations (one-sided)
                   null: spatial randomness
                   alternative: the observed bb is greater than under randomness
    mean_bb      : average of permuted bb values
    min_bb       : minimum of permuted bb values
    max_bb       : maximum of permuted bb values
    sim_bw       : array (if permutations>0)
                   vector of bw values for permuted samples
    p_sim_bw     : array (if permutations>0)
                   p-value based on permutations (one-sided)
                   null: spatial randomness
                   alternative: the observed bw is greater than under randomness
    mean_bw      : average of permuted bw values
    min_bw       : minimum of permuted bw values
    max_bw       : maximum of permuted bw values


    Examples
    --------

    Replicate example from anselin and rey

    >>> import numpy as np
    >>> w = pysal.lat2W(4, 4)
    >>> y = np.ones(16)
    >>> y[0:8] = 0
    >>> np.random.seed(12345)
    >>> jc = pysal.Join_Counts(y, w)
    >>> jc.bb
    10.0
    >>> jc.bw
    4.0
    >>> jc.ww
    10.0
    >>> jc.J
    24.0
    >>> len(jc.sim_bb)
    999
    >>> jc.p_sim_bb
    0.0030000000000000001
    >>> np.mean(jc.sim_bb)
    5.5465465465465469
    >>> np.max(jc.sim_bb)
    10.0
    >>> np.min(jc.sim_bb)
    0.0
    >>> len(jc.sim_bw)
    999
    >>> jc.p_sim_bw
    1.0
    >>> np.mean(jc.sim_bw)
    12.811811811811811
    >>> np.max(jc.sim_bw)
    24.0
    >>> np.min(jc.sim_bw)
    7.0
    >>>
    """
    def __init__(self, y, w, permutations=PERMUTATIONS):
        w.transformation = 'b'  # ensure we have binary weights
        self.w = w
        self.y = y
        self.permutations = permutations
        self.J = w.s0 / 2.
        self.bb, self.ww, self.bw = self.__calc(self.y)

        if permutations:
            sim = [self.__calc(np.random.permutation(self.y))
                   for i in xrange(permutations)]
            sim_jc = np.array(sim)
            self.sim_bb = sim_jc[:, 0]
            self.min_bb = np.min(self.sim_bb)
            self.mean_bb = np.mean(self.sim_bb)
            self.max_bb = np.max(self.sim_bb)
            self.sim_bw = sim_jc[:, 2]
            self.min_bw = np.min(self.sim_bw)
            self.mean_bw = np.mean(self.sim_bw)
            self.max_bw = np.max(self.sim_bw)
            p_sim_bb = self.__pseudop(self.sim_bb, self.bb)
            p_sim_bw = self.__pseudop(self.sim_bw, self.bw)
            self.p_sim_bb = p_sim_bb
            self.p_sim_bw = p_sim_bw

    def __calc(self, z):
        zl = pysal.lag_spatial(self.w, z)
        bb = sum(z * zl) / 2.0
        zw = 1 - z
        zl = pysal.lag_spatial(self.w, zw)
        ww = sum(zw * zl) / 2.0
        bw = self.J - (bb + ww)
        return (bb, ww, bw)

    def __pseudop(self, sim, jc):
        above = sim >= jc
        larger = sum(above)
        psim = (larger + 1.) / (self.permutations + 1.)
        return psim

########NEW FILE########
__FILENAME__ = mapclassify
"""
A module of classification schemes for choropleth mapping.
"""
__author__ = "Sergio J. Rey"
__credits__ = "Copyright (c) 2009-10 Sergio J. Rey"

__all__ = ['Map_Classifier', 'quantile', 'Box_Plot', 'Equal_Interval',
           'Fisher_Jenks', 'Fisher_Jenks_Sampled', 'Jenks_Caspall',
           'Jenks_Caspall_Forced', 'Jenks_Caspall_Sampled', 
           'Max_P_Classifier', 'Maximum_Breaks', 'Natural_Breaks',
           'Quantiles', 'Percentiles', 'Std_Mean', 'User_Defined',
           'gadf', 'K_classifiers']

from pysal.common import *

K = 5  # default number of classes in any map scheme with this as an argument


def quantile(y, k=4):
    """
    Calculates the quantiles for an array

    Parameters
    ----------
    y : array (n,1)
        values to classify
    k : int
        number of quantiles

    Returns
    -------
    implicit  : array (n,1)
                quantile values

    Examples
    --------
    >>> x = np.arange(1000)
    >>> quantile(x)
    array([ 249.75,  499.5 ,  749.25,  999.  ])
    >>> quantile(x, k = 3)
    array([ 333.,  666.,  999.])
    >>>

    Note that if there are enough ties that the quantile values repeat, we
    collapse to pseudo quantiles in which case the number of classes will be less than k

    >>> x = [1.0] * 100
    >>> x.extend([3.0] * 40)
    >>> len(x)
    140
    >>> y = np.array(x)
    >>> quantile(y)
    array([ 1.,  3.])
    """
    w = 100. / k
    p = np.arange(w, 100 + w, w)
    if p[-1] > 100.0:
        p[-1] = 100.0
    q = np.array([stats.scoreatpercentile(y, pct) for pct in p])
    return np.unique(q)


def binC(y, bins):
    """
    Bin categorical/qualitative data

    Parameters
    ----------
    y : array (n,q)
        categorical values
    bins :  array (k,1)
        unique values associated with each bin

    Return
    ------
    b : array (n,q)
        bin membership, values between 0 and k-1

    Examples
    --------
    >>> np.random.seed(1)
    >>> x = np.random.randint(2, 8, (10, 3))
    >>> bins = range(2, 8)
    >>> x
    array([[7, 5, 6],
           [2, 3, 5],
           [7, 2, 2],
           [3, 6, 7],
           [6, 3, 4],
           [6, 7, 4],
           [6, 5, 6],
           [4, 6, 7],
           [4, 6, 3],
           [3, 2, 7]])
    >>> y = binC(x, bins)
    >>> y
    array([[5, 3, 4],
           [0, 1, 3],
           [5, 0, 0],
           [1, 4, 5],
           [4, 1, 2],
           [4, 5, 2],
           [4, 3, 4],
           [2, 4, 5],
           [2, 4, 1],
           [1, 0, 5]])
    >>>
    """

    if np.rank(y) == 1:
        k = 1
        n = np.shape(y)[0]
    else:
        n, k = np.shape(y)
    b = np.zeros((n, k), dtype='int')
    for i, bin in enumerate(bins):
        b[np.nonzero(y == bin)] = i

    # check for non-binned items and print a warning if needed
    vals = set(y.flatten())
    for val in vals:
        if val not in bins:
            print 'warning: value not in bin: ', val
            print 'bins: ', bins

    return b


def bin(y, bins):
    """
    bin interval/ratio data

    Parameters
    ----------
    y : array (n,q)
        values to bin
    bins : array (k,1)
        upper bounds of each bin (monotonic)

    Returns
    -------
    b : array (n,q)
        values of values between 0 and k-1

    Examples
    --------
    >>> np.random.seed(1)
    >>> x = np.random.randint(2, 20, (10, 3))
    >>> bins = [10, 15, 20]
    >>> b = bin(x, bins)
    >>> x
    array([[ 7, 13, 14],
           [10, 11, 13],
           [ 7, 17,  2],
           [18,  3, 14],
           [ 9, 15,  8],
           [ 7, 13, 12],
           [16,  6, 11],
           [19,  2, 15],
           [11, 11,  9],
           [ 3,  2, 19]])
    >>> b
    array([[0, 1, 1],
           [0, 1, 1],
           [0, 2, 0],
           [2, 0, 1],
           [0, 1, 0],
           [0, 1, 1],
           [2, 0, 1],
           [2, 0, 1],
           [1, 1, 0],
           [0, 0, 2]])
    >>>
    """
    if np.rank(y) == 1:
        k = 1
        n = np.shape(y)[0]
    else:
        n, k = np.shape(y)
    b = np.zeros((n, k), dtype='int')
    i = len(bins)
    if type(bins) != list:
        bins = bins.tolist()
    binsc = copy.copy(bins)
    while binsc:
        i -= 1
        c = binsc.pop(-1)
        b[np.nonzero(y <= c)] = i
    return b


def bin1d(x, bins):
    """
    place values of a 1-d array into bins and determine counts of values in
    each bin

    Parameters
    ----------
    y : 1-d array
        values to bin
    bins : array (k,1)
        upper bounds of each bin (monotonic)

    Returns
    -------
    tuple(binIds,counts)

    binIds: 1-d array of integer bin Ids

    counts: number of elements of x falling in each bin

    Examples
    --------
    >>> x = np.arange(100, dtype = 'float')
    >>> bins = [25, 74, 100]
    >>> binIds, counts = bin1d(x, bins)
    >>> binIds
    array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
           0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
           1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
           2, 2, 2, 2, 2, 2, 2, 2])
    >>> counts
    array([26, 49, 25])
    """
    left = [-sys.maxint]
    left.extend(bins[0:-1])
    right = bins
    cuts = zip(left, right)
    k = len(bins)
    binIds = np.zeros(x.shape, dtype='int')
    while cuts:
        k -= 1
        l, r = cuts.pop(-1)
        binIds += (x > l) * (x <= r) * k
    counts = np.bincount(binIds)
    return (binIds, counts)


def load_example():
    """
    Helper function for doc tests"""
    import pysal
    np.random.seed(10)
    dat = pysal.open(pysal.examples.get_path('calempdensity.csv'))
    cal = np.array([record[-1] for record in dat])
    return cal


def natural_breaks(values, k=5, itmax=100):
    """
    natural breaks helper function
    """
    values = np.array(values)
    n = len(values)
    uv = np.unique(values)
    uvk = len(uv)
    if  uvk < k:
        print 'Warning: Not enough unique values in array to form k classes'
        print "Warning: setting k to %d" % uvk
        k = uvk
    sids = np.random.permutation(range(len(uv)))[0:k]
    seeds = uv[sids]
    seeds.sort()
    diffs = abs(np.matrix([values - seed for seed in seeds]))
    c0 = diffs.argmin(axis=0)
    c0 = np.array(c0)[0]
    solving = True
    solved = False
    rk = range(k)
    it = 0
    while solving:
        # get centroids of clusters
        seeds = [np.median(values[c0 == c]) for c in rk]
        seeds.sort()
        # for each value find closest centroid
        diffs = abs(np.matrix([values - seed for seed in seeds]))
        # assign value to that centroid
        c1 = diffs.argmin(axis=0)
        c1 = np.array(c1)[0]
        #compare new classids to previous
        d = abs(c1 - c0)
        if d.sum() == 0:
            solving = False
            solved = True
        else:
            c0 = c1
        it += 1
        if it == itmax:
            solving = False
    class_ids = c1
    cuts = [max(values[c1 == c]) for c in rk]
    return sids, seeds, diffs, class_ids, solved, it, cuts


def _fisher_jenks_means(values, classes=5, sort=True):
    """
    Jenks Optimal (Natural Breaks) algorithm implemented in Python.
    The original Python code comes from here:
    http://danieljlewis.org/2010/06/07/jenks-natural-breaks-algorithm-in-python/
    and is based on a JAVA and Fortran code available here:
    https://stat.ethz.ch/pipermail/r-sig-geo/2006-March/000811.html

    Returns class breaks such that classes are internally homogeneous while
    assuring heterogeneity among classes.

    """

    if sort:
        values.sort()
    mat1 = []
    for i in range(0, len(values) + 1):
        temp = []
        for j in range(0, classes + 1):
            temp.append(0)
        mat1.append(temp)
    mat2 = []
    for i in range(0, len(values) + 1):
        temp = []
        for j in range(0, classes + 1):
            temp.append(0)
        mat2.append(temp)
    for i in range(1, classes + 1):
        mat1[1][i] = 1
        mat2[1][i] = 0
        for j in range(2, len(values) + 1):
            mat2[j][i] = float('inf')
    v = 0.0
    for l in range(2, len(values) + 1):
        s1 = 0.0
        s2 = 0.0
        w = 0.0
        for m in range(1, l + 1):
            i3 = l - m + 1
            val = float(values[i3 - 1])
            s2 += val * val
            s1 += val
            w += 1
            v = s2 - (s1 * s1) / w
            i4 = i3 - 1
            if i4 != 0:
                for j in range(2, classes + 1):
                    if mat2[l][j] >= (v + mat2[i4][j - 1]):
                        mat1[l][j] = i3
                        mat2[l][j] = v + mat2[i4][j - 1]
        mat1[l][1] = 1
        mat2[l][1] = v

    k = len(values)

    kclass = []
    for i in range(0, classes + 1):
        kclass.append(0)
    kclass[classes] = float(values[len(values) - 1])
    kclass[0] = float(values[0])
    countNum = classes
    while countNum >= 2:
        pivot = mat1[k][countNum]
        id = int(pivot - 2)
        kclass[countNum - 1] = values[id]
        k = int(pivot - 1)
        countNum -= 1
    return kclass


class Map_Classifier:
    """
    Abstract class for all map classifications
    For an array :math:`y` of :math:`n` values, a map classifier places each value
    :math:`y_i` into one of :math:`k` mutually exclusive and exhaustive classes.
    Each classifer defines the classes based on different criteria, but in all
    cases the following hold for the classifiers in PySAL:

    .. math::

              C_j^l < y_i \le C_j^u \  forall  i \in C_j

    where :math:`C_j` denotes class :math:`j` which has lower bound :math:`C_j^l` and upper bound :math:`C_j^u`.


        

    Map Classifiers Supported

    * :class:`~pysal.esda.mapclassify.Box_Plot`
    * :class:`~pysal.esda.mapclassify.Equal_Interval`
    * :class:`~pysal.esda.mapclassify.Fisher_Jenks`
    * :class:`~pysal.esda.mapclassify.Jenks_Caspall`
    * :class:`~pysal.esda.mapclassify.Jenks_Caspall_Forced`
    * :class:`~pysal.esda.mapclassify.Jenks_Caspall_Sampled`
    * :class:`~pysal.esda.mapclassify.Max_P_Classifier`
    * :class:`~pysal.esda.mapclassify.Maximum_Breaks`
    * :class:`~pysal.esda.mapclassify.Natural_Breaks`
    * :class:`~pysal.esda.mapclassify.Quantiles`
    * :class:`~pysal.esda.mapclassify.Percentiles`
    * :class:`~pysal.esda.mapclassify.Std_Mean`
    * :class:`~pysal.esda.mapclassify.User_Defined`

    Utilities:

    In addition to the classifiers, there are several utility functions that can be used to evaluate the properties of a specific classifier for different parameter values, or for automatic selection of a classifier and number of classes.

    * :func:`~pysal.esda.mapclassify.gadf`
    * :class:`~pysal.esda.mapclassify.K_classifiers`

    References
    ----------

    Slocum, T.A., R.B. McMaster, F.C. Kessler and H.H. Howard (2009) *Thematic Cartography and Geovisualization*. Pearson Prentice Hall, Upper Saddle River.

    """

    def __init__(self, y):
        self.name = 'Map Classifier'
        if hasattr(y, 'values'):
            y = y.values # fix for pandas
        self.y = y
        self._classify()
        self._summary()

    def _summary(self):
        yb = self.yb
        self.classes = [np.nonzero(yb == c)[0].tolist() for c in range(self.k)]
        self.tss = self.get_tss()
        self.adcm = self.get_adcm()
        self.gadf = self.get_gadf()

    def _classify(self):
        self._set_bins()
        self.yb, self.counts = bin1d(self.y, self.bins)

    def __str__(self):
        st = self._table_string()
        return st

    def __repr__(self):
        return self._table_string()

    def get_tss(self):
        """
        Total sum of squares around class means

        Returns sum of squares over all class means
        """
        tss = 0
        for class_def in self.classes:
            if len(class_def) > 0:
                yc = self.y[class_def]
                css = yc - yc.mean()
                css *= css
                tss += sum(css)
        return tss

    def _set_bins(self):
        pass

    def get_adcm(self):
        """
        Absolute deviation around class median (ADCM).

        Calculates the absolute deviations of each observation about its class
        median as a measure of fit for the classification method.

        Returns sum of ADCM over all classes
        """
        adcm = 0
        for class_def in self.classes:
            if len(class_def) > 0:
                yc = self.y[class_def]
                yc_med = np.median(yc)
                ycd = np.abs(yc - yc_med)
                adcm += sum(ycd)
        return adcm

    def get_gadf(self):
        """
        Goodness of absolute deviation of fit
        """
        adam = (np.abs(self.y - np.median(self.y))).sum()
        gadf = 1 - self.adcm / adam
        return gadf

    def _table_string(self, width=12, decimal=3):
        fmt = ".%df" % decimal
        fmt = "%" + fmt
        largest = max([len(fmt % i) for i in self.bins])
        width = largest
        fmt = "%d.%df" % (width, decimal)
        fmt = "%" + fmt
        k1 = self.k - 1
        h1 = "Lower"
        h1 = h1.center(largest)
        h2 = " "
        h2 = h2.center(10)
        h3 = "Upper"
        h3 = h3.center(largest + 1)

        largest = "%d" % max(self.counts)
        largest = len(largest) + 15
        h4 = "Count"

        h4 = h4.rjust(largest)
        table = []
        header = h1 + h2 + h3 + h4
        table.append(header)
        table.append("=" * len(header))

        rows = []
        for i, up in enumerate(self.bins):
            if i == 0:
                left = " " * width
                left += "   x[i] <= "
            else:
                left = fmt % self.bins[i - 1]
                left += " < x[i] <= "
            right = fmt % self.bins[i]
            row = left + right
            cnt = "%d" % self.counts[i]
            cnt = cnt.rjust(largest)
            row += cnt
            table.append(row)
        name = self.name
        top = name.center(len(row))
        table.insert(0, top)
        table.insert(1, " ")
        table = "\n".join(table)
        return table


class Equal_Interval(Map_Classifier):
    """
    Equal Interval Classification

    Parameters
    ----------
    y : array (n,1)
        values to classify
    k : int
        number of classes required

    Attributes
    ----------

    yb      : array (n,1)
              bin ids for observations,
              each value is the id of the class the observation belongs to
              yb[i] = j  for j>=1  if bins[j-1] < y[i] <= bins[j], yb[i] = 0  otherwise
    bins    : array (k,1)
              the upper bounds of each class
    k       : int
              the number of classes
    counts  : array (k,1)
              the number of observations falling in each class

    Examples
    --------
    >>> cal = load_example()
    >>> ei = Equal_Interval(cal, k = 5)
    >>> ei.k
    5
    >>> ei.counts
    array([57,  0,  0,  0,  1])
    >>> ei.bins
    array([  822.394,  1644.658,  2466.922,  3289.186,  4111.45 ])
    >>>


    Notes
    -----
    Intervals defined to have equal width:

    .. math::

        bins_j = min(y)+w*(j+1)

    with :math:`w=\\frac{max(y)-min(j)}{k}`
    """

    def __init__(self, y, k=K):
        """
        see class docstring

        """

        self.k = k
        Map_Classifier.__init__(self, y)
        self.name = 'Equal Interval'

    def _set_bins(self):
        y = self.y
        k = self.k
        max_y = max(y)
        min_y = min(y)
        rg = max_y - min_y
        width = rg * 1. / k
        cuts = np.arange(min_y + width, max_y + width, width)
        if len(cuts) > self.k:  # handle overshooting
            cuts = cuts[0:k]
        cuts[-1] = max_y
        bins = cuts.copy()
        self.bins = bins


class Percentiles(Map_Classifier):
    """
    Percentiles Map Classification

    Parameters
    ----------

    y    : array
           attribute to classify
    pct  : array
           percentiles default=[1,10,50,90,99,100]

    Attributes
    ----------
    yb     : array
             bin ids for observations (numpy array n x 1)

    bins   : array
             the upper bounds of each class (numpy array k x 1)

    k      : int
             the number of classes

    counts : int
             the number of observations falling in each class (numpy array k x 1)

    Examples
    --------
    >>> cal = load_example()
    >>> p = Percentiles(cal)
    >>> p.bins
    array([  1.35700000e-01,   5.53000000e-01,   9.36500000e+00,
             2.13914000e+02,   2.17994800e+03,   4.11145000e+03])
    >>> p.counts
    array([ 1,  5, 23, 23,  5,  1])
    >>> p2 = Percentiles(cal, pct = [50, 100])
    >>> p2.bins
    array([    9.365,  4111.45 ])
    >>> p2.counts
    array([29, 29])
    >>> p2.k
    2
    """

    def __init__(self, y, pct=[1, 10, 50, 90, 99, 100]):
        self.pct = pct
        Map_Classifier.__init__(self, y)
        self.name = 'Percentiles'

    def _set_bins(self):
        y = self.y
        pct = self.pct
        self.bins = np.array([stats.scoreatpercentile(y, p) for p in pct])
        self.k = len(self.bins)


class Box_Plot(Map_Classifier):
    """
    Box_Plot Map Classification


    Parameters
    ----------
    y     : array
            attribute to classify
    hinge : float
            multiplier for IQR

    Attributes
    ----------
    yb : array (n,1)
        bin ids for observations
    bins : array (n,1)
        the upper bounds of each class  (monotonic)
    k : int
        the number of classes
    counts : array (k,1)
        the number of observations falling in each class
    low_outlier_ids : array
        indices of observations that are low outliers
    high_outlier_ids : array
        indices of observations that are high outliers

    Notes
    -----

    The bins are set as follows::

        bins[0] = q[0]-hinge*IQR
        bins[1] = q[0]
        bins[2] = q[1]
        bins[3] = q[2]
        bins[4] = q[2]+hinge*IQR
        bins[5] = inf  (see Notes)

    where q is an array of the first three quartiles of y and
    IQR=q[2]-q[0]


    If q[2]+hinge*IQR > max(y) there will only be 5 classes and no high outliers,
        otherwise, there will be 6 classes and at least one high outlier.

    Examples
    --------
    >>> cal = load_example()
    >>> bp = Box_Plot(cal)
    >>> bp.bins
    array([ -5.28762500e+01,   2.56750000e+00,   9.36500000e+00,
             3.95300000e+01,   9.49737500e+01,   4.11145000e+03])
    >>> bp.counts
    array([ 0, 15, 14, 14,  6,  9])
    >>> bp.high_outlier_ids
    array([ 0,  6, 18, 29, 33, 36, 37, 40, 42])
    >>> cal[bp.high_outlier_ids]
    array([  329.92,   181.27,   370.5 ,   722.85,   192.05,   110.74,
            4111.45,   317.11,   264.93])
    >>> bx = Box_Plot(np.arange(100))
    >>> bx.bins
    array([ -49.5 ,   24.75,   49.5 ,   74.25,  148.5 ])

    """

    def __init__(self, y, hinge=1.5):
        """
        Parameters
        ----------
        y : array (n,1)
            attribute to classify
        hinge : float
            multiple of inter-quartile range (default=1.5)
        """
        self.hinge = hinge
        Map_Classifier.__init__(self, y)
        self.name = 'Box Plot'

    def _set_bins(self):
        y = self.y
        pct = [25, 50, 75, 100]
        bins = [stats.scoreatpercentile(y, p) for p in pct]
        iqr = bins[-2] - bins[0]
        self.iqr = iqr
        pivot = self.hinge * iqr
        left_fence = bins[0] - pivot
        right_fence = bins[-2] + pivot
        if right_fence < bins[-1]:
            bins.insert(-1, right_fence)
        else:
            bins[-1] = right_fence
        bins.insert(0, left_fence)
        self.bins = np.array(bins)
        self.k = len(pct)

    def _classify(self):
        Map_Classifier._classify(self)
        self.low_outlier_ids = np.nonzero(self.yb == 0)[0]
        self.high_outlier_ids = np.nonzero(self.yb == 5)[0]


class Quantiles(Map_Classifier):
    """Quantile Map Classification

    Parameters
    ----------
    y : array (n,1)
        values to classify
    k : int
        number of classes required

    Attributes
    ----------

    yb      : array (n,1)
              bin ids for observations,
              each value is the id of the class the observation belongs to
              yb[i] = j  for j>=1  if bins[j-1] < y[i] <= bins[j], yb[i] = 0  otherwise
    bins    : array (k,1)
              the upper bounds of each class
    k       : int
              the number of classes
    counts  : array (k,1)
              the number of observations falling in each class


    Examples
    --------
    >>> cal = load_example()
    >>> q = Quantiles(cal, k = 5)
    >>> q.bins
    array([  1.46400000e+00,   5.79800000e+00,   1.32780000e+01,
             5.46160000e+01,   4.11145000e+03])
    >>> q.counts
    array([12, 11, 12, 11, 12])
    >>>
    """

    def __init__(self, y, k=K):
        self.k = k
        Map_Classifier.__init__(self, y)
        self.name = 'Quantiles'

    def _set_bins(self):
        y = self.y
        k = self.k
        self.bins = quantile(y, k=k)


class Std_Mean(Map_Classifier):
    """
    Standard Deviation and Mean Map Classification

    Parameters
    ----------
    y         : array (n,1)
                values to classify
    multiples : array
                the multiples of the standard deviation to add/subtract from
                the sample mean to define the bins, default=[-2,-1,1,2]

    Attributes
    ----------

    yb      : array (n,1)
              bin ids for observations,
    bins    : array (k,1)
              the upper bounds of each class
    k       : int
              the number of classes
    counts  : array (k,1)
              the number of observations falling in each class


    Examples
    --------
    >>> cal = load_example()
    >>> st = Std_Mean(cal)
    >>> st.k
    5
    >>> st.bins
    array([ -967.36235382,  -420.71712519,   672.57333208,  1219.21856072,
            4111.45      ])
    >>> st.counts
    array([ 0,  0, 56,  1,  1])
    >>>
    >>> st3 = Std_Mean(cal, multiples = [-3, -1.5, 1.5, 3])
    >>> st3.bins
    array([-1514.00758246,  -694.03973951,   945.8959464 ,  1765.86378936,
            4111.45      ])
    >>> st3.counts
    array([ 0,  0, 57,  0,  1])
    >>>

    """
    def __init__(self, y, multiples=[-2, -1, 1, 2]):
        self.multiples = multiples
        Map_Classifier.__init__(self, y)
        self.name = 'Std_Mean'

    def _set_bins(self):
        y = self.y
        s = y.std(ddof=1)
        m = y.mean()
        cuts = [m + s * w for w in self.multiples]
        y_max = y.max()
        if cuts[-1] < y_max:
            cuts.append(y_max)
        self.bins = np.array(cuts)
        self.k = len(cuts)


class Maximum_Breaks(Map_Classifier):
    """
    Maximum Breaks Map Classification

    Parameters
    ----------
    y  : array (n x 1)
         values to classify

    k  : int
         number of classes required

    Attributes
    ----------
    yb : array (nx1)
         bin ids for observations

    bins : array (kx1)
           the upper bounds of each class

    k    : int
           the number of classes

    counts : array (kx1)
             the number of observations falling in each class (numpy array k x 1)

    Examples
    --------
    >>> cal = load_example()
    >>> mb = Maximum_Breaks(cal, k = 5)
    >>> mb.k
    5
    >>> mb.bins
    array([  146.005,   228.49 ,   546.675,  2417.15 ,  4111.45 ])
    >>> mb.counts
    array([50,  2,  4,  1,  1])
    >>>

    """
    def __init__(self, y, k=K, mindiff=0):
        self.k = k
        self.mindiff = mindiff
        Map_Classifier.__init__(self, y)
        self.name = 'Maximum_Breaks'

    def _set_bins(self):
        xs = self.y.copy()
        y = self.y.copy()
        k = self.k
        xs.sort()
        min_diff = self.mindiff
        d = xs[1:] - xs[:-1]
        diffs = d[np.nonzero(d > min_diff)]
        diffs = sp.unique(diffs)
        k1 = k - 1
        if len(diffs) > k1:
            diffs = diffs[-k1:]
        mp = []
        self.cids = []
        for diff in diffs:
            ids = np.nonzero(d == diff)
            for id in ids:
                self.cids.append(id[0])
                cp = ((xs[id] + xs[id + 1]) / 2.)
                mp.append(cp[0])
        mp.append(xs[-1])
        mp.sort()
        self.bins = np.array(mp)


class Natural_Breaks(Map_Classifier):
    """
    Natural Breaks Map Classification

    Parameters
    ----------
    y       : array (n,1)
              values to classify
    k       : int
              number of classes required
    initial : int (default=100)
              number of initial solutions to generate

    Attributes
    ----------

    yb      : array (n,1)
              bin ids for observations,
    bins    : array (k,1)
              the upper bounds of each class
    k       : int
              the number of classes
    counts  : array (k,1)
              the number of observations falling in each class

    Examples
    --------
    >>> import numpy as np
    >>> np.random.seed(10)
    >>> cal = load_example()
    >>> nb = Natural_Breaks(cal, k = 5)
    >>> nb.k
    5
    >>> nb.counts
    array([14, 13, 14, 10,  7])
    >>> nb.bins
    [1.8100000000000001, 7.5999999999999996, 29.82, 181.27000000000001, 4111.4499999999998]
    >>> x = np.array([1] * 50)
    >>> x[-1] = 20
    >>> nb = Natural_Breaks(x, k = 5, initial = 0)
    Warning: Not enough unique values in array to form k classes
    Warning: setting k to 2
    >>> nb.bins
    [1, 20]
    >>> nb.counts
    array([49,  1])


    Notes
    -----
    There is a tradeoff here between speed and consistency of the
    classification
    If you want more speed, set initial to a smaller value (0
    would result in the best speed, if you want more consistent classes in
    multiple runs of Natural_Breaks on the same data, set initial to a
    higher value.


    """
    def __init__(self, y, k=K, initial=100):
        self.k = k
        self.initial = initial
        Map_Classifier.__init__(self, y)
        self.name = 'Natural_Breaks'

    def _set_bins(self):

        x = self.y.copy()
        k = self.k
        res0 = natural_breaks(x, k)
        fit = res0[2].sum()
        for i in xrange(self.initial):
            res = natural_breaks(x, k)
            fit_i = res[2].sum()
            if fit_i < fit:
                res0 = res
        self.bins = res0[-1]
        self.k = len(self.bins)
        self.iterations = res0[-2]


class Fisher_Jenks(Map_Classifier):
    """
    Fisher Jenks optimal classifier - mean based

    Parameters
    ----------
    y : array (n,1)
        values to classify
    k : int
        number of classes required

    Attributes
    ----------

    yb      : array (n,1)
              bin ids for observations
    bins    : array (k,1)
              the upper bounds of each class
    k       : int
              the number of classes
    counts  : array (k,1)
              the number of observations falling in each class


    Examples
    --------

    >>> cal = load_example()
    >>> fj = Fisher_Jenks(cal)
    >>> fj.adcm
    799.24000000000001
    >>> fj.bins
    [75.290000000000006, 192.05000000000001, 370.5, 722.85000000000002, 4111.45]
    >>> fj.counts
    array([49,  3,  4,  1,  1])
    >>>
    """

    def __init__(self, y, k=K):
        self.k = k
        Map_Classifier.__init__(self, y)
        self.name = "Fisher_Jenks"

    def _set_bins(self):
        x = self.y.copy()
        self.bins = _fisher_jenks_means(x, classes=self.k)[1:]


class Fisher_Jenks_Sampled(Map_Classifier):
    """
    Fisher Jenks optimal classifier - mean based using random sample

    Parameters
    ----------
    y      : array (n,1)
             values to classify
    k      : int
             number of classes required
    pct    : float
             The percentage of n that should form the sample
             If pct is specified such that n*pct > 1000, then 
             pct = 1000./n, unless truncate is False
    truncate : binary (Default True)
             truncate pct in cases where pct * n > 1000.

    Attributes
    ----------

    yb      : array (n,1)
              bin ids for observations
    bins    : array (k,1)
              the upper bounds of each class
    k       : int
              the number of classes
    counts  : array (k,1)
              the number of observations falling in each class

    Examples
    --------

    (Turned off due to timing being different across hardware)

    """

    def __init__(self, y, k=K, pct=0.10, truncate=True):
        self.k = k
        n = y.size

        if (pct * n > 1000) and truncate:
            pct = 1000. / n
        ids = np.random.random_integers(0, n - 1, n * pct)
        yr = y[ids]
        yr[-1] = max(y)  # make sure we have the upper bound
        yr[0] = min(y)  # make sure we have the min
        self.original_y = y
        self.pct = pct
        self.yr = yr
        self.yr_n = yr.size
        Map_Classifier.__init__(self, yr)
        self.yb, self.counts = bin1d(y, self.bins)
        self.name = "Fisher_Jenks_Sampled"
        self.y = y
        self._summary()  # have to recalculate summary stats

    def _set_bins(self):
        fj = Fisher_Jenks(self.y, self.k)
        self.bins = fj.bins


class Jenks_Caspall(Map_Classifier):
    """
    Jenks Caspall  Map Classification

    Parameters
    ----------
    y : array (n,1)
        values to classify
    k : int
        number of classes required

    Attributes
    ----------

    yb      : array (n,1)
              bin ids for observations,
    bins    : array (k,1)
              the upper bounds of each class
    k       : int
              the number of classes
    counts  : array (k,1)
              the number of observations falling in each class


    Examples
    --------
    >>> cal = load_example()
    >>> jc = Jenks_Caspall(cal, k = 5)
    >>> jc.bins
    array([  1.81000000e+00,   7.60000000e+00,   2.98200000e+01,
             1.81270000e+02,   4.11145000e+03])
    >>> jc.counts
    array([14, 13, 14, 10,  7])

    """
    def __init__(self, y, k=K):
        self.k = k
        Map_Classifier.__init__(self, y)
        self.name = "Jenks_Caspall"

    def _set_bins(self):
        x = self.y.copy()
        k = self.k
        # start with quantiles
        q = quantile(x, k)
        solving = True
        xb, cnts = bin1d(x, q)
        #class means
        if x.ndim == 1:
            x.shape = (x.size, 1)
        n, k = x.shape
        xm = [np.median(x[xb == i]) for i in np.unique(xb)]
        xb0 = xb.copy()
        q = xm
        it = 0
        rk = range(self.k)
        while solving:
            xb = np.zeros(xb0.shape, int)
            d = abs(x - q)
            xb = d.argmin(axis=1)
            if (xb0 == xb).all():
                solving = False
            else:
                xb0 = xb
            it += 1
            q = np.array([np.median(x[xb == i]) for i in rk])
        cuts = np.array([max(x[xb == i]) for i in sp.unique(xb)])
        cuts.shape = (len(cuts),)
        self.bins = cuts
        self.iterations = it


class Jenks_Caspall_Sampled(Map_Classifier):
    """
    Jenks Caspall Map Classification using a random sample

    Parameters
    ----------

    y       : array (n,1)
              values to classify
    k       : int
              number of classes required
    pct     : float
              The percentage of n that should form the sample
              If pct is specified such that n*pct > 1000, then pct = 1000./n

    Attributes
    ----------

    yb      : array (n,1)
              bin ids for observations,
    bins    : array (k,1)
              the upper bounds of each class
    k       : int
              the number of classes
    counts  : array (k,1)
              the number of observations falling in each class


    Examples
    --------

    >>> cal = load_example()
    >>> x = np.random.random(100000)
    >>> jc = Jenks_Caspall(x)
    >>> jcs = Jenks_Caspall_Sampled(x)
    >>> jc.bins
    array([ 0.19770952,  0.39695769,  0.59588617,  0.79716865,  0.99999425])
    >>> jcs.bins
    array([ 0.18877882,  0.39341638,  0.6028286 ,  0.80070925,  0.99999425])
    >>> jc.counts
    array([19804, 20005, 19925, 20178, 20088])
    >>> jcs.counts
    array([18922, 20521, 20980, 19826, 19751])
    >>>

    # not for testing since we get different times on different hardware
    # just included for documentation of likely speed gains
    #>>> t1 = time.time(); jc = Jenks_Caspall(x); t2 = time.time()
    #>>> t1s = time.time(); jcs = Jenks_Caspall_Sampled(x); t2s = time.time()
    #>>> t2 - t1; t2s - t1s
    #1.8292930126190186
    #0.061631917953491211

    Notes
    -----
    This is intended for large n problems. The logic is to apply
    Jenks_Caspall to a random subset of the y space and then bin the
    complete vector y on the bins obtained from the subset. This would
    trade off some "accuracy" for a gain in speed.

    """

    def __init__(self, y, k=K, pct=0.10):
        self.k = k
        n = y.size
        if pct * n > 1000:
            pct = 1000. / n
        ids = np.random.random_integers(0, n - 1, n * pct)
        yr = y[ids]
        yr[0] = max(y)  # make sure we have the upper bound
        self.original_y = y
        self.pct = pct
        self.yr = yr
        self.yr_n = yr.size
        Map_Classifier.__init__(self, yr)
        self.yb, self.counts = bin1d(y, self.bins)
        self.name = "Jenks_Caspall_Sampled"
        self.y = y
        self._summary()  # have to recalculate summary stats

    def _set_bins(self):
        jc = Jenks_Caspall(self.y, self.k)
        self.bins = jc.bins
        self.iterations = jc.iterations


class Jenks_Caspall_Forced(Map_Classifier):
    """

    Jenks Caspall  Map Classification with forced movements

    Parameters
    ----------
    y : array (n,1)
        values to classify
    k : int
        number of classes required

    Attributes
    ----------

    yb      : array (n,1)
              bin ids for observations,
    bins    : array (k,1)
              the upper bounds of each class
    k       : int
              the number of classes
    counts  : array (k,1)
              the number of observations falling in each class


    Examples
    --------
    >>> cal = load_example()
    >>> jcf = Jenks_Caspall_Forced(cal, k = 5)
    >>> jcf.k
    5
    >>> jcf.bins
    array([[  1.34000000e+00],
           [  5.90000000e+00],
           [  1.67000000e+01],
           [  5.06500000e+01],
           [  4.11145000e+03]])
    >>> jcf.counts
    array([12, 12, 13,  9, 12])
    >>> jcf4 = Jenks_Caspall_Forced(cal, k = 4)
    >>> jcf4.k
    4
    >>> jcf4.bins
    array([[  2.51000000e+00],
           [  8.70000000e+00],
           [  3.66800000e+01],
           [  4.11145000e+03]])
    >>> jcf4.counts
    array([15, 14, 14, 15])
    >>>
    """
    def __init__(self, y, k=K):
        self.k = k
        Map_Classifier.__init__(self, y)
        self.name = "Jenks_Caspall_Forced"

    def _set_bins(self):
        x = self.y.copy()
        k = self.k
        q = quantile(x, k)
        solving = True
        xb, cnt = bin1d(x, q)
        #class means
        if x.ndim == 1:
            x.shape = (x.size, 1)
        n, tmp = x.shape
        xm = [x[xb == i].mean() for i in np.unique(xb)]
        xb0 = xb.copy()
        q = xm
        xbar = np.array([xm[xbi] for xbi in xb])
        xbar.shape = (n, 1)
        ss = x - xbar
        ss *= ss
        ss = sum(ss)
        maxk = k - 1
        down_moves = up_moves = 0
        solving = True
        it = 0
        while solving:
            # try upward moves first
            moving_up = True
            while moving_up:
                class_ids = sp.unique(xb)
                nk = [sum(xb == j) for j in class_ids]
                candidates = nk[:-1]
                i = 0
                up_moves = 0
                while candidates:
                    nki = candidates.pop(0)
                    if nki > 1:
                        ids = np.nonzero(xb == class_ids[i])
                        mover = max(ids[0])
                        tmp = xb.copy()
                        tmp[mover] = xb[mover] + 1
                        tm = [x[tmp == j].mean() for j in sp.unique(tmp)]
                        txbar = np.array([tm[xbi] for xbi in tmp])
                        txbar.shape = (n, 1)
                        tss = x - txbar
                        tss *= tss
                        tss = sum(tss)
                        if tss < ss:
                            xb = tmp
                            ss = tss
                            candidates = []
                            up_moves += 1
                    i += 1
                if not up_moves:
                    moving_up = False
            moving_down = True
            while moving_down:
                class_ids = sp.unique(xb)
                nk = [sum(xb == j) for j in class_ids]
                candidates = nk[1:]
                i = 1
                down_moves = 0
                while candidates:
                    nki = candidates.pop(0)
                    if nki > 1:
                        ids = np.nonzero(xb == class_ids[i])
                        mover = min(ids[0])
                        mover_class = xb[mover]
                        target_class = mover_class - 1
                        tmp = xb.copy()
                        tmp[mover] = target_class
                        tm = [x[tmp == j].mean() for j in sp.unique(tmp)]
                        txbar = np.array([tm[xbi] for xbi in tmp])
                        txbar.shape = (n, 1)
                        tss = x - txbar
                        tss *= tss
                        tss = sum(tss)
                        if tss < ss:
                            xb = tmp
                            ss = tss
                            candidates = []
                            down_moves += 1
                    i += 1
                if not down_moves:
                    moving_down = False
            if not up_moves and not down_moves:
                solving = False
            it += 1
        cuts = [max(x[xb == i]) for i in sp.unique(xb)]
        self.bins = np.array(cuts)
        self.iterations = it


class User_Defined(Map_Classifier):
    """
    User Specified Binning


    Parameters
    ----------
    y    : array (n,1)
           values to classify
    bins : array (k,1)
           upper bounds of classes (have to be monotically increasing)

    Attributes
    ----------

    yb      : array (n,1)
              bin ids for observations,
    bins    : array (k,1)
              the upper bounds of each class
    k       : int
              the number of classes
    counts  : array (k,1)
              the number of observations falling in each class


    Examples
    --------
    >>> cal = load_example()
    >>> bins = [20, max(cal)]
    >>> bins
    [20, 4111.4499999999998]
    >>> ud = User_Defined(cal, bins)
    >>> ud.bins
    array([   20.  ,  4111.45])
    >>> ud.counts
    array([37, 21])
    >>> bins = [20, 30]
    >>> ud = User_Defined(cal, bins)
    >>> ud.bins
    array([   20.  ,    30.  ,  4111.45])
    >>> ud.counts
    array([37,  4, 17])
    >>>


    Notes
    -----
    If upper bound of user bins does not exceed max(y) we append an
    additional bin.

    """

    def __init__(self, y, bins):
        if bins[-1] < max(y):
            bins.append(max(y))
        self.k = len(bins)
        self.bins = np.array(bins)
        self.y = y
        Map_Classifier.__init__(self, y)
        self.name = 'User Defined'

    def _set_bins(self):
        pass


class Max_P_Classifier(Map_Classifier):
    """
    Max_P Map Classification

    Based on Max_p regionalization algorithm

    Parameters
    ----------
    y       : array (n,1)
              values to classify
    k       : int
              number of classes required
    initial : int
              number of initial solutions to use prior to swapping

    Attributes
    ----------

    yb      : array (n,1)
              bin ids for observations,
    bins    : array (k,1)
              the upper bounds of each class
    k       : int
              the number of classes
    counts  : array (k,1)
              the number of observations falling in each class

    Examples
    --------
    >>> import pysal
    >>> cal = pysal.esda.mapclassify.load_example()
    >>> mp = pysal.Max_P_Classifier(cal)
    >>> mp.bins
    [8.6999999999999993, 16.699999999999999, 20.469999999999999, 66.260000000000005, 4111.4499999999998]
    >>> mp.counts
    array([29,  8,  1, 10, 10])

    """
    def __init__(self, y, k=K, initial=1000):
        self.k = k
        self.initial = initial
        Map_Classifier.__init__(self, y)
        self.name = "Max_P"

    def _set_bins(self):
        x = self.y.copy()
        k = self.k
        q = quantile(x, k)
        if x.ndim == 1:
            x.shape = (x.size, 1)
        n, tmp = x.shape
        x.sort(axis=0)
        # find best of initial solutions
        solution = 0
        best_tss = x.var() * x.shape[0]
        tss_all = np.zeros((self.initial, 1))
        while solution < self.initial:
            remaining = range(n)
            seeds = [np.nonzero(di == min(
                di))[0][0] for di in [np.abs(x - qi) for qi in q]]
            rseeds = np.random.permutation(range(k)).tolist()
            tmp = [remaining.remove(seed) for seed in seeds]
            self.classes = classes = []
            tmp = [classes.append([seed]) for seed in seeds]
            while rseeds:
                seed_id = rseeds.pop()
                current = classes[seed_id]
                growing = True
                while growing:
                    current = classes[seed_id]
                    low = current[0]
                    high = current[-1]
                    left = low - 1
                    right = high + 1
                    move_made = False
                    if left in remaining:
                        current.insert(0, left)
                        remaining.remove(left)
                        move_made = True
                    if right in remaining:
                        current.append(right)
                        remaining.remove(right)
                        move_made = True
                    if move_made:
                        classes[seed_id] = current
                    else:
                        growing = False
            tss = _fit(self.y, classes)
            tss_all[solution] = tss
            if tss < best_tss:
                best_solution = classes
                best_it = solution
                best_tss = tss
            solution += 1
        classes = best_solution
        self.best_it = best_it
        self.tss = best_tss
        self.a2c = a2c = {}
        self.tss_all = tss_all
        for r, cl in enumerate(classes):
            for a in cl:
                a2c[a] = r
        swapping = True
        it = 0
        while swapping:
            rseeds = np.random.permutation(range(k)).tolist()
            total_moves = 0
            while rseeds:
                id = rseeds.pop()
                growing = True
                total_moves = 0
                while growing:
                    target = classes[id]
                    left = target[0] - 1
                    right = target[-1] + 1
                    n_moves = 0
                    if left in a2c:
                        left_class = classes[a2c[left]]
                        if len(left_class) > 1:
                            a = left_class[-1]
                            if self._swap(left_class, target, a):
                                target.insert(0, a)
                                left_class.remove(a)
                                a2c[a] = id
                                n_moves += 1
                    if right in a2c:
                        right_class = classes[a2c[right]]
                        if len(right_class) > 1:
                            a = right_class[0]
                            if self._swap(right_class, target, a):
                                target.append(a)
                                right_class.remove(a)
                                n_moves += 1
                                a2c[a] = id
                    if not n_moves:
                        growing = False
                total_moves += n_moves
            if not total_moves:
                swapping = False
        xs = self.y.copy()
        xs.sort()
        self.bins = [xs[cl][-1] for cl in classes]

    def _ss(self, class_def):
        """calculates sum of squares for a class"""
        yc = self.y[class_def]
        css = yc - yc.mean()
        css *= css
        return sum(css)

    def _swap(self, class1, class2, a):
        """evaluate cost of moving a from class1 to class2"""
        ss1 = self._ss(class1)
        ss2 = self._ss(class2)
        tss1 = ss1 + ss2
        class1c = copy.copy(class1)
        class2c = copy.copy(class2)
        class1c.remove(a)
        class2c.append(a)
        ss1 = self._ss(class1c)
        ss2 = self._ss(class2c)
        tss2 = ss1 + ss2
        if tss1 < tss2:
            return False
        else:
            return True


def _fit(y, classes):
    """Calculate the total sum of squares for a vector y classified into
    classes

    Parameters
    ----------
    y : array, variable to be classified

    classes : array, integer values denoting class membership

    """
    tss = 0
    for class_def in classes:
        yc = y[class_def]
        css = yc - yc.mean()
        css *= css
        tss += sum(css)
    return tss

kmethods = {}
kmethods["Quantiles"] = Quantiles
kmethods["Fisher_Jenks"] = Fisher_Jenks
kmethods['Natural_Breaks'] = Natural_Breaks
kmethods['Maximum_Breaks'] = Maximum_Breaks


def gadf(y, method="Quantiles", maxk=15, pct=0.8):
    """
    Evaluate the Goodness of Absolute Deviation Fit of a Classifier
    Finds the minimum value of k for which gadf>pct

    Parameters
    ----------

    y      : array (nx1)
             values to be classified
    method : string
             Name of classifier ["Quantiles,"Fisher_Jenks","Maximum_Breaks",
             "Natural_Breaks"]
    maxk   : int
             maximum value of k to evaluate
    pct    : float
             The percentage of GADF to exceed

    Returns
    -------

    implicit : tuple
               first value is k, second value is instance of classifier at k,
               third is the pct obtained

    Examples
    --------
    >>> cal = load_example()
    >>> qgadf = gadf(cal)
    >>> qgadf[0]
    15
    >>> qgadf[-1]
    0.37402575909092828

    Quantiles fail to exceed 0.80 before 15 classes. If we lower the bar to
    0.2 we see quintiles as a result

    >>> qgadf2 = gadf(cal, pct = 0.2)
    >>> qgadf2[0]
    5
    >>> qgadf2[-1]
    0.21710231966462412
    >>>

    Notes
    -----

    The GADF is defined as:

        .. math::

            GADF = 1 - \sum_c \sum_{i \in c} |y_i - y_{c,med}|  / \sum_i |y_i - y_{med}|

        where :math:`y_{med}` is the global median and :math:`y_{c,med}` is
        the median for class :math:`c`.

    See Also
    --------
    K_classifiers
    """

    y = np.array(y)
    adam = (np.abs(y - np.median(y))).sum()
    for k in range(2, maxk + 1):
        cl = kmethods[method](y, k)
        gadf = 1 - cl.adcm / adam
        if gadf > pct:
            break
    return (k, cl, gadf)


class K_classifiers:
    """
    Evaluate all k-classifers and pick optimal based on k and GADF

    Parameters
    ----------
    y      : array (nx1)
             values to be classified
    pct    : float
             The percentage of GADF to exceed

    Attributes
    ----------
    best   :  instance of Map_Classifier
              the optimal classifer
    results : dictionary
              keys are classifier names, values are the Map_Classifier instances with the best pct for each classifer

    Examples
    --------

    >>> cal = load_example()
    >>> ks = K_classifiers(cal)
    >>> ks.best.name
    'Fisher_Jenks'
    >>> ks.best.k
    4
    >>> ks.best.gadf
    0.84810327199081048
    >>>

    Notes
    -----
    This can be used to suggest a classification scheme.

    See Also
    --------
    gadf

    """
    def __init__(self, y, pct=0.8):
        results = {}
        c = 0
        best = gadf(y, "Fisher_Jenks", maxk=len(y) - 1, pct=pct)
        pct0 = best[0]
        k0 = best[-1]
        keys = kmethods.keys()
        keys.remove("Fisher_Jenks")
        results["Fisher_Jenks"] = best
        for method in keys:
            results[method] = gadf(y, method, maxk=len(y) - 1, pct=pct)
            k1 = results[method][0]
            pct1 = results[method][-1]
            if (k1 < k0) or (k1 == k0 and pct0 < pct1):
                best = results[method]
                k0 = k1
                pct0 = pct1
        self.results = results
        self.best = best[1]


def fj(x, k=5):
    y = x.copy()
    y.sort()
    d = {}
    initial = opt_part(y)
    # d has key = number of groups
    # value: list of ids, list of group tss, group size
    split_id = [initial[0]]
    tss = initial[1:]  # left and right within tss
    sizes = [split_id - 1, len(y) - split_id]
    d[2] = [split_id, tss, sizes]
    return d


def opt_part(x):
    """
    Find optimal bi-partition of x values
    """

    n = len(x)
    tss = np.inf
    opt_i = -999
    for i in xrange(1, n):
        print i
        left = x[:i].var() * i
        right = x[i:].var() * (n - i)
        tss_i = left + right
        if tss_i < tss:
            opt_i = i
            tss = tss_i
            left_min = left
            right_min = right
    return (opt_i, tss, left_min, right_min)

########NEW FILE########
__FILENAME__ = mixture_smoothing
"""
Emprical Bayesian smoother using non-parametric mixture models
to specify the prior distribution of risks

This module is a python translation of mixlag function
in CAMAN R package that is originally written by Peter Schlattmann.
"""

__author__ = "Myunghwa Hwang <mhwang4@gmail.com>, Luc Anselin <luc.anselin@asu.edu>, Serge Rey <srey@asu.edu"

import numpy as np
from scipy.stats import poisson
import math
__all__ = ['NP_Mixture_Smoother']


class NP_Mixture_Smoother(object):
    """Empirical Bayesian Rate Smoother Using Mixture Prior Distributions
    It goes through 1) defining an initial set of subpopulations,
    2) VEM algorithm to determine the number of major subpopulations,
    3) EM algorithm, 4) combining simialr subpopulations, and 5) estimating
    EB rates from a mixture of prior distributions from subpopulation
    models.

    Parameters
    ----------
    e           : array (n, 1)
                  event variable measured across n spatial units
    b           : array (n, 1)
                  population at risk variable measured across n spatial units
    k           : integer
                  a seed number to specify the number of subpopulations
    acc         : float
                  convergence criterion; VEM and EM loops stop
                  when the increase of log likelihood is less than acc
    numiter     : integer
                  the maximum number of iterations for VEM and EM loops
    limit       : float
                  a parameter to cotrol the limit for combing subpopulation
                  models

    Attributes
    ----------
    e           : same as e in parameters
    b           : same as b in parameters
    n           : integer
                  the number of observations
    w           : float
                  a global weight value, 1 devided by n
    k           : integer
                  the number of subpopulations
    acc         : same as acc in parameters
    numiter     : same as numiter in parameters
    limit       : same as limit in parameters
    p           : array (k, 1)
                  the proportions of individual subpopulations
    t           : array (k, 1)
                  prior risks of individual subpopulations
    r           : array (n, 1)
                  estimated rate values
    category    : array (n, 1)
                  indices of subpopulations to which each observation belongs

    Examples
    --------

    importing pysal, numpy, and NP_Mixture_Smoother

    >>> import pysal
    >>> import numpy as np
    >>> from pysal.esda.mixture_smoothing import NP_Mixture_Smoother

    creating an arrary including event values

    >>> e = np.array([10, 5, 12, 20])

    creating an array including population-at-risk values

    >>> b = np.array([100, 150, 80, 200])

    applying non-parametric mixture smoothing to e and b

    >>> mixture = NP_Mixture_Smoother(e,b)

    extracting the smoothed rates through the property r of the NP_Mixture_Smoother instance

    >>> mixture.r
    array([ 0.10982278,  0.03445531,  0.11018404,  0.11018604])

    Checking the subpopulations to which each observation belongs

    >>> mixture.category
    array([1, 0, 1, 1])

    computing an initial set of prior distributions for the subpopulations

    >>> mixture.getSeed()
    (array([ 0.5,  0.5]), array([ 0.03333333,  0.15      ]))


    applying the mixture algorithm

    >>> mixture.mixalg()
    {'mix_den': array([ 0.,  0.,  0.,  0.]), 'gradient': array([ 0.]), 'k': 1, 'p': array([ 1.]), 'grid': array([ 11.27659574]), 'accuracy': 1.0}

    estimating empirical Bayesian smoothed rates

    >>> mixture.getRateEstimates()
    (array([ 0.0911574,  0.0911574,  0.0911574,  0.0911574]), array([1, 1, 1, 1]))

    """

    def __init__(self, e, b, k=50, acc=1.E-7, numiter=5000, limit=0.01):
        self.e = e
        self.b = b
        self.n = len(e)
        self.w = 1. / self.n
        self.k = k
        self.acc = acc
        self.numiter = numiter
        self.limit = limit
        r = self.mixalg()
        self.p = r['p']
        self.t = r['grid']
        self.r, self.category = self.getRateEstimates()

    def getSeed(self):
        self.raw_r = self.e * 1.0 / self.b
        r_max, r_min = self.raw_r.max(), self.raw_r.min()
        r_diff = r_max - r_min
        step = r_diff / (self.k - 1)
        grid = np.arange(r_min, r_max + step, step)
        p = np.ones(self.k) * 1. / self.k
        return p, grid

    def getMixedProb(self, grid):
        mix = np.zeros((self.n, self.k))
        for i in range(self.n):
            for j in range(self.k):
                mix[i, j] = poisson.pmf(self.e[i], self.b[i] * grid[j])
        return mix

    def getGradient(self, mix, p):
        mix_p = mix * p
        mix_den = mix_p.sum(axis=1)
        obs_id = mix_den > 1.E-13
        for i in range(self.k):
            mix_den_len = len(mix_den)
            if (mix_den > 1.E-13).sum() == mix_den_len:
                mix_p[:, i] = (1. / mix_den_len) * mix[:, i] / mix_den
        gradient = []
        for i in range(self.k):
            gradient.append(mix_p[:, i][obs_id].sum())
        return np.array(gradient), mix_den

    def getMaxGradient(self, gradient):
        grad_max = gradient.max()
        grad_max_inx = gradient.argmax()
        if grad_max <= 0:
            return (0, 1)
        return (grad_max, grad_max_inx)

    def getMinGradient(self, gradient, p):
        p_fil = p > 1.E-8
        grad_fil = gradient[p_fil]
        grad_min = grad_fil.min()
        grad_min_inx = np.where(p_fil)[0][grad_fil.argmin()]
        if grad_min >= 1.E+7:
            return (1.E+7, 1)
        return (grad_min, grad_min_inx)

    def getStepsize(self, mix_den, ht):
        mix_den_fil = np.fabs(mix_den) > 1.E-7
        a = ht[mix_den_fil] / mix_den[mix_den_fil]
        b = 1.0 + a
        b_fil = np.fabs(b) > 1.E-7
        w = self.w
        sl = w * ht[b_fil] / b[b_fil]
        s11 = sl.sum()
        s0 = (w * ht).sum()

        step, oldstep = 0., 0.
        for i in range(50):
            grad1, grad2 = 0., 0.
            for j in range(self.n):
                a = mix_den[j] + step * ht[j]
            if math.fabs(a) > 1.E-7:
                b = ht[j] / a
                grad1 = grad1 + w * b
                grad2 = grad2 - w * b * b
            if math.fabs(grad2) > 1.E-10:
                step = step - grad1 / grad2
            if oldstep > 1.0 and step > oldstep:
                step = 1.
                break
            if grad1 < 1.E-7:
                break
            oldstep = step
        if step > 1.0:
            return 1.0
        return step

    def vem(self, mix, p, grid):
        res = {}
        for it in range(self.numiter):
            grad, mix_den = self.getGradient(mix, p)
            grad_max, grad_max_inx = self.getMaxGradient(grad)
            grad_min, grad_min_inx = self.getMinGradient(grad, p)
            ht = (mix[:, grad_max_inx] - mix[:, grad_min_inx]
                  ) * p[grad_min_inx]
            st = self.getStepsize(mix_den, ht)
            xs = st * p[grad_min_inx]
            p[grad_min_inx] = p[grad_min_inx] - xs
            p[grad_max_inx] = p[grad_max_inx] + xs
            if (grad_max - 1.0) < self.acc or it == (self.numiter - 1):
                res = {'k': self.k, 'accuracy': grad_max - 1.0, 'p': p, 'grid': grid, 'gradient': grad, 'mix_den': mix_den}
                break
        return res

    def update(self, p, grid):
        p_inx = p > 1.E-3
        new_p = p[p_inx]
        new_grid = grid[p_inx]
        self.k = len(new_p)
        return new_p, new_grid

    def em(self, nstep, grid, p):
        l = self.k - 1
        w, n, e, b = self.w, self.n, self.e, self.b
        if self.k == 1:
            s11 = (w * b / np.ones(n)).sum()
            s12 = (w * e / np.ones(n)).sum()
            grid[l] = s11 / s12
            p[l] = 1.
            mix = self.getMixedProb(grid)
            grad, mix_den = self.getGradient(mix, p)
            grad_max, grad_max_inx = self.getMaxGradient(grad)
            return {'accuracy': math.fabs(grad_max - 1), 'k': self.k, 'p': p, 'grid': grid, 'gradient': grad, 'mix_den': mix_den}
        else:
            res = {}
            for counter in range(nstep):
                mix = self.getMixedProb(grid)
                grad, mix_den = self.getGradient(mix, p)
                p = p * grad
                su = p[:-1].sum()
                p[l] = 1. - su
                for j in range(self.k):
                    mix_den_fil = mix_den > 1.E-10
                    f_len = len(mix_den_fil)
                    s11 = (w * e[mix_den_fil] / np.ones(f_len) * mix[mix_den_fil, j] / mix_den[mix_den_fil]).sum()
                    s12 = (w * b[mix_den_fil] * (mix[mix_den_fil, j] / np.ones(f_len)) / mix_den[mix_den_fil]).sum()
                    if s12 > 1.E-12:
                        grid[j] = s11 / s12
                grad_max, grad_max_inx = self.getMaxGradient(grad)
                res = {'accuracy': math.fabs(grad_max - 1.), 'step': counter + 1, 'k': self.k, 'p': p, 'grid': grid, 'gradient': grad, 'mix_den': mix_den}
                if res['accuracy'] < self.acc and counter > 10:
                    break
        return res

    def getLikelihood(self, mix_den):
        mix_den_fil = mix_den > 0
        r = np.log(mix_den[mix_den_fil]).sum()
        return r

    def combine(self, res):
        p, grid, k = res['p'], res['grid'], self.k
        diff = np.fabs(grid[:-1] - grid[1:])
        bp_seeds = (diff >= self.limit).nonzero()[0] + 1
        if k - len(bp_seeds) > 1:
            bp = [0]
            if len(bp_seeds) == 1:
                bp.append(bp_seeds[0])
                bp.append(k - 1)
            else:
                if bp_seeds[1] - bp_seeds[0] > 1:
                    bp.append(bp_seeds[0])
                for i in range(1, len(bp_seeds)):
                    if bp_seeds[i] - bp_seeds[i - 1] > 1:
                        bp.append(a[i])
            new_grid, new_p = [], []
            for i in range(len(bp) - 1):
                new_grid.append(grid[bp[i]])
                new_p.append(p[bp[i]:bp[i + 1]].sum())
            self.k = new_k = len(new_p)
            new_grid, new_p = np.array(new_grid), np.array(new_p)
            mix = self.getMixedProb(new_grid)
            grad, mix_den = self.getGradient(mix, new_p)
            res = self.em(1, new_grid, new_p)
            if res is not None:
                res['likelihood'] = self.getLikelihood(mix_den)
        return res

    def mixalg(self):
        e, b, k, n = self.e, self.b, self.k, self.n
        p, grid = self.getSeed()
        mix = self.getMixedProb(grid)
        vem_res = self.vem(mix, p, grid)
        p, grid, k = vem_res['p'], vem_res['grid'], vem_res['k']
        n_p, n_g = self.update(p, grid)
        em_res = self.em(self.numiter, n_g, n_p)
        com_res = self.combine(em_res)
        return com_res

    def getRateEstimates(self):
        mix = self.getMixedProb(self.t)
        mix_p = mix * self.p
        denom = mix_p.sum(axis=1)
        categ = (mix_p / denom.reshape((self.n, 1))).argmax(axis=1)
        r = (self.t * mix_p).sum(axis=1) / denom
        return r, categ


########NEW FILE########
__FILENAME__ = moran
"""
Moran's I Spatial Autocorrelation Statistics

"""
__author__ = "Sergio J. Rey <srey@asu.edu>"
from pysal.weights.spatial_lag import lag_spatial as slag
from pysal.esda.smoothing import assuncao_rate
import scipy.stats as stats
import numpy as np

__all__ = ["Moran", "Moran_Local", "Moran_BV", "Moran_BV_matrix",
           "Moran_Rate", "Moran_Local_Rate"]


PERMUTATIONS = 999


class Moran:
    """Moran's I Global Autocorrelation Statistic

    Parameters
    ----------

    y               : array
                      variable measured across n spatial units
    w               : W
                      spatial weights instance
    transformation  : string
                      weights transformation,  default is row-standardized "r".
                      Other options include "B": binary,  "D":
                      doubly-standardized,  "U": untransformed
                      (general weights), "V": variance-stabilizing.
    permutations    : int
                      number of random permutations for calculation of
                      pseudo-p_values
    two_tailed      : boolean
                      If True (default) analytical p-values for Moran are two
                      tailed, otherwise if False, they are one-tailed.

    Attributes
    ----------
    y            : array
                   original variable
    w            : W
                   original w object
    permutations : int
                   number of permutations
    I            : float
                   value of Moran's I
    EI           : float
                   expected value under normality assumption
    VI_norm      : float
                   variance of I under normality assumption
    seI_norm     : float
                   standard deviation of I under normality assumption
    z_norm       : float
                   z-value of I under normality assumption
    p_norm       : float
                   p-value of I under normality assumption 
    VI_rand      : float
                   variance of I under randomization assumption
    seI_rand     : float
                   standard deviation of I under randomization assumption
    z_rand       : float
                   z-value of I under randomization assumption
    p_rand       : float
                   p-value of I under randomization assumption 
    two_tailed   : Boolean
                   If True p_norm and p_rand are two-tailed, otherwise they
                   are one-tailed.
    sim          : array (if permutations>0)
                   vector of I values for permuted samples
    p_sim        : array (if permutations>0)
                   p-value based on permutations (one-tailed)
                   null: spatial randomness
                   alternative: the observed I is extreme if
                   it is either extremely greater or extremely lower
                   than the values obtained based on permutations
    EI_sim       : float (if permutations>0)
                   average value of I from permutations
    VI_sim       : float (if permutations>0)
                   variance of I from permutations
    seI_sim      : float (if permutations>0)
                   standard deviation of I under permutations.
    z_sim        : float (if permutations>0)
                   standardized I based on permutations
    p_z_sim      : float (if permutations>0)
                   p-value based on standard normal approximation from
                   permutations

    Examples
    --------
    >>> import pysal
    >>> w = pysal.open(pysal.examples.get_path("stl.gal")).read()
    >>> f = pysal.open(pysal.examples.get_path("stl_hom.txt"))
    >>> y = np.array(f.by_col['HR8893'])
    >>> mi = Moran(y,  w)
    >>> "%7.5f" % mi.I
    '0.24366'
    >>> mi.EI
    -0.012987012987012988
    >>> mi.p_norm
    0.00027147862770937614

    SIDS example replicating OpenGeoda

    >>> w = pysal.open(pysal.examples.get_path("sids2.gal")).read()
    >>> f = pysal.open(pysal.examples.get_path("sids2.dbf"))
    >>> SIDR = np.array(f.by_col("SIDR74"))
    >>> mi = pysal.Moran(SIDR,  w)
    >>> "%6.4f" % mi.I
    '0.2477'
    >>> mi.p_norm
    0.0001158330781489969

    One-tailed

    >>> mi_1 = pysal.Moran(SIDR,  w, two_tailed=False)
    >>> "%6.4f" % mi_1.I
    '0.2477'
    >>> mi_1.p_norm
    5.7916539074498452e-05


    5.7916539074498452e-05
    """
    def __init__(self, y, w, transformation="r", permutations=PERMUTATIONS,
        two_tailed=True):
        self.y = y
        w.transform = transformation
        self.w = w
        self.permutations = permutations
        self.__moments()
        self.I = self.__calc(self.z)
        self.z_norm = (self.I - self.EI) / self.seI_norm
        self.z_rand = (self.I - self.EI) / self.seI_rand

        if self.z_norm > 0:
            self.p_norm = 1 - stats.norm.cdf(self.z_norm)
            self.p_rand = 1 - stats.norm.cdf(self.z_rand)
        else:
            self.p_norm = stats.norm.cdf(self.z_norm)
            self.p_rand = stats.norm.cdf(self.z_rand)

        if two_tailed:
            self.p_norm *= 2.
            self.p_rand *= 2.


        if permutations:
            sim = [self.__calc(np.random.permutation(self.z))
                   for i in xrange(permutations)]
            self.sim = sim = np.array(sim)
            above = sim >= self.I
            larger = sum(above)
            if (self.permutations - larger) < larger:
                larger = self.permutations - larger
            self.p_sim = (larger + 1.) / (permutations + 1.)
            self.EI_sim = sum(sim) / permutations
            self.seI_sim = np.array(sim).std()
            self.VI_sim = self.seI_sim ** 2
            self.z_sim = (self.I - self.EI_sim) / self.seI_sim
            if self.z_sim > 0:
                self.p_z_sim = 1 - stats.norm.cdf(self.z_sim)
            else:
                self.p_z_sim = stats.norm.cdf(self.z_sim)

    def __moments(self):
        self.n = len(self.y)
        y = self.y
        #z = (y-y.mean())/y.std()
        z = y - y.mean()
        self.z = z
        self.z2ss = sum(z * z)
        self.EI = -1. / (self.n - 1)
        n = self.n
        s1 = self.w.s1
        s0 = self.w.s0
        s2 = self.w.s2
        s02 = s0 * s0
        v_num = n * n * s1 - n * s2 + 3 * s0 * s0
        v_den = (n - 1) * (n + 1) * s0 * s0
        self.VI_norm = v_num / v_den - (1.0 / (n - 1)) ** 2
        self.seI_norm = self.VI_norm ** (1 / 2.)

        k = (1 / (sum(z ** 4)) * ((sum(z ** 2)) ** 2))
        vi = (1 / (((n - 1) ** 3) * s02)) * ((n * ((n * n - 3 * n + 3)
                                                   * s1 - n * s2 + 3 * s02))
                                             - (k * ((n * n - n) * s1 - 2 * n *
                                                     s2 + 6 * s02)))
        self.VI_rand = vi
        self.seI_rand = vi ** (1 / 2.)

    def __calc(self, z):
        zl = slag(self.w, z)
        inum = sum(z * zl)
        return self.n / self.w.s0 * inum / self.z2ss


class Moran_BV:
    """
    Bivariate Moran's I

    Parameters
    ----------
    x : array
        x-axis variable
    y : array
        (wy will be on y axis)
    w : W
        weight instance assumed to be aligned with y
    transformation  : string
                      weights transformation, default is row-standardized "r".
                      Other options include
                      "B": binary,
                      "D": doubly-standardized,
                      "U": untransformed (general weights),
                      "V": variance-stabilizing.
    permutations    : int
                      number of random permutations for calculation of pseudo
                      p_values

    Attributes
    ----------
    zx            : array
                    original x variable standardized by mean and std
    zy            : array
                    original y variable standardized by mean and std
    w             : W
                    original w object
    permutation   : int
                    number of permutations
    I             : float
                    value of bivariate Moran's I
    sim           : array (if permutations>0)
                    vector of I values for permuted samples
    p_sim         : float (if permutations>0)
                    p-value based on permutations (one-sided)
                    null: spatial randomness
                    alternative: the observed I is extreme
                    it is either extremely high or extremely low
    EI_sim        : array (if permutations>0)
                    average value of I from permutations
    VI_sim        : array (if permutations>0)
                    variance of I from permutations
    seI_sim       : array (if permutations>0)
                    standard deviation of I under permutations.
    z_sim         : array (if permutations>0)
                    standardized I based on permutations
    p_z_sim       : float  (if permutations>0)
                    p-value based on standard normal approximation from
                    permutations

    Notes
    -----

    Inference is only based on permutations as analytical results are none too
    reliable.

    Examples
    --------
    >>> import pysal
    >>> import numpy as np

    Set random number generator seed so we can replicate the example

    >>> np.random.seed(10)

    Open the sudden infant death dbf file and read in rates for 74 and 79
    converting each to a numpy array

    >>> f = pysal.open(pysal.examples.get_path("sids2.dbf"))
    >>> SIDR74 = np.array(f.by_col['SIDR74'])
    >>> SIDR79 = np.array(f.by_col['SIDR79'])

    Read a GAL file and construct our spatial weights object

    >>> w = pysal.open(pysal.examples.get_path("sids2.gal")).read()

    Create an instance of Moran_BV

    >>> mbi = Moran_BV(SIDR79,  SIDR74,  w)

    What is the bivariate Moran's I value

    >>> print mbi.I
    0.156131961696

    Based on 999 permutations, what is the p-value of our statistic

    >>> mbi.p_z_sim
    0.0014186617421765302


    """
    def __init__(self, x, y, w, transformation="r", permutations=PERMUTATIONS):
        zy = (y - y.mean()) / y.std(ddof=1)
        zx = (x - x.mean()) / x.std(ddof=1)
        self.zx = zx
        self.zy = zy
        w.transform = transformation
        self.w = w
        self.I = self.__calc(zy)
        if permutations:
            nrp = np.random.permutation
            sim = [self.__calc(nrp(zy)) for i in xrange(permutations)]
            self.sim = sim = np.array(sim)
            above = sim >= self.I
            larger = sum(above)
            if (permutations - larger) < larger:
                larger = permutations - larger
            self.p_sim = (larger + 1.) / (permutations + 1.)
            self.EI_sim = sum(sim) / permutations
            self.seI_sim = np.array(sim).std()
            self.VI_sim = self.seI_sim ** 2
            self.z_sim = (self.I - self.EI_sim) / self.seI_sim
            if self.z_sim > 0:
                self.p_z_sim = 1 - stats.norm.cdf(self.z_sim)
            else:
                self.p_z_sim = stats.norm.cdf(self.z_sim)

    def __calc(self, zy):
        wzy = slag(self.w, zy)
        self.num = sum(self.zx * wzy)
        self.den = sum(zy * zy)
        return self.num / self.den


def Moran_BV_matrix(variables, w, permutations=0, varnames=None):
    """Bivariate Moran Matrix

    Calculates bivariate Moran between all pairs of a set of variables.

    Parameters
    ----------
    variables    : list
                   sequence of variables
    w            : W
                   a spatial weights object
    permutations : int
                   number of permutations
    varnames     : list
                   strings for variable names. If specified runtime summary is
                   printed

    Returns
    -------
    results      : dictionary
                   (i,  j) is the key for the pair of variables, values are
                   the Moran_BV objects.

    Examples
    --------
    >>> import pysal

    open dbf

    >>> f = pysal.open(pysal.examples.get_path("sids2.dbf"))

    pull of selected variables from dbf and create numpy arrays for each

    >>> varnames = ['SIDR74',  'SIDR79',  'NWR74',  'NWR79']
    >>> vars = [np.array(f.by_col[var]) for var in varnames]

    create a contiguity matrix from an external gal file

    >>> w = pysal.open(pysal.examples.get_path("sids2.gal")).read()

    create an instance of Moran_BV_matrix

    >>> res = Moran_BV_matrix(vars,  w,  varnames = varnames)

    check values

    >>> print round(res[(0,  1)].I,7)
    0.1936261
    >>> print round(res[(3,  0)].I,7)
    0.3770138


    """

    k = len(variables)
    rk = range(0, k - 1)
    results = {}
    for i in rk:
        for j in range(i + 1, k):
            y1 = variables[i]
            y2 = variables[j]
            results[i, j] = Moran_BV(y1, y2, w, permutations=permutations)
            results[j, i] = Moran_BV(y2, y1, w, permutations=permutations)
    return results


class Moran_Rate(Moran):
    """
    Adjusted Moran's I Global Autocorrelation Statistic for Rate Variables

    Parameters
    ----------

    e               : array
                      an event variable measured across n spatial units
    b               : array
                      a population-at-risk variable measured across n spatial
                      units
    w               : W
                      spatial weights instance
    adjusted        : boolean
                      whether or not Moran's I needs to be adjusted for rate
                      variable
    transformation  : string
                      weights transformation, default is row-standardized "r".
                      Other options include
                      "B": binary,
                      "D": doubly-standardized,
                      "U": untransformed (general weights),
                      "V": variance-stabilizing.
    two_tailed      : Boolean
                      If True (default), analytical p-values for Moran's I are
                      two-tailed, otherwise they are one tailed.
    permutations    : int
                      number of random permutations for calculation of pseudo
                      p_values

    Attributes
    ----------
    y            : array
                   rate variable computed from parameters e and b
                   if adjusted is True, y is standardized rates
                   otherwise, y is raw rates
    w            : W
                   original w object
    permutations : int
                   number of permutations
    I            : float
                   value of Moran's I
    EI           : float
                   expected value under normality assumption
    VI_norm      : float
                   variance of I under normality assumption
    seI_norm     : float
                   standard deviation of I under normality assumption
    z_norm       : float
                   z-value of I under normality assumption
    p_norm       : float
                   p-value of I under normality assumption 
    VI_rand      : float
                   variance of I under randomization assumption
    seI_rand     : float
                   standard deviation of I under randomization assumption
    z_rand       : float
                   z-value of I under randomization assumption
    p_rand       : float
                   p-value of I under randomization assumption
    two_tailed   : Boolean
                   If True, p_norm and p_rand are two-tailed p-values,
                   otherwise they are one-tailed.
    sim          : array (if permutations>0)
                   vector of I values for permuted samples
    p_sim        : array (if permutations>0)
                   p-value based on permutations (one-sided)
                   null: spatial randomness
                   alternative: the observed I is extreme if it is
                   either extremely greater or extremely lower than the values
                   obtained from permutaitons
    EI_sim       : float (if permutations>0)
                   average value of I from permutations
    VI_sim       : float (if permutations>0)
                   variance of I from permutations
    seI_sim      : float (if permutations>0)
                   standard deviation of I under permutations.
    z_sim        : float (if permutations>0)
                   standardized I based on permutations
    p_z_sim      : float (if permutations>0)
                   p-value based on standard normal approximation from

    References
    ----------
    Assuncao, R. E. and Reis, E. A. 1999. A new proposal to adjust Moran's I
    for population density. Statistics in Medicine. 18, 2147-2162

    Examples
    --------
    >>> import pysal
    >>> w = pysal.open(pysal.examples.get_path("sids2.gal")).read()
    >>> f = pysal.open(pysal.examples.get_path("sids2.dbf"))
    >>> e = np.array(f.by_col('SID79'))
    >>> b = np.array(f.by_col('BIR79'))
    >>> mi = pysal.esda.moran.Moran_Rate(e, b,  w, two_tailed=False)
    >>> "%6.4f" % mi.I
    '0.1662'
    >>> "%6.4f" % mi.p_norm
    '0.0042'
    """

    def __init__(self, e, b, w, adjusted=True, transformation="r",
                 permutations=PERMUTATIONS, two_tailed=True):
        if adjusted:
            y = assuncao_rate(e, b)
        else:
            y = e * 1.0 / b
        Moran.__init__(self, y, w, transformation=transformation,
                       permutations=permutations, two_tailed=two_tailed)


class Moran_Local:
    """Local Moran Statistics


    Parameters
    ----------
    y : n*1 array

    w : weight instance assumed to be aligned with y

    transformation : string
                     weights transformation,  default is row-standardized "r".
                     Other options include
                     "B": binary,
                     "D": doubly-standardized,
                     "U": untransformed (general weights),
                     "V": variance-stabilizing.

    permutations   : number of random permutations for calculation of pseudo
                     p_values

    Attributes
    ----------

    y            : array
                   original variable
    w            : W
                   original w object
    permutations : int
                   number of random permutations for calculation of pseudo
                   p_values
    Is           : float
                   value of Moran's I
    q            : array (if permutations>0)
                   values indicate quadrat location 1 HH,  2 LH,  3 LL,  4 HL
    sim          : array (if permutations>0)
                   vector of I values for permuted samples
    p_sim        : array (if permutations>0)
                   p-value based on permutations (one-sided)
                   null: spatial randomness
                   alternative: the observed Ii is further away or extreme
                   from the median of simulated values. It is either extremelyi
                   high or extremely low in the distribution of simulated Is.
    EI_sim       : float (if permutations>0)
                   average value of I from permutations
    VI_sim       : float (if permutations>0)
                   variance of I from permutations
    seI_sim      : float (if permutations>0)
                   standard deviation of I under permutations.
    z_sim        : float (if permutations>0)
                   standardized I based on permutations
    p_z_sim      : float (if permutations>0)
                   p-value based on standard normal approximation from
                   permutations (one-sided)
                   for two-sided tests, these values should be multiplied by 2

    Examples
    --------
    >>> import pysal
    >>> import numpy as np
    >>> np.random.seed(10)
    >>> w = pysal.open(pysal.examples.get_path("desmith.gal")).read()
    >>> f = pysal.open(pysal.examples.get_path("desmith.txt"))
    >>> y = np.array(f.by_col['z'])
    >>> lm = Moran_Local(y, w, transformation = "r", permutations = 99)
    >>> lm.q
    array([4, 4, 4, 2, 3, 3, 1, 4, 3, 3])
    >>> lm.p_z_sim[0]
    0.46756830387716064

    Note random components result is slightly different values across
    architectures so the results have been removed from doctests and will be
    moved into unittests that are conditional on architectures
    """
    def __init__(self, y, w, transformation="r", permutations=PERMUTATIONS):
        self.y = y
        n = len(y)
        self.n = n
        self.n_1 = n - 1
        z = y - y.mean()
        # setting for floating point noise
        orig_settings = np.seterr()
        np.seterr(all="ignore")
        sy = y.std()
        z /= sy
        np.seterr(**orig_settings)
        self.z = z
        w.transform = transformation
        self.w = w
        self.permutations = permutations
        self.den = sum(z * z)
        self.Is = self.calc(self.w, self.z)
        self.__quads()
        if permutations:
            self.__crand()
            sim = np.transpose(self.rlisas)
            above = sim >= self.Is
            larger = np.sum(above, axis=0)
            low_extreme = (self.permutations - larger) < larger
            larger[low_extreme] = self.permutations - larger[low_extreme]
            self.p_sim = (larger + 1.0) / (permutations + 1.0)
            self.sim = sim
            self.EI_sim = sim.mean()
            self.seI_sim = sim.std()
            self.VI_sim = self.seI_sim * self.seI_sim
            self.z_sim = (self.Is - self.EI_sim) / self.seI_sim
            self.p_z_sim = 1 - stats.norm.cdf(np.abs(self.z_sim))

    def calc(self, w, z):
        zl = slag(w, z)
        return self.n_1 * self.z * zl / self.den

    def __crand(self):
        """
        conditional randomization

        for observation i with ni neighbors,  the candidate set cannot include
        i (we don't want i being a neighbor of i). we have to sample without
        replacement from a set of ids that doesn't include i. numpy doesn't
        directly support sampling wo replacement and it is expensive to
        implement this. instead we omit i from the original ids,  permutate the
        ids and take the first ni elements of the permuted ids as the
        neighbors to i in each randomization.

        """
        z = self.z
        lisas = np.zeros((self.n, self.permutations))
        n_1 = self.n - 1
        prange = range(self.permutations)
        k = self.w.max_neighbors + 1
        nn = self.n - 1
        rids = np.array([np.random.permutation(nn)[0:k] for i in prange])
        ids = np.arange(self.w.n)
        ido = self.w.id_order
        w = [self.w.weights[ido[i]] for i in ids]
        wc = [self.w.cardinalities[ido[i]] for i in ids]

        for i in xrange(self.w.n):
            idsi = ids[ids != i]
            np.random.shuffle(idsi)
            tmp = z[idsi[rids[:, 0:wc[i]]]]
            lisas[i] = z[i] * (w[i] * tmp).sum(1)
        self.rlisas = (n_1 / self.den) * lisas

    def __quads(self):
        zl = slag(self.w, self.z)
        zp = self.z > 0
        lp = zl > 0
        pp = zp * lp
        np = (1 - zp) * lp
        nn = (1 - zp) * (1 - lp)
        pn = zp * (1 - lp)
        self.q = 1 * pp + 2 * np + 3 * nn + 4 * pn


class Moran_Local_Rate(Moran_Local):
    """
    Adjusted Local Moran Statistics for Rate Variables

    Parameters
    ----------
    e : n*1 array
        an event variable across n spatial units
    b : n*1 array
        a population-at-risk variable across n spatial units
    w : weight instance assumed to be aligned with y
    adjusted: boolean
              whether or not local Moran statistics need to be adjusted for
              rate variable
    transformation : string
                     weights transformation,  default is row-standardized "r".
                     Other options include
                     "B": binary,
                     "D": doubly-standardized,
                     "U": untransformed (general weights),
                     "V": variance-stabilizing.
    permutations   : number of random permutations for calculation of pseudo
                     p_values

    Attributes
    ----------
    y            : array
                   rate variables computed from parameters e and b
                   if adjusted is True, y is standardized rates
                   otherwise, y is raw rates
    w            : W
                   original w object
    permutations : int
                   number of random permutations for calculation of pseudo
                   p_values
    I            : float
                   value of Moran's I
    q            : array (if permutations>0)
                   values indicate quadrat location 1 HH,  2 LH,  3 LL,  4 HL
    sim          : array (if permutations>0)
                   vector of I values for permuted samples
    p_sim        : array (if permutations>0)
                   p-value based on permutations (one-sided)
                   null: spatial randomness
                   alternative: the observed Ii is further away or extreme
                   from the median of simulated Iis. It is either extremely
                   high or extremely low in the distribution of simulated Is
    EI_sim       : float (if permutations>0)
                   average value of I from permutations
    VI_sim       : float (if permutations>0)
                   variance of I from permutations
    seI_sim      : float (if permutations>0)
                   standard deviation of I under permutations.
    z_sim        : float (if permutations>0)
                   standardized I based on permutations
    p_z_sim      : float (if permutations>0)
                   p-value based on standard normal approximation from
                   permutations (one-sided)
                   for two-sided tests, these values should be multiplied by 2

    References
    ----------
    Assuncao, R. E. and Reis, E. A. 1999. A new proposal to adjust Moran's I
    for population density. Statistics in Medicine. 18, 2147-2162

    Examples
    --------
    >>> import pysal
    >>> import numpy as np
    >>> np.random.seed(10)
    >>> w = pysal.open(pysal.examples.get_path("sids2.gal")).read()
    >>> f = pysal.open(pysal.examples.get_path("sids2.dbf"))
    >>> e = np.array(f.by_col('SID79'))
    >>> b = np.array(f.by_col('BIR79'))
    >>> lm = pysal.esda.moran.Moran_Local_Rate(e, b, w, \
                                               transformation = "r", \
                                               permutations = 99)
    >>> lm.q[:10]
    array([2, 4, 3, 1, 2, 1, 1, 4, 2, 4])
    >>> lm.p_z_sim[0]
    0.39319552026912641

    Note random components result is slightly different values across
    architectures so the results have been removed from doctests and will be
    moved into unittests that are conditional on architectures
    """

    def __init__(self, e, b, w, adjusted=True, transformation="r",
                 permutations=PERMUTATIONS):
        if adjusted:
            y = assuncao_rate(e, b)
        else:
            y = e * 1.0 / b
        Moran_Local.__init__(self, y, w,
                             transformation=transformation,
                             permutations=permutations)


def _test():
    import doctest
    # the following line could be used to define an alternative to the
    # '<BLANKLINE>' flag
    # doctest.BLANKLINE_MARKER = 'something better than <BLANKLINE>'
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)

if __name__ == '__main__':
    _test()

########NEW FILE########
__FILENAME__ = smoothing
"""
Apply smoothing to rate computation

[Longer Description]

Author(s):
    Myunghwa Hwang mhwang4@gmail.com
    David Folch dfolch@asu.edu
    Luc Anselin luc.anselin@asu.edu
    Serge Rey srey@asu.edu

"""

__author__ = "Myunghwa Hwang <mhwang4@gmail.com>, David Folch <dfolch@asu.edu>, Luc Anselin <luc.anselin@asu.edu>, Serge Rey <srey@asu.edu"

import pysal
from pysal.weights import comb, Kernel
from pysal.cg import Point, Ray, LineSegment
from pysal.cg import get_angle_between, get_points_dist, get_segment_point_dist
from pysal.cg import get_point_at_angle_and_dist, convex_hull
from pysal.common import np, KDTree
from pysal.weights.spatial_lag import lag_spatial as slag
from scipy.stats import gamma, norm, chi2, poisson

__all__ = ['Excess_Risk', 'Empirical_Bayes', 'Spatial_Empirical_Bayes', 'Spatial_Rate', 'Kernel_Smoother', 'Age_Adjusted_Smoother', 'Disk_Smoother', 'Spatial_Median_Rate', 'Spatial_Filtering', 'Headbanging_Triples', 'Headbanging_Median_Rate', 'flatten', 'weighted_median', 'sum_by_n', 'crude_age_standardization', 'direct_age_standardization', 'indirect_age_standardization', 'standardized_mortality_ratio', 'choynowski', 'assuncao_rate']


def flatten(l, unique=True):
    """flatten a list of lists

    Parameters
    ----------
    l          : list of lists
    unique     : boolean
                 whether or not only unique items are wanted

    Returns
    -------
               : list of single items

    Examples
    --------

    Creating a sample list whose elements are lists of integers

    >>> l = [[1, 2], [3, 4, ], [5, 6]]

    Applying flatten function

    >>> flatten(l)
    [1, 2, 3, 4, 5, 6]

    """
    l = reduce(lambda x, y: x + y, l)
    if not unique:
        return list(l)
    return list(set(l))


def weighted_median(d, w):
    """A utility function to find a median of d based on w

    Parameters
    ----------
    d          : array (n, 1)
                 variable for which median will be found
    w          : array (n, 1)
                 variable on which d's medain will be decided

    Notes
    -----
    d and w are arranged in the same order

    Returns
    -------
               : numeric
                 median of d

    Examples
    --------

    Creating an array including five integers.
    We will get the median of these integers.

    >>> d = np.array([5,4,3,1,2])

    Creating another array including weight values for the above integers.
    The median of d will be decided with a consideration to these weight
    values.

    >>> w = np.array([10, 22, 9, 2, 5])

    Applying weighted_median function

    >>> weighted_median(d, w)
    4

    """
    dtype = [('w', '%s' % w.dtype), ('v', '%s' % d.dtype)]
    d_w = np.array(zip(w, d), dtype=dtype)
    d_w.sort(order='v')
    reordered_w = d_w['w'].cumsum()
    cumsum_threshold = reordered_w[-1] * 1.0 / 2
    median_inx = (reordered_w >= cumsum_threshold).nonzero()[0][0]
    if reordered_w[median_inx] == cumsum_threshold and len(d) - 1 > median_inx:
        return np.sort(d)[median_inx:median_inx + 2].mean()
    return np.sort(d)[median_inx]


def sum_by_n(d, w, n):
    """A utility function to summarize a data array into n values
       after weighting the array with another weight array w

    Parameters
    ----------
    d          : array(t, 1)
                 numerical values
    w          : array(t, 1)
                 numerical values for weighting
    n          : integer
                 the number of groups
                 t = c*n (c is a constant)

    Returns
    -------
               : array(n, 1)
                 an array with summarized values

    Examples
    --------

    Creating an array including four integers.
    We will compute weighted means for every two elements.

    >>> d = np.array([10, 9, 20, 30])

    Here is another array with the weight values for d's elements.

    >>> w = np.array([0.5, 0.1, 0.3, 0.8])

    We specify the number of groups for which the weighted mean is computed.

    >>> n = 2

    Applying sum_by_n function

    >>> sum_by_n(d, w, n)
    array([  5.9,  30. ])

    """
    t = len(d)
    h = t / n
    d = d * w
    return np.array([sum(d[i: i + h]) for i in range(0, t, h)])


def crude_age_standardization(e, b, n):
    """A utility function to compute rate through crude age standardization

    Parameters
    ----------
    e          : array(n*h, 1)
                 event variable measured for each age group across n spatial units
    b          : array(n*h, 1)
                 population at risk variable measured for each age group across n spatial units
    n          : integer
                 the number of spatial units

    Notes
    -----
    e and b are arranged in the same order

    Returns
    -------
               : array(n, 1)
                 age standardized rate

    Examples
    --------

    Creating an array of an event variable (e.g., the number of cancer patients)
    for 2 regions in each of which 4 age groups are available.
    The first 4 values are event values for 4 age groups in the region 1,
    and the next 4 values are for 4 age groups in the region 2.

    >>> e = np.array([30, 25, 25, 15, 33, 21, 30, 20])

    Creating another array of a population-at-risk variable (e.g., total population)
    for the same two regions.
    The order for entering values is the same as the case of e.

    >>> b = np.array([100, 100, 110, 90, 100, 90, 110, 90])

    Specifying the number of regions.

    >>> n = 2

    Applying crude_age_standardization function to e and b

    >>> crude_age_standardization(e, b, n)
    array([ 0.2375    ,  0.26666667])

    """
    r = e * 1.0 / b
    b_by_n = sum_by_n(b, 1.0, n)
    age_weight = b * 1.0 / b_by_n.repeat(len(e) / n)
    return sum_by_n(r, age_weight, n)


def direct_age_standardization(e, b, s, n, alpha=0.05):
    """A utility function to compute rate through direct age standardization

    Parameters
    ----------
    e          : array(n*h, 1)
                 event variable measured for each age group across n spatial units
    b          : array(n*h, 1)
                 population at risk variable measured for each age group across n spatial units
    s          : array(n*h, 1)
                 standard population for each age group across n spatial units
    n          : integer
                 the number of spatial units
    alpha      : float
                 significance level for confidence interval

    Notes
    -----
    e, b, and s are arranged in the same order

    Returns
    -------
               : a list of n tuples; a tuple has a rate and its lower and upper limits
                 age standardized rates and confidence intervals

    Examples
    --------

    Creating an array of an event variable (e.g., the number of cancer patients)
    for 2 regions in each of which 4 age groups are available.
    The first 4 values are event values for 4 age groups in the region 1,
    and the next 4 values are for 4 age groups in the region 2.

    >>> e = np.array([30, 25, 25, 15, 33, 21, 30, 20])

    Creating another array of a population-at-risk variable (e.g., total population)
    for the same two regions.
    The order for entering values is the same as the case of e.

    >>> b = np.array([1000, 1000, 1100, 900, 1000, 900, 1100, 900])

    For direct age standardization, we also need the data for standard population.
    Standard population is a reference population-at-risk (e.g., population distribution for the U.S.)
    whose age distribution can be used as a benchmarking point for comparing age distributions
    across regions (e.g., popoulation distribution for Arizona and California).
    Another array including standard population is created.

    >>> s = np.array([1000, 900, 1000, 900, 1000, 900, 1000, 900])

    Specifying the number of regions.

    >>> n = 2

    Applying direct_age_standardization function to e and b

    >>> [i[0] for i in direct_age_standardization(e, b, s, n)]
    [0.023744019138755977, 0.026650717703349279]

    """
    age_weight = (1.0 / b) * (s * 1.0 / sum_by_n(s, 1.0, n).repeat(len(s) / n))
    adjusted_r = sum_by_n(e, age_weight, n)
    var_estimate = sum_by_n(e, np.square(age_weight), n)
    g_a = np.square(adjusted_r) / var_estimate
    g_b = var_estimate / adjusted_r
    k = [age_weight[i:i + len(b) / n].max() for i in range(0, len(b),
                                                           len(b) / n)]
    g_a_k = np.square(adjusted_r + k) / (var_estimate + np.square(k))
    g_b_k = (var_estimate + np.square(k)) / (adjusted_r + k)
    summed_b = sum_by_n(b, 1.0, n)
    res = []
    for i in range(len(adjusted_r)):
        if adjusted_r[i] == 0:
            upper = 0.5 * chi2(1 - 0.5 * alpha)
            lower = 0.0
        else:
            lower = gamma.ppf(0.5 * alpha, g_a[i], scale=g_b[i])
            upper = gamma.ppf(1 - 0.5 * alpha, g_a_k[i], scale=g_b_k[i])
        res.append((adjusted_r[i], lower, upper))
    return res


def indirect_age_standardization(e, b, s_e, s_b, n, alpha=0.05):
    """A utility function to compute rate through indirect age standardization

    Parameters
    ----------
    e          : array(n*h, 1)
                 event variable measured for each age group across n spatial units
    b          : array(n*h, 1)
                 population at risk variable measured for each age group across n spatial units
    s_e        : array(n*h, 1)
                 event variable measured for each age group across n spatial units in a standard population
    s_b        : array(n*h, 1)
                 population variable measured for each age group across n spatial units in a standard population
    n          : integer
                 the number of spatial units
    alpha      : float
                 significance level for confidence interval

    Notes
    -----
    e, b, s_e, and s_b are arranged in the same order

    Returns
    -------
               : a list of n tuples; a tuple has a rate and its lower and upper limits
                 age standardized rate

    Examples
    --------

    Creating an array of an event variable (e.g., the number of cancer patients)
    for 2 regions in each of which 4 age groups are available.
    The first 4 values are event values for 4 age groups in the region 1,
    and the next 4 values are for 4 age groups in the region 2.

    >>> e = np.array([30, 25, 25, 15, 33, 21, 30, 20])

    Creating another array of a population-at-risk variable (e.g., total population)
    for the same two regions.
    The order for entering values is the same as the case of e.

    >>> b = np.array([100, 100, 110, 90, 100, 90, 110, 90])

    For indirect age standardization, we also need the data for standard population and event.
    Standard population is a reference population-at-risk (e.g., population distribution for the U.S.)
    whose age distribution can be used as a benchmarking point for comparing age distributions
    across regions (e.g., popoulation distribution for Arizona and California).
    When the same concept is applied to the event variable,
    we call it standard event (e.g., the number of cancer patients in the U.S.).
    Two additional arrays including standard population and event are created.

    >>> s_e = np.array([100, 45, 120, 100, 50, 30, 200, 80])
    >>> s_b = np.array([1000, 900, 1000, 900, 1000, 900, 1000, 900])

    Specifying the number of regions.

    >>> n = 2

    Applying indirect_age_standardization function to e and b

    >>> [i[0] for i in indirect_age_standardization(e, b, s_e, s_b, n)]
    [0.23723821989528798, 0.2610803324099723]

    """
    smr = standardized_mortality_ratio(e, b, s_e, s_b, n)
    s_r_all = sum(s_e * 1.0) / sum(s_b * 1.0)
    adjusted_r = s_r_all * smr

    e_by_n = sum_by_n(e, 1.0, n)
    log_smr = np.log(smr)
    log_smr_sd = 1.0 / np.sqrt(e_by_n)
    norm_thres = norm.ppf(1 - 0.5 * alpha)
    log_smr_lower = log_smr - norm_thres * log_smr_sd
    log_smr_upper = log_smr + norm_thres * log_smr_sd
    smr_lower = np.exp(log_smr_lower) * s_r_all
    smr_upper = np.exp(log_smr_upper) * s_r_all
    res = zip(adjusted_r, smr_lower, smr_upper)
    return res


def standardized_mortality_ratio(e, b, s_e, s_b, n):
    """A utility function to compute standardized mortality ratio (SMR).

    Parameters
    ----------
    e          : array(n*h, 1)
                 event variable measured for each age group across n spatial units
    b          : array(n*h, 1)
                 population at risk variable measured for each age group across n spatial units
    s_e        : array(n*h, 1)
                 event variable measured for each age group across n spatial units in a standard population
    s_b        : array(n*h, 1)
                 population variable measured for each age group across n spatial units in a standard population
    n          : integer
                 the number of spatial units

    Notes
    -----
    e, b, s_e, and s_b are arranged in the same order

    Returns
    -------
               : array (nx1)

    Examples
    --------

    Creating an array of an event variable (e.g., the number of cancer patients)
    for 2 regions in each of which 4 age groups are available.
    The first 4 values are event values for 4 age groups in the region 1,
    and the next 4 values are for 4 age groups in the region 2.

    >>> e = np.array([30, 25, 25, 15, 33, 21, 30, 20])

    Creating another array of a population-at-risk variable (e.g., total population)
    for the same two regions.
    The order for entering values is the same as the case of e.

    >>> b = np.array([100, 100, 110, 90, 100, 90, 110, 90])

    To compute standardized mortality ratio (SMR),
    we need two additional arrays for standard population and event.
    Creating s_e and s_b for standard event and population, respectively.

    >>> s_e = np.array([100, 45, 120, 100, 50, 30, 200, 80])
    >>> s_b = np.array([1000, 900, 1000, 900, 1000, 900, 1000, 900])

    Specifying the number of regions.

    >>> n = 2

    Applying indirect_age_standardization function to e and b

    >>> standardized_mortality_ratio(e, b, s_e, s_b, n)
    array([ 2.48691099,  2.73684211])

    """
    s_r = s_e * 1.0 / s_b
    e_by_n = sum_by_n(e, 1.0, n)
    expected = sum_by_n(b, s_r, n)
    smr = e_by_n * 1.0 / expected
    return smr


def choynowski(e, b, n, threshold=None):
    """Choynowski map probabilities.

    Parameters
    ----------
    e          : array(n*h, 1)
                 event variable measured for each age group across n spatial units
    b          : array(n*h, 1)
                 population at risk variable measured for each age group across n spatial units
    n          : integer
                 the number of spatial units
    threshold  : float
                 Returns zero for any p-value greater than threshold

    Notes
    -----
    e and b are arranged in the same order

    Returns
    -------
               : array (nx1)

    References
    ----------
    [1] M. Choynowski. 1959. Maps based on probabilities. Journal of the
        American Statistical Association, 54, 385-388.

    Examples
    --------

    Creating an array of an event variable (e.g., the number of cancer patients)
    for 2 regions in each of which 4 age groups are available.
    The first 4 values are event values for 4 age groups in the region 1,
    and the next 4 values are for 4 age groups in the region 2.

    >>> e = np.array([30, 25, 25, 15, 33, 21, 30, 20])

    Creating another array of a population-at-risk variable (e.g., total population)
    for the same two regions.
    The order for entering values is the same as the case of e.

    >>> b = np.array([100, 100, 110, 90, 100, 90, 110, 90])

    Specifying the number of regions.

    >>> n = 2

    Applying indirect_age_standardization function to e and b

    >>> print choynowski(e, b, n)
    [ 0.30437751  0.29367033]

    """
    e_by_n = sum_by_n(e, 1.0, n)
    b_by_n = sum_by_n(b, 1.0, n)
    r_by_n = sum(e_by_n) * 1.0 / sum(b_by_n)
    expected = r_by_n * b_by_n
    p = []
    for index, i in enumerate(e_by_n):
        if i <= expected[index]:
            p.append(poisson.cdf(i, expected[index]))
        else:
            p.append(1 - poisson.cdf(i - 1, expected[index]))
    if threshold:
        p = [i if i < threshold else 0.0 for i in p]
    return np.array(p)


def assuncao_rate(e, b):
    """The standardized rates where the mean and stadard deviation used for
    the standardization are those of Empirical Bayes rate estimates
    The standardized rates resulting from this function are used to compute
    Moran's I corrected for rate variables.

    Parameters
    ----------
    e          : array(n, 1)
                 event variable measured at n spatial units
    b          : array(n, 1)
                 population at risk variable measured at n spatial units

    Notes
    -----
    e and b are arranged in the same order

    Returns
    -------
               : array (nx1)

    References
    ----------
    [1] Assuncao R. M. and Reis E. A., 1999, A new proposal to adjust Moran's I
    for population density. Statistics in Medicine, 18, 2147-2162.

    Examples
    --------

    Creating an array of an event variable (e.g., the number of cancer patients)
    for 8 regions.

    >>> e = np.array([30, 25, 25, 15, 33, 21, 30, 20])

    Creating another array of a population-at-risk variable (e.g., total population)
    for the same 8 regions.
    The order for entering values is the same as the case of e.

    >>> b = np.array([100, 100, 110, 90, 100, 90, 110, 90])

    Computing the rates

    >>> print assuncao_rate(e, b)[:4]
    [ 1.04319254 -0.04117865 -0.56539054 -1.73762547]

    """

    y = e * 1.0 / b
    e_sum, b_sum = sum(e), sum(b)
    ebi_b = e_sum * 1.0 / b_sum
    s2 = sum(b * ((y - ebi_b) ** 2)) / b_sum
    ebi_a = s2 - ebi_b / (b_sum / len(e))
    ebi_v = ebi_a + ebi_b / b
    return (y - ebi_b) / np.sqrt(ebi_v)


class Excess_Risk:
    """Excess Risk

    Parameters
    ----------
    e           : array (n, 1)
                  event variable measured across n spatial units
    b           : array (n, 1)
                  population at risk variable measured across n spatial units

    Attributes
    ----------
    r           : array (n, 1)
                  execess risk values

    Examples
    --------

    Reading data in stl_hom.csv into stl to extract values
    for event and population-at-risk variables

    >>> stl = pysal.open(pysal.examples.get_path('stl_hom.csv'), 'r')

    The 11th and 14th columns in stl_hom.csv includes the number of homocides and population.
    Creating two arrays from these columns.

    >>> stl_e, stl_b = np.array(stl[:,10]), np.array(stl[:,13])

    Creating an instance of Excess_Risk class using stl_e and stl_b

    >>> er = Excess_Risk(stl_e, stl_b)

    Extracting the excess risk values through the property r of the Excess_Risk instance, er

    >>> er.r[:10]
    array([ 0.20665681,  0.43613787,  0.42078261,  0.22066928,  0.57981596,
            0.35301709,  0.56407549,  0.17020994,  0.3052372 ,  0.25821905])

    """
    def __init__(self, e, b):
        r_mean = e.sum() * 1.0 / b.sum()
        self.r = e * 1.0 / (b * r_mean)


class Empirical_Bayes:
    """Aspatial Empirical Bayes Smoothing

    Parameters
    ----------
    e           : array (n, 1)
                  event variable measured across n spatial units
    b           : array (n, 1)
                  population at risk variable measured across n spatial units

    Attributes
    ----------
    r           : array (n, 1)
                  rate values from Empirical Bayes Smoothing

    Examples
    --------

    Reading data in stl_hom.csv into stl to extract values
    for event and population-at-risk variables

    >>> stl = pysal.open(pysal.examples.get_path('stl_hom.csv'), 'r')

    The 11th and 14th columns in stl_hom.csv includes the number of homocides and population.
    Creating two arrays from these columns.

    >>> stl_e, stl_b = np.array(stl[:,10]), np.array(stl[:,13])

    Creating an instance of Empirical_Bayes class using stl_e and stl_b

    >>> eb = Empirical_Bayes(stl_e, stl_b)

    Extracting the risk values through the property r of the Empirical_Bayes instance, eb

    >>> eb.r[:10]
    array([  2.36718950e-05,   4.54539167e-05,   4.78114019e-05,
             2.76907146e-05,   6.58989323e-05,   3.66494122e-05,
             5.79952721e-05,   2.03064590e-05,   3.31152999e-05,
             3.02748380e-05])

    """
    def __init__(self, e, b):
        e_sum, b_sum = e.sum() * 1.0, b.sum() * 1.0
        r_mean = e_sum / b_sum
        rate = e * 1.0 / b
        r_variat = rate - r_mean
        r_var_left = (b * r_variat * r_variat).sum() * 1.0 / b_sum
        r_var_right = r_mean * 1.0 / b.mean()
        r_var = r_var_left - r_var_right
        weight = r_var / (r_var + r_mean / b)
        self.r = weight * rate + (1.0 - weight) * r_mean


class Spatial_Empirical_Bayes:
    """Spatial Empirical Bayes Smoothing

    Parameters
    ----------
    e           : array (n, 1)
                  event variable measured across n spatial units
    b           : array (n, 1)
                  population at risk variable measured across n spatial units
    w           : spatial weights instance

    Attributes
    ----------
    r           : array (n, 1)
                  rate values from Empirical Bayes Smoothing

    Examples
    --------

    Reading data in stl_hom.csv into stl to extract values
    for event and population-at-risk variables

    >>> stl = pysal.open(pysal.examples.get_path('stl_hom.csv'), 'r')

    The 11th and 14th columns in stl_hom.csv includes the number of homocides and population.
    Creating two arrays from these columns.

    >>> stl_e, stl_b = np.array(stl[:,10]), np.array(stl[:,13])

    Creating a spatial weights instance by reading in stl.gal file.

    >>> stl_w = pysal.open(pysal.examples.get_path('stl.gal'), 'r').read()

    Ensuring that the elements in the spatial weights instance are ordered
    by the given sequential numbers from 1 to the number of observations in stl_hom.csv

    >>> if not stl_w.id_order_set: stl_w.id_order = range(1,len(stl) + 1)

    Creating an instance of Spatial_Empirical_Bayes class using stl_e, stl_b, and stl_w

    >>> s_eb = Spatial_Empirical_Bayes(stl_e, stl_b, stl_w)

    Extracting the risk values through the property r of s_eb

    >>> s_eb.r[:10]
    array([  4.01485749e-05,   3.62437513e-05,   4.93034844e-05,
             5.09387329e-05,   3.72735210e-05,   3.69333797e-05,
             5.40245456e-05,   2.99806055e-05,   3.73034109e-05,
             3.47270722e-05])
    """
    def __init__(self, e, b, w):
        if not w.id_order_set:
            raise ValueError("w id_order must be set to align with the order of e an b")
        r_mean = Spatial_Rate(e, b, w).r
        rate = e * 1.0 / b
        r_var_left = np.ones(len(e)) * 1.
        ngh_num = np.ones(len(e))
        bi = slag(w, b) + b
        for i, idv in enumerate(w.id_order):
            ngh = w[idv].keys() + [idv]
            nghi = [w.id2i[k] for k in ngh]
            ngh_num[i] = len(nghi)
            v = sum(np.square(rate[nghi] - r_mean[i]) * b[nghi])
            r_var_left[i] = v
        r_var_left = r_var_left / bi
        r_var_right = r_mean / (bi / ngh_num)
        r_var = r_var_left - r_var_right
        r_var[r_var < 0] = 0.0
        self.r = r_mean + (rate - r_mean) * (r_var / (r_var + (r_mean / b)))


class Spatial_Rate:
    """Spatial Rate Smoothing

    Parameters
    ----------
    e           : array (n, 1)
                  event variable measured across n spatial units
    b           : array (n, 1)
                  population at risk variable measured across n spatial units
    w           : spatial weights instance

    Attributes
    ----------
    r           : array (n, 1)
                  rate values from spatial rate smoothing

    Examples
    --------

    Reading data in stl_hom.csv into stl to extract values
    for event and population-at-risk variables

    >>> stl = pysal.open(pysal.examples.get_path('stl_hom.csv'), 'r')

    The 11th and 14th columns in stl_hom.csv includes the number of homocides and population.
    Creating two arrays from these columns.

    >>> stl_e, stl_b = np.array(stl[:,10]), np.array(stl[:,13])

    Creating a spatial weights instance by reading in stl.gal file.

    >>> stl_w = pysal.open(pysal.examples.get_path('stl.gal'), 'r').read()

    Ensuring that the elements in the spatial weights instance are ordered
    by the given sequential numbers from 1 to the number of observations in stl_hom.csv

    >>> if not stl_w.id_order_set: stl_w.id_order = range(1,len(stl) + 1)

    Creating an instance of Spatial_Rate class using stl_e, stl_b, and stl_w

    >>> sr = Spatial_Rate(stl_e,stl_b,stl_w)

    Extracting the risk values through the property r of sr

    >>> sr.r[:10]
    array([  4.59326407e-05,   3.62437513e-05,   4.98677081e-05,
             5.09387329e-05,   3.72735210e-05,   4.01073093e-05,
             3.79372794e-05,   3.27019246e-05,   4.26204928e-05,
             3.47270722e-05])
    """
    def __init__(self, e, b, w):
        if not w.id_order_set:
            raise ValueError("w id_order must be set to align with the order of e and b")
        else:
            w.transform = 'b'
            w_e, w_b = slag(w, e), slag(w, b)
            self.r = (e + w_e) / (b + w_b)
            w.transform = 'o'


class Kernel_Smoother:
    """Kernal smoothing

    Parameters
    ----------
    e           : array (n, 1)
                  event variable measured across n spatial units
    b           : array (n, 1)
                  population at risk variable measured across n spatial units
    w           : Kernel weights instance

    Attributes
    ----------
    r           : array (n, 1)
                  rate values from spatial rate smoothing

    Examples
    --------

    Creating an array including event values for 6 regions

    >>> e = np.array([10, 1, 3, 4, 2, 5])

    Creating another array including population-at-risk values for the 6 regions

    >>> b = np.array([100, 15, 20, 20, 80, 90])

    Creating a list containing geographic coordinates of the 6 regions' centroids

    >>> points=[(10, 10), (20, 10), (40, 10), (15, 20), (30, 20), (30, 30)]

    Creating a kernel-based spatial weights instance by using the above points

    >>> kw=Kernel(points)

    Ensuring that the elements in the kernel-based weights are ordered
    by the given sequential numbers from 0 to 5

    >>> if not kw.id_order_set: kw.id_order = range(0,len(points))

    Applying kernel smoothing to e and b

    >>> kr = Kernel_Smoother(e, b, kw)

    Extracting the smoothed rates through the property r of the Kernel_Smoother instance

    >>> kr.r
    array([ 0.10543301,  0.0858573 ,  0.08256196,  0.09884584,  0.04756872,
            0.04845298])
    """
    def __init__(self, e, b, w):
        if type(w) != Kernel:
            raise Error('w must be an instance of Kernel weights')
        if not w.id_order_set:
            raise ValueError("w id_order must be set to align with the order of e and b")
        else:
            w_e, w_b = slag(w, e), slag(w, b)
            self.r = w_e / w_b


class Age_Adjusted_Smoother:
    """Age-adjusted rate smoothing

    Parameters
    ----------
    e           : array (n*h, 1)
                  event variable measured for each age group across n spatial units
    b           : array (n*h, 1)
                  population at risk variable measured for each age group across n spatial units
    w           : spatial weights instance
    s           : array (n*h, 1)
                  standard population for each age group across n spatial units

    Attributes
    ----------
    r           : array (n, 1)
                  rate values from spatial rate smoothing

    Notes
    -----
    Weights used to smooth age-specific events and populations are simple binary weights

    Examples
    --------

    Creating an array including 12 values for the 6 regions with 2 age groups

    >>> e = np.array([10, 8, 1, 4, 3, 5, 4, 3, 2, 1, 5, 3])

    Creating another array including 12 population-at-risk values for the 6 regions

    >>> b = np.array([100, 90, 15, 30, 25, 20, 30, 20, 80, 80, 90, 60])

    For age adjustment, we need another array of values containing standard population
    s includes standard population data for the 6 regions

    >>> s = np.array([98, 88, 15, 29, 20, 23, 33, 25, 76, 80, 89, 66])

    Creating a list containing geographic coordinates of the 6 regions' centroids

    >>> points=[(10, 10), (20, 10), (40, 10), (15, 20), (30, 20), (30, 30)]

    Creating a kernel-based spatial weights instance by using the above points

    >>> kw=Kernel(points)

    Ensuring that the elements in the kernel-based weights are ordered
    by the given sequential numbers from 0 to 5

    >>> if not kw.id_order_set: kw.id_order = range(0,len(points))

    Applying age-adjusted smoothing to e and b

    >>> ar = Age_Adjusted_Smoother(e, b, kw, s)

    Extracting the smoothed rates through the property r of the Age_Adjusted_Smoother instance

    >>> ar.r
    array([ 0.10519625,  0.08494318,  0.06440072,  0.06898604,  0.06952076,
            0.05020968])
    """
    def __init__(self, e, b, w, s, alpha=0.05):
        t = len(e)
        h = t / w.n
        w.transform = 'b'
        e_n, b_n = [], []
        for i in range(h):
            e_n.append(slag(w, e[i::h]).tolist())
            b_n.append(slag(w, b[i::h]).tolist())
        e_n = np.array(e_n).reshape((1, t), order='F')[0]
        b_n = np.array(b_n).reshape((1, t), order='F')[0]
        r = direct_age_standardization(e_n, b_n, s, w.n, alpha=alpha)
        self.r = np.array([i[0] for i in r])
        w.transform = 'o'


class Disk_Smoother:
    """Locally weighted averages or disk smoothing

    Parameters
    ----------
    e           : array (n, 1)
                  event variable measured across n spatial units
    b           : array (n, 1)
                  population at risk variable measured across n spatial units
    w           : spatial weights matrix

    Attributes
    ----------
    r           : array (n, 1)
                  rate values from disk smoothing

    Examples
    --------

    Reading data in stl_hom.csv into stl to extract values
    for event and population-at-risk variables

    >>> stl = pysal.open(pysal.examples.get_path('stl_hom.csv'), 'r')

    The 11th and 14th columns in stl_hom.csv includes the number of homocides and population.
    Creating two arrays from these columns.

    >>> stl_e, stl_b = np.array(stl[:,10]), np.array(stl[:,13])

    Creating a spatial weights instance by reading in stl.gal file.

    >>> stl_w = pysal.open(pysal.examples.get_path('stl.gal'), 'r').read()

    Ensuring that the elements in the spatial weights instance are ordered
    by the given sequential numbers from 1 to the number of observations in stl_hom.csv

    >>> if not stl_w.id_order_set: stl_w.id_order = range(1,len(stl) + 1)

    Applying disk smoothing to stl_e and stl_b

    >>> sr = Disk_Smoother(stl_e,stl_b,stl_w)

    Extracting the risk values through the property r of s_eb

    >>> sr.r[:10]
    array([  4.56502262e-05,   3.44027685e-05,   3.38280487e-05,
             4.78530468e-05,   3.12278573e-05,   2.22596997e-05,
             2.67074856e-05,   2.36924573e-05,   3.48801587e-05,
             3.09511832e-05])
    """

    def __init__(self, e, b, w):
        if not w.id_order_set:
            raise ValueError("w id_order must be set to align with the order of e and b")
        else:
            r = e * 1.0 / b
            weight_sum = []
            for i in w.id_order:
                weight_sum.append(sum(w.weights[i]))
            self.r = slag(w, r) / np.array(weight_sum)


class Spatial_Median_Rate:
    """Spatial Median Rate Smoothing

    Parameters
    ----------
    e           : array (n, 1)
                  event variable measured across n spatial units
    b           : array (n, 1)
                  population at risk variable measured across n spatial units
    w           : spatial weights instance
    aw          : array (n, 1)
                  auxiliary weight variable measured across n spatial units
    iteration   : integer
                  the number of interations

    Attributes
    ----------
    r           : array (n, 1)
                  rate values from spatial median rate smoothing
    w           : spatial weights instance
    aw          : array (n, 1)
                  auxiliary weight variable measured across n spatial units

    Examples
    --------

    Reading data in stl_hom.csv into stl to extract values
    for event and population-at-risk variables

    >>> stl = pysal.open(pysal.examples.get_path('stl_hom.csv'), 'r')

    The 11th and 14th columns in stl_hom.csv includes the number of homocides and population.
    Creating two arrays from these columns.

    >>> stl_e, stl_b = np.array(stl[:,10]), np.array(stl[:,13])

    Creating a spatial weights instance by reading in stl.gal file.

    >>> stl_w = pysal.open(pysal.examples.get_path('stl.gal'), 'r').read()

    Ensuring that the elements in the spatial weights instance are ordered
    by the given sequential numbers from 1 to the number of observations in stl_hom.csv

    >>> if not stl_w.id_order_set: stl_w.id_order = range(1,len(stl) + 1)

    Computing spatial median rates without iteration

    >>> smr0 = Spatial_Median_Rate(stl_e,stl_b,stl_w)

    Extracting the computed rates through the property r of the Spatial_Median_Rate instance

    >>> smr0.r[:10]
    array([  3.96047383e-05,   3.55386859e-05,   3.28308921e-05,
             4.30731238e-05,   3.12453969e-05,   1.97300409e-05,
             3.10159267e-05,   2.19279204e-05,   2.93763432e-05,
             2.93763432e-05])

    Recomputing spatial median rates with 5 iterations

    >>> smr1 = Spatial_Median_Rate(stl_e,stl_b,stl_w,iteration=5)

    Extracting the computed rates through the property r of the Spatial_Median_Rate instance

    >>> smr1.r[:10]
    array([  3.11293620e-05,   2.95956330e-05,   3.11293620e-05,
             3.10159267e-05,   2.98436066e-05,   2.76406686e-05,
             3.10159267e-05,   2.94788171e-05,   2.99460806e-05,
             2.96981070e-05])

    Computing spatial median rates by using the base variable as auxilliary weights
    without iteration

    >>> smr2 = Spatial_Median_Rate(stl_e,stl_b,stl_w,aw=stl_b)

    Extracting the computed rates through the property r of the Spatial_Median_Rate instance

    >>> smr2.r[:10]
    array([  5.77412020e-05,   4.46449551e-05,   5.77412020e-05,
             5.77412020e-05,   4.46449551e-05,   3.61363528e-05,
             3.61363528e-05,   4.46449551e-05,   5.77412020e-05,
             4.03987355e-05])

    Recomputing spatial median rates by using the base variable as auxilliary weights
    with 5 iterations

    >>> smr3 = Spatial_Median_Rate(stl_e,stl_b,stl_w,aw=stl_b,iteration=5)

    Extracting the computed rates through the property r of the Spatial_Median_Rate instance

    >>> smr3.r[:10]
    array([  3.61363528e-05,   4.46449551e-05,   3.61363528e-05,
             3.61363528e-05,   4.46449551e-05,   3.61363528e-05,
             3.61363528e-05,   4.46449551e-05,   3.61363528e-05,
             4.46449551e-05])
    >>>
    """
    def __init__(self, e, b, w, aw=None, iteration=1):
        if not w.id_order_set:
            raise ValueError("w id_order must be set to align with the order of e and b")
        self.r = e * 1.0 / b
        self.aw, self.w = aw, w
        while iteration:
            self.__search_median()
            iteration -= 1

    def __search_median(self):
        r, aw, w = self.r, self.aw, self.w
        new_r = []
        if self.aw is None:
            for i, id in enumerate(w.id_order):
                r_disk = np.append(r[i], r[w.neighbor_offsets[id]])
                new_r.append(np.median(r_disk))
        else:
            for i, id in enumerate(w.id_order):
                id_d = [i] + list(w.neighbor_offsets[id])
                aw_d, r_d = aw[id_d], r[id_d]
                new_r.append(weighted_median(r_d, aw_d))
        self.r = np.array(new_r)


class Spatial_Filtering:
    """Spatial Filtering

    Parameters
    ----------
    bbox        : a list of two lists where each list is a pair of coordinates
                  a bounding box for the entire n spatial units
    data        : array (n, 2)
                  x, y coordinates
    e           : array (n, 1)
                  event variable measured across n spatial units
    b           : array (n, 1)
                  population at risk variable measured across n spatial units
    x_grid      : integer
                  the number of cells on x axis
    y_grid      : integer
                  the number of cells on y axis
    r           : float
                  fixed radius of a moving window
    pop         : integer
                  population threshold to create adaptive moving windows

    Attributes
    ----------
    grid        : array (x_grid*y_grid, 2)
                  x, y coordinates for grid points
    r           : array (x_grid*y_grid, 1)
                  rate values for grid points

    Notes
    -----
    No tool is provided to find an optimal value for r or pop.

    Examples
    --------

    Reading data in stl_hom.csv into stl to extract values
    for event and population-at-risk variables

    >>> stl = pysal.open(pysal.examples.get_path('stl_hom.csv'), 'r')

    Reading the stl data in the WKT format so that
    we can easily extract polygon centroids

    >>> fromWKT = pysal.core.util.WKTParser()
    >>> stl.cast('WKT',fromWKT)

    Extracting polygon centroids through iteration

    >>> d = np.array([i.centroid for i in stl[:,0]])

    Specifying the bounding box for the stl_hom data.
    The bbox should includes two points for the left-bottom and the right-top corners

    >>> bbox = [[-92.700676, 36.881809], [-87.916573, 40.3295669]]

    The 11th and 14th columns in stl_hom.csv includes the number of homocides and population.
    Creating two arrays from these columns.

    >>> stl_e, stl_b = np.array(stl[:,10]), np.array(stl[:,13])

    Applying spatial filtering by using a 10*10 mesh grid and a moving window
    with 2 radius

    >>> sf_0 = Spatial_Filtering(bbox,d,stl_e,stl_b,10,10,r=2)

    Extracting the resulting rates through the property r of the Spatial_Filtering instance

    >>> sf_0.r[:10]
    array([  4.23561763e-05,   4.45290850e-05,   4.56456221e-05,
             4.49133384e-05,   4.39671835e-05,   4.44903042e-05,
             4.19845497e-05,   4.11936548e-05,   3.93463504e-05,
             4.04376345e-05])

    Applying another spatial filtering by allowing the moving window to grow until
    600000 people are found in the window

    >>> sf = Spatial_Filtering(bbox,d,stl_e,stl_b,10,10,pop=600000)

    Checking the size of the reulting array including the rates

    >>> sf.r.shape
    (100,)

    Extracting the resulting rates through the property r of the Spatial_Filtering instance

    >>> sf.r[:10]
    array([  3.73728738e-05,   4.04456300e-05,   4.04456300e-05,
             3.81035327e-05,   4.54831940e-05,   4.54831940e-05,
             3.75658628e-05,   3.75658628e-05,   3.75658628e-05,
             3.75658628e-05])
    """

    def __init__(self, bbox, data, e, b, x_grid, y_grid, r=None, pop=None):
        data_tree = KDTree(data)
        x_range = bbox[1][0] - bbox[0][0]
        y_range = bbox[1][1] - bbox[0][1]
        x, y = np.mgrid[bbox[0][0]:bbox[1][0]:x_range / x_grid,
                        bbox[0][1]:bbox[1][1]:y_range / y_grid]
        self.grid = zip(x.ravel(), y.ravel())
        self.r = []
        if r is None and pop is None:
            raise ValueError("Either r or pop should not be None")
        if r is not None:
            pnts_in_disk = data_tree.query_ball_point(self.grid, r=r)
            for i in pnts_in_disk:
                r = e[i].sum() * 1.0 / b[i].sum()
                self.r.append(r)
        if pop is not None:
            half_nearest_pnts = data_tree.query(self.grid, k=len(e))[1]
            for i in half_nearest_pnts:
                e_n, b_n = e[i].cumsum(), b[i].cumsum()
                b_n_filter = b_n <= pop
                e_n_f, b_n_f = e_n[b_n_filter], b_n[b_n_filter]
                if len(e_n_f) == 0:
                    e_n_f = e_n[[0]]
                    b_n_f = b_n[[0]]
                self.r.append(e_n_f[-1] * 1.0 / b_n_f[-1])
        self.r = np.array(self.r)


class Headbanging_Triples:
    """Generate a pseudo spatial weights instance that contains headbaning triples

    Parameters
    ----------
    data        : array (n, 2)
                  numpy array of x, y coordinates
    w           : spatial weights instance
    k           : integer number of nearest neighbors
    t           : integer
                  the number of triples
    angle       : integer between 0 and 180
                  the angle criterium for a set of triples
    edgecorr    : boolean
                  whether or not correction for edge points is made

    Attributes
    ----------
    triples     : dictionary
                  key is observation record id, value is a list of lists of triple ids
    extra       : dictionary
                  key is observation record id, value is a list of the following:
                  tuple of original triple observations
                  distance between original triple observations
                  distance between an original triple observation and its extrapolated point

    Examples
    --------

    importing k-nearest neighbor weights creator

    >>> from pysal import knnW

    Reading data in stl_hom.csv into stl_db to extract values
    for event and population-at-risk variables

    >>> stl_db = pysal.open(pysal.examples.get_path('stl_hom.csv'),'r')

    Reading the stl data in the WKT format so that
    we can easily extract polygon centroids

    >>> fromWKT = pysal.core.util.WKTParser()
    >>> stl_db.cast('WKT',fromWKT)

    Extracting polygon centroids through iteration

    >>> d = np.array([i.centroid for i in stl_db[:,0]])

    Using the centroids, we create a 5-nearst neighbor weights

    >>> w = knnW(d,k=5)

    Ensuring that the elements in the spatial weights instance are ordered
    by the order of stl_db's IDs

    >>> if not w.id_order_set: w.id_order = w.id_order

    Finding headbaning triples by using 5 nearest neighbors

    >>> ht = Headbanging_Triples(d,w,k=5)

    Checking the members of triples

    >>> for k, item in ht.triples.items()[:5]: print k, item
    0 [(5, 6), (10, 6)]
    1 [(4, 7), (4, 14), (9, 7)]
    2 [(0, 8), (10, 3), (0, 6)]
    3 [(4, 2), (2, 12), (8, 4)]
    4 [(8, 1), (12, 1), (8, 9)]

    Opening sids2.shp file

    >>> sids = pysal.open(pysal.examples.get_path('sids2.shp'),'r')

    Extracting the centroids of polygons in the sids data

    >>> sids_d = np.array([i.centroid for i in sids])

    Creating a 5-nearest neighbors weights from the sids centroids

    >>> sids_w = knnW(sids_d,k=5)

    Ensuring that the members in sids_w are ordered by
    the order of sids_d's ID

    >>> if not sids_w.id_order_set: sids_w.id_order = sids_w.id_order

    Finding headbaning triples by using 5 nearest neighbors

    >>> s_ht = Headbanging_Triples(sids_d,sids_w,k=5)

    Checking the members of the found triples

    >>> for k, item in s_ht.triples.items()[:5]: print k, item
    0 [(1, 18), (1, 21), (1, 33)]
    1 [(2, 40), (2, 22), (22, 40)]
    2 [(39, 22), (1, 9), (39, 17)]
    3 [(16, 6), (19, 6), (20, 6)]
    4 [(5, 15), (27, 15), (35, 15)]

    Finding headbanging tirpes by using 5 nearest neighbors with edge correction

    >>> s_ht2 = Headbanging_Triples(sids_d,sids_w,k=5,edgecor=True)

    Checking the members of the found triples

    >>> for k, item in s_ht2.triples.items()[:5]: print k, item
    0 [(1, 18), (1, 21), (1, 33)]
    1 [(2, 40), (2, 22), (22, 40)]
    2 [(39, 22), (1, 9), (39, 17)]
    3 [(16, 6), (19, 6), (20, 6)]
    4 [(5, 15), (27, 15), (35, 15)]

    Checking the extrapolated point that is introduced into the triples
    during edge correction

    >>> extrapolated = s_ht2.extra[72]

    Checking the observation IDs constituting the extrapolated triple

    >>> extrapolated[0]
    (89, 77)

    Checking the distances between the exploated point and the observation 89 and 77

    >>> round(extrapolated[1],5), round(extrapolated[2],6)
    (0.33753, 0.302707)
    """
    def __init__(self, data, w, k=5, t=3, angle=135.0, edgecor=False):
        if k < 3:
            raise ValueError("w should be NeareastNeighbors instance & the number of neighbors should be more than 3.")
        if not w.id_order_set:
            raise ValueError("w id_order must be set to align with the order of data")
        self.triples, points = {}, {}
        for i, pnt in enumerate(data):
            ng = w.neighbor_offsets[i]
            points[(i, Point(pnt))] = dict(zip(ng, [Point(d)
                                                    for d in data[ng]]))
        for i, pnt in points.keys():
            ng = points[(i, pnt)]
            tr, tr_dis = {}, []
            for c in comb(ng.keys(), 2):
                p2, p3 = ng[c[0]], ng[c[-1]]
                ang = get_angle_between(Ray(pnt, p2), Ray(pnt, p3))
                if ang > angle or (ang < 0.0 and ang + 360 > angle):
                    tr[tuple(c)] = (p2, p3)
            if len(tr) > t:
                for c in tr.keys():
                    p2, p3 = tr[c]
                    tr_dis.append((get_segment_point_dist(
                        LineSegment(p2, p3), pnt), c))
                tr_dis = sorted(tr_dis)[:t]
                self.triples[i] = [trp for dis, trp in tr_dis]
            else:
                self.triples[i] = tr.keys()
        if edgecor:
            self.extra = {}
            ps = dict([(p, i) for i, p in points.keys()])
            chull = convex_hull(ps.keys())
            chull = [p for p in chull if len(self.triples[ps[p]]) == 0]
            for point in chull:
                key = (ps[point], point)
                ng = points[key]
                ng_dist = [(get_points_dist(point, p), p) for p in ng.values()]
                ng_dist_s = sorted(ng_dist, reverse=True)
                extra = None
                while extra is None and len(ng_dist_s) > 0:
                    p2 = ng_dist_s.pop()[-1]
                    p3s = ng.values()
                    p3s.remove(p2)
                    for p3 in p3s:
                        dist_p2_p3 = get_points_dist(p2, p3)
                        dist_p_p2 = get_points_dist(point, p2)
                        dist_p_p3 = get_points_dist(point, p3)
                        if dist_p_p2 <= dist_p_p3:
                            ray1, ray2, s_pnt, dist, c = Ray(p2, point), Ray(p2, p3), p2, dist_p_p2, (ps[p2], ps[p3])
                        else:
                            ray1, ray2, s_pnt, dist, c = Ray(p3, point), Ray(p3, p2), p3, dist_p_p3, (ps[p3], ps[p2])
                        ang = get_angle_between(ray1, ray2)
                        if ang >= 90 + angle / 2 or (ang < 0 and ang + 360 >= 90 + angle / 2):
                            ex_point = get_point_at_angle_and_dist(
                                ray1, angle, dist)
                            extra = [c, dist_p2_p3, get_points_dist(
                                s_pnt, ex_point)]
                            break
                self.triples[ps[point]].append(extra[0])
                self.extra[ps[point]] = extra


class Headbanging_Median_Rate:
    """Headbaning Median Rate Smoothing

    Parameters
    ----------
    e           : array (n, 1)
                  event variable measured across n spatial units
    b           : array (n, 1)
                  population at risk variable measured across n spatial units
    t           : Headbanging_Triples instance
    aw          : array (n, 1)
                  auxilliary weight variable measured across n spatial units
    iteration   : integer
                  the number of iterations

    Attributes
    ----------
    r           : array (n, 1)
                  rate values from headbaning median smoothing

    Examples
    --------

    importing k-nearest neighbor weights creator

    >>> from pysal import knnW

    opening the sids2 shapefile

    >>> sids = pysal.open(pysal.examples.get_path('sids2.shp'), 'r')

    extracting the centroids of polygons in the sids2 data

    >>> sids_d = np.array([i.centroid for i in sids])

    creating a 5-nearest neighbors weights from the centroids

    >>> sids_w = knnW(sids_d,k=5)

    ensuring that the members in sids_w are ordered

    >>> if not sids_w.id_order_set: sids_w.id_order = sids_w.id_order

    finding headbanging triples by using 5 neighbors

    >>> s_ht = Headbanging_Triples(sids_d,sids_w,k=5)

    reading in the sids2 data table

    >>> sids_db = pysal.open(pysal.examples.get_path('sids2.dbf'), 'r')

    extracting the 10th and 9th columns in the sids2.dbf and
    using data values as event and population-at-risk variables

    >>> s_e, s_b = np.array(sids_db[:,9]), np.array(sids_db[:,8])

    computing headbanging median rates from s_e, s_b, and s_ht

    >>> sids_hb_r = Headbanging_Median_Rate(s_e,s_b,s_ht)

    extracting the computed rates through the property r of the Headbanging_Median_Rate instance

    >>> sids_hb_r.r[:5]
    array([ 0.00075586,  0.        ,  0.0008285 ,  0.0018315 ,  0.00498891])

    recomputing headbanging median rates with 5 iterations

    >>> sids_hb_r2 = Headbanging_Median_Rate(s_e,s_b,s_ht,iteration=5)

    extracting the computed rates through the property r of the Headbanging_Median_Rate instance

    >>> sids_hb_r2.r[:5]
    array([ 0.0008285 ,  0.00084331,  0.00086896,  0.0018315 ,  0.00498891])

    recomputing headbanging median rates by considring a set of auxilliary weights

    >>> sids_hb_r3 = Headbanging_Median_Rate(s_e,s_b,s_ht,aw=s_b)

    extracting the computed rates through the property r of the Headbanging_Median_Rate instance

    >>> sids_hb_r3.r[:5]
    array([ 0.00091659,  0.        ,  0.00156838,  0.0018315 ,  0.00498891])
    """
    def __init__(self, e, b, t, aw=None, iteration=1):
        self.r = e * 1.0 / b
        self.tr, self.aw = t.triples, aw
        if hasattr(t, 'exta'):
            self.extra = t.extra
        while iteration:
            self.__search_headbanging_median()
            iteration -= 1

    def __get_screens(self, id, triples, weighted=False):
        r, tr = self.r, self.tr
        if len(triples) == 0:
            return r[id]
        if hasattr(self, 'extra') and id in self.extra:
            extra = self.extra
            trp_r = r[list(triples[0])]
            trp_r[-1] = trp_r[0] + (trp_r[0] - trp_r[-1]) * (
                extra[id][-1] * 1.0 / extra[id][1])
            trp_r = sorted(trp_r)
            if not weighted:
                return r, trp_r[0], trp_r[-1]
            else:
                trp_aw = self.aw[trp]
                extra_w = trp_aw[0] + (trp_aw[0] - trp_aw[-
                                                          1]) * (extra[id][-1] * 1.0 / extra[id][1])
                return r, trp_r[0], trp_r[-1], self.aw[id], trp_aw[0] + extra_w
        if not weighted:
            lowest, highest = [], []
            for trp in triples:
                trp_r = np.sort(r[list(trp)])
                lowest.append(trp_r[0])
                highest.append(trp_r[-1])
            return r[id], np.median(np.array(lowest)), np.median(np.array(highest))
        if weighted:
            lowest, highest = [], []
            lowest_aw, highest_aw = [], []
            for trp in triples:
                trp_r = r[list(trp)]
                dtype = [('r', '%s' % trp_r.dtype), ('w',
                                                     '%s' % self.aw.dtype)]
                trp_r = np.array(zip(trp_r, list(trp)), dtype=dtype)
                trp_r.sort(order='r')
                lowest.append(trp_r['r'][0])
                highest.append(trp_r['r'][-1])
                lowest_aw.append(self.aw[trp_r['w'][0]])
                highest_aw.append(self.aw[trp_r['w'][-1]])
            wm_lowest = weighted_median(np.array(lowest), np.array(lowest_aw))
            wm_highest = weighted_median(
                np.array(highest), np.array(highest_aw))
            triple_members = flatten(triples, unique=False)
            return r[id], wm_lowest, wm_highest, self.aw[id] * len(triples), self.aw[triple_members].sum()

    def __get_median_from_screens(self, screens):
        if isinstance(screens, float):
            return screens
        elif len(screens) == 3:
            return np.median(np.array(screens))
        elif len(screens) == 5:
            rk, wm_lowest, wm_highest, w1, w2 = screens
            if rk >= wm_lowest and rk <= wm_highest:
                return rk
            elif rk < wm_lowest and w1 < w2:
                return wm_lowest
            elif rk > wm_highest and w1 < w2:
                return wm_highest
            else:
                return rk

    def __search_headbanging_median(self):
        r, tr = self.r, self.tr
        new_r = []
        for k in tr.keys():
            screens = self.__get_screens(
                k, tr[k], weighted=(self.aw is not None))
            new_r.append(self.__get_median_from_screens(screens))
        self.r = np.array(new_r)

########NEW FILE########
__FILENAME__ = test_gamma
import unittest
import numpy as np
import pysal
from pysal.esda.gamma import Gamma


class Gamma_Tester(unittest.TestCase):
    """Unit test for Gamma Index"""
    def setUp(self):
        self.w = pysal.lat2W(4, 4)
        self.y = np.ones(16)
        self.y[0:8] = 0

    def test_Gamma(self):
        """Test method"""
        np.random.seed(12345)
        g = Gamma(self.y, self.w)
        self.assertAlmostEquals(g.g, 20.0)
        self.assertAlmostEquals(g.g_z, 3.1879280354548638)
        self.assertAlmostEquals(g.p_sim_g, 0.0030000000000000001)
        self.assertAlmostEquals(g.min_g, 0.0)
        self.assertAlmostEquals(g.max_g, 20.0)
        self.assertAlmostEquals(g.mean_g, 11.093093093093094)
        np.random.seed(12345)
        g1 = Gamma(self.y, self.w, operation='s')
        self.assertAlmostEquals(g1.g, 8.0)
        self.assertAlmostEquals(g1.g_z, -3.7057554345954791)
        self.assertAlmostEquals(g1.p_sim_g, 0.001)
        self.assertAlmostEquals(g1.min_g, 14.0)
        self.assertAlmostEquals(g1.max_g, 48.0)
        self.assertAlmostEquals(g1.mean_g, 25.623623623623622)
        np.random.seed(12345)
        g2 = Gamma(self.y, self.w, operation='a')
        self.assertAlmostEquals(g2.g, 8.0)
        self.assertAlmostEquals(g2.g_z, -3.7057554345954791)
        self.assertAlmostEquals(g2.p_sim_g, 0.001)
        self.assertAlmostEquals(g2.min_g, 14.0)
        self.assertAlmostEquals(g2.max_g, 48.0)
        self.assertAlmostEquals(g2.mean_g, 25.623623623623622)
        np.random.seed(12345)
        g3 = Gamma(self.y, self.w, standardize='y')
        self.assertAlmostEquals(g3.g, 32.0)
        self.assertAlmostEquals(g3.g_z, 3.7057554345954791)
        self.assertAlmostEquals(g3.p_sim_g, 0.001)
        self.assertAlmostEquals(g3.min_g, -48.0)
        self.assertAlmostEquals(g3.max_g, 20.0)
        self.assertAlmostEquals(g3.mean_g, -3.2472472472472473)
        np.random.seed(12345)

        def func(z, i, j):
            q = z[i] * z[j]
            return q

        g4 = Gamma(self.y, self.w, operation=func)
        self.assertAlmostEquals(g4.g, 20.0)
        self.assertAlmostEquals(g4.g_z, 3.1879280354548638)
        self.assertAlmostEquals(g4.p_sim_g, 0.0030000000000000001)


suite = unittest.TestSuite()
test_classes = [Gamma_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_geary
"""Geary Unittest."""
import unittest
import pysal
from pysal.esda import geary
import numpy as np


class Geary_Tester(unittest.TestCase):
    """Geary class for unit tests."""
    def setUp(self):
        self.w = pysal.open(pysal.examples.get_path("book.gal")).read()
        f = pysal.open(pysal.examples.get_path("book.txt"))
        self.y = np.array(f.by_col['y'])

    def test_Geary(self):
        c = geary.Geary(self.y, self.w, permutations=0)
        self.assertAlmostEquals(c.C, 0.33301083591331254)
        self.assertAlmostEquals(c.EC, 1.0)

        self.assertAlmostEquals(c.VC_norm, 0.031805300245097874)
        self.assertAlmostEquals(c.p_norm, 9.2018240680169505e-05)
        self.assertAlmostEquals(c.z_norm, -3.7399778367629564)
        self.assertAlmostEquals(c.seC_norm, 0.17834040553138225)

        self.assertAlmostEquals(c.VC_rand, 0.018437747611029367)
        self.assertAlmostEquals(c.p_rand, 4.5059156794646782e-07)
        self.assertAlmostEquals(c.z_rand, -4.9120733751216008)
        self.assertAlmostEquals(c.seC_rand, 0.13578566791465646)

        np.random.seed(12345)
        c = geary.Geary(self.y, self.w, permutations=999)
        self.assertAlmostEquals(c.C, 0.33301083591331254)
        self.assertAlmostEquals(c.EC, 1.0)

        self.assertAlmostEquals(c.VC_norm, 0.031805300245097874)
        self.assertAlmostEquals(c.p_norm, 9.2018240680169505e-05)
        self.assertAlmostEquals(c.z_norm, -3.7399778367629564)
        self.assertAlmostEquals(c.seC_norm, 0.17834040553138225)

        self.assertAlmostEquals(c.VC_rand, 0.018437747611029367)
        self.assertAlmostEquals(c.p_rand, 4.5059156794646782e-07)
        self.assertAlmostEquals(c.z_rand, -4.9120733751216008)
        self.assertAlmostEquals(c.seC_rand, 0.13578566791465646)

        self.assertAlmostEquals(c.EC_sim, 0.9980676303238214)
        self.assertAlmostEquals(c.VC_sim, 0.034430408799858946)
        self.assertAlmostEquals(c.p_sim, 0.001)
        self.assertAlmostEquals(c.p_z_sim, 0.00016908100514811952)
        self.assertAlmostEquals(c.z_sim, -3.5841621159171746)
        self.assertAlmostEquals(c.seC_sim, 0.18555432843202269)


suite = unittest.TestSuite()
test_classes = [Geary_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_getisord
import unittest
from pysal.weights.Distance import DistanceBand
from pysal.esda import getisord
import numpy as np

POINTS = [(10, 10), (20, 10), (40, 10), (15, 20), (30, 20), (30, 30)]
W = DistanceBand(POINTS, threshold=15)
Y = np.array([2, 3, 3.2, 5, 8, 7])


class G_Tester(unittest.TestCase):

    def setUp(self):
        self.w = W
        self.y = Y
        np.random.seed(10)

    def test_G(self):
        g = getisord.G(self.y, self.w)
        self.assertAlmostEquals(g.G, 0.55709779, places=8)
        self.assertAlmostEquals(g.p_norm, 0.1729, places=4)


class G_Local_Tester(unittest.TestCase):

    def setUp(self):
        self.w = W
        self.y = Y
        np.random.seed(10)

    def test_G_Local_Binary(self):
        lg = getisord.G_Local(self.y, self.w, transform='B')
        self.assertAlmostEquals(lg.Zs[0], -1.0136729, places=7)
        self.assertAlmostEquals(lg.p_sim[0], 0.10100000000000001, places=7)

    def test_G_Local_Row_Standardized(self):
        lg = getisord.G_Local(self.y, self.w, transform='R')
        self.assertAlmostEquals(lg.Zs[0], -0.62074534, places=7)
        self.assertAlmostEquals(lg.p_sim[0], 0.10100000000000001, places=7)

    def test_G_star_Local_Binary(self):
        lg = getisord.G_Local(self.y, self.w, transform='B', star=True)
        self.assertAlmostEquals(lg.Zs[0], -1.39727626, places=8)
        self.assertAlmostEquals(lg.p_sim[0], 0.10100000000000001, places=7)

    def test_G_star_Row_Standardized(self):
        lg = getisord.G_Local(self.y, self.w, transform='R', star=True)
        self.assertAlmostEquals(lg.Zs[0], -0.62488094, places=8)
        self.assertAlmostEquals(lg.p_sim[0], 0.10100000000000001, places=7)

suite = unittest.TestSuite()
test_classes = [G_Tester, G_Local_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_join_counts
import unittest
import numpy as np
import pysal
from pysal.esda.join_counts import Join_Counts


class Join_Counts_Tester(unittest.TestCase):
    """Unit test for Join Counts"""
    def setUp(self):
        self.w = pysal.lat2W(4, 4)
        self.y = np.ones(16)
        self.y[0:8] = 0

    def test_Join_Counts(self):
        """Test method"""
        np.random.seed(12345)
        jc = Join_Counts(self.y, self.w)
        self.assertAlmostEquals(jc.bb, 10.0)
        self.assertAlmostEquals(jc.bw, 4.0)
        self.assertAlmostEquals(jc.ww, 10.0)
        self.assertAlmostEquals(jc.J, 24.0)
        self.assertAlmostEquals(len(jc.sim_bb), 999)
        self.assertAlmostEquals(jc.p_sim_bb, 0.0030000000000000001)
        self.assertAlmostEquals(np.mean(jc.sim_bb), 5.5465465465465469)
        self.assertAlmostEquals(np.max(jc.sim_bb), 10.0)
        self.assertAlmostEquals(np.min(jc.sim_bb), 0.0)
        self.assertAlmostEquals(len(jc.sim_bw), 999)
        self.assertAlmostEquals(jc.p_sim_bw, 1.0)
        self.assertAlmostEquals(np.mean(jc.sim_bw), 12.811811811811811)
        self.assertAlmostEquals(np.max(jc.sim_bw), 24.0)
        self.assertAlmostEquals(np.min(jc.sim_bw), 7.0)

suite = unittest.TestSuite()
test_classes = [Join_Counts_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_mapclassify
import pysal
from pysal.esda.mapclassify import *
from pysal.esda.mapclassify import binC, bin, bin1d
import numpy as np
import unittest


class TestQuantile(unittest.TestCase):
    def test_quantile(self):
        y = np.arange(1000)
        expected = np.array([333., 666., 999.])
        np.testing.assert_almost_equal(expected, quantile(y, k=3))

    def test_quantile_k4(self):
        x = np.arange(1000)
        qx = quantile(x, k=4)
        expected = np.array([249.75, 499.5, 749.25, 999.])
        np.testing.assert_array_almost_equal(expected, qx)

    def test_quantile_k(self):
        y = np.random.random(1000)
        for k in range(5, 10):
            np.testing.assert_almost_equal(k, len(quantile(y, k)))
            self.assertEqual(k, len(quantile(y, k)))


class TestBinC(unittest.TestCase):
    def test_bin_c(self):
        bins = range(2, 8)
        y = np.array([[7, 5, 6],
                      [2, 3, 5],
                      [7, 2, 2],
                      [3, 6, 7],
                      [6, 3, 4],
                      [6, 7, 4],
                      [6, 5, 6],
                      [4, 6, 7],
                      [4, 6, 3],
                      [3, 2, 7]])

        expected = np.array([[5, 3, 4],
                             [0, 1, 3],
                             [5, 0, 0],
                             [1, 4, 5],
                             [4, 1, 2],
                             [4, 5, 2],
                             [4, 3, 4],
                             [2, 4, 5],
                             [2, 4, 1],
                             [1, 0, 5]])
        np.testing.assert_array_equal(expected, binC(y, bins))


class TestBin(unittest.TestCase):
    def test_bin(self):
        y = np.array([[7, 13, 14],
                      [10, 11, 13],
                      [7, 17, 2],
                      [18, 3, 14],
                      [9, 15, 8],
                      [7, 13, 12],
                      [16, 6, 11],
                      [19, 2, 15],
                      [11, 11, 9],
                      [3, 2, 19]])
        bins = [10, 15, 20]
        expected = np.array([[0, 1, 1],
                             [0, 1, 1],
                             [0, 2, 0],
                             [2, 0, 1],
                             [0, 1, 0],
                             [0, 1, 1],
                             [2, 0, 1],
                             [2, 0, 1],
                             [1, 1, 0],
                             [0, 0, 2]])

        np.testing.assert_array_equal(expected, bin(y, bins))


class TestBin1d(unittest.TestCase):
    def test_bin1d(self):
        y = np.arange(100, dtype='float')
        bins = [25, 74, 100]
        binIds = np.array(
            [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
             1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
             2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
        counts = np.array([26, 49, 25])

        np.testing.assert_array_equal(binIds, bin1d(y, bins)[0])
        np.testing.assert_array_equal(counts, bin1d(y, bins)[1])


class TestNaturalBreaks(unittest.TestCase):
    def setUp(self):
        dat = pysal.open(pysal.examples.get_path("calempdensity.csv"))
        self.V = np.array([record[-1] for record in dat])

    def test_natural_breaks(self):
        # self.assertEqual(expected, natural_breaks(values, k, itmax))
        assert True  # TODO: implement your test here

    def test_Natural_Breaks(self):
        nb = Natural_Breaks(self.V, 5)
        self.assertEquals(nb.k, 5)
        self.assertEquals(len(nb.counts), 5)
        np.testing.assert_array_almost_equal(
            nb.counts, np.array([14, 13, 14, 10, 7]))

    def test_Natural_Breaks_stability(self):
        for i in range(10):
            nb = Natural_Breaks(self.V, 5)
            self.assertEquals(nb.k, 5)
            self.assertEquals(len(nb.counts), 5)

    def test_Natural_Breaks_randomData(self):
        for i in range(10):
            V = np.random.random(50) * (i + 1)
            nb = Natural_Breaks(V, 5)
            self.assertEquals(nb.k, 5)
            self.assertEquals(len(nb.counts), 5)


class TestMapClassifier(unittest.TestCase):
    def test_Map_Classifier(self):
        # map__classifier = Map_Classifier(y)
        assert True  # TODO: implement your test here

    def test___repr__(self):
        # map__classifier = Map_Classifier(y)
        # self.assertEqual(expected, map__classifier.__repr__())
        assert True  # TODO: implement your test here

    def test___str__(self):
        # map__classifier = Map_Classifier(y)
        # self.assertEqual(expected, map__classifier.__str__())
        assert True  # TODO: implement your test here

    def test_get_adcm(self):
        # map__classifier = Map_Classifier(y)
        # self.assertEqual(expected, map__classifier.get_adcm())
        assert True  # TODO: implement your test here

    def test_get_gadf(self):
        # map__classifier = Map_Classifier(y)
        # self.assertEqual(expected, map__classifier.get_gadf())
        assert True  # TODO: implement your test here

    def test_get_tss(self):
        # map__classifier = Map_Classifier(y)
        # self.assertEqual(expected, map__classifier.get_tss())
        assert True  # TODO: implement your test here


class TestEqualInterval(unittest.TestCase):
    def setUp(self):
        dat = pysal.open(pysal.examples.get_path("calempdensity.csv"))
        self.V = np.array([record[-1] for record in dat])

    def test_Equal_Interval(self):
        ei = Equal_Interval(self.V)
        np.testing.assert_array_almost_equal(ei.counts,
                                             np.array([57, 0, 0, 0, 1]))
        np.testing.assert_array_almost_equal(ei.bins,
                                             np.array(
                                                 [822.394, 1644.658, 2466.922, 3289.186,
                                                  4111.45]))


class TestPercentiles(unittest.TestCase):
    def setUp(self):
        dat = pysal.open(pysal.examples.get_path("calempdensity.csv"))
        self.V = np.array([record[-1] for record in dat])

    def test_Percentiles(self):
        pc = Percentiles(self.V)
        np.testing.assert_array_almost_equal(pc.bins, np.array([
            1.35700000e-01, 5.53000000e-01, 9.36500000e+00, 2.13914000e+02,
            2.17994800e+03, 4.11145000e+03]))
        np.testing.assert_array_almost_equal(pc.counts,
                                             np.array([1, 5, 23, 23, 5, 1]))


class TestBoxPlot(unittest.TestCase):
    def setUp(self):
        dat = pysal.open(pysal.examples.get_path("calempdensity.csv"))
        self.V = np.array([record[-1] for record in dat])

    def test_Box_Plot(self):
        bp = Box_Plot(self.V)
        bins = np.array([-5.28762500e+01, 2.56750000e+00, 9.36500000e+00,
                         3.95300000e+01, 9.49737500e+01, 4.11145000e+03])
        np.testing.assert_array_almost_equal(bp.bins, bins)


class TestQuantiles(unittest.TestCase):
    def setUp(self):
        dat = pysal.open(pysal.examples.get_path("calempdensity.csv"))
        self.V = np.array([record[-1] for record in dat])

    def test_Quantiles(self):
        q = Quantiles(self.V, k=5)
        np.testing.assert_array_almost_equal(q.bins,
                                             np.array(
                                                 [1.46400000e+00, 5.79800000e+00,
                                                  1.32780000e+01, 5.46160000e+01, 4.11145000e+03]))
        np.testing.assert_array_almost_equal(q.counts,
                                             np.array([12, 11, 12, 11, 12]))


class TestStdMean(unittest.TestCase):
    def setUp(self):
        dat = pysal.open(pysal.examples.get_path("calempdensity.csv"))
        self.V = np.array([record[-1] for record in dat])

    def test_Std_Mean(self):
        s = Std_Mean(self.V)
        np.testing.assert_array_almost_equal(s.bins,
                                             np.array(
                                                 [-967.36235382, -420.71712519, 672.57333208,
                                                  1219.21856072, 4111.45]))
        np.testing.assert_array_almost_equal(s.counts,
                                             np.array([0, 0, 56, 1, 1]))


class TestMaximumBreaks(unittest.TestCase):
    def setUp(self):
        dat = pysal.open(pysal.examples.get_path("calempdensity.csv"))
        self.V = np.array([record[-1] for record in dat])

    def test_Maximum_Breaks(self):
        mb = Maximum_Breaks(self.V, k=5)
        self.assertEquals(mb.k, 5)
        np.testing.assert_array_almost_equal(mb.bins,
                                             np.array(
                                                 [146.005, 228.49, 546.675, 2417.15,
                                                  4111.45]))
        np.testing.assert_array_almost_equal(mb.counts,
                                             np.array([50, 2, 4, 1, 1]))


class TestFisherJenks(unittest.TestCase):
    def setUp(self):
        dat = pysal.open(pysal.examples.get_path("calempdensity.csv"))
        self.V = np.array([record[-1] for record in dat])

    def test_Fisher_Jenks(self):
        fj = Fisher_Jenks(self.V)
        self.assertEquals(fj.adcm, 799.24000000000001)
        self.assertEquals(fj.bins, [75.290000000000006, 192.05000000000001,
                                    370.5, 722.85000000000002, 4111.45])
        np.testing.assert_array_almost_equal(fj.counts, np.array([49, 3, 4,
                                                                  1, 1]))


class TestJenksCaspall(unittest.TestCase):
    def setUp(self):
        dat = pysal.open(pysal.examples.get_path("calempdensity.csv"))
        self.V = np.array([record[-1] for record in dat])

    def test_Jenks_Caspall(self):
        np.random.seed(10)
        jc = Jenks_Caspall(self.V, k=5)
        np.testing.assert_array_almost_equal(jc.counts,
                                             np.array([14, 13, 14, 10, 7]))
        np.testing.assert_array_almost_equal(jc.bins,
                                             np.array( [1.81000000e+00, 7.60000000e+00,
                                                  2.98200000e+01, 1.81270000e+02,
                                                  4.11145000e+03]))


class TestJenksCaspallSampled(unittest.TestCase):
    def setUp(self):
        dat = pysal.open(pysal.examples.get_path("calempdensity.csv"))
        self.V = np.array([record[-1] for record in dat])

    def test_Jenks_Caspall_Sampled(self):
        np.random.seed(100)
        x = np.random.random(100000)
        jc = Jenks_Caspall(x)
        jcs = Jenks_Caspall_Sampled(x)
        np.testing.assert_array_almost_equal(jc.bins,
                                             np.array([0.19718393,
                                                       0.39655886,
                                                       0.59648522,
                                                       0.79780763,
                                                       0.99997979]))
        np.testing.assert_array_almost_equal(jcs.bins,
                                             np.array([0.20856569,
                                                       0.41513931,
                                                       0.62457691,
                                                       0.82561423,
                                                       0.99997979]))


class TestJenksCaspallForced(unittest.TestCase):
    def setUp(self):
        dat = pysal.open(pysal.examples.get_path("calempdensity.csv"))
        self.V = np.array([record[-1] for record in dat])

    def test_Jenks_Caspall_Forced(self):
        np.random.seed(100)
        jcf = Jenks_Caspall_Forced(self.V, k=5)
        np.testing.assert_array_almost_equal(jcf.bins,
                                             np.array([[1.34000000e+00],
                                                       [5.90000000e+00],
                                                       [1.67000000e+01],
                                                       [5.06500000e+01],
                                                       [4.11145000e+03]]))
        np.testing.assert_array_almost_equal(jcf.counts,
                                             np.array([12, 12, 13, 9, 12]))


class TestUserDefined(unittest.TestCase):
    def setUp(self):
        dat = pysal.open(pysal.examples.get_path("calempdensity.csv"))
        self.V = np.array([record[-1] for record in dat])

    def test_User_Defined(self):
        bins = [20, max(self.V)]
        ud = User_Defined(self.V, bins)
        np.testing.assert_array_almost_equal(ud.bins,
                                             np.array([20., 4111.45]))
        np.testing.assert_array_almost_equal(ud.counts,
                                             np.array([37, 21]))


class TestMaxPClassifier(unittest.TestCase):
    def setUp(self):
        dat = pysal.open(pysal.examples.get_path("calempdensity.csv"))
        self.V = np.array([record[-1] for record in dat])

    def test_Max_P_Classifier(self):
        np.random.seed(100)
        mp = Max_P_Classifier(self.V)
        np.testing.assert_array_almost_equal(mp.bins,
                                             np.array(
                                                 [8.6999999999999993, 16.699999999999999,
                                                  20.469999999999999, 66.260000000000005,
                                                  4111.4499999999998]))
        np.testing.assert_array_almost_equal(mp.counts,
                                             np.array([29, 8, 1, 10, 10]))


class TestGadf(unittest.TestCase):
    def setUp(self):
        dat = pysal.open(pysal.examples.get_path("calempdensity.csv"))
        self.V = np.array([record[-1] for record in dat])

    def test_gadf(self):
        qgadf = gadf(self.V)
        self.assertEquals(qgadf[0], 15)
        self.assertEquals(qgadf[-1], 0.37402575909092828)


class TestKClassifiers(unittest.TestCase):
    def setUp(self):
        dat = pysal.open(pysal.examples.get_path("calempdensity.csv"))
        self.V = np.array([record[-1] for record in dat])

    def test_K_classifiers(self):
        np.random.seed(100)
        ks = K_classifiers(self.V)
        self.assertEquals(ks.best.name, 'Fisher_Jenks')
        self.assertEquals(ks.best.gadf, 0.84810327199081048)
        self.assertEquals(ks.best.k, 4)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_mixture_smoothing
import unittest
import numpy as np
import pysal
from pysal.esda import mixture_smoothing as m_s


class MS_Tester(unittest.TestCase):
    """Mixture_Smoothing Unit Tests"""
    def setUp(self):
        self.e = np.array([10, 5, 12, 20])
        self.b = np.array([100, 150, 80, 200])

    def test_NP_Mixture_Smoother(self):
        """Test the main class"""
        mix = m_s.NP_Mixture_Smoother(self.e, self.b)
        np.testing.assert_array_almost_equal(mix.r, np.array(
            [0.10982278, 0.03445531, 0.11018404, 0.11018604]))
        np.testing.assert_array_almost_equal(
            mix.category, np.array([1, 0, 1, 1]))
        #self.failUnless(mix.getSeed(), (np.array([ 0.5,  0.5]), np.array([ 0.03333333,
        #    0.15      ])))
        left, right = mix.getSeed()
        np.testing.assert_array_almost_equal(left, np.array([0.5, 0.5]))
        np.testing.assert_array_almost_equal(
            right, np.array([0.03333333, 0.15]))
        d = mix.mixalg()
        np.testing.assert_array_almost_equal(
            d['mix_den'], np.array([0., 0., 0., 0.]))
        np.testing.assert_array_almost_equal(d['gradient'], np.array([0.]))
        np.testing.assert_array_almost_equal(d['p'], np.array([1.]))
        np.testing.assert_array_almost_equal(
            d['grid'], np.array([11.27659574]))
        self.assertEqual(d['k'], 1)
        self.assertEqual(d['accuracy'], 1.0)
        left, right = mix.getRateEstimates()
        np.testing.assert_array_almost_equal(
            left, np.array([0.0911574, 0.0911574,
                            0.0911574, 0.0911574]))
        np.testing.assert_array_almost_equal(right, np.array([1, 1, 1, 1]))


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_moran
import unittest
import pysal
from pysal.esda import moran
import numpy as np


class Moran_Tester(unittest.TestCase):
    def setUp(self):
        self.w = pysal.open(pysal.examples.get_path("stl.gal")).read()
        f = pysal.open(pysal.examples.get_path("stl_hom.txt"))
        self.y = np.array(f.by_col['HR8893'])

    def test_moran(self):
        mi = moran.Moran(self.y, self.w, two_tailed=False)
        self.assertAlmostEquals(mi.I, 0.24365582621771659, 7)
        self.assertAlmostEquals(mi.p_norm,0.00013573931385468807)

    def test_sids(self):
        w = pysal.open(pysal.examples.get_path("sids2.gal")).read()
        f = pysal.open(pysal.examples.get_path("sids2.dbf"))
        SIDR = np.array(f.by_col("SIDR74"))
        mi = pysal.Moran(SIDR, w, two_tailed=False)
        self.assertAlmostEquals(mi.I, 0.24772519320480135)
        self.assertAlmostEquals(mi.p_norm,  5.7916539074498452e-05)


class Moran_Rate_Tester(unittest.TestCase):
    def setUp(self):
        self.w = pysal.open(pysal.examples.get_path("sids2.gal")).read()
        f = pysal.open(pysal.examples.get_path("sids2.dbf"))
        self.e = np.array(f.by_col['SID79'])
        self.b = np.array(f.by_col['BIR79'])

    def test_moran_rate(self):
        mi = moran.Moran_Rate(self.e, self.b, self.w, two_tailed=False)
        self.assertAlmostEquals(mi.I, 0.16622343552567395, 7)
        self.assertAlmostEquals(mi.p_norm, 0.004191499504892171)


class Moran_BV_matrix_Tester(unittest.TestCase):
    def setUp(self):
        f = pysal.open(pysal.examples.get_path("sids2.dbf"))
        varnames = ['SIDR74', 'SIDR79', 'NWR74', 'NWR79']
        self.names = varnames
        vars = [np.array(f.by_col[var]) for var in varnames]
        self.vars = vars
        self.w = pysal.open(pysal.examples.get_path("sids2.gal")).read()

    def test_Moran_BV_matrix(self):
        res = moran.Moran_BV_matrix(self.vars, self.w, varnames=self.names)
        self.assertAlmostEquals(res[(0, 1)].I, 0.19362610652874668)
        self.assertAlmostEquals(res[(3, 0)].I, 0.37701382542927858)


class Moran_Local_Tester(unittest.TestCase):
    def setUp(self):
        np.random.seed(10)
        self.w = pysal.open(pysal.examples.get_path("desmith.gal")).read()
        f = pysal.open(pysal.examples.get_path("desmith.txt"))
        self.y = np.array(f.by_col['z'])

    def test_Moran_Local(self):
        lm = moran.Moran_Local(
            self.y, self.w, transformation="r", permutations=99)
        self.assertAlmostEquals(lm.z_sim[0], -0.081383956359666748)
        self.assertAlmostEquals(lm.p_z_sim[0], 0.46756830387716064)
        self.assertAlmostEquals(lm.VI_sim, 0.2067126047680822)


class Moran_Local_Rate_Tester(unittest.TestCase):
    def setUp(self):
        np.random.seed(10)
        self.w = pysal.open(pysal.examples.get_path("sids2.gal")).read()
        f = pysal.open(pysal.examples.get_path("sids2.dbf"))
        self.e = np.array(f.by_col['SID79'])
        self.b = np.array(f.by_col['BIR79'])

    def test_moran_rate(self):
        lm = moran.Moran_Local_Rate(self.e, self.b, self.w,
                                    transformation="r", permutations=99)
        self.assertAlmostEquals(lm.z_sim[0], -0.27099998923550017)
        self.assertAlmostEquals(lm.p_z_sim[0], 0.39319552026912641)
        self.assertAlmostEquals(lm.VI_sim, 0.21879403675396222)


suite = unittest.TestSuite()
test_classes = [Moran_Tester, Moran_BV_matrix_Tester, Moran_Local_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_smoothing
import unittest
import pysal
from pysal.esda import smoothing as sm
from pysal import knnW
import numpy as np


class TestFlatten(unittest.TestCase):
    def setUp(self):
        self.input = [[1, 2], [3, 3, 4], [5, 6]]

    def test_flatten(self):
        out1 = sm.flatten(self.input)
        out2 = sm.flatten(self.input, unique=False)
        self.assertEquals(out1, [1, 2, 3, 4, 5, 6])
        self.assertEquals(out2, [1, 2, 3, 3, 4, 5, 6])


class TestWMean(unittest.TestCase):
    def setUp(self):
        self.d = np.array([5, 4, 3, 1, 2])
        self.w1 = np.array([10, 22, 9, 2, 5])
        self.w2 = np.array([10, 14, 17, 2, 5])

    def test_weighted_median(self):
        out1 = sm.weighted_median(self.d, self.w1)
        out2 = sm.weighted_median(self.d, self.w2)
        self.assertEquals(out1, 4)
        self.assertEquals(out2, 3.5)


class TestAgeStd(unittest.TestCase):
    def setUp(self):
        self.e = np.array([30, 25, 25, 15, 33, 21, 30, 20])
        self.b = np.array([1000, 1000, 1100, 900, 1000, 900, 1100, 900])
        self.s_e = np.array([100, 45, 120, 100, 50, 30, 200, 80])
        self.s_b = s = np.array([1000, 900, 1000, 900, 1000, 900, 1000, 900])
        self.n = 2

    def test_crude_age_standardization(self):
        crude = sm.crude_age_standardization(self.e, self.b, self.n).round(8)
        crude_exp = np.array([0.02375000, 0.02666667])
        self.assertEquals(list(crude), list(crude_exp))

    def test_direct_age_standardization(self):
        direct = np.array(sm.direct_age_standardization(
            self.e, self.b, self.s_b, self.n)).round(8)
        direct_exp = np.array([[0.02374402, 0.01920491,
                                0.02904848], [0.02665072, 0.02177143, 0.03230508]])
        self.assertEquals(list(direct.flatten()), list(direct_exp.flatten()))

    def test_indirect_age_standardization(self):
        indirect = np.array(sm.indirect_age_standardization(
            self.e, self.b, self.s_e, self.s_b, self.n)).round(8)
        indirect_exp = np.array([[0.02372382, 0.01940230,
                                  0.02900789], [0.02610803, .02154304, 0.03164035]])
        self.assertEquals(
            list(indirect.flatten()), list(indirect_exp.flatten()))


class TestSRate(unittest.TestCase):
    def setUp(self):
        sids = pysal.open(pysal.examples.get_path('sids2.dbf'), 'r')
        self.w = pysal.open(pysal.examples.get_path('sids2.gal'), 'r').read()
        self.b, self.e = np.array(sids[:, 8]), np.array(sids[:, 9])
        er = [0.453433, 0.000000, 0.775871, 0.973810, 3.133190]
        eb = [0.0016973, 0.0017054, 0.0017731, 0.0020129, 0.0035349]
        sr = [0.0009922, 0.0012639, 0.0009740, 0.0007605, 0.0050154]
        smr = [0.00083622, 0.00109402, 0.00081567, 0.0, 0.0048209]
        smr_w = [0.00127146, 0.00127146, 0.0008433, 0.0, 0.0049889]
        smr2 = [0.00091659, 0.00087641, 0.00091073, 0.0, 0.00467633]
        self.er = [round(i, 5) for i in er]
        self.eb = [round(i, 7) for i in eb]
        self.sr = [round(i, 7) for i in sr]
        self.smr = [round(i, 7) for i in smr]
        self.smr_w = [round(i, 7) for i in smr_w]
        self.smr2 = [round(i, 7) for i in smr2]

    def test_Excess_Risk(self):
        out_er = sm.Excess_Risk(self.e, self.b).r
        out_er = [round(i, 5) for i in out_er[:5]]
        self.assertEquals(out_er, self.er)

    def test_Empirical_Bayes(self):
        out_eb = sm.Empirical_Bayes(self.e, self.b).r
        out_eb = [round(i, 7) for i in out_eb[:5]]
        self.assertEquals(out_eb, self.eb)

    def test_Spatial_Empirical_Bayes(self):
        stl = pysal.open(pysal.examples.get_path('stl_hom.csv'), 'r')
        stl_e, stl_b = np.array(stl[:, 10]), np.array(stl[:, 13])
        stl_w = pysal.open(pysal.examples.get_path('stl.gal'), 'r').read()
        if not stl_w.id_order_set:
            stl_w.id_order = range(1, len(stl) + 1)
        s_eb = sm.Spatial_Empirical_Bayes(stl_e, stl_b, stl_w)
        s_ebr10 = np.array([4.01485749e-05, 3.62437513e-05,
                            4.93034844e-05, 5.09387329e-05, 3.72735210e-05,
                            3.69333797e-05, 5.40245456e-05, 2.99806055e-05,
                            3.73034109e-05, 3.47270722e-05])
        np.testing.assert_array_almost_equal(s_ebr10, s_eb.r[:10])

    def test_Spatial_Rate(self):
        out_sr = sm.Spatial_Rate(self.e, self.b, self.w).r
        out_sr = [round(i, 7) for i in out_sr[:5]]
        self.assertEquals(out_sr, self.sr)

    def test_Spatial_Median_Rate(self):
        out_smr = sm.Spatial_Median_Rate(self.e, self.b, self.w).r
        out_smr_w = sm.Spatial_Median_Rate(self.e, self.b, self.w, aw=self.b).r
        out_smr2 = sm.Spatial_Median_Rate(
            self.e, self.b, self.w, iteration=2).r
        out_smr = [round(i, 7) for i in out_smr[:5]]
        out_smr_w = [round(i, 7) for i in out_smr_w[:5]]
        out_smr2 = [round(i, 7) for i in out_smr2[:5]]
        self.assertEquals(out_smr, self.smr)
        self.assertEquals(out_smr_w, self.smr_w)
        self.assertEquals(out_smr2, self.smr2)


class TestHB(unittest.TestCase):
    def setUp(self):
        sids = pysal.open(pysal.examples.get_path('sids2.shp'), 'r')
        self.sids = sids
        self.d = np.array([i.centroid for i in sids])
        self.w = knnW(self.d, k=5)
        if not self.w.id_order_set:
            self.w.id_order = self.w.id_order
        sids_db = pysal.open(pysal.examples.get_path('sids2.dbf'), 'r')
        self.b, self.e = np.array(sids_db[:, 8]), np.array(sids_db[:, 9])

    def test_Headbanging_Triples(self):
        ht = sm.Headbanging_Triples(self.d, self.w)
        self.assertEquals(len(ht.triples), len(self.d))
        ht2 = sm.Headbanging_Triples(self.d, self.w, edgecor=True)
        self.assertTrue(hasattr(ht2, 'extra'))
        self.assertEquals(len(ht2.triples), len(self.d))
        htr = sm.Headbanging_Median_Rate(self.e, self.b, ht2, iteration=5)
        self.assertEquals(len(htr.r), len(self.e))
        for i in htr.r:
            self.assertTrue(i is not None)

    def test_Headbanging_Median_Rate(self):
        sids_d = np.array([i.centroid for i in self.sids])
        sids_w = pysal.knnW(sids_d, k=5)
        if not sids_w.id_order_set:
            sids_w.id_order = sids_w.id_order
        s_ht = sm.Headbanging_Triples(sids_d, sids_w, k=5)
        sids_db = pysal.open(pysal.examples.get_path('sids2.dbf'), 'r')
        s_e, s_b = np.array(sids_db[:, 9]), np.array(sids_db[:, 8])
        sids_hb_r = sm.Headbanging_Median_Rate(s_e, s_b, s_ht)
        sids_hb_rr5 = np.array([0.00075586, 0.,
                                0.0008285, 0.0018315, 0.00498891])
        np.testing.assert_array_almost_equal(sids_hb_rr5, sids_hb_r.r[:5])
        sids_hb_r2 = sm.Headbanging_Median_Rate(s_e, s_b, s_ht, iteration=5)
        sids_hb_r2r5 = np.array([0.0008285, 0.00084331,
                                 0.00086896, 0.0018315, 0.00498891])
        np.testing.assert_array_almost_equal(sids_hb_r2r5, sids_hb_r2.r[:5])
        sids_hb_r3 = sm.Headbanging_Median_Rate(s_e, s_b, s_ht, aw=s_b)
        sids_hb_r3r5 = np.array([0.00091659, 0.,
                                 0.00156838, 0.0018315, 0.00498891])
        np.testing.assert_array_almost_equal(sids_hb_r3r5, sids_hb_r3.r[:5])


class TestKernel_AgeAdj_SM(unittest.TestCase):
    def setUp(self):
        self.e = np.array([10, 1, 3, 4, 2, 5])
        self.b = np.array([100, 15, 20, 20, 80, 90])
        self.e1 = np.array([10, 8, 1, 4, 3, 5, 4, 3, 2, 1, 5, 3])
        self.b1 = np.array([100, 90, 15, 30, 25, 20, 30, 20, 80, 80, 90, 60])
        self.s = np.array([98, 88, 15, 29, 20, 23, 33, 25, 76, 80, 89, 66])
        self.points = [(
            10, 10), (20, 10), (40, 10), (15, 20), (30, 20), (30, 30)]
        self.kw = pysal.weights.Kernel(self.points)
        if not self.kw.id_order_set:
            self.kw.id_order = range(0, len(self.points))

    def test_Kernel_Smoother(self):
        kr = sm.Kernel_Smoother(self.e, self.b, self.kw)
        exp = [0.10543301, 0.0858573, 0.08256196, 0.09884584,
               0.04756872, 0.04845298]
        self.assertEquals(list(kr.r.round(8)), exp)

    def test_Age_Adjusted_Smoother(self):
        ar = sm.Age_Adjusted_Smoother(self.e1, self.b1, self.kw, self.s)
        exp = [0.10519625, 0.08494318, 0.06440072, 0.06898604,
               0.06952076, 0.05020968]
        self.assertEquals(list(ar.r.round(8)), exp)

    def test_Disk_Smoother(self):
        self.kw.transform = 'b'
        exp = [0.12222222000000001, 0.10833333, 0.08055556,
               0.08944444, 0.09944444, 0.09351852]
        disk = sm.Disk_Smoother(self.e, self.b, self.kw)
        self.assertEqual(list(disk.r.round(8)), exp)

    def test_Spatial_Filtering(self):
        points = np.array(self.points)
        bbox = [[0, 0], [45, 45]]
        sf = sm.Spatial_Filtering(bbox, points, self.e, self.b, 2, 2, r=30)
        exp = [0.11111111, 0.11111111, 0.20000000000000001, 0.085106379999999995,
               0.076923080000000005, 0.05789474, 0.052173909999999997, 0.066666669999999997, 0.04117647]
        self.assertEqual(list(sf.r.round(8)), exp)


class TestUtils(unittest.TestCase):
    def test_sum_by_n(self):
        d = np.array([10, 9, 20, 30])
        w = np.array([0.5, 0.1, 0.3, 0.8])
        n = 2
        exp_sum = np.array([5.9, 30.])
        np.testing.assert_array_almost_equal(exp_sum, sm.sum_by_n(d, w, n))

    def test_standardized_mortality_ratio(self):
        e = np.array([30, 25, 25, 15, 33, 21, 30, 20])
        b = np.array([100, 100, 110, 90, 100, 90, 110, 90])
        s_e = np.array([100, 45, 120, 100, 50, 30, 200, 80])
        s_b = np.array([1000, 900, 1000, 900, 1000, 900, 1000, 900])
        n = 2
        exp_smr = np.array([2.48691099, 2.73684211])
        np.testing.assert_array_almost_equal(exp_smr,
                                             sm.standardized_mortality_ratio(e, b, s_e, s_b, n))

    def test_choynowski(self):
        e = np.array([30, 25, 25, 15, 33, 21, 30, 20])
        b = np.array([100, 100, 110, 90, 100, 90, 110, 90])
        n = 2
        exp_choy = np.array([0.30437751, 0.29367033])
        np.testing.assert_array_almost_equal(exp_choy, sm.choynowski(e, b, n))

    def test_assuncao_rate(self):
        e = np.array([30, 25, 25, 15, 33, 21, 30, 20])
        b = np.array([100, 100, 110, 90, 100, 90, 110, 90])
        exp_assuncao = np.array(
            [1.04319254, -0.04117865, -0.56539054, -1.73762547])
        np.testing.assert_array_almost_equal(
            exp_assuncao, sm.assuncao_rate(e, b)[:4])


suite = unittest.TestSuite()
test_classes = [TestFlatten, TestWMean, TestAgeStd, TestSRate, TestHB,
                TestKernel_AgeAdj_SM, TestUtils]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = gini
"""
Gini based Inequality Metrics
"""

__author__ = "Sergio J. Rey <srey@asu.edu> "

#from pysal.common import *
import numpy as np
from scipy.stats import norm as NORM

__all__ = ['Gini', 'Gini_Spatial']


class Gini:
    """
    Classic Gini coefficient in absolute deviation form

    Parameters
    ----------

    y : array (n,1)
       attribute

    Attributes
    ----------

    g : float
       Gini coefficient

    """

    def __init__(self, x):

        x.shape = (x.shape[0],)
        d = np.abs(np.array([x - xi for xi in x]))
        n = len(x)
        xbar = x.mean()
        den = xbar * 2 * n**2
        dtotal = d.sum()
        self.g = dtotal/den


class Gini_Spatial:
    """
    Spatial Gini coefficient

    Provides for computationally based inference regarding the contribution of
    spatial neighbor pairs to overall inequality across a set of regions. [1]_

    Parameters
    ----------

    y : array (n,1)
       attribute

    w : binary spatial weights object

    permutations : int (default = 99)
       number of permutations for inference

    Attributes
    ----------

    g : float
       Gini coefficient

    wg : float
       Neighbor inequality component (geographic inequality)

    wcg : float
       Non-neighbor inequality component (geographic complement inequality)

    wcg_share : float
       Share of inequality in non-neighbor component

    If Permuations > 0

    p_sim : float
       pseudo p-value for spatial gini

    e_wcg : float
       expected value of non-neighbor inequality component (level) from permutations

    s_wcg : float
           standard deviation non-neighbor inequality component (level) from permutations

    z_wcg : float
           z-value non-neighbor inequality component (level) from permutations

    p_z_sim : float
             pseudo  p-value based on standard normal approximation of permutation based values


    Examples
    --------
    >>> import pysal
    >>> import numpy as np

    Use data from the 32 Mexican States, Decade frequency 1940-2010

    >>> f=pysal.open(pysal.examples.get_path("mexico.csv"))
    >>> vnames=["pcgdp%d"%dec for dec in range(1940,2010,10)]
    >>> y=np.transpose(np.array([f.by_col[v] for v in vnames]))

    Define regime neighbors

    >>> regimes=np.array(f.by_col('hanson98'))
    >>> w = pysal.regime_weights(regimes)
    >>> np.random.seed(12345)
    >>> gs = pysal.inequality.gini.Gini_Spatial(y[:,0],w)
    >>> gs.p_sim
    0.01
    >>> gs.wcg
    4353856.0
    >>> gs.e_wcg
    1067629.2525252525
    >>> gs.s_wcg
    95869.167798782844
    >>> gs.z_wcg
    34.2782442252145
    >>> gs.p_z_sim
    0.0

    Thus, the amount of inequality between pairs of states that are not in the
    same regime (neighbors) is significantly higher than what is expected
    under the null of random spatial inequality.


    References
    ----------

    .. [1] Rey, S.J. and R. Smith (2012) "A spatial decomposition of the Gini
        coefficient." Letters in Spatial and Resource Sciences. DOI 10.1007/s12076-012-00860z

    """
    def __init__(self, x, w, permutations=99):
        x.shape = (x.shape[0],)
        d = np.abs(np.array([x - xi for xi in x]))
        n = len(x)
        xbar = x.mean()
        den = xbar * 2 * n**2
        wg = w.sparse.multiply(d).sum()
        self.wg = wg  # spatial inequality component
        dtotal = d.sum()
        wcg = dtotal - wg  # complement to spatial inequality component
        self.wcg = wcg
        self.g = dtotal / den
        self.wcg_share = wcg / dtotal
        self.dtotal = dtotal
        self.den = den

        if permutations:
            ids = np.arange(n)
            wcgp = np.zeros((permutations, 1))
            for perm in xrange(permutations):
                # permute rows/cols of d
                np.random.shuffle(ids)
                wcgp[perm] = w.sparse.multiply(d[ids, :][:, ids]).sum()
            above = wcgp >= self.wcg
            larger = above.sum()
            if (permutations - larger) < larger:
                larger = permutations - larger
            self.p_sim = (larger + 1.) / (permutations + 1.)
            self.e_wcg = wcgp.mean()
            self.s_wcg = wcgp.std()
            self.z_wcg = (self.wcg - self.e_wcg) / self.s_wcg
            self.p_z_sim = 1.0 - NORM.cdf(self.z_wcg)

########NEW FILE########
__FILENAME__ = test_theil
import unittest
import pysal
import numpy as np
from pysal.inequality.theil import *


class test_Theil(unittest.TestCase):

    def test___init__(self):
        # theil = Theil(y)
        f = pysal.open(pysal.examples.get_path("mexico.csv"))
        vnames = ["pcgdp%d" % dec for dec in range(1940, 2010, 10)]
        y = np.transpose(np.array([f.by_col[v] for v in vnames]))
        theil_y = Theil(y)
        np.testing.assert_almost_equal(theil_y.T, np.array([0.20894344, 0.15222451, 0.10472941, 0.10194725, 0.09560113, 0.10511256, 0.10660832]))


class test_TheilD(unittest.TestCase):
    def test___init__(self):
        # theil_d = TheilD(y, partition)
        f = pysal.open(pysal.examples.get_path("mexico.csv"))
        vnames = ["pcgdp%d" % dec for dec in range(1940, 2010, 10)]
        y = np.transpose(np.array([f.by_col[v] for v in vnames]))
        regimes = np.array(f.by_col('hanson98'))
        theil_d = TheilD(y, regimes)
        np.testing.assert_almost_equal(theil_d.bg, np.array([0.0345889, 0.02816853, 0.05260921, 0.05931219, 0.03205257, 0.02963731, 0.03635872]))


class test_TheilDSim(unittest.TestCase):
    def test___init__(self):
        f = pysal.open(pysal.examples.get_path("mexico.csv"))
        vnames = ["pcgdp%d" % dec for dec in range(1940, 2010, 10)]
        y = np.transpose(np.array([f.by_col[v] for v in vnames]))
        regimes = np.array(f.by_col('hanson98'))
        np.random.seed(10)
        theil_ds = TheilDSim(y, regimes, 999)
        np.testing.assert_almost_equal(theil_ds.bg_pvalue, np.array(
            [0.4, 0.344, 0.001, 0.001, 0.034, 0.072, 0.032]))


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = theil
"""Theil Inequality metrics

"""
__author__ = "Sergio J. Rey <srey@asu.edu> "

from pysal.common import *
import numpy as np
__all__ = ['Theil', 'TheilD', 'TheilDSim']

SMALL = np.finfo('float').tiny


class Theil:
    """
    Classic Theil measure of inequality

        .. math::

            T = \sum_{i=1}^n \left( \\frac{y_i}{\sum_{i=1}^n y_i} \ln \left[ N \\frac{y_i}{\sum_{i=1}^n y_i}\\right] \\right)

    Parameters
    ----------
    y   : array (n,t) or (n,)
          with n taken as the observations across which inequality is
          calculated.  If y is (n,) then a scalar inequality value is
          determined. If y is (n,t) then an array of inequality values are
          determined, one value for each column in y.

    Attributes
    ----------

    T   : array (t,) or (1,)
          Theil's T for each column of y

    Notes
    -----
    This computation involves natural logs. To prevent ln[0] from occurring, a
    small value is added to each element of y before beginning the computation.

    Examples
    --------
    >>> import pysal
    >>> f=pysal.open(pysal.examples.get_path("mexico.csv"))
    >>> vnames=["pcgdp%d"%dec for dec in range(1940,2010,10)]
    >>> y=np.transpose(np.array([f.by_col[v] for v in vnames]))
    >>> theil_y=Theil(y)
    >>> theil_y.T
    array([ 0.20894344,  0.15222451,  0.10472941,  0.10194725,  0.09560113,
            0.10511256,  0.10660832])
    """

    def __init__(self, y):

        n = len(y)
        y = y + SMALL * (y == 0)  # can't have 0 values
        yt = y.sum(axis=0)
        s = y / (yt * 1.0)
        lns = np.log(n * s)
        slns = s * lns
        t = sum(slns)
        self.T = t


class TheilD:
    """Decomposition of Theil's T based on partitioning of
    observations into exhaustive and mutually exclusive groups

    Parameters
    ----------
    y         : array  (n,t) or (n, )
                with n taken as the observations across which inequality is
                calculated If y is (n,) then a scalar inequality value is
                determined. If y is (n,t) then an array of inequality values are
                determined, one value for each column in y.
    partition : array (n, )
                elements indicating which partition each observation belongs
                to. These are assumed to be exhaustive.

    Attributes
    ----------
    T  : array (n,t) or (n,)
         global inequality T
    bg : array (n,t) or (n,)
         between group inequality
    wg : array (n,t) or (n,)
         within group inequality

    Examples
    --------
    >>> import pysal
    >>> f=pysal.open(pysal.examples.get_path("mexico.csv"))
    >>> vnames=["pcgdp%d"%dec for dec in range(1940,2010,10)]
    >>> y=np.transpose(np.array([f.by_col[v] for v in vnames]))
    >>> regimes=np.array(f.by_col('hanson98'))
    >>> theil_d=TheilD(y,regimes)
    >>> theil_d.bg
    array([ 0.0345889 ,  0.02816853,  0.05260921,  0.05931219,  0.03205257,
            0.02963731,  0.03635872])
    >>> theil_d.wg
    array([ 0.17435454,  0.12405598,  0.0521202 ,  0.04263506,  0.06354856,
            0.07547525,  0.0702496 ])
   """
    def __init__(self, y, partition):
        groups = np.unique(partition)
        T = Theil(y).T
        ytot = y.sum(axis=0)

        #group totals
        gtot = np.array([y[partition == gid].sum(axis=0) for gid in groups])
        mm = np.dot

        if ytot.size == 1:  # y is 1-d
            sg = gtot / (ytot * 1.)
            sg.shape = (sg.size, 1)
        else:
            sg = mm(gtot, np.diag(1. / ytot))
        ng = np.array([sum(partition == gid) for gid in groups])
        ng.shape = (ng.size,)  # ensure ng is 1-d
        n = y.shape[0]
        # between group inequality
        bg = np.multiply(sg, np.log(mm(np.diag(n * 1. / ng), sg))).sum(axis=0)
        self.T = T
        self.bg = bg
        self.wg = T - bg


class TheilDSim:
    """Random permutation based inference on Theil's inequality decomposition.

    Provides for computationally based inference regarding the inequality
    decomposition using random spatial permutations. [1]_

    Parameters
    ----------
    y            : array  (n,t) or (n, )
                   with n taken as the observations across which inequality is
                   calculated If y is (n,) then a scalar inequality value is
                   determined. If y is (n,t) then an array of inequality values are
                   determined, one value for each column in y.
    partition    : array (n, )
                   elements indicating which partition each observation belongs
                   to. These are assumed to be exhaustive.
    permutations : int
                   Number of random spatial permutations for computationally
                   based inference on the decomposition.

    Attributes
    ----------

    observed   : array (n,t) or (n,)
                 TheilD instance for the observed data.

    bg         : array (permutations+1,t)
                 between group inequality

    bg_pvalue  : array (t,1)
                 p-value for the between group measure.  Measures the
                 percentage of the realized values that were greater than
                 or equal to the observed bg value. Includes the observed
                 value.

    wg         : array (size=permutations+1)
                 within group inequality Depending on the shape of y, 1 or 2-dimensional

    Examples
    --------
    >>> import pysal
    >>> f=pysal.open(pysal.examples.get_path("mexico.csv"))
    >>> vnames=["pcgdp%d"%dec for dec in range(1940,2010,10)]
    >>> y=np.transpose(np.array([f.by_col[v] for v in vnames]))
    >>> regimes=np.array(f.by_col('hanson98'))
    >>> np.random.seed(10)
    >>> theil_ds=TheilDSim(y,regimes,999)
    >>> theil_ds.bg_pvalue
    array([ 0.4  ,  0.344,  0.001,  0.001,  0.034,  0.072,  0.032])

    References
    ----------
    .. [1] Rey, S.J. (2004) "Spatial analysis of regional economic growth,
       inequality and change," in  M.F. Goodchild and D.G. Jannelle (eds.)
       Spatially Integrated Social Science. Oxford University Press: Oxford.
       Pages 280-299.

    """
    def __init__(self, y, partition, permutations=99):

        observed = TheilD(y, partition)
        bg_ct = observed.bg == observed.bg  # already have one extreme value
        bg_ct = bg_ct * 1.0
        results = [observed]
        for perm in range(permutations):
            yp = np.random.permutation(y)
            t = TheilD(yp, partition)
            bg_ct += (1.0 * t.bg >= observed.bg)
            results.append(t)
        self.results = results
        self.T = observed.T
        self.bg_pvalue = bg_ct / (permutations * 1.0 + 1)
        self.bg = np.array([r.bg for r in results])
        self.wg = np.array([r.wg for r in results])


########NEW FILE########
__FILENAME__ = _indices
'''
Diversity indices as suggested in Nijkamp & Poot (2013)
'''

import itertools
import numpy as np

def abundance(x):
    '''
    Abundance index
    ...

    Arguments
    ---------
    x       : array
              N x k array containing N rows (one per neighborhood) and k columns
              (one per cultural group)
    Returns
    -------
    a       : float
              Abundance index
    '''
    xs = x.sum(axis=0)
    return np.sum([1 for i in xs if i>0])

def margalev_md(x):
    '''
    Margalev MD index
    ...

    Arguments
    ---------
    x       : array
              N x k array containing N rows (one per neighborhood) and k columns
              (one per cultural group)
    Returns
    -------
    a       : float
              Margalev MD index
    '''
    a = abundance(x)
    return (a - 1.) / np.log(x.sum())

def menhinick_mi(x):
    '''
    Menhinick MI index
    ...

    Arguments
    ---------
    x       : array
              N x k array containing N rows (one per neighborhood) and k columns
              (one per cultural group)
    Returns
    -------
    a       : float
              Menhinick MI index
    '''
    a = abundance(x)
    return (a - 1.) / np.sqrt(x.sum())

def simpson_so(x):
    '''
    Simpson diversity index SO
    ...

    Arguments
    ---------
    x       : array
              N x k array containing N rows (one per neighborhood) and k columns
              (one per cultural group)
    Returns
    -------
    a       : float
              Simpson diversity index SO
    '''
    xs0 = x.sum(axis=0)
    xs = x.sum()
    num = (xs0 * (xs0 - 1.)).sum()
    den = xs * (xs - 1.)
    return num / den

def simpson_sd(x):
    '''
    Simpson diversity index SD
    ...

    Arguments
    ---------
    x       : array
              N x k array containing N rows (one per neighborhood) and k columns
              (one per cultural group)
    Returns
    -------
    a       : float
              Simpson diversity index SD
    '''
    return 1. - simpson_so(x)

def herfindahl_hd(x):
    '''
    Herfindahl index HD
    ...

    Arguments
    ---------
    x       : array
              N x k array containing N rows (one per neighborhood) and k columns
              (one per cultural group)
    Returns
    -------
    a       : float
              Herfindahl index HD
    '''
    pgs = x.sum(axis=0)
    p = pgs.sum()
    return ((pgs * 1. / p)**2).sum()

def fractionalization_gs(x):
    '''
    Fractionalization Gini-Simpson index GS
    ...

    Arguments
    ---------
    x       : array
              N x k array containing N rows (one per neighborhood) and k columns
              (one per cultural group)
    Returns
    -------
    a       : float
              Fractionalization Gini-Simpson index GS
    '''
    return 1. - herfindahl_hd(x)

def polarization(x):
    return 'Not implemented'

def shannon_se(x):
    '''
    Shannon index SE
    ...

    Arguments
    ---------
    x       : array
              N x k array containing N rows (one per neighborhood) and k columns
              (one per cultural group)
    Returns
    -------
    a       : float
              Shannon index SE
    '''
    pgs = x.sum(axis=0)
    p = pgs.sum()
    ratios = pgs * 1. / p
    return - (ratios * np.log(ratios)).sum()

def gini_gi(x):
    '''
    Gini GI index

    NOTE: based on 3rd eq. of "Calculation" in:

            http://en.wikipedia.org/wiki/Gini_coefficient

         Returns same value as `gini` method in the R package `reldist` (see
         http://rss.acs.unt.edu/Rdoc/library/reldist/html/gini.html) if every
         category has at least one observation
    ...

    Arguments
    ---------
    x       : array
              N x k array containing N rows (one per neighborhood) and k columns
              (one per cultural group)
    Returns
    -------
    a       : float
              Gini GI index
    '''
    ys = x.sum(axis=0)
    return _gini(ys)

def gini_gi_m(x):
    '''
    Gini GI index (equivalent to `gini_gi`, not vectorized)

    NOTE: based on Wolfram Mathworld formula in:

            http://mathworld.wolfram.com/GiniCoefficient.html

         Returns same value as `gini_gi`.
    ...

    Arguments
    ---------
    x       : array
              N x k array containing N rows (one per neighborhood) and k columns
              (one per cultural group)
    Returns
    -------
    a       : float
              Gini GI index
    '''
    xs = x.sum(axis=0)
    num = np.sum([np.abs(xi - xj) for xi, xj in itertools.permutations(xs, 2)])
    den = 2. * xs.shape[0]**2 * np.mean(xs)
    return num / den

def _gini(ys):
    '''
    Gini for a single row to be used both by `gini_gi` and `gini_gig`
    '''
    n = ys.flatten().shape[0]
    ys.sort()
    num = 2. * ((np.arange(n)+1) * ys).sum()
    den = n * ys.sum()
    return (num / den) - ((n + 1.) / n)

def hoover_hi(x):
    '''
    Hoover index HI

    NOTE: based on

            http://en.wikipedia.org/wiki/Hoover_index

    ...

    Arguments
    ---------
    x       : array
              N x k array containing N rows (one per neighborhood) and k columns
              (one per cultural group)
    Returns
    -------
    a       : float
              Hoover HI index
    '''
    es = x.sum(axis=0)
    e_total = es.sum()
    a_total = es.shape[0]
    s = np.abs((es*1./e_total) - (1./a_total)).sum()
    return s / 2.

def similarity_w_wd(x, tau):
    '''
    Similarity weighted diversity
    ...

    Arguments
    ---------
    x       : array
              N x k array containing N rows (one per neighborhood) and k columns
              (one per cultural group)
    tau     : array
              k x k array where tau_ij represents dissimilarity between group
              i and group j. Diagonal elements are assumed to be one.

    Returns
    -------
    a       : float
              Similarity weighted diversity index
    '''
    pgs = x.sum(axis=0)
    pgs = pgs * 1. / pgs.sum()
    s = sum([pgs[i] * pgs[j] * tau[i, j] for i,j in \
            itertools.product(np.arange(pgs.shape[0]), repeat=2)])
    return 1. - s

def segregation_gsg(x):
    '''
    Segregation index GS

    This is a Duncan&Duncan index of a group against the rest combined
    ...

    Arguments
    ---------
    x       : array
              N x k array containing N rows (one per neighborhood) and k columns
              (one per cultural group)
    Returns
    -------
    a       : array
              Array with GSg indices for the k groups
    '''
    pgs = x.sum(axis=0)
    pas = x.sum(axis=1)
    p = pgs.sum()
    first = (x.T * 1. / pgs[:, None]).T
    paMpga = pas[:, None] - x
    pMpg = p - pgs
    second = paMpga * 1. / pMpg[None, :]
    return 0.5 * (np.abs(first - second)).sum(axis=0)

def modified_segregation_msg(x):
    '''
    Modified segregation index GS

    This is a modified version of GSg index as used by Van Mourik et al. (1989)
    ...

    Arguments
    ---------
    x       : array
              N x k array containing N rows (one per neighborhood) and k columns
              (one per cultural group)
    Returns
    -------
    a       : array
              Array with MSg indices for the k groups
    '''
    pgs = x.sum(axis=0)
    p = pgs.sum()
    ms_inds = segregation_gsg(x) # To be updated in loop below
    for gi in np.arange(x.shape[1]):
        pg = pgs[gi]
        pgp = pg * 1. / p
        ms_inds[gi] = 2. * pgp * (1. - pgp) * ms_inds[gi]
    return ms_inds

def isolation_isg(x):
    '''
    Isolation index IS

    ...

    Arguments
    ---------
    x       : array
              N x k array containing N rows (one per neighborhood) and k columns
              (one per cultural group)
    Returns
    -------
    a       : array
              Array with ISg indices for the k groups
    '''
    ws = x * 1. / x.sum(axis=0)
    pgapa = (x.T * 1. / x.sum(axis=1)).T
    pgp = x.sum(axis=0) * 1. / x.sum()
    return (ws * pgapa / pgp).sum(axis=0)

def gini_gig(x):
    '''
    Gini GI index

    NOTE: based on Wolfram Mathworld formula in:

            http://mathworld.wolfram.com/GiniCoefficient.html

         Returns same value as `gini_gi`.
    ...

    Arguments
    ---------
    x       : array
              N x k array containing N rows (one per neighborhood) and k columns
              (one per cultural group)
    Returns
    -------
    a       : array
              Gini GI index for every group k
    '''
    return np.apply_along_axis(_gini, 0, x)

def ellison_glaeser_egg(x, hs=None):
    '''
    Ellison and Glaeser (1997) [1]_ index of concentration. Implemented as in
    equation (5) of original reference
    ...

    Arguments
    ---------
    x       : array
              N x k array containing N rows (one per area) and k columns
              (one per industry). Each cell indicates employment figures for
              area n and industry k
    hs      : array
              [Optional] Array of dimension (k,) containing the Herfindahl
              indices of each industry's plant sizes. If not passed, it is
              assumed every plant contains one and only one worker and thus
              H_k = 1 / P_k, where P_k is the total employment in k

    Returns
    -------
    a       : array
              EG index for every group k

    References
    ----------

    .. [1] Ellison, G. and Glaeser, E. L. "Geographic Concentration in U.S.
    Manufacturing Industries: A Dartboard Approach". Journal of Political
    Economy. 105: 889-927

    '''
    industry_totals = x.sum(axis=0)
    if hs==None:
        hs = 1. / industry_totals
    xs = x.sum(axis=1) * 1. / x.sum()
    part = 1. - (xs**2).sum()
    eg_inds = np.zeros(x.shape[1])
    for gi in np.arange(x.shape[1]):
        ss = x[:, gi] * 1. / industry_totals[gi]
        g = ((ss - xs)**2).sum()
        h = hs[gi]
        eg_inds[gi] = (g - part * h) / (part * (1. - h))
    return eg_inds

def ellison_glaeser_egg_pop(x):
    '''
    Ellison and Glaeser (1997) [1]_ index of concentration. Implemented to be
    computed with data about people (segregation/diversity) rather than as
    industry concentration, following Mare et al (2012) [2]_
    ...

    Arguments
    ---------
    x       : array
              N x k array containing N rows (one per neighborhood) and k columns
              (one per cultural group)
    Returns
    -------
    a       : array
              EG index for every group k

    References
    ----------

    .. [1] Ellison, G. and Glaeser, E. L. "Geographic Concentration in U.S.
    Manufacturing Industries: A Dartboard Approach". Journal of Political
    Economy. 105: 889-927

    .. [2] Mare, D., Pinkerton, R., Poot, J. and Coleman, A. (2012)
    Residential Sorting Across Auckland Neighbourhoods. Mimeo. Wellington:
    Motu Economic and Public Policy Research.

    '''
    pas = x.sum(axis=1)
    pgs = x.sum(axis=0)
    p = pas.sum()
    pap = pas * 1. / p
    opg = 1./ pgs
    oopg = 1. - opg
    eg_inds = np.zeros(x.shape[1])
    for g in np.arange(x.shape[1]):
        pgas = x[:, g]
        pg = pgs[g]
        num1n = (((pgas * 1. / pg) - (pas * 1. / p))**2).sum()
        num1d = 1. - ((pas * 1. / p)**2).sum()
        num2 = opg[g]
        den = oopg[g]
        eg_inds[g] = ((num1n / num1d) - num2) / den
    return eg_inds

def maurel_sedillot_msg(x, hs=None):
    '''
    Maurel and Sedillot (1999) [1]_ index of concentration. Implemented as in
    equation (7) of original reference
    ...

    Arguments
    ---------
    x       : array
              N x k array containing N rows (one per neighborhood) and k columns
              (one per cultural group)
    hs      : array
              [Optional] Array of dimension (k,) containing the Herfindahl
              indices of each industry's plant sizes. If not passed, it is
              assumed every plant contains one and only one worker and thus
              H_k = 1 / P_k, where P_k is the total employment in k

    Returns
    -------
    a       : array
              MS index for every group k

    References
    ----------

    .. [1] Maurel, F. and Sedillot, B. (1999). "A Measure of the Geographic
    Concentration in French Manufacturing Industries". Regional Science
    and Urban Economics 29: 575-604
    
    '''
    industry_totals = x.sum(axis=0)
    if hs==None:
        hs = 1. / industry_totals
    x2s = np.sum((x.sum(axis=1) * 1. / x.sum())**2)
    ms_inds = np.zeros(x.shape[1])
    for gi in np.arange(x.shape[1]):
        s2s = np.sum((x[:, gi] * 1. / industry_totals[gi])**2)
        h = hs[gi]
        num = ((s2s - x2s) / (1. - x2s)) - h
        den = 1. - h
        ms_inds[gi] = num / den
    return ms_inds

def maurel_sedillot_msg_pop(x):
    '''
    Maurel and Sedillot (1999) [1]_ index of concentration. Implemented to be
    computed with data about people (segregation/diversity) rather than as
    industry concentration, following Mare et al (2012) [2]_

    ...

    Arguments
    ---------
    x       : array
              N x k array containing N rows (one per neighborhood) and k columns
              (one per cultural group)
    Returns
    -------
    a       : array
              MS index for every group k

    References
    ----------

    .. [1] Maurel, F. and Sedillot, B. (1999). "A Measure of the Geographic
    Concentration in French Manufacturing Industries". Regional Science
    and Urban Economics 29: 575-604

    .. [2] Mare, D., Pinkerton, R., Poot, J. and Coleman, A. (2012)
    Residential Sorting Across Auckland Neighbourhoods. Mimeo. Wellington:
    Motu Economic and Public Policy Research.
    
    '''
    pas = x.sum(axis=1)
    pgs = x.sum(axis=0)
    p = pas.sum()
    pap = pas * 1. / p
    eg_inds = np.zeros(x.shape[1])
    for g in np.arange(x.shape[1]):
        pgas = x[:, g]
        pg = pgs[g]
        num1n = ((pgas * 1. / pg)**2 - (pas * 1. / p)**2).sum()
        num1d = 1. - ((pas * 1. / p)**2).sum()
        num2 = 1. / pg
        den = 1. - (1. / pg)
        eg_inds[g] = ((num1n / num1d) - num2) / den
    return eg_inds

if __name__=='__main__':
    np.random.seed(1)
    x = np.round(np.random.random((10, 3)) * 100).astype(int)
    #x[:, 2] = 0
    ids = [abundance, \
            margalev_md, \
            menhinick_mi, \
            simpson_so, \
            simpson_sd, \
            fractionalization_gs, \
            herfindahl_hd, \
            shannon_se, \
            gini_gi, \
            gini_gi_m, \
            hoover_hi, \
            segregation_gsg, \
            modified_segregation_msg, \
            isolation_isg, \
            gini_gig, \
            ellison_glaeser_egg, \
            ellison_glaeser_egg_pop, \
            maurel_sedillot_msg, \
            maurel_sedillot_msg_pop, \
            ]
    res = [(f_i.func_name, f_i(x)) for f_i in ids]
    print '\nIndices'
    for r in res:
        print r[1], '\t', r[0]

    tau = np.random.random((x.shape[1], x.shape[1]))
    for i in range(tau.shape[0]):
        tau[i, i] = 1.
    print similarity_w_wd(x, tau)


########NEW FILE########
__FILENAME__ = wmd
"""
Weights Meta Data

Prototyping meta data functions and classes for weights provenance


Based on Anselin, L., S.J. Rey and W. Li (2014) "Metadata and provenance for
spatial analysis: the case of spatial weights." International Journal of
Geographical Information Science.  DOI:10.1080/13658816.2014.917313



TODO
----

- Document each public function with working doctest
- Abstract the test files as they currently assume location is source
  directory
- have wmd_reader take either a wmd file or a wmd dictionary/object
"""
__author__ = "Sergio J. Rey <srey@asu.edu>, Wenwen Li <wenwen@asu.edu>"
import pysal as ps
import io, json
import httplib
from urlparse import urlparse
import urllib2 as urllib
import copy
import numpy as np

def wmd_reader(fileName):
    """

    Examples
    --------
    >>> import wmd
    >>> wr = wmd.wmd_reader('w1rook.wmd')
    wmd_reader failed:  w1rook.wmd
    >>> wr = wmd.wmd_reader('wrook1.wmd')
    >>> wr.neighbors[2]
    [0, 1, 3, 4]
    """

    try:
        meta_data = _uri_reader(fileName)
    except:
        try:
            with open(fileName, 'r') as fp:
                meta_data = json.load(fp)
                global fullmeta
                fullmeta = {}
                fullmeta['root'] =  copy.deepcopy(meta_data)
                w = _wmd_parser(meta_data)
                return w
        except:
            print 'wmd_reader failed: ', fileName

class WMD(ps.W):
    """Weights Meta Data Class"""
    def __init__(self, neighbors=None,  weights=None, id_order=None):
        self.meta_data = {}
        super(WMD, self).__init__(neighbors, weights, id_order)

    # override transform property to record any post-instantiation
    # transformations in meta data

    @ps.W.transform.setter
    def transform(self, value):
        super(WMD, WMD).transform.__set__(self, value)
        self.meta_data['transform'] = self._transform

    def write(self, fileName, data=False):
        """

        Examples
        --------
        >>> import wmd
        >>> wr = wmd.wmd_reader('w1rook.wmd')
        wmd_reader failed:  w1rook.wmd
        >>> wr = wmd.wmd_reader('wrook1.wmd')
        >>> wr.write('wr1.wmd')
        >>> wr1 = wmd.wmd_reader('wr1.wmd')
        >>> wr.neighbors[2]
        [0, 1, 3, 4]
        >>> wr1.neighbors[2]
        [0, 1, 3, 4]
        >>>

        """
        _wmd_writer(self, fileName, data=data)

######################### Private functions #########################
    
def _wmd_writer(wmd_object, fileName, data=False):
    try:
        with open(fileName, 'w') as f:
            if data:
                wmd_object.meta_data['data'] = {}
                wmd_object.meta_data['data']['weights'] = wmd_object.weights
                wmd_object.meta_data['data']['neighbors'] = wmd_object.neighbors
            json.dump(wmd_object.meta_data,
                    f,
                    indent=4,
                    separators=(',', ': '))
    except:
        print 'wmd_writer failed.'

def _block(arg_dict):
    """
    General handler for block weights


    Examples
    --------
    >>> w = wmd_reader('taz_block.wmd')
    >>> w.n
    4109
    >>> w.meta_data
    {'root': {u'input1': {u'data1': {u'type': u'dbf', u'uri': u'http://toae.org/pub/taz.dbf'}}, u'weight_type': u'block', u'transform': u'O', u'parameters': {u'block_variable': u'CNTY'}}}

    """
    input1 = arg_dict['input1']
    for key in input1:
        input1 = input1[key]
        break
    uri = input1['uri']
    weight_type = arg_dict['weight_type'].lower()
    file_name = uri

    var_name = arg_dict['parameters']['block_variable']
    dbf = ps.open(uri)
    block = np.array(dbf.by_col(var_name))
    dbf.close()
    w = ps.weights.util.regime_weights(block)
    w = WMD(w.neighbors, w.weights)
    w.meta_data = {}
    w.meta_data['input1'] = {"type": 'dbf', 'uri': uri}
    w.meta_data['transform'] = w.transform
    w.meta_data['weight_type'] = weight_type
    w.meta_data['parameters'] = {'block_variable':var_name}

    return w

def _contiguity(arg_dict):
    """
    General handler for building contiguity weights from shapefiles

    Examples
    --------

    >>> w = wmd_reader('wrook1.wmd')
    >>> w.n
    49
    >>> w.meta_data
    {'root': {u'input1': {u'data1': {u'type': u'shp', u'uri': u'http://toae.org/pub/columbus.shp'}}, u'weight_type': u'rook', u'transform': u'O'}}
    """
    input1 = arg_dict['input1']
    for key in input1:
        input1 = input1[key]
        break
    uri = input1['uri']
    weight_type = arg_dict['weight_type']
    weight_type = weight_type.lower()
    if weight_type == 'rook':
        w = ps.rook_from_shapefile(uri)
    elif weight_type == 'queen':
        w = ps.queen_from_shapefile(uri)
    else:
        print "Unsupported contiguity criterion: ",weight_type
        return None
    if 'parameters' in arg_dict:
        order = arg_dict['parameters'].get('order',1) # default to 1st order
        lower = arg_dict['parameters'].get('lower',0) # default to exclude lower orders
        if order > 1:
            w_orig = w
            w = ps.higher_order(w,order)
            if lower:
                for o in xrange(order-1,1,-1):
                    w = ps.weights.w_union(ps.higher_order(w_orig,o), w)
                w = ps.weights.w_union(w, w_orig)
        parameters = arg_dict['parameters']
    else:
        parameters = {'lower': 0, 'order':1 }
    w = WMD(w.neighbors, w.weights)
    w.meta_data = {}
    w.meta_data["input1"] = {"type": 'shp', 'uri':uri}
    w.meta_data["transform"] = w.transform
    w.meta_data["weight_type"] =  weight_type
    w.meta_data['parameters'] = parameters
    return w

def _kernel(arg_dict):
    """
    General handler for building kernel based weights from shapefiles

    Examples
    --------

    >>> w = wmd_reader('kernel.wmd')
    >>> w.n
    49
    >>> w.meta_data
    {'root': {u'input1': {u'data1': {u'type': u'shp', u'uri': u'../examples/columbus.shp'}}, u'weight_type': u'kernel', u'transform': u'O', u'parameters': {u'function': u'triangular', u'bandwidths': None, u'k': 2}}}



    """
    input1 = arg_dict['input1']['data1']
    uri = input1['uri']
    weight_type = arg_dict['weight_type']
    weight_type = weight_type.lower()
    k = 2
    bandwidths = None
    function = 'triangular'
    if 'parameters' in arg_dict:
        k = arg_dict['parameters'].get('k',k) # set default to 2
        bandwidths = arg_dict['parameters'].get('bandwidths',bandwidths)
        function = arg_dict['parameters'].get('function', function)
    else:
        parameters = {}
        parameters['k'] = k
        parameters['bandwidths'] = bandwidths
        parameters['function'] = function
        arg_dict['parameters'] = parameters


    if weight_type == 'akernel':
        # adaptive kernel
        w = ps.adaptive_kernelW_from_shapefile(uri, bandwidths = bandwidths,
                k=k, function = function)
    elif weight_type == 'kernel':
        w = ps.kernelW_from_shapefile(uri, k=k, function = function)
    else:
        print "Unsupported kernel: ",weight_type
        return None
    w = WMD(w.neighbors, w.weights)
    w.meta_data = {}
    w.meta_data["input1"] = {"type": 'shp', 'uri':uri}
    w.meta_data["transform"] = w.transform
    w.meta_data["weight_type"] =  weight_type
    w.meta_data['parameters'] = arg_dict['parameters']
    return w

def _distance(arg_dict):
    """
    General handler for distance based weights obtained from shapefiles
    """
    input1 = arg_dict['input1']
    uri = input1['uri']
    weight_type = arg_dict['weight_type']
    weight_type = weight_type.lower()
    k = 2
    id_variable = None
    p = 2
    radius = None
    if 'parameters' in arg_dict:
        k = arg_dict['parameters'].get('k',k) # set default to 2
        id_variable = arg_dict['parameters'].get('id_variable', id_variable)
        p = arg_dict['parameters'].get('p',p)
        radius = arg_dict['parameters'].get('radius', radius)

    else:
        parameters = {}
        parameters['k'] = 2
        parameters['id_variable'] = None
        parameters['radius'] = None
        parameters['p'] = 2
        arg_dict['parameters'] = parameters

    if weight_type == 'knn':
        w = ps.knnW_from_shapefile(uri,k=k,p=p,idVariable=id_variable,
                radius=radius)
        w = WMD(w.neighbors, w.weights)
        w.meta_data = {}
        w.meta_data["input1"] = {"type": 'shp', 'uri':uri}
        w.meta_data["weight_type"] =  'knn'
        w.meta_data["transform"] = w.transform
        w.meta_data['parameters'] = arg_dict['parameters']
        return w

def _higher_order(arg_dict):
    wmd = arg_dict['wmd']
    order = 2
    if 'parameters' in arg_dict:
        order = arg_dict['parameters'].get('order', order)
    else:
        parameters = {}
        parameters['order'] = order
        arg_dict['parameters'] = parameters


    w = ps.higher_order(wmd, order)
    w = WMD(w.neighbors, w.weights)
    w.meta_data = {}
    w.meta_data['input1'] = arg_dict['input1']
    w.meta_data['parameters'] = arg_dict['parameters']

    return w

def _intersection(arg_dict):
    #wmd = arg_dict['wmd']
    w1 = arg_dict['input1']['data1']['uri']
    w2 = arg_dict['input1']['data2']['uri']
    w = ps.w_intersection(w1,w2)
    w = WMD(w.neighbors, w.weights)
    return w

def _geojsonf(arg_dict):
    """
    Handler for local geojson files
    """
    input1 = arg_dict['input1']
    uri = input1['uri']
    weight_type = arg_dict['weight_type']
    weight_type = weight_type.lower()
    id_variable = None

    if weight_type == 'queen_geojsonf':
        w = ps.weights.user.queen_from_geojsonf(uri)
        w.meta_data = {}
        w.meta_data["input1"] = {"type": 'geojsonf', 'uri':uri}
        w.meta_data["weight_type"] =  'queen'
        w.meta_data["transform"] = w.transform
        return w

# wrapper dict that maps specific weights types to a handler function that
# builds the specific weights instance
WEIGHT_TYPES = {}
WEIGHT_TYPES['rook'] = _contiguity
WEIGHT_TYPES['queen'] = _contiguity
WEIGHT_TYPES['akernel'] = _kernel
WEIGHT_TYPES['kernel'] = _kernel
WEIGHT_TYPES['knn'] = _distance
WEIGHT_TYPES['higher_order'] = _higher_order
WEIGHT_TYPES['block'] = _block
WEIGHT_TYPES['intersection'] = _intersection
#WEIGHT_TYPES['queen_geojsonf'] = geojsonf
#WEIGHT_TYPES['geojsons'] = geojsons


def _uri_reader(uri):
    j = json.load(urllib.urlopen(uri))
    return j

def _wmd_read_only(fileName):
    try:
        meta_data = _uri_reader(fileName)
    except:
        try:
            with open(fileName, 'r') as fp:
                meta_data = json.load(fp)
                return meta_data
        except:
            print '_wmd_read_only failed: ', fileName

def _wmd_parser(wmd_object):
    if 'root' in wmd_object:
        wmd_object = wmd_object['root']
    weight_type = wmd_object['weight_type'].lower()
    for key in wmd_object['input1']:
        #print key
        if wmd_object['input1'][key]['type'] == 'prov':
            #      call wmd_reader
            uri = wmd_object['input1'][key]['uri']

            meta_data = _wmd_read_only(uri)
            fullmeta[uri] = copy.deepcopy(meta_data) #add full metadata
            wmd = _wmd_parser(meta_data)
            wmd_object['input1'][key]['uri'] = wmd
        else:
            # handle distributed files
            uri = wmd_object['input1'][key]['uri']
            try:
                tmp = open(uri)
                #print ' tmp: ', tmp
                wmd_object['input1'][key]['uri'] = uri
            except:
                _download_shapefiles(uri)
                uri = uri.split("/")[-1]
                wmd_object['input1'][key]['uri'] = uri # use local copy

    if weight_type in WEIGHT_TYPES:
        #print weight_type
        wmd  = WEIGHT_TYPES[weight_type](wmd_object)
        wmd.meta_data = fullmeta
    else:
        print 'Unsupported weight type: ', weight_type

    return wmd

def _download_shapefiles(file_name):
    file_parts = file_name.split("/")
    file_prefix = file_parts[-1].split(".")[0]
    exts = [ ".shp", ".dbf", ".shx" ]
    for ext in exts:
        # rebuild url
        file_name = file_prefix + ext
        file_parts[-1] = file_name
        new_url = "/".join(file_parts)
        #print file_name, new_url
        u = urllib.urlopen(new_url)
        f = open(file_name, 'wb')
        meta = u.info()
        file_size = int(meta.getheaders("Content-Length")[0])
        #print "Downloading: %s Bytes: %s" % (file_name, file_size)
        file_size_dl = 0
        block_sz = 8192
        while True:
            bf = u.read(block_sz)
            if not bf:
                break
            file_size_dl += len(bf)
            f.write(bf)
            status = r"%10d [%3.2f%%]" % (file_size_dl, file_size_dl * 100. /
                    file_size)
            status = status + chr(8)* (len(status)+1)
        #print status, f.close()

if __name__ == '__main__':

    # distributed file
    w1 = wmd_reader("wrook1.wmd")

##    # order
##    w1o = wmd_reader('wrooko1.wmd')
##    w2o = wmd_reader('wrooko2.wmd')
##    w2ol = wmd_reader('wrooko2l.wmd')
##
##    # kernels
    ak1 = wmd_reader('akern1.wmd')
    kern = wmd_reader('kernel.wmd')
##
##    # knn
##    knn = wmd_reader('knn.wmd')
##
##
##
##    # moran workflow
##    import pysal as ps


    # geojson
    #wj = wmd_reader("wgeojson.wmd")


    # here we test chaining
#    r1 = wmd_reader('chain2inputs.wmd')
#    print "full metadata is listed below: \n", fullmeta
    # r2 = wmd_reader('chain2.wmd')

    taz_int = wmd_reader("taz_intersection.wmd")



    ## intersection between queen and block weights
    #import numpy as np
    #w = ps.lat2W(4,4)
    #block_variable = np.ones((w.n,1))
    #block_variable[:8] = 0
    #w_block = ps.weights.util.regime_weights(block_variable)

    #w_intersection = ps.w_intersection(w, w_block)


    ## with Columbus example using EW as the block and queen
    #dbf = ps.open("columbus.dbf")
    #ew = np.array(dbf.by_col("EW"))
    #dbf.close()
    #w_ew = ps.weights.util.regime_weights(ew)
    #wr = ps.rook_from_shapefile("columbus.shp")
    #w_int = ps.w_intersection(w_ew, wr)


    #blk = wmd_reader('block2.wmd')

    #taz_int = wmd_reader("http://spatial.csf.asu.edu/taz_intersection.wmd")


########NEW FILE########
__FILENAME__ = analytics
# Analytic functions/classes for network module

"""
 - Global Network Autocorrelation (gincs)
 - Global K-Functions
 - Local Indicators of Network-Constrained Clusters (lincs)
 - Local K-Functions
 - Network Kernels
 - Accessibility Indices
"""

import networkw
import pysal as ps

def gincs(wed, y, permutations=999, segment=False):
    if segment:
        # segment wed and y
        # get new wed and extract new y
        raise NotImplementedError

    w = networkw.w_links(wed)
    mi = ps.Moran(y, w, permutations=permutations)
    return mi

def lincs(wed, y, permutations=999, segment=False):
    if segment:
        # segment wed and y
        # get new wed and extract new y
        raise NotImplementedError

    w = networkw.w_links(wed)
    # lisa from PySAL
    lisa = ps.Moran_Local(y, w, permutations=permutations)
    return lisa


########NEW FILE########
__FILENAME__ = data
# Data structures for network module

__author__ = "Sergio Rey <sjsrey@gmail.com>, Jay Laura <jlaura@asu.edu>"

import operator
import math
import numpy as np
import pysal as ps
import util
import networkw


class WED(object):
    """Winged-Edge Data Structure


    """

    def __init__(self, edges=None, coords=None):

        self.start_c = None
        self.start_cc = None
        self.end_c = None
        self.end_cc = None
        self.region_edge = None
        self.node_edge = None
        self.right_polygon = None
        self.left_polygon = None
        self.start_node = None
        self.end_node = None
        self.node_coords = None
        self.edge_list = []
        self.node_list = []

        if edges is not None and coords is not None:
            #Check for single edges and double if needed
            edges = self.check_edges(edges)
            self.edge_list[:] = edges

            #Create the WED object:w

            self.extract_wed(edges, coords)

    def check_edges(self, edges):
        """
        Validator to ensure that edges are double.

        Parameters
        ----------
        edges: list
            edges connecting nodes in the network

        Returns
        -------
        dbl_edges / edges: list
            Either the original edges or double edges
        """

        seen = set()
        seen_add = seen.add
        seen_twice = set()
        for e in edges:
            if e in seen:
                seen_twice.add(e)
            seen_add(e)
            seen_add((e[1], e[0]))
        if len(list(seen_twice)) != len(edges) / 2:
            dbl_edges = []
            for e in edges:
                dbl_edges.append(e)
                dbl_edges.append((e[1], e[0]))
            return dbl_edges
        else:
            return edges

    def _filament_links_node(self, node, node_edge, start_c, end_c):
        """
        Private method that duplicates enum_links_around_node, but
         is callable before the WED is generated.  This is used
         for filament insertion.
        """
        links = []
        if node not in node_edge:
            return links
        l0 = node_edge[node]
        links.append(l0)
        l = l0
        v = node
        searching = True
        while searching:
            if v == l[0]:
                l = start_c[l]
            else:
                l = end_c[l]
            if (l is None) or (set(l) == set(l0)):
                searching = False
            else:
                links.append(l)
        return links

    def extract_wed(self, edges, coords):
        # helper functions to determine relative position of vectors
        def _dotproduct(v1, v2):
            return sum((a * b) for a, b in zip(v1, v2))

        def _length(v):
            return math.sqrt(_dotproduct(v, v))

        def _angle(v1, v2):
            return math.acos(_dotproduct(v1, v2) / (_length(v1) * _length(v2)))

        """
        Extract the Winged Edge Data structure for a planar graph


        Arguments
        ---------

        edges:  list
                tuples of origin, destination nodes for each edge

        coords: dict
                key is node id, value is a tuple of x,y coordinates for the node


        Returns
        -------
        wed: Dictionary holding the WED with 10 keys

            start_node: dict
                        key is node, value is edge with node as start node

            end_node:   dict
                        key is node, value is edge with node as end node

            right_polygon: dict
                            key is edge, value is id of right polygon to edge

            left_polygon: dict
                        key is edge, value is id of left polygon to edge

            node_edge: dict
                        key is node, value is edge associated with the node

            region_edge: dict
                        key is region, value is an edge on perimeter of region

            start_c:   dict
                        key is edge, value is first edge encountered when rotating
                        clockwise around edge start node

            start_cc:  dict
                        key is edge, value is first edge encountered when rotating
                        counterclockwise around edge start node

            end_c:     dict
                        key is edge, value is first edge encountered when rotating
                        clockwise around edge start end node

            end_cc:    dict
                        key is edge, value is first edge encountered when rotating
                        counterclockwise around edge start end node

        """

        # coords will be destroyed so keep a copy around
        coords_org = coords.copy()

        # find minimum cycles, filaments and isolated nodes
        pos = coords.values()
        mcb = self.regions_from_graph(coords, edges)

        regions = mcb['regions']
        edges = mcb['edges']
        start_node = {}
        end_node = {}
        for edge in edges:
            if edge[0] != edge[1]:  # no self-loops
                start_node[edge] = edge[0]
                end_node[edge] = edge[1]

        right_polygon = {}
        left_polygon = {}
        region_edge = {}
        start_c = {}
        start_cc = {}
        end_c = {}
        end_cc = {}
        node_edge = {}
        for ri, region in enumerate(regions):
            # regions are ccw in mcb
            region.reverse()
            r = [region[-2]]
            r.extend(region)
            r.append(region[1])
            for i in range(len(region) - 1):
                edge = r[i + 1], r[i + 2]
                if edge[0] not in node_edge:
                    node_edge[edge[0]] = edge
                if edge[1] not in node_edge:
                    node_edge[edge[1]] = edge
                start_c[edge] = r[i], r[i + 1]
                end_cc[edge] = r[i + 2], r[i + 3]
                right_polygon[edge] = ri
                twin = edge[1], edge[0]
                left_polygon[twin] = ri
                start_cc[twin] = end_cc[edge]
                end_c[twin] = start_c[edge]
            region_edge[ri] = edge

        rpkeys = right_polygon.keys()  # only minimum cycle regions have explicit right polygons
        noleft_poly = [k for k in rpkeys if k not in left_polygon]

        for edge in noleft_poly:
            left_polygon[edge] = ri + 1
        # Fill out s_c, s_cc, e_c, e_cc pointers for each edge (before filaments are added)
        regions = region_edge.keys()

        # Find the union of adjacent faces/regions
        unions = []
        while noleft_poly:
            path = []
            current = noleft_poly.pop()
            path_head = current[0]
            tail = current[1]
            path.append(current)
            while tail != path_head:
                candidates = [edge for edge in noleft_poly if edge[0] == tail]
                j = 0
                if len(candidates) > 1:
                    # we want candidate that forms largest ccw angle from current
                    angles = []
                    origin = pos[current[1]]
                    x0 = pos[current[0]][0] - origin[0]
                    y0 = pos[current[0]][1] - origin[1]
                    maxangle = 0.0
                    v0 = (x0, y0)

                    for i, candidate in enumerate(candidates):
                        x1 = pos[candidate[1]][0] - origin[0]
                        y1 = pos[candidate[1]][1] - origin[1]
                        v1 = (x1, y1)
                        v0_v1 = _angle(v0, v1)
                        if v0_v1 > maxangle:
                            maxangle = v0_v1
                            j = i
                        angles.append(v0_v1)

                next_edge = candidates[j]
                path.append(next_edge)
                noleft_poly.remove(next_edge)
                tail = next_edge[1]
            unions.append(path)

        for union in unions:
            for prev, edge in enumerate(union[1:-1]):
                start_cc[edge] = union[prev]
                end_c[edge] = union[prev + 2]
            start_cc[union[0]] = union[-1]
            end_c[union[0]] = union[1]
            end_c[union[-1]] = union[0]
            start_cc[union[-1]] = union[-2]

        regions = [set(region) for region in mcb['regions']]
        filaments = mcb['filaments']
        filament_region = {}
        for f, filament in enumerate(filaments):
            filament_region[f] = []
            # set up pointers on filament edges prior to insertion
            ecc, ec, scc, sc, node_edge = self.filament_pointers(filament, node_edge)
            end_cc.update(ecc)
            start_c.update(sc)
            start_cc.update(scc)
            end_c.update(ec)

            # find which regions the filament is incident to
            sf = set(filament)
            incident_nodes = set()
            incident_regions = set()
            for r, region in enumerate(regions):
                sfi = sf.intersection(region)
                while sfi:
                    incident_nodes.add(sfi.pop())
                    incident_regions.add(r)

            while incident_nodes:
                incident_node = incident_nodes.pop()
                incident_links = self._filament_links_node(incident_node, node_edge, start_c, end_c)

                #Polar coordinates centered on incident node, no rotation from x-axis
                origin = coords_org[incident_node]

                #Logic: If the filament has 2 nodes, grab the other one
                # If the filament has 3+, grab the first and last segments
                if filament.index(incident_node) == 0:
                    f = filament[1]
                elif filament.index(incident_node) == 1:
                    f = filament[0]
                else:
                    f = filament[-2]
                filament_end = coords_org[f]
                #print "Filament:{}, Incident_Node:{} ".format(f, incident_node)
                #Determine the relationship between the origin and the filament end
                filamentx = filament_end[0] - origin[0]
                filamenty = filament_end[1] - origin[1]
                filament_theta = math.atan2(filamenty, filamentx) * 180 / math.pi
                if filament_theta < 0:
                    filament_theta += 360
                #Find the rotation necessary to get the filament to theta 0
                f_rotation = 360 - filament_theta

                link_angles = {}
                for link in incident_links:
                    if link[0] == incident_node:
                        link_node = link[1]
                    else:
                        link_node = link[0]
                    #Get the end coord of the incident link
                    link_node_coords = coords_org[link_node]
                    y = link_node_coords[1] - origin[1]
                    x = link_node_coords[0] - origin[0]
                    r = math.sqrt(x**2 + y**2)
                    node_theta = math.atan2(y, x) * 180 / math.pi
                    if node_theta < 0:
                        node_theta += 360
                    #Rotate the edge node to match the new polar axis
                    node_theta += f_rotation
                    if node_theta > 360:
                        node_theta -= 360
                    link_angles[link] = node_theta

                #Get the bisected edges
                ccwise = min(link_angles, key=link_angles.get)
                cwise = max(link_angles, key=link_angles.get)
                #Fix the direction of the bisected edges
                if ccwise.index(incident_node) != 1:
                    ccwise = (ccwise[1], ccwise[0])
                if cwise.index(incident_node) != 1:
                    cwise = (cwise[1], cwise[0])
                #Update the filament pointer in the direction (segment end, incident node)
                end_c[(f, incident_node)] = (cwise[1], cwise[0])
                end_cc[(f, incident_node)] = (ccwise[1], ccwise[0])
                #Reverse the edge direction
                start_c[(incident_node, f)] = (tuple(cwise))
                start_cc[(incident_node, f)] = (tuple(ccwise))
                #Update the bisected edge points in the direction(segment end, incident node)
                #Cwise link
                end_cc[cwise] = (incident_node, f)
                start_cc[(cwise[1], cwise[0])] = (incident_node, f)
                #CCWise link
                start_c[(ccwise[1], ccwise[0])] = (incident_node, f)
                end_c[ccwise] = (incident_node, f)

                for r in incident_regions:
                    poly = ps.cg.Polygon([coords_org[v] for v in regions[r]])
                    if poly.contains_point((coords_org[filament[1]]) or poly.contains_point(coords_org[filament[0]])):
                        for n in range(len(filament)-1):
                            right_polygon[(filament[n], filament[n+1])] = r
                            left_polygon[(filament[n], filament[n+1])] = r
                            right_polygon[(filament[n+1], filament[n])] = r
                            left_polygon[(filament[n+1], filament[n])] = r

        #Fill in start_c and end_cc for external links
        for k, v in start_cc.iteritems():
            if k not in end_cc.keys():
                end_cc[k] = v
        for k, v in end_c.iteritems():
            if k not in start_c.keys():
                start_c[k] = v

        self.start_c = start_c
        self.start_cc = start_cc
        self.end_c = end_c
        self.end_cc = end_cc
        self.region_edge = region_edge
        self.node_edge = node_edge
        self.right_polygon = right_polygon
        self.left_polygon = left_polygon
        self.start_node = start_node
        self.end_node = end_node
        self.node_coords = coords_org
        self.node_list = [n for n in self.node_coords.keys()]

    @staticmethod
    def filament_pointers(filament, node_edge={}):
        """
        Define the edge pointers for a filament


        Arguments
        ---------

        filament:   list
                    ordered nodes defining a graph filament where a filament is
                    defined as a sequence of ordered nodes with at least one
                    internal node having incidence=2

        node_edge:  dict
                    key is a node, value is the edge the node is assigned to

        Returns
        -------

        ecc:    dict
                key is edge, value is first edge encountered when rotating
                counterclockwise around edge start end node

        ec:     dict
                key is edge, value is first edge encountered when rotating
                clockwise around edge start end node


        scc:    dict
                key is edge, value is first edge encountered when rotating
                counterclockwise around edge start node


        sc:     dict
                key is edge, value is first edge encountered when rotating
                clockwise around edge start node

        node_edge: dict
                key is a node, value is the edge the node is assigned to

        """

        nv = len(filament)
        ec = {}
        ecc = {}
        sc = {}
        scc = {}
        for i in range(nv - 2):
            s0 = filament[i]
            e0 = filament[i + 1]
            s1 = filament[i + 2]
            ecc[s0, e0] = e0, s1
            ecc[s1, e0] = e0, s0
            ec[s0, e0] = e0, s1
            sc[e0, s1] = s0, e0
            scc[e0, s1] = s0, e0
            if s0 not in node_edge:
                node_edge[s0] = s0, e0
            if e0 not in node_edge:
                node_edge[e0] = s0, e0
            if s1 not in node_edge:
                node_edge[s1] = e0, s1
        # wrapper pointers for first and last edges
        ecc[filament[-2], filament[-1]] = filament[-1], filament[-2]
        ec[filament[-2], filament[-1]] = filament[-2], filament[-1]
        ecc[filament[1], filament[0]] = filament[0], filament[1]
        ec[filament[1], filament[0]] = filament[1], filament[0]
        sc[filament[0], filament[1]] = filament[0], filament[1]
        # technically filaments have to have at least intermediate node with incidence 2
        # if there is a single edge it isn't a filament, but we handle it here just in case
        # since the "first" edge not be treated in the for loop (which isn't entered)
        if nv == 2:
            sc[filament[0], filament[1]] = filament[0], filament[1]
            ec[filament[0], filament[1]] = filament[0], filament[1]
            ecc[filament[0], filament[1]] = filament[1], filament[0]
            scc[filament[0], filament[1]] = filament[0], filament[1]
            if filament[0] not in node_edge:
                node_edge[filament[0]] = filament[0], filament[1]
            if filament[1] not in node_edge:
                node_edge[filament[1]] = filament[0], filament[1]
        return ecc, ec, scc, sc, node_edge

    @staticmethod
    def regions_from_graph(nodes, edges, remove_holes=False):
        """
        Extract regions from nodes and edges of a planar graph

        Arguments
        ---------

        nodes: dict
            vertex id as key, coordinates of vertex as value

        edges: list
            (head,tail), (tail, head) edges

        Returns
        ------

        regions: list
                lists of nodes defining a region. Includes the external region

        filaments:  list
                    lists of nodes defining filaments and isolated vertices



        Examples
        --------
        >>> vertices = {0: (1, 8), 1: (1, 7), 2: (4, 7), 3: (0, 4), 4: (5, 4), 5: (3, 5), 6: (2, 4.5), 7: (6.5, 9), 8: (6.2, 5), 9: (5.5, 3), 10: (7, 3), 11: (7.5, 7.25), 12: (8, 4), 13: (11.5, 7.25), 14: (9, 1), 15: (11, 3), 16: (12, 2), 17: (12, 5), 18: (13.5, 6), 19: (14, 7.25), 20: (16, 4), 21: (18, 8.5), 22: (16, 1), 23: (21, 1), 24: (21, 4), 25: (18, 3.5), 26: (17, 2), 27: (19, 2)}
        >>> edges = [(1, 2),(1, 3),(2, 1),(2, 4),(2, 7),(3, 1),(3, 4),(4, 2),(4, 3),(4, 5),(5, 4),(5, 6),(6, 5),(7, 2),(7, 11),(8, 9),(8, 10),(9, 8),(9, 10),(10, 8),(10, 9),(11, 7),(11, 12),(11, 13),(12, 11),(12, 13),(12, 20),(13, 11),(13, 12),(13, 18),(14, 15),(15, 14),(15, 16),(16, 15),(18, 13),(18, 19),(19, 18),(19, 20),(19, 21),(20, 12),(20, 19),(20, 21),(20, 22),(20, 24),(21, 19),(21, 20),(22, 20),(22, 23),(23, 22),(23, 24),(24, 20),(24, 23),(25, 26),(25, 27),(26, 25),(26, 27),(27, 25),(27, 26)]
        >>> r = WED.regions_from_graph(vertices, edges)
        >>> r['filaments']
        [[6, 5, 4], [2, 7, 11], [14, 15, 16]]
        >>> r['regions']
        [[3, 4, 2, 1, 3], [9, 10, 8, 9], [11, 12, 13, 11], [12, 20, 19, 18, 13, 12], [19, 20, 21, 19], [22, 23, 24, 20, 22], [26, 27, 25, 26]]

        Notes
        -----
        Based on
        Eberly http://www.geometrictools.com/Documentation/MinimalCycleBasis.pdf.
        """

        def adj_nodes(start_key, edges):
            """Finds all nodes adjacent to start_key.

            Parameters
            ----------
            start_key: int
                The id of the node to find neighbors of.

            edges: list
                All edges in the graph

            Returns
            -------
            vnext: list
                List of adjacent nodes.
            """
            start_key
            vnext = []
            for edge in edges:
                if edge[0] == start_key:
                    vnext.append(edge[1])
            if len(vnext) == 0:
                pass
                #print "Vertex is end point."
            return vnext

        def find_start_node(nodes, node_coord):
            start_node = []
            minx = float('inf')
            for key, node in nodes.items():
                if node[0] <= minx:
                    minx = node[0]
                    start_node.append(key)
            if len(start_node) > 1:
                miny = float('inf')
                for i in range(len(start_node)):
                    if nodes[i][1] < miny:
                        miny = nodes[i][1]
                    else:
                        start_node.remove(i)
            return nodes[start_node[0]], node_coord[nodes[start_node[0]]]

        def clockwise(nodes, vnext, start_key, v_prev, vertices=None):
            v_curr = np.asarray(nodes[start_key])
            v_next = None
            if v_prev is None:
                v_prev = np.asarray([0, -1])  #This should be a vertical tangent to the start node at initialization.
            else:
                pass
            d_curr = v_curr - v_prev

            for v_adj in vnext:
                #No backtracking
                if np.array_equal(np.asarray(nodes[v_adj]), v_prev) is True:
                    continue
                if type(v_prev) == int:
                    if v_adj == v_prev:
                        continue
                #The potential direction to move in
                d_adj = np.asarray(nodes[v_adj]) - v_curr
                #Select the first candidate
                if v_next is None:
                    v_next = np.asarray(nodes[v_adj])
                    d_next = d_adj
                    convex = d_next[0] * d_curr[1] - d_next[1] * d_curr[0]
                    if convex <= 0:
                        convex = True
                    else:
                        convex = False
                #Update if the next candidate is clockwise of the current clock-wise most
                if convex is True:
                    if (d_curr[0]*d_adj[1] - d_curr[1]*d_adj[0]) < 0 or (d_next[0]*d_adj[1]-d_next[1]*d_adj[0]) < 0:
                        v_next = np.asarray(nodes[v_adj])
                        d_next = d_adj
                        convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]
                        if convex <= 0:
                            convex = True
                        else:
                            convex = False
                else:
                    if (d_curr[0]*d_adj[1] - d_curr[1]*d_adj[0]) < 0 and (d_next[0]*d_adj[1]-d_next[1]*d_adj[0]) < 0:
                        v_next = np.asarray(nodes[v_adj])
                        d_next = d_adj
                        convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]
                        if convex <= 0:
                            convex = True
                        else:
                            convex = False
            prev_key = start_key
            if vertices == None:
                return tuple(v_next.tolist()), node_coord[tuple(v_next.tolist())], prev_key
            else:
                return tuple(v_next.tolist()), vertices[tuple(v_next.tolist())], prev_key

        def counterclockwise(nodes, vnexts, start_key, prev_key):
            v_next = None
            v_prev = np.asarray(nodes[prev_key])
            v_curr = np.asarray(nodes[start_key])
            d_curr = v_curr - v_prev

            for v_adj in vnexts:
                #Prohibit Back-tracking
                if v_adj == prev_key:
                    continue
                d_adj = np.asarray(nodes[v_adj]) - v_curr

                if v_next == None:
                    v_next = np.asarray(nodes[v_adj])
                    d_next = d_adj
                    convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]

                if convex <= 0:
                    if d_curr[0]*d_adj[1] - d_curr[1]*d_adj[0] > 0 and d_next[0]*d_adj[1] - d_next[1]*d_adj[0] > 0:
                        v_next = np.asarray(nodes[v_adj])
                        d_next = d_adj
                        convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]
                    else:
                        pass
                else:
                    if d_curr[0]*d_adj[1] - d_curr[1]*d_adj[0] > 0 or d_next[0]*d_adj[1]-d_next[1]*d_adj[0] > 0:
                        v_next = np.asarray(nodes[v_adj])
                        d_next = d_adj
                        convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]
                    else:
                        pass
            prev_key = start_key
            try:
                return tuple(v_next.tolist()), node_coord[tuple(v_next.tolist())], prev_key
            except:
                return v_next, None, prev_key
        def remove_edge(v0,v1,edges, ext_edges):
            try:
                ext_edges.append((v0,v1))
                ext_edges.append((v1,v0))
                edges.remove((v0,v1))
                edges.remove((v1,v0))
            except:
                pass
            return edges, ext_edges

        def remove_heap(v0,sorted_nodes):
            sorted_nodes[:] = [x for x in sorted_nodes if x[0] != v0]
            return sorted_nodes

        def remove_node(v0, nodes, nodes_coord, vertices):
            vertices[v0] = nodes[v0]
            del nodes_coord[nodes[v0]]
            del nodes[v0]
            return nodes, nodes_coord, vertices

        def extractisolated(nodes,node_coord,v0,primitives, vertices, ext_edges):
            primitives.append(v0)
            nodes, node_coord, vertices = remove_node(v0, nodes, node_coord, vertices)
            return nodes, node_coord, primitives, vertices, ext_edges

        def extractfilament(v0,v1, nodes, node_coord,sorted_nodes, edges, primitives,cycle_edge, vertices, ext_edges, iscycle=False):
            if (v0,v1) in cycle_edge or (v1,v0) in cycle_edge:
                iscycle = True
            if iscycle == True:
            #This deletes edges that are part of a cycle, but does not add them as primitives.
                if len(adj_nodes(v0,edges)) >= 3:
                    edges, ext_edges = remove_edge(v0,v1,edges, ext_edges)
                    v0 = v1
                    if len(adj_nodes(v0, edges)) == 1:
                        v1 = adj_nodes(v0, edges)[0]
                while len(adj_nodes(v0, edges)) == 1:
                    v1 = adj_nodes(v0, edges)[0]
                    #Here I need to do the cycle check again.
                    iscycle = False
                    if (v0,v1) in cycle_edge or (v1,v0) in cycle_edge:
                        iscycle = True

                    if iscycle == True:
                        edges, ext_edges = remove_edge(v0,v1,edges, ext_edges)
                        nodes, node_coord, vertices = remove_node(v0, nodes, node_coord, vertices)
                        sorted_nodes = remove_heap(v0, sorted_nodes)
                        v0 = v1
                    else:
                        break
                if len(adj_nodes(v0, edges)) == 0:

                    nodes, node_coord, vertices = remove_node(v0, nodes, node_coord, vertices)
                    sorted_nodes = remove_heap(v0, sorted_nodes)
            else:
                #Filament found
                primitive = []
                if len(adj_nodes(v0,edges)) >= 3:
                    primitive.append(v0)
                    edges, ext_edges = remove_edge(v0,v1,edges, ext_edges)
                    v0 = v1
                    if len(adj_nodes(v0, edges)) == 1:
                        v1 = adj_nodes(v0, edges)[0]

                while len(adj_nodes(v0, edges)) == 1:
                    primitive.append(v0)
                    v1 = adj_nodes(v0, edges)[0]
                    sorted_nodes = remove_heap(v0, sorted_nodes)
                    edges, ext_edges = remove_edge(v0, v1, edges, ext_edges)
                    nodes, node_coord, vertices = remove_node(v0, nodes, node_coord, vertices)
                    v0 = v1

                primitive.append(v0)
                if len(adj_nodes(v0, edges)) == 0:

                    sorted_nodes = remove_heap(v0, sorted_nodes)
                    edges, ext_edges = remove_edge(v0, v1, edges, ext_edges)
                    nodes, node_coord, vertices = remove_node(v0, nodes, node_coord, vertices)
                primitives.append((primitive))

            return sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges

        def extract_primitives(start_key,sorted_nodes, edges, nodes, node_coord, primitives,minimal_cycles,cycle_edge, vertices, ext_edges):
            v0 = start_key
            visited = []
            sequence = []
            sequence.append(v0)

            #Find the CWise most vertex
            vnext = adj_nodes(start_key, edges)
            start_node,v1,v_prev = clockwise(nodes,vnext,start_key,prev_key)
            v_curr = v1
            v_prev = v0
            #Find minimal cycle using CCWise rule
            process = True
            if v_curr == None:
                process = False
            elif v_curr == v0:
                process = False
            elif v_curr in visited:
                process = False

            while process == True:
                sequence.append(v_curr)
                visited.append(v_curr)
                vnext = adj_nodes(v_curr, edges)

                v_curr_coords,v_next,v_prev = counterclockwise(nodes,vnext,v_curr, v_prev)
                v_curr = v_next
                if v_curr == None:
                    process = False
                elif v_curr == v0:
                    process = False
                elif v_curr in visited:
                    process = False

            if v_curr is None:
                #Filament found, not necessarily at start_key
                sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges = extractfilament(v_prev, adj_nodes(v_prev, edges)[0],nodes, node_coord, sorted_nodes, edges, primitives, cycle_edge, vertices, ext_edges)

            elif v_curr == v0:
                #Minimal cycle found
                #primitive = []
                #iscycle=True
                sequence.append(v0)
                minimal_cycles.append(list(sequence))
                #Remove the v0, v1 edges from the graph.
                edges, ext_edges = remove_edge(v0,v1,edges, ext_edges)
                sorted_nodes = remove_heap(v0, sorted_nodes)#Not in pseudo-code, but in source.
                #Mark all the edges as being part of a minimal cycle.
                if len(adj_nodes(v0, edges)) == 1:
                    cycle_edge.append((v0, adj_nodes(v0, edges)[0]))
                    cycle_edge.append((adj_nodes(v0, edges)[0], v0))
                    sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges = extractfilament(v0, adj_nodes(v0, edges)[0],nodes, node_coord, sorted_nodes, edges, primitives,cycle_edge, vertices, ext_edges)
                if len(adj_nodes(v1, edges)) == 1:
                    cycle_edge.append((v1, adj_nodes(v1, edges)[0]))
                    cycle_edge.append((adj_nodes(v1, edges)[0],v1))
                    sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges = extractfilament(v1, adj_nodes(v1, edges)[0],nodes, node_coord, sorted_nodes, edges, primitives, cycle_edge, vertices, ext_edges)

                for i,v in enumerate(sequence[1:-1]):
                    cycle_edge.append((v,sequence[i]))
                    cycle_edge.append((sequence[i],v))

            else:
                #vcurr was visited earlier, so traverse the filament to find the end
                while len(adj_nodes(v0,edges)) == 2:
                    if adj_nodes(v0,edges)[0] != v1:
                        v1 = v0
                        v0 = adj_nodes(v0,edges)[0]
                    else:
                        v1 = v0
                        v0 = adj_nodes(v0, edges)[1]
                sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges = extractfilament(v0,v1,nodes, node_coord, sorted_nodes, edges, primitives,cycle_edge,vertices,ext_edges)

            return sorted_nodes, edges, nodes, node_coord, primitives, minimal_cycles,cycle_edge, vertices, ext_edges
        #1.
        sorted_nodes = sorted(nodes.iteritems(), key=operator.itemgetter(1))
        node_coord = dict (zip(nodes.values(),nodes.keys()))

        #2.
        primitives = []
        minimal_cycles = []
        cycle_edge = []
        prev_key = None #This is only true for the first iteration.
        #This handles edge and node deletion we need populated later.
        vertices = {}
        ext_edges = []

        #3.
        while sorted_nodes: #Iterate through the sorted list
            start_key = sorted_nodes[0][0]
            numadj = len(adj_nodes(start_key, edges))

            if numadj == 0:
                nodes, node_coord, primitives, vertices, ext_edges = extractisolated(nodes,node_coord,start_key,primitives, vertices, ext_edges)
                sorted_nodes.pop(0)
            elif numadj == 1:
                sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges = extractfilament(start_key, adj_nodes(start_key, edges)[0],nodes, node_coord, sorted_nodes, edges,primitives,cycle_edge, vertices, ext_edges)
            else:
                sorted_nodes, edges, nodes, node_coord, primitives, minimal_cycles,cycle_edge, vertices, ext_edges = extract_primitives(start_key,sorted_nodes, edges, nodes, node_coord, primitives, minimal_cycles,cycle_edge, vertices, ext_edges)

        #4. Remove holes from the graph
        if remove_holes == True:
            polys = []
            for cycle in minimal_cycles:
                polys.append(ps.cg.Polygon([ps.cg.Point(vertices[pnt]) for pnt in cycle]))

            pl = ps.cg.PolygonLocator(polys)

            # find all overlapping polygon mbrs
            overlaps ={}
            nump = len(minimal_cycles)
            for i in range(nump):
                overlaps[i] = pl.overlapping(polys[i].bounding_box)

            # for overlapping mbrs (left,right) check if right polygon is contained in left
            holes = []
            for k in overlaps:
                for  pc in overlaps[k]:
                    s = sum( [polys[k].contains_point(v) for v in pc.vertices])
                    if s == len(pc.vertices):
                        # print k, pc
                        holes.append((k,pc))

            for hole in holes:
                outer, inner = hole
                inner = polys.index(inner)
                minimal_cycles.pop(inner)

        #5. Remove isolated vertices
        filaments = []
        for index, primitive in enumerate(primitives):
            if type(primitive) == list:
                filaments.append(primitive)

        results = {}
        results['regions'] = minimal_cycles
        results['filaments'] = filaments
        results['vertices'] = vertices
        results['edges'] = ext_edges
        results['nodes'] = vertices
        return results

    def enum_links_node(self, node):
        return util.enum_links_node(self, node)

    def enum_edges_region(self, region):
        return util.enum_edges_region(self, region)

    def edge_length(self):
        return util.edge_length(self)

    def w_links(self):
        return networkw.w_links(self)

########NEW FILE########
__FILENAME__ = dcel
"""
doubly connected edge list 

representation for network algorithms
"""


# example of edges from de berg fig 2.6

import  pysal as ps

import networkx as nx


class Vertex:
    """ """
    def __init__(self, coordinates, incident_edge):
        self.coordinates = coordinates
        incident_edge = incident_edge

class Face:
    """ """
    def __init__(self, outer_component=None, inner_component=None):

        self.outer_component = outer_component
        self.inner_component = inner_component

class Half_Edge:
    """ """
    def __init__(self, origin, twin, incident_face, Next, Prev):
        self.origin = origin
        self.twin = twin
        self.incident_face = incident_face
        self.Next = Next
        self.Prev = Prev


class DCEL:
    """Doubly connected edge list"""
    def __init__(self, graph):

        edges = {}
        vertices = {}
        faces = {}
        half_edges = {}

        cycles = nx.cycle_basis(graph)
        fi = 0
        for cycle in cycles:
            n = len(cycle)
            for i in range(n-1):
                e = (cycle[i], cycle[i+1])
                if e not in edges:
                    edges[e] = fi
                    twin_a = e[0], e[1]
                    twin_b = e[1], e[0]
                    if twin_a not in half_edges:
                        half_edges[twin_a] = fi
                    if twin_b not in half_edges:
                        half_edges[twin_b] = None
            e = cycle[n-1], cycle[0]
            if e not in edges:
                edges[e] = fi
            faces[fi] = e


            fi += 1

        self.edges = edges
        self.faces = faces
        self.half_edges = half_edges


if __name__ == '__main__':


    p1 = [
            [1,12],
            [6,12],
            [11,11],
            [14,13],
            [19,14],
            [22,9],
            [20,5],
            [16,0],
            [11,2],
            [5,1],
            [0,7],
            [2,9],
            [1,12]]

    h1 = [
            [3,7],
            [5,5],
            [8,5],
            [5,8],
            [3,7]
            ]

    h2 = [
            [4,10],
            [5,8],
            [8,5],
            [9,8],
            [4,10]
            ]

    h3 = [
            [12,6],
            [15,4],
            [18,5],
            [19,7],
            [17,9],
            [14,9],
            [12,6]
            ]
    # note that h1 union h2 forms a single hole in p1

    faces = [p1, h1, h2, h3]
    G = nx.Graph()

    for face in faces:
        n = len(face)
        for i in range(n-1):
            G.add_edge(tuple(face[i]), tuple(face[i+1]))


    cycles = nx.cycle_basis(G)
    # len of cycles is equal to the number of faces (not including external face

    # find cycles that share a vertex
    node2cycle = {}
    multi_nodes = set()
    for i,cycle in enumerate(cycles):
        for node in cycle:
            if node in node2cycle:
                node2cycle[node].append(i)
                multi_nodes.add(node)
            else:
                node2cycle[node] = [i]

    

    # check if there are nodes belonging to multiple cycles
    if multi_nodes:
        
        # put nodes for each edge in lexicographic order
        edges = [ sorted(edge) for edge in G.edges()]



########NEW FILE########
__FILENAME__ = fileio
# IO handlers for network module go here

import cPickle
import json
import ast
import os
import pysal as ps
from data import WED


def wed_to_json(wed, outfile, binary=True):
    #keys need to be strings
    new_wed = {}
    for key, value in vars(wed).iteritems():
        nested_attr = {}
        if isinstance(value, dict):
            for k2, v2 in value.iteritems():
                nested_attr[str(k2)] = v2
            new_wed[key] = nested_attr
        else:
            new_wed[key] = value
    #print new_wed['edge_list']
    if binary:
        with open(outfile, 'w') as outfile:
            outfile.write(cPickle.dumps(new_wed, 1))
    else:
        with open(outfile, 'w') as outfile:
            json_str = json.dumps(new_wed, sort_keys=True, indent=4)
            outfile.write(json_str)


def wed_from_json(infile, binary=True):
    wed = WED()
    if binary:
        with open(infile, 'r') as f:
            data = cPickle.load(f)
    else:
        with open(infile, 'r') as f:
            data = json.loads(f)

    wed.start_c = {ast.literal_eval(key):value for key, value in data['start_c'].iteritems()}
    wed.start_cc = {ast.literal_eval(key):value for key, value in data['start_cc'].iteritems()}
    wed.end_c = {ast.literal_eval(key):value for key, value in data['end_c'].iteritems()}
    wed.end_cc = {ast.literal_eval(key):value for key, value in data['end_cc'].iteritems()}
    wed.region_edge = {ast.literal_eval(key):value for key, value in data['region_edge'].iteritems()}
    wed.node_edge = {ast.literal_eval(key):value for key, value in data['node_edge'].iteritems()}
    wed.right_polygon = {ast.literal_eval(key):value for key, value in data['right_polygon'].iteritems()}
    wed.left_polygon = {ast.literal_eval(key):value for key, value in data['left_polygon'].iteritems()}
    wed.start_node = {ast.literal_eval(key):value for key, value in data['start_node'].iteritems()}
    wed.end_node = {ast.literal_eval(key):value for key, value in data['end_node'].iteritems()}
    wed.node_coords = {ast.literal_eval(key):value for key, value in data['node_coords'].iteritems()}
    wed.edge_list = data['edge_list']

    return wed

def reader(shp_file_name, doubleEdges=True):
    """
    Read a PySAL network (geographic graph) shapefile and create edges and
    coordinates data structures


    Parameters
    ----------

    shp_file_name: Path to shapefile with .shp extension. Has to have been
    created by contrib/spatialnet/

    doubleEdges:  Boolean if True create a twin for each edge

    Returns
    -------

    coords: dict with key a node id and the value a pair of x,y coordinates
    for the node's embedding in the plane

    edges: list of edges (t,f) where t and f are ids of the nodes
    """


    dir_name = os.path.dirname(shp_file_name)
    base_name = os.path.basename(shp_file_name)
    pre,suf = base_name.split(".")
    shp_file = os.path.join(dir_name,pre+".shp")
    dbf_file = os.path.join(dir_name,pre+".dbf")
    sf = ps.open(shp_file)
    df = ps.open(dbf_file)
    edges = []
    coords = {}
    records = df.read()
    df.close()
    for record in records:
        t = record[0]
        f = record[1]
        edges.append((t,f))
    df.close()
    i = 0
    shps = sf.read()
    sf.close()
    for shp in shps:
        t_xy, f_xy = shp.vertices
        t = edges[i][0]
        f = edges[i][1]
        if t not in coords:
            coords[t] = t_xy
        if f not in coords:
            coords[f] = f_xy
        i += 1

    if doubleEdges:
        for edge in edges:
            twin = edge[1],edge[0]
            if twin not in edges:
                edges.append(twin)
    return coords, edges


########NEW FILE########
__FILENAME__ = networkw
"""
Weights for PySAL Network Module
"""

__author__ = "Sergio Rey <sjsrey@gmail.com>, Jay Laura <jlaura@asu.edu>"

from itertools import combinations
import numpy as np
import pysal as ps
from util import threshold_distance, edge_length, knn_distance


def w_links(wed):
    """
    Generate Weights object for links in a WED

    Parameters
    ----------
    wed: PySAL Winged Edged Data Structure

    Returns

    ps.W(neighbors): PySAL Weights Dict
    """
    nodes = wed.node_edge.keys()
    neighbors = {}
    for node in nodes:
        lnks = wed.enum_links_node(node)
        # put i,j s.t. i < j
        lnks = [tuple(sorted(lnk)) for lnk in lnks]
        for comb in combinations(range(len(lnks)), 2):
            l, r = comb
            if lnks[l] not in neighbors:
                neighbors[lnks[l]] = []
            neighbors[lnks[l]].append(lnks[r])
            if lnks[r] not in neighbors:
                neighbors[lnks[r]] = []
            neighbors[lnks[r]].append(lnks[l])
    return ps.W(neighbors)


def w_distance(wed, threshold, cost=None, alpha=-1.0, binary=True, ids=None):
    '''
    Generate a Weights object based on a threshold
     distance using a WED

    Parameters
    ----------
    wed: PySAL Winged Edged Data Structure
    distance: float network threshold distance for neighbor membership
    cost: defaults to length, can be any cost dicationary {(edge): cost}

    Returns
    -------
    ps.W(neighbors): PySAL Weights Dict
    '''

    if cost is None:
        cost = edge_length(wed)
    if ids:
        ids = np.array(ids)
    else:
        ids = np.arange(len(wed.node_list))
    neighbors = {}
    if binary is True:
        for node in wed.node_list:
            near, pred = threshold_distance(wed, cost, node, threshold)
            neighbors[ids[node]] = near
        return ps.W(neighbors, None, ids)
    elif binary is False:
        weights = {}
        for node in wed.node_list:
            wt = []
            near, pred = threshold_distance(wed, cost, node, threshold)
            near.remove(node)
            neighbors[ids[node]] = near
            for end in near:
                path = [end]
                previous = pred[end]
                while previous != node:
                    path.append(previous)
                    end = previous
                    previous = pred[end]
                path.append(node)
                cum_cost = 0
                for p in range(len(path) - 1):
                    cum_cost += cost[(path[p], path[p + 1])]
                wt.append(cum_cost ** alpha)
            weights[ids[node]] = wt
        return ps.W(neighbors, weights, ids)


def w_knn(wed, n, cost=None, ids=None):
    '''
    Generate w Weights object based on the k-nearest
     network neighbors.

    Parameters
    ----------
    wed: PySAL Winged Edged Data Structure
    n: integer number of neighbors for each node
    cost: defaults to length, can be any cost dictionary

    Returns
    -------
    ps.W(neighbors): PySAL Weights Dict
    '''

    if cost is None:
        cost = edge_length(wed)
    if ids:
        ids = np.array(ids)
    else:
        ids = np.arange(len(wed.node_list))
    neighbors = {}
    for node in wed.node_list:
        neighbors[node] = knn_distance(wed, cost, node, n=n)
    return ps.W(neighbors, id_order=ids)

########NEW FILE########
__FILENAME__ = net_shp_io
"""
Reader and writer for PySAL network shapefiles
"""


import pysal as ps
import os

def reader(shp_file_name, doubleEdges=True):
    """
    Read a PySAL network (geographic graph) shapefile and create edges and
    coordinates data structures


    Parameters
    ----------

    shp_file_name: Path to shapefile with .shp extension. Has to have been
    created by contrib/spatialnet/

    doubleEdges:  Boolean if True create a twin for each edge



    Returns
    -------

    coords: dict with key a node id and the value a pair of x,y coordinates
    for the node's embedding in the plane

    edges: list of edges (t,f) where t and f are ids of the nodes
    """


    dir_name = os.path.dirname(shp_file_name)
    base_name = os.path.basename(shp_file_name)
    pre,suf = base_name.split(".")
    shp_file = os.path.join(dir_name,pre+".shp")
    dbf_file = os.path.join(dir_name,pre+".dbf")
    sf = ps.open(shp_file)
    df = ps.open(dbf_file)
    edges = []
    coords = {}
    records = df.read()
    df.close()
    for record in records:
        t = record[0]
        f = record[1]
        edges.append((t,f))
    df.close()
    i = 0
    shps = sf.read()
    sf.close()
    for shp in shps:
        t_xy, f_xy = shp.vertices
        t = edges[i][0]
        f = edges[i][1]
        if t not in coords:
            coords[t] = t_xy
        if f not in coords:
            coords[f] = f_xy
        i += 1

    if doubleEdges:
        for edge in edges:
            twin = edge[1],edge[0]
            if twin not in edges:
                edges.append(twin)
    return coords, edges

    

if __name__ == '__main__':

    file_name = "../contrib/spatialnet/eberly_net.shp"
    coords, edges = reader(file_name)
    coords1, edges1 = reader(file_name, doubleEdges=False)







########NEW FILE########
__FILENAME__ = shp2graph

# each polygon chain is converted to a series of connected edges

import pysal as ps
import numpy as np
shpf = ps.open(ps.examples.get_path("geodanet/streets.shp"))
shps = []
for shp in shpf:
    shps.append(shp)
shpf.close()

edge2id = {}
id2edge = {}

id2coord = {}
coord2id = {}
n_edges = n_nodes = 0
for i,shp in enumerate(shps):
    print i, len(shp.vertices)
    print shp.vertices
    for j in range(1, len(shp.vertices)):
        o = shp.vertices[j-1]
        d = shp.vertices[j]
        print o,d
        #raw_input('here')
        if o not in coord2id:
            id2coord[n_nodes] = o
            coord2id[o] = n_nodes
            n_nodes += 1
        if d not in coord2id:
            id2coord[n_nodes] = d
            coord2id[d] = n_nodes
            did = n_nodes
            n_nodes+=1

        oid = coord2id[o]
        did = coord2id[d]
        edge = tuple(np.sort((oid,did)))
        if edge not in edge2id:
            edge2id[edge] = n_edges
            n_edges+=1
        id2edge[edge2id[edge]] = edge
    print o,d

coords = id2coord
edges = id2edge.values()


# this will replace createSpatialNetworkShapefile in contrib/spatialnet

shp_out = ps.open("streets_net.shp", 'w')
dbf_out = ps.open("streets_net.dbf", 'w')
dbf_out.header = ["FNODE","TNODE","ONEWAY"]
dbf_out.field_spec = [('N',20,0),('N',20,0),('L',1,0)]
ids = id2coord.keys()
ids.sort()
for edge in edges:
    o = coords[edge[0]]
    d = coords[edge[1]]
    feat = ps.cg.Chain([o,d])
    rec = (edge[0],edge[1],False)
    dbf_out.write(rec)
    shp_out.write(feat)
dbf_out.close()
shp_out.close()

import net_shp_io
file_name = "streets_net.shp"
coords, edges = net_shp_io.reader(file_name)
coords1, edges1 = net_shp_io.reader(file_name, doubleEdges=False)


import wed

wed_streets = wed.WED(edges, coords)
#wed1_streets = wed.extract_wed(edges1, coords1)

regions = wed_streets.region_edge.keys()





########NEW FILE########
__FILENAME__ = test_wed
import unittest
from pysal.network.data import WED
import pysal.network.net_shp_io as net_shp_io

import pysal as ps


class TestWedOrdered(unittest.TestCase):

    def setUp(self):
        # Generate the test graph

        # from eberly http: //www.geometrictools.com/Documentation/MinimalCycleBasis.pdf
        self.coords = {0: (1, 8), 1: (1, 7), 2: (4, 7), 3: (0, 4), 4: (5, 4), 5: (3, 5),
                       6: (2, 4.5), 7: (6.5, 9), 8: (6.2, 5), 9: (5.5, 3), 10: (7, 3),
                       11: (7.5, 7.25), 12: (8, 4), 13: (11.5, 7.25), 14: (9, 1),
                       15: (11, 3), 16: (12, 2), 17: (12, 5), 18: (13.5, 6),
                       19: (14, 7.25), 20: (16, 4), 21: (18, 8.5), 22: (16, 1),
                       23: (21, 1), 24: (21, 4), 25: (18, 3.5), 26: (17, 2),
                       27: (19, 2)}
        # adjacency lists
        vertices = {}
        for v in range(28):
            vertices[v] = []

        vertices[1] = [2, 3]
        vertices[2] = [1, 4, 7]
        vertices[3] = [1, 4]
        vertices[4] = [2, 3, 5]
        vertices[5] = [4, 6]
        vertices[6] = [5]
        vertices[7] = [2, 11]
        vertices[8] = [9, 10]
        vertices[9] = [8, 10]
        vertices[10] = [8, 9]
        vertices[11] = [7, 12, 13]
        vertices[12] = [11, 13, 20]
        vertices[13] = [11, 12, 18]
        vertices[14] = [15]
        vertices[15] = [14, 16]
        vertices[16] = [15]
        vertices[18] = [13, 19]
        vertices[19] = [18, 20, 21]
        vertices[20] = [12, 19, 21, 22, 24]
        vertices[21] = [19, 20]
        vertices[22] = [20, 23]
        vertices[23] = [22, 24]
        vertices[24] = [20, 23]
        vertices[25] = [26, 27]
        vertices[26] = [25, 27]
        vertices[27] = [25, 26]

        self.edges = []
        for vert in vertices:
            for dest in vertices[vert]:
                self.edges.append((vert, dest))

    def test_wed(self):
        wed = WED(self.edges, self.coords)
        self.assertEqual(wed.enum_edges_region(0),
                        [(4, 3), (3, 1), (1, 2), (2, 4), (4, 5),
                            (5, 6), (6, 5), (5, 4), (4, 3)])
        self.assertEqual(wed.enum_links_node(20),
                        [(19, 20), (21, 20), (20, 24), (22, 20), (20, 12)])


class TestWedUnordered(unittest.TestCase):

    def setUp(self):
        self.coords = {0: (0.0, 4.0), 1: (1.0, 7.0), 2: (2.0, 4.5), 3: (3.0, 5.0), 4: (4.0, 7.0),
                       5: (5.0, 4.0), 6: (5.5, 3.0), 7: (6.2, 5.0), 8: (6.5, 9.0), 9: (7.0, 3.0),
                       10: (7.5, 7.25), 11: (8.0, 4.0), 12: (9.0, 1.0), 13: (11.0, 3.0), 14: (11.5, 7.25),
                       15: (12.0, 2.0), 16: (13.5, 6.0), 17: (14.0, 7.25), 18: (16.0, 1.0),
                       19: (16.0, 4.0), 20: (17.0, 2.0), 21: (18.0, 3.5), 22: (18.0, 8.5),
                       23: (19.0, 2.0), 24: (21.0, 1.0), 25: (21.0, 4.0)}

        self.edges = [(1, 0), (4, 1), (4, 5), (4, 8), (0, 5), (5, 3), (3, 2),
                      (8, 10), (7, 6), (7, 9), (6, 9), (10, 11), (10, 14),
                      (11, 14), (11, 19), (14, 16), (12, 13), (13, 15),
                      (16, 17), (17, 19), (17, 22), (19, 22), (19, 18),
                      (19, 25), (18, 24), (24, 25), (21, 20), (21, 23),
                      (20, 23)]

    def test_wed(self):
        wed = WED(self.edges, self.coords)
        self.assertEqual(wed.enum_edges_region(3),
                         [(19, 11), (11, 14), (14, 16), (16, 17), (17, 19), (19, 11)])
        self.assertEqual(wed.enum_links_node(19),
                         [(17, 19), (22, 19), (19, 25), (18, 19), (19, 11)])


class TestWedIOReader(unittest.TestCase):

    def setUp(self):
        self.coords, self.edges = net_shp_io.reader(ps.examples.get_path('eberly_net.shp'))

    def test_wed(self):
        wed = WED(self.edges, self.coords)
        self.assertEqual(wed.enum_edges_region(0),
                         [(5, 0), (0, 1), (1, 4), (4, 5), (5, 3), (3, 2), (2, 3), (3, 5), (5, 0)])
        self.assertEqual(wed.enum_links_node(19),
                         [(17, 19), (22, 19), (19, 25), (18, 19), (19, 11)])

        self.assertEqual(wed.w_links().histogram, [(1, 3), (2, 9), (3, 8), (4, 4), (5, 3), (6, 2)])
if __name__ == '__main__':
  unittest.main()

########NEW FILE########
__FILENAME__ = test_wed
# set up okabe wed for eberly graph

"""
to do

 - add in holes
 - add in isolated cycles/regions
 - add in end node filaments

 Currently working for enumeration around nodes and around polygons for other

 cases
"""

import pysal as ps
import numpy as np

nodes = range(27)

# put polygons in cw order
# region will be to the right of these edges
r0 = [1,2,4,3,1]
r1 = [11,13,12,11]
r2 = [12,13,18,19,20,12]
r3 = [19,21,20,19]
r4 = [20,24,23,22,20]
# external polygon
r_1 = [ 1,3, 4, 2,7, 11,12,20,22,23,24,20,21,19,18, 13, 11,7, 2,1]

regions = [r0, r1, r2, r3, r4, r_1]
left_region = {}
right_region = {}
edges = {}
region_edge = {}
start_c = {}
end_c = {}
start_cc = {}
end_cc = {}

node_edge = {}

for region in regions:
    r = [region[-2]]
    r.extend(region)
    r.append(region[1])
    for i in range(len(region)-1):
        edge = r[i+1],r[i+2]
        if edge[0] not in node_edge:
            node_edge[edge[0]] = edge
        if edge[1] not in node_edge:
            node_edge[edge[1]] = edge
        s_c = r[i],r[i+1]
        e_cc = r[i+2],r[i+3]
        right_region[edge] = region
        region_edge[tuple(region)] = edge
        start_c[edge] = s_c
        end_cc[edge] = e_cc

        left_region[edge[1],edge[0]] = region
        start_cc[edge[1], edge[0] ] = end_cc[edge]
        end_c[edge[1], edge[0] ] = start_c[edge]

        edges[edge] = edge

wed = {}
wed['node_edge'] = node_edge
wed['end_c'] = end_c
wed['start_c'] = start_c
wed['start_cc'] = start_cc
wed['end_cc'] = end_cc
wed['edges'] = edges
wed['region_edge'] = region_edge
wed['right_region'] = right_region
wed['left_region'] = left_region

def enum_links_node(wed,v0):
    links = []
    l0 = wed['node_edge'][v0]
    links.append(l0)
    l = l0
    v = v0

    searching = True
    while searching:
        if v == l[0]:
            l = wed['start_c'][l]
        else:
            l = wed['end_c'][l]
        if (l is None) or (set(l) == set(l0)):
            searching = False 
        else:
            links.append(l)
            
    return links

def enum_links_region(wed,region):
    l0 = wed['region_edge'][tuple(region)]
    links = []
    l = l0
    links.append(l)
    searching = True
    while searching:
        if wed['right_region'][l] == region:
            l = wed['end_cc'][l]
        else:
            l = wed['start_cc'][l]
        if l == l0:
            searching = False
        else:
            links.append(l)
    return links

print enum_links_node(wed,4)


# handle internal filament with end node
print 'before'
print 'enum around node 4', enum_links_node(wed,4)
print 'enum around region 0', enum_links_region(wed,r0)

# make local adjustments
# new edges first
wed['edges'][4,5] = 4,5
wed['edges'][5,6] = 5,6
wed['edges'][6,5] = 6,5
wed['node_edge'][5] = 4,5
wed['node_edge'][6] = 5,6
wed['right_region'][4,5] = r0
wed['left_region'][4,5] = r0

wed['start_c'][4,5] = 2,4
wed['end_cc'][4,5] = 5,6
wed['start_cc'][4,5] = 4,3
wed['end_c'][4,5] = 5,6

wed['start_c'][5,6] = 4,5
wed['end_cc'][5,6] = 5,6
wed['start_cc'][5,6] = 4,5
wed['end_c'][5,6] = 5,6


# need these to pick up 4,5 when enumerating edges around node 4
wed['start_cc'][4,2] = 4,5
wed['end_c'][3,4] = 4,5

# as long as end_cc pointers for non-filament edges defining the region are
# not modified due to insertion of an end-node-filament, traversing around the
# edges of a region works

print 'after internal end-node filament'
print 'enum around node 4', enum_links_node(wed,4)
print 'enum around region 0', enum_links_region(wed,r0)


# now try an end-point filament that is external, but linked to a region


print 'before external end-node-filament'
print 'enum around node 3', enum_links_node(wed,3)
print 'enum around region 0', enum_links_region(wed,r0)

wed['edges'][3,28] = 3,28
wed['edges'][28,29] = 28,29
wed['node_edge'][28] = 3,28
wed['node_edge'][29] = 28,29
wed['right_region'][28,29] = r_1
wed['right_region'][3,28] = r_1
wed['left_region'][28,29] = r_1
wed['left_region'][3,28] = r_1

wed['start_cc'][3,28] = 3,4
wed['end_c'][3,28] = 28,29
wed['start_c'][3,28] = 1,3
wed['end_cc'][3,28]= 28,29


wed['start_c'][28,29] = 3,28 
wed['end_cc'][28,29]= 28,29
wed['start_cc'][28,29] = 3,28
wed['end_c'][28,29] = 28,29

wed['start_c'][4,3] = 3,28
wed['end_c'][4,3] = 3,28



print 'after external end-node filament'
print 'enum around node 3', enum_links_node(wed,3)
print 'enum around region 0', enum_links_region(wed,r0)


print 'enum links around 28: ', enum_links_node(wed,28)
print 'enum links around 29: ', enum_links_node(wed,29)


# adding isolated cases and holes for connected component checks
r6 = [8,10,9,8]
node_edge[8]= 9,8
node_edge[9]= 9,8
node_edge[10]= 8,10
right_region[9,8] = r6
right_region[8,10] = r6
right_region[10,9] = r6
left_region[8,9] = r6
left_region[10,8] = r6
left_region[9,10] = r6
region_edge[tuple(r6)] = 8,9


end_cc[9,8] = 8,10
start_c[9,8] = 10,9

end_cc[10,9] = 10,9
start_c[10,9] = 8,10


end_cc[8, 10] = 10,9
start_c[8,10] = 9,8


end_c[9,8] = 8,10
start_cc[9,8] = 10,9

end_c[8,10] = 10,9
start_cc[8,10] = 9,8

end_c[10,9] = 9,8
start_cc[10,9] = 8,10

wed['node_edge'] = node_edge
wed['right_region'] = right_region
wed['left_region'] = left_region
wed['end_cc'] = end_cc
wed['start_c'] = start_c



#enum_links_node(wed,9)

r5 = [25,27,26,25] # hole

wed['region_edge'][tuple(r5)] = 26,25
wed['node_edge'][25] = 26,25
wed['node_edge'][26] = 26,25
wed['node_edge'][27] = 25,27
wed['right_region'][26,25] = r5
wed['right_region'][25,27] = r5
wed['right_region'][27,26] = r5
wed['start_c'][26,25] = 27,26
wed['start_cc'][26,25] = 27,26
wed['end_cc'][26,25] = 25,27
wed['end_c'][26,25] = 25,27
wed['start_c'][25,27] = 26,25
wed['start_cc'][25,27] = 26,25
wed['end_cc'][25,27] = 27,26
wed['end_c'][25,27] = 27,26
wed['start_c'][27,26] = 25,27
wed['start_cc'][27,26] = 25,27
wed['end_cc'][27,26] = 26,25
wed['end_c'][27,26] = 26,25


# now the isolated filament
wed['end_cc'][14,15] = 15,16
wed['end_c'][14,15] = 15,16
wed['start_c'][14,15] = 14,15
wed['start_c'][15,16] = 14,15
wed['end_c'][15,16] = 15,16
wed['node_edge'][14] = 14,15
wed['node_edge'][15] = 15,16
wed['node_edge'][16] = 15,16

# last case - isolated nodes
wed['node_edge'][0] = None
wed['node_edge'][17] = None


def connected_components(wed):
    """
    Find all connected components in a WED

    """

    nodes = wed['node_edge'].keys()
    components = []
    while nodes:
        start = nodes.pop()
        component = connected_component(wed, start)
        if len(component) > 1:
            for node in component:
                if node in nodes:
                    nodes.remove(node)
        components.append(component)
    return components


def connected_component(wed,start_node):
    """
    Find connected component containing start_node
    """

    if not wed['node_edge'][start_node]:
        return [start_node]
    stack = [start_node]
    children = enum_links_node(wed, start_node)
    A = {}
    A[start_node] = set()
    visited =  []
    for child in children:
        if child[0] == start_node:
            A[start_node].add(child[1])
        else:
            A[start_node].add(child[0])
    searching = True
    visited.append(start_node)
    while searching:
        current = stack[-1]
        if current not in A:
            children = enum_links_node(wed, current)
            A[current] = set()
            for child in children:
                if child[0] == current:
                    A[current].add(child[1])
                else:
                    A[current].add(child[0])
        else:
            if A[current]:
                child = A[current].pop()
                if child not in visited:
                    visited.append(child)
                    stack.append(child)
            else:  # current has no more children
                stack.remove(current)
                if not stack:
                    searching = False



    return visited

########NEW FILE########
__FILENAME__ = test_wed2
import pysal as ps
import numpy as np
import operator

def adj_nodes(start_key, edges):
    start_key
    vnext = []
    for edge in edges:
        if edge[0] == start_key:
            vnext.append(edge[1])
    if len(vnext) == 0:
        pass
        #print "Vertex is end point."
    return vnext

def regions_from_graph(nodes, edges, remove_holes = False):
    """
    Extract regions from nodes and edges of a planar graph

    Arguments
    ---------

    nodes: dictionary with vertex id as key, coordinates of vertex as value

    edges: list of (head,tail), (tail, head) edges

    Returns
    ------

    regions: list of lists of nodes defining a region. Includes the external
    region
    filaments: list of lists of nodes defining filaments and isolated
    vertices



    Examples
    --------
    >>> vertices = {0: (1, 8), 1: (1, 7), 2: (4, 7), 3: (0, 4), 4: (5, 4), 5: (3, 5), 6: (2, 4.5), 7: (6.5, 9), 8: (6.2, 5), 9: (5.5, 3), 10: (7, 3), 11: (7.5, 7.25), 12: (8, 4), 13: (11.5, 7.25), 14: (9, 1), 15: (11, 3), 16: (12, 2), 17: (12, 5), 18: (13.5, 6), 19: (14, 7.25), 20: (16, 4), 21: (18, 8.5), 22: (16, 1), 23: (21, 1), 24: (21, 4), 25: (18, 3.5), 26: (17, 2), 27: (19, 2)}
    >>> edges = [(1, 2),(1, 3),(2, 1),(2, 4),(2, 7),(3, 1),(3, 4),(4, 2),(4, 3),(4, 5),(5, 4),(5, 6),(6, 5),(7, 2),(7, 11),(8, 9),(8, 10),(9, 8),(9, 10),(10, 8),(10, 9),(11, 7),(11, 12),(11, 13),(12, 11),(12, 13),(12, 20),(13, 11),(13, 12),(13, 18),(14, 15),(15, 14),(15, 16),(16, 15),(18, 13),(18, 19),(19, 18),(19, 20),(19, 21),(20, 12),(20, 19),(20, 21),(20, 22),(20, 24),(21, 19),(21, 20),(22, 20),(22, 23),(23, 22),(23, 24),(24, 20),(24, 23),(25, 26),(25, 27),(26, 25),(26, 27),(27, 25),(27, 26)]
    >>> r = regions_from_graph(vertices, edges)
    >>> r['filaments']
    [[6, 5, 4], 0, [2, 7, 11], [14, 15, 16], 17]
    >>> r['regions']
    [[3, 4, 2, 1, 3], [9, 10, 8, 9], [11, 12, 13, 11], [12, 20, 19, 18, 13, 12], [19, 20, 21, 19], [22, 23, 24, 20, 22], [26, 27, 25, 26]]

    Notes
    -----
    Based on
    Eberly http://www.geometrictools.com/Documentation/MinimalCycleBasis.pdf.
    """
    def find_start_node(nodes,node_coord):
        start_node = []
        minx = float('inf')
        for key,node in nodes.items():
            if node[0] <= minx:
                minx = node[0]
                start_node.append(key)
        if len(start_node) > 1:
            miny = float('inf')
            for i in range(len(start_node)):
                if nodes[i][1] < miny:
                    miny = nodes[i][1]
                else:
                    start_node.remove(i)
        return nodes[start_node[0]], node_coord[nodes[start_node[0]]]
    
    def clockwise(nodes,vnext,start_key, v_prev, vertices=None):
        v_curr = np.asarray(nodes[start_key])
        v_next = None
        if v_prev == None:
            v_prev = np.asarray([0,-1]) #This should be a vertical tangent to the start node at initialization.
        else:
            pass
        d_curr = v_curr - v_prev
        
        for v_adj in vnext:
            #No backtracking
            if np.array_equal(np.asarray(nodes[v_adj]),v_prev) == True:
                
                continue
            if type(v_prev) == int:
                if v_adj == v_prev:
                    continue
            
            #The potential direction to move in
            d_adj = np.asarray(nodes[v_adj]) - v_curr
            
            #Select the first candidate
            if v_next is None:
                v_next = np.asarray(nodes[v_adj])
                d_next = d_adj
                convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]
                if convex <= 0:
                    convex = True
                else:
                    convex = False
            
            #Update if the next candidate is clockwise of the current clock-wise most
            if convex == True:
                if (d_curr[0]*d_adj[1] - d_curr[1]*d_adj[0]) < 0 or (d_next[0]*d_adj[1]-d_next[1]*d_adj[0]) < 0:
                    v_next = np.asarray(nodes[v_adj])
                    d_next = d_adj 
                    convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]
                    if convex <= 0:
                        convex = True
                    else:
                        convex = False
            else:
                if (d_curr[0]*d_adj[1] - d_curr[1]*d_adj[0]) < 0 and (d_next[0]*d_adj[1]-d_next[1]*d_adj[0]) < 0:
                    v_next = np.asarray(nodes[v_adj])
                    d_next = d_adj 
                    convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0] 
                    if convex <= 0:
                        convex = True
                    else:
                        convex = False
        prev_key = start_key
        if vertices == None:
            return tuple(v_next.tolist()), node_coord[tuple(v_next.tolist())], prev_key
        else:
            return tuple(v_next.tolist()), vertices[tuple(v_next.tolist())], prev_key
    def counterclockwise(nodes, vnexts, start_key, prev_key):
        v_next = None
        v_prev = np.asarray(nodes[prev_key])
        v_curr = np.asarray(nodes[start_key])
        d_curr = v_curr - v_prev
        
        for v_adj in vnexts: 
            #Prohibit Back-tracking
            if v_adj == prev_key:
                continue
            d_adj = np.asarray(nodes[v_adj]) - v_curr
            
            if v_next == None:
                v_next = np.asarray(nodes[v_adj])
                d_next = d_adj
                convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]
                
            if convex <= 0:
                if d_curr[0]*d_adj[1] - d_curr[1]*d_adj[0] > 0 and d_next[0]*d_adj[1] - d_next[1]*d_adj[0] > 0:
                    v_next = np.asarray(nodes[v_adj])
                    d_next = d_adj 
                    convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]
                else:
                    pass
            else:
                if d_curr[0]*d_adj[1] - d_curr[1]*d_adj[0] > 0 or d_next[0]*d_adj[1]-d_next[1]*d_adj[0] > 0:
                    v_next = np.asarray(nodes[v_adj])
                    d_next = d_adj 
                    convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]
                else:
                    pass
        prev_key = start_key
        try:
            return tuple(v_next.tolist()), node_coord[tuple(v_next.tolist())], prev_key
        except: 
            return v_next, None, prev_key
    def remove_edge(v0,v1,edges, ext_edges):
        try:
            ext_edges.append((v0,v1))
            ext_edges.append((v1,v0))
            edges.remove((v0,v1))
            edges.remove((v1,v0))
        except:
            pass
        return edges, ext_edges
    
    def remove_heap(v0,sorted_nodes):
        sorted_nodes[:] = [x for x in sorted_nodes if x[0] != v0]
        return sorted_nodes
            
    def remove_node(v0, nodes, nodes_coord, vertices):
        vertices[v0] = nodes[v0]
        del nodes_coord[nodes[v0]]
        del nodes[v0]
        return nodes, nodes_coord, vertices
        
    def extractisolated(nodes,node_coord,v0,primitives, vertices, ext_edges):
        primitives.append(v0)
        nodes, node_coord, vertices = remove_node(v0, nodes, node_coord, vertices)
        return nodes, node_coord, primitives, vertices, ext_edges
    
    def extractfilament(v0,v1, nodes, node_coord,sorted_nodes, edges, primitives,cycle_edge, vertices, ext_edges, iscycle=False):
        if (v0,v1) in cycle_edge or (v1,v0) in cycle_edge:
            iscycle = True
        if iscycle == True:
        #This deletes edges that are part of a cycle, but does not add them as primitives.
            if len(adj_nodes(v0,edges)) >= 3:
                edges, ext_edges = remove_edge(v0,v1,edges, ext_edges)
                v0 = v1
                if len(adj_nodes(v0, edges)) == 1:
                    v1 = adj_nodes(v0, edges)[0]
            while len(adj_nodes(v0, edges)) == 1:
                v1 = adj_nodes(v0, edges)[0]
                #Here I need to do the cycle check again.
                iscycle = False
                if (v0,v1) in cycle_edge or (v1,v0) in cycle_edge:
                    iscycle = True
                
                if iscycle == True:
                    edges, ext_edges = remove_edge(v0,v1,edges, ext_edges)
                    nodes, node_coord, vertices = remove_node(v0, nodes, node_coord, vertices)
                    sorted_nodes = remove_heap(v0, sorted_nodes)
                    v0 = v1
                else:
                    break
            if len(adj_nodes(v0, edges)) == 0:
                
                nodes, node_coord, vertices = remove_node(v0, nodes, node_coord, vertices)
                sorted_nodes = remove_heap(v0, sorted_nodes)
        else:
            #Filament found
            primitive = []
            if len(adj_nodes(v0,edges)) >= 3:
                primitive.append(v0)
                edges, ext_edges = remove_edge(v0,v1,edges, ext_edges)
                v0 = v1
                if len(adj_nodes(v0, edges)) == 1:
                    v1 = adj_nodes(v0, edges)[0]
    
            while len(adj_nodes(v0, edges)) == 1:
                primitive.append(v0)
                v1 = adj_nodes(v0, edges)[0]
                sorted_nodes = remove_heap(v0, sorted_nodes)
                edges, ext_edges = remove_edge(v0, v1, edges, ext_edges)
                nodes, node_coord, vertices = remove_node(v0, nodes, node_coord, vertices)
                v0 = v1
            
            primitive.append(v0)
            if len(adj_nodes(v0, edges)) == 0:
                sorted_nodes = remove_heap(v0, sorted_nodes)
                edges, ext_edges = remove_edge(v0, v1, edges, ext_edges)
                nodes, node_coord, vertices = remove_node(v0, nodes, node_coord, vertices)
            primitives.append((primitive))
        return sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges
    
    def extract_primitives(start_key,sorted_nodes, edges, nodes, node_coord, primitives,minimal_cycles,cycle_edge, vertices, ext_edges):
        v0 = start_key
        visited = []
        sequence = []
        sequence.append(v0)
    
        #Find the CWise most vertex
        vnext = adj_nodes(start_key, edges)
        start_node,v1,v_prev = clockwise(nodes,vnext,start_key,prev_key)
        v_curr = v1
        v_prev = v0
        #Find minimal cycle using CCWise rule
        process = True
        if v_curr == None:
            process = False
        elif v_curr == v0:
            process = False
        elif v_curr in visited:
            process = False
        
        while process == True:
            sequence.append(v_curr)
            visited.append(v_curr)
            vnext = adj_nodes(v_curr, edges)
            v_curr_coords,v_next,v_prev = counterclockwise(nodes,vnext,v_curr, v_prev)
            v_curr = v_next
            if v_curr == None:
                process = False
            elif v_curr == v0:
                process = False
            elif v_curr in visited:
                process = False
        
        if v_curr is None:
            #Filament found, not necessarily at start_key
            sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges = extractfilament(v_prev, adj_nodes(v_prev, edges)[0],nodes, node_coord, sorted_nodes, edges, primitives, cycle_edge, vertices, ext_edges)
            
        elif v_curr == v0:
            #Minimal cycle found
            primitive = []
            iscycle=True
            sequence.append(v0)
            minimal_cycles.append(list(sequence))
            #Remove the v0, v1 edges from the graph.
            edges, ext_edges = remove_edge(v0,v1,edges, ext_edges)
            sorted_nodes = remove_heap(v0, sorted_nodes)#Not in pseudo-code, but in source.
            #Mark all the edges as being part of a minimal cycle.
            if len(adj_nodes(v0, edges)) == 1:
                cycle_edge.append((v0, adj_nodes(v0, edges)[0]))
                cycle_edge.append((adj_nodes(v0, edges)[0], v0))
                sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges = extractfilament(v0, adj_nodes(v0, edges)[0],nodes, node_coord, sorted_nodes, edges, primitives,cycle_edge, vertices, ext_edges)
            if len(adj_nodes(v1, edges)) == 1:
                cycle_edge.append((v1, adj_nodes(v1, edges)[0]))
                cycle_edge.append((adj_nodes(v1, edges)[0],v1))
                sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges = extractfilament(v1, adj_nodes(v1, edges)[0],nodes, node_coord, sorted_nodes, edges, primitives, cycle_edge, vertices, ext_edges)
           
            for i,v in enumerate(sequence[1:-1]):
                cycle_edge.append((v,sequence[i]))
                cycle_edge.append((sequence[i],v))
            
        else:
            #vcurr was visited earlier, so traverse the filament to find the end
            while len(adj_nodes(v0,edges)) == 2:
                if adj_nodes(v0,edges)[0] != v1:
                    v1 = v0
                    v0 = adj_nodes(v0,edges)[0]
                else:
                    v1 = v0
                    v0 = adj_nodes(v0, edges)[1]
            sorted_nodes, edges, nodes, node_coord, primitives = extractfilament(v0,v1,nodes, node_coord, sorted_nodes, edges, primitives,cycle_edge)
        return sorted_nodes, edges, nodes, node_coord, primitives, minimal_cycles,cycle_edge, vertices, ext_edges
    #1.
    sorted_nodes = sorted(nodes.iteritems(), key=operator.itemgetter(1))
    node_coord = dict (zip(nodes.values(),nodes.keys()))
    
    #2.
    primitives = []
    minimal_cycles = []
    cycle_edge = []
    prev_key = None #This is only true for the first iteration.
    #This handles edge and node deletion we need populated later.
    vertices = {}
    ext_edges = []    
    
    #3.
    while sorted_nodes: #Iterate through the sorted list
        start_key = sorted_nodes[0][0]
        numadj = len(adj_nodes(start_key, edges))
        if numadj == 0:
            nodes, node_coord, primitives, vertices, ext_edges = extractisolated(nodes,node_coord,start_key,primitives, vertices, ext_edges)
            sorted_nodes.pop(0)
        elif numadj == 1:
            sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges = extractfilament(start_key, adj_nodes(start_key, edges)[0],nodes, node_coord, sorted_nodes, edges,primitives,cycle_edge, vertices, ext_edges)
        else:
            sorted_nodes, edges, nodes, node_coord, primitives, minimal_cycles,cycle_edge, vertices, ext_edges = extract_primitives(start_key,sorted_nodes, edges, nodes, node_coord, primitives, minimal_cycles,cycle_edge, vertices, ext_edges)
    
    #4. Remove holes from the graph
    if remove_holes == True:
        polys = []
        for cycle in minimal_cycles:
            polys.append(ps.cg.Polygon([ps.cg.Point(vertices[pnt]) for pnt in cycle]))
            
        pl = ps.cg.PolygonLocator(polys)
           
        # find all overlapping polygon mbrs
        overlaps ={}
        nump = len(minimal_cycles)
        for i in range(nump):
            overlaps[i] = pl.overlapping(polys[i].bounding_box)
        
        # for overlapping mbrs (left,right) check if right polygon is contained in left
        holes = []
        for k in overlaps:
            for  pc in overlaps[k]:
                s = sum( [polys[k].contains_point(v) for v in pc.vertices])
                if s == len(pc.vertices):
                    # print k, pc
                    holes.append((k,pc))    
        
        for hole in holes:
            outer, inner = hole
            inner = polys.index(inner)
            minimal_cycles.pop(inner)
    
    #5. Remove isolated vertices
    filaments = []
    for index, primitive in enumerate(primitives):
        if type(primitive) == list:
            filaments.append(primitive)
    
    results = {}
    results['regions'] = minimal_cycles
    results['filaments'] = filaments
    results['vertices'] = vertices
    results['edges'] = ext_edges
    results['nodes'] = vertices
    return results

def internal_or_external(polys,filament, vertices):
    #Modification of Serge's code to find poly in poly for line in poly
    #pl = ps.cg.PolygonLocator(polys) #Trying to use this for polyline in polygon
    
    #Spatial Index of Filaments and minimal cycles (polygons)
    polyline = ps.cg.Chain([ps.cg.Point(vertices[pnt]) for pnt in filament])
    polyline_mbr =  polyline.bounding_box
    pl = ps.cg.PolygonLocator(polys)
    overlaps = pl.overlapping(polyline_mbr)
    
    #For the overlapping MBRs check to see if the polyline is internal or external to the min cycle
    for k in range(len(overlaps)):
        s = sum(overlaps[k].contains_point(v) for v in polyline.vertices)
        if s == len(polyline.vertices):
            #Internal
            return True
        else:
            return False

def classify_filaments(filaments, cycles, edges, vertices):
    classified_filaments = {}
    #Dict matching nodes to cycles
    node_mem = {}
    bridge_mem = {}
    for cycle in cycles:
        for node in cycle:
            node_mem[node] = cycle
    
    #DS to hold classifications
    bridge_filaments = []
    isolated_filaments = []
    external_filaments = []
    internal_filaments = []
    
    #Polygon Spatial Index
    # build polygon spatial index
    polys = []
    for cycle in cycles:
        polys.append(ps.cg.Polygon([ps.cg.Point(vertices[pnt]) for pnt in cycle]))        
    
    for filament in filaments:
        if filament[0] in node_mem.keys() and filament[-1] in node_mem.keys():
            bridge_filaments.append(filament)
            for node in filament:
                bridge_mem[node] = filament
        elif filament[0] not in node_mem.keys() and filament[-1] not in node_mem.keys():
            if filament[0] not in bridge_mem.keys() and filament[-1] not in bridge_mem.keys():
                isolated_filaments.append(filament)
            else:
                #Check internal or external
                if internal_or_external(polys,filament, vertices) == True:
                    internal_filaments.append(filament)
                else:
                    external_filaments.append(filament)
        else:
            #Check internal or external
            if internal_or_external(polys,filament, vertices) == True:
                internal_filaments.append(filament)
            else:
                external_filaments.append(filament)
    
    classified_filaments['isolated'] = isolated_filaments
    classified_filaments['bridge'] = bridge_filaments
    classified_filaments['external'] = external_filaments
    classified_filaments['internal'] = internal_filaments
    
    return classified_filaments

def generate_wed(regions):
    left_region = {}
    right_region = {}
    edges = {}
    region_edge = {}
    start_c = {}
    end_c = {}
    start_cc = {}
    end_cc = {}
    
    node_edge = {}
    
    for region in regions:
        r = [region[-2]]
        r.extend(region)
        r.append(region[1])
        for i in range(len(region)-1):
            edge = r[i+1],r[i+2]
            if edge[0] not in node_edge:
                node_edge[edge[0]] = edge
            if edge[1] not in node_edge:
                node_edge[edge[1]] = edge
            s_c = r[i],r[i+1]
            e_cc = r[i+2],r[i+3]
            right_region[edge] = region
            region_edge[tuple(region)] = edge
            start_c[edge] = s_c
            end_cc[edge] = e_cc
    
            left_region[edge[1],edge[0]] = region
            start_cc[edge[1], edge[0] ] = end_cc[edge]
            end_c[edge[1], edge[0] ] = start_c[edge]
    
            edges[edge] = edge
    
    wed = {}
    wed['node_edge'] = node_edge
    wed['end_c'] = end_c
    wed['start_c'] = start_c
    wed['start_cc'] = start_cc
    wed['end_cc'] = end_cc
    wed['edges'] = edges
    wed['region_edge'] = region_edge
    wed['right_region'] = right_region
    wed['left_region'] = left_region    

    return wed

def connected_components(wed):
    """
    Find all connected components in a WED

    """

    nodes = wed['node_edge'].keys()
    components = []
    while nodes:
        start = nodes.pop()
        component = connected_component(wed, start)
        if len(component) > 1:
            for node in component:
                if node in nodes:
                    nodes.remove(node)
        components.append(component)
    return components

def connected_component(wed,start_node):
    """
    Find connected component containing start_node
    """

    if not wed['node_edge'][start_node]:
        return [start_node]
    stack = [start_node]
    children = enum_links_node(wed, start_node)
    A = {}
    A[start_node] = set()
    visited =  []
    for child in children:
        if child[0] == start_node:
            A[start_node].add(child[1])
        else:
            A[start_node].add(child[0])
    searching = True
    visited.append(start_node)
    while searching:
        current = stack[-1]
        if current not in A:
            children = enum_links_node(wed, current)
            A[current] = set()
            for child in children:
                if child[0] == current:
                    A[current].add(child[1])
                else:
                    A[current].add(child[0])
        else:
            if A[current]:
                child = A[current].pop()
                if child not in visited:
                    visited.append(child)
                    stack.append(child)
            else:  # current has no more children
                stack.remove(current)
                if not stack:
                    searching = False

    return visited

if __name__ == "__main__":
    
    #Eberly
    vertices = {0: (1, 8), 1: (1, 7), 2: (4, 7), 3: (0, 4), 4: (5, 4), 5: (3, 5), 6: (2, 4.5), 7: (6.5, 9), 8: (6.2, 5), 9: (5.5, 3), 10: (7, 3), 11: (7.5, 7.25), 12: (8, 4), 13: (11.5, 7.25), 14: (9, 1), 15: (11, 3), 16: (12, 2), 17: (12, 5), 18: (13.5, 6), 19: (14, 7.25), 20: (16, 4), 21: (18, 8.5), 22: (16, 1), 23: (21, 1), 24: (21, 4), 25: (18, 3.5), 26: (17, 2), 27: (19, 2)}
    
    edges = [(1, 2),(1, 3),(2, 1),(2, 4),(2, 7),(3, 1),(3, 4),(4, 2),(4, 3),(4, 5),(5, 4),(5, 6),(6, 5),(7, 2),(7, 11),(8, 9),(8, 10),(9, 8),(9, 10),(10, 8),(10, 9),(11, 7),(11, 12),(11, 13),(12, 11),(12, 13),(12, 20),(13, 11),(13, 12),(13, 18),(14, 15),(15, 14),(15, 16),(16, 15),(18, 13),(18, 19),(19, 18),(19, 20),(19, 21),(20, 12),(20, 19),(20, 21),(20, 22),(20, 24),(21, 19),(21, 20),(22, 20),(22, 23),(23, 22),(23, 24),(24, 20),(24, 23),(25, 26),(25, 27),(26, 25),(26, 27),(27, 25),(27, 26)]    
    
    cycles = regions_from_graph(vertices, edges)
    
    print "Minimal Cycles: ", cycles['regions']
    print "Filaments: ", cycles['filaments']

    filaments = classify_filaments(cycles['filaments'],cycles['regions'], cycles['edges'], cycles['vertices'])
    print "Isolated Filaments: ",filaments['isolated']
    print "Bridge Filaments: ", filaments['bridge']
    print "Internal Filaments: ", filaments['internal']
    print "External Filaments: ", filaments['external']
    
    wed = generate_wed(cycles['regions'])
    
    print wed['node_edge']
    
    components = connected_components(wed)
    print "Components are: ",components
    
    n2_connected = connected_component(wed, 2)
    print "Starting at node 2, I visited: ", n2_connected
    
    n25_connected = connected_component(wed,25)
    print "I am starting in a hole.  I visited: ", n25_connected
    
    n15_connected = connected_component(wed, 15)
    print "I am on a disconnected component and visited: ", n15_connected
    
########NEW FILE########
__FILENAME__ = util
"""
Utilities for PySAL Network Module
"""

__author__ = "Sergio Rey <sjsrey@gmail.com>, Jay Laura <jlaura@asu.edu>"
import copy
import math
from collections import OrderedDict
from random import uniform
import multiprocessing as mp
from heapq import nsmallest

import pysal as ps
import numpy as np
from pysal.cg.standalone import get_points_dist


class SortedEdges(OrderedDict):
    def next_key(self, key):
        next = self._OrderedDict__map[key][1]
        if next is self._OrderedDict__root:
            raise ValueError("{!r} is the last key.".format(key))
        return next[2]
    def first_key(self):
        for key in self: return key
        raise ValueError("No sorted edges remain.")

def enum_links_node(wed, node):
    """
    Enumerate links in cw order around a node

    Parameters
    ----------

    node: string/int
        id for the node in wed


    Returns
    -------

    links: list
        links ordered cw around node
    """

    links = []
    if node not in wed.node_edge:
        return links
    l0 = wed.node_edge[node]
    links.append(l0)
    l = l0
    v = node
    searching = True
    while searching:
        if v == l[0]:
            l = wed.start_c[l]
        else:
            l = wed.end_c[l]
        if (l is None) or (set(l) == set(l0)):
            searching = False
        else:
            links.append(l)
    return links


def enum_edges_region(wed, region):
    """
    Enumerate the edges of a region/polygon in cw order

    Parameters
    ----------

    region: id for the region in wed


    Returns
    -------

    links: list of links ordered cw that define the region/polygon

    """
    right_polygon = wed.right_polygon
    end_cc = wed.end_cc
    start_cc = wed.start_cc
    region_edge = wed.region_edge
    l0 = region_edge[region]
    l = copy.copy(l0)
    edges = []
    edges.append(l)
    traveling = True
    while traveling:
        if region == right_polygon[l]:
            l = end_cc[l]
        else:
            l = start_cc[l]
        edges.append(l)
        if set(l) == set(l0):
            traveling = False
    return edges


def edge_length(wed, half=False):
    """
    Compute the cartesian length of all edges.  This is a helper
        function to allow for ratio data with spatial autocorrelation
        analysis.

    Parameters
    ----------
    wed: PySAL Winged Edged Data structure
    half: Double edge length by default, flag to set to single

    Returns
    -------
    length : dict {tuple(edge): float(length)}
        The length of each edge.
    """

    lengths = {}
    for edge in wed.edge_list:
        if half:
            if (edge[1], edge[0]) not in lengths.keys():
                lengths[edge] = get_points_dist(wed.node_coords[edge[0]],
                                                wed.node_coords[edge[1]])
        else:
            lengths[edge] = get_points_dist(wed.node_coords[edge[0]],
                                            wed.node_coords[edge[1]])
    return lengths


def snap_to_edges(wed, points):
    """
    Snaps observations to the netwrok edge.

    Parameters
    wed: PySAL Winged Edged Data Structure

    Returns:
    --------
    obs_to_edge: a dict of dicts {edge:{point_id:(x,y)}}
    """

    obs_to_edge = {}
    for region in wed.region_edge.keys():
        verts = []
        region_edges = enum_edges_region(wed, region)
        for edge in region_edges:
            if edge[0] not in verts:
                verts.append(edge[0])
            elif edge[1] not in verts:
                verts.append(edge[1])
        verts.append(verts[0])
        poly = ps.cg.Polygon([wed.node_coords[v] for v in verts])

        for pt_index, point in enumerate(points):
            x0 = point[0]
            y0 = point[1]
            if ps.cg.standalone.get_polygon_point_intersect(poly, point):
                d = {}
                vectors = {}
                c = 0
                ccw_edges = region_edges[::-1]
                for i in range(len(ccw_edges)-1):
                    edge = ccw_edges[i]
                    xi = wed.node_coords[edge[0]][0]
                    yi = wed.node_coords[edge[0]][1]
                    xi1 = wed.node_coords[edge[1]][0]
                    yi1 = wed.node_coords[edge[1]][1]
                    num = ((yi1 - yi)*(x0-xi)-(xi1-xi)*(y0-yi))
                    denom = ((yi1-yi)**2 + (xi1-xi)**2)
                    #num = abs(((xi - xi1) * (y0 - yi)) - ((yi - yi1) * (x0 - xi)))
                    #denom = math.sqrt(((xi - xi1)**2) + (yi-yi1)**2)
                    k = num / denom
                    distance = abs(num) / math.sqrt(((yi1-yi)**2 + (xi1-xi)**2))
                    vectors[c] = (xi, xi1, yi, yi1,k,edge)
                    d[distance] = c
                    c += 1
                min_dist = SortedEdges(sorted(d.items()))
                for dist, vector_id in min_dist.iteritems():
                    value = vectors[vector_id]
                    xi = value[0]
                    xi1 = value[1]
                    yi = value[2]
                    yi1 = value[3]
                    k = value[4]
                    edge = value[5]
                    #Okabe Method
                    x = x0 - k * (yi1 - yi)
                    y = y0 + k * (xi1 - xi)
                    if xi <= x <= xi1 or xi1 <= x <= xi and yi <= y <= yi1 or yi1 <=y <= yi:
                        #print "{} intersections edge {} at {}".format(pt_index, edge, (x,y))
                        if edge not in obs_to_edge.keys():
                            obs_to_edge[edge] = {pt_index:(x,y)}
                        else:
                            obs_to_edge[edge][pt_index] = (x,y)
                        break
                    else:
                        #either pi or pi+1 are the nearest point on that edge.
                        #If this point is closer than the next distance, we can break, the
                        # observation intersects the node with the shorter
                        # distance.
                        pi = (xi, yi)
                        pi1 = (xi1, yi1)
                        p0 = (x0,y0)
                        dist_pi = ps.cg.standalone.get_points_dist(p0, pi)
                        dist_pi1 = ps.cg.standalone.get_points_dist(p0, pi1)
                        if dist_pi < dist_pi1:
                            node_dist = dist_pi
                            (x,y) = pi
                        else:
                            node_dist = dist_pi1
                            (x,y) = pi1
                        if node_dist < min_dist.next_key(dist):
                            if edge not in obs_to_edge.keys():
                                obs_to_edge[edge] = {pt_index:(x,y)}
                            else:
                                obs_to_edge[edge][pt_index] = (x,y)
                            break

    return obs_to_edge


def count_per_edge(obs_on_network):
    """
    Snaps observations to the nearest edge and then counts
        the number of observations per edge.

    Parameters
    ----------
    obs_on_network: dict of observations on the network
        {(edge): {pt_id: (coords)}} or {edge: [(coord), (coord), (coord)]}
    Returns
    -------
    counts: dict {(edge):count}
    """

    counts = {}
    for key in obs_on_network.iterkeys():
        counts[key] = len(obs_on_network[key])
    return counts


def simulate_observations(wed, count, distribution='uniform'):
    """
    Generates simulated points to test for NCSR

    Parameters
    ----------
    wed: PySAL Winged Edged Data Structure
    count: integer number of points to create
    distirbution: distribution of random points

    Returns
    -------
    random_pts: dict with {(edge):[(x,y), (x1, y1), ... , (xn,yn)]}
    """
    if distribution is 'uniform':
        lengths = wed.edge_length()
        single_lengths = []
        edges = []
        for edge, l in lengths.iteritems():
            if (edge[0], edge[1]) or (edge[1], edge[0]) not in edges:
                edges.append(edge)
                single_lengths.append(l)
        line = np.array([single_lengths])
        offsets = np.cumsum(line)
        total_length = offsets[-1]
        starts = np.concatenate((np.array([0]), offsets[:-1]), axis=0)
        random_pts = {}
        for x in range(count):
            random_pt = uniform(0,total_length)
            start_index = np.where(starts <= random_pt)[0][-1]
            assignment_edge = edges[start_index]
            distance_from_start = random_pt - offsets[start_index - 1]
            x0, y0 = newpoint_coords(wed, assignment_edge, distance_from_start)
            if assignment_edge not in random_pts.keys():
                random_pts[assignment_edge] = [(x0,y0)]
            else:
                random_pts[assignment_edge].append((x0,y0))
    return random_pts


def newpoint_coords(wed, edge, distance):
    x1 = wed.node_coords[edge[0]][0]
    y1 = wed.node_coords[edge[0]][1]
    x2 = wed.node_coords[edge[1]][0]
    y2 = wed.node_coords[edge[1]][1]
    m = (y2 - y1) / (x2 - x1)
    b1 = y1 - m * (x1)
    if x1 > x2:
        x0 = x1 - distance / math.sqrt(1 + m**2)
    elif x1 < x2:
        x0 = x1 + distance / math.sqrt(1 + m**2)
    y0 = m * (x0 - x1) + y1
    return x0, y0


def dijkstra(wed, cost, node, n=float('inf')):
    """
    Compute the shortest path between a start node and
        all other nodes in the wed.

    Parameters
    ----------
    wed: PySAL Winged Edged Data Structure
    cost: Cost per edge to travel, e.g. distance
    node: Start node ID
    n: integer break point to stop iteration and return n
     neighbors

    Returns:
    distance: List of distances from node to all other nodes
    pred : List of preceeding nodes for traversal route
    """

    v0 = node
    distance = [float('inf') for x in wed.node_list]
    distance[wed.node_list.index(v0)] = 0
    pred = [None for x in wed.node_list]
    a = set()
    a.add(v0)
    while len(a) > 0:
        #Get node with the lowest value from distance
        dist = float('inf')
        for node in a:
            if distance[node] < dist:
                dist = distance[node]
                v = node
        #Remove that node from the set
        a.remove(v)
        last = v
        #4. Get the neighbors to the current node
        neighbors = get_neighbor_distances(wed, v, cost)
        for v1, indiv_cost in neighbors.iteritems():
            if distance[v1] > distance[v] + indiv_cost:
                distance[v1] = distance[v] + indiv_cost
                pred[v1] = v
                a.add(v1)
    return distance, pred


def shortest_path(wed, cost, start, end):
    distance, pred = dijkstra(wed, cost, start)
    path = [end]
    previous = pred[end]
    while previous != start:
        path.append(previous)
        end = previous
        previous = pred[end]
    path.append(start)
    return path


def check_connectivity(wed, cost):
    distance, pred = dijkstra(wed, cost, 0)
    if float('inf') in distance:
        return False
    else:
        return True


def get_neighbor_distances(wed, v0, l):
    edges = wed.enum_links_node(v0)
    neighbors = {}
    for e in edges:
        if e[0] != v0:
            neighbors[e[0]] = l[e]
        else:
            neighbors[e[1]] = l[e]
    return neighbors

def newpointer(k,v,c):
    '''
    Helper function for node insertion.

    Parameters
    ----------
    k: key of the pointer to update
    v: current value of the key
    c: value to replace the non-shared (k:v) value with

    Returns
    -------
    l: tuple new value of the pointer k
    '''
    mask = [(i == j) for i, j in zip(k, v)]
    if all(x == False for x in mask):
        mask = [(i == j) for i, j in zip(k, (v[1], v[0]))]
    replace = mask.index(False)
    l = list(v)
    l[replace] = c
    return tuple(l)


def insert_node(wed, edge, distance, segment=False):
    """
    Insert a node into an existing edge at a fixed distance from
     the start node.

    Parameters
    ----------
    wed: PySAL Winged Edge Data Structure
    edge: The edge to insert a node into
    distance: float, distance from the start node of the edge
    segment: a flag that returns the modified WED and the new node id

    Returns
    -------
    wed: Modified PySAL WED data structure
    """
    #Get the coordinates of the new point and update the node_coords
    x0, y0 = newpoint_coords(wed, edge, distance)
    newcoord_id = max(wed.node_list) + 1
    wed.node_list.append(newcoord_id)
    wed.node_coords[newcoord_id] = (x0, y0)
    #Update the region edge
    new_edge = (edge[0], newcoord_id)
    if edge in wed.region_edge.keys():
        wed.region_edge[new_edge] = wed.region_edge.pop(edge)
    if (edge[1], edge[0]) in wed.region_edge.keys():
        wed.region_edge[(new_edge[1], new_edge[0])] = wed.region_edge.pop((edge[1], edge[0]))

    a = edge[0]
    b = edge[1]
    c = newcoord_id
    #Update the edge list
    idx = wed.edge_list.index(edge)
    wed.edge_list.pop(idx)
    wed.edge_list += [(a, c), (c, a)]
    idx = wed.edge_list.index((b, a))
    wed.edge_list.pop(idx)
    wed.edge_list += [(b, c), (c, b)]
    #Update the start and end nodes
        #Remove the old start and end node pointers
    wed.start_node.pop(edge)
    wed.start_node.pop((b, a))
    wed.end_node.pop(edge)
    wed.end_node.pop((b, a))
        #Add the 4 new pointers
    wed.start_node[(a, c)] = a
    wed.end_node[(a, c)] = c
    wed.start_node[(c, a)] = c
    wed.end_node[(c, a)] = a
    wed.start_node[(c, b)] = c
    wed.end_node[(c, b)] = b
    wed.start_node[(b, c)] = b
    wed.end_node[(b, c)] = c
    #Update the startc, startcc, enc, endcc of the new links
        #Replace the old pointers with new pointers
    wed.start_c[(a, c)] = wed.start_c.pop(edge)
    wed.start_cc[(a, c)] = wed.start_cc.pop(edge)
    wed.end_c[(c, b)] = wed.end_c.pop(edge)
    wed.end_cc[(c, b)] = wed.end_cc.pop(edge)
    rev_edge = (b, a)
    wed.start_c[(b, c)] = wed.start_c.pop(rev_edge)
    wed.start_cc[(b, c)] = wed.start_cc.pop(rev_edge)
    wed.end_c[(c, a)] = wed.end_c.pop(rev_edge)
    wed.end_cc[(c, a)] = wed.end_cc.pop(rev_edge)
        #Add brand new pointers for the new edges
    wed.start_c[(c, a)] = (c, b)
    wed.start_cc[(c, a)] = (c, b)
    wed.end_c[(a, c)] = (c, b)
    wed.end_cc[(a, c)] = (c, b)
    wed.start_c[(c, b)] = (c, a)
    wed.start_cc[(c, b)] = (c, a)
    wed.end_c[(b, c)] = (c, a)
    wed.end_cc[(b, c)] = (c, a)
    #Update the pointer to the nodes incident to start / end of the original link
    for k, v in wed.start_c.iteritems():
        if v == edge:
            wed.start_c[k] = newpointer(k,v,c)
        elif v == rev_edge:
            wed.start_c[k] = newpointer(k,v,c)
    for k, v in wed.start_cc.iteritems():
        if v == edge:
            wed.start_cc[k] = newpointer(k,v,c)
        elif v == rev_edge:
            wed.start_cc[k] = newpointer(k,v,c)
    for k, v in wed.end_c.iteritems():
        if v == edge:
            wed.end_c[k] = newpointer(k,v,c)
        elif v == rev_edge:
            wed.end_c[k] = newpointer(k,v,c)
    for k, v in wed.end_cc.iteritems():
        if v == edge:
            wed.end_cc[k] = newpointer(k,v,c)
        elif v == rev_edge:
            wed.end_cc[k] = newpointer(k,v,c)
    #Update the node_edge pointer
    wed.node_edge[c] = (a, c)
    wed.node_edge[a] = (a, c)
    wed.node_edge[b] = (b, c)
    #update right and left polygon regions
    if edge in wed.right_polygon.keys():
        right = wed.right_polygon.pop(edge)
        wed.right_polygon[(a, c)] = right
        wed.right_polygon[(c, b)] = right
    if edge in wed.left_polygon.keys():
        left = wed.left_polygon.pop(edge)
        wed.left_polygon[(a, c)] = left
        wed.left_polygon[(c, b)] = left
    if rev_edge in wed.right_polygon.keys():
        right = wed.right_polygon.pop(rev_edge)
        wed.right_polygon[(b, c)] = right
        wed.right_polygon[(c, a)] = right
    if rev_edge in wed.left_polygon.keys():
        left = wed.left_polygon.pop(rev_edge)
        wed.left_polygon[(b, c)] = left
        wed.left_polygon[(c, a)] = left

    if segment:
        return wed, c
    else:
        return wed


def segment_edges(wed, distance=None, count=None):
    '''
    Segment all of the edges in the network at either
    a fixed distance or a fixed number of segments.

    Parameters
    ----------
    wed: PySAL Winged Edged Data Structure
    distance: float distance at which edges are split
    count: integer count of the number of desired segments
    '''
    if count != None:
        assert(type(count) == int)

    def segment(count, distance, wed, start, end):
        '''
        Recursive segmentation of each edge until count is reached.
        '''
        if count == 1:
            return wed

        edge = (start, end)
        wed, start = insert_node(wed, edge, distance, segment=True)
        segment(count - 1, distance, wed, start, end)
        return wed

    #Any segmentation has float inconsistencies.  On the order of 1x10^-14
    if count == None and distance == None or count != None and distance != None:
        print '''
        Please supply either a distance at which to
        segment edges or a count of the number of
        segments to generate per edge.
        '''
        return
    lengths = edge_length(wed, half=True)
    if count != None:
        for k, l in lengths.iteritems():
            interval = l / count
            wed = segment(count, interval, wed, k[0], k[1])
    elif distance:
        for k, l in lengths.iteritems():
            if distance >= l or l / distance == 0:
                continue
            count = l / distance
            print count
            wed = segment(count, distance, wed, k[0], k[1])
    return wed


def threshold_distance(wed, cost, node, threshold, midpoint=False):
    """
    Compute the shortest path between a start node and
        all other nodes in the wed.

    Parameters
    ----------
    wed: PySAL Winged Edged Data Structure
    cost: Cost per edge to travel, e.g. distance
    node: Start node ID
    threshold: float, distance to which neighbors are included
    midpoint: Boolean to indicate whether distance is computed from the start
     node or the midpoint of the edge

    Returns
    -------
    distance: List of distances from node to all other nodes
    pred : List of preceeding nodes for traversal route
    """

    near = []
    v0 = node
    distance = [float('inf') for x in wed.node_list]
    distance[wed.node_list.index(v0)] = 0
    pred = [None for x in wed.node_list]
    a = set()
    a.add(v0)
    while len(a) > 0:
        #Get node with the lowest value from distance
        dist = float('inf')
        for node in a:
            if distance[node] < dist:
                dist = distance[node]
                v = node
        #Remove that node from the set
        a.remove(v)
        last = v
        if distance[v] <= threshold:
            near.append(v)
        elif distance[v] > threshold:
            break
        #4. Get the neighbors to the current node
        neighbors = get_neighbor_distances(wed, v, cost)
        for v1, indiv_cost in neighbors.iteritems():
            if distance[v1] > distance[v] + indiv_cost:
                distance[v1] = distance[v] + indiv_cost
                pred[v1] = v
                a.add(v1)
    return near,pred


def knn_distance(wed, cost, node, n):
    """
    Compute the shortest path between a start node and
        all other nodes in the wed.

    Parameters
    ----------
    wed: PySAL Winged Edged Data Structure
    cost: Cost per edge to travel, e.g. distance
    node: Start node ID
    n: integer number of nearest neighbors
    Returns:
    distance: List of distances from node to all other nodes
    pred : List of preceeding nodes for traversal route
    """

    near = []  #Set because we can update distance more than once
    v0 = node
    distance = [float('inf') for x in wed.node_list]
    distance[wed.node_list.index(v0)] = 0
    pred = [None for x in wed.node_list]
    a = set()
    a.add(v0)
    while len(a) > 0:
        #Get node with the lowest value from distance
        dist = float('inf')
        for node in a:
            if distance[node] < dist:
                dist = distance[node]
                v = node
                near.append(v)
        #Remove that node from the set
        a.remove(v)
        last = v
        #4. Get the neighbors to the current node
        neighbors = get_neighbor_distances(wed, v, cost)
        for v1, indiv_cost in neighbors.iteritems():
            if distance[v1] > distance[v] + indiv_cost:
                distance[v1] = distance[v] + indiv_cost
                pred[v1] = v
                a.add(v1)
    near = nsmallest(n + 1, distance)
    near.remove(0)  #Remove obs from near
    for i,n in enumerate(near):
        near[i] = distance.index(n)
    return near

def lat2Network(k):
    """helper function to create a network from a square lattice.

    Used for testing purposes
    """
    lat = ps.lat2W(k+1,k+1)
    k1 = k+1
    nodes = {}
    edges = []
    for node in lat.id_order:
        for neighbor in lat[node]:
            edges.append((node,neighbor))
        nodes[node] = ( node/k1, node%k1 )

    res = {"nodes": nodes, "edges": edges}

    return res


def polyShp2Network(shpFile):
    nodes = {}
    edges = {}
    f = ps.open(shpFile, 'r')
    for shp in f:
        verts = shp.vertices
        nv = len(verts)
        for v in range(nv-1):
            start = verts[v]
            end = verts[v+1]
            nodes[start] = start
            nodes[end] = end
            edges[(start,end)] = (start,end)
    f.close()
    return {"nodes": nodes, "edges": edges.values() }


def euler_nonplaner_test(e, v):
    """
    Testing for nonplanarity based on necessary condition for planarity

    Parameters
    ----------

    e: int
       number of edges
    v: int
       number of vertices


    Returns
    -------

    True if planarity condition is violated, otherwise false.

    Notes
    -----

    This is only a necessary but not sufficient condition for planarity. In
    other words violating this means the graph is nonplanar, but passing it
    does not guarantee the graph is planar.

    """

    if e <= (3*v - 6):
        return False
    else:
        return True

def area2(A, B, C):
    return (B[0]-A[0]) * (C[1]-A[1]) - (C[0]-A[0]) * (B[1]-A[1])

def isLeft(A, B, C):
    return area2(A,B,C) > 0

def isRight(A, B, C):
    return area2(A,B,C) < 0

def isCollinear(A, B, C):
    return area2(A, B, C) == 0

def intersect(A, B, C, D):

    if isCollinear(A, B, D) or isCollinear(A, B, C):
        return True
    elif isLeft(A, B, D) * isRight(A, B, C):
        if isLeft(C, D, A) != isLeft(C, D, B):
            return True
        else:
            return False
    elif isLeft(A, B, C) * isRight(A, B, D):
        if isLeft(C, D, A) != isLeft(C, D, B):
            return True
        else:
            return False
    else:
        return False

def segIntersect(s1, s2):
    A, B = s1
    C, D = s2
    return intersect(A, B, C, D)

def intersection_sweep(segments, findAll = True):
    """
    Plane sweep segment intersection detection.


    Parameters
    ----------

    segments: list of lists
              each segment is a list containing tuples of segment start/end points

    findAll : boolean
              If True return all segment intersections, otherwise stop after
              first detection

    Examples
    --------

    >>> import util
    >>> segments = [ [(4.5,0), (4.5,4.5)], [(4.5,1), (4.5,2)], [(4,4), (1,4)], [(2,3), (5,3)], [(5,0), (5,10)] ]
    >>> util.intersection_sweep(segments)
    [(0, 3), (4, 3)]
    >>> util.intersection_sweep(segments, findAll=False)
    [(0, 3)]

    """               
    Q = []
    slopes = []
    intercepts = []
    for i,seg in enumerate(segments):
        seg.sort()
        l,r = seg
        Q.append([l,i])
        Q.append([r,i])

        m = r[1] - l[1]  
        dx = r[1] - r[0]
        if dx == 0:
            m = 0
            intercept = r[1] # vertical segment
        else:
            m = m / dx
            intercept = r[1] - m * r[0]
        slopes.append(m)
        intercepts.append(intercept)

    Q.sort()  # event point que sorted on x coord
    status = []

    visited = [0] * len(segments)
    intersections = []


    while Q:
        event_point, i = Q.pop(0)
        if visited[i]:
            # right end point so we are leaving
            # check for intersection between i's left and right neighbors on
            # the status
            if position > 0 and position < ns-1:
                left = sorted_y[position-1][1] 
                right = sorted_y[position+1][1]
                p0,p1 = segments[left]
                p2,p3 = segments[right]
                if intersect(p0, p1, p2, p3):
                    intersections.append( (left,right) )
                    if not findAll:
                        Q = []
            # remove i from status
            status.remove(i)
        else:
            sorted_y = []
            visited[i] = 1
            xi = event_point[0]
            yi = event_point[1]
            sorted_y.append( (yi, i) )

            # insert in status
            # have to handle vertical segments differently
            for seg in status:
                y = slopes[seg] * xi + intercepts[seg]
                sorted_y.append( (y, seg) )
            sorted_y.sort()
            position = sorted_y.index( (yi, i) )
            ns = len(sorted_y)
            # check for intersection with left neighbor
            if position  > 0:
                left = sorted_y[position-1][1] 
                p0,p1 = segments[left]
                p2,p3 = segments[i]
                if intersect(p0, p1, p2, p3):
                    intersections.append( (left,i) )
                    if not findAll:
                        Q = []

            # check for intersection with right neighbor
            if position < ns-1:
                right = sorted_y[position+1][1] 
                p0,p1 = segments[right]
                p2,p3 = segments[i]
                if intersect(p0, p1, p2, p3):
                    intersections.append( (i, right))
                    if not findAll:
                        Q = []
            status.append(i)

    return intersections


def isPlanar(segments):
    """
    Test if a sequence of segments represents a planar network

    Parameters
    ----------

    segments: list of lists
              each segment is a list containing tuples of segment start/end points

    Notes
    -----

    Only proper intersections result in detection of non-planarity. Collinear
    intersections will not result in non-planarity.

    """
    intersections = intersection_sweep(segments, findAll=True)
    proper = []
    for intersection in intersections:
        i,j = intersection
        si = set(segments[i])
        sj = set(segments[j])
        if len(si.union(sj)) == 4:
            return False
    return True





def _test():
    import doctest
    # the following line could be used to define an alternative to the '<BLANKLINE>' flag
    #doctest.BLANKLINE_MARKER = 'something better than <BLANKLINE>'
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)


if __name__ == '__main__':
    #_test()
    pass


########NEW FILE########
__FILENAME__ = utils
#Utilities to support analysis with a WED



########NEW FILE########
__FILENAME__ = wed
"""Winged-Edge Data Structure and Functions


TO DO

- handle hole region explicitly
- relax assumption about integer node ids
- test on other edge cases besides Eberly

"""


__author__ = "Sergio Rey <sjsrey@gmail.com>, Jay Laura <jlaura@asu.edu>"

import ast
import json
import cPickle
import numpy as np
import pysal as ps
from pysal.cg import Point, Polygon, LineSegment, KDTree
from pysal.cg.standalone import get_points_dist
import copy
import operator
import math
from itertools import combinations
import collections


class WED(object):
    """Winged-Edge Data Structure


    """

    def __init__(self, edges=None, coords=None):

        self.start_c = None
        self.start_cc = None
        self.end_c = None
        self.end_cc = None
        self.region_edge = None
        self.node_edge = None
        self.right_polygon = None
        self.left_polygon = None
        self.start_node = None
        self.end_node = None
        self.node_coords = None
        self.edge_list = []

        if edges is not None and coords is not None:
            #Check for single edges and double if needed
            edges = self.check_edges(edges)
            self.edge_list[:] = edges

            #Create the WED object
            self.extract_wed(edges, coords)

    def check_edges(self, edges):
        """
        Validator to ensure that edges are double.

        Parameters
        ----------
        edges: list
            edges connecting nodes in the network

        Returns
        -------
        dbl_edges / edges: list
            Either the original edges or double edges
        """

        seen = set()
        seen_add = seen.add
        seen_twice = set()
        for e in edges:
            if e in seen:
                seen_twice.add(e)
            seen_add(e)
            seen_add((e[1], e[0]))
        if len(list(seen_twice)) != len(edges) / 2:
            dbl_edges = []
            for e in edges:
                dbl_edges.append(e)
                dbl_edges.append((e[1], e[0]))
            return dbl_edges
        else:
            return edges

    def enum_links_node(self, node):
        """
        Enumerate links in cw order around a node

        Parameters
        ----------

        node: string/int
            id for the node in wed


        Returns
        -------

        links: list
            links ordered cw around node
        """

        links = []
        if node not in self.node_edge:
            return links
        l0 = self.node_edge[node]
        links.append(l0)
        l = l0
        v = node
        searching = True
        while searching:
            if v == l[0]:
                l = self.start_c[l]
            else:
                l = self.end_c[l]
            if (l is None) or (set(l) == set(l0)):
                searching = False
            else:
                links.append(l)
        return links

    def enum_edges_region(self, region):
        """
        Enumerate the edges of a region/polygon in cw order

        Parameters
        ----------

        region: id for the region in wed


        Returns
        -------

        links: list of links ordered cw that define the region/polygon

        """
        right_polygon = self.right_polygon
        end_cc = self.end_cc
        start_cc = self.start_cc
        region_edge = self.region_edge
        l0 = region_edge[region]
        l = copy.copy(l0)
        edges = []
        edges.append(l)
        traveling = True
        while traveling:
            if region == right_polygon[l]:
                l = end_cc[l]
            else:
                l = start_cc[l]
            edges.append(l)
            if set(l) == set(l0):
                traveling = False
        return edges

    def w_links(self):
        """
        Generate Weights object for links in a WED

        Parameters
        ----------
        None

        Returns

        ps.W(neighbors): PySAL Weights Dict
        """
        nodes = self.node_edge.keys()
        neighbors = {}
        for node in nodes:
            lnks = self.enum_links_node(node)
            # put i,j s.t. i < j
            lnks = [tuple(sorted(lnk)) for lnk in lnks]
            for comb in combinations(range(len(lnks)), 2):
                l, r = comb
                if lnks[l] not in neighbors:
                    neighbors[lnks[l]] = []
                neighbors[lnks[l]].append(lnks[r])
                if lnks[r] not in neighbors:
                    neighbors[lnks[r]] = []
                neighbors[lnks[r]].append(lnks[l])
        return ps.W(neighbors)

    def _filament_links_node(self, node, node_edge, start_c, end_c):
        """
        Private method that duplicates enum_links_around_node, but
         is callable before the WED is generated.  This is used
         for filament insertion.
        """
        links = []
        if node not in node_edge:
            return links
        l0 = node_edge[node]
        links.append(l0)
        l = l0
        v = node
        searching = True
        while searching:
            if v == l[0]:
                l = start_c[l]
            else:
                l = end_c[l]
            if (l is None) or (set(l) == set(l0)):
                searching = False
            else:
                links.append(l)
        return links

    def extract_wed(self, edges, coords):
        # helper functions to determine relative position of vectors
        def _dotproduct(v1, v2):
            return sum((a * b) for a, b in zip(v1, v2))

        def _length(v):
            return math.sqrt(_dotproduct(v, v))

        def _angle(v1, v2):
            return math.acos(_dotproduct(v1, v2) / (_length(v1) * _length(v2)))

        """
        Extract the Winged Edge Data structure for a planar graph


        Arguments
        ---------

        edges:  list
                tuples of origin, destination nodes for each edge

        coords: dict
                key is node id, value is a tuple of x,y coordinates for the node


        Returns
        -------
        wed: Dictionary holding the WED with 10 keys

            start_node: dict
                        key is node, value is edge with node as start node

            end_node:   dict
                        key is node, value is edge with node as end node

            right_polygon: dict
                            key is edge, value is id of right polygon to edge

            left_polygon: dict
                        key is edge, value is id of left polygon to edge

            node_edge: dict
                        key is node, value is edge associated with the node

            region_edge: dict
                        key is region, value is an edge on perimeter of region

            start_c:   dict
                        key is edge, value is first edge encountered when rotating
                        clockwise around edge start node

            start_cc:  dict
                        key is edge, value is first edge encountered when rotating
                        counterclockwise around edge start node

            end_c:     dict
                        key is edge, value is first edge encountered when rotating
                        clockwise around edge start end node

            end_cc:    dict
                        key is edge, value is first edge encountered when rotating
                        counterclockwise around edge start end node

        """

        # coords will be destroyed so keep a copy around
        coords_org = coords.copy()

        # find minimum cycles, filaments and isolated nodes
        pos = coords.values()
        mcb = self.regions_from_graph(coords, edges)

        regions = mcb['regions']
        edges = mcb['edges']
        vertices = mcb['vertices']
        start_node = {}
        end_node = {}
        for edge in edges:
            if edge[0] != edge[1]:  # no self-loops
                start_node[edge] = edge[0]
                end_node[edge] = edge[1]

        # Right polygon for each edge in each region primitive
        #
        # Also define start_c, end_cc for each polygon edge and
        #  start_cc and end_c for its twin

        right_polygon = {}
        left_polygon = {}
        region_edge = {}
        start_c = {}
        start_cc = {}
        end_c = {}
        end_cc = {}
        node_edge = {}
        for ri, region in enumerate(regions):
            # regions are ccw in mcb
            region.reverse()
            r = [region[-2]]
            r.extend(region)
            r.append(region[1])
            for i in range(len(region) - 1):
                edge = r[i + 1], r[i + 2]
                if edge[0] not in node_edge:
                    node_edge[edge[0]] = edge
                if edge[1] not in node_edge:
                    node_edge[edge[1]] = edge
                start_c[edge] = r[i], r[i + 1]
                end_cc[edge] = r[i + 2], r[i + 3]
                right_polygon[edge] = ri
                twin = edge[1], edge[0]
                left_polygon[twin] = ri
                start_cc[twin] = end_cc[edge]
                end_c[twin] = start_c[edge]
            region_edge[ri] = edge


        # Test for holes
        # Need to add

        #lp = right_polygon[20,24] # for now just assign as placeholder
        #left_polygon[26,25] = lp
        #left_polygon[25,27] = lp
        #left_polygon[27,26] = lp

        # Edges belonging to a minimum cycle at this point without a left
        # region have external bounding polygon as implicit left poly. Assign this
        # explicitly
        rpkeys = right_polygon.keys() # only minimum cycle regions have explicit right polygons
        noleft_poly = [k for k in rpkeys if k not in left_polygon]

        for edge in noleft_poly:
            left_polygon[edge] = ri+1
        # Fill out s_c, s_cc, e_c, e_cc pointers for each edge (before filaments are added)
        regions = region_edge.keys()

        # Find the union of adjacent faces/regions
        unions = []
        while noleft_poly:
            path =[]
            current = noleft_poly.pop()
            path_head = current[0]
            tail = current[1]
            path.append(current)
            while tail != path_head:
                candidates = [ edge for edge in noleft_poly if edge[0] == tail ]
                j=0
                if len(candidates) > 1:
                    # we want candidate that forms largest ccw angle from current
                    angles = []
                    origin = pos[current[1]]
                    x0 = pos[current[0]][0] - origin[0]
                    y0 = pos[current[0]][1] - origin[1]
                    maxangle = 0.0
                    v0 = (x0,y0)

                    for i,candidate in enumerate(candidates):
                        x1 = pos[candidate[1]][0] - origin[0]
                        y1 = pos[candidate[1]][1] - origin[1]
                        v1 = (x1,y1)
                        v0_v1 = _angle(v0, v1)
                        if v0_v1 > maxangle:
                            maxangle = v0_v1
                            j=i
                        angles.append(v0_v1)

                next_edge = candidates[j]
                path.append(next_edge)
                noleft_poly.remove(next_edge)
                tail = next_edge[1]
            unions.append(path)


        # unions has the paths that trace out the unions of contiguous regions (in cw order)


        # Walk around each union in cw fashion
        # start_cc[current] = prev
        # end_c[current] = next
        for union in unions:
            for prev, edge in enumerate(union[1:-1]):
                start_cc[edge] = union[prev]
                end_c[edge] = union[prev+2]
            start_cc[union[0]] = union[-1]
            end_c[union[0]] = union[1]
            end_c[union[-1]] = union[0]
            start_cc[union[-1]] = union[-2]

        # after this find the holes in the external polygon (these should be the connected components)

        # Fill out s_c, s_cc, e_c, e_cc pointers for each edge after filaments are inserted

        regions = [set(region) for region in mcb['regions']]
        filaments = mcb['filaments']
        filament_region = {}
        for f, filament in enumerate(filaments):
            filament_region[f] = []
            #print "Filament: ", filament
            # set up pointers on filament edges prior to insertion
            ecc, ec, scc, sc, node_edge = self.filament_pointers(filament, node_edge)
            end_cc.update(ecc)
            start_c.update(sc)
            start_cc.update(scc)
            end_c.update(ec)

            # find which regions the filament is incident to
            sf = set(filament)
            incident_nodes = set()
            incident_regions = set()
            for r, region in enumerate(regions):
                internal = False
                sfi = sf.intersection(region)
                while sfi:
                    incident_nodes.add(sfi.pop())
                    incident_regions.add(r)

            while incident_nodes:
                incident_node = incident_nodes.pop()
                incident_links = self._filament_links_node(incident_node,node_edge, start_c, end_c)

                #Polar coordinates centered on incident node, no rotation from x-axis
                origin = coords_org[incident_node]

                #Logic: If the filament has 2 nodes, grab the other one
                # If the filament has 3+, grab the first and last segments
                if filament.index(incident_node) == 0:
                    f = filament[1]
                elif filament.index(incident_node) == 1:
                    f = filament[0]
                else:
                    f = filament[-2]
                filament_end = coords_org[f]
                #print "Filament:{}, Incident_Node:{} ".format(f, incident_node)
                #Determine the relationship between the origin and the filament end
                filamentx = filament_end[0] - origin[0]
                filamenty = filament_end[1] - origin[1]
                filament_theta = math.atan2(filamenty, filamentx) * 180 / math.pi
                if filament_theta < 0:
                    filament_theta += 360
                #Find the rotation necessary to get the filament to theta 0
                f_rotation = 360 - filament_theta

                link_angles = {}
                for link in incident_links:
                    if link[0] == incident_node:
                        link_node = link[1]
                    else:
                        link_node = link[0]
                    #Get the end coord of the incident link
                    link_node_coords = coords_org[link_node]
                    y = link_node_coords[1] - origin[1]
                    x = link_node_coords[0] - origin[0]
                    r = math.sqrt(x**2 + y**2)
                    node = coords_org[link_node]
                    node_theta = math.atan2(y, x) * 180 / math.pi
                    if node_theta < 0:
                        node_theta += 360
                    #Rotate the edge node to match the new polar axis
                    node_theta += f_rotation
                    if node_theta > 360:
                        node_theta -= 360
                    link_angles[link] = node_theta

                #Get the bisected edges
                ccwise = min(link_angles, key=link_angles.get)
                cwise = max(link_angles, key=link_angles.get)
                #Fix the direction of the bisected edges
                if ccwise.index(incident_node) != 1:
                    ccwise = (ccwise[1], ccwise[0])
                if cwise.index(incident_node) != 1:
                    cwise = (cwise[1], cwise[0])
                #Update the filament pointer in the direction (segment end, incident node)
                end_c[(f, incident_node)] = (cwise[1], cwise[0])
                end_cc[(f, incident_node)] = (ccwise[1], ccwise[0])
                #Reverse the edge direction
                start_c[(incident_node, f)] = (tuple(cwise))
                start_cc[(incident_node, f)] = (tuple(ccwise))
                #Update the bisected edge points in the direction(segment end, incident node)
                #Cwise link
                end_cc[cwise] = (incident_node, f)
                start_cc[(cwise[1],cwise[0])] = (incident_node, f)
                #CCWise link
                start_c[(ccwise[1], ccwise[0])] = (incident_node, f)
                end_c[ccwise] = (incident_node, f)
                #Now we need to update the right and left polygon for the filament.
                for r in incident_regions:
                     poly = ps.cg.Polygon([coords_org[v] for v in regions[r]])
                     if poly.contains_point((coords_org[filament[1]]) or pr.contains_point(coords_org[filament[0]])):
                            for n in range(len(filament)-1):
                                right_polygon[(filament[n], filament[n+1])] = r
                                left_polygon[(filament[n], filament[n+1])] = r
                                right_polygon[(filament[n+1], filament[n])] = r
                                left_polygon[(filament[n+1], filament[n])] = r

                #print "For filament {}: ".format((incident_node, f))
                #print "    CCW Most edge is {}".format(ccwise)
                #print "    CW Most edge is {}".format(cwise)







            '''
            if incident_regions:
                for r in incident_regions:
                    poly_region = ps.cg.Polygon([coords_org[v] for v in regions[r]])
                    print poly_region
            '''
            """
                if sfi:
                    node = sfi.pop()
                    filament_region[f].append(r)
                    #print "Region: ",filament_region[f]
                    # The logic here is that, if the filament is internal to the
                    # region we are good to go.  If it is external to the region it
                    # will never break and we are good to go.  If the filament is
                    # internal to one region and external to one or mroe regions,
                    # it will set the pointers incorrectly until it hits the
                    # internal region.  Then it sets the pointers based on the
                    # internal region and breaks.
                    region = []
                    for v in regions[r]:
                        region.append(coords_org[v])
                    pr = ps.cg.Polygon(region)
                    #print filament, filament[1]
                    if pr.contains_point(coords_org[filament[1]]) or pr.contains_point(coords_org[filament[0]]):
                        #print "Internal: ", r, filament
                        internal = True
                    # find edges in region that that are adjacent to sfi
                    # find which pair of edges in the region that the filament bisects
                    if mcb['regions'][r].count(node) == 2:
                        e1 = node, mcb['regions'][r][-2]
                        e2 = node, mcb['regions'][r][1]
                    else:
                        i = mcb['regions'][r].index(node)
                        e1 = node, mcb['regions'][r][i - 1]
                        e2 = node, mcb['regions'][r][i + 1]
                    # get filament edge
                    fi = filament.index(node)
                    fstart = True # start of filament is adjacent node to region
                    if filament[-1] == filament[fi]:
                        filament.reverse() # put end node at tail of list
                        fstart = False # end of filament is adjacent node to region
                    fi = 0
                    fj = 1
                    A = vertices[e1[1]]
                    B = vertices[e1[0]]
                    C = vertices[filament[fj]]
                    area_abc = A[0] * (B[1] - C[1]) + B[0] * (C[1] - A[1]) + C[0] * (A[1] - B[1])
                    D = vertices[e2[0]]
                    E = vertices[e2[1]]
                    area_dec = D[0] * (E[1] - C[1]) + E[0] * (C[1] - D[1]) + C[0] * (D[1] - E[1])

                    if area_abc < 0 and area_dec < 0:
                        # inside a region
                        end_cc[e1[1],e1[0]] = filament[fi],filament[fj]
                        start_c[e2] = filament[fi],filament[fj]
                        start_c[filament[fi],filament[fj]] = e1[1],e1[0]
                        start_cc[filament[fi],filament[fj]] = e2
                        right_polygon[filament[fi],filament[fj]] = r
                        left_polygon[filament[fi],filament[fj]] = r
                        right_polygon[filament[fj], filament[fi]] = r
                        left_polygon[filament[fj], filament[fi]] = r
                        end_cc[filament[fj], filament[fi]] = e2 # twin of first internal edge so enumerate region works

                        n_f = len(filament) - 1 # number of filament edges
                        for j in range(1, n_f):
                            sj = j
                            ej = j + 1
                            right_polygon[filament[sj], filament[ej]] = r
                            left_polygon[filament[sj], filament[ej]] = r
                            right_polygon[filament[ej], filament[sj]] = r
                            left_polygon[filament[ej], filament[sj]] = r
                        #last edge
                        right_polygon[filament[-1], filament[-2]] = r
                        left_polygon[filament[-1], filament[-2]] = r
                        right_polygon[filament[-2], filament[-1]] = r
                        left_polygon[filament[-2], filament[-1]] = r

                    else:
                        #print 'outside', filament[fi], filament[fj
                        end_c[e1[1],e1[0]] = filament[fi],filament[fj]
                        start_cc[e2] = filament[fi],filament[fj]
                        start_cc[filament[fi],filament[fj]] = e1[1],e1[0]
                        start_c[filament[fi],filament[fj]] = e2

                        n_f = len(filament) - 1 # number of filament edges
                        for j in range(1,n_f):
                            sj = j
                            ej = j + 1
                            start_c[filament[sj],filament[ej]] = filament[sj-1], filament[sj]
                            start_cc[filament[sj],filament[ej]] = filament[sj-1], filament[sj]
                            end_c[filament[sj-1], filament[sj]] = filament[sj],filament[ej]
                            end_cc[filament[sj-1], filament[sj]] = filament[sj],filament[ej]

                        # last edge
                        end_c[filament[-2],filament[-1]] = filament[-2],filament[-1]
                        end_cc[filament[-2],filament[-1]] = filament[-2],filament[-1]

                if internal is True:
                    break
            """
        self.start_c = start_c
        self.start_cc = start_cc
        self.end_c = end_c
        self.end_cc = end_cc
        self.region_edge = region_edge
        self.node_edge = node_edge
        self.right_polygon = right_polygon
        self.left_polygon = left_polygon
        self.start_node = start_node
        self.end_node = end_node
        self.node_coords = coords_org



    @staticmethod
    def filament_pointers(filament, node_edge={}):
        """
        Define the edge pointers for a filament


        Arguments
        ---------

        filament:   list
                    ordered nodes defining a graph filament where a filament is
                    defined as a sequence of ordered nodes with at least one
                    internal node having incidence=2

        node_edge:  dict
                    key is a node, value is the edge the node is assigned to

        Returns
        -------

        ecc:    dict
                key is edge, value is first edge encountered when rotating
                counterclockwise around edge start end node

        ec:     dict
                key is edge, value is first edge encountered when rotating
                clockwise around edge start end node


        scc:    dict
                key is edge, value is first edge encountered when rotating
                counterclockwise around edge start node


        sc:     dict
                key is edge, value is first edge encountered when rotating
                clockwise around edge start node

        node_edge: dict
                key is a node, value is the edge the node is assigned to

        """

        nv = len(filament)
        ec = {}
        ecc = {}
        sc = {}
        scc = {}
        for i in range(nv - 2):
            s0 = filament[i]
            e0 = filament[i + 1]
            s1 = filament[i + 2]
            ecc[s0, e0] = e0, s1
            ecc[s1, e0] = e0, s0
            ec[s0, e0] = e0, s1
            sc[e0, s1] = s0, e0
            scc[e0, s1] = s0, e0
            if s0 not in node_edge:
                node_edge[s0] = s0, e0
            if e0 not in node_edge:
                node_edge[e0] = s0, e0
            if s1 not in node_edge:
                node_edge[s1] = e0, s1
        # wrapper pointers for first and last edges
        ecc[filament[-2], filament[-1]] = filament[-1], filament[-2]
        ec[filament[-2], filament[-1]] = filament[-2], filament[-1]
        ecc[filament[1], filament[0]] = filament[0], filament[1]
        ec[filament[1], filament[0]] = filament[1], filament[0]
        sc[filament[0], filament[1]] = filament[0], filament[1]
        # technically filaments have to have at least intermediate node with incidence 2
        # if there is a single edge it isn't a filament, but we handle it here just in case
        # since the "first" edge not be treated in the for loop (which isn't entered)
        if nv == 2:
            sc[filament[0], filament[1]] = filament[0], filament[1]
            ec[filament[0], filament[1]] = filament[0], filament[1]
            ecc[filament[0], filament[1]] = filament[1], filament[0]
            scc[filament[0], filament[1]] = filament[0], filament[1]
            if filament[0] not in node_edge:
                node_edge[filament[0]] = filament[0], filament[1]
            if filament[1] not in node_edge:
                node_edge[filament[1]] = filament[0], filament[1]
        return ecc, ec, scc, sc, node_edge

    @staticmethod
    def regions_from_graph(nodes, edges, remove_holes=False):
        """
        Extract regions from nodes and edges of a planar graph

        Arguments
        ---------

        nodes: dict
            vertex id as key, coordinates of vertex as value

        edges: list
            (head,tail), (tail, head) edges

        Returns
        ------

        regions: list
                lists of nodes defining a region. Includes the external region

        filaments:  list
                    lists of nodes defining filaments and isolated vertices



        Examples
        --------
        >>> vertices = {0: (1, 8), 1: (1, 7), 2: (4, 7), 3: (0, 4), 4: (5, 4), 5: (3, 5), 6: (2, 4.5), 7: (6.5, 9), 8: (6.2, 5), 9: (5.5, 3), 10: (7, 3), 11: (7.5, 7.25), 12: (8, 4), 13: (11.5, 7.25), 14: (9, 1), 15: (11, 3), 16: (12, 2), 17: (12, 5), 18: (13.5, 6), 19: (14, 7.25), 20: (16, 4), 21: (18, 8.5), 22: (16, 1), 23: (21, 1), 24: (21, 4), 25: (18, 3.5), 26: (17, 2), 27: (19, 2)}
        >>> edges = [(1, 2),(1, 3),(2, 1),(2, 4),(2, 7),(3, 1),(3, 4),(4, 2),(4, 3),(4, 5),(5, 4),(5, 6),(6, 5),(7, 2),(7, 11),(8, 9),(8, 10),(9, 8),(9, 10),(10, 8),(10, 9),(11, 7),(11, 12),(11, 13),(12, 11),(12, 13),(12, 20),(13, 11),(13, 12),(13, 18),(14, 15),(15, 14),(15, 16),(16, 15),(18, 13),(18, 19),(19, 18),(19, 20),(19, 21),(20, 12),(20, 19),(20, 21),(20, 22),(20, 24),(21, 19),(21, 20),(22, 20),(22, 23),(23, 22),(23, 24),(24, 20),(24, 23),(25, 26),(25, 27),(26, 25),(26, 27),(27, 25),(27, 26)]
        >>> r = WED.regions_from_graph(vertices, edges)
        >>> r['filaments']
        [[6, 5, 4], [2, 7, 11], [14, 15, 16]]
        >>> r['regions']
        [[3, 4, 2, 1, 3], [9, 10, 8, 9], [11, 12, 13, 11], [12, 20, 19, 18, 13, 12], [19, 20, 21, 19], [22, 23, 24, 20, 22], [26, 27, 25, 26]]

        Notes
        -----
        Based on
        Eberly http://www.geometrictools.com/Documentation/MinimalCycleBasis.pdf.
        """

        def adj_nodes(start_key, edges):
            """Finds all nodes adjacent to start_key.

            Parameters
            ----------
            start_key: int
                The id of the node to find neighbors of.

            edges: list
                All edges in the graph

            Returns
            -------
            vnext: list
                List of adjacent nodes.
            """
            start_key
            vnext = []
            for edge in edges:
                if edge[0] == start_key:
                    vnext.append(edge[1])
            if len(vnext) == 0:
                pass
                #print "Vertex is end point."
            return vnext

        def find_start_node(nodes, node_coord):
            start_node = []
            minx = float('inf')
            for key, node in nodes.items():
                if node[0] <= minx:
                    minx = node[0]
                    start_node.append(key)
            if len(start_node) > 1:
                miny = float('inf')
                for i in range(len(start_node)):
                    if nodes[i][1] < miny:
                        miny = nodes[i][1]
                    else:
                        start_node.remove(i)
            return nodes[start_node[0]], node_coord[nodes[start_node[0]]]

        def clockwise(nodes, vnext, start_key, v_prev, vertices=None):
            v_curr = np.asarray(nodes[start_key])
            v_next = None
            if v_prev is None:
                v_prev = np.asarray([0, -1]) #This should be a vertical tangent to the start node at initialization.
            else:
                pass
            d_curr = v_curr - v_prev

            for v_adj in vnext:
                #No backtracking
                if np.array_equal(np.asarray(nodes[v_adj]),v_prev) == True:
                    continue
                if type(v_prev) == int:
                    if v_adj == v_prev:
                        continue
                #The potential direction to move in
                d_adj = np.asarray(nodes[v_adj]) - v_curr
                #Select the first candidate
                if v_next is None:
                    v_next = np.asarray(nodes[v_adj])
                    d_next = d_adj
                    convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]
                    if convex <= 0:
                        convex = True
                    else:
                        convex = False
                #Update if the next candidate is clockwise of the current clock-wise most
                if convex is True:
                    if (d_curr[0]*d_adj[1] - d_curr[1]*d_adj[0]) < 0 or (d_next[0]*d_adj[1]-d_next[1]*d_adj[0]) < 0:
                        v_next = np.asarray(nodes[v_adj])
                        d_next = d_adj
                        convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]
                        if convex <= 0:
                            convex = True
                        else:
                            convex = False
                else:
                    if (d_curr[0]*d_adj[1] - d_curr[1]*d_adj[0]) < 0 and (d_next[0]*d_adj[1]-d_next[1]*d_adj[0]) < 0:
                        v_next = np.asarray(nodes[v_adj])
                        d_next = d_adj
                        convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]
                        if convex <= 0:
                            convex = True
                        else:
                            convex = False
            prev_key = start_key
            if vertices == None:
                return tuple(v_next.tolist()), node_coord[tuple(v_next.tolist())], prev_key
            else:
                return tuple(v_next.tolist()), vertices[tuple(v_next.tolist())], prev_key
        def counterclockwise(nodes, vnexts, start_key, prev_key):
            v_next = None
            v_prev = np.asarray(nodes[prev_key])
            v_curr = np.asarray(nodes[start_key])
            d_curr = v_curr - v_prev

            for v_adj in vnexts:
                #Prohibit Back-tracking
                if v_adj == prev_key:
                    continue
                d_adj = np.asarray(nodes[v_adj]) - v_curr

                if v_next == None:
                    v_next = np.asarray(nodes[v_adj])
                    d_next = d_adj
                    convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]

                if convex <= 0:
                    if d_curr[0]*d_adj[1] - d_curr[1]*d_adj[0] > 0 and d_next[0]*d_adj[1] - d_next[1]*d_adj[0] > 0:
                        v_next = np.asarray(nodes[v_adj])
                        d_next = d_adj
                        convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]
                    else:
                        pass
                else:
                    if d_curr[0]*d_adj[1] - d_curr[1]*d_adj[0] > 0 or d_next[0]*d_adj[1]-d_next[1]*d_adj[0] > 0:
                        v_next = np.asarray(nodes[v_adj])
                        d_next = d_adj
                        convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]
                    else:
                        pass
            prev_key = start_key
            try:
                return tuple(v_next.tolist()), node_coord[tuple(v_next.tolist())], prev_key
            except:
                return v_next, None, prev_key
        def remove_edge(v0,v1,edges, ext_edges):
            try:
                ext_edges.append((v0,v1))
                ext_edges.append((v1,v0))
                edges.remove((v0,v1))
                edges.remove((v1,v0))
            except:
                pass
            return edges, ext_edges

        def remove_heap(v0,sorted_nodes):
            sorted_nodes[:] = [x for x in sorted_nodes if x[0] != v0]
            return sorted_nodes

        def remove_node(v0, nodes, nodes_coord, vertices):
            vertices[v0] = nodes[v0]
            del nodes_coord[nodes[v0]]
            del nodes[v0]
            return nodes, nodes_coord, vertices

        def extractisolated(nodes,node_coord,v0,primitives, vertices, ext_edges):
            primitives.append(v0)
            nodes, node_coord, vertices = remove_node(v0, nodes, node_coord, vertices)
            return nodes, node_coord, primitives, vertices, ext_edges

        def extractfilament(v0,v1, nodes, node_coord,sorted_nodes, edges, primitives,cycle_edge, vertices, ext_edges, iscycle=False):
            if (v0,v1) in cycle_edge or (v1,v0) in cycle_edge:
                iscycle = True
            if iscycle == True:
            #This deletes edges that are part of a cycle, but does not add them as primitives.
                if len(adj_nodes(v0,edges)) >= 3:
                    edges, ext_edges = remove_edge(v0,v1,edges, ext_edges)
                    v0 = v1
                    if len(adj_nodes(v0, edges)) == 1:
                        v1 = adj_nodes(v0, edges)[0]
                while len(adj_nodes(v0, edges)) == 1:
                    v1 = adj_nodes(v0, edges)[0]
                    #Here I need to do the cycle check again.
                    iscycle = False
                    if (v0,v1) in cycle_edge or (v1,v0) in cycle_edge:
                        iscycle = True

                    if iscycle == True:
                        edges, ext_edges = remove_edge(v0,v1,edges, ext_edges)
                        nodes, node_coord, vertices = remove_node(v0, nodes, node_coord, vertices)
                        sorted_nodes = remove_heap(v0, sorted_nodes)
                        v0 = v1
                    else:
                        break
                if len(adj_nodes(v0, edges)) == 0:

                    nodes, node_coord, vertices = remove_node(v0, nodes, node_coord, vertices)
                    sorted_nodes = remove_heap(v0, sorted_nodes)
            else:
                #Filament found
                primitive = []
                if len(adj_nodes(v0,edges)) >= 3:
                    primitive.append(v0)
                    edges, ext_edges = remove_edge(v0,v1,edges, ext_edges)
                    v0 = v1
                    if len(adj_nodes(v0, edges)) == 1:
                        v1 = adj_nodes(v0, edges)[0]

                while len(adj_nodes(v0, edges)) == 1:
                    primitive.append(v0)
                    v1 = adj_nodes(v0, edges)[0]
                    sorted_nodes = remove_heap(v0, sorted_nodes)
                    edges, ext_edges = remove_edge(v0, v1, edges, ext_edges)
                    nodes, node_coord, vertices = remove_node(v0, nodes, node_coord, vertices)
                    v0 = v1

                primitive.append(v0)
                if len(adj_nodes(v0, edges)) == 0:

                    sorted_nodes = remove_heap(v0, sorted_nodes)
                    edges, ext_edges = remove_edge(v0, v1, edges, ext_edges)
                    nodes, node_coord, vertices = remove_node(v0, nodes, node_coord, vertices)
                primitives.append((primitive))

            return sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges

        def extract_primitives(start_key,sorted_nodes, edges, nodes, node_coord, primitives,minimal_cycles,cycle_edge, vertices, ext_edges):
            v0 = start_key
            visited = []
            sequence = []
            sequence.append(v0)

            #Find the CWise most vertex
            vnext = adj_nodes(start_key, edges)
            start_node,v1,v_prev = clockwise(nodes,vnext,start_key,prev_key)
            v_curr = v1
            v_prev = v0
            #Find minimal cycle using CCWise rule
            process = True
            if v_curr == None:
                process = False
            elif v_curr == v0:
                process = False
            elif v_curr in visited:
                process = False

            while process == True:
                sequence.append(v_curr)
                visited.append(v_curr)
                vnext = adj_nodes(v_curr, edges)

                v_curr_coords,v_next,v_prev = counterclockwise(nodes,vnext,v_curr, v_prev)
                v_curr = v_next
                if v_curr == None:
                    process = False
                elif v_curr == v0:
                    process = False
                elif v_curr in visited:
                    process = False

            if v_curr is None:
                #Filament found, not necessarily at start_key
                sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges = extractfilament(v_prev, adj_nodes(v_prev, edges)[0],nodes, node_coord, sorted_nodes, edges, primitives, cycle_edge, vertices, ext_edges)

            elif v_curr == v0:
                #Minimal cycle found
                primitive = []
                iscycle=True
                sequence.append(v0)
                minimal_cycles.append(list(sequence))
                #Remove the v0, v1 edges from the graph.
                edges, ext_edges = remove_edge(v0,v1,edges, ext_edges)
                sorted_nodes = remove_heap(v0, sorted_nodes)#Not in pseudo-code, but in source.
                #Mark all the edges as being part of a minimal cycle.
                if len(adj_nodes(v0, edges)) == 1:
                    cycle_edge.append((v0, adj_nodes(v0, edges)[0]))
                    cycle_edge.append((adj_nodes(v0, edges)[0], v0))
                    sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges = extractfilament(v0, adj_nodes(v0, edges)[0],nodes, node_coord, sorted_nodes, edges, primitives,cycle_edge, vertices, ext_edges)
                if len(adj_nodes(v1, edges)) == 1:
                    cycle_edge.append((v1, adj_nodes(v1, edges)[0]))
                    cycle_edge.append((adj_nodes(v1, edges)[0],v1))
                    sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges = extractfilament(v1, adj_nodes(v1, edges)[0],nodes, node_coord, sorted_nodes, edges, primitives, cycle_edge, vertices, ext_edges)

                for i,v in enumerate(sequence[1:-1]):
                    cycle_edge.append((v,sequence[i]))
                    cycle_edge.append((sequence[i],v))

            else:
                #vcurr was visited earlier, so traverse the filament to find the end
                while len(adj_nodes(v0,edges)) == 2:
                    if adj_nodes(v0,edges)[0] != v1:
                        v1 = v0
                        v0 = adj_nodes(v0,edges)[0]
                    else:
                        v1 = v0
                        v0 = adj_nodes(v0, edges)[1]
                sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges = extractfilament(v0,v1,nodes, node_coord, sorted_nodes, edges, primitives,cycle_edge,vertices,ext_edges)

            return sorted_nodes, edges, nodes, node_coord, primitives, minimal_cycles,cycle_edge, vertices, ext_edges
        #1.
        sorted_nodes = sorted(nodes.iteritems(), key=operator.itemgetter(1))
        node_coord = dict (zip(nodes.values(),nodes.keys()))

        #2.
        primitives = []
        minimal_cycles = []
        cycle_edge = []
        prev_key = None #This is only true for the first iteration.
        #This handles edge and node deletion we need populated later.
        vertices = {}
        ext_edges = []

        #3.
        while sorted_nodes: #Iterate through the sorted list
            start_key = sorted_nodes[0][0]
            numadj = len(adj_nodes(start_key, edges))

            if numadj == 0:
                nodes, node_coord, primitives, vertices, ext_edges = extractisolated(nodes,node_coord,start_key,primitives, vertices, ext_edges)
                sorted_nodes.pop(0)
            elif numadj == 1:
                sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges = extractfilament(start_key, adj_nodes(start_key, edges)[0],nodes, node_coord, sorted_nodes, edges,primitives,cycle_edge, vertices, ext_edges)
            else:
                sorted_nodes, edges, nodes, node_coord, primitives, minimal_cycles,cycle_edge, vertices, ext_edges = extract_primitives(start_key,sorted_nodes, edges, nodes, node_coord, primitives, minimal_cycles,cycle_edge, vertices, ext_edges)

        #4. Remove holes from the graph
        if remove_holes == True:
            polys = []
            for cycle in minimal_cycles:
                polys.append(ps.cg.Polygon([ps.cg.Point(vertices[pnt]) for pnt in cycle]))

            pl = ps.cg.PolygonLocator(polys)

            # find all overlapping polygon mbrs
            overlaps ={}
            nump = len(minimal_cycles)
            for i in range(nump):
                overlaps[i] = pl.overlapping(polys[i].bounding_box)

            # for overlapping mbrs (left,right) check if right polygon is contained in left
            holes = []
            for k in overlaps:
                for  pc in overlaps[k]:
                    s = sum( [polys[k].contains_point(v) for v in pc.vertices])
                    if s == len(pc.vertices):
                        # print k, pc
                        holes.append((k,pc))

            for hole in holes:
                outer, inner = hole
                inner = polys.index(inner)
                minimal_cycles.pop(inner)

        #5. Remove isolated vertices
        filaments = []
        for index, primitive in enumerate(primitives):
            if type(primitive) == list:
                filaments.append(primitive)

        results = {}
        results['regions'] = minimal_cycles
        results['filaments'] = filaments
        results['vertices'] = vertices
        results['edges'] = ext_edges
        results['nodes'] = vertices
        return results

    @classmethod
    def wed_from_json(cls,infile, binary=True):
        wed = WED()
        if binary:
            with open(infile, 'r') as f:
                data = cPickle.load(f)
        else:
            with open(infile, 'r') as f:
                data = json.loads(f)

        wed.start_c = {ast.literal_eval(key):value for key, value in data['start_c'].iteritems()}
        wed.start_cc = {ast.literal_eval(key):value for key, value in data['start_cc'].iteritems()}
        wed.end_c = {ast.literal_eval(key):value for key, value in data['end_c'].iteritems()}
        wed.end_cc = {ast.literal_eval(key):value for key, value in data['end_cc'].iteritems()}
        wed.region_edge = {ast.literal_eval(key):value for key, value in data['region_edge'].iteritems()}
        wed.node_edge = {ast.literal_eval(key):value for key, value in data['node_edge'].iteritems()}
        wed.right_polygon = {ast.literal_eval(key):value for key, value in data['right_polygon'].iteritems()}
        wed.left_polygon = {ast.literal_eval(key):value for key, value in data['left_polygon'].iteritems()}
        wed.start_node = {ast.literal_eval(key):value for key, value in data['start_node'].iteritems()}
        wed.end_node = {ast.literal_eval(key):value for key, value in data['end_node'].iteritems()}
        wed.node_coords = {ast.literal_eval(key):value for key, value in data['node_coords'].iteritems()}
        wed.edge_list = data['edge_list']

        return wed

    def wed_to_json(self, outfile, binary=True):
        #keys need to be strings
        new_wed = {}
        for key, value in vars(self).iteritems():
            nested_attr = {}
            if isinstance(value, dict):
                for k2, v2 in value.iteritems():
                    nested_attr[str(k2)] = v2
                new_wed[key] = nested_attr
            else:
                new_wed[key] = value
        #print new_wed['edge_list']
        if binary:
            with open(outfile, 'w') as outfile:
                outfile.write(cPickle.dumps(new_wed, 1))
        else:
            with open(outfile, 'w') as outfile:
                json_str = json.dumps(new_wed, sort_keys=True, indent=4)
                outfile.write(json_str)


    def nearest_point_on_edge(self,v1):
        """
        Computes the distance from the start_node of point observation
         tagged to an edge. Okabe 3.3.3.8 (pg.65)
        """
        pass

    def edge_length(self):
        """
        Compute the cartesian length of all edges.  This is a helper
         function to allow for ratio data with spatial autocorrelation
         analysis.

        Parameters
        ----------
        None

        Returns
        -------
        length : dict {tuple(edge): float(length)}
            The length of each edge.
        """

        lengths = {}
        for edge in self.edge_list:
            lengths[edge] = get_points_dist(self.node_coords[edge[0]],
                                            self.node_coords[edge[1]])
        return lengths

    def assign_points_to_nodes(self, pts):
        #Setup a dictionary that stores node_id:[observations values]
        obs_to_node = {}
        for x in self.node_coords.iterkeys():
            obs_to_node[x] = set()

        #Generate a KDTree of all of the nodes in the wed
        kd_tree = KDTree([node for node in self.node_coords.itervalues()])

        #Iterate over each observation and query the KDTree.
        for index, point in enumerate(pts):
            nn = kd_tree.query(point, k=1)
            obs_to_node[nn[1]].add(attribs[index])

        return obs_to_node


    def assign_points_to_edges(self,points):
        """
        Assigns point observations to network edges

        Arguments
        ---------

        pts: (list) PySAL point objects or tuples of x,y coords

        Returns
        -------

        obs_to_edge: (dict) where key is the edge and value is the observation index.

        Notes
        -----
        Assumes double edges and sets observations to both edges, e.g. [2,0] and [0,2]
        Wrapped in a try / except block incase the edges are single .
        """
        #We can't enumerate over an array of points, so convert to list.
        if not isinstance(points, list):
            pts = points.tolist()
        else:
            pts = points

        #Empty dict with all the edges
        obs_to_edge = {}
        for e in self.edge_list:
            obs_to_edge[e] = set()

        #Build PySAL polygon objects from each region
        polys = {}
        for r in range(len(self.region_edge)):
            edges = self.enum_edges_region(r)
            poly = []
            for e in edges:
                poly.append(Point(self.node_coords[e[0]]))
            polys[r] = (Polygon(poly))

        #Brute force check point in polygon
        for pt_index, pt in enumerate(pts):
            for key, poly in polys.iteritems():
                internal = False
                if ps.cg.standalone.get_polygon_point_intersect(poly, pt):
                    internal = True
                    potential_edges = self.enum_edges_region(key)[:-1]
                    #Flags
                    dist = np.inf
                    e = None
                    #Brute force check all edges of the region
                    for edge in potential_edges:
                        seg = LineSegment(self.node_coords[edge[0]], self.node_coords[edge[1]])
                        ndist = ps.cg.standalone.get_segment_point_dist(seg, pt)[0]
                        if ndist < dist:
                            e = edge
                            dist = ndist
                    obs_to_edge[e].add(pt_index)
                    try:
                        obs_to_edge[e[1], e[0]].add(pt_index)
                    except:
                        pass
                    break
            #Exceptionally brute force - if we aren't in a poly, check all edges
            # added to test how this functions, must be optimized.
            if internal == False:
                #The point is not internal to a polygon.  Now we need to
                # brute force check against all of the unshared edges.
                # Shared edges are known to be internal, so we can skip them.
                dist = np.inf
                e = None
                for edge in self.edge_list:
                    seg = LineSegment(self.node_coords[edge[0]], self.node_coords[edge[1]])
                    ndist = ps.cg.standalone.get_segment_point_dist(seg, pt)[0]
                    if ndist < dist:
                        e = edge
                        dist = ndist
                obs_to_edge[e].add(pt_index)
                try:
                    obs_to_edge[e[1], e[0]].add(pt_index)
                except:
                    pass

        return obs_to_edge

if __name__ == '__main__':
    #Best way to add calls to the doctests in the tests directory?
    pass

###########Redundant or Unused?#################

"""
def connected_component(adjacency, node):

    Find the connected component that a node belongs to

    Arguments
    ---------

    adjacency: (dict) key is a node, value is a list of adjacent nodes

    node: id of node

    Returns
    -------

    visited: list of nodes comprising the connected component containing node

    Notes
    -----
    Relies on a depth first search of the graph

    A = copy.deepcopy(adjacency)
    if node not in A:
        # isolated node
        return [node]
    stack = [node]
    visited = []
    searching = True
    visited.append(node)
    while searching:
        current = stack[-1]
        if A[current]:
            child = A[current].pop()
            if child not in visited:
                visited.append(child)
                stack.append(child)
        else:
            stack.remove(current)
            if not stack:
                searching = False
    return visited


def connected_components(adjacency):

    Find all the connected components in a graph

    Arguments
    ---------
    adjacency: dict
               key is a node, value is a list of adjacent nodes

    Returns
    -------

    components: list of lists for connected components

    nodes = adjacency.keys()
    components = []
    while nodes:
        start = nodes.pop()
        component = connected_component(adjacency, start)
        if len(component) > 1:
            for node in component:
                if node in nodes:
                    nodes.remove(node)
        components.append(component)
    return components
"""


########NEW FILE########
__FILENAME__ = wed_modular
"""Winged-Edge Data Structure and Functions


TO DO

- handle hole region explicitly
- relax assumption about integer node ids
- test on other edge cases besides Eberly

"""


__author__ = "Sergio Rey <sjsrey@gmail.com>, Jay Laura <jlaura@asu.edu>"


import numpy as np
import pysal as ps
import copy
import networkx as nx
from numpy import array
import operator
import math
import net_shp_io

def enum_links_node(wed, node):
    """
    Enumerate links in cw order around a node

    Parameters
    ----------

    wed: Winged edge data instance (see extract_wed function below)

    node: string/int
          id for the node in wed


    Returns
    -------

    links: list
           links ordered cw around node
    """

    start_c = wed['start_c']
    end_c = wed['end_c']
    node_edge = wed['node_edge']
    
    links = []
    if node not in node_edge:
        return links
    l0 = node_edge[node]
    links.append(l0)
    l = l0
    v = node
    searching = True
    while searching:
        if v == l[0]:
            l = start_c[l]
        else:
            l = end_c[l]
        if (l is None) or (set(l) == set(l0)):
            searching = False
        else:
            links.append(l)
    return links
    
def enum_edges_region(wed, region):
    """
    Enumerate the edges of a region/polygon in cw order

    Parameters
    ----------

    wed: Winged edge data instance (see extract_wed function below)

    region: id for the region in wed


    Returns
    -------

    links: list of links ordered cw that define the region/polygon

    """
    right_polygon = wed['right_polygon']
    end_cc = wed['end_cc']
    start_cc = wed['start_cc']
    region_edge = wed['region_edge']
    l0 = region_edge[region]
    l = copy.copy(l0)
    edges = []
    edges.append(l)
    traveling = True
    while traveling:
        if region == right_polygon[l]:
            l = end_cc[l]
        else:
            l = start_cc[l]
        edges.append(l)
        if set(l) == set(l0):
            traveling = False
    return edges

def connected_component(adjacency, node):
    """
    Find the connected component that a node belongs to

    Arguments
    ---------

    adjacency: (dict) key is a node, value is a list of adjacent nodes

    node: id of node

    Returns
    -------

    visited: list of nodes comprising the connected component containing node

    Notes
    -----
    Relies on a depth first search of the graph
    """
    A = copy.deepcopy(adjacency)
    if node not in A:
        # isolated node
        return [node]
    stack = [node]
    visited = []
    children = A[node]
    searching = True
    visited.append(node)
    while searching:
        current = stack[-1]
        if A[current]:
            child = A[current].pop()
            if child not in visited:
                visited.append(child)
                stack.append(child)
        else:
            stack.remove(current)
            if not stack:
                searching = False
    return visited

def connected_components(adjacency):
    """
    Find all the connected components in a graph

    Arguments
    ---------
    adjacency: dict
               key is a node, value is a list of adjacent nodes

    Returns
    -------

    components: list of lists for connected components
    """
    nodes = adjacency.keys()
    components = []
    while nodes:
        start = nodes.pop()
        component = connected_component(adjacency, start)
        if len(component) > 1:
            for node in component:
                if node in nodes:
                    nodes.remove(node)
        components.append(component)
    return components



# helper functions to determine relative position of vectors
def _dotproduct(v1, v2):
    return sum((a*b) for a,b in zip(v1, v2))

def _length(v):
    return math.sqrt(_dotproduct(v,v))

def _angle(v1, v2):
    return math.acos(_dotproduct(v1, v2) / (_length(v1) * _length(v2)))



def filament_pointers(filament, node_edge={}):
    """
    Define the edge pointers for a filament


    Arguments
    ---------

    filament:   list
                ordered nodes defining a graph filament where a filament is
                defined as a sequence of ordered nodes with at least one
                internal node having incidence=2

    node_edge:  dict
                key is a node, value is the edge the node is assigned to

    Returns
    -------

    ecc:    dict
            key is edge, value is first edge encountered when rotating
            counterclockwise around edge start end node

    ec:     dict
            key is edge, value is first edge encountered when rotating
            clockwise around edge start end node
 

    scc:    dict
            key is edge, value is first edge encountered when rotating
            counterclockwise around edge start node


    sc:     dict
            key is edge, value is first edge encountered when rotating clockwise around edge start node

    node_edge: dict
               key is a node, value is the edge the node is assigned to

    """

    nv = len(filament)
    ec = {}
    ecc = {}
    sc = {}
    scc = {}
    for i in range(nv-2):
        s0 = filament[i]
        e0 = filament[i+1]
        s1 = filament[i+2]
        ecc[s0,e0] = e0,s1
        ecc[s1,e0] = e0,s0
        ec[s0,e0] = e0,s1
        sc[e0,s1] = s0,e0
        scc[e0,s1] = s0,e0
        if s0 not in node_edge:
            node_edge[s0] = s0,e0
        if e0 not in node_edge:
            node_edge[e0] = s0,e0
        if s1 not in node_edge:
            node_edge[s1] = e0, s1
    # wrapper pointers for first and last edges
    ecc[filament[-2], filament[-1]] = filament[-1], filament[-2]
    ec[filament[-2], filament[-1]] = filament[-2], filament[-1]
    ecc[filament[1],filament[0]] = filament[0], filament[1]
    ec[filament[1], filament[0]] = filament[1], filament[0]
    sc[filament[0], filament[1]] = filament[0], filament[1]
    # technically filaments have to have at least intermediate node with incidence 2
    # if there is a single edge it isn't a filament, but we handle it here just in case
    # since the "first" edge not be treated in the for loop (which isn't entered)
    if nv == 2:
        sc[filament[0], filament[1]] = filament[0], filament[1]
        ec[filament[0], filament[1]] = filament[0], filament[1]
        ecc[filament[0], filament[1]] = filament[1], filament[0]
        scc[filament[0], filament[1]] = filament[0], filament[1]
        if filament[0] not in node_edge:
            node_edge[filament[0]] = filament[0], filament[1]
        if filament[1] not in node_edge:
            node_edge[filament[1]] = filament[0], filament[1]
    return ecc, ec, scc, sc, node_edge
     
def adj_nodes(start_key, edges):
    start_key
    vnext = []
    for edge in edges:
        if edge[0] == start_key:
            vnext.append(edge[1])
    if len(vnext) == 0:
        pass
        #print "Vertex is end point."
    return vnext

def regions_from_graph(nodes, edges, remove_holes = False):
    """
    Extract regions from nodes and edges of a planar graph

    Arguments
    ---------

    nodes: dict
           vertex id as key, coordinates of vertex as value

    edges: list
           (head,tail), (tail, head) edges

    Returns
    ------

    regions: list
             lists of nodes defining a region. Includes the external region

    filaments:  list
                lists of nodes defining filaments and isolated vertices



    Examples
    --------
    >>> vertices = {0: (1, 8), 1: (1, 7), 2: (4, 7), 3: (0, 4), 4: (5, 4), 5: (3, 5), 6: (2, 4.5), 7: (6.5, 9), 8: (6.2, 5), 9: (5.5, 3), 10: (7, 3), 11: (7.5, 7.25), 12: (8, 4), 13: (11.5, 7.25), 14: (9, 1), 15: (11, 3), 16: (12, 2), 17: (12, 5), 18: (13.5, 6), 19: (14, 7.25), 20: (16, 4), 21: (18, 8.5), 22: (16, 1), 23: (21, 1), 24: (21, 4), 25: (18, 3.5), 26: (17, 2), 27: (19, 2)}
    >>> edges = [(1, 2),(1, 3),(2, 1),(2, 4),(2, 7),(3, 1),(3, 4),(4, 2),(4, 3),(4, 5),(5, 4),(5, 6),(6, 5),(7, 2),(7, 11),(8, 9),(8, 10),(9, 8),(9, 10),(10, 8),(10, 9),(11, 7),(11, 12),(11, 13),(12, 11),(12, 13),(12, 20),(13, 11),(13, 12),(13, 18),(14, 15),(15, 14),(15, 16),(16, 15),(18, 13),(18, 19),(19, 18),(19, 20),(19, 21),(20, 12),(20, 19),(20, 21),(20, 22),(20, 24),(21, 19),(21, 20),(22, 20),(22, 23),(23, 22),(23, 24),(24, 20),(24, 23),(25, 26),(25, 27),(26, 25),(26, 27),(27, 25),(27, 26)]
    >>> r = regions_from_graph(vertices, edges)
    >>> r['filaments']
    [[6, 5, 4], [2, 7, 11], [14, 15, 16]]
    >>> r['regions']
    [[3, 4, 2, 1, 3], [9, 10, 8, 9], [11, 12, 13, 11], [12, 20, 19, 18, 13, 12], [19, 20, 21, 19], [22, 23, 24, 20, 22], [26, 27, 25, 26]]

    Notes
    -----
    Based on
    Eberly http://www.geometrictools.com/Documentation/MinimalCycleBasis.pdf.
    """
    def find_start_node(nodes,node_coord):
        start_node = []
        minx = float('inf')
        for key,node in nodes.items():
            if node[0] <= minx:
                minx = node[0]
                start_node.append(key)
        if len(start_node) > 1:
            miny = float('inf')
            for i in range(len(start_node)):
                if nodes[i][1] < miny:
                    miny = nodes[i][1]
                else:
                    start_node.remove(i)
        return nodes[start_node[0]], node_coord[nodes[start_node[0]]]
    
    def clockwise(nodes,vnext,start_key, v_prev, vertices=None):
        v_curr = np.asarray(nodes[start_key])
        v_next = None
        if v_prev == None:
            v_prev = np.asarray([0,-1]) #This should be a vertical tangent to the start node at initialization.
        else:
            pass
        d_curr = v_curr - v_prev
        
        for v_adj in vnext:
            #No backtracking
            if np.array_equal(np.asarray(nodes[v_adj]),v_prev) == True:
                
                continue
            if type(v_prev) == int:
                if v_adj == v_prev:
                    continue
            
            #The potential direction to move in
            d_adj = np.asarray(nodes[v_adj]) - v_curr
            
            #Select the first candidate
            if v_next is None:
                v_next = np.asarray(nodes[v_adj])
                d_next = d_adj
                convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]
                if convex <= 0:
                    convex = True
                else:
                    convex = False
            
            #Update if the next candidate is clockwise of the current clock-wise most
            if convex == True:
                if (d_curr[0]*d_adj[1] - d_curr[1]*d_adj[0]) < 0 or (d_next[0]*d_adj[1]-d_next[1]*d_adj[0]) < 0:
                    v_next = np.asarray(nodes[v_adj])
                    d_next = d_adj 
                    convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]
                    if convex <= 0:
                        convex = True
                    else:
                        convex = False
            else:
                if (d_curr[0]*d_adj[1] - d_curr[1]*d_adj[0]) < 0 and (d_next[0]*d_adj[1]-d_next[1]*d_adj[0]) < 0:
                    v_next = np.asarray(nodes[v_adj])
                    d_next = d_adj 
                    convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0] 
                    if convex <= 0:
                        convex = True
                    else:
                        convex = False
        prev_key = start_key
        if vertices == None:
            return tuple(v_next.tolist()), node_coord[tuple(v_next.tolist())], prev_key
        else:
            return tuple(v_next.tolist()), vertices[tuple(v_next.tolist())], prev_key
    def counterclockwise(nodes, vnexts, start_key, prev_key):
        v_next = None
        v_prev = np.asarray(nodes[prev_key])
        v_curr = np.asarray(nodes[start_key])
        d_curr = v_curr - v_prev
        
        for v_adj in vnexts: 
            #Prohibit Back-tracking
            if v_adj == prev_key:
                continue
            d_adj = np.asarray(nodes[v_adj]) - v_curr
            
            if v_next == None:
                v_next = np.asarray(nodes[v_adj])
                d_next = d_adj
                convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]
                
            if convex <= 0:
                if d_curr[0]*d_adj[1] - d_curr[1]*d_adj[0] > 0 and d_next[0]*d_adj[1] - d_next[1]*d_adj[0] > 0:
                    v_next = np.asarray(nodes[v_adj])
                    d_next = d_adj 
                    convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]
                else:
                    pass
            else:
                if d_curr[0]*d_adj[1] - d_curr[1]*d_adj[0] > 0 or d_next[0]*d_adj[1]-d_next[1]*d_adj[0] > 0:
                    v_next = np.asarray(nodes[v_adj])
                    d_next = d_adj 
                    convex = d_next[0]*d_curr[1] - d_next[1]*d_curr[0]
                else:
                    pass
        prev_key = start_key
        try:
            return tuple(v_next.tolist()), node_coord[tuple(v_next.tolist())], prev_key
        except: 
            return v_next, None, prev_key
    def remove_edge(v0,v1,edges, ext_edges):
        try:
            ext_edges.append((v0,v1))
            ext_edges.append((v1,v0))
            edges.remove((v0,v1))
            edges.remove((v1,v0))
        except:
            pass
        return edges, ext_edges
    
    def remove_heap(v0,sorted_nodes):
        sorted_nodes[:] = [x for x in sorted_nodes if x[0] != v0]
        return sorted_nodes
            
    def remove_node(v0, nodes, nodes_coord, vertices):
        vertices[v0] = nodes[v0]
        del nodes_coord[nodes[v0]]
        del nodes[v0]
        return nodes, nodes_coord, vertices
        
    def extractisolated(nodes,node_coord,v0,primitives, vertices, ext_edges):
        primitives.append(v0)
        nodes, node_coord, vertices = remove_node(v0, nodes, node_coord, vertices)
        return nodes, node_coord, primitives, vertices, ext_edges
    
    def extractfilament(v0,v1, nodes, node_coord,sorted_nodes, edges, primitives,cycle_edge, vertices, ext_edges, iscycle=False):
        if (v0,v1) in cycle_edge or (v1,v0) in cycle_edge:
            iscycle = True
        if iscycle == True:
        #This deletes edges that are part of a cycle, but does not add them as primitives.
            if len(adj_nodes(v0,edges)) >= 3:
                edges, ext_edges = remove_edge(v0,v1,edges, ext_edges)
                v0 = v1
                if len(adj_nodes(v0, edges)) == 1:
                    v1 = adj_nodes(v0, edges)[0]
            while len(adj_nodes(v0, edges)) == 1:
                v1 = adj_nodes(v0, edges)[0]
                #Here I need to do the cycle check again.
                iscycle = False
                if (v0,v1) in cycle_edge or (v1,v0) in cycle_edge:
                    iscycle = True
                
                if iscycle == True:
                    edges, ext_edges = remove_edge(v0,v1,edges, ext_edges)
                    nodes, node_coord, vertices = remove_node(v0, nodes, node_coord, vertices)
                    sorted_nodes = remove_heap(v0, sorted_nodes)
                    v0 = v1
                else:
                    break
            if len(adj_nodes(v0, edges)) == 0:
                
                nodes, node_coord, vertices = remove_node(v0, nodes, node_coord, vertices)
                sorted_nodes = remove_heap(v0, sorted_nodes)
        else:
            #Filament found
            primitive = []
            if len(adj_nodes(v0,edges)) >= 3:
                primitive.append(v0)
                edges, ext_edges = remove_edge(v0,v1,edges, ext_edges)
                v0 = v1
                if len(adj_nodes(v0, edges)) == 1:
                    v1 = adj_nodes(v0, edges)[0]
    
            while len(adj_nodes(v0, edges)) == 1:
                primitive.append(v0)
                v1 = adj_nodes(v0, edges)[0]
                sorted_nodes = remove_heap(v0, sorted_nodes)
                edges, ext_edges = remove_edge(v0, v1, edges, ext_edges)
                nodes, node_coord, vertices = remove_node(v0, nodes, node_coord, vertices)
                v0 = v1
            
            primitive.append(v0)
            if len(adj_nodes(v0, edges)) == 0:

                sorted_nodes = remove_heap(v0, sorted_nodes)
                edges, ext_edges = remove_edge(v0, v1, edges, ext_edges)
                nodes, node_coord, vertices = remove_node(v0, nodes, node_coord, vertices)
            primitives.append((primitive))
    
        return sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges
    
    def extract_primitives(start_key,sorted_nodes, edges, nodes, node_coord, primitives,minimal_cycles,cycle_edge, vertices, ext_edges):
        v0 = start_key
        visited = []
        sequence = []
        sequence.append(v0)

        #Find the CWise most vertex
        vnext = adj_nodes(start_key, edges)
        start_node,v1,v_prev = clockwise(nodes,vnext,start_key,prev_key)
        v_curr = v1
        v_prev = v0
        #Find minimal cycle using CCWise rule
        process = True
        if v_curr == None:
            process = False
        elif v_curr == v0:
            process = False
        elif v_curr in visited:
            process = False
  
        while process == True:
            sequence.append(v_curr)
            visited.append(v_curr)
            vnext = adj_nodes(v_curr, edges)
    
            v_curr_coords,v_next,v_prev = counterclockwise(nodes,vnext,v_curr, v_prev)
            v_curr = v_next
            if v_curr == None:
                process = False
            elif v_curr == v0:
                process = False
            elif v_curr in visited:
                process = False
    
        if v_curr is None:
            #Filament found, not necessarily at start_key
            sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges = extractfilament(v_prev, adj_nodes(v_prev, edges)[0],nodes, node_coord, sorted_nodes, edges, primitives, cycle_edge, vertices, ext_edges)
            
        elif v_curr == v0:
            #Minimal cycle found
            primitive = []
            iscycle=True
            sequence.append(v0)
            minimal_cycles.append(list(sequence))
            #Remove the v0, v1 edges from the graph.
            edges, ext_edges = remove_edge(v0,v1,edges, ext_edges)
            sorted_nodes = remove_heap(v0, sorted_nodes)#Not in pseudo-code, but in source.
            #Mark all the edges as being part of a minimal cycle.
            if len(adj_nodes(v0, edges)) == 1:
                cycle_edge.append((v0, adj_nodes(v0, edges)[0]))
                cycle_edge.append((adj_nodes(v0, edges)[0], v0))
                sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges = extractfilament(v0, adj_nodes(v0, edges)[0],nodes, node_coord, sorted_nodes, edges, primitives,cycle_edge, vertices, ext_edges)
            if len(adj_nodes(v1, edges)) == 1:
                cycle_edge.append((v1, adj_nodes(v1, edges)[0]))
                cycle_edge.append((adj_nodes(v1, edges)[0],v1))
                sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges = extractfilament(v1, adj_nodes(v1, edges)[0],nodes, node_coord, sorted_nodes, edges, primitives, cycle_edge, vertices, ext_edges)
           
            for i,v in enumerate(sequence[1:-1]):
                cycle_edge.append((v,sequence[i]))
                cycle_edge.append((sequence[i],v))
            
        else:
            #vcurr was visited earlier, so traverse the filament to find the end
            while len(adj_nodes(v0,edges)) == 2:
                if adj_nodes(v0,edges)[0] != v1:
                    v1 = v0
                    v0 = adj_nodes(v0,edges)[0]
                else:
                    v1 = v0
                    v0 = adj_nodes(v0, edges)[1]
            sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges = extractfilament(v0,v1,nodes, node_coord, sorted_nodes, edges, primitives,cycle_edge,vertices,ext_edges)

        return sorted_nodes, edges, nodes, node_coord, primitives, minimal_cycles,cycle_edge, vertices, ext_edges
    #1.
    sorted_nodes = sorted(nodes.iteritems(), key=operator.itemgetter(1))
    node_coord = dict (zip(nodes.values(),nodes.keys()))
    
    #2.
    primitives = []
    minimal_cycles = []
    cycle_edge = []
    prev_key = None #This is only true for the first iteration.
    #This handles edge and node deletion we need populated later.
    vertices = {}
    ext_edges = []    
    
    #3.
    while sorted_nodes: #Iterate through the sorted list
        start_key = sorted_nodes[0][0]
        numadj = len(adj_nodes(start_key, edges))
        
        if numadj == 0:
            nodes, node_coord, primitives, vertices, ext_edges = extractisolated(nodes,node_coord,start_key,primitives, vertices, ext_edges)
            sorted_nodes.pop(0)
        elif numadj == 1:
            sorted_nodes, edges, nodes, node_coord, primitives, vertices, ext_edges = extractfilament(start_key, adj_nodes(start_key, edges)[0],nodes, node_coord, sorted_nodes, edges,primitives,cycle_edge, vertices, ext_edges)
        else:
            sorted_nodes, edges, nodes, node_coord, primitives, minimal_cycles,cycle_edge, vertices, ext_edges = extract_primitives(start_key,sorted_nodes, edges, nodes, node_coord, primitives, minimal_cycles,cycle_edge, vertices, ext_edges)
    
    #4. Remove holes from the graph
    if remove_holes == True:
        polys = []
        for cycle in minimal_cycles:
            polys.append(ps.cg.Polygon([ps.cg.Point(vertices[pnt]) for pnt in cycle]))
            
        pl = ps.cg.PolygonLocator(polys)
           
        # find all overlapping polygon mbrs
        overlaps ={}
        nump = len(minimal_cycles)
        for i in range(nump):
            overlaps[i] = pl.overlapping(polys[i].bounding_box)
        
        # for overlapping mbrs (left,right) check if right polygon is contained in left
        holes = []
        for k in overlaps:
            for  pc in overlaps[k]:
                s = sum( [polys[k].contains_point(v) for v in pc.vertices])
                if s == len(pc.vertices):
                    # print k, pc
                    holes.append((k,pc))    
        
        for hole in holes:
            outer, inner = hole
            inner = polys.index(inner)
            minimal_cycles.pop(inner)
    
    #5. Remove isolated vertices
    filaments = []
    for index, primitive in enumerate(primitives):
        if type(primitive) == list:
            filaments.append(primitive)
    
    results = {}
    results['regions'] = minimal_cycles
    results['filaments'] = filaments
    results['vertices'] = vertices
    results['edges'] = ext_edges
    results['nodes'] = vertices
    return results

def extract_wed(edges, coords):
    """
    Extract the Winged Edge Data structure for a planar graph


    Arguments
    ---------

    edges:  dict
            key is edge id, value is list of adjacent nodes

    coords: dict
            key is node id, value is a tuple of x,y coordinates for the node


    Returns
    -------
    wed: Dictionary holding the WED with 10 keys
        
         start_node: dict
                     key is node, value is edge with node as start node

         end_node:   dict
                     key is node, value is edge with node as end node

         right_polygon: dict
                        key is edge, value is id of right polygon to edge

         left_polygon: dict
                       key is edge, value is id of left polygon to edge

         node_edge: dict
                    key is node, value is edge associated with the node

         region_edge: dict
                      key is region, value is an edge on perimeter of region

         start_c:   dict
                    key is edge, value is first edge encountered when rotating
                    clockwise around edge start node

         start_cc:  dict
                    key is edge, value is first edge encountered when rotating
                    counterclockwise around edge start node

         end_c:     dict
                    key is edge, value is first edge encountered when rotating
                    clockwise around edge start end node

         end_cc:    dict
                    key is edge, value is first edge encountered when rotating
                    counterclockwise around edge start end node

    """

    # find minimum cycles, filaments and isolated nodes
    pos = coords.values()
    mcb = regions_from_graph(coords,edges)
    # Edge pointers
    # 
    # - start_node[edge]
    # - end_node[edge]

    regions = mcb['regions']
    edges = mcb['edges']
    vertices = mcb['vertices']
    start_node = {}
    end_node = {}
    for edge in edges:
        start_node[edge] = edge[0]
        end_node[edge] = edge[1]

    # Right polygon for each edge in each region primitive
    # 
    # Also define start_c, end_cc for each polygon edge and start_cc and end_c for its twin

    right_polygon = {}
    left_polygon = {}
    region_edge = {}
    start_c = {}
    start_cc = {}
    end_c = {}
    end_cc = {}
    node_edge = {}
    for ri,region in enumerate(regions):
        # regions are ccw in mcb
        region.reverse()
        r = [region[-2]]
        r.extend(region)
        r.append(region[1])
        for i in range(len(region)-1):
            edge = r[i+1], r[i+2]
            if edge[0] not in node_edge:
                node_edge[edge[0]] = edge
            if edge[1] not in node_edge:
                node_edge[edge[1]] = edge
            start_c[edge] = r[i],r[i+1]
            end_cc[edge] = r[i+2], r[i+3]
            right_polygon[edge] = ri
            twin = edge[1],edge[0]
            left_polygon[twin] = ri
            start_cc[twin] = end_cc[edge]
            end_c[twin] = start_c[edge]
        region_edge[ri] = edge
         

    # Test for holes
    # Need to add

    #lp = right_polygon[20,24] # for now just assign as placeholder
    #left_polygon[26,25] = lp
    #left_polygon[25,27] = lp
    #left_polygon[27,26] = lp


    # Edges belonging to a minimum cycle at this point without a left
    # region have external bounding polygon as implicit left poly. Assign this
    # explicitly

    rpkeys = right_polygon.keys() # only minimum cycle regions have explicit right polygons
    noleft_poly = [k for k in rpkeys if k not in left_polygon]

    for edge in noleft_poly:
        left_polygon[edge] = ri+1


    # Fill out s_c, s_cc, e_c, e_cc pointers for each edge (before filaments are added)

    regions = region_edge.keys()

    # Find the union of adjacent faces/regions
    unions = []
    while noleft_poly:
        path =[]
        current = noleft_poly.pop()
        path_head = current[0]
        tail = current[1]
        path.append(current)
        while tail != path_head:
            candidates = [ edge for edge in noleft_poly if edge[0] == tail ]
            j=0
            if len(candidates) > 1:
                # we want candidate that forms largest ccw angle from current
                angles = []
                origin = pos[current[1]]
                x0 = pos[current[0]][0] - origin[0]
                y0 = pos[current[0]][1] - origin[1]
                maxangle = 0.0
                v0 = (x0,y0)
                
                for i,candidate in enumerate(candidates):
                    x1 = pos[candidate[1]][0] - origin[0]
                    y1 = pos[candidate[1]][1] - origin[1]
                    v1 = (x1,y1)
                    v0_v1 = _angle(v0, v1)
                    if v0_v1 > maxangle:
                        maxangle = v0_v1
                        j=i
                    angles.append(v0_v1)
                    
            next_edge = candidates[j]
            path.append(next_edge)
            noleft_poly.remove(next_edge)
            tail = next_edge[1]
        unions.append(path)


    # unions has the paths that trace out the unions of contiguous regions (in cw order) 


    # Walk around each union in cw fashion
    # start_cc[current] = prev
    # end_c[current] = next
    for union in unions:
        for prev, edge in enumerate(union[1:-1]):
            start_cc[edge] = union[prev]
            end_c[edge] = union[prev+2]
        start_cc[union[0]] = union[-1]
        end_c[union[0]] = union[1]
        end_c[union[-1]] = union[0]
        start_cc[union[-1]] = union[-2]

    # we need to attach filaments at this point
    # internal filaments get left and right poly set to containing poly
    # external filaments get left and right poly set to external poly
    # bridge filaments get left and right poly set to external poly
    # isolated filaments (not contained in mcb-regions) have left and right poly set to external poly

    # after this find the holes in the external polygon (these should be the connected components)

    # Fill out s_c, s_cc, e_c, e_cc pointers for each edge after filaments are inserted
           
    regions = [set(region) for region in mcb['regions']]
    filaments = mcb['filaments']
    filament_region = {}
    for f,filament in enumerate(filaments):
        filament_region[f] = []
        # set up pointers on filament edges prior to insertion 
        ecc, ec, scc, sc, node_edge = filament_pointers(filament, node_edge)
        end_cc.update(ecc)
        start_c.update(sc)
        start_cc.update(scc)
        end_c.update(ec)
        
        # find which regions the filament is adjacent to
        sf = set(filament)
        for r,region in enumerate(regions):
            sfi = sf.intersection(region)
            if sfi:
                node = sfi.pop()
                filament_region[f].append(r)
                # find edges in region that that are adjacent to sfi
                # find which pair of edges in the region that the filament bisects
                if mcb['regions'][r].count(node) == 2:
                    e1 = node, mcb['regions'][r][-2]
                    e2 = node, mcb['regions'][r][1]
                else:
                    i = mcb['regions'][r].index(node)
                    e1 = node, mcb['regions'][r][i-1]
                    e2 = node, mcb['regions'][r][i+1]
                
                # get filament edge
                fi = filament.index(node)
                fstart = True # start of filament is adjacent node to region
                if filament[-1] == filament[fi]:
                    filament.reverse() # put end node at tail of list
                    fstart = False # end of filament is adjacent node to region
                fi = 0
                fj = 1
                A = vertices[e1[1]]
                B = vertices[e1[0]]
                C = vertices[filament[fj]]
                area_abc = A[0] * (B[1]-C[1]) + B[0] * (C[1]-A[1]) + C[0] * (A[1]- B[1])
                D = vertices[e2[0]]
                E = vertices[e2[1]]
                area_dec = D[0] * (E[1] - C[1]) + E[0] * (C[1] - D[1]) + C[0] * (D[1] - E[1])

                if area_abc < 0 and area_dec < 0:
                    # inside a region
                    end_cc[e1[1],e1[0]] = filament[fi],filament[fj]
                    start_c[e2] = filament[fi],filament[fj]
                    start_c[filament[fi],filament[fj]] = e1[1],e1[0]
                    start_cc[filament[fi],filament[fj]] = e2
                    right_polygon[filament[fi],filament[fj]] = r
                    left_polygon[filament[fi],filament[fj]] = r
                    right_polygon[filament[fj], filament[fi]] = r
                    left_polygon[filament[fj], filament[fi]] = r
                    end_cc[filament[fj], filament[fi]] = e2 # twin of first internal edge so enumerate region works
                    
                    n_f = len(filament) - 1 # number of filament edges
                    for j in range(1,n_f):
                        sj = j
                        ej = j + 1
                        right_polygon[filament[sj],filament[ej]] = r
                        left_polygon[filament[sj],filament[ej]] = r
                        right_polygon[filament[ej],filament[sj]] = r
                        left_polygon[filament[ej],filament[sj]] = r
                    #last edge
                    right_polygon[filament[-1],filament[-2]] = r
                    left_polygon[filament[-1],filament[-2]] = r
                    right_polygon[filament[-2],filament[-1]] = r
                    left_polygon[filament[-2],filament[-1]] = r
                    
                else:
                    #print 'outside', filament[fi], filament[fj]
                    end_c[e1[1],e1[0]] = filament[fi],filament[fj]
                    start_cc[e2] = filament[fi],filament[fj]
                    start_cc[filament[fi],filament[fj]] = e1[1],e1[0]
                    start_c[filament[fi],filament[fj]] = e2
                    
                    n_f = len(filament) - 1 # number of filament edges
                    for j in range(1,n_f):
                        sj = j
                        ej = j + 1
                        start_c[filament[sj],filament[ej]] = filament[sj-1], filament[sj]
                        start_cc[filament[sj],filament[ej]] = filament[sj-1], filament[sj]
                        end_c[filament[sj-1], filament[sj]] = filament[sj],filament[ej]
                        end_cc[filament[sj-1], filament[sj]] = filament[sj],filament[ej]

                    # last edge
                    end_c[filament[-2],filament[-1]] = filament[-2],filament[-1]
                    end_cc[filament[-2],filament[-1]] = filament[-2],filament[-1]
                   
    wed = {}
    wed['start_c'] = start_c
    wed['start_cc'] = start_cc
    wed['end_c'] = end_c
    wed['end_cc'] = end_cc
    wed['region_edge'] = region_edge
    wed['node_edge'] = node_edge
    wed['right_polygon'] = right_polygon
    wed['left_polygon'] = left_polygon
    wed['start_node'] = start_node
    wed['end_node'] = end_node
    return wed

if __name__ == '__main__':

    # Generate the test graph

    # from eberly http://www.geometrictools.com/Documentation/MinimalCycleBasis.pdf
    coords = {}
    coords[0] = 1,8
    coords[1] = 1,7
    coords[2] = 4,7
    coords[3] = 0,4
    coords[4] = 5,4
    coords[5] = 3,5
    coords[6] = 2, 4.5
    coords[7] = 6.5, 9
    coords[8] = 6.2, 5
    coords[9] = 5.5,3
    coords[10] = 7,3
    coords[11] = 7.5, 7.25
    coords[12] = 8,4
    coords[13] = 11.5, 7.25
    coords[14] = 9, 1
    coords[15] = 11, 3
    coords[16] = 12, 2
    coords[17] = 12, 5
    coords[18] = 13.5, 6
    coords[19] = 14, 7.25
    coords[20] = 16, 4
    coords[21] = 18, 8.5
    coords[22] = 16, 1
    coords[23] = 21, 1
    coords[24] = 21, 4
    coords[25] = 18, 3.5
    coords[26] = 17, 2
    coords[27] = 19, 2

    # adjacency lists
    vertices = {}
    for v in range(28):
        vertices[v] = []
        
    vertices[1] = [2,3]
    vertices[2] = [1,4,7]
    vertices[3] = [1,4]
    vertices[4] = [2,3,5]
    vertices[5] = [4,6]
    vertices[6] = [5]
    vertices[7] = [2,11]
    vertices[8] = [9,10]
    vertices[9] = [8,10]
    vertices[10] = [8,9]
    vertices[11] = [7,12,13]
    vertices[12] = [11,13,20]
    vertices[13] = [11,12,18]
    vertices[14] = [15]
    vertices[15] = [14, 16]
    vertices[16] = [15]
    vertices[18] = [13,19]
    vertices[19] = [18,20,21]
    vertices[20] = [12,19,21,22,24]
    vertices[21] = [19,20]
    vertices[22] = [20,23]
    vertices[23] = [22,24]
    vertices[24] = [20,23]
    vertices[25] = [26,27]
    vertices[26] = [25,27]
    vertices[27] = [25,26]

    eberly = vertices.copy()
    pos = coords.values()

    #eg = nx.Graph(vertices)
    #g = nx.Graph(vertices)
    #nx.draw(g,pos = pos)


    edges = []
    for vert in vertices:
        for dest in vertices[vert]:
            edges.append((vert,dest))
            
    wed_res = extract_wed(edges, coords)

    print "Enumeration of links around nodes"
    for node in range(0,28):
        print node, enum_links_node(wed_res, node)

    print "Enumeration of links around regions"
    for region in range(7):
        print region, enum_edges_region(wed_res, region)

    print "Eberly Test Completed \n"

    # new test from eberly shapefile after converting with contrib\spatialnet
    
    coords = {0: (0.0, 4.0),
     1: (1.0, 7.0),
     2: (2.0, 4.5),
     3: (3.0, 5.0),
     4: (4.0, 7.0),
     5: (5.0, 4.0),
     6: (5.5, 3.0),
     7: (6.2, 5.0),
     8: (6.5, 9.0),
     9: (7.0, 3.0),
     10: (7.5, 7.25),
     11: (8.0, 4.0),
     12: (9.0, 1.0),
     13: (11.0, 3.0),
     14: (11.5, 7.25),
     15: (12.0, 2.0),
     16: (13.5, 6.0),
     17: (14.0, 7.25),
     18: (16.0, 1.0),
     19: (16.0, 4.0),
     20: (17.0, 2.0),
     21: (18.0, 3.5),
     22: (18.0, 8.5),
     23: (19.0, 2.0),
     24: (21.0, 1.0),
     25: (21.0, 4.0)}


    edges = [(1, 0),
         (4, 1),
         (4, 5),
         (4, 8),
         (0, 5),
         (5, 3),
         (3, 2),
         (8, 10),
         (7, 6),
         (7, 9),
         (6, 9),
         (10, 11),
         (10, 14),
         (11, 14),
         (11, 19),
         (14, 16),
         (12, 13),
         (13, 15),
         (16, 17),
         (17, 19),
         (17, 22),
         (19, 22),
         (19, 18),
         (19, 25),
         (18, 24),
         (24, 25),
         (21, 20),
         (21, 23),
         (20, 23)]

    
    #Eberly expects double edges.  
    dbl_edges = []
    for e in edges:
        dbl_edges.append(e)
        dbl_edges.append((e[1], e[0]))
        
    wed_1 = extract_wed(dbl_edges, coords)

    print "Enumeration of links around nodes"
    for node in range(0,26):
        print node, enum_links_node(wed_1, node)

    print "Enumeration of links around regions"
    for region in range(7):
        print region, enum_edges_region(wed_1, region)    
        
    print "Eberly Shapefile (non-ordered nodes) Complete"


    # testing reader
    coords, edges = net_shp_io.reader("../contrib/spatialnet/eberly_net.shp")
    wed_2 = extract_wed(edges, coords)


    print "Enumeration of links around nodes"
    for node in range(0,26):
        print node, enum_links_node(wed_1, node)

    print "Enumeration of links around regions"
    for region in range(7):
        print region, enum_edges_region(wed_1, region)    
        
    print "Eberly read Shapefile (non-ordered nodes) Complete"



########NEW FILE########
__FILENAME__ = components
"""
Checking for connected components in a graph.
"""
__author__ = "Sergio J. Rey <srey@asu.edu>"


__all__ = ["check_contiguity"]

from operator import lt


def is_component(w, ids):
    """Check if the set of ids form a single connected component

    Parameters
    ----------

    w   : spatial weights boject

    ids : list
          identifiers of units that are tested to be a single connected
          component


    Returns
    -------

    True    : if the list of ids represents a single connected component

    False   : if the list of ids forms more than a single connected component

    """

    components = 0
    marks = dict([(node, 0) for node in ids])
    q = []
    for node in ids:
        if marks[node] == 0:
            components += 1
            q.append(node)
            if components > 1:
                return False
        while q:
            node = q.pop()
            marks[node] = components
            others = [neighbor for neighbor in w.neighbors[node]
                      if neighbor in ids]
            for other in others:
                if marks[other] == 0 and other not in q:
                    q.append(other)
    return True


def check_contiguity(w, neighbors, leaver):
    """Check if contiguity is maintained if leaver is removed from neighbors


    Parameters
    ----------

    w           : spatial weights object
                  simple contiguity based weights
    neighbors   : list
                  nodes that are to be checked if they form a single \
                          connected component
    leaver      : id
                  a member of neighbors to check for removal


    Returns
    -------

    True        : if removing leaver from neighbors does not break contiguity
                  of remaining set
                  in neighbors
    False       : if removing leaver from neighbors breaks contiguity

    Example
    -------

    Setup imports and a 25x25 spatial weights matrix on a 5x5 square region.

    >>> import pysal
    >>> w = pysal.lat2W(5, 5)

    Test removing various areas from a subset of the region's areas.  In the
    first case the subset is defined as observations 0, 1, 2, 3 and 4. The
    test shows that observations 0, 1, 2 and 3 remain connected even if
    observation 4 is removed.

    >>> pysal.region.check_contiguity(w,[0,1,2,3,4],4)
    True
    >>> pysal.region.check_contiguity(w,[0,1,2,3,4],3)
    False
    >>> pysal.region.check_contiguity(w,[0,1,2,3,4],0)
    True
    >>> pysal.region.check_contiguity(w,[0,1,2,3,4],1)
    False
    >>>
    """

    ids = neighbors[:]
    ids.remove(leaver)
    return is_component(w, ids)


class Graph(object):
    def __init__(self, undirected=True):
        self.nodes = set()
        self.edges = {}
        self.cluster_lookup = {}
        self.no_link = {}
        self.undirected = undirected

    def add_edge(self, n1, n2, w):
        self.nodes.add(n1)
        self.nodes.add(n2)
        self.edges.setdefault(n1, {}).update({n2: w})
        if self.undirected:
            self.edges.setdefault(n2, {}).update({n1: w})

    def connected_components(self, threshold=0.9, op=lt):
        if not self.undirected:
            warn = "Warning, connected _components not "
            warn += "defined for a directed graph"
            print warn
            return None
        else:
            nodes = set(self.nodes)
            components, visited = [], set()
            while len(nodes) > 0:
                connected, visited = self.dfs(
                    nodes.pop(), visited, threshold, op)
                connected = set(connected)
                for node in connected:
                    if node in nodes:
                        nodes.remove(node)
                subgraph = Graph()
                subgraph.nodes = connected
                subgraph.no_link = self.no_link
                for s in subgraph.nodes:
                    for k, v in self.edges.get(s, {}).iteritems():
                        if k in subgraph.nodes:
                            subgraph.edges.setdefault(s, {}).update({k: v})
                    if s in self.cluster_lookup:
                        subgraph.cluster_lookup[s] = self.cluster_lookup[s]
                components.append(subgraph)
            return components

    def dfs(self, v, visited, threshold, op=lt, first=None):
        aux = [v]
        visited.add(v)
        if first is None:
            first = v
        for i in (n for n, w in self.edges.get(v, {}).iteritems()
                  if op(w, threshold) and n not in visited):
            x, y = self.dfs(i, visited, threshold, op, first)
            aux.extend(x)
            visited = visited.union(y)
        return aux, visited

########NEW FILE########
__FILENAME__ = maxp
"""
Max p regionalization

Heuristically form the maximum number (p) of regions given a set of n
areas and a floor constraint.
"""

__author__ = "Serge Rey <srey@asu.edu>, David Folch <david.folch@asu.edu>"


import pysal
from components import check_contiguity
import copy
import random
import numpy as np
#from pysal.common import *
from pysal.region import randomregion as RR

__all__ = ["Maxp", "Maxp_LISA"]

LARGE = 10 ** 6
MAX_ATTEMPTS = 100


class Maxp:
    """Try to find the maximum number of regions for a set of areas such that
    each region combines contiguous areas that satisfy a given threshold
    constraint.


    Parameters
    ----------

    w               : W
                      spatial weights object

    z               : array
                      n*m array of observations on m attributes across n
                      areas. This is used to calculate intra-regional
                      homogeneity
    floor           : int
                      a minimum bound for a variable that has to be
                      obtained in each region
    floor_variable  : array
                      n*1 vector of observations on variable for the floor
    initial         : int
                      number of initial solutions to generate
    verbose         : binary
                      if true debugging information is printed
    seeds           : list
                      ids of observations to form initial seeds. If
                      len(ids) is less than the number of observations, the
                      complementary ids are added to the end of seeds. Thus
                      the specified seeds get priority in the solution

    Attributes
    ----------

    area2region     : dict
                      mapping of areas to region. key is area id, value is
                      region id
    regions         : list
                      list of lists of regions (each list has the ids of areas
                      in that region)
    p               : int
                      number of regions
    swap_iterations : int
                      number of swap iterations
    total_moves     : int
                      number of moves into internal regions

    Examples
    --------

    Setup imports and set seeds for random number generators to insure the
    results are identical for each run.

    >>> import random
    >>> import numpy as np
    >>> import pysal
    >>> random.seed(100)
    >>> np.random.seed(100)

    Setup a spatial weights matrix describing the connectivity of a square
    community with 100 areas.  Generate two random data attributes for each
    area in the community (a 100x2 array) called z. p is the data vector used to
    compute the floor for a region, and floor is the floor value; in this case
    p is simply a vector of ones and the floor is set to three. This means
    that each region will contain at least three areas.  In other cases the
    floor may be computed based on a minimum population count for example.

    >>> import random
    >>> import numpy as np
    >>> import pysal
    >>> random.seed(100)
    >>> np.random.seed(100)
    >>> w = pysal.lat2W(10,10)
    >>> z = np.random.random_sample((w.n,2))
    >>> p = np.ones((w.n,1), float)
    >>> floor = 3
    >>> solution = pysal.region.Maxp(w, z, floor, floor_variable=p, initial=100)
    >>> solution.p
    29
    >>> min([len(region) for region in solution.regions])
    3
    >>> solution.regions[0]
    [4, 14, 5, 24, 3]
    >>>

    """
    def __init__(self, w, z, floor, floor_variable,
                 verbose=False, initial=100, seeds=[]):

        self.w = w
        self.z = z
        self.floor = floor
        self.floor_variable = floor_variable
        self.verbose = verbose
        self.seeds = seeds
        self.initial_solution()
        if not self.p:
            self.feasible = False
        else:
            self.feasible = True
            best_val = self.objective_function()
            self.current_regions = copy.copy(self.regions)
            self.current_area2region = copy.copy(self.area2region)
            self.initial_wss = []
            self.attempts = 0
            for i in range(initial):
                self.initial_solution()
                if self.p:
                    val = self.objective_function()
                    self.initial_wss.append(val)
                    if self.verbose:
                        print 'initial solution: ', i, val, best_val
                    if val < best_val:
                        self.current_regions = copy.copy(self.regions)
                        self.current_area2region = copy.copy(self.area2region)
                        best_val = val
                    self.attempts += 1
            self.regions = copy.copy(self.current_regions)
            self.p = len(self.regions)
            self.area2region = self.current_area2region
            if verbose:
                print "smallest region ifs: ", min([len(region) for region in self.regions])
                raw_input='wait'

            self.swap()

    def initial_solution(self):
        self.p = 0
        solving = True
        attempts = 0
        while solving and attempts <= MAX_ATTEMPTS:
            regions = []
            enclaves = []
            if not self.seeds:
                candidates = copy.copy(self.w.id_order)
                candidates = np.random.permutation(candidates)
                candidates = candidates.tolist()
            else:
                seeds = copy.copy(self.seeds)
                nonseeds = [i for i in self.w.id_order if i not in seeds]
                candidates = seeds
                candidates.extend(nonseeds)
            while candidates:
                seed = candidates.pop(0)
                # try to grow it till threshold constraint is satisfied
                region = [seed]
                building_region = True
                while building_region:
                    # check if floor is satisfied
                    if self.check_floor(region):
                        regions.append(region)
                        building_region = False
                    else:
                        potential = []
                        for area in region:
                            neighbors = self.w.neighbors[area]
                            neighbors = [neigh for neigh in neighbors if neigh in candidates]
                            neighbors = [neigh for neigh in neighbors if neigh not in region]
                            neighbors = [neigh for neigh in neighbors if neigh not in potential]
                            potential.extend(neighbors)
                        if potential:
                            # add a random neighbor
                            neigID = random.randint(0, len(potential) - 1)
                            neigAdd = potential.pop(neigID)
                            region.append(neigAdd)
                            # remove it from candidates
                            candidates.remove(neigAdd)
                        else:
                            #print 'enclave'
                            #print region
                            enclaves.extend(region)
                            building_region = False
            # check to see if any regions were made before going to enclave stage
            if regions:
                feasible = True
            else:
                attempts += 1
                break
            self.enclaves = enclaves[:]
            a2r = {}
            for r, region in enumerate(regions):
                for area in region:
                    a2r[area] = r
            encCount = len(enclaves)
            encAttempts = 0
            while enclaves and encAttempts != encCount:
                enclave = enclaves.pop(0)
                neighbors = self.w.neighbors[enclave]
                neighbors = [neighbor for neighbor in neighbors if neighbor not in enclaves]
                candidates = []
                for neighbor in neighbors:
                    region = a2r[neighbor]
                    if region not in candidates:
                        candidates.append(region)
                if candidates:
                    # add enclave to random region
                    regID = random.randint(0, len(candidates) - 1)
                    rid = candidates[regID]
                    regions[rid].append(enclave)
                    a2r[enclave] = rid
                    # structure to loop over enclaves until no more joining is possible
                    encCount = len(enclaves)
                    encAttempts = 0
                    feasible = True
                else:
                    # put back on que, no contiguous regions yet
                    enclaves.append(enclave)
                    encAttempts += 1
                    feasible = False
            if feasible:
                solving = False
                self.regions = regions
                self.area2region = a2r
                self.p = len(regions)
            else:
                if attempts == MAX_ATTEMPTS:
                    print 'No initial solution found'
                    self.p = 0
                attempts += 1

    def swap(self):
        swapping = True
        swap_iteration = 0
        if self.verbose:
            print 'Initial solution, objective function: ', self.objective_function()
        total_moves = 0
        self.k = len(self.regions)
        changed_regions = [1] * self.k
        nr = range(self.k)
        while swapping:
            moves_made = 0
            regionIds = [r for r in nr if changed_regions[r]]
            np.random.permutation(regionIds)
            changed_regions = [0] * self.k
            swap_iteration += 1
            for seed in regionIds:
                local_swapping = True
                local_attempts = 0
                while local_swapping:
                    local_moves = 0
                    # get neighbors
                    members = self.regions[seed]
                    neighbors = []
                    for member in members:
                        candidates = self.w.neighbors[member]
                        candidates = [candidate for candidate in candidates if candidate not in members]
                        candidates = [candidate for candidate in candidates if candidate not in neighbors]
                        neighbors.extend(candidates)
                    candidates = []
                    for neighbor in neighbors:
                        block = copy.copy(self.regions[self.area2region[
                            neighbor]])
                        if check_contiguity(self.w, block, neighbor):
                            block.remove(neighbor)
                            fv = self.check_floor(block)
                            if fv:
                                candidates.append(neighbor)
                    # find the best local move
                    if not candidates:
                        local_swapping = False
                    else:
                        nc = len(candidates)
                        moves = np.zeros([nc, 1], float)
                        best = None
                        cv = 0.0
                        for area in candidates:
                            current_internal = self.regions[seed]
                            current_outter = self.regions[self.area2region[
                                area]]
                            current = self.objective_function([current_internal, current_outter])
                            new_internal = copy.copy(current_internal)
                            new_outter = copy.copy(current_outter)
                            new_internal.append(area)
                            new_outter.remove(area)
                            new = self.objective_function([new_internal,
                                                           new_outter])
                            change = new - current
                            if change < cv:
                                best = area
                                cv = change
                        if best:
                            # make the move
                            area = best
                            old_region = self.area2region[area]
                            self.regions[old_region].remove(area)
                            self.area2region[area] = seed
                            self.regions[seed].append(area)
                            moves_made += 1
                            changed_regions[seed] = 1
                            changed_regions[old_region] = 1
                        else:
                            # no move improves the solution
                            local_swapping = False
                    local_attempts += 1
                    if self.verbose:
                        print 'swap_iteration: ', swap_iteration, 'moves_made: ', moves_made
                        print 'number of regions: ', len(self.regions)
                        print 'number of changed regions: ', sum(
                            changed_regions)
                        print 'internal region: ', seed, 'local_attempts: ', local_attempts
                        print 'objective function: ', self.objective_function()
                        print 'smallest region size: ',min([len(region) for region in self.regions])
            total_moves += moves_made
            if moves_made == 0:
                swapping = False
                self.swap_iterations = swap_iteration
                self.total_moves = total_moves
            if self.verbose:
                print 'moves_made: ', moves_made
                print 'objective function: ', self.objective_function()

    def check_floor(self, region):
        selectionIDs = [self.w.id_order.index(i) for i in region]
        cv = sum(self.floor_variable[selectionIDs])
        if cv >= self.floor:
            #print len(selectionIDs)
            return True
        else:
            return False

    def objective_function(self, solution=None):
        # solution is a list of lists of region ids [[1,7,2],[0,4,3],...] such
        # that the first region has areas 1,7,2 the second region 0,4,3 and so
        # on. solution does not have to be exhaustive
        if not solution:
            solution = self.regions
        wss = 0
        for region in solution:
            selectionIDs = [self.w.id_order.index(i) for i in region]
            m = self.z[selectionIDs, :]
            var = m.var(axis=0)
            wss += sum(np.transpose(var)) * len(region)
        return wss

    def inference(self, nperm=99):
        """Compare the within sum of squares for the solution against
        simulated solutions where areas are randomly assigned to regions that
        maintain the cardinality of the original solution.

        Parameters
        ----------

        nperm       : int
                      number of random permutations for calculation of
                      pseudo-p_values

        Attributes
        ----------

        pvalue      : float
                      pseudo p_value

        Examples
        --------

        Setup is the same as shown above except using a 5x5 community.

        >>> import random
        >>> import numpy as np
        >>> import pysal
        >>> random.seed(100)
        >>> np.random.seed(100)
        >>> w=pysal.weights.lat2W(5,5)
        >>> z=np.random.random_sample((w.n,2))
        >>> p=np.ones((w.n,1),float)
        >>> floor=3
        >>> solution=pysal.region.Maxp(w,z,floor,floor_variable=p,initial=100)

        Set nperm to 9 meaning that 9 random regions are computed and used for
        the computation of a pseudo-p-value for the actual Max-p solution. In
        empirical work this would typically be set much higher, e.g. 999 or
        9999.

        >>> solution.inference(nperm=9)
        >>> solution.pvalue
        0.2

        """
        ids = self.w.id_order
        num_regions = len(self.regions)
        wsss = np.zeros(nperm + 1)
        self.wss = self.objective_function()
        cards = [len(i) for i in self.regions]
        sim_solutions = RR.Random_Regions(ids, num_regions,
                                          cardinality=cards, permutations=nperm)
        cv = 1
        c = 1
        for solution in sim_solutions.solutions_feas:
            wss = self.objective_function(solution.regions)
            wsss[c] = wss
            if wss <= self.wss:
                cv += 1
            c += 1
        self.pvalue = cv / (1. + len(sim_solutions.solutions_feas))
        self.wss_perm = wsss
        self.wss_perm[0] = self.wss

    def cinference(self, nperm=99, maxiter=1000):
        """Compare the within sum of squares for the solution against
        conditional simulated solutions where areas are randomly assigned to
        regions that maintain the cardinality of the original solution and
        respect contiguity relationships.

        Parameters
        ----------

        nperm       : int
                      number of random permutations for calculation of
                      pseudo-p_values

        maxiter     : int
                      maximum number of attempts to find each permutation

        Attributes
        ----------

        pvalue      : float
                      pseudo p_value

        feas_sols   : int
                      number of feasible solutions found

        Notes
        -----

        it is possible for the number of feasible solutions (feas_sols) to be
        less than the number of permutations requested (nperm); an exception
        is raised if this occurs.

        Examples
        --------

        Setup is the same as shown above except using a 5x5 community.

        >>> import random
        >>> import numpy as np
        >>> import pysal
        >>> random.seed(100)
        >>> np.random.seed(100)
        >>> w=pysal.weights.lat2W(5,5)
        >>> z=np.random.random_sample((w.n,2))
        >>> p=np.ones((w.n,1),float)
        >>> floor=3
        >>> solution=pysal.region.Maxp(w,z,floor,floor_variable=p,initial=100)

        Set nperm to 9 meaning that 9 random regions are computed and used for
        the computation of a pseudo-p-value for the actual Max-p solution. In
        empirical work this would typically be set much higher, e.g. 999 or
        9999.

        >>> solution.cinference(nperm=9, maxiter=100)
        >>> solution.cpvalue
        0.1

        """
        ids = self.w.id_order
        num_regions = len(self.regions)
        wsss = np.zeros(nperm + 1)
        self.cwss = self.objective_function()
        cards = [len(i) for i in self.regions]
        sim_solutions = RR.Random_Regions(ids, num_regions,
                                          cardinality=cards, contiguity=self.w,
                                          maxiter=maxiter, permutations=nperm)
        self.cfeas_sols = len(sim_solutions.solutions_feas)
        if self.cfeas_sols < nperm:
            raise Exception('not enough feasible solutions found')
        cv = 1
        c = 1
        for solution in sim_solutions.solutions_feas:
            wss = self.objective_function(solution.regions)
            wsss[c] = wss
            if wss <= self.cwss:
                cv += 1
            c += 1
        self.cpvalue = cv / (1. + self.cfeas_sols)
        self.cwss_perm = wsss
        self.cwss_perm[0] = self.cwss


class Maxp_LISA(Maxp):
    """Max-p regionalization using LISA seeds

    Parameters
    ----------

    w              : W
                     spatial weights object
    z              : array
                     nxk array of n observations on k variables used to
                     measure similarity between areas within the regions.
    y              : array
                     nx1 array used to calculate the LISA statistics and
                     to set the intial seed order
    floor          : float
                     value that each region must obtain on floor_variable
    floor_variable : array
                     nx1 array of values for regional floor threshold
    initial        : int
                     number of initial feasible solutions to generate
                     prior to swapping

    Attributes
    ----------

    area2region     : dict
                      mapping of areas to region. key is area id, value is
                      region id
    regions         : list
                      list of lists of regions (each list has the ids of areas
                      in that region)
    swap_iterations : int
                      number of swap iterations
    total_moves     : int
                      number of moves into internal regions


    Notes
    -----

    We sort the observations based on the value of the LISAs. This
    ordering then gives the priority for seeds forming the p regions. The
    initial priority seeds are not guaranteed to be separated in the final
    solution.

    Examples
    --------

    Setup imports and set seeds for random number generators to insure the
    results are identical for each run.

    >>> import random
    >>> import numpy as np
    >>> import pysal
    >>> random.seed(100)
    >>> np.random.seed(100)

    Setup a spatial weights matrix describing the connectivity of a square
    community with 100 areas.  Generate two random data attributes for each area
    in the community (a 100x2 array) called z. p is the data vector used to
    compute the floor for a region, and floor is the floor value; in this case
    p is simply a vector of ones and the floor is set to three. This means
    that each region will contain at least three areas.  In other cases the
    floor may be computed based on a minimum population count for example.

    >>> w=pysal.lat2W(10,10)
    >>> z=np.random.random_sample((w.n,2))
    >>> p=np.ones(w.n)
    >>> mpl=pysal.region.Maxp_LISA(w,z,p,floor=3,floor_variable=p)
    >>> mpl.p
    31
    >>> mpl.regions[0]
    [99, 89, 98]

    """
    def __init__(self, w, z, y, floor, floor_variable, initial=100):

        lis = pysal.Moran_Local(y, w)
        ids = np.argsort(lis.Is)
        ids = ids[range(w.n - 1, -1, -1)]
        ids = ids.tolist()
        mp = Maxp.__init__(
            self, w, z, floor=floor, floor_variable=floor_variable,
            initial=initial, seeds=ids)


########NEW FILE########
__FILENAME__ = randomregion
"""
Generate random regions

Randomly form regions given various types of constraints on cardinality and
composition.
"""

__author__ = "David Folch dfolch@asu.edu, Serge Rey srey@asu.edu"

import numpy as np
from pysal.common import copy

__all__ = ["Random_Regions", "Random_Region"]


class Random_Regions:
    """Generate a list of Random_Region instances.

    Parameters
    ----------

    area_ids        : list
                      IDs indexing the areas to be grouped into regions (must
                      be in the same order as spatial weights matrix if this
                      is provided)

    num_regions     : integer
                      number of regions to generate (if None then this is
                      chosen randomly from 2 to n where n is the number of
                      areas)

    cardinality     : list
                      list containing the number of areas to assign to regions
                      (if num_regions is also provided then len(cardinality)
                      must equal num_regions; if cardinality=None then a list
                      of length num_regions will be generated randomly)

    contiguity      : W
                      spatial weights object (if None then contiguity will be
                      ignored)

    maxiter         : int
                      maximum number attempts (for each permutation) at finding
                      a feasible solution (only affects contiguity constrained
                      regions)

    compact         : boolean
                      attempt to build compact regions, note (only affects
                      contiguity constrained regions)

    max_swaps       : int
                      maximum number of swaps to find a feasible solution
                      (only affects contiguity constrained regions)

    permutations    : int
                      number of Random_Region instances to generate

    Attributes
    ----------

    solutions       : list
                      list of length permutations containing all Random_Region instances generated

    solutions_feas  : list
                      list of the Random_Region instances that resulted in feasible solutions

    Examples
    --------

    Setup the data

    >>> import random
    >>> import numpy as np
    >>> import pysal
    >>> nregs = 13
    >>> cards = range(2,14) + [10]
    >>> w = pysal.lat2W(10,10,rook=False)
    >>> ids = w.id_order

    Unconstrained

    >>> random.seed(10)
    >>> np.random.seed(10)
    >>> t0 = pysal.region.Random_Regions(ids, permutations=2)
    >>> t0.solutions[0].regions[0]
    [19, 14, 43, 37, 66, 3, 79, 41, 38, 68, 2, 1, 60]

    Cardinality and contiguity constrained (num_regions implied)

    >>> random.seed(60)
    >>> np.random.seed(60)
    >>> t1 = pysal.region.Random_Regions(ids, num_regions=nregs, cardinality=cards, contiguity=w, permutations=2)
    >>> t1.solutions[0].regions[0]
    [88, 97, 98, 89, 99, 86, 78, 59, 49, 69, 68, 79, 77]

    Cardinality constrained (num_regions implied)

    >>> random.seed(100)
    >>> np.random.seed(100)
    >>> t2 = pysal.region.Random_Regions(ids, num_regions=nregs, cardinality=cards, permutations=2)
    >>> t2.solutions[0].regions[0]
    [37, 62]

    Number of regions and contiguity constrained

    >>> random.seed(100)
    >>> np.random.seed(100)
    >>> t3 = pysal.region.Random_Regions(ids, num_regions=nregs, contiguity=w, permutations=2)
    >>> t3.solutions[0].regions[1]
    [71, 72, 70, 93, 51, 91, 85, 74, 63, 73, 61, 62, 82]

    Cardinality and contiguity constrained

    >>> random.seed(60)
    >>> np.random.seed(60)
    >>> t4 = pysal.region.Random_Regions(ids, cardinality=cards, contiguity=w, permutations=2)
    >>> t4.solutions[0].regions[0]
    [88, 97, 98, 89, 99, 86, 78, 59, 49, 69, 68, 79, 77]

    Number of regions constrained

    >>> random.seed(100)
    >>> np.random.seed(100)
    >>> t5 = pysal.region.Random_Regions(ids, num_regions=nregs, permutations=2)
    >>> t5.solutions[0].regions[0]
    [37, 62, 26, 41, 35, 25, 36]

    Cardinality constrained

    >>> random.seed(100)
    >>> np.random.seed(100)
    >>> t6 = pysal.region.Random_Regions(ids, cardinality=cards, permutations=2)
    >>> t6.solutions[0].regions[0]
    [37, 62]

    Contiguity constrained

    >>> random.seed(100)
    >>> np.random.seed(100)
    >>> t7 = pysal.region.Random_Regions(ids, contiguity=w, permutations=2)
    >>> t7.solutions[0].regions[1]
    [62, 52, 51, 50]

    """
    def __init__(
        self, area_ids, num_regions=None, cardinality=None, contiguity=None,
        maxiter=100, compact=False, max_swaps=1000000, permutations=99):

        solutions = []
        for i in range(permutations):
            solutions.append(Random_Region(area_ids, num_regions, cardinality,
                                           contiguity, maxiter, compact, max_swaps))
        self.solutions = solutions
        self.solutions_feas = []
        for i in solutions:
            if i.feasible == True:
                self.solutions_feas.append(i)


class Random_Region:
    """Randomly combine a given set of areas into two or more regions based
    on various constraints.


    Parameters
    ----------

    area_ids        : list
                      IDs indexing the areas to be grouped into regions (must
                      be in the same order as spatial weights matrix if this
                      is provided)

    num_regions     : integer
                      number of regions to generate (if None then this is
                      chosen randomly from 2 to n where n is the number of
                      areas)

    cardinality     : list
                      list containing the number of areas to assign to regions
                      (if num_regions is also provided then len(cardinality)
                      must equal num_regions; if cardinality=None then a list
                      of length num_regions will be generated randomly)

    contiguity      : W
                      spatial weights object (if None then contiguity will be
                      ignored)

    maxiter         : int
                      maximum number attempts at finding a feasible solution
                      (only affects contiguity constrained regions)

    compact         : boolean
                      attempt to build compact regions (only affects
                      contiguity constrained regions)

    max_swaps       : int
                      maximum number of swaps to find a feasible solution
                      (only affects contiguity constrained regions)

    Attributes
    ----------

    feasible        : boolean
                      if True then solution was found

    regions         : list
                      list of lists of regions (each list has the ids of areas
                      in that region)

    Examples
    --------

    Setup the data

    >>> import random
    >>> import numpy as np
    >>> import pysal
    >>> nregs = 13
    >>> cards = range(2,14) + [10]
    >>> w = pysal.weights.lat2W(10,10,rook=False)
    >>> ids = w.id_order

    Unconstrained

    >>> random.seed(10)
    >>> np.random.seed(10)
    >>> t0 = pysal.region.Random_Region(ids)
    >>> t0.regions[0]
    [19, 14, 43, 37, 66, 3, 79, 41, 38, 68, 2, 1, 60]

    Cardinality and contiguity constrained (num_regions implied)

    >>> random.seed(60)
    >>> np.random.seed(60)
    >>> t1 = pysal.region.Random_Region(ids, num_regions=nregs, cardinality=cards, contiguity=w)
    >>> t1.regions[0]
    [88, 97, 98, 89, 99, 86, 78, 59, 49, 69, 68, 79, 77]

    Cardinality constrained (num_regions implied)

    >>> random.seed(100)
    >>> np.random.seed(100)
    >>> t2 = pysal.region.Random_Region(ids, num_regions=nregs, cardinality=cards)
    >>> t2.regions[0]
    [37, 62]

    Number of regions and contiguity constrained

    >>> random.seed(100)
    >>> np.random.seed(100)
    >>> t3 = pysal.region.Random_Region(ids, num_regions=nregs, contiguity=w)
    >>> t3.regions[1]
    [71, 72, 70, 93, 51, 91, 85, 74, 63, 73, 61, 62, 82]

    Cardinality and contiguity constrained

    >>> random.seed(60)
    >>> np.random.seed(60)
    >>> t4 = pysal.region.Random_Region(ids, cardinality=cards, contiguity=w)
    >>> t4.regions[0]
    [88, 97, 98, 89, 99, 86, 78, 59, 49, 69, 68, 79, 77]

    Number of regions constrained

    >>> random.seed(100)
    >>> np.random.seed(100)
    >>> t5 = pysal.region.Random_Region(ids, num_regions=nregs)
    >>> t5.regions[0]
    [37, 62, 26, 41, 35, 25, 36]

    Cardinality constrained

    >>> random.seed(100)
    >>> np.random.seed(100)
    >>> t6 = pysal.region.Random_Region(ids, cardinality=cards)
    >>> t6.regions[0]
    [37, 62]

    Contiguity constrained

    >>> random.seed(100)
    >>> np.random.seed(100)
    >>> t7 = pysal.region.Random_Region(ids, contiguity=w)
    >>> t7.regions[0]
    [37, 27, 36, 17]

    """
    def __init__(
        self, area_ids, num_regions=None, cardinality=None, contiguity=None,
                    maxiter=1000, compact=False, max_swaps=1000000):

        self.n = len(area_ids)
        ids = copy.copy(area_ids)
        self.ids = list(np.random.permutation(ids))
        self.area_ids = area_ids
        self.regions = []
        self.feasible = True

        # tests for input argument consistency
        if cardinality:
            if self.n != sum(cardinality):
                self.feasible = False
                raise Exception('number of areas does not match cardinality')
        if contiguity:
            if area_ids != contiguity.id_order:
                self.feasible = False
                raise Exception('order of area_ids must match order in contiguity')
        if num_regions and cardinality:
            if num_regions != len(cardinality):
                self.feasible = False
                raise Exception('number of regions does not match cardinality')

        # dispatches the appropriate algorithm
        if num_regions and cardinality and contiguity:
            # conditioning on cardinality and contiguity (number of regions implied)
            self.build_contig_regions(num_regions, cardinality, contiguity,
                                      maxiter, compact, max_swaps)
        elif num_regions and cardinality:
            # conditioning on cardinality (number of regions implied)
            region_breaks = self.cards2breaks(cardinality)
            self.build_noncontig_regions(num_regions, region_breaks)
        elif num_regions and contiguity:
            # conditioning on number of regions and contiguity
            cards = self.get_cards(num_regions)
            self.build_contig_regions(num_regions, cards, contiguity,
                                      maxiter, compact, max_swaps)
        elif cardinality and contiguity:
            # conditioning on cardinality and contiguity
            num_regions = len(cardinality)
            self.build_contig_regions(num_regions, cardinality, contiguity,
                                      maxiter, compact, max_swaps)
        elif num_regions:
            # conditioning on number of regions only
            region_breaks = self.get_region_breaks(num_regions)
            self.build_noncontig_regions(num_regions, region_breaks)
        elif cardinality:
            # conditioning on number of cardinality only
            num_regions = len(cardinality)
            region_breaks = self.cards2breaks(cardinality)
            self.build_noncontig_regions(num_regions, region_breaks)
        elif contiguity:
            # conditioning on number of contiguity only
            num_regions = self.get_num_regions()
            cards = self.get_cards(num_regions)
            self.build_contig_regions(num_regions, cards, contiguity,
                                      maxiter, compact, max_swaps)
        else:
            # unconditioned
            num_regions = self.get_num_regions()
            region_breaks = self.get_region_breaks(num_regions)
            self.build_noncontig_regions(num_regions, region_breaks)

    def get_num_regions(self):
        return np.random.random_integers(2, self.n)

    def get_region_breaks(self, num_regions):
        region_breaks = set([])
        while len(region_breaks) < num_regions - 1:
            region_breaks.add(np.random.random_integers(1, self.n - 1))
        region_breaks = list(region_breaks)
        region_breaks.sort()
        return region_breaks

    def get_cards(self, num_regions):
        region_breaks = self.get_region_breaks(num_regions)
        cards = []
        start = 0
        for i in region_breaks:
            cards.append(i - start)
            start = i
        cards.append(self.n - start)
        return cards

    def cards2breaks(self, cards):
        region_breaks = []
        break_point = 0
        for i in cards:
            break_point += i
            region_breaks.append(break_point)
        region_breaks.pop()
        return region_breaks

    def build_noncontig_regions(self, num_regions, region_breaks):
        start = 0
        for i in region_breaks:
            self.regions.append(self.ids[start:i])
            start = i
        self.regions.append(self.ids[start:])

    def grow_compact(self, w, test_card, region, candidates, potential):
        # try to build a compact region by exhausting all existing
        # potential areas before adding new potential areas
        add_areas = []
        while potential and len(region) < test_card:
            pot_index = np.random.random_integers(0, len(potential) - 1)
            add_area = potential[pot_index]
            region.append(add_area)
            candidates.remove(add_area)
            potential.remove(add_area)
            add_areas.append(add_area)
        for i in add_areas:
            potential.extend([j for j in w.neighbors[i]
                                 if j not in region and
                                    j not in potential and
                                    j in candidates])
        return region, candidates, potential

    def grow_free(self, w, test_card, region, candidates, potential):
        # increment potential areas after each new area is
        # added to the region (faster than the grow_compact)
        pot_index = np.random.random_integers(0, len(potential) - 1)
        add_area = potential[pot_index]
        region.append(add_area)
        candidates.remove(add_area)
        potential.remove(add_area)
        potential.extend([i for i in w.neighbors[add_area]
                             if i not in region and
                                i not in potential and
                                i in candidates])
        return region, candidates, potential

    def build_contig_regions(self, num_regions, cardinality, w,
                                maxiter, compact, max_swaps):
        if compact:
            grow_region = self.grow_compact
        else:
            grow_region = self.grow_free
        iter = 0
        while iter < maxiter:

            # regionalization setup
            regions = []
            size_pre = 0
            counter = -1
            area2region = {}
            self.feasible = False
            swap_count = 0
            cards = copy.copy(cardinality)
            cards.sort()  # try to build largest regions first (pop from end of list)
            candidates = copy.copy(self.ids)  # these are already shuffled

            # begin building regions
            while candidates and swap_count < max_swaps:
                # setup test to determine if swapping is needed
                if size_pre == len(regions):
                    counter += 1
                else:
                    counter = 0
                    size_pre = len(regions)
                # test if swapping is needed
                if counter == len(candidates):

                    # start swapping
                    # swapping simply changes the candidate list
                    swap_in = None   # area to become new candidate
                    while swap_in is None:  # PEP8 E711
                        swap_count += 1
                        swap_out = candidates.pop(0)  # area to remove from candidates
                        swap_neighs = copy.copy(w.neighbors[swap_out])
                        swap_neighs = list(np.random.permutation(swap_neighs))
                        # select area to add to candidates (i.e. remove from an existing region)
                        for i in swap_neighs:
                            if i not in candidates:
                                join = i  # area linking swap_in to swap_out
                                swap_index = area2region[join]
                                swap_region = regions[swap_index]
                                swap_region = list(np.random.permutation(swap_region))
                                for j in swap_region:
                                    if j != join:  # leave the join area to ensure regional connectivity
                                        swap_in = j
                                        break
                            if swap_in is not None:  # PEP8 E711
                                break
                        else:
                            candidates.append(swap_out)
                    # swapping cleanup
                    regions[swap_index].remove(swap_in)
                    regions[swap_index].append(swap_out)
                    area2region.pop(swap_in)
                    area2region[swap_out] = swap_index
                    candidates.append(swap_in)
                    counter = 0

                # setup to build a single region
                building = True
                seed = candidates.pop(0)
                region = [seed]
                potential = [i for i in w.neighbors[seed] if i in candidates]
                test_card = cards.pop()

                # begin building single region
                while building and len(region) < test_card:
                    if potential:
                        region, candidates, potential = grow_region(
                            w, test_card,
                                        region, candidates, potential)
                    else:
                        # not enough potential neighbors to reach test_card size
                        building = False
                        cards.append(test_card)
                        if len(region) in cards:
                            # constructed region matches another candidate region size
                            cards.remove(len(region))
                        else:
                            # constructed region doesn't match a candidate region size
                            candidates.extend(region)
                            region = []

                # cleanup when successful region built
                if region:
                    regions.append(region)
                    region_index = len(regions) - 1
                    for i in region:
                        area2region[i] = region_index   # area2region needed for swapping

            # handling of regionalization result
            if len(regions) < num_regions:
                # regionalization failed
                self.ids = list(np.random.permutation(self.ids))
                regions = []
                iter += 1
            else:
                # regionalization successful
                self.feasible = True
                iter = maxiter
        self.regions = regions


########NEW FILE########
__FILENAME__ = test_components

import unittest
import pysal


class Test_Components(unittest.TestCase):
    def setUp(self):
        self.w = pysal.lat2W(5, 5)

    def test_check_contiguity(self):
        result = pysal.region.check_contiguity(self.w, [0, 1, 2, 3, 4], 4)
        self.assertEquals(result, True)
        result = pysal.region.check_contiguity(self.w, [0, 1, 2, 3, 4], 3)
        self.assertEquals(result, False)
        result = pysal.region.check_contiguity(self.w, [0, 1, 2, 3, 4], 0)
        self.assertEquals(result, True)
        result = pysal.region.check_contiguity(self.w, [0, 1, 2, 3, 4], 1)
        self.assertEquals(result, False)


suite = unittest.TestLoader().loadTestsFromTestCase(Test_Components)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_maxp

import unittest
import pysal
import numpy as np
import random


class Test_Maxp(unittest.TestCase):
    def setUp(self):
        random.seed(100)
        np.random.seed(100)

    def test_Maxp(self):
        w = pysal.lat2W(10, 10)
        z = np.random.random_sample((w.n, 2))
        p = np.ones((w.n, 1), float)
        floor = 3
        solution = pysal.region.Maxp(
            w, z, floor, floor_variable=p, initial=100)
        self.assertEquals(solution.p, 29)
        self.assertEquals(solution.regions[0], [4, 14, 5, 24, 3])

    def test_inference(self):
        w = pysal.weights.lat2W(5, 5)
        z = np.random.random_sample((w.n, 2))
        p = np.ones((w.n, 1), float)
        floor = 3
        solution = pysal.region.Maxp(
            w, z, floor, floor_variable=p, initial=100)
        solution.inference(nperm=9)
        self.assertAlmostEquals(solution.pvalue, 0.20000000000000001, 10)

    def test_cinference(self):
        w = pysal.weights.lat2W(5, 5)
        z = np.random.random_sample((w.n, 2))
        p = np.ones((w.n, 1), float)
        floor = 3
        solution = pysal.region.Maxp(
            w, z, floor, floor_variable=p, initial=100)
        solution.cinference(nperm=9, maxiter=100)
        self.assertAlmostEquals(solution.cpvalue, 0.10000000000000001, 10)

    def test_Maxp_LISA(self):
        w = pysal.lat2W(10, 10)
        z = np.random.random_sample((w.n, 2))
        p = np.ones(w.n)
        mpl = pysal.region.Maxp_LISA(w, z, p, floor=3, floor_variable=p)
        self.assertEquals(mpl.p, 31)
        self.assertEquals(mpl.regions[0], [99, 89, 98])


suite = unittest.TestLoader().loadTestsFromTestCase(Test_Maxp)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_randomregion

import unittest
import pysal
import numpy as np
import random


class Test_Random_Regions(unittest.TestCase):
    def setUp(self):
        self.nregs = 13
        self.cards = range(2, 14) + [10]
        self.w = pysal.lat2W(10, 10, rook=False)
        self.ids = self.w.id_order

    def test_Random_Regions(self):
        random.seed(10)
        np.random.seed(10)
        t0 = pysal.region.Random_Regions(self.ids, permutations=2)
        result = [19, 14, 43, 37, 66, 3, 79, 41, 38, 68, 2, 1, 60]
        for i in range(len(result)):
            self.assertEquals(t0.solutions[0].regions[0][i], result[i])
        for i in range(len(t0.solutions)):
            self.assertEquals(t0.solutions_feas[i], t0.solutions[i])

        random.seed(60)
        np.random.seed(60)
        t0 = pysal.region.Random_Regions(self.ids, num_regions=self.nregs,
                                         cardinality=self.cards, contiguity=self.w, permutations=2)
        result = [88, 97, 98, 89, 99, 86, 78, 59, 49, 69, 68, 79, 77]
        for i in range(len(result)):
            self.assertEquals(t0.solutions[0].regions[0][i], result[i])
        for i in range(len(t0.solutions)):
            self.assertEquals(t0.solutions_feas[i], t0.solutions[i])

        random.seed(100)
        np.random.seed(100)
        t0 = pysal.region.Random_Regions(self.ids, num_regions=self.nregs,
                                         cardinality=self.cards, permutations=2)
        result = [37, 62]
        for i in range(len(result)):
            self.assertEquals(t0.solutions[0].regions[0][i], result[i])
        for i in range(len(t0.solutions)):
            self.assertEquals(t0.solutions_feas[i], t0.solutions[i])

        random.seed(100)
        np.random.seed(100)
        t0 = pysal.region.Random_Regions(self.ids,
                                         num_regions=self.nregs, contiguity=self.w, permutations=2)
        result = [71, 72, 70, 93, 51, 91, 85, 74, 63, 73, 61, 62, 82]
        for i in range(len(result)):
            self.assertEquals(t0.solutions[0].regions[1][i], result[i])
        for i in range(len(t0.solutions)):
            self.assertEquals(t0.solutions_feas[i], t0.solutions[i])

        random.seed(60)
        np.random.seed(60)
        t0 = pysal.region.Random_Regions(self.ids,
                                         cardinality=self.cards, contiguity=self.w, permutations=2)
        result = [88, 97, 98, 89, 99, 86, 78, 59, 49, 69, 68, 79, 77]
        for i in range(len(result)):
            self.assertEquals(t0.solutions[0].regions[0][i], result[i])
        for i in range(len(t0.solutions)):
            self.assertEquals(t0.solutions_feas[i], t0.solutions[i])

        random.seed(100)
        np.random.seed(100)
        t0 = pysal.region.Random_Regions(
            self.ids, num_regions=self.nregs, permutations=2)
        result = [37, 62, 26, 41, 35, 25, 36]
        for i in range(len(result)):
            self.assertEquals(t0.solutions[0].regions[0][i], result[i])
        for i in range(len(t0.solutions)):
            self.assertEquals(t0.solutions_feas[i], t0.solutions[i])

        random.seed(100)
        np.random.seed(100)
        t0 = pysal.region.Random_Regions(
            self.ids, cardinality=self.cards, permutations=2)
        result = [37, 62]
        for i in range(len(result)):
            self.assertEquals(t0.solutions[0].regions[0][i], result[i])
        for i in range(len(t0.solutions)):
            self.assertEquals(t0.solutions_feas[i], t0.solutions[i])

        random.seed(100)
        np.random.seed(100)
        t0 = pysal.region.Random_Regions(
            self.ids, contiguity=self.w, permutations=2)
        result = [62, 52, 51, 50]
        for i in range(len(result)):
            self.assertEquals(t0.solutions[0].regions[1][i], result[i])
        for i in range(len(t0.solutions)):
            self.assertEquals(t0.solutions_feas[i], t0.solutions[i])

    def test_Random_Region(self):
        random.seed(10)
        np.random.seed(10)
        t0 = pysal.region.Random_Region(self.ids)
        t0.regions[0]
        result = [19, 14, 43, 37, 66, 3, 79, 41, 38, 68, 2, 1, 60]
        for i in range(len(result)):
            self.assertEquals(t0.regions[0][i], result[i])
        self.assertEquals(t0.feasible, True)

        random.seed(60)
        np.random.seed(60)
        t0 = pysal.region.Random_Region(self.ids, num_regions=self.nregs,
                                        cardinality=self.cards, contiguity=self.w)
        t0.regions[0]
        result = [88, 97, 98, 89, 99, 86, 78, 59, 49, 69, 68, 79, 77]
        for i in range(len(result)):
            self.assertEquals(t0.regions[0][i], result[i])
        self.assertEquals(t0.feasible, True)

        random.seed(100)
        np.random.seed(100)
        t0 = pysal.region.Random_Region(
            self.ids, num_regions=self.nregs, cardinality=self.cards)
        t0.regions[0]
        result = [37, 62]
        for i in range(len(result)):
            self.assertEquals(t0.regions[0][i], result[i])
        self.assertEquals(t0.feasible, True)

        random.seed(100)
        np.random.seed(100)
        t0 = pysal.region.Random_Region(
            self.ids, num_regions=self.nregs, contiguity=self.w)
        t0.regions[1]
        result = [71, 72, 70, 93, 51, 91, 85, 74, 63, 73, 61, 62, 82]
        for i in range(len(result)):
            self.assertEquals(t0.regions[1][i], result[i])
        self.assertEquals(t0.feasible, True)

        random.seed(60)
        np.random.seed(60)
        t0 = pysal.region.Random_Region(
            self.ids, cardinality=self.cards, contiguity=self.w)
        t0.regions[0]
        result = [88, 97, 98, 89, 99, 86, 78, 59, 49, 69, 68, 79, 77]
        for i in range(len(result)):
            self.assertEquals(t0.regions[0][i], result[i])
        self.assertEquals(t0.feasible, True)

        random.seed(100)
        np.random.seed(100)
        t0 = pysal.region.Random_Region(self.ids, num_regions=self.nregs)
        t0.regions[0]
        result = [37, 62, 26, 41, 35, 25, 36]
        for i in range(len(result)):
            self.assertEquals(t0.regions[0][i], result[i])
        self.assertEquals(t0.feasible, True)

        random.seed(100)
        np.random.seed(100)
        t0 = pysal.region.Random_Region(self.ids, cardinality=self.cards)
        t0.regions[0]
        result = [37, 62]
        for i in range(len(result)):
            self.assertEquals(t0.regions[0][i], result[i])
        self.assertEquals(t0.feasible, True)

        random.seed(100)
        np.random.seed(100)
        t0 = pysal.region.Random_Region(self.ids, contiguity=self.w)
        t0.regions[0]
        result = [37, 27, 36, 17]
        for i in range(len(result)):
            self.assertEquals(t0.regions[0][i], result[i])
        self.assertEquals(t0.feasible, True)


suite = unittest.TestLoader().loadTestsFromTestCase(Test_Random_Regions)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = directional
"""
Directional Analysis of Dynamic LISAs

"""
__author__ = "Sergio J. Rey <srey@asu.edu"

__all__ = ['rose']

import numpy as np
import pysal


def rose(Y, w, k=8, permutations=0):
    """
    Calculation of rose diagram for local indicators of spatial association

    Parameters
    ----------

    Y: array (n,2)
       variable observed on n spatial units over 2 time periods

    w: spatial weights object

    k: int
       number of circular sectors in rose diagram

    permutations: int
       number of random spatial permutations for calculation of pseudo
       p-values

    Returns
    -------

    results: dictionary (keys defined below)

    counts:  array (k,1)
        number of vectors with angular movement falling in each sector

    cuts: array (k,1)
        intervals defining circular sectors (in radians)

    random_counts: array (permutations,k)
        counts from random permutations

    pvalues: array (kx1)
        one sided (upper tail) pvalues for observed counts

    Notes
    -----
    Based on Rey, Murray, and Anselin (2011) [1]_

    Examples
    --------

    Constructing data for illustration of directional LISA analytics.
    Data is for the 48 lower US states over the period 1969-2009 and
    includes per capita income normalized to the national average. 

    Load comma delimited data file in and convert to a numpy array

    >>> f=open(pysal.examples.get_path("spi_download.csv"),'r')
    >>> lines=f.readlines()
    >>> f.close()
    >>> lines=[line.strip().split(",") for line in lines]
    >>> names=[line[2] for line in lines[1:-5]]
    >>> data=np.array([map(int,line[3:]) for line in lines[1:-5]])

    Bottom of the file has regional data which we don't need for this example
    so we will subset only those records that match a state name

    >>> sids=range(60)
    >>> out=['"United States 3/"',
    ...      '"Alaska 3/"',
    ...      '"District of Columbia"',
    ...      '"Hawaii 3/"',
    ...      '"New England"',
    ...      '"Mideast"',
    ...      '"Great Lakes"',
    ...      '"Plains"',
    ...      '"Southeast"',
    ...      '"Southwest"',
    ...      '"Rocky Mountain"',
    ...      '"Far West 3/"']
    >>> snames=[name for name in names if name not in out]
    >>> sids=[names.index(name) for name in snames]
    >>> states=data[sids,:]
    >>> us=data[0]
    >>> years=np.arange(1969,2009)

    Now we convert state incomes to express them relative to the national
    average

    >>> rel=states/(us*1.)

    Create our contiguity matrix from an external GAL file and row standardize
    the resulting weights

    >>> gal=pysal.open(pysal.examples.get_path('states48.gal'))
    >>> w=gal.read()
    >>> w.transform='r'

    Take the first and last year of our income data as the interval to do the
    directional directional analysis

    >>> Y=rel[:,[0,-1]]

    Set the random seed generator which is used in the permutation based
    inference for the rose diagram so that we can replicate our example
    results

    >>> np.random.seed(100)

    Call the rose function to construct the directional histogram for the
    dynamic LISA statistics. We will use four circular sectors for our
    histogram

    >>> r4=rose(Y,w,k=4,permutations=999)

    What are the cut-offs for our histogram - in radians

    >>> r4['cuts']
    array([ 0.        ,  1.57079633,  3.14159265,  4.71238898,  6.28318531])

    How many vectors fell in each sector

    >>> r4['counts']
    array([32,  5,  9,  2])

    What are the pseudo-pvalues for these counts based on 999 random spatial
    permutations of the state income data

    >>> r4['pvalues']
    array([ 0.02 ,  0.001,  0.001,  0.001])

    Repeat the exercise but now for 8 rather than 4 sectors

    >>> r8=rose(Y,w,permutations=999)
    >>> r8['counts']
    array([19, 13,  3,  2,  7,  2,  1,  1])
    >>> r8['pvalues']
    array([ 0.445,  0.042,  0.079,  0.003,  0.005,  0.1  ,  0.269,  0.002])

    References
    ----------

    .. [1] Rey, S.J., A.T. Murray and L. Anselin. 2011. "Visualizing
        regional income distribution dynamics." Letters in Spatial and Resource Sciences, 4: 81-90.

    """
    results = {}
    sw = 2 * np.pi / k
    cuts = np.arange(0.0, 2 * np.pi + sw, sw)
    wY = pysal.lag_spatial(w, Y)
    dx = Y[:, -1] - Y[:, 0]
    dy = wY[:, -1] - wY[:, 0]
    theta = np.arctan2(dy, dx)
    neg = theta < 0.0
    utheta = theta * (1 - neg) + neg * (2 * np.pi + theta)
    counts, bins = np.histogram(utheta, cuts)
    results['counts'] = counts
    results['cuts'] = cuts
    if permutations:
        n, k1 = Y.shape
        ids = np.arange(n)
        all_counts = np.zeros((permutations, k))
        for i in range(permutations):
            rid = np.random.permutation(ids)
            YR = Y[rid, :]
            wYR = pysal.lag_spatial(w, YR)
            dx = YR[:, -1] - YR[:, 0]
            dy = wYR[:, -1] - wYR[:, 0]
            theta = np.arctan2(dy, dx)
            neg = theta < 0.0
            utheta = theta * (1 - neg) + neg * (2 * np.pi + theta)
            c, b = np.histogram(utheta, cuts)
            c.shape = (1, k)
            all_counts[i, :] = c
        larger = sum(all_counts >= counts)
        p_l = permutations - larger
        extreme = (p_l) < larger
        extreme = np.where(extreme, p_l, larger)
        p = (extreme + 1.) / (permutations + 1.)
        results['pvalues'] = p
        results['random_counts'] = all_counts

    return results

########NEW FILE########
__FILENAME__ = ergodic
"""
Summary measures for ergodic Markov chains
"""
__author__ = "Sergio J. Rey <srey@asu.edu>"

__all__ = ['steady_state', 'fmpt', 'var_fmpt']

import numpy as np
import numpy.linalg as la


def steady_state(P):
    """
    Calculates the steady state probability vector for a regular Markov
    transition matrix P

    Parameters
    ----------

    P  : matrix (kxk)
         an ergodic Markov transition probability matrix

    Returns
    -------

    implicit : matrix (kx1)
               steady state distribution

    Examples
    --------
    Taken from Kemeny and Snell.  Land of Oz example where the states are
    Rain, Nice and Snow, so there is 25 percent chance that if it
    rained in Oz today, it will snow tomorrow, while if it snowed today in
    Oz there is a 50 percent chance of snow again tomorrow and a 25
    percent chance of a nice day (nice, like when the witch with the monkeys
    is melting).

    >>> import numpy as np
    >>> p=np.matrix([[.5, .25, .25],[.5,0,.5],[.25,.25,.5]])
    >>> steady_state(p)
    matrix([[ 0.4],
            [ 0.2],
            [ 0.4]])

    Thus, the long run distribution for Oz is to have 40 percent of the
    days classified as Rain, 20 percent as Nice, and 40 percent as Snow
    (states are mutually exclusive).

    """

    v, d = la.eig(np.transpose(P))

    # for a regular P maximum eigenvalue will be 1
    mv = max(v)
    # find its position
    i = v.tolist().index(mv)

    # normalize eigenvector corresponding to the eigenvalue 1
    return d[:, i] / sum(d[:, i])


def fmpt(P):
    """
    Calculates the matrix of first mean passage times for an
    ergodic transition probability matrix.

    Parameters
    ----------

    P    : matrix (kxk)
           an ergodic Markov transition probability matrix

    Returns
    -------

    M    : matrix (kxk)
           elements are the expected value for the number of intervals
           required for  a chain starting in state i to first enter state j
           If i=j then this is the recurrence time.

    Examples
    --------

    >>> import numpy as np
    >>> p=np.matrix([[.5, .25, .25],[.5,0,.5],[.25,.25,.5]])
    >>> fm=fmpt(p)
    >>> fm
    matrix([[ 2.5       ,  4.        ,  3.33333333],
            [ 2.66666667,  5.        ,  2.66666667],
            [ 3.33333333,  4.        ,  2.5       ]])


    Thus, if it is raining today in Oz we can expect a nice day to come
    along in another 4 days, on average, and snow to hit in 3.33 days. We can
    expect another rainy day in 2.5 days. If it is nice today in Oz, we would
    experience a change in the weather (either rain or snow) in 2.67 days from
    today. (That wicked witch can only die once so I reckon that is the
    ultimate absorbing state).

    Notes
    -----

    Uses formulation (and examples on p. 218) in Kemeny and Snell (1976)

    References
    ----------

    .. [1] Kemeny, John, G. and J. Laurie Snell (1976) Finite Markov
         Chains. Springer-Verlag. Berlin

    """
    A = np.zeros_like(P)
    ss = steady_state(P)
    k = ss.shape[0]
    for i in range(k):
        A[:, i] = ss
    A = A.transpose()
    I = np.identity(k)
    Z = la.inv(I - P + A)
    E = np.ones_like(Z)
    D = np.diag(1. / np.diag(A))
    Zdg = np.diag(np.diag(Z))
    M = (I - Z + E * Zdg) * D
    return M


def var_fmpt(P):
    """
    Variances of first mean passage times for an ergodic transition
    probability matrix

    Parameters
    ----------

    P    : matrix (kxk)
           an ergodic Markov transition probability matrix

    Returns
    -------

    implic : matrix (kxk)
             elements are the variances for the number of intervals
             required for  a chain starting in state i to first enter state j

    Examples
    --------

    >>> import numpy as np
    >>> p=np.matrix([[.5, .25, .25],[.5,0,.5],[.25,.25,.5]])
    >>> vfm=var_fmpt(p)
    >>> vfm
    matrix([[  5.58333333,  12.        ,   6.88888889],
            [  6.22222222,  12.        ,   6.22222222],
            [  6.88888889,  12.        ,   5.58333333]])



    Notes
    -----

    Uses formulation (and examples on p. 83) in Kemeny and Snell (1976)


    """
    A = P ** 1000
    n, k = A.shape
    I = np.identity(k)
    Z = la.inv(I - P + A)
    E = np.ones_like(Z)
    D = np.diag(1. / np.diag(A))
    Zdg = np.diag(np.diag(Z))
    M = (I - Z + E * Zdg) * D
    ZM = Z * M
    ZMdg = np.diag(np.diag(ZM))
    W = M * (2 * Zdg * D - I) + 2 * (ZM - E * ZMdg)
    return W - np.multiply(M, M)


########NEW FILE########
__FILENAME__ = interaction
"""
Methods for identifying space-time interaction in spatio-temporal event
data.
"""
__author__ = "Nicholas Malizia <nmalizia@asu.edu>", "Sergio J. Rey \
<srey@asu.edu>", "Philip Stephens <philip.stephens@asu.edu"

import pysal
import numpy as np
import scipy.stats as stats
import pysal.weights.Distance as Distance
from pysal import cg
from pysal.spatial_dynamics import util
from datetime import date

__all__ = ['SpaceTimeEvents', 'knox', 'mantel', 'jacquez', 'modified_knox']


class SpaceTimeEvents:
    """
    Method for reformatting event data stored in a shapefile for use in
    calculating metrics of spatio-temporal interaction.

    Parameters
    ----------
    path            : string
                      the path to the appropriate shapefile, including the
                      file name, but excluding the extension
    time            : string
                      column header in the DBF file indicating the column
                      containing the time stamp
    infer_timestamp : boolean
                      if the column containing the timestamp is formatted as
                      calendar dates, try to coerce them into Python datetime objects

    Attributes
    ----------
    n               : int
                      number of events
    x               : array
                      n x 1 array of the x coordinates for the events
    y               : array
                      n x 1 array of the y coordinates for the events
    t               : array
                      n x 1 array of the temporal coordinates for the events
    space           : array
                      n x 2 array of the spatial coordinates (x,y) for the
                      events
    time            : array
                      n x 2 array of the temporal coordinates (t,1) for the
                      events, the second column is a vector of ones

    Examples
    --------

    Read in the example shapefile data, ensuring to omit the file
    extension. In order to successfully create the event data the .dbf file
    associated with the shapefile should have a column of values that are a
    timestamp for the events. This timestamp may be a numerical value
    or a date. Date inference was added in version 1.6.

    >>> path = pysal.examples.get_path("burkitt")

    Create an instance of SpaceTimeEvents from a shapefile, where the
    temporal information is stored in a column named "T".

    >>> events = SpaceTimeEvents(path,'T')

    See how many events are in the instance.

    >>> events.n
    188

    Check the spatial coordinates of the first event.

    >>> events.space[0]
    array([ 300.,  302.])

    Check the time of the first event.

    >>> events.t[0]
    array([ 413.])

    Calculate the time difference between the first two events.

    >>> events.t[1] - events.t[0]
    array([ 59.])

    New, in 1.6, date support:

    Now, create an instance of SpaceTimeEvents from a shapefile, where the
    temporal information is stored in a column named "DATE".

    >>> events = SpaceTimeEvents(path,'DATE')

    See how many events are in the instance.

    >>> events.n
    188

    Check the spatial coordinates of the first event.

    >>> events.space[0]
    array([ 300.,  302.])

    Check the time of the first event. Note that this value is equivalent to
    413 days after January 1, 1900.

    >>> events.t[0][0]
    datetime.date(1901, 2, 16)

    Calculate the time difference between the first two events.

    >>> (events.t[1][0] - events.t[0][0]).days
    59

    """
    def __init__(self, path, time_col, infer_timestamp=False):
        shp = pysal.open(path + '.shp')
        dbf = pysal.open(path + '.dbf')

        # extract the spatial coordinates from the shapefile
        x = [coords[0] for coords in shp]
        y = [coords[1] for coords in shp]

        self.n = n = len(shp)
        x = np.array(x)
        y = np.array(y)
        self.x = np.reshape(x, (n, 1))
        self.y = np.reshape(y, (n, 1))
        self.space = np.hstack((self.x, self.y))

        # extract the temporal information from the database
        if infer_timestamp:
            col = dbf.by_col(time_col)
            if isinstance(col[0], date):
                day1 = min(col)
                col = [(d - day1).days for d in col]
                t = np.array(col)
            else:
                print("Unable to parse your time column as Python datetime \
                      objects, proceeding as integers.")
                t = np.array(col)
        else:
            t = np.array(dbf.by_col(time_col))
        line = np.ones((n, 1))
        self.t = np.reshape(t, (n, 1))
        self.time = np.hstack((self.t, line))

        # close open objects
        dbf.close()
        shp.close()


def knox(s_coords, t_coords, delta, tau, permutations=99, debug=False):
    """
    Knox test for spatio-temporal interaction. [1]_

    Parameters
    ----------
    s_coords        : array
                      nx2 spatial coordinates
    t_coords        : array
                      nx1 temporal coordinates
    delta           : float
                      threshold for proximity in space
    tau             : float
                      threshold for proximity in time
    permutations    : int
                      the number of permutations used to establish pseudo-
                      significance (default is 99)
    debug           : bool
                      if true, debugging information is printed

    Returns
    -------
    knox_result     : dictionary
                      contains the statistic (stat) for the test and the
                      associated p-value (pvalue)
    stat            : float
                      value of the knox test for the dataset
    pvalue          : float
                      pseudo p-value associated with the statistic
    counts          : int
                      count of space time neighbors

    References
    ----------
    .. [1] E. Knox. 1964. The detection of space-time
       interactions. Journal of the Royal Statistical Society. Series C
       (Applied Statistics), 13(1):25-30.

    Examples
    --------
    >>> import numpy as np
    >>> import pysal

    Read in the example data and create an instance of SpaceTimeEvents.

    >>> path = pysal.examples.get_path("burkitt")
    >>> events = SpaceTimeEvents(path,'T')

    Set the random seed generator. This is used by the permutation based
    inference to replicate the pseudo-significance of our example results -
    the end-user will normally omit this step.

    >>> np.random.seed(100)

    Run the Knox test with distance and time thresholds of 20 and 5,
    respectively. This counts the events that are closer than 20 units in
    space, and 5 units in time.

    >>> result = knox(events.space, events.t, delta=20, tau=5, permutations=99)

    Next, we examine the results. First, we call the statistic from the
    results dictionary. This reports that there are 13 events close
    in both space and time, according to our threshold definitions.

    >>> result['stat'] == 13
    True

    Next, we look at the pseudo-significance of this value, calculated by
    permuting the timestamps and rerunning the statistics. In this case,
    the results indicate there is likely no space-time interaction between
    the events.

    >>> print("%2.2f"%result['pvalue'])
    0.17

    """

    # Do a kdtree on space first as the number of ties (identical points) is
    # likely to be lower for space than time.

    kd_s = pysal.cg.KDTree(s_coords)
    neigh_s = kd_s.query_pairs(delta)
    tau2 = tau * tau
    ids = np.array(list(neigh_s))

    # For the neighboring pairs in space, determine which are also time
    # neighbors

    d_t = (t_coords[ids[:, 0]] - t_coords[ids[:, 1]]) ** 2
    n_st = sum(d_t <= tau2)

    knox_result = {'stat': n_st[0]}

    if permutations:
        joint = np.zeros((permutations, 1), int)
        for p in xrange(permutations):
            np.random.shuffle(t_coords)
            d_t = (t_coords[ids[:, 0]] - t_coords[ids[:, 1]]) ** 2
            joint[p] = np.sum(d_t <= tau2)

        larger = sum(joint >= n_st[0])
        if (permutations - larger) < larger:
            larger = permutations - larger
        p_sim = (larger + 1.) / (permutations + 1.)
        knox_result['pvalue'] = p_sim
    return knox_result


def mantel(s_coords, t_coords, permutations=99, scon=1.0, spow=-1.0, tcon=1.0, tpow=-1.0):
    """
    Standardized Mantel test for spatio-temporal interaction. [2]_

    Parameters
    ----------
    s_coords        : array
                      nx2 spatial coordinates

    t_coords        : array
                      nx1 temporal coordinates

    permutations    : int
                      the number of permutations used to establish pseudo-
                      significance (default is 99)

    scon            : float
                      constant added to spatial distances

    spow            : float
                      value for power transformation for spatial distances

    tcon            : float
                      constant added to temporal distances

    tpow            : float
                      value for power transformation for temporal distances


    Returns
    -------
    mantel_result   : dictionary
                      contains the statistic (stat) for the test and the
                      associated p-value (pvalue)
    stat            : float
                      value of the knox test for the dataset
    pvalue          : float
                      pseudo p-value associated with the statistic

    References
    ----------
    .. [2] N. Mantel. 1967. The detection of disease clustering and a
        generalized regression approach. Cancer Research, 27(2):209-220.

    Examples
    --------
    >>> import numpy as np
    >>> import pysal

    Read in the example data and create an instance of SpaceTimeEvents.

    >>> path = pysal.examples.get_path("burkitt")
    >>> events = SpaceTimeEvents(path,'T')

    Set the random seed generator. This is used by the permutation based
    inference to replicate the pseudo-significance of our example results -
    the end-user will normally omit this step.

    >>> np.random.seed(100)

    The standardized Mantel test is a measure of matrix correlation between
    the spatial and temporal distance matrices of the event dataset. The
    following example runs the standardized Mantel test without a constant
    or transformation; however, as recommended by Mantel (1967) [2]_, these
    should be added by the user. This can be done by adjusting the constant
    and power parameters.

    >>> result = mantel(events.space, events.t, 99, scon=1.0, spow=-1.0, tcon=1.0, tpow=-1.0)

    Next, we examine the result of the test.

    >>> print("%6.6f"%result['stat'])
    0.048368

    Finally, we look at the pseudo-significance of this value, calculated by
    permuting the timestamps and rerunning the statistic for each of the 99
    permutations. According to these parameters, the results indicate
    space-time interaction between the events.

    >>> print("%2.2f"%result['pvalue'])
    0.01


    """

    t = t_coords
    s = s_coords
    n = len(t)

    # calculate the spatial and temporal distance matrices for the events
    distmat = cg.distance_matrix(s)
    timemat = cg.distance_matrix(t)

    # calculate the transformed standardized statistic
    timevec = (util.get_lower(timemat) + tcon) ** tpow
    distvec = (util.get_lower(distmat) + scon) ** spow
    stat = stats.pearsonr(timevec, distvec)[0].sum()

    # return the results (if no inference)
    if not permutations:
        return stat

    # loop for generating a random distribution to assess significance
    dist = []
    for i in range(permutations):
        trand = util.shuffle_matrix(timemat, range(n))
        timevec = (util.get_lower(trand) + tcon) ** tpow
        m = stats.pearsonr(timevec, distvec)[0].sum()
        dist.append(m)

    ## establish the pseudo significance of the observed statistic
    distribution = np.array(dist)
    greater = np.ma.masked_greater_equal(distribution, stat)
    count = np.ma.count_masked(greater)
    pvalue = (count + 1.0) / (permutations + 1.0)

    # report the results
    mantel_result = {'stat': stat, 'pvalue': pvalue}
    return mantel_result


def jacquez(s_coords, t_coords, k, permutations=99):
    """
    Jacquez k nearest neighbors test for spatio-temporal interaction. [3]_

    Parameters
    ----------
    s_coords        : array
                      nx2 spatial coordinates

    t_coords        : array
                      nx1 temporal coordinates

    k               : int
                      the number of nearest neighbors to be searched

    permutations    : int
                      the number of permutations used to establish pseudo-
                      significance (default is 99)

    Returns
    -------
    jacquez_result  : dictionary
                      contains the statistic (stat) for the test and the
                      associated p-value (pvalue)
    stat            : float
                      value of the Jacquez k nearest neighbors test for the
                      dataset
    pvalue          : float
                      p-value associated with the statistic (normally
                      distributed with k-1 df)

    References
    ----------
    .. [3] G. Jacquez. 1996. A k nearest neighbour test for space-time
       interaction. Statistics in Medicine, 15(18):1935-1949.


    Examples
    --------
    >>> import numpy as np
    >>> import pysal

    Read in the example data and create an instance of SpaceTimeEvents.

    >>> path = pysal.examples.get_path("burkitt")
    >>> events = SpaceTimeEvents(path,'T')

    The Jacquez test counts the number of events that are k nearest
    neighbors in both time and space. The following runs the Jacquez test
    on the example data and reports the resulting statistic. In this case,
    there are 13 instances where events are nearest neighbors in both space
    and time.

    >>> np.random.seed(100)
    >>> result = jacquez(events.space, events.t ,k=3,permutations=99)
    >>> print result['stat']
    13

    The significance of this can be assessed by calling the p-
    value from the results dictionary, as shown below. Again, no
    space-time interaction is observed.

    >>> result['pvalue'] < 0.01
    False

    """
    time = t_coords
    space = s_coords
    n = len(time)

    # calculate the nearest neighbors in space and time separately
    knnt = Distance.knnW(time, k)
    knns = Distance.knnW(space, k)

    nnt = knnt.neighbors
    nns = knns.neighbors
    knn_sum = 0

    # determine which events are nearest neighbors in both space and time
    for i in range(n):
        t_neighbors = nnt[i]
        s_neighbors = nns[i]
        check = set(t_neighbors)
        inter = check.intersection(s_neighbors)
        count = len(inter)
        knn_sum += count

    stat = knn_sum

    # return the results (if no inference)
    if not permutations:
        return stat

    # loop for generating a random distribution to assess significance
    dist = []
    for p in range(permutations):
        j = 0
        trand = np.random.permutation(time)
        knnt = Distance.knnW(trand, k)
        nnt = knnt.neighbors
        for i in range(n):
            t_neighbors = nnt[i]
            s_neighbors = nns[i]
            check = set(t_neighbors)
            inter = check.intersection(s_neighbors)
            count = len(inter)
            j += count

        dist.append(j)

    # establish the pseudo significance of the observed statistic
    distribution = np.array(dist)
    greater = np.ma.masked_greater_equal(distribution, stat)
    count = np.ma.count_masked(greater)
    pvalue = (count + 1.0) / (permutations + 1.0)

    # report the results
    jacquez_result = {'stat': stat, 'pvalue': pvalue}
    return jacquez_result


def modified_knox(s_coords, t_coords, delta, tau, permutations=99):
    """
    Baker's modified Knox test for spatio-temporal interaction. [4]_

    Parameters
    ----------
    s_coords        : array
                      nx2 spatial coordinates
    t_coords        : array
                      nx1 temporal coordinates
    delta           : float
                      threshold for proximity in space
    tau             : float
                      threshold for proximity in time
    permutations    : int
                      the number of permutations used to establish pseudo-
                      significance (default is 99)

    Returns
    -------
    modknox_result  : dictionary
                      contains the statistic (stat) for the test and the
                      associated p-value (pvalue)
    stat            : float
                      value of the modified knox test for the dataset
    pvalue          : float
                      pseudo p-value associated with the statistic

    References
    ----------
    .. [4] R.D. Baker. Identifying space-time disease clusters. Acta Tropica,
       91(3):291-299, 2004

    Examples
    --------
    >>> import numpy as np
    >>> import pysal

    Read in the example data and create an instance of SpaceTimeEvents.

    >>> path = pysal.examples.get_path("burkitt")
    >>> events = SpaceTimeEvents(path, 'T')

    Set the random seed generator. This is used by the permutation based
    inference to replicate the pseudo-significance of our example results -
    the end-user will normally omit this step.

    >>> np.random.seed(100)

    Run the modified Knox test with distance and time thresholds of 20 and 5,
    respectively. This counts the events that are closer than 20 units in
    space, and 5 units in time.

    >>> result = modified_knox(events.space, events.t, delta=20, tau=5, permutations=99)

    Next, we examine the results. First, we call the statistic from the
    results dictionary. This reports the difference between the observed
    and expected Knox statistic.

    >>> print("%2.8f" % result['stat'])
    2.81016043

    Next, we look at the pseudo-significance of this value, calculated by
    permuting the timestamps and rerunning the statistics. In this case,
    the results indicate there is likely no space-time interaction.

    >>> print("%2.2f" % result['pvalue'])
    0.11

    """
    s = s_coords
    t = t_coords
    n = len(t)

    # calculate the spatial and temporal distance matrices for the events
    sdistmat = cg.distance_matrix(s)
    tdistmat = cg.distance_matrix(t)

    # identify events within thresholds
    spacmat = np.ones((n, n))
    spacbin = sdistmat <= delta
    spacmat = spacmat * spacbin
    timemat = np.ones((n, n))
    timebin = tdistmat <= tau
    timemat = timemat * timebin

    # calculate the observed (original) statistic
    knoxmat = timemat * spacmat
    obsstat = (knoxmat.sum() - n)

    # calculate the expectated value
    ssumvec = np.reshape((spacbin.sum(axis=0) - 1), (n, 1))
    tsumvec = np.reshape((timebin.sum(axis=0) - 1), (n, 1))
    expstat = (ssumvec * tsumvec).sum()

    # calculate the modified stat
    stat = (obsstat - (expstat / (n - 1.0))) / 2.0

    # return results (if no inference)
    if not permutations:
        return stat
    distribution = []

    # loop for generating a random distribution to assess significance
    for p in range(permutations):
        rtdistmat = util.shuffle_matrix(tdistmat, range(n))
        timemat = np.ones((n, n))
        timebin = rtdistmat <= tau
        timemat = timemat * timebin

        # calculate the observed knox again
        knoxmat = timemat * spacmat
        obsstat = (knoxmat.sum() - n)

        # calculate the expectated value again
        ssumvec = np.reshape((spacbin.sum(axis=0) - 1), (n, 1))
        tsumvec = np.reshape((timebin.sum(axis=0) - 1), (n, 1))
        expstat = (ssumvec * tsumvec).sum()

        # calculate the modified stat
        tempstat = (obsstat - (expstat / (n - 1.0))) / 2.0
        distribution.append(tempstat)

    # establish the pseudo significance of the observed statistic
    distribution = np.array(distribution)
    greater = np.ma.masked_greater_equal(distribution, stat)
    count = np.ma.count_masked(greater)
    pvalue = (count + 1.0) / (permutations + 1.0)

    # return results
    modknox_result = {'stat': stat, 'pvalue': pvalue}
    return modknox_result

if __name__ == "__main__":
    import doctest
    doctest.testmod()

########NEW FILE########
__FILENAME__ = markov
"""
Markov based methods for spatial dynamics
"""
__author__ = "Sergio J. Rey <srey@asu.edu"

import numpy as np
import numpy.linalg as la
from pysal.spatial_dynamics.ergodic import fmpt, steady_state
from scipy import stats
import pysal
from operator import gt

__all__ = ["Markov", "LISA_Markov", "Spatial_Markov", "kullback",
           "prais", "shorrock"]

# TT predefine LISA transitions
# TT[i,j] is the transition type from i to j
# i = quadrant in period 0
# j = quadrant in period 1
# uses one offset so first row and col of TT are ignored
TT = np.zeros((5, 5), int)
c = 1
for i in range(1, 5):
    for j in range(1, 5):
        TT[i, j] = c
        c += 1

# MOVE_TYPES is a dictionary that returns the move type of a LISA transition
# filtered on the significance of the LISA end points
# True indicates significant LISA in a particular period
# e.g. a key of (1, 3, True, False) indicates a significant LISA located in
# quadrant 1 in period 0 moved to quadrant 3 in period 1 but was not
# significant in quadrant 3.

MOVE_TYPES = {}
c = 1
cases = (True, False)
sig_keys = [(i, j) for i in cases for j in cases]

for i, sig_key in enumerate(sig_keys):
    c = 1 + i * 16
    for i in range(1, 5):
        for j in range(1, 5):
            key = (i, j, sig_key[0], sig_key[1])
            MOVE_TYPES[key] = c
            c += 1


class Markov:
    """
    Classic Markov transition matrices

    Parameters
    ----------
    class_ids    : array (n, t)
                   One row per observation, one column recording the state of each
                   observation, with as many columns as time periods
    classes      : array (k)
                   All different classes (bins) of the matrix

    Attributes
    ----------
    p            : matrix (k, k)
                   transition probability matrix

    steady_state : matrix (k, 1)
                   ergodic distribution

    transitions  : matrix (k, k)
                   count of transitions between each state i and j

    Examples
    --------
    >>> c = np.array([['b','a','c'],['c','c','a'],['c','b','c'],['a','a','b'],['a','b','c']])
    >>> m = Markov(c)
    >>> m.classes
    array(['a', 'b', 'c'], 
          dtype='|S1')
    >>> m.p
    matrix([[ 0.25      ,  0.5       ,  0.25      ],
            [ 0.33333333,  0.        ,  0.66666667],
            [ 0.33333333,  0.33333333,  0.33333333]])
    >>> m.steady_state
    matrix([[ 0.30769231],
            [ 0.28846154],
            [ 0.40384615]])

    US nominal per capita income 48 states 81 years 1929-2009

    >>> import pysal
    >>> f = pysal.open(pysal.examples.get_path("usjoin.csv"))
    >>> pci = np.array([f.by_col[str(y)] for y in range(1929,2010)])

    set classes to quintiles for each year

    >>> q5 = np.array([pysal.Quantiles(y).yb for y in pci]).transpose()
    >>> m = Markov(q5)
    >>> m.transitions
    array([[ 729.,   71.,    1.,    0.,    0.],
           [  72.,  567.,   80.,    3.,    0.],
           [   0.,   81.,  631.,   86.,    2.],
           [   0.,    3.,   86.,  573.,   56.],
           [   0.,    0.,    1.,   57.,  741.]])
    >>> m.p
    matrix([[ 0.91011236,  0.0886392 ,  0.00124844,  0.        ,  0.        ],
            [ 0.09972299,  0.78531856,  0.11080332,  0.00415512,  0.        ],
            [ 0.        ,  0.10125   ,  0.78875   ,  0.1075    ,  0.0025    ],
            [ 0.        ,  0.00417827,  0.11977716,  0.79805014,  0.07799443],
            [ 0.        ,  0.        ,  0.00125156,  0.07133917,  0.92740926]])
    >>> m.steady_state
    matrix([[ 0.20774716],
            [ 0.18725774],
            [ 0.20740537],
            [ 0.18821787],
            [ 0.20937187]])

    Relative incomes

    >>> pci = pci.transpose()
    >>> rpci = pci/(pci.mean(axis=0))
    >>> rq = pysal.Quantiles(rpci.flatten()).yb
    >>> rq.shape = (48,81)
    >>> mq = Markov(rq)
    >>> mq.transitions
    array([[ 707.,   58.,    7.,    1.,    0.],
           [  50.,  629.,   80.,    1.,    1.],
           [   4.,   79.,  610.,   73.,    2.],
           [   0.,    7.,   72.,  650.,   37.],
           [   0.,    0.,    0.,   48.,  724.]])
    >>> mq.steady_state
    matrix([[ 0.17957376],
            [ 0.21631443],
            [ 0.21499942],
            [ 0.21134662],
            [ 0.17776576]])

    """
    def __init__(self, class_ids, classes=[]):
        #pylint; Dangerous default value [] as argument
        if len(classes):
            self.classes = classes
        else:
            self.classes = np.unique(class_ids)

        n, t = class_ids.shape
        k = len(self.classes)
        js = range(t - 1)

        classIds = self.classes.tolist()
        transitions = np.zeros((k, k))
        for state_0 in js:
            state_1 = state_0 + 1
            state_0 = class_ids[:, state_0]
            state_1 = class_ids[:, state_1]
            initial = np.unique(state_0)
            for i in initial:
                ending = state_1[state_0 == i]
                uending = np.unique(ending)
                row = classIds.index(i)
                for j in uending:
                    col = classIds.index(j)
                    transitions[row, col] += sum(ending == j)
        self.transitions = transitions
        row_sum = transitions.sum(axis=1)
        p = np.dot(np.diag(1 / (row_sum + (row_sum == 0))), transitions)
        self.p = np.matrix(p)

        # steady_state vector
        v, d = la.eig(np.transpose(self.p))
        # for a regular P maximum eigenvalue will be 1
        mv = max(v)
        # find its position
        i = v.tolist().index(mv)
        # normalize eigenvector corresponding to the eigenvalue 1
        self.steady_state = d[:, i] / sum(d[:, i])


class Spatial_Markov:
    """
    Markov transitions conditioned on the value of the spatial lag

    Parameters
    ----------

    y               : array (n,t)
                      One row per observation, one column per state of each
                      observation, with as many columns as time periods

    w               : spatial weights object

    k               : integer
                      number of classes (quantiles)

    permutations    : int
                      number of permutations (default=0) for use in randomization
                      based inference

    fixed           : boolean
                      If true, quantiles are taken over the entire n*t
                      pooled series. If false, quantiles are taken each
                      time period over n.

    Attributes
    ----------
    p               : matrix (k, k)
                      transition probability matrix for a-spatial Markov
    s               : matrix (k, 1)
                      ergodic distribution for a-spatial Markov
    transitions     : matrix (k, k)
                      counts of transitions between each state i and j
                      for a-spatial Markov
    T               : matrix (k, k, k)
                      counts of transitions for each conditional Markov
                      T[0] is the matrix of transitions for observations with
                      lags in the 0th quantile, T[k-1] is the transitions for
                      the observations with lags in the k-1th
    P               : matrix(k, k, k)
                      transition probability matrix for spatial Markov
                      first dimension is the conditioned on the lag
    S               : matrix(k, k)
                      steady state distributions for spatial Markov
                      each row is a conditional steady_state
    F               : matrix(k, k, k)
                      first mean passage times
                      first dimension is conditioned on the lag
    shtest          : list (k elements)
                      each element of the list is a tuple for a multinomial
                      difference test between the steady state distribution from
                      a conditional distribution versus the overall steady state
                      distribution, first element of the tuple is the chi2 value,
                      second its p-value and the third the degrees of freedom
    chi2            : list (k elements)
                      each element of the list is a tuple for a chi-squared test
                      of the difference between the conditional transition
                      matrix against the overall transition matrix, first
                      element of the tuple is the chi2 value, second its
                      p-value and the third the degrees of freedom
    x2              : float
                      sum of the chi2 values for each of the conditional tests
                      has an asymptotic chi2 distribution with k(k-1)(k-1)
                      degrees of freedom under the null that transition
                      probabilities are spatially homogeneous
                      (see chi2 above)
    x2_dof          : int
                      degrees of freedom for homogeneity test
    x2_pvalue       : float
                      pvalue for homogeneity test based on analytic
                      distribution
    x2_rpvalue       : float (if permutations>0)
                      pseudo p-value for x2 based on random spatial permutations
                      of the rows of the original transitions
    x2_realizations : array (permutations,1)
                      the values of x2 for the random permutations

    Notes
    -----
    Based on  Rey (2001) [1]_

    The shtest and chi2 tests should be used with caution as they are based on
    classic theory assuming random transitions. The x2 based test is
    preferable since it simulates the randomness under the null. It is an
    experimental test requiring further analysis.


    Examples
    --------
    >>> import pysal
    >>> f = pysal.open(pysal.examples.get_path("usjoin.csv"))
    >>> pci = np.array([f.by_col[str(y)] for y in range(1929,2010)])
    >>> pci = pci.transpose()
    >>> rpci = pci/(pci.mean(axis=0))
    >>> w = pysal.open(pysal.examples.get_path("states48.gal")).read()
    >>> w.transform = 'r'
    >>> sm = Spatial_Markov(rpci, w, fixed=True, k=5)
    >>> for p in sm.P:
    ...     print p
    ...
    [[ 0.96341463  0.0304878   0.00609756  0.          0.        ]
     [ 0.06040268  0.83221477  0.10738255  0.          0.        ]
     [ 0.          0.14        0.74        0.12        0.        ]
     [ 0.          0.03571429  0.32142857  0.57142857  0.07142857]
     [ 0.          0.          0.          0.16666667  0.83333333]]
    [[ 0.79831933  0.16806723  0.03361345  0.          0.        ]
     [ 0.0754717   0.88207547  0.04245283  0.          0.        ]
     [ 0.00537634  0.06989247  0.8655914   0.05913978  0.        ]
     [ 0.          0.          0.06372549  0.90196078  0.03431373]
     [ 0.          0.          0.          0.19444444  0.80555556]]
    [[ 0.84693878  0.15306122  0.          0.          0.        ]
     [ 0.08133971  0.78947368  0.1291866   0.          0.        ]
     [ 0.00518135  0.0984456   0.79274611  0.0984456   0.00518135]
     [ 0.          0.          0.09411765  0.87058824  0.03529412]
     [ 0.          0.          0.          0.10204082  0.89795918]]
    [[ 0.8852459   0.09836066  0.          0.01639344  0.        ]
     [ 0.03875969  0.81395349  0.13953488  0.          0.00775194]
     [ 0.0049505   0.09405941  0.77722772  0.11881188  0.0049505 ]
     [ 0.          0.02339181  0.12865497  0.75438596  0.09356725]
     [ 0.          0.          0.          0.09661836  0.90338164]]
    [[ 0.33333333  0.66666667  0.          0.          0.        ]
     [ 0.0483871   0.77419355  0.16129032  0.01612903  0.        ]
     [ 0.01149425  0.16091954  0.74712644  0.08045977  0.        ]
     [ 0.          0.01036269  0.06217617  0.89637306  0.03108808]
     [ 0.          0.          0.          0.02352941  0.97647059]]

    The probability of a poor state remaining poor is 0.963 if their
    neighbors are in the 1st quintile and 0.798 if their neighbors are
    in the 2nd quintile. The probability of a rich economy remaining
    rich is 0.977 if their neighbors are in the 5th quintile, but if their
    neighbors are in the 4th quintile this drops to 0.903.

    Test if the transitional dynamics are homogeneous across the lag classes

    >>> sm.x2
    200.8911757045552
    >>> sm.x2_dof
    80
    >>> sm.x2_pvalue
    2.2487567363782546e-12

    >>> sm.S
    array([[ 0.43509425,  0.2635327 ,  0.20363044,  0.06841983,  0.02932278],
           [ 0.13391287,  0.33993305,  0.25153036,  0.23343016,  0.04119356],
           [ 0.12124869,  0.21137444,  0.2635101 ,  0.29013417,  0.1137326 ],
           [ 0.0776413 ,  0.19748806,  0.25352636,  0.22480415,  0.24654013],
           [ 0.01776781,  0.19964349,  0.19009833,  0.25524697,  0.3372434 ]])

    The long run distribution for states with poor (rich) neighbors has
    0.435 (0.018) of the values in the first quintile, 0.263 (0.200) in
    the second quintile, 0.204 (0.190) in the third, 0.0684 (0.255) in the
    fourth and 0.029 (0.337) in the fifth quintile.

    >>> for f in sm.F:
    ...     print f
    ...
    [[   2.29835259   28.95614035   46.14285714   80.80952381  279.42857143]
     [  33.86549708    3.79459555   22.57142857   57.23809524  255.85714286]
     [  43.60233918    9.73684211    4.91085714   34.66666667  233.28571429]
     [  46.62865497   12.76315789    6.25714286   14.61564626  198.61904762]
     [  52.62865497   18.76315789   12.25714286    6.           34.1031746 ]]
    [[   7.46754205    9.70574606   25.76785714   74.53116883  194.23446197]
     [  27.76691978    2.94175577   24.97142857   73.73474026  193.4380334 ]
     [  53.57477715   28.48447637    3.97566318   48.76331169  168.46660482]
     [  72.03631562   46.94601483   18.46153846    4.28393653  119.70329314]
     [  77.17917276   52.08887197   23.6043956     5.14285714   24.27564033]]
    [[   8.24751154    6.53333333   18.38765432   40.70864198  112.76732026]
     [  47.35040872    4.73094099   11.85432099   34.17530864  106.23398693]
     [  69.42288828   24.76666667    3.794921     22.32098765   94.37966594]
     [  83.72288828   39.06666667   14.3           3.44668119   76.36702977]
     [  93.52288828   48.86666667   24.1           9.8           8.79255406]]
    [[  12.87974382   13.34847151   19.83446328   28.47257282   55.82395142]
     [  99.46114206    5.06359731   10.54545198   23.05133495   49.68944423]
     [ 117.76777159   23.03735526    3.94436301   15.0843986    43.57927247]
     [ 127.89752089   32.4393006    14.56853107    4.44831643   31.63099455]
     [ 138.24752089   42.7893006    24.91853107   10.35          4.05613474]]
    [[  56.2815534     1.5          10.57236842   27.02173913  110.54347826]
     [  82.9223301     5.00892857    9.07236842   25.52173913  109.04347826]
     [  97.17718447   19.53125       5.26043557   21.42391304  104.94565217]
     [ 127.1407767    48.74107143   33.29605263    3.91777427   83.52173913]
     [ 169.6407767    91.24107143   75.79605263   42.5           2.96521739]]

    States with incomes in the first quintile with neighbors in the
    first quintile return to the first quartile after 2.298 years, after
    leaving the first quintile. They enter the fourth quintile after
    80.810 years after leaving the first quintile, on average.
    Poor states within neighbors in the fourth quintile return to the
    first quintile, on average, after 12.88 years, and would enter the
    fourth quintile after 28.473 years.

    >>> np.matrix(sm.chi2)
    matrix([[  4.06139105e+01,   6.32961385e-04,   1.60000000e+01],
            [  5.55485793e+01,   2.88879565e-06,   1.60000000e+01],
            [  1.77772638e+01,   3.37100315e-01,   1.60000000e+01],
            [  4.00925436e+01,   7.54729084e-04,   1.60000000e+01],
            [  4.68588786e+01,   7.16364084e-05,   1.60000000e+01]])
    >>> np.matrix(sm.shtest)
    matrix([[  4.61209613e+02,   0.00000000e+00,   4.00000000e+00],
            [  1.48140694e+02,   0.00000000e+00,   4.00000000e+00],
            [  6.33129261e+01,   5.83089133e-13,   4.00000000e+00],
            [  7.22778509e+01,   7.54951657e-15,   4.00000000e+00],
            [  2.32659201e+02,   0.00000000e+00,   4.00000000e+00]])


    References
    ----------

    .. [1] Rey, S.J. 2001. "Spatial empirics for economic growth
       and convergence", 34 Geographical Analysis, 33, 195-214.

    """
    def __init__(self, y, w, k=4, permutations=0, fixed=False):

        self.y = y
        rows, cols = y.shape
        self.cols = cols
        npm = np.matrix
        npa = np.array
        self.fixed = fixed
        if fixed:
            yf = y.flatten()
            yb = pysal.Quantiles(yf, k=k).yb
            yb.shape = (rows, cols)
            classes = yb
        else:
            classes = npa([pysal.Quantiles(y[:, i], k=k)
                           .yb for i in np.arange(cols)]).transpose()
        classic = Markov(classes)
        self.classes = classes
        self.p = classic.p
        self.s = classic.steady_state
        self.transitions = classic.transitions
        T, P, ss, F = self._calc(y, w, classes, k=k)
        self.T = T
        self.P = P
        self.S = ss
        self.F = F
        self.shtest = self._mn_test()
        self.chi2 = self._chi2_test()
        self.x2 = sum([c[0] for c in self.chi2])
        dof = k * (k - 1) * (k - 1)
        self.x2_pvalue = 1 - stats.chi2.cdf(self.x2, dof)
        self.x2_dof = dof

        if permutations:
            nrp = np.random.permutation
            rp = range(permutations)
            counter = 0
            x2_realizations = np.zeros((permutations, 1))
            x2ss = []
            for perm in range(permutations):
                T, P, ss, F = self._calc(nrp(y), w, classes, k=k)
                x2 = [chi2(T[i], self.transitions)[0] for i in range(k)]
                x2s = sum(x2)
                x2_realizations[perm] = x2s
                if x2s >= self.x2:
                    counter += 1
            self.x2_rpvalue = (counter + 1.0) / (permutations + 1.)
            self.x2_realizations = x2_realizations

    def _calc(self, y, w, classes, k):
        # lag markov
        ly = pysal.lag_spatial(w, y)
        npm = np.matrix
        npa = np.array
        if self.fixed:
            l_classes = pysal.Quantiles(ly.flatten(), k=k).yb
            l_classes.shape = ly.shape
        else:
            l_classes = npa([pysal.Quantiles(
                ly[:, i], k=k).yb for i in np.arange(self.cols)])
            l_classes = l_classes.transpose()
        l_classic = Markov(l_classes)
        T = np.zeros((k, k, k))
        n, t = y.shape
        for t1 in range(t - 1):
            t2 = t1 + 1
            for i in range(n):
                T[l_classes[i, t1], classes[i, t1], classes[i, t2]] += 1

        P = np.zeros_like(T)
        F = np.zeros_like(T)  # fmpt
        ss = np.zeros_like(T[0])
        for i, mat in enumerate(T):
            row_sum = mat.sum(axis=1)
            row_sum = row_sum + (row_sum == 0)
            p_i = np.matrix(np.diag(1. / row_sum) * np.matrix(mat))
            #print i
            #print mat
            #print p_i
            ss[i] = steady_state(p_i).transpose()
            try:
                F[i] = fmpt(p_i)
            except:
                #pylint; "No exception type(s) specified"
                print "Singlular fmpt matrix for class ", i
            P[i] = p_i
        return T, P, ss, F

    def _mn_test(self):
        """
        helper to calculate tests of differences between steady state
        distributions from the conditional and overall distributions.
        """
        n, t = self.y.shape
        nt = n * (t - 1)
        n0, n1, n2 = self.T.shape
        rn = range(n0)
        mat = [self._ssmnp_test(
            self.s, self.S[i], self.T[i].sum()) for i in rn]
        return mat

    def _ssmnp_test(self, p1, p2, nt):
        """
        Steady state multinomial probability difference test

        Arguments
        ---------

        p1       :  array (k, 1)
                    first steady state probability distribution

        p1       :  array (k, 1)
                    second steady state probability distribution

        nt       :  int
                    number of transitions to base the test on


        Returns
        -------

        implicit : tuple (3 elements)
                   (chi2 value, pvalue, degrees of freedom)

        """
        p1 = np.array(p1)
        k, c = p1.shape
        p1.shape = (k, )
        o = nt * p2
        e = nt * p1
        d = np.multiply((o - e), (o - e))
        d = d / e
        chi2 = d.sum()
        pvalue = 1 - stats.chi2.cdf(chi2, k - 1)
        return (chi2, pvalue, k - 1)

    def _chi2_test(self):
        """
        helper to calculate tests of differences between the conditional
        transition matrices and the overall transitions matrix.
        """
        n, t = self.y.shape
        n0, n1, n2 = self.T.shape
        rn = range(n0)
        mat = [chi2(self.T[i], self.transitions) for i in rn]
        return mat


def chi2(T1, T2):
    """
    chi-squared test of difference between two transition matrices.

    Parameters
    ----------

    T1   : matrix (k, k)
           matrix of transitions (counts)

    T2   : matrix (k, k)
           matrix of transitions (counts) to use to form the probabilities
           under the null


    Returns
    -------

    implicit : tuple (3 elements)
               (chi2 value, pvalue, degrees of freedom)

    Examples
    --------

    >>> import pysal
    >>> f = pysal.open(pysal.examples.get_path("usjoin.csv"))
    >>> pci = np.array([f.by_col[str(y)] for y in range(1929,2010)]).transpose()
    >>> rpci = pci/(pci.mean(axis=0))
    >>> w = pysal.open(pysal.examples.get_path("states48.gal")).read()
    >>> w.transform='r'
    >>> sm = Spatial_Markov(rpci, w, fixed=True)
    >>> T1 = sm.T[0]
    >>> T1
    array([[ 562.,   22.,    1.,    0.],
           [  12.,  201.,   22.,    0.],
           [   0.,   17.,   97.,    4.],
           [   0.,    0.,    3.,   19.]])
    >>> T2 = sm.transitions
    >>> T2
    array([[ 884.,   77.,    4.,    0.],
           [  68.,  794.,   87.,    3.],
           [   1.,   92.,  815.,   51.],
           [   1.,    0.,   60.,  903.]])
    >>> chi2(T1,T2)
    (23.422628044813656, 0.0053137895983268457, 9)

    Notes
    -----

    Second matrix is used to form the probabilities under the null.
    Marginal sums from first matrix are distributed across these probabilities
    under the null. In other words the observed transitions are taken from T1
    while the expected transitions are formed as follows

    .. math::

            E_{i,j} = \sum_j T1_{i,j} * T2_{i,j}/\sum_j T2_{i,j}

    Degrees of freedom corrected for any rows in either T1 or T2 that have
    zero total transitions.
    """
    rs2 = T2.sum(axis=1)
    rs1 = T1.sum(axis=1)
    rs2nz = rs2 > 0
    rs1nz = rs1 > 0
    dof1 = sum(rs1nz)
    dof2 = sum(rs2nz)
    rs2 = rs2 + rs2nz
    dof = (dof1 - 1) * (dof2 - 1)
    p = np.diag(1 / rs2) * np.matrix(T2)
    E = np.diag(rs1) * np.matrix(p)
    num = T1 - E
    num = np.multiply(num, num)
    E = E + (E == 0)
    chi2 = num / E
    chi2 = chi2.sum()
    pvalue = 1 - stats.chi2.cdf(chi2, dof)
    return chi2, pvalue, dof


class LISA_Markov(Markov):
    """
    Markov for Local Indicators of Spatial Association

    Parameters
    ----------

    y  : array (n,t)
         n cross-sectional units observed over t time periods
    w  : weights instance
    permutations : int
                   number of permutations used to determine LISA significance
                   default = 0
    significance_level : float
                         significance level (two-sided) for filtering significant LISA end
                         points in a transition
                         default = 0.05

    Attributes
    ----------
    chi_2        : tuple (3 elements)
                   chi square test statistic
                   p-value
                   degrees of freedom
                   for test that dynamics of y are independent of dynamics of wy
    classes      : array (4, 1)
                   1=HH, 2=LH, 3=LL, 4=HL (own, lag)
    expected_t   : array (4, 4)
                   expected number of transitions under the null that dynamics
                   of y are independent of dynamics of wy
    move_types   : matrix (n, t-1)
                   integer values indicating which type of LISA transition
                   occurred (q1 is quadrant in period 1, q2 is quadrant in
                   period 2)

    .. Table:: Move Types

                   ==  ==     ========
                   q1  q2     move_type
                   ==  ==     ========
                   1   1      1
                   1   2      2
                   1   3      3
                   1   4      4
                   2   1      5
                   2   2      6
                   2   3      7
                   2   4      8
                   3   1      9
                   3   2      10
                   3   3      11
                   3   4      12
                   4   1      13
                   4   2      14
                   4   3      15
                   4   4      16
                   ==  ==     ========

    p            : matrix (k, k)
                   transition probability matrix
    p_values     : (if permutations > 0)
                   matrix (n, t)
                   LISA p-values for each end point
    significant_moves    : (if permutations > 0)
                       matrix (n, t-1)
                       integer values indicating the type and significance of a LISA
                       transition. st = 1 if significant in period t, else
                       st=0

    .. Table:: Significant Moves

                       ===============  ===================
                       (s1,s2)          move_type
                       ===============  ===================
                       (1,1)            [1, 16]
                       (1,0)            [17, 32]
                       (0,1)            [33, 48]
                       (0,0)            [49, 64]
                       ===============  ===================


                       == ==  ==  ==  =========
                       q1 q2  s1  s2  move_type
                       == ==  ==  ==  =========
                        1  1   1   1   1
                        1  2   1   1   2
                        1  3   1   1   3
                        1  4   1   1   4
                        2  1   1   1   5
                        2  2   1   1   6
                        2  3   1   1   7
                        2  4   1   1   8
                        3  1   1   1   9
                        3  2   1   1   10
                        3  3   1   1   11
                        3  4   1   1   12
                        4  1   1   1   13
                        4  2   1   1   14
                        4  3   1   1   15
                        4  4   1   1   16
                        1  1   1   0   17
                        1  2   1   0   18
                        .  .   .   .    .
                        .  .   .   .    .
                        4  3   1   0   31
                        4  4   1   0   32
                        1  1   0   1   33
                        1  2   0   1   34
                        .  .   .   .    .
                        .  .   .   .    .
                        4  3   0   1   47
                        4  4   0   1   48
                        1  1   0   0   49
                        1  2   0   0   50
                        .  .   .   .    .
                        .  .   .   .    .
                        4  3   0   0   63
                        4  4   0   0   64
                       == ==  ==  ==  =========

    steady_state : matrix (k, 1)
                   ergodic distribution
    transitions  : matrix (4, 4)
                   count of transitions between each state i and j
    spillover    : binary array (n, 1)
                   locations that were not part of a cluster in period 1 but
                   joined a prexisting cluster in period 2

    Examples
    --------

    >>> import numpy as np
    >>> f = pysal.open(pysal.examples.get_path("usjoin.csv"))
    >>> pci = np.array([f.by_col[str(y)] for y in range(1929,2010)]).transpose()
    >>> w = pysal.open(pysal.examples.get_path("states48.gal")).read()
    >>> lm = LISA_Markov(pci,w)
    >>> lm.classes
    array([1, 2, 3, 4])
    >>> lm.steady_state
    matrix([[ 0.28561505],
            [ 0.14190226],
            [ 0.40493672],
            [ 0.16754598]])
    >>> lm.transitions
    array([[  1.08700000e+03,   4.40000000e+01,   4.00000000e+00,
              3.40000000e+01],
           [  4.10000000e+01,   4.70000000e+02,   3.60000000e+01,
              1.00000000e+00],
           [  5.00000000e+00,   3.40000000e+01,   1.42200000e+03,
              3.90000000e+01],
           [  3.00000000e+01,   1.00000000e+00,   4.00000000e+01,
              5.52000000e+02]])
    >>> lm.p
    matrix([[ 0.92985458,  0.03763901,  0.00342173,  0.02908469],
            [ 0.07481752,  0.85766423,  0.06569343,  0.00182482],
            [ 0.00333333,  0.02266667,  0.948     ,  0.026     ],
            [ 0.04815409,  0.00160514,  0.06420546,  0.88603531]])
    >>> lm.move_types
    array([[11, 11, 11, ..., 11, 11, 11],
           [ 6,  6,  6, ...,  6,  7, 11],
           [11, 11, 11, ..., 11, 11, 11],
           ..., 
           [ 6,  6,  6, ...,  6,  6,  6],
           [ 1,  1,  1, ...,  6,  6,  6],
           [16, 16, 16, ..., 16, 16, 16]])

    Now consider only moves with one, or both, of the LISA end points being
    significant

    >>> np.random.seed(10)
    >>> lm_random = pysal.LISA_Markov(pci, w, permutations=99)
    >>> lm_random.significant_moves
    array([[11, 11, 11, ..., 59, 59, 59],
           [54, 54, 54, ..., 54, 55, 59],
           [11, 11, 11, ..., 11, 59, 59],
           ..., 
           [54, 54, 54, ..., 54, 54, 54],
           [49, 49, 49, ..., 54, 54, 54],
           [64, 64, 64, ..., 64, 64, 64]])

    Any value less than 49 indicates at least one of the LISA end points was
    significant. So for example, the first spatial unit experienced a
    transition of type 11 (LL, LL)  during the first three and last tree
    intervals (according to lm.move_types), however, the last three of these
    transitions involved insignificant LISAS in both the start and ending year
    of each transition.

    Test whether the moves of y are independent of the moves of wy

    >>> lm.chi_2
    (162.47505958346289, 0.0, 9)

    Actual transitions of LISAs

    >>> lm.transitions
    array([[  1.08700000e+03,   4.40000000e+01,   4.00000000e+00,
              3.40000000e+01],
           [  4.10000000e+01,   4.70000000e+02,   3.60000000e+01,
              1.00000000e+00],
           [  5.00000000e+00,   3.40000000e+01,   1.42200000e+03,
              3.90000000e+01],
           [  3.00000000e+01,   1.00000000e+00,   4.00000000e+01,
              5.52000000e+02]])

    Expected transitions of LISAs under the null y and wy are moving
    independently of one another

    >>> lm.expected_t
    array([[  1.12328098e+03,   1.15377356e+01,   3.47522158e-01,
              3.38337644e+01],
           [  3.50272664e+00,   5.28473882e+02,   1.59178880e+01,
              1.05503814e-01],
           [  1.53878082e-01,   2.32163556e+01,   1.46690710e+03,
              9.72266513e+00],
           [  9.60775143e+00,   9.86856346e-02,   6.23537392e+00,
              6.07058189e+02]])
    """
    def __init__(self, y, w, permutations=0,
                 significance_level=0.05):
        y = y.transpose()
        pml = pysal.Moran_Local

        #################################################################
        # have to optimize conditional spatial permutations over a
        # time series - this is a place holder for the foreclosure paper
        ml = [pml(yi, w, permutations=permutations) for yi in y]
        #################################################################

        q = np.array([mli.q for mli in ml]).transpose()
        classes = np.arange(1, 5)  # no guarantee all 4 quadrants are visited
        Markov.__init__(self, q, classes)
        self.q = q
        self.w = w
        n, k = q.shape
        k -= 1
        self.significance_level = significance_level
        move_types = np.zeros((n, k), int)
        sm = np.zeros((n, k), int)
        self.significance_level = significance_level
        if permutations > 0:
            p = np.array([mli.p_z_sim for mli in ml]).transpose()
            self.p_values = p
            pb = p <= significance_level
        else:
            pb = np.zeros_like(y.T)
        for t in range(k):
            origin = q[:, t]
            dest = q[:, t + 1]
            p_origin = pb[:, t]
            p_dest = pb[:, t]
            for r in range(n):
                move_types[r, t] = TT[origin[r], dest[r]]
                key = (origin[r], dest[r], p_origin[r], p_dest[r])
                sm[r, t] = MOVE_TYPES[key]
        if permutations > 0:
            self.significant_moves = sm
        self.move_types = move_types

        # null of own and lag moves being independent

        ybar = y.mean(axis=0)
        r = y / ybar
        ylag = np.array([pysal.lag_spatial(w, yt) for yt in y])
        rlag = ylag / ybar
        rc = r < 1.
        rlagc = rlag < 1.
        markov_y = pysal.Markov(rc)
        markov_ylag = pysal.Markov(rlagc)
        A = np.matrix([[1, 0, 0, 0],
                       [0, 0, 1, 0],
                       [0, 0, 0, 1],
                       [0, 1, 0, 0]])

        kp = A * np.kron(markov_y.p, markov_ylag.p) * A.T
        trans = self.transitions.sum(axis=1)
        t1 = np.diag(trans) * kp
        t2 = self.transitions
        t1 = t1.getA()
        self.chi_2 = pysal.spatial_dynamics.markov.chi2(t1, t2)
        self.expected_t = t1
        self.permutations = permutations

    def spillover(self, quadrant=1, neighbors_on=False):
        """
        Detect spillover locations for diffusion in LISA Markov

        Parameters
        ----------
        quadrant : int
                   which quadrant in the scatterplot should form the core of a
                   cluster

        neighbors_on : binary
                   If false then only the 1st order neighbors of a core
                   location are included in the cluster.
                   If true, neighbors of cluster core 1st order neighbors are
                   included in the cluster

        Returns
        -------
        dictionary : two keys - values pairs
                    'components' - array (n, t)
                    values are integer ids (starting at 1) indicating which
                    component/cluster observation i in period t belonged to
                    'spillover' - array (n, t-1)
                    binary values indicating if the location was a spill-over
                    location that became a new member of a previously existing
                    cluster

        Examples
        --------

        >>> f = pysal.open(pysal.examples.get_path("usjoin.csv"))
        >>> pci = np.array([f.by_col[str(y)] for y in range(1929,2010)]).transpose()
        >>> w = pysal.open(pysal.examples.get_path("states48.gal")).read()
        >>> np.random.seed(10)
        >>> lm_random = pysal.LISA_Markov(pci, w, permutations=99)
        >>> r = lm_random.spillover()
        >>> r['components'][:,12]
        array([ 0.,  1.,  0.,  1.,  0.,  2.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,
                0.,  0.,  0.,  0.,  2.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,
                2.,  2.,  0.,  2.,  0.,  0.,  0.,  0.,  1.,  2.,  2.,  0.,  0.,
                0.,  0.,  0.,  2.,  0.,  0.,  0.,  0.,  0.])
        >>> r['components'][:,13]
        array([ 0.,  2.,  0.,  2.,  0.,  1.,  1.,  0.,  0.,  2.,  0.,  0.,  0.,
                0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,
                0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,  2.,  1.,  1.,  0.,  0.,
                0.,  0.,  2.,  1.,  0.,  2.,  0.,  0.,  0.])
        >>> r['spill_over'][:,12]
        array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,
                0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
                0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
                0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.])

        Including neighbors of core neighbors

        >>> rn = lm_random.spillover(neighbors_on=True)
        >>> rn['components'][:,12]
        array([ 0.,  2.,  0.,  2.,  2.,  1.,  1.,  0.,  0.,  2.,  0.,  0.,  0.,
                0.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,
                1.,  1.,  2.,  1.,  0.,  0.,  1.,  0.,  2.,  1.,  1.,  0.,  0.,
                0.,  0.,  2.,  1.,  1.,  2.,  1.,  0.,  0.])
        >>> rn["components"][:,13]
        array([ 0.,  2.,  0.,  2.,  2.,  1.,  1.,  0.,  0.,  2.,  0.,  0.,  0.,
                0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  2.,  0.,  2.,
                1.,  1.,  2.,  1.,  0.,  0.,  1.,  0.,  2.,  1.,  1.,  0.,  0.,
                0.,  0.,  2.,  1.,  1.,  2.,  1.,  0.,  2.])
        >>> rn["spill_over"][:,12]
        array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
                0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,
                0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
                0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.])


        """
        n, k = self.q.shape
        if self.permutations:
            spill_over = np.zeros((n, k - 1))
            components = np.zeros((n, k))
            i2id = {}  # handle string keys
            for key in self.w.neighbors.keys():
                id = self.w.id2i[key]  # pylint "redefining built-in 'id'
                i2id[id] = key
            sig_lisas = (self.q == quadrant) \
                * (self.p_values <= self.significance_level)
            sig_ids = [np.nonzero(
                sig_lisas[:, i])[0].tolist() for i in range(k)]

            neighbors = self.w.neighbors
            for t in range(k - 1):
                s1 = sig_ids[t]
                s2 = sig_ids[t + 1]
                g1 = pysal.region.components.Graph(undirected=True)
                for i in s1:
                    for neighbor in neighbors[i2id[i]]:
                        g1.add_edge(i2id[i], neighbor, 1.0)
                        if neighbors_on:
                            for nn in neighbors[neighbor]:
                                g1.add_edge(neighbor, nn, 1.0)
                components1 = g1.connected_components(op=gt)
                components1 = [list(c.nodes) for c in components1]
                g2 = pysal.region.components.Graph(undirected=True)
                for i in s2:
                    for neighbor in neighbors[i2id[i]]:
                        g2.add_edge(i2id[i], neighbor, 1.0)
                        if neighbors_on:
                            for nn in neighbors[neighbor]:
                                g2.add_edge(neighbor, nn, 1.0)
                components2 = g2.connected_components(op=gt)
                components2 = [list(c.nodes) for c in components2]
                c2 = []
                c1 = []
                for c in components2:
                    c2.extend(c)
                for c in components1:
                    c1.extend(c)

                new_ids = [j for j in c2 if j not in c1]
                spill_ids = []
                for j in new_ids:
                    # find j's component in period 2
                    cj = [c for c in components2 if j in c][0]
                    # for members of j's component in period 2, check if they belonged to
                    # any components in period 1
                    for i in cj:
                        if i in c1:
                            spill_ids.append(j)
                            break
                for spill_id in spill_ids:
                    id = self.w.id2i[spill_id]
                    spill_over[id, t] = 1
                for c, component in enumerate(components1):
                    for i in component:
                        ii = self.w.id2i[i]
                        components[ii, t] = c + 1
            results = {}
            results['components'] = components
            results['spill_over'] = spill_over
            return results

        else:
            return None


def kullback(F):
    """
    Kullback information based test of Markov Homogeneity

    Parameters
    ----------
    F : array (s, r, r)
       Values are transitions (not probabilities) for
       s strata
       r initial states
       r terminal states

    Returns
    -------

    Results : Dictionary (key - value)

        Conditional homogeneity - (float) test statistic for homogeneity of
        transition probabilities across strata

        Conditional homogeneity pvalue - (float) p-value for test statistic

        Conditional homogeneity dof - (int) degrees of freedom =  r(s-1)(r-1)

    Notes
    -----

    Based on  Kullback, Kupperman and Ku (1962) [2]_
    Example below is taken from Table 9.2 

    Examples
    --------

    >>> s1 = np.array([
    ...         [ 22, 11, 24,  2,  2,  7],
    ...         [ 5, 23, 15,  3, 42,  6],
    ...         [ 4, 21, 190, 25, 20, 34],
    ...         [0, 2, 14, 56, 14, 28],
    ...         [32, 15, 20, 10, 56, 14],
    ...         [5, 22, 31, 18, 13, 134]
    ...     ])
    >>> s2 = np.array([
    ...     [3, 6, 9, 3, 0, 8],
    ...     [1, 9, 3, 12, 27, 5],
    ...     [2, 9, 208, 32, 5, 18],
    ...     [0, 14, 32, 108, 40, 40],
    ...     [22, 14, 9, 26, 224, 14],
    ...     [1, 5, 13, 53, 13, 116]
    ...     ])
    >>>
    >>> F = np.array([s1, s2])
    >>> res = kullback(F)
    >>> res['Conditional homogeneity']
    160.96060031170782
    >>> res['Conditional homogeneity dof']
    30
    >>> res['Conditional homogeneity pvalue']
    0.0

    References
    ----------

    .. [2] Kullback, S. Kupperman, M. and H.H. Ku. (1962) "Tests for
       contigency tables and Markov chains", Technometrics : 4, 573--608.

    """

    F1 = F == 0
    F1 = F + F1
    FLF = F * np.log(F1)
    T1 = 2 * FLF.sum()

    FdJK = F.sum(axis=0)
    FdJK1 = FdJK + (FdJK == 0)
    FdJKLFdJK = FdJK * np.log(FdJK1)
    T2 = 2 * FdJKLFdJK.sum()

    FdJd = F.sum(axis=0).sum(axis=1)
    FdJd1 = FdJd + (FdJd == 0)
    T3 = 2 * (FdJd * np.log(FdJd1)).sum()

    FIJd = F[:, :].sum(axis=1)
    FIJd1 = FIJd + (FIJd == 0)
    T4 = 2 * (FIJd * np.log(FIJd1)).sum()

    FIdd = F.sum(axis=1).sum(axis=1)
    T5 = 2 * (FIdd * np.log(FIdd)).sum()

    T6 = F.sum()
    T6 = 2 * T6 * np.log(T6)

    s, r, r1 = F.shape
    chom = T1 - T4 - T2 + T3
    cdof = r * (s - 1) * (r - 1)
    results = {}
    results['Conditional homogeneity'] = chom
    results['Conditional homogeneity dof'] = cdof
    results['Conditional homogeneity pvalue'] = 1 - stats.chi2.cdf(chom, cdof)
    return results


def prais(pmat):
    """
    Prais conditional mobility measure

    Parameters
    ----------

    pmat : kxk matrix
          Markov probability transition matrix

    Returns
    -------

    pr : 1xk matrix
          Conditional mobility measures for each of the k classes.

    Notes
    -----

    Prais' conditional mobility measure for a class is defined as:

    .. math::

            pr_i = 1 - \sum_j p_{i,j}

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> f = pysal.open(pysal.examples.get_path("usjoin.csv"))
    >>> pci = np.array([f.by_col[str(y)] for y in range(1929,2010)])
    >>> q5 = np.array([pysal.Quantiles(y).yb for y in pci]).transpose()
    >>> m = pysal.Markov(q5)
    >>> m.transitions
    array([[ 729.,   71.,    1.,    0.,    0.],
           [  72.,  567.,   80.,    3.,    0.],
           [   0.,   81.,  631.,   86.,    2.],
           [   0.,    3.,   86.,  573.,   56.],
           [   0.,    0.,    1.,   57.,  741.]])
    >>> m.p
    matrix([[ 0.91011236,  0.0886392 ,  0.00124844,  0.        ,  0.        ],
            [ 0.09972299,  0.78531856,  0.11080332,  0.00415512,  0.        ],
            [ 0.        ,  0.10125   ,  0.78875   ,  0.1075    ,  0.0025    ],
            [ 0.        ,  0.00417827,  0.11977716,  0.79805014,  0.07799443],
            [ 0.        ,  0.        ,  0.00125156,  0.07133917,  0.92740926]])
    >>> pysal.spatial_dynamics.markov.prais(m.p)
    matrix([[ 0.08988764,  0.21468144,  0.21125   ,  0.20194986,  0.07259074]])

    """
    pr = (pmat.sum(axis=1) - np.diag(pmat))[0]
    return pr


def shorrock(pmat):
    """
    Shorrocks mobility measure

    Parameters
    ----------

    pmat : kxk matrix
          Markov probability transition matrix

    Returns
    -------

    sh : scalar
          Conditional mobility measure



    Notes
    -----

    Shorock's mobility measure is defined as

    .. math::

         sh = (k  - \sum_{j=1}^{k} p_{j,j})/(k - 1)

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> f = pysal.open(pysal.examples.get_path("usjoin.csv"))
    >>> pci = np.array([f.by_col[str(y)] for y in range(1929,2010)])
    >>> q5 = np.array([pysal.Quantiles(y).yb for y in pci]).transpose()
    >>> m = pysal.Markov(q5)
    >>> m.transitions
    array([[ 729.,   71.,    1.,    0.,    0.],
           [  72.,  567.,   80.,    3.,    0.],
           [   0.,   81.,  631.,   86.,    2.],
           [   0.,    3.,   86.,  573.,   56.],
           [   0.,    0.,    1.,   57.,  741.]])
    >>> m.p
    matrix([[ 0.91011236,  0.0886392 ,  0.00124844,  0.        ,  0.        ],
            [ 0.09972299,  0.78531856,  0.11080332,  0.00415512,  0.        ],
            [ 0.        ,  0.10125   ,  0.78875   ,  0.1075    ,  0.0025    ],
            [ 0.        ,  0.00417827,  0.11977716,  0.79805014,  0.07799443],
            [ 0.        ,  0.        ,  0.00125156,  0.07133917,  0.92740926]])
    >>> pysal.spatial_dynamics.markov.shorrock(m.p)
    0.19758992000997844

    """
    t = np.trace(pmat)
    k = pmat.shape[1]
    sh = (k - t) / (k - 1)
    return sh



########NEW FILE########
__FILENAME__ = rank
"""
Rank and spatial rank mobility measures
"""
__author__ = "Sergio J. Rey <srey@asu.edu> "

#from pysal.common import *
from scipy.stats.mstats import rankdata
from scipy.special import erfc
import pysal
import numpy as np
import scipy as sp
from numpy.random import permutation as NRP

__all__ = ['SpatialTau', 'Tau', 'Theta', ]


class Theta:
    """
    Regime mobility measure

    For sequence of time periods Theta measures the extent to which rank
    changes for a variable measured over n locations are in the same direction
    within mutually exclusive and exhaustive partitions (regimes) of the n locations.

    Theta is defined as the sum of the absolute sum of rank changes within
    the regimes over the sum of all absolute rank changes.


    Parameters
    ----------
    y            : array (n,k) with k>=2
                   successive columns of y are later moments in time (years,
                   months,etc)
    regime       : array (n,)
                   values corresponding to which regime each observation belongs to
    permutations : int
                   number of random spatial permutations to generate for
                   computationally based inference

    Attributes
    ----------
    ranks        : array
                   ranks of the original y array (by columns)
    regimes      : array
                   the original regimes array
    total        : array (k-1,)
                   the total number of rank changes for each of the k periods
    max_total    : int
                   the theoretical maximum number of rank changes for n
                   observations
    theta        : array (k-1,)
                   the theta statistic for each of the k-1 intervals
    permutations : int
                   the number of permutations
    pvalue_left  : float
                   p-value for test that observed theta is significantly lower
                   than its expectation under complete spatial randomness
    pvalue_right : float
                   p-value for test that observed theta is significantly
                   greater than its expectation under complete spatial randomness


    References
    ----------
    Rey, S.J. (2004) "Spatial dependence in the evolution of regional income
    distributions," in A. Getis, J. Mur and H.Zoeller (eds). Spatial
    Econometrics and Spatial Statistics. Palgrave, London, pp. 194-213.


    Examples
    --------
    >>> import pysal
    >>> f=pysal.open(pysal.examples.get_path("mexico.csv"))
    >>> vnames=["pcgdp%d"%dec for dec in range(1940,2010,10)]
    >>> y=np.transpose(np.array([f.by_col[v] for v in vnames]))
    >>> regime=np.array(f.by_col['esquivel99'])
    >>> np.random.seed(10)
    >>> t=Theta(y,regime,999)
    >>> t.theta
    array([[ 0.41538462,  0.28070175,  0.61363636,  0.62222222,  0.33333333,
             0.47222222]])
    >>> t.pvalue_left
    array([ 0.307,  0.077,  0.823,  0.552,  0.045,  0.735])
    >>> t.total
    array([ 130.,  114.,   88.,   90.,   90.,   72.])
    >>> t.max_total
    512
    >>>
    """
    def __init__(self, y, regime, permutations=999):
        ranks = rankdata(y, axis=0)
        self.ranks = ranks
        n, k = y.shape
        ranks_d = ranks[:, range(1, k)] - ranks[:, range(k - 1)]
        self.ranks_d = ranks_d
        regimes = sp.unique(regime)
        self.regimes = regimes
        self.total = sum(abs(ranks_d))
        self.max_total = sum([abs(i - n + i - 1) for i in range(1, n + 1)])
        self._calc(regime)
        self.theta = self._calc(regime)
        self.permutations = permutations
        if permutations:
            np.perm = np.random.permutation
            sim = np.array([self._calc(
                np.perm(regime)) for i in xrange(permutations)])
            self.theta.shape = (1, len(self.theta))
            sim = np.concatenate((self.theta, sim))
            self.sim = sim
            den = permutations + 1.
            self.pvalue_left = (sim <= sim[0]).sum(axis=0) / den
            self.pvalue_right = (sim > sim[0]).sum(axis=0) / den
            self.z = (sim[0] - sim.mean(axis=0)) / sim.std(axis=0)

    def _calc(self, regime):
        within = [abs(
            sum(self.ranks_d[regime == reg])) for reg in self.regimes]
        return np.array(sum(within) / self.total)


class Tau:
    """
    Kendall's Tau is based on a comparison of the number of pairs of n
    observations that have concordant ranks between two variables.

    Parameters
    ----------
    x            : array (n,)
                   first variable
    y            : array (n,)
                   second variable

    Attributes
    ----------
    tau          : float
                   The classic Tau statistic

    tau_p       : float
                  asymptotic p-value

    Notes
    -----

    Modification of algorithm suggested by Christensen (2005).
    PySAL implementation uses a list based representation of a binary tree for
    the accumulation of the concordance measures. Ties are handled by this
    implementation (in other words, if there are ties in either x, or y, or
    both, the calculation returns Tau_b, if no ties classic Tau is returned.)

    References
    ----------

    Christensen, D. (2005) Fast algorithms for the calculation of
    Kendall's tau. Computational Statistics, 20: 51-62.


    Examples
    --------

    # from scipy example

    >>> from scipy.stats import kendalltau
    >>> x1 = [12, 2, 1, 12, 2]
    >>> x2 = [1, 4, 7, 1, 0]
    >>> kt = Tau(x1,x2)
    >>> kt.tau
    -0.47140452079103173
    >>> kt.tau_p
    0.24821309157521476
    >>> skt = kendalltau(x1,x2)
    >>> skt
    (-0.47140452079103173, 0.24821309157521476)

    """

    def __init__(self, x, y):
        res = self._calc(x, y)
        self.tau = res[0]
        self.tau_p = res[1]
        self.concordant = res[2]
        self.discordant = res[3]
        self.extraX = res[4]
        self.extraY = res[5]

    def _calc(self, x, y):
        """
        List based implementation of binary tree algorithm for concordance
        measure after Christensen (2005).

        """
        x = np.array(x)
        y = np.array(y)
        n = len(y)
        perm = range(n)
        perm.sort(key=lambda a: (x[a], y[a]))
        vals = y[perm]
        ExtraY = 0
        ExtraX = 0
        ACount = 0
        BCount = 0
        CCount = 0
        DCount = 0
        ECount = 0
        DCount = 0
        Concordant = 0
        Discordant = 0
        # ids for left child
        li = [None] * (n - 1)
        # ids for right child
        ri = [None] * (n - 1)
        # number of left descendants for a node
        ld = np.zeros(n)
        # number of values equal to value i
        nequal = np.zeros(n)

        for i in range(1, n):
            NumBefore = 0
            NumEqual = 1
            root = 0
            x0 = x[perm[i - 1]]
            y0 = y[perm[i - 1]]
            x1 = x[perm[i]]
            y1 = y[perm[i]]
            if x0 != x1:
                DCount = 0
                ECount = 1
            else:
                if y0 == y1:
                    ECount += 1
                else:
                    DCount += ECount
                    ECount = 1
            root = 0
            inserting = True
            while inserting:
                current = y[perm[i]]
                if current > y[perm[root]]:
                    # right branch
                    NumBefore += 1 + ld[root] + nequal[root]
                    if ri[root] is None:
                        # insert as right child to root
                        ri[root] = i
                        inserting = False
                    else:
                        root = ri[root]
                elif current < y[perm[root]]:
                    # increment number of left descendants
                    ld[root] += 1
                    if li[root] is None:
                        # insert as left child to root
                        li[root] = i
                        inserting = False
                    else:
                        root = li[root]
                elif current == y[perm[root]]:
                    NumBefore += ld[root]
                    NumEqual += nequal[root] + 1
                    nequal[root] += 1
                    inserting = False

            ACount = NumBefore - DCount
            BCount = NumEqual - ECount
            CCount = i - (ACount + BCount + DCount + ECount - 1)
            ExtraY += DCount
            ExtraX += BCount
            Concordant += ACount
            Discordant += CCount

        cd = Concordant + Discordant
        num = Concordant - Discordant
        tau = num / np.sqrt((cd + ExtraX) * (cd + ExtraY))
        v = (4. * n + 10) / (9. * n * (n - 1))
        z = tau / np.sqrt(v)
        pval = erfc(np.abs(z) / 1.4142136)  # follow scipy
        return tau, pval, Concordant, Discordant, ExtraX, ExtraY


class SpatialTau:
    """
    Spatial version of Kendall's rank correlation statistic

    Kendall's Tau is based on a comparison of the number of pairs of n
    observations that have concordant ranks between two variables. The spatial
    Tau decomposes these pairs into those that are spatial neighbors and those
    that are not, and examines whether the rank correlation is different
    between the two sets relative to what would be expected under spatial randomness.

    Parameters
    ----------
    x            : array (n,)
                   first variable
    y            : array (n,)
                   second variable
    w            : W
                   spatial weights object
    permutations : int
                   number of random spatial permutations for computationally
                   based inference

    Attributes
    ----------
    tau          : float
                   The classic Tau statistic
    tau_spatial  : float
                   Value of Tau for pairs that are spatial neighbors
    taus         : array (permtuations x 1)
                   Values of simulated tau_spatial values under random spatial permutations in both periods. (Same permutation used for start and ending period).
    pairs_spatial : int
                    Number of spatial pairs
    concordant   : float
                   Number of concordant pairs
    concordant_spatial : float
                   Number of concordant pairs that are spatial neighbors
    extraX       : float
                   Number of extra X pairs
    extraY       : float
                   Number of extra Y pairs
    discordant   : float
                   Number of discordant pairs
    discordant_spatial   : float
                   Number of discordant pairs that are spatial neighbors
    taus         : float
                   spatial tau values for permuted samples (if permutations>0)
    tau_spatial_psim:
                 : float
                   pseudo p-value for observed tau_spatial under the null of spatial randomness (if permutations>0)

    Notes
    -----

    Algorithm has two stages. The first calculates classic Tau using a list
    based implementation of the algorithm from Christensen (2005). Second
    stage calculates concordance measures for neighboring pairs of locations
    using a modification of the algorithm from Press et al (2007). See Rey
    (2014) for details.

    References
    ----------

    Christensen, D. (2005) "Fast algorithms for the calculation of
    Kendall's tau." Computational Statistics, 20: 51-62.

    Press, W.H, S. A Teukolsky, W.T. Vetterling and B. P. Flannery (2007).
    Numerical Recipes: The Art of Scientific Computing. Cambridge. Pg 752.

    Rey, S.J. (2004) "Spatial dependence in the evolution of regional income
    distributions," in A. Getis, J. Mur and H.Zoeller (eds). Spatial
    Econometrics and Spatial Statistics. Palgrave, London, pp. 194-213.

    Rey, S.J. (2014) "Fast algorithms for calculation of a space-time
    concordance measure." Computational Statistics. Forthcoming.


    Examples
    --------
    >>> import pysal
    >>> f=pysal.open(pysal.examples.get_path("mexico.csv"))
    >>> vnames=["pcgdp%d"%dec for dec in range(1940,2010,10)]
    >>> y=np.transpose(np.array([f.by_col[v] for v in vnames]))
    >>> regime=np.array(f.by_col['esquivel99'])
    >>> w=pysal.weights.regime_weights(regime)
    >>> np.random.seed(12345)
    >>> res=[SpatialTau(y[:,i],y[:,i+1],w,99) for i in range(6)]
    >>> for r in res:
    ...     ev = r.taus.mean()
    ...     "%8.3f %8.3f %8.3f"%(r.tau_spatial, ev, r.tau_spatial_psim)
    ...
    '   0.281    0.466    0.010'
    '   0.348    0.499    0.010'
    '   0.460    0.546    0.020'
    '   0.505    0.532    0.210'
    '   0.483    0.499    0.270'
    '   0.572    0.579    0.280'
    """

    def __init__(self, x, y, w, permutations=0):

        w.transform = 'b'
        self.n = len(x)
        res = Tau(x, y)
        self.tau = res.tau
        self.tau_p = res.tau_p
        self.concordant = res.concordant
        self.discordant = res.discordant
        self.extraX = res.extraX
        self.extraY = res.extraY
        res = self._calc(x, y, w)
        self.tau_spatial = res[0]
        self.pairs_spatial = int(w.s0 / 2.)
        self.concordant_spatial = res[1]
        self.discordant_spatial = res[2]

        if permutations > 0:
            taus = np.zeros(permutations)
            ids = np.arange(self.n)
            for r in xrange(permutations):
                rids = np.random.permutation(ids)
                taus[r] = self._calc(x[rids], y[rids], w)[0]
            self.taus = taus
            self.tau_spatial_psim = pseudop(taus, self.tau_spatial,
                                            permutations)

    def _calc(self, x, y, w):
        n1 = n2 = iS = gc = 0
        ijs = {}
        for i in w.id_order:
            xi = x[i]
            yi = y[i]
            for j in w.neighbors[i]:
                if i < j:
                    ijs[(i, j)] = (i, j)
                    xj = x[j]
                    yj = y[j]
                    dx = xi - xj
                    dy = yi - yj
                    dxdy = dx * dy
                    if dxdy != 0:
                        n1 += 1
                        n2 += 2
                        if dxdy > 0.0:
                            gc += 1
                            iS += 1
                        else:
                            iS -= 1
                    else:
                        if dx != 0.0:
                            n1 += 1
                        if dy != 0.0:
                            n2 += 1
        tau_g = iS / (np.sqrt(n1) * np.sqrt(n2))
        gd = gc - iS
        return [tau_g, gc, gd]


def pseudop(sim, observed, nperm):
    above = sim >= observed
    larger = above.sum()
    psim = (larger + 1.) / (nperm + 1.)
    if psim > 0.5:
        psim = (nperm - larger + 1.) / (nperm + 1.)
    return psim


########NEW FILE########
__FILENAME__ = test_directional
import unittest
import pysal
from pysal.spatial_dynamics import directional
import numpy as np


class Rose_Tester(unittest.TestCase):
    def setUp(self):
        f = open(pysal.examples.get_path("spi_download.csv"), 'r')
        lines = f.readlines()
        f.close()
        lines = [line.strip().split(",") for line in lines]
        names = [line[2] for line in lines[1:-5]]
        data = np.array([map(int, line[3:]) for line in lines[1:-5]])
        sids = range(60)
        out = ['"United States 3/"',
               '"Alaska 3/"',
               '"District of Columbia"',
               '"Hawaii 3/"',
               '"New England"',
               '"Mideast"',
               '"Great Lakes"',
               '"Plains"',
               '"Southeast"',
               '"Southwest"',
               '"Rocky Mountain"',
               '"Far West 3/"']
        snames = [name for name in names if name not in out]
        sids = [names.index(name) for name in snames]
        states = data[sids, :]
        us = data[0]
        years = np.arange(1969, 2009)
        rel = states / (us * 1.)
        gal = pysal.open(pysal.examples.get_path('states48.gal'))
        self.w = gal.read()
        self.w.transform = 'r'
        self.Y = rel[:, [0, -1]]

    def test_rose(self):
        k = 4
        np.random.seed(100)
        r4 = directional.rose(self.Y, self.w, k, permutations=999)
        exp = [0., 1.57079633, 3.14159265, 4.71238898, 6.28318531]
        obs = list(r4['cuts'])
        for i in range(k + 1):
            self.assertAlmostEqual(exp[i], obs[i])
        self.assertEquals(list(r4['counts']), [32, 5, 9, 2])
        exp = [0.02, 0.001, 0.001, 0.001]
        obs = list(r4['pvalues'])
        for i in range(k):
            self.assertAlmostEqual(exp[i], obs[i])


suite = unittest.TestSuite()
test_classes = [Rose_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_ergodic
import unittest
import pysal
from pysal.spatial_dynamics import ergodic
import numpy as np


class SteadyState_Tester(unittest.TestCase):
    def setUp(self):
        self.p = np.matrix([[.5, .25, .25], [.5, 0, .5], [.25, .25, .5]])

    def test_steady_state(self):
        obs = ergodic.steady_state(self.p).tolist()
        exp = np.matrix([[0.4], [0.2], [0.4]]).tolist()
        k = self.p.shape[0]
        for i in range(k):
            self.assertAlmostEqual(exp[i][0], obs[i][0])


class Fmpt_Tester(unittest.TestCase):
    def setUp(self):
        self.p = np.matrix([[.5, .25, .25], [.5, 0, .5], [.25, .25, .5]])

    def test_fmpt(self):
        k = self.p.shape[0]
        obs = ergodic.fmpt(self.p).flatten().tolist()[0]
        exp = np.matrix([[2.5, 4., 3.33333333], [2.66666667, 5.,
                                                 2.66666667], [3.33333333, 4., 2.5]])
        exp = exp.flatten().tolist()[0]
        for i in range(k):
            self.assertAlmostEqual(exp[i], obs[i])


class VarFmpt_Tester(unittest.TestCase):
    def setUp(self):
        self.p = np.matrix([[.5, .25, .25], [.5, 0, .5], [.25, .25, .5]])

    def test_var_fmpt(self):
        k = self.p.shape[0]
        obs = ergodic.var_fmpt(self.p).flatten().tolist()[0]
        exp = np.matrix([[5.58333333, 12., 6.88888889], [6.22222222,
                                                         12., 6.22222222], [6.88888889, 12., 5.58333333]])
        exp = exp.flatten().tolist()[0]
        for i in range(k):
            self.assertAlmostEqual(exp[i], obs[i])


suite = unittest.TestSuite()
test_classes = [SteadyState_Tester, Fmpt_Tester, VarFmpt_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_interaction
import unittest
import pysal
from pysal.spatial_dynamics import interaction
import numpy as np


class SpaceTimeEvents_Tester(unittest.TestCase):
    def setUp(self):
        self.path = pysal.examples.get_path("burkitt")

    def test_SpaceTimeEvents(self):
        events = interaction.SpaceTimeEvents(self.path, 'T')
        self.assertEquals(events.n, 188)
        self.assertEquals(list(events.space[0]), [300., 302.])
        self.assertEquals(list(events.t[0]), [413])


class Knox_Tester(unittest.TestCase):
    def setUp(self):
        path = pysal.examples.get_path("burkitt")
        self.events = interaction.SpaceTimeEvents(path, 'T')

    def test_knox(self):
        result = interaction.knox(
            self.events.space,
            self.events.t, delta=20, tau=5, permutations=1)
        self.assertEquals(result['stat'], 13.0)


class Mantel_Tester(unittest.TestCase):
    def setUp(self):
        path = pysal.examples.get_path("burkitt")
        self.events = interaction.SpaceTimeEvents(path, 'T')

    def test_mantel(self):
        result = interaction.mantel(self.events.space,
                self.events.time, 1, scon=0.0, spow=1.0, tcon=0.0, tpow=1.0)
        self.assertAlmostEquals(result['stat'], 0.014154, 6)


class Jacquez_Tester(unittest.TestCase):
    def setUp(self):
        path = pysal.examples.get_path("burkitt")
        self.events = interaction.SpaceTimeEvents(path, 'T')

    def test_jacquez(self):
        result = interaction.jacquez(self.events.space,
                self.events.t, k=3, permutations=1)
        self.assertEquals(result['stat'], 13)


class ModifiedKnox_Tester(unittest.TestCase):
    def setUp(self):
        path = pysal.examples.get_path("burkitt")
        self.events = interaction.SpaceTimeEvents(path, 'T')

    def test_modified_knox(self):
        result = interaction.modified_knox(
            self.events.space,
            self.events.t, delta=20, tau=5, permutations=1)
        self.assertAlmostEquals(result['stat'], 2.810160, 6)


suite = unittest.TestSuite()
test_classes = [SpaceTimeEvents_Tester, Knox_Tester, Mantel_Tester,
                Jacquez_Tester, ModifiedKnox_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_markov
import unittest
import pysal
from pysal.spatial_dynamics import markov
import numpy as np


class test_Markov(unittest.TestCase):
    def test___init__(self):
        # markov = Markov(class_ids, classes)
        import pysal
        f = pysal.open(pysal.examples.get_path('usjoin.csv'))
        pci = np.array([f.by_col[str(y)] for y in range(1929, 2010)])
        q5 = np.array([pysal.Quantiles(y).yb for y in pci]).transpose()
        m = pysal.Markov(q5)
        expected = np.array([[729., 71., 1., 0., 0.],
                             [72., 567., 80., 3., 0.],
                             [0., 81., 631., 86., 2.],
                             [0., 3., 86., 573., 56.],
                             [0., 0., 1., 57., 741.]])
        np.testing.assert_array_equal(m.transitions, expected)
        expected = np.matrix([[0.91011236, 0.0886392,
                               0.00124844, 0., 0.],
                              [0.09972299, 0.78531856, 0.11080332, 0.00415512,
                                  0.],
                              [0., 0.10125, 0.78875, 0.1075, 0.0025],
                              [0., 0.00417827, 0.11977716, 0.79805014,
                                  0.07799443],
                              [0., 0., 0.00125156, 0.07133917, 0.92740926]])
        np.testing.assert_array_almost_equal(m.p.getA(), expected.getA())
        expected = np.matrix([[0.20774716],
                              [0.18725774],
                              [0.20740537],
                              [0.18821787],
                              [0.20937187]]).getA()
        np.testing.assert_array_almost_equal(m.steady_state.getA(), expected)


class test_Spatial_Markov(unittest.TestCase):
    def test___init__(self):
        import pysal
        f = pysal.open(pysal.examples.get_path('usjoin.csv'))
        pci = np.array([f.by_col[str(y)] for y in range(1929, 2010)])
        pci = pci.transpose()
        rpci = pci / (pci.mean(axis=0))
        w = pysal.open(pysal.examples.get_path("states48.gal")).read()
        w.transform = 'r'
        sm = pysal.Spatial_Markov(rpci, w, fixed=True, k=5)
        S = np.array(
            [[0.43509425, 0.2635327, 0.20363044, 0.06841983, 0.02932278],
            [0.13391287, 0.33993305, 0.25153036, 0.23343016, 0.04119356],
            [0.12124869, 0.21137444, 0.2635101, 0.29013417, 0.1137326],
            [0.0776413, 0.19748806, 0.25352636, 0.22480415, 0.24654013],
            [0.01776781, 0.19964349, 0.19009833, 0.25524697, 0.3372434]])
        np.testing.assert_array_almost_equal(S, sm.S)


class test_chi2(unittest.TestCase):
    def test_chi2(self):
        import pysal
        f = pysal.open(pysal.examples.get_path('usjoin.csv'))
        pci = np.array([f.by_col[str(y)] for y in range(1929, 2010)])
        pci = pci.transpose()
        rpci = pci / (pci.mean(axis=0))
        w = pysal.open(pysal.examples.get_path("states48.gal")).read()
        w.transform = 'r'
        sm = pysal.Spatial_Markov(rpci, w, fixed=True, k=5)
        chi = np.matrix([[4.06139105e+01, 6.32961385e-04, 1.60000000e+01],
                         [5.55485793e+01, 2.88879565e-06, 1.60000000e+01],
                         [1.77772638e+01, 3.37100315e-01, 1.60000000e+01],
                         [4.00925436e+01, 7.54729084e-04, 1.60000000e+01],
                         [4.68588786e+01, 7.16364084e-05, 1.60000000e+01]]).getA()
        obs = np.matrix(sm.chi2).getA()
        np.testing.assert_array_almost_equal(obs, chi)
        obs = np.matrix(
            [[4.61209613e+02, 0.00000000e+00, 4.00000000e+00],
             [1.48140694e+02, 0.00000000e+00, 4.00000000e+00],
             [6.33129261e+01, 5.83089133e-13, 4.00000000e+00],
             [7.22778509e+01, 7.54951657e-15, 4.00000000e+00],
             [2.32659201e+02, 0.00000000e+00, 4.00000000e+00]])
        np.testing.assert_array_almost_equal(obs.getA(),
                                             np.matrix(sm.shtest).getA())


class test_LISA_Markov(unittest.TestCase):
    def test___init__(self):
        import numpy as np
        f = pysal.open(pysal.examples.get_path('usjoin.csv'))
        pci = np.array(
            [f.by_col[str(y)] for y in range(1929, 2010)]).transpose()
        w = pysal.open(pysal.examples.get_path("states48.gal")).read()
        lm = pysal.LISA_Markov(pci, w)
        obs = np.array([1, 2, 3, 4])
        np.testing.assert_array_almost_equal(obs, lm.classes)
        """
        >>> lm.steady_state
        matrix([[ 0.28561505],
                [ 0.14190226],
                [ 0.40493672],
                [ 0.16754598]])
        >>> lm.transitions
        array([[  1.08700000e+03,   4.40000000e+01,   4.00000000e+00,
                  3.40000000e+01],
               [  4.10000000e+01,   4.70000000e+02,   3.60000000e+01,
                  1.00000000e+00],
               [  5.00000000e+00,   3.40000000e+01,   1.42200000e+03,
                  3.90000000e+01],
               [  3.00000000e+01,   1.00000000e+00,   4.00000000e+01,
                  5.52000000e+02]])
        >>> lm.p
        matrix([[ 0.92985458,  0.03763901,  0.00342173,  0.02908469],
                [ 0.07481752,  0.85766423,  0.06569343,  0.00182482],
                [ 0.00333333,  0.02266667,  0.948     ,  0.026     ],
                [ 0.04815409,  0.00160514,  0.06420546,  0.88603531]])
        >>> lm.move_types
        array([[ 11.,  11.,  11., ...,  11.,  11.,  11.],
               [  6.,   6.,   6., ...,   6.,   7.,  11.],
               [ 11.,  11.,  11., ...,  11.,  11.,  11.],
               ...,
               [  6.,   6.,   6., ...,   6.,   6.,   6.],
               [  1.,   1.,   1., ...,   6.,   6.,   6.],
               [ 16.,  16.,  16., ...,  16.,  16.,  16.]])
        >>> np.random.seed(10)
        >>> lm_random = pysal.LISA_Markov(pci, w, permutations=99)
        >>> lm_random.significant_moves
        array([[11, 11, 11, ..., 59, 59, 59],
               [54, 54, 54, ..., 54, 55, 59],
               [11, 11, 11, ..., 11, 59, 59],
               ...,
               [54, 54, 54, ..., 54, 54, 54],
               [49, 49, 49, ..., 54, 54, 54],
               [64, 64, 64, ..., 64, 64, 64]])

        """


class test_kullback(unittest.TestCase):
    def test___init__(self):
        import numpy as np
        s1 = np.array([
                      [22, 11, 24, 2, 2, 7],
                      [5, 23, 15, 3, 42, 6],
                      [4, 21, 190, 25, 20, 34],
                      [0, 2, 14, 56, 14, 28],
                      [32, 15, 20, 10, 56, 14],
                      [5, 22, 31, 18, 13, 134]
                      ])
        s2 = np.array([
            [3, 6, 9, 3, 0, 8],
            [1, 9, 3, 12, 27, 5],
            [2, 9, 208, 32, 5, 18],
            [0, 14, 32, 108, 40, 40],
            [22, 14, 9, 26, 224, 14],
            [1, 5, 13, 53, 13, 116]
        ])

        F = np.array([s1, s2])
        res = markov.kullback(F)
        np.testing.assert_array_almost_equal(160.96060031170782,
                                             res['Conditional homogeneity'])
        np.testing.assert_array_almost_equal(30,
                                             res['Conditional homogeneity dof'])
        np.testing.assert_array_almost_equal(0.0,
                                             res['Conditional homogeneity pvalue'])


class test_prais(unittest.TestCase):
    def test___init__(self):
        import numpy as np
        f = pysal.open(pysal.examples.get_path('usjoin.csv'))
        pci = np.array([f.by_col[str(y)] for y in range(1929, 2010)])
        q5 = np.array([pysal.Quantiles(y).yb for y in pci]).transpose()
        m = pysal.Markov(q5)
        res = np.matrix([[0.08988764, 0.21468144,
                          0.21125, 0.20194986, 0.07259074]])
        np.testing.assert_array_almost_equal(markov.prais(m.p), res)


class test_shorrock(unittest.TestCase):
    def test___init__(self):
        import numpy as np
        f = pysal.open(pysal.examples.get_path('usjoin.csv'))
        pci = np.array([f.by_col[str(y)] for y in range(1929, 2010)])
        q5 = np.array([pysal.Quantiles(y).yb for y in pci]).transpose()
        m = pysal.Markov(q5)
        np.testing.assert_array_almost_equal(markov.shorrock(m.p),
                                             0.19758992000997844)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_rank
import unittest
import pysal
from pysal.spatial_dynamics import rank
import numpy as np


class Theta_Tester(unittest.TestCase):
    def setUp(self):
        f = pysal.open(pysal.examples.get_path('mexico.csv'))
        vnames = ["pcgdp%d" % dec for dec in range(1940, 2010, 10)]
        self.y = np.transpose(np.array([f.by_col[v] for v in vnames]))
        self.regime = np.array(f.by_col['esquivel99'])

    def test_Theta(self):
        np.random.seed(10)
        t = rank.Theta(self.y, self.regime, 999)
        k = self.y.shape[1]
        obs = t.theta.tolist()
        exp = [[0.41538462, 0.28070175, 0.61363636, 0.62222222,
                0.33333333, 0.47222222]]
        for i in range(k - 1):
            self.assertAlmostEqual(exp[0][i], obs[0][i])
        obs = t.pvalue_left.tolist()
        exp = [0.307, 0.077, 0.823, 0.552, 0.045, 0.735]
        for i in range(k - 1):
            self.assertAlmostEqual(exp[i], obs[i])
        obs = t.total.tolist()
        exp = [130., 114., 88., 90., 90., 72.]
        for i in range(k - 1):
            self.assertAlmostEqual(exp[i], obs[i])
        self.assertEqual(t.max_total, 512)


class SpatialTau_Tester(unittest.TestCase):
    def setUp(self):
        f = pysal.open(pysal.examples.get_path('mexico.csv'))
        vnames = ["pcgdp%d" % dec for dec in range(1940, 2010, 10)]
        self.y = np.transpose(np.array([f.by_col[v] for v in vnames]))
        regime = np.array(f.by_col['esquivel99'])
        self.w = pysal.weights.regime_weights(regime)

    def test_SpatialTau(self):
        np.random.seed(12345)
        k = self.y.shape[1]
        obs = [rank.SpatialTau(self.y[:, i], self.y[:, i + 1],
                               self.w, 99) for i in range(k - 1)]
        tau_s = [0.281, 0.348, 0.460, 0.505, 0.483, 0.572]
        ev_tau_s = [0.466, 0.499, 0.546, 0.532, 0.499, 0.579]
        p_vals = [0.010, 0.010, 0.020, 0.210, 0.270, 0.280]
        for i in range(k - 1):
            self.assertAlmostEqual(tau_s[i], obs[i].tau_spatial, 3)
            self.assertAlmostEqual(ev_tau_s[i], obs[i].taus.mean(), 3)
            self.assertAlmostEqual(p_vals[i], obs[i].tau_spatial_psim, 3)


class Tau_Tester(unittest.TestCase):
    def test_Tau(self):
        x1 = [12, 2, 1, 12, 2]
        x2 = [1, 4, 7, 1, 0]
        kt = rank.Tau(x1, x2)
        self.assertAlmostEqual(kt.tau, -0.47140452079103173, 5)
        self.assertAlmostEqual(kt.tau_p, 0.24821309157521476, 5)


suite = unittest.TestSuite()
test_classes = [Theta_Tester, SpatialTau_Tester, Tau_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_util
import unittest
import pysal
from pysal.spatial_dynamics import util
import numpy as np


class ShuffleMatrix_Tester(unittest.TestCase):
    def setUp(self):
        self.X = np.arange(16)
        self.X.shape = (4, 4)

    def test_shuffle_matrix(self):
        np.random.seed(10)
        obs = util.shuffle_matrix(self.X, range(4)).flatten().tolist()
        exp = [10, 8, 11, 9, 2, 0, 3, 1, 14, 12, 15, 13, 6, 4, 7, 5]
        for i in range(16):
            self.assertEqual(exp[i], obs[i])


class GetLower_Tester(unittest.TestCase):
    def setUp(self):
        self.X = np.arange(16)
        self.X.shape = (4, 4)

    def test_get_lower(self):
        np.random.seed(10)
        obs = util.get_lower(self.X).flatten().tolist()
        exp = [4, 8, 9, 12, 13, 14]
        for i in range(6):
            self.assertEqual(exp[i], obs[i])


suite = unittest.TestSuite()
test_classes = [ShuffleMatrix_Tester, GetLower_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = util
"""
Utilities for the spatial dynamics module.
"""
import numpy as np

__all__ = ['shuffle_matrix', 'get_lower']


def shuffle_matrix(X, ids):
    """
    Random permutation of rows and columns of a matrix

    Parameters
    ----------
    X   : array (k,k)
          array to be permutated
    ids : range (k,)

    Returns
    -------
    X   : array (k,k)
          with rows and columns randomly shuffled

    Examples
    --------
    >>> X=np.arange(16)
    >>> X.shape=(4,4)
    >>> np.random.seed(10)
    >>> shuffle_matrix(X,range(4))
    array([[10,  8, 11,  9],
           [ 2,  0,  3,  1],
           [14, 12, 15, 13],
           [ 6,  4,  7,  5]])

    """
    np.random.shuffle(ids)
    return X[ids, :][:, ids]


def get_lower(matrix):
    """
    Flattens the lower part of an n x n matrix into an n*(n-1)/2 x 1 vector.

    Parameters
    ----------
    matrix          : numpy array
                      a distance matrix (n x n)

    Returns
    -------
    lowvec          : numpy array
                      the lower half of the distance matrix flattened into
                      a vector of length n*(n-1)/2

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> test = np.array([[0,1,2,3],[1,0,1,2],[2,1,0,1],[4,2,1,0]])
    >>> lower = get_lower(test)
    >>> lower
    array([[1],
           [2],
           [1],
           [4],
           [2],
           [1]])

    """
    n = matrix.shape[0]
    lowerlist = []
    for i in range(n):
        for j in range(n):
            if i > j:
                lowerlist.append(matrix[i, j])
    veclen = n * (n - 1) / 2
    lowvec = np.reshape(lowerlist, (veclen, 1))
    return lowvec


########NEW FILE########
__FILENAME__ = diagnostics
"""
Diagnostics for regression estimations. 
        
"""
__author__ = "Luc Anselin luc.anselin@asu.edu, Nicholas Malizia nicholas.malizia@asu.edu "

import pysal
from pysal.common import *
import scipy.sparse as SP
from math import sqrt
from utils import spmultiply, sphstack, spmin, spmax


__all__ = [
    "f_stat", "t_stat", "r2", "ar2", "se_betas", "log_likelihood", "akaike", "schwarz",
    "condition_index", "jarque_bera", "breusch_pagan", "white", "koenker_bassett", "vif", "likratiotest"]


def f_stat(reg):
    """
    Calculates the f-statistic and associated p-value of the regression.
    (For two stage least squares see f_stat_tsls)

    Parameters
    ----------
    reg             : regression object
                      output instance from a regression model

    Returns
    ----------
    fs_result       : tuple
                      includes value of F statistic and associated p-value

    References
    ----------
    .. [1] W. Greene. 2003. Econometric Analysis. Prentice Hall, Upper
       Saddle River. 

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> import diagnostics
    >>> from ols import OLS

    Read the DBF associated with the Columbus data. 

    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),"r")

    Create the dependent variable vector. 

    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))

    Create the matrix of independent variables. 

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T

    Run an OLS regression.    

    >>> reg = OLS(y,X)

    Calculate the F-statistic for the regression. 

    >>> testresult = diagnostics.f_stat(reg)

    Print the results tuple, including the statistic and its significance.

    >>> print("%12.12f"%testresult[0],"%12.12f"%testresult[1])
    ('28.385629224695', '0.000000009341')

    """
    k = reg.k            # (scalar) number of ind. vars (includes constant)
    n = reg.n            # (scalar) number of observations
    utu = reg.utu        # (scalar) residual sum of squares
    predy = reg.predy    # (array) vector of predicted values (n x 1)
    mean_y = reg.mean_y  # (scalar) mean of dependent observations
    Q = utu
    U = np.sum((predy - mean_y) ** 2)
    fStat = (U / (k - 1)) / (Q / (n - k))
    pValue = stats.f.sf(fStat, k - 1, n - k)
    fs_result = (fStat, pValue)
    return fs_result


def t_stat(reg, z_stat=False):
    """
    Calculates the t-statistics (or z-statistics) and associated p-values.

    Parameters
    ----------
    reg             : regression object
                      output instance from a regression model
    z_stat          : boolean
                      If True run z-stat instead of t-stat

    Returns
    -------
    ts_result       : list of tuples
                      each tuple includes value of t statistic (or z
                      statistic) and associated p-value

    References
    ----------

    .. [1] W. Greene. 2003. Econometric Analysis. Prentice Hall, Upper
       Saddle River. 

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> import diagnostics
    >>> from ols import OLS

    Read the DBF associated with the Columbus data. 

    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),"r")

    Create the dependent variable vector. 

    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))

    Create the matrix of independent variables. 

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T

    Run an OLS regression.    

    >>> reg = OLS(y,X)

    Calculate t-statistics for the regression coefficients. 

    >>> testresult = diagnostics.t_stat(reg)

    Print the tuples that contain the t-statistics and their significances.

    >>> print("%12.12f"%testresult[0][0], "%12.12f"%testresult[0][1], "%12.12f"%testresult[1][0], "%12.12f"%testresult[1][1], "%12.12f"%testresult[2][0], "%12.12f"%testresult[2][1])
    ('14.490373143689', '0.000000000000', '-4.780496191297', '0.000018289595', '-2.654408642718', '0.010874504910')
    """

    k = reg.k           # (scalar) number of ind. vars (includes constant)
    n = reg.n           # (scalar) number of observations
    vm = reg.vm         # (array) coefficients of variance matrix (k x k)
    betas = reg.betas   # (array) coefficients of the regressors (1 x k)
    variance = vm.diagonal()
    tStat = betas[range(0, len(vm))].reshape(len(vm),) / np.sqrt(variance)
    ts_result = []
    for t in tStat:
        if z_stat:
            ts_result.append((t, stats.norm.sf(abs(t)) * 2))
        else:
            ts_result.append((t, stats.t.sf(abs(t), n - k) * 2))
    return ts_result


def r2(reg):
    """
    Calculates the R^2 value for the regression. 

    Parameters
    ----------
    reg             : regression object
                      output instance from a regression model

    Returns
    ----------
    r2_result       : float
                      value of the coefficient of determination for the
                      regression 

    References
    ----------
    .. [1] W. Greene. 2003. Econometric Analysis. Prentice Hall, Upper
       Saddle River. 

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> import diagnostics
    >>> from ols import OLS

    Read the DBF associated with the Columbus data.

    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),"r")

    Create the dependent variable vector.

    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))

    Create the matrix of independent variables. 

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T

    Run an OLS regression. 

    >>> reg = OLS(y,X)

    Calculate the R^2 value for the regression. 

    >>> testresult = diagnostics.r2(reg)

    Print the result. 

    >>> print("%1.8f"%testresult)
    0.55240404

    """
    y = reg.y               # (array) vector of dep observations (n x 1)
    mean_y = reg.mean_y     # (scalar) mean of dep observations
    utu = reg.utu           # (scalar) residual sum of squares
    ss_tot = ((y - mean_y) ** 2).sum(0)
    r2 = 1 - utu / ss_tot
    r2_result = r2[0]
    return r2_result


def ar2(reg):
    """
    Calculates the adjusted R^2 value for the regression. 

    Parameters
    ----------
    reg             : regression object
                      output instance from a regression model   

    Returns
    ----------
    ar2_result      : float
                      value of R^2 adjusted for the number of explanatory
                      variables.

    References
    ----------
    .. [1] W. Greene. 2003. Econometric Analysis. Prentice Hall, Upper
       Saddle River. 

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> import diagnostics
    >>> from ols import OLS

    Read the DBF associated with the Columbus data.

    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),"r")

    Create the dependent variable vector. 

    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))

    Create the matrix of independent variables. 

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T

    Run an OLS regression. 

    >>> reg = OLS(y,X)

    Calculate the adjusted R^2 value for the regression. 
    >>> testresult = diagnostics.ar2(reg)

    Print the result. 

    >>> print("%1.8f"%testresult)
    0.53294335

    """
    k = reg.k       # (scalar) number of ind. variables (includes constant)
    n = reg.n       # (scalar) number of observations
    ar2_result = 1 - (1 - r2(reg)) * (n - 1) / (n - k)
    return ar2_result


def se_betas(reg):
    """
    Calculates the standard error of the regression coefficients.

    Parameters
    ----------
    reg             : regression object
                      output instance from a regression model

    Returns
    ----------
    se_result       : array
                      includes standard errors of each coefficient (1 x k)

    References
    ----------
    .. [1] W. Greene. 2003. Econometric Analysis. Prentice Hall, Upper
       Saddle River. 

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> import diagnostics
    >>> from ols import OLS

    Read the DBF associated with the Columbus data. 

    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),"r")

    Create the dependent variable vector.

    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))

    Create the matrix of independent variables. 

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T

    Run an OLS regression. 

    >>> reg = OLS(y,X)

    Calculate the standard errors of the regression coefficients. 

    >>> testresult = diagnostics.se_betas(reg)

    Print the vector of standard errors. 

    >>> testresult
    array([ 4.73548613,  0.33413076,  0.10319868])

    """
    vm = reg.vm         # (array) coefficients of variance matrix (k x k)
    variance = vm.diagonal()
    se_result = np.sqrt(variance)
    return se_result


def log_likelihood(reg):
    """
    Calculates the log-likelihood value for the regression. 

    Parameters
    ----------
    reg             : regression object
                      output instance from a regression model

    Returns
    -------
    ll_result       : float
                      value for the log-likelihood of the regression.

    References
    ----------
    .. [1] W. Greene. 2003. Econometric Analysis. Prentice Hall, Upper
       Saddle River. 

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> import diagnostics
    >>> from ols import OLS

    Read the DBF associated with the Columbus data. 
    
    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),"r")

    Create the dependent variable vector. 

    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))

    Create the matrix of independent variables. 

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T

    Run an OLS regression.

    >>> reg = OLS(y,X)

    Calculate the log-likelihood for the regression. 

    >>> testresult = diagnostics.log_likelihood(reg)

    Print the result. 

    >>> testresult
    -187.3772388121491

    """
    n = reg.n       # (scalar) number of observations
    utu = reg.utu   # (scalar) residual sum of squares
    ll_result = -0.5 * \
        (n * (np.log(2 * math.pi)) + n * np.log(utu / n) + (utu / (utu / n)))
    return ll_result


def akaike(reg):
    """
    Calculates the Akaike Information Criterion.

    Parameters
    ----------
    reg             : regression object
                      output instance from a regression model

    Returns
    -------
    aic_result      : scalar
                      value for Akaike Information Criterion of the
                      regression. 

    References
    ----------
    .. [1] H. Akaike. 1974. A new look at the statistical identification
       model. IEEE Transactions on Automatic Control, 19(6):716-723.

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> import diagnostics
    >>> from ols import OLS

    Read the DBF associated with the Columbus data.

    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),"r")

    Create the dependent variable vector. 

    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))

    Create the matrix of independent variables. 

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T

    Run an OLS regression.

    >>> reg = OLS(y,X)

    Calculate the Akaike Information Criterion (AIC). 

    >>> testresult = diagnostics.akaike(reg)

    Print the result. 

    >>> testresult
    380.7544776242982

    """
    k = reg.k       # (scalar) number of explanatory vars (including constant)
    try:   # ML estimation, logll already exists
        # spatial coefficient included in k
        aic_result = 2.0 * k - 2.0 * reg.logll
    except AttributeError:           # OLS case
        n = reg.n       # (scalar) number of observations
        utu = reg.utu   # (scalar) residual sum of squares
        aic_result = 2 * k + n * (np.log((2 * np.pi * utu) / n) + 1)
    return aic_result


def schwarz(reg):
    """
    Calculates the Schwarz Information Criterion.

    Parameters
    ----------
    reg             : regression object
                      output instance from a regression model

    Returns
    -------
    bic_result      : scalar
                      value for Schwarz (Bayesian) Information Criterion of
                      the regression. 

    References
    ----------
    .. [1] G. Schwarz. 1978. Estimating the dimension of a model. The
       Annals of Statistics, pages 461-464. 

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> import diagnostics
    >>> from ols import OLS

    Read the DBF associated with the Columbus data.

    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),"r")

    Create the dependent variable vector. 

    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))

    Create the matrix of independent variables. 

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T

    Run an OLS regression.

    >>> reg = OLS(y,X)

    Calculate the Schwarz Information Criterion. 

    >>> testresult = diagnostics.schwarz(reg)

    Print the results. 

    >>> testresult
    386.42993851863008

    """
    n = reg.n      # (scalar) number of observations
    k = reg.k      # (scalar) number of ind. variables (including constant)
    try:  # ML case logll already computed
        # spatial coeff included in k
        sc_result = k * np.log(n) - 2.0 * reg.logll
    except AttributeError:          # OLS case
        utu = reg.utu  # (scalar) residual sum of squares
        sc_result = k * np.log(n) + n * (np.log((2 * np.pi * utu) / n) + 1)
    return sc_result


def condition_index(reg):
    """
    Calculates the multicollinearity condition index according to Belsey,
    Kuh and Welsh (1980).

    Parameters
    ----------
    reg             : regression object
                      output instance from a regression model

    Returns
    -------
    ci_result       : float
                      scalar value for the multicollinearity condition
                      index. 

    References
    ----------
    .. [1] D. Belsley, E. Kuh, and R. Welsch. 1980. Regression
       Diagnostics. New York: Wiley.

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> import diagnostics
    >>> from ols import OLS

    Read the DBF associated with the Columbus data.

    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),"r")

    Create the dependent variable vector. 

    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))

    Create the matrix of independent variables. 

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T

    Run an OLS regression.

    >>> reg = OLS(y,X)

    Calculate the condition index to check for multicollinearity.

    >>> testresult = diagnostics.condition_index(reg)

    Print the result.

    >>> print("%1.3f"%testresult)
    6.542

    """
    if hasattr(reg, 'xtx'):
        xtx = reg.xtx   # (array) k x k projection matrix (includes constant)
    elif hasattr(reg, 'hth'):
        xtx = reg.hth   # (array) k x k projection matrix (includes constant)
    diag = np.diagonal(xtx)
    scale = xtx / diag
    eigval = np.linalg.eigvals(scale)
    max_eigval = max(eigval)
    min_eigval = min(eigval)
    ci_result = sqrt(max_eigval / min_eigval)
    return ci_result


def jarque_bera(reg):
    """
    Jarque-Bera test for normality in the residuals. 

    Parameters
    ----------
    reg             : regression object
                      output instance from a regression model

    Returns
    ------- 
    jb_result       : dictionary
                      contains the statistic (jb) for the Jarque-Bera test
                      and the associated p-value (p-value)
    df              : integer
                      degrees of freedom for the test (always 2)
    jb              : float
                      value of the test statistic
    pvalue          : float
                      p-value associated with the statistic (chi^2
                      distributed with 2 df)

    References
    ----------
    .. [1] C. Jarque and A. Bera. 1980. Efficient tests for normality,
       homoscedasticity and serial independence of regression residuals.
       Economics Letters, 6(3):255-259.

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> import diagnostics
    >>> from ols import OLS

    Read the DBF associated with the Columbus data.

    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"), "r")

    Create the dependent variable vector. 

    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))

    Create the matrix of independent variables. 

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T

    Run an OLS regression.

    >>> reg = OLS(y,X)

    Calculate the Jarque-Bera test for normality of residuals.

    >>> testresult = diagnostics.jarque_bera(reg)

    Print the degrees of freedom for the test.

    >>> testresult['df']
    2

    Print the test statistic. 

    >>> print("%1.3f"%testresult['jb'])
    1.836

    Print the associated p-value.

    >>> print("%1.4f"%testresult['pvalue'])
    0.3994

    """
    n = reg.n               # (scalar) number of observations
    u = reg.u               # (array) residuals from regression
    u2 = u ** 2
    u3 = u ** 3
    u4 = u ** 4
    mu2 = np.mean(u2)
    mu3 = np.mean(u3)
    mu4 = np.mean(u4)
    S = mu3 / (mu2 ** (1.5))    # skewness measure
    K = (mu4 / (mu2 ** 2))      # kurtosis measure
    jb = n * (((S ** 2) / 6) + ((K - 3) ** 2) / 24)
    pvalue = stats.chisqprob(jb, 2)
    jb_result = {"df": 2, "jb": jb, 'pvalue': pvalue}
    return jb_result


def breusch_pagan(reg, z=None):
    """
    Calculates the Breusch-Pagan test statistic to check for
    heteroscedasticity. 

    Parameters
    ----------
    reg             : regression object
                      output instance from a regression model
    z               : array
                      optional input for specifying an alternative set of
                      variables (Z) to explain the observed variance. By
                      default this is a matrix of the squared explanatory
                      variables (X**2) with a constant added to the first
                      column if not already present. In the default case,
                      the explanatory variables are squared to eliminate
                      negative values. 

    Returns
    -------
    bp_result       : dictionary
                      contains the statistic (bp) for the test and the
                      associated p-value (p-value)
    bp              : float
                      scalar value for the Breusch-Pagan test statistic
    df              : integer
                      degrees of freedom associated with the test (k)
    pvalue          : float
                      p-value associated with the statistic (chi^2
                      distributed with k df)

    Notes
    -----
    x attribute in the reg object must have a constant term included. This is
    standard for spreg.OLS so no testing done to confirm constant.

    References
    ----------

    .. [1] T. Breusch and A. Pagan. 1979. A simple test for
       heteroscedasticity and random coefficient variation. Econometrica:
       Journal of the Econometric Society, 47(5):1287-1294.

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> import diagnostics
    >>> from ols import OLS

    Read the DBF associated with the Columbus data.

    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"), "r")
 
    Create the dependent variable vector. 

    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))

    Create the matrix of independent variables. 

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T

    Run an OLS regression.

    >>> reg = OLS(y,X)

    Calculate the Breusch-Pagan test for heteroscedasticity.

    >>> testresult = diagnostics.breusch_pagan(reg)

    Print the degrees of freedom for the test.

    >>> testresult['df']
    2

    Print the test statistic.

    >>> print("%1.3f"%testresult['bp'])
    7.900

    Print the associated p-value. 

    >>> print("%1.4f"%testresult['pvalue'])
    0.0193

    """
    e2 = reg.u ** 2
    e = reg.u
    n = reg.n
    k = reg.k
    ete = reg.utu

    den = ete / n
    g = e2 / den - 1.0

    if z == None:
        x = reg.x
        #constant = constant_check(x)
        # if constant == False:
        #    z = np.hstack((np.ones((n,1)),x))**2
        # else:
        #    z = x**2
        z = spmultiply(x, x)
    else:
        #constant = constant_check(z)
        # if constant == False:
        #    z = np.hstack((np.ones((n,1)),z))
        pass

    n, p = z.shape

    # Check to identify any duplicate columns in Z
    omitcolumn = []
    for i in range(p):
        current = z[:, i]
        for j in range(p):
            check = z[:, j]
            if i < j:
                test = abs(current - check).sum()
                if test == 0:
                    omitcolumn.append(j)

    uniqueomit = set(omitcolumn)
    omitcolumn = list(uniqueomit)

    # Now the identified columns must be removed (done in reverse to
    # prevent renumbering)
    omitcolumn.sort()
    omitcolumn.reverse()
    for c in omitcolumn:
        z = np.delete(z, c, 1)
    n, p = z.shape

    df = p - 1

    # Now that the variables are prepared, we calculate the statistic
    zt = np.transpose(z)
    gt = np.transpose(g)
    gtz = np.dot(gt, z)
    ztg = np.dot(zt, g)
    ztz = np.dot(zt, z)
    ztzi = la.inv(ztz)

    part1 = np.dot(gtz, ztzi)
    part2 = np.dot(part1, ztg)
    bp_array = 0.5 * part2
    bp = bp_array[0, 0]

    pvalue = stats.chisqprob(bp, df)
    bp_result = {'df': df, 'bp': bp, 'pvalue': pvalue}
    return bp_result


def white(reg):
    """
    Calculates the White test to check for heteroscedasticity.

    Parameters
    ----------
    reg             : regression object
                      output instance from a regression model

    Returns
    -------
    white_result    : dictionary
                      contains the statistic (white), degrees of freedom
                      (df) and the associated p-value (pvalue) for the
                      White test. 
    white           : float
                      scalar value for the White test statistic.
    df              : integer
                      degrees of freedom associated with the test
    pvalue          : float
                      p-value associated with the statistic (chi^2
                      distributed with k df)

    Notes
    -----
    x attribute in the reg object must have a constant term included. This is
    standard for spreg.OLS so no testing done to confirm constant.

    References
    ----------
    .. [1] H. White. 1980. A heteroscedasticity-consistent covariance
       matrix estimator and a direct test for heteroskdasticity.
       Econometrica. 48(4) 817-838. 

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> import diagnostics
    >>> from ols import OLS

    Read the DBF associated with the Columbus data.

    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),"r")

    Create the dependent variable vector. 

    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))

    Create the matrix of independent variables. 

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T

    Run an OLS regression.

    >>> reg = OLS(y,X)

    Calculate the White test for heteroscedasticity.

    >>> testresult = diagnostics.white(reg)

    Print the degrees of freedom for the test.

    >>> print testresult['df']
    5

    Print the test statistic.

    >>> print("%1.3f"%testresult['wh'])
    19.946

    Print the associated p-value. 

    >>> print("%1.4f"%testresult['pvalue'])
    0.0013

    """
    e = reg.u ** 2
    k = reg.k
    n = reg.n
    y = reg.y
    X = reg.x
    #constant = constant_check(X)

    # Check for constant, if none add one, see Greene 2003, pg. 222
    # if constant == False:
    #    X = np.hstack((np.ones((n,1)),X))

    # Check for multicollinearity in the X matrix
    ci = condition_index(reg)
    if ci > 30:
        white_result = "Not computed due to multicollinearity."
        return white_result

    # Compute cross-products and squares of the regression variables
    if type(X).__name__ == 'ndarray':
        A = np.zeros((n, (k * (k + 1)) / 2.))
    elif type(X).__name__ == 'csc_matrix' or type(X).__name__ == 'csr_matrix':
        # this is probably inefficient
        A = SP.lil_matrix((n, (k * (k + 1)) / 2.))
    else:
        raise Exception, "unknown X type, %s" % type(X).__name__
    counter = 0
    for i in range(k):
        for j in range(i, k):
            v = spmultiply(X[:, i], X[:, j], False)
            A[:, counter] = v
            counter += 1

    # Append the original variables
    A = sphstack(X, A)   # note: this also converts a LIL to CSR
    n, k = A.shape

    # Check to identify any duplicate or constant columns in A
    omitcolumn = []
    for i in range(k):
        current = A[:, i]
        # remove all constant terms (will add a constant back later)
        if spmax(current) == spmin(current):
            omitcolumn.append(i)
            pass
        # do not allow duplicates
        for j in range(k):
            check = A[:, j]
            if i < j:
                test = abs(current - check).sum()
                if test == 0:
                    omitcolumn.append(j)
    uniqueomit = set(omitcolumn)
    omitcolumn = list(uniqueomit)

    # Now the identified columns must be removed
    if type(A).__name__ == 'ndarray':
        A = np.delete(A, omitcolumn, 1)
    elif type(A).__name__ == 'csc_matrix' or type(A).__name__ == 'csr_matrix':
        # this is probably inefficient
        keepcolumn = range(k)
        for i in omitcolumn:
            keepcolumn.remove(i)
        A = A[:, keepcolumn]
    else:
        raise Exception, "unknown A type, %s" % type(X).__name__
    A = sphstack(np.ones((A.shape[0], 1)), A)   # add a constant back in
    n, k = A.shape

    # Conduct the auxiliary regression and calculate the statistic
    import ols as OLS
    aux_reg = OLS.BaseOLS(e, A)
    aux_r2 = r2(aux_reg)
    wh = aux_r2 * n
    df = k - 1
    pvalue = stats.chisqprob(wh, df)
    white_result = {'df': df, 'wh': wh, 'pvalue': pvalue}
    return white_result


def koenker_bassett(reg, z=None):
    """
    Calculates the Koenker-Bassett test statistic to check for
    heteroscedasticity. 

    Parameters
    ----------
    reg             : regression output
                      output from an instance of a regression class
    z               : array
                      optional input for specifying an alternative set of
                      variables (Z) to explain the observed variance. By
                      default this is a matrix of the squared explanatory
                      variables (X**2) with a constant added to the first
                      column if not already present. In the default case,
                      the explanatory variables are squared to eliminate
                      negative values. 

    Returns
    -------
    kb_result       : dictionary
                      contains the statistic (kb), degrees of freedom (df)
                      and the associated p-value (pvalue) for the test. 
    kb              : float
                      scalar value for the Koenker-Bassett test statistic.
    df              : integer
                      degrees of freedom associated with the test
    pvalue          : float
                      p-value associated with the statistic (chi^2
                      distributed)

    Notes
    -----
    x attribute in the reg object must have a constant term included. This is
    standard for spreg.OLS so no testing done to confirm constant.

    References
    ----------
    .. [1] R. Koenker and G. Bassett. 1982. Robust tests for
       heteroscedasticity based on regression quantiles. Econometrica,
       50(1):43-61. 
    .. [2] W. Greene. 2003. Econometric Analysis. Prentice Hall, Upper
       Saddle River. 

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> import diagnostics
    >>> from ols import OLS

    Read the DBF associated with the Columbus data.

    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),"r")

    Create the dependent variable vector. 

    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))

    Create the matrix of independent variables. 

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T

    Run an OLS regression.

    >>> reg = OLS(y,X)

    Calculate the Koenker-Bassett test for heteroscedasticity.

    >>> testresult = diagnostics.koenker_bassett(reg)

    Print the degrees of freedom for the test.

    >>> testresult['df']
    2

    Print the test statistic.

    >>> print("%1.3f"%testresult['kb'])
    5.694

    Print the associated p-value. 

    >>> print("%1.4f"%testresult['pvalue'])
    0.0580

    """
    # The notation here matches that of Greene (2003).
    u = reg.u ** 2
    e = reg.u
    n = reg.n
    k = reg.k
    x = reg.x
    ete = reg.utu
    #constant = constant_check(x)

    ubar = ete / n
    ubari = ubar * np.ones((n, 1))
    g = u - ubari
    v = (1.0 / n) * np.sum((u - ubar) ** 2)

    if z == None:
        x = reg.x
        #constant = constant_check(x)
        # if constant == False:
        #    z = np.hstack((np.ones((n,1)),x))**2
        # else:
        #    z = x**2
        z = spmultiply(x, x)
    else:
        #constant = constant_check(z)
        # if constant == False:
        #    z = np.hstack((np.ones((n,1)),z))
        pass

    n, p = z.shape

    # Check to identify any duplicate columns in Z
    omitcolumn = []
    for i in range(p):
        current = z[:, i]
        for j in range(p):
            check = z[:, j]
            if i < j:
                test = abs(current - check).sum()
                if test == 0:
                    omitcolumn.append(j)

    uniqueomit = set(omitcolumn)
    omitcolumn = list(uniqueomit)

    # Now the identified columns must be removed (done in reverse to
    # prevent renumbering)
    omitcolumn.sort()
    omitcolumn.reverse()
    for c in omitcolumn:
        z = np.delete(z, c, 1)
    n, p = z.shape

    df = p - 1

    # Conduct the auxiliary regression.
    zt = np.transpose(z)
    gt = np.transpose(g)
    gtz = np.dot(gt, z)
    ztg = np.dot(zt, g)
    ztz = np.dot(zt, z)
    ztzi = la.inv(ztz)

    part1 = np.dot(gtz, ztzi)
    part2 = np.dot(part1, ztg)
    kb_array = (1.0 / v) * part2
    kb = kb_array[0, 0]

    pvalue = stats.chisqprob(kb, df)
    kb_result = {'kb': kb, 'df': df, 'pvalue': pvalue}
    return kb_result


def vif(reg):
    """
    Calculates the variance inflation factor for each independent variable.
    For the ease of indexing the results, the constant is currently
    included. This should be omitted when reporting the results to the
    output text.

    Parameters
    ----------
    reg             : regression object
                      output instance from a regression model

    Returns
    -------    
    vif_result      : list of tuples
                      each tuple includes the vif and the tolerance, the
                      order of the variables corresponds to their order in
                      the reg.x matrix

    References
    ----------
    .. [1] W. Greene. 2003. Econometric Analysis. Prentice Hall, Upper
       Saddle River. 

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> import diagnostics
    >>> from ols import OLS

    Read the DBF associated with the Columbus data.

    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),"r")

    Create the dependent variable vector. 

    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))

    Create the matrix of independent variables. 

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T

    Run an OLS regression.

    >>> reg = OLS(y,X)

    Calculate the variance inflation factor (VIF). 
    >>> testresult = diagnostics.vif(reg)

    Select the tuple for the income variable. 

    >>> incvif = testresult[1]

    Print the VIF for income. 

    >>> print("%12.12f"%incvif[0])
    1.333117497189

    Print the tolerance for income. 

    >>> print("%12.12f"%incvif[1])
    0.750121427487

    Repeat for the home value variable. 

    >>> hovalvif = testresult[2]
    >>> print("%12.12f"%hovalvif[0])
    1.333117497189
    >>> print("%12.12f"%hovalvif[1])
    0.750121427487

    """
    X = reg.x
    n, k = X.shape
    vif_result = []

    for j in range(k):
        Z = X.copy()
        Z = np.delete(Z, j, 1)
        y = X[:, j]
        import ols as OLS
        aux = OLS.BaseOLS(y, Z)
        mean_y = aux.mean_y
        utu = aux.utu
        ss_tot = sum((y - mean_y) ** 2)
        if ss_tot == 0:
            resj = pysal.MISSINGVALUE
        else:
            r2aux = 1 - utu / ss_tot
            tolj = 1 - r2aux
            vifj = 1 / tolj
            resj = (vifj, tolj)
        vif_result.append(resj)
    return vif_result


def constant_check(array):
    """
    Checks to see numpy array includes a constant.

    Parameters
    ----------
    array           : array
                      an array of variables to be inspected 

    Returns
    -------
    constant        : boolean
                      true signifies the presence of a constant

    Example
    -------

    >>> import numpy as np
    >>> import pysal
    >>> import diagnostics
    >>> from ols import OLS
    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))
    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T
    >>> reg = OLS(y,X)
    >>> diagnostics.constant_check(reg.x)
    True

    """

    n, k = array.shape
    constant = False
    for j in range(k):
        variable = array[:, j]
        varmin = variable.min()
        varmax = variable.max()
        if varmin == varmax:
            constant = True
            break
    return constant


def likratiotest(reg0, reg1):
    """
    Likelihood ratio test statistic

    Parameters
    ----------
    
    reg0         : regression object for constrained model (H0)
    reg1         : regression object for unconstrained model (H1)
    
    Returns
    -------
    
    likratio     : dictionary
                   contains the statistic (likr), the degrees of
                   freedom (df) and the p-value (pvalue)
    likr         : float
                   likelihood ratio statistic
    df           : integer
                   degrees of freedom
    p-value      : float
                   p-value
                   
    References
    ----------
    .. [1] W. Greene. 2012. Econometric Analysis. Prentice Hall, Upper
       Saddle River.
       
    Examples
    --------

    >>> import numpy as np
    >>> import pysal as ps
    >>> import scipy.stats as stats
    >>> import pysal.spreg.ml_lag as lag
    
    Use the baltim sample data set
    
    >>> db =  ps.open(ps.examples.get_path("baltim.dbf"),'r')
    >>> y_name = "PRICE"
    >>> y = np.array(db.by_col(y_name)).T
    >>> y.shape = (len(y),1)
    >>> x_names = ["NROOM","NBATH","PATIO","FIREPL","AC","GAR","AGE","LOTSZ","SQFT"]
    >>> x = np.array([db.by_col(var) for var in x_names]).T
    >>> ww = ps.open(ps.examples.get_path("baltim_q.gal"))
    >>> w = ww.read()
    >>> ww.close()
    >>> w.transform = 'r'
    
    OLS regression
    
    >>> ols1 = ps.spreg.OLS(y,x)
    
    ML Lag regression
    
    >>> mllag1 = lag.ML_Lag(y,x,w)
    
    >>> lr = likratiotest(ols1,mllag1)
    
    >>> print "Likelihood Ratio Test: {0:.4f}       df: {1}        p-value: {2:.4f}".format(lr["likr"],lr["df"],lr["p-value"])
    Likelihood Ratio Test: 44.5721       df: 1        p-value: 0.0000
  
    """

    likratio = {}

    try:
        likr = 2.0 * (reg1.logll - reg0.logll)
    except AttributeError:
        raise Exception, "Missing or improper log-likelihoods in regression objects"
    if likr < 0.0:  # always enforces positive likelihood ratio
        likr = -likr
    pvalue = stats.chisqprob(likr, 1)
    likratio = {"likr": likr, "df": 1, "p-value": pvalue}
    return likratio


def _test():
    import doctest
    doctest.testmod()

if __name__ == '__main__':
    _test()

########NEW FILE########
__FILENAME__ = diagnostics_sp
"""
Spatial diagnostics module
"""
__author__ = "Luc Anselin luc.anselin@asu.edu, Daniel Arribas-Bel darribas@asu.edu"

from utils import spdot
from scipy.stats.stats import chisqprob
from scipy.stats import norm
import numpy as np
import numpy.linalg as la

__all__ = ['LMtests', 'MoranRes', 'AKtest']


class LMtests:

    """
    Lagrange Multiplier tests. Implemented as presented in Anselin et al.
    (1996) [1]_
    ...

    Attributes
    ----------

    ols         : OLS
                  OLS regression object
    w           : W
                  Spatial weights instance
    tests       : list
                  Lists of strings with the tests desired to be performed.
                  Values may be:

                   * 'all': runs all the options (default)
                   * 'lme': LM error test
                   * 'rlme': Robust LM error test
                   * 'lml' : LM lag test
                   * 'rlml': Robust LM lag test

    Parameters
    ----------

    lme         : tuple
                  (Only if 'lme' or 'all' was in tests). Pair of statistic and
                  p-value for the LM error test.
    lml         : tuple
                  (Only if 'lml' or 'all' was in tests). Pair of statistic and
                  p-value for the LM lag test.
    rlme        : tuple
                  (Only if 'rlme' or 'all' was in tests). Pair of statistic
                  and p-value for the Robust LM error test.
    rlml        : tuple
                  (Only if 'rlml' or 'all' was in tests). Pair of statistic
                  and p-value for the Robust LM lag test.
    sarma       : tuple
                  (Only if 'rlml' or 'all' was in tests). Pair of statistic
                  and p-value for the SARMA test.

    References
    ----------
    .. [1] Anselin, L., Bera, A. K., Florax, R., Yoon, M. J. (1996) "Simple
       diagnostic tests for spatial dependence". Regional Science and Urban
       Economics, 26, 77-104.

    Examples
    --------

    >>> import numpy as np
    >>> import pysal
    >>> from ols import OLS

    Open the csv file to access the data for analysis

    >>> csv = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')

    Pull out from the csv the files we need ('HOVAL' as dependent as well as
    'INC' and 'CRIME' as independent) and directly transform them into nx1 and
    nx2 arrays, respectively

    >>> y = np.array([csv.by_col('HOVAL')]).T
    >>> x = np.array([csv.by_col('INC'), csv.by_col('CRIME')]).T

    Create the weights object from existing .gal file

    >>> w = pysal.open(pysal.examples.get_path('columbus.gal'), 'r').read()

    Row-standardize the weight object (not required although desirable in some
    cases)

    >>> w.transform='r'

    Run an OLS regression

    >>> ols = OLS(y, x)

    Run all the LM tests in the residuals. These diagnostics test for the
    presence of remaining spatial autocorrelation in the residuals of an OLS
    model and give indication about the type of spatial model. There are five
    types: presence of a spatial lag model (simple and robust version),
    presence of a spatial error model (simple and robust version) and joint presence
    of both a spatial lag as well as a spatial error model.

    >>> lms = pysal.spreg.diagnostics_sp.LMtests(ols, w)

    LM error test:

    >>> print round(lms.lme[0],4), round(lms.lme[1],4)
    3.0971 0.0784

    LM lag test:

    >>> print round(lms.lml[0],4), round(lms.lml[1],4)
    0.9816 0.3218

    Robust LM error test:

    >>> print round(lms.rlme[0],4), round(lms.rlme[1],4)
    3.2092 0.0732

    Robust LM lag test:

    >>> print round(lms.rlml[0],4), round(lms.rlml[1],4)
    1.0936 0.2957

    LM SARMA test:

    >>> print round(lms.sarma[0],4), round(lms.sarma[1],4)
    4.1907 0.123
    """

    def __init__(self, ols, w, tests=['all']):
        cache = spDcache(ols, w)
        if tests == ['all']:
            tests = ['lme', 'lml', 'rlme', 'rlml', 'sarma']
        if 'lme' in tests:
            self.lme = lmErr(ols, w, cache)
        if 'lml' in tests:
            self.lml = lmLag(ols, w, cache)
        if 'rlme' in tests:
            self.rlme = rlmErr(ols, w, cache)
        if 'rlml' in tests:
            self.rlml = rlmLag(ols, w, cache)
        if 'sarma' in tests:
            self.sarma = lmSarma(ols, w, cache)


class MoranRes:

    """
    Moran's I for spatial autocorrelation in residuals from OLS regression
    ...

    Parameters
    ----------

    ols         : OLS
                  OLS regression object
    w           : W
                  Spatial weights instance
    z           : boolean
                  If set to True computes attributes eI, vI and zI. Due to computational burden of vI, defaults to False.

    Attributes
    ----------
    I           : float
                  Moran's I statistic
    eI          : float
                  Moran's I expectation
    vI          : float
                  Moran's I variance
    zI          : float
                  Moran's I standardized value

    Examples
    --------

    >>> import numpy as np
    >>> import pysal
    >>> from ols import OLS

    Open the csv file to access the data for analysis

    >>> csv = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')

    Pull out from the csv the files we need ('HOVAL' as dependent as well as
    'INC' and 'CRIME' as independent) and directly transform them into nx1 and
    nx2 arrays, respectively

    >>> y = np.array([csv.by_col('HOVAL')]).T
    >>> x = np.array([csv.by_col('INC'), csv.by_col('CRIME')]).T

    Create the weights object from existing .gal file

    >>> w = pysal.open(pysal.examples.get_path('columbus.gal'), 'r').read()

    Row-standardize the weight object (not required although desirable in some
    cases)

    >>> w.transform='r'

    Run an OLS regression

    >>> ols = OLS(y, x)

    Run Moran's I test for residual spatial autocorrelation in an OLS model.
    This computes the traditional statistic applying a correction in the
    expectation and variance to account for the fact it comes from residuals
    instead of an independent variable

    >>> m = pysal.spreg.diagnostics_sp.MoranRes(ols, w, z=True)

    Value of the Moran's I statistic:

    >>> print round(m.I,4)
    0.1713

    Value of the Moran's I expectation:

    >>> print round(m.eI,4)
    -0.0345

    Value of the Moran's I variance:

    >>> print round(m.vI,4)
    0.0081

    Value of the Moran's I standardized value. This is
    distributed as a standard Normal(0, 1)

    >>> print round(m.zI,4)
    2.2827

    P-value of the standardized Moran's I value (z):

    >>> print round(m.p_norm,4)
    0.0224
    """

    def __init__(self, ols, w, z=False):
        cache = spDcache(ols, w)
        self.I = get_mI(ols, w, cache)
        if z:
            self.eI = get_eI(ols, w, cache)
            self.vI = get_vI(ols, w, self.eI, cache)
            self.zI, self.p_norm = get_zI(self.I, self.eI, self.vI)


class AKtest:

    """
    Moran's I test of spatial autocorrelation for IV estimation.
    Implemented following the original reference Anselin and Kelejian
    (1997) [AK97]_
    ...

    Parameters
    ----------

    iv          : TSLS
                  Regression object from TSLS class
    w           : W
                  Spatial weights instance
    case        : string
                  Flag for special cases (default to 'nosp'):

                   * 'nosp': Only NO spatial end. reg.
                   * 'gen': General case (spatial lag + end. reg.)

    Attributes
    ----------

    mi          : float
                  Moran's I statistic for IV residuals
    ak          : float
                  Square of corrected Moran's I for residuals::

                  .. math::

                        ak = \dfrac{N \times I^*}{\phi^2}

                  Note: if case='nosp' then it simplifies to the LMerror
    p           : float
                  P-value of the test

    References
    ----------

    .. [AK97] Anselin, L., Kelejian, H. (1997) "Testing for spatial error
        autocorrelation in the presence of endogenous regressors".
        Interregional Regional Science Review, 20, 1.
    .. [2] Kelejian, H.H., Prucha, I.R. and Yuzefovich, Y. (2004)
        "Instrumental variable estimation of a spatial autorgressive model with
        autoregressive disturbances: large and small sample results".
        Advances in Econometrics, 18, 163-198.

    Examples
    --------

    We first need to import the needed modules. Numpy is needed to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis. The TSLS is required to run the model on
    which we will perform the tests.

    >>> import numpy as np
    >>> import pysal
    >>> from twosls import TSLS
    >>> from twosls_sp import GM_Lag

    Open data on Columbus neighborhood crime (49 areas) using pysal.open().
    This is the DBF associated with the Columbus shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.

    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),'r')

    Before being able to apply the diagnostics, we have to run a model and,
    for that, we need the input variables. Extract the CRIME column (crime
    rates) from the DBF file and make it the dependent variable for the
    regression. Note that PySAL requires this to be an numpy array of shape
    (n, 1) as opposed to the also common shape of (n, ) that other packages
    accept.

    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))

    Extract INC (income) vector from the DBF to be used as
    independent variables in the regression.  Note that PySAL requires this to
    be an nxj numpy array, where j is the number of independent variables (not
    including a constant). By default this model adds a vector of ones to the
    independent variables passed in, but this can be overridden by passing
    constant=False.

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X = np.array(X).T

    In this case, we consider HOVAL (home value) as an endogenous regressor,
    so we acknowledge that by reading it in a different category.

    >>> yd = []
    >>> yd.append(db.by_col("HOVAL"))
    >>> yd = np.array(yd).T

    In order to properly account for the endogeneity, we have to pass in the
    instruments. Let us consider DISCBD (distance to the CBD) is a good one:

    >>> q = []
    >>> q.append(db.by_col("DISCBD"))
    >>> q = np.array(q).T

    Now we are good to run the model. It is an easy one line task.

    >>> reg = TSLS(y, X, yd, q=q)

    Now we are concerned with whether our non-spatial model presents spatial
    autocorrelation in the residuals. To assess this possibility, we can run
    the Anselin-Kelejian test, which is a version of the classical LM error
    test adapted for the case of residuals from an instrumental variables (IV)
    regression. First we need an extra object, the weights matrix, which
    includes the spatial configuration of the observations
    into the error component of the model. To do that, we can open an already
    existing gal file or create a new one. In this case, we will create one
    from ``columbus.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))

    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, this allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'

    We are good to run the test. It is a very simple task:

    >>> ak = AKtest(reg, w)

    And explore the information obtained:

    >>> print('AK test: %f\tP-value: %f'%(ak.ak, ak.p))
    AK test: 4.642895      P-value: 0.031182

    The test also accomodates the case when the residuals come from an IV
    regression that includes a spatial lag of the dependent variable. The only
    requirement needed is to modify the ``case`` parameter when we call
    ``AKtest``. First, let us run a spatial lag model:

    >>> reg_lag = GM_Lag(y, X, yd, q=q, w=w)

    And now we can run the AK test and obtain similar information as in the
    non-spatial model.

    >>> ak_sp = AKtest(reg, w, case='gen')
    >>> print('AK test: %f\tP-value: %f'%(ak_sp.ak, ak_sp.p))
    AK test: 1.157593      P-value: 0.281965

    """

    def __init__(self, iv, w, case='nosp'):
        if case == 'gen':
            cache = spDcache(iv, w)
            self.mi, self.ak, self.p = akTest(iv, w, cache)
        elif case == 'nosp':
            cache = spDcache(iv, w)
            self.mi = get_mI(iv, w, cache)
            self.ak, self.p = lmErr(iv, w, cache)
        else:
            print """\n
            Fix the optional argument 'case' to match the requirements:
                * 'gen': General case (spatial lag + end. reg.)
                * 'nosp': No spatial end. reg.
            \n"""


class spDcache:

    """
    Helper class to compute reusable pieces in the spatial diagnostics module
    ...

    Parameters
    ----------

    reg         : OLS_dev, TSLS_dev, STSLS_dev
                  Instance from a regression class
    w           : W
                  Spatial weights instance

    Attributes
    ----------

    j           : array
                  1x1 array with the result from:

                  .. math::

                        J = \dfrac{1}{[(WX\beta)' M (WX\beta) + T \sigma^2]}

    wu          : array
                  nx1 array with spatial lag of the residuals

    utwuDs      : array
                  1x1 array with the result from:

                  .. math::

                        utwuDs = \dfrac{u' W u}{\tilde{\sigma^2}}

    utwyDs      : array
                  1x1 array with the result from:

                  .. math::

                        utwyDs = \dfrac{u' W y}{\tilde{\sigma^2}}


    t           : array
                  1x1 array with the result from :

                  .. math::

                        T = tr[(W' + W) W]

    trA         : float
                  Trace of A as in Cliff & Ord (1981)

    """

    def __init__(self, reg, w):
        self.reg = reg
        self.w = w
        self._cache = {}

    @property
    def j(self):
        if 'j' not in self._cache:
            wxb = self.w.sparse * self.reg.predy
            wxb2 = np.dot(wxb.T, wxb)
            xwxb = spdot(self.reg.x.T, wxb)
            num1 = wxb2 - np.dot(xwxb.T, np.dot(self.reg.xtxi, xwxb))
            num = num1 + (self.t * self.reg.sig2n)
            den = self.reg.n * self.reg.sig2n
            self._cache['j'] = num / den
        return self._cache['j']

    @property
    def wu(self):
        if 'wu' not in self._cache:
            self._cache['wu'] = self.w.sparse * self.reg.u
        return self._cache['wu']

    @property
    def utwuDs(self):
        if 'utwuDs' not in self._cache:
            res = np.dot(self.reg.u.T, self.wu) / self.reg.sig2n
            self._cache['utwuDs'] = res
        return self._cache['utwuDs']

    @property
    def utwyDs(self):
        if 'utwyDs' not in self._cache:
            res = np.dot(self.reg.u.T, self.w.sparse * self.reg.y)
            self._cache['utwyDs'] = res / self.reg.sig2n
        return self._cache['utwyDs']

    @property
    def t(self):
        if 't' not in self._cache:
            prod = (self.w.sparse.T + self.w.sparse) * self.w.sparse
            self._cache['t'] = np.sum(prod.diagonal())
        return self._cache['t']

    @property
    def trA(self):
        if 'trA' not in self._cache:
            xtwx = spdot(self.reg.x.T, spdot(self.w.sparse, self.reg.x))
            mw = np.dot(self.reg.xtxi, xtwx)
            self._cache['trA'] = np.sum(mw.diagonal())
        return self._cache['trA']

    @property
    def AB(self):
        """
        Computes A and B matrices as in Cliff-Ord 1981, p. 203
        """
        if 'AB' not in self._cache:
            U = (self.w.sparse + self.w.sparse.T) / 2.
            z = spdot(U, self.reg.x, array_out=False)
            c1 = spdot(self.reg.x.T, z, array_out=False)
            c2 = spdot(z.T, z, array_out=False)
            G = self.reg.xtxi
            A = spdot(G, c1)
            B = spdot(G, c2)
            self._cache['AB'] = [A, B]
        return self._cache['AB']


def lmErr(reg, w, spDcache):
    """
    LM error test. Implemented as presented in eq. (9) of Anselin et al.
    (1996) [1]_
    ...

    Attributes
    ----------

    reg         : OLS_dev, TSLS_dev, STSLS_dev
                  Instance from a regression class
    w           : W
                  Spatial weights instance
    spDcache    : spDcache
                  Instance of spDcache class

    Returns
    -------

    lme         : tuple
                  Pair of statistic and p-value for the LM error test.

    References
    ----------
    .. _ Anselin, L., Bera, A. K., Florax, R., Yoon, M. J. (1996) "Simple
       diagnostic tests for spatial dependence". Regional Science and Urban
       Economics, 26, 77-104.
    """
    lm = spDcache.utwuDs ** 2 / spDcache.t
    pval = chisqprob(lm, 1)
    return (lm[0][0], pval[0][0])


def lmLag(ols, w, spDcache):
    """
    LM lag test. Implemented as presented in eq. (13) of Anselin et al.
    (1996) [1]_
    ...

    Attributes
    ----------

    ols         : OLS_dev
                  Instance from an OLS_dev regression
    w           : W
                  Spatial weights instance
    spDcache     : spDcache
                  Instance of spDcache class

    Returns
    -------

    lml         : tuple
                  Pair of statistic and p-value for the LM lag test.

    References
    ----------
    .. _ Anselin, L., Bera, A. K., Florax, R., Yoon, M. J. (1996) "Simple
       diagnostic tests for spatial dependence". Regional Science and Urban
       Economics, 26, 77-104.
    """
    lm = spDcache.utwyDs ** 2 / (ols.n * spDcache.j)
    pval = chisqprob(lm, 1)
    return (lm[0][0], pval[0][0])


def rlmErr(ols, w, spDcache):
    """
    Robust LM error test. Implemented as presented in eq. (8) of Anselin et al. (1996) [1]_

    NOTE: eq. (8) has an errata, the power -1 in the denominator should be inside the square bracket.
    ...

    Attributes
    ----------

    ols         : OLS_dev
                  Instance from an OLS_dev regression
    w           : W
                  Spatial weights instance
    spDcache     : spDcache
                  Instance of spDcache class

    Returns
    -------

    rlme        : tuple
                  Pair of statistic and p-value for the Robust LM error test.

    References
    ----------
    .. _ Anselin, L., Bera, A. K., Florax, R., Yoon, M. J. (1996) "Simple
       diagnostic tests for spatial dependence". Regional Science and Urban
       Economics, 26, 77-104.
    """
    nj = ols.n * spDcache.j
    num = (spDcache.utwuDs - (spDcache.t * spDcache.utwyDs) / nj) ** 2
    den = spDcache.t * (1. - (spDcache.t / nj))
    lm = num / den
    pval = chisqprob(lm, 1)
    return (lm[0][0], pval[0][0])


def rlmLag(ols, w, spDcache):
    """
    Robust LM lag test. Implemented as presented in eq. (12) of Anselin et al.
    (1996) [1]_
    ...

    Attributes
    ----------

    ols             : OLS_dev
                      Instance from an OLS_dev regression
    w               : W
                      Spatial weights instance
    spDcache        : spDcache
                      Instance of spDcache class

    Returns
    -------

    rlml            : tuple
                      Pair of statistic and p-value for the Robust LM lag test.

    References
    ----------
    .. _ Anselin, L., Bera, A. K., Florax, R., Yoon, M. J. (1996) "Simple
       diagnostic tests for spatial dependence". Regional Science and Urban
       Economics, 26, 77-104.
    """
    lm = (spDcache.utwyDs - spDcache.utwuDs) ** 2 / \
        ((ols.n * spDcache.j) - spDcache.t)
    pval = chisqprob(lm, 1)
    return (lm[0][0], pval[0][0])


def lmSarma(ols, w, spDcache):
    """
    LM error test. Implemented as presented in eq. (15) of Anselin et al.
    (1996) [1]_
    ...

    Attributes
    ----------

    ols         : OLS_dev
                  Instance from an OLS_dev regression
    w           : W
                  Spatial weights instance
    spDcache     : spDcache
                  Instance of spDcache class

    Returns
    -------

    sarma       : tuple
                  Pair of statistic and p-value for the LM sarma test.

    References
    ----------
    .. _ Anselin, L., Bera, A. K., Florax, R., Yoon, M. J. (1996) "Simple
       diagnostic tests for spatial dependence". Regional Science and Urban
       Economics, 26, 77-104.
    """

    first = (spDcache.utwyDs - spDcache.utwuDs) ** 2 / \
        (w.n * spDcache.j - spDcache.t)
    secnd = spDcache.utwuDs ** 2 / spDcache.t
    lm = first + secnd
    pval = chisqprob(lm, 2)
    return (lm[0][0], pval[0][0])


def get_mI(reg, w, spDcache):
    """
    Moran's I statistic of spatial autocorrelation as showed in Cliff & Ord
    (1981) [CO81]_, p. 201-203
    ...

    Attributes
    ----------

    reg             : OLS_dev, TSLS_dev, STSLS_dev
                      Instance from a regression class
    w               : W
                      Spatial weights instance
    spDcache        : spDcache
                      Instance of spDcache class

    Returns
    -------

    moran           : float
                      Statistic Moran's I test.

    References
    ----------
    .. [CO81] Cliff, AD., Ord, JK. (1981) "Spatial processes: models & applications".
       Pion London
    """
    mi = (w.n * np.dot(reg.u.T, spDcache.wu)) / (w.s0 * reg.utu)
    return mi[0][0]


def get_vI(ols, w, ei, spDcache):
    """
    Moran's I variance coded as in Cliff & Ord 1981 (p. 201-203) and R's spdep
    """
    A = spDcache.AB[0]
    trA2 = np.dot(A, A)
    trA2 = np.sum(trA2.diagonal())

    B = spDcache.AB[1]
    trB = np.sum(B.diagonal()) * 4.
    vi = (w.n ** 2 / (w.s0 ** 2 * (w.n - ols.k) * (w.n - ols.k + 2.))) * \
         (w.s1 + 2. * trA2 - trB -
          ((2. * (spDcache.trA ** 2)) / (w.n - ols.k)))
    return vi


def get_eI(ols, w, spDcache):
    """
    Moran's I expectation using matrix M
    """
    return - (w.n * spDcache.trA) / (w.s0 * (w.n - ols.k))


def get_zI(I, ei, vi):
    """
    Standardized I

    Returns two-sided p-values as provided in the GeoDa family
    """
    z = abs((I - ei) / np.sqrt(vi))
    pval = norm.sf(z) * 2.
    return (z, pval)


def akTest(iv, w, spDcache):
    """
    Computes AK-test for the general case (end. reg. + sp. lag)
    ...

    Parameters
    ----------

    iv          : STSLS_dev
                  Instance from spatial 2SLS regression
    w           : W
                  Spatial weights instance
   spDcache     : spDcache
                  Instance of spDcache class

    Attributes
    ----------
    mi          : float
                  Moran's I statistic for IV residuals
    ak          : float
                  Square of corrected Moran's I for residuals::

                  .. math::
                        ak = \dfrac{N \times I^*}{\phi^2}

    p           : float
                  P-value of the test

    ToDo:
        * Code in as Nancy
        * Compare both
    """
    mi = get_mI(iv, w, spDcache)
    # Phi2
    etwz = spdot(iv.u.T, spdot(w.sparse, iv.z))
    a = np.dot(etwz, np.dot(iv.varb, etwz.T))
    s12 = (w.s0 / w.n) ** 2
    phi2 = (spDcache.t + (4.0 / iv.sig2n) * a) / (s12 * w.n)
    ak = w.n * mi ** 2 / phi2
    pval = chisqprob(ak, 1)
    return (mi, ak[0][0], pval[0][0])


def _test():
    import doctest
    doctest.testmod()

if __name__ == '__main__':
    _test()

########NEW FILE########
__FILENAME__ = diagnostics_tsls
"""
Diagnostics for two stage least squares regression estimations. 
        
"""

__author__ = "Luc Anselin luc.anselin@asu.edu, Nicholas Malizia nicholas.malizia@asu.edu "

from pysal.common import *
from scipy.stats import pearsonr

__all__ = ["t_stat", "pr2_aspatial", "pr2_spatial"]


def t_stat(reg, z_stat=False):
    """
    Calculates the t-statistics (or z-statistics) and associated p-values.

    Parameters
    ----------
    reg             : regression object
                      output instance from a regression model
    z_stat          : boolean
                      If True run z-stat instead of t-stat

    Returns
    -------    
    ts_result       : list of tuples
                      each tuple includes value of t statistic (or z
                      statistic) and associated p-value

    References
    ----------
    .. [1] W. Greene. 2003. Econometric Analysis. Prentice Hall, Upper
       Saddle River.

    Examples
    --------

    We first need to import the needed modules. Numpy is needed to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis. The ``diagnostics`` module is used for the tests
    we will show here and the OLS and TSLS are required to run the models on
    which we will perform the tests.

    >>> import numpy as np
    >>> import pysal
    >>> import pysal.spreg.diagnostics as diagnostics
    >>> from pysal.spreg.ols import OLS
    >>> from twosls import TSLS

    Open data on Columbus neighborhood crime (49 areas) using pysal.open().
    This is the DBF associated with the Columbus shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),'r')
    
    Before being able to apply the diagnostics, we have to run a model and,
    for that, we need the input variables. Extract the CRIME column (crime
    rates) from the DBF file and make it the dependent variable for the
    regression. Note that PySAL requires this to be an numpy array of shape
    (n, 1) as opposed to the also common shape of (n, ) that other packages
    accept.

    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))

    Extract INC (income) and HOVAL (home value) vector from the DBF to be used as
    independent variables in the regression.  Note that PySAL requires this to
    be an nxj numpy array, where j is the number of independent variables (not
    including a constant). By default this model adds a vector of ones to the
    independent variables passed in, but this can be overridden by passing
    constant=False.

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T

    Run an OLS regression. Since it is a non-spatial model, all we need is the
    dependent and the independent variable.

    >>> reg = OLS(y,X)

    Now we can perform a t-statistic on the model:

    >>> testresult = diagnostics.t_stat(reg)
    >>> print("%12.12f"%testresult[0][0], "%12.12f"%testresult[0][1], "%12.12f"%testresult[1][0], "%12.12f"%testresult[1][1], "%12.12f"%testresult[2][0], "%12.12f"%testresult[2][1])
    ('14.490373143689', '0.000000000000', '-4.780496191297', '0.000018289595', '-2.654408642718', '0.010874504910')

    We can also use the z-stat. For that, we re-build the model so we consider
    HOVAL as endogenous, instrument for it using DISCBD and carry out two
    stage least squares (TSLS) estimation.

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X = np.array(X).T    
    >>> yd = []
    >>> yd.append(db.by_col("HOVAL"))
    >>> yd = np.array(yd).T
    >>> q = []
    >>> q.append(db.by_col("DISCBD"))
    >>> q = np.array(q).T

    Once the variables are read as different objects, we are good to run the
    model.

    >>> reg = TSLS(y, X, yd, q)

    With the output of the TSLS regression, we can perform a z-statistic:

    >>> testresult = diagnostics.t_stat(reg, z_stat=True)
    >>> print("%12.10f"%testresult[0][0], "%12.10f"%testresult[0][1], "%12.10f"%testresult[1][0], "%12.10f"%testresult[1][1], "%12.10f"%testresult[2][0], "%12.10f"%testresult[2][1])
    ('5.8452644705', '0.0000000051', '0.3676015668', '0.7131703463', '-1.9946891308', '0.0460767956')
    """

    k = reg.k           # (scalar) number of ind. vas (includes constant)
    n = reg.n           # (scalar) number of observations
    vm = reg.vm         # (array) coefficients of variance matrix (k x k)
    betas = reg.betas   # (array) coefficients of the regressors (1 x k)
    variance = vm.diagonal()
    tStat = betas.reshape(len(betas),) / np.sqrt(variance)
    ts_result = []
    for t in tStat:
        if z_stat:
            ts_result.append((t, stats.norm.sf(abs(t)) * 2))
        else:
            ts_result.append((t, stats.t.sf(abs(t), n - k) * 2))
    return ts_result


def pr2_aspatial(tslsreg):
    """
    Calculates the pseudo r^2 for the two stage least squares regression.

    Parameters
    ----------
    tslsreg             : two stage least squares regression object
                          output instance from a two stage least squares
                          regression model

    Returns
    -------
    pr2_result          : float
                          value of the squared pearson correlation between
                          the y and tsls-predicted y vectors

    Examples
    --------

    We first need to import the needed modules. Numpy is needed to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis. The TSLS is required to run the model on
    which we will perform the tests.

    >>> import numpy as np
    >>> import pysal
    >>> from twosls import TSLS

    Open data on Columbus neighborhood crime (49 areas) using pysal.open().
    This is the DBF associated with the Columbus shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),'r')

    Before being able to apply the diagnostics, we have to run a model and,
    for that, we need the input variables. Extract the CRIME column (crime
    rates) from the DBF file and make it the dependent variable for the
    regression. Note that PySAL requires this to be an numpy array of shape
    (n, 1) as opposed to the also common shape of (n, ) that other packages
    accept.

    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))

    Extract INC (income) vector from the DBF to be used as
    independent variables in the regression.  Note that PySAL requires this to
    be an nxj numpy array, where j is the number of independent variables (not
    including a constant). By default this model adds a vector of ones to the
    independent variables passed in, but this can be overridden by passing
    constant=False.

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X = np.array(X).T

    In this case, we consider HOVAL (home value) as an endogenous regressor,
    so we acknowledge that by reading it in a different category.

    >>> yd = []
    >>> yd.append(db.by_col("HOVAL"))
    >>> yd = np.array(yd).T

    In order to properly account for the endogeneity, we have to pass in the
    instruments. Let us consider DISCBD (distance to the CBD) is a good one:

    >>> q = []
    >>> q.append(db.by_col("DISCBD"))
    >>> q = np.array(q).T

    Now we are good to run the model. It is an easy one line task.

    >>> reg = TSLS(y, X, yd, q=q)

    In order to perform the pseudo R^2, we pass the regression object to the
    function and we are done!

    >>> result = pr2_aspatial(reg)
    >>> print("%1.6f"%result)    
    0.279361

    """

    y = tslsreg.y
    predy = tslsreg.predy
    pr = pearsonr(y, predy)[0]
    pr2_result = float(pr ** 2)
    return pr2_result


def pr2_spatial(tslsreg):
    """
    Calculates the pseudo r^2 for the spatial two stage least squares 
    regression.

    Parameters
    ----------
    stslsreg            : spatial two stage least squares regression object
                          output instance from a spatial two stage least 
                          squares regression model

    Returns
    -------    
    pr2_result          : float
                          value of the squared pearson correlation between
                          the y and stsls-predicted y vectors

    Examples
    --------

    We first need to import the needed modules. Numpy is needed to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis. The GM_Lag is required to run the model on
    which we will perform the tests and the ``pysal.spreg.diagnostics`` module
    contains the function with the test.

    >>> import numpy as np
    >>> import pysal
    >>> import pysal.spreg.diagnostics as D
    >>> from twosls_sp import GM_Lag

    Open data on Columbus neighborhood crime (49 areas) using pysal.open().
    This is the DBF associated with the Columbus shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),'r')

    Extract the HOVAL column (home value) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y = np.array(db.by_col("HOVAL"))
    >>> y = np.reshape(y, (49,1))

    Extract INC (income) vectors from the DBF to be used as
    independent variables in the regression.  Note that PySAL requires this to
    be an nxj numpy array, where j is the number of independent variables (not
    including a constant). By default this model adds a vector of ones to the
    independent variables passed in, but this can be overridden by passing
    constant=False.

    >>> X = np.array(db.by_col("INC"))
    >>> X = np.reshape(X, (49,1))

    In this case, we consider CRIME (crime rates) as an endogenous regressor,
    so we acknowledge that by reading it in a different category.

    >>> yd = np.array(db.by_col("CRIME"))
    >>> yd = np.reshape(yd, (49,1))

    In order to properly account for the endogeneity, we have to pass in the
    instruments. Let us consider DISCBD (distance to the CBD) is a good one:

    >>> q = np.array(db.by_col("DISCBD"))
    >>> q = np.reshape(q, (49,1))

    Since this test has a spatial component, we need to specify the spatial
    weights matrix that includes the spatial configuration of the observations
    into the error component of the model. To do that, we can open an already
    existing gal file or create a new one. In this case, we will create one
    from ``columbus.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp")) 

    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, this allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'

    Now we are good to run the spatial lag model. Make sure you pass all the
    parameters correctly and, if desired, pass the names of the variables as
    well so when you print the summary (reg.summary) they are included:

    >>> reg = GM_Lag(y, X, w=w, yend=yd, q=q, w_lags=2, name_x=['inc'], name_y='hoval', name_yend=['crime'], name_q=['discbd'], name_ds='columbus')

    Once we have a regression object, we can perform the spatial version of
    the pesudo R^2. It is as simple as one line!

    >>> result = pr2_spatial(reg)
    >>> print("%1.6f"%result)
    0.299649

    """

    y = tslsreg.y
    predy_e = tslsreg.predy_e
    pr = pearsonr(y, predy_e)[0]
    pr2_result = float(pr ** 2)
    return pr2_result


def _test():
    import doctest
    doctest.testmod()


if __name__ == '__main__':
    _test()

########NEW FILE########
__FILENAME__ = error_sp
"""
Spatial Error Models module
"""

__author__ = "Luc Anselin luc.anselin@asu.edu, \
        Daniel Arribas-Bel darribas@asu.edu, \
        Pedro V. Amaral pedro.amaral@asu.edu"

import numpy as np
from numpy import linalg as la
import ols as OLS
from pysal import lag_spatial
from utils import power_expansion, set_endog, iter_msg, sp_att
from utils import get_A1_hom, get_A2_hom, get_A1_het, optim_moments, get_spFilter, get_lags, _moments2eqs
from utils import spdot, RegressionPropsY, set_warn
import twosls as TSLS
import user_output as USER
import summary_output as SUMMARY

__all__ = ["GM_Error", "GM_Endog_Error", "GM_Combo"]


class BaseGM_Error(RegressionPropsY):

    """
    GMM method for a spatial error model (note: no consistency checks
    diagnostics or constant added); based on Kelejian and Prucha 
    (1998, 1999)[1]_ [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    w            : Sparse matrix
                   Spatial weights sparse matrix   

    Attributes
    ----------
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    sig2         : float
                   Sigma squared used in computations


    References
    ----------

    .. [1] Kelejian, H.R., Prucha, I.R. (1998) "A generalized spatial
        two-stage least squares procedure for estimating a spatial autoregressive
        model with autoregressive disturbances". The Journal of Real State
        Finance and Economics, 17, 1.

    .. [2] Kelejian, H.R., Prucha, I.R. (1999) "A Generalized Moments
        Estimator for the Autoregressive Parameter in a Spatial Model".
        International Economic Review, 40, 2.

    Examples
    --------

    >>> import pysal
    >>> import numpy as np
    >>> dbf = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    >>> y = np.array([dbf.by_col('HOVAL')]).T
    >>> x = np.array([dbf.by_col('INC'), dbf.by_col('CRIME')]).T
    >>> x = np.hstack((np.ones(y.shape),x))
    >>> w = pysal.open(pysal.examples.get_path("columbus.gal"), 'r').read() 
    >>> w.transform='r'
    >>> model = BaseGM_Error(y, x, w=w.sparse)
    >>> np.around(model.betas, decimals=4)
    array([[ 47.6946],
           [  0.7105],
           [ -0.5505],
           [  0.3257]])
    """

    def __init__(self, y, x, w):

        # 1a. OLS --> \tilde{betas}
        ols = OLS.BaseOLS(y=y, x=x)
        self.n, self.k = ols.x.shape
        self.x = ols.x
        self.y = ols.y

        # 1b. GMM --> \tilde{\lambda1}
        moments = _momentsGM_Error(w, ols.u)
        lambda1 = optim_moments(moments)

        # 2a. OLS -->\hat{betas}
        xs = get_spFilter(w, lambda1, self.x)
        ys = get_spFilter(w, lambda1, self.y)
        ols2 = OLS.BaseOLS(y=ys, x=xs)

        # Output
        self.predy = spdot(self.x, ols2.betas)
        self.u = y - self.predy
        self.betas = np.vstack((ols2.betas, np.array([[lambda1]])))
        self.sig2 = ols2.sig2n
        self.e_filtered = self.u - lambda1 * w * self.u

        self.vm = self.sig2 * ols2.xtxi
        se_betas = np.sqrt(self.vm.diagonal())
        self._cache = {}


class GM_Error(BaseGM_Error):

    """
    GMM method for a spatial error model, with results and diagnostics; based
    on Kelejian and Prucha (1998, 1999)[1]_ [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    w            : pysal W object
                   Spatial weights object (always needed)   
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
    vm           : array
                   Variance covariance matrix (kxk)
    sig2         : float
                   Sigma squared used in computations
    std_err      : array
                   1xk array of standard errors of the betas    
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    title        : string
                   Name of the regression method used

    References
    ----------

    .. [1] Kelejian, H.R., Prucha, I.R. (1998) "A generalized spatial
        two-stage least squares procedure for estimating a spatial autoregressive
        model with autoregressive disturbances". The Journal of Real State
        Finance and Economics, 17, 1.
    .. [2] Kelejian, H.R., Prucha, I.R. (1999) "A Generalized Moments
        Estimator for the Autoregressive Parameter in a Spatial Model".
        International Economic Review, 40, 2.

    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import pysal
    >>> import numpy as np

    Open data on Columbus neighborhood crime (49 areas) using pysal.open().
    This is the DBF associated with the Columbus shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> dbf = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')

    Extract the HOVAL column (home values) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y = np.array([dbf.by_col('HOVAL')]).T

    Extract CRIME (crime) and INC (income) vectors from the DBF to be used as
    independent variables in the regression.  Note that PySAL requires this to
    be an nxj numpy array, where j is the number of independent variables (not
    including a constant). By default this class adds a vector of ones to the
    independent variables passed in.

    >>> names_to_extract = ['INC', 'CRIME']
    >>> x = np.array([dbf.by_col(name) for name in names_to_extract]).T

    Since we want to run a spatial error model, we need to specify the spatial
    weights matrix that includes the spatial configuration of the observations
    into the error component of the model. To do that, we can open an already
    existing gal file or create a new one. In this case, we will use
    ``columbus.gal``, which contains contiguity relationships between the
    observations in the Columbus dataset we are using throughout this example.
    Note that, in order to read the file, not only to open it, we need to
    append '.read()' at the end of the command.

    >>> w = pysal.open(pysal.examples.get_path("columbus.gal"), 'r').read() 

    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, his allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform='r'

    We are all set with the preliminars, we are good to run the model. In this
    case, we will need the variables and the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.

    >>> model = GM_Error(y, x, w=w, name_y='hoval', name_x=['income', 'crime'], name_ds='columbus')

    Once we have run the model, we can explore a little bit the output. The
    regression object we have created has many attributes so take your time to
    discover them. Note that because we are running the classical GMM error
    model from 1998/99, the spatial parameter is obtained as a point estimate, so
    although you get a value for it (there are for coefficients under
    model.betas), you cannot perform inference on it (there are only three
    values in model.se_betas).

    >>> print model.name_x
    ['CONSTANT', 'income', 'crime', 'lambda']
    >>> np.around(model.betas, decimals=4)
    array([[ 47.6946],
           [  0.7105],
           [ -0.5505],
           [  0.3257]])
    >>> np.around(model.std_err, decimals=4)
    array([ 12.412 ,   0.5044,   0.1785])
    >>> np.around(model.z_stat, decimals=6)
    array([[  3.84261100e+00,   1.22000000e-04],
           [  1.40839200e+00,   1.59015000e-01],
           [ -3.08424700e+00,   2.04100000e-03]])
    >>> round(model.sig2,4)
    198.5596

    """

    def __init__(self, y, x, w,
                 vm=False, name_y=None, name_x=None,
                 name_w=None, name_ds=None):

        n = USER.check_arrays(y, x)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        x_constant = USER.check_constant(x)
        BaseGM_Error.__init__(self, y=y, x=x_constant, w=w.sparse)
        self.title = "SPATIALLY WEIGHTED LEAST SQUARES"
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_y = USER.set_name_y(name_y)
        self.name_x = USER.set_name_x(name_x, x)
        self.name_x.append('lambda')
        self.name_w = USER.set_name_w(name_w, w)
        SUMMARY.GM_Error(reg=self, w=w, vm=vm)


class BaseGM_Endog_Error(RegressionPropsY):

    '''
    GMM method for a spatial error model with endogenous variables (note: no
    consistency checks, diagnostics or constant added); based on Kelejian and
    Prucha (1998, 1999)[1]_[2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note: 
                   this should not contain any variables from x)
    w            : Sparse matrix
                   Spatial weights sparse matrix 

    Attributes
    ----------
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    z            : array
                   nxk array of variables (combination of x and yend)
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    sig2         : float
                   Sigma squared used in computations

    References
    ----------

    .. [1] Kelejian, H.R., Prucha, I.R. (1998) "A generalized spatial
        two-stage least squares procedure for estimating a spatial autoregressive
        model with autoregressive disturbances". The Journal of Real State
        Finance and Economics, 17, 1.

    .. [2] Kelejian, H.R., Prucha, I.R. (1999) "A Generalized Moments
        Estimator for the Autoregressive Parameter in a Spatial Model".
        International Economic Review, 40, 2.

    Examples
    --------

    >>> import pysal
    >>> import numpy as np
    >>> dbf = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    >>> y = np.array([dbf.by_col('CRIME')]).T
    >>> x = np.array([dbf.by_col('INC')]).T
    >>> x = np.hstack((np.ones(y.shape),x))
    >>> yend = np.array([dbf.by_col('HOVAL')]).T
    >>> q = np.array([dbf.by_col('DISCBD')]).T
    >>> w = pysal.open(pysal.examples.get_path("columbus.gal"), 'r').read() 
    >>> w.transform='r'
    >>> model = BaseGM_Endog_Error(y, x, yend, q, w=w.sparse)
    >>> np.around(model.betas, decimals=4)
    array([[ 82.573 ],
           [  0.581 ],
           [ -1.4481],
           [  0.3499]])

    '''

    def __init__(self, y, x, yend, q, w):

        # 1a. TSLS --> \tilde{betas}
        tsls = TSLS.BaseTSLS(y=y, x=x, yend=yend, q=q)
        self.n, self.k = tsls.z.shape
        self.x = tsls.x
        self.y = tsls.y
        self.yend, self.z = tsls.yend, tsls.z

        # 1b. GMM --> \tilde{\lambda1}
        moments = _momentsGM_Error(w, tsls.u)
        lambda1 = optim_moments(moments)

        # 2a. 2SLS -->\hat{betas}
        xs = get_spFilter(w, lambda1, self.x)
        ys = get_spFilter(w, lambda1, self.y)
        yend_s = get_spFilter(w, lambda1, self.yend)
        tsls2 = TSLS.BaseTSLS(ys, xs, yend_s, h=tsls.h)

        # Output
        self.betas = np.vstack((tsls2.betas, np.array([[lambda1]])))
        self.predy = spdot(tsls.z, tsls2.betas)
        self.u = y - self.predy
        self.sig2 = float(np.dot(tsls2.u.T, tsls2.u)) / self.n
        self.e_filtered = self.u - lambda1 * w * self.u
        self.vm = self.sig2 * tsls2.varb
        self._cache = {}


class GM_Endog_Error(BaseGM_Endog_Error):

    '''
    GMM method for a spatial error model with endogenous variables, with
    results and diagnostics; based on Kelejian and Prucha (1998, 1999)[1]_[2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note: 
                   this should not contain any variables from x)
    w            : pysal W object
                   Spatial weights object (always needed)   
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_yend    : list of strings
                   Names of endogenous variables for use in output
    name_q       : list of strings
                   Names of instruments for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    z            : array
                   nxk array of variables (combination of x and yend)
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
    sig2         : float
                   Sigma squared used in computations
    std_err      : array
                   1xk array of standard errors of the betas    
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
    name_y        : string
                    Name of dependent variable for use in output
    name_x        : list of strings
                    Names of independent variables for use in output
    name_yend     : list of strings
                    Names of endogenous variables for use in output
    name_z        : list of strings
                    Names of exogenous and endogenous variables for use in 
                    output
    name_q        : list of strings
                    Names of external instruments
    name_h        : list of strings
                    Names of all instruments used in ouput
    name_w        : string
                    Name of weights matrix for use in output
    name_ds       : string
                    Name of dataset for use in output
    title         : string
                    Name of the regression method used

    References
    ----------

    .. [1] Kelejian, H.R., Prucha, I.R. (1998) "A generalized spatial
        two-stage least squares procedure for estimating a spatial autoregressive
        model with autoregressive disturbances". The Journal of Real State
        Finance and Economics, 17, 1.

    .. [2] Kelejian, H.R., Prucha, I.R. (1999) "A Generalized Moments
        Estimator for the Autoregressive Parameter in a Spatial Model".
        International Economic Review, 40, 2.

    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import pysal
    >>> import numpy as np

    Open data on Columbus neighborhood crime (49 areas) using pysal.open().
    This is the DBF associated with the Columbus shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> dbf = pysal.open(pysal.examples.get_path("columbus.dbf"),'r')

    Extract the CRIME column (crime rates) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y = np.array([dbf.by_col('CRIME')]).T

    Extract INC (income) vector from the DBF to be used as
    independent variables in the regression.  Note that PySAL requires this to
    be an nxj numpy array, where j is the number of independent variables (not
    including a constant). By default this model adds a vector of ones to the
    independent variables passed in.

    >>> x = np.array([dbf.by_col('INC')]).T

    In this case we consider HOVAL (home value) is an endogenous regressor.
    We tell the model that this is so by passing it in a different parameter
    from the exogenous variables (x).

    >>> yend = np.array([dbf.by_col('HOVAL')]).T

    Because we have endogenous variables, to obtain a correct estimate of the
    model, we need to instrument for HOVAL. We use DISCBD (distance to the
    CBD) for this and hence put it in the instruments parameter, 'q'.

    >>> q = np.array([dbf.by_col('DISCBD')]).T

    Since we want to run a spatial error model, we need to specify the spatial
    weights matrix that includes the spatial configuration of the observations
    into the error component of the model. To do that, we can open an already
    existing gal file or create a new one. In this case, we will use
    ``columbus.gal``, which contains contiguity relationships between the
    observations in the Columbus dataset we are using throughout this example.
    Note that, in order to read the file, not only to open it, we need to
    append '.read()' at the end of the command.

    >>> w = pysal.open(pysal.examples.get_path("columbus.gal"), 'r').read() 

    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, this allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform='r'

    We are all set with the preliminars, we are good to run the model. In this
    case, we will need the variables (exogenous and endogenous), the
    instruments and the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.

    >>> model = GM_Endog_Error(y, x, yend, q, w=w, name_x=['inc'], name_y='crime', name_yend=['hoval'], name_q=['discbd'], name_ds='columbus')

    Once we have run the model, we can explore a little bit the output. The
    regression object we have created has many attributes so take your time to
    discover them. Note that because we are running the classical GMM error
    model from 1998/99, the spatial parameter is obtained as a point estimate, so
    although you get a value for it (there are for coefficients under
    model.betas), you cannot perform inference on it (there are only three
    values in model.se_betas). Also, this regression uses a two stage least
    squares estimation method that accounts for the endogeneity created by the
    endogenous variables included.

    >>> print model.name_z
    ['CONSTANT', 'inc', 'hoval', 'lambda']
    >>> np.around(model.betas, decimals=4)
    array([[ 82.573 ],
           [  0.581 ],
           [ -1.4481],
           [  0.3499]])
    >>> np.around(model.std_err, decimals=4)
    array([ 16.1381,   1.3545,   0.7862])
    
    '''

    def __init__(self, y, x, yend, q, w,
                 vm=False, name_y=None, name_x=None,
                 name_yend=None, name_q=None,
                 name_w=None, name_ds=None):

        n = USER.check_arrays(y, x, yend, q)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        x_constant = USER.check_constant(x)
        BaseGM_Endog_Error.__init__(
            self, y=y, x=x_constant, w=w.sparse, yend=yend, q=q)
        self.title = "SPATIALLY WEIGHTED TWO STAGE LEAST SQUARES"
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_y = USER.set_name_y(name_y)
        self.name_x = USER.set_name_x(name_x, x)
        self.name_yend = USER.set_name_yend(name_yend, yend)
        self.name_z = self.name_x + self.name_yend
        self.name_z.append('lambda')
        self.name_q = USER.set_name_q(name_q, q)
        self.name_h = USER.set_name_h(self.name_x, self.name_q)
        self.name_w = USER.set_name_w(name_w, w)
        SUMMARY.GM_Endog_Error(reg=self, w=w, vm=vm)


class BaseGM_Combo(BaseGM_Endog_Error):

    """
    GMM method for a spatial lag and error model, with endogenous variables
    (note: no consistency checks, diagnostics or constant added); based on 
    Kelejian and Prucha (1998, 1999)[1]_[2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note: 
                   this should not contain any variables from x)
    w            : Sparse matrix
                   Spatial weights sparse matrix  
    w_lags       : integer
                   Orders of W to include as instruments for the spatially
                   lagged dependent variable. For example, w_lags=1, then
                   instruments are WX; if w_lags=2, then WX, WWX; and so on.
    lag_q        : boolean
                   If True, then include spatial lags of the additional 
                   instruments (q).

    Attributes
    ----------
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    z            : array
                   nxk array of variables (combination of x and yend)
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    sig2         : float
                   Sigma squared used in computations

    References
    ----------

    .. [1] Kelejian, H.R., Prucha, I.R. (1998) "A generalized spatial
        two-stage least squares procedure for estimating a spatial autoregressive
        model with autoregressive disturbances". The Journal of Real State
        Finance and Economics, 17, 1.

    .. [2] Kelejian, H.R., Prucha, I.R. (1999) "A Generalized Moments
        Estimator for the Autoregressive Parameter in a Spatial Model".
        International Economic Review, 40, 2.

    Examples
    --------

    >>> import numpy as np
    >>> import pysal
    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))
    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X = np.array(X).T
    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
    >>> w.transform = 'r'
    >>> w_lags = 1
    >>> yd2, q2 = pysal.spreg.utils.set_endog(y, X, w, None, None, w_lags, True)
    >>> X = np.hstack((np.ones(y.shape),X))

    Example only with spatial lag

    >>> reg = BaseGM_Combo(y, X, yend=yd2, q=q2, w=w.sparse)

    Print the betas

    >>> print np.around(np.hstack((reg.betas[:-1],np.sqrt(reg.vm.diagonal()).reshape(3,1))),3)
    [[ 39.059  11.86 ]
     [ -1.404   0.391]
     [  0.467   0.2  ]]

    And lambda

    >>> print 'Lamda: ', np.around(reg.betas[-1], 3)
    Lamda:  [-0.048]

    Example with both spatial lag and other endogenous variables

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X = np.array(X).T
    >>> yd = []
    >>> yd.append(db.by_col("HOVAL"))
    >>> yd = np.array(yd).T
    >>> q = []
    >>> q.append(db.by_col("DISCBD"))
    >>> q = np.array(q).T
    >>> yd2, q2 = pysal.spreg.utils.set_endog(y, X, w, yd, q, w_lags, True)
    >>> X = np.hstack((np.ones(y.shape),X))
    >>> reg = BaseGM_Combo(y, X, yd2, q2, w=w.sparse)
    >>> betas = np.array([['CONSTANT'],['INC'],['HOVAL'],['W_CRIME']])
    >>> print np.hstack((betas, np.around(np.hstack((reg.betas[:-1], np.sqrt(reg.vm.diagonal()).reshape(4,1))),4)))
    [['CONSTANT' '50.0944' '14.3593']
     ['INC' '-0.2552' '0.5667']
     ['HOVAL' '-0.6885' '0.3029']
     ['W_CRIME' '0.4375' '0.2314']]

        """

    def __init__(self, y, x, yend=None, q=None,
                 w=None, w_lags=1, lag_q=True):

        BaseGM_Endog_Error.__init__(self, y=y, x=x, w=w, yend=yend, q=q)


class GM_Combo(BaseGM_Combo):

    """
    GMM method for a spatial lag and error model with endogenous variables,
    with results and diagnostics; based on Kelejian and Prucha (1998,
    1999)[1]_[2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note: 
                   this should not contain any variables from x)
    w            : pysal W object
                   Spatial weights object (always needed)   
    w_lags       : integer
                   Orders of W to include as instruments for the spatially
                   lagged dependent variable. For example, w_lags=1, then
                   instruments are WX; if w_lags=2, then WX, WWX; and so on.
    lag_q        : boolean
                   If True, then include spatial lags of the additional 
                   instruments (q).
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_yend    : list of strings
                   Names of endogenous variables for use in output
    name_q       : list of strings
                   Names of instruments for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    e_pred       : array
                   nx1 array of residuals (using reduced form)
    predy        : array
                   nx1 array of predicted y values
    predy_e      : array
                   nx1 array of predicted y values (using reduced form)
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    z            : array
                   nxk array of variables (combination of x and yend)
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
    pr2_e        : float
                   Pseudo R squared (squared correlation between y and ypred_e
                   (using reduced form))
    sig2         : float
                   Sigma squared used in computations (based on filtered
                   residuals)
    std_err      : array
                   1xk array of standard errors of the betas    
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
    name_y        : string
                    Name of dependent variable for use in output
    name_x        : list of strings
                    Names of independent variables for use in output
    name_yend     : list of strings
                    Names of endogenous variables for use in output
    name_z        : list of strings
                    Names of exogenous and endogenous variables for use in 
                    output
    name_q        : list of strings
                    Names of external instruments
    name_h        : list of strings
                    Names of all instruments used in ouput
    name_w        : string
                    Name of weights matrix for use in output
    name_ds       : string
                    Name of dataset for use in output
    title         : string
                    Name of the regression method used

    References
    ----------

    .. [1] Kelejian, H.R., Prucha, I.R. (1998) "A generalized spatial
        two-stage least squares procedure for estimating a spatial autoregressive
        model with autoregressive disturbances". The Journal of Real State
        Finance and Economics, 17, 1.

    .. [2] Kelejian, H.R., Prucha, I.R. (1999) "A Generalized Moments
        Estimator for the Autoregressive Parameter in a Spatial Model".
        International Economic Review, 40, 2.

    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import numpy as np
    >>> import pysal

    Open data on Columbus neighborhood crime (49 areas) using pysal.open().
    This is the DBF associated with the Columbus shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),'r')

    Extract the CRIME column (crime rates) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))

    Extract INC (income) vector from the DBF to be used as
    independent variables in the regression.  Note that PySAL requires this to
    be an nxj numpy array, where j is the number of independent variables (not
    including a constant). By default this model adds a vector of ones to the
    independent variables passed in.

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X = np.array(X).T

    Since we want to run a spatial error model, we need to specify the spatial
    weights matrix that includes the spatial configuration of the observations
    into the error component of the model. To do that, we can open an already
    existing gal file or create a new one. In this case, we will create one
    from ``columbus.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))

    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, this allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'

    The Combo class runs an SARAR model, that is a spatial lag+error model.
    In this case we will run a simple version of that, where we have the
    spatial effects as well as exogenous variables. Since it is a spatial
    model, we have to pass in the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.

    >>> reg = GM_Combo(y, X, w=w, name_y='crime', name_x=['income'], name_ds='columbus')

    Once we have run the model, we can explore a little bit the output. The
    regression object we have created has many attributes so take your time to
    discover them. Note that because we are running the classical GMM error
    model from 1998/99, the spatial parameter is obtained as a point estimate, so
    although you get a value for it (there are for coefficients under
    model.betas), you cannot perform inference on it (there are only three
    values in model.se_betas). Also, this regression uses a two stage least
    squares estimation method that accounts for the endogeneity created by the
    spatial lag of the dependent variable. We can check the betas:

    >>> print reg.name_z
    ['CONSTANT', 'income', 'W_crime', 'lambda']
    >>> print np.around(np.hstack((reg.betas[:-1],np.sqrt(reg.vm.diagonal()).reshape(3,1))),3)
    [[ 39.059  11.86 ]
     [ -1.404   0.391]
     [  0.467   0.2  ]]

    And lambda:

    >>> print 'lambda: ', np.around(reg.betas[-1], 3)
    lambda:  [-0.048]

    This class also allows the user to run a spatial lag+error model with the
    extra feature of including non-spatial endogenous regressors. This means
    that, in addition to the spatial lag and error, we consider some of the
    variables on the right-hand side of the equation as endogenous and we
    instrument for this. As an example, we will include HOVAL (home value) as
    endogenous and will instrument with DISCBD (distance to the CSB). We first
    need to read in the variables:

    >>> yd = []
    >>> yd.append(db.by_col("HOVAL"))
    >>> yd = np.array(yd).T
    >>> q = []
    >>> q.append(db.by_col("DISCBD"))
    >>> q = np.array(q).T

    And then we can run and explore the model analogously to the previous combo:

    >>> reg = GM_Combo(y, X, yd, q, w=w, name_x=['inc'], name_y='crime', name_yend=['hoval'], name_q=['discbd'], name_ds='columbus')
    >>> print reg.name_z
    ['CONSTANT', 'inc', 'hoval', 'W_crime', 'lambda']
    >>> names = np.array(reg.name_z).reshape(5,1)
    >>> print np.hstack((names[0:4,:], np.around(np.hstack((reg.betas[:-1], np.sqrt(reg.vm.diagonal()).reshape(4,1))),4)))
    [['CONSTANT' '50.0944' '14.3593']
     ['inc' '-0.2552' '0.5667']
     ['hoval' '-0.6885' '0.3029']
     ['W_crime' '0.4375' '0.2314']]

    >>> print 'lambda: ', np.around(reg.betas[-1], 3)
    lambda:  [ 0.254]

    """

    def __init__(self, y, x, yend=None, q=None,
                 w=None, w_lags=1, lag_q=True,
                 vm=False, name_y=None, name_x=None,
                 name_yend=None, name_q=None,
                 name_w=None, name_ds=None):

        n = USER.check_arrays(y, x, yend, q)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        yend2, q2 = set_endog(y, x, w, yend, q, w_lags, lag_q)
        x_constant = USER.check_constant(x)
        BaseGM_Combo.__init__(
            self, y=y, x=x_constant, w=w.sparse, yend=yend2, q=q2,
            w_lags=w_lags, lag_q=lag_q)
        self.rho = self.betas[-2]
        self.predy_e, self.e_pred, warn = sp_att(w, self.y,
                                                 self.predy, yend2[:, -1].reshape(self.n, 1), self.rho)
        set_warn(self, warn)
        self.title = "SPATIALLY WEIGHTED TWO STAGE LEAST SQUARES"
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_y = USER.set_name_y(name_y)
        self.name_x = USER.set_name_x(name_x, x)
        self.name_yend = USER.set_name_yend(name_yend, yend)
        self.name_yend.append(USER.set_name_yend_sp(self.name_y))
        self.name_z = self.name_x + self.name_yend
        self.name_z.append('lambda')
        self.name_q = USER.set_name_q(name_q, q)
        self.name_q.extend(
            USER.set_name_q_sp(self.name_x, w_lags, self.name_q, lag_q))
        self.name_h = USER.set_name_h(self.name_x, self.name_q)
        self.name_w = USER.set_name_w(name_w, w)
        SUMMARY.GM_Combo(reg=self, w=w, vm=vm)


def _momentsGM_Error(w, u):
    try:
        wsparse = w.sparse
    except:
        wsparse = w
    n = wsparse.shape[0]
    u2 = np.dot(u.T, u)
    wu = wsparse * u
    uwu = np.dot(u.T, wu)
    wu2 = np.dot(wu.T, wu)
    wwu = wsparse * wu
    uwwu = np.dot(u.T, wwu)
    wwu2 = np.dot(wwu.T, wwu)
    wuwwu = np.dot(wu.T, wwu)
    wtw = wsparse.T * wsparse
    trWtW = np.sum(wtw.diagonal())
    g = np.array([[u2[0][0], wu2[0][0], uwu[0][0]]]).T / n
    G = np.array(
        [[2 * uwu[0][0], -wu2[0][0], n], [2 * wuwwu[0][0], -wwu2[0][0], trWtW],
         [uwwu[0][0] + wu2[0][0], -wuwwu[0][0], 0.]]) / n
    return [G, g]


def _test():
    import doctest
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)

if __name__ == '__main__':

    _test()
    import pysal
    import numpy as np
    dbf = pysal.open(pysal.examples.get_path('columbus.dbf'), 'r')
    y = np.array([dbf.by_col('HOVAL')]).T
    names_to_extract = ['INC', 'CRIME']
    x = np.array([dbf.by_col(name) for name in names_to_extract]).T
    w = pysal.open(pysal.examples.get_path("columbus.gal"), 'r').read()
    w.transform = 'r'
    model = GM_Error(y, x, w, name_y='hoval',
                     name_x=['income', 'crime'], name_ds='columbus')
    print model.summary

########NEW FILE########
__FILENAME__ = error_sp_het
'''
Spatial Error with Heteroskedasticity family of models
'''

__author__ = "Luc Anselin luc.anselin@asu.edu, \
        Pedro V. Amaral pedro.amaral@asu.edu, \
        Daniel Arribas-Bel darribas@asu.edu, \
        David C. Folch david.folch@asu.edu \
        Ran Wei rwei5@asu.edu"

import numpy as np
import numpy.linalg as la
import ols as OLS
import user_output as USER
import summary_output as SUMMARY
import twosls as TSLS
import utils as UTILS
from utils import RegressionPropsY, spdot, set_endog, sphstack
from scipy import sparse as SP
from pysal import lag_spatial

__all__ = ["GM_Error_Het", "GM_Endog_Error_Het", "GM_Combo_Het"]


class BaseGM_Error_Het(RegressionPropsY):

    """
    GMM method for a spatial error model with heteroskedasticity (note: no
    consistency checks, diagnostics or constant added); based on Arraiz
    et al [1]_, following Anselin [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    w            : Sparse matrix
                   Spatial weights sparse matrix 
    max_iter     : int
                   Maximum number of iterations of steps 2a and 2b from Arraiz
                   et al. Note: epsilon provides an additional stop condition.
    epsilon      : float
                   Minimum change in lambda required to stop iterations of
                   steps 2a and 2b from Arraiz et al. Note: max_iter provides
                   an additional stop condition.
    step1c       : boolean
                   If True, then include Step 1c from Arraiz et al. 

    Attributes
    ----------
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    iter_stop    : string
                   Stop criterion reached during iteration of steps 2a and 2b
                   from Arraiz et al.
    iteration    : integer
                   Number of iterations of steps 2a and 2b from Arraiz et al.
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    xtx          : float
                   X'X

    References
    ----------

    .. [1] Arraiz, I., Drukker, D. M., Kelejian, H., Prucha, I. R. (2010) "A
    Spatial Cliff-Ord-Type Model with Heteroskedastic Innovations: Small and
    Large Sample Results". Journal of Regional Science, Vol. 60, No. 2, pp.
    592-614.

    .. [2] Anselin, L. GMM Estimation of Spatial Error Autocorrelation with Heteroskedasticity

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    >>> y = np.array(db.by_col("HOVAL"))
    >>> y = np.reshape(y, (49,1))
    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("CRIME"))
    >>> X = np.array(X).T
    >>> X = np.hstack((np.ones(y.shape),X))
    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
    >>> w.transform = 'r'
    >>> reg = BaseGM_Error_Het(y, X, w.sparse, step1c=True)
    >>> print np.around(np.hstack((reg.betas,np.sqrt(reg.vm.diagonal()).reshape(4,1))),4)
    [[ 47.9963  11.479 ]
     [  0.7105   0.3681]
     [ -0.5588   0.1616]
     [  0.4118   0.168 ]]
    """

    def __init__(self, y, x, w,
                 max_iter=1, epsilon=0.00001, step1c=False):

        self.step1c = step1c
        # 1a. OLS --> \tilde{betas}
        ols = OLS.BaseOLS(y=y, x=x)
        self.x, self.y, self.n, self.k, self.xtx = ols.x, ols.y, ols.n, ols.k, ols.xtx
        wA1 = UTILS.get_A1_het(w)

        # 1b. GMM --> \tilde{\lambda1}
        moments = UTILS._moments2eqs(wA1, w, ols.u)
        lambda1 = UTILS.optim_moments(moments)

        if step1c:
            # 1c. GMM --> \tilde{\lambda2}
            sigma = get_psi_sigma(w, ols.u, lambda1)
            vc1 = get_vc_het(w, wA1, sigma)
            lambda2 = UTILS.optim_moments(moments, vc1)
        else:
            lambda2 = lambda1
        lambda_old = lambda2

        self.iteration, eps = 0, 1
        while self.iteration < max_iter and eps > epsilon:
            # 2a. reg -->\hat{betas}
            xs = UTILS.get_spFilter(w, lambda_old, self.x)
            ys = UTILS.get_spFilter(w, lambda_old, self.y)
            ols_s = OLS.BaseOLS(y=ys, x=xs)
            self.predy = spdot(self.x, ols_s.betas)
            self.u = self.y - self.predy

            # 2b. GMM --> \hat{\lambda}
            sigma_i = get_psi_sigma(w, self.u, lambda_old)
            vc_i = get_vc_het(w, wA1, sigma_i)
            moments_i = UTILS._moments2eqs(wA1, w, self.u)
            lambda3 = UTILS.optim_moments(moments_i, vc_i)
            eps = abs(lambda3 - lambda_old)
            lambda_old = lambda3
            self.iteration += 1

        self.iter_stop = UTILS.iter_msg(self.iteration, max_iter)

        sigma = get_psi_sigma(w, self.u, lambda3)
        vc3 = get_vc_het(w, wA1, sigma)
        self.vm = get_vm_het(moments_i[0], lambda3, self, w, vc3)
        self.betas = np.vstack((ols_s.betas, lambda3))
        self.e_filtered = self.u - lambda3 * w * self.u
        self._cache = {}


class GM_Error_Het(BaseGM_Error_Het):

    """
    GMM method for a spatial error model with heteroskedasticity, with results
    and diagnostics; based on Arraiz et al [1]_, following Anselin [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    w            : pysal W object
                   Spatial weights object   
    max_iter     : int
                   Maximum number of iterations of steps 2a and 2b from Arraiz
                   et al. Note: epsilon provides an additional stop condition.
    epsilon      : float
                   Minimum change in lambda required to stop iterations of
                   steps 2a and 2b from Arraiz et al. Note: max_iter provides
                   an additional stop condition.
    step1c       : boolean
                   If True, then include Step 1c from Arraiz et al. 
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    iter_stop    : string
                   Stop criterion reached during iteration of steps 2a and 2b
                   from Arraiz et al.
    iteration    : integer
                   Number of iterations of steps 2a and 2b from Arraiz et al.
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
    vm           : array
                   Variance covariance matrix (kxk)
    std_err      : array
                   1xk array of standard errors of the betas    
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
    xtx          : float
                   X'X
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    title        : string
                   Name of the regression method used

    References
    ----------

    .. [1] Arraiz, I., Drukker, D. M., Kelejian, H., Prucha, I. R. (2010) "A
        Spatial Cliff-Ord-Type Model with Heteroskedastic Innovations: Small and
        Large Sample Results". Journal of Regional Science, Vol. 60, No. 2, pp.
        592-614.

    .. [2] Anselin, L. GMM Estimation of Spatial Error Autocorrelation with Heteroskedasticity

    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import numpy as np
    >>> import pysal

    Open data on Columbus neighborhood crime (49 areas) using pysal.open().
    This is the DBF associated with the Columbus shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')

    Extract the HOVAL column (home values) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y = np.array(db.by_col("HOVAL"))
    >>> y = np.reshape(y, (49,1))

    Extract INC (income) and CRIME (crime) vectors from the DBF to be used as
    independent variables in the regression.  Note that PySAL requires this to
    be an nxj numpy array, where j is the number of independent variables (not
    including a constant). By default this class adds a vector of ones to the
    independent variables passed in.

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("CRIME"))
    >>> X = np.array(X).T

    Since we want to run a spatial error model, we need to specify the spatial
    weights matrix that includes the spatial configuration of the observations
    into the error component of the model. To do that, we can open an already
    existing gal file or create a new one. In this case, we will create one
    from ``columbus.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))

    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, his allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'

    We are all set with the preliminaries, we are good to run the model. In this
    case, we will need the variables and the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.

    >>> reg = GM_Error_Het(y, X, w=w, step1c=True, name_y='home value', name_x=['income', 'crime'], name_ds='columbus')

    Once we have run the model, we can explore a little bit the output. The
    regression object we have created has many attributes so take your time to
    discover them. This class offers an error model that explicitly accounts
    for heteroskedasticity and that unlike the models from
    ``pysal.spreg.error_sp``, it allows for inference on the spatial
    parameter.

    >>> print reg.name_x
    ['CONSTANT', 'income', 'crime', 'lambda']

    Hence, we find the same number of betas as of standard errors,
    which we calculate taking the square root of the diagonal of the
    variance-covariance matrix:

    >>> print np.around(np.hstack((reg.betas,np.sqrt(reg.vm.diagonal()).reshape(4,1))),4)
    [[ 47.9963  11.479 ]
     [  0.7105   0.3681]
     [ -0.5588   0.1616]
     [  0.4118   0.168 ]]

    """

    def __init__(self, y, x, w,
                 max_iter=1, epsilon=0.00001, step1c=False,
                 vm=False, name_y=None, name_x=None,
                 name_w=None, name_ds=None):

        n = USER.check_arrays(y, x)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        x_constant = USER.check_constant(x)
        BaseGM_Error_Het.__init__(
            self, y, x_constant, w.sparse, max_iter=max_iter,
            step1c=step1c, epsilon=epsilon)
        self.title = "SPATIALLY WEIGHTED LEAST SQUARES (HET)"
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_y = USER.set_name_y(name_y)
        self.name_x = USER.set_name_x(name_x, x)
        self.name_x.append('lambda')
        self.name_w = USER.set_name_w(name_w, w)
        SUMMARY.GM_Error_Het(reg=self, w=w, vm=vm)


class BaseGM_Endog_Error_Het(RegressionPropsY):

    """
    GMM method for a spatial error model with heteroskedasticity and
    endogenous variables (note: no consistency checks, diagnostics or constant
    added); based on Arraiz et al [1]_, following Anselin [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note: 
                   this should not contain any variables from x)
    w            : Sparse matrix
                   Spatial weights sparse matrix   
    max_iter     : int
                   Maximum number of iterations of steps 2a and 2b from Arraiz
                   et al. Note: epsilon provides an additional stop condition.
    epsilon      : float
                   Minimum change in lambda required to stop iterations of
                   steps 2a and 2b from Arraiz et al. Note: max_iter provides
                   an additional stop condition.
    step1c       : boolean
                   If True, then include Step 1c from Arraiz et al. 
    inv_method   : string
                   If "power_exp", then compute inverse using the power
                   expansion. If "true_inv", then compute the true inverse.
                   Note that true_inv will fail for large n.


    Attributes
    ----------
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable used as instruments 
    z            : array
                   nxk array of variables (combination of x and yend)
    h            : array
                   nxl array of instruments (combination of x and q)
    iter_stop    : string
                   Stop criterion reached during iteration of steps 2a and 2b
                   from Arraiz et al.
    iteration    : integer
                   Number of iterations of steps 2a and 2b from Arraiz et al.
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    hth          : float
                   H'H


    References
    ----------

    .. [1] Arraiz, I., Drukker, D. M., Kelejian, H., Prucha, I. R. (2010) "A
    Spatial Cliff-Ord-Type Model with Heteroskedastic Innovations: Small and
    Large Sample Results". Journal of Regional Science, Vol. 60, No. 2, pp.
    592-614.

    .. [2] Anselin, L. GMM Estimation of Spatial Error Autocorrelation with Heteroskedasticity

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    >>> y = np.array(db.by_col("HOVAL"))
    >>> y = np.reshape(y, (49,1))
    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X = np.array(X).T
    >>> X = np.hstack((np.ones(y.shape),X))
    >>> yd = []
    >>> yd.append(db.by_col("CRIME"))
    >>> yd = np.array(yd).T
    >>> q = []
    >>> q.append(db.by_col("DISCBD"))
    >>> q = np.array(q).T
    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
    >>> w.transform = 'r'
    >>> reg = BaseGM_Endog_Error_Het(y, X, yd, q, w=w.sparse, step1c=True)
    >>> print np.around(np.hstack((reg.betas,np.sqrt(reg.vm.diagonal()).reshape(4,1))),4)
    [[ 55.3971  28.8901]
     [  0.4656   0.7731]
     [ -0.6704   0.468 ]
     [  0.4114   0.1777]]
    """

    def __init__(self, y, x, yend, q, w,
                 max_iter=1, epsilon=0.00001,
                 step1c=False, inv_method='power_exp'):

        self.step1c = step1c
        # 1a. reg --> \tilde{betas}
        tsls = TSLS.BaseTSLS(y=y, x=x, yend=yend, q=q)
        self.x, self.z, self.h, self.y = tsls.x, tsls.z, tsls.h, tsls.y
        self.yend, self.q, self.n, self.k, self.hth = tsls.yend, tsls.q, tsls.n, tsls.k, tsls.hth
        wA1 = UTILS.get_A1_het(w)

        # 1b. GMM --> \tilde{\lambda1}
        moments = UTILS._moments2eqs(wA1, w, tsls.u)
        lambda1 = UTILS.optim_moments(moments)

        if step1c:
            # 1c. GMM --> \tilde{\lambda2}
            self.u = tsls.u
            zs = UTILS.get_spFilter(w, lambda1, self.z)
            vc1 = get_vc_het_tsls(w, wA1, self, lambda1,
                                  tsls.pfora1a2, zs, inv_method, filt=False)
            lambda2 = UTILS.optim_moments(moments, vc1)
        else:
            lambda2 = lambda1
        lambda_old = lambda2

        self.iteration, eps = 0, 1
        while self.iteration < max_iter and eps > epsilon:
            # 2a. reg -->\hat{betas}
            xs = UTILS.get_spFilter(w, lambda_old, self.x)
            ys = UTILS.get_spFilter(w, lambda_old, self.y)
            yend_s = UTILS.get_spFilter(w, lambda_old, self.yend)
            tsls_s = TSLS.BaseTSLS(ys, xs, yend_s, h=self.h)
            self.predy = spdot(self.z, tsls_s.betas)
            self.u = self.y - self.predy

            # 2b. GMM --> \hat{\lambda}
            vc2 = get_vc_het_tsls(w, wA1, self, lambda_old,
                                  tsls_s.pfora1a2, sphstack(xs, yend_s), inv_method)
            moments_i = UTILS._moments2eqs(wA1, w, self.u)
            lambda3 = UTILS.optim_moments(moments_i, vc2)
            eps = abs(lambda3 - lambda_old)
            lambda_old = lambda3
            self.iteration += 1

        self.iter_stop = UTILS.iter_msg(self.iteration, max_iter)

        zs = UTILS.get_spFilter(w, lambda3, self.z)
        P = get_P_hat(self, tsls.hthi, zs)
        vc3 = get_vc_het_tsls(w, wA1, self, lambda3, P,
                              zs, inv_method, save_a1a2=True)
        self.vm = get_Omega_GS2SLS(w, lambda3, self, moments_i[0], vc3, P)
        self.betas = np.vstack((tsls_s.betas, lambda3))
        self.e_filtered = self.u - lambda3 * w * self.u
        self._cache = {}


class GM_Endog_Error_Het(BaseGM_Endog_Error_Het):

    """
    GMM method for a spatial error model with heteroskedasticity and
    endogenous variables, with results and diagnostics; based on Arraiz et al
    [1]_, following Anselin [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note: 
                   this should not contain any variables from x)
    w            : pysal W object
                   Spatial weights object   
    max_iter     : int
                   Maximum number of iterations of steps 2a and 2b from Arraiz
                   et al. Note: epsilon provides an additional stop condition.
    epsilon      : float
                   Minimum change in lambda required to stop iterations of
                   steps 2a and 2b from Arraiz et al. Note: max_iter provides
                   an additional stop condition.
    step1c       : boolean
                   If True, then include Step 1c from Arraiz et al. 
    inv_method   : string
                   If "power_exp", then compute inverse using the power
                   expansion. If "true_inv", then compute the true inverse.
                   Note that true_inv will fail for large n.
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_yend    : list of strings
                   Names of endogenous variables for use in output
    name_q       : list of strings
                   Names of instruments for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable used as instruments 
    z            : array
                   nxk array of variables (combination of x and yend)
    h            : array
                   nxl array of instruments (combination of x and q)
    iter_stop    : string
                   Stop criterion reached during iteration of steps 2a and 2b
                   from Arraiz et al.
    iteration    : integer
                   Number of iterations of steps 2a and 2b from Arraiz et al.
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
    std_err      : array
                   1xk array of standard errors of the betas    
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
    name_y        : string
                    Name of dependent variable for use in output
    name_x        : list of strings
                    Names of independent variables for use in output
    name_yend     : list of strings
                    Names of endogenous variables for use in output
    name_z        : list of strings
                    Names of exogenous and endogenous variables for use in 
                    output
    name_q        : list of strings
                    Names of external instruments
    name_h        : list of strings
                    Names of all instruments used in ouput
    name_w        : string
                    Name of weights matrix for use in output
    name_ds       : string
                    Name of dataset for use in output
    title         : string
                    Name of the regression method used
    hth          : float
                   H'H

    References
    ----------

    .. [1] Arraiz, I., Drukker, D. M., Kelejian, H., Prucha, I. R. (2010) "A
        Spatial Cliff-Ord-Type Model with Heteroskedastic Innovations: Small and
        Large Sample Results". Journal of Regional Science, Vol. 60, No. 2, pp.
        592-614.

    .. [2] Anselin, L. GMM Estimation of Spatial Error Autocorrelation with Heteroskedasticity

    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import numpy as np
    >>> import pysal

    Open data on Columbus neighborhood crime (49 areas) using pysal.open().
    This is the DBF associated with the Columbus shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')

    Extract the HOVAL column (home values) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y = np.array(db.by_col("HOVAL"))
    >>> y = np.reshape(y, (49,1))

    Extract INC (income) vector from the DBF to be used as
    independent variables in the regression.  Note that PySAL requires this to
    be an nxj numpy array, where j is the number of independent variables (not
    including a constant). By default this class adds a vector of ones to the
    independent variables passed in.

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X = np.array(X).T

    In this case we consider CRIME (crime rates) is an endogenous regressor.
    We tell the model that this is so by passing it in a different parameter
    from the exogenous variables (x).

    >>> yd = []
    >>> yd.append(db.by_col("CRIME"))
    >>> yd = np.array(yd).T

    Because we have endogenous variables, to obtain a correct estimate of the
    model, we need to instrument for CRIME. We use DISCBD (distance to the
    CBD) for this and hence put it in the instruments parameter, 'q'.

    >>> q = []
    >>> q.append(db.by_col("DISCBD"))
    >>> q = np.array(q).T

    Since we want to run a spatial error model, we need to specify the spatial
    weights matrix that includes the spatial configuration of the observations
    into the error component of the model. To do that, we can open an already
    existing gal file or create a new one. In this case, we will create one
    from ``columbus.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))

    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, his allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'

    We are all set with the preliminaries, we are good to run the model. In this
    case, we will need the variables (exogenous and endogenous), the
    instruments and the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.

    >>> reg = GM_Endog_Error_Het(y, X, yd, q, w=w, step1c=True, name_x=['inc'], name_y='hoval', name_yend=['crime'], name_q=['discbd'], name_ds='columbus')

    Once we have run the model, we can explore a little bit the output. The
    regression object we have created has many attributes so take your time to
    discover them. This class offers an error model that explicitly accounts
    for heteroskedasticity and that unlike the models from
    ``pysal.spreg.error_sp``, it allows for inference on the spatial
    parameter. Hence, we find the same number of betas as of standard errors,
    which we calculate taking the square root of the diagonal of the
    variance-covariance matrix:

    >>> print reg.name_z
    ['CONSTANT', 'inc', 'crime', 'lambda']
    >>> print np.around(np.hstack((reg.betas,np.sqrt(reg.vm.diagonal()).reshape(4,1))),4)
    [[ 55.3971  28.8901]
     [  0.4656   0.7731]
     [ -0.6704   0.468 ]
     [  0.4114   0.1777]]

    """

    def __init__(self, y, x, yend, q, w,
                 max_iter=1, epsilon=0.00001,
                 step1c=False, inv_method='power_exp',
                 vm=False, name_y=None, name_x=None,
                 name_yend=None, name_q=None,
                 name_w=None, name_ds=None):

        n = USER.check_arrays(y, x, yend, q)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        x_constant = USER.check_constant(x)
        BaseGM_Endog_Error_Het.__init__(self, y=y, x=x_constant, yend=yend,
                                        q=q, w=w.sparse, max_iter=max_iter,
                                        step1c=step1c, epsilon=epsilon, inv_method=inv_method)
        self.title = "SPATIALLY WEIGHTED TWO STAGE LEAST SQUARES (HET)"
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_y = USER.set_name_y(name_y)
        self.name_x = USER.set_name_x(name_x, x)
        self.name_yend = USER.set_name_yend(name_yend, yend)
        self.name_z = self.name_x + self.name_yend
        self.name_z.append('lambda')  # listing lambda last
        self.name_q = USER.set_name_q(name_q, q)
        self.name_h = USER.set_name_h(self.name_x, self.name_q)
        self.name_w = USER.set_name_w(name_w, w)
        SUMMARY.GM_Endog_Error_Het(reg=self, w=w, vm=vm)


class BaseGM_Combo_Het(BaseGM_Endog_Error_Het):

    """
    GMM method for a spatial lag and error model with heteroskedasticity and
    endogenous variables (note: no consistency checks, diagnostics or constant
    added); based on Arraiz et al [1]_, following Anselin [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note: 
                   this should not contain any variables from x)
    w            : Sparse matrix
                   Spatial weights sparse matrix 
    w_lags       : integer
                   Orders of W to include as instruments for the spatially
                   lagged dependent variable. For example, w_lags=1, then
                   instruments are WX; if w_lags=2, then WX, WWX; and so on.
    lag_q        : boolean
                   If True, then include spatial lags of the additional 
                   instruments (q).
    max_iter     : int
                   Maximum number of iterations of steps 2a and 2b from Arraiz
                   et al. Note: epsilon provides an additional stop condition.
    epsilon      : float
                   Minimum change in lambda required to stop iterations of
                   steps 2a and 2b from Arraiz et al. Note: max_iter provides
                   an additional stop condition.
    step1c       : boolean
                   If True, then include Step 1c from Arraiz et al. 
    inv_method   : string
                   If "power_exp", then compute inverse using the power
                   expansion. If "true_inv", then compute the true inverse.
                   Note that true_inv will fail for large n.


    Attributes
    ----------
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable used as instruments 
    z            : array
                   nxk array of variables (combination of x and yend)
    h            : array
                   nxl array of instruments (combination of x and q)
    iter_stop    : string
                   Stop criterion reached during iteration of steps 2a and 2b
                   from Arraiz et al.
    iteration    : integer
                   Number of iterations of steps 2a and 2b from Arraiz et al.
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    hth          : float
                   H'H

    References
    ----------

    .. [1] Arraiz, I., Drukker, D. M., Kelejian, H., Prucha, I. R. (2010) "A
    Spatial Cliff-Ord-Type Model with Heteroskedastic Innovations: Small and
    Large Sample Results". Journal of Regional Science, Vol. 60, No. 2, pp.
    592-614.

    .. [2] Anselin, L. GMM Estimation of Spatial Error Autocorrelation with Heteroskedasticity

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    >>> y = np.array(db.by_col("HOVAL"))
    >>> y = np.reshape(y, (49,1))
    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X = np.array(X).T
    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
    >>> w.transform = 'r'
    >>> w_lags = 1
    >>> yd2, q2 = pysal.spreg.utils.set_endog(y, X, w, None, None, w_lags, True)
    >>> X = np.hstack((np.ones(y.shape),X))

    Example only with spatial lag

    >>> reg = BaseGM_Combo_Het(y, X, yend=yd2, q=q2, w=w.sparse, step1c=True)
    >>> print np.around(np.hstack((reg.betas,np.sqrt(reg.vm.diagonal()).reshape(4,1))),4)
    [[  9.9753  14.1435]
     [  1.5742   0.374 ]
     [  0.1535   0.3978]
     [  0.2103   0.3924]]

    Example with both spatial lag and other endogenous variables

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X = np.array(X).T
    >>> yd = []
    >>> yd.append(db.by_col("CRIME"))
    >>> yd = np.array(yd).T
    >>> q = []
    >>> q.append(db.by_col("DISCBD"))
    >>> q = np.array(q).T
    >>> yd2, q2 = pysal.spreg.utils.set_endog(y, X, w, yd, q, w_lags, True)
    >>> X = np.hstack((np.ones(y.shape),X))
    >>> reg = BaseGM_Combo_Het(y, X, yd2, q2, w=w.sparse, step1c=True)
    >>> betas = np.array([['CONSTANT'],['inc'],['crime'],['lag_hoval'],['lambda']])
    >>> print np.hstack((betas, np.around(np.hstack((reg.betas, np.sqrt(reg.vm.diagonal()).reshape(5,1))),5)))
    [['CONSTANT' '113.91292' '64.38815']
     ['inc' '-0.34822' '1.18219']
     ['crime' '-1.35656' '0.72482']
     ['lag_hoval' '-0.57657' '0.75856']
     ['lambda' '0.65608' '0.15719']]
    """

    def __init__(self, y, x, yend=None, q=None,
                 w=None, w_lags=1, lag_q=True,
                 max_iter=1, epsilon=0.00001,
                 step1c=False, inv_method='power_exp'):

        BaseGM_Endog_Error_Het.__init__(
            self, y=y, x=x, w=w, yend=yend, q=q, max_iter=max_iter,
            step1c=step1c, epsilon=epsilon, inv_method=inv_method)


class GM_Combo_Het(BaseGM_Combo_Het):

    """
    GMM method for a spatial lag and error model with heteroskedasticity and
    endogenous variables, with results and diagnostics; based on Arraiz et al
    [1]_, following Anselin [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note: 
                   this should not contain any variables from x)
    w            : pysal W object
                   Spatial weights object (always needed)   
    w_lags       : integer
                   Orders of W to include as instruments for the spatially
                   lagged dependent variable. For example, w_lags=1, then
                   instruments are WX; if w_lags=2, then WX, WWX; and so on.
    lag_q        : boolean
                   If True, then include spatial lags of the additional 
                   instruments (q).
    max_iter     : int
                   Maximum number of iterations of steps 2a and 2b from Arraiz
                   et al. Note: epsilon provides an additional stop condition.
    epsilon      : float
                   Minimum change in lambda required to stop iterations of
                   steps 2a and 2b from Arraiz et al. Note: max_iter provides
                   an additional stop condition.
    step1c       : boolean
                   If True, then include Step 1c from Arraiz et al. 
    inv_method   : string
                   If "power_exp", then compute inverse using the power
                   expansion. If "true_inv", then compute the true inverse.
                   Note that true_inv will fail for large n.
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_yend    : list of strings
                   Names of endogenous variables for use in output
    name_q       : list of strings
                   Names of instruments for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    e_pred       : array
                   nx1 array of residuals (using reduced form)
    predy        : array
                   nx1 array of predicted y values
    predy_e      : array
                   nx1 array of predicted y values (using reduced form)
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable used as instruments 
    z            : array
                   nxk array of variables (combination of x and yend)
    h            : array
                   nxl array of instruments (combination of x and q)
    iter_stop    : string
                   Stop criterion reached during iteration of steps 2a and 2b
                   from Arraiz et al.
    iteration    : integer
                   Number of iterations of steps 2a and 2b from Arraiz et al.
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
    pr2_e        : float
                   Pseudo R squared (squared correlation between y and ypred_e
                   (using reduced form))
    std_err      : array
                   1xk array of standard errors of the betas    
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
    name_y        : string
                    Name of dependent variable for use in output
    name_x        : list of strings
                    Names of independent variables for use in output
    name_yend     : list of strings
                    Names of endogenous variables for use in output
    name_z        : list of strings
                    Names of exogenous and endogenous variables for use in 
                    output
    name_q        : list of strings
                    Names of external instruments
    name_h        : list of strings
                    Names of all instruments used in ouput
    name_w        : string
                    Name of weights matrix for use in output
    name_ds       : string
                    Name of dataset for use in output
    title         : string
                    Name of the regression method used
    hth          : float
                   H'H

    References
    ----------

    .. [1] Arraiz, I., Drukker, D. M., Kelejian, H., Prucha, I. R. (2010) "A
        Spatial Cliff-Ord-Type Model with Heteroskedastic Innovations: Small and
        Large Sample Results". Journal of Regional Science, Vol. 60, No. 2, pp.
        592-614.

    .. [2] Anselin, L. GMM Estimation of Spatial Error Autocorrelation with Heteroskedasticity

    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import numpy as np
    >>> import pysal

    Open data on Columbus neighborhood crime (49 areas) using pysal.open().
    This is the DBF associated with the Columbus shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    
    Extract the HOVAL column (home values) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y = np.array(db.by_col("HOVAL"))
    >>> y = np.reshape(y, (49,1))

    Extract INC (income) vector from the DBF to be used as
    independent variables in the regression.  Note that PySAL requires this to
    be an nxj numpy array, where j is the number of independent variables (not
    including a constant). By default this class adds a vector of ones to the
    independent variables passed in.

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X = np.array(X).T

    Since we want to run a spatial error model, we need to specify the spatial
    weights matrix that includes the spatial configuration of the observations
    into the error component of the model. To do that, we can open an already
    existing gal file or create a new one. In this case, we will create one
    from ``columbus.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
    
    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, his allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'

    The Combo class runs an SARAR model, that is a spatial lag+error model.
    In this case we will run a simple version of that, where we have the
    spatial effects as well as exogenous variables. Since it is a spatial
    model, we have to pass in the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.

    >>> reg = GM_Combo_Het(y, X, w=w, step1c=True, name_y='hoval', name_x=['income'], name_ds='columbus')
   
    Once we have run the model, we can explore a little bit the output. The
    regression object we have created has many attributes so take your time to
    discover them. This class offers an error model that explicitly accounts
    for heteroskedasticity and that unlike the models from
    ``pysal.spreg.error_sp``, it allows for inference on the spatial
    parameter. Hence, we find the same number of betas as of standard errors,
    which we calculate taking the square root of the diagonal of the
    variance-covariance matrix:

    >>> print reg.name_z
    ['CONSTANT', 'income', 'W_hoval', 'lambda']
    >>> print np.around(np.hstack((reg.betas,np.sqrt(reg.vm.diagonal()).reshape(4,1))),4)
    [[  9.9753  14.1435]
     [  1.5742   0.374 ]
     [  0.1535   0.3978]
     [  0.2103   0.3924]]
        
    This class also allows the user to run a spatial lag+error model with the
    extra feature of including non-spatial endogenous regressors. This means
    that, in addition to the spatial lag and error, we consider some of the
    variables on the right-hand side of the equation as endogenous and we
    instrument for this. As an example, we will include CRIME (crime rates) as
    endogenous and will instrument with DISCBD (distance to the CSB). We first
    need to read in the variables:

    >>> yd = []
    >>> yd.append(db.by_col("CRIME"))
    >>> yd = np.array(yd).T
    >>> q = []
    >>> q.append(db.by_col("DISCBD"))
    >>> q = np.array(q).T

    And then we can run and explore the model analogously to the previous combo:

    >>> reg = GM_Combo_Het(y, X, yd, q, w=w, step1c=True, name_x=['inc'], name_y='hoval', name_yend=['crime'], name_q=['discbd'], name_ds='columbus')
    >>> print reg.name_z
    ['CONSTANT', 'inc', 'crime', 'W_hoval', 'lambda']
    >>> print np.round(reg.betas,4)
    [[ 113.9129]
     [  -0.3482]
     [  -1.3566]
     [  -0.5766]
     [   0.6561]]
    
    """

    def __init__(self, y, x, yend=None, q=None,
                 w=None, w_lags=1, lag_q=True,
                 max_iter=1, epsilon=0.00001,
                 step1c=False, inv_method='power_exp',
                 vm=False, name_y=None, name_x=None,
                 name_yend=None, name_q=None,
                 name_w=None, name_ds=None):

        n = USER.check_arrays(y, x, yend, q)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        yend2, q2 = set_endog(y, x, w, yend, q, w_lags, lag_q)
        x_constant = USER.check_constant(x)
        BaseGM_Combo_Het.__init__(self, y=y, x=x_constant, yend=yend2, q=q2,
                                  w=w.sparse, w_lags=w_lags,
                                  max_iter=max_iter, step1c=step1c, lag_q=lag_q,
                                  epsilon=epsilon, inv_method=inv_method)
        self.rho = self.betas[-2]
        self.predy_e, self.e_pred, warn = UTILS.sp_att(w, self.y, self.predy,
                                                       yend2[:, -1].reshape(self.n, 1), self.rho)
        UTILS.set_warn(self, warn)
        self.title = "SPATIALLY WEIGHTED TWO STAGE LEAST SQUARES (HET)"
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_y = USER.set_name_y(name_y)
        self.name_x = USER.set_name_x(name_x, x)
        self.name_yend = USER.set_name_yend(name_yend, yend)
        self.name_yend.append(USER.set_name_yend_sp(self.name_y))
        self.name_z = self.name_x + self.name_yend
        self.name_z.append('lambda')  # listing lambda last
        self.name_q = USER.set_name_q(name_q, q)
        self.name_q.extend(
            USER.set_name_q_sp(self.name_x, w_lags, self.name_q, lag_q))
        self.name_h = USER.set_name_h(self.name_x, self.name_q)
        self.name_w = USER.set_name_w(name_w, w)
        SUMMARY.GM_Combo_Het(reg=self, w=w, vm=vm)


# Functions

def get_psi_sigma(w, u, lamb):
    """
    Computes the Sigma matrix needed to compute Psi

    Parameters
    ----------
    w           : Sparse matrix
                  Spatial weights sparse matrix
    u           : array
                  nx1 vector of residuals
    lamb        : float
                  Lambda

    """

    e = (u - lamb * (w * u)) ** 2
    E = SP.dia_matrix((e.flat, 0), shape=(w.shape[0], w.shape[0]))
    return E.tocsr()


def get_vc_het(w, wA1, E):
    """
    Computes the VC matrix Psi based on lambda as in Arraiz et al [1]_:

    ..math::

        \tilde{Psi} = \left(\begin{array}{c c}
                            \psi_{11} & \psi_{12} \\
                            \psi_{21} & \psi_{22} \\
                      \end{array} \right)

    NOTE: psi12=psi21

    ...

    Parameters
    ----------

    w           : Sparse matrix
                  Spatial weights sparse matrix

    E           : sparse matrix
                  Sigma
 
    Returns
    -------

    Psi         : array
                  2x2 array with estimator of the variance-covariance matrix

    References
    ----------

    .. [1] Arraiz, I., Drukker, D. M., Kelejian, H., Prucha, I. R. (2010) "A
    Spatial Cliff-Ord-Type Model with Heteroskedastic Innovations: Small and
    Large Sample Results". Journal of Regional Science, Vol. 60, No. 2, pp.
    592-614.

    """
    aPatE = 2 * wA1 * E
    wPwtE = (w + w.T) * E

    psi11 = aPatE * aPatE
    psi12 = aPatE * wPwtE
    psi22 = wPwtE * wPwtE
    psi = map(np.sum, [psi11.diagonal(), psi12.diagonal(), psi22.diagonal()])
    return np.array([[psi[0], psi[1]], [psi[1], psi[2]]]) / (2. * w.shape[0])


def get_vm_het(G, lamb, reg, w, psi):
    """
    Computes the variance-covariance matrix Omega as in Arraiz et al [1]_:
    ...

    Parameters
    ----------

    G           : array
                  G from moments equations

    lamb        : float
                  Final lambda from spHetErr estimation

    reg         : regression object
                  output instance from a regression model

    u           : array
                  nx1 vector of residuals

    w           : Sparse matrix
                  Spatial weights sparse matrix

    psi         : array
                  2x2 array with the variance-covariance matrix of the moment equations
 
    Returns
    -------

    vm          : array
                  (k+1)x(k+1) array with the variance-covariance matrix of the parameters

    References
    ----------

    .. [1] Arraiz, I., Drukker, D. M., Kelejian, H., Prucha, I. R. (2010) "A
    Spatial Cliff-Ord-Type Model with Heteroskedastic Innovations: Small and
    Large Sample Results". Journal of Regional Science, Vol. 60, No. 2, pp.
    592-614.

    """

    J = np.dot(G, np.array([[1], [2 * lamb]]))
    Zs = UTILS.get_spFilter(w, lamb, reg.x)
    ZstEZs = spdot((Zs.T * get_psi_sigma(w, reg.u, lamb)), Zs)
    ZsZsi = la.inv(spdot(Zs.T, Zs))
    omega11 = w.shape[0] * np.dot(np.dot(ZsZsi, ZstEZs), ZsZsi)
    omega22 = la.inv(np.dot(np.dot(J.T, la.inv(psi)), J))
    zero = np.zeros((reg.k, 1), float)
    vm = np.vstack((np.hstack((omega11, zero)), np.hstack((zero.T, omega22)))) / \
        w.shape[0]
    return vm


def get_P_hat(reg, hthi, zf):
    """
    P_hat from Appendix B, used for a1 a2, using filtered Z
    """
    htzf = spdot(reg.h.T, zf)
    P1 = spdot(hthi, htzf)
    P2 = spdot(htzf.T, P1)
    P2i = la.inv(P2)
    return reg.n * np.dot(P1, P2i)


def get_a1a2(w, wA1, reg, lambdapar, P, zs, inv_method, filt):
    """
    Computes the a1 in psi assuming residuals come from original regression
    ...

    Parameters
    ----------

    w           : Sparse matrix
                  Spatial weights sparse matrix 

    reg         : TSLS
                  Two stage least quare regression instance
                  
    lambdapar   : float
                  Spatial autoregressive parameter
 
    Returns
    -------

    [a1, a2]    : list
                  a1 and a2 are two nx1 array in psi equation

    References
    ----------

    .. [1] Anselin, L. GMM Estimation of Spatial Error Autocorrelation with Heteroskedasticity
    
    """
    us = UTILS.get_spFilter(w, lambdapar, reg.u)
    alpha1 = (-2.0 / w.shape[0]) * (np.dot(spdot(zs.T, wA1), us))
    alpha2 = (-1.0 / w.shape[0]) * (np.dot(spdot(zs.T, (w + w.T)), us))
    a1 = np.dot(spdot(reg.h, P), alpha1)
    a2 = np.dot(spdot(reg.h, P), alpha2)
    if not filt:
        a1 = UTILS.inverse_prod(
            w, a1, lambdapar, post_multiply=True, inv_method=inv_method).T
        a2 = UTILS.inverse_prod(
            w, a2, lambdapar, post_multiply=True, inv_method=inv_method).T
    return [a1, a2]


def get_vc_het_tsls(w, wA1, reg, lambdapar, P, zs, inv_method, filt=True, save_a1a2=False):

    sigma = get_psi_sigma(w, reg.u, lambdapar)
    vc1 = get_vc_het(w, wA1, sigma)
    a1, a2 = get_a1a2(w, wA1, reg, lambdapar, P, zs, inv_method, filt)
    a1s = a1.T * sigma
    a2s = a2.T * sigma
    psi11 = float(np.dot(a1s, a1))
    psi12 = float(np.dot(a1s, a2))
    psi21 = float(np.dot(a2s, a1))
    psi22 = float(np.dot(a2s, a2))
    psi0 = np.array([[psi11, psi12], [psi21, psi22]]) / w.shape[0]
    if save_a1a2:
        psi = (vc1 + psi0, a1, a2)
    else:
        psi = vc1 + psi0
    return psi


def get_Omega_GS2SLS(w, lamb, reg, G, psi, P):
    """
    Computes the variance-covariance matrix for GS2SLS:
    ...

    Parameters
    ----------

    w           : Sparse matrix
                  Spatial weights sparse matrix 

    lamb        : float
                  Spatial autoregressive parameter
                  
    reg         : GSTSLS
                  Generalized Spatial two stage least quare regression instance
    G           : array
                  Moments
    psi         : array
                  Weighting matrix
 
    Returns
    -------

    omega       : array
                  (k+1)x(k+1)                 
    """
    psi, a1, a2 = psi
    sigma = get_psi_sigma(w, reg.u, lamb)
    psi_dd_1 = (1.0 / w.shape[0]) * reg.h.T * sigma
    psi_dd = spdot(psi_dd_1, reg.h)
    psi_dl = spdot(psi_dd_1, np.hstack((a1, a2)))
    psi_o = np.hstack(
        (np.vstack((psi_dd, psi_dl.T)), np.vstack((psi_dl, psi))))
    psii = la.inv(psi)

    j = np.dot(G, np.array([[1.], [2 * lamb]]))
    jtpsii = np.dot(j.T, psii)
    jtpsiij = np.dot(jtpsii, j)
    jtpsiiji = la.inv(jtpsiij)
    omega_1 = np.dot(jtpsiiji, jtpsii)
    omega_2 = np.dot(np.dot(psii, j), jtpsiiji)
    om_1_s = omega_1.shape
    om_2_s = omega_2.shape
    p_s = P.shape
    omega_left = np.hstack((np.vstack((P.T, np.zeros((om_1_s[0], p_s[0])))),
                           np.vstack((np.zeros((p_s[1], om_1_s[1])), omega_1))))
    omega_right = np.hstack((np.vstack((P, np.zeros((om_2_s[0], p_s[1])))),
                            np.vstack((np.zeros((p_s[0], om_2_s[1])), omega_2))))
    omega = np.dot(np.dot(omega_left, psi_o), omega_right)
    return omega / w.shape[0]


def _test():
    import doctest
    doctest.testmod()

if __name__ == '__main__':
    _test()

########NEW FILE########
__FILENAME__ = error_sp_het_regimes
'''
Spatial Error with Heteroskedasticity and Regimes family of models
'''
__author__ = "Luc Anselin luc.anselin@asu.edu, Pedro V. Amaral pedro.amaral@asu.edu"

import numpy as np
import multiprocessing as mp
import user_output as USER
import summary_output as SUMMARY
import utils as UTILS
import regimes as REGI
from ols import BaseOLS
from twosls import BaseTSLS
from error_sp_het import BaseGM_Error_Het, BaseGM_Endog_Error_Het, get_psi_sigma, get_vc_het, get_vm_het, get_P_hat, get_a1a2, get_vc_het_tsls, get_Omega_GS2SLS
from utils import RegressionPropsY, spdot, set_endog, sphstack, set_warn, sp_att
from scipy import sparse as SP
from pysal import lag_spatial
from platform import system


class GM_Error_Het_Regimes(RegressionPropsY, REGI.Regimes_Frame):

    """
    GMM method for a spatial error model with heteroskedasticity and regimes;
    based on Arraiz et al [1]_, following Anselin [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    w            : pysal W object
                   Spatial weights object
    constant_regi: ['one', 'many']
                   Switcher controlling the constant term setup. It may take
                   the following values:
                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime (default)
    cols2regi    : list, 'all'
                   Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all' (default), all the variables vary by regime.
    regime_err_sep: boolean
                   If True, a separate regression is run for each regime.
    max_iter     : int
                   Maximum number of iterations of steps 2a and 2b from Arraiz
                   et al. Note: epsilon provides an additional stop condition.
    epsilon      : float
                   Minimum change in lambda required to stop iterations of
                   steps 2a and 2b from Arraiz et al. Note: max_iter provides
                   an additional stop condition.
    step1c       : boolean
                   If True, then include Step 1c from Arraiz et al.
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    cores        : integer
                   Specifies the number of cores to be used in multiprocessing
                   Default: all cores available (specified as cores=None).
                   Note: Multiprocessing currently not available on Windows.
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    name_regimes : string
                   Name of regime variable for use in the output

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    iter_stop    : string
                   Stop criterion reached during iteration of steps 2a and 2b
                   from Arraiz et al.
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    iteration    : integer
                   Number of iterations of steps 2a and 2b from Arraiz et al.
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    vm           : array
                   Variance covariance matrix (kxk)
    sig2         : float
                   Sigma squared used in computations
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    std_err      : array
                   1xk array of standard errors of the betas
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    name_regimes : string
                   Name of regime variable for use in the output
    title        : string
                   Name of the regression method used
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    constant_regi: ['one', 'many']
                   Ignored if regimes=False. Constant option for regimes.
                   Switcher controlling the constant term setup. It may take
                   the following values:
                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime
    cols2regi    : list, 'all'
                   Ignored if regimes=False. Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all', all the variables vary by regime.
    regime_err_sep : boolean
                   If True, a separate regression is run for each regime.
    kr           : int
                   Number of variables/columns to be "regimized" or subject
                   to change by regime. These will result in one parameter
                   estimate by regime for each variable (i.e. nr parameters per
                   variable)
    kf           : int
                   Number of variables/columns to be considered fixed or
                   global across regimes and hence only obtain one parameter
                   estimate
    nr           : int
                   Number of different regimes in the 'regimes' list
    multi         : dictionary
                    Only available when multiple regressions are estimated,
                    i.e. when regime_err_sep=True and no variable is fixed
                    across regimes.
                    Contains all attributes of each individual regression

    References
    ----------

    .. [1] Arraiz, I., Drukker, D. M., Kelejian, H., Prucha, I. R. (2010) "A
        Spatial Cliff-Ord-Type Model with Heteroskedastic Innovations: Small and
        Large Sample Results". Journal of Regional Science, Vol. 60, No. 2, pp.
        592-614.

    .. [2] Anselin, L. GMM Estimation of Spatial Error Autocorrelation with Heteroskedasticity

    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import numpy as np
    >>> import pysal

    Open data on NCOVR US County Homicides (3085 areas) using pysal.open().
    This is the DBF associated with the NAT shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.

    >>> db = pysal.open(pysal.examples.get_path("NAT.dbf"),'r')

    Extract the HR90 column (homicide rates in 1990) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y_var = 'HR90'
    >>> y = np.array([db.by_col(y_var)]).reshape(3085,1)

    Extract UE90 (unemployment rate) and PS90 (population structure) vectors from
    the DBF to be used as independent variables in the regression. Other variables
    can be inserted by adding their names to x_var, such as x_var = ['Var1','Var2','...]
    Note that PySAL requires this to be an nxj numpy array, where j is the
    number of independent variables (not including a constant). By default
    this model adds a vector of ones to the independent variables passed in.

    >>> x_var = ['PS90','UE90']
    >>> x = np.array([db.by_col(name) for name in x_var]).T

    The different regimes in this data are given according to the North and
    South dummy (SOUTH).

    >>> r_var = 'SOUTH'
    >>> regimes = db.by_col(r_var)

    Since we want to run a spatial error model, we need to specify
    the spatial weights matrix that includes the spatial configuration of the
    observations. To do that, we can open an already existing gal file or
    create a new one. In this case, we will create one from ``NAT.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("NAT.shp"))

    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, this allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'

    We are all set with the preliminaries, we are good to run the model. In this
    case, we will need the variables and the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.

    >>> reg = GM_Error_Het_Regimes(y, x, regimes, w=w, step1c=True, name_y=y_var, name_x=x_var, name_regimes=r_var, name_ds='NAT.dbf')

    Once we have run the model, we can explore a little bit the output. The
    regression object we have created has many attributes so take your time to
    discover them. This class offers an error model that explicitly accounts
    for heteroskedasticity and that unlike the models from
    ``pysal.spreg.error_sp``, it allows for inference on the spatial
    parameter. Alternatively, we can have a summary of the
    output by typing: model.summary

    >>> print reg.name_x
    ['0_CONSTANT', '0_PS90', '0_UE90', '1_CONSTANT', '1_PS90', '1_UE90', 'lambda']
    >>> np.around(reg.betas, decimals=6)
    array([[ 0.009121],
           [ 0.812973],
           [ 0.549355],
           [ 5.00279 ],
           [ 1.200929],
           [ 0.614681],
           [ 0.429277]])
    >>> np.around(reg.std_err, decimals=6)
    array([ 0.355844,  0.221743,  0.059276,  0.686764,  0.35843 ,  0.092788,
            0.02524 ])

    """

    def __init__(
        self, y, x, regimes, w, max_iter=1, epsilon=0.00001, step1c=False,
        constant_regi='many', cols2regi='all', regime_err_sep=False,
        cores=None, vm=False, name_y=None, name_x=None, name_w=None,
            name_ds=None, name_regimes=None):

        n = USER.check_arrays(y, x)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        self.constant_regi = constant_regi
        self.cols2regi = cols2regi
        self.regime_err_sep = regime_err_sep
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_y = USER.set_name_y(name_y)
        self.name_w = USER.set_name_w(name_w, w)
        self.name_regimes = USER.set_name_ds(name_regimes)
        self.n, self.step1c = n, step1c
        self.y = y

        x_constant = USER.check_constant(x)
        name_x = USER.set_name_x(name_x, x)
        self.name_x_r = name_x

        cols2regi = REGI.check_cols2regi(constant_regi, cols2regi, x)
        self.regimes_set = REGI._get_regimes_set(regimes)
        self.regimes = regimes
        USER.check_regimes(self.regimes_set, self.n, x.shape[1])
        self.regime_err_sep = regime_err_sep

        if regime_err_sep == True:
            if set(cols2regi) == set([True]):
                self._error_regimes_multi(y, x, regimes, w, cores,
                                          max_iter, epsilon, step1c,
                                          cols2regi, vm, name_x)
            else:
                raise Exception, "All coefficients must vary accross regimes if regime_err_sep = True."
        else:
            self.x, self.name_x = REGI.Regimes_Frame.__init__(self, x_constant,
                                                              regimes, constant_regi=None, cols2regi=cols2regi, names=name_x)
            ols = BaseOLS(y=y, x=self.x)
            self.k = ols.x.shape[1]
            wA1 = UTILS.get_A1_het(w.sparse)

            # 1b. GMM --> \tilde{\lambda1}
            moments = UTILS._moments2eqs(wA1, w.sparse, ols.u)
            lambda1 = UTILS.optim_moments(moments)

            if step1c:
                # 1c. GMM --> \tilde{\lambda2}
                sigma = get_psi_sigma(w.sparse, ols.u, lambda1)
                vc1 = get_vc_het(w.sparse, wA1, sigma)
                lambda2 = UTILS.optim_moments(moments, vc1)
            else:
                lambda2 = lambda1
            lambda_old = lambda2

            self.iteration, eps = 0, 1
            while self.iteration < max_iter and eps > epsilon:
                # 2a. reg -->\hat{betas}
                xs = UTILS.get_spFilter(w, lambda_old, x_constant)
                ys = UTILS.get_spFilter(w, lambda_old, y)
                xs = REGI.Regimes_Frame.__init__(self, xs,
                                                 regimes, constant_regi=None, cols2regi=cols2regi)[0]
                ols_s = BaseOLS(y=ys, x=xs)
                self.predy = spdot(self.x, ols_s.betas)
                self.u = self.y - self.predy

                # 2b. GMM --> \hat{\lambda}
                sigma_i = get_psi_sigma(w.sparse, self.u, lambda_old)
                vc_i = get_vc_het(w.sparse, wA1, sigma_i)
                moments_i = UTILS._moments2eqs(wA1, w.sparse, self.u)
                lambda3 = UTILS.optim_moments(moments_i, vc_i)
                eps = abs(lambda3 - lambda_old)
                lambda_old = lambda3
                self.iteration += 1

            self.iter_stop = UTILS.iter_msg(self.iteration, max_iter)

            sigma = get_psi_sigma(w.sparse, self.u, lambda3)
            vc3 = get_vc_het(w.sparse, wA1, sigma)
            self.vm = get_vm_het(moments_i[0], lambda3, self, w.sparse, vc3)
            self.betas = np.vstack((ols_s.betas, lambda3))
            self.e_filtered = self.u - lambda3 * lag_spatial(w, self.u)
            self.title = "SPATIALLY WEIGHTED LEAST SQUARES (HET) - REGIMES"
            self.name_x.append('lambda')
            self.kf += 1
            self.chow = REGI.Chow(self)
            self._cache = {}

            SUMMARY.GM_Error_Het(reg=self, w=w, vm=vm, regimes=True)

    def _error_regimes_multi(self, y, x, regimes, w, cores,
                             max_iter, epsilon, step1c, cols2regi, vm, name_x):

        regi_ids = dict((r, list(np.where(np.array(regimes) == r)[0]))
                        for r in self.regimes_set)
        results_p = {}
        for r in self.regimes_set:
            if system() == 'Windows':
                is_win = True
                results_p[r] = _work_error(
                    *(y, x, regi_ids, r, w, max_iter, epsilon, step1c,
                      self.name_ds, self.name_y, name_x + ['lambda'], self.name_w, self.name_regimes))
            else:
                pool = mp.Pool(cores)
                results_p[r] = pool.apply_async(_work_error, args=(
                    y, x, regi_ids, r, w, max_iter, epsilon, step1c, self.name_ds, self.name_y, name_x + ['lambda'], self.name_w, self.name_regimes, ))
                is_win = False
        self.kryd = 0
        self.kr = len(cols2regi) + 1
        self.kf = 0
        self.nr = len(self.regimes_set)
        self.vm = np.zeros((self.nr * self.kr, self.nr * self.kr), float)
        self.betas = np.zeros((self.nr * self.kr, 1), float)
        self.u = np.zeros((self.n, 1), float)
        self.predy = np.zeros((self.n, 1), float)
        self.e_filtered = np.zeros((self.n, 1), float)
        if not is_win:
            pool.close()
            pool.join()
        results = {}
        self.name_y, self.name_x = [], []
        counter = 0
        for r in self.regimes_set:
            if is_win:
                results[r] = results_p[r]
            else:
                results[r] = results_p[r].get()
            self.vm[(counter * self.kr):((counter + 1) * self.kr),
                    (counter * self.kr):((counter + 1) * self.kr)] = results[r].vm
            self.betas[(counter * self.kr):((counter + 1) * self.kr),
                       ] = results[r].betas
            self.u[regi_ids[r], ] = results[r].u
            self.predy[regi_ids[r], ] = results[r].predy
            self.e_filtered[regi_ids[r], ] = results[r].e_filtered
            self.name_y += results[r].name_y
            self.name_x += results[r].name_x
            counter += 1
        self.chow = REGI.Chow(self)
        self.multi = results
        SUMMARY.GM_Error_Het_multi(
            reg=self, multireg=self.multi, vm=vm, regimes=True)


class GM_Endog_Error_Het_Regimes(RegressionPropsY, REGI.Regimes_Frame):

    """
    GMM method for a spatial error model with heteroskedasticity, regimes and
    endogenous variables, with results and diagnostics; based on Arraiz et al
    [1]_, following Anselin [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note:
                   this should not contain any variables from x)
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    w            : pysal W object
                   Spatial weights object
    constant_regi: ['one', 'many']
                   Switcher controlling the constant term setup. It may take
                   the following values:
                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime (default)
    cols2regi    : list, 'all'
                   Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all' (default), all the variables vary by regime.
    regime_err_sep : boolean
                   If True, a separate regression is run for each regime.
    max_iter     : int
                   Maximum number of iterations of steps 2a and 2b from Arraiz
                   et al. Note: epsilon provides an additional stop condition.
    epsilon      : float
                   Minimum change in lambda required to stop iterations of
                   steps 2a and 2b from Arraiz et al. Note: max_iter provides
                   an additional stop condition.
    step1c       : boolean
                   If True, then include Step 1c from Arraiz et al.
    inv_method   : string
                   If "power_exp", then compute inverse using the power
                   expansion. If "true_inv", then compute the true inverse.
                   Note that true_inv will fail for large n.
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    cores        : integer
                   Specifies the number of cores to be used in multiprocessing
                   Default: all cores available (specified as cores=None).
                   Note: Multiprocessing currently not available on Windows.
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_yend    : list of strings
                   Names of endogenous variables for use in output
    name_q       : list of strings
                   Names of instruments for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    name_regimes : string
                   Name of regime variable for use in the output

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable used as instruments
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    z            : array
                   nxk array of variables (combination of x and yend)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    h            : array
                   nxl array of instruments (combination of x and q)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    iter_stop    : string
                   Stop criterion reached during iteration of steps 2a and 2b
                   from Arraiz et al.
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    iteration    : integer
                   Number of iterations of steps 2a and 2b from Arraiz et al.
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    std_err      : array
                   1xk array of standard errors of the betas
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    name_y        : string
                    Name of dependent variable for use in output
    name_x        : list of strings
                    Names of independent variables for use in output
    name_yend     : list of strings
                    Names of endogenous variables for use in output
    name_z        : list of strings
                    Names of exogenous and endogenous variables for use in
                    output
    name_q        : list of strings
                    Names of external instruments
    name_h        : list of strings
                    Names of all instruments used in ouput
    name_w        : string
                    Name of weights matrix for use in output
    name_ds       : string
                    Name of dataset for use in output
    name_regimes  : string
                    Name of regimes variable for use in output
    title         : string
                    Name of the regression method used
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    regimes       : list
                    List of n values with the mapping of each
                    observation to a regime. Assumed to be aligned with 'x'.
    constant_regi : ['one', 'many']
                    Ignored if regimes=False. Constant option for regimes.
                    Switcher controlling the constant term setup. It may take
                    the following values:
                      *  'one': a vector of ones is appended to x and held
                                constant across regimes
                      * 'many': a vector of ones is appended to x and considered
                                different per regime
    cols2regi     : list, 'all'
                    Ignored if regimes=False. Argument indicating whether each
                    column of x should be considered as different per regime
                    or held constant across regimes (False).
                    If a list, k booleans indicating for each variable the
                    option (True if one per regime, False to be held constant).
                    If 'all', all the variables vary by regime.
    regime_err_sep : boolean
                   If True, a separate regression is run for each regime.
    kr            : int
                    Number of variables/columns to be "regimized" or subject
                    to change by regime. These will result in one parameter
                    estimate by regime for each variable (i.e. nr parameters per
                    variable)
    kf            : int
                    Number of variables/columns to be considered fixed or
                    global across regimes and hence only obtain one parameter
                    estimate
    nr            : int
                    Number of different regimes in the 'regimes' list
    multi         : dictionary
                    Only available when multiple regressions are estimated,
                    i.e. when regime_err_sep=True and no variable is fixed
                    across regimes.
                    Contains all attributes of each individual regression

    References
    ----------

    .. [1] Arraiz, I., Drukker, D. M., Kelejian, H., Prucha, I. R. (2010) "A
        Spatial Cliff-Ord-Type Model with Heteroskedastic Innovations: Small and
        Large Sample Results". Journal of Regional Science, Vol. 60, No. 2, pp.
        592-614.

    .. [2] Anselin, L. GMM Estimation of Spatial Error Autocorrelation with Heteroskedasticity

    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import numpy as np
    >>> import pysal

    Open data on NCOVR US County Homicides (3085 areas) using pysal.open().
    This is the DBF associated with the NAT shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.

    >>> db = pysal.open(pysal.examples.get_path("NAT.dbf"),'r')

    Extract the HR90 column (homicide rates in 1990) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y_var = 'HR90'
    >>> y = np.array([db.by_col(y_var)]).reshape(3085,1)

    Extract UE90 (unemployment rate) and PS90 (population structure) vectors from
    the DBF to be used as independent variables in the regression. Other variables
    can be inserted by adding their names to x_var, such as x_var = ['Var1','Var2','...]
    Note that PySAL requires this to be an nxj numpy array, where j is the
    number of independent variables (not including a constant). By default
    this model adds a vector of ones to the independent variables passed in.

    >>> x_var = ['PS90','UE90']
    >>> x = np.array([db.by_col(name) for name in x_var]).T

    For the endogenous models, we add the endogenous variable RD90 (resource deprivation)
    and we decide to instrument for it with FP89 (families below poverty):

    >>> yd_var = ['RD90']
    >>> yend = np.array([db.by_col(name) for name in yd_var]).T
    >>> q_var = ['FP89']
    >>> q = np.array([db.by_col(name) for name in q_var]).T

    The different regimes in this data are given according to the North and
    South dummy (SOUTH).

    >>> r_var = 'SOUTH'
    >>> regimes = db.by_col(r_var)

    Since we want to run a spatial error model, we need to specify the spatial
    weights matrix that includes the spatial configuration of the observations
    into the error component of the model. To do that, we can open an already
    existing gal file or create a new one. In this case, we will create one
    from ``NAT.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("NAT.shp"))

    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, this allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'

    We are all set with the preliminaries, we are good to run the model. In this
    case, we will need the variables (exogenous and endogenous), the
    instruments and the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.

    >>> reg = GM_Endog_Error_Het_Regimes(y, x, yend, q, regimes, w=w, step1c=True, name_y=y_var, name_x=x_var, name_yend=yd_var, name_q=q_var, name_regimes=r_var, name_ds='NAT.dbf')

    Once we have run the model, we can explore a little bit the output. The
    regression object we have created has many attributes so take your time to
    discover them. This class offers an error model that explicitly accounts
    for heteroskedasticity and that unlike the models from
    ``pysal.spreg.error_sp``, it allows for inference on the spatial
    parameter. Hence, we find the same number of betas as of standard errors,
    which we calculate taking the square root of the diagonal of the
    variance-covariance matrix Alternatively, we can have a summary of the
    output by typing: model.summary

    >>> print reg.name_z
    ['0_CONSTANT', '0_PS90', '0_UE90', '1_CONSTANT', '1_PS90', '1_UE90', '0_RD90', '1_RD90', 'lambda']

    >>> print np.around(reg.betas,4)
    [[ 3.5944]
     [ 1.065 ]
     [ 0.1587]
     [ 9.184 ]
     [ 1.8784]
     [-0.2466]
     [ 2.4617]
     [ 3.5756]
     [ 0.2908]]

    >>> print np.around(np.sqrt(reg.vm.diagonal()),4)
    [ 0.5043  0.2132  0.0581  0.6681  0.3504  0.0999  0.3686  0.3402  0.028 ]

    """

    def __init__(self, y, x, yend, q, regimes, w,
                 max_iter=1, epsilon=0.00001, step1c=False,
                 constant_regi='many', cols2regi='all', regime_err_sep=False,
                 inv_method='power_exp', cores=None,
                 vm=False, name_y=None, name_x=None,
                 name_yend=None, name_q=None, name_w=None, name_ds=None,
                 name_regimes=None, summ=True, add_lag=False):

        n = USER.check_arrays(y, x, yend, q)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        self.constant_regi = constant_regi
        self.cols2regi = cols2regi
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_regimes = USER.set_name_ds(name_regimes)
        self.name_w = USER.set_name_w(name_w, w)
        self.n, self.step1c = n, step1c
        self.y = y

        name_x = USER.set_name_x(name_x, x)
        if summ:
            name_yend = USER.set_name_yend(name_yend, yend)
            self.name_y = USER.set_name_y(name_y)
            name_q = USER.set_name_q(name_q, q)
        self.name_x_r = name_x + name_yend

        cols2regi = REGI.check_cols2regi(
            constant_regi, cols2regi, x, yend=yend)
        self.regimes_set = REGI._get_regimes_set(regimes)
        self.regimes = regimes
        USER.check_regimes(self.regimes_set, self.n, x.shape[1])
        self.regime_err_sep = regime_err_sep

        if regime_err_sep == True:
            if set(cols2regi) == set([True]):
                self._endog_error_regimes_multi(
                    y, x, regimes, w, yend, q, cores,
                    max_iter, epsilon, step1c, inv_method, cols2regi, vm,
                    name_x, name_yend, name_q, add_lag)
            else:
                raise Exception, "All coefficients must vary accross regimes if regime_err_sep = True."
        else:
            x_constant = USER.check_constant(x)
            q, name_q = REGI.Regimes_Frame.__init__(self, q,
                                                    regimes, constant_regi=None, cols2regi='all', names=name_q)
            x, name_x = REGI.Regimes_Frame.__init__(self, x_constant,
                                                    regimes, constant_regi=None, cols2regi=cols2regi,
                                                    names=name_x)
            yend2, name_yend = REGI.Regimes_Frame.__init__(self, yend,
                                                           regimes, constant_regi=None,
                                                           cols2regi=cols2regi, yend=True, names=name_yend)

            # 1a. S2SLS --> \tilde{\delta}
            tsls = BaseTSLS(y=y, x=x, yend=yend2, q=q)
            self.k = tsls.z.shape[1]
            self.x = tsls.x
            self.yend, self.z, self.h = tsls.yend, tsls.z, tsls.h
            wA1 = UTILS.get_A1_het(w.sparse)

            # 1b. GMM --> \tilde{\lambda1}
            moments = UTILS._moments2eqs(wA1, w.sparse, tsls.u)
            lambda1 = UTILS.optim_moments(moments)

            if step1c:
                # 1c. GMM --> \tilde{\lambda2}
                self.u = tsls.u
                zs = UTILS.get_spFilter(w, lambda1, self.z)
                vc1 = get_vc_het_tsls(
                    w.sparse, wA1, self, lambda1, tsls.pfora1a2, zs, inv_method, filt=False)
                lambda2 = UTILS.optim_moments(moments, vc1)
            else:
                lambda2 = lambda1
            lambda_old = lambda2

            self.iteration, eps = 0, 1
            while self.iteration < max_iter and eps > epsilon:
                # 2a. reg -->\hat{betas}
                xs = UTILS.get_spFilter(w, lambda1, x_constant)
                xs = REGI.Regimes_Frame.__init__(self, xs,
                                                 regimes, constant_regi=None, cols2regi=cols2regi)[0]
                ys = UTILS.get_spFilter(w, lambda1, y)
                yend_s = UTILS.get_spFilter(w, lambda1, yend)
                yend_s = REGI.Regimes_Frame.__init__(self, yend_s,
                                                     regimes, constant_regi=None, cols2regi=cols2regi,
                                                     yend=True)[0]
                tsls_s = BaseTSLS(ys, xs, yend_s, h=tsls.h)
                self.predy = spdot(self.z, tsls_s.betas)
                self.u = self.y - self.predy

                # 2b. GMM --> \hat{\lambda}
                vc2 = get_vc_het_tsls(w.sparse, wA1, self, lambda_old,
                                      tsls_s.pfora1a2, sphstack(xs, yend_s), inv_method)
                moments_i = UTILS._moments2eqs(wA1, w.sparse, self.u)
                lambda3 = UTILS.optim_moments(moments_i, vc2)
                eps = abs(lambda3 - lambda_old)
                lambda_old = lambda3
                self.iteration += 1

            self.iter_stop = UTILS.iter_msg(self.iteration, max_iter)

            zs = UTILS.get_spFilter(w, lambda3, self.z)
            P = get_P_hat(self, tsls.hthi, zs)
            vc3 = get_vc_het_tsls(w.sparse, wA1, self,
                                  lambda3, P, zs, inv_method, save_a1a2=True)
            self.vm = get_Omega_GS2SLS(
                w.sparse, lambda3, self, moments_i[0], vc3, P)
            self.betas = np.vstack((tsls_s.betas, lambda3))
            self.e_filtered = self.u - lambda3 * lag_spatial(w, self.u)
            self.name_x = USER.set_name_x(name_x, x, constant=True)
            self.name_yend = USER.set_name_yend(name_yend, yend)
            self.name_z = self.name_x + self.name_yend
            self.name_z.append('lambda')  # listing lambda last
            self.name_q = USER.set_name_q(name_q, q)
            self.name_h = USER.set_name_h(self.name_x, self.name_q)
            self.kf += 1
            self.chow = REGI.Chow(self)
            self._cache = {}
            if summ:
                self.title = "SPATIALLY WEIGHTED TWO STAGE LEAST SQUARES (HET) - REGIMES"
                SUMMARY.GM_Endog_Error_Het(reg=self, w=w, vm=vm, regimes=True)

    def _endog_error_regimes_multi(self, y, x, regimes, w, yend, q, cores,
                                   max_iter, epsilon, step1c, inv_method, cols2regi, vm,
                                   name_x, name_yend, name_q, add_lag):

        regi_ids = dict((r, list(np.where(np.array(regimes) == r)[0]))
                        for r in self.regimes_set)
        if add_lag != False:
            self.cols2regi += [True]
            cols2regi += [True]
            self.predy_e = np.zeros((self.n, 1), float)
            self.e_pred = np.zeros((self.n, 1), float)
        results_p = {}
        for r in self.regimes_set:
            if system() == 'Windows':
                is_win = True
                results_p[r] = _work_endog_error(
                    *(y, x, yend, q, regi_ids, r, w, max_iter, epsilon, step1c, inv_method,
                      self.name_ds, self.name_y, name_x, name_yend, name_q, self.name_w, self.name_regimes, add_lag))
            else:
                pool = mp.Pool(cores)
                results_p[r] = pool.apply_async(
                    _work_endog_error, args=(
                        y, x, yend, q, regi_ids, r, w, max_iter, epsilon, step1c,
                        inv_method, self.name_ds, self.name_y, name_x, name_yend, name_q, self.name_w, self.name_regimes, add_lag, ))
                is_win = False
        self.kryd, self.kf = 0, 0
        self.kr = len(cols2regi) + 1
        self.nr = len(self.regimes_set)
        self.vm = np.zeros((self.nr * self.kr, self.nr * self.kr), float)
        self.betas = np.zeros((self.nr * self.kr, 1), float)
        self.u = np.zeros((self.n, 1), float)
        self.predy = np.zeros((self.n, 1), float)
        self.e_filtered = np.zeros((self.n, 1), float)
        if not is_win:
            pool.close()
            pool.join()
        results = {}
        self.name_y, self.name_x, self.name_yend, self.name_q, self.name_z, self.name_h = [
        ], [], [], [], [], []
        counter = 0
        for r in self.regimes_set:
            if is_win:
                results[r] = results_p[r]
            else:
                results[r] = results_p[r].get()
            self.vm[(counter * self.kr):((counter + 1) * self.kr),
                    (counter * self.kr):((counter + 1) * self.kr)] = results[r].vm
            self.betas[(counter * self.kr):((counter + 1) * self.kr),
                       ] = results[r].betas
            self.u[regi_ids[r], ] = results[r].u
            self.predy[regi_ids[r], ] = results[r].predy
            self.e_filtered[regi_ids[r], ] = results[r].e_filtered
            self.name_y += results[r].name_y
            self.name_x += results[r].name_x
            self.name_yend += results[r].name_yend
            self.name_q += results[r].name_q
            self.name_z += results[r].name_z
            self.name_h += results[r].name_h
            if add_lag != False:
                self.predy_e[regi_ids[r], ] = results[r].predy_e
                self.e_pred[regi_ids[r], ] = results[r].e_pred
            counter += 1
        self.chow = REGI.Chow(self)
        self.multi = results
        if add_lag != False:
            SUMMARY.GM_Combo_Het_multi(
                reg=self, multireg=self.multi, vm=vm, regimes=True)
        else:
            SUMMARY.GM_Endog_Error_Het_multi(
                reg=self, multireg=self.multi, vm=vm, regimes=True)


class GM_Combo_Het_Regimes(GM_Endog_Error_Het_Regimes):

    """
    GMM method for a spatial lag and error model with heteroskedasticity,
    regimes and endogenous variables, with results and diagnostics;
    based on Arraiz et al [1]_, following Anselin [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note:
                   this should not contain any variables from x)
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    w            : pysal W object
                   Spatial weights object (always needed)
    constant_regi : ['one', 'many']
                    Switcher controlling the constant term setup. It may take
                    the following values:
                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime (default)
    cols2regi    : list, 'all'
                   Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all' (default), all the variables vary by regime.
    regime_err_sep : boolean
                     If True, a separate regression is run for each regime.
    regime_lag_sep   : boolean
                       If True, the spatial parameter for spatial lag is also
                       computed according to different regimes. If False (default),
                       the spatial parameter is fixed accross regimes.
    w_lags       : integer
                   Orders of W to include as instruments for the spatially
                   lagged dependent variable. For example, w_lags=1, then
                   instruments are WX; if w_lags=2, then WX, WWX; and so on.
    lag_q        : boolean
                   If True, then include spatial lags of the additional
                   instruments (q).
    max_iter     : int
                   Maximum number of iterations of steps 2a and 2b from Arraiz
                   et al. Note: epsilon provides an additional stop condition.
    epsilon      : float
                   Minimum change in lambda required to stop iterations of
                   steps 2a and 2b from Arraiz et al. Note: max_iter provides
                   an additional stop condition.
    step1c       : boolean
                   If True, then include Step 1c from Arraiz et al.
    inv_method   : string
                   If "power_exp", then compute inverse using the power
                   expansion. If "true_inv", then compute the true inverse.
                   Note that true_inv will fail for large n.
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    cores        : integer
                   Specifies the number of cores to be used in multiprocessing
                   Default: all cores available (specified as cores=None).
                   Note: Multiprocessing currently not available on Windows.
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_yend    : list of strings
                   Names of endogenous variables for use in output
    name_q       : list of strings
                   Names of instruments for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    name_regimes : string
                   Name of regime variable for use in the output

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    e_pred       : array
                   nx1 array of residuals (using reduced form)
    predy        : array
                   nx1 array of predicted y values
    predy_e      : array
                   nx1 array of predicted y values (using reduced form)
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable used as instruments
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    z            : array
                   nxk array of variables (combination of x and yend)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    h            : array
                   nxl array of instruments (combination of x and q)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    iter_stop    : string
                   Stop criterion reached during iteration of steps 2a and 2b
                   from Arraiz et al.
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    iteration    : integer
                   Number of iterations of steps 2a and 2b from Arraiz et al.
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    pr2_e        : float
                   Pseudo R squared (squared correlation between y and ypred_e
                   (using reduced form))
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    std_err      : array
                   1xk array of standard errors of the betas
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    name_y        : string
                    Name of dependent variable for use in output
    name_x        : list of strings
                    Names of independent variables for use in output
    name_yend     : list of strings
                    Names of endogenous variables for use in output
    name_z        : list of strings
                    Names of exogenous and endogenous variables for use in
                    output
    name_q        : list of strings
                    Names of external instruments
    name_h        : list of strings
                    Names of all instruments used in ouput
    name_w        : string
                    Name of weights matrix for use in output
    name_ds       : string
                    Name of dataset for use in output
    name_regimes  : string
                    Name of regimes variable for use in output
    title         : string
                    Name of the regression method used
                    Only available in dictionary 'multi' when multiple regressions
                    (see 'multi' below for details)
    regimes       : list
                    List of n values with the mapping of each
                    observation to a regime. Assumed to be aligned with 'x'.
    constant_regi : ['one', 'many']
                    Ignored if regimes=False. Constant option for regimes.
                    Switcher controlling the constant term setup. It may take
                    the following values:
                      *  'one': a vector of ones is appended to x and held
                                constant across regimes
                      * 'many': a vector of ones is appended to x and considered
                                different per regime
    cols2regi     : list, 'all'
                    Ignored if regimes=False. Argument indicating whether each
                    column of x should be considered as different per regime
                    or held constant across regimes (False).
                    If a list, k booleans indicating for each variable the
                    option (True if one per regime, False to be held constant).
                    If 'all', all the variables vary by regime.
    regime_err_sep : boolean
                     If True, a separate regression is run for each regime.
    regime_lag_sep : boolean
                     If True, the spatial parameter for spatial lag is also
                     computed according to different regimes. If False (default),
                     the spatial parameter is fixed accross regimes.
    kr            : int
                    Number of variables/columns to be "regimized" or subject
                    to change by regime. These will result in one parameter
                    estimate by regime for each variable (i.e. nr parameters per
                    variable)
    kf            : int
                    Number of variables/columns to be considered fixed or
                    global across regimes and hence only obtain one parameter
                    estimate
    nr            : int
                    Number of different regimes in the 'regimes' list
    multi         : dictionary
                    Only available when multiple regressions are estimated,
                    i.e. when regime_err_sep=True and no variable is fixed
                    across regimes.
                    Contains all attributes of each individual regression

    References
    ----------

    .. [1] Arraiz, I., Drukker, D. M., Kelejian, H., Prucha, I. R. (2010) "A
        Spatial Cliff-Ord-Type Model with Heteroskedastic Innovations: Small and
        Large Sample Results". Journal of Regional Science, Vol. 60, No. 2, pp.
        592-614.

    .. [2] Anselin, L. GMM Estimation of Spatial Error Autocorrelation with Heteroskedasticity

    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import numpy as np
    >>> import pysal

    Open data on NCOVR US County Homicides (3085 areas) using pysal.open().
    This is the DBF associated with the NAT shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.

    >>> db = pysal.open(pysal.examples.get_path("NAT.dbf"),'r')

    Extract the HR90 column (homicide rates in 1990) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y_var = 'HR90'
    >>> y = np.array([db.by_col(y_var)]).reshape(3085,1)

    Extract UE90 (unemployment rate) and PS90 (population structure) vectors from
    the DBF to be used as independent variables in the regression. Other variables
    can be inserted by adding their names to x_var, such as x_var = ['Var1','Var2','...]
    Note that PySAL requires this to be an nxj numpy array, where j is the
    number of independent variables (not including a constant). By default
    this model adds a vector of ones to the independent variables passed in.

    >>> x_var = ['PS90','UE90']
    >>> x = np.array([db.by_col(name) for name in x_var]).T

    The different regimes in this data are given according to the North and
    South dummy (SOUTH).

    >>> r_var = 'SOUTH'
    >>> regimes = db.by_col(r_var)

    Since we want to run a spatial combo model, we need to specify
    the spatial weights matrix that includes the spatial configuration of the
    observations. To do that, we can open an already existing gal file or
    create a new one. In this case, we will create one from ``NAT.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("NAT.shp"))

    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, this allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'

    We are all set with the preliminaries, we are good to run the model. In this
    case, we will need the variables and the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.

    Example only with spatial lag

    The Combo class runs an SARAR model, that is a spatial lag+error model.
    In this case we will run a simple version of that, where we have the
    spatial effects as well as exogenous variables. Since it is a spatial
    model, we have to pass in the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.  We can have a
    summary of the output by typing: model.summary
    Alternatively, we can check the betas:

    >>> reg = GM_Combo_Het_Regimes(y, x, regimes, w=w, step1c=True, name_y=y_var, name_x=x_var, name_regimes=r_var, name_ds='NAT')
    >>> print reg.name_z
    ['0_CONSTANT', '0_PS90', '0_UE90', '1_CONSTANT', '1_PS90', '1_UE90', '_Global_W_HR90', 'lambda']
    >>> print np.around(reg.betas,4)
    [[ 1.4613]
     [ 0.9587]
     [ 0.5658]
     [ 9.1157]
     [ 1.1324]
     [ 0.6518]
     [-0.4587]
     [ 0.7174]]

    This class also allows the user to run a spatial lag+error model with the
    extra feature of including non-spatial endogenous regressors. This means
    that, in addition to the spatial lag and error, we consider some of the
    variables on the right-hand side of the equation as endogenous and we
    instrument for this. In this case we consider RD90 (resource deprivation)
    as an endogenous regressor.  We use FP89 (families below poverty)
    for this and hence put it in the instruments parameter, 'q'.

    >>> yd_var = ['RD90']
    >>> yd = np.array([db.by_col(name) for name in yd_var]).T
    >>> q_var = ['FP89']
    >>> q = np.array([db.by_col(name) for name in q_var]).T

    And then we can run and explore the model analogously to the previous combo:

    >>> reg = GM_Combo_Het_Regimes(y, x, regimes, yd, q, w=w, step1c=True, name_y=y_var, name_x=x_var, name_yend=yd_var, name_q=q_var, name_regimes=r_var, name_ds='NAT')
    >>> print reg.name_z
    ['0_CONSTANT', '0_PS90', '0_UE90', '1_CONSTANT', '1_PS90', '1_UE90', '0_RD90', '1_RD90', '_Global_W_HR90', 'lambda']
    >>> print reg.betas
    [[ 3.41936197]
     [ 1.04071048]
     [ 0.16747219]
     [ 8.85820215]
     [ 1.847382  ]
     [-0.24545394]
     [ 2.43189808]
     [ 3.61328423]
     [ 0.03132164]
     [ 0.29544224]]
    >>> print np.sqrt(reg.vm.diagonal())
    [ 0.53103804  0.20835827  0.05755679  1.00496234  0.34332131  0.10259525
      0.3454436   0.37932794  0.07611667  0.07067059]
    >>> print 'lambda: ', np.around(reg.betas[-1], 4)
    lambda:  [ 0.2954]

    """

    def __init__(self, y, x, regimes, yend=None, q=None,
                 w=None, w_lags=1, lag_q=True,
                 max_iter=1, epsilon=0.00001, step1c=False,
                 cores=None, inv_method='power_exp',
                 constant_regi='many', cols2regi='all',
                 regime_err_sep=False, regime_lag_sep=False,
                 vm=False, name_y=None, name_x=None,
                 name_yend=None, name_q=None,
                 name_w=None, name_ds=None, name_regimes=None):

        n = USER.check_arrays(y, x)
        self.step1c = step1c
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        name_x = USER.set_name_x(name_x, x, constant=True)
        self.name_y = USER.set_name_y(name_y)
        name_yend = USER.set_name_yend(name_yend, yend)
        name_q = USER.set_name_q(name_q, q)
        name_q.extend(USER.set_name_q_sp(name_x, w_lags, name_q, lag_q,
                      force_all=True))

        cols2regi = REGI.check_cols2regi(
            constant_regi, cols2regi, x, yend=yend, add_cons=False)
        self.regimes_set = REGI._get_regimes_set(regimes)
        self.regimes = regimes
        USER.check_regimes(self.regimes_set, n, x.shape[1])
        self.regime_err_sep = regime_err_sep
        self.regime_lag_sep = regime_lag_sep

        if regime_lag_sep == True:
            if regime_err_sep == False:
                raise Exception, "For spatial combo models, if spatial lag is set by regimes (regime_lag_sep=True), spatial error must also be set by regimes (regime_err_sep=True)."
            add_lag = [w_lags, lag_q]
        else:
            cols2regi += [False]
            add_lag = False
            if regime_err_sep == True:
                raise Exception, "For spatial combo models, if spatial error is set by regimes (regime_err_sep=True), all coefficients including lambda (regime_lag_sep=True) must be set by regimes."
            yend, q = set_endog(y, x, w, yend, q, w_lags, lag_q)
        name_yend.append(USER.set_name_yend_sp(self.name_y))

        GM_Endog_Error_Het_Regimes.__init__(self, y=y, x=x, yend=yend,
                                            q=q, regimes=regimes, w=w, constant_regi=constant_regi,
                                            cols2regi=cols2regi, regime_err_sep=regime_err_sep,
                                            max_iter=max_iter, epsilon=epsilon,
                                            step1c=step1c, inv_method=inv_method, cores=cores,
                                            vm=vm, name_y=name_y, name_x=name_x, name_yend=name_yend,
                                            name_q=name_q, name_w=name_w, name_ds=name_ds,
                                            name_regimes=name_regimes, summ=False, add_lag=add_lag)

        if regime_err_sep != True:
            self.rho = self.betas[-2]
            self.predy_e, self.e_pred, warn = UTILS.sp_att(w, self.y,
                                                           self.predy, yend[:, -1].reshape(self.n, 1), self.rho)
            UTILS.set_warn(self, warn)
            self.regime_lag_sep = regime_lag_sep
            self.title = "SPATIALLY WEIGHTED TWO STAGE LEAST SQUARES (HET) - REGIMES"
            SUMMARY.GM_Combo_Het(reg=self, w=w, vm=vm, regimes=True)


def _work_error(y, x, regi_ids, r, w, max_iter, epsilon, step1c, name_ds, name_y, name_x, name_w, name_regimes):
    w_r, warn = REGI.w_regime(w, regi_ids[r], r, transform=True)
    y_r = y[regi_ids[r]]
    x_r = x[regi_ids[r]]
    x_constant = USER.check_constant(x_r)
    model = BaseGM_Error_Het(y_r, x_constant, w_r.sparse,
                             max_iter=max_iter, epsilon=epsilon, step1c=step1c)
    set_warn(model, warn)
    model.w = w_r
    model.title = "SPATIALLY WEIGHTED LEAST SQUARES ESTIMATION (HET) - REGIME %s" % r
    model.name_ds = name_ds
    model.name_y = '%s_%s' % (str(r), name_y)
    model.name_x = ['%s_%s' % (str(r), i) for i in name_x]
    model.name_w = name_w
    model.name_regimes = name_regimes
    return model


def _work_endog_error(y, x, yend, q, regi_ids, r, w, max_iter, epsilon, step1c, inv_method, name_ds, name_y, name_x, name_yend, name_q, name_w, name_regimes, add_lag):
    w_r, warn = REGI.w_regime(w, regi_ids[r], r, transform=True)
    y_r = y[regi_ids[r]]
    x_r = x[regi_ids[r]]
    if yend != None:
        yend_r = yend[regi_ids[r]]
        q_r = q[regi_ids[r]]
    else:
        yend_r, q_r = None, None
    if add_lag != False:
        yend_r, q_r = set_endog(
            y_r, x_r, w_r, yend_r, q_r, add_lag[0], add_lag[1])
    x_constant = USER.check_constant(x_r)
    model = BaseGM_Endog_Error_Het(y_r, x_constant, yend_r, q_r, w_r.sparse,
                                   max_iter=max_iter, epsilon=epsilon, step1c=step1c, inv_method=inv_method)
    set_warn(model, warn)
    if add_lag != False:
        model.rho = model.betas[-2]
        model.predy_e, model.e_pred, warn = sp_att(w_r, model.y,
                                                   model.predy, model.yend[:, -1].reshape(model.n, 1), model.rho)
        set_warn(model, warn)
    model.title = "SPATIALLY WEIGHTED TWO STAGE LEAST SQUARES (HET) - REGIME %s" % r
    model.name_ds = name_ds
    model.name_y = '%s_%s' % (str(r), name_y)
    model.name_x = ['%s_%s' % (str(r), i) for i in name_x]
    model.name_yend = ['%s_%s' % (str(r), i) for i in name_yend]
    model.name_z = model.name_x + model.name_yend + ['lambda']
    model.name_q = ['%s_%s' % (str(r), i) for i in name_q]
    model.name_h = model.name_x + model.name_q
    model.name_w = name_w
    model.name_regimes = name_regimes
    return model


def _test():
    import doctest
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)

if __name__ == '__main__':
    _test()

########NEW FILE########
__FILENAME__ = error_sp_hom
'''
Hom family of models based on: 

    Drukker, D. M., Egger, P., Prucha, I. R. (2010)
    "On Two-step Estimation of a Spatial Autoregressive Model with Autoregressive
    Disturbances and Endogenous Regressors". Working paper.
    
Following:

    Anselin, L. (2011) "GMM Estimation of Spatial Error Autocorrelation with
    and without Heteroskedasticity".

'''

__author__ = "Luc Anselin luc.anselin@asu.edu, Daniel Arribas-Bel darribas@asu.edu"

from scipy import sparse as SP
import numpy as np
from numpy import linalg as la
import ols as OLS
from pysal import lag_spatial
from utils import power_expansion, set_endog, iter_msg, sp_att
from utils import get_A1_hom, get_A2_hom, get_A1_het, optim_moments
from utils import get_spFilter, get_lags, _moments2eqs
from utils import spdot, RegressionPropsY, set_warn
import twosls as TSLS
import user_output as USER
import summary_output as SUMMARY

__all__ = ["GM_Error_Hom", "GM_Endog_Error_Hom", "GM_Combo_Hom"]


class BaseGM_Error_Hom(RegressionPropsY):

    '''
    GMM method for a spatial error model with homoskedasticity (note: no
    consistency checks, diagnostics or constant added); based on 
    Drukker et al. (2010) [1]_, following Anselin (2011) [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    w            : Sparse matrix
                   Spatial weights sparse matrix   
    max_iter     : int
                   Maximum number of iterations of steps 2a and 2b from Arraiz
                   et al. Note: epsilon provides an additional stop condition.
    epsilon      : float
                   Minimum change in lambda required to stop iterations of
                   steps 2a and 2b from Arraiz et al. Note: max_iter provides
                   an additional stop condition.
    A1           : string
                   If A1='het', then the matrix A1 is defined as in Arraiz et
                   al. If A1='hom', then as in Anselin (2011) (default).  If
                   A1='hom_sc' (default), then as in Drukker, Egger and Prucha (2010)
                   and Drukker, Prucha and Raciborski (2010).

    Attributes
    ----------
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    iter_stop    : string
                   Stop criterion reached during iteration of steps 2a and 2b
                   from Arraiz et al.
    iteration    : integer
                   Number of iterations of steps 2a and 2b from Arraiz et al.
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    sig2         : float
                   Sigma squared used in computations
    xtx          : float
                   X'X

    References
    ----------

    .. [1] Drukker, D. M., Egger, P., Prucha, I. R. (2010)
    "On Two-step Estimation of a Spatial Autoregressive Model with Autoregressive
    Disturbances and Endogenous Regressors". Working paper.
 
    .. [2] Anselin, L. (2011) "GMM Estimation of Spatial Error Autocorrelation
    with and without Heteroskedasticity". 

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    >>> y = np.array(db.by_col("HOVAL"))
    >>> y = np.reshape(y, (49,1))
    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("CRIME"))
    >>> X = np.array(X).T
    >>> X = np.hstack((np.ones(y.shape),X))
    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
    >>> w.transform = 'r'

    Model commands

    >>> reg = BaseGM_Error_Hom(y, X, w=w.sparse, A1='hom_sc')
    >>> print np.around(np.hstack((reg.betas,np.sqrt(reg.vm.diagonal()).reshape(4,1))),4)
    [[ 47.9479  12.3021]
     [  0.7063   0.4967]
     [ -0.556    0.179 ]
     [  0.4129   0.1835]]
    >>> print np.around(reg.vm, 4)
    [[  1.51340700e+02  -5.29060000e+00  -1.85650000e+00  -2.40000000e-03]
     [ -5.29060000e+00   2.46700000e-01   5.14000000e-02   3.00000000e-04]
     [ -1.85650000e+00   5.14000000e-02   3.21000000e-02  -1.00000000e-04]
     [ -2.40000000e-03   3.00000000e-04  -1.00000000e-04   3.37000000e-02]]
    '''

    def __init__(self, y, x, w,
                 max_iter=1, epsilon=0.00001, A1='hom_sc'):
        if A1 == 'hom':
            wA1 = get_A1_hom(w)
        elif A1 == 'hom_sc':
            wA1 = get_A1_hom(w, scalarKP=True)
        elif A1 == 'het':
            wA1 = get_A1_het(w)

        wA2 = get_A2_hom(w)

        # 1a. OLS --> \tilde{\delta}
        ols = OLS.BaseOLS(y=y, x=x)
        self.x, self.y, self.n, self.k, self.xtx = ols.x, ols.y, ols.n, ols.k, ols.xtx

        # 1b. GM --> \tilde{\rho}
        moments = moments_hom(w, wA1, wA2, ols.u)
        lambda1 = optim_moments(moments)
        lambda_old = lambda1

        self.iteration, eps = 0, 1
        while self.iteration < max_iter and eps > epsilon:
            # 2a. SWLS --> \hat{\delta}
            x_s = get_spFilter(w, lambda_old, self.x)
            y_s = get_spFilter(w, lambda_old, self.y)
            ols_s = OLS.BaseOLS(y=y_s, x=x_s)
            self.predy = spdot(self.x, ols_s.betas)
            self.u = self.y - self.predy

            # 2b. GM 2nd iteration --> \hat{\rho}
            moments = moments_hom(w, wA1, wA2, self.u)
            psi = get_vc_hom(w, wA1, wA2, self, lambda_old)[0]
            lambda2 = optim_moments(moments, psi)
            eps = abs(lambda2 - lambda_old)
            lambda_old = lambda2
            self.iteration += 1

        self.iter_stop = iter_msg(self.iteration, max_iter)

        # Output
        self.betas = np.vstack((ols_s.betas, lambda2))
        self.vm, self.sig2 = get_omega_hom_ols(
            w, wA1, wA2, self, lambda2, moments[0])
        self.e_filtered = self.u - lambda2 * w * self.u
        self._cache = {}


class GM_Error_Hom(BaseGM_Error_Hom):

    '''
    GMM method for a spatial error model with homoskedasticity, with results
    and diagnostics; based on Drukker et al. (2010) [1]_, following Anselin
    (2011) [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    w            : pysal W object
                   Spatial weights object   
    max_iter     : int
                   Maximum number of iterations of steps 2a and 2b from Arraiz
                   et al. Note: epsilon provides an additional stop condition.
    epsilon      : float
                   Minimum change in lambda required to stop iterations of
                   steps 2a and 2b from Arraiz et al. Note: max_iter provides
                   an additional stop condition.
    A1           : string
                   If A1='het', then the matrix A1 is defined as in Arraiz et
                   al. If A1='hom', then as in Anselin (2011).  If
                   A1='hom_sc' (default), then as in Drukker, Egger and Prucha (2010)
                   and Drukker, Prucha and Raciborski (2010).
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output


    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    iter_stop    : string
                   Stop criterion reached during iteration of steps 2a and 2b
                   from Arraiz et al.
    iteration    : integer
                   Number of iterations of steps 2a and 2b from Arraiz et al.
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
    vm           : array
                   Variance covariance matrix (kxk)
    sig2         : float
                   Sigma squared used in computations
    std_err      : array
                   1xk array of standard errors of the betas    
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
    xtx          : float
                   X'X
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    title        : string
                   Name of the regression method used

    References
    ----------

    .. [1] Drukker, D. M., Egger, P., Prucha, I. R. (2010)
    "On Two-step Estimation of a Spatial Autoregressive Model with Autoregressive
    Disturbances and Endogenous Regressors". Working paper.
 
    .. [2] Anselin, L. (2011) "GMM Estimation of Spatial Error Autocorrelation
    with and without Heteroskedasticity". 

    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import numpy as np
    >>> import pysal

    Open data on Columbus neighborhood crime (49 areas) using pysal.open().
    This is the DBF associated with the Columbus shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    
    Extract the HOVAL column (home values) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y = np.array(db.by_col("HOVAL"))
    >>> y = np.reshape(y, (49,1))

    Extract INC (income) and CRIME (crime) vectors from the DBF to be used as
    independent variables in the regression.  Note that PySAL requires this to
    be an nxj numpy array, where j is the number of independent variables (not
    including a constant). By default this class adds a vector of ones to the
    independent variables passed in.

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("CRIME"))
    >>> X = np.array(X).T

    Since we want to run a spatial error model, we need to specify the spatial
    weights matrix that includes the spatial configuration of the observations
    into the error component of the model. To do that, we can open an already
    existing gal file or create a new one. In this case, we will create one
    from ``columbus.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
    
    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, his allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'

    We are all set with the preliminars, we are good to run the model. In this
    case, we will need the variables and the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.

    >>> reg = GM_Error_Hom(y, X, w=w, A1='hom_sc', name_y='home value', name_x=['income', 'crime'], name_ds='columbus')
   
    Once we have run the model, we can explore a little bit the output. The
    regression object we have created has many attributes so take your time to
    discover them. This class offers an error model that assumes
    homoskedasticity but that unlike the models from
    ``pysal.spreg.error_sp``, it allows for inference on the spatial
    parameter. This is why you obtain as many coefficient estimates as
    standard errors, which you calculate taking the square root of the
    diagonal of the variance-covariance matrix of the parameters:
   
    >>> print np.around(np.hstack((reg.betas,np.sqrt(reg.vm.diagonal()).reshape(4,1))),4)
    [[ 47.9479  12.3021]
     [  0.7063   0.4967]
     [ -0.556    0.179 ]
     [  0.4129   0.1835]]

    '''

    def __init__(self, y, x, w,
                 max_iter=1, epsilon=0.00001, A1='hom_sc',
                 vm=False, name_y=None, name_x=None,
                 name_w=None, name_ds=None):

        n = USER.check_arrays(y, x)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        x_constant = USER.check_constant(x)
        BaseGM_Error_Hom.__init__(self, y=y, x=x_constant, w=w.sparse, A1=A1,
                                  max_iter=max_iter, epsilon=epsilon)
        self.title = "SPATIALLY WEIGHTED LEAST SQUARES (HOM)"
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_y = USER.set_name_y(name_y)
        self.name_x = USER.set_name_x(name_x, x)
        self.name_x.append('lambda')
        self.name_w = USER.set_name_w(name_w, w)
        SUMMARY.GM_Error_Hom(reg=self, w=w, vm=vm)


class BaseGM_Endog_Error_Hom(RegressionPropsY):

    '''
    GMM method for a spatial error model with homoskedasticity and
    endogenous variables (note: no consistency checks, diagnostics or constant
    added); based on Drukker et al. (2010) [1]_, following Anselin (2011) [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note: 
                   this should not contain any variables from x)
    w            : Sparse matrix
                   Spatial weights sparse matrix   
    max_iter     : int
                   Maximum number of iterations of steps 2a and 2b from Arraiz
                   et al. Note: epsilon provides an additional stop condition.
    epsilon      : float
                   Minimum change in lambda required to stop iterations of
                   steps 2a and 2b from Arraiz et al. Note: max_iter provides
                   an additional stop condition.
    A1           : string
                   If A1='het', then the matrix A1 is defined as in Arraiz et
                   al. If A1='hom', then as in Anselin (2011).  If
                   A1='hom_sc' (default), then as in Drukker, Egger and Prucha (2010)
                   and Drukker, Prucha and Raciborski (2010).

    Attributes
    ----------
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable used as instruments 
    z            : array
                   nxk array of variables (combination of x and yend)
    h            : array
                   nxl array of instruments (combination of x and q)
    iter_stop    : string
                   Stop criterion reached during iteration of steps 2a and 2b
                   from Arraiz et al.
    iteration    : integer
                   Number of iterations of steps 2a and 2b from Arraiz et al.
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    sig2         : float
                   Sigma squared used in computations
    hth          : float
                   H'H

    References
    ----------

    .. [1] Drukker, D. M., Egger, P., Prucha, I. R. (2010)
    "On Two-step Estimation of a Spatial Autoregressive Model with Autoregressive
    Disturbances and Endogenous Regressors". Working paper.
 
    .. [2] Anselin, L. (2011) "GMM Estimation of Spatial Error Autocorrelation
    with and without Heteroskedasticity". 

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    >>> y = np.array(db.by_col("HOVAL"))
    >>> y = np.reshape(y, (49,1))
    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X = np.array(X).T
    >>> X = np.hstack((np.ones(y.shape),X))
    >>> yd = []
    >>> yd.append(db.by_col("CRIME"))
    >>> yd = np.array(yd).T
    >>> q = []
    >>> q.append(db.by_col("DISCBD"))
    >>> q = np.array(q).T
    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
    >>> w.transform = 'r'
    >>> reg = BaseGM_Endog_Error_Hom(y, X, yd, q, w=w.sparse, A1='hom_sc')
    >>> print np.around(np.hstack((reg.betas,np.sqrt(reg.vm.diagonal()).reshape(4,1))),4)
    [[ 55.3658  23.496 ]
     [  0.4643   0.7382]
     [ -0.669    0.3943]
     [  0.4321   0.1927]]

    
    '''

    def __init__(self, y, x, yend, q, w,
                 max_iter=1, epsilon=0.00001, A1='hom_sc'):

        if A1 == 'hom':
            wA1 = get_A1_hom(w)
        elif A1 == 'hom_sc':
            wA1 = get_A1_hom(w, scalarKP=True)
        elif A1 == 'het':
            wA1 = get_A1_het(w)

        wA2 = get_A2_hom(w)

        # 1a. S2SLS --> \tilde{\delta}
        tsls = TSLS.BaseTSLS(y=y, x=x, yend=yend, q=q)
        self.x, self.z, self.h, self.y, self.hth = tsls.x, tsls.z, tsls.h, tsls.y, tsls.hth
        self.yend, self.q, self.n, self.k = tsls.yend, tsls.q, tsls.n, tsls.k

        # 1b. GM --> \tilde{\rho}
        moments = moments_hom(w, wA1, wA2, tsls.u)
        lambda1 = optim_moments(moments)
        lambda_old = lambda1

        self.iteration, eps = 0, 1
        while self.iteration < max_iter and eps > epsilon:
            # 2a. GS2SLS --> \hat{\delta}
            x_s = get_spFilter(w, lambda_old, self.x)
            y_s = get_spFilter(w, lambda_old, self.y)
            yend_s = get_spFilter(w, lambda_old, self.yend)
            tsls_s = TSLS.BaseTSLS(y=y_s, x=x_s, yend=yend_s, h=self.h)
            self.predy = spdot(self.z, tsls_s.betas)
            self.u = self.y - self.predy

            # 2b. GM 2nd iteration --> \hat{\rho}
            moments = moments_hom(w, wA1, wA2, self.u)
            psi = get_vc_hom(w, wA1, wA2, self, lambda_old, tsls_s.z)[0]
            lambda2 = optim_moments(moments, psi)
            eps = abs(lambda2 - lambda_old)
            lambda_old = lambda2
            self.iteration += 1

        self.iter_stop = iter_msg(self.iteration, max_iter)

        # Output
        self.betas = np.vstack((tsls_s.betas, lambda2))
        self.vm, self.sig2 = get_omega_hom(
            w, wA1, wA2, self, lambda2, moments[0])
        self.e_filtered = self.u - lambda2 * w * self.u
        self._cache = {}


class GM_Endog_Error_Hom(BaseGM_Endog_Error_Hom):

    '''
    GMM method for a spatial error model with homoskedasticity and endogenous
    variables, with results and diagnostics; based on Drukker et al. (2010) [1]_,
    following Anselin (2011) [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note: 
                   this should not contain any variables from x)
    w            : pysal W object
                   Spatial weights object   
    max_iter     : int
                   Maximum number of iterations of steps 2a and 2b from Arraiz
                   et al. Note: epsilon provides an additional stop condition.
    epsilon      : float
                   Minimum change in lambda required to stop iterations of
                   steps 2a and 2b from Arraiz et al. Note: max_iter provides
                   an additional stop condition.
    A1           : string
                   If A1='het', then the matrix A1 is defined as in Arraiz et
                   al. If A1='hom', then as in Anselin (2011).  If
                   A1='hom_sc' (default), then as in Drukker, Egger and Prucha (2010)
                   and Drukker, Prucha and Raciborski (2010).
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_yend    : list of strings
                   Names of endogenous variables for use in output
    name_q       : list of strings
                   Names of instruments for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable used as instruments 
    z            : array
                   nxk array of variables (combination of x and yend)
    h            : array
                   nxl array of instruments (combination of x and q)
    iter_stop    : string
                   Stop criterion reached during iteration of steps 2a and 2b
                   from Arraiz et al.
    iteration    : integer
                   Number of iterations of steps 2a and 2b from Arraiz et al.
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
    sig2         : float
                   Sigma squared used in computations
    std_err      : array
                   1xk array of standard errors of the betas    
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
    name_y        : string
                    Name of dependent variable for use in output
    name_x        : list of strings
                    Names of independent variables for use in output
    name_yend     : list of strings
                    Names of endogenous variables for use in output
    name_z        : list of strings
                    Names of exogenous and endogenous variables for use in 
                    output
    name_q        : list of strings
                    Names of external instruments
    name_h        : list of strings
                    Names of all instruments used in ouput
    name_w        : string
                    Name of weights matrix for use in output
    name_ds       : string
                    Name of dataset for use in output
    title         : string
                    Name of the regression method used
    hth          : float
                   H'H


    References
    ----------

    .. [1] Drukker, D. M., Egger, P., Prucha, I. R. (2010)
    "On Two-step Estimation of a Spatial Autoregressive Model with Autoregressive
    Disturbances and Endogenous Regressors". Working paper.
 
    .. [2] Anselin, L. (2011) "GMM Estimation of Spatial Error Autocorrelation
    with and without Heteroskedasticity". 

    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import numpy as np
    >>> import pysal

    Open data on Columbus neighborhood crime (49 areas) using pysal.open().
    This is the DBF associated with the Columbus shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    
    Extract the HOVAL column (home values) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y = np.array(db.by_col("HOVAL"))
    >>> y = np.reshape(y, (49,1))

    Extract INC (income) vector from the DBF to be used as
    independent variables in the regression.  Note that PySAL requires this to
    be an nxj numpy array, where j is the number of independent variables (not
    including a constant). By default this class adds a vector of ones to the
    independent variables passed in.

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X = np.array(X).T

    In this case we consider CRIME (crime rates) is an endogenous regressor.
    We tell the model that this is so by passing it in a different parameter
    from the exogenous variables (x).

    >>> yd = []
    >>> yd.append(db.by_col("CRIME"))
    >>> yd = np.array(yd).T

    Because we have endogenous variables, to obtain a correct estimate of the
    model, we need to instrument for CRIME. We use DISCBD (distance to the
    CBD) for this and hence put it in the instruments parameter, 'q'.

    >>> q = []
    >>> q.append(db.by_col("DISCBD"))
    >>> q = np.array(q).T

    Since we want to run a spatial error model, we need to specify the spatial
    weights matrix that includes the spatial configuration of the observations
    into the error component of the model. To do that, we can open an already
    existing gal file or create a new one. In this case, we will create one
    from ``columbus.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
    
    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, his allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'

    We are all set with the preliminars, we are good to run the model. In this
    case, we will need the variables (exogenous and endogenous), the
    instruments and the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.

    >>> reg = GM_Endog_Error_Hom(y, X, yd, q, w=w, A1='hom_sc', name_x=['inc'], name_y='hoval', name_yend=['crime'], name_q=['discbd'], name_ds='columbus')
   
    Once we have run the model, we can explore a little bit the output. The
    regression object we have created has many attributes so take your time to
    discover them. This class offers an error model that assumes
    homoskedasticity but that unlike the models from
    ``pysal.spreg.error_sp``, it allows for inference on the spatial
    parameter. Hence, we find the same number of betas as of standard errors,
    which we calculate taking the square root of the diagonal of the
    variance-covariance matrix:

    >>> print reg.name_z
    ['CONSTANT', 'inc', 'crime', 'lambda']
    >>> print np.around(np.hstack((reg.betas,np.sqrt(reg.vm.diagonal()).reshape(4,1))),4)
    [[ 55.3658  23.496 ]
     [  0.4643   0.7382]
     [ -0.669    0.3943]
     [  0.4321   0.1927]]

    '''

    def __init__(self, y, x, yend, q, w,
                 max_iter=1, epsilon=0.00001, A1='hom_sc',
                 vm=False, name_y=None, name_x=None,
                 name_yend=None, name_q=None,
                 name_w=None, name_ds=None):

        n = USER.check_arrays(y, x, yend, q)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        x_constant = USER.check_constant(x)
        BaseGM_Endog_Error_Hom.__init__(
            self, y=y, x=x_constant, w=w.sparse, yend=yend, q=q,
            A1=A1, max_iter=max_iter, epsilon=epsilon)
        self.title = "SPATIALLY WEIGHTED TWO STAGE LEAST SQUARES (HOM)"
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_y = USER.set_name_y(name_y)
        self.name_x = USER.set_name_x(name_x, x)
        self.name_yend = USER.set_name_yend(name_yend, yend)
        self.name_z = self.name_x + self.name_yend
        self.name_z.append('lambda')  # listing lambda last
        self.name_q = USER.set_name_q(name_q, q)
        self.name_h = USER.set_name_h(self.name_x, self.name_q)
        self.name_w = USER.set_name_w(name_w, w)
        SUMMARY.GM_Endog_Error_Hom(reg=self, w=w, vm=vm)


class BaseGM_Combo_Hom(BaseGM_Endog_Error_Hom):

    '''
    GMM method for a spatial lag and error model with homoskedasticity and
    endogenous variables (note: no consistency checks, diagnostics or constant
    added); based on Drukker et al. (2010) [1]_, following Anselin (2011) [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note: 
                   this should not contain any variables from x)
    w            : Sparse matrix
                   Spatial weights sparse matrix   
    w_lags       : integer
                   Orders of W to include as instruments for the spatially
                   lagged dependent variable. For example, w_lags=1, then
                   instruments are WX; if w_lags=2, then WX, WWX; and so on.
    lag_q        : boolean
                   If True, then include spatial lags of the additional 
                   instruments (q).
    max_iter     : int
                   Maximum number of iterations of steps 2a and 2b from Arraiz
                   et al. Note: epsilon provides an additional stop condition.
    epsilon      : float
                   Minimum change in lambda required to stop iterations of
                   steps 2a and 2b from Arraiz et al. Note: max_iter provides
                   an additional stop condition.
    A1           : string
                   If A1='het', then the matrix A1 is defined as in Arraiz et
                   al. If A1='hom', then as in Anselin (2011).  If
                   A1='hom_sc' (default), then as in Drukker, Egger and Prucha (2010)
                   and Drukker, Prucha and Raciborski (2010).


    Attributes
    ----------
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable used as instruments 
    z            : array
                   nxk array of variables (combination of x and yend)
    h            : array
                   nxl array of instruments (combination of x and q)
    iter_stop    : string
                   Stop criterion reached during iteration of steps 2a and 2b
                   from Arraiz et al.
    iteration    : integer
                   Number of iterations of steps 2a and 2b from Arraiz et al.
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    sig2         : float
                   Sigma squared used in computations
    hth          : float
                   H'H


    References
    ----------

    .. [1] Drukker, D. M., Egger, P., Prucha, I. R. (2010)
    "On Two-step Estimation of a Spatial Autoregressive Model with Autoregressive
    Disturbances and Endogenous Regressors". Working paper.
 
    .. [2] Anselin, L. (2011) "GMM Estimation of Spatial Error Autocorrelation
    with and without Heteroskedasticity". 

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    >>> y = np.array(db.by_col("HOVAL"))
    >>> y = np.reshape(y, (49,1))
    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X = np.array(X).T
    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
    >>> w.transform = 'r'
    >>> w_lags = 1
    >>> yd2, q2 = pysal.spreg.utils.set_endog(y, X, w, None, None, w_lags, True)
    >>> X = np.hstack((np.ones(y.shape),X))

    Example only with spatial lag

    >>> reg = BaseGM_Combo_Hom(y, X, yend=yd2, q=q2, w=w.sparse, A1='hom_sc')
    >>> print np.around(np.hstack((reg.betas,np.sqrt(reg.vm.diagonal()).reshape(4,1))),4)
    [[ 10.1254  15.2871]
     [  1.5683   0.4407]
     [  0.1513   0.4048]
     [  0.2103   0.4226]]


    Example with both spatial lag and other endogenous variables

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X = np.array(X).T
    >>> yd = []
    >>> yd.append(db.by_col("CRIME"))
    >>> yd = np.array(yd).T
    >>> q = []
    >>> q.append(db.by_col("DISCBD"))
    >>> q = np.array(q).T
    >>> yd2, q2 = pysal.spreg.utils.set_endog(y, X, w, yd, q, w_lags, True)
    >>> X = np.hstack((np.ones(y.shape),X))
    >>> reg = BaseGM_Combo_Hom(y, X, yd2, q2, w=w.sparse, A1='hom_sc')
    >>> betas = np.array([['CONSTANT'],['inc'],['crime'],['W_hoval'],['lambda']])
    >>> print np.hstack((betas, np.around(np.hstack((reg.betas, np.sqrt(reg.vm.diagonal()).reshape(5,1))),5)))
    [['CONSTANT' '111.7705' '67.75191']
     ['inc' '-0.30974' '1.16656']
     ['crime' '-1.36043' '0.6841']
     ['W_hoval' '-0.52908' '0.84428']
     ['lambda' '0.60116' '0.18605']]

    '''

    def __init__(self, y, x, yend=None, q=None,
                 w=None, w_lags=1, lag_q=True,
                 max_iter=1, epsilon=0.00001, A1='hom_sc'):

        BaseGM_Endog_Error_Hom.__init__(
            self, y=y, x=x, w=w, yend=yend, q=q, A1=A1,
            max_iter=max_iter, epsilon=epsilon)


class GM_Combo_Hom(BaseGM_Combo_Hom):

    '''
    GMM method for a spatial lag and error model with homoskedasticity and
    endogenous variables, with results and diagnostics; based on Drukker et
    al. (2010) [1]_, following Anselin (2011) [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note: 
                   this should not contain any variables from x)
    w            : pysal W object
                   Spatial weights object (always necessary)   
    w_lags       : integer
                   Orders of W to include as instruments for the spatially
                   lagged dependent variable. For example, w_lags=1, then
                   instruments are WX; if w_lags=2, then WX, WWX; and so on.
    lag_q        : boolean
                   If True, then include spatial lags of the additional 
                   instruments (q).
    max_iter     : int
                   Maximum number of iterations of steps 2a and 2b from Arraiz
                   et al. Note: epsilon provides an additional stop condition.
    epsilon      : float
                   Minimum change in lambda required to stop iterations of
                   steps 2a and 2b from Arraiz et al. Note: max_iter provides
                   an additional stop condition.
    A1           : string
                   If A1='het', then the matrix A1 is defined as in Arraiz et
                   al. If A1='hom', then as in Anselin (2011).  If
                   A1='hom_sc' (default), then as in Drukker, Egger and Prucha (2010)
                   and Drukker, Prucha and Raciborski (2010).
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_yend    : list of strings
                   Names of endogenous variables for use in output
    name_q       : list of strings
                   Names of instruments for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    e_pred       : array
                   nx1 array of residuals (using reduced form)
    predy        : array
                   nx1 array of predicted y values
    predy_e      : array
                   nx1 array of predicted y values (using reduced form)
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable used as instruments 
    z            : array
                   nxk array of variables (combination of x and yend)
    h            : array
                   nxl array of instruments (combination of x and q)
    iter_stop    : string
                   Stop criterion reached during iteration of steps 2a and 2b
                   from Arraiz et al.
    iteration    : integer
                   Number of iterations of steps 2a and 2b from Arraiz et al.
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
    pr2_e        : float
                   Pseudo R squared (squared correlation between y and ypred_e
                   (using reduced form))
    sig2         : float
                   Sigma squared used in computations (based on filtered
                   residuals)
    std_err      : array
                   1xk array of standard errors of the betas    
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
    name_y        : string
                    Name of dependent variable for use in output
    name_x        : list of strings
                    Names of independent variables for use in output
    name_yend     : list of strings
                    Names of endogenous variables for use in output
    name_z        : list of strings
                    Names of exogenous and endogenous variables for use in 
                    output
    name_q        : list of strings
                    Names of external instruments
    name_h        : list of strings
                    Names of all instruments used in ouput
    name_w        : string
                    Name of weights matrix for use in output
    name_ds       : string
                    Name of dataset for use in output
    title         : string
                    Name of the regression method used
    hth          : float
                   H'H


    References
    ----------

    .. [1] Drukker, D. M., Egger, P., Prucha, I. R. (2010)
    "On Two-step Estimation of a Spatial Autoregressive Model with Autoregressive
    Disturbances and Endogenous Regressors". Working paper.
 
    .. [2] Anselin, L. (2011) "GMM Estimation of Spatial Error Autocorrelation
    with and without Heteroskedasticity". 

    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import numpy as np
    >>> import pysal

    Open data on Columbus neighborhood crime (49 areas) using pysal.open().
    This is the DBF associated with the Columbus shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    
    Extract the HOVAL column (home values) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y = np.array(db.by_col("HOVAL"))
    >>> y = np.reshape(y, (49,1))

    Extract INC (income) vector from the DBF to be used as
    independent variables in the regression.  Note that PySAL requires this to
    be an nxj numpy array, where j is the number of independent variables (not
    including a constant). By default this class adds a vector of ones to the
    independent variables passed in.

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X = np.array(X).T

    Since we want to run a spatial error model, we need to specify the spatial
    weights matrix that includes the spatial configuration of the observations
    into the error component of the model. To do that, we can open an already
    existing gal file or create a new one. In this case, we will create one
    from ``columbus.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
    
    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, his allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'

    Example only with spatial lag

    The Combo class runs an SARAR model, that is a spatial lag+error model.
    In this case we will run a simple version of that, where we have the
    spatial effects as well as exogenous variables. Since it is a spatial
    model, we have to pass in the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.

    >>> reg = GM_Combo_Hom(y, X, w=w, A1='hom_sc', name_x=['inc'],\
            name_y='hoval', name_yend=['crime'], name_q=['discbd'],\
            name_ds='columbus')
    >>> print np.around(np.hstack((reg.betas,np.sqrt(reg.vm.diagonal()).reshape(4,1))),4)
    [[ 10.1254  15.2871]
     [  1.5683   0.4407]
     [  0.1513   0.4048]
     [  0.2103   0.4226]]
        
    This class also allows the user to run a spatial lag+error model with the
    extra feature of including non-spatial endogenous regressors. This means
    that, in addition to the spatial lag and error, we consider some of the
    variables on the right-hand side of the equation as endogenous and we
    instrument for this. As an example, we will include CRIME (crime rates) as
    endogenous and will instrument with DISCBD (distance to the CSB). We first
    need to read in the variables:


    >>> yd = []
    >>> yd.append(db.by_col("CRIME"))
    >>> yd = np.array(yd).T
    >>> q = []
    >>> q.append(db.by_col("DISCBD"))
    >>> q = np.array(q).T

    And then we can run and explore the model analogously to the previous combo:

    >>> reg = GM_Combo_Hom(y, X, yd, q, w=w, A1='hom_sc', \
            name_ds='columbus')
    >>> betas = np.array([['CONSTANT'],['inc'],['crime'],['W_hoval'],['lambda']])
    >>> print np.hstack((betas, np.around(np.hstack((reg.betas, np.sqrt(reg.vm.diagonal()).reshape(5,1))),5)))
    [['CONSTANT' '111.7705' '67.75191']
     ['inc' '-0.30974' '1.16656']
     ['crime' '-1.36043' '0.6841']
     ['W_hoval' '-0.52908' '0.84428']
     ['lambda' '0.60116' '0.18605']]

    '''

    def __init__(self, y, x, yend=None, q=None,
                 w=None, w_lags=1, lag_q=True,
                 max_iter=1, epsilon=0.00001, A1='hom_sc',
                 vm=False, name_y=None, name_x=None,
                 name_yend=None, name_q=None,
                 name_w=None, name_ds=None):

        n = USER.check_arrays(y, x, yend, q)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        yend2, q2 = set_endog(y, x, w, yend, q, w_lags, lag_q)
        x_constant = USER.check_constant(x)
        BaseGM_Combo_Hom.__init__(
            self, y=y, x=x_constant, w=w.sparse, yend=yend2, q=q2,
            w_lags=w_lags, A1=A1, lag_q=lag_q,
            max_iter=max_iter, epsilon=epsilon)
        self.rho = self.betas[-2]
        self.predy_e, self.e_pred, warn = sp_att(w, self.y, self.predy,
                                                 yend2[:, -1].reshape(self.n, 1), self.rho)
        set_warn(self, warn)
        self.title = "SPATIALLY WEIGHTED TWO STAGE LEAST SQUARES (HOM)"
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_y = USER.set_name_y(name_y)
        self.name_x = USER.set_name_x(name_x, x)
        self.name_yend = USER.set_name_yend(name_yend, yend)
        self.name_yend.append(USER.set_name_yend_sp(self.name_y))
        self.name_z = self.name_x + self.name_yend
        self.name_z.append('lambda')  # listing lambda last
        self.name_q = USER.set_name_q(name_q, q)
        self.name_q.extend(
            USER.set_name_q_sp(self.name_x, w_lags, self.name_q, lag_q))
        self.name_h = USER.set_name_h(self.name_x, self.name_q)
        self.name_w = USER.set_name_w(name_w, w)
        SUMMARY.GM_Combo_Hom(reg=self, w=w, vm=vm)


# Functions

def moments_hom(w, wA1, wA2, u):
    '''
    Compute G and g matrices for the spatial error model with homoscedasticity
    as in Anselin [1]_ (2011).
    ...

    Parameters
    ----------

    w           : Sparse matrix
                  Spatial weights sparse matrix   

    u           : array
                  Residuals. nx1 array assumed to be aligned with w
 
    Attributes
    ----------

    moments     : list
                  List of two arrays corresponding to the matrices 'G' and
                  'g', respectively.


    References
    ----------

    .. [1] Anselin, L. (2011) "GMM Estimation of Spatial Error Autocorrelation
    with and without Heteroskedasticity". 
    '''
    n = w.shape[0]
    A1u = wA1 * u
    A2u = wA2 * u
    wu = w * u

    g1 = np.dot(u.T, A1u)
    g2 = np.dot(u.T, A2u)
    g = np.array([[g1][0][0], [g2][0][0]]) / n

    G11 = 2 * np.dot(wu.T * wA1, u)
    G12 = -np.dot(wu.T * wA1, wu)
    G21 = 2 * np.dot(wu.T * wA2, u)
    G22 = -np.dot(wu.T * wA2, wu)
    G = np.array([[G11[0][0], G12[0][0]], [G21[0][0], G22[0][0]]]) / n
    return [G, g]


def get_vc_hom(w, wA1, wA2, reg, lambdapar, z_s=None, for_omegaOLS=False):
    '''
    VC matrix \psi of Spatial error with homoscedasticity. As in 
    Anselin (2011) [1]_ (p. 20)
    ...

    Parameters
    ----------
    w               :   Sparse matrix
                        Spatial weights sparse matrix
    reg             :   reg
                        Regression object
    lambdapar       :   float
                        Spatial parameter estimated in previous step of the
                        procedure
    z_s             :   array
                        optional argument for spatially filtered Z (to be
                        passed only if endogenous variables are present)
    for_omegaOLS    :   boolean
                        If True (default=False), it also returns P, needed
                        only in the computation of Omega

    Returns
    -------

    psi         : array
                  2x2 VC matrix
    a1          : array
                  nx1 vector a1. If z_s=None, a1 = 0.
    a2          : array
                  nx1 vector a2. If z_s=None, a2 = 0.
    p           : array
                  P matrix. If z_s=None or for_omegaOLS=False, p=0.

    References
    ----------

    .. [1] Anselin, L. (2011) "GMM Estimation of Spatial Error Autocorrelation
    with and without Heteroskedasticity". 

    '''
    u_s = get_spFilter(w, lambdapar, reg.u)
    n = float(w.shape[0])
    sig2 = np.dot(u_s.T, u_s) / n
    mu3 = np.sum(u_s ** 3) / n
    mu4 = np.sum(u_s ** 4) / n

    tr11 = wA1 * wA1
    tr11 = np.sum(tr11.diagonal())
    tr12 = wA1 * (wA2 * 2)
    tr12 = np.sum(tr12.diagonal())
    tr22 = wA2 * wA2 * 2
    tr22 = np.sum(tr22.diagonal())
    vecd1 = np.array([wA1.diagonal()]).T

    psi11 = 2 * sig2 ** 2 * tr11 + \
        (mu4 - 3 * sig2 ** 2) * np.dot(vecd1.T, vecd1)
    psi12 = sig2 ** 2 * tr12
    psi22 = sig2 ** 2 * tr22

    a1, a2, p = 0., 0., 0.

    if for_omegaOLS:
        x_s = get_spFilter(w, lambdapar, reg.x)
        p = la.inv(spdot(x_s.T, x_s) / n)

    if issubclass(type(z_s), np.ndarray) or \
            issubclass(type(z_s), SP.csr.csr_matrix) or \
            issubclass(type(z_s), SP.csc.csc_matrix):
        alpha1 = (-2 / n) * spdot(z_s.T, wA1 * u_s)
        alpha2 = (-2 / n) * spdot(z_s.T, wA2 * u_s)

        hth = spdot(reg.h.T, reg.h)
        hthni = la.inv(hth / n)
        htzsn = spdot(reg.h.T, z_s) / n
        p = spdot(hthni, htzsn)
        p = spdot(p, la.inv(spdot(htzsn.T, p)))
        hp = spdot(reg.h, p)
        a1 = spdot(hp, alpha1)
        a2 = spdot(hp, alpha2)

        psi11 = psi11 + \
            sig2 * spdot(a1.T, a1) + \
            2 * mu3 * spdot(a1.T, vecd1)
        psi12 = psi12 + \
            sig2 * spdot(a1.T, a2) + \
            mu3 * spdot(a2.T, vecd1)  # 3rd term=0
        psi22 = psi22 + \
            sig2 * spdot(a2.T, a2)  # 3rd&4th terms=0 bc vecd2=0

    psi = np.array(
        [[psi11[0][0], psi12[0][0]], [psi12[0][0], psi22[0][0]]]) / n
    return psi, a1, a2, p


def get_omega_hom(w, wA1, wA2, reg, lamb, G):
    '''
    Omega VC matrix for Hom models with endogenous variables computed as in
    Anselin (2011) [1]_ (p. 21).
    ...

    Parameters
    ----------
    w       :   Sparse matrix
                Spatial weights sparse matrix
    reg     :   reg
                Regression object
    lamb    :   float
                Spatial parameter estimated in previous step of the
                procedure
    G       :   array
                Matrix 'G' of the moment equation

    Returns
    -------
    omega   :   array
                Omega matrix of VC of the model

    References
    ----------

    .. [1] Anselin, L. (2011) "GMM Estimation of Spatial Error Autocorrelation
    with and without Heteroskedasticity". 

    '''
    n = float(w.shape[0])
    z_s = get_spFilter(w, lamb, reg.z)
    u_s = get_spFilter(w, lamb, reg.u)
    sig2 = np.dot(u_s.T, u_s) / n
    mu3 = np.sum(u_s ** 3) / n
    vecdA1 = np.array([wA1.diagonal()]).T
    psi, a1, a2, p = get_vc_hom(w, wA1, wA2, reg, lamb, z_s)
    j = np.dot(G, np.array([[1.], [2 * lamb]]))
    psii = la.inv(psi)
    t2 = spdot(reg.h.T, np.hstack((a1, a2)))
    psiDL = (mu3 * spdot(reg.h.T, np.hstack((vecdA1, np.zeros((n, 1))))) +
             sig2 * spdot(reg.h.T, np.hstack((a1, a2)))) / n

    oDD = spdot(la.inv(spdot(reg.h.T, reg.h)), spdot(reg.h.T, z_s))
    oDD = sig2 * la.inv(spdot(z_s.T, spdot(reg.h, oDD)))
    oLL = la.inv(spdot(j.T, spdot(psii, j))) / n
    oDL = spdot(spdot(spdot(p.T, psiDL), spdot(psii, j)), oLL)

    o_upper = np.hstack((oDD, oDL))
    o_lower = np.hstack((oDL.T, oLL))
    return np.vstack((o_upper, o_lower)), float(sig2)


def get_omega_hom_ols(w, wA1, wA2, reg, lamb, G):
    '''
    Omega VC matrix for Hom models without endogenous variables (OLS) computed
    as in Anselin (2011) [1]_.
    ...

    Parameters
    ----------
    w       :   Sparse matrix
                Spatial weights sparse matrix
    reg     :   reg
                Regression object
    lamb    :   float
                Spatial parameter estimated in previous step of the
                procedure
    G       :   array
                Matrix 'G' of the moment equation

    Returns
    -------
    omega   :   array
                Omega matrix of VC of the model

    References
    ----------

    .. [1] Anselin, L. (2011) "GMM Estimation of Spatial Error Autocorrelation
    with and without Heteroskedasticity". 

    '''
    n = float(w.shape[0])
    x_s = get_spFilter(w, lamb, reg.x)
    u_s = get_spFilter(w, lamb, reg.u)
    sig2 = np.dot(u_s.T, u_s) / n
    vecdA1 = np.array([wA1.diagonal()]).T
    psi, a1, a2, p = get_vc_hom(w, wA1, wA2, reg, lamb, for_omegaOLS=True)
    j = np.dot(G, np.array([[1.], [2 * lamb]]))
    psii = la.inv(psi)

    oDD = sig2 * la.inv(spdot(x_s.T, x_s))
    oLL = la.inv(spdot(j.T, spdot(psii, j))) / n
    #oDL = np.zeros((oDD.shape[0], oLL.shape[1]))
    mu3 = np.sum(u_s ** 3) / n
    psiDL = (mu3 * spdot(reg.x.T, np.hstack((vecdA1, np.zeros((n, 1)))))) / n
    oDL = spdot(spdot(spdot(p.T, psiDL), spdot(psii, j)), oLL)

    o_upper = np.hstack((oDD, oDL))
    o_lower = np.hstack((oDL.T, oLL))
    return np.vstack((o_upper, o_lower)), float(sig2)


def _test():
    import doctest
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)

if __name__ == '__main__':

    _test()

########NEW FILE########
__FILENAME__ = error_sp_hom_regimes
'''
Hom family of models with regimes. 
'''

__author__ = "Luc Anselin luc.anselin@asu.edu, Pedro V. Amaral pedro.amaral@asu.edu, Daniel Arribas-Bel darribas@asu.edu"

from scipy import sparse as SP
import numpy as np
import multiprocessing as mp
from numpy import linalg as la
from pysal import lag_spatial
from utils import power_expansion, set_endog, iter_msg, sp_att
from utils import get_A1_hom, get_A2_hom, get_A1_het, optim_moments
from utils import get_spFilter, get_lags, _moments2eqs
from utils import spdot, RegressionPropsY, set_warn
from ols import BaseOLS
from twosls import BaseTSLS
from error_sp_hom import BaseGM_Error_Hom, BaseGM_Endog_Error_Hom, moments_hom, get_vc_hom, get_omega_hom, get_omega_hom_ols
import regimes as REGI
import user_output as USER
import summary_output as SUMMARY
from platform import system


class GM_Error_Hom_Regimes(RegressionPropsY, REGI.Regimes_Frame):

    '''
    GMM method for a spatial error model with homoskedasticity, with regimes,
    results and diagnostics; based on Drukker et al. (2010) [1]_, following
    Anselin (2011) [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    w            : pysal W object
                   Spatial weights object
    constant_regi: ['one', 'many']
                   Switcher controlling the constant term setup. It may take
                   the following values:
                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime (default)
    cols2regi    : list, 'all'
                   Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all' (default), all the variables vary by regime.
    regime_err_sep : boolean
                   If True, a separate regression is run for each regime.
    max_iter     : int
                   Maximum number of iterations of steps 2a and 2b from Arraiz
                   et al. Note: epsilon provides an additional stop condition.
    epsilon      : float
                   Minimum change in lambda required to stop iterations of
                   steps 2a and 2b from Arraiz et al. Note: max_iter provides
                   an additional stop condition.
    A1           : string
                   If A1='het', then the matrix A1 is defined as in Arraiz et
                   al. If A1='hom', then as in Anselin (2011).  If
                   A1='hom_sc', then as in Drukker, Egger and Prucha (2010)
                   and Drukker, Prucha and Raciborski (2010).
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    cores        : integer
                   Specifies the number of cores to be used in multiprocessing
                   Default: all cores available (specified as cores=None).
                   Note: Multiprocessing currently not available on Windows.
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    name_regimes : string
                   Name of regime variable for use in the output

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    iter_stop    : string
                   Stop criterion reached during iteration of steps 2a and 2b
                   from Arraiz et al.
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    iteration    : integer
                   Number of iterations of steps 2a and 2b from Arraiz et al.
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    vm           : array
                   Variance covariance matrix (kxk)
    sig2         : float
                   Sigma squared used in computations
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    std_err      : array
                   1xk array of standard errors of the betas
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    xtx          : float
                   X'X
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    name_regimes : string
                   Name of regime variable for use in the output
    title        : string
                   Name of the regression method used
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    constant_regi: ['one', 'many']
                   Ignored if regimes=False. Constant option for regimes.
                   Switcher controlling the constant term setup. It may take
                   the following values:
                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime
    cols2regi    : list, 'all'
                   Ignored if regimes=False. Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all', all the variables vary by regime.
    regime_err_sep : boolean
                   If True, a separate regression is run for each regime.
    kr           : int
                   Number of variables/columns to be "regimized" or subject
                   to change by regime. These will result in one parameter
                   estimate by regime for each variable (i.e. nr parameters per
                   variable)
    kf           : int
                   Number of variables/columns to be considered fixed or
                   global across regimes and hence only obtain one parameter
                   estimate
    nr           : int
                   Number of different regimes in the 'regimes' list
    multi        : dictionary
                   Only available when multiple regressions are estimated,
                   i.e. when regime_err_sep=True and no variable is fixed
                   across regimes.
                   Contains all attributes of each individual regression

    References
    ----------

    .. [1] Drukker, D. M., Egger, P., Prucha, I. R. (2010)
    "On Two-step Estimation of a Spatial Autoregressive Model with Autoregressive
    Disturbances and Endogenous Regressors". Working paper.

    .. [2] Anselin, L. (2011) "GMM Estimation of Spatial Error Autocorrelation
    with and without Heteroskedasticity".

    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import numpy as np
    >>> import pysal

    Open data on NCOVR US County Homicides (3085 areas) using pysal.open().
    This is the DBF associated with the NAT shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.

    >>> db = pysal.open(pysal.examples.get_path("NAT.dbf"),'r')

    Extract the HR90 column (homicide rates in 1990) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y_var = 'HR90'
    >>> y = np.array([db.by_col(y_var)]).reshape(3085,1)

    Extract UE90 (unemployment rate) and PS90 (population structure) vectors from
    the DBF to be used as independent variables in the regression. Other variables
    can be inserted by adding their names to x_var, such as x_var = ['Var1','Var2','...]
    Note that PySAL requires this to be an nxj numpy array, where j is the
    number of independent variables (not including a constant). By default
    this model adds a vector of ones to the independent variables passed in.

    >>> x_var = ['PS90','UE90']
    >>> x = np.array([db.by_col(name) for name in x_var]).T

    The different regimes in this data are given according to the North and
    South dummy (SOUTH).

    >>> r_var = 'SOUTH'
    >>> regimes = db.by_col(r_var)

    Since we want to run a spatial lag model, we need to specify
    the spatial weights matrix that includes the spatial configuration of the
    observations. To do that, we can open an already existing gal file or
    create a new one. In this case, we will create one from ``NAT.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("NAT.shp"))

    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, this allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'

    We are all set with the preliminaries, we are good to run the model. In this
    case, we will need the variables and the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.

    >>> reg = GM_Error_Hom_Regimes(y, x, regimes, w=w, name_y=y_var, name_x=x_var, name_ds='NAT')

    Once we have run the model, we can explore a little bit the output. The
    regression object we have created has many attributes so take your time to
    discover them. This class offers an error model that assumes
    homoskedasticity but that unlike the models from
    ``pysal.spreg.error_sp``, it allows for inference on the spatial
    parameter. This is why you obtain as many coefficient estimates as
    standard errors, which you calculate taking the square root of the
    diagonal of the variance-covariance matrix of the parameters. Alternatively,
    we can have a summary of the output by typing: model.summary
    >>> print reg.name_x
    ['0_CONSTANT', '0_PS90', '0_UE90', '1_CONSTANT', '1_PS90', '1_UE90', 'lambda']

    >>> print np.around(reg.betas,4)
    [[ 0.069 ]
     [ 0.7885]
     [ 0.5398]
     [ 5.0948]
     [ 1.1965]
     [ 0.6018]
     [ 0.4104]]

    >>> print np.sqrt(reg.vm.diagonal())
    [ 0.39105854  0.15664624  0.05254328  0.48379958  0.20018799  0.05834139
      0.01882401]

    '''

    def __init__(self, y, x, regimes, w,
                 max_iter=1, epsilon=0.00001, A1='het', cores=None,
                 constant_regi='many', cols2regi='all', regime_err_sep=False,
                 vm=False, name_y=None, name_x=None,
                 name_w=None, name_ds=None, name_regimes=None):

        n = USER.check_arrays(y, x)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        self.constant_regi = constant_regi
        self.cols2regi = cols2regi
        self.regime_err_sep = regime_err_sep
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_y = USER.set_name_y(name_y)
        self.name_w = USER.set_name_w(name_w, w)
        self.name_regimes = USER.set_name_ds(name_regimes)
        self.n = n
        self.y = y

        x_constant = USER.check_constant(x)
        name_x = USER.set_name_x(name_x, x)
        self.name_x_r = name_x

        cols2regi = REGI.check_cols2regi(constant_regi, cols2regi, x)
        self.regimes_set = REGI._get_regimes_set(regimes)
        self.regimes = regimes
        USER.check_regimes(self.regimes_set, self.n, x.shape[1])
        self.regime_err_sep = regime_err_sep

        if regime_err_sep == True:
            if set(cols2regi) == set([True]):
                self._error_regimes_multi(y, x, regimes, w, cores,
                                          max_iter, epsilon, A1, cols2regi, vm, name_x)
            else:
                raise Exception, "All coefficients must vary accross regimes if regime_err_sep = True."
        else:
            if A1 == 'hom':
                wA1 = get_A1_hom(w.sparse)
            elif A1 == 'hom_sc':
                wA1 = get_A1_hom(w.sparse, scalarKP=True)
            elif A1 == 'het':
                wA1 = get_A1_het(w.sparse)

            wA2 = get_A2_hom(w.sparse)

            # 1a. OLS --> \tilde{\delta}
            self.x, self.name_x = REGI.Regimes_Frame.__init__(self, x_constant,
                                                              regimes, constant_regi=None, cols2regi=cols2regi, names=name_x)
            ols = BaseOLS(y=y, x=self.x)
            self.k = ols.x.shape[1]

            # 1b. GM --> \tilde{\rho}
            moments = moments_hom(w.sparse, wA1, wA2, ols.u)
            lambda1 = optim_moments(moments)
            lambda_old = lambda1

            self.iteration, eps = 0, 1
            while self.iteration < max_iter and eps > epsilon:
                # 2a. SWLS --> \hat{\delta}
                xs = get_spFilter(w, lambda1, x_constant)
                ys = get_spFilter(w, lambda1, y)
                xs = REGI.Regimes_Frame.__init__(self, xs,
                                                 regimes, constant_regi=None, cols2regi=cols2regi)[0]
                ols_s = BaseOLS(y=ys, x=xs)
                self.predy = spdot(self.x, ols_s.betas)
                self.u = self.y - self.predy

                # 2b. GM 2nd iteration --> \hat{\rho}
                moments = moments_hom(w.sparse, wA1, wA2, self.u)
                psi = get_vc_hom(w.sparse, wA1, wA2, self, lambda_old)[0]
                lambda2 = optim_moments(moments, psi)
                eps = abs(lambda2 - lambda_old)
                lambda_old = lambda2
                self.iteration += 1

            self.iter_stop = iter_msg(self.iteration, max_iter)

            # Output
            self.betas = np.vstack((ols_s.betas, lambda2))
            self.vm, self.sig2 = get_omega_hom_ols(
                w.sparse, wA1, wA2, self, lambda2, moments[0])
            self.e_filtered = self.u - lambda2 * lag_spatial(w, self.u)
            self.title = "SPATIALLY WEIGHTED LEAST SQUARES (HOM) - REGIMES"
            self.name_x.append('lambda')
            self.kf += 1
            self.chow = REGI.Chow(self)
            self._cache = {}
            SUMMARY.GM_Error_Hom(reg=self, w=w, vm=vm, regimes=True)

    def _error_regimes_multi(self, y, x, regimes, w, cores,
                             max_iter, epsilon, A1, cols2regi, vm, name_x):

        regi_ids = dict((r, list(np.where(np.array(regimes) == r)[0]))
                        for r in self.regimes_set)
        results_p = {}
        for r in self.regimes_set:
            if system() == 'Windows':
                is_win = True
                results_p[r] = _work_error(
                    *(y, x, regi_ids, r, w, max_iter, epsilon, A1,
                      self.name_ds, self.name_y, name_x + ['lambda'], self.name_w, self.name_regimes))
            else:
                pool = mp.Pool(cores)
                results_p[r] = pool.apply_async(_work_error, args=(
                    y, x, regi_ids, r, w, max_iter, epsilon, A1, self.name_ds, self.name_y, name_x + ['lambda'], self.name_w, self.name_regimes, ))
                is_win = False
        self.kryd = 0
        self.kr = len(cols2regi) + 1
        self.kf = 0
        self.nr = len(self.regimes_set)
        self.vm = np.zeros((self.nr * self.kr, self.nr * self.kr), float)
        self.betas = np.zeros((self.nr * self.kr, 1), float)
        self.u = np.zeros((self.n, 1), float)
        self.predy = np.zeros((self.n, 1), float)
        self.e_filtered = np.zeros((self.n, 1), float)
        self.name_y, self.name_x = [], []
        if not is_win:
            pool.close()
            pool.join()
        results = {}
        counter = 0
        for r in self.regimes_set:
            if is_win:
                results[r] = results_p[r]
            else:
                results[r] = results_p[r].get()
            self.vm[(counter * self.kr):((counter + 1) * self.kr),
                    (counter * self.kr):((counter + 1) * self.kr)] = results[r].vm
            self.betas[(counter * self.kr):((counter + 1) * self.kr),
                       ] = results[r].betas
            self.u[regi_ids[r], ] = results[r].u
            self.predy[regi_ids[r], ] = results[r].predy
            self.e_filtered[regi_ids[r], ] = results[r].e_filtered
            self.name_y += results[r].name_y
            self.name_x += results[r].name_x
            counter += 1
        self.chow = REGI.Chow(self)
        self.multi = results
        SUMMARY.GM_Error_Hom_multi(
            reg=self, multireg=self.multi, vm=vm, regimes=True)


class GM_Endog_Error_Hom_Regimes(RegressionPropsY, REGI.Regimes_Frame):

    '''
    GMM method for a spatial error model with homoskedasticity, regimes and
    endogenous variables.
    Based on Drukker et al. (2010) [1]_, following Anselin (2011) [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note:
                   this should not contain any variables from x)
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    w            : pysal W object
                   Spatial weights object
    constant_regi: ['one', 'many']
                   Switcher controlling the constant term setup. It may take
                   the following values:
                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime (default)
    cols2regi    : list, 'all'
                   Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all' (default), all the variables vary by regime.
    regime_err_sep : boolean
                   If True, a separate regression is run for each regime.
    max_iter     : int
                   Maximum number of iterations of steps 2a and 2b from Arraiz
                   et al. Note: epsilon provides an additional stop condition.
    epsilon      : float
                   Minimum change in lambda required to stop iterations of
                   steps 2a and 2b from Arraiz et al. Note: max_iter provides
                   an additional stop condition.
    A1           : string
                   If A1='het', then the matrix A1 is defined as in Arraiz et
                   al. If A1='hom', then as in Anselin (2011).  If
                   A1='hom_sc', then as in Drukker, Egger and Prucha (2010)
                   and Drukker, Prucha and Raciborski (2010).
    cores        : integer
                   Specifies the number of cores to be used in multiprocessing
                   Default: all cores available (specified as cores=None).
                   Note: Multiprocessing currently not available on Windows.
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_yend    : list of strings
                   Names of endogenous variables for use in output
    name_q       : list of strings
                   Names of instruments for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    name_regimes : string
                   Name of regime variable for use in the output

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable used as instruments
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    z            : array
                   nxk array of variables (combination of x and yend)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    h            : array
                   nxl array of instruments (combination of x and q)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    iter_stop    : string
                   Stop criterion reached during iteration of steps 2a and 2b
                   from Arraiz et al.
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    iteration    : integer
                   Number of iterations of steps 2a and 2b from Arraiz et al.
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    sig2         : float
                   Sigma squared used in computations
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    std_err      : array
                   1xk array of standard errors of the betas
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    hth          : float
                   H'H
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    name_y        : string
                    Name of dependent variable for use in output
    name_x        : list of strings
                    Names of independent variables for use in output
    name_yend     : list of strings
                    Names of endogenous variables for use in output
    name_z        : list of strings
                    Names of exogenous and endogenous variables for use in
                    output
    name_q        : list of strings
                    Names of external instruments
    name_h        : list of strings
                    Names of all instruments used in ouput
    name_w        : string
                    Name of weights matrix for use in output
    name_ds       : string
                    Name of dataset for use in output
    name_regimes  : string
                    Name of regimes variable for use in output
    title         : string
                    Name of the regression method used
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    regimes       : list
                    List of n values with the mapping of each
                    observation to a regime. Assumed to be aligned with 'x'.
    constant_regi : ['one', 'many']
                    Ignored if regimes=False. Constant option for regimes.
                    Switcher controlling the constant term setup. It may take
                    the following values:
                      *  'one': a vector of ones is appended to x and held
                                constant across regimes
                      * 'many': a vector of ones is appended to x and considered
                                different per regime
    cols2regi     : list, 'all'
                    Ignored if regimes=False. Argument indicating whether each
                    column of x should be considered as different per regime
                    or held constant across regimes (False).
                    If a list, k booleans indicating for each variable the
                    option (True if one per regime, False to be held constant).
                    If 'all', all the variables vary by regime.
    regime_err_sep : boolean
                   If True, a separate regression is run for each regime.
    kr            : int
                    Number of variables/columns to be "regimized" or subject
                    to change by regime. These will result in one parameter
                    estimate by regime for each variable (i.e. nr parameters per
                    variable)
    kf            : int
                    Number of variables/columns to be considered fixed or
                    global across regimes and hence only obtain one parameter
                    estimate
    nr            : int
                    Number of different regimes in the 'regimes' list
    multi         : dictionary
                    Only available when multiple regressions are estimated,
                    i.e. when regime_err_sep=True and no variable is fixed
                    across regimes.
                    Contains all attributes of each individual regression

    References
    ----------

    .. [1] Drukker, D. M., Egger, P., Prucha, I. R. (2010)
    "On Two-step Estimation of a Spatial Autoregressive Model with Autoregressive
    Disturbances and Endogenous Regressors". Working paper.

    .. [2] Anselin, L. (2011) "GMM Estimation of Spatial Error Autocorrelation
    with and without Heteroskedasticity".


    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import numpy as np
    >>> import pysal

    Open data on NCOVR US County Homicides (3085 areas) using pysal.open().
    This is the DBF associated with the NAT shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.

    >>> db = pysal.open(pysal.examples.get_path("NAT.dbf"),'r')

    Extract the HR90 column (homicide rates in 1990) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y_var = 'HR90'
    >>> y = np.array([db.by_col(y_var)]).reshape(3085,1)

    Extract UE90 (unemployment rate) and PS90 (population structure) vectors from
    the DBF to be used as independent variables in the regression. Other variables
    can be inserted by adding their names to x_var, such as x_var = ['Var1','Var2','...]
    Note that PySAL requires this to be an nxj numpy array, where j is the
    number of independent variables (not including a constant). By default
    this model adds a vector of ones to the independent variables passed in.

    >>> x_var = ['PS90','UE90']
    >>> x = np.array([db.by_col(name) for name in x_var]).T

    For the endogenous models, we add the endogenous variable RD90 (resource deprivation)
    and we decide to instrument for it with FP89 (families below poverty):

    >>> yd_var = ['RD90']
    >>> yend = np.array([db.by_col(name) for name in yd_var]).T
    >>> q_var = ['FP89']
    >>> q = np.array([db.by_col(name) for name in q_var]).T

    The different regimes in this data are given according to the North and
    South dummy (SOUTH).

    >>> r_var = 'SOUTH'
    >>> regimes = db.by_col(r_var)

    Since we want to run a spatial error model, we need to specify the spatial
    weights matrix that includes the spatial configuration of the observations
    into the error component of the model. To do that, we can open an already
    existing gal file or create a new one. In this case, we will create one
    from ``NAT.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("NAT.shp"))

    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, this allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'

    We are all set with the preliminaries, we are good to run the model. In this
    case, we will need the variables (exogenous and endogenous), the
    instruments and the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.

    >>> reg = GM_Endog_Error_Hom_Regimes(y, x, yend, q, regimes, w=w, A1='hom_sc', name_y=y_var, name_x=x_var, name_yend=yd_var, name_q=q_var, name_regimes=r_var, name_ds='NAT.dbf')

    Once we have run the model, we can explore a little bit the output. The
    regression object we have created has many attributes so take your time to
    discover them. This class offers an error model that assumes
    homoskedasticity but that unlike the models from
    ``pysal.spreg.error_sp``, it allows for inference on the spatial
    parameter. Hence, we find the same number of betas as of standard errors,
    which we calculate taking the square root of the diagonal of the
    variance-covariance matrix. Alternatively, we can have a summary of the
    output by typing: model.summary

    >>> print reg.name_z
    ['0_CONSTANT', '0_PS90', '0_UE90', '1_CONSTANT', '1_PS90', '1_UE90', '0_RD90', '1_RD90', 'lambda']

    >>> print np.around(reg.betas,4)
    [[ 3.5973]
     [ 1.0652]
     [ 0.1582]
     [ 9.198 ]
     [ 1.8809]
     [-0.2489]
     [ 2.4616]
     [ 3.5796]
     [ 0.2541]]

    >>> print np.around(np.sqrt(reg.vm.diagonal()),4)
    [ 0.5204  0.1371  0.0629  0.4721  0.1824  0.0725  0.2992  0.2395  0.024 ]

    '''

    def __init__(self, y, x, yend, q, regimes, w,
                 constant_regi='many', cols2regi='all', regime_err_sep=False,
                 max_iter=1, epsilon=0.00001, A1='het', cores=None,
                 vm=False, name_y=None, name_x=None,
                 name_yend=None, name_q=None, name_w=None,
                 name_ds=None, name_regimes=None, summ=True, add_lag=False):

        n = USER.check_arrays(y, x, yend, q)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        self.constant_regi = constant_regi
        self.cols2regi = cols2regi
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_regimes = USER.set_name_ds(name_regimes)
        self.name_w = USER.set_name_w(name_w, w)
        self.n = n
        self.y = y

        name_x = USER.set_name_x(name_x, x)
        if summ:
            name_yend = USER.set_name_yend(name_yend, yend)
            self.name_y = USER.set_name_y(name_y)
            name_q = USER.set_name_q(name_q, q)
        self.name_x_r = name_x + name_yend

        cols2regi = REGI.check_cols2regi(
            constant_regi, cols2regi, x, yend=yend)
        self.regimes_set = REGI._get_regimes_set(regimes)
        self.regimes = regimes
        USER.check_regimes(self.regimes_set, self.n, x.shape[1])
        self.regime_err_sep = regime_err_sep

        if regime_err_sep == True:
            if set(cols2regi) == set([True]):
                self._endog_error_regimes_multi(
                    y, x, regimes, w, yend, q, cores,
                    max_iter, epsilon, A1, cols2regi, vm,
                    name_x, name_yend, name_q, add_lag)
            else:
                raise Exception, "All coefficients must vary accross regimes if regime_err_sep = True."
        else:
            x_constant = USER.check_constant(x)
            q, name_q = REGI.Regimes_Frame.__init__(self, q,
                                                    regimes, constant_regi=None, cols2regi='all', names=name_q)
            x, name_x = REGI.Regimes_Frame.__init__(self, x_constant,
                                                    regimes, constant_regi=None, cols2regi=cols2regi,
                                                    names=name_x)
            yend2, name_yend = REGI.Regimes_Frame.__init__(self, yend,
                                                           regimes, constant_regi=None,
                                                           cols2regi=cols2regi, yend=True, names=name_yend)

            if A1 == 'hom':
                wA1 = get_A1_hom(w.sparse)
            elif A1 == 'hom_sc':
                wA1 = get_A1_hom(w.sparse, scalarKP=True)
            elif A1 == 'het':
                wA1 = get_A1_het(w.sparse)

            wA2 = get_A2_hom(w.sparse)

            # 1a. S2SLS --> \tilde{\delta}
            tsls = BaseTSLS(y=y, x=x, yend=yend2, q=q)
            self.k = tsls.z.shape[1]
            self.x = tsls.x
            self.yend, self.z, self.h = tsls.yend, tsls.z, tsls.h

            # 1b. GM --> \tilde{\rho}
            moments = moments_hom(w.sparse, wA1, wA2, tsls.u)
            lambda1 = optim_moments(moments)
            lambda_old = lambda1

            self.iteration, eps = 0, 1
            while self.iteration < max_iter and eps > epsilon:
                # 2a. GS2SLS --> \hat{\delta}
                xs = get_spFilter(w, lambda1, x_constant)
                xs = REGI.Regimes_Frame.__init__(self, xs,
                                                 regimes, constant_regi=None, cols2regi=cols2regi)[0]
                ys = get_spFilter(w, lambda1, y)
                yend_s = get_spFilter(w, lambda1, yend)
                yend_s = REGI.Regimes_Frame.__init__(self, yend_s,
                                                     regimes, constant_regi=None, cols2regi=cols2regi,
                                                     yend=True)[0]
                tsls_s = BaseTSLS(ys, xs, yend_s, h=tsls.h)
                self.predy = spdot(self.z, tsls_s.betas)
                self.u = self.y - self.predy

                # 2b. GM 2nd iteration --> \hat{\rho}
                moments = moments_hom(w.sparse, wA1, wA2, self.u)
                psi = get_vc_hom(w.sparse, wA1, wA2, self,
                                 lambda_old, tsls_s.z)[0]
                lambda2 = optim_moments(moments, psi)
                eps = abs(lambda2 - lambda_old)
                lambda_old = lambda2
                self.iteration += 1

            self.iter_stop = iter_msg(self.iteration, max_iter)

            # Output
            self.betas = np.vstack((tsls_s.betas, lambda2))
            self.vm, self.sig2 = get_omega_hom(
                w.sparse, wA1, wA2, self, lambda2, moments[0])
            self.e_filtered = self.u - lambda2 * lag_spatial(w, self.u)
            self.name_x = USER.set_name_x(name_x, x, constant=True)
            self.name_yend = USER.set_name_yend(name_yend, yend)
            self.name_z = self.name_x + self.name_yend
            self.name_z.append('lambda')
            self.name_q = USER.set_name_q(name_q, q)
            self.name_h = USER.set_name_h(self.name_x, self.name_q)
            self.kf += 1
            self.chow = REGI.Chow(self)
            self._cache = {}
            if summ:
                self.title = "SPATIALLY WEIGHTED TWO STAGE LEAST SQUARES - REGIMES"
                SUMMARY.GM_Endog_Error_Hom(reg=self, w=w, vm=vm, regimes=True)

    def _endog_error_regimes_multi(self, y, x, regimes, w, yend, q, cores,
                                   max_iter, epsilon, A1, cols2regi, vm,
                                   name_x, name_yend, name_q, add_lag):

        regi_ids = dict((r, list(np.where(np.array(regimes) == r)[0]))
                        for r in self.regimes_set)
        if add_lag != False:
            self.cols2regi += [True]
            cols2regi += [True]
            self.predy_e = np.zeros((self.n, 1), float)
            self.e_pred = np.zeros((self.n, 1), float)
        results_p = {}
        for r in self.regimes_set:
            if system() == 'Windows':
                is_win = True
                results_p[r] = _work_endog_error(
                    *(y, x, yend, q, regi_ids, r, w, max_iter, epsilon, A1,
                      self.name_ds, self.name_y, name_x, name_yend, name_q, self.name_w, self.name_regimes, add_lag))
            else:
                pool = mp.Pool(cores)
                results_p[r] = pool.apply_async(
                    _work_endog_error, args=(
                        y, x, yend, q, regi_ids, r, w, max_iter, epsilon,
                        A1, self.name_ds, self.name_y, name_x, name_yend, name_q, self.name_w, self.name_regimes, add_lag, ))
                is_win = False
        self.kryd, self.kf = 0, 0
        self.kr = len(cols2regi) + 1
        self.nr = len(self.regimes_set)
        self.vm = np.zeros((self.nr * self.kr, self.nr * self.kr), float)
        self.betas = np.zeros((self.nr * self.kr, 1), float)
        self.u = np.zeros((self.n, 1), float)
        self.predy = np.zeros((self.n, 1), float)
        self.e_filtered = np.zeros((self.n, 1), float)
        if not is_win:
            pool.close()
            pool.join()
        results = {}
        self.name_y, self.name_x, self.name_yend, self.name_q, self.name_z, self.name_h = [
        ], [], [], [], [], []
        counter = 0
        for r in self.regimes_set:
            if is_win:
                results[r] = results_p[r]
            else:
                results[r] = results_p[r].get()
            self.vm[(counter * self.kr):((counter + 1) * self.kr),
                    (counter * self.kr):((counter + 1) * self.kr)] = results[r].vm
            self.betas[(counter * self.kr):((counter + 1) * self.kr),
                       ] = results[r].betas
            self.u[regi_ids[r], ] = results[r].u
            self.predy[regi_ids[r], ] = results[r].predy
            self.e_filtered[regi_ids[r], ] = results[r].e_filtered
            self.name_y += results[r].name_y
            self.name_x += results[r].name_x
            self.name_yend += results[r].name_yend
            self.name_q += results[r].name_q
            self.name_z += results[r].name_z
            self.name_h += results[r].name_h
            if add_lag != False:
                self.predy_e[regi_ids[r], ] = results[r].predy_e
                self.e_pred[regi_ids[r], ] = results[r].e_pred
            counter += 1
        self.chow = REGI.Chow(self)
        self.multi = results
        if add_lag != False:
            SUMMARY.GM_Combo_Hom_multi(
                reg=self, multireg=self.multi, vm=vm, regimes=True)
        else:
            SUMMARY.GM_Endog_Error_Hom_multi(
                reg=self, multireg=self.multi, vm=vm, regimes=True)


class GM_Combo_Hom_Regimes(GM_Endog_Error_Hom_Regimes):

    '''
    GMM method for a spatial lag and error model with homoskedasticity,
    regimes and endogenous variables, with results and diagnostics;
    based on Drukker et al. (2010) [1]_, following Anselin (2011) [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note:
                   this should not contain any variables from x)
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    w            : pysal W object
                   Spatial weights object (always needed)
    constant_regi: ['one', 'many']
                   Switcher controlling the constant term setup. It may take
                   the following values:
                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime (default)
    cols2regi    : list, 'all'
                   Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all' (default), all the variables vary by regime.
    regime_err_sep : boolean
                   If True, a separate regression is run for each regime.
    regime_lag_sep   : boolean
                   If True, the spatial parameter for spatial lag is also
                   computed according to different regimes. If False (default),
                   the spatial parameter is fixed accross regimes.
    w_lags       : integer
                   Orders of W to include as instruments for the spatially
                   lagged dependent variable. For example, w_lags=1, then
                   instruments are WX; if w_lags=2, then WX, WWX; and so on.
    lag_q        : boolean
                   If True, then include spatial lags of the additional
                   instruments (q).
    max_iter     : int
                   Maximum number of iterations of steps 2a and 2b from Arraiz
                   et al. Note: epsilon provides an additional stop condition.
    epsilon      : float
                   Minimum change in lambda required to stop iterations of
                   steps 2a and 2b from Arraiz et al. Note: max_iter provides
                   an additional stop condition.
    A1           : string
                   If A1='het', then the matrix A1 is defined as in Arraiz et
                   al. If A1='hom', then as in Anselin (2011).  If
                   A1='hom_sc', then as in Drukker, Egger and Prucha (2010)
                   and Drukker, Prucha and Raciborski (2010).
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    cores        : integer
                   Specifies the number of cores to be used in multiprocessing
                   Default: all cores available (specified as cores=None).
                   Note: Multiprocessing currently not available on Windows.
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_yend    : list of strings
                   Names of endogenous variables for use in output
    name_q       : list of strings
                   Names of instruments for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    name_regimes : string
                   Name of regime variable for use in the output

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    e_pred       : array
                   nx1 array of residuals (using reduced form)
    predy        : array
                   nx1 array of predicted y values
    predy_e      : array
                   nx1 array of predicted y values (using reduced form)
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable used as instruments
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    z            : array
                   nxk array of variables (combination of x and yend)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    h            : array
                   nxl array of instruments (combination of x and q)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    iter_stop    : string
                   Stop criterion reached during iteration of steps 2a and 2b
                   from Arraiz et al.
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    iteration    : integer
                   Number of iterations of steps 2a and 2b from Arraiz et al.
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    pr2_e        : float
                   Pseudo R squared (squared correlation between y and ypred_e
                   (using reduced form))
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    sig2         : float
                   Sigma squared used in computations (based on filtered
                   residuals)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    std_err      : array
                   1xk array of standard errors of the betas
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    name_y        : string
                    Name of dependent variable for use in output
    name_x        : list of strings
                    Names of independent variables for use in output
    name_yend     : list of strings
                    Names of endogenous variables for use in output
    name_z        : list of strings
                    Names of exogenous and endogenous variables for use in
                    output
    name_q        : list of strings
                    Names of external instruments
    name_h        : list of strings
                    Names of all instruments used in ouput
    name_w        : string
                    Name of weights matrix for use in output
    name_ds       : string
                    Name of dataset for use in output
    name_regimes  : string
                    Name of regimes variable for use in output
    title         : string
                    Name of the regression method used
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    regimes       : list
                    List of n values with the mapping of each
                    observation to a regime. Assumed to be aligned with 'x'.
    constant_regi : ['one', 'many']
                    Ignored if regimes=False. Constant option for regimes.
                    Switcher controlling the constant term setup. It may take
                    the following values:
                      *  'one': a vector of ones is appended to x and held
                                constant across regimes
                      * 'many': a vector of ones is appended to x and considered
                                different per regime
    cols2regi     : list, 'all'
                    Ignored if regimes=False. Argument indicating whether each
                    column of x should be considered as different per regime
                    or held constant across regimes (False).
                    If a list, k booleans indicating for each variable the
                    option (True if one per regime, False to be held constant).
                    If 'all', all the variables vary by regime.
    regime_err_sep : boolean
                   If True, a separate regression is run for each regime.
    regime_lag_sep    : boolean
                    If True, the spatial parameter for spatial lag is also
                    computed according to different regimes. If False (default),
                    the spatial parameter is fixed accross regimes.
    kr            : int
                    Number of variables/columns to be "regimized" or subject
                    to change by regime. These will result in one parameter
                    estimate by regime for each variable (i.e. nr parameters per
                    variable)
    kf            : int
                    Number of variables/columns to be considered fixed or
                    global across regimes and hence only obtain one parameter
                    estimate
    nr            : int
                    Number of different regimes in the 'regimes' list
    multi         : dictionary
                    Only available when multiple regressions are estimated,
                    i.e. when regime_err_sep=True and no variable is fixed
                    across regimes.
                    Contains all attributes of each individual regression


    References
    ----------

    .. [1] Drukker, D. M., Egger, P., Prucha, I. R. (2010)
    "On Two-step Estimation of a Spatial Autoregressive Model with Autoregressive
    Disturbances and Endogenous Regressors". Working paper.

    .. [2] Anselin, L. (2011) "GMM Estimation of Spatial Error Autocorrelation
    with and without Heteroskedasticity".

    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import numpy as np
    >>> import pysal

    Open data on NCOVR US County Homicides (3085 areas) using pysal.open().
    This is the DBF associated with the NAT shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.

    >>> db = pysal.open(pysal.examples.get_path("NAT.dbf"),'r')

    Extract the HR90 column (homicide rates in 1990) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y_var = 'HR90'
    >>> y = np.array([db.by_col(y_var)]).reshape(3085,1)

    Extract UE90 (unemployment rate) and PS90 (population structure) vectors from
    the DBF to be used as independent variables in the regression. Other variables
    can be inserted by adding their names to x_var, such as x_var = ['Var1','Var2','...]
    Note that PySAL requires this to be an nxj numpy array, where j is the
    number of independent variables (not including a constant). By default
    this model adds a vector of ones to the independent variables passed in.

    >>> x_var = ['PS90','UE90']
    >>> x = np.array([db.by_col(name) for name in x_var]).T

    The different regimes in this data are given according to the North and
    South dummy (SOUTH).

    >>> r_var = 'SOUTH'
    >>> regimes = db.by_col(r_var)

    Since we want to run a spatial combo model, we need to specify
    the spatial weights matrix that includes the spatial configuration of the
    observations. To do that, we can open an already existing gal file or
    create a new one. In this case, we will create one from ``NAT.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("NAT.shp"))

    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, this allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'

    We are all set with the preliminaries, we are good to run the model. In this
    case, we will need the variables and the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.

    Example only with spatial lag

    The Combo class runs an SARAR model, that is a spatial lag+error model.
    In this case we will run a simple version of that, where we have the
    spatial effects as well as exogenous variables. Since it is a spatial
    model, we have to pass in the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.  We can have a
    summary of the output by typing: model.summary
    Alternatively, we can check the betas:

    >>> reg = GM_Combo_Hom_Regimes(y, x, regimes, w=w, A1='hom_sc', name_y=y_var, name_x=x_var, name_regimes=r_var, name_ds='NAT')
    >>> print reg.name_z
    ['0_CONSTANT', '0_PS90', '0_UE90', '1_CONSTANT', '1_PS90', '1_UE90', '_Global_W_HR90', 'lambda']
    >>> print np.around(reg.betas,4)
    [[ 1.4607]
     [ 0.9579]
     [ 0.5658]
     [ 9.1129]
     [ 1.1339]
     [ 0.6517]
     [-0.4583]
     [ 0.6634]]

    This class also allows the user to run a spatial lag+error model with the
    extra feature of including non-spatial endogenous regressors. This means
    that, in addition to the spatial lag and error, we consider some of the
    variables on the right-hand side of the equation as endogenous and we
    instrument for this. In this case we consider RD90 (resource deprivation)
    as an endogenous regressor.  We use FP89 (families below poverty)
    for this and hence put it in the instruments parameter, 'q'.

    >>> yd_var = ['RD90']
    >>> yd = np.array([db.by_col(name) for name in yd_var]).T
    >>> q_var = ['FP89']
    >>> q = np.array([db.by_col(name) for name in q_var]).T

    And then we can run and explore the model analogously to the previous combo:

    >>> reg = GM_Combo_Hom_Regimes(y, x, regimes, yd, q, w=w, A1='hom_sc', name_y=y_var, name_x=x_var, name_yend=yd_var, name_q=q_var, name_regimes=r_var, name_ds='NAT')
    >>> print reg.name_z
    ['0_CONSTANT', '0_PS90', '0_UE90', '1_CONSTANT', '1_PS90', '1_UE90', '0_RD90', '1_RD90', '_Global_W_HR90', 'lambda']
    >>> print reg.betas
    [[ 3.4196478 ]
     [ 1.04065595]
     [ 0.16630304]
     [ 8.86570777]
     [ 1.85134286]
     [-0.24921597]
     [ 2.43007651]
     [ 3.61656899]
     [ 0.03315061]
     [ 0.22636055]]
    >>> print np.sqrt(reg.vm.diagonal())
    [ 0.53989913  0.13506086  0.06143434  0.77049956  0.18089997  0.07246848
      0.29218837  0.25378655  0.06184801  0.06323236]
    >>> print 'lambda: ', np.around(reg.betas[-1], 4)
    lambda:  [ 0.2264]

    '''

    def __init__(self, y, x, regimes, yend=None, q=None,
                 w=None, w_lags=1, lag_q=True, cores=None,
                 max_iter=1, epsilon=0.00001, A1='het',
                 constant_regi='many', cols2regi='all',
                 regime_err_sep=False, regime_lag_sep=False,
                 vm=False, name_y=None, name_x=None,
                 name_yend=None, name_q=None,
                 name_w=None, name_ds=None, name_regimes=None):

        n = USER.check_arrays(y, x)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        name_x = USER.set_name_x(name_x, x, constant=True)
        self.name_y = USER.set_name_y(name_y)
        name_yend = USER.set_name_yend(name_yend, yend)
        name_q = USER.set_name_q(name_q, q)
        name_q.extend(USER.set_name_q_sp(name_x, w_lags, name_q, lag_q,
                      force_all=True))

        cols2regi = REGI.check_cols2regi(
            constant_regi, cols2regi, x, yend=yend, add_cons=False)
        self.regimes_set = REGI._get_regimes_set(regimes)
        self.regimes = regimes
        USER.check_regimes(self.regimes_set, n, x.shape[1])
        self.regime_err_sep = regime_err_sep
        self.regime_lag_sep = regime_lag_sep

        if regime_lag_sep == True:
            if regime_err_sep == False:
                raise Exception, "For spatial combo models, if spatial lag is set by regimes (regime_lag_sep=True), spatial error must also be set by regimes (regime_err_sep=True)."
            add_lag = [w_lags, lag_q]
        else:
            cols2regi += [False]
            add_lag = False
            if regime_err_sep == True:
                raise Exception, "For spatial combo models, if spatial error is set by regimes (regime_err_sep=True), all coefficients including lambda (regime_lag_sep=True) must be set by regimes."
            yend, q = set_endog(y, x, w, yend, q, w_lags, lag_q)
        name_yend.append(USER.set_name_yend_sp(self.name_y))

        GM_Endog_Error_Hom_Regimes.__init__(self, y=y, x=x, yend=yend,
                                            q=q, regimes=regimes, w=w, vm=vm, constant_regi=constant_regi,
                                            cols2regi=cols2regi, regime_err_sep=regime_err_sep,
                                            max_iter=max_iter, epsilon=epsilon, A1=A1, cores=cores,
                                            name_y=self.name_y, name_x=name_x, name_yend=name_yend,
                                            name_q=name_q, name_w=name_w, name_ds=name_ds,
                                            name_regimes=name_regimes, summ=False, add_lag=add_lag)

        if regime_err_sep != True:
            self.rho = self.betas[-2]
            self.predy_e, self.e_pred, warn = sp_att(w, self.y,
                                                     self.predy, yend[:, -1].reshape(self.n, 1), self.rho)
            set_warn(self, warn)
            self.regime_lag_sep = regime_lag_sep
            self.title = "SPATIALLY WEIGHTED TWO STAGE LEAST SQUARES (HOM) - REGIMES"
            SUMMARY.GM_Combo_Hom(reg=self, w=w, vm=vm, regimes=True)


def _work_error(y, x, regi_ids, r, w, max_iter, epsilon, A1, name_ds, name_y, name_x, name_w, name_regimes):
    w_r, warn = REGI.w_regime(w, regi_ids[r], r, transform=True)
    y_r = y[regi_ids[r]]
    x_r = x[regi_ids[r]]
    x_constant = USER.check_constant(x_r)
    model = BaseGM_Error_Hom(y_r, x_constant, w_r.sparse,
                             max_iter=max_iter, epsilon=epsilon, A1=A1)
    set_warn(model, warn)
    model.w = w_r
    model.title = "SPATIALLY WEIGHTED LEAST SQUARES ESTIMATION (HOM) - REGIME %s" % r
    model.name_ds = name_ds
    model.name_y = '%s_%s' % (str(r), name_y)
    model.name_x = ['%s_%s' % (str(r), i) for i in name_x]
    model.name_w = name_w
    model.name_regimes = name_regimes
    return model


def _work_endog_error(y, x, yend, q, regi_ids, r, w, max_iter, epsilon, A1, name_ds, name_y, name_x, name_yend, name_q, name_w, name_regimes, add_lag):
    w_r, warn = REGI.w_regime(w, regi_ids[r], r, transform=True)
    y_r = y[regi_ids[r]]
    x_r = x[regi_ids[r]]
    if yend != None:
        yend_r = yend[regi_ids[r]]
        q_r = q[regi_ids[r]]
    else:
        yend_r, q_r = None, None
    if add_lag != False:
        yend_r, q_r = set_endog(
            y_r, x_r, w_r, yend_r, q_r, add_lag[0], add_lag[1])
    x_constant = USER.check_constant(x_r)
    model = BaseGM_Endog_Error_Hom(
        y_r, x_constant, yend_r, q_r, w_r.sparse, max_iter=max_iter, epsilon=epsilon, A1=A1)
    set_warn(model, warn)
    if add_lag != False:
        model.rho = model.betas[-2]
        model.predy_e, model.e_pred, warn = sp_att(w_r, model.y,
                                                   model.predy, model.yend[:, -1].reshape(model.n, 1), model.rho)
        set_warn(model, warn)
    model.title = "SPATIALLY WEIGHTED TWO STAGE LEAST SQUARES (HOM) - REGIME %s" % r
    model.name_ds = name_ds
    model.name_y = '%s_%s' % (str(r), name_y)
    model.name_x = ['%s_%s' % (str(r), i) for i in name_x]
    model.name_yend = ['%s_%s' % (str(r), i) for i in name_yend]
    model.name_z = model.name_x + model.name_yend + ['lambda']
    model.name_q = ['%s_%s' % (str(r), i) for i in name_q]
    model.name_h = model.name_x + model.name_q
    model.name_w = name_w
    model.name_regimes = name_regimes
    return model


def _test():
    import doctest
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)

if __name__ == '__main__':
    _test()

########NEW FILE########
__FILENAME__ = error_sp_regimes
"""
Spatial Error Models with regimes module
"""

__author__ = "Luc Anselin luc.anselin@asu.edu, Pedro V. Amaral pedro.amaral@asu.edu"

import numpy as np
import multiprocessing as mp
import regimes as REGI
import user_output as USER
import summary_output as SUMMARY
from pysal import lag_spatial
from ols import BaseOLS
from twosls import BaseTSLS
from error_sp import BaseGM_Error, BaseGM_Endog_Error, _momentsGM_Error
from utils import set_endog, iter_msg, sp_att, set_warn
from utils import optim_moments, get_spFilter, get_lags
from utils import spdot, RegressionPropsY
from platform import system


class GM_Error_Regimes(RegressionPropsY, REGI.Regimes_Frame):

    """
    GMM method for a spatial error model with regimes, with results and diagnostics;
    based on Kelejian and Prucha (1998, 1999)[1]_ [2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    w            : pysal W object
                   Spatial weights object   
    constant_regi: ['one', 'many']
                   Switcher controlling the constant term setup. It may take
                   the following values:
                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime (default)
    cols2regi    : list, 'all'
                   Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all' (default), all the variables vary by regime.
    regime_err_sep : boolean
                   If True, a separate regression is run for each regime.
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    cores        : integer
                   Specifies the number of cores to be used in multiprocessing
                   Default: all cores available (specified as cores=None).
                   Note: Multiprocessing currently not available on Windows.
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    name_regimes : string
                   Name of regime variable for use in the output


    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    vm           : array
                   Variance covariance matrix (kxk)
    sig2         : float
                   Sigma squared used in computations
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    std_err      : array
                   1xk array of standard errors of the betas    
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    name_regimes : string
                   Name of regime variable for use in the output
    title        : string
                   Name of the regression method used
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    constant_regi: ['one', 'many']
                   Ignored if regimes=False. Constant option for regimes.
                   Switcher controlling the constant term setup. It may take
                   the following values:                    
                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime
    cols2regi    : list, 'all'
                   Ignored if regimes=False. Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all', all the variables vary by regime.
    regime_err_sep : boolean
                   If True, a separate regression is run for each regime.
    kr           : int
                   Number of variables/columns to be "regimized" or subject
                   to change by regime. These will result in one parameter
                   estimate by regime for each variable (i.e. nr parameters per
                   variable)
    kf           : int
                   Number of variables/columns to be considered fixed or
                   global across regimes and hence only obtain one parameter
                   estimate
    nr           : int
                   Number of different regimes in the 'regimes' list
    multi        : dictionary
                   Only available when multiple regressions are estimated,
                   i.e. when regime_err_sep=True and no variable is fixed
                   across regimes.
                   Contains all attributes of each individual regression

    References
    ----------

    .. [1] Kelejian, H.R., Prucha, I.R. (1998) "A generalized spatial
    two-stage least squares procedure for estimating a spatial autoregressive
    model with autoregressive disturbances". The Journal of Real State
    Finance and Economics, 17, 1.

    .. [2] Kelejian, H.R., Prucha, I.R. (1999) "A Generalized Moments
    Estimator for the Autoregressive Parameter in a Spatial Model".
    International Economic Review, 40, 2.

    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import pysal
    >>> import numpy as np

    Open data on NCOVR US County Homicides (3085 areas) using pysal.open().
    This is the DBF associated with the NAT shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> db = pysal.open(pysal.examples.get_path("NAT.dbf"),'r')
    
    Extract the HR90 column (homicide rates in 1990) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y_var = 'HR90'
    >>> y = np.array([db.by_col(y_var)]).reshape(3085,1)
    
    Extract UE90 (unemployment rate) and PS90 (population structure) vectors from
    the DBF to be used as independent variables in the regression. Other variables
    can be inserted by adding their names to x_var, such as x_var = ['Var1','Var2','...]
    Note that PySAL requires this to be an nxj numpy array, where j is the
    number of independent variables (not including a constant). By default
    this model adds a vector of ones to the independent variables passed in.

    >>> x_var = ['PS90','UE90']
    >>> x = np.array([db.by_col(name) for name in x_var]).T

    The different regimes in this data are given according to the North and 
    South dummy (SOUTH).

    >>> r_var = 'SOUTH'
    >>> regimes = db.by_col(r_var)

    Since we want to run a spatial error model, we need to specify
    the spatial weights matrix that includes the spatial configuration of the
    observations. To do that, we can open an already existing gal file or 
    create a new one. In this case, we will create one from ``NAT.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("NAT.shp"))

    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, this allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'

    We are all set with the preliminaries, we are good to run the model. In this
    case, we will need the variables and the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.

    >>> model = GM_Error_Regimes(y, x, regimes, w=w, name_y=y_var, name_x=x_var, name_regimes=r_var, name_ds='NAT.dbf')

    Once we have run the model, we can explore a little bit the output. The
    regression object we have created has many attributes so take your time to
    discover them. Note that because we are running the classical GMM error
    model from 1998/99, the spatial parameter is obtained as a point estimate, so
    although you get a value for it (there are for coefficients under
    model.betas), you cannot perform inference on it (there are only three
    values in model.se_betas). Alternatively, we can have a summary of the
    output by typing: model.summary
    
    >>> print model.name_x
    ['0_CONSTANT', '0_PS90', '0_UE90', '1_CONSTANT', '1_PS90', '1_UE90', 'lambda']
    >>> np.around(model.betas, decimals=6)
    array([[ 0.074807],
           [ 0.786107],
           [ 0.538849],
           [ 5.103756],
           [ 1.196009],
           [ 0.600533],
           [ 0.364103]])
    >>> np.around(model.std_err, decimals=6)
    array([ 0.379864,  0.152316,  0.051942,  0.471285,  0.19867 ,  0.057252])
    >>> np.around(model.z_stat, decimals=6)
    array([[  0.196932,   0.843881],
           [  5.161042,   0.      ],
           [ 10.37397 ,   0.      ],
           [ 10.829455,   0.      ],
           [  6.02007 ,   0.      ],
           [ 10.489215,   0.      ]])
    >>> np.around(model.sig2, decimals=6)
    28.172732

    """

    def __init__(self, y, x, regimes, w,
                 vm=False, name_y=None, name_x=None, name_w=None,
                 constant_regi='many', cols2regi='all', regime_err_sep=False,
                 cores=None, name_ds=None, name_regimes=None):

        n = USER.check_arrays(y, x)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        self.constant_regi = constant_regi
        self.cols2regi = cols2regi
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_y = USER.set_name_y(name_y)
        self.name_w = USER.set_name_w(name_w, w)
        self.name_regimes = USER.set_name_ds(name_regimes)
        self.n = n
        self.y = y

        x_constant = USER.check_constant(x)
        name_x = USER.set_name_x(name_x, x)
        self.name_x_r = name_x

        cols2regi = REGI.check_cols2regi(constant_regi, cols2regi, x)
        self.regimes_set = REGI._get_regimes_set(regimes)
        self.regimes = regimes
        USER.check_regimes(self.regimes_set, self.n, x.shape[1])
        self.regime_err_sep = regime_err_sep
        if regime_err_sep == True:
            if set(cols2regi) == set([True]):
                self._error_regimes_multi(y, x, regimes, w, cores,
                                          cols2regi, vm, name_x)
            else:
                raise Exception, "All coefficients must vary accross regimes if regime_err_sep = True."
        else:
            self.x, self.name_x = REGI.Regimes_Frame.__init__(self, x_constant,
                                                              regimes, constant_regi=None, cols2regi=cols2regi, names=name_x)
            ols = BaseOLS(y=y, x=self.x)
            self.k = ols.x.shape[1]
            moments = _momentsGM_Error(w, ols.u)
            lambda1 = optim_moments(moments)
            xs = get_spFilter(w, lambda1, x_constant)
            ys = get_spFilter(w, lambda1, y)
            xs = REGI.Regimes_Frame.__init__(self, xs,
                                             regimes, constant_regi=None, cols2regi=cols2regi)[0]
            ols2 = BaseOLS(y=ys, x=xs)

            # Output
            self.predy = spdot(self.x, ols2.betas)
            self.u = y - self.predy
            self.betas = np.vstack((ols2.betas, np.array([[lambda1]])))
            self.sig2 = ols2.sig2n
            self.e_filtered = self.u - lambda1 * lag_spatial(w, self.u)
            self.vm = self.sig2 * ols2.xtxi
            self.title = "SPATIALLY WEIGHTED LEAST SQUARES - REGIMES"
            self.name_x.append('lambda')
            self.kf += 1
            self.chow = REGI.Chow(self)
            self._cache = {}
            SUMMARY.GM_Error(reg=self, w=w, vm=vm, regimes=True)

    def _error_regimes_multi(self, y, x, regimes, w, cores,
                             cols2regi, vm, name_x):
        regi_ids = dict((r, list(np.where(np.array(regimes) == r)[0]))
                        for r in self.regimes_set)
        results_p = {}
        for r in self.regimes_set:
            if system() == 'Windows':
                results_p[r] = _work_error(
                    *(y, x, regi_ids, r, w, self.name_ds, self.name_y, name_x + ['lambda'], self.name_w, self.name_regimes))
                is_win = True
            else:
                pool = mp.Pool(cores)
                results_p[r] = pool.apply_async(_work_error, args=(
                    y, x, regi_ids, r, w, self.name_ds, self.name_y, name_x + ['lambda'], self.name_w, self.name_regimes, ))
                is_win = False
        self.kryd = 0
        self.kr = len(cols2regi)
        self.kf = 0
        self.nr = len(self.regimes_set)
        self.vm = np.zeros((self.nr * self.kr, self.nr * self.kr), float)
        self.betas = np.zeros((self.nr * (self.kr + 1), 1), float)
        self.u = np.zeros((self.n, 1), float)
        self.predy = np.zeros((self.n, 1), float)
        self.e_filtered = np.zeros((self.n, 1), float)
        if not is_win:
            pool.close()
            pool.join()
        results = {}
        self.name_y, self.name_x = [], []
        counter = 0
        for r in self.regimes_set:
            if is_win:
                results[r] = results_p[r]
            else:
                results[r] = results_p[r].get()
            self.vm[(counter * self.kr):((counter + 1) * self.kr),
                    (counter * self.kr):((counter + 1) * self.kr)] = results[r].vm
            self.betas[(counter * (self.kr + 1)):
                       ((counter + 1) * (self.kr + 1)), ] = results[r].betas
            self.u[regi_ids[r], ] = results[r].u
            self.predy[regi_ids[r], ] = results[r].predy
            self.e_filtered[regi_ids[r], ] = results[r].e_filtered
            self.name_y += results[r].name_y
            self.name_x += results[r].name_x
            counter += 1
        self.chow = REGI.Chow(self)
        self.multi = results
        SUMMARY.GM_Error_multi(
            reg=self, multireg=self.multi, vm=vm, regimes=True)


class GM_Endog_Error_Regimes(RegressionPropsY, REGI.Regimes_Frame):

    '''
    GMM method for a spatial error model with regimes and endogenous variables, with
    results and diagnostics; based on Kelejian and Prucha (1998, 1999)[1]_[2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note: 
                   this should not contain any variables from x)
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    w            : pysal W object
                   Spatial weights object   
    constant_regi: ['one', 'many']
                   Switcher controlling the constant term setup. It may take
                   the following values:
                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime (default)
    cols2regi    : list, 'all'
                   Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all' (default), all the variables vary by regime.
    regime_err_sep : boolean
                   If True, a separate regression is run for each regime.
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    cores        : integer
                   Specifies the number of cores to be used in multiprocessing
                   Default: all cores available (specified as cores=None).
                   Note: Multiprocessing currently not available on Windows.
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_yend    : list of strings
                   Names of endogenous variables for use in output
    name_q       : list of strings
                   Names of instruments for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    name_regimes : string
                   Name of regime variable for use in the output

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    z            : array
                   nxk array of variables (combination of x and yend)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    sig2         : float
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
                   Sigma squared used in computations
    std_err      : array
                   1xk array of standard errors of the betas    
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    name_y        : string
                    Name of dependent variable for use in output
    name_x        : list of strings
                    Names of independent variables for use in output
    name_yend     : list of strings
                    Names of endogenous variables for use in output
    name_z        : list of strings
                    Names of exogenous and endogenous variables for use in 
                    output
    name_q        : list of strings
                    Names of external instruments
    name_h        : list of strings
                    Names of all instruments used in ouput
    name_w        : string
                    Name of weights matrix for use in output
    name_ds       : string
                    Name of dataset for use in output
    name_regimes  : string
                    Name of regimes variable for use in output
    title         : string
                    Name of the regression method used
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    regimes       : list
                    List of n values with the mapping of each
                    observation to a regime. Assumed to be aligned with 'x'.
    constant_regi : ['one', 'many']
                    Ignored if regimes=False. Constant option for regimes.
                    Switcher controlling the constant term setup. It may take
                    the following values:
                      *  'one': a vector of ones is appended to x and held
                                constant across regimes
                      * 'many': a vector of ones is appended to x and considered
                                different per regime
    cols2regi     : list, 'all'
                    Ignored if regimes=False. Argument indicating whether each
                    column of x should be considered as different per regime
                    or held constant across regimes (False).
                    If a list, k booleans indicating for each variable the
                    option (True if one per regime, False to be held constant).
                    If 'all', all the variables vary by regime.
    regime_err_sep : boolean
                   If True, a separate regression is run for each regime.
    kr            : int
                    Number of variables/columns to be "regimized" or subject
                    to change by regime. These will result in one parameter
                    estimate by regime for each variable (i.e. nr parameters per
                    variable)
    kf            : int
                    Number of variables/columns to be considered fixed or
                    global across regimes and hence only obtain one parameter
                    estimate
    nr            : int
                    Number of different regimes in the 'regimes' list
    multi        : dictionary
                   Only available when multiple regressions are estimated,
                   i.e. when regime_err_sep=True and no variable is fixed
                   across regimes.
                   Contains all attributes of each individual regression

    References
    ----------

    .. [1] Kelejian, H.R., Prucha, I.R. (1998) "A generalized spatial
    two-stage least squares procedure for estimating a spatial autoregressive
    model with autoregressive disturbances". The Journal of Real State
    Finance and Economics, 17, 1.

    .. [2] Kelejian, H.R., Prucha, I.R. (1999) "A Generalized Moments
    Estimator for the Autoregressive Parameter in a Spatial Model".
    International Economic Review, 40, 2.

    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import pysal
    >>> import numpy as np

    Open data on NCOVR US County Homicides (3085 areas) using pysal.open().
    This is the DBF associated with the NAT shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> db = pysal.open(pysal.examples.get_path("NAT.dbf"),'r')
    
    Extract the HR90 column (homicide rates in 1990) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y_var = 'HR90'
    >>> y = np.array([db.by_col(y_var)]).reshape(3085,1)
    
    Extract UE90 (unemployment rate) and PS90 (population structure) vectors from
    the DBF to be used as independent variables in the regression. Other variables
    can be inserted by adding their names to x_var, such as x_var = ['Var1','Var2','...]
    Note that PySAL requires this to be an nxj numpy array, where j is the
    number of independent variables (not including a constant). By default
    this model adds a vector of ones to the independent variables passed in.

    >>> x_var = ['PS90','UE90']
    >>> x = np.array([db.by_col(name) for name in x_var]).T

    For the endogenous models, we add the endogenous variable RD90 (resource deprivation)
    and we decide to instrument for it with FP89 (families below poverty):

    >>> yd_var = ['RD90']
    >>> yend = np.array([db.by_col(name) for name in yd_var]).T
    >>> q_var = ['FP89']
    >>> q = np.array([db.by_col(name) for name in q_var]).T

    The different regimes in this data are given according to the North and 
    South dummy (SOUTH).

    >>> r_var = 'SOUTH'
    >>> regimes = db.by_col(r_var)

    Since we want to run a spatial error model, we need to specify the spatial
    weights matrix that includes the spatial configuration of the observations
    into the error component of the model. To do that, we can open an already 
    existing gal file or create a new one. In this case, we will create one 
    from ``NAT.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("NAT.shp"))

    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, this allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'

    We are all set with the preliminaries, we are good to run the model. In this
    case, we will need the variables (exogenous and endogenous), the
    instruments and the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.

    >>> model = GM_Endog_Error_Regimes(y, x, yend, q, regimes, w=w, name_y=y_var, name_x=x_var, name_yend=yd_var, name_q=q_var, name_regimes=r_var, name_ds='NAT.dbf')

    Once we have run the model, we can explore a little bit the output. The
    regression object we have created has many attributes so take your time to
    discover them. Note that because we are running the classical GMM error
    model from 1998/99, the spatial parameter is obtained as a point estimate, so
    although you get a value for it (there are for coefficients under
    model.betas), you cannot perform inference on it (there are only three
    values in model.se_betas). Also, this regression uses a two stage least
    squares estimation method that accounts for the endogeneity created by the
    endogenous variables included. Alternatively, we can have a summary of the
    output by typing: model.summary

    >>> print model.name_z
    ['0_CONSTANT', '0_PS90', '0_UE90', '1_CONSTANT', '1_PS90', '1_UE90', '0_RD90', '1_RD90', 'lambda']
    >>> np.around(model.betas, decimals=5)
    array([[ 3.59718],
           [ 1.0652 ],
           [ 0.15822],
           [ 9.19754],
           [ 1.88082],
           [-0.24878],
           [ 2.46161],
           [ 3.57943],
           [ 0.25564]])
    >>> np.around(model.std_err, decimals=6)
    array([ 0.522633,  0.137555,  0.063054,  0.473654,  0.18335 ,  0.072786,
            0.300711,  0.240413])
    
    '''

    def __init__(self, y, x, yend, q, regimes, w, cores=None,
                 vm=False, constant_regi='many', cols2regi='all',
                 regime_err_sep=False, name_y=None,
                 name_x=None, name_yend=None, name_q=None, name_w=None,
                 name_ds=None, name_regimes=None, summ=True, add_lag=False):

        n = USER.check_arrays(y, x, yend, q)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        self.constant_regi = constant_regi
        self.cols2regi = cols2regi
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_regimes = USER.set_name_ds(name_regimes)
        self.name_w = USER.set_name_w(name_w, w)
        self.n = n
        self.y = y

        name_x = USER.set_name_x(name_x, x)
        if summ:
            name_yend = USER.set_name_yend(name_yend, yend)
            self.name_y = USER.set_name_y(name_y)
            name_q = USER.set_name_q(name_q, q)
        self.name_x_r = name_x + name_yend

        cols2regi = REGI.check_cols2regi(
            constant_regi, cols2regi, x, yend=yend)
        self.regimes_set = REGI._get_regimes_set(regimes)
        self.regimes = regimes
        USER.check_regimes(self.regimes_set, self.n, x.shape[1])
        self.regime_err_sep = regime_err_sep

        if regime_err_sep == True:
            if set(cols2regi) == set([True]):
                self._endog_error_regimes_multi(
                    y, x, regimes, w, yend, q, cores,
                    cols2regi, vm, name_x, name_yend, name_q, add_lag)
            else:
                raise Exception, "All coefficients must vary accross regimes if regime_err_sep = True."
        else:
            x_constant = USER.check_constant(x)
            q, name_q = REGI.Regimes_Frame.__init__(self, q,
                                                    regimes, constant_regi=None, cols2regi='all', names=name_q)
            x, name_x = REGI.Regimes_Frame.__init__(self, x_constant,
                                                    regimes, constant_regi=None, cols2regi=cols2regi,
                                                    names=name_x)
            yend2, name_yend = REGI.Regimes_Frame.__init__(self, yend,
                                                           regimes, constant_regi=None,
                                                           cols2regi=cols2regi, yend=True, names=name_yend)

            tsls = BaseTSLS(y=y, x=x, yend=yend2, q=q)
            self.k = tsls.z.shape[1]
            self.x = tsls.x
            self.yend, self.z = tsls.yend, tsls.z
            moments = _momentsGM_Error(w, tsls.u)
            lambda1 = optim_moments(moments)
            xs = get_spFilter(w, lambda1, x_constant)
            xs = REGI.Regimes_Frame.__init__(self, xs,
                                             regimes, constant_regi=None, cols2regi=cols2regi)[0]
            ys = get_spFilter(w, lambda1, y)
            yend_s = get_spFilter(w, lambda1, yend)
            yend_s = REGI.Regimes_Frame.__init__(self, yend_s,
                                                 regimes, constant_regi=None, cols2regi=cols2regi,
                                                 yend=True)[0]
            tsls2 = BaseTSLS(ys, xs, yend_s, h=tsls.h)

            # Output
            self.betas = np.vstack((tsls2.betas, np.array([[lambda1]])))
            self.predy = spdot(tsls.z, tsls2.betas)
            self.u = y - self.predy
            self.sig2 = float(np.dot(tsls2.u.T, tsls2.u)) / self.n
            self.e_filtered = self.u - lambda1 * lag_spatial(w, self.u)
            self.vm = self.sig2 * tsls2.varb
            self.name_x = USER.set_name_x(name_x, x, constant=True)
            self.name_yend = USER.set_name_yend(name_yend, yend)
            self.name_z = self.name_x + self.name_yend
            self.name_z.append('lambda')
            self.name_q = USER.set_name_q(name_q, q)
            self.name_h = USER.set_name_h(self.name_x, self.name_q)
            self.kf += 1
            self.chow = REGI.Chow(self)
            self._cache = {}
            if summ:
                self.title = "SPATIALLY WEIGHTED TWO STAGE LEAST SQUARES - REGIMES"
                SUMMARY.GM_Endog_Error(reg=self, w=w, vm=vm, regimes=True)

    def _endog_error_regimes_multi(self, y, x, regimes, w, yend, q, cores,
                                   cols2regi, vm, name_x, name_yend, name_q, add_lag):

        regi_ids = dict((r, list(np.where(np.array(regimes) == r)[0]))
                        for r in self.regimes_set)
        if add_lag != False:
            self.cols2regi += [True]
            cols2regi += [True]
            self.predy_e = np.zeros((self.n, 1), float)
            self.e_pred = np.zeros((self.n, 1), float)
        results_p = {}
        for r in self.regimes_set:
            if system() == 'Windows':
                results_p[r] = _work_endog_error(
                    *(y, x, yend, q, regi_ids, r, w, self.name_ds, self.name_y, name_x, name_yend, name_q, self.name_w, self.name_regimes, add_lag))
                is_win = True
            else:
                pool = mp.Pool(cores)
                results_p[r] = pool.apply_async(_work_endog_error, args=(
                    y, x, yend, q, regi_ids, r, w, self.name_ds, self.name_y, name_x, name_yend, name_q, self.name_w, self.name_regimes, add_lag, ))
                is_win = False
        self.kryd, self.kf = 0, 0
        self.kr = len(cols2regi)
        self.nr = len(self.regimes_set)
        self.vm = np.zeros((self.nr * self.kr, self.nr * self.kr), float)
        self.betas = np.zeros((self.nr * (self.kr + 1), 1), float)
        self.u = np.zeros((self.n, 1), float)
        self.predy = np.zeros((self.n, 1), float)
        self.e_filtered = np.zeros((self.n, 1), float)
        if not is_win:
            pool.close()
            pool.join()
        results = {}
        self.name_y, self.name_x, self.name_yend, self.name_q, self.name_z, self.name_h = [
        ], [], [], [], [], []
        counter = 0
        for r in self.regimes_set:
            if is_win:
                results[r] = results_p[r]
            else:
                results[r] = results_p[r].get()
            self.vm[(counter * self.kr):((counter + 1) * self.kr),
                    (counter * self.kr):((counter + 1) * self.kr)] = results[r].vm
            self.betas[(counter * (self.kr + 1)):
                       ((counter + 1) * (self.kr + 1)), ] = results[r].betas
            self.u[regi_ids[r], ] = results[r].u
            self.predy[regi_ids[r], ] = results[r].predy
            self.e_filtered[regi_ids[r], ] = results[r].e_filtered
            self.name_y += results[r].name_y
            self.name_x += results[r].name_x
            self.name_yend += results[r].name_yend
            self.name_q += results[r].name_q
            self.name_z += results[r].name_z
            self.name_h += results[r].name_h
            if add_lag != False:
                self.predy_e[regi_ids[r], ] = results[r].predy_e
                self.e_pred[regi_ids[r], ] = results[r].e_pred
            counter += 1
        self.chow = REGI.Chow(self)
        self.multi = results
        if add_lag != False:
            SUMMARY.GM_Combo_multi(
                reg=self, multireg=self.multi, vm=vm, regimes=True)
        else:
            SUMMARY.GM_Endog_Error_multi(
                reg=self, multireg=self.multi, vm=vm, regimes=True)


class GM_Combo_Regimes(GM_Endog_Error_Regimes, REGI.Regimes_Frame):

    """
    GMM method for a spatial lag and error model with regimes and endogenous
    variables, with results and diagnostics; based on Kelejian and Prucha (1998,
    1999)[1]_[2]_.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note: 
                   this should not contain any variables from x)
    w            : pysal W object
                   Spatial weights object (always needed)   
    constant_regi: ['one', 'many']
                   Switcher controlling the constant term setup. It may take
                   the following values:
                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime (default)
    cols2regi    : list, 'all'
                   Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all' (default), all the variables vary by regime.
    regime_err_sep : boolean
                   If True, a separate regression is run for each regime.
    regime_lag_sep   : boolean
                   If True, the spatial parameter for spatial lag is also
                   computed according to different regimes. If False (default), 
                   the spatial parameter is fixed accross regimes.
    w_lags       : integer
                   Orders of W to include as instruments for the spatially
                   lagged dependent variable. For example, w_lags=1, then
                   instruments are WX; if w_lags=2, then WX, WWX; and so on.
    lag_q        : boolean
                   If True, then include spatial lags of the additional 
                   instruments (q).
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    cores        : integer
                   Specifies the number of cores to be used in multiprocessing
                   Default: all cores available (specified as cores=None).
                   Note: Multiprocessing currently not available on Windows.
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_yend    : list of strings
                   Names of endogenous variables for use in output
    name_q       : list of strings
                   Names of instruments for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    name_regimes : string
                   Name of regime variable for use in the output

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    e_pred       : array
                   nx1 array of residuals (using reduced form)
    predy        : array
                   nx1 array of predicted y values
    predy_e      : array
                   nx1 array of predicted y values (using reduced form)
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    z            : array
                   nxk array of variables (combination of x and yend)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    pr2_e        : float
                   Pseudo R squared (squared correlation between y and ypred_e
                   (using reduced form))
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    sig2         : float
                   Sigma squared used in computations (based on filtered
                   residuals)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    std_err      : array
                   1xk array of standard errors of the betas    
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    name_y        : string
                    Name of dependent variable for use in output
    name_x        : list of strings
                    Names of independent variables for use in output
    name_yend     : list of strings
                    Names of endogenous variables for use in output
    name_z        : list of strings
                    Names of exogenous and endogenous variables for use in 
                    output
    name_q        : list of strings
                    Names of external instruments
    name_h        : list of strings
                    Names of all instruments used in ouput
    name_w        : string
                    Name of weights matrix for use in output
    name_ds       : string
                    Name of dataset for use in output
    name_regimes  : string
                    Name of regimes variable for use in output
    title         : string
                    Name of the regression method used
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    regimes       : list
                    List of n values with the mapping of each
                    observation to a regime. Assumed to be aligned with 'x'.
    constant_regi : ['one', 'many']
                    Ignored if regimes=False. Constant option for regimes.
                    Switcher controlling the constant term setup. It may take
                    the following values:
                      *  'one': a vector of ones is appended to x and held
                                constant across regimes
                      * 'many': a vector of ones is appended to x and considered
                                different per regime
    cols2regi     : list, 'all'
                    Ignored if regimes=False. Argument indicating whether each
                    column of x should be considered as different per regime
                    or held constant across regimes (False).
                    If a list, k booleans indicating for each variable the
                    option (True if one per regime, False to be held constant).
                    If 'all', all the variables vary by regime.
    regime_err_sep  : boolean
                   If True, a separate regression is run for each regime.
    regime_lag_sep    : boolean
                    If True, the spatial parameter for spatial lag is also
                    computed according to different regimes. If False (default), 
                    the spatial parameter is fixed accross regimes.
    kr            : int
                    Number of variables/columns to be "regimized" or subject
                    to change by regime. These will result in one parameter
                    estimate by regime for each variable (i.e. nr parameters per
                    variable)
    kf            : int
                    Number of variables/columns to be considered fixed or
                    global across regimes and hence only obtain one parameter
                    estimate
    nr            : int
                    Number of different regimes in the 'regimes' list
    multi        : dictionary
                   Only available when multiple regressions are estimated,
                   i.e. when regime_err_sep=True and no variable is fixed
                   across regimes.
                   Contains all attributes of each individual regression

    References
    ----------

    .. [1] Kelejian, H.R., Prucha, I.R. (1998) "A generalized spatial
    two-stage least squares procedure for estimating a spatial autoregressive
    model with autoregressive disturbances". The Journal of Real State
    Finance and Economics, 17, 1.

    .. [2] Kelejian, H.R., Prucha, I.R. (1999) "A Generalized Moments
    Estimator for the Autoregressive Parameter in a Spatial Model".
    International Economic Review, 40, 2.

    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import numpy as np
    >>> import pysal

    Open data on NCOVR US County Homicides (3085 areas) using pysal.open().
    This is the DBF associated with the NAT shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> db = pysal.open(pysal.examples.get_path("NAT.dbf"),'r')
    
    Extract the HR90 column (homicide rates in 1990) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y_var = 'HR90'
    >>> y = np.array([db.by_col(y_var)]).reshape(3085,1)
    
    Extract UE90 (unemployment rate) and PS90 (population structure) vectors from
    the DBF to be used as independent variables in the regression. Other variables
    can be inserted by adding their names to x_var, such as x_var = ['Var1','Var2','...]
    Note that PySAL requires this to be an nxj numpy array, where j is the
    number of independent variables (not including a constant). By default
    this model adds a vector of ones to the independent variables passed in.

    >>> x_var = ['PS90','UE90']
    >>> x = np.array([db.by_col(name) for name in x_var]).T

    The different regimes in this data are given according to the North and 
    South dummy (SOUTH).

    >>> r_var = 'SOUTH'
    >>> regimes = db.by_col(r_var)

    Since we want to run a spatial lag model, we need to specify
    the spatial weights matrix that includes the spatial configuration of the
    observations. To do that, we can open an already existing gal file or 
    create a new one. In this case, we will create one from ``NAT.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("NAT.shp"))

    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, this allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'

    The Combo class runs an SARAR model, that is a spatial lag+error model.
    In this case we will run a simple version of that, where we have the
    spatial effects as well as exogenous variables. Since it is a spatial
    model, we have to pass in the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.

    >>> model = GM_Combo_Regimes(y, x, regimes, w=w, name_y=y_var, name_x=x_var, name_regimes=r_var, name_ds='NAT')

    Once we have run the model, we can explore a little bit the output. The
    regression object we have created has many attributes so take your time to
    discover them. Note that because we are running the classical GMM error
    model from 1998/99, the spatial parameter is obtained as a point estimate, so
    although you get a value for it (there are for coefficients under
    model.betas), you cannot perform inference on it (there are only three
    values in model.se_betas). Also, this regression uses a two stage least
    squares estimation method that accounts for the endogeneity created by the
    spatial lag of the dependent variable. We can have a summary of the
    output by typing: model.summary 
    Alternatively, we can check the betas:

    >>> print model.name_z
    ['0_CONSTANT', '0_PS90', '0_UE90', '1_CONSTANT', '1_PS90', '1_UE90', '_Global_W_HR90', 'lambda']
    >>> print np.around(model.betas,4)
    [[ 1.4607]
     [ 0.958 ]
     [ 0.5658]
     [ 9.113 ]
     [ 1.1338]
     [ 0.6517]
     [-0.4583]
     [ 0.6136]]

    And lambda:

    >>> print 'lambda: ', np.around(model.betas[-1], 4)
    lambda:  [ 0.6136]
        
    This class also allows the user to run a spatial lag+error model with the
    extra feature of including non-spatial endogenous regressors. This means
    that, in addition to the spatial lag and error, we consider some of the
    variables on the right-hand side of the equation as endogenous and we
    instrument for this. In this case we consider RD90 (resource deprivation)
    as an endogenous regressor.  We use FP89 (families below poverty)
    for this and hence put it in the instruments parameter, 'q'.

    >>> yd_var = ['RD90']
    >>> yd = np.array([db.by_col(name) for name in yd_var]).T
    >>> q_var = ['FP89']
    >>> q = np.array([db.by_col(name) for name in q_var]).T

    And then we can run and explore the model analogously to the previous combo:

    >>> model = GM_Combo_Regimes(y, x, regimes, yd, q, w=w, name_y=y_var, name_x=x_var, name_yend=yd_var, name_q=q_var, name_regimes=r_var, name_ds='NAT')
    >>> print model.name_z
    ['0_CONSTANT', '0_PS90', '0_UE90', '1_CONSTANT', '1_PS90', '1_UE90', '0_RD90', '1_RD90', '_Global_W_HR90', 'lambda']
    >>> print model.betas
    [[ 3.41963782]
     [ 1.04065841]
     [ 0.16634393]
     [ 8.86544628]
     [ 1.85120528]
     [-0.24908469]
     [ 2.43014046]
     [ 3.61645481]
     [ 0.03308671]
     [ 0.18684992]]
    >>> print np.sqrt(model.vm.diagonal())
    [ 0.53067577  0.13271426  0.06058025  0.76406411  0.17969783  0.07167421
      0.28943121  0.25308326  0.06126529]
    >>> print 'lambda: ', np.around(model.betas[-1], 4)
    lambda:  [ 0.1868]
    """

    def __init__(self, y, x, regimes, yend=None, q=None,
                 w=None, w_lags=1, lag_q=True, cores=None,
                 constant_regi='many', cols2regi='all',
                 regime_err_sep=False, regime_lag_sep=False,
                 vm=False, name_y=None, name_x=None,
                 name_yend=None, name_q=None,
                 name_w=None, name_ds=None, name_regimes=None):

        n = USER.check_arrays(y, x)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        name_x = USER.set_name_x(name_x, x, constant=True)
        self.name_y = USER.set_name_y(name_y)
        name_yend = USER.set_name_yend(name_yend, yend)
        name_q = USER.set_name_q(name_q, q)
        name_q.extend(USER.set_name_q_sp(name_x, w_lags, name_q, lag_q,
                      force_all=True))

        cols2regi = REGI.check_cols2regi(
            constant_regi, cols2regi, x, yend=yend, add_cons=False)
        self.regimes_set = REGI._get_regimes_set(regimes)
        self.regimes = regimes
        USER.check_regimes(self.regimes_set, n, x.shape[1])
        self.regime_err_sep = regime_err_sep
        self.regime_lag_sep = regime_lag_sep

        if regime_lag_sep == True:
            if regime_err_sep == False:
                raise Exception, "For spatial combo models, if spatial lag is set by regimes (regime_lag_sep=True), spatial error must also be set by regimes (regime_err_sep=True)."
            add_lag = [w_lags, lag_q]
        else:
            if regime_err_sep == True:
                raise Exception, "For spatial combo models, if spatial error is set by regimes (regime_err_sep=True), all coefficients including lambda (regime_lag_sep=True) must be set by regimes."
            cols2regi += [False]
            add_lag = False
            yend, q = set_endog(y, x, w, yend, q, w_lags, lag_q)
        name_yend.append(USER.set_name_yend_sp(self.name_y))

        GM_Endog_Error_Regimes.__init__(self, y=y, x=x, yend=yend,
                                        q=q, regimes=regimes, w=w, vm=vm, constant_regi=constant_regi,
                                        cols2regi=cols2regi, regime_err_sep=regime_err_sep, cores=cores,
                                        name_y=self.name_y, name_x=name_x,
                                        name_yend=name_yend, name_q=name_q, name_w=name_w,
                                        name_ds=name_ds, name_regimes=name_regimes, summ=False, add_lag=add_lag)

        if regime_err_sep != True:
            self.rho = self.betas[-2]
            self.predy_e, self.e_pred, warn = sp_att(w, self.y,
                                                     self.predy, yend[:, -1].reshape(self.n, 1), self.rho)
            set_warn(self, warn)
            self.title = "SPATIALLY WEIGHTED TWO STAGE LEAST SQUARES - REGIMES"
            SUMMARY.GM_Combo(reg=self, w=w, vm=vm, regimes=True)


def _work_error(y, x, regi_ids, r, w, name_ds, name_y, name_x, name_w, name_regimes):
    w_r, warn = REGI.w_regime(w, regi_ids[r], r, transform=True)
    y_r = y[regi_ids[r]]
    x_r = x[regi_ids[r]]
    x_constant = USER.check_constant(x_r)
    model = BaseGM_Error(y_r, x_constant, w_r.sparse)
    set_warn(model, warn)
    model.w = w_r
    model.title = "SPATIALLY WEIGHTED LEAST SQUARES ESTIMATION - REGIME %s" % r
    model.name_ds = name_ds
    model.name_y = '%s_%s' % (str(r), name_y)
    model.name_x = ['%s_%s' % (str(r), i) for i in name_x]
    model.name_w = name_w
    model.name_regimes = name_regimes
    return model


def _work_endog_error(y, x, yend, q, regi_ids, r, w, name_ds, name_y, name_x, name_yend, name_q, name_w, name_regimes, add_lag):
    w_r, warn = REGI.w_regime(w, regi_ids[r], r, transform=True)
    y_r = y[regi_ids[r]]
    x_r = x[regi_ids[r]]
    if yend != None:
        yend_r = yend[regi_ids[r]]
        q_r = q[regi_ids[r]]
    else:
        yend_r, q_r = None, None
    if add_lag != False:
        yend_r, q_r = set_endog(
            y_r, x_r, w_r, yend_r, q_r, add_lag[0], add_lag[1])
    x_constant = USER.check_constant(x_r)
    model = BaseGM_Endog_Error(y_r, x_constant, yend_r, q_r, w_r.sparse)
    set_warn(model, warn)
    if add_lag != False:
        model.rho = model.betas[-2]
        model.predy_e, model.e_pred, warn = sp_att(w_r, model.y,
                                                   model.predy, model.yend[:, -1].reshape(model.n, 1), model.rho)
        set_warn(model, warn)
    model.w = w_r
    model.title = "SPATIALLY WEIGHTED TWO STAGE LEAST SQUARES - REGIME %s" % r
    model.name_ds = name_ds
    model.name_y = '%s_%s' % (str(r), name_y)
    model.name_x = ['%s_%s' % (str(r), i) for i in name_x]
    model.name_yend = ['%s_%s' % (str(r), i) for i in name_yend]
    model.name_z = model.name_x + model.name_yend + ['lambda']
    model.name_q = ['%s_%s' % (str(r), i) for i in name_q]
    model.name_h = model.name_x + model.name_q
    model.name_w = name_w
    model.name_regimes = name_regimes
    return model


def _test():
    import doctest
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)

if __name__ == '__main__':

    _test()
    import pysal
    import numpy as np
    dbf = pysal.open(pysal.examples.get_path('columbus.dbf'), 'r')
    y = np.array([dbf.by_col('CRIME')]).T
    names_to_extract = ['INC']
    x = np.array([dbf.by_col(name) for name in names_to_extract]).T
    yd_var = ['HOVAL']
    yend = np.array([dbf.by_col(name) for name in yd_var]).T
    q_var = ['DISCBD']
    q = np.array([dbf.by_col(name) for name in q_var]).T
    regimes = regimes = dbf.by_col('NSA')
    w = pysal.open(pysal.examples.get_path("columbus.gal"), 'r').read()
    w.transform = 'r'
    model = GM_Error_Regimes(y, x, regimes=regimes, w=w, name_y='crime', name_x=[
                             'income'], name_regimes='nsa', name_ds='columbus', regime_err_sep=True)
    print model.summary

########NEW FILE########
__FILENAME__ = ml_error
"""
ML Estimation of Spatial Error Model
"""

__author__ = "Luc Anselin luc.anselin@asu.edu, Serge Rey srey@asu.edu"

import numpy as np
import numpy.linalg as la
import pysal as ps
from pysal.spreg.utils import RegressionPropsY, RegressionPropsVM
import diagnostics as DIAG
import user_output as USER
import summary_output as SUMMARY
import regimes as REGI
from w_utils import symmetrize
try:
    from scipy.optimize import minimize_scalar
except ImportError as e:
    print(e, "Maximum Likelihood in PySAL requires SciPy version 0.11 or newer.")

__all__ = ["ML_Error"]


class BaseML_Error(RegressionPropsY, RegressionPropsVM, REGI.Regimes_Frame):

    """
    ML estimation of the spatial error model (note no consistency
    checks, diagnostics or constants added); Anselin (1988) [1]_

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    w            : Sparse matrix
                   Spatial weights sparse matrix
    method       : string
                   if 'full', brute force calculation (full matrix expressions)
    epsilon      : float
                   tolerance criterion in mimimize_scalar function and inverse_product
    regimes_att  : dictionary
                   Dictionary containing elements to be used in case of a regimes model,
                   i.e. 'x' before regimes, 'regimes' list and 'cols2regi'


    Attributes
    ----------
    betas        : array
                   kx1 array of estimated coefficients
    lam          : float
                   estimate of spatial autoregressive coefficient
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant, excluding the rho)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    method       : string
                   log Jacobian method
                   if 'full': brute force (full matrix computations)
                   if 'ord' : Ord eigenvalue method
    epsilon      : float
                   tolerance criterion used in minimize_scalar function and inverse_product
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (k+1 x k+1) - includes lambda
    vm1          : array
                   2x2 array of variance covariance for lambda, sigma
    sig2         : float
                   Sigma squared used in computations
    logll        : float
                   maximized log-likelihood (including constant terms)

    Examples
    --------
    >>> import numpy as np
    >>> import pysal as ps
    >>> np.set_printoptions(suppress=True) #prevent scientific format
    >>> db = ps.open(ps.examples.get_path("south.dbf"),'r')
    >>> y_name = "HR90"
    >>> y = np.array(db.by_col(y_name))
    >>> y.shape = (len(y),1)
    >>> x_names = ["RD90","PS90","UE90","DV90"]
    >>> x = np.array([db.by_col(var) for var in x_names]).T
    >>> x = np.hstack((np.ones((len(y),1)),x))
    >>> ww = ps.open(ps.examples.get_path("south_q.gal"))
    >>> w = ww.read()
    >>> ww.close()
    >>> w.transform = 'r'
    >>> mlerr = BaseML_Error(y,x,w)
    >>> "{0:.6f}".format(mlerr.lam)
    '0.299078'
    >>> np.around(mlerr.betas, decimals=4)
    array([[ 6.1492],
           [ 4.4024],
           [ 1.7784],
           [-0.3781],
           [ 0.4858],
           [ 0.2991]])
    >>> "{0:.6f}".format(mlerr.mean_y)
    '9.549293'
    >>> "{0:.6f}".format(mlerr.std_y)
    '7.038851'
    >>> np.diag(mlerr.vm)
    array([ 1.06476526,  0.05548248,  0.04544514,  0.00614425,  0.01481356,
            0.00143001])
    >>> "{0:.6f}".format(mlerr.sig2[0][0])
    '32.406854'
    >>> "{0:.6f}".format(mlerr.logll)
    '-4471.407067'
    >>> mlerr1 = BaseML_Error(y,x,w,method='ord')
    >>> "{0:.6f}".format(mlerr1.lam)
    '0.299078'
    >>> np.around(mlerr1.betas, decimals=4)
    array([[ 6.1492],
           [ 4.4024],
           [ 1.7784],
           [-0.3781],
           [ 0.4858],
           [ 0.2991]])
    >>> "{0:.6f}".format(mlerr1.mean_y)
    '9.549293'
    >>> "{0:.6f}".format(mlerr1.std_y)
    '7.038851'
    >>> np.around(np.diag(mlerr1.vm), decimals=4)
    array([ 1.0648,  0.0555,  0.0454,  0.0061,  0.0148,  0.0014])
    >>> "{0:.4f}".format(mlerr1.sig2[0][0])
    '32.4069'
    >>> "{0:.4f}".format(mlerr1.logll)
    '-4471.4071'

    References
    ----------

    .. [1] Anselin, L. (1988) "Spatial Econometrics: Methods and Models".
    Kluwer Academic Publishers. Dordrecht.

    """

    def __init__(self, y, x, w, method='full', epsilon=0.0000001, regimes_att=None):
        # set up main regression variables and spatial filters
        self.y = y
        if regimes_att:
            self.x = x.toarray()
        else:
            self.x = x
        self.n, self.k = self.x.shape
        self.method = method
        self.epsilon = epsilon
        W = w.full()[0]

        ylag = ps.lag_spatial(w, self.y)
        xlag = self.get_x_lag(w, regimes_att)

        # call minimizer using concentrated log-likelihood to get lambda
        methodML = method.upper()
        if methodML in ['FULL', 'ORD']:
            if methodML == 'FULL':
                res = minimize_scalar(err_c_loglik, 0.0, bounds=(-1.0, 1.0),
                                      args=(self.n, self.y, ylag, self.x,
                                            xlag, W), method='bounded',
                                      tol=epsilon)
            elif methodML == 'ORD':
                # check on symmetry structure
                if w.asymmetry(intrinsic=False) == []:
                    ww = symmetrize(w)
                    WW = ww.todense()
                    evals = la.eigvalsh(WW)
                else:
                    evals = la.eigvals(W)
                res = minimize_scalar(
                    err_c_loglik_ord, 0.0, bounds=(-1.0, 1.0),
                    args=(self.n, self.y, ylag, self.x,
                          xlag, evals), method='bounded',
                    tol=epsilon)
        else:
            raise Exception, "{0} is an unsupported method".format(method)

        self.lam = res.x

        # compute full log-likelihood, including constants
        ln2pi = np.log(2.0 * np.pi)
        llik = -res.fun - self.n / 2.0 * ln2pi - self.n / 2.0

        self.logll = llik

        # b, residuals and predicted values

        ys = self.y - self.lam * ylag
        xs = self.x - self.lam * xlag
        xsxs = np.dot(xs.T, xs)
        xsxsi = np.linalg.inv(xsxs)
        xsys = np.dot(xs.T, ys)
        b = np.dot(xsxsi, xsys)

        self.betas = np.vstack((b, self.lam))

        self.u = y - np.dot(self.x, b)
        self.predy = self.y - self.u

        # residual variance

        self.e_filtered = self.u - self.lam * ps.lag_spatial(w, self.u)
        self.sig2 = np.dot(self.e_filtered.T, self.e_filtered) / self.n

        # variance-covariance matrix betas

        varb = self.sig2 * xsxsi

        # variance-covariance matrix lambda, sigma

        a = -self.lam * W
        np.fill_diagonal(a, 1.0)
        ai = la.inv(a)
        wai = np.dot(W, ai)
        tr1 = np.trace(wai)

        wai2 = np.dot(wai, wai)
        tr2 = np.trace(wai2)

        waiTwai = np.dot(wai.T, wai)
        tr3 = np.trace(waiTwai)

        v1 = np.vstack((tr2 + tr3,
                       tr1 / self.sig2))
        v2 = np.vstack((tr1 / self.sig2,
                       self.n / (2.0 * self.sig2 ** 2)))

        v = np.hstack((v1, v2))

        self.vm1 = np.linalg.inv(v)

        # create variance matrix for beta, lambda
        vv = np.hstack((varb, np.zeros((self.k, 1))))
        vv1 = np.hstack(
            (np.zeros((1, self.k)), self.vm1[0, 0] * np.ones((1, 1))))

        self.vm = np.vstack((vv, vv1))

        self._cache = {}

    def get_x_lag(self, w, regimes_att):
        if regimes_att:
            xlag = ps.lag_spatial(w, regimes_att['x'])
            xlag = REGI.Regimes_Frame.__init__(self, xlag,
                                               regimes_att['regimes'], constant_regi=None, cols2regi=regimes_att['cols2regi'])[0]
            xlag = xlag.toarray()
        else:
            xlag = ps.lag_spatial(w, self.x)
        return xlag


class ML_Error(BaseML_Error):

    """
    ML estimation of the spatial lag model with all results and diagnostics;
    Anselin (1988) [1]_

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    w            : Sparse matrix
                   Spatial weights sparse matrix
    method       : string
                   if 'full', brute force calculation (full matrix expressions)
                   ir 'ord', Ord eigenvalue method
    epsilon      : float
                   tolerance criterion in mimimize_scalar function and inverse_product
    spat_diag    : boolean
                   if True, include spatial diagnostics
    vm           : boolean
                   if True, include variance-covariance matrix in summary
                   results
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output

    Attributes
    ----------
    betas        : array
                   (k+1)x1 array of estimated coefficients (rho first)
    lam          : float
                   estimate of spatial autoregressive coefficient
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant, excluding lambda)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    method       : string
                   log Jacobian method
                   if 'full': brute force (full matrix computations)
    epsilon      : float
                   tolerance criterion used in minimize_scalar function and inverse_product
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    varb         : array
                   Variance covariance matrix (k+1 x k+1) - includes var(lambda)
    vm1          : array
                   variance covariance matrix for lambda, sigma (2 x 2)
    sig2         : float
                   Sigma squared used in computations
    logll        : float
                   maximized log-likelihood (including constant terms)
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
    utu          : float
                   Sum of squared residuals
    std_err      : array
                   1xk array of standard errors of the betas
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    title        : string
                   Name of the regression method used

    Examples
    --------

    >>> import numpy as np
    >>> import pysal as ps
    >>> np.set_printoptions(suppress=True)  #prevent scientific format
    >>> db = ps.open(ps.examples.get_path("south.dbf"),'r')
    >>> ds_name = "south.dbf"
    >>> y_name = "HR90"
    >>> y = np.array(db.by_col(y_name))
    >>> y.shape = (len(y),1)
    >>> x_names = ["RD90","PS90","UE90","DV90"]
    >>> x = np.array([db.by_col(var) for var in x_names]).T
    >>> ww = ps.open(ps.examples.get_path("south_q.gal"))
    >>> w = ww.read()
    >>> ww.close()
    >>> w_name = "south_q.gal"
    >>> w.transform = 'r'
    >>> mlerr = ML_Error(y,x,w,name_y=y_name,name_x=x_names,\
               name_w=w_name,name_ds=ds_name)
    >>> np.around(mlerr.betas, decimals=4)
    array([[ 6.1492],
           [ 4.4024],
           [ 1.7784],
           [-0.3781],
           [ 0.4858],
           [ 0.2991]])
    >>> "{0:.4f}".format(mlerr.lam)
    '0.2991'
    >>> "{0:.4f}".format(mlerr.mean_y)
    '9.5493'
    >>> "{0:.4f}".format(mlerr.std_y)
    '7.0389'
    >>> np.around(np.diag(mlerr.vm), decimals=4)
    array([ 1.0648,  0.0555,  0.0454,  0.0061,  0.0148,  0.0014])
    >>> np.around(mlerr.sig2, decimals=4)
    array([[ 32.4069]])
    >>> "{0:.4f}".format(mlerr.logll)
    '-4471.4071'
    >>> "{0:.4f}".format(mlerr.aic)
    '8952.8141'
    >>> "{0:.4f}".format(mlerr.schwarz)
    '8979.0779'
    >>> "{0:.4f}".format(mlerr.pr2)
    '0.3058'
    >>> "{0:.4f}".format(mlerr.utu)
    '48534.9148'
    >>> np.around(mlerr.std_err, decimals=4)
    array([ 1.0319,  0.2355,  0.2132,  0.0784,  0.1217,  0.0378])
    >>> np.around(mlerr.z_stat, decimals=4)
    array([[  5.9593,   0.    ],
           [ 18.6902,   0.    ],
           [  8.3422,   0.    ],
           [ -4.8233,   0.    ],
           [  3.9913,   0.0001],
           [  7.9089,   0.    ]])
    >>> mlerr.name_y
    'HR90'
    >>> mlerr.name_x
    ['CONSTANT', 'RD90', 'PS90', 'UE90', 'DV90', 'lambda']
    >>> mlerr.name_w
    'south_q.gal'
    >>> mlerr.name_ds
    'south.dbf'
    >>> mlerr.title
    'MAXIMUM LIKELIHOOD SPATIAL ERROR (METHOD = FULL)'


    References
    ----------

    .. [1] Anselin, L. (1988) "Spatial Econometrics: Methods and Models".
    Kluwer Academic Publishers. Dordrecht.

    """

    def __init__(self, y, x, w, method='full', epsilon=0.0000001,
                 spat_diag=False, vm=False, name_y=None, name_x=None,
                 name_w=None, name_ds=None):
        n = USER.check_arrays(y, x)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        x_constant = USER.check_constant(x)
        method = method.upper()
        if method in ['FULL', 'ORD']:
            BaseML_Error.__init__(self, y=y, x=x_constant,
                                  w=w, method=method, epsilon=epsilon)
            self.title = "MAXIMUM LIKELIHOOD SPATIAL ERROR" + \
                " (METHOD = " + method + ")"
            self.name_ds = USER.set_name_ds(name_ds)
            self.name_y = USER.set_name_y(name_y)
            self.name_x = USER.set_name_x(name_x, x)
            self.name_x.append('lambda')
            self.name_w = USER.set_name_w(name_w, w)
            self.aic = DIAG.akaike(reg=self)
            self.schwarz = DIAG.schwarz(reg=self)
            SUMMARY.ML_Error(reg=self, w=w, vm=vm, spat_diag=spat_diag)
        else:
            raise Exception, "{0} is an unsupported method".format(method)


def err_c_loglik(lam, n, y, ylag, x, xlag, W):
    # concentrated log-lik for error model, no constants, brute force
    ys = y - lam * ylag
    xs = x - lam * xlag
    ysys = np.dot(ys.T, ys)
    xsxs = np.dot(xs.T, xs)
    xsxsi = np.linalg.inv(xsxs)
    xsys = np.dot(xs.T, ys)
    x1 = np.dot(xsxsi, xsys)
    x2 = np.dot(xsys.T, x1)
    ee = ysys - x2
    sig2 = ee[0][0] / n
    nlsig2 = (n / 2.0) * np.log(sig2)
    a = -lam * W
    np.fill_diagonal(a, 1.0)
    jacob = np.log(np.linalg.det(a))
    # this is the negative of the concentrated log lik for minimization
    clik = nlsig2 - jacob
    return clik


def err_c_loglik_ord(lam, n, y, ylag, x, xlag, evals):
    # concentrated log-lik for error model, no constants, brute force
    ys = y - lam * ylag
    xs = x - lam * xlag
    ysys = np.dot(ys.T, ys)
    xsxs = np.dot(xs.T, xs)
    xsxsi = np.linalg.inv(xsxs)
    xsys = np.dot(xs.T, ys)
    x1 = np.dot(xsxsi, xsys)
    x2 = np.dot(xsys.T, x1)
    ee = ysys - x2
    sig2 = ee[0][0] / n
    nlsig2 = (n / 2.0) * np.log(sig2)
    revals = lam * evals
    jacob = np.log(1 - revals).sum()
    if isinstance(jacob, complex):
        jacob = jacob.real
    # this is the negative of the concentrated log lik for minimization
    clik = nlsig2 - jacob
    return clik


def _test():
    import doctest
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)

if __name__ == "__main__":
    _test()

    db = ps.open(ps.examples.get_path("south.dbf"), 'r')
    ds_name = "south.dbf"
    y_name = "HR90"
    y = np.array(db.by_col(y_name))
    y.shape = (len(y), 1)
    x_names = ["RD90", "PS90", "UE90", "DV90"]
    x = np.array([db.by_col(var) for var in x_names]).T
    ww = ps.open(ps.examples.get_path("south_q.gal"))
    w = ww.read()
    ww.close()
    w_name = "south_q.gal"
    w.transform = 'r'
    mlerror = ML_Error(y, x, w, name_y=y_name, name_x=x_names,
                       name_w=w_name, name_ds=ds_name)
    print mlerror.summary
    mlerror1 = ML_Error(y, x, w, method='ord', name_y=y_name, name_x=x_names,
                        name_w=w_name, name_ds=ds_name)
    print mlerror1.summary

########NEW FILE########
__FILENAME__ = ml_error_regimes
"""
ML Estimation of Spatial Error Model
"""

__author__ = "Luc Anselin luc.anselin@asu.edu, Pedro V. Amaral pedro.amaral@asu.edu"

import pysal
import numpy as np
import multiprocessing as mp
import regimes as REGI
import user_output as USER
import summary_output as SUMMARY
import diagnostics as DIAG
from utils import set_warn
from ml_error import BaseML_Error
from platform import system

__all__ = ["ML_Error_Regimes"]


class ML_Error_Regimes(BaseML_Error, REGI.Regimes_Frame):

    """
    ML estimation of the spatial error model with regimes (note no consistency 
    checks, diagnostics or constants added); Anselin (1988) [1]_
    
    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    constant_regi : ['one', 'many']
                    Switcher controlling the constant term setup. It may take
                    the following values:

                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime (default)

    cols2regi    : list, 'all'
                   Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all' (default), all the variables vary by regime.
    w            : Sparse matrix
                   Spatial weights sparse matrix 
    method       : string
                   if 'full', brute force calculation (full matrix expressions)
    epsilon      : float
                   tolerance criterion in mimimize_scalar function and inverse_product
    regime_err_sep : boolean
                   If True, a separate regression is run for each regime.
    cores        : integer
                   Specifies the number of cores to be used in multiprocessing
                   Default: all cores available (specified as cores=None).
                   Note: Multiprocessing currently not available on Windows.
    spat_diag    : boolean
                   if True, include spatial diagnostics
    vm           : boolean
                   if True, include variance-covariance matrix in summary
                   results
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    name_regimes : string
                   Name of regimes variable for use in output

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   (k+1)x1 array of estimated coefficients (rho first)
    lam          : float
                   estimate of spatial autoregressive coefficient
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    u            : array
                   nx1 array of residuals
    e_filtered   : array
                   nx1 array of spatially filtered residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant, excluding the rho)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    method       : string
                   log Jacobian method
                   if 'full': brute force (full matrix computations)
    epsilon      : float
                   tolerance criterion used in minimize_scalar function and inverse_product
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (k+1 x k+1), all coefficients
    vm1          : array
                   variance covariance matrix for lambda, sigma (2 x 2)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    sig2         : float
                   Sigma squared used in computations
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    logll        : float
                   maximized log-likelihood (including constant terms)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    std_err      : array
                   1xk array of standard errors of the betas    
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    name_regimes : string
                   Name of regimes variable for use in output
    title        : string
                   Name of the regression method used
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    constant_regi: ['one', 'many']
                   Ignored if regimes=False. Constant option for regimes.
                   Switcher controlling the constant term setup. It may take
                   the following values:
                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime
    cols2regi    : list, 'all'
                   Ignored if regimes=False. Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all', all the variables vary by regime.
    regime_lag_sep   : boolean
                   If True, the spatial parameter for spatial lag is also
                   computed according to different regimes. If False (default), 
                   the spatial parameter is fixed accross regimes.
    kr           : int
                   Number of variables/columns to be "regimized" or subject
                   to change by regime. These will result in one parameter
                   estimate by regime for each variable (i.e. nr parameters per
                   variable)
    kf           : int
                   Number of variables/columns to be considered fixed or
                   global across regimes and hence only obtain one parameter
                   estimate
    nr           : int
                   Number of different regimes in the 'regimes' list
    multi        : dictionary
                   Only available when multiple regressions are estimated,
                   i.e. when regime_err_sep=True and no variable is fixed
                   across regimes.
                   Contains all attributes of each individual regression

    References
    ----------

    .. [1] Anselin, L. (1988) "Spatial Econometrics: Methods and Models".
        Kluwer Academic Publishers. Dordrecht.

    Examples
    --------

    Open data baltim.dbf using pysal and create the variables matrices and weights matrix.

    >>> import numpy as np
    >>> import pysal as ps
    >>> db =  ps.open(ps.examples.get_path("baltim.dbf"),'r')
    >>> ds_name = "baltim.dbf"
    >>> y_name = "PRICE"
    >>> y = np.array(db.by_col(y_name)).T
    >>> y.shape = (len(y),1)
    >>> x_names = ["NROOM","AGE","SQFT"]
    >>> x = np.array([db.by_col(var) for var in x_names]).T
    >>> ww = ps.open(ps.examples.get_path("baltim_q.gal"))
    >>> w = ww.read()
    >>> ww.close()
    >>> w_name = "baltim_q.gal"
    >>> w.transform = 'r'

    Since in this example we are interested in checking whether the results vary
    by regimes, we use CITCOU to define whether the location is in the city or
    outside the city (in the county):

    >>> regimes = db.by_col("CITCOU")

    Now we can run the regression with all parameters:

    >>> mlerr = ML_Error_Regimes(y,x,regimes,w=w,name_y=y_name,name_x=x_names,\
               name_w=w_name,name_ds=ds_name,name_regimes="CITCOU")
    >>> np.around(mlerr.betas, decimals=4)
    array([[ -2.3949],
           [  4.8738],
           [ -0.0291],
           [  0.3328],
           [ 31.7962],
           [  2.981 ],
           [ -0.2371],
           [  0.8058],
           [  0.6177]])
    >>> "{0:.6f}".format(mlerr.lam)
    '0.617707'
    >>> "{0:.6f}".format(mlerr.mean_y)
    '44.307180'
    >>> "{0:.6f}".format(mlerr.std_y)
    '23.606077'
    >>> np.around(mlerr.vm1, decimals=4)
    array([[   0.005 ,   -0.3535],
           [  -0.3535,  441.3039]])
    >>> np.around(np.diag(mlerr.vm), decimals=4)
    array([ 58.5055,   2.4295,   0.0072,   0.0639,  80.5925,   3.161 ,
             0.012 ,   0.0499,   0.005 ])
    >>> np.around(mlerr.sig2, decimals=4)
    array([[ 209.6064]])
    >>> "{0:.6f}".format(mlerr.logll)
    '-870.333106'
    >>> "{0:.6f}".format(mlerr.aic)
    '1756.666212'
    >>> "{0:.6f}".format(mlerr.schwarz)
    '1783.481077'
    >>> mlerr.title
    'MAXIMUM LIKELIHOOD SPATIAL ERROR - REGIMES (METHOD = full)'
    """

    def __init__(self, y, x, regimes, w=None, constant_regi='many',
                 cols2regi='all', method='full', epsilon=0.0000001,
                 regime_err_sep=False, cores=None, spat_diag=False,
                 vm=False, name_y=None, name_x=None,
                 name_w=None, name_ds=None, name_regimes=None):

        n = USER.check_arrays(y, x)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        self.constant_regi = constant_regi
        self.cols2regi = cols2regi
        self.regime_err_sep = regime_err_sep
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_y = USER.set_name_y(name_y)
        self.name_w = USER.set_name_w(name_w, w)
        self.name_regimes = USER.set_name_ds(name_regimes)
        self.n = n
        self.y = y

        x_constant = USER.check_constant(x)
        name_x = USER.set_name_x(name_x, x)
        self.name_x_r = name_x

        cols2regi = REGI.check_cols2regi(constant_regi, cols2regi, x)
        self.regimes_set = REGI._get_regimes_set(regimes)
        self.regimes = regimes
        USER.check_regimes(self.regimes_set, self.n, x.shape[1])
        self.regime_err_sep = regime_err_sep

        if regime_err_sep == True:
            if set(cols2regi) == set([True]):
                self._error_regimes_multi(y, x, regimes, w, cores,
                                          method, epsilon, cols2regi, vm, name_x, spat_diag)
            else:
                raise Exception, "All coefficients must vary accross regimes if regime_err_sep = True."
        else:
            regimes_att = {}
            regimes_att['x'] = x_constant
            regimes_att['regimes'] = regimes
            regimes_att['cols2regi'] = cols2regi
            x, name_x = REGI.Regimes_Frame.__init__(self, x_constant,
                                                    regimes, constant_regi=None, cols2regi=cols2regi,
                                                    names=name_x)

            BaseML_Error.__init__(
                self, y=y, x=x, w=w, method=method, epsilon=epsilon, regimes_att=regimes_att)

            self.title = "MAXIMUM LIKELIHOOD SPATIAL ERROR - REGIMES" + \
                " (METHOD = " + method + ")"
            self.name_x = USER.set_name_x(name_x, x, constant=True)
            self.name_x.append('lambda')
            self.kf += 1  # Adding a fixed k to account for lambda.
            self.chow = REGI.Chow(self)
            self.aic = DIAG.akaike(reg=self)
            self.schwarz = DIAG.schwarz(reg=self)
            self._cache = {}
            SUMMARY.ML_Error(reg=self, w=w, vm=vm,
                             spat_diag=spat_diag, regimes=True)

    def _error_regimes_multi(self, y, x, regimes, w, cores,
                             method, epsilon, cols2regi, vm, name_x, spat_diag):

        regi_ids = dict((r, list(np.where(np.array(regimes) == r)[0]))
                        for r in self.regimes_set)
        results_p = {}
        for r in self.regimes_set:
            if system() == 'Windows':
                is_win = True
                results_p[r] = _work_error(
                    *(y, x, regi_ids, r, w, method, epsilon, self.name_ds, self.name_y, name_x + ['lambda'], self.name_w, self.name_regimes))
            else:
                pool = mp.Pool(cores)
                results_p[r] = pool.apply_async(_work_error, args=(
                    y, x, regi_ids, r, w, method, epsilon, self.name_ds, self.name_y, name_x + ['lambda'], self.name_w, self.name_regimes, ))
                is_win = False
        self.kryd = 0
        self.kr = len(cols2regi) + 1
        self.kf = 0
        self.nr = len(self.regimes_set)
        self.vm = np.zeros((self.nr * self.kr, self.nr * self.kr), float)
        self.betas = np.zeros((self.nr * self.kr, 1), float)
        self.u = np.zeros((self.n, 1), float)
        self.predy = np.zeros((self.n, 1), float)
        self.e_filtered = np.zeros((self.n, 1), float)
        self.name_y, self.name_x = [], []
        if not is_win:
            pool.close()
            pool.join()
        results = {}
        counter = 0
        for r in self.regimes_set:
            if is_win:
                results[r] = results_p[r]
            else:
                results[r] = results_p[r].get()
            self.vm[(counter * self.kr):((counter + 1) * self.kr),
                    (counter * self.kr):((counter + 1) * self.kr)] = results[r].vm
            self.betas[(counter * self.kr):((counter + 1) * self.kr),
                       ] = results[r].betas
            self.u[regi_ids[r], ] = results[r].u
            self.predy[regi_ids[r], ] = results[r].predy
            self.e_filtered[regi_ids[r], ] = results[r].e_filtered
            self.name_y += results[r].name_y
            self.name_x += results[r].name_x
            counter += 1
        self.chow = REGI.Chow(self)
        self.multi = results
        SUMMARY.ML_Error_multi(reg=self, multireg=self.multi,
                               vm=vm, spat_diag=spat_diag, regimes=True, w=w)


def _work_error(y, x, regi_ids, r, w, method, epsilon, name_ds, name_y, name_x, name_w, name_regimes):
    w_r, warn = REGI.w_regime(w, regi_ids[r], r, transform=True)
    y_r = y[regi_ids[r]]
    x_r = x[regi_ids[r]]
    x_constant = USER.check_constant(x_r)
    model = BaseML_Error(y=y_r, x=x_constant, w=w_r,
                         method=method, epsilon=epsilon)
    set_warn(model, warn)
    model.w = w_r
    model.title = "MAXIMUM LIKELIHOOD SPATIAL ERROR - REGIME " + \
        str(r) + " (METHOD = " + method + ")"
    model.name_ds = name_ds
    model.name_y = '%s_%s' % (str(r), name_y)
    model.name_x = ['%s_%s' % (str(r), i) for i in name_x]
    model.name_w = name_w
    model.name_regimes = name_regimes
    model.aic = DIAG.akaike(reg=model)
    model.schwarz = DIAG.schwarz(reg=model)
    return model


def _test():
    import doctest
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)

if __name__ == "__main__":
    _test()
    import numpy as np
    import pysal as ps

    db = ps.open(ps.examples.get_path("baltim.dbf"), 'r')
    ds_name = "baltim.dbf"
    y_name = "PRICE"
    y = np.array(db.by_col(y_name)).T
    y.shape = (len(y), 1)
    x_names = ["NROOM", "NBATH", "PATIO", "FIREPL",
               "AC", "GAR", "AGE", "LOTSZ", "SQFT"]
    x = np.array([db.by_col(var) for var in x_names]).T
    ww = ps.open(ps.examples.get_path("baltim_q.gal"))
    w = ww.read()
    ww.close()
    w_name = "baltim_q.gal"
    w.transform = 'r'

    regimes = []
    y_coord = np.array(db.by_col("Y"))
    for i in y_coord:
        if i > 544.5:
            regimes.append("North")
        else:
            regimes.append("South")

    mlerror = ML_Error_Regimes(
        y, x, regimes, w=w, method='full', name_y=y_name,
        name_x=x_names, name_w=w_name, name_ds=ds_name, regime_err_sep=False)
    print mlerror.summary

########NEW FILE########
__FILENAME__ = ml_lag
"""
ML Estimation of Spatial Lag Model
"""

__author__ = "Luc Anselin luc.anselin@asu.edu, Serge Rey srey@asu.edu"

import numpy as np
import numpy.linalg as la
import pysal as ps
from pysal.spreg.utils import RegressionPropsY, RegressionPropsVM, inverse_prod
from utils import spdot
import diagnostics as DIAG
import user_output as USER
import summary_output as SUMMARY
from w_utils import symmetrize
try:
    from scipy.optimize import minimize_scalar
except ImportError as e:
    print(e, "Maximum Likelihood in PySAL requires SciPy version 0.11 or newer.")

__all__ = ["ML_Lag"]


class BaseML_Lag(RegressionPropsY, RegressionPropsVM):

    """
    ML estimation of the spatial lag model (note no consistency
    checks, diagnostics or constants added); Anselin (1988) [1]_

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    w            : pysal W object
                   Spatial weights object
    method       : string
                   if 'full', brute force calculation (full matrix expressions)
    epsilon      : float
                   tolerance criterion in mimimize_scalar function and inverse_product

    Attributes
    ----------
    betas        : array
                   (k+1)x1 array of estimated coefficients (rho first)
    rho          : float
                   estimate of spatial autoregressive coefficient
    u            : array
                   nx1 array of residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant, excluding the rho)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    method       : string
                   log Jacobian method
                   if 'full': brute force (full matrix computations)
                   if 'ord' : Ord eigenvalue method
    epsilon      : float
                   tolerance criterion used in minimize_scalar function and inverse_product
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (k+1 x k+1)
    vm1          : array
                   Variance covariance matrix (k+2 x k+2) includes sigma2
    sig2         : float
                   Sigma squared used in computations
    logll        : float
                   maximized log-likelihood (including constant terms)
    predy_e      : array
                   predicted values from reduced form
    e_pred       : array
                   prediction errors using reduced form predicted values


    Examples
    --------

    >>> import numpy as np
    >>> import pysal as ps
    >>> db =  ps.open(ps.examples.get_path("baltim.dbf"),'r')
    >>> ds_name = "baltim.dbf"
    >>> y_name = "PRICE"
    >>> y = np.array(db.by_col(y_name)).T
    >>> y.shape = (len(y),1)
    >>> x_names = ["NROOM","NBATH","PATIO","FIREPL","AC","GAR","AGE","LOTSZ","SQFT"]
    >>> x = np.array([db.by_col(var) for var in x_names]).T
    >>> x = np.hstack((np.ones((len(y),1)),x))
    >>> ww = ps.open(ps.examples.get_path("baltim_q.gal"))
    >>> w = ww.read()
    >>> ww.close()
    >>> w.transform = 'r'
    >>> w_name = "baltim_q.gal"
    >>> mllag = BaseML_Lag(y,x,w,method='ord')
    >>> "{0:.6f}".format(mllag.rho)
    '0.425885'
    >>> np.around(mllag.betas, decimals=4)
    array([[ 4.3675],
           [ 0.7502],
           [ 5.6116],
           [ 7.0497],
           [ 7.7246],
           [ 6.1231],
           [ 4.6375],
           [-0.1107],
           [ 0.0679],
           [ 0.0794],
           [ 0.4259]])
    >>> "{0:.6f}".format(mllag.mean_y)
    '44.307180'
    >>> "{0:.6f}".format(mllag.std_y)
    '23.606077'
    >>> np.around(np.diag(mllag.vm1), decimals=4)
    array([  23.8716,    1.1222,    3.0593,    7.3416,    5.6695,    5.4698,
              2.8684,    0.0026,    0.0002,    0.0266,    0.0032,  220.1292])
    >>> np.around(np.diag(mllag.vm), decimals=4)
    array([ 23.8716,   1.1222,   3.0593,   7.3416,   5.6695,   5.4698,
             2.8684,   0.0026,   0.0002,   0.0266,   0.0032])
    >>> "{0:.6f}".format(mllag.sig2)
    '151.458698'
    >>> "{0:.6f}".format(mllag.logll)
    '-832.937174'
    >>> mllag = BaseML_Lag(y,x,w)
    >>> "{0:.6f}".format(mllag.rho)
    '0.425885'
    >>> np.around(mllag.betas, decimals=4)
    array([[ 4.3675],
           [ 0.7502],
           [ 5.6116],
           [ 7.0497],
           [ 7.7246],
           [ 6.1231],
           [ 4.6375],
           [-0.1107],
           [ 0.0679],
           [ 0.0794],
           [ 0.4259]])
    >>> "{0:.6f}".format(mllag.mean_y)
    '44.307180'
    >>> "{0:.6f}".format(mllag.std_y)
    '23.606077'
    >>> np.around(np.diag(mllag.vm1), decimals=4)
    array([  23.8716,    1.1222,    3.0593,    7.3416,    5.6695,    5.4698,
              2.8684,    0.0026,    0.0002,    0.0266,    0.0032,  220.1292])
    >>> np.around(np.diag(mllag.vm), decimals=4)
    array([ 23.8716,   1.1222,   3.0593,   7.3416,   5.6695,   5.4698,
             2.8684,   0.0026,   0.0002,   0.0266,   0.0032])
    >>> "{0:.6f}".format(mllag.sig2)
    '151.458698'
    >>> "{0:.6f}".format(mllag.logll)
    '-832.937174'


    References
    ----------

    .. [1] Anselin, L. (1988) "Spatial Econometrics: Methods and Models".
    Kluwer Academic Publishers. Dordrecht.

    """

    def __init__(self, y, x, w, method='full', epsilon=0.0000001):
        # set up main regression variables and spatial filters
        self.y = y
        self.x = x
        self.n, self.k = self.x.shape
        self.method = method
        self.epsilon = epsilon
        W = w.full()[0]
        ylag = ps.lag_spatial(w, y)
        # b0, b1, e0 and e1
        xtx = spdot(self.x.T, self.x)
        xtxi = la.inv(xtx)
        xty = spdot(self.x.T, self.y)
        xtyl = spdot(self.x.T, ylag)
        b0 = np.dot(xtxi, xty)
        b1 = np.dot(xtxi, xtyl)
        e0 = self.y - spdot(x, b0)
        e1 = ylag - spdot(x, b1)
        methodML = method.upper()
        # call minimizer using concentrated log-likelihood to get rho
        if methodML in ['FULL', 'ORD']:
            if methodML == 'FULL':
                res = minimize_scalar(lag_c_loglik, 0.0, bounds=(-1.0, 1.0),
                                      args=(self.n, e0, e1,
                                            W), method='bounded',
                                      tol=epsilon)
            elif methodML == 'ORD':
                # check on symmetry structure
                if w.asymmetry(intrinsic=False) == []:
                    ww = symmetrize(w)
                    WW = ww.todense()
                    evals = la.eigvalsh(WW)
                else:
                    evals = la.eigvals(W)
                res = minimize_scalar(
                    lag_c_loglik_ord, 0.0, bounds=(-1.0, 1.0),
                    args=(self.n, e0, e1,
                          evals), method='bounded',
                    tol=epsilon)
        else:
            # program will crash, need to catch
            print "{0} is an unsupported method".format(methodML)
            self = None
            return

        self.rho = res.x[0][0]

        # compute full log-likelihood, including constants
        ln2pi = np.log(2.0 * np.pi)
        llik = -res.fun - self.n / 2.0 * ln2pi - self.n / 2.0
        self.logll = llik[0][0]

        # b, residuals and predicted values

        b = b0 - self.rho * b1
        self.betas = np.vstack((b, self.rho))   # rho added as last coefficient
        self.u = e0 - self.rho * e1
        self.predy = self.y - self.u

        xb = spdot(x, b)

        self.predy_e = inverse_prod(
            w.sparse, xb, self.rho, inv_method="power_exp", threshold=epsilon)
        self.e_pred = self.y - self.predy_e

        # residual variance
        self._cache = {}
        self.sig2 = self.sig2n  # no allowance for division by n-k

        # information matrix
        a = -self.rho * W
        np.fill_diagonal(a, 1.0)
        ai = la.inv(a)
        wai = np.dot(W, ai)
        tr1 = np.trace(wai)

        wai2 = np.dot(wai, wai)
        tr2 = np.trace(wai2)

        waiTwai = np.dot(wai.T, wai)
        tr3 = np.trace(waiTwai)

        wpredy = ps.lag_spatial(w, self.predy_e)
        wpyTwpy = np.dot(wpredy.T, wpredy)
        xTwpy = spdot(x.T, wpredy)

        # order of variables is beta, rho, sigma2

        v1 = np.vstack(
            (xtx / self.sig2, xTwpy.T / self.sig2, np.zeros((1, self.k))))
        v2 = np.vstack(
            (xTwpy / self.sig2, tr2 + tr3 + wpyTwpy / self.sig2, tr1 / self.sig2))
        v3 = np.vstack(
            (np.zeros((self.k, 1)), tr1 / self.sig2, self.n / (2.0 * self.sig2 ** 2)))

        v = np.hstack((v1, v2, v3))

        self.vm1 = la.inv(v)  # vm1 includes variance for sigma2
        self.vm = self.vm1[:-1, :-1]  # vm is for coefficients only


class ML_Lag(BaseML_Lag):

    """
    ML estimation of the spatial lag model with all results and diagnostics;
    Anselin (1988) [1]_

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    w            : pysal W object
                   Spatial weights object
    method       : string
                   if 'full', brute force calculation (full matrix expressions)
                   if 'ord', Ord eigenvalue method
    epsilon      : float
                   tolerance criterion in mimimize_scalar function and inverse_product
    spat_diag    : boolean
                   if True, include spatial diagnostics
    vm           : boolean
                   if True, include variance-covariance matrix in summary
                   results
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output

    Attributes
    ----------
    betas        : array
                   (k+1)x1 array of estimated coefficients (rho first)
    rho          : float
                   estimate of spatial autoregressive coefficient
    u            : array
                   nx1 array of residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant, excluding the rho)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    method       : string
                   log Jacobian method
                   if 'full': brute force (full matrix computations)
    epsilon      : float
                   tolerance criterion used in minimize_scalar function and inverse_product
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (k+1 x k+1), all coefficients
    vm1          : array
                   Variance covariance matrix (k+2 x k+2), includes sig2
    sig2         : float
                   Sigma squared used in computations
    logll        : float
                   maximized log-likelihood (including constant terms)
    aic          : float
                   Akaike information criterion
    schwarz      : float
                   Schwarz criterion
    predy_e      : array
                   predicted values from reduced form
    e_pred       : array
                   prediction errors using reduced form predicted values
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
    pr2_e        : float
                   Pseudo R squared (squared correlation between y and ypred_e
                   (using reduced form))
    utu          : float
                   Sum of squared residuals
    std_err      : array
                   1xk array of standard errors of the betas
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    title        : string
                   Name of the regression method used

    Examples
    --------

    >>> import numpy as np
    >>> import pysal as ps
    >>> db =  ps.open(ps.examples.get_path("baltim.dbf"),'r')
    >>> ds_name = "baltim.dbf"
    >>> y_name = "PRICE"
    >>> y = np.array(db.by_col(y_name)).T
    >>> y.shape = (len(y),1)
    >>> x_names = ["NROOM","NBATH","PATIO","FIREPL","AC","GAR","AGE","LOTSZ","SQFT"]
    >>> x = np.array([db.by_col(var) for var in x_names]).T
    >>> ww = ps.open(ps.examples.get_path("baltim_q.gal"))
    >>> w = ww.read()
    >>> ww.close()
    >>> w_name = "baltim_q.gal"
    >>> w.transform = 'r'
    >>> mllag = ML_Lag(y,x,w,name_y=y_name,name_x=x_names,\
               name_w=w_name,name_ds=ds_name)
    >>> np.around(mllag.betas, decimals=4)
    array([[ 4.3675],
           [ 0.7502],
           [ 5.6116],
           [ 7.0497],
           [ 7.7246],
           [ 6.1231],
           [ 4.6375],
           [-0.1107],
           [ 0.0679],
           [ 0.0794],
           [ 0.4259]])
    >>> "{0:.6f}".format(mllag.rho)
    '0.425885'
    >>> "{0:.6f}".format(mllag.mean_y)
    '44.307180'
    >>> "{0:.6f}".format(mllag.std_y)
    '23.606077'
    >>> np.around(np.diag(mllag.vm1), decimals=4)
    array([  23.8716,    1.1222,    3.0593,    7.3416,    5.6695,    5.4698,
              2.8684,    0.0026,    0.0002,    0.0266,    0.0032,  220.1292])
    >>> np.around(np.diag(mllag.vm), decimals=4)
    array([ 23.8716,   1.1222,   3.0593,   7.3416,   5.6695,   5.4698,
             2.8684,   0.0026,   0.0002,   0.0266,   0.0032])
    >>> "{0:.6f}".format(mllag.sig2)
    '151.458698'
    >>> "{0:.6f}".format(mllag.logll)
    '-832.937174'
    >>> "{0:.6f}".format(mllag.aic)
    '1687.874348'
    >>> "{0:.6f}".format(mllag.schwarz)
    '1724.744787'
    >>> "{0:.6f}".format(mllag.pr2)
    '0.727081'
    >>> "{0:.4f}".format(mllag.pr2_e)
    '0.7062'
    >>> "{0:.4f}".format(mllag.utu)
    '31957.7853'
    >>> np.around(mllag.std_err, decimals=4)
    array([ 4.8859,  1.0593,  1.7491,  2.7095,  2.3811,  2.3388,  1.6936,
            0.0508,  0.0146,  0.1631,  0.057 ])
    >>> np.around(mllag.z_stat, decimals=4)
    array([[ 0.8939,  0.3714],
           [ 0.7082,  0.4788],
           [ 3.2083,  0.0013],
           [ 2.6018,  0.0093],
           [ 3.2442,  0.0012],
           [ 2.6181,  0.0088],
           [ 2.7382,  0.0062],
           [-2.178 ,  0.0294],
           [ 4.6487,  0.    ],
           [ 0.4866,  0.6266],
           [ 7.4775,  0.    ]])
    >>> mllag.name_y
    'PRICE'
    >>> mllag.name_x
    ['CONSTANT', 'NROOM', 'NBATH', 'PATIO', 'FIREPL', 'AC', 'GAR', 'AGE', 'LOTSZ', 'SQFT', 'W_PRICE']
    >>> mllag.name_w
    'baltim_q.gal'
    >>> mllag.name_ds
    'baltim.dbf'
    >>> mllag.title
    'MAXIMUM LIKELIHOOD SPATIAL LAG (METHOD = FULL)'
    >>> mllag = ML_Lag(y,x,w,method='ord',name_y=y_name,name_x=x_names,\
               name_w=w_name,name_ds=ds_name)
    >>> np.around(mllag.betas, decimals=4)
    array([[ 4.3675],
           [ 0.7502],
           [ 5.6116],
           [ 7.0497],
           [ 7.7246],
           [ 6.1231],
           [ 4.6375],
           [-0.1107],
           [ 0.0679],
           [ 0.0794],
           [ 0.4259]])
    >>> "{0:.6f}".format(mllag.rho)
    '0.425885'
    >>> "{0:.6f}".format(mllag.mean_y)
    '44.307180'
    >>> "{0:.6f}".format(mllag.std_y)
    '23.606077'
    >>> np.around(np.diag(mllag.vm1), decimals=4)
    array([  23.8716,    1.1222,    3.0593,    7.3416,    5.6695,    5.4698,
              2.8684,    0.0026,    0.0002,    0.0266,    0.0032,  220.1292])
    >>> np.around(np.diag(mllag.vm), decimals=4)
    array([ 23.8716,   1.1222,   3.0593,   7.3416,   5.6695,   5.4698,
             2.8684,   0.0026,   0.0002,   0.0266,   0.0032])
    >>> "{0:.6f}".format(mllag.sig2)
    '151.458698'
    >>> "{0:.6f}".format(mllag.logll)
    '-832.937174'
    >>> "{0:.6f}".format(mllag.aic)
    '1687.874348'
    >>> "{0:.6f}".format(mllag.schwarz)
    '1724.744787'
    >>> "{0:.6f}".format(mllag.pr2)
    '0.727081'
    >>> "{0:.6f}".format(mllag.pr2_e)
    '0.706198'
    >>> "{0:.4f}".format(mllag.utu)
    '31957.7853'
    >>> np.around(mllag.std_err, decimals=4)
    array([ 4.8859,  1.0593,  1.7491,  2.7095,  2.3811,  2.3388,  1.6936,
            0.0508,  0.0146,  0.1631,  0.057 ])
    >>> np.around(mllag.z_stat, decimals=4)
    array([[ 0.8939,  0.3714],
           [ 0.7082,  0.4788],
           [ 3.2083,  0.0013],
           [ 2.6018,  0.0093],
           [ 3.2442,  0.0012],
           [ 2.6181,  0.0088],
           [ 2.7382,  0.0062],
           [-2.178 ,  0.0294],
           [ 4.6487,  0.    ],
           [ 0.4866,  0.6266],
           [ 7.4775,  0.    ]])
    >>> mllag.name_y
    'PRICE'
    >>> mllag.name_x
    ['CONSTANT', 'NROOM', 'NBATH', 'PATIO', 'FIREPL', 'AC', 'GAR', 'AGE', 'LOTSZ', 'SQFT', 'W_PRICE']
    >>> mllag.name_w
    'baltim_q.gal'
    >>> mllag.name_ds
    'baltim.dbf'
    >>> mllag.title
    'MAXIMUM LIKELIHOOD SPATIAL LAG (METHOD = ORD)'

    References
    ----------

    .. [1] Anselin, L. (1988) "Spatial Econometrics: Methods and Models".
        Kluwer Academic Publishers. Dordrecht.

    """

    def __init__(self, y, x, w, method='full', epsilon=0.0000001,
                 spat_diag=False, vm=False, name_y=None, name_x=None,
                 name_w=None, name_ds=None):
        n = USER.check_arrays(y, x)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        x_constant = USER.check_constant(x)
        method = method.upper()
        if method in ['FULL', 'ORD']:
            BaseML_Lag.__init__(self, y=y, x=x_constant,
                                w=w, method=method, epsilon=epsilon)
            # increase by 1 to have correct aic and sc, include rho in count
            self.k += 1
            self.title = "MAXIMUM LIKELIHOOD SPATIAL LAG" + \
                " (METHOD = " + method + ")"
            self.name_ds = USER.set_name_ds(name_ds)
            self.name_y = USER.set_name_y(name_y)
            self.name_x = USER.set_name_x(name_x, x)
            name_ylag = USER.set_name_yend_sp(self.name_y)
            self.name_x.append(name_ylag)  # rho changed to last position
            self.name_w = USER.set_name_w(name_w, w)
            self.aic = DIAG.akaike(reg=self)
            self.schwarz = DIAG.schwarz(reg=self)
            SUMMARY.ML_Lag(reg=self, w=w, vm=vm, spat_diag=spat_diag)
        else:
            raise Exception, "{0} is an unsupported method".format(method)


def lag_c_loglik(rho, n, e0, e1, W):
    # concentrated log-lik for lag model, no constants, brute force
    er = e0 - rho * e1
    sig2 = np.dot(er.T, er) / n
    nlsig2 = (n / 2.0) * np.log(sig2)
    a = -rho * W
    np.fill_diagonal(a, 1.0)
    jacob = np.log(np.linalg.det(a))
    # this is the negative of the concentrated log lik for minimization
    clik = nlsig2 - jacob
    return clik


def lag_c_loglik_ord(rho, n, e0, e1, evals):
    # concentrated log-lik for lag model, no constants, Ord eigenvalue method
    er = e0 - rho * e1
    sig2 = np.dot(er.T, er) / n
    nlsig2 = (n / 2.0) * np.log(sig2)
    revals = rho * evals
    jacob = np.log(1 - revals).sum()
    if isinstance(jacob, complex):
        jacob = jacob.real
    # this is the negative of the concentrated log lik for minimization
    clik = nlsig2 - jacob
    return clik


def _test():
    import doctest
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)

if __name__ == "__main__":
    _test()

    """
    db = ps.open(ps.examples.get_path("NAT.dbf"),'r')
    ds_name = "NAT.DBF"
    y_name = "HR90"
    y = np.array(db.by_col(y_name))
    y.shape = (len(y),1)
    x_names = ["RD90","PS90","UE90","DV90","MA90"]
    x = np.array([db.by_col(var) for var in x_names]).T
    ww = ps.open(ps.examples.get_path("nat_queen.gal"))
    w = ww.read()
    ww.close()
    w_name = "nat_queen.gal"
    """
    db = ps.open(ps.examples.get_path("baltim.dbf"), 'r')
    ds_name = "baltim.dbf"
    y_name = "PRICE"
    y = np.array(db.by_col(y_name)).T
    y.shape = (len(y), 1)
    x_names = ["NROOM", "NBATH", "PATIO", "FIREPL",
               "AC", "GAR", "AGE", "LOTSZ", "SQFT"]
    x = np.array([db.by_col(var) for var in x_names]).T
    ww = ps.open(ps.examples.get_path("baltim_q.gal"))
    w = ww.read()
    ww.close()
    w_name = "baltim_q.gal"

    w.transform = 'r'
    mllag = ML_Lag(y, x, w, method='full', name_y=y_name, name_x=x_names,
                   name_w=w_name, name_ds=ds_name)
    print mllag.summary
    mllag1 = ML_Lag(y, x, w, method='ord', name_y=y_name, name_x=x_names,
                    name_w=w_name, name_ds=ds_name)
    print mllag1.summary

########NEW FILE########
__FILENAME__ = ml_lag_regimes
"""
ML Estimation of Spatial Lag Model with Regimes
"""

__author__ = "Luc Anselin luc.anselin@asu.edu, Pedro V. Amaral pedro.amaral@asu.edu"

import pysal
import numpy as np
import regimes as REGI
import user_output as USER
import summary_output as SUMMARY
import diagnostics as DIAG
import multiprocessing as mp
from ml_lag import BaseML_Lag
from utils import set_warn
from platform import system

__all__ = ["ML_Lag_Regimes"]


class ML_Lag_Regimes(BaseML_Lag, REGI.Regimes_Frame):

    """
    ML estimation of the spatial lag model with regimes (note no consistency 
    checks, diagnostics or constants added); Anselin (1988) [1]_
    
    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    constant_regi : ['one', 'many']
                    Switcher controlling the constant term setup. It may take
                    the following values:

                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime (default)

    cols2regi    : list, 'all'
                   Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all' (default), all the variables vary by regime.
    w            : Sparse matrix
                   Spatial weights sparse matrix 
    method       : string
                   if 'full', brute force calculation (full matrix expressions)
    epsilon      : float
                   tolerance criterion in mimimize_scalar function and inverse_product
    regime_lag_sep: boolean
                   If True (default), the spatial parameter for spatial lag is also
                   computed according to different regimes. If False, 
                   the spatial parameter is fixed accross regimes.
    cores        : integer
                   Specifies the number of cores to be used in multiprocessing
                   Default: all cores available (specified as cores=None).
                   Note: Multiprocessing currently not available on Windows.
    spat_diag    : boolean
                   if True, include spatial diagnostics
    vm           : boolean
                   if True, include variance-covariance matrix in summary
                   results
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    name_regimes : string
                   Name of regimes variable for use in output

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   (k+1)x1 array of estimated coefficients (rho first)
    rho          : float
                   estimate of spatial autoregressive coefficient
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    u            : array
                   nx1 array of residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant, excluding the rho)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    method       : string
                   log Jacobian method
                   if 'full': brute force (full matrix computations)
    epsilon      : float
                   tolerance criterion used in minimize_scalar function and inverse_product
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (k+1 x k+1), all coefficients
    vm1          : array
                   Variance covariance matrix (k+2 x k+2), includes sig2
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    sig2         : float
                   Sigma squared used in computations
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    logll        : float
                   maximized log-likelihood (including constant terms)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    aic          : float
                   Akaike information criterion
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    schwarz      : float
                   Schwarz criterion
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    predy_e      : array
                   predicted values from reduced form
    e_pred       : array
                   prediction errors using reduced form predicted values
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    pr2_e        : float
                   Pseudo R squared (squared correlation between y and ypred_e
                   (using reduced form))
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    std_err      : array
                   1xk array of standard errors of the betas    
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    name_regimes : string
                   Name of regimes variable for use in output
    title        : string
                   Name of the regression method used
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    constant_regi : ['one', 'many']
                    Ignored if regimes=False. Constant option for regimes.
                    Switcher controlling the constant term setup. It may take
                    the following values:

                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime

    cols2regi    : list, 'all'
                   Ignored if regimes=False. Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all', all the variables vary by regime.
    regime_lag_sep  : boolean
                    If True, the spatial parameter for spatial lag is also
                    computed according to different regimes. If False (default), 
                    the spatial parameter is fixed accross regimes.
    kr           : int
                   Number of variables/columns to be "regimized" or subject
                   to change by regime. These will result in one parameter
                   estimate by regime for each variable (i.e. nr parameters per
                   variable)
    kf           : int
                   Number of variables/columns to be considered fixed or
                   global across regimes and hence only obtain one parameter
                   estimate
    nr           : int
                   Number of different regimes in the 'regimes' list
    multi        : dictionary
                   Only available when multiple regressions are estimated,
                   i.e. when regime_err_sep=True and no variable is fixed
                   across regimes.
                   Contains all attributes of each individual regression

    References
    ----------

    .. [1] Anselin, L. (1988) "Spatial Econometrics: Methods and Models".
       Kluwer Academic Publishers. Dordrecht.

    Examples
    --------

    Open data baltim.dbf using pysal and create the variables matrices and weights matrix.

    >>> import numpy as np
    >>> import pysal as ps
    >>> db =  ps.open(ps.examples.get_path("baltim.dbf"),'r')
    >>> ds_name = "baltim.dbf"
    >>> y_name = "PRICE"
    >>> y = np.array(db.by_col(y_name)).T
    >>> y.shape = (len(y),1)
    >>> x_names = ["NROOM","AGE","SQFT"]
    >>> x = np.array([db.by_col(var) for var in x_names]).T
    >>> ww = ps.open(ps.examples.get_path("baltim_q.gal"))
    >>> w = ww.read()
    >>> ww.close()
    >>> w_name = "baltim_q.gal"
    >>> w.transform = 'r'    

    Since in this example we are interested in checking whether the results vary
    by regimes, we use CITCOU to define whether the location is in the city or 
    outside the city (in the county):

    >>> regimes = db.by_col("CITCOU")

    Now we can run the regression with all parameters:

    >>> mllag = ML_Lag_Regimes(y,x,regimes,w=w,name_y=y_name,name_x=x_names,\
               name_w=w_name,name_ds=ds_name,name_regimes="CITCOU")
    >>> np.around(mllag.betas, decimals=4)
    array([[-15.0059],
           [  4.496 ],
           [ -0.0318],
           [  0.35  ],
           [ -4.5404],
           [  3.9219],
           [ -0.1702],
           [  0.8194],
           [  0.5385]])
    >>> "{0:.6f}".format(mllag.rho)
    '0.538503'
    >>> "{0:.6f}".format(mllag.mean_y)
    '44.307180'
    >>> "{0:.6f}".format(mllag.std_y)
    '23.606077'
    >>> np.around(np.diag(mllag.vm1), decimals=4)
    array([  47.42  ,    2.3953,    0.0051,    0.0648,   69.6765,    3.2066,
              0.0116,    0.0486,    0.004 ,  390.7274])
    >>> np.around(np.diag(mllag.vm), decimals=4)
    array([ 47.42  ,   2.3953,   0.0051,   0.0648,  69.6765,   3.2066,
             0.0116,   0.0486,   0.004 ])
    >>> "{0:.6f}".format(mllag.sig2)
    '200.044334'
    >>> "{0:.6f}".format(mllag.logll)
    '-864.985056'
    >>> "{0:.6f}".format(mllag.aic)
    '1745.970112'
    >>> "{0:.6f}".format(mllag.schwarz)
    '1772.784977'
    >>> mllag.title
    'MAXIMUM LIKELIHOOD SPATIAL LAG - REGIMES (METHOD = full)'
    """

    def __init__(self, y, x, regimes, w=None, constant_regi='many',
                 cols2regi='all', method='full', epsilon=0.0000001,
                 regime_lag_sep=False, cores=None, spat_diag=False,
                 vm=False, name_y=None, name_x=None,
                 name_w=None, name_ds=None, name_regimes=None):

        n = USER.check_arrays(y, x)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        USER.check_spat_diag(spat_diag, w)
        name_y = USER.set_name_y(name_y)
        self.name_y = name_y
        self.name_x_r = USER.set_name_x(
            name_x, x) + [USER.set_name_yend_sp(name_y)]
        self.method = method
        self.epsilon = epsilon
        self.name_regimes = USER.set_name_ds(name_regimes)
        self.constant_regi = constant_regi
        self.n = n
        cols2regi = REGI.check_cols2regi(
            constant_regi, cols2regi, x, add_cons=False)
        self.cols2regi = cols2regi
        self.regimes_set = REGI._get_regimes_set(regimes)
        self.regimes = regimes
        self.regime_lag_sep = regime_lag_sep
        self._cache = {}
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_w = USER.set_name_w(name_w, w)
        USER.check_regimes(self.regimes_set, self.n, x.shape[1])

        if regime_lag_sep == True:
            if not (set(cols2regi) == set([True]) and constant_regi == 'many'):
                raise Exception, "All variables must vary by regimes if regime_lag_sep = True."
            cols2regi += [True]
            w_i, regi_ids, warn = REGI.w_regimes(
                w, regimes, self.regimes_set, transform=True, get_ids=True, min_n=len(cols2regi) + 1)
            set_warn(self, warn)
        else:
            cols2regi += [False]

        if set(cols2regi) == set([True]) and constant_regi == 'many':
            self.y = y
            self.ML_Lag_Regimes_Multi(y, x, w_i, w, regi_ids,
                                      cores=cores, cols2regi=cols2regi, method=method, epsilon=epsilon,
                                      spat_diag=spat_diag, vm=vm, name_y=name_y, name_x=name_x,
                                      name_regimes=self.name_regimes,
                                      name_w=name_w, name_ds=name_ds)
        else:
            # if regime_lag_sep == True:
            #    w = REGI.w_regimes_union(w, w_i, self.regimes_set)
            name_x = USER.set_name_x(name_x, x, constant=True)
            x, self.name_x = REGI.Regimes_Frame.__init__(self, x,
                                                         regimes, constant_regi, cols2regi=cols2regi[:-1], names=name_x)
            self.name_x.append("_Global_" + USER.set_name_yend_sp(name_y))
            BaseML_Lag.__init__(self, y=y, x=x, w=w,
                                method=method, epsilon=epsilon)
            self.kf += 1  # Adding a fixed k to account for spatial lag.
            self.chow = REGI.Chow(self)
            self.aic = DIAG.akaike(reg=self)
            self.schwarz = DIAG.schwarz(reg=self)
            self.regime_lag_sep = regime_lag_sep
            self.title = "MAXIMUM LIKELIHOOD SPATIAL LAG - REGIMES" + \
                " (METHOD = " + method + ")"
            SUMMARY.ML_Lag(reg=self, w=w, vm=vm,
                           spat_diag=spat_diag, regimes=True)

    def ML_Lag_Regimes_Multi(self, y, x, w_i, w, regi_ids,
                             cores, cols2regi, method, epsilon,
                             spat_diag, vm, name_y, name_x,
                             name_regimes, name_w, name_ds):
        pool = mp.Pool(cores)
        name_x = USER.set_name_x(name_x, x) + [USER.set_name_yend_sp(name_y)]
        results_p = {}
        for r in self.regimes_set:
            if system() == 'Windows':
                is_win = True
                results_p[r] = _work(
                    *(y, x, regi_ids, r, w_i[r], method, epsilon, name_ds, name_y, name_x, name_w, name_regimes))
            else:
                results_p[r] = pool.apply_async(_work, args=(
                    y, x, regi_ids, r, w_i[r], method, epsilon, name_ds, name_y, name_x, name_w, name_regimes, ))
                is_win = False
        self.kryd = 0
        self.kr = len(cols2regi) + 1
        self.kf = 0
        self.nr = len(self.regimes_set)
        self.name_x_r = name_x
        self.name_regimes = name_regimes
        self.vm = np.zeros((self.nr * self.kr, self.nr * self.kr), float)
        self.betas = np.zeros((self.nr * self.kr, 1), float)
        self.u = np.zeros((self.n, 1), float)
        self.predy = np.zeros((self.n, 1), float)
        self.predy_e = np.zeros((self.n, 1), float)
        self.e_pred = np.zeros((self.n, 1), float)
        if not is_win:
            pool.close()
            pool.join()
        results = {}
        self.name_y, self.name_x = [], []
        counter = 0
        for r in self.regimes_set:
            if is_win:
                results[r] = results_p[r]
            else:
                results[r] = results_p[r].get()
            self.vm[(counter * self.kr):((counter + 1) * self.kr),
                    (counter * self.kr):((counter + 1) * self.kr)] = results[r].vm
            self.betas[(counter * self.kr):((counter + 1) * self.kr),
                       ] = results[r].betas
            self.u[regi_ids[r], ] = results[r].u
            self.predy[regi_ids[r], ] = results[r].predy
            self.predy_e[regi_ids[r], ] = results[r].predy_e
            self.e_pred[regi_ids[r], ] = results[r].e_pred
            self.name_y += results[r].name_y
            self.name_x += results[r].name_x
            counter += 1
        self.multi = results
        self.chow = REGI.Chow(self)
        SUMMARY.ML_Lag_multi(reg=self, multireg=self.multi,
                             vm=vm, spat_diag=spat_diag, regimes=True, w=w)


def _work(y, x, regi_ids, r, w_r, method, epsilon, name_ds, name_y, name_x, name_w, name_regimes):
    y_r = y[regi_ids[r]]
    x_r = x[regi_ids[r]]
    x_constant = USER.check_constant(x_r)
    model = BaseML_Lag(y_r, x_constant, w_r, method=method, epsilon=epsilon)
    model.title = "MAXIMUM LIKELIHOOD SPATIAL LAG - REGIME " + \
        str(r) + " (METHOD = " + method + ")"
    model.name_ds = name_ds
    model.name_y = '%s_%s' % (str(r), name_y)
    model.name_x = ['%s_%s' % (str(r), i) for i in name_x]
    model.name_w = name_w
    model.name_regimes = name_regimes
    model.aic = DIAG.akaike(reg=model)
    model.schwarz = DIAG.schwarz(reg=model)
    return model


def _test():
    import doctest
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)

if __name__ == "__main__":
    _test()
    import numpy as np
    import pysal as ps
    db = ps.open(ps.examples.get_path("baltim.dbf"), 'r')
    ds_name = "baltim.dbf"
    y_name = "PRICE"
    y = np.array(db.by_col(y_name)).T
    y.shape = (len(y), 1)
    x_names = ["NROOM", "NBATH", "PATIO", "FIREPL",
               "AC", "GAR", "AGE", "LOTSZ", "SQFT"]
    x = np.array([db.by_col(var) for var in x_names]).T
    ww = ps.open(ps.examples.get_path("baltim_q.gal"))
    w = ww.read()
    ww.close()
    w_name = "baltim_q.gal"
    w.transform = 'r'
    regimes = db.by_col("CITCOU")

    mllag = ML_Lag_Regimes(
        y, x, regimes, w=w, method='full', name_y=y_name, name_x=x_names,
        name_w=w_name, name_ds=ds_name, regime_lag_sep=True, constant_regi='many')
    print mllag.summary

########NEW FILE########
__FILENAME__ = ols
"""Ordinary Least Squares regression classes."""

__author__ = "Luc Anselin luc.anselin@asu.edu, David C. Folch david.folch@asu.edu"
import numpy as np
import copy as COPY
import numpy.linalg as la
import user_output as USER
import summary_output as SUMMARY
import robust as ROBUST
from utils import spdot, sphstack, RegressionPropsY, RegressionPropsVM

__all__ = ["OLS"]


class BaseOLS(RegressionPropsY, RegressionPropsVM):

    """
    Ordinary least squares (OLS) (note: no consistency checks, diagnostics or
    constant added)

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    robust       : string
                   If 'white', then a White consistent estimator of the
                   variance-covariance matrix is given.  If 'hac', then a
                   HAC consistent estimator of the variance-covariance
                   matrix is given. Default set to None. 
    gwk          : pysal W object
                   Kernel spatial weights needed for HAC estimation. Note:
                   matrix must have ones along the main diagonal.
    sig2n_k      : boolean
                   If True, then use n-k to estimate sigma^2. If False, use n.
               
    Attributes
    ----------
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    utu          : float
                   Sum of squared residuals
    sig2         : float
                   Sigma squared used in computations
    sig2n        : float
                   Sigma squared (computed with n in the denominator)
    sig2n_k      : float
                   Sigma squared (computed with n-k in the denominator)
    xtx          : float
                   X'X
    xtxi         : float
                   (X'X)^-1

              
    Examples
    --------

    >>> import numpy as np
    >>> import pysal
    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    >>> y = np.array(db.by_col("HOVAL"))
    >>> y = np.reshape(y, (49,1))
    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("CRIME"))
    >>> X = np.array(X).T
    >>> X = np.hstack((np.ones(y.shape),X))
    >>> ols=BaseOLS(y,X)
    >>> ols.betas
    array([[ 46.42818268],
           [  0.62898397],
           [ -0.48488854]])
    >>> ols.vm
    array([[ 174.02245348,   -6.52060364,   -2.15109867],
           [  -6.52060364,    0.28720001,    0.06809568],
           [  -2.15109867,    0.06809568,    0.03336939]])
    """

    def __init__(self, y, x, robust=None, gwk=None, sig2n_k=True):
        self.x = x
        self.xtx = spdot(self.x.T, self.x)
        xty = spdot(self.x.T, y)

        self.xtxi = la.inv(self.xtx)
        self.betas = np.dot(self.xtxi, xty)
        predy = spdot(self.x, self.betas)

        u = y - predy
        self.u = u
        self.predy = predy
        self.y = y
        self.n, self.k = self.x.shape

        if robust:
            self.vm = ROBUST.robust_vm(reg=self, gwk=gwk, sig2n_k=sig2n_k)

        self._cache = {}
        if sig2n_k:
            self.sig2 = self.sig2n_k
        else:
            self.sig2 = self.sig2n


class OLS(BaseOLS):

    """
    Ordinary least squares with results and diagnostics.
    
    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    w            : pysal W object
                   Spatial weights object (required if running spatial
                   diagnostics)
    robust       : string
                   If 'white', then a White consistent estimator of the
                   variance-covariance matrix is given.  If 'hac', then a
                   HAC consistent estimator of the variance-covariance
                   matrix is given. Default set to None. 
    gwk          : pysal W object
                   Kernel spatial weights needed for HAC estimation. Note:
                   matrix must have ones along the main diagonal.
    sig2n_k      : boolean
                   If True, then use n-k to estimate sigma^2. If False, use n.
    nonspat_diag : boolean
                   If True, then compute non-spatial diagnostics on
                   the regression.
    spat_diag    : boolean
                   If True, then compute Lagrange multiplier tests (requires
                   w). Note: see moran for further tests.
    moran        : boolean
                   If True, compute Moran's I on the residuals. Note:
                   requires spat_diag=True.
    white_test   : boolean
                   If True, compute White's specification robust test.
                   (requires nonspat_diag=True)
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_gwk     : string
                   Name of kernel weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    robust       : string
                   Adjustment for robust standard errors
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    r2           : float
                   R squared
    ar2          : float
                   Adjusted R squared
    utu          : float
                   Sum of squared residuals
    sig2         : float
                   Sigma squared used in computations
    sig2ML       : float
                   Sigma squared (maximum likelihood)
    f_stat       : tuple
                   Statistic (float), p-value (float)
    logll        : float
                   Log likelihood
    aic          : float
                   Akaike information criterion 
    schwarz      : float
                   Schwarz information criterion     
    std_err      : array
                   1xk array of standard errors of the betas    
    t_stat       : list of tuples
                   t statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
    mulColli     : float
                   Multicollinearity condition number
    jarque_bera  : dictionary
                   'jb': Jarque-Bera statistic (float); 'pvalue': p-value
                   (float); 'df': degrees of freedom (int)  
    breusch_pagan : dictionary
                    'bp': Breusch-Pagan statistic (float); 'pvalue': p-value
                    (float); 'df': degrees of freedom (int)  
    koenker_bassett : dictionary
                      'kb': Koenker-Bassett statistic (float); 'pvalue':
                      p-value (float); 'df': degrees of freedom (int)  
    white         : dictionary
                    'wh': White statistic (float); 'pvalue': p-value (float);
                    'df': degrees of freedom (int)  
    lm_error      : tuple
                    Lagrange multiplier test for spatial error model; tuple
                    contains the pair (statistic, p-value), where each is a
                    float 
    lm_lag        : tuple
                    Lagrange multiplier test for spatial lag model; tuple
                    contains the pair (statistic, p-value), where each is a
                    float 
    rlm_error     : tuple
                    Robust lagrange multiplier test for spatial error model;
                    tuple contains the pair (statistic, p-value), where each
                    is a float
    rlm_lag       : tuple
                    Robust lagrange multiplier test for spatial lag model;
                    tuple contains the pair (statistic, p-value), where each
                    is a float
    lm_sarma      : tuple
                    Lagrange multiplier test for spatial SARMA model; tuple
                    contains the pair (statistic, p-value), where each is a
                    float
    moran_res     : tuple
                    Moran's I for the residuals; tuple containing the triple
                    (Moran's I, standardized Moran's I, p-value)
    name_y        : string
                    Name of dependent variable for use in output
    name_x        : list of strings
                    Names of independent variables for use in output
    name_w        : string
                    Name of weights matrix for use in output
    name_gwk      : string
                    Name of kernel weights matrix for use in output
    name_ds       : string
                    Name of dataset for use in output
    title         : string
                    Name of the regression method used
    sig2n        : float
                   Sigma squared (computed with n in the denominator)
    sig2n_k      : float
                   Sigma squared (computed with n-k in the denominator)
    xtx          : float
                   X'X
    xtxi         : float
                   (X'X)^-1

    
    Examples
    --------
    >>> import numpy as np
    >>> import pysal

    Open data on Columbus neighborhood crime (49 areas) using pysal.open().
    This is the DBF associated with the Columbus shapefile.  Note that
    pysal.open() also reads data in CSV format; also, the actual OLS class
    requires data to be passed in as numpy arrays so the user can read their
    data in using any method.  

    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    
    Extract the HOVAL column (home values) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an nx1 numpy array.
    
    >>> hoval = db.by_col("HOVAL")
    >>> y = np.array(hoval)
    >>> y.shape = (len(hoval), 1)

    Extract CRIME (crime) and INC (income) vectors from the DBF to be used as
    independent variables in the regression.  Note that PySAL requires this to
    be an nxj numpy array, where j is the number of independent variables (not
    including a constant). pysal.spreg.OLS adds a vector of ones to the
    independent variables passed in.

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("CRIME"))
    >>> X = np.array(X).T

    The minimum parameters needed to run an ordinary least squares regression
    are the two numpy arrays containing the independent variable and dependent
    variables respectively.  To make the printed results more meaningful, the
    user can pass in explicit names for the variables used; this is optional.

    >>> ols = OLS(y, X, name_y='home value', name_x=['income','crime'], name_ds='columbus', white_test=True)

    pysal.spreg.OLS computes the regression coefficients and their standard
    errors, t-stats and p-values. It also computes a large battery of
    diagnostics on the regression. In this example we compute the white test
    which by default isn't ('white_test=True'). All of these results can be independently
    accessed as attributes of the regression object created by running
    pysal.spreg.OLS.  They can also be accessed at one time by printing the
    summary attribute of the regression object. In the example below, the
    parameter on crime is -0.4849, with a t-statistic of -2.6544 and p-value
    of 0.01087.

    >>> ols.betas
    array([[ 46.42818268],
           [  0.62898397],
           [ -0.48488854]])
    >>> print round(ols.t_stat[2][0],3)
    -2.654
    >>> print round(ols.t_stat[2][1],3)
    0.011
    >>> print round(ols.r2,3)
    0.35

    Or we can easily obtain a full summary of all the results nicely formatted and
    ready to be printed:

    >>> print ols.summary
    REGRESSION
    ----------
    SUMMARY OF OUTPUT: ORDINARY LEAST SQUARES
    -----------------------------------------
    Data set            :    columbus
    Dependent Variable  :  home value                Number of Observations:          49
    Mean dependent var  :     38.4362                Number of Variables   :           3
    S.D. dependent var  :     18.4661                Degrees of Freedom    :          46
    R-squared           :      0.3495
    Adjusted R-squared  :      0.3212
    Sum squared residual:   10647.015                F-statistic           :     12.3582
    Sigma-square        :     231.457                Prob(F-statistic)     :   5.064e-05
    S.E. of regression  :      15.214                Log likelihood        :    -201.368
    Sigma-square ML     :     217.286                Akaike info criterion :     408.735
    S.E of regression ML:     14.7406                Schwarz criterion     :     414.411
    <BLANKLINE>
    ------------------------------------------------------------------------------------
                Variable     Coefficient       Std.Error     t-Statistic     Probability
    ------------------------------------------------------------------------------------
                CONSTANT      46.4281827      13.1917570       3.5194844       0.0009867
                   crime      -0.4848885       0.1826729      -2.6544086       0.0108745
                  income       0.6289840       0.5359104       1.1736736       0.2465669
    ------------------------------------------------------------------------------------
    <BLANKLINE>
    REGRESSION DIAGNOSTICS
    MULTICOLLINEARITY CONDITION NUMBER           12.538
    <BLANKLINE>
    TEST ON NORMALITY OF ERRORS
    TEST                             DF        VALUE           PROB
    Jarque-Bera                       2          39.706           0.0000
    <BLANKLINE>
    DIAGNOSTICS FOR HETEROSKEDASTICITY
    RANDOM COEFFICIENTS
    TEST                             DF        VALUE           PROB
    Breusch-Pagan test                2           5.767           0.0559
    Koenker-Bassett test              2           2.270           0.3214
    <BLANKLINE>
    SPECIFICATION ROBUST TEST
    TEST                             DF        VALUE           PROB
    White                             5           2.906           0.7145
    ================================ END OF REPORT =====================================

    If the optional parameters w and spat_diag are passed to pysal.spreg.OLS,
    spatial diagnostics will also be computed for the regression.  These
    include Lagrange multiplier tests and Moran's I of the residuals.  The w
    parameter is a PySAL spatial weights matrix. In this example, w is built
    directly from the shapefile columbus.shp, but w can also be read in from a
    GAL or GWT file.  In this case a rook contiguity weights matrix is built,
    but PySAL also offers queen contiguity, distance weights and k nearest
    neighbor weights among others. In the example, the Moran's I of the
    residuals is 0.204 with a standardized value of 2.592 and a p-value of
    0.0095.

    >>> w = pysal.weights.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
    >>> ols = OLS(y, X, w, spat_diag=True, moran=True, name_y='home value', name_x=['income','crime'], name_ds='columbus')
    >>> ols.betas
    array([[ 46.42818268],
           [  0.62898397],
           [ -0.48488854]])
    >>> print round(ols.moran_res[0],3)
    0.204
    >>> print round(ols.moran_res[1],3)
    2.592
    >>> print round(ols.moran_res[2],4)
    0.0095

    """

    def __init__(self, y, x,
                 w=None,
                 robust=None, gwk=None, sig2n_k=True,
                 nonspat_diag=True, spat_diag=False, moran=False,
                 white_test=False, vm=False, name_y=None, name_x=None,
                 name_w=None, name_gwk=None, name_ds=None):

        n = USER.check_arrays(y, x)
        USER.check_y(y, n)
        USER.check_weights(w, y)
        USER.check_robust(robust, gwk)
        USER.check_spat_diag(spat_diag, w)
        x_constant = USER.check_constant(x)
        BaseOLS.__init__(self, y=y, x=x_constant, robust=robust,
                         gwk=gwk, sig2n_k=sig2n_k)
        self.title = "ORDINARY LEAST SQUARES"
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_y = USER.set_name_y(name_y)
        self.name_x = USER.set_name_x(name_x, x)
        self.robust = USER.set_robust(robust)
        self.name_w = USER.set_name_w(name_w, w)
        self.name_gwk = USER.set_name_w(name_gwk, gwk)
        SUMMARY.OLS(reg=self, vm=vm, w=w, nonspat_diag=nonspat_diag,
                    spat_diag=spat_diag, moran=moran, white_test=white_test)


def _test():
    import doctest
    # the following line could be used to define an alternative to the '<BLANKLINE>' flag
    #doctest.BLANKLINE_MARKER = 'something better than <BLANKLINE>'
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)

if __name__ == '__main__':
    _test()

    import numpy as np
    import pysal
    db = pysal.open(pysal.examples.get_path("columbus.dbf"), 'r')
    y_var = 'CRIME'
    y = np.array([db.by_col(y_var)]).reshape(49, 1)
    x_var = ['INC', 'HOVAL']
    x = np.array([db.by_col(name) for name in x_var]).T
    w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
    w.transform = 'r'
    ols = OLS(
        y, x, w=w, nonspat_diag=True, spat_diag=True, name_y=y_var, name_x=x_var,
        name_ds='columbus', name_w='columbus.gal', robust='white', sig2n_k=True, moran=True)
    print ols.summary

########NEW FILE########
__FILENAME__ = ols_regimes
"""
Ordinary Least Squares regression with regimes.
"""

__author__ = "Luc Anselin luc.anselin@asu.edu, Pedro V. Amaral pedro.amaral@asu.edu, Daniel Arribas-Bel darribas@asu.edu"

import regimes as REGI
import user_output as USER
import multiprocessing as mp
from ols import BaseOLS
from utils import set_warn, spbroadcast, RegressionProps_basic, RegressionPropsY, spdot
from robust import hac_multi
import summary_output as SUMMARY
import numpy as np
from platform import system
import scipy.sparse as SP


class OLS_Regimes(BaseOLS, REGI.Regimes_Frame, RegressionPropsY):

    """
    Ordinary least squares with results and diagnostics.
    
    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    w            : pysal W object
                   Spatial weights object (required if running spatial
                   diagnostics)
    robust       : string
                   If 'white', then a White consistent estimator of the
                   variance-covariance matrix is given.  If 'hac', then a
                   HAC consistent estimator of the variance-covariance
                   matrix is given. Default set to None. 
    gwk          : pysal W object
                   Kernel spatial weights needed for HAC estimation. Note:
                   matrix must have ones along the main diagonal.
    sig2n_k      : boolean
                   If True, then use n-k to estimate sigma^2. If False, use n.
    nonspat_diag : boolean
                   If True, then compute non-spatial diagnostics on
                   the regression.
    spat_diag    : boolean
                   If True, then compute Lagrange multiplier tests (requires
                   w). Note: see moran for further tests.
    moran        : boolean
                   If True, compute Moran's I on the residuals. Note:
                   requires spat_diag=True.
    white_test   : boolean
                   If True, compute White's specification robust test.
                   (requires nonspat_diag=True)
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    constant_regi: ['one', 'many']
                   Switcher controlling the constant term setup. It may take
                   the following values:
                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime (default)
    cols2regi    : list, 'all'
                   Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all' (default), all the variables vary by regime.
    regime_err_sep  : boolean
                   If True, a separate regression is run for each regime.
    cores        : integer
                   Specifies the number of cores to be used in multiprocessing
                   Default: all cores available (specified as cores=None).
                   Note: Multiprocessing currently not available on Windows.
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_gwk     : string
                   Name of kernel weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    name_regimes : string
                   Name of regime variable for use in the output
    

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    robust       : string
                   Adjustment for robust standard errors
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)                  
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    r2           : float
                   R squared
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    ar2          : float
                   Adjusted R squared
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    utu          : float
                   Sum of squared residuals
    sig2         : float
                   Sigma squared used in computations
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    sig2ML       : float
                   Sigma squared (maximum likelihood)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    f_stat       : tuple
                   Statistic (float), p-value (float)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    logll        : float
                   Log likelihood
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    aic          : float
                   Akaike information criterion 
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    schwarz      : float
                   Schwarz information criterion     
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    std_err      : array
                   1xk array of standard errors of the betas    
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    t_stat       : list of tuples
                   t statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    mulColli     : float
                   Multicollinearity condition number
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    jarque_bera  : dictionary
                   'jb': Jarque-Bera statistic (float); 'pvalue': p-value
                   (float); 'df': degrees of freedom (int)  
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    breusch_pagan : dictionary
                    'bp': Breusch-Pagan statistic (float); 'pvalue': p-value
                    (float); 'df': degrees of freedom (int)  
                    Only available in dictionary 'multi' when multiple regressions
                    (see 'multi' below for details)
    koenker_bassett : dictionary
                      'kb': Koenker-Bassett statistic (float); 'pvalue':
                      p-value (float); 'df': degrees of freedom (int)  
                      Only available in dictionary 'multi' when multiple regressions
                      (see 'multi' below for details)
    white         : dictionary
                    'wh': White statistic (float); 'pvalue': p-value (float);
                    'df': degrees of freedom (int)  
                    Only available in dictionary 'multi' when multiple regressions
                    (see 'multi' below for details)
    lm_error      : tuple
                    Lagrange multiplier test for spatial error model; tuple
                    contains the pair (statistic, p-value), where each is a
                    float 
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    lm_lag        : tuple
                    Lagrange multiplier test for spatial lag model; tuple
                    contains the pair (statistic, p-value), where each is a
                    float 
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    rlm_error     : tuple
                    Robust lagrange multiplier test for spatial error model;
                    tuple contains the pair (statistic, p-value), where each
                    is a float
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    rlm_lag       : tuple
                    Robust lagrange multiplier test for spatial lag model;
                    tuple contains the pair (statistic, p-value), where each
                    is a float
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    lm_sarma      : tuple
                    Lagrange multiplier test for spatial SARMA model; tuple
                    contains the pair (statistic, p-value), where each is a
                    float
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    moran_res     : tuple
                    Moran's I for the residuals; tuple containing the triple
                    (Moran's I, standardized Moran's I, p-value)
    name_y        : string
                    Name of dependent variable for use in output
    name_x        : list of strings
                    Names of independent variables for use in output
    name_w        : string
                    Name of weights matrix for use in output
    name_gwk      : string
                    Name of kernel weights matrix for use in output
    name_ds       : string
                    Name of dataset for use in output
    name_regimes : string
                   Name of regime variable for use in the output
    title         : string
                    Name of the regression method used
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    sig2n        : float
                   Sigma squared (computed with n in the denominator)
    sig2n_k      : float
                   Sigma squared (computed with n-k in the denominator)
    xtx          : float
                   X'X
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    xtxi         : float
                   (X'X)^-1
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    constant_regi: ['one', 'many']
                   Ignored if regimes=False. Constant option for regimes.
                   Switcher controlling the constant term setup. It may take
                   the following values:
                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime
    cols2regi    : list, 'all'
                   Ignored if regimes=False. Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all', all the variables vary by regime.
    regime_err_sep  : boolean
                   If True, a separate regression is run for each regime.
    kr           : int
                   Number of variables/columns to be "regimized" or subject
                   to change by regime. These will result in one parameter
                   estimate by regime for each variable (i.e. nr parameters per
                   variable)
    kf           : int
                   Number of variables/columns to be considered fixed or
                   global across regimes and hence only obtain one parameter
                   estimate
    nr           : int
                   Number of different regimes in the 'regimes' list
    multi        : dictionary
                   Only available when multiple regressions are estimated,
                   i.e. when regime_err_sep=True and no variable is fixed
                   across regimes.
                   Contains all attributes of each individual regression
    
    Examples
    --------
    >>> import numpy as np
    >>> import pysal

    Open data on NCOVR US County Homicides (3085 areas) using pysal.open().
    This is the DBF associated with the NAT shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> db = pysal.open(pysal.examples.get_path("NAT.dbf"),'r')
 
    Extract the HR90 column (homicide rates in 1990) from the DBF file and make it
    the dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.
    
    >>> y_var = 'HR90'
    >>> y = db.by_col(y_var)
    >>> y = np.array(y).reshape(len(y), 1)

    Extract UE90 (unemployment rate) and PS90 (population structure) vectors from
    the DBF to be used as independent variables in the regression. Other variables
    can be inserted by adding their names to x_var, such as x_var = ['Var1','Var2','...]
    Note that PySAL requires this to be an nxj numpy array, where j is the
    number of independent variables (not including a constant). By default
    this model adds a vector of ones to the independent variables passed in.

    >>> x_var = ['PS90','UE90']
    >>> x = np.array([db.by_col(name) for name in x_var]).T

    The different regimes in this data are given according to the North and 
    South dummy (SOUTH).

    >>> r_var = 'SOUTH'
    >>> regimes = db.by_col(r_var)

    We can now run the regression and then have a summary of the output
    by typing: olsr.summary
    Alternatively, we can just check the betas and standard errors of the
    parameters:

    >>> olsr = OLS_Regimes(y, x, regimes, nonspat_diag=False, name_y=y_var, name_x=['PS90','UE90'], name_regimes=r_var, name_ds='NAT')
    >>> olsr.betas
    array([[ 0.39642899],
           [ 0.65583299],
           [ 0.48703937],
           [ 5.59835   ],
           [ 1.16210453],
           [ 0.53163886]])
    >>> np.sqrt(olsr.vm.diagonal())
    array([ 0.24816345,  0.09662678,  0.03628629,  0.46894564,  0.21667395,
            0.05945651])
    >>> olsr.cols2regi
    'all'
    """

    def __init__(self, y, x, regimes,
                 w=None, robust=None, gwk=None, sig2n_k=True,
                 nonspat_diag=True, spat_diag=False, moran=False, white_test=False,
                 vm=False, constant_regi='many', cols2regi='all',
                 regime_err_sep=True, cores=None,
                 name_y=None, name_x=None, name_regimes=None,
                 name_w=None, name_gwk=None, name_ds=None):

        n = USER.check_arrays(y, x)
        USER.check_y(y, n)
        USER.check_weights(w, y)
        USER.check_robust(robust, gwk)
        USER.check_spat_diag(spat_diag, w)
        self.name_x_r = USER.set_name_x(name_x, x)
        self.constant_regi = constant_regi
        self.cols2regi = cols2regi
        self.name_w = USER.set_name_w(name_w, w)
        self.name_gwk = USER.set_name_w(name_gwk, gwk)
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_y = USER.set_name_y(name_y)
        self.name_regimes = USER.set_name_ds(name_regimes)
        self.n = n
        cols2regi = REGI.check_cols2regi(
            constant_regi, cols2regi, x, add_cons=False)
        self.regimes_set = REGI._get_regimes_set(regimes)
        self.regimes = regimes
        USER.check_regimes(self.regimes_set, self.n, x.shape[1])
        if regime_err_sep == True and robust == 'hac':
            set_warn(
                self, "Error by regimes is incompatible with HAC estimation. Hence, error by regimes has been disabled for this model.")
            regime_err_sep = False
        self.regime_err_sep = regime_err_sep
        if regime_err_sep == True and set(cols2regi) == set([True]) and constant_regi == 'many':
            self.y = y
            name_x = USER.set_name_x(name_x, x)
            regi_ids = dict((r, list(np.where(np.array(regimes) == r)[0]))
                            for r in self.regimes_set)
            self._ols_regimes_multi(x, w, regi_ids, cores,
                                    gwk, sig2n_k, robust, nonspat_diag, spat_diag, vm, name_x, moran, white_test)
        else:
            name_x = USER.set_name_x(name_x, x, constant=True)
            x, self.name_x = REGI.Regimes_Frame.__init__(self, x,
                                                         regimes, constant_regi, cols2regi, name_x)
            BaseOLS.__init__(self, y=y, x=x, robust=robust,
                             gwk=gwk, sig2n_k=sig2n_k)
            if regime_err_sep == True and robust == None:
                y2, x2 = REGI._get_weighted_var(
                    regimes, self.regimes_set, sig2n_k, self.u, y, x)
                ols2 = BaseOLS(y=y2, x=x2, sig2n_k=sig2n_k)
                RegressionProps_basic(self, betas=ols2.betas, vm=ols2.vm)
                self.title = "ORDINARY LEAST SQUARES - REGIMES (Group-wise heteroskedasticity)"
                nonspat_diag = None
                set_warn(
                    self, "Residuals treated as homoskedastic for the purpose of diagnostics.")
            else:
                self.title = "ORDINARY LEAST SQUARES - REGIMES"
            self.robust = USER.set_robust(robust)
            self.chow = REGI.Chow(self)
            SUMMARY.OLS(reg=self, vm=vm, w=w, nonspat_diag=nonspat_diag,
                        spat_diag=spat_diag, moran=moran, white_test=white_test, regimes=True)

    def _ols_regimes_multi(self, x, w, regi_ids, cores,
                           gwk, sig2n_k, robust, nonspat_diag, spat_diag, vm, name_x, moran, white_test):
        results_p = {}
        for r in self.regimes_set:
            if system() == 'Windows':
                is_win = True
                results_p[r] = _work(
                    *(self.y, x, w, regi_ids, r, robust, sig2n_k,
                      self.name_ds, self.name_y, name_x, self.name_w, self.name_regimes))
            else:
                pool = mp.Pool(cores)
                results_p[r] = pool.apply_async(_work, args=(
                    self.y, x, w, regi_ids, r, robust, sig2n_k, self.name_ds, self.name_y, name_x, self.name_w, self.name_regimes))
                is_win = False
        self.kryd = 0
        self.kr = x.shape[1] + 1
        self.kf = 0
        self.nr = len(self.regimes_set)
        self.vm = np.zeros((self.nr * self.kr, self.nr * self.kr), float)
        self.betas = np.zeros((self.nr * self.kr, 1), float)
        self.u = np.zeros((self.n, 1), float)
        self.predy = np.zeros((self.n, 1), float)
        if not is_win:
            pool.close()
            pool.join()
        results = {}
        self.name_y, self.name_x = [], []
        counter = 0
        for r in self.regimes_set:
            if is_win:
                results[r] = results_p[r]
            else:
                results[r] = results_p[r].get()
            self.vm[(counter * self.kr):((counter + 1) * self.kr),
                    (counter * self.kr):((counter + 1) * self.kr)] = results[r].vm
            self.betas[(counter * self.kr):((counter + 1) * self.kr),
                       ] = results[r].betas
            self.u[regi_ids[r], ] = results[r].u
            self.predy[regi_ids[r], ] = results[r].predy
            self.name_y += results[r].name_y
            self.name_x += results[r].name_x
            counter += 1
        self.multi = results
        self.hac_var = x
        if robust == 'hac':
            hac_multi(self, gwk)
        self.chow = REGI.Chow(self)
        if spat_diag:
            self._get_spat_diag_props(x, sig2n_k)
        SUMMARY.OLS_multi(
            reg=self, multireg=self.multi, vm=vm, nonspat_diag=nonspat_diag,
            spat_diag=spat_diag, moran=moran, white_test=white_test, regimes=True, w=w)

    def _get_spat_diag_props(self, x, sig2n_k):
        self.k = self.kr
        self._cache = {}
        x = np.hstack((np.ones((x.shape[0], 1)), x))
        self.x = REGI.regimeX_setup(
            x, self.regimes, [True] * x.shape[1], self.regimes_set)
        self.xtx = spdot(self.x.T, self.x)
        self.xtxi = np.linalg.inv(self.xtx)


def _work(y, x, w, regi_ids, r, robust, sig2n_k, name_ds, name_y, name_x, name_w, name_regimes):
    y_r = y[regi_ids[r]]
    x_r = x[regi_ids[r]]
    x_constant = USER.check_constant(x_r)
    if robust == 'hac':
        robust = None
    model = BaseOLS(y_r, x_constant, robust=robust, sig2n_k=sig2n_k)
    model.title = "ORDINARY LEAST SQUARES ESTIMATION - REGIME %s" % r
    model.robust = USER.set_robust(robust)
    model.name_ds = name_ds
    model.name_y = '%s_%s' % (str(r), name_y)
    model.name_x = ['%s_%s' % (str(r), i) for i in name_x]
    model.name_w = name_w
    model.name_regimes = name_regimes
    if w:
        w_r, warn = REGI.w_regime(w, regi_ids[r], r, transform=True)
        set_warn(model, warn)
        model.w = w_r
    return model


def _test():
    import doctest
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)

if __name__ == '__main__':
    _test()
    import numpy as np
    import pysal
    db = pysal.open(pysal.examples.get_path('columbus.dbf'), 'r')
    y_var = 'CRIME'
    y = np.array([db.by_col(y_var)]).reshape(49, 1)
    x_var = ['INC', 'HOVAL']
    x = np.array([db.by_col(name) for name in x_var]).T
    r_var = 'NSA'
    regimes = db.by_col(r_var)
    w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
    w.transform = 'r'
    olsr = OLS_Regimes(
        y, x, regimes, w=w, constant_regi='many', nonspat_diag=False, spat_diag=False, name_y=y_var, name_x=['INC', 'HOVAL'],
        name_ds='columbus', name_regimes=r_var, name_w='columbus.gal', regime_err_sep=True, cols2regi=[True, True], sig2n_k=True, robust='white')
    print olsr.summary

########NEW FILE########
__FILENAME__ = probit
"""Probit regression class and diagnostics."""

__author__ = "Luc Anselin luc.anselin@asu.edu, Pedro V. Amaral pedro.amaral@asu.edu"

import numpy as np
import numpy.linalg as la
import scipy.optimize as op
from scipy.stats import norm, chisqprob
import scipy.sparse as SP
import user_output as USER
import summary_output as SUMMARY

__all__ = ["Probit"]


class BaseProbit:

    """
    Probit class to do all the computations

    Parameters
    ----------

    x           : array
                  nxk array of independent variables (assumed to be aligned with y)
    y           : array
                  nx1 array of dependent binary variable
    w           : W
                  PySAL weights instance aligned with y
    optim       : string
                  Optimization method.
                  Default: 'newton' (Newton-Raphson).
                  Alternatives: 'ncg' (Newton-CG), 'bfgs' (BFGS algorithm)
    scalem      : string
                  Method to calculate the scale of the marginal effects.
                  Default: 'phimean' (Mean of individual marginal effects)
                  Alternative: 'xmean' (Marginal effects at variables mean)
    maxiter     : int
                  Maximum number of iterations until optimizer stops                  
              
    Attributes
    ----------

    x           : array
                  Two dimensional array with n rows and one column for each
                  independent (exogenous) variable, including the constant
    y           : array
                  nx1 array of dependent variable
    betas       : array
                  kx1 array with estimated coefficients
    predy       : array
                  nx1 array of predicted y values
    n           : int
                  Number of observations
    k           : int
                  Number of variables
    vm          : array
                  Variance-covariance matrix (kxk)
    z_stat      : list of tuples
                  z statistic; each tuple contains the pair (statistic,
                  p-value), where each is a float                  
    xmean       : array
                  Mean of the independent variables (kx1)
    predpc      : float
                  Percent of y correctly predicted
    logl        : float
                  Log-Likelihhod of the estimation
    scalem      : string
                  Method to calculate the scale of the marginal effects.
    scale       : float
                  Scale of the marginal effects.
    slopes      : array
                  Marginal effects of the independent variables (k-1x1)
                  Note: Disregards the presence of dummies.
    slopes_vm   : array
                  Variance-covariance matrix of the slopes (k-1xk-1)
    LR          : tuple
                  Likelihood Ratio test of all coefficients = 0
                  (test statistics, p-value)
    Pinkse_error: float
                  Lagrange Multiplier test against spatial error correlation.
                  Implemented as presented in Pinkse (2004)              
    KP_error    : float
                  Moran's I type test against spatial error correlation.
                  Implemented as presented in Kelejian and Prucha (2001)
    PS_error    : float
                  Lagrange Multiplier test against spatial error correlation.
                  Implemented as presented in Pinkse and Slade (1998)
    warning     : boolean
                  if True Maximum number of iterations exceeded or gradient 
                  and/or function calls not changing.

    References
    ----------
    .. [1] Pinkse, J. (2004). Moran-flavored tests with nuisance parameter. In: Anselin,
    L., Florax, R. J., Rey, S. J. (editors) Advances in Spatial Econometrics,
    pages 67-77. Springer-Verlag, Heidelberg.
    .. [2] Kelejian, H., Prucha, I. (2001) "On the asymptotic distribution of the
    Moran I test statistic with applications". Journal of Econometrics, 104(2):219-57.
    .. [3] Pinkse, J., Slade, M. E. (1998) "Contracting in space: an application of
    spatial statistics to discrete-choice models". Journal of Econometrics, 85(1):125-54.

    Examples
    --------
    >>> import numpy as np
    >>> import pysal
    >>> dbf = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    >>> y = np.array([dbf.by_col('CRIME')]).T
    >>> x = np.array([dbf.by_col('INC'), dbf.by_col('HOVAL')]).T
    >>> x = np.hstack((np.ones(y.shape),x))
    >>> w = pysal.open(pysal.examples.get_path("columbus.gal"), 'r').read()
    >>> w.transform='r'
    >>> model = BaseProbit((y>40).astype(float), x, w=w)    
    >>> np.around(model.betas, decimals=6)
    array([[ 3.353811],
           [-0.199653],
           [-0.029514]])
           
    >>> np.around(model.vm, decimals=6)
    array([[ 0.852814, -0.043627, -0.008052],
           [-0.043627,  0.004114, -0.000193],
           [-0.008052, -0.000193,  0.00031 ]])

    >>> tests = np.array([['Pinkse_error','KP_error','PS_error']])
    >>> stats = np.array([[model.Pinkse_error[0],model.KP_error[0],model.PS_error[0]]])
    >>> pvalue = np.array([[model.Pinkse_error[1],model.KP_error[1],model.PS_error[1]]])
    >>> print np.hstack((tests.T,np.around(np.hstack((stats.T,pvalue.T)),6)))
    [['Pinkse_error' '3.131719' '0.076783']
     ['KP_error' '1.721312' '0.085194']
     ['PS_error' '2.558166' '0.109726']]
    """

    def __init__(self, y, x, w=None, optim='newton', scalem='phimean', maxiter=100):
        self.y = y
        self.x = x
        self.n, self.k = x.shape
        self.optim = optim
        self.scalem = scalem
        self.w = w
        self.maxiter = maxiter
        par_est, self.warning = self.par_est()
        self.betas = np.reshape(par_est[0], (self.k, 1))
        self.logl = -float(par_est[1])
        self._cache = {}

    @property
    def vm(self):
        if 'vm' not in self._cache:
            H = self.hessian(self.betas)
            self._cache['vm'] = -la.inv(H)
        return self._cache['vm']

    @property
    def z_stat(self):
        if 'z_stat' not in self._cache:
            variance = self.vm.diagonal()
            zStat = self.betas.reshape(len(self.betas),) / np.sqrt(variance)
            rs = {}
            for i in range(len(self.betas)):
                rs[i] = (zStat[i], norm.sf(abs(zStat[i])) * 2)
            self._cache['z_stat'] = rs.values()
        return self._cache['z_stat']

    @property
    def slopes_std_err(self):
        if 'slopes_std_err' not in self._cache:
            variance = self.slopes_vm.diagonal()
            self._cache['slopes_std_err'] = np.sqrt(variance)
        return self._cache['slopes_std_err']

    @property
    def slopes_z_stat(self):
        if 'slopes_z_stat' not in self._cache:
            zStat = self.slopes.reshape(
                len(self.slopes),) / self.slopes_std_err
            rs = {}
            for i in range(len(self.slopes)):
                rs[i] = (zStat[i], norm.sf(abs(zStat[i])) * 2)
            self._cache['slopes_z_stat'] = rs.values()
        return self._cache['slopes_z_stat']

    @property
    def xmean(self):
        if 'xmean' not in self._cache:
            self._cache['xmean'] = np.reshape(
                sum(self.x) / self.n, (self.k, 1))
        return self._cache['xmean']

    @property
    def xb(self):
        if 'xb' not in self._cache:
            self._cache['xb'] = np.dot(self.x, self.betas)
        return self._cache['xb']

    @property
    def predy(self):
        if 'predy' not in self._cache:
            self._cache['predy'] = norm.cdf(self.xb)
        return self._cache['predy']

    @property
    def predpc(self):
        if 'predpc' not in self._cache:
            predpc = abs(self.y - self.predy)
            for i in range(len(predpc)):
                if predpc[i] > 0.5:
                    predpc[i] = 0
                else:
                    predpc[i] = 1
            self._cache['predpc'] = float(100 * np.sum(predpc) / self.n)
        return self._cache['predpc']

    @property
    def phiy(self):
        if 'phiy' not in self._cache:
            self._cache['phiy'] = norm.pdf(self.xb)
        return self._cache['phiy']

    @property
    def scale(self):
        if 'scale' not in self._cache:
            if self.scalem == 'phimean':
                self._cache['scale'] = float(1.0 * np.sum(self.phiy) / self.n)
            if self.scalem == 'xmean':
                self._cache['scale'] = float(
                    norm.pdf(np.dot(self.xmean.T, self.betas)))
        return self._cache['scale']

    @property
    def slopes(self):
        if 'slopes' not in self._cache:
            # Disregard the presence of dummies.
            self._cache['slopes'] = self.betas[1:] * self.scale
        return self._cache['slopes']

    @property
    def slopes_vm(self):
        if 'slopes_vm' not in self._cache:
            x = self.xmean
            b = self.betas
            dfdb = np.eye(self.k) - np.dot(b.T, x) * np.dot(b, x.T)
            slopes_vm = (self.scale ** 2) * \
                np.dot(np.dot(dfdb, self.vm), dfdb.T)
            self._cache['slopes_vm'] = slopes_vm[1:, 1:]
        return self._cache['slopes_vm']

    @property
    def LR(self):
        if 'LR' not in self._cache:
            P = 1.0 * np.sum(self.y) / self.n
            LR = float(
                -2 * (self.n * (P * np.log(P) + (1 - P) * np.log(1 - P)) - self.logl))
            self._cache['LR'] = (LR, chisqprob(LR, self.k))
        return self._cache['LR']

    @property
    def u_naive(self):
        if 'u_naive' not in self._cache:
            u_naive = self.y - self.predy
            self._cache['u_naive'] = u_naive
        return self._cache['u_naive']

    @property
    def u_gen(self):
        if 'u_gen' not in self._cache:
            Phi_prod = self.predy * (1 - self.predy)
            u_gen = self.phiy * (self.u_naive / Phi_prod)
            self._cache['u_gen'] = u_gen
        return self._cache['u_gen']

    @property
    def Pinkse_error(self):
        if 'Pinkse_error' not in self._cache:
            self._cache['Pinkse_error'], self._cache[
                'KP_error'], self._cache['PS_error'] = sp_tests(self)
        return self._cache['Pinkse_error']

    @property
    def KP_error(self):
        if 'KP_error' not in self._cache:
            self._cache['Pinkse_error'], self._cache[
                'KP_error'], self._cache['PS_error'] = sp_tests(self)
        return self._cache['KP_error']

    @property
    def PS_error(self):
        if 'PS_error' not in self._cache:
            self._cache['Pinkse_error'], self._cache[
                'KP_error'], self._cache['PS_error'] = sp_tests(self)
        return self._cache['PS_error']

    def par_est(self):
        start = np.dot(la.inv(np.dot(self.x.T, self.x)),
                       np.dot(self.x.T, self.y))
        flogl = lambda par: -self.ll(par)
        if self.optim == 'newton':
            fgrad = lambda par: self.gradient(par)
            fhess = lambda par: self.hessian(par)
            par_hat = newton(flogl, start, fgrad, fhess, self.maxiter)
            warn = par_hat[2]
        else:
            fgrad = lambda par: -self.gradient(par)
            if self.optim == 'bfgs':
                par_hat = op.fmin_bfgs(
                    flogl, start, fgrad, full_output=1, disp=0)
                warn = par_hat[6]
            if self.optim == 'ncg':
                fhess = lambda par: -self.hessian(par)
                par_hat = op.fmin_ncg(
                    flogl, start, fgrad, fhess=fhess, full_output=1, disp=0)
                warn = par_hat[5]
        if warn > 0:
            warn = True
        else:
            warn = False
        return par_hat, warn

    def ll(self, par):
        beta = np.reshape(np.array(par), (self.k, 1))
        q = 2 * self.y - 1
        qxb = q * np.dot(self.x, beta)
        ll = sum(np.log(norm.cdf(qxb)))
        return ll

    def gradient(self, par):
        beta = np.reshape(np.array(par), (self.k, 1))
        q = 2 * self.y - 1
        qxb = q * np.dot(self.x, beta)
        lamb = q * norm.pdf(qxb) / norm.cdf(qxb)
        gradient = np.dot(lamb.T, self.x)[0]
        return gradient

    def hessian(self, par):
        beta = np.reshape(np.array(par), (self.k, 1))
        q = 2 * self.y - 1
        xb = np.dot(self.x, beta)
        qxb = q * xb
        lamb = q * norm.pdf(qxb) / norm.cdf(qxb)
        hessian = np.dot((self.x.T), (-lamb * (lamb + xb) * self.x))
        return hessian


class Probit(BaseProbit):

    """
    Classic non-spatial Probit and spatial diagnostics. The class includes a
    printout that formats all the results and tests in a nice format.

    The diagnostics for spatial dependence currently implemented are:

        * Pinkse Error [1]_
        * Kelejian and Prucha Moran's I [2]_
        * Pinkse & Slade Error [3]_

    Parameters
    ----------

    x           : array
                  nxk array of independent variables (assumed to be aligned with y)
    y           : array
                  nx1 array of dependent binary variable
    w           : W
                  PySAL weights instance aligned with y
    optim       : string
                  Optimization method.
                  Default: 'newton' (Newton-Raphson).
                  Alternatives: 'ncg' (Newton-CG), 'bfgs' (BFGS algorithm)
    scalem      : string
                  Method to calculate the scale of the marginal effects.
                  Default: 'phimean' (Mean of individual marginal effects)
                  Alternative: 'xmean' (Marginal effects at variables mean)
    maxiter     : int
                  Maximum number of iterations until optimizer stops                  
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
                   
    Attributes
    ----------

    x           : array
                  Two dimensional array with n rows and one column for each
                  independent (exogenous) variable, including the constant
    y           : array
                  nx1 array of dependent variable
    betas       : array
                  kx1 array with estimated coefficients
    predy       : array
                  nx1 array of predicted y values
    n           : int
                  Number of observations
    k           : int
                  Number of variables
    vm          : array
                  Variance-covariance matrix (kxk)
    z_stat      : list of tuples
                  z statistic; each tuple contains the pair (statistic,
                  p-value), where each is a float                  
    xmean       : array
                  Mean of the independent variables (kx1)
    predpc      : float
                  Percent of y correctly predicted
    logl        : float
                  Log-Likelihhod of the estimation
    scalem      : string
                  Method to calculate the scale of the marginal effects.
    scale       : float
                  Scale of the marginal effects.
    slopes      : array
                  Marginal effects of the independent variables (k-1x1)
    slopes_vm   : array
                  Variance-covariance matrix of the slopes (k-1xk-1)
    LR          : tuple
                  Likelihood Ratio test of all coefficients = 0
                  (test statistics, p-value)
    Pinkse_error: float
                  Lagrange Multiplier test against spatial error correlation.
                  Implemented as presented in Pinkse (2004)              
    KP_error    : float
                  Moran's I type test against spatial error correlation.
                  Implemented as presented in Kelejian and Prucha (2001)
    PS_error    : float
                  Lagrange Multiplier test against spatial error correlation.
                  Implemented as presented in Pinkse and Slade (1998)
    warning     : boolean
                  if True Maximum number of iterations exceeded or gradient 
                  and/or function calls not changing.
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    title        : string
                   Name of the regression method used
                   
    References
    ----------
    .. [1] Pinkse, J. (2004). Moran-flavored tests with nuisance parameter. In: Anselin, L., Florax, R. J., Rey, S. J. (editors) Advances in Spatial Econometrics, pages 67-77. Springer-Verlag, Heidelberg.
    .. [2] Kelejian, H., Prucha, I. (2001) "On the asymptotic distribution of the Moran I test statistic with applications". Journal of Econometrics, 104(2):219-57.
    .. [3] Pinkse, J., Slade, M. E. (1998) "Contracting in space: an application of spatial statistics to discrete-choice models". Journal of Econometrics, 85(1):125-54.

    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.
    
    >>> import numpy as np
    >>> import pysal

    Open data on Columbus neighborhood crime (49 areas) using pysal.open().
    This is the DBF associated with the Columbus shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> dbf = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    
    Extract the CRIME column (crime) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept. Since we want to run a probit model and for this
    example we use the Columbus data, we also need to transform the continuous
    CRIME variable into a binary variable. As in McMillen, D. (1992) "Probit with
    spatial autocorrelation". Journal of Regional Science 32(3):335-48, we define
    y = 1 if CRIME > 40.

    >>> y = np.array([dbf.by_col('CRIME')]).T
    >>> y = (y>40).astype(float)

    Extract HOVAL (home values) and INC (income) vectors from the DBF to be used as
    independent variables in the regression.  Note that PySAL requires this to
    be an nxj numpy array, where j is the number of independent variables (not
    including a constant). By default this class adds a vector of ones to the
    independent variables passed in.

    >>> names_to_extract = ['INC', 'HOVAL']
    >>> x = np.array([dbf.by_col(name) for name in names_to_extract]).T

    Since we want to the test the probit model for spatial dependence, we need to
    specify the spatial weights matrix that includes the spatial configuration of
    the observations into the error component of the model. To do that, we can open
    an already existing gal file or create a new one. In this case, we will use
    ``columbus.gal``, which contains contiguity relationships between the
    observations in the Columbus dataset we are using throughout this example.
    Note that, in order to read the file, not only to open it, we need to
    append '.read()' at the end of the command.

    >>> w = pysal.open(pysal.examples.get_path("columbus.gal"), 'r').read() 
    
    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. In PySAL, this
    can be easily performed in the following way:

    >>> w.transform='r'

    We are all set with the preliminaries, we are good to run the model. In this
    case, we will need the variables and the weights matrix. If we want to
    have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional. 

    >>> model = Probit(y, x, w=w, name_y='crime', name_x=['income','home value'], name_ds='columbus', name_w='columbus.gal')
    
    Once we have run the model, we can explore a little bit the output. The
    regression object we have created has many attributes so take your time to
    discover them.
    
    >>> np.around(model.betas, decimals=6)
    array([[ 3.353811],
           [-0.199653],
           [-0.029514]])
           
    >>> np.around(model.vm, decimals=6)
    array([[ 0.852814, -0.043627, -0.008052],
           [-0.043627,  0.004114, -0.000193],
           [-0.008052, -0.000193,  0.00031 ]])

    Since we have provided a spatial weigths matrix, the diagnostics for
    spatial dependence have also been computed. We can access them and their
    p-values individually:

    >>> tests = np.array([['Pinkse_error','KP_error','PS_error']])
    >>> stats = np.array([[model.Pinkse_error[0],model.KP_error[0],model.PS_error[0]]])
    >>> pvalue = np.array([[model.Pinkse_error[1],model.KP_error[1],model.PS_error[1]]])
    >>> print np.hstack((tests.T,np.around(np.hstack((stats.T,pvalue.T)),6)))
    [['Pinkse_error' '3.131719' '0.076783']
     ['KP_error' '1.721312' '0.085194']
     ['PS_error' '2.558166' '0.109726']]

    Or we can easily obtain a full summary of all the results nicely formatted and
    ready to be printed simply by typing 'print model.summary'

    """

    def __init__(
        self, y, x, w=None, optim='newton', scalem='phimean', maxiter=100,
        vm=False, name_y=None, name_x=None, name_w=None, name_ds=None,
            spat_diag=False):

        n = USER.check_arrays(y, x)
        USER.check_y(y, n)
        if w:
            USER.check_weights(w, y)
            spat_diag = True
        x_constant = USER.check_constant(x)
        BaseProbit.__init__(self, y=y, x=x_constant, w=w,
                            optim=optim, scalem=scalem, maxiter=maxiter)
        self.title = "CLASSIC PROBIT ESTIMATOR"
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_y = USER.set_name_y(name_y)
        self.name_x = USER.set_name_x(name_x, x)
        self.name_w = USER.set_name_w(name_w, w)
        SUMMARY.Probit(reg=self, w=w, vm=vm, spat_diag=spat_diag)


def newton(flogl, start, fgrad, fhess, maxiter):
    """
    Calculates the Newton-Raphson method

    Parameters
    ----------

    flogl       : lambda
                  Function to calculate the log-likelihood
    start       : array
                  kx1 array of starting values
    fgrad       : lambda
                  Function to calculate the gradient
    fhess       : lambda
                  Function to calculate the hessian
    maxiter     : int
                  Maximum number of iterations until optimizer stops                
    """
    warn = 0
    iteration = 0
    par_hat0 = start
    m = 1
    while (iteration < maxiter and m >= 1e-04):
        H = -la.inv(fhess(par_hat0))
        g = fgrad(par_hat0).reshape(start.shape)
        Hg = np.dot(H, g)
        par_hat0 = par_hat0 + Hg
        iteration += 1
        m = np.dot(g.T, Hg)
    if iteration == maxiter:
        warn = 1
    logl = flogl(par_hat0)
    return (par_hat0, logl, warn)


def sp_tests(reg):
    """
    Calculates tests for spatial dependence in Probit models

    Parameters
    ----------

    reg         : regression object
                  output instance from a probit model            
    """
    if reg.w:
        w = reg.w.sparse
        Phi = reg.predy
        phi = reg.phiy
        # Pinkse_error:
        Phi_prod = Phi * (1 - Phi)
        u_naive = reg.u_naive
        u_gen = reg.u_gen
        sig2 = np.sum((phi * phi) / Phi_prod) / reg.n
        LM_err_num = np.dot(u_gen.T, (w * u_gen)) ** 2
        trWW = np.sum((w * w).diagonal())
        trWWWWp = trWW + np.sum((w * w.T).diagonal())
        LM_err = float(1.0 * LM_err_num / (sig2 ** 2 * trWWWWp))
        LM_err = np.array([LM_err, chisqprob(LM_err, 1)])
        # KP_error:
        moran = moran_KP(reg.w, u_naive, Phi_prod)
        # Pinkse-Slade_error:
        u_std = u_naive / np.sqrt(Phi_prod)
        ps_num = np.dot(u_std.T, (w * u_std)) ** 2
        trWpW = np.sum((w.T * w).diagonal())
        ps = float(ps_num / (trWW + trWpW))
        # chi-square instead of bootstrap.
        ps = np.array([ps, chisqprob(ps, 1)])
    else:
        raise Exception, "W matrix not provided to calculate spatial test."
    return LM_err, moran, ps


def moran_KP(w, u, sig2i):
    """
    Calculates Moran-flavoured tests 

    Parameters
    ----------

    w           : W
                  PySAL weights instance aligned with y
    u           : array
                  nx1 array of naive residuals
    sig2i       : array
                  nx1 array of individual variance               
    """
    w = w.sparse
    moran_num = np.dot(u.T, (w * u))
    E = SP.lil_matrix(w.get_shape())
    E.setdiag(sig2i.flat)
    E = E.asformat('csr')
    WE = w * E
    moran_den = np.sqrt(np.sum((WE * WE + (w.T * E) * WE).diagonal()))
    moran = float(1.0 * moran_num / moran_den)
    moran = np.array([moran, norm.sf(abs(moran)) * 2.])
    return moran


def _test():
    import doctest
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)

if __name__ == '__main__':
    _test()
    import numpy as np
    import pysal
    dbf = pysal.open(pysal.examples.get_path('columbus.dbf'), 'r')
    y = np.array([dbf.by_col('CRIME')]).T
    var_x = ['INC', 'HOVAL']
    x = np.array([dbf.by_col(name) for name in var_x]).T
    w = pysal.open(pysal.examples.get_path("columbus.gal"), 'r').read()
    w.transform = 'r'
    probit1 = Probit(
        (y > 40).astype(float), x, w=w, name_x=var_x, name_y="CRIME",
        name_ds="Columbus", name_w="columbus.dbf")
    # print probit1.summary

########NEW FILE########
__FILENAME__ = regimes
import numpy as np
import pysal
import scipy.sparse as SP
import itertools as iter
from scipy.stats import f, chisqprob
import numpy.linalg as la
from utils import spbroadcast

"""
Tools for different regimes procedure estimations
"""

__author__ = "Luc Anselin luc.anselin@asu.edu, \
        Daniel Arribas-Bel darribas@asu.edu, \
        Pedro V. Amaral pedro.amaral@asu.edu"


class Chow:

    '''
    Chow test of coefficient stability across regimes. The test is a
    particular case of the Wald statistic in which the constraint are setup
    according to the spatial or other type of regime structure
    ...

    Parameters
    ==========
    reg     : regression object
              Regression object from PySAL.spreg which is assumed to have the
              following attributes:
                    
                    * betas : coefficient estimates
                    * vm    : variance covariance matrix of betas
                    * kr    : Number of variables varying across regimes
                    * kryd  : Number of endogenous variables varying across regimes
                    * kf    : Number of variables fixed (global) across regimes
                    * nr    : Number of regimes

    Attributes
    ==========
    joint   : tuple
              Pair of Wald statistic and p-value for the setup of global
              regime stability, that is all betas are the same across
              regimes.
    regi    : array
              kr x 2 array with Wald statistic (col 0) and its p-value (col 1)
              for each beta that varies across regimes. The restrictions
              are setup to test for the global stability (all regimes have the
              same parameter) of the beta.

    Examples
    ========
    >>> import numpy as np
    >>> import pysal
    >>> from ols_regimes import OLS_Regimes
    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    >>> y_var = 'CRIME'
    >>> y = np.array([db.by_col(y_var)]).reshape(49,1)
    >>> x_var = ['INC','HOVAL']
    >>> x = np.array([db.by_col(name) for name in x_var]).T
    >>> r_var = 'NSA'
    >>> regimes = db.by_col(r_var)
    >>> olsr = OLS_Regimes(y, x, regimes, constant_regi='many', nonspat_diag=False, spat_diag=False, name_y=y_var, name_x=x_var, name_ds='columbus', name_regimes=r_var, regime_err_sep=False)
    >>> print olsr.name_x_r #x_var
    ['CONSTANT', 'INC', 'HOVAL']
    >>> print olsr.chow.regi
    [[ 0.01020844  0.91952121]
     [ 0.46024939  0.49750745]
     [ 0.55477371  0.45637369]]
    >>> print 'Joint test:'
    Joint test:
    >>> print olsr.chow.joint
    (0.6339319928978806, 0.8886223520178802)
    '''

    def __init__(self, reg):
        kr, kf, kryd, nr, betas, vm = reg.kr, reg.kf, reg.kryd, reg.nr, reg.betas, reg.vm
        if betas.shape[0] != vm.shape[0]:
            if kf > 0:
                betas = betas[0:vm.shape[0], :]
                kf = kf - 1
            else:
                brange = []
                for i in range(nr):
                    brange.extend(range(i * (kr + 1), i * (kr + 1) + kr))
                betas = betas[brange, :]
        r_global = []
        regi = np.zeros((reg.kr, 2))
        for vari in np.arange(kr):
            r_vari = buildR1var(vari, kr, kf, kryd, nr)
            r_global.append(r_vari)
            q = np.zeros((r_vari.shape[0], 1))
            regi[vari, :] = wald_test(betas, r_vari, q, vm)
        r_global = np.vstack(tuple(r_global))
        q = np.zeros((r_global.shape[0], 1))
        joint = wald_test(betas, r_global, q, vm)
        self.joint = joint
        self.regi = regi


class Wald:

    '''
    Chi sq. Wald statistic to test for restriction of coefficients.
    Implementation following Greene [1]_ eq. (17-24), p. 488
    ...

    Parameters
    ==========
    reg     : regression object
              Regression object from PySAL.spreg
    r       : array
              Array of dimension Rxk (R being number of restrictions) with constrain setup.
    q       : array
              Rx1 array with constants in the constraint setup. See Greene
              [1]_ for reference.

    Attributes
    ==========
    w       : float
              Wald statistic
    pvalue  : float
              P value for Wald statistic calculated as a Chi sq. distribution
              with R degrees of freedom

    References
    ==========
    .. [1] W. Greene. 2003. Econometric Analysis (Fifth Edtion). Prentice Hall, Upper
       Saddle River. 
    '''

    def __init__(self, reg, r, q=None):
        if not q:
            q = np.zeros((r.shape[0], 1))
        self.w, self.pvalue = wald_test(reg.betas, r, q, reg.vm)


class Regimes_Frame:

    '''
    Setup framework to work with regimes. Basically it involves:
        * Dealing with the constant in a regimes world
        * Creating a sparse representation of X 
        * Generating a list of names of X taking into account regimes
    ...

    Parameters
    ==========
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    constant_regi: [False, 'one', 'many']
                   Switcher controlling the constant term setup. It may take
                   the following values:
                    
                     *  False: no constant term is appended in any way
                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime (default)
    cols2regi    : list, 'all'
                   Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all' (default), all the variables vary by regime.
    names         : None, list of strings
                   Names of independent variables for use in output

    Returns
    =======
    x            : csr sparse matrix
                   Sparse matrix containing X variables properly aligned for
                   regimes regression. 'xsp' is of dimension (n, k*r) where 'r'
                   is the number of different regimes
                   The structure of the alignent is X1r1 X2r1 ... X1r2 X2r2 ...
    names        : None, list of strings
                   Names of independent variables for use in output
                   conveniently arranged by regimes. The structure of the name
                   is "regimeName_-_varName"
    kr           : int
                   Number of variables/columns to be "regimized" or subject
                   to change by regime. These will result in one parameter
                   estimate by regime for each variable (i.e. nr parameters per
                   variable)
    kf           : int
                   Number of variables/columns to be considered fixed or
                   global across regimes and hence only obtain one parameter
                   estimate
    nr           : int
                   Number of different regimes in the 'regimes' list

    '''

    def __init__(self, x, regimes, constant_regi, cols2regi, names=None, yend=False):
        if cols2regi == 'all':
            cols2regi = [True] * x.shape[1]
        else:
            if yend:
                cols2regi = cols2regi[-x.shape[1]:]
            else:
                cols2regi = cols2regi[0:x.shape[1]]
        if constant_regi:
            x = np.hstack((np.ones((x.shape[0], 1)), x))
            if constant_regi == 'one':
                cols2regi.insert(0, False)
            elif constant_regi == 'many':
                cols2regi.insert(0, True)
            else:
                raise Exception, "Invalid argument (%s) passed for 'constant_regi'. Please secify a valid term." % str(
                    constant)
        try:
            x = regimeX_setup(x, regimes, cols2regi,
                              self.regimes_set, constant=constant_regi)
        except AttributeError:
            self.regimes_set = _get_regimes_set(regimes)
            x = regimeX_setup(x, regimes, cols2regi,
                              self.regimes_set, constant=constant_regi)

        kr = len(np.where(np.array(cols2regi) == True)[0])
        if yend:
            self.kr += kr
            self.kf += len(cols2regi) - kr
            self.kryd = kr
        else:
            self.kr = kr
            self.kf = len(cols2regi) - self.kr
            self.kryd = 0
        self.nr = len(set(regimes))

        if names:
            names = set_name_x_regimes(
                names, regimes, constant_regi, cols2regi, self.regimes_set)

        return (x, names)


def wald_test(betas, r, q, vm):
    '''
    Chi sq. Wald statistic to test for restriction of coefficients.
    Implementation following Greene [1]_ eq. (17-24), p. 488
    ...

    Parameters
    ==========
    betas   : array
              kx1 array with coefficient estimates
    r       : array
              Array of dimension Rxk (R being number of restrictions) with constrain setup.
    q       : array
              Rx1 array with constants in the constraint setup. See Greene
              [1]_ for reference.
    vm      : array
              kxk variance-covariance matrix of coefficient estimates

    Returns
    =======
    w       : float
              Wald statistic
    pvalue  : float
              P value for Wald statistic calculated as a Chi sq. distribution
              with R degrees of freedom

    References
    ==========
    .. [1] W. Greene. 2003. Econometric Analysis (Fifth Edtion). Prentice Hall, Upper
       Saddle River. 
    '''
    rbq = np.dot(r, betas) - q
    rvri = la.inv(np.dot(r, np.dot(vm, r.T)))
    w = np.dot(rbq.T, np.dot(rvri, rbq))[0][0]
    df = r.shape[0]
    pvalue = chisqprob(w, df)
    return w, pvalue


def buildR(kr, kf, nr):
    '''
    Build R matrix to globally test for spatial heterogeneity across regimes.
    The constraint setup reflects the null every beta is the same
    across regimes
    ...

    Parameters
    ==========
    kr      : int
              Number of variables that vary across regimes ("regimized")
    kf      : int
              Number of variables that do not vary across regimes ("fixed" or
              global)
    nr      : int
              Number of regimes

    Returns
    =======
    R       : array
              Array with constrain setup to test stability across regimes of
              one variable
    '''
    return np.vstack(tuple(map(buildR1var, np.arange(kr), [kr] * kr, [kf] * kr, [nr] * kr)))


def buildR1var(vari, kr, kf, kryd, nr):
    '''
    Build R matrix to test for spatial heterogeneity across regimes in one
    variable. The constraint setup reflects the null betas for variable 'vari'
    are the same across regimes
    ...

    Parameters
    ==========
    vari    : int
              Position of the variable to be tested (order in the sequence of
              variables per regime)
    kr      : int
              Number of variables that vary across regimes ("regimized")
    kf      : int
              Number of variables that do not vary across regimes ("fixed" or
              global)
    nr      : int
              Number of regimes

    Returns
    =======
    R       : array
              Array with constrain setup to test stability across regimes of
              one variable
    '''
    ncols = (kr * nr)
    nrows = nr - 1
    r = np.zeros((nrows, ncols), dtype=int)
    rbeg = 0
    krexog = kr - kryd
    if vari < krexog:
        kr_j = krexog
        cbeg = vari
    else:
        kr_j = kryd
        cbeg = krexog * (nr - 1) + vari
    r[rbeg: rbeg + nrows, cbeg] = 1
    for j in np.arange(nrows):
        r[rbeg + j, kr_j + cbeg + j * kr_j] = -1
    return np.hstack((r, np.zeros((nrows, kf), dtype=int)))


def regimeX_setup(x, regimes, cols2regi, regimes_set, constant=False):
    '''
    Flexible full setup of a regime structure

    NOTE: constant term, if desired in the model, should be included in the x
    already
    ...

    Parameters
    ==========
    x           : np.array
                  Dense array of dimension (n, k) with values for all observations
                  IMPORTANT: constant term (if desired in the model) should be
                  included
    regimes     : list
                  list of n values with the mapping of each observation to a
                  regime. Assumed to be aligned with 'x'.
    cols2regi   : list
                  List of k booleans indicating whether each column should be
                  considered as different per regime (True) or held constant
                  across regimes (False)
    regimes_set : list
                  List of ordered regimes tags
    constant    : [False, 'one', 'many']
                  Switcher controlling the constant term setup. It may take
                  the following values:
                    
                    *  False: no constant term is appended in any way
                    *  'one': a vector of ones is appended to x and held
                              constant across regimes
                    * 'many': a vector of ones is appended to x and considered
                              different per regime

    Returns
    =======
    xsp         : csr sparse matrix
                  Sparse matrix containing the full setup for a regimes model
                  as specified in the arguments passed
                  NOTE: columns are reordered so first are all the regime
                  columns then all the global columns (this makes it much more
                  efficient)
                  Structure of the output matrix (assuming X1, X2 to vary
                  across regimes and constant term, X3 and X4 to be global):
                    X1r1, X2r1, ... , X1r2, X2r2, ... , constant, X3, X4
    '''
    cols2regi = np.array(cols2regi)
    if set(cols2regi) == set([True]):
        xsp = x2xsp(x, regimes, regimes_set)
    elif set(cols2regi) == set([False]):
        xsp = SP.csr_matrix(x)
    else:
        not_regi = x[:, np.where(cols2regi == False)[0]]
        regi_subset = x[:, np.where(cols2regi)[0]]
        regi_subset = x2xsp(regi_subset, regimes, regimes_set)
        xsp = SP.hstack((regi_subset, SP.csr_matrix(not_regi)), format='csr')
    return xsp


def set_name_x_regimes(name_x, regimes, constant_regi, cols2regi, regimes_set):
    '''
    Generate the set of variable names in a regimes setup, according to the
    order of the betas

    NOTE: constant term, if desired in the model, should be included in the x
    already
    ...

    Parameters
    ==========
    name_x          : list/None
                      If passed, list of strings with the names of the
                      variables aligned with the original dense array x
                      IMPORTANT: constant term (if desired in the model) should be
                      included
    regimes         : list
                      list of n values with the mapping of each observation to a
                      regime. Assumed to be aligned with 'x'.
    constant_regi   : [False, 'one', 'many']
                      Switcher controlling the constant term setup. It may take
                      the following values:
                    
                         *  False: no constant term is appended in any way
                         *  'one': a vector of ones is appended to x and held
                                   constant across regimes
                         * 'many': a vector of ones is appended to x and considered
                                   different per regime
    cols2regi       : list
                      List of k booleans indicating whether each column should be
                      considered as different per regime (True) or held constant
                      across regimes (False)
    regimes_set     : list
                      List of ordered regimes tags
    Returns
    =======
    name_x_regi
    '''
    k = len(cols2regi)
    if constant_regi:
        k -= 1
    if not name_x:
        name_x = ['var_' + str(i + 1) for i in range(k)]
    if constant_regi:
        name_x.insert(0, 'CONSTANT')
    nxa = np.array(name_x)
    c2ra = np.array(cols2regi)
    vars_regi = nxa[np.where(c2ra == True)]
    vars_glob = nxa[np.where(c2ra == False)]
    name_x_regi = []
    for r in regimes_set:
        rl = ['%s_%s' % (str(r), i) for i in vars_regi]
        name_x_regi.extend(rl)
    name_x_regi.extend(['_Global_%s' % i for i in vars_glob])
    return name_x_regi


def w_regime(w, regi_ids, regi_i, transform=True, min_n=None):
    '''
    Returns the subset of W matrix according to a given regime ID
    ...

    Attributes
    ==========
    w           : pysal W object
                  Spatial weights object
    regi_ids    : list
                  Contains the location of observations in y that are assigned to regime regi_i
    regi_i      : string or float
                  The regime for which W will be subset

    Returns
    =======
    w_regi_i    : pysal W object
                  Subset of W for regime regi_i
    '''
    w_ids = map(w.id_order.__getitem__, regi_ids)
    warn = None
    w_regi_i = pysal.weights.w_subset(w, w_ids, silent_island_warning=True)
    if min_n:
        if w_regi_i.n < min_n:
            raise Exception, "There are less observations than variables in regime %s." % regi_i
    if transform:
        w_regi_i.transform = w.get_transform()
    if w_regi_i.islands:
        warn = "The regimes operation resulted in islands for regime %s." % regi_i
    return w_regi_i, warn


def w_regimes(w, regimes, regimes_set, transform=True, get_ids=None, min_n=None):
    '''
    ######### DEPRECATED ##########
    Subsets W matrix according to regimes
    ...

    Attributes
    ==========
    w           : pysal W object
                  Spatial weights object
    regimes     : list
                  list of n values with the mapping of each observation to a
                  regime. Assumed to be aligned with 'x'.
    regimes_set : list
                  List of ordered regimes tags

    Returns
    =======
    w_regi      : dictionary
                  Dictionary containing the subsets of W according to regimes: [r1:w1, r2:w2, ..., rR:wR]
    '''
    regi_ids = dict((r, list(np.where(np.array(regimes) == r)[0]))
                    for r in regimes_set)
    w_ids = dict((r, map(w.id_order.__getitem__, regi_ids[r]))
                 for r in regimes_set)
    w_regi_i = {}
    warn = None
    for r in regimes_set:
        w_regi_i[r] = pysal.weights.w_subset(w, w_ids[r],
                                             silent_island_warning=True)
        if min_n:
            if w_regi_i[r].n < min_n:
                raise Exception, "There are less observations than variables in regime %s." % r
        if transform:
            w_regi_i[r].transform = w.get_transform()
        if w_regi_i[r].islands:
            warn = "The regimes operation resulted in islands for regime %s." % r
    if get_ids:
        get_ids = regi_ids
    return w_regi_i, get_ids, warn


def w_regimes_union(w, w_regi_i, regimes_set):
    '''
    Combines the subsets of the W matrix according to regimes
    ...

    Attributes
    ==========
    w           : pysal W object
                  Spatial weights object
    w_regi_i    : dictionary
                  Dictionary containing the subsets of W according to regimes: [r1:w1, r2:w2, ..., rR:wR]
    regimes_set : list
                  List of ordered regimes tags

    Returns
    =======
    w_regi      : pysal W object
                  Spatial weights object containing the union of the subsets of W
    '''
    w_regi = pysal.weights.w_union(w_regi_i[regimes_set[0]],
                                   w_regi_i[regimes_set[1]], silent_island_warning=True)
    if len(regimes_set) > 2:
        for i in range(len(regimes_set))[2:]:
            w_regi = pysal.weights.w_union(w_regi,
                                           w_regi_i[regimes_set[i]], silent_island_warning=True)
    w_regi = pysal.weights.remap_ids(w_regi, dict((i, i)
                                     for i in w_regi.id_order), w.id_order)
    w_regi.transform = w.get_transform()
    return w_regi


def x2xsp(x, regimes, regimes_set):
    '''
    Convert X matrix with regimes into a sparse X matrix that accounts for the
    regimes
    ...

    Attributes
    ==========
    x           : np.array
                  Dense array of dimension (n, k) with values for all observations
    regimes     : list
                  list of n values with the mapping of each observation to a
                  regime. Assumed to be aligned with 'x'.
    regimes_set : list
                  List of ordered regimes tags
    Returns
    =======
    xsp         : csr sparse matrix
                  Sparse matrix containing X variables properly aligned for
                  regimes regression. 'xsp' is of dimension (n, k*r) where 'r'
                  is the number of different regimes
                  The structure of the alignent is X1r1 X2r1 ... X1r2 X2r2 ...
    '''
    n, k = x.shape
    data = x.flatten()
    R = len(regimes_set)
    # X1r1 X2r1 ... X1r2 X2r2 ...
    regime_by_row = np.array([[r] * k for r in list(regimes_set)]).flatten()
    row_map = dict((r, np.where(regime_by_row == r)[0]) for r in regimes_set)
    indices = np.array([row_map[row] for row in regimes]).flatten()
    indptr = np.zeros((n + 1, ), dtype=int)
    indptr[:-1] = list(np.arange(n) * k)
    indptr[-1] = n * k
    return SP.csr_matrix((data, indices, indptr))


def check_cols2regi(constant_regi, cols2regi, x, yend=None, add_cons=True):
    ''' Checks if dimensions of list cols2regi match number of variables. '''

    if add_cons:
        is_cons = 1
        if constant_regi == 'many':
            regi_cons = [True]
        elif constant_regi == 'one':
            regi_cons = [False]
    else:
        is_cons = 0
        regi_cons = []
    try:
        tot_k = x.shape[1] + yend.shape[1]
    except:
        tot_k = x.shape[1]
    if cols2regi == 'all':
        cols2regi = regi_cons + [True] * tot_k
    else:
        cols2regi = regi_cons + cols2regi
    if len(cols2regi) - is_cons != tot_k:
        raise Exception, "The lenght of list 'cols2regi' must be equal to the amount of variables (exogenous + endogenous) when not using cols2regi=='all'."
    return cols2regi


def _get_regimes_set(regimes):
    ''' Creates a list with regimes in alphabetical order. '''
    regimes_set = list(set(regimes))
    if isinstance(regimes_set[0], float):
        regimes_set1 = list(set(map(int, regimes_set)))
        if len(regimes_set1) == len(regimes_set):
            regimes_set = regimes_set1
    regimes_set.sort()
    return regimes_set


def _get_weighted_var(regimes, regimes_set, sig2n_k, u, y, x, yend=None, q=None):
    regi_ids = dict((r, list(np.where(np.array(regimes) == r)[0]))
                    for r in regimes_set)
    if sig2n_k:
        sig = dict((r, np.dot(u[regi_ids[r]].T, u[regi_ids[r]]) / (len(regi_ids[r]) - x.shape[1]))
                   for r in regimes_set)
    else:
        sig = dict((r, np.dot(u[regi_ids[r]].T, u[regi_ids[r]]) / len(regi_ids[r]))
                   for r in regimes_set)
    sig_vec = np.zeros(y.shape, float)
    y2 = np.zeros(y.shape, float)
    for r in regimes_set:
        sig_vec[regi_ids[r]] = 1 / float(np.sqrt(sig[r]))
        y2[regi_ids[r]] = y[regi_ids[r]] / float(np.sqrt(sig[r]))
    x2 = spbroadcast(x, sig_vec)
    if yend != None:
        yend2 = spbroadcast(yend, sig_vec)
        q2 = spbroadcast(q, sig_vec)
        return y2, x2, yend2, q2
    else:
        return y2, x2


def _test():
    import doctest
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)

if __name__ == '__main__':
    _test()
    import numpy as np
    import pysal
    from ols_regimes import OLS_Regimes
    db = pysal.open(pysal.examples.get_path('columbus.dbf'), 'r')
    y_var = 'CRIME'
    y = np.array([db.by_col(y_var)]).reshape(49, 1)
    x_var = ['INC', 'HOVAL']
    x = np.array([db.by_col(name) for name in x_var]).T
    r_var = 'NSA'
    regimes = db.by_col(r_var)
    w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
    w.transform = 'r'
    olsr = OLS_Regimes(
        y, x, regimes, w=w, constant_regi='many', nonspat_diag=False, spat_diag=False,
        name_y=y_var, name_x=x_var, name_ds='columbus', name_regimes=r_var, name_w='columbus.gal')
    print olsr.summary

########NEW FILE########
__FILENAME__ = robust
__author__ = "Luc Anselin luc.anselin@asu.edu, \
        Pedro V. Amaral pedro.amaral@asu.edu, \
        David C. Folch david.folch@asu.edu"

import numpy as np
import numpy.linalg as la
from pysal import lag_spatial
from utils import spdot, spbroadcast
from user_output import check_constant


def robust_vm(reg, gwk=None, sig2n_k=False):
    """
    Robust estimation of the variance-covariance matrix. Estimated by White (default) or HAC (if wk is provided). 
        
    Parameters
    ----------
    
    reg             : Regression object (OLS or TSLS)
                      output instance from a regression model

    gwk             : PySAL weights object
                      Optional. Spatial weights based on kernel functions
                      If provided, returns the HAC variance estimation
    sig2n_k         : boolean
                      If True, then use n-k to rescale the vc matrix.
                      If False, use n. (White only)
                      
    Returns
    --------
    
    psi             : kxk array
                      Robust estimation of the variance-covariance
                      
    Examples
    --------
    
    >>> import numpy as np
    >>> import pysal
    >>> from ols import OLS
    >>> from twosls import TSLS
    >>> db=pysal.open(pysal.examples.get_path("NAT.dbf"),"r")
    >>> y = np.array(db.by_col("HR90"))
    >>> y = np.reshape(y, (y.shape[0],1))
    >>> X = []
    >>> X.append(db.by_col("RD90"))
    >>> X.append(db.by_col("DV90"))
    >>> X = np.array(X).T                       

    Example with OLS with unadjusted standard errors

    >>> ols = OLS(y,X)
    >>> ols.vm
    array([[ 0.17004545,  0.00226532, -0.02243898],
           [ 0.00226532,  0.00941319, -0.00031638],
           [-0.02243898, -0.00031638,  0.00313386]])

    Example with OLS and White
    
    >>> ols = OLS(y,X, robust='white')
    >>> ols.vm
    array([[ 0.24515481,  0.01093322, -0.03441966],
           [ 0.01093322,  0.01798616, -0.00071414],
           [-0.03441966, -0.00071414,  0.0050153 ]])
    
    Example with OLS and HAC

    >>> wk = pysal.kernelW_from_shapefile(pysal.examples.get_path('NAT.shp'),k=15,function='triangular', fixed=False)
    >>> wk.transform = 'o'
    >>> ols = OLS(y,X, robust='hac', gwk=wk)
    >>> ols.vm
    array([[ 0.29213532,  0.01670361, -0.03948199],
           [ 0.01655557,  0.02295829, -0.00116874],
           [-0.03941483, -0.00119077,  0.00568314]])

    Example with 2SLS and White

    >>> yd = []
    >>> yd.append(db.by_col("UE90"))
    >>> yd = np.array(yd).T
    >>> q = []
    >>> q.append(db.by_col("UE80"))
    >>> q = np.array(q).T
    >>> tsls = TSLS(y, X, yd, q=q, robust='white')
    >>> tsls.vm
    array([[ 0.29569954,  0.04119843, -0.02496858, -0.01640185],
           [ 0.04119843,  0.03647762,  0.004702  , -0.00987345],
           [-0.02496858,  0.004702  ,  0.00648262, -0.00292891],
           [-0.01640185, -0.00987345, -0.00292891,  0.0053322 ]])

    Example with 2SLS and HAC

    >>> tsls = TSLS(y, X, yd, q=q, robust='hac', gwk=wk)
    >>> tsls.vm
    array([[ 0.41985329,  0.06823119, -0.02883889, -0.02788116],
           [ 0.06867042,  0.04887508,  0.00497443, -0.01367746],
           [-0.02856454,  0.00501402,  0.0072195 , -0.00321604],
           [-0.02810131, -0.01364908, -0.00318197,  0.00713251]])

    """
    if hasattr(reg, 'h'):  # If reg has H, do 2SLS estimator. OLS otherwise.
        tsls = True
        xu = spbroadcast(reg.h, reg.u)
    else:
        tsls = False
        xu = spbroadcast(reg.x, reg.u)

    if gwk:  # If gwk do HAC. White otherwise.
        gwkxu = lag_spatial(gwk, xu)
        psi0 = spdot(xu.T, gwkxu)
    else:
        psi0 = spdot(xu.T, xu)
        if sig2n_k:
            psi0 = psi0 * (1. * reg.n / (reg.n - reg.k))
    if tsls:
        psi1 = spdot(reg.varb, reg.zthhthi)
        psi = spdot(psi1, np.dot(psi0, psi1.T))
    else:
        psi = spdot(reg.xtxi, np.dot(psi0, reg.xtxi))

    return psi


def hac_multi(reg, gwk, constant=False):
    """
    HAC robust estimation of the variance-covariance matrix for multi-regression object 
        
    Parameters
    ----------
    
    reg             : Regression object (OLS or TSLS)
                      output instance from a regression model

    gwk             : PySAL weights object
                      Spatial weights based on kernel functions
                      
    Returns
    --------
    
    psi             : kxk array
                      Robust estimation of the variance-covariance

    """
    if not constant:
        reg.hac_var = check_constant(reg.hac_var)
    xu = spbroadcast(reg.hac_var, reg.u)
    gwkxu = lag_spatial(gwk, xu)
    psi0 = spdot(xu.T, gwkxu)
    counter = 0
    for m in reg.multi:
        reg.multi[m].robust = 'hac'
        reg.multi[m].name_gwk = reg.name_gwk
        try:
            psi1 = spdot(reg.multi[m].varb, reg.multi[m].zthhthi)
            reg.multi[m].vm = spdot(psi1, np.dot(psi0, psi1.T))
        except:
            reg.multi[m].vm = spdot(
                reg.multi[m].xtxi, np.dot(psi0, reg.multi[m].xtxi))
        reg.vm[(counter*reg.kr):((counter+1)*reg.kr), (counter*reg.kr)               :((counter+1)*reg.kr)] = reg.multi[m].vm
        counter += 1


def _test():
    import doctest
    doctest.testmod()

if __name__ == '__main__':
    _test()

########NEW FILE########
__FILENAME__ = summary_output
"""Internal helper files for user output."""

__author__ = "Luc Anselin luc.anselin@asu.edu, David C. Folch david.folch@asu.edu, Pedro V. Amaral pedro.amaral@asu.edu, Jing Yao jingyao@asu.edu"

import textwrap as TW
import numpy as np
import copy as COPY
import diagnostics as diagnostics
import diagnostics_tsls as diagnostics_tsls
import diagnostics_sp as diagnostics_sp
import pysal
import scipy
from scipy.sparse.csr import csr_matrix

__all__ = []


#
# Primary functions for running summary diagnostics #############
#

"""
This section contains one function for each user level regression class. These
are called directly from the user class. Each one mixes and matches smaller
functions located later in this module.
"""


def OLS(reg, vm, w, nonspat_diag, spat_diag, moran, white_test, regimes=False):
    reg.__summary = {}
    # compute diagnostics and organize summary output
    beta_diag_ols(reg, reg.robust)
    if nonspat_diag:
        # compute diagnostics
        reg.sig2ML = reg.sig2n
        reg.f_stat = diagnostics.f_stat(reg)
        reg.logll = diagnostics.log_likelihood(reg)
        reg.aic = diagnostics.akaike(reg)
        reg.schwarz = diagnostics.schwarz(reg)
        reg.mulColli = diagnostics.condition_index(reg)
        reg.jarque_bera = diagnostics.jarque_bera(reg)
        reg.breusch_pagan = diagnostics.breusch_pagan(reg)
        reg.koenker_bassett = diagnostics.koenker_bassett(reg)
        if white_test:
            reg.white = diagnostics.white(reg)
        # organize summary output
        reg.__summary['summary_nonspat_diag_1'] = summary_nonspat_diag_1(reg)
        reg.__summary['summary_nonspat_diag_2'] = summary_nonspat_diag_2(reg)
    if spat_diag:
        # compute diagnostics and organize summary output
        spat_diag_ols(reg, w, moran)
    if regimes:
        summary_regimes(reg)
    summary_warning(reg)
    summary(reg=reg, vm=vm, instruments=False,
            nonspat_diag=nonspat_diag, spat_diag=spat_diag)


def OLS_multi(reg, multireg, vm, nonspat_diag, spat_diag, moran, white_test, regimes=False, sur=False, w=False):
    for m in multireg:
        mreg = multireg[m]
        mreg.__summary = {}
        # compute diagnostics and organize summary output
        beta_diag_ols(mreg, mreg.robust)
        if nonspat_diag:
            # compute diagnostics
            mreg.sig2ML = mreg.sig2n
            mreg.f_stat = diagnostics.f_stat(mreg)
            mreg.logll = diagnostics.log_likelihood(mreg)
            mreg.aic = diagnostics.akaike(mreg)
            mreg.schwarz = diagnostics.schwarz(mreg)
            mreg.mulColli = diagnostics.condition_index(mreg)
            mreg.jarque_bera = diagnostics.jarque_bera(mreg)
            mreg.breusch_pagan = diagnostics.breusch_pagan(mreg)
            mreg.koenker_bassett = diagnostics.koenker_bassett(mreg)
            if white_test:
                mreg.white = diagnostics.white(mreg)
            # organize summary output
            mreg.__summary[
                'summary_nonspat_diag_1'] = summary_nonspat_diag_1(mreg)
            mreg.__summary[
                'summary_nonspat_diag_2'] = summary_nonspat_diag_2(mreg)
        if spat_diag:
            # compute diagnostics and organize summary output
            spat_diag_ols(mreg, mreg.w, moran)
        if regimes:
            summary_regimes(mreg, chow=False)
        if sur:
            summary_sur(mreg)
        summary_warning(mreg)
        multireg[m].__summary = mreg.__summary
    reg.__summary = {}
    if regimes:
        summary_chow(reg)
    if sur:
        summary_sur(reg, u_cov=True)
    if spat_diag:
        # compute global diagnostics and organize summary output
        spat_diag_ols(reg, w, moran)
    summary_warning(reg)
    summary_multi(reg=reg, multireg=multireg, vm=vm, instruments=False,
                  nonspat_diag=nonspat_diag, spat_diag=spat_diag)


def TSLS(reg, vm, w, spat_diag, regimes=False):
    reg.__summary = {}
    # compute diagnostics and organize summary output
    beta_diag(reg, reg.robust)
    if spat_diag:
        # compute diagnostics and organize summary output
        spat_diag_instruments(reg, w)
    # build coefficients table body
    build_coefs_body_instruments(reg)
    if regimes:
        summary_regimes(reg)
    summary_warning(reg)
    summary(reg=reg, vm=vm, instruments=True,
            nonspat_diag=False, spat_diag=spat_diag)


def TSLS_multi(reg, multireg, vm, spat_diag, regimes=False, sur=False, w=False):
    for m in multireg:
        mreg = multireg[m]
        mreg.__summary = {}
        # compute diagnostics and organize summary output
        beta_diag(mreg, mreg.robust)
        if spat_diag:
            # compute diagnostics and organize summary output
            spat_diag_instruments(mreg, mreg.w)
        # build coefficients table body
        build_coefs_body_instruments(mreg)
        if regimes:
            summary_regimes(mreg, chow=False)
        if sur:
            summary_sur(mreg)
        summary_warning(mreg)
        multireg[m].__summary = mreg.__summary
    reg.__summary = {}
    if regimes:
        summary_chow(reg)
    if sur:
        summary_sur(reg, u_cov=True)
    if spat_diag:
        # compute global diagnostics and organize summary output
        spat_diag_instruments(reg, w)
    summary_warning(reg)
    summary_multi(reg=reg, multireg=multireg, vm=vm,
                  instruments=True, nonspat_diag=False, spat_diag=spat_diag)


def GM_Lag(reg, vm, w, spat_diag, regimes=False):
    reg.__summary = {}
    # compute diagnostics and organize summary output
    beta_diag_lag(reg, reg.robust, error=False)
    if spat_diag:
        # compute diagnostics and organize summary output
        spat_diag_instruments(reg, w)
    # build coefficients table body
    summary_coefs_allx(reg, reg.z_stat)
    summary_coefs_instruments(reg)
    if regimes:
        summary_regimes(reg)
    summary_warning(reg)
    summary(reg=reg, vm=vm, instruments=True,
            nonspat_diag=False, spat_diag=spat_diag)


def GM_Lag_multi(reg, multireg, vm, spat_diag, regimes=False, sur=False, w=False):
    for m in multireg:
        mreg = multireg[m]
        mreg.__summary = {}
        # compute diagnostics and organize summary output
        beta_diag_lag(mreg, mreg.robust, error=False)
        if spat_diag:
            # compute diagnostics and organize summary output
            spat_diag_instruments(mreg, mreg.w)
        # build coefficients table body
        summary_coefs_allx(mreg, mreg.z_stat)
        summary_coefs_instruments(mreg)
        if regimes:
            summary_regimes(mreg, chow=False)
        if sur:
            summary_sur(mreg)
        summary_warning(mreg)
        multireg[m].__summary = mreg.__summary
    reg.__summary = {}
    if regimes:
        summary_chow(reg)
    if spat_diag:
        pass
        # compute global diagnostics and organize summary output
        #spat_diag_instruments(reg, w)
    summary_warning(reg)
    summary_multi(reg=reg, multireg=multireg, vm=vm,
                  instruments=True, nonspat_diag=False, spat_diag=spat_diag)


def ML_Lag(reg, w, vm, spat_diag, regimes=False):  # extra space d
    reg.__summary = {}
    # compute diagnostics and organize summary output
    beta_diag_lag(reg, robust=None, error=False)
    reg.__summary[
        'summary_r2'] += "%-20s:%12.3f                %-22s:%12.3f\n" % ('Sigma-square ML', reg.sig2, 'Log likelihood', reg.logll)
    reg.__summary['summary_r2'] += "%-20s:%12.3f                %-22s:%12.3f\n" % (
        'S.E of regression', np.sqrt(reg.sig2), 'Akaike info criterion', reg.aic)
    reg.__summary[
        'summary_r2'] += "                                                 %-22s:%12.3f\n" % ('Schwarz criterion', reg.schwarz)
    # build coefficients table body
    summary_coefs_allx(reg, reg.z_stat)
    if regimes:
        summary_regimes(reg)
    summary_warning(reg)
    summary(reg=reg, vm=vm, instruments=False,
            nonspat_diag=False, spat_diag=spat_diag)

# extra space d


def ML_Lag_multi(reg, multireg, vm, spat_diag, regimes=False, sur=False, w=False):
    for m in multireg:
        mreg = multireg[m]
        mreg.__summary = {}
        # compute diagnostics and organize summary output
        beta_diag_lag(mreg, robust=None, error=False)
        mreg.__summary[
            'summary_r2'] += "%-20s:%12.3f                %-22s:%12.3f\n" % ('Sigma-square ML', mreg.sig2, 'Log likelihood', mreg.logll)
        mreg.__summary['summary_r2'] += "%-20s:%12.3f                %-22s:%12.3f\n" % (
            'S.E of regression', np.sqrt(mreg.sig2), 'Akaike info criterion', mreg.aic)
        mreg.__summary[
            'summary_r2'] += "                                                 %-22s:%12.3f\n" % ('Schwarz criterion', mreg.schwarz)
        # build coefficients table body
        summary_coefs_allx(mreg, mreg.z_stat)
        if regimes:
            summary_regimes(mreg, chow=False)
        summary_warning(mreg)
        multireg[m].__summary = mreg.__summary
    reg.__summary = {}
    if regimes:
        summary_chow(reg)
    summary_warning(reg)
    summary_multi(reg=reg, multireg=multireg, vm=vm,
                  instruments=False, nonspat_diag=False, spat_diag=spat_diag)


def ML_Error(reg, w, vm, spat_diag, regimes=False):   # extra space d
    reg.__summary = {}
    # compute diagnostics and organize summary output
    beta_diag(reg, robust=None)
    reg.__summary[
        'summary_r2'] += "%-20s:%12.3f                %-22s:%12.3f\n" % ('Sigma-square ML', reg.sig2, 'Log likelihood', reg.logll)
    reg.__summary['summary_r2'] += "%-20s:%12.3f                %-22s:%12.3f\n" % (
        'S.E of regression', np.sqrt(reg.sig2), 'Akaike info criterion', reg.aic)
    reg.__summary[
        'summary_r2'] += "                                                 %-22s:%12.3f\n" % ('Schwarz criterion', reg.schwarz)
    # build coefficients table body
    summary_coefs_allx(reg, reg.z_stat)
    if regimes:
        summary_regimes(reg)
    summary_warning(reg)
    summary(reg=reg, vm=vm, instruments=False,
            nonspat_diag=False, spat_diag=spat_diag)

# extra space d


def ML_Error_multi(reg, multireg, vm, spat_diag, regimes=False, sur=False, w=False):
    for m in multireg:
        mreg = multireg[m]
        mreg.__summary = {}
        # compute diagnostics and organize summary output
        beta_diag(mreg, robust=None)
        mreg.__summary[
            'summary_r2'] += "%-20s:%12.3f                %-22s:%12.3f\n" % ('Sigma-square ML', mreg.sig2, 'Log likelihood', mreg.logll)
        mreg.__summary['summary_r2'] += "%-20s:%12.3f                %-22s:%12.3f\n" % (
            'S.E of regression', np.sqrt(mreg.sig2), 'Akaike info criterion', mreg.aic)
        mreg.__summary[
            'summary_r2'] += "                                                 %-22s:%12.3f\n" % ('Schwarz criterion', mreg.schwarz)
        # build coefficients table body
        summary_coefs_allx(mreg, mreg.z_stat)
        if regimes:
            summary_regimes(mreg, chow=False)
        summary_warning(mreg)
        multireg[m].__summary = mreg.__summary
    reg.__summary = {}
    if regimes:
        summary_chow(reg)
    summary_warning(reg)
    summary_multi(reg=reg, multireg=multireg, vm=vm,
                  instruments=False, nonspat_diag=False, spat_diag=spat_diag)


def GM_Error(reg, vm, w, regimes=False):
    reg.__summary = {}
    # compute diagnostics and organize summary output
    beta_diag(reg, None)
    # build coefficients table body
    beta_position = summary_coefs_somex(reg, reg.z_stat)
    summary_coefs_lambda(reg, reg.z_stat)
    if regimes:
        summary_regimes(reg)
    summary_warning(reg)
    summary(reg=reg, vm=vm, instruments=False,
            nonspat_diag=False, spat_diag=False)


def GM_Error_multi(reg, multireg, vm, regimes=False):
    for m in multireg:
        mreg = multireg[m]
        mreg.__summary = {}
        # compute diagnostics and organize summary output
        beta_diag(mreg, None)
        # build coefficients table body
        beta_position = summary_coefs_somex(mreg, mreg.z_stat)
        summary_coefs_lambda(mreg, mreg.z_stat)
        if regimes:
            summary_regimes(mreg, chow=False)
        summary_warning(mreg)
        multireg[m].__summary = mreg.__summary
    reg.__summary = {}
    summary_chow(reg)
    summary_warning(reg)
    summary_multi(reg=reg, multireg=multireg, vm=vm,
                  instruments=False, nonspat_diag=False, spat_diag=False)


def GM_Endog_Error(reg, vm, w, regimes=False):
    reg.__summary = {}
    # compute diagnostics and organize summary output
    beta_diag(reg, None)
    # build coefficients table body
    summary_coefs_allx(reg, reg.z_stat, lambd=True)
    summary_coefs_lambda(reg, reg.z_stat)
    summary_coefs_instruments(reg)
    if regimes:
        summary_regimes(reg)
    summary_warning(reg)
    summary(reg=reg, vm=vm, instruments=True,
            nonspat_diag=False, spat_diag=False)


def GM_Endog_Error_multi(reg, multireg, vm, regimes=False):
    for m in multireg:
        mreg = multireg[m]
        mreg.__summary = {}
        # compute diagnostics and organize summary output
        beta_diag(mreg, None)
        # build coefficients table body
        summary_coefs_allx(mreg, mreg.z_stat, lambd=True)
        summary_coefs_lambda(mreg, mreg.z_stat)
        summary_coefs_instruments(mreg)
        if regimes:
            summary_regimes(mreg, chow=False)
        summary_warning(mreg)
        multireg[m].__summary = mreg.__summary
    reg.__summary = {}
    summary_chow(reg)
    summary_warning(reg)
    summary_multi(reg=reg, multireg=multireg, vm=vm,
                  instruments=True, nonspat_diag=False, spat_diag=False)


def GM_Error_Hom(reg, vm, w, regimes=False):
    reg.__summary = {}
    # compute diagnostics and organize summary output
    beta_diag(reg, None)
    summary_iteration(reg)
    # build coefficients table body
    beta_position = summary_coefs_somex(reg, reg.z_stat)
    summary_coefs_lambda(reg, reg.z_stat)
    if regimes:
        summary_regimes(reg)
    summary_warning(reg)
    summary(reg=reg, vm=vm, instruments=False,
            nonspat_diag=False, spat_diag=False)


def GM_Error_Hom_multi(reg, multireg, vm, regimes=False):
    for m in multireg:
        mreg = multireg[m]
        mreg.__summary = {}
        # compute diagnostics and organize summary output
        summary_iteration(mreg)
        beta_diag(mreg, None)
        # build coefficients table body
        beta_position = summary_coefs_somex(mreg, mreg.z_stat)
        summary_coefs_lambda(mreg, mreg.z_stat)
        if regimes:
            summary_regimes(mreg, chow=False)
        summary_warning(mreg)
        multireg[m].__summary = mreg.__summary
    reg.__summary = {}
    summary_chow(reg, lambd=True)
    summary_warning(reg)
    summary_multi(reg=reg, multireg=multireg, vm=vm,
                  instruments=False, nonspat_diag=False, spat_diag=False)


def GM_Endog_Error_Hom(reg, vm, w, regimes=False):
    reg.__summary = {}
    # compute diagnostics and organize summary output
    beta_diag(reg, None)
    summary_iteration(reg)
    # build coefficients table body
    summary_coefs_allx(reg, reg.z_stat, lambd=True)
    summary_coefs_lambda(reg, reg.z_stat)
    summary_coefs_instruments(reg)
    if regimes:
        summary_regimes(reg)
    summary_warning(reg)
    summary(reg=reg, vm=vm, instruments=True,
            nonspat_diag=False, spat_diag=False)


def GM_Endog_Error_Hom_multi(reg, multireg, vm, regimes=False):
    for m in multireg:
        mreg = multireg[m]
        mreg.__summary = {}
        # compute diagnostics and organize summary output
        beta_diag(mreg, None)
        summary_iteration(mreg)
        # build coefficients table body
        summary_coefs_allx(mreg, mreg.z_stat, lambd=True)
        summary_coefs_lambda(mreg, mreg.z_stat)
        summary_coefs_instruments(mreg)
        if regimes:
            summary_regimes(mreg, chow=False)
        summary_warning(mreg)
        multireg[m].__summary = mreg.__summary
    reg.__summary = {}
    summary_chow(reg, lambd=True)
    summary_warning(reg)
    summary_multi(reg=reg, multireg=multireg, vm=vm,
                  instruments=True, nonspat_diag=False, spat_diag=False)


def GM_Error_Het(reg, vm, w, regimes=False):
    reg.__summary = {}
    # compute diagnostics and organize summary output
    beta_diag(reg, 'het')
    summary_iteration(reg)
    # build coefficients table body
    beta_position = summary_coefs_somex(reg, reg.z_stat)
    summary_coefs_lambda(reg, reg.z_stat)
    if regimes:
        summary_regimes(reg)
    summary_warning(reg)
    summary(reg=reg, vm=vm, instruments=False,
            nonspat_diag=False, spat_diag=False)


def GM_Error_Het_multi(reg, multireg, vm, regimes=False):
    for m in multireg:
        mreg = multireg[m]
        mreg.__summary = {}
        # compute diagnostics and organize summary output
        beta_diag(mreg, 'het')
        summary_iteration(mreg)
        # build coefficients table body
        beta_position = summary_coefs_somex(mreg, mreg.z_stat)
        summary_coefs_lambda(mreg, mreg.z_stat)
        if regimes:
            summary_regimes(mreg, chow=False)
        summary_warning(mreg)
        multireg[m].__summary = mreg.__summary
    reg.__summary = {}
    summary_chow(reg, lambd=True)
    summary_warning(reg)
    summary_multi(reg=reg, multireg=multireg, vm=vm,
                  instruments=False, nonspat_diag=False, spat_diag=False)


def GM_Endog_Error_Het(reg, vm, w, regimes=False):
    reg.__summary = {}
    # compute diagnostics and organize summary output
    beta_diag(reg, 'het')
    summary_iteration(reg)
    # build coefficients table body
    summary_coefs_allx(reg, reg.z_stat, lambd=True)
    summary_coefs_lambda(reg, reg.z_stat)
    summary_coefs_instruments(reg)
    if regimes:
        summary_regimes(reg)
    summary_warning(reg)
    summary(reg=reg, vm=vm, instruments=True,
            nonspat_diag=False, spat_diag=False)


def GM_Endog_Error_Het_multi(reg, multireg, vm, regimes=False):
    for m in multireg:
        mreg = multireg[m]
        mreg.__summary = {}
        # compute diagnostics and organize summary output
        beta_diag(mreg, 'het')
        summary_iteration(mreg)
        # build coefficients table body
        summary_coefs_allx(mreg, mreg.z_stat, lambd=True)
        summary_coefs_lambda(mreg, mreg.z_stat)
        summary_coefs_instruments(mreg)
        if regimes:
            summary_regimes(mreg, chow=False)
        summary_warning(mreg)
        multireg[m].__summary = mreg.__summary
    reg.__summary = {}
    summary_chow(reg, lambd=True)
    summary_warning(reg)
    summary_multi(reg=reg, multireg=multireg, vm=vm,
                  instruments=True, nonspat_diag=False, spat_diag=False)


def GM_Combo(reg, vm, w, regimes=False):
    reg.__summary = {}
    # compute diagnostics and organize summary output
    beta_diag_lag(reg, None)
    # build coefficients table body
    summary_coefs_allx(reg, reg.z_stat, lambd=True)
    summary_coefs_lambda(reg, reg.z_stat)
    summary_coefs_instruments(reg)
    summary_warning(reg)
    if regimes:
        summary_regimes(reg)
    summary_warning(reg)
    summary(reg=reg, vm=vm, instruments=True,
            nonspat_diag=False, spat_diag=False)


def GM_Combo_multi(reg, multireg, vm, regimes=False):
    for m in multireg:
        mreg = multireg[m]
        mreg.__summary = {}
        # compute diagnostics and organize summary output
        beta_diag_lag(mreg, None)
        # build coefficients table body
        summary_coefs_allx(mreg, mreg.z_stat, lambd=True)
        summary_coefs_lambda(mreg, mreg.z_stat)
        summary_coefs_instruments(mreg)
        if regimes:
            summary_regimes(mreg, chow=False)
        summary_warning(mreg)
        multireg[m].__summary = mreg.__summary
    reg.__summary = {}
    if regimes:
        summary_chow(reg)
    summary_warning(reg)
    summary_multi(reg=reg, multireg=multireg, vm=vm,
                  instruments=True, nonspat_diag=False, spat_diag=False)


def GM_Combo_Hom(reg, vm, w, regimes=False):
    reg.__summary = {}
    # compute diagnostics and organize summary output
    beta_diag_lag(reg, None)
    summary_iteration(reg)
    # build coefficients table body
    summary_coefs_allx(reg, reg.z_stat, lambd=True)
    summary_coefs_lambda(reg, reg.z_stat)
    summary_coefs_instruments(reg)
    if regimes:
        summary_regimes(reg)
    summary_warning(reg)
    summary(reg=reg, vm=vm, instruments=True,
            nonspat_diag=False, spat_diag=False)


def GM_Combo_Hom_multi(reg, multireg, vm, regimes=False):
    for m in multireg:
        mreg = multireg[m]
        mreg.__summary = {}
        # compute diagnostics and organize summary output
        beta_diag_lag(mreg, None)
        summary_iteration(mreg)
        # build coefficients table body
        summary_coefs_allx(mreg, mreg.z_stat, lambd=True)
        summary_coefs_lambda(mreg, mreg.z_stat)
        summary_coefs_instruments(mreg)
        if regimes:
            summary_regimes(mreg, chow=False)
        summary_warning(mreg)
        multireg[m].__summary = mreg.__summary
    reg.__summary = {}
    if regimes:
        summary_chow(reg)
    summary_warning(reg)
    summary_multi(reg=reg, multireg=multireg, vm=vm,
                  instruments=True, nonspat_diag=False, spat_diag=False)


def GM_Combo_Het(reg, vm, w, regimes=False):
    reg.__summary = {}
    # compute diagnostics and organize summary output
    beta_diag_lag(reg, 'het')
    summary_iteration(reg)
    # build coefficients table body
    summary_coefs_allx(reg, reg.z_stat, lambd=True)
    summary_coefs_lambda(reg, reg.z_stat)
    summary_coefs_instruments(reg)
    if regimes:
        summary_regimes(reg)
    summary_warning(reg)
    summary(reg=reg, vm=vm, instruments=True,
            nonspat_diag=False, spat_diag=False)


def GM_Combo_Het_multi(reg, multireg, vm, regimes=False):
    for m in multireg:
        mreg = multireg[m]
        mreg.__summary = {}
        # compute diagnostics and organize summary output
        beta_diag_lag(mreg, 'het')
        summary_iteration(mreg)
        # build coefficients table body
        summary_coefs_allx(mreg, mreg.z_stat, lambd=True)
        summary_coefs_lambda(mreg, mreg.z_stat)
        summary_coefs_instruments(mreg)
        if regimes:
            summary_regimes(mreg, chow=False)
        summary_warning(mreg)
        multireg[m].__summary = mreg.__summary
    reg.__summary = {}
    if regimes:
        summary_regimes(reg)
    summary_warning(reg)
    summary_multi(reg=reg, multireg=multireg, vm=vm,
                  instruments=True, nonspat_diag=False, spat_diag=False)


def Probit(reg, vm, w, spat_diag):
    reg.__summary = {}
    # compute diagnostics and organize summary output
    beta_diag(reg, None)
    # organize summary output
    if spat_diag:
        reg.__summary['summary_spat_diag'] = summary_spat_diag_probit(reg)
    reg.__summary[
        'summary_r2'] = "%-21s: %3.2f\n" % ('% correctly predicted', reg.predpc)
    reg.__summary[
        'summary_r2'] += "%-21s: %3.4f\n" % ('Log-Likelihood', reg.logl)
    reg.__summary['summary_r2'] += "%-21s: %3.4f\n" % ('LR test', reg.LR[0])
    reg.__summary[
        'summary_r2'] += "%-21s: %3.4f\n" % ('LR test (p-value)', reg.LR[1])
    if reg.warning:
        reg.__summary[
            'summary_r2'] += "\nMaximum number of iterations exceeded or gradient and/or function calls not changing\n"
    # build coefficients table body
    beta_position = summary_coefs_allx(reg, reg.z_stat)
    reg.__summary['summary_other_mid'] = summary_coefs_slopes(reg)
    summary(reg=reg, vm=vm, instruments=False,
            short_intro=True, spat_diag=spat_diag)

#


#
# Helper functions for running summary diagnostics #############
#
def beta_diag_ols(reg, robust):
    # compute diagnostics
    reg.std_err = diagnostics.se_betas(reg)
    reg.t_stat = diagnostics.t_stat(reg)
    reg.r2 = diagnostics.r2(reg)
    reg.ar2 = diagnostics.ar2(reg)
    # organize summary output
    reg.__summary['summary_std_err'] = robust
    reg.__summary['summary_zt'] = 't'
    reg.__summary[
        'summary_r2'] = "%-20s:%12.4f\n%-20s:%12.4f\n" % ('R-squared', reg.r2, 'Adjusted R-squared', reg.ar2)
    # build coefficients table body
    position = summary_coefs_allx(reg, reg.t_stat)


def beta_diag(reg, robust):
    # compute diagnostics
    reg.std_err = diagnostics.se_betas(reg)
    reg.z_stat = diagnostics.t_stat(reg, z_stat=True)
    reg.pr2 = diagnostics_tsls.pr2_aspatial(reg)
    # organize summary output
    reg.__summary['summary_std_err'] = robust
    reg.__summary['summary_zt'] = 'z'
    reg.__summary[
        'summary_r2'] = "%-20s:%12.4f\n" % ('Pseudo R-squared', reg.pr2)


def beta_diag_lag(reg, robust, error=True):
    # compute diagnostics
    reg.std_err = diagnostics.se_betas(reg)
    reg.z_stat = diagnostics.t_stat(reg, z_stat=True)
    reg.pr2 = diagnostics_tsls.pr2_aspatial(reg)
    # organize summary output
    reg.__summary['summary_std_err'] = robust
    reg.__summary['summary_zt'] = 'z'
    reg.__summary[
        'summary_r2'] = "%-20s:      %5.4f\n" % ('Pseudo R-squared', reg.pr2)
    if np.abs(reg.rho) < 1:
        reg.pr2_e = diagnostics_tsls.pr2_spatial(reg)
        reg.__summary[
            'summary_r2'] += "%-20s:  %5.4f\n" % ('Spatial Pseudo R-squared', reg.pr2_e)
    else:
        reg.__summary[
            'summary_r2'] += "Spatial Pseudo R-squared: omitted due to rho outside the boundary (-1, 1)."


def build_coefs_body_instruments(reg):
    beta_position = summary_coefs_allx(reg, reg.z_stat)
    summary_coefs_allx(reg, reg.z_stat)
    summary_coefs_instruments(reg)


def spat_diag_ols(reg, w, moran):
    # compute diagnostics
    lm_tests = diagnostics_sp.LMtests(reg, w)
    reg.lm_error = lm_tests.lme
    reg.lm_lag = lm_tests.lml
    reg.rlm_error = lm_tests.rlme
    reg.rlm_lag = lm_tests.rlml
    reg.lm_sarma = lm_tests.sarma
    if moran:
        moran_res = diagnostics_sp.MoranRes(reg, w, z=True)
        reg.moran_res = moran_res.I, moran_res.zI, moran_res.p_norm
    # organize summary output
    reg.__summary['summary_spat_diag'] = summary_spat_diag_ols(reg, moran)


def spat_diag_instruments(reg, w):
    # compute diagnostics
    cache = diagnostics_sp.spDcache(reg, w)
    mi, ak, ak_p = diagnostics_sp.akTest(reg, w, cache)
    reg.ak_test = ak, ak_p
    # organize summary output
    reg.__summary[
        'summary_spat_diag'] = "%-27s      %2d    %12.3f       %9.4f\n" % ("Anselin-Kelejian Test", 1, reg.ak_test[0], reg.ak_test[1])


def summary(reg, vm, instruments, short_intro=False, nonspat_diag=False, spat_diag=False, other_end=False):
    summary = summary_open()
    summary += summary_intro(reg, short_intro)
    summary += reg.__summary['summary_r2']
    if nonspat_diag:
        summary += reg.__summary['summary_nonspat_diag_1']
    try:
        summary += reg.__summary['summary_other_top']
    except:
        pass
    summary += summary_coefs_intro(reg)
    summary += reg.__summary['summary_coefs']
    summary += "------------------------------------------------------------------------------------\n"
    if instruments:
        summary += reg.__summary['summary_coefs_instruments']
    try:
        summary += reg.__summary['summary_other_mid']
    except:
        pass
    if nonspat_diag:
        summary += reg.__summary['summary_nonspat_diag_2']
    if spat_diag:
        summary += summary_spat_diag_intro()
        summary += reg.__summary['summary_spat_diag']
    if vm:
        summary += summary_vm(reg, instruments)
    try:
        summary += reg.__summary['summary_chow']
    except:
        pass
    if other_end:
        summary += reg.__summary['summary_other_end']
    summary += summary_close()
    reg.summary = summary


def summary_multi(reg, multireg, vm, instruments, short_intro=False, nonspat_diag=False, spat_diag=False, other_end=False):
    summary = summary_open(multi=True)
    for m in multireg:
        mreg = multireg[m]
        summary += "----------\n\n"
        summary += summary_intro(mreg, short_intro)
        summary += mreg.__summary['summary_r2']
        if nonspat_diag:
            summary += mreg.__summary['summary_nonspat_diag_1']
        try:
            summary += reg.__summary['summary_other_top']
        except:
            pass
        summary += summary_coefs_intro(mreg)
        summary += mreg.__summary['summary_coefs']
        summary += "------------------------------------------------------------------------------------\n"
        if instruments:
            summary += mreg.__summary['summary_coefs_instruments']
        try:
            summary += mreg.__summary['summary_other_mid']
        except:
            pass
        if m == multireg.keys()[-1]:
            try:
                summary += reg.__summary['summary_other_mid']
            except:
                pass
        if nonspat_diag:
            summary += mreg.__summary['summary_nonspat_diag_2']
        if spat_diag:
            summary += summary_spat_diag_intro()
            summary += mreg.__summary['summary_spat_diag']
        if vm:
            summary += summary_vm(mreg, instruments)
        if other_end:
            summary += mreg.__summary['summary_other_end']
        if m == multireg.keys()[-1]:
            try:
                summary += reg.__summary['summary_chow']
            except:
                pass
            if spat_diag:
                try:
                    spat_diag_str = reg.__summary['summary_spat_diag']
                    summary += summary_spat_diag_intro_global()
                    summary += spat_diag_str
                except:
                    pass
            try:
                summary += reg.__summary['summary_other_end']
            except:
                pass
    summary += summary_close()
    reg.summary = summary


def _get_var_indices(reg, lambd=False):
    try:
        var_names = reg.name_z
    except:
        var_names = reg.name_x
    last_v = len(var_names)
    if lambd:
        last_v += -1
    indices = []
    try:
        kf = reg.kf
        if lambd:
            kf += -1
        krex = reg.kr - reg.kryd
        try:
            kfyd = reg.yend.shape[1] - reg.nr * reg.kryd
        except:
            kfyd = 0
        j_con = 0
        if reg.constant_regi == 'many':
            j_con = 1
        for i in range(reg.nr):
            j = i * krex
            jyd = krex * reg.nr + i * reg.kryd + kf - kfyd
            name_reg = var_names[j + j_con:j + krex] + \
                var_names[jyd:jyd + reg.kryd]
            name_reg.sort()
            if reg.constant_regi == 'many':
                indices += [j] + [var_names.index(ind) for ind in name_reg]
            else:
                indices += [var_names.index(ind) for ind in name_reg]
        if reg.constant_regi == 'one':
            indices += [krex * reg.nr]
        if len(indices) < last_v:
            name_reg = var_names[krex * reg.nr + 1 - j_con:krex * reg.nr + kf - kfyd] + \
                var_names[reg.kr * reg.nr + kf - kfyd:reg.kr * reg.nr + kf]
            name_reg.sort()
            indices += [var_names.index(ind) for ind in name_reg]
    except:
        indices = [0] + (np.argsort(var_names[1:last_v]) + 1).tolist()
    return var_names, indices

#


#
# Guts of the summary printout #################################
#
"""
This section contains the pieces needed to put together the summary printout.
"""


def summary_open(multi=False):
    strSummary = ""
    strSummary += "REGRESSION\n"
    if not multi:
        strSummary += "----------\n"
    return strSummary


def summary_intro(reg, short):  # extra space d
    title = "SUMMARY OF OUTPUT: " + reg.title + "\n"
    strSummary = title
    strSummary += "-" * (len(title) - 1) + "\n"
    strSummary += "%-20s:%12s\n" % ('Data set', reg.name_ds)
    if reg.name_w:
        strSummary += "%-20s:%12s\n" % ('Weights matrix', reg.name_w)
    strSummary += "%-20s:%12s                %-22s:%12d\n" % ('Dependent Variable',
                                                              reg.name_y, 'Number of Observations', reg.n)
    if not short:
        strSummary += "%-20s:%12.4f                %-22s:%12d\n" % ('Mean dependent var',
                                                                    reg.mean_y, 'Number of Variables', reg.k)
        strSummary += "%-20s:%12.4f                %-22s:%12d\n" % ('S.D. dependent var',
                                                                    reg.std_y, 'Degrees of Freedom', reg.n - reg.k)
    #strSummary += '\n'
    return strSummary


def summary_coefs_intro(reg):
    strSummary = "\n"
    if reg.__summary['summary_std_err']:
        if reg.__summary['summary_std_err'].lower() == 'white':
            strSummary += "White Standard Errors\n"
        elif reg.__summary['summary_std_err'].lower() == 'hac':
            strSummary += "HAC Standard Errors; Kernel Weights: " + \
                reg.name_gwk + "\n"
        # elif reg.__summary['summary_std_err'].lower() == 'het':
            #strSummary += "Heteroskedastic Corrected Standard Errors\n"
    strSummary += "------------------------------------------------------------------------------------\n"
    strSummary += "            Variable     Coefficient       Std.Error     %1s-Statistic     Probability\n" % (
        reg.__summary['summary_zt'])
    strSummary += "------------------------------------------------------------------------------------\n"
    return strSummary


def summary_coefs_allx(reg, zt_stat, lambd=False):
    strSummary = ""
    var_names, indices = _get_var_indices(reg, lambd)
    for i in indices:
        strSummary += "%20s    %12.7f    %12.7f    %12.7f    %12.7f\n"   \
            % (var_names[i], reg.betas[i][0], reg.std_err[i], zt_stat[i][0], zt_stat[i][1])
    reg.__summary['summary_coefs'] = strSummary
    return i


def summary_coefs_somex(reg, zt_stat):
    """This is a special case needed for models that do not have inference on
    the lambda term
    """
    strSummary = ""
    var_names, indices = _get_var_indices(reg, lambd=True)
    for i in indices:
        strSummary += "%20s    %12.7f    %12.7f    %12.7f    %12.7f\n"   \
            % (reg.name_x[i], reg.betas[i][0], reg.std_err[i], zt_stat[i][0], zt_stat[i][1])
    reg.__summary['summary_coefs'] = strSummary
    return i
"""
def summary_coefs_yend(reg, zt_stat, lambd=False):
    strSummary = ""
    indices = _get_var_indices(reg, lambd) 
    for i in indices:
        strSummary += "%20s    %12.7f    %12.7f    %12.7f    %12.7f\n"   \
                     % (reg.name_z[i],reg.betas[i][0],reg.std_err[i],zt_stat[i][0],zt_stat[i][1])              
    reg.__summary['summary_coefs'] = strSummary
"""


def summary_coefs_lambda(reg, zt_stat):
    try:
        name_var = reg.name_z
    except:
        name_var = reg.name_x
    if len(reg.betas) == len(zt_stat):
        reg.__summary['summary_coefs'] += "%20s    %12.7f    %12.7f    %12.7f    %12.7f\n"   \
            % (name_var[-1], reg.betas[-1][0], reg.std_err[-1], zt_stat[-1][0], zt_stat[-1][1])
    else:
        reg.__summary[
            'summary_coefs'] += "%20s    %12.7f    \n" % (name_var[-1], reg.betas[-1][0])


def summary_coefs_instruments(reg):
    """Generates a list of the instruments used.
    """
    insts = "Instruments: "
    for name in sorted(reg.name_q):
        insts += name + ", "
    text_wrapper = TW.TextWrapper(width=76, subsequent_indent="             ")
    insts = text_wrapper.fill(insts[:-2])
    insts += "\n"
    inst2 = "Instrumented: "
    for name in sorted(reg.name_yend):
        inst2 += name + ", "
    text_wrapper = TW.TextWrapper(width=76, subsequent_indent="             ")
    inst2 = text_wrapper.fill(inst2[:-2])
    inst2 += "\n"
    inst2 += insts
    reg.__summary['summary_coefs_instruments'] = inst2


def summary_iteration(reg):  # extra space d
    """Reports the number of iterations computed.
    """
    try:
        if reg.step1c:
            step1c = 'Yes'
        else:
            step1c = 'No'
        txt = "%-20s:%12s                %-22s:%12s\n" % ('N. of iterations',
                                                          reg.iteration, 'Step1c computed', step1c)
    except:
        txt = "%-20s:%12s\n" % ('N. of iterations', reg.iteration)
    try:
        reg.__summary['summary_other_top'] += txt
    except:
        reg.__summary['summary_other_top'] = txt


def summary_regimes(reg, chow=True):
    """Lists the regimes variable used.
    """
    try:
        reg.__summary[
            'summary_other_mid'] += "Regimes variable: %s\n" % reg.name_regimes
    except:
        reg.__summary[
            'summary_other_mid'] = "Regimes variable: %s\n" % reg.name_regimes
    if chow:
        summary_chow(reg)


def summary_sur(reg, u_cov=False):
    """Lists the equation ID variable used.
    """
    if u_cov:
        str_ucv = "\nERROR COVARIANCE MATRIX\n"
        for i in range(reg.u_cov.shape[0]):
            for j in range(reg.u_cov.shape[1]):
                str_ucv += "%12.6f" % (reg.u_cov[i][j])
            str_ucv += "\n"
        try:
            reg.__summary['summary_other_end'] += str_ucv
        except:
            reg.__summary['summary_other_end'] = str_ucv
    else:
        try:
            reg.__summary[
                'summary_other_mid'] += "Equation ID: %s\n" % reg.name_multiID
        except:
            reg.__summary[
                'summary_other_mid'] = "Equation ID: %s\n" % reg.name_multiID
        try:
            reg.__summary[
                'summary_r2'] += "%-20s: %3.4f\n" % ('Log-Likelihood', reg.logl)
        except:
            pass


def summary_chow(reg, lambd=False):
    reg.__summary['summary_chow'] = "\nREGIMES DIAGNOSTICS - CHOW TEST\n"
    reg.__summary[
        'summary_chow'] += "                 VARIABLE        DF        VALUE           PROB\n"
    if reg.cols2regi == 'all':
        names_chow = reg.name_x_r[1:]
    else:
        names_chow = [reg.name_x_r[1:][i] for i in np.where(reg.cols2regi)[0]]
    if reg.constant_regi == 'many':
        indices = [0] + (np.argsort(names_chow) + 1).tolist()
        names_chow = ['CONSTANT'] + names_chow
    else:
        indices = (np.argsort(names_chow)).tolist()
    if lambd:
        indices += [-1]
        names_chow += ['lambda']
    for i in indices:
        reg.__summary[
            'summary_chow'] += "%25s        %2d    %12.3f        %9.4f\n" % (names_chow[i], reg.nr - 1, reg.chow.regi[i, 0], reg.chow.regi[i, 1])
    reg.__summary['summary_chow'] += "%25s        %2d    %12.3f        %9.4f\n" % (
        'Global test', reg.kr * (reg.nr - 1), reg.chow.joint[0], reg.chow.joint[1])


def summary_warning(reg):
    try:
        if reg.warning:
            try:
                reg.__summary['summary_other_mid'] += reg.warning
            except:
                reg.__summary['summary_other_mid'] = reg.warning
    except:
        pass


def summary_coefs_slopes(reg):
    strSummary = "\nMARGINAL EFFECTS\n"
    if reg.scalem == 'phimean':
        strSummary += "Method: Mean of individual marginal effects\n"
    elif reg.scalem == 'xmean':
        strSummary += "Method: Marginal effects at variables mean\n"
    strSummary += "------------------------------------------------------------------------------------\n"
    strSummary += "            Variable           Slope       Std.Error     %1s-Statistic     Probability\n" % (
        reg.__summary['summary_zt'])
    strSummary += "------------------------------------------------------------------------------------\n"
    indices = np.argsort(reg.name_x[1:]).tolist()
    for i in indices:
        strSummary += "%20s    %12.7f    %12.7f    %12.7f    %12.7f\n"   \
            % (reg.name_x[i + 1], reg.slopes[i][0], reg.slopes_std_err[i], reg.slopes_z_stat[i][0], reg.slopes_z_stat[i][1])
    return strSummary + "\n\n"
"""
def summary_r2(reg, ols, spatial_lag):
    if ols:
        strSummary = "%-20s:%12.4f\n%-20s:%12.4f\n" % ('R-squared',reg.r2,'Adjusted R-squared',reg.ar2)
    else:
        strSummary = "%-20s:%12.4f\n" % ('Pseudo R-squared',reg.pr2)
        if spatial_lag:
            if reg.pr2_e != None: 
                strSummary += "%-20s:%12.4f\n" % ('Spatial Pseudo R-squared',reg.pr2_e)
    return strSummary
"""


def summary_nonspat_diag_1(reg):  # extra space d
    strSummary = ""
    strSummary += "%-20s:%12.3f                %-22s:%12.4f\n" % ('Sum squared residual',
                                                                  reg.utu, 'F-statistic', reg.f_stat[0])
    strSummary += "%-20s:%12.3f                %-22s:%12.4g\n" % ('Sigma-square',
                                                                  reg.sig2, 'Prob(F-statistic)', reg.f_stat[1])
    strSummary += "%-20s:%12.3f                %-22s:%12.3f\n" % ('S.E. of regression',
                                                                  np.sqrt(reg.sig2), 'Log likelihood', reg.logll)
    strSummary += "%-20s:%12.3f                %-22s:%12.3f\n" % ('Sigma-square ML',
                                                                  reg.sig2ML, 'Akaike info criterion', reg.aic)
    strSummary += "%-20s:%12.4f                %-22s:%12.3f\n" % (
        'S.E of regression ML', np.sqrt(reg.sig2ML), 'Schwarz criterion', reg.schwarz)
    return strSummary


def summary_nonspat_diag_2(reg):
    strSummary = ""
    strSummary += "\nREGRESSION DIAGNOSTICS\n"
    if reg.mulColli:
        strSummary += "MULTICOLLINEARITY CONDITION NUMBER %16.3f\n\n" % (reg.mulColli)
    strSummary += "TEST ON NORMALITY OF ERRORS\n"
    strSummary += "TEST                             DF        VALUE           PROB\n"
    strSummary += "%-27s      %2d  %14.3f        %9.4f\n\n" % ('Jarque-Bera',
                                                               reg.jarque_bera['df'], reg.jarque_bera['jb'], reg.jarque_bera['pvalue'])
    strSummary += "DIAGNOSTICS FOR HETEROSKEDASTICITY\n"
    strSummary += "RANDOM COEFFICIENTS\n"
    strSummary += "TEST                             DF        VALUE           PROB\n"
    strSummary += "%-27s      %2d    %12.3f        %9.4f\n" % ('Breusch-Pagan test',
                                                               reg.breusch_pagan['df'], reg.breusch_pagan['bp'], reg.breusch_pagan['pvalue'])
    strSummary += "%-27s      %2d    %12.3f        %9.4f\n" % ('Koenker-Bassett test',
                                                               reg.koenker_bassett['df'], reg.koenker_bassett['kb'], reg.koenker_bassett['pvalue'])
    try:
        if reg.white:
            strSummary += "\nSPECIFICATION ROBUST TEST\n"
            if len(reg.white) > 3:
                strSummary += reg.white + '\n'
            else:
                strSummary += "TEST                             DF        VALUE           PROB\n"
                strSummary += "%-27s      %2d    %12.3f        %9.4f\n" % ('White',
                                                                           reg.white['df'], reg.white['wh'], reg.white['pvalue'])
    except:
        pass
    return strSummary


def summary_spat_diag_intro():
    strSummary = ""
    strSummary += "\nDIAGNOSTICS FOR SPATIAL DEPENDENCE\n"
    strSummary += "TEST                           MI/DF       VALUE           PROB\n"
    return strSummary


def summary_spat_diag_intro_global():
    strSummary = ""
    strSummary += "\nDIAGNOSTICS FOR GLOBAL SPATIAL DEPENDENCE\n"
    strSummary += "Residuals are treated as homoskedastic for the purpose of these tests\n"
    strSummary += "TEST                           MI/DF       VALUE           PROB\n"
    return strSummary


def summary_spat_diag_ols(reg, moran):
    strSummary = ""
    if moran:
        strSummary += "%-27s  %8.4f     %9.3f        %9.4f\n" % ("Moran's I (error)",
                                                                 reg.moran_res[0], reg.moran_res[1], reg.moran_res[2])
    strSummary += "%-27s      %2d    %12.3f        %9.4f\n" % ("Lagrange Multiplier (lag)",
                                                               1, reg.lm_lag[0], reg.lm_lag[1])
    strSummary += "%-27s      %2d    %12.3f        %9.4f\n" % ("Robust LM (lag)",
                                                               1, reg.rlm_lag[0], reg.rlm_lag[1])
    strSummary += "%-27s      %2d    %12.3f        %9.4f\n" % ("Lagrange Multiplier (error)",
                                                               1, reg.lm_error[0], reg.lm_error[1])
    strSummary += "%-27s      %2d    %12.3f        %9.4f\n" % ("Robust LM (error)",
                                                               1, reg.rlm_error[0], reg.rlm_error[1])
    strSummary += "%-27s      %2d    %12.3f        %9.4f\n\n" % (
        "Lagrange Multiplier (SARMA)", 2, reg.lm_sarma[0], reg.lm_sarma[1])
    return strSummary


def summary_spat_diag_probit(reg):
    strSummary = ""
    strSummary += "%-27s      %2d    %12.3f       %9.4f\n" % ("Kelejian-Prucha (error)",
                                                              1, reg.KP_error[0], reg.KP_error[1])
    strSummary += "%-27s      %2d    %12.3f       %9.4f\n" % ("Pinkse (error)",
                                                              1, reg.Pinkse_error[0], reg.Pinkse_error[1])
    strSummary += "%-27s      %2d    %12.3f       %9.4f\n\n" % ("Pinkse-Slade (error)",
                                                                1, reg.PS_error[0], reg.PS_error[1])
    return strSummary


def summary_vm(reg, instruments):
    strVM = "\n"
    strVM += "COEFFICIENTS VARIANCE MATRIX\n"
    strVM += "----------------------------\n"
    if instruments:
        for name in reg.name_z:
            strVM += "%12s" % (name)
    else:
        for name in reg.name_x:
            strVM += "%12s" % (name)
    strVM += "\n"
    nrow = reg.vm.shape[0]
    ncol = reg.vm.shape[1]
    for i in range(nrow):
        for j in range(ncol):
            strVM += "%12.6f" % (reg.vm[i][j])
        strVM += "\n"
    return strVM


def summary_pred(reg):
    strPred = "\n\n"
    strPred += "%16s%16s%16s%16s\n" % ('OBS',
                                       reg.name_y, 'PREDICTED', 'RESIDUAL')
    for i in range(reg.n):
        strPred += "%16d%16.5f%16.5f%16.5f\n" % (i + 1,
                                                 reg.y[i][0], reg.predy[i][0], reg.u[i][0])
    return strPred


def summary_close():
    return "================================ END OF REPORT ====================================="

#


def _test():
    import doctest
    doctest.testmod()

if __name__ == '__main__':
    _test()

########NEW FILE########
__FILENAME__ = test_diagnostics
import unittest
import numpy as np
import pysal
from pysal.spreg import diagnostics
from pysal.spreg.ols import OLS 


# create regression object used by all the tests below
db = pysal.open(pysal.examples.get_path("columbus.dbf"), "r")
y = np.array(db.by_col("CRIME"))
y = np.reshape(y, (49,1))
X = []
X.append(db.by_col("INC"))
X.append(db.by_col("HOVAL"))
X = np.array(X).T
reg = OLS(y,X)


class TestFStat(unittest.TestCase):
    def test_f_stat(self):
        obs = diagnostics.f_stat(reg)
        exp = (28.385629224695, 0.000000009341)
        for i in range(2):
            self.assertAlmostEquals(obs[i],exp[i])

class TestTStat(unittest.TestCase):
    def test_t_stat(self):
        obs = diagnostics.t_stat(reg)
        exp = [(14.490373143689094, 9.2108899889173982e-19),
               (-4.7804961912965762, 1.8289595070843232e-05),
               (-2.6544086427176916, 0.010874504909754612)]
        for i in range(3):
            for j in range(2):
                self.assertAlmostEquals(obs[i][j],exp[i][j])

class TestR2(unittest.TestCase):
    def test_r2(self):
        obs = diagnostics.r2(reg)
        exp = 0.55240404083742334
        self.assertAlmostEquals(obs,exp)

class TestAr2(unittest.TestCase):
    def test_ar2(self):
        obs = diagnostics.ar2(reg)
        exp = 0.5329433469607896
        self.assertAlmostEquals(obs,exp)

class TestSeBetas(unittest.TestCase):
    def test_se_betas(self):
        obs = diagnostics.se_betas(reg)
        exp = np.array([4.73548613, 0.33413076, 0.10319868])
        np.testing.assert_array_almost_equal(obs,exp)

class TestLogLikelihood(unittest.TestCase):
    def test_log_likelihood(self):
        obs = diagnostics.log_likelihood(reg)
        exp = -187.3772388121491
        self.assertAlmostEquals(obs,exp)

class TestAkaike(unittest.TestCase):
    def test_akaike(self):
        obs = diagnostics.akaike(reg)
        exp = 380.7544776242982
        self.assertAlmostEquals(obs,exp)

class TestSchwarz(unittest.TestCase):
    def test_schwarz(self):
        obs = diagnostics.schwarz(reg)
        exp = 386.42993851863008
        self.assertAlmostEquals(obs,exp)

class TestConditionIndex(unittest.TestCase):
    def test_condition_index(self):
        obs = diagnostics.condition_index(reg)
        exp = 6.541827751444
        self.assertAlmostEquals(obs,exp)

class TestJarqueBera(unittest.TestCase):
    def test_jarque_bera(self):
        obs = diagnostics.jarque_bera(reg)
        exp = {'df':2, 'jb':1.835752520076, 'pvalue':0.399366291249}
        self.assertEquals(obs['df'],exp['df'])
        self.assertAlmostEquals(obs['jb'],exp['jb'])
        self.assertAlmostEquals(obs['pvalue'],exp['pvalue'])

class TestBreuschPagan(unittest.TestCase):
    def test_breusch_pagan(self):
        obs = diagnostics.breusch_pagan(reg)
        exp = {'df':2, 'bp':7.900441675960, 'pvalue':0.019250450075}
        self.assertEquals(obs['df'],exp['df'])
        self.assertAlmostEquals(obs['bp'],exp['bp'])
        self.assertAlmostEquals(obs['pvalue'],exp['pvalue'])

class TestWhite(unittest.TestCase):
    def test_white(self):
        obs = diagnostics.white(reg)
        exp = {'df':5, 'wh':19.946008239903, 'pvalue':0.001279222817}
        self.assertEquals(obs['df'],exp['df'])
        self.assertAlmostEquals(obs['wh'],exp['wh'])
        self.assertAlmostEquals(obs['pvalue'],exp['pvalue'])

class TestKoenkerBassett(unittest.TestCase):
    def test_koenker_bassett(self):
        obs = diagnostics.koenker_bassett(reg)
        exp = {'df':2, 'kb':5.694087931707, 'pvalue':0.058015563638}
        self.assertEquals(obs['df'],exp['df'])
        self.assertAlmostEquals(obs['kb'],exp['kb'])
        self.assertAlmostEquals(obs['pvalue'],exp['pvalue'])

class TestVif(unittest.TestCase):
    def test_vif(self):
        obs = diagnostics.vif(reg)
        exp = [(0.0, 0.0),  # note [0][1] should actually be infiniity...
               (1.3331174971891975, 0.75012142748740696),
               (1.3331174971891973, 0.75012142748740707)]
        for i in range(1,3):
            for j in range(2):
                self.assertAlmostEquals(obs[i][j],exp[i][j])

class TestConstantCheck(unittest.TestCase):
    def test_constant_check(self):
        obs = diagnostics.constant_check(reg.x)
        exp = True
        self.assertEquals(obs,exp)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_diagnostics_sp
import unittest
import numpy as np
import pysal
from pysal.spreg import diagnostics
from pysal.spreg.ols import OLS as OLS
from pysal.spreg.twosls import TSLS as TSLS
from pysal.spreg.twosls_sp import GM_Lag
from pysal.spreg.diagnostics_sp import LMtests, MoranRes, spDcache, AKtest


class TestLMtests(unittest.TestCase):
    def setUp(self):
        db = pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        X = np.array(X).T
        self.y = y
        self.X = X
        ols = OLS(self.y, self.X)
        self.ols = ols
        w = pysal.open(pysal.examples.get_path('columbus.gal'), 'r').read()
        w.transform='r'
        self.w = w

    def test_lm_err(self):
        lms = LMtests(self.ols, self.w)
        lme = np.array([3.097094,  0.078432])
        np.testing.assert_array_almost_equal(lms.lme, lme, decimal=6)

    def test_lm_lag(self):
        lms = LMtests(self.ols, self.w)
        lml = np.array([ 0.981552,  0.321816])
        np.testing.assert_array_almost_equal(lms.lml, lml, decimal=6)

    def test_rlm_err(self):
        lms = LMtests(self.ols, self.w)
        rlme = np.array([ 3.209187,  0.073226])
        np.testing.assert_array_almost_equal(lms.rlme, rlme, decimal=6)

    def test_rlm_lag(self):
        lms = LMtests(self.ols, self.w)
        rlml = np.array([ 1.093645,  0.295665])
        np.testing.assert_array_almost_equal(lms.rlml, rlml, decimal=6)

    def test_lm_sarma(self):
        lms = LMtests(self.ols, self.w)
        sarma = np.array([ 4.190739,  0.123025])
        np.testing.assert_array_almost_equal(lms.sarma, sarma, decimal=6)


class TestMoranRes(unittest.TestCase):
    def setUp(self):
        db = pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        X = np.array(X).T
        self.y = y
        self.X = X
        ols = OLS(self.y, self.X)
        self.ols = ols
        w = pysal.open(pysal.examples.get_path('columbus.gal'), 'r').read()
        w.transform='r'
        self.w = w
    
    def test_get_m_i(self):
        m = MoranRes(self.ols, self.w, z=True)
        np.testing.assert_array_almost_equal(m.I, 0.17130999999999999, decimal=6)

    def test_get_v_i(self):
        m = MoranRes(self.ols, self.w, z=True)
        np.testing.assert_array_almost_equal(m.vI, 0.0081300000000000001, decimal=6)

    def test_get_e_i(self):
        m = MoranRes(self.ols, self.w, z=True)
        np.testing.assert_array_almost_equal(m.eI, -0.034522999999999998, decimal=6)

    def test_get_z_i(self):
        m = MoranRes(self.ols, self.w, z=True)
        np.testing.assert_array_almost_equal(m.zI, 2.2827389999999999, decimal=6)


class TestAKTest(unittest.TestCase):
    def setUp(self):
        db = pysal.open(pysal.examples.get_path("columbus.dbf"),'r')
        y = np.array(db.by_col("CRIME"))
        y = np.reshape(y, (49,1))
        self.y = y
        X = []
        X.append(db.by_col("INC"))
        X = np.array(X).T
        self.X = X
        yd = []
        yd.append(db.by_col("HOVAL"))
        yd = np.array(yd).T
        self.yd = yd
        q = []
        q.append(db.by_col("DISCBD"))
        q = np.array(q).T
        self.q = q
        reg = TSLS(y, X, yd, q=q)
        self.reg = reg
        w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        w.transform = 'r'
        self.w = w

    def test_gen_mi(self):
        ak = AKtest(self.reg, self.w)
        np.testing.assert_array_almost_equal(ak.mi, 0.2232672865437263, decimal=6)

    def test_gen_ak(self):
        ak = AKtest(self.reg, self.w)
        np.testing.assert_array_almost_equal(ak.ak, 4.6428948758930852, decimal=6)

    def test_gen_p(self):
        ak = AKtest(self.reg, self.w)
        np.testing.assert_array_almost_equal(ak.p, 0.031182360054340875, decimal=6)

    def test_sp_mi(self):
        ak = AKtest(self.reg, self.w, case='gen')
        np.testing.assert_array_almost_equal(ak.mi, 0.2232672865437263, decimal=6)

    def test_sp_ak(self):
        ak = AKtest(self.reg, self.w,case='gen')
        np.testing.assert_array_almost_equal(ak.ak, 1.1575928784397795, decimal=6)

    def test_sp_p(self):
        ak = AKtest(self.reg, self.w, case='gen')
        np.testing.assert_array_almost_equal(ak.p, 0.28196531619791054, decimal=6)

class TestSpDcache(unittest.TestCase):
    def setUp(self):
        db = pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        X = np.array(X).T
        self.y = y
        self.X = X
        ols = OLS(self.y, self.X)
        self.ols = ols
        w = pysal.open(pysal.examples.get_path('columbus.gal'), 'r').read()
        w.transform='r'
        self.w = w

    def test_j(self):
        cache = spDcache(self.ols, self.w)
        np.testing.assert_array_almost_equal(cache.j[0][0], 0.62330311259039439, decimal=6)

    def test_t(self):
        cache = spDcache(self.ols, self.w)
        np.testing.assert_array_almost_equal(cache.t, 22.751186696900984, decimal=6)

    def test_trA(self):
        cache = spDcache(self.ols, self.w)
        np.testing.assert_array_almost_equal(cache.trA, 1.5880426389276328, decimal=6)

    def test_utwuDs(self):
        cache = spDcache(self.ols, self.w)
        np.testing.assert_array_almost_equal(cache.utwuDs[0][0], 8.3941977502916068, decimal=6)

    def test_utwyDs(self):
        cache = spDcache(self.ols, self.w)
        np.testing.assert_array_almost_equal(cache.utwyDs[0][0], 5.475255215067957, decimal=6)

    def test_wu(self):
        cache = spDcache(self.ols, self.w)
        np.testing.assert_array_almost_equal(cache.wu[0][0], -10.681344941514411, decimal=6)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_diagnostics_tsls
import unittest
import numpy as np
import pysal
import pysal.spreg.diagnostics_tsls as diagnostics_tsls
import pysal.spreg.diagnostics as diagnostics
from pysal.spreg.ols import OLS as OLS
from pysal.spreg.twosls import TSLS as TSLS
from pysal.spreg.twosls_sp import GM_Lag
from scipy.stats import pearsonr


# create regression object used by the apatial tests
db = pysal.open(pysal.examples.get_path("columbus.dbf"),'r')
y = np.array(db.by_col("CRIME"))
y = np.reshape(y, (49,1))
X = []
X.append(db.by_col("INC"))
X = np.array(X).T    
yd = []
yd.append(db.by_col("HOVAL"))
yd = np.array(yd).T
q = []
q.append(db.by_col("DISCBD"))
q = np.array(q).T
reg = TSLS(y, X, yd, q)

# create regression object for spatial test
db = pysal.open(pysal.examples.get_path("columbus.dbf"),'r')
y = np.array(db.by_col("HOVAL"))
y = np.reshape(y, (49,1))
X = np.array(db.by_col("INC"))
X = np.reshape(X, (49,1))
yd = np.array(db.by_col("CRIME"))
yd = np.reshape(yd, (49,1))
q = np.array(db.by_col("DISCBD"))
q = np.reshape(q, (49,1))
w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp")) 
w.transform = 'r'
regsp = GM_Lag(y, X, w=w, yend=yd, q=q, w_lags=2)


class TestTStat(unittest.TestCase):
    def test_t_stat(self):
        obs = diagnostics_tsls.t_stat(reg)
        exp = [(5.8452644704588588, 4.9369075950019865e-07),
               (0.36760156683572748, 0.71485634049075841),
               (-1.9946891307832111, 0.052021795864651159)]
        for i in range(3):
            for j in range(2):
                self.assertAlmostEquals(obs[i][j],exp[i][j])

class TestPr2Aspatial(unittest.TestCase):
    def test_pr2_aspatial(self):
        obs = diagnostics_tsls.pr2_aspatial(reg)
        exp = 0.2793613712817381
        self.assertAlmostEquals(obs,exp)

class TestPr2Spatial(unittest.TestCase):
    def test_pr2_spatial(self):
        obs = diagnostics_tsls.pr2_spatial(regsp)
        exp = 0.29964855438065163
        self.assertAlmostEquals(obs,exp)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_error_sp
import unittest
import pysal
import numpy as np
from pysal.spreg import error_sp as SP

class TestBaseGMError(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        reg = SP.BaseGM_Error(self.y, self.X, self.w.sparse)
        betas = np.array([[ 47.94371455], [  0.70598088], [ -0.55571746], [  0.37230161]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        u = np.array([ 27.4739775])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        predy = np.array([ 52.9930255])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        n = 49
        self.assertAlmostEqual(reg.n,n,4)
        k = 3
        self.assertAlmostEqual(reg.k,k,4)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,4)
        x = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x[0],x,4)
        e = np.array([ 31.89620319])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,4)
        predy = np.array([ 52.9930255])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        my = 38.43622446938776
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([[  1.51884943e+02,  -5.37622793e+00,  -1.86970286e+00], [ -5.37622793e+00,   2.48972661e-01,   5.26564244e-02], [ -1.86970286e+00,   5.26564244e-02, 3.18930650e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,4)
        sig2 = 191.73716465732355
        self.assertAlmostEqual(reg.sig2,sig2,4)

class TestGMError(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        reg = SP.GM_Error(self.y, self.X, self.w)
        betas = np.array([[ 47.94371455], [  0.70598088], [ -0.55571746], [  0.37230161]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        u = np.array([ 27.4739775])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        predy = np.array([ 52.9930255])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        n = 49
        self.assertAlmostEqual(reg.n,n,4)
        k = 3
        self.assertAlmostEqual(reg.k,k,4)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,4)
        x = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x[0],x,4)
        e = np.array([ 31.89620319])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,4)
        predy = np.array([ 52.9930255])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        my = 38.43622446938776
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([[  1.51884943e+02,  -5.37622793e+00,  -1.86970286e+00], [ -5.37622793e+00,   2.48972661e-01,   5.26564244e-02], [ -1.86970286e+00,   5.26564244e-02, 3.18930650e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,4)
        sig2 = 191.73716465732355
        self.assertAlmostEqual(reg.sig2,sig2,4)
        pr2 = 0.3495097406012179
        self.assertAlmostEqual(reg.pr2,pr2)
        std_err = np.array([ 12.32416094,   0.4989716 ,   0.1785863 ])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,4)
        z_stat = np.array([[  3.89022140e+00,   1.00152805e-04], [  1.41487186e+00,   1.57106070e-01], [ -3.11175868e+00,   1.85976455e-03]])
        np.testing.assert_array_almost_equal(reg.z_stat,z_stat,4)

class TestBaseGMEndogError(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        self.X = np.array(X).T
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        yd = []
        yd.append(db.by_col("CRIME"))
        self.yd = np.array(yd).T
        q = []
        q.append(db.by_col("DISCBD"))
        self.q = np.array(q).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        reg = SP.BaseGM_Endog_Error(self.y, self.X, self.yd, self.q, self.w.sparse)
        betas = np.array([[ 55.36095292], [  0.46411479], [ -0.66883535], [  0.38989939]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        u = np.array([ 26.55951566])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        e = np.array([ 31.23925425])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,4)
        predy = np.array([ 53.9074875])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 3
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,4)
        x = np.array([  1.   ,  19.531])
        np.testing.assert_array_almost_equal(reg.x[0],x,4)
        yend = np.array([  15.72598])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,4)
        z = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.z[0],z,4)
        my = 38.43622446938776
        self.assertAlmostEqual(reg.mean_y,my)
        #std_y
        sy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,sy)
        #vm
        vm = np.array([[  5.29158422e+02,  -1.57833675e+01,  -8.38021080e+00],
       [ -1.57833675e+01,   5.40235041e-01,   2.31120327e-01],
       [ -8.38021080e+00,   2.31120327e-01,   1.44977385e-01]])
        np.testing.assert_array_almost_equal(reg.vm,vm,4)
        sig2 = 192.50022721929574
        self.assertAlmostEqual(reg.sig2,sig2,4)

class TestGMEndogError(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        self.X = np.array(X).T
        yd = []
        yd.append(db.by_col("CRIME"))
        self.yd = np.array(yd).T
        q = []
        q.append(db.by_col("DISCBD"))
        self.q = np.array(q).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        reg = SP.GM_Endog_Error(self.y, self.X, self.yd, self.q, self.w)
        betas = np.array([[ 55.36095292], [  0.46411479], [ -0.66883535], [  0.38989939]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        u = np.array([ 26.55951566])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        e = np.array([ 31.23925425])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,4)
        predy = np.array([ 53.9074875])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 3
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,4)
        x = np.array([  1.   ,  19.531])
        np.testing.assert_array_almost_equal(reg.x[0],x,4)
        yend = np.array([  15.72598])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,4)
        z = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.z[0],z,4)
        my = 38.43622446938776
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([[  5.29158422e+02,  -1.57833675e+01,  -8.38021080e+00],
       [ -1.57833675e+01,   5.40235041e-01,   2.31120327e-01],
       [ -8.38021080e+00,   2.31120327e-01,   1.44977385e-01]])
        np.testing.assert_array_almost_equal(reg.vm,vm,4)
        pr2 = 0.346472557570858
        self.assertAlmostEqual(reg.pr2,pr2)
        sig2 = 192.50022721929574
        self.assertAlmostEqual(reg.sig2,sig2,4)
        std_err = np.array([ 23.003401  ,   0.73500657,   0.38075777])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,4)
        z_stat = np.array([[ 2.40664208,  0.01609994], [ 0.63144305,  0.52775088], [-1.75659016,  0.07898769]])
        np.testing.assert_array_almost_equal(reg.z_stat,z_stat,4)

class TestBaseGMCombo(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        # Only spatial lag
        yd2, q2 = pysal.spreg.utils.set_endog(self.y, self.X, self.w, None, None, 1, True)
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        reg = SP.BaseGM_Combo(self.y, self.X, yend=yd2, q=q2, w=self.w.sparse)
        betas = np.array([[ 57.61123461],[  0.73441314], [ -0.59459416], [ -0.21762921], [  0.54732051]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        u = np.array([ 25.57932637])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        e_filtered = np.array([ 31.65374945])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e_filtered,4)
        predy = np.array([ 54.88767663])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 4
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,4)
        x = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x[0],x,4)
        yend = np.array([  35.4585005])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,4)
        z = np.array([  1.       ,  19.531    ,  15.72598  ,  35.4585005])
        np.testing.assert_array_almost_equal(reg.z[0],z,4)
        my = 38.43622446938776
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([  5.22438365e+02,   2.38012873e-01,   3.20924172e-02,
         2.15753599e-01])
        np.testing.assert_array_almost_equal(np.diag(reg.vm),vm,4)
        sig2 = 181.78650186468832
        self.assertAlmostEqual(reg.sig2,sig2,4)

class TestGMCombo(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
    def test_model(self):
        # Only spatial lag
        reg = SP.GM_Combo(self.y, self.X, w=self.w)
        e_reduced = np.array([ 28.18617481])
        np.testing.assert_array_almost_equal(reg.e_pred[0],e_reduced,4)
        predy_e = np.array([ 52.28082782])
        np.testing.assert_array_almost_equal(reg.predy_e[0],predy_e,4)
        betas = np.array([[ 57.61123515],[  0.73441313], [ -0.59459416], [ -0.21762921], [  0.54732051]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        u = np.array([ 25.57932637])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        e_filtered = np.array([ 31.65374945])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e_filtered,4)
        predy = np.array([ 54.88767685])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 4
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,4)
        x = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x[0],x,4)
        yend = np.array([  35.4585005])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,4)
        z = np.array([  1.       ,  19.531    ,  15.72598  ,  35.4585005])
        np.testing.assert_array_almost_equal(reg.z[0],z,4)
        my = 38.43622446938776
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([  5.22438333e+02,   2.38012875e-01,   3.20924173e-02,
         2.15753579e-01])
        np.testing.assert_array_almost_equal(np.diag(reg.vm),vm,4)
        sig2 = 181.78650186468832
        self.assertAlmostEqual(reg.sig2,sig2,4)
        pr2 = 0.3018280166937799
        self.assertAlmostEqual(reg.pr2,pr2,4)
        pr2_e = 0.3561355586759414
        self.assertAlmostEqual(reg.pr2_e,pr2_e,4)
        std_err = np.array([ 22.85692222,  0.48786559,  0.17914356,  0.46449318])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,4)
        z_stat = np.array([[  2.52051597e+00,   1.17182922e-02], [  1.50535954e+00,   1.32231664e-01], [ -3.31909311e+00,   9.03103123e-04], [ -4.68530506e-01,   6.39405261e-01]])
        np.testing.assert_array_almost_equal(reg.z_stat,z_stat,4)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_error_sp_het
import unittest
import pysal
import numpy as np
from pysal.spreg import error_sp_het as HET

class TestBaseGMErrorHet(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        reg = HET.BaseGM_Error_Het(self.y, self.X, self.w.sparse, step1c=True)
        betas = np.array([[ 47.99626638], [  0.71048989], [ -0.55876126], [  0.41178776]])
        np.testing.assert_array_almost_equal(reg.betas,betas,7)
        u = np.array([ 27.38122697])
        np.testing.assert_array_almost_equal(reg.u[0],u,7)
        ef = np.array([ 32.29765975])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],ef,7)
        predy = np.array([ 53.08577603])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,7)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 3
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,7)
        x = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x[0],x,7)
        i_s = 'Maximum number of iterations reached.'
        np.testing.assert_string_equal(reg.iter_stop,i_s)
        its = 1
        self.assertAlmostEqual(reg.iteration,its,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        stdy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,stdy)
        vm = np.array([[  1.31767529e+02,  -3.58368748e+00,  -1.65090647e+00,
              0.00000000e+00],
           [ -3.58368748e+00,   1.35513711e-01,   3.77539055e-02,
              0.00000000e+00],
           [ -1.65090647e+00,   3.77539055e-02,   2.61042702e-02,
              0.00000000e+00],
           [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
              2.82398517e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        xtx = np.array([[  4.90000000e+01,   7.04371999e+02,   1.72131237e+03],
           [  7.04371999e+02,   1.16866734e+04,   2.15575320e+04],
           [  1.72131237e+03,   2.15575320e+04,   7.39058986e+04]])
        np.testing.assert_array_almost_equal(reg.xtx,xtx,4)
             
class TestGMErrorHet(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        reg = HET.GM_Error_Het(self.y, self.X, self.w, step1c=True)
        betas = np.array([[ 47.99626638], [  0.71048989], [ -0.55876126], [  0.41178776]])
        np.testing.assert_array_almost_equal(reg.betas,betas,7)
        u = np.array([ 27.38122697])
        np.testing.assert_array_almost_equal(reg.u[0],u,7)
        ef = np.array([ 32.29765975])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],ef,7)
        predy = np.array([ 53.08577603])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,7)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 3
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,7)
        x = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x[0],x,7)
        i_s = 'Maximum number of iterations reached.'
        np.testing.assert_string_equal(reg.iter_stop,i_s)
        its = 1
        self.assertAlmostEqual(reg.iteration,its,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        stdy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,stdy)
        vm = np.array([[  1.31767529e+02,  -3.58368748e+00,  -1.65090647e+00,
              0.00000000e+00],
           [ -3.58368748e+00,   1.35513711e-01,   3.77539055e-02,
              0.00000000e+00],
           [ -1.65090647e+00,   3.77539055e-02,   2.61042702e-02,
              0.00000000e+00],
           [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
              2.82398517e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        pr2 = 0.34951013222581306
        self.assertAlmostEqual(reg.pr2,pr2)
        stde = np.array([ 11.47900385,   0.36812187,   0.16156816,   0.16804717])
        np.testing.assert_array_almost_equal(reg.std_err,stde,4)
        z_stat = np.array([[  4.18122226e+00,   2.89946274e-05],
           [  1.93003988e+00,   5.36018970e-02],
           [ -3.45836247e+00,   5.43469673e-04],
           [  2.45042960e+00,   1.42685863e-02]])
        np.testing.assert_array_almost_equal(reg.z_stat,z_stat,4)
        xtx = np.array([[  4.90000000e+01,   7.04371999e+02,   1.72131237e+03],
           [  7.04371999e+02,   1.16866734e+04,   2.15575320e+04],
           [  1.72131237e+03,   2.15575320e+04,   7.39058986e+04]])
        np.testing.assert_array_almost_equal(reg.xtx,xtx,4)

class TestBaseGMEndogErrorHet(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        self.X = np.array(X).T
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        yd = []
        yd.append(db.by_col("CRIME"))
        self.yd = np.array(yd).T
        q = []
        q.append(db.by_col("DISCBD"))
        self.q = np.array(q).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        reg = HET.BaseGM_Endog_Error_Het(self.y, self.X, self.yd, self.q, self.w.sparse, step1c=True)
        betas = np.array([[ 55.39707924], [  0.46563046], [ -0.67038326], [  0.41135023]])
        np.testing.assert_array_almost_equal(reg.betas,betas,7)
        u = np.array([ 26.51812895])
        np.testing.assert_array_almost_equal(reg.u[0],u,7)
        ef = np.array([ 31.46604707])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],ef,7)
        predy = np.array([ 53.94887405])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,7)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 3
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,7)
        x = np.array([  1.   ,  19.531])
        np.testing.assert_array_almost_equal(reg.x[0],x,7)
        yend = np.array([ 15.72598])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,7)
        q = np.array([ 5.03])
        np.testing.assert_array_almost_equal(reg.q[0],q,7)
        z = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.z[0],z,7)
        h = np.array([  1.   ,  19.531,   5.03 ])
        np.testing.assert_array_almost_equal(reg.h[0],h,7)
        i_s = 'Maximum number of iterations reached.'
        np.testing.assert_string_equal(reg.iter_stop,i_s)
        its = 1
        self.assertAlmostEqual(reg.iteration,its,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        stdy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,stdy)
        vm = np.array([[  8.34637805e+02,  -2.16932259e+01,  -1.33327894e+01,
                  1.65840848e+00],
               [ -2.16932259e+01,   5.97683070e-01,   3.39503523e-01,
                 -3.90111107e-02],
               [ -1.33327894e+01,   3.39503523e-01,   2.19008080e-01,
                 -2.81929695e-02],
               [  1.65840848e+00,  -3.90111107e-02,  -2.81929695e-02,
                  3.15686105e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        hth = np.array([[    49.        ,    704.371999  ,    139.75      ],
               [   704.371999  ,  11686.67338121,   2246.12800625],
               [   139.75      ,   2246.12800625,    498.5851    ]])
        np.testing.assert_array_almost_equal(reg.hth,hth,6)
        
class TestGMEndogErrorHet(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        self.X = np.array(X).T
        yd = []
        yd.append(db.by_col("CRIME"))
        self.yd = np.array(yd).T
        q = []
        q.append(db.by_col("DISCBD"))
        self.q = np.array(q).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        reg = HET.GM_Endog_Error_Het(self.y, self.X, self.yd, self.q, self.w, step1c=True)
        betas = np.array([[ 55.39707924], [  0.46563046], [ -0.67038326], [  0.41135023]])
        np.testing.assert_array_almost_equal(reg.betas,betas,7)
        u = np.array([ 26.51812895])
        np.testing.assert_array_almost_equal(reg.u[0],u,7)
        predy = np.array([ 53.94887405])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,7)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 3
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,7)
        x = np.array([  1.   ,  19.531])
        np.testing.assert_array_almost_equal(reg.x[0],x,7)
        yend = np.array([ 15.72598])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,7)
        q = np.array([ 5.03])
        np.testing.assert_array_almost_equal(reg.q[0],q,7)
        z = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.z[0],z,7)
        h = np.array([  1.   ,  19.531,   5.03 ])
        np.testing.assert_array_almost_equal(reg.h[0],h,7)
        i_s = 'Maximum number of iterations reached.'
        np.testing.assert_string_equal(reg.iter_stop,i_s)
        its = 1
        self.assertAlmostEqual(reg.iteration,its,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        stdy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,stdy)
        vm = np.array([[  8.34637805e+02,  -2.16932259e+01,  -1.33327894e+01,
                  1.65840848e+00],
               [ -2.16932259e+01,   5.97683070e-01,   3.39503523e-01,
                 -3.90111107e-02],
               [ -1.33327894e+01,   3.39503523e-01,   2.19008080e-01,
                 -2.81929695e-02],
               [  1.65840848e+00,  -3.90111107e-02,  -2.81929695e-02,
                  3.15686105e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        pr2 = 0.34648011338954804
        self.assertAlmostEqual(reg.pr2,pr2,7)
        std_err = np.array([ 28.89009873,  0.77309965,  0.46798299,
            0.17767558])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,6)
        z_stat = np.array([(1.9175109006819244, 0.055173057472126787), (0.60229035155742305, 0.54698088217644414), (-1.4324949211864271, 0.15200223057569454), (2.3151759776869496, 0.020603303355572443)])
        np.testing.assert_array_almost_equal(reg.z_stat,z_stat,6)
        hth = np.array([[    49.        ,    704.371999  ,    139.75      ],
               [   704.371999  ,  11686.67338121,   2246.12800625],
               [   139.75      ,   2246.12800625,    498.5851    ]])
        np.testing.assert_array_almost_equal(reg.hth,hth,6)
 
class TestBaseGMComboHet(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        # Only spatial lag
        yd2, q2 = pysal.spreg.utils.set_endog(self.y, self.X, self.w, None, None, 1, True)
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        reg = HET.BaseGM_Combo_Het(self.y, self.X, yend=yd2, q=q2, w=self.w.sparse, step1c=True)
        betas = np.array([[ 57.7778574 ], [  0.73034922], [ -0.59257362], [ -0.2230231 ], [  0.56636724]])
        np.testing.assert_array_almost_equal(reg.betas,betas,7)
        u = np.array([ 25.65156033])
        np.testing.assert_array_almost_equal(reg.u[0],u,7)
        ef = np.array([ 31.87664403])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],ef,7)
        predy = np.array([ 54.81544267])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,7)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 4
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,7)
        x = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x[0],x,7)
        yend = np.array([ 35.4585005])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,7)
        q = np.array([ 18.594    ,  24.7142675])
        np.testing.assert_array_almost_equal(reg.q[0],q,7)
        z = np.array([  1.       ,  19.531    ,  15.72598  ,  35.4585005])
        np.testing.assert_array_almost_equal(reg.z[0],z,7)
        i_s = 'Maximum number of iterations reached.'
        np.testing.assert_string_equal(reg.iter_stop,i_s)
        its = 1
        self.assertAlmostEqual(reg.iteration,its,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        stdy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,stdy,7)
        vm = np.array([[  4.86218274e+02,  -2.77268729e+00,  -1.59987770e+00,
             -1.01969471e+01,   2.74302006e+00],
           [ -2.77268729e+00,   1.04680972e-01,   2.51172238e-02,
              1.95136385e-03,   3.70052723e-03],
           [ -1.59987770e+00,   2.51172238e-02,   2.15655720e-02,
              7.65868344e-03,  -7.30173070e-03],
           [ -1.01969471e+01,   1.95136385e-03,   7.65868344e-03,
              2.78273684e-01,  -6.89402590e-02],
           [  2.74302006e+00,   3.70052723e-03,  -7.30173070e-03,
             -6.89402590e-02,   7.12034037e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        hth = np.array([[  4.90000000e+01,   7.04371999e+02,   1.72131237e+03,
              7.24743592e+02,   1.70735413e+03],
           [  7.04371999e+02,   1.16866734e+04,   2.15575320e+04,
              1.10925200e+04,   2.23848036e+04],
           [  1.72131237e+03,   2.15575320e+04,   7.39058986e+04,
              2.34796298e+04,   6.70145378e+04],
           [  7.24743592e+02,   1.10925200e+04,   2.34796298e+04,
              1.16146226e+04,   2.30304624e+04],
           [  1.70735413e+03,   2.23848036e+04,   6.70145378e+04,
              2.30304624e+04,   6.69879858e+04]])
        np.testing.assert_array_almost_equal(reg.hth,hth,4)

class TestGMComboHet(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        # Only spatial lag
        reg = HET.GM_Combo_Het(self.y, self.X, w=self.w, step1c=True)
        betas = np.array([[ 57.7778574 ], [  0.73034922], [ -0.59257362], [ -0.2230231 ], [  0.56636724]])
        np.testing.assert_array_almost_equal(reg.betas,betas,7)
        u = np.array([ 25.65156033])
        np.testing.assert_array_almost_equal(reg.u[0],u,7)
        ef = np.array([ 31.87664403])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],ef,7)
        ep = np.array([ 28.30648145])
        np.testing.assert_array_almost_equal(reg.e_pred[0],ep,7)
        pe = np.array([ 52.16052155])
        np.testing.assert_array_almost_equal(reg.predy_e[0],pe,7)
        predy = np.array([ 54.81544267])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,7)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 4
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,7)
        x = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x[0],x,7)
        yend = np.array([ 35.4585005])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,7)
        q = np.array([ 18.594    ,  24.7142675])
        np.testing.assert_array_almost_equal(reg.q[0],q,7)
        z = np.array([  1.       ,  19.531    ,  15.72598  ,  35.4585005])
        np.testing.assert_array_almost_equal(reg.z[0],z,7)
        i_s = 'Maximum number of iterations reached.'
        np.testing.assert_string_equal(reg.iter_stop,i_s)
        its = 1
        self.assertAlmostEqual(reg.iteration,its,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        stdy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,stdy)
        vm = np.array([[  4.86218274e+02,  -2.77268729e+00,  -1.59987770e+00,
             -1.01969471e+01,   2.74302006e+00],
           [ -2.77268729e+00,   1.04680972e-01,   2.51172238e-02,
              1.95136385e-03,   3.70052723e-03],
           [ -1.59987770e+00,   2.51172238e-02,   2.15655720e-02,
              7.65868344e-03,  -7.30173070e-03],
           [ -1.01969471e+01,   1.95136385e-03,   7.65868344e-03,
              2.78273684e-01,  -6.89402590e-02],
           [  2.74302006e+00,   3.70052723e-03,  -7.30173070e-03,
             -6.89402590e-02,   7.12034037e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        pr2 = 0.3001582877472412
        self.assertAlmostEqual(reg.pr2,pr2,7)
        pr2_e = 0.35613102283621967
        self.assertAlmostEqual(reg.pr2_e,pr2_e,7)
        std_err = np.array([ 22.05035768,  0.32354439,  0.14685221,  0.52751653,  0.26683966])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,6)
        z_stat = np.array([(2.6202684885795335, 0.00878605635338265), (2.2573385444145524, 0.023986928627746887), (-4.0351698589183433, 5.456281036278686e-05), (-0.42277935292121521, 0.67245625315942159), (2.1225002455741895, 0.033795752094112265)])
        np.testing.assert_array_almost_equal(reg.z_stat,z_stat,6)
        hth = np.array([[  4.90000000e+01,   7.04371999e+02,   1.72131237e+03,
              7.24743592e+02,   1.70735413e+03],
           [  7.04371999e+02,   1.16866734e+04,   2.15575320e+04,
              1.10925200e+04,   2.23848036e+04],
           [  1.72131237e+03,   2.15575320e+04,   7.39058986e+04,
              2.34796298e+04,   6.70145378e+04],
           [  7.24743592e+02,   1.10925200e+04,   2.34796298e+04,
              1.16146226e+04,   2.30304624e+04],
           [  1.70735413e+03,   2.23848036e+04,   6.70145378e+04,
              2.30304624e+04,   6.69879858e+04]])
        np.testing.assert_array_almost_equal(reg.hth,hth,4)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_error_sp_het_regimes
import unittest
import pysal
import numpy as np
from pysal.spreg import error_sp_het_regimes as SP
from pysal.spreg.error_sp_het import GM_Error_Het, GM_Endog_Error_Het, GM_Combo_Het

class TestGM_Error_Het_Regimes(unittest.TestCase):
    def setUp(self):
        #Columbus:
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("CRIME"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("HOVAL"))
        X.append(db.by_col("INC"))
        self.X = np.array(X).T
        X2 = []
        X2.append(db.by_col("INC"))
        self.X2 = np.array(X2).T
        yd = []
        yd.append(db.by_col("HOVAL"))
        self.yd = np.array(yd).T
        q = []
        q.append(db.by_col("DISCBD"))
        self.q = np.array(q).T
        self.w = pysal.queen_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
        self.r_var = 'NSA'
        self.regimes = db.by_col(self.r_var)
        #Artficial:
        n = 256
        self.n2 = n/2
        self.x_a1 = np.random.uniform(-10,10,(n,1))
        self.x_a2 = np.random.uniform(1,5,(n,1))
        self.q_a = self.x_a2 + np.random.normal(0,1,(n,1))
        self.x_a = np.hstack((self.x_a1,self.x_a2))
        self.y_a = np.dot(np.hstack((np.ones((n,1)),self.x_a)),np.array([[1],[0.5],[2]])) + np.random.normal(0,1,(n,1))
        latt = int(np.sqrt(n))
        self.w_a = pysal.lat2W(latt,latt)
        self.w_a.transform='r'
        self.regi_a = [0]*(n/2) + [1]*(n/2)
        self.w_a1 = pysal.lat2W(latt/2,latt)
        self.w_a1.transform='r'
        
    def test_model(self):
        reg = SP.GM_Error_Het_Regimes(self.y, self.X, self.regimes, self.w)
        betas = np.array([[ 62.95986466],
       [ -0.15660795],
       [ -1.49054832],
       [ 60.98577615],
       [ -0.3358993 ],
       [ -0.82129289],
       [  0.54662719]])
        np.testing.assert_array_almost_equal(reg.betas,betas,6)
        u = np.array([-2.19031456])
        np.testing.assert_array_almost_equal(reg.u[0],u,6)
        predy = np.array([ 17.91629456])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,6)
        n = 49
        self.assertAlmostEqual(reg.n,n,6)
        k = 6
        self.assertAlmostEqual(reg.k,k,6)
        y = np.array([ 15.72598])
        np.testing.assert_array_almost_equal(reg.y[0],y,6)
        x = np.array([[  0.      ,   0.      ,   0.      ,   1.      ,  80.467003,  19.531   ]])
        np.testing.assert_array_almost_equal(reg.x[0].toarray(),x,6)
        e = np.array([ 2.77847355])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,6)
        my = 35.128823897959187
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 16.732092091229699
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([  3.86154100e+01,  -2.51553730e-01,  -8.20138673e-01,
         1.71714184e+00,  -1.94929113e-02,   1.23118051e-01,
         0.00000000e+00])
        np.testing.assert_array_almost_equal(reg.vm[0],vm,6)
        pr2 = 0.5515791216043385
        self.assertAlmostEqual(reg.pr2,pr2)
        std_err = np.array([ 6.21412987,  0.15340022,  0.44060473,  7.6032169 ,  0.19353719,
        0.73621596,  0.13968272])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,6)
        chow_r = np.array([[ 0.04190799,  0.83779526],
       [ 0.5736724 ,  0.44880328],
       [ 0.62498575,  0.42920056]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,6)
        chow_j = 0.72341901308525713
        self.assertAlmostEqual(reg.chow.joint[0],chow_j)

    def test_model_regi_error(self):
        #Columbus:
        reg = SP.GM_Error_Het_Regimes(self.y, self.X, self.regimes, self.w, regime_err_sep=True)
        betas = np.array([[ 60.74090229],
       [ -0.17492294],
       [ -1.33383387],
       [  0.68303064],
       [ 66.30374279],
       [ -0.31841139],
       [ -1.27502813],
       [  0.11515312]])
        np.testing.assert_array_almost_equal(reg.betas,betas,6)
        vm = np.array([ 44.9411672 ,  -0.34343354,  -0.39946055,   0.        ,
         0.        ,   0.        ,   0.        ,   0.        ])
        np.testing.assert_array_almost_equal(reg.vm[0],vm,6)
        u = np.array([-0.05357818])
        np.testing.assert_array_almost_equal(reg.u[0],u,6)
        predy = np.array([ 15.77955818])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,6)
        e = np.array([ 0.70542044])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,6)
        chow_r = np.array([[  3.11061225e-01,   5.77029704e-01],
       [  3.39747489e-01,   5.59975012e-01],
       [  3.86371771e-03,   9.50436364e-01],
       [  4.02884201e+00,   4.47286322e-02]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,6)
        chow_j = 4.7467070503995412
        self.assertAlmostEqual(reg.chow.joint[0],chow_j)
        #Artficial:
        model = SP.GM_Error_Het_Regimes(self.y_a, self.x_a, self.regi_a, w=self.w_a, regime_err_sep=True)
        model1 = GM_Error_Het(self.y_a[0:(self.n2)].reshape((self.n2),1), self.x_a[0:(self.n2)], w=self.w_a1)
        model2 = GM_Error_Het(self.y_a[(self.n2):].reshape((self.n2),1), self.x_a[(self.n2):], w=self.w_a1)
        tbetas = np.vstack((model1.betas, model2.betas))
        np.testing.assert_array_almost_equal(model.betas,tbetas)
        vm = np.hstack((model1.vm.diagonal(),model2.vm.diagonal()))
        np.testing.assert_array_almost_equal(model.vm.diagonal(), vm, 6)

    def test_model_endog(self):
        reg = SP.GM_Endog_Error_Het_Regimes(self.y, self.X2, self.yd, self.q, self.regimes, self.w)
        betas = np.array([[ 77.26679984],
       [  4.45992905],
       [ 78.59534391],
       [  0.41432319],
       [ -3.20196286],
       [ -1.13672283],
       [  0.2174965 ]])
        np.testing.assert_array_almost_equal(reg.betas,betas,6)
        u = np.array([ 20.50716917])
        np.testing.assert_array_almost_equal(reg.u[0],u,6)
        e = np.array([ 25.13517175])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,6)
        predy = np.array([-4.78118917])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,6)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 6
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 15.72598])
        np.testing.assert_array_almost_equal(reg.y[0],y,6)
        x = np.array([[  0.   ,   0.   ,   1.   ,  19.531]])
        np.testing.assert_array_almost_equal(reg.x[0].toarray(),x,6)
        yend = np.array([[  0.      ,  80.467003]])
        np.testing.assert_array_almost_equal(reg.yend[0].toarray(),yend,6)
        z = np.array([[  0.      ,   0.      ,   1.      ,  19.531   ,   0.      ,
         80.467003]])
        np.testing.assert_array_almost_equal(reg.z[0].toarray(),z,6)
        my = 35.128823897959187
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 16.732092091229699
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([ 509.66122149,  150.5845341 ,    9.64413821,    5.54782831,
        -80.95846045,   -2.25308524,   -3.2045214 ])
        np.testing.assert_array_almost_equal(reg.vm[0],vm,5)
        pr2 = 0.19776512679331681
        self.assertAlmostEqual(reg.pr2,pr2)
        std_err = np.array([ 22.57567765,  11.34616946,  17.43881791,   1.30953812,
         5.4830829 ,   0.74634612,   0.29973079])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,6)
        chow_r = np.array([[ 0.0022216 ,  0.96240654],
       [ 0.13127347,  0.7171153 ],
       [ 0.14367307,  0.70465645]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,6)
        chow_j = 1.2329971019087163
        self.assertAlmostEqual(reg.chow.joint[0],chow_j)

    def test_model_endog_regi_error(self):
        #Columbus:
        reg = SP.GM_Endog_Error_Het_Regimes(self.y, self.X2, self.yd, self.q, self.regimes, self.w, regime_err_sep=True)
        betas = np.array([[  7.92747424e+01],
       [  5.78086230e+00],
       [ -3.83173581e+00],
       [  2.23210962e-01],
       [  8.26255251e+01],
       [  5.48294187e-01],
       [ -1.28432891e+00],
       [  3.57661629e-02]])
        np.testing.assert_array_almost_equal(reg.betas,betas,6)
        vm = np.array([  7.55988579e+02,   2.53659722e+02,  -1.34288316e+02,
        -2.66141766e-01,   0.00000000e+00,   0.00000000e+00,
         0.00000000e+00,   0.00000000e+00])
        np.testing.assert_array_almost_equal(reg.vm[0],vm,6)
        u = np.array([ 25.73781918])
        np.testing.assert_array_almost_equal(reg.u[0],u,6)
        predy = np.array([-10.01183918])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,6)
        e = np.array([ 26.5449135])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,6)
        chow_r = np.array([[ 0.00998573,  0.92040097],
       [ 0.12660165,  0.72198192],
       [ 0.12737281,  0.72117171],
       [ 0.43507956,  0.50950696]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,6)
        chow_j = 1.3756768204399892
        self.assertAlmostEqual(reg.chow.joint[0],chow_j)
        #Artficial:
        model = SP.GM_Endog_Error_Het_Regimes(self.y_a, self.x_a1, yend=self.x_a2, q=self.q_a, regimes=self.regi_a, w=self.w_a, regime_err_sep=True)
        model1 = GM_Endog_Error_Het(self.y_a[0:(self.n2)].reshape((self.n2),1), self.x_a1[0:(self.n2)], yend=self.x_a2[0:(self.n2)], q=self.q_a[0:(self.n2)], w=self.w_a1)
        model2 = GM_Endog_Error_Het(self.y_a[(self.n2):].reshape((self.n2),1), self.x_a1[(self.n2):], yend=self.x_a2[(self.n2):], q=self.q_a[(self.n2):], w=self.w_a1)
        tbetas = np.vstack((model1.betas, model2.betas))
        np.testing.assert_array_almost_equal(model.betas,tbetas)
        vm = np.hstack((model1.vm.diagonal(),model2.vm.diagonal()))
        np.testing.assert_array_almost_equal(model.vm.diagonal(), vm, 6)

    def test_model_combo(self):
        reg = SP.GM_Combo_Het_Regimes(self.y, self.X2, self.regimes, self.yd, self.q, w=self.w)
        betas = np.array([[  3.69372678e+01],
       [ -8.29474998e-01],
       [  3.08667517e+01],
       [ -7.23753444e-01],
       [ -3.01900940e-01],
       [ -2.21328949e-01],
       [  6.41902155e-01],
       [ -2.45714919e-02]])
        np.testing.assert_array_almost_equal(reg.betas,betas,6)
        u = np.array([ 0.94039246])
        np.testing.assert_array_almost_equal(reg.u[0],u,6)
        e_filtered = np.array([ 0.8737864])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e_filtered,5)
        predy_e = np.array([ 18.68732105])
        np.testing.assert_array_almost_equal(reg.predy_e[0],predy_e,6)
        predy = np.array([ 14.78558754])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,6)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 7
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 15.72598])
        np.testing.assert_array_almost_equal(reg.y[0],y,6)
        x = np.array([[  0.   ,   0.   ,   1.   ,  19.531]])
        np.testing.assert_array_almost_equal(reg.x[0].toarray(),x,6)
        yend = np.array([[  0.       ,  80.467003 ,  24.7142675]])
        np.testing.assert_array_almost_equal(reg.yend[0].toarray(),yend,6)
        z = np.array([[  0.       ,   0.       ,   1.       ,  19.531    ,   0.       ,
         80.467003 ,  24.7142675]])
        np.testing.assert_array_almost_equal(reg.z[0].toarray(),z,6)
        my = 35.128823897959187
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 16.732092091229699
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([ 71.26851365,  -0.58278032,  50.53169815,  -0.74561147,
        -0.79510274,  -0.10823496,  -0.98141395,   1.16575965])
        np.testing.assert_array_almost_equal(reg.vm[0],vm,4)
        pr2 = 0.6504148883602958
        self.assertAlmostEqual(reg.pr2,pr2)
        pr2_e = 0.527136896994038
        self.assertAlmostEqual(reg.pr2_e,pr2_e)
        std_err = np.array([ 8.44206809,  0.72363219,  9.85790968,  0.77218082,  0.34084146,
        0.21752916,  0.14371614,  0.39226478])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,5)
        chow_r = np.array([[ 0.54688708,  0.45959243],
       [ 0.01035136,  0.91896175],
       [ 0.03981108,  0.84185042]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,6)
        chow_j = 0.78070369988354349
        self.assertAlmostEqual(reg.chow.joint[0],chow_j)

    def test_model_combo_regi_error(self):
        #Columbus:
        reg = SP.GM_Combo_Het_Regimes(self.y, self.X2, self.regimes, self.yd, self.q, w=self.w, regime_lag_sep=True, regime_err_sep=True)
        betas = np.array([[ 42.01151458],
       [ -0.13917151],
       [ -0.65300184],
       [  0.54737064],
       [  0.2629229 ],
       [ 34.21569751],
       [ -0.15236089],
       [ -0.49175217],
       [  0.65733173],
       [ -0.07713581]])
        np.testing.assert_array_almost_equal(reg.betas,betas,6)
        vm = np.array([ 77.49519689,   0.57226879,  -1.18856422,  -1.28088712,
         0.866752  ,   0.        ,   0.        ,   0.        ,
         0.        ,   0.        ])
        np.testing.assert_array_almost_equal(reg.vm[0],vm,6)
        u = np.array([ 7.81039418])
        np.testing.assert_array_almost_equal(reg.u[0],u,6)
        predy = np.array([ 7.91558582])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,6)
        e = np.array([ 7.22996911])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,6)
        chow_r = np.array([[  1.90869079e-01,   6.62194273e-01],
       [  4.56118982e-05,   9.94611401e-01],
       [  3.12104263e-02,   8.59771748e-01],
       [  1.56368204e-01,   6.92522476e-01],
       [  7.52928732e-01,   3.85550558e-01]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,6)
        chow_j = 1.1316136604755913
        self.assertAlmostEqual(reg.chow.joint[0],chow_j)
        #Artficial:
        model = SP.GM_Combo_Het_Regimes(self.y_a, self.x_a1, yend=self.x_a2, q=self.q_a, regimes=self.regi_a, w=self.w_a, regime_err_sep=True, regime_lag_sep=True)
        model1 = GM_Combo_Het(self.y_a[0:(self.n2)].reshape((self.n2),1), self.x_a1[0:(self.n2)], yend=self.x_a2[0:(self.n2)], q=self.q_a[0:(self.n2)], w=self.w_a1)
        model2 = GM_Combo_Het(self.y_a[(self.n2):].reshape((self.n2),1), self.x_a1[(self.n2):], yend=self.x_a2[(self.n2):], q=self.q_a[(self.n2):], w=self.w_a1)
        tbetas = np.vstack((model1.betas, model2.betas))
        np.testing.assert_array_almost_equal(model.betas,tbetas)
        vm = np.hstack((model1.vm.diagonal(),model2.vm.diagonal()))


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_error_sp_het_sparse
import unittest
import pysal
import numpy as np
from scipy import sparse
from pysal.spreg import error_sp_het as HET

class TestBaseGMErrorHet(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.X = sparse.csr_matrix(self.X)
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        reg = HET.BaseGM_Error_Het(self.y, self.X, self.w.sparse, step1c=True)
        betas = np.array([[ 47.99626638], [  0.71048989], [ -0.55876126], [  0.41178776]])
        np.testing.assert_array_almost_equal(reg.betas,betas,7)
        u = np.array([ 27.38122697])
        np.testing.assert_array_almost_equal(reg.u[0],u,7)
        ef = np.array([ 32.29765975])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],ef,7)
        predy = np.array([ 53.08577603])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,7)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 3
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,7)
        x = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x[0].toarray()[0],x,7)
        i_s = 'Maximum number of iterations reached.'
        np.testing.assert_string_equal(reg.iter_stop,i_s)
        its = 1
        self.assertAlmostEqual(reg.iteration,its,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        stdy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,stdy)
        vm = np.array([[  1.31767529e+02,  -3.58368748e+00,  -1.65090647e+00,
              0.00000000e+00],
           [ -3.58368748e+00,   1.35513711e-01,   3.77539055e-02,
              0.00000000e+00],
           [ -1.65090647e+00,   3.77539055e-02,   2.61042702e-02,
              0.00000000e+00],
           [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
              2.82398517e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        xtx = np.array([[  4.90000000e+01,   7.04371999e+02,   1.72131237e+03],
           [  7.04371999e+02,   1.16866734e+04,   2.15575320e+04],
           [  1.72131237e+03,   2.15575320e+04,   7.39058986e+04]])
        np.testing.assert_array_almost_equal(reg.xtx,xtx,4)
             
class TestGMErrorHet(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.X = sparse.csr_matrix(self.X)
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        reg = HET.GM_Error_Het(self.y, self.X, self.w, step1c=True)
        betas = np.array([[ 47.99626638], [  0.71048989], [ -0.55876126], [  0.41178776]])
        np.testing.assert_array_almost_equal(reg.betas,betas,7)
        u = np.array([ 27.38122697])
        np.testing.assert_array_almost_equal(reg.u[0],u,7)
        ef = np.array([ 32.29765975])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],ef,7)
        predy = np.array([ 53.08577603])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,7)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 3
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,7)
        x = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x[0].toarray()[0],x,7)
        i_s = 'Maximum number of iterations reached.'
        np.testing.assert_string_equal(reg.iter_stop,i_s)
        its = 1
        self.assertAlmostEqual(reg.iteration,its,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        stdy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,stdy)
        vm = np.array([[  1.31767529e+02,  -3.58368748e+00,  -1.65090647e+00,
              0.00000000e+00],
           [ -3.58368748e+00,   1.35513711e-01,   3.77539055e-02,
              0.00000000e+00],
           [ -1.65090647e+00,   3.77539055e-02,   2.61042702e-02,
              0.00000000e+00],
           [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
              2.82398517e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        pr2 = 0.34951013222581306
        self.assertAlmostEqual(reg.pr2,pr2)
        stde = np.array([ 11.47900385,   0.36812187,   0.16156816,   0.16804717])
        np.testing.assert_array_almost_equal(reg.std_err,stde,4)
        z_stat = np.array([[  4.18122226e+00,   2.89946274e-05],
           [  1.93003988e+00,   5.36018970e-02],
           [ -3.45836247e+00,   5.43469673e-04],
           [  2.45042960e+00,   1.42685863e-02]])
        np.testing.assert_array_almost_equal(reg.z_stat,z_stat,4)
        xtx = np.array([[  4.90000000e+01,   7.04371999e+02,   1.72131237e+03],
           [  7.04371999e+02,   1.16866734e+04,   2.15575320e+04],
           [  1.72131237e+03,   2.15575320e+04,   7.39058986e+04]])
        np.testing.assert_array_almost_equal(reg.xtx,xtx,4)

class TestBaseGMEndogErrorHet(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        self.X = np.array(X).T
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.X = sparse.csr_matrix(self.X)
        yd = []
        yd.append(db.by_col("CRIME"))
        self.yd = np.array(yd).T
        q = []
        q.append(db.by_col("DISCBD"))
        self.q = np.array(q).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        reg = HET.BaseGM_Endog_Error_Het(self.y, self.X, self.yd, self.q, self.w.sparse, step1c=True)
        betas = np.array([[ 55.39707924], [  0.46563046], [ -0.67038326], [  0.41135023]])
        np.testing.assert_array_almost_equal(reg.betas,betas,7)
        u = np.array([ 26.51812895])
        np.testing.assert_array_almost_equal(reg.u[0],u,7)
        ef = np.array([ 31.46604707])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],ef,7)
        predy = np.array([ 53.94887405])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,7)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 3
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,7)
        x = np.array([  1.   ,  19.531])
        np.testing.assert_array_almost_equal(reg.x[0].toarray()[0],x,7)
        yend = np.array([ 15.72598])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,7)
        q = np.array([ 5.03])
        np.testing.assert_array_almost_equal(reg.q[0],q,7)
        z = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.z[0].toarray()[0],z,7)
        h = np.array([  1.   ,  19.531,   5.03 ])
        np.testing.assert_array_almost_equal(reg.h[0].toarray()[0],h,7)
        i_s = 'Maximum number of iterations reached.'
        np.testing.assert_string_equal(reg.iter_stop,i_s)
        its = 1
        self.assertAlmostEqual(reg.iteration,its,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        stdy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,stdy)
        vm = np.array([[  8.34637805e+02,  -2.16932259e+01,  -1.33327894e+01,
                  1.65840848e+00],
               [ -2.16932259e+01,   5.97683070e-01,   3.39503523e-01,
                 -3.90111107e-02],
               [ -1.33327894e+01,   3.39503523e-01,   2.19008080e-01,
                 -2.81929695e-02],
               [  1.65840848e+00,  -3.90111107e-02,  -2.81929695e-02,
                  3.15686105e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        hth = np.array([[    49.        ,    704.371999  ,    139.75      ],
               [   704.371999  ,  11686.67338121,   2246.12800625],
               [   139.75      ,   2246.12800625,    498.5851    ]])
        np.testing.assert_array_almost_equal(reg.hth,hth,6)
        
class TestGMEndogErrorHet(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        self.X = np.array(X).T
        self.X = sparse.csr_matrix(self.X)
        yd = []
        yd.append(db.by_col("CRIME"))
        self.yd = np.array(yd).T
        q = []
        q.append(db.by_col("DISCBD"))
        self.q = np.array(q).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        reg = HET.GM_Endog_Error_Het(self.y, self.X, self.yd, self.q, self.w, step1c=True)
        betas = np.array([[ 55.39707924], [  0.46563046], [ -0.67038326], [  0.41135023]])
        np.testing.assert_array_almost_equal(reg.betas,betas,7)
        u = np.array([ 26.51812895])
        np.testing.assert_array_almost_equal(reg.u[0],u,7)
        predy = np.array([ 53.94887405])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,7)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 3
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,7)
        x = np.array([  1.   ,  19.531])
        np.testing.assert_array_almost_equal(reg.x[0].toarray()[0],x,7)
        yend = np.array([ 15.72598])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,7)
        q = np.array([ 5.03])
        np.testing.assert_array_almost_equal(reg.q[0],q,7)
        z = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.z[0].toarray()[0],z,7)
        h = np.array([  1.   ,  19.531,   5.03 ])
        np.testing.assert_array_almost_equal(reg.h[0].toarray()[0],h,7)
        i_s = 'Maximum number of iterations reached.'
        np.testing.assert_string_equal(reg.iter_stop,i_s)
        its = 1
        self.assertAlmostEqual(reg.iteration,its,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        stdy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,stdy)
        vm = np.array([[  8.34637805e+02,  -2.16932259e+01,  -1.33327894e+01,
                  1.65840848e+00],
               [ -2.16932259e+01,   5.97683070e-01,   3.39503523e-01,
                 -3.90111107e-02],
               [ -1.33327894e+01,   3.39503523e-01,   2.19008080e-01,
                 -2.81929695e-02],
               [  1.65840848e+00,  -3.90111107e-02,  -2.81929695e-02,
                  3.15686105e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        pr2 = 0.34648011338954804
        self.assertAlmostEqual(reg.pr2,pr2,7)
        std_err = np.array([ 28.89009873,  0.77309965,  0.46798299,
            0.17767558])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,6)
        z_stat = np.array([(1.9175109006819244, 0.055173057472126787), (0.60229035155742305, 0.54698088217644414), (-1.4324949211864271, 0.15200223057569454), (2.3151759776869496, 0.020603303355572443)])
        np.testing.assert_array_almost_equal(reg.z_stat,z_stat,6)
        hth = np.array([[    49.        ,    704.371999  ,    139.75      ],
               [   704.371999  ,  11686.67338121,   2246.12800625],
               [   139.75      ,   2246.12800625,    498.5851    ]])
        np.testing.assert_array_almost_equal(reg.hth,hth,6)
 
class TestBaseGMComboHet(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        # Only spatial lag
        yd2, q2 = pysal.spreg.utils.set_endog(self.y, self.X, self.w, None, None, 1, True)
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.X = sparse.csr_matrix(self.X)
        reg = HET.BaseGM_Combo_Het(self.y, self.X, yend=yd2, q=q2, w=self.w.sparse, step1c=True)
        betas = np.array([[ 57.7778574 ], [  0.73034922], [ -0.59257362], [ -0.2230231 ], [  0.56636724]])
        np.testing.assert_array_almost_equal(reg.betas,betas,7)
        u = np.array([ 25.65156033])
        np.testing.assert_array_almost_equal(reg.u[0],u,7)
        ef = np.array([ 31.87664403])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],ef,7)
        predy = np.array([ 54.81544267])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,7)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 4
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,7)
        x = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x[0].toarray()[0],x,7)
        yend = np.array([ 35.4585005])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,7)
        q = np.array([ 18.594    ,  24.7142675])
        np.testing.assert_array_almost_equal(reg.q[0],q,7)
        z = np.array([  1.       ,  19.531    ,  15.72598  ,  35.4585005])
        np.testing.assert_array_almost_equal(reg.z[0].toarray()[0],z,7)
        i_s = 'Maximum number of iterations reached.'
        np.testing.assert_string_equal(reg.iter_stop,i_s)
        its = 1
        self.assertAlmostEqual(reg.iteration,its,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        stdy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,stdy,7)
        vm = np.array([[  4.86218274e+02,  -2.77268729e+00,  -1.59987770e+00,
             -1.01969471e+01,   2.74302006e+00],
           [ -2.77268729e+00,   1.04680972e-01,   2.51172238e-02,
              1.95136385e-03,   3.70052723e-03],
           [ -1.59987770e+00,   2.51172238e-02,   2.15655720e-02,
              7.65868344e-03,  -7.30173070e-03],
           [ -1.01969471e+01,   1.95136385e-03,   7.65868344e-03,
              2.78273684e-01,  -6.89402590e-02],
           [  2.74302006e+00,   3.70052723e-03,  -7.30173070e-03,
             -6.89402590e-02,   7.12034037e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        hth = np.array([[  4.90000000e+01,   7.04371999e+02,   1.72131237e+03,
              7.24743592e+02,   1.70735413e+03],
           [  7.04371999e+02,   1.16866734e+04,   2.15575320e+04,
              1.10925200e+04,   2.23848036e+04],
           [  1.72131237e+03,   2.15575320e+04,   7.39058986e+04,
              2.34796298e+04,   6.70145378e+04],
           [  7.24743592e+02,   1.10925200e+04,   2.34796298e+04,
              1.16146226e+04,   2.30304624e+04],
           [  1.70735413e+03,   2.23848036e+04,   6.70145378e+04,
              2.30304624e+04,   6.69879858e+04]])
        np.testing.assert_array_almost_equal(reg.hth,hth,4)

class TestGMComboHet(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.X = sparse.csr_matrix(self.X)
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        # Only spatial lag
        reg = HET.GM_Combo_Het(self.y, self.X, w=self.w, step1c=True)
        betas = np.array([[ 57.7778574 ], [  0.73034922], [ -0.59257362], [ -0.2230231 ], [  0.56636724]])
        np.testing.assert_array_almost_equal(reg.betas,betas,7)
        u = np.array([ 25.65156033])
        np.testing.assert_array_almost_equal(reg.u[0],u,7)
        ef = np.array([ 31.87664403])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],ef,7)
        ep = np.array([ 28.30648145])
        np.testing.assert_array_almost_equal(reg.e_pred[0],ep,7)
        pe = np.array([ 52.16052155])
        np.testing.assert_array_almost_equal(reg.predy_e[0],pe,7)
        predy = np.array([ 54.81544267])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,7)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 4
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,7)
        x = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x[0].toarray()[0],x,7)
        yend = np.array([ 35.4585005])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,7)
        q = np.array([ 18.594    ,  24.7142675])
        np.testing.assert_array_almost_equal(reg.q[0].toarray()[0],q,7)
        z = np.array([  1.       ,  19.531    ,  15.72598  ,  35.4585005])
        np.testing.assert_array_almost_equal(reg.z[0].toarray()[0],z,7)
        i_s = 'Maximum number of iterations reached.'
        np.testing.assert_string_equal(reg.iter_stop,i_s)
        its = 1
        self.assertAlmostEqual(reg.iteration,its,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        stdy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,stdy)
        vm = np.array([[  4.86218274e+02,  -2.77268729e+00,  -1.59987770e+00,
             -1.01969471e+01,   2.74302006e+00],
           [ -2.77268729e+00,   1.04680972e-01,   2.51172238e-02,
              1.95136385e-03,   3.70052723e-03],
           [ -1.59987770e+00,   2.51172238e-02,   2.15655720e-02,
              7.65868344e-03,  -7.30173070e-03],
           [ -1.01969471e+01,   1.95136385e-03,   7.65868344e-03,
              2.78273684e-01,  -6.89402590e-02],
           [  2.74302006e+00,   3.70052723e-03,  -7.30173070e-03,
             -6.89402590e-02,   7.12034037e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        pr2 = 0.3001582877472412
        self.assertAlmostEqual(reg.pr2,pr2,7)
        pr2_e = 0.35613102283621967
        self.assertAlmostEqual(reg.pr2_e,pr2_e,7)
        std_err = np.array([ 22.05035768,  0.32354439,  0.14685221,  0.52751653,  0.26683966])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,6)
        z_stat = np.array([(2.6202684885795335, 0.00878605635338265), (2.2573385444145524, 0.023986928627746887), (-4.0351698589183433, 5.456281036278686e-05), (-0.42277935292121521, 0.67245625315942159), (2.1225002455741895, 0.033795752094112265)])
        np.testing.assert_array_almost_equal(reg.z_stat,z_stat,6)
        hth = np.array([[  4.90000000e+01,   7.04371999e+02,   1.72131237e+03,
              7.24743592e+02,   1.70735413e+03],
           [  7.04371999e+02,   1.16866734e+04,   2.15575320e+04,
              1.10925200e+04,   2.23848036e+04],
           [  1.72131237e+03,   2.15575320e+04,   7.39058986e+04,
              2.34796298e+04,   6.70145378e+04],
           [  7.24743592e+02,   1.10925200e+04,   2.34796298e+04,
              1.16146226e+04,   2.30304624e+04],
           [  1.70735413e+03,   2.23848036e+04,   6.70145378e+04,
              2.30304624e+04,   6.69879858e+04]])
        np.testing.assert_array_almost_equal(reg.hth,hth,4)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_error_sp_hom
'''
Unittests for spreg.error_sp_hom module

'''
import unittest
import pysal
from pysal.spreg import error_sp_hom as HOM
import numpy as np

class BaseGM_Error_Hom_Tester(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
    def test_model(self):
        reg = HOM.BaseGM_Error_Hom(self.y, self.X, self.w.sparse, A1='hom_sc')
        np.testing.assert_array_almost_equal(reg.y[0],np.array([80.467003]),7)
        x = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x[0],x,7)
        betas = np.array([[ 47.9478524 ], [  0.70633223], [ -0.55595633], [  0.41288558]])
        np.testing.assert_array_almost_equal(reg.betas,betas,7)
        np.testing.assert_array_almost_equal(reg.u[0],np.array([27.466734]),6)
        np.testing.assert_array_almost_equal(reg.e_filtered[0],np.array([ 32.37298547]),7)
        i_s = 'Maximum number of iterations reached.'
        self.assertAlmostEqual(reg.iter_stop,i_s,7)
        np.testing.assert_array_almost_equal(reg.predy[0],np.array([ 53.000269]),6)
        self.assertAlmostEquals(reg.n,49,7)
        self.assertAlmostEquals(reg.k,3,7)
        sig2 = 189.94459439729718
        self.assertAlmostEqual(reg.sig2,sig2)
        vm = np.array([[  1.51340717e+02,  -5.29057506e+00,  -1.85654540e+00, -2.39139054e-03], [ -5.29057506e+00,   2.46669610e-01, 5.14259101e-02, 3.19241302e-04], [ -1.85654540e+00,   5.14259101e-02, 3.20510550e-02,  -5.95640240e-05], [ -2.39139054e-03,   3.19241302e-04, -5.95640240e-05,  3.36690159e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        xtx = np.array([[  4.90000000e+01,   7.04371999e+02, 1.72131237e+03], [  7.04371999e+02,   1.16866734e+04,   2.15575320e+04], [  1.72131237e+03,   2.15575320e+04, 7.39058986e+04]])
        np.testing.assert_array_almost_equal(reg.xtx,xtx,4)

class GM_Error_Hom_Tester(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
    def test_model(self):
        reg = HOM.GM_Error_Hom(self.y, self.X, self.w, A1='hom_sc')
        np.testing.assert_array_almost_equal(reg.y[0],np.array([80.467003]),7)
        x = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x[0],x,7)
        betas = np.array([[ 47.9478524 ], [  0.70633223], [ -0.55595633], [  0.41288558]])
        np.testing.assert_array_almost_equal(reg.betas,betas,7)
        np.testing.assert_array_almost_equal(reg.u[0],np.array([27.46673388]),6)
        np.testing.assert_array_almost_equal(reg.e_filtered[0],np.array([ 32.37298547]),7)
        np.testing.assert_array_almost_equal(reg.predy[0],np.array([ 53.00026912]),6)
        self.assertAlmostEquals(reg.n,49,7)
        self.assertAlmostEquals(reg.k,3,7)
        vm = np.array([[  1.51340717e+02,  -5.29057506e+00,  -1.85654540e+00, -2.39139054e-03], [ -5.29057506e+00,   2.46669610e-01, 5.14259101e-02, 3.19241302e-04], [ -1.85654540e+00,   5.14259101e-02, 3.20510550e-02,  -5.95640240e-05], [ -2.39139054e-03,   3.19241302e-04, -5.95640240e-05,  3.36690159e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        i_s = 'Maximum number of iterations reached.'
        self.assertAlmostEqual(reg.iter_stop,i_s,7)
        self.assertAlmostEqual(reg.iteration,1,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        std_y = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,std_y)
        pr2 = 0.34950977055969729
        self.assertAlmostEqual(reg.pr2,pr2)
        sig2 = 189.94459439729718
        self.assertAlmostEqual(reg.sig2,sig2)
        std_err = np.array([ 12.30206149,   0.49665844,   0.17902808, 0.18349119])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,6)
        z_stat = np.array([[  3.89754616e+00,   9.71723059e-05], [  1.42216900e+00,   1.54977196e-01], [ -3.10541409e+00,   1.90012806e-03], [  2.25016500e+00,   2.44384731e-02]])
        np.testing.assert_array_almost_equal(reg.z_stat,z_stat,6)
        xtx = np.array([[  4.90000000e+01,   7.04371999e+02, 1.72131237e+03], [  7.04371999e+02,   1.16866734e+04,   2.15575320e+04], [  1.72131237e+03,   2.15575320e+04, 7.39058986e+04]])
        np.testing.assert_array_almost_equal(reg.xtx,xtx,4)


class BaseGM_Endog_Error_Hom_Tester(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        self.X = np.array(X).T
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        yd = []
        yd.append(db.by_col("CRIME"))
        self.yd = np.array(yd).T
        q = []
        q.append(db.by_col("DISCBD"))
        self.q = np.array(q).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
    def test_model(self):
        reg = HOM.BaseGM_Endog_Error_Hom(self.y, self.X, self.yd, self.q, self.w.sparse, A1='hom_sc')
        np.testing.assert_array_almost_equal(reg.y[0],np.array([ 80.467003]),7)
        x = np.array([  1.     ,  19.531])
        np.testing.assert_array_almost_equal(reg.x[0],x,7)
        z = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.z[0],z,7)
        h = np.array([  1.   ,  19.531,   5.03 ])
        np.testing.assert_array_almost_equal(reg.h[0],h,7)
        yend = np.array([ 15.72598])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,7)
        q = np.array([ 5.03])
        np.testing.assert_array_almost_equal(reg.q[0],q,7)
        betas = np.array([[ 55.36575166], [  0.46432416], [ -0.66904404], [  0.43205526]])
        np.testing.assert_array_almost_equal(reg.betas,betas,6)
        u = np.array([ 26.55390939])
        np.testing.assert_array_almost_equal(reg.u[0],u,6)
        np.testing.assert_array_almost_equal(reg.e_filtered[0],np.array([ 31.74114306]),7)
        predy = np.array([ 53.91309361])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,6)
        self.assertAlmostEquals(reg.n,49,7)
        self.assertAlmostEquals(reg.k,3,7)
        sig2 = 190.59435238060928
        self.assertAlmostEqual(reg.sig2,sig2)
        vm = np.array([[  5.52064057e+02,  -1.61264555e+01,  -8.86360735e+00, 1.04251912e+00], [ -1.61264555e+01,   5.44898242e-01, 2.39518645e-01, -1.88092950e-02], [ -8.86360735e+00,   2.39518645e-01, 1.55501840e-01, -2.18638648e-02], [  1.04251912e+00, -1.88092950e-02, -2.18638648e-02, 3.71222222e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        i_s = 'Maximum number of iterations reached.'
        self.assertAlmostEqual(reg.iter_stop,i_s,7)
        its = 1
        self.assertAlmostEqual(reg.iteration,its,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        std_y = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,std_y)
        sig2 = 0
        #self.assertAlmostEqual(reg.sig2,sig2)
        hth = np.array([[    49.        ,    704.371999  ,    139.75      ], [   704.371999  ,  11686.67338121,   2246.12800625], [   139.75      ,   2246.12800625,    498.5851]])
        np.testing.assert_array_almost_equal(reg.hth,hth,4)

class GM_Endog_Error_Hom_Tester(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        self.X = np.array(X).T
        yd = []
        yd.append(db.by_col("CRIME"))
        self.yd = np.array(yd).T
        q = []
        q.append(db.by_col("DISCBD"))
        self.q = np.array(q).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
    def test_model(self):
        reg = HOM.GM_Endog_Error_Hom(self.y, self.X, self.yd, self.q, self.w, A1='hom_sc')
        np.testing.assert_array_almost_equal(reg.y[0],np.array([ 80.467003]),7)
        x = np.array([  1.     ,  19.531])
        np.testing.assert_array_almost_equal(reg.x[0],x,7)
        z = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.z[0],z,7)
        h = np.array([  1.   ,  19.531,   5.03 ])
        np.testing.assert_array_almost_equal(reg.h[0],h,7)
        yend = np.array([ 15.72598])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,7)
        q = np.array([ 5.03])
        np.testing.assert_array_almost_equal(reg.q[0],q,7)
        betas = np.array([[ 55.36575166], [  0.46432416], [ -0.66904404], [  0.43205526]])
        np.testing.assert_array_almost_equal(reg.betas,betas,6)
        u = np.array([ 26.55390939])
        np.testing.assert_array_almost_equal(reg.u[0],u,6)
        np.testing.assert_array_almost_equal(reg.e_filtered[0],np.array([ 31.74114306]),7)
        predy = np.array([ 53.91309361])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,6)
        self.assertAlmostEquals(reg.n,49,7)
        self.assertAlmostEquals(reg.k,3,7)
        vm = np.array([[  5.52064057e+02,  -1.61264555e+01,  -8.86360735e+00, 1.04251912e+00], [ -1.61264555e+01,   5.44898242e-01, 2.39518645e-01, -1.88092950e-02], [ -8.86360735e+00,   2.39518645e-01, 1.55501840e-01, -2.18638648e-02], [  1.04251912e+00, -1.88092950e-02, -2.18638648e-02, 3.71222222e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        i_s = 'Maximum number of iterations reached.'
        self.assertAlmostEqual(reg.iter_stop,i_s,7)
        its = 1
        self.assertAlmostEqual(reg.iteration,its,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        std_y = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,std_y)
        pr2 = 0.34647366525657419
        self.assertAlmostEqual(reg.pr2,pr2)
        sig2 = 190.59435238060928
        self.assertAlmostEqual(reg.sig2,sig2)
        #std_err
        std_err = np.array([ 23.49604343,   0.73817223,   0.39433722, 0.19267128])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,6)
        z_stat = np.array([[ 2.35638617,  0.01845372], [ 0.62901874,  0.52933679], [-1.69662923,  0.08976678], [ 2.24244556,  0.02493259]])
        np.testing.assert_array_almost_equal(reg.z_stat,z_stat,6)


class BaseGM_Combo_Hom_Tester(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        self.X = np.array(X).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
    def test_model(self):
        yd2, q2 = pysal.spreg.utils.set_endog(self.y, self.X, self.w, None, None, 1, True)
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        reg = HOM.BaseGM_Combo_Hom(self.y, self.X, yend=yd2, q=q2, w=self.w.sparse, A1='hom_sc')
        np.testing.assert_array_almost_equal(reg.y[0],np.array([80.467003]),7)
        x = np.array([  1.     ,  19.531])
        np.testing.assert_array_almost_equal(reg.x[0],x,7)
        betas = np.array([[ 10.12541428], [  1.56832263], [  0.15132076], [  0.21033397]])
        np.testing.assert_array_almost_equal(reg.betas,betas,7)
        np.testing.assert_array_almost_equal(reg.u[0],np.array([34.3450723]),7)
        np.testing.assert_array_almost_equal(reg.e_filtered[0],np.array([ 36.6149682]),7)
        np.testing.assert_array_almost_equal(reg.predy[0],np.array([ 46.1219307]),7)
        self.assertAlmostEquals(reg.n,49,7)
        self.assertAlmostEquals(reg.k,3,7)
        vm = np.array([[  2.33694742e+02,  -6.66856869e-01,  -5.58304254e+00, 4.85488380e+00], [ -6.66856869e-01,   1.94241504e-01, -5.42327138e-02, 5.37225570e-02], [ -5.58304254e+00,  -5.42327138e-02, 1.63860721e-01, -1.44425498e-01], [  4.85488380e+00, 5.37225570e-02, -1.44425498e-01, 1.78622255e-01]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        z = np.array([  1.       ,  19.531    ,  35.4585005])
        np.testing.assert_array_almost_equal(reg.z[0],z,7)
        h = np.array([  1.   ,  19.531,  18.594])
        np.testing.assert_array_almost_equal(reg.h[0],h,7)
        yend = np.array([ 35.4585005])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,7)
        q = np.array([ 18.594])
        np.testing.assert_array_almost_equal(reg.q[0],q,7)
        i_s = 'Maximum number of iterations reached.'
        self.assertAlmostEqual(reg.iter_stop,i_s,7)
        its = 1
        self.assertAlmostEqual(reg.iteration,its,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        std_y = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,std_y)
        sig2 = 232.22680651270042
        self.assertAlmostEqual(reg.sig2,sig2)
        hth = np.array([[    49.        ,    704.371999  ,    724.7435916 ], [   704.371999  ,  11686.67338121,  11092.519988  ], [   724.7435916 ,  11092.519988  , 11614.62257048]])
        np.testing.assert_array_almost_equal(reg.hth,hth,4)


class GM_Combo_Hom_Tester(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        self.X = np.array(X).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
    def test_model(self):
        reg = HOM.GM_Combo_Hom(self.y, self.X, w=self.w, A1='hom_sc')
        np.testing.assert_array_almost_equal(reg.y[0],np.array([80.467003]),7)
        x = np.array([  1.     ,  19.531])
        np.testing.assert_array_almost_equal(reg.x[0],x,7)
        betas = np.array([[ 10.12541428], [  1.56832263], [  0.15132076], [  0.21033397]])
        np.testing.assert_array_almost_equal(reg.betas,betas,7)
        np.testing.assert_array_almost_equal(reg.u[0],np.array([34.3450723]),7)
        np.testing.assert_array_almost_equal(reg.e_filtered[0],np.array([ 36.6149682]),7)
        np.testing.assert_array_almost_equal(reg.e_pred[0],np.array([ 32.90372983]),7)
        np.testing.assert_array_almost_equal(reg.predy[0],np.array([ 46.1219307]),7)
        np.testing.assert_array_almost_equal(reg.predy_e[0],np.array([47.56327317]),7)
        self.assertAlmostEquals(reg.n,49,7)
        self.assertAlmostEquals(reg.k,3,7)
        z = np.array([  1.       ,  19.531    ,  35.4585005])
        np.testing.assert_array_almost_equal(reg.z[0],z,7)
        h = np.array([  1.   ,  19.531,  18.594])
        np.testing.assert_array_almost_equal(reg.h[0],h,7)
        yend = np.array([ 35.4585005])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,7)
        q = np.array([ 18.594])
        np.testing.assert_array_almost_equal(reg.q[0],q,7)
        i_s = 'Maximum number of iterations reached.'
        self.assertAlmostEqual(reg.iter_stop,i_s,7)
        self.assertAlmostEqual(reg.iteration,1,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        std_y = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,std_y)
        pr2 = 0.28379825632694394
        self.assertAlmostEqual(reg.pr2,pr2)
        pr2_e = 0.25082892555141506
        self.assertAlmostEqual(reg.pr2_e,pr2_e)
        sig2 = 232.22680651270042
        self.assertAlmostEqual(reg.sig2,sig2)
        std_err = np.array([ 15.28707761,   0.44072838,   0.40479714, 0.42263726])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,6)
        z_stat = np.array([[  6.62351206e-01,   5.07746167e-01], [  3.55847888e+00,   3.73008780e-04], [  3.73818749e-01,   7.08539170e-01], [  4.97670189e-01,   6.18716523e-01]])
        np.testing.assert_array_almost_equal(reg.z_stat,z_stat,6)
        vm = np.array([[  2.33694742e+02,  -6.66856869e-01,  -5.58304254e+00, 4.85488380e+00], [ -6.66856869e-01,   1.94241504e-01, -5.42327138e-02, 5.37225570e-02], [ -5.58304254e+00,  -5.42327138e-02, 1.63860721e-01, -1.44425498e-01], [  4.85488380e+00, 5.37225570e-02, -1.44425498e-01, 1.78622255e-01]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)

suite = unittest.TestSuite()
test_classes = [BaseGM_Error_Hom_Tester, GM_Error_Hom_Tester,\
        BaseGM_Endog_Error_Hom_Tester, GM_Endog_Error_Hom_Tester, \
        BaseGM_Combo_Hom_Tester, GM_Combo_Hom_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)


########NEW FILE########
__FILENAME__ = test_error_sp_hom_regimes
import unittest
import pysal
import numpy as np
from pysal.spreg import error_sp_hom_regimes as SP
from pysal.spreg.error_sp_hom import GM_Error_Hom, GM_Endog_Error_Hom, GM_Combo_Hom

class TestGM_Error_Hom_Regimes(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("CRIME"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("HOVAL"))
        X.append(db.by_col("INC"))
        self.X = np.array(X).T
        X2 = []
        X2.append(db.by_col("INC"))
        self.X2 = np.array(X2).T
        yd = []
        yd.append(db.by_col("HOVAL"))
        self.yd = np.array(yd).T
        q = []
        q.append(db.by_col("DISCBD"))
        self.q = np.array(q).T
        self.w = pysal.queen_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
        self.r_var = 'NSA'
        self.regimes = db.by_col(self.r_var)
        #Artficial:
        n = 256
        self.n2 = n/2
        self.x_a1 = np.random.uniform(-10,10,(n,1))
        self.x_a2 = np.random.uniform(1,5,(n,1))
        self.q_a = self.x_a2 + np.random.normal(0,1,(n,1))
        self.x_a = np.hstack((self.x_a1,self.x_a2))
        self.y_a = np.dot(np.hstack((np.ones((n,1)),self.x_a)),np.array([[1],[0.5],[2]])) + np.random.normal(0,1,(n,1))
        latt = int(np.sqrt(n))
        self.w_a = pysal.lat2W(latt,latt)
        self.w_a.transform='r'
        self.regi_a = [0]*(n/2) + [1]*(n/2)
        self.w_a1 = pysal.lat2W(latt/2,latt)
        self.w_a1.transform='r'
        
    def test_model(self):
        reg = SP.GM_Error_Hom_Regimes(self.y, self.X, self.regimes, self.w, A1='het')
        betas = np.array([[ 62.95986466],
       [ -0.15660795],
       [ -1.49054832],
       [ 60.98577615],
       [ -0.3358993 ],
       [ -0.82129289],
       [  0.54033921]])
        np.testing.assert_array_almost_equal(reg.betas,betas,6)
        u = np.array([-2.19031456])
        np.testing.assert_array_almost_equal(reg.u[0],u,6)
        predy = np.array([ 17.91629456])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,6)
        n = 49
        self.assertAlmostEqual(reg.n,n,6)
        k = 6
        self.assertAlmostEqual(reg.k,k,6)
        y = np.array([ 15.72598])
        np.testing.assert_array_almost_equal(reg.y[0],y,6)
        x = np.array([[  0.      ,   0.      ,   0.      ,   1.      ,  80.467003,  19.531   ]])
        np.testing.assert_array_almost_equal(reg.x[0].toarray(),x,6)
        e = np.array([ 2.72131648])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,6)
        my = 35.128823897959187
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 16.732092091229699
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([ 49.16245801,  -0.12493165,  -1.89294614,   5.71968257,
        -0.0571525 ,   0.05745855,   0.        ])
        np.testing.assert_array_almost_equal(reg.vm[0],vm,6)
        sig2 = 96.96108341267626
        self.assertAlmostEqual(reg.sig2,sig2,5)
        pr2 = 0.5515791216023577
        self.assertAlmostEqual(reg.pr2,pr2)
        std_err = np.array([ 7.01159454,  0.20701411,  0.56905515,  7.90537942,  0.10268949,
        0.56660879,  0.15659504])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,6)
        chow_r = np.array([[ 0.03888544,  0.84367579],
       [ 0.61613446,  0.43248738],
       [ 0.72632441,  0.39407719]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,6)
        chow_j = 0.92133276766189676
        self.assertAlmostEqual(reg.chow.joint[0],chow_j)

    def test_model_regi_error(self):
        #Artficial:
        model = SP.GM_Error_Hom_Regimes(self.y_a, self.x_a, self.regi_a, w=self.w_a, regime_err_sep=True, A1='het')
        model1 = GM_Error_Hom(self.y_a[0:(self.n2)].reshape((self.n2),1), self.x_a[0:(self.n2)], w=self.w_a1, A1='het')
        model2 = GM_Error_Hom(self.y_a[(self.n2):].reshape((self.n2),1), self.x_a[(self.n2):], w=self.w_a1, A1='het')
        tbetas = np.vstack((model1.betas, model2.betas))
        np.testing.assert_array_almost_equal(model.betas,tbetas)
        vm = np.hstack((model1.vm.diagonal(),model2.vm.diagonal()))
        np.testing.assert_array_almost_equal(model.vm.diagonal(), vm, 6)
        #Columbus:
        reg = SP.GM_Error_Hom_Regimes(self.y, self.X2, self.regimes, self.w, regime_err_sep=True, A1='het')
        betas = np.array([[ 60.66668194],
       [ -1.72708492],
       [  0.62170311],
       [ 61.4526885 ],
       [ -1.90700858],
       [  0.1102755 ]])
        np.testing.assert_array_almost_equal(reg.betas,betas,6)
        vm = np.array([ 45.57956967,  -1.65365774,   0.        ,   0.        ,
         0.        ,   0.        ])
        np.testing.assert_array_almost_equal(reg.vm[0],vm,6)
        u = np.array([-8.48092392])
        np.testing.assert_array_almost_equal(reg.u[0],u,6)
        predy = np.array([ 24.20690392])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,6)
        e = np.array([-8.33982604])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,6)
        chow_r = np.array([[ 0.0050892 ,  0.94312823],
       [ 0.05746619,  0.81054651],
       [ 1.65677138,  0.19803981]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,6)
        chow_j = 1.7914221673031792
        self.assertAlmostEqual(reg.chow.joint[0],chow_j)

    def test_model_endog(self):
        reg = SP.GM_Endog_Error_Hom_Regimes(self.y, self.X2, self.yd, self.q, self.regimes, self.w, A1='het')
        betas = np.array([[ 77.26679984],
       [  4.45992905],
       [ 78.59534391],
       [  0.41432319],
       [ -3.20196286],
       [ -1.13672283],
       [  0.22178164]])
        np.testing.assert_array_almost_equal(reg.betas,betas,6)
        u = np.array([ 20.50716917])
        np.testing.assert_array_almost_equal(reg.u[0],u,6)
        e = np.array([ 25.22635318])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,6)
        predy = np.array([-4.78118917])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,6)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 6
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 15.72598])
        np.testing.assert_array_almost_equal(reg.y[0],y,6)
        x = np.array([[  0.   ,   0.   ,   1.   ,  19.531]])
        np.testing.assert_array_almost_equal(reg.x[0].toarray(),x,6)
        yend = np.array([[  0.      ,  80.467003]])
        np.testing.assert_array_almost_equal(reg.yend[0].toarray(),yend,6)
        z = np.array([[  0.      ,   0.      ,   1.      ,  19.531   ,   0.      ,
         80.467003]])
        np.testing.assert_array_almost_equal(reg.z[0].toarray(),z,6)
        my = 35.128823897959187
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 16.732092091229699
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([ 403.76852704,   69.06920553,   19.8388512 ,    3.62501395,
        -40.30472224,   -1.6601927 ,   -1.64319352])
        np.testing.assert_array_almost_equal(reg.vm[0],vm,5)
        pr2 = 0.19776512679498906
        self.assertAlmostEqual(reg.pr2,pr2)
        sig2 = 644.23810259214
        self.assertAlmostEqual(reg.sig2,sig2,5)
        std_err = np.array([ 20.09399231,   7.03617703,  23.64968032,   2.176846  ,
         3.40352278,   0.92377997,   0.24462006])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,6)
        chow_r = np.array([[ 0.00191145,  0.96512749],
       [ 0.31031517,  0.57748685],
       [ 0.34994619,  0.55414359]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,6)
        chow_j = 1.248410480025556
        self.assertAlmostEqual(reg.chow.joint[0],chow_j)

    def test_model_endog_regi_error(self):
        #Columbus:
        reg = SP.GM_Endog_Error_Hom_Regimes(self.y, self.X2, self.yd, self.q, self.regimes, self.w, regime_err_sep=True, A1='het')
        betas = np.array([[  7.92747424e+01],
       [  5.78086230e+00],
       [ -3.83173581e+00],
       [  2.14725610e-01],
       [  8.26255251e+01],
       [  5.48294187e-01],
       [ -1.28432891e+00],
       [  2.98658172e-02]])
        np.testing.assert_array_almost_equal(reg.betas,betas,6)
        vm = np.array([ 867.50930457,  161.04430783,  -92.35637083,   -1.13838767,
          0.        ,    0.        ,    0.        ,    0.        ])
        np.testing.assert_array_almost_equal(reg.vm[0],vm,6)
        u = np.array([ 25.73781918])
        np.testing.assert_array_almost_equal(reg.u[0],u,6)
        predy = np.array([-10.01183918])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,6)
        e = np.array([26.41176711])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,6)
        chow_r = np.array([[ 0.00909777,  0.92401124],
       [ 0.24034941,  0.62395386],
       [ 0.24322564,  0.62188603],
       [ 0.32572159,  0.5681893 ]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,6)
        chow_j = 1.4485058522307526
        self.assertAlmostEqual(reg.chow.joint[0],chow_j)
        #Artficial:
        model = SP.GM_Endog_Error_Hom_Regimes(self.y_a, self.x_a1, yend=self.x_a2, q=self.q_a, regimes=self.regi_a, w=self.w_a, regime_err_sep=True, A1='het')
        model1 = GM_Endog_Error_Hom(self.y_a[0:(self.n2)].reshape((self.n2),1), self.x_a1[0:(self.n2)], yend=self.x_a2[0:(self.n2)], q=self.q_a[0:(self.n2)], w=self.w_a1, A1='het')
        model2 = GM_Endog_Error_Hom(self.y_a[(self.n2):].reshape((self.n2),1), self.x_a1[(self.n2):], yend=self.x_a2[(self.n2):], q=self.q_a[(self.n2):], w=self.w_a1, A1='het')
        tbetas = np.vstack((model1.betas, model2.betas))
        np.testing.assert_array_almost_equal(model.betas,tbetas)
        vm = np.hstack((model1.vm.diagonal(),model2.vm.diagonal()))
        np.testing.assert_array_almost_equal(model.vm.diagonal(), vm, 6)

    def test_model_combo(self):
        reg = SP.GM_Combo_Hom_Regimes(self.y, self.X2, self.regimes, self.yd, self.q, w=self.w, A1='het')
        betas = np.array([[ 36.93726782],
       [ -0.829475  ],
       [ 30.86675168],
       [ -0.72375344],
       [ -0.30190094],
       [ -0.22132895],
       [  0.64190215],
       [ -0.07314671]])
        np.testing.assert_array_almost_equal(reg.betas,betas,6)
        u = np.array([ 0.94039246])
        np.testing.assert_array_almost_equal(reg.u[0],u,6)
        e_filtered = np.array([ 0.74211331])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e_filtered,5)
        predy_e = np.array([ 18.68732105])
        np.testing.assert_array_almost_equal(reg.predy_e[0],predy_e,6)
        predy = np.array([ 14.78558754])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,6)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 7
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 15.72598])
        np.testing.assert_array_almost_equal(reg.y[0],y,6)
        x = np.array([[  0.   ,   0.   ,   1.   ,  19.531]])
        np.testing.assert_array_almost_equal(reg.x[0].toarray(),x,6)
        yend = np.array([[  0.       ,  80.467003 ,  24.7142675]])
        np.testing.assert_array_almost_equal(reg.yend[0].toarray(),yend,6)
        z = np.array([[  0.       ,   0.       ,   1.       ,  19.531    ,   0.       ,
         80.467003 ,  24.7142675]])
        np.testing.assert_array_almost_equal(reg.z[0].toarray(),z,6)
        my = 35.128823897959187
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 16.732092091229699
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([ 111.54419614,   -0.23476709,   83.37295278,   -1.74452409,
         -1.60256796,   -0.13151396,   -1.43857915,    2.19420848])
        np.testing.assert_array_almost_equal(reg.vm[0],vm,4)
        sig2 = 95.57694234438294
        self.assertAlmostEqual(reg.sig2,sig2,4)
        pr2 = 0.6504148883591536
        self.assertAlmostEqual(reg.pr2,pr2)
        pr2_e = 0.5271368969923579
        self.assertAlmostEqual(reg.pr2_e,pr2_e)
        std_err = np.array([ 10.56144858,   0.93986958,  11.52977369,   0.61000358,
         0.44419535,   0.16191882,   0.1630835 ,   0.41107528])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,5)
        chow_r = np.array([[ 0.47406771,  0.49112176],
       [ 0.00879838,  0.92526827],
       [ 0.02943577,  0.86377672]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,6)
        chow_j = 0.59098559257602923
        self.assertAlmostEqual(reg.chow.joint[0],chow_j)

    def test_model_combo_regi_error(self):
        #Columbus:
        reg = SP.GM_Combo_Hom_Regimes(self.y, self.X2, self.regimes, self.yd, self.q, w=self.w, regime_lag_sep=True, regime_err_sep=True, A1='het')
        betas = np.array([[  4.20115146e+01],
       [ -1.39171512e-01],
       [ -6.53001838e-01],
       [  5.47370644e-01],
       [  2.61860326e-01],
       [  3.42156975e+01],
       [ -1.52360889e-01],
       [ -4.91752171e-01],
       [  6.57331733e-01],
       [ -2.68716241e-02]])
        np.testing.assert_array_almost_equal(reg.betas,betas,6)
        vm = np.array([ 154.23356187,    2.99104716,   -3.29036767,   -2.473113  ,
          1.65247551,    0.        ,    0.        ,    0.        ,
          0.        ,    0.        ])
        np.testing.assert_array_almost_equal(reg.vm[0],vm,6)
        u = np.array([ 7.81039418])
        np.testing.assert_array_almost_equal(reg.u[0],u,6)
        predy = np.array([ 7.91558582])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,6)
        e = np.array([ 7.60819283])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,6)
        chow_r = np.array([[  9.59590706e-02,   7.56733881e-01],
       [  6.53130455e-05,   9.93551847e-01],
       [  4.65270134e-02,   8.29220655e-01],
       [  7.68939379e-02,   7.81551631e-01],
       [  5.04560098e-01,   4.77503278e-01]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,6)
        chow_j = 0.74134991257940286
        self.assertAlmostEqual(reg.chow.joint[0],chow_j)
        #Artficial:
        model = SP.GM_Combo_Hom_Regimes(self.y_a, self.x_a1, yend=self.x_a2, q=self.q_a, regimes=self.regi_a, w=self.w_a, regime_err_sep=True, regime_lag_sep=True, A1='het')
        model1 = GM_Combo_Hom(self.y_a[0:(self.n2)].reshape((self.n2),1), self.x_a1[0:(self.n2)], yend=self.x_a2[0:(self.n2)], q=self.q_a[0:(self.n2)], w=self.w_a1, A1='het')
        model2 = GM_Combo_Hom(self.y_a[(self.n2):].reshape((self.n2),1), self.x_a1[(self.n2):], yend=self.x_a2[(self.n2):], q=self.q_a[(self.n2):], w=self.w_a1, A1='het')
        tbetas = np.vstack((model1.betas, model2.betas))
        np.testing.assert_array_almost_equal(model.betas,tbetas)
        vm = np.hstack((model1.vm.diagonal(),model2.vm.diagonal()))


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_error_sp_hom_sparse
'''
Unittests for spreg.error_sp_hom module

'''
import unittest
import pysal
from pysal.spreg import error_sp_hom as HOM
from scipy import sparse
import numpy as np

class BaseGM_Error_Hom_Tester(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.X = sparse.csr_matrix(self.X)
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
    def test_model(self):
        reg = HOM.BaseGM_Error_Hom(self.y, self.X, self.w.sparse, A1='hom_sc')
        np.testing.assert_array_almost_equal(reg.y[0],np.array([80.467003]),7)
        x = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x[0].toarray()[0],x,7)
        betas = np.array([[ 47.9478524 ], [  0.70633223], [ -0.55595633], [  0.41288558]])
        np.testing.assert_array_almost_equal(reg.betas,betas,7)
        np.testing.assert_array_almost_equal(reg.u[0],np.array([27.466734]),6)
        np.testing.assert_array_almost_equal(reg.e_filtered[0],np.array([ 32.37298547]),7)
        i_s = 'Maximum number of iterations reached.'
        self.assertAlmostEqual(reg.iter_stop,i_s,7)
        np.testing.assert_array_almost_equal(reg.predy[0],np.array([ 53.000269]),6)
        self.assertAlmostEquals(reg.n,49,7)
        self.assertAlmostEquals(reg.k,3,7)
        sig2 = 189.94459439729718
        self.assertAlmostEqual(reg.sig2,sig2)
        vm = np.array([[  1.51340717e+02,  -5.29057506e+00,  -1.85654540e+00, -2.39139054e-03], [ -5.29057506e+00,   2.46669610e-01, 5.14259101e-02, 3.19241302e-04], [ -1.85654540e+00,   5.14259101e-02, 3.20510550e-02,  -5.95640240e-05], [ -2.39139054e-03,   3.19241302e-04, -5.95640240e-05,  3.36690159e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        xtx = np.array([[  4.90000000e+01,   7.04371999e+02, 1.72131237e+03], [  7.04371999e+02,   1.16866734e+04,   2.15575320e+04], [  1.72131237e+03,   2.15575320e+04, 7.39058986e+04]])
        np.testing.assert_array_almost_equal(reg.xtx,xtx,4)

class GM_Error_Hom_Tester(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.X = sparse.csr_matrix(self.X)
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
    def test_model(self):
        reg = HOM.GM_Error_Hom(self.y, self.X, self.w, A1='hom_sc')
        np.testing.assert_array_almost_equal(reg.y[0],np.array([80.467003]),7)
        x = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x[0].toarray()[0],x,7)
        betas = np.array([[ 47.9478524 ], [  0.70633223], [ -0.55595633], [  0.41288558]])
        np.testing.assert_array_almost_equal(reg.betas,betas,7)
        np.testing.assert_array_almost_equal(reg.u[0],np.array([27.46673388]),6)
        np.testing.assert_array_almost_equal(reg.e_filtered[0],np.array([ 32.37298547]),7)
        np.testing.assert_array_almost_equal(reg.predy[0],np.array([ 53.00026912]),6)
        self.assertAlmostEquals(reg.n,49,7)
        self.assertAlmostEquals(reg.k,3,7)
        vm = np.array([[  1.51340717e+02,  -5.29057506e+00,  -1.85654540e+00, -2.39139054e-03], [ -5.29057506e+00,   2.46669610e-01, 5.14259101e-02, 3.19241302e-04], [ -1.85654540e+00,   5.14259101e-02, 3.20510550e-02,  -5.95640240e-05], [ -2.39139054e-03,   3.19241302e-04, -5.95640240e-05,  3.36690159e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        i_s = 'Maximum number of iterations reached.'
        self.assertAlmostEqual(reg.iter_stop,i_s,7)
        self.assertAlmostEqual(reg.iteration,1,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        std_y = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,std_y)
        pr2 = 0.34950977055969729
        self.assertAlmostEqual(reg.pr2,pr2)
        sig2 = 189.94459439729718
        self.assertAlmostEqual(reg.sig2,sig2)
        std_err = np.array([ 12.30206149,   0.49665844,   0.17902808, 0.18349119])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,6)
        z_stat = np.array([[  3.89754616e+00,   9.71723059e-05], [  1.42216900e+00,   1.54977196e-01], [ -3.10541409e+00,   1.90012806e-03], [  2.25016500e+00,   2.44384731e-02]])
        np.testing.assert_array_almost_equal(reg.z_stat,z_stat,6)
        xtx = np.array([[  4.90000000e+01,   7.04371999e+02, 1.72131237e+03], [  7.04371999e+02,   1.16866734e+04,   2.15575320e+04], [  1.72131237e+03,   2.15575320e+04, 7.39058986e+04]])
        np.testing.assert_array_almost_equal(reg.xtx,xtx,4)


class BaseGM_Endog_Error_Hom_Tester(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        self.X = np.array(X).T
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.X = sparse.csr_matrix(self.X)
        yd = []
        yd.append(db.by_col("CRIME"))
        self.yd = np.array(yd).T
        q = []
        q.append(db.by_col("DISCBD"))
        self.q = np.array(q).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
    def test_model(self):
        reg = HOM.BaseGM_Endog_Error_Hom(self.y, self.X, self.yd, self.q, self.w.sparse, A1='hom_sc')
        np.testing.assert_array_almost_equal(reg.y[0],np.array([ 80.467003]),7)
        x = np.array([  1.     ,  19.531])
        np.testing.assert_array_almost_equal(reg.x[0].toarray()[0],x,7)
        z = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.z[0].toarray()[0],z,7)
        h = np.array([  1.   ,  19.531,   5.03 ])
        np.testing.assert_array_almost_equal(reg.h[0].toarray()[0],h,7)
        yend = np.array([ 15.72598])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,7)
        q = np.array([ 5.03])
        np.testing.assert_array_almost_equal(reg.q[0],q,7)
        betas = np.array([[ 55.36575166], [  0.46432416], [ -0.66904404], [  0.43205526]])
        np.testing.assert_array_almost_equal(reg.betas,betas,6)
        u = np.array([ 26.55390939])
        np.testing.assert_array_almost_equal(reg.u[0],u,6)
        np.testing.assert_array_almost_equal(reg.e_filtered[0],np.array([ 31.74114306]),7)
        predy = np.array([ 53.91309361])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,6)
        self.assertAlmostEquals(reg.n,49,7)
        self.assertAlmostEquals(reg.k,3,7)
        sig2 = 190.59435238060928
        self.assertAlmostEqual(reg.sig2,sig2)
        vm = np.array([[  5.52064057e+02,  -1.61264555e+01,  -8.86360735e+00, 1.04251912e+00], [ -1.61264555e+01,   5.44898242e-01, 2.39518645e-01, -1.88092950e-02], [ -8.86360735e+00,   2.39518645e-01, 1.55501840e-01, -2.18638648e-02], [  1.04251912e+00, -1.88092950e-02, -2.18638648e-02, 3.71222222e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        i_s = 'Maximum number of iterations reached.'
        self.assertAlmostEqual(reg.iter_stop,i_s,7)
        its = 1
        self.assertAlmostEqual(reg.iteration,its,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        std_y = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,std_y)
        sig2 = 0
        #self.assertAlmostEqual(reg.sig2,sig2)
        hth = np.array([[    49.        ,    704.371999  ,    139.75      ], [   704.371999  ,  11686.67338121,   2246.12800625], [   139.75      ,   2246.12800625,    498.5851]])
        np.testing.assert_array_almost_equal(reg.hth,hth,4)

class GM_Endog_Error_Hom_Tester(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        self.X = np.array(X).T
        self.X = sparse.csr_matrix(self.X)
        yd = []
        yd.append(db.by_col("CRIME"))
        self.yd = np.array(yd).T
        q = []
        q.append(db.by_col("DISCBD"))
        self.q = np.array(q).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
    def test_model(self):
        reg = HOM.GM_Endog_Error_Hom(self.y, self.X, self.yd, self.q, self.w, A1='hom_sc')
        np.testing.assert_array_almost_equal(reg.y[0],np.array([ 80.467003]),7)
        x = np.array([  1.     ,  19.531])
        np.testing.assert_array_almost_equal(reg.x[0].toarray()[0],x,7)
        z = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.z[0].toarray()[0],z,7)
        h = np.array([  1.   ,  19.531,   5.03 ])
        np.testing.assert_array_almost_equal(reg.h[0].toarray()[0],h,7)
        yend = np.array([ 15.72598])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,7)
        q = np.array([ 5.03])
        np.testing.assert_array_almost_equal(reg.q[0],q,7)
        betas = np.array([[ 55.36575166], [  0.46432416], [ -0.66904404], [  0.43205526]])
        np.testing.assert_array_almost_equal(reg.betas,betas,6)
        u = np.array([ 26.55390939])
        np.testing.assert_array_almost_equal(reg.u[0],u,6)
        np.testing.assert_array_almost_equal(reg.e_filtered[0],np.array([ 31.74114306]),7)
        predy = np.array([ 53.91309361])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,6)
        self.assertAlmostEquals(reg.n,49,7)
        self.assertAlmostEquals(reg.k,3,7)
        vm = np.array([[  5.52064057e+02,  -1.61264555e+01,  -8.86360735e+00, 1.04251912e+00], [ -1.61264555e+01,   5.44898242e-01, 2.39518645e-01, -1.88092950e-02], [ -8.86360735e+00,   2.39518645e-01, 1.55501840e-01, -2.18638648e-02], [  1.04251912e+00, -1.88092950e-02, -2.18638648e-02, 3.71222222e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        i_s = 'Maximum number of iterations reached.'
        self.assertAlmostEqual(reg.iter_stop,i_s,7)
        its = 1
        self.assertAlmostEqual(reg.iteration,its,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        std_y = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,std_y)
        pr2 = 0.34647366525657419
        self.assertAlmostEqual(reg.pr2,pr2)
        sig2 = 190.59435238060928
        self.assertAlmostEqual(reg.sig2,sig2)
        #std_err
        std_err = np.array([ 23.49604343,   0.73817223,   0.39433722, 0.19267128])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,6)
        z_stat = np.array([[ 2.35638617,  0.01845372], [ 0.62901874,  0.52933679], [-1.69662923,  0.08976678], [ 2.24244556,  0.02493259]])
        np.testing.assert_array_almost_equal(reg.z_stat,z_stat,6)


class BaseGM_Combo_Hom_Tester(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        self.X = np.array(X).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
    def test_model(self):
        yd2, q2 = pysal.spreg.utils.set_endog(self.y, self.X, self.w, None, None, 1, True)
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.X = sparse.csr_matrix(self.X)
        reg = HOM.BaseGM_Combo_Hom(self.y, self.X, yend=yd2, q=q2, w=self.w.sparse, A1='hom_sc')
        np.testing.assert_array_almost_equal(reg.y[0],np.array([80.467003]),7)
        x = np.array([  1.     ,  19.531])
        np.testing.assert_array_almost_equal(reg.x[0].toarray()[0],x,7)
        betas = np.array([[ 10.12541428], [  1.56832263], [  0.15132076], [  0.21033397]])
        np.testing.assert_array_almost_equal(reg.betas,betas,7)
        np.testing.assert_array_almost_equal(reg.u[0],np.array([34.3450723]),7)
        np.testing.assert_array_almost_equal(reg.e_filtered[0],np.array([ 36.6149682]),7)
        np.testing.assert_array_almost_equal(reg.predy[0],np.array([ 46.1219307]),7)
        self.assertAlmostEquals(reg.n,49,7)
        self.assertAlmostEquals(reg.k,3,7)
        vm = np.array([[  2.33694742e+02,  -6.66856869e-01,  -5.58304254e+00, 4.85488380e+00], [ -6.66856869e-01,   1.94241504e-01, -5.42327138e-02, 5.37225570e-02], [ -5.58304254e+00,  -5.42327138e-02, 1.63860721e-01, -1.44425498e-01], [  4.85488380e+00, 5.37225570e-02, -1.44425498e-01, 1.78622255e-01]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        z = np.array([  1.       ,  19.531    ,  35.4585005])
        np.testing.assert_array_almost_equal(reg.z[0].toarray()[0],z,7)
        h = np.array([  1.   ,  19.531,  18.594])
        np.testing.assert_array_almost_equal(reg.h[0].toarray()[0],h,7)
        yend = np.array([ 35.4585005])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,7)
        q = np.array([ 18.594])
        np.testing.assert_array_almost_equal(reg.q[0],q,7)
        i_s = 'Maximum number of iterations reached.'
        self.assertAlmostEqual(reg.iter_stop,i_s,7)
        its = 1
        self.assertAlmostEqual(reg.iteration,its,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        std_y = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,std_y)
        sig2 = 232.22680644168395
        self.assertAlmostEqual(reg.sig2,sig2, places=6)
        hth = np.array([[    49.        ,    704.371999  ,    724.7435916 ], [   704.371999  ,  11686.67338121,  11092.519988  ], [   724.7435916 ,  11092.519988  , 11614.62257048]])
        np.testing.assert_array_almost_equal(reg.hth,hth,4)


class GM_Combo_Hom_Tester(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        self.X = np.array(X).T
        self.X = sparse.csr_matrix(self.X)
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
    def test_model(self):
        reg = HOM.GM_Combo_Hom(self.y, self.X, w=self.w, A1='hom_sc')
        np.testing.assert_array_almost_equal(reg.y[0],np.array([80.467003]),7)
        x = np.array([  1.     ,  19.531])
        np.testing.assert_array_almost_equal(reg.x[0].toarray()[0],x,7)
        betas = np.array([[ 10.12541428], [  1.56832263], [  0.15132076], [  0.21033397]])
        np.testing.assert_array_almost_equal(reg.betas,betas,7)
        np.testing.assert_array_almost_equal(reg.u[0],np.array([34.3450723]),7)
        np.testing.assert_array_almost_equal(reg.e_filtered[0],np.array([ 36.6149682]),7)
        np.testing.assert_array_almost_equal(reg.e_pred[0],np.array([ 32.90372983]),7)
        np.testing.assert_array_almost_equal(reg.predy[0],np.array([ 46.1219307]),7)
        np.testing.assert_array_almost_equal(reg.predy_e[0],np.array([47.56327317]),7)
        self.assertAlmostEquals(reg.n,49,7)
        self.assertAlmostEquals(reg.k,3,7)
        z = np.array([  1.       ,  19.531    ,  35.4585005])
        np.testing.assert_array_almost_equal(reg.z[0].toarray()[0],z,7)
        h = np.array([  1.   ,  19.531,  18.594])
        np.testing.assert_array_almost_equal(reg.h[0].toarray()[0],h,7)
        yend = np.array([ 35.4585005])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,7)
        q = np.array([ 18.594])
        np.testing.assert_array_almost_equal(reg.q[0].toarray()[0],q,7)
        i_s = 'Maximum number of iterations reached.'
        self.assertAlmostEqual(reg.iter_stop,i_s,7)
        self.assertAlmostEqual(reg.iteration,1,7)
        my = 38.436224469387746
        self.assertAlmostEqual(reg.mean_y,my)
        std_y = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,std_y)
        pr2 = 0.28379825632694394
        self.assertAlmostEqual(reg.pr2,pr2)
        pr2_e = 0.25082892555141506
        self.assertAlmostEqual(reg.pr2_e,pr2_e)
        sig2 = 232.22680644168395
        self.assertAlmostEqual(reg.sig2,sig2, places=6)
        std_err = np.array([ 15.28707761,   0.44072838,   0.40479714, 0.42263726])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,6)
        z_stat = np.array([[  6.62351206e-01,   5.07746167e-01], [  3.55847888e+00,   3.73008780e-04], [  3.73818749e-01,   7.08539170e-01], [  4.97670189e-01,   6.18716523e-01]])
        np.testing.assert_array_almost_equal(reg.z_stat,z_stat,6)
        vm = np.array([[  2.33694742e+02,  -6.66856869e-01,  -5.58304254e+00, 4.85488380e+00], [ -6.66856869e-01,   1.94241504e-01, -5.42327138e-02, 5.37225570e-02], [ -5.58304254e+00,  -5.42327138e-02, 1.63860721e-01, -1.44425498e-01], [  4.85488380e+00, 5.37225570e-02, -1.44425498e-01, 1.78622255e-01]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)

suite = unittest.TestSuite()
test_classes = [BaseGM_Error_Hom_Tester, GM_Error_Hom_Tester,\
        BaseGM_Endog_Error_Hom_Tester, GM_Endog_Error_Hom_Tester, \
        BaseGM_Combo_Hom_Tester, GM_Combo_Hom_Tester]
for i in test_classes:
    a = unittest.TestLoader().loadTestsFromTestCase(i)
    suite.addTest(a)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)


########NEW FILE########
__FILENAME__ = test_error_sp_regimes
import unittest
import pysal
import numpy as np
from pysal.spreg import error_sp_regimes as SP
from pysal.spreg.error_sp import GM_Error, GM_Endog_Error, GM_Combo

class TestGM_Error_Regimes(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("CRIME"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("HOVAL"))
        X.append(db.by_col("INC"))
        self.X = np.array(X).T
        self.w = pysal.queen_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
        self.r_var = 'NSA'
        self.regimes = db.by_col(self.r_var)
        X1 = []
        X1.append(db.by_col("INC"))
        self.X1 = np.array(X1).T
        yd = []
        yd.append(db.by_col("HOVAL"))
        self.yd = np.array(yd).T
        q = []
        q.append(db.by_col("DISCBD"))
        self.q = np.array(q).T
        #Artficial:
        n = 256
        self.n2 = n/2
        self.x_a1 = np.random.uniform(-10,10,(n,1))
        self.x_a2 = np.random.uniform(1,5,(n,1))
        self.q_a = self.x_a2 + np.random.normal(0,1,(n,1))
        self.x_a = np.hstack((self.x_a1,self.x_a2))
        self.y_a = np.dot(np.hstack((np.ones((n,1)),self.x_a)),np.array([[1],[0.5],[2]])) + np.random.normal(0,1,(n,1))
        latt = int(np.sqrt(n))
        self.w_a = pysal.lat2W(latt,latt)
        self.w_a.transform='r'
        self.regi_a = [0]*(n/2) + [1]*(n/2)
        self.w_a1 = pysal.lat2W(latt/2,latt)
        self.w_a1.transform='r'
        
    def test_model(self):
        reg = SP.GM_Error_Regimes(self.y, self.X, self.regimes, self.w)
        betas = np.array([[ 63.3443073 ],
       [ -0.15468   ],
       [ -1.52186509],
       [ 61.40071412],
       [ -0.33550084],
       [ -0.85076108],
       [  0.38671608]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        u = np.array([-2.06177251])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        predy = np.array([ 17.78775251])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        n = 49
        self.assertAlmostEqual(reg.n,n,4)
        k = 6
        self.assertAlmostEqual(reg.k,k,4)
        y = np.array([ 15.72598])
        np.testing.assert_array_almost_equal(reg.y[0],y,4)
        x = np.array([[  0.      ,   0.      ,   0.      ,   1.      ,  80.467003,  19.531   ]])
        np.testing.assert_array_almost_equal(reg.x[0].toarray(),x,4)
        e = np.array([ 1.40747232])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,4)
        my = 35.128823897959187
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 16.732092091229699
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([ 50.55875289,  -0.14444487,  -2.05735489,   0.        ,
         0.        ,   0.        ])
        np.testing.assert_array_almost_equal(reg.vm[0],vm,4)
        sig2 = 102.13050615267227
        self.assertAlmostEqual(reg.sig2,sig2,4)
        pr2 = 0.5525102200608539
        self.assertAlmostEqual(reg.pr2,pr2)
        std_err = np.array([ 7.11046784,  0.21879293,  0.58477864,  7.50596504,  0.10800686,
        0.57365981])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,4)
        chow_r = np.array([[ 0.03533785,  0.85088948],
       [ 0.54918491,  0.45865093],
       [ 0.67115641,  0.41264872]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,4)
        chow_j = 0.81985446000130979
        self.assertAlmostEqual(reg.chow.joint[0],chow_j)
    """
    def test_model_regi_error(self):
        #Columbus:
        reg = SP.GM_Error_Regimes(self.y, self.X, self.regimes, self.w, regime_err_sep=True)
        betas = np.array([[ 60.45730439],
       [ -0.17732134],
       [ -1.30936328],
       [  0.51314713],
       [ 66.5487126 ],
       [ -0.31845995],
       [ -1.29047149],
       [  0.08092997]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        vm = np.array([ 39.33656288,  -0.08420799,  -1.50350999,   0.        ,
         0.        ,   0.        ])
        np.testing.assert_array_almost_equal(reg.vm[0],vm,4)
        u = np.array([ 0.00698341])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        predy = np.array([ 15.71899659])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        e = np.array([ 0.53685671])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,4)
        chow_r = np.array([[  3.63674458e-01,   5.46472584e-01],
       [  4.29607250e-01,   5.12181727e-01],
       [  5.44739543e-04,   9.81379339e-01]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,4)
        chow_j = 0.70119418251625387
        self.assertAlmostEqual(reg.chow.joint[0],chow_j,4)
        #Artficial:
        model = SP.GM_Error_Regimes(self.y_a, self.x_a, self.regi_a, w=self.w_a, regime_err_sep=True)
        model1 = GM_Error(self.y_a[0:(self.n2)].reshape((self.n2),1), self.x_a[0:(self.n2)], w=self.w_a1)
        model2 = GM_Error(self.y_a[(self.n2):].reshape((self.n2),1), self.x_a[(self.n2):], w=self.w_a1)
        tbetas = np.vstack((model1.betas, model2.betas))
        np.testing.assert_array_almost_equal(model.betas,tbetas)
        vm = np.hstack((model1.vm.diagonal(),model2.vm.diagonal()))
        np.testing.assert_array_almost_equal(model.vm.diagonal(), vm, 4)
    """
    def test_model_endog(self):
        reg = SP.GM_Endog_Error_Regimes(self.y, self.X1, self.yd, self.q, self.regimes, self.w)
        betas = np.array([[ 77.48385551,   4.52986622,  78.93209405,   0.42186261,
         -3.23823854,  -1.1475775 ,   0.20222108]])
        np.testing.assert_array_almost_equal(reg.betas.T,betas,4)
        u = np.array([ 20.89660904])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        e = np.array([ 25.21818724])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,4)
        predy = np.array([-5.17062904])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 6
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 15.72598])
        np.testing.assert_array_almost_equal(reg.y[0],y,4)
        x = np.array([[  0.   ,   0.   ,   1.   ,  19.531]])
        np.testing.assert_array_almost_equal(reg.x[0].toarray(),x,4)
        yend = np.array([[  0.      ,  80.467003]])
        np.testing.assert_array_almost_equal(reg.yend[0].toarray(),yend,4)
        z = np.array([[  0.      ,   0.      ,   1.      ,  19.531   ,   0.      ,
         80.467003]])
        np.testing.assert_array_almost_equal(reg.z[0].toarray(),z,4)
        my = 35.128823897959187
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 16.732092091229699
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([ 390.88250241,   52.25924084,    0.        ,    0.        ,
        -32.64274729,    0.        ])
        np.testing.assert_array_almost_equal(reg.vm[0],vm,4)
        pr2 = 0.19623994206233333
        self.assertAlmostEqual(reg.pr2,pr2,4)
        sig2 = 649.4011
        self.assertAlmostEqual(round(reg.sig2,4),round(sig2,4),4)
        std_err = np.array([ 19.77074866,   6.07667394,  24.32254786,   2.17776972,
         2.97078606,   0.94392418])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,4)
        chow_r = np.array([[ 0.0021348 ,  0.96314775],
       [ 0.40499741,  0.5245196 ],
       [ 0.4498365 ,  0.50241261]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,4)
        chow_j = 1.2885590185243503
        self.assertAlmostEqual(reg.chow.joint[0],chow_j)

    def test_model_endog_regi_error(self):
        #Columbus:
        reg = SP.GM_Endog_Error_Regimes(self.y, self.X1, self.yd, self.q, self.regimes, self.w, regime_err_sep=True)
        betas = np.array([[  7.91729500e+01],
       [  5.80693176e+00],
       [ -3.84036576e+00],
       [  1.46462983e-01],
       [  8.24723791e+01],
       [  5.68908920e-01],
       [ -1.28824699e+00],
       [  6.70387351e-02]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        vm = np.array([ 791.86679123,  140.12967794,  -81.37581255,    0.        ,
          0.        ,    0.        ])
        np.testing.assert_array_almost_equal(reg.vm[0],vm,4)
        u = np.array([ 25.80361497])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        predy = np.array([-10.07763497])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        e = np.array([ 27.32251813])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,4)
        chow_r = np.array([[ 0.00926459,  0.92331985],
       [ 0.26102777,  0.60941494],
       [ 0.26664581,  0.60559072]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,4)
        chow_j = 1.1184631131987004
        self.assertAlmostEqual(reg.chow.joint[0],chow_j)
        #Artficial:
        model = SP.GM_Endog_Error_Regimes(self.y_a, self.x_a1, yend=self.x_a2, q=self.q_a, regimes=self.regi_a, w=self.w_a, regime_err_sep=True)
        model1 = GM_Endog_Error(self.y_a[0:(self.n2)].reshape((self.n2),1), self.x_a1[0:(self.n2)], yend=self.x_a2[0:(self.n2)], q=self.q_a[0:(self.n2)], w=self.w_a1)
        model2 = GM_Endog_Error(self.y_a[(self.n2):].reshape((self.n2),1), self.x_a1[(self.n2):], yend=self.x_a2[(self.n2):], q=self.q_a[(self.n2):], w=self.w_a1)
        tbetas = np.vstack((model1.betas, model2.betas))
        np.testing.assert_array_almost_equal(model.betas,tbetas)
        vm = np.hstack((model1.vm.diagonal(),model2.vm.diagonal()))
        np.testing.assert_array_almost_equal(model.vm.diagonal(), vm, 4)

    def test_model_combo(self):
        reg = SP.GM_Combo_Regimes(self.y, self.X1, self.regimes, self.yd, self.q, w=self.w)
        predy_e = np.array([ 18.82774339])
        np.testing.assert_array_almost_equal(reg.predy_e[0],predy_e,4)
        betas = np.array([[ 36.44798052],
       [ -0.7974482 ],
       [ 30.53782661],
       [ -0.72602806],
       [ -0.30953121],
       [ -0.21736652],
       [  0.64801059],
       [ -0.16601265]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        u = np.array([ 0.84393304])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        e_filtered = np.array([ 0.4040027])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e_filtered,4)
        predy = np.array([ 14.88204696])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 7
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 15.72598])
        np.testing.assert_array_almost_equal(reg.y[0],y,4)
        x = np.array([[  0.   ,   0.   ,   1.   ,  19.531]])
        np.testing.assert_array_almost_equal(reg.x[0].toarray(),x,4)
        yend = np.array([[  0.       ,  80.467003 ,  24.7142675]])
        np.testing.assert_array_almost_equal(reg.yend[0].toarray(),yend,4)
        z = np.array([[  0.       ,   0.       ,   1.       ,  19.531    ,   0.       ,
         80.467003 ,  24.7142675]])
        np.testing.assert_array_almost_equal(reg.z[0].toarray(),z,4)
        my = 35.128823897959187
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 16.732092091229699
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([ 109.23549239,   -0.19754121,   84.29574673,   -1.99317178,
         -1.60123994,   -0.1252719 ,   -1.3930344 ])
        np.testing.assert_array_almost_equal(reg.vm[0],vm,4)
        sig2 = 94.98610921110007
        self.assertAlmostEqual(reg.sig2,sig2,4)
        pr2 = 0.6493586702255537
        self.assertAlmostEqual(reg.pr2,pr2)
        pr2_e = 0.5255332447240576
        self.assertAlmostEqual(reg.pr2_e,pr2_e)
        std_err = np.array([ 10.45157846,   0.93942923,  11.38484969,   0.60774708,
         0.44461334,   0.15871227,   0.15738141])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,4)
        chow_r = np.array([[ 0.49716076,  0.48075032],
       [ 0.00405377,  0.94923363],
       [ 0.03866684,  0.84411016]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,4)
        chow_j = 0.64531386285872072
        self.assertAlmostEqual(reg.chow.joint[0],chow_j)

    def test_model_combo_regi_error(self):
        #Columbus:
        reg = SP.GM_Combo_Regimes(self.y, self.X1, self.regimes, self.yd, self.q, w=self.w, regime_lag_sep=True, regime_err_sep=True)
        betas = np.array([[ 42.01035248],
       [ -0.13938772],
       [ -0.6528306 ],
       [  0.54737621],
       [  0.2684419 ],
       [ 34.02473255],
       [ -0.14920259],
       [ -0.48972903],
       [  0.65883658],
       [ -0.17174845]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        vm = np.array([ 153.58614432,    2.96302131,   -3.26211855,   -2.46914703,
          0.        ,    0.        ,    0.        ,    0.        ])
        np.testing.assert_array_almost_equal(reg.vm[0],vm,4)
        u = np.array([ 7.73968703])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        predy = np.array([ 7.98629297])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        e = np.array([ 6.45052714])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,4)
        chow_r = np.array([[  1.00886404e-01,   7.50768497e-01],
       [  3.61843271e-05,   9.95200481e-01],
       [  4.69585772e-02,   8.28442711e-01],
       [  8.13275259e-02,   7.75506385e-01]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,4)
        chow_j = 0.28479988992843119
        self.assertAlmostEqual(reg.chow.joint[0],chow_j)
        #Artficial:
        model = SP.GM_Combo_Regimes(self.y_a, self.x_a1, yend=self.x_a2, q=self.q_a, regimes=self.regi_a, w=self.w_a, regime_err_sep=True, regime_lag_sep=True)
        model1 = GM_Combo(self.y_a[0:(self.n2)].reshape((self.n2),1), self.x_a1[0:(self.n2)], yend=self.x_a2[0:(self.n2)], q=self.q_a[0:(self.n2)], w=self.w_a1)
        model2 = GM_Combo(self.y_a[(self.n2):].reshape((self.n2),1), self.x_a1[(self.n2):], yend=self.x_a2[(self.n2):], q=self.q_a[(self.n2):], w=self.w_a1)
        tbetas = np.vstack((model1.betas, model2.betas))
        np.testing.assert_array_almost_equal(model.betas,tbetas)
        vm = np.hstack((model1.vm.diagonal(),model2.vm.diagonal()))

if __name__ == '__main__':
    unittest.main()



########NEW FILE########
__FILENAME__ = test_error_sp_sparse
import unittest
import pysal
import numpy as np
from pysal.spreg import error_sp as SP
from scipy import sparse

class TestBaseGMError(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.X = sparse.csr_matrix(self.X)
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        reg = SP.BaseGM_Error(self.y, self.X, self.w.sparse)
        betas = np.array([[ 47.94371455], [  0.70598088], [ -0.55571746], [  0.37230161]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        u = np.array([ 27.4739775])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        predy = np.array([ 52.9930255])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        n = 49
        self.assertAlmostEqual(reg.n,n,4)
        k = 3
        self.assertAlmostEqual(reg.k,k,4)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,4)
        x = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x.toarray()[0],x,4)
        e = np.array([ 31.89620319])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,4)
        predy = np.array([ 52.9930255])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        my = 38.43622446938776
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([[  1.51884943e+02,  -5.37622793e+00,  -1.86970286e+00], [ -5.37622793e+00,   2.48972661e-01,   5.26564244e-02], [ -1.86970286e+00,   5.26564244e-02, 3.18930650e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,4)
        sig2 = 191.73716465732355
        self.assertAlmostEqual(reg.sig2,sig2,4)

class TestGMError(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.X = sparse.csr_matrix(self.X)
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        reg = SP.GM_Error(self.y, self.X, self.w)
        betas = np.array([[ 47.94371455], [  0.70598088], [ -0.55571746], [  0.37230161]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        u = np.array([ 27.4739775])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        predy = np.array([ 52.9930255])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        n = 49
        self.assertAlmostEqual(reg.n,n,4)
        k = 3
        self.assertAlmostEqual(reg.k,k,4)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,4)
        x = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x.toarray()[0],x,4)
        e = np.array([ 31.89620319])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,4)
        predy = np.array([ 52.9930255])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        my = 38.43622446938776
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([[  1.51884943e+02,  -5.37622793e+00,  -1.86970286e+00], [ -5.37622793e+00,   2.48972661e-01,   5.26564244e-02], [ -1.86970286e+00,   5.26564244e-02, 3.18930650e-02]])
        np.testing.assert_array_almost_equal(reg.vm,vm,4)
        sig2 = 191.73716465732355
        self.assertAlmostEqual(reg.sig2,sig2,4)
        pr2 = 0.3495097406012179
        self.assertAlmostEqual(reg.pr2,pr2)
        std_err = np.array([ 12.32416094,   0.4989716 ,   0.1785863 ])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,4)
        z_stat = np.array([[  3.89022140e+00,   1.00152805e-04], [  1.41487186e+00,   1.57106070e-01], [ -3.11175868e+00,   1.85976455e-03]])
        np.testing.assert_array_almost_equal(reg.z_stat,z_stat,4)

class TestBaseGMEndogError(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        self.X = np.array(X).T
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.X = sparse.csr_matrix(self.X)
        yd = []
        yd.append(db.by_col("CRIME"))
        self.yd = np.array(yd).T
        q = []
        q.append(db.by_col("DISCBD"))
        self.q = np.array(q).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        reg = SP.BaseGM_Endog_Error(self.y, self.X, self.yd, self.q, self.w.sparse)
        betas = np.array([[ 55.36095292], [  0.46411479], [ -0.66883535], [  0.38989939]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        u = np.array([ 26.55951566])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        e = np.array([ 31.23925425])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,4)
        predy = np.array([ 53.9074875])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 3
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,4)
        x = np.array([  1.   ,  19.531])
        np.testing.assert_array_almost_equal(reg.x.toarray()[0],x,4)
        yend = np.array([  15.72598])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,4)
        z = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.z.toarray()[0],z,4)
        my = 38.43622446938776
        self.assertAlmostEqual(reg.mean_y,my)
        #std_y
        sy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,sy)
        #vm
        vm = np.array([[ 529.15840986,  -15.78336736,   -8.38021053],
       [ -15.78336736,    0.54023504,    0.23112032],
       [  -8.38021053,    0.23112032,    0.14497738]])
        np.testing.assert_array_almost_equal(reg.vm,vm,4)
        sig2 = 192.5002
        self.assertAlmostEqual(round(reg.sig2,4),round(sig2,4),4)

class TestGMEndogError(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        self.X = np.array(X).T
        self.X = sparse.csr_matrix(self.X)
        yd = []
        yd.append(db.by_col("CRIME"))
        self.yd = np.array(yd).T
        q = []
        q.append(db.by_col("DISCBD"))
        self.q = np.array(q).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        reg = SP.GM_Endog_Error(self.y, self.X, self.yd, self.q, self.w)
        betas = np.array([[ 55.36095292], [  0.46411479], [ -0.66883535], [  0.38989939]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        u = np.array([ 26.55951566])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        e = np.array([ 31.23925425])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,4)
        predy = np.array([ 53.9074875])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 3
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,4)
        x = np.array([  1.   ,  19.531])
        np.testing.assert_array_almost_equal(reg.x.toarray()[0],x,4)
        yend = np.array([  15.72598])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,4)
        z = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.z.toarray()[0],z,4)
        my = 38.43622446938776
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([[ 529.15840986,  -15.78336736,   -8.38021053],
       [ -15.78336736,    0.54023504,    0.23112032],
       [  -8.38021053,    0.23112032,    0.14497738]])
        np.testing.assert_array_almost_equal(reg.vm,vm,4)
        pr2 = 0.346472557570858
        self.assertAlmostEqual(reg.pr2,pr2)
        sig2 = 192.5002
        self.assertAlmostEqual(round(reg.sig2,4),round(sig2,4),4)
        std_err = np.array([ 23.003401  ,   0.73500657,   0.38075777])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,4)
        z_stat = np.array([[ 2.40664208,  0.01609994], [ 0.63144305,  0.52775088], [-1.75659016,  0.07898769]])
        np.testing.assert_array_almost_equal(reg.z_stat,z_stat,4)

class TestBaseGMCombo(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        # Only spatial lag
        yd2, q2 = pysal.spreg.utils.set_endog(self.y, self.X, self.w, None, None, 1, True)
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.X = sparse.csr_matrix(self.X)
        reg = SP.BaseGM_Combo(self.y, self.X, yend=yd2, q=q2, w=self.w.sparse)
        betas = np.array([[ 57.61123461],[  0.73441314], [ -0.59459416], [ -0.21762921], [  0.54732051]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        u = np.array([ 25.57932637])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        e_filtered = np.array([ 31.65374945])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e_filtered,4)
        predy = np.array([ 54.88767663])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 4
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,4)
        x = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x.toarray()[0],x,4)
        yend = np.array([  35.4585005])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,4)
        z = np.array([  1.       ,  19.531    ,  15.72598  ,  35.4585005])
        np.testing.assert_array_almost_equal(reg.z.toarray()[0],z,4)
        my = 38.43622446938776
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([[ 522.43841148,   -6.07256915,   -1.91429117,   -8.97133162],
       [  -6.07256915,    0.23801287,    0.0470161 ,    0.02809628],
       [  -1.91429117,    0.0470161 ,    0.03209242,    0.00314973],
       [  -8.97133162,    0.02809628,    0.00314973,    0.21575363]])
        np.testing.assert_array_almost_equal(reg.vm,vm,4)
        sig2 = 181.78650186468832
        self.assertAlmostEqual(reg.sig2,sig2,4)

class TestGMCombo(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.X = sparse.csr_matrix(self.X)
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
    def test_model(self):
        # Only spatial lag
        reg = SP.GM_Combo(self.y, self.X, w=self.w)
        e_reduced = np.array([ 28.18617481])
        np.testing.assert_array_almost_equal(reg.e_pred[0],e_reduced,4)
        predy_e = np.array([ 52.28082782])
        np.testing.assert_array_almost_equal(reg.predy_e[0],predy_e,4)
        betas = np.array([[ 57.61123515],[  0.73441313], [ -0.59459416], [ -0.21762921], [  0.54732051]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        u = np.array([ 25.57932637])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        e_filtered = np.array([ 31.65374945])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e_filtered,4)
        predy = np.array([ 54.88767685])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        n = 49
        self.assertAlmostEqual(reg.n,n)
        k = 4
        self.assertAlmostEqual(reg.k,k)
        y = np.array([ 80.467003])
        np.testing.assert_array_almost_equal(reg.y[0],y,4)
        x = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x.toarray()[0],x,4)
        yend = np.array([  35.4585005])
        np.testing.assert_array_almost_equal(reg.yend[0],yend,4)
        z = np.array([  1.       ,  19.531    ,  15.72598  ,  35.4585005])
        np.testing.assert_array_almost_equal(reg.z.toarray()[0],z,4)
        my = 38.43622446938776
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 18.466069465206047
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([[ 522.43841148,   -6.07256915,   -1.91429117,   -8.97133162],
       [  -6.07256915,    0.23801287,    0.0470161 ,    0.02809628],
       [  -1.91429117,    0.0470161 ,    0.03209242,    0.00314973],
       [  -8.97133162,    0.02809628,    0.00314973,    0.21575363]])
        np.testing.assert_array_almost_equal(reg.vm,vm,4)
        sig2 = 181.78650186468832
        self.assertAlmostEqual(reg.sig2,sig2,4)
        pr2 = 0.3018280166937799
        self.assertAlmostEqual(reg.pr2,pr2,4)
        pr2_e = 0.3561355587000738
        self.assertAlmostEqual(reg.pr2_e,pr2_e,4)
        std_err = np.array([ 22.85692222,  0.48786559,  0.17914356,  0.46449318])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,4)
        z_stat = np.array([[  2.52051597e+00,   1.17182922e-02], [  1.50535954e+00,   1.32231664e-01], [ -3.31909311e+00,   9.03103123e-04], [ -4.68530506e-01,   6.39405261e-01]])
        np.testing.assert_array_almost_equal(reg.z_stat,z_stat,4)

if __name__ == '__main__':
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True) 
    unittest.main()
    np.set_printoptions(suppress=start_suppress)


########NEW FILE########
__FILENAME__ = test_ml_error
import unittest
import pysal
import numpy as np
from pysal.spreg.ml_error import ML_Error
from pysal.spreg import utils

class TestMLError(unittest.TestCase):
    def setUp(self):
        db = pysal.open(pysal.examples.get_path("south.dbf"),'r')
        self.y_name = "HR90"
        self.y = np.array(db.by_col(self.y_name))
        self.y.shape = (len(self.y),1)
        self.x_names = ["RD90","PS90","UE90","DV90"]
        self.x = np.array([db.by_col(var) for var in self.x_names]).T
        ww = pysal.open(pysal.examples.get_path("south_q.gal"))
        self.w = ww.read()
        ww.close()
        self.w.transform = 'r'

    def test_model(self):
        reg = ML_Error(self.y,self.x,w=self.w,name_y=self.y_name,name_x=self.x_names,\
               name_w="south_q.gal")
        betas = np.array([[ 6.1492], [ 4.4024], [ 1.7784], [-0.3781], [ 0.4858], [ 0.2991]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        u = np.array([-5.97649777])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        predy = np.array([ 6.92258051])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        n = 1412
        self.assertAlmostEqual(reg.n,n,4)
        k = 5
        self.assertAlmostEqual(reg.k,k,4)
        y = np.array([ 0.94608274])
        np.testing.assert_array_almost_equal(reg.y[0],y,4)
        x = np.array([ 1.        , -0.39902838,  0.89645344,  6.85780705,  7.2636377 ])
        np.testing.assert_array_almost_equal(reg.x[0],x,4)
        e = np.array([-4.92843327])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,4)
        my = 9.5492931620846928
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 7.0388508798387219
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([ 1.06476526,  0.05548248,  0.04544514,  0.00614425,  0.01481356,
        0.00143001])
        np.testing.assert_array_almost_equal(reg.vm.diagonal(),vm,4)
        sig2 = [ 32.40685441]
        self.assertAlmostEqual(reg.sig2,sig2,4)
        pr2 = 0.3057664820364818
        self.assertAlmostEqual(reg.pr2,pr2)
        std_err = np.array([ 1.03187463,  0.23554719,  0.21317867,  0.07838525,  0.12171098,
        0.03781546])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,4)
        z_stat = [(5.9592751097983534, 2.5335926307459251e-09),
 (18.690182928021841, 5.9508619446611137e-78),
 (8.3421632936950338, 7.2943630281051907e-17),
 (-4.8232686291115678, 1.4122456582517099e-06),
 (3.9913060809142995, 6.5710406838016854e-05),
 (7.9088780724028922, 2.5971882547279339e-15)]
        np.testing.assert_array_almost_equal(reg.z_stat,z_stat,4)
        logll = -4471.407066887894
        self.assertAlmostEqual(reg.logll,logll,4)
        aic = 8952.8141337757879
        self.assertAlmostEqual(reg.aic,aic,4)
        schwarz = 8979.0779458660545
        self.assertAlmostEqual(reg.schwarz,schwarz,4)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_ml_error_regimes
import unittest
import pysal
import numpy as np
from pysal.spreg.ml_error_regimes import ML_Error_Regimes
from pysal.spreg.ml_error import ML_Error
from pysal.spreg import utils

class TestMLError(unittest.TestCase):
    def setUp(self):
        """
        db =  pysal.open(pysal.examples.get_path("baltim.dbf"),'r')
        self.ds_name = "baltim.dbf"
        self.y_name = "PRICE"
        self.y = np.array(db.by_col(self.y_name)).T
        self.y.shape = (len(self.y),1)
        self.x_names = ["NROOM","AGE","SQFT"]
        self.x = np.array([db.by_col(var) for var in self.x_names]).T
        ww = pysal.open(pysal.examples.get_path("baltim_q.gal"))
        self.w = ww.read()
        ww.close()
        self.w_name = "baltim_q.gal"
        self.w.transform = 'r'
        self.regimes = db.by_col("CITCOU")
        """
        #Artficial:
        n = 256
        self.n2 = n/2
        self.x_a1 = np.random.uniform(-10,10,(n,1))
        self.x_a2 = np.random.uniform(1,5,(n,1))
        self.q_a = self.x_a2 + np.random.normal(0,1,(n,1))
        self.x_a = np.hstack((self.x_a1,self.x_a2))
        self.y_a = np.dot(np.hstack((np.ones((n,1)),self.x_a)),np.array([[1],[0.5],[2]])) + np.random.normal(0,1,(n,1))
        latt = int(np.sqrt(n))
        self.w_a = pysal.lat2W(latt,latt)
        self.w_a.transform='r'
        self.regi_a = [0]*(n/2) + [1]*(n/2)
        self.w_a1 = pysal.lat2W(latt/2,latt)
        self.w_a1.transform='r'
    """
    def test_model1(self):
        reg = ML_Error_Regimes(self.y,self.x,self.regimes,w=self.w,name_y=self.y_name,name_x=self.x_names,\
               name_w=self.w_name,name_ds=self.ds_name,name_regimes="CITCOU", regime_err_sep=False)
        betas = np.array([[ -2.39491278],
       [  4.873757  ],
       [ -0.02911854],
       [  0.33275008],
       [ 31.79618475],
       [  2.98102401],
       [ -0.23710892],
       [  0.80581127],
       [  0.61770744]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        u = np.array([ 30.46599009])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        predy = np.array([ 16.53400991])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        n = 211
        self.assertAlmostEqual(reg.n,n,4)
        k = 8
        self.assertAlmostEqual(reg.k,k,4)
        y = np.array([ 47.])
        np.testing.assert_array_almost_equal(reg.y[0],y,4)
        x = np.array([   1.  ,    4.  ,  148.  ,   11.25,    0.  ,    0.  ,    0.  ,    0.  ])
        np.testing.assert_array_almost_equal(reg.x[0],x,4)
        e = np.array([ 34.69181334])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,4)
        my = 44.307180094786695
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 23.606076835380495
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([ 58.50551173,   2.42952002,   0.00721525,   0.06391736,
        80.59249161,   3.1610047 ,   0.0119782 ,   0.0499432 ,   0.00502785])
        np.testing.assert_array_almost_equal(reg.vm.diagonal(),vm,4)
        sig2 = np.array([[ 209.60639741]])
        self.assertAlmostEqual(reg.sig2,sig2,4)
        pr2 = 0.43600837301477025
        self.assertAlmostEqual(reg.pr2,pr2)
        std_err = np.array([ 7.64888957,  1.55869177,  0.08494262,  0.25281882,  8.9773321 ,
        1.77792146,  0.10944497,  0.22347975,  0.07090735])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,4)
        logll = -870.3331059537576
        self.assertAlmostEqual(reg.logll,logll,4)
        aic = 1756.6662119075154
        self.assertAlmostEqual(reg.aic,aic,4)
        schwarz = 1783.481076975324
        self.assertAlmostEqual(reg.schwarz,schwarz,4)
        chow_r = np.array([[ 8.40437046,  0.0037432 ],
       [ 0.64080535,  0.42341932],
       [ 2.25389396,  0.13327865],
       [ 1.96544702,  0.16093197]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,4)
        chow_j = 25.367913028011799
        self.assertAlmostEqual(reg.chow.joint[0],chow_j,4)

    def test_model2(self):
        reg = ML_Error_Regimes(self.y,self.x,self.regimes,w=self.w,name_y=self.y_name,name_x=self.x_names,\
               name_w=self.w_name,name_ds=self.ds_name,name_regimes="CITCOU", regime_err_sep=True)
        betas = np.array([[  3.66158216],
       [  4.55700255],
       [ -0.08045502],
       [  0.44800318],
       [  0.17774677],
       [ 33.3086368 ],
       [  2.44709405],
       [ -0.18803509],
       [  0.68956598],
       [  0.75599089]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        vm = np.array([ 40.60994599,  -7.25413138,  -0.16605501,   0.48961884,
         0.        ,   0.        ,   0.        ,   0.        ,
         0.        ,   0.        ])
        np.testing.assert_array_almost_equal(reg.vm[0],vm,4)
        u = np.array([ 31.97771505])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        predy = np.array([ 15.02228495])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        e = np.array([ 33.83065421])
        np.testing.assert_array_almost_equal(reg.e_filtered[0],e,4)
        chow_r = np.array([[  6.88023639,   0.0087154 ],
       [  0.90512612,   0.34141092],
       [  0.75996258,   0.38334023],
       [  0.56882946,   0.45072443],
       [ 12.18358581,   0.00048212]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,4)
        chow_j = 26.673798071789673
        self.assertAlmostEqual(reg.chow.joint[0],chow_j,4)
        #Artficial:
        model = ML_Error_Regimes(self.y_a, self.x_a, self.regi_a, w=self.w_a, regime_err_sep=True)
        model1 = ML_Error(self.y_a[0:(self.n2)].reshape((self.n2),1), self.x_a[0:(self.n2)], w=self.w_a1)
        model2 = ML_Error(self.y_a[(self.n2):].reshape((self.n2),1), self.x_a[(self.n2):], w=self.w_a1)
        tbetas = np.vstack((model1.betas, model2.betas))
        np.testing.assert_array_almost_equal(model.betas,tbetas)
        vm = np.hstack((model1.vm.diagonal(),model2.vm.diagonal()))
        np.testing.assert_array_almost_equal(model.vm.diagonal(), vm, 4)
    """
if __name__ == '__main__':
    unittest.main()


########NEW FILE########
__FILENAME__ = test_ml_lag
import unittest
import pysal
import numpy as np
from pysal.spreg.ml_lag import ML_Lag
from pysal.spreg import utils

class TestMLError(unittest.TestCase):
    def setUp(self):
        db =  pysal.open(pysal.examples.get_path("baltim.dbf"),'r')
        self.ds_name = "baltim.dbf"
        self.y_name = "PRICE"
        self.y = np.array(db.by_col(self.y_name)).T
        self.y.shape = (len(self.y),1)
        self.x_names = ["NROOM","AGE","SQFT"]
        self.x = np.array([db.by_col(var) for var in self.x_names]).T
        ww = pysal.open(pysal.examples.get_path("baltim_q.gal"))
        self.w = ww.read()
        ww.close()
        self.w_name = "baltim_q.gal"
        self.w.transform = 'r'

    def test_model1(self):
        reg = ML_Lag(self.y,self.x,w=self.w,name_y=self.y_name,name_x=self.x_names,\
               name_w=self.w_name,name_ds=self.ds_name)
        betas = np.array([[-6.04040164],
       [ 3.48995114],
       [-0.20103955],
       [ 0.65462382],
       [ 0.62351143]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        u = np.array([ 47.51218398])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        predy = np.array([-0.51218398])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        n = 211
        self.assertAlmostEqual(reg.n,n,4)
        k = 5
        self.assertAlmostEqual(reg.k,k,4)
        y = np.array([ 47.])
        np.testing.assert_array_almost_equal(reg.y[0],y,4)
        x = np.array([   1.  ,    4.  ,  148.  ,   11.25])
        np.testing.assert_array_almost_equal(reg.x[0],x,4)
        e = np.array([ 41.99251608])
        np.testing.assert_array_almost_equal(reg.e_pred[0],e,4)
        my = 44.307180094786695
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 23.606076835380495
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([ 28.57288755,   1.42341656,   0.00288068,   0.02956392,   0.00332139])
        np.testing.assert_array_almost_equal(reg.vm.diagonal(),vm,4)
        sig2 = 216.27525647243797
        self.assertAlmostEqual(reg.sig2,sig2,4)
        pr2 = 0.6133020721559487
        self.assertAlmostEqual(reg.pr2,pr2)
        std_err = np.array([ 5.34536131,  1.19307022,  0.05367198,  0.17194162,  0.05763147])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,4)
        logll = -875.92771143484833
        self.assertAlmostEqual(reg.logll,logll,4)
        aic = 1761.8554228696967
        self.assertAlmostEqual(reg.aic,aic,4)
        schwarz = 1778.614713537077
        self.assertAlmostEqual(reg.schwarz,schwarz,4)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_ml_lag_regimes
import unittest
import pysal
import numpy as np
from pysal.spreg.ml_lag_regimes import ML_Lag_Regimes
from pysal.spreg.ml_lag import ML_Lag
from pysal.spreg import utils
 
class TestMLError(unittest.TestCase):
    def setUp(self):
        db =  pysal.open(pysal.examples.get_path("baltim.dbf"),'r')
        self.ds_name = "baltim.dbf"
        self.y_name = "PRICE"
        self.y = np.array(db.by_col(self.y_name)).T
        self.y.shape = (len(self.y),1)
        self.x_names = ["NROOM","AGE","SQFT"]
        self.x = np.array([db.by_col(var) for var in self.x_names]).T
        ww = pysal.open(pysal.examples.get_path("baltim_q.gal"))
        self.w = ww.read()
        ww.close()
        self.w_name = "baltim_q.gal"
        self.w.transform = 'r'
        self.regimes = db.by_col("CITCOU")

    def test_model1(self):
        reg = ML_Lag_Regimes(self.y,self.x,self.regimes,w=self.w,name_y=self.y_name,name_x=self.x_names,\
               name_w=self.w_name,name_ds=self.ds_name,name_regimes="CITCOU", regime_lag_sep=False)
        betas = np.array([[-15.00586577],
       [  4.49600801],
       [ -0.03180518],
       [  0.34995882],
       [ -4.54040395],
       [  3.92187578],
       [ -0.17021393],
       [  0.81941371],
       [  0.53850323]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        u = np.array([ 32.73718478])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        predy = np.array([ 14.26281522])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        n = 211
        self.assertAlmostEqual(reg.n,n,4)
        k = 8
        self.assertAlmostEqual(reg.k,k,4)
        y = np.array([ 47.])
        np.testing.assert_array_almost_equal(reg.y[0],y,4)
        x = np.array([[   1.  ,    4.  ,  148.  ,   11.25,    0.  ,    0.  ,    0.  ,
           0.  ]])
        np.testing.assert_array_almost_equal(reg.x[0].toarray(),x,4)
        e = np.array([ 29.45407124])
        np.testing.assert_array_almost_equal(reg.e_pred[0],e,4)
        my = 44.307180094786695
        self.assertAlmostEqual(reg.mean_y,my)
        sy = 23.606076835380495
        self.assertAlmostEqual(reg.std_y,sy)
        vm = np.array([ 47.42000914,   2.39526578,   0.00506895,   0.06480022,
        69.67653371,   3.20661492,   0.01156766,   0.04862014,   0.00400775])
        np.testing.assert_array_almost_equal(reg.vm.diagonal(),vm,4)
        sig2 = 200.04433357145007
        self.assertAlmostEqual(reg.sig2,sig2,4)
        pr2 = 0.6404460298085746
        self.assertAlmostEqual(reg.pr2,pr2)
        std_err = np.array([ 6.88621878,  1.54766462,  0.07119654,  0.25455888,  8.34724707,
        1.79070235,  0.10755305,  0.22049975,  0.0633068 ])
        np.testing.assert_array_almost_equal(reg.std_err,std_err,4)
        logll = -864.98505596489736
        self.assertAlmostEqual(reg.logll,logll,4)
        aic = 1745.9701119297947
        self.assertAlmostEqual(reg.aic,aic,4)
        schwarz = 1772.7849769976033
        self.assertAlmostEqual(reg.schwarz,schwarz,4)
        chow_r = np.array([[ 1.00180776,  0.31687348],
       [ 0.05904944,  0.8080047 ],
       [ 1.16987812,  0.27942629],
       [ 1.95931177,  0.16158694]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,4)
        chow_j = 21.648337464039283
        self.assertAlmostEqual(reg.chow.joint[0],chow_j,4)

    def test_model2(self):
        reg = ML_Lag_Regimes(self.y,self.x,self.regimes,w=self.w,name_y=self.y_name,name_x=self.x_names,\
               name_w=self.w_name,name_ds=self.ds_name,name_regimes="CITCOU", regime_lag_sep=True)
        betas = np.array([[-0.71589799],
       [ 4.40910538],
       [-0.08652467],
       [ 0.46266265],
       [ 0.1627765 ],
       [-5.00594358],
       [ 2.91060349],
       [-0.18207394],
       [ 0.71129227],
       [ 0.66753263]])
        np.testing.assert_array_almost_equal(reg.betas,betas,4)
        vm = np.array([ 55.3593679 ,  -7.22927797,  -0.19487326,   0.6030953 ,
        -0.52249569,   0.        ,   0.        ,   0.        ,
         0.        ,   0.        ])
        np.testing.assert_array_almost_equal(reg.vm[0],vm,4)
        u = np.array([ 34.03630518])
        np.testing.assert_array_almost_equal(reg.u[0],u,4)
        predy = np.array([ 12.96369482])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,4)
        e = np.array([ 32.46466912])
        np.testing.assert_array_almost_equal(reg.e_pred[0],e,4)
        chow_r = np.array([[  0.15654726,   0.69235548],
       [  0.43533847,   0.509381  ],
       [  0.60552514,   0.43647766],
       [  0.59214981,   0.441589  ],
       [ 11.69437282,   0.00062689]])
        np.testing.assert_array_almost_equal(reg.chow.regi,chow_r,4)
        chow_j = 21.978012275873063
        self.assertAlmostEqual(reg.chow.joint[0],chow_j,4)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_ols
import unittest
import numpy as np
import pysal
from pysal.spreg import utils
import pysal.spreg as EC

PEGP = pysal.examples.get_path

class TestBaseOLS(unittest.TestCase):
    def setUp(self):
        db = pysal.open(PEGP('columbus.dbf'),'r')
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.w = pysal.weights.rook_from_shapefile(PEGP("columbus.shp"))

    def test_ols(self):
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        ols = EC.ols.BaseOLS(self.y,self.X)
        np.testing.assert_array_almost_equal(ols.betas, np.array([[
            46.42818268], [  0.62898397], [ -0.48488854]]))
        vm = np.array([[  1.74022453e+02,  -6.52060364e+00,  -2.15109867e+00],
           [ -6.52060364e+00,   2.87200008e-01,   6.80956787e-02],
           [ -2.15109867e+00,   6.80956787e-02,   3.33693910e-02]])
        np.testing.assert_array_almost_equal(ols.vm, vm,6)

    def test_ols_white1(self):
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        ols = EC.ols.BaseOLS(self.y,self.X,robust='white', sig2n_k=True)
        np.testing.assert_array_almost_equal(ols.betas, np.array([[
            46.42818268], [  0.62898397], [ -0.48488854]]))
        vm = np.array([[  2.05819450e+02,  -6.83139266e+00,  -2.64825846e+00],
       [ -6.83139266e+00,   2.58480813e-01,   8.07733167e-02],
       [ -2.64825846e+00,   8.07733167e-02,   3.75817181e-02]])
        np.testing.assert_array_almost_equal(ols.vm, vm,6)

    def test_ols_white2(self):
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        ols = EC.ols.BaseOLS(self.y,self.X,robust='white', sig2n_k=False)
        np.testing.assert_array_almost_equal(ols.betas, np.array([[
            46.42818268], [  0.62898397], [ -0.48488854]]))
        vm = np.array([[  1.93218259e+02,  -6.41314413e+00,  -2.48612018e+00],
       [ -6.41314413e+00,   2.42655457e-01,   7.58280116e-02],
       [ -2.48612018e+00,   7.58280116e-02,   3.52807966e-02]])
        np.testing.assert_array_almost_equal(ols.vm, vm,6)

    def test_OLS(self):
        ols = EC.OLS(self.y, self.X, self.w, spat_diag=True, moran=True, \
                white_test=True, name_y='home value', name_x=['income','crime'], \
                name_ds='columbus')
        
        np.testing.assert_array_almost_equal(ols.aic, \
                408.73548964604873 ,7)
        np.testing.assert_array_almost_equal(ols.ar2, \
                0.32123239427957662 ,7)
        np.testing.assert_array_almost_equal(ols.betas, \
                np.array([[ 46.42818268], [  0.62898397], \
                    [ -0.48488854]]), 7) 
        bp = np.array([2, 5.7667905131212587, 0.05594449410070558])
        ols_bp = np.array([ols.breusch_pagan['df'], ols.breusch_pagan['bp'], ols.breusch_pagan['pvalue']])
        np.testing.assert_array_almost_equal(bp, ols_bp, 7)
        np.testing.assert_array_almost_equal(ols.f_stat, \
            (12.358198885356581, 5.0636903313953024e-05), 7)
        jb = np.array([2, 39.706155069114878, 2.387360356860208e-09])
        ols_jb = np.array([ols.jarque_bera['df'], ols.jarque_bera['jb'], ols.jarque_bera['pvalue']])
        np.testing.assert_array_almost_equal(ols_jb,jb, 7)
        white = np.array([5, 2.90606708, 0.71446484])
        ols_white = np.array([ols.white['df'], ols.white['wh'], ols.white['pvalue']])
        np.testing.assert_array_almost_equal(ols_white,white, 7)
        np.testing.assert_equal(ols.k,  3)
        kb = {'df': 2, 'kb': 2.2700383871478675, 'pvalue': 0.32141595215434604}
        for key in kb:
            self.assertAlmostEqual(ols.koenker_bassett[key],  kb[key], 7)
        np.testing.assert_array_almost_equal(ols.lm_error, \
            (4.1508117035117893, 0.041614570655392716),7)
        np.testing.assert_array_almost_equal(ols.lm_lag, \
            (0.98279980617162233, 0.32150855529063727), 7)
        np.testing.assert_array_almost_equal(ols.lm_sarma, \
                (4.3222725729143736, 0.11519415308749938), 7)
        np.testing.assert_array_almost_equal(ols.logll, \
                -201.3677448230244 ,7)
        np.testing.assert_array_almost_equal(ols.mean_y, \
            38.436224469387746,7)
        np.testing.assert_array_almost_equal(ols.moran_res[0], \
            0.20373540938,7)
        np.testing.assert_array_almost_equal(ols.moran_res[1], \
            2.59180452208,7)
        np.testing.assert_array_almost_equal(ols.moran_res[2], \
            0.00954740031251,7)
        np.testing.assert_array_almost_equal(ols.mulColli, \
            12.537554873824675 ,7)
        np.testing.assert_equal(ols.n,  49)
        np.testing.assert_equal(ols.name_ds,  'columbus')
        np.testing.assert_equal(ols.name_gwk,  None)
        np.testing.assert_equal(ols.name_w,  'unknown')
        np.testing.assert_equal(ols.name_x,  ['CONSTANT', 'income', 'crime'])
        np.testing.assert_equal(ols.name_y,  'home value')
        np.testing.assert_array_almost_equal(ols.predy[3], np.array([
            33.53969014]),7)
        np.testing.assert_array_almost_equal(ols.r2, \
                0.34951437785126105 ,7)
        np.testing.assert_array_almost_equal(ols.rlm_error, \
                (3.3394727667427513, 0.067636278225568919),7)
        np.testing.assert_array_almost_equal(ols.rlm_lag, \
            (0.17146086940258459, 0.67881673703455414), 7)
        np.testing.assert_equal(ols.robust,  'unadjusted')
        np.testing.assert_array_almost_equal(ols.schwarz, \
            414.41095054038061,7 )
        np.testing.assert_array_almost_equal(ols.sig2, \
            231.4568494392652,7 )
        np.testing.assert_array_almost_equal(ols.sig2ML, \
            217.28602192257551,7 )
        np.testing.assert_array_almost_equal(ols.sig2n, \
                217.28602192257551, 7)
 
        np.testing.assert_array_almost_equal(ols.t_stat[2][0], \
                -2.65440864272,7)
        np.testing.assert_array_almost_equal(ols.t_stat[2][1], \
                0.0108745049098,7)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_ols_regimes
import unittest
import numpy as np
import pysal
from pysal.spreg.ols import OLS
from pysal.spreg.ols_regimes import OLS_Regimes

PEGP = pysal.examples.get_path

class TestOLS_regimes(unittest.TestCase):
    def setUp(self):
        db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
        self.y_var = 'CRIME'
        self.y = np.array([db.by_col(self.y_var)]).reshape(49,1)
        self.x_var = ['INC','HOVAL']
        self.x = np.array([db.by_col(name) for name in self.x_var]).T
        self.r_var = 'NSA'
        self.regimes = db.by_col(self.r_var)
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_OLS(self):
        start_suppress = np.get_printoptions()['suppress']
        np.set_printoptions(suppress=True)    
        ols = OLS_Regimes(self.y, self.x, self.regimes, w=self.w, regime_err_sep=False, constant_regi='many', nonspat_diag=False, spat_diag=True, name_y=self.y_var, name_x=self.x_var, name_ds='columbus', name_regimes=self.r_var, name_w='columbus.gal')        
        #np.testing.assert_array_almost_equal(ols.aic, 408.73548964604873 ,7)
        np.testing.assert_array_almost_equal(ols.ar2,0.50761700679873101 ,7)
        np.testing.assert_array_almost_equal(ols.betas,np.array([[ 68.78670869],\
                [ -1.9864167 ],[ -0.10887962],[ 67.73579559],[ -1.36937552],[ -0.31792362]])) 
        vm = np.array([ 48.81339213,  -2.14959579,  -0.19968157,   0.        ,
         0.        ,   0.        ])
        np.testing.assert_array_almost_equal(ols.vm[0], vm, 6)
        np.testing.assert_array_almost_equal(ols.lm_error, \
            (5.92970357,  0.01488775),7)
        np.testing.assert_array_almost_equal(ols.lm_lag, \
            (8.78315751,  0.00304024), 7)
        np.testing.assert_array_almost_equal(ols.lm_sarma, \
                (8.89955982,  0.01168114), 7)
        np.testing.assert_array_almost_equal(ols.mean_y, \
            35.1288238979591,7)
        np.testing.assert_equal(ols.k, 6)
        np.testing.assert_equal(ols.kf, 0)
        np.testing.assert_equal(ols.kr, 3)
        np.testing.assert_equal(ols.n, 49)
        np.testing.assert_equal(ols.nr, 2)
        np.testing.assert_equal(ols.name_ds,  'columbus')
        np.testing.assert_equal(ols.name_gwk,  None)
        np.testing.assert_equal(ols.name_w,  'columbus.gal')
        np.testing.assert_equal(ols.name_x,  ['0_CONSTANT', '0_INC', '0_HOVAL', '1_CONSTANT', '1_INC', '1_HOVAL'])
        np.testing.assert_equal(ols.name_y,  'CRIME')
        np.testing.assert_array_almost_equal(ols.predy[3], np.array([
            51.05003696]),7)
        np.testing.assert_array_almost_equal(ols.r2, \
                0.55890690192386316 ,7)
        np.testing.assert_array_almost_equal(ols.rlm_error, \
                (0.11640231,  0.73296972),7)
        np.testing.assert_array_almost_equal(ols.rlm_lag, \
            (2.96985625,  0.08482939), 7)
        np.testing.assert_equal(ols.robust,  'unadjusted')
        np.testing.assert_array_almost_equal(ols.sig2, \
            137.84897351821013,7 )
        np.testing.assert_array_almost_equal(ols.sig2n, \
                120.96950737312316, 7)
        np.testing.assert_array_almost_equal(ols.t_stat[2][0], \
                -0.43342216706091791,7)
        np.testing.assert_array_almost_equal(ols.t_stat[2][1], \
                0.66687472578594531,7)
        np.set_printoptions(suppress=start_suppress)        
    """
    def test_OLS_regi(self):
        #Artficial:
        n = 256
        x1 = np.random.uniform(-10,10,(n,1))
        y = np.dot(np.hstack((np.ones((n,1)),x1)),np.array([[1],[0.5]])) + np.random.normal(0,1,(n,1))
        latt = int(np.sqrt(n))
        regi = [0]*(n/2) + [1]*(n/2)
        model = OLS_Regimes(y, x1, regimes=regi, regime_err_sep=True, sig2n_k=False)
        model1 = OLS(y[0:(n/2)].reshape((n/2),1), x1[0:(n/2)], sig2n_k=False)
        model2 = OLS(y[(n/2):n].reshape((n/2),1), x1[(n/2):n], sig2n_k=False)
        tbetas = np.vstack((model1.betas, model2.betas))
        np.testing.assert_array_almost_equal(model.betas,tbetas)
        vm = np.hstack((model1.vm.diagonal(),model2.vm.diagonal()))
        np.testing.assert_array_almost_equal(model.vm.diagonal(), vm, 6)
        #Columbus:  
        reg = OLS_Regimes(self.y, self.x, self.regimes, w=self.w, constant_regi='many', nonspat_diag=True, spat_diag=True, name_y=self.y_var, name_x=self.x_var, name_ds='columbus', name_regimes=self.r_var, name_w='columbus.gal', regime_err_sep=True)        
        np.testing.assert_array_almost_equal(reg.multi[0].aic, 192.96044303402897 ,7)
        tbetas = np.array([[ 68.78670869],
       [ -1.9864167 ],
       [ -0.10887962],
       [ 67.73579559],
       [ -1.36937552],
       [ -0.31792362]])
        np.testing.assert_array_almost_equal(tbetas, reg.betas)
        vm = np.array([ 41.68828023,  -1.83582717,  -0.17053478,   0.        ,
         0.        ,   0.        ])
        np.testing.assert_array_almost_equal(reg.vm[0], vm, 6)
        u_3 = np.array([[ 0.31781838],
       [-5.6905584 ],
       [-6.8819715 ]])
        np.testing.assert_array_almost_equal(reg.u[0:3], u_3, 7)
        predy_3 = np.array([[ 15.40816162],
       [ 24.4923124 ],
       [ 37.5087525 ]])
        np.testing.assert_array_almost_equal(reg.predy[0:3], predy_3, 7)
        chow_regi = np.array([[ 0.01002733,  0.92023592],
       [ 0.46017009,  0.49754449],
       [ 0.60732697,  0.43579603]])
        np.testing.assert_array_almost_equal(reg.chow.regi, chow_regi, 7)
        self.assertAlmostEqual(reg.chow.joint[0], 0.67787986791767096, 7)
    """
    def test_OLS_fixed(self):
        start_suppress = np.get_printoptions()['suppress']
        np.set_printoptions(suppress=True)    
        ols = OLS_Regimes(self.y, self.x, self.regimes, w=self.w, cols2regi=[False,True], regime_err_sep=True, constant_regi='one', nonspat_diag=False, spat_diag=True, name_y=self.y_var, name_x=self.x_var, name_ds='columbus', name_regimes=self.r_var, name_w='columbus.gal')        
        np.testing.assert_array_almost_equal(ols.betas,np.array([[ -0.24385565], [ -0.26335026], [ 68.89701137], [ -1.67389685]])) 
        vm = np.array([ 0.02354271,  0.01246677,  0.00424658, -0.04921356])
        np.testing.assert_array_almost_equal(ols.vm[0], vm, 6)
        np.testing.assert_array_almost_equal(ols.lm_error, \
            (5.62668744,  0.01768903),7)
        np.testing.assert_array_almost_equal(ols.lm_lag, \
            (9.43264957,  0.00213156), 7)
        np.testing.assert_array_almost_equal(ols.mean_y, \
            35.12882389795919,7)
        np.testing.assert_equal(ols.kf, 2)
        np.testing.assert_equal(ols.kr, 1)
        np.testing.assert_equal(ols.n, 49)
        np.testing.assert_equal(ols.nr, 2)
        np.testing.assert_equal(ols.name_ds,  'columbus')
        np.testing.assert_equal(ols.name_gwk,  None)
        np.testing.assert_equal(ols.name_w,  'columbus.gal')
        np.testing.assert_equal(ols.name_x,  ['0_HOVAL', '1_HOVAL', '_Global_CONSTANT', '_Global_INC'])
        np.testing.assert_equal(ols.name_y,  'CRIME')
        np.testing.assert_array_almost_equal(ols.predy[3], np.array([
            52.65974636]),7)
        np.testing.assert_array_almost_equal(ols.r2, \
                0.5525561183786056 ,7)
        np.testing.assert_equal(ols.robust,  'unadjusted')
        np.testing.assert_array_almost_equal(ols.t_stat[2][0], \
                13.848705206463748,7)
        np.testing.assert_array_almost_equal(ols.t_stat[2][1], \
                7.776650625274256e-18,7)
        np.set_printoptions(suppress=start_suppress)
        
if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_ols_sparse
import unittest
import numpy as np
import pysal
import pysal.spreg as EC
from scipy import sparse

PEGP = pysal.examples.get_path

class TestBaseOLS(unittest.TestCase):
    def setUp(self):
        db = pysal.open(PEGP('columbus.dbf'),'r')
        y = np.array(db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("CRIME"))
        self.X = np.array(X).T
        self.w = pysal.weights.rook_from_shapefile(PEGP("columbus.shp"))

    def test_ols(self):
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.X = sparse.csr_matrix(self.X)
        ols = EC.ols.BaseOLS(self.y,self.X)
        np.testing.assert_array_almost_equal(ols.betas, np.array([[
            46.42818268], [  0.62898397], [ -0.48488854]]))
        vm = np.array([[  1.74022453e+02,  -6.52060364e+00,  -2.15109867e+00],
           [ -6.52060364e+00,   2.87200008e-01,   6.80956787e-02],
           [ -2.15109867e+00,   6.80956787e-02,   3.33693910e-02]])
        np.testing.assert_array_almost_equal(ols.vm, vm,6)

    def test_OLS(self):
        self.X = sparse.csr_matrix(self.X)
        ols = EC.OLS(self.y, self.X, self.w, spat_diag=True, moran=True, \
                name_y='home value', name_x=['income','crime'], \
                name_ds='columbus', nonspat_diag=True, white_test=True)
        
        np.testing.assert_array_almost_equal(ols.aic, \
                408.73548964604873 ,7)
        np.testing.assert_array_almost_equal(ols.ar2, \
                0.32123239427957662 ,7)
        np.testing.assert_array_almost_equal(ols.betas, \
                np.array([[ 46.42818268], [  0.62898397], \
                    [ -0.48488854]]), 7) 
        bp = np.array([2, 5.7667905131212587, 0.05594449410070558])
        ols_bp = np.array([ols.breusch_pagan['df'], ols.breusch_pagan['bp'], ols.breusch_pagan['pvalue']])
        np.testing.assert_array_almost_equal(bp, ols_bp, 7)
        np.testing.assert_array_almost_equal(ols.f_stat, \
            (12.358198885356581, 5.0636903313953024e-05), 7)
        jb = np.array([2, 39.706155069114878, 2.387360356860208e-09])
        ols_jb = np.array([ols.jarque_bera['df'], ols.jarque_bera['jb'], ols.jarque_bera['pvalue']])
        np.testing.assert_array_almost_equal(ols_jb,jb, 7)
        white = np.array([5, 2.90606708, 0.71446484])
        ols_white = np.array([ols.white['df'], ols.white['wh'], ols.white['pvalue']])
        np.testing.assert_array_almost_equal(ols_white,white, 7)
        np.testing.assert_equal(ols.k,  3)
        kb = {'df': 2, 'kb': 2.2700383871478675, 'pvalue': 0.32141595215434604}
        for key in kb:
            self.assertAlmostEqual(ols.koenker_bassett[key],  kb[key], 7)
        np.testing.assert_array_almost_equal(ols.lm_error, \
            (4.1508117035117893, 0.041614570655392716),7)
        np.testing.assert_array_almost_equal(ols.lm_lag, \
            (0.98279980617162233, 0.32150855529063727), 7)
        np.testing.assert_array_almost_equal(ols.lm_sarma, \
                (4.3222725729143736, 0.11519415308749938), 7)
        np.testing.assert_array_almost_equal(ols.logll, \
                -201.3677448230244 ,7)
        np.testing.assert_array_almost_equal(ols.mean_y, \
            38.436224469387746,7)
        np.testing.assert_array_almost_equal(ols.moran_res[0], \
            0.20373540938,7)
        np.testing.assert_array_almost_equal(ols.moran_res[1], \
            2.59180452208,7)
        np.testing.assert_array_almost_equal(ols.moran_res[2], \
            0.00954740031251,7)
        np.testing.assert_array_almost_equal(ols.mulColli, \
            12.537554873824675 ,7)
        np.testing.assert_equal(ols.n,  49)
        np.testing.assert_equal(ols.name_ds,  'columbus')
        np.testing.assert_equal(ols.name_gwk,  None)
        np.testing.assert_equal(ols.name_w,  'unknown')
        np.testing.assert_equal(ols.name_x,  ['CONSTANT', 'income', 'crime'])
        np.testing.assert_equal(ols.name_y,  'home value')
        np.testing.assert_array_almost_equal(ols.predy[3], np.array([
            33.53969014]),7)
        np.testing.assert_array_almost_equal(ols.r2, \
                0.34951437785126105 ,7)
        np.testing.assert_array_almost_equal(ols.rlm_error, \
                (3.3394727667427513, 0.067636278225568919),7)
        np.testing.assert_array_almost_equal(ols.rlm_lag, \
            (0.17146086940258459, 0.67881673703455414), 7)
        np.testing.assert_equal(ols.robust,  'unadjusted')
        np.testing.assert_array_almost_equal(ols.schwarz, \
            414.41095054038061,7 )
        np.testing.assert_array_almost_equal(ols.sig2, \
            231.4568494392652,7 )
        np.testing.assert_array_almost_equal(ols.sig2ML, \
            217.28602192257551,7 )
        np.testing.assert_array_almost_equal(ols.sig2n, \
                217.28602192257551, 7)
 
        np.testing.assert_array_almost_equal(ols.t_stat[2][0], \
                -2.65440864272,7)
        np.testing.assert_array_almost_equal(ols.t_stat[2][1], \
                0.0108745049098,7)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_probit
import unittest
import pysal
import numpy as np
from pysal.spreg import probit as PB

class TestBaseProbit(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("CRIME"))
        y = np.reshape(y, (49,1))
        self.y = (y>40).astype(float)
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("HOVAL"))
        self.X = np.array(X).T
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        reg = PB.BaseProbit(self.y, self.X, w=self.w)
        betas = np.array([[ 3.35381078], [-0.1996531 ], [-0.02951371]])
        np.testing.assert_array_almost_equal(reg.betas,betas,6)
        predy = np.array([ 0.00174739])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,6)
        n = 49
        self.assertAlmostEqual(reg.n,n,6)
        k = 3
        self.assertAlmostEqual(reg.k,k,6)
        y = np.array([ 0.])
        np.testing.assert_array_almost_equal(reg.y[0],y,6)
        x = np.array([  1.      ,  19.531   ,  80.467003])
        np.testing.assert_array_almost_equal(reg.x[0],x,6)
        vm = np.array([[  8.52813879e-01,  -4.36272459e-02,  -8.05171472e-03], [ -4.36272459e-02,   4.11381444e-03,  -1.92834842e-04], [ -8.05171472e-03,  -1.92834842e-04,   3.09660240e-04]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        xmean = np.array([[  1.        ], [ 14.37493876], [ 38.43622447 ]])
        np.testing.assert_array_almost_equal(reg.xmean,xmean,6)        
        predpc = 85.714285714285708
        self.assertAlmostEqual(reg.predpc,predpc,5)
        logl = -20.06009093055782
        self.assertAlmostEqual(reg.logl,logl,5)
        scale = 0.23309310130643665
        self.assertAlmostEqual(reg.scale,scale,5)
        slopes = np.array([[-0.04653776], [-0.00687944]])
        np.testing.assert_array_almost_equal(reg.slopes,slopes,6)
        slopes_vm = np.array([[  1.77101993e-04,  -1.65021168e-05], [ -1.65021168e-05,   1.60575016e-05]])
        np.testing.assert_array_almost_equal(reg.slopes_vm,slopes_vm,6)
        LR = 25.317683245671716
        self.assertAlmostEqual(reg.LR[0],LR,5)
        Pinkse_error = 2.9632385352516728
        self.assertAlmostEqual(reg.Pinkse_error[0],Pinkse_error,5)
        KP_error = 1.6509224700582124
        self.assertAlmostEqual(reg.KP_error[0],KP_error,5)
        PS_error = 2.3732463777623511
        self.assertAlmostEqual(reg.PS_error[0],PS_error,5)

class TestProbit(unittest.TestCase):
    def setUp(self):
        db=pysal.open(pysal.examples.get_path("columbus.dbf"),"r")
        y = np.array(db.by_col("CRIME"))
        y = np.reshape(y, (49,1))
        self.y = (y>40).astype(float)
        X = []
        X.append(db.by_col("INC"))
        X.append(db.by_col("HOVAL"))
        self.X = np.array(X).T
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'

    def test_model(self):
        reg = PB.Probit(self.y, self.X, w=self.w)
        betas = np.array([[ 3.35381078], [-0.1996531 ], [-0.02951371]])
        np.testing.assert_array_almost_equal(reg.betas,betas,6)
        predy = np.array([ 0.00174739])
        np.testing.assert_array_almost_equal(reg.predy[0],predy,6)
        n = 49
        self.assertAlmostEqual(reg.n,n,6)
        k = 3
        self.assertAlmostEqual(reg.k,k,6)
        y = np.array([ 0.])
        np.testing.assert_array_almost_equal(reg.y[0],y,6)
        x = np.array([  1.      ,  19.531   ,  80.467003])
        np.testing.assert_array_almost_equal(reg.x[0],x,6)
        vm = np.array([[  8.52813879e-01,  -4.36272459e-02,  -8.05171472e-03], [ -4.36272459e-02,   4.11381444e-03,  -1.92834842e-04], [ -8.05171472e-03,  -1.92834842e-04,   3.09660240e-04]])
        np.testing.assert_array_almost_equal(reg.vm,vm,6)
        xmean = np.array([[  1.        ], [ 14.37493876], [ 38.43622447 ]])
        np.testing.assert_array_almost_equal(reg.xmean,xmean,6)        
        predpc = 85.714285714285708
        self.assertAlmostEqual(reg.predpc,predpc,5)
        logl = -20.06009093055782
        self.assertAlmostEqual(reg.logl,logl,5)
        scale = 0.23309310130643665
        self.assertAlmostEqual(reg.scale,scale,5)
        slopes = np.array([[-0.04653776], [-0.00687944]])
        np.testing.assert_array_almost_equal(reg.slopes,slopes,6)
        slopes_vm = np.array([[  1.77101993e-04,  -1.65021168e-05], [ -1.65021168e-05,   1.60575016e-05]])
        np.testing.assert_array_almost_equal(reg.slopes_vm,slopes_vm,6)
        LR = 25.317683245671716
        self.assertAlmostEqual(reg.LR[0],LR,5)
        Pinkse_error = 2.9632385352516728
        self.assertAlmostEqual(reg.Pinkse_error[0],Pinkse_error,5)
        KP_error = 1.6509224700582124
        self.assertAlmostEqual(reg.KP_error[0],KP_error,5)
        PS_error = 2.3732463777623511
        self.assertAlmostEqual(reg.PS_error[0],PS_error,5)
        
if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_twosls
import unittest
import numpy as np
import pysal
from pysal.spreg.twosls import BaseTSLS, TSLS

class TestBaseTSLS(unittest.TestCase):
    def setUp(self):
        db = pysal.open(pysal.examples.get_path("columbus.dbf"),'r')
        self.y = np.array(db.by_col("CRIME"))
        self.y = np.reshape(self.y, (49,1))
        self.X = []
        self.X.append(db.by_col("INC"))
        self.X = np.array(self.X).T
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.yd = []
        self.yd.append(db.by_col("HOVAL"))
        self.yd = np.array(self.yd).T
        self.q = []
        self.q.append(db.by_col("DISCBD"))
        self.q = np.array(self.q).T

    def test_basic(self):
        reg = BaseTSLS(self.y, self.X, self.yd, self.q)
        betas = np.array([[ 88.46579584], [  0.5200379 ], [ -1.58216593]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        h_0 = np.array([  1.   ,  19.531,   5.03 ])
        np.testing.assert_array_almost_equal(reg.h[0], h_0)
        hth = np.array([[    49.        ,    704.371999  ,    139.75      ],
                        [   704.371999  ,  11686.67338121,   2246.12800625],
                        [   139.75      ,   2246.12800625,    498.5851    ]])
        np.testing.assert_array_almost_equal(reg.hth, hth, 7)
        hthi = np.array([[ 0.1597275 , -0.00762011, -0.01044191],
                        [-0.00762011,  0.00100135, -0.0023752 ],
                        [-0.01044191, -0.0023752 ,  0.01563276]]) 
        np.testing.assert_array_almost_equal(reg.hthi, hthi, 7)
        self.assertEqual(reg.k, 3)
        self.assertEqual(reg.kstar, 1)
        self.assertAlmostEqual(reg.mean_y, 35.128823897959187, 7)
        self.assertEqual(reg.n, 49)
        pfora1a2 = np.array([[ 9.58156106, -0.22744226, -0.13820537],
                             [ 0.02580142,  0.08226331, -0.03143731],
                             [-3.13896453, -0.33487872,  0.20690965]]) 
        np.testing.assert_array_almost_equal(reg.pfora1a2, pfora1a2, 7)
        predy_5 = np.array([[-28.68949467], [ 28.99484984], [ 55.07344824], [ 38.26609504], [ 57.57145851]]) 
        np.testing.assert_array_almost_equal(reg.predy[0:5], predy_5, 7)
        q_5 = np.array([[ 5.03], [ 4.27], [ 3.89], [ 3.7 ], [ 2.83]])
        np.testing.assert_array_equal(reg.q[0:5], q_5)
        self.assertAlmostEqual(reg.sig2n_k, 587.56797852699822, 7)
        self.assertAlmostEqual(reg.sig2n, 551.5944288212637, 7)
        self.assertAlmostEqual(reg.sig2, 551.5944288212637, 7)
        self.assertAlmostEqual(reg.std_y, 16.732092091229699, 7)
        u_5 = np.array([[ 44.41547467], [-10.19309584], [-24.44666724], [ -5.87833504], [ -6.83994851]]) 
        np.testing.assert_array_almost_equal(reg.u[0:5], u_5, 7)
        self.assertAlmostEqual(reg.utu, 27028.127012241919, 7)
        varb = np.array([[ 0.41526237,  0.01879906, -0.01730372],
                         [ 0.01879906,  0.00362823, -0.00184604],
                         [-0.01730372, -0.00184604,  0.0011406 ]]) 
        np.testing.assert_array_almost_equal(reg.varb, varb, 7)
        vm = np.array([[ 229.05640809,   10.36945783,   -9.54463414],
                       [  10.36945783,    2.0013142 ,   -1.01826408],
                       [  -9.54463414,   -1.01826408,    0.62914915]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)
        x_0 = np.array([  1.   ,  19.531])
        np.testing.assert_array_almost_equal(reg.x[0], x_0, 7)
        y_5 = np.array([[ 15.72598 ], [ 18.801754], [ 30.626781], [ 32.38776 ], [ 50.73151 ]]) 
        np.testing.assert_array_almost_equal(reg.y[0:5], y_5, 7)
        yend_5 = np.array([[ 80.467003], [ 44.567001], [ 26.35    ], [ 33.200001], [ 23.225   ]]) 
        np.testing.assert_array_almost_equal(reg.yend[0:5], yend_5, 7)
        z_0 = np.array([  1.      ,  19.531   ,  80.467003]) 
        np.testing.assert_array_almost_equal(reg.z[0], z_0, 7)
        zthhthi = np.array([[  1.00000000e+00,  -1.66533454e-16,   4.44089210e-16],
                            [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00],
                            [  1.26978671e+01,   1.05598709e+00,   3.70212359e+00]]) 
        np.testing.assert_array_almost_equal(reg.zthhthi, zthhthi, 7)
        
    def test_n_k(self):
        reg = BaseTSLS(self.y, self.X, self.yd, self.q, sig2n_k=True)
        betas = np.array([[ 88.46579584], [  0.5200379 ], [ -1.58216593]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array([[ 243.99486949,   11.04572682,  -10.16711028],
                       [  11.04572682,    2.13183469,   -1.08467261],
                       [ -10.16711028,   -1.08467261,    0.67018062]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)

    def test_white(self):
        reg = BaseTSLS(self.y, self.X, self.yd, self.q, robust='white')
        betas = np.array([[ 88.46579584], [  0.5200379 ], [ -1.58216593]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array([[ 208.27139316,   15.6687805 ,  -11.53686154],
                       [  15.6687805 ,    2.26882747,   -1.30312033],
                       [ -11.53686154,   -1.30312033,    0.81940656]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)

    def test_hac(self):
        gwk = pysal.kernelW_from_shapefile(pysal.examples.get_path('columbus.shp'),k=15,function='triangular', fixed=False)
        reg = BaseTSLS(self.y, self.X, self.yd, self.q, robust='hac', gwk=gwk)
        betas = np.array([[ 88.46579584], [  0.5200379 ], [ -1.58216593]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array([[ 231.07254978,   15.42050291,  -11.3941033 ],
                       [  15.01376346,    1.92422887,   -1.11865505],
                       [ -11.34381641,   -1.1279227 ,    0.72053806]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)

class TestTSLS(unittest.TestCase):
    def setUp(self):
        db = pysal.open(pysal.examples.get_path("columbus.dbf"),'r')
        self.y = np.array(db.by_col("CRIME"))
        self.y = np.reshape(self.y, (49,1))
        self.X = []
        self.X.append(db.by_col("INC"))
        self.X = np.array(self.X).T
        self.yd = []
        self.yd.append(db.by_col("HOVAL"))
        self.yd = np.array(self.yd).T
        self.q = []
        self.q.append(db.by_col("DISCBD"))
        self.q = np.array(self.q).T

    def test_basic(self):
        reg = TSLS(self.y, self.X, self.yd, self.q)
        betas = np.array([[ 88.46579584], [  0.5200379 ], [ -1.58216593]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        h_0 = np.array([  1.   ,  19.531,   5.03 ])
        np.testing.assert_array_almost_equal(reg.h[0], h_0)
        hth = np.array([[    49.        ,    704.371999  ,    139.75      ],
                        [   704.371999  ,  11686.67338121,   2246.12800625],
                        [   139.75      ,   2246.12800625,    498.5851    ]])
        np.testing.assert_array_almost_equal(reg.hth, hth, 7)
        hthi = np.array([[ 0.1597275 , -0.00762011, -0.01044191],
                        [-0.00762011,  0.00100135, -0.0023752 ],
                        [-0.01044191, -0.0023752 ,  0.01563276]]) 
        np.testing.assert_array_almost_equal(reg.hthi, hthi, 7)
        self.assertEqual(reg.k, 3)
        self.assertEqual(reg.kstar, 1)
        self.assertAlmostEqual(reg.mean_y, 35.128823897959187, 7)
        self.assertEqual(reg.n, 49)
        pfora1a2 = np.array([[ 9.58156106, -0.22744226, -0.13820537],
                             [ 0.02580142,  0.08226331, -0.03143731],
                             [-3.13896453, -0.33487872,  0.20690965]]) 
        np.testing.assert_array_almost_equal(reg.pfora1a2, pfora1a2, 7)
        predy_5 = np.array([[-28.68949467], [ 28.99484984], [ 55.07344824], [ 38.26609504], [ 57.57145851]]) 
        np.testing.assert_array_almost_equal(reg.predy[0:5], predy_5, 7)
        q_5 = np.array([[ 5.03], [ 4.27], [ 3.89], [ 3.7 ], [ 2.83]])
        np.testing.assert_array_equal(reg.q[0:5], q_5)
        self.assertAlmostEqual(reg.sig2n_k, 587.56797852699822, 7)
        self.assertAlmostEqual(reg.sig2n, 551.5944288212637, 7)
        self.assertAlmostEqual(reg.sig2, 551.5944288212637, 7)
        self.assertAlmostEqual(reg.std_y, 16.732092091229699, 7)
        u_5 = np.array([[ 44.41547467], [-10.19309584], [-24.44666724], [ -5.87833504], [ -6.83994851]]) 
        np.testing.assert_array_almost_equal(reg.u[0:5], u_5, 7)
        self.assertAlmostEqual(reg.utu, 27028.127012241919, 7)
        varb = np.array([[ 0.41526237,  0.01879906, -0.01730372],
                         [ 0.01879906,  0.00362823, -0.00184604],
                         [-0.01730372, -0.00184604,  0.0011406 ]]) 
        np.testing.assert_array_almost_equal(reg.varb, varb, 7)
        vm = np.array([[ 229.05640809,   10.36945783,   -9.54463414],
                       [  10.36945783,    2.0013142 ,   -1.01826408],
                       [  -9.54463414,   -1.01826408,    0.62914915]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)
        x_0 = np.array([  1.   ,  19.531])
        np.testing.assert_array_almost_equal(reg.x[0], x_0, 7)
        y_5 = np.array([[ 15.72598 ], [ 18.801754], [ 30.626781], [ 32.38776 ], [ 50.73151 ]]) 
        np.testing.assert_array_almost_equal(reg.y[0:5], y_5, 7)
        yend_5 = np.array([[ 80.467003], [ 44.567001], [ 26.35    ], [ 33.200001], [ 23.225   ]]) 
        np.testing.assert_array_almost_equal(reg.yend[0:5], yend_5, 7)
        z_0 = np.array([  1.      ,  19.531   ,  80.467003]) 
        np.testing.assert_array_almost_equal(reg.z[0], z_0, 7)
        zthhthi = np.array([[  1.00000000e+00,  -1.66533454e-16,   4.44089210e-16],
                            [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00],
                            [  1.26978671e+01,   1.05598709e+00,   3.70212359e+00]]) 
        np.testing.assert_array_almost_equal(reg.zthhthi, zthhthi, 7)
        self.assertAlmostEqual(reg.pr2, 0.27936137128173893, 7)
        z_stat = np.array([[  5.84526447e+00,   5.05764078e-09],
                           [  3.67601567e-01,   7.13170346e-01],
                           [ -1.99468913e+00,   4.60767956e-02]])
        np.testing.assert_array_almost_equal(reg.z_stat, z_stat, 7)
        title = 'TWO STAGE LEAST SQUARES'
        self.assertEqual(reg.title, title)
        
    def test_n_k(self):
        reg = TSLS(self.y, self.X, self.yd, self.q, sig2n_k=True)
        betas = np.array([[ 88.46579584], [  0.5200379 ], [ -1.58216593]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array([[ 243.99486949,   11.04572682,  -10.16711028],
                       [  11.04572682,    2.13183469,   -1.08467261],
                       [ -10.16711028,   -1.08467261,    0.67018062]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)

    def test_white(self):
        reg = TSLS(self.y, self.X, self.yd, self.q, robust='white')
        betas = np.array([[ 88.46579584], [  0.5200379 ], [ -1.58216593]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array([[ 208.27139316,   15.6687805 ,  -11.53686154],
                       [  15.6687805 ,    2.26882747,   -1.30312033],
                       [ -11.53686154,   -1.30312033,    0.81940656]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)
        self.assertEqual(reg.robust, 'white')

    def test_hac(self):
        gwk = pysal.kernelW_from_shapefile(pysal.examples.get_path('columbus.shp'),k=5,function='triangular', fixed=False)
        reg = TSLS(self.y, self.X, self.yd, self.q, robust='hac', gwk=gwk)
        betas = np.array([[ 88.46579584], [  0.5200379 ], [ -1.58216593]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array([[ 225.0795089 ,   17.11660041,  -12.22448566],
                       [  17.67097154,    2.47483461,   -1.4183641 ],
                       [ -12.45093722,   -1.40495464,    0.8700441 ]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)
        self.assertEqual(reg.robust, 'hac')

    def test_spatial(self):
        w = pysal.queen_from_shapefile(pysal.examples.get_path('columbus.shp'))
        reg = TSLS(self.y, self.X, self.yd, self.q, spat_diag=True, w=w)
        betas = np.array([[ 88.46579584], [  0.5200379 ], [ -1.58216593]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array([[ 229.05640809,   10.36945783,   -9.54463414],
                       [  10.36945783,    2.0013142 ,   -1.01826408],
                       [  -9.54463414,   -1.01826408,    0.62914915]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)
        ak_test = np.array([ 1.16816972,  0.27977763])
        np.testing.assert_array_almost_equal(reg.ak_test, ak_test, 7)

    def test_names(self):
        w = pysal.queen_from_shapefile(pysal.examples.get_path('columbus.shp'))
        gwk = pysal.kernelW_from_shapefile(pysal.examples.get_path('columbus.shp'),k=5,function='triangular', fixed=False)
        name_x = ['inc']
        name_y = 'crime'
        name_yend = ['hoval']
        name_q = ['discbd']
        name_w = 'queen'
        name_gwk = 'k=5'
        name_ds = 'columbus'
        reg = TSLS(self.y, self.X, self.yd, self.q,
                spat_diag=True, w=w, robust='hac', gwk=gwk,
                name_x=name_x, name_y=name_y, name_q=name_q, name_w=name_w,
                name_yend=name_yend, name_gwk=name_gwk, name_ds=name_ds)
        betas = np.array([[ 88.46579584], [  0.5200379 ], [ -1.58216593]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array([[ 225.0795089 ,   17.11660041,  -12.22448566],
                       [  17.67097154,    2.47483461,   -1.4183641 ],
                       [ -12.45093722,   -1.40495464,    0.8700441 ]])
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)
        self.assertListEqual(reg.name_x, ['CONSTANT']+name_x)
        self.assertListEqual(reg.name_yend, name_yend)
        self.assertListEqual(reg.name_q, name_q)
        self.assertEqual(reg.name_y, name_y)
        self.assertEqual(reg.name_w, name_w)
        self.assertEqual(reg.name_gwk, name_gwk)
        self.assertEqual(reg.name_ds, name_ds)

    


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_twosls_regimes
import unittest
import numpy as np
import pysal
from pysal.spreg.twosls_regimes import TSLS_Regimes
from pysal.spreg.twosls import TSLS

class TestTSLS(unittest.TestCase):
    def setUp(self):
        db = pysal.open(pysal.examples.get_path("columbus.dbf"),'r')
        self.y = np.array(db.by_col("CRIME"))
        self.y = np.reshape(self.y, (49,1))
        self.x = []
        self.x.append(db.by_col("INC"))
        self.x = np.array(self.x).T
        self.yd = []
        self.yd.append(db.by_col("HOVAL"))
        self.yd = np.array(self.yd).T
        self.q = []
        self.q.append(db.by_col("DISCBD"))
        self.q = np.array(self.q).T
        self.r_var = 'NSA'
        self.regimes = db.by_col(self.r_var)

    def test_basic(self):
        reg = TSLS_Regimes(self.y, self.x, self.yd, self.q, self.regimes, regime_err_sep=False)        
        betas = np.array([[ 80.23408166],[  5.48218125],[ 82.98396737],[  0.49775429],[ -3.72663211],[ -1.27451485]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        h_0 = np.array([[  0.   ,   0.   ,   1.   ,  19.531,   0.   ,   5.03 ]])
        np.testing.assert_array_almost_equal(reg.h[0]*np.eye(6), h_0)
        hth = np.array([[   25.        ,   416.378999  ,     0.        ,     0.        ,\
           76.03      ,     0.        ],\
       [  416.378999  ,  7831.05477839,     0.        ,     0.        ,\
         1418.65422625,     0.        ],\
       [    0.        ,     0.        ,    24.        ,   287.993     ,\
            0.        ,    63.72      ],\
       [    0.        ,     0.        ,   287.993     ,  3855.61860282,\
            0.        ,   827.47378   ],\
       [   76.03      ,  1418.65422625,     0.        ,     0.        ,\
          291.9749    ,     0.        ],\
       [    0.        ,     0.        ,    63.72      ,   827.47378   ,\
            0.        ,   206.6102    ]])
        np.testing.assert_array_almost_equal(reg.hth, hth, 7)
        hthi = np.array([[ 0.3507855 , -0.0175615 ,  0.        ,  0.        , -0.00601601,\
         0.        ],\
       [-0.0175615 ,  0.00194521, -0.        , -0.        , -0.00487844,\
        -0.        ],\
       [ 0.        ,  0.        ,  0.42327489, -0.02563036,  0.        ,\
        -0.02789128],\
       [-0.        , -0.        , -0.02563036,  0.00339841, -0.        ,\
        -0.00570605],\
       [-0.00601601, -0.00487844,  0.        ,  0.        ,  0.02869498,\
         0.        ],\
       [ 0.        ,  0.        , -0.02789128, -0.00570605,  0.        ,\
         0.03629464]])
        np.testing.assert_array_almost_equal(reg.hthi, hthi, 7)
        self.assertEqual(reg.k, 6)
        self.assertEqual(reg.kstar, 2)
        self.assertAlmostEqual(reg.mean_y, 35.128823897959187, 7)
        np.testing.assert_equal(reg.kf, 0)
        np.testing.assert_equal(reg.kr, 3)
        np.testing.assert_equal(reg.n, 49)
        np.testing.assert_equal(reg.nr, 2)
        pfora1a2 = np.array([[ 17.80208995,  -0.46997739,   0.        ,   0.        ,\
         -0.21344994,   0.        ],\
       [ -0.36293902,   0.41200496,   0.        ,   0.        ,\
         -0.17308863,   0.        ],\
       [  0.        ,   0.        ,  23.8584271 ,  -0.96035493,\
          0.        ,  -0.26149141],\
       [  0.        ,   0.        ,  -0.61800983,   0.2269828 ,\
          0.        ,  -0.05349643],\
       [ -3.22151864,  -2.10181214,   0.        ,   0.        ,\
          1.01810757,   0.        ],\
       [  0.        ,   0.        ,  -5.42403871,  -0.6641704 ,\
          0.        ,   0.34027606]]) 
        np.testing.assert_array_almost_equal(reg.pfora1a2, pfora1a2, 7)
        predy_5 = np.array([[ -9.85078372],[ 36.75098196],[ 57.34266859],[ 42.89851907],[ 58.9840913 ]]) 
        np.testing.assert_array_almost_equal(reg.predy[0:5], predy_5, 7)
        q_5 = np.array([ 5.03,  4.27,  3.89,  3.7 ,  2.83])
        np.testing.assert_array_equal((reg.q[0:5].T*np.eye(5))[1,:], q_5)
        self.assertAlmostEqual(reg.sig2n_k, 990.00750983736714, 7)
        self.assertAlmostEqual(reg.sig2n, 868.78210046952631, 7)
        self.assertAlmostEqual(reg.sig2, 990.00750983736714, 7)
        self.assertAlmostEqual(reg.std_y, 16.732092091229699, 7)
        u_5 = np.array([[ 25.57676372],[-17.94922796],[-26.71588759],[-10.51075907],[ -8.2525813 ]]) 
        np.testing.assert_array_almost_equal(reg.u[0:5], u_5, 7)
        self.assertAlmostEqual(reg.utu, 42570.322923006788, 7)
        varb = np.array([[ 0.50015831,  0.07969376,  0.        ,  0.        , -0.04760541,\
         0.        ],\
       [ 0.07969376,  0.06523527,  0.        ,  0.        , -0.03105915,\
         0.        ],\
       [ 0.        ,  0.        ,  0.73944792,  0.01132445,  0.        ,\
        -0.02117969],\
       [ 0.        ,  0.        ,  0.01132445,  0.00756336,  0.        ,\
        -0.00259344],\
       [-0.04760541, -0.03105915, -0.        , -0.        ,  0.0150449 ,\
        -0.        ],\
       [-0.        , -0.        , -0.02117969, -0.00259344, -0.        ,\
         0.0013287 ]]) 
        np.testing.assert_array_almost_equal(reg.varb, varb, 7)
        vm = np.array([[ 495.16048523,   78.89742341,    0.        ,    0.        ,\
         -47.12971066,    0.        ],\
       [  78.89742341,   64.58341083,    0.        ,    0.        ,\
         -30.74878934,    0.        ],\
       [   0.        ,    0.        ,  732.05899155,   11.21128921,\
           0.        ,  -20.96804956],\
       [   0.        ,    0.        ,   11.21128921,    7.48778398,\
           0.        ,   -2.56752553],\
       [ -47.12971066,  -30.74878934,    0.        ,    0.        ,\
          14.89456384,    0.        ],\
       [   0.        ,    0.        ,  -20.96804956,   -2.56752553,\
           0.        ,    1.3154267 ]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)
        x_0 = np.array([[  0.   ,   0.   ,   1.   ,  19.531]])
        np.testing.assert_array_almost_equal(reg.x[0]*np.eye(4), x_0, 7)
        y_5 = np.array([[ 15.72598 ], [ 18.801754], [ 30.626781], [ 32.38776 ], [ 50.73151 ]]) 
        np.testing.assert_array_almost_equal(reg.y[0:5], y_5, 7)
        yend_3 = np.array([[  0.      ,  80.467003],[  0.      ,  44.567001],[  0.      ,  26.35    ]]) 
        np.testing.assert_array_almost_equal(reg.yend[0:3]*np.eye(2), yend_3, 7)
        z_0 = np.array([[  0.      ,   0.      ,   1.      ,  19.531   ,   0.      , 80.467003]]) 
        np.testing.assert_array_almost_equal(reg.z[0]*np.eye(6), z_0, 7)
        zthhthi = np.array([[  1.00000000e+00,   0.00000000e+00,   0.00000000e+00,\
          0.00000000e+00,  -4.44089210e-16,   0.00000000e+00],\
       [ -1.24344979e-14,   1.00000000e+00,   0.00000000e+00,\
          0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\
       [  0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\
          0.00000000e+00,   0.00000000e+00,  -1.11022302e-16],\
       [  0.00000000e+00,   0.00000000e+00,  -3.55271368e-15,\
          1.00000000e+00,   0.00000000e+00,   0.00000000e+00],\
       [  2.87468088e+00,   1.82963841e+00,   0.00000000e+00,\
          0.00000000e+00,   1.38104644e+00,   0.00000000e+00],\
       [  0.00000000e+00,   0.00000000e+00,   1.19237474e+01,\
          1.13018165e+00,   0.00000000e+00,   5.22645427e+00]]) 
        np.testing.assert_array_almost_equal(reg.zthhthi, zthhthi, 7)
        self.assertAlmostEqual(reg.pr2, 0.17729324026706564, 7)
        z_stat = np.array([[  3.60566933e+00,   3.11349387e-04],\
       [  6.82170447e-01,   4.95131179e-01],\
       [  3.06705211e+00,   2.16181168e-03],\
       [  1.81902371e-01,   8.55659343e-01],\
       [ -9.65611937e-01,   3.34238400e-01],\
       [ -1.11124949e+00,   2.66460976e-01]])
        np.testing.assert_array_almost_equal(np.array(reg.z_stat), z_stat, 7)
        chow_regi = np.array([[ 0.00616179,  0.93743265],
       [ 0.3447218 ,  0.55711631],
       [ 0.37093662,  0.54249417]])
        np.testing.assert_array_almost_equal(reg.chow.regi, chow_regi, 7)
        self.assertAlmostEqual(reg.chow.joint[0], 1.1353790779820598, 7)
        title = 'TWO STAGE LEAST SQUARES - REGIMES'
        self.assertEqual(reg.title, title)
        
    def test_n_k(self):
        reg = TSLS_Regimes(self.y, self.x, self.yd, self.q, self.regimes, sig2n_k=True, regime_err_sep=False)
        betas = np.array([[ 80.23408166],[  5.48218125],[ 82.98396737],[  0.49775429],[ -3.72663211],[ -1.27451485]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array([[ 495.16048523,   78.89742341,    0.        ,    0.        ,\
         -47.12971066,    0.        ],\
       [  78.89742341,   64.58341083,    0.        ,    0.        ,\
         -30.74878934,    0.        ],\
       [   0.        ,    0.        ,  732.05899155,   11.21128921,\
           0.        ,  -20.96804956],\
       [   0.        ,    0.        ,   11.21128921,    7.48778398,\
           0.        ,   -2.56752553],\
       [ -47.12971066,  -30.74878934,    0.        ,    0.        ,\
          14.89456384,    0.        ],\
       [   0.        ,    0.        ,  -20.96804956,   -2.56752553,\
           0.        ,    1.3154267 ]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)

    def test_spatial(self):
        w = pysal.queen_from_shapefile(pysal.examples.get_path('columbus.shp'))
        reg = TSLS_Regimes(self.y, self.x, self.yd, self.q, self.regimes, spat_diag=True, w=w, regime_err_sep=False)
        betas = np.array([[ 80.23408166],[  5.48218125],[ 82.98396737],[  0.49775429],[ -3.72663211],[ -1.27451485]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array([[ 495.16048523,   78.89742341,    0.        ,    0.        ,\
         -47.12971066,    0.        ],\
       [  78.89742341,   64.58341083,    0.        ,    0.        ,\
         -30.74878934,    0.        ],\
       [   0.        ,    0.        ,  732.05899155,   11.21128921,\
           0.        ,  -20.96804956],\
       [   0.        ,    0.        ,   11.21128921,    7.48778398,\
           0.        ,   -2.56752553],\
       [ -47.12971066,  -30.74878934,    0.        ,    0.        ,\
          14.89456384,    0.        ],\
       [   0.        ,    0.        ,  -20.96804956,   -2.56752553,\
           0.        ,    1.3154267 ]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)
        ak_test = np.array([ 0.69774552,  0.40354227])
        np.testing.assert_array_almost_equal(reg.ak_test, ak_test, 7)   

    def test_names(self):
        w = pysal.queen_from_shapefile(pysal.examples.get_path('columbus.shp'))
        gwk = pysal.kernelW_from_shapefile(pysal.examples.get_path('columbus.shp'),k=5,function='triangular', fixed=False)
        name_x = ['inc']
        name_y = 'crime'
        name_yend = ['hoval']
        name_q = ['discbd']
        name_w = 'queen'
        name_gwk = 'k=5'
        name_ds = 'columbus'
        name_regimes= 'nsa'
        reg = TSLS_Regimes(self.y, self.x, self.yd, self.q, self.regimes, regime_err_sep=False,
                spat_diag=True, w=w, robust='hac', gwk=gwk, name_regimes=name_regimes,
                name_x=name_x, name_y=name_y, name_q=name_q, name_w=name_w,
                name_yend=name_yend, name_gwk=name_gwk, name_ds=name_ds)
        betas = np.array([[ 80.23408166],[  5.48218125],[ 82.98396737],[  0.49775429],[ -3.72663211],[ -1.27451485]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array([[ 522.75813101,  120.64940697,  -15.60303241,   -0.976389  ,\
         -67.15556574,    0.64553579],\
       [ 122.83491674,  122.62303068,   -5.52270916,    0.05023488,\
         -57.89404902,    0.15750428],\
       [   0.1983661 ,   -0.03539147,  335.24731378,   17.40764168,\
          -0.26447114,  -14.3375455 ],\
       [  -0.13612426,   -0.43622084,   18.46644989,    2.70320508,\
           0.20398876,   -1.31821991],\
       [ -68.0704928 ,  -58.03685405,    2.66225388,    0.00323082,\
          27.68512974,   -0.08124602],\
       [  -0.08001296,    0.13575504,  -14.6998294 ,   -1.28225201,\
          -0.05193056,    0.79845124]])
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)
        self.assertEqual(reg.name_x, ['0_CONSTANT', '0_inc', '1_CONSTANT', '1_inc'])
        self.assertEqual(reg.name_yend, ['0_hoval', '1_hoval'])
        self.assertEqual(reg.name_q, ['0_discbd', '1_discbd'])
        self.assertEqual(reg.name_y, name_y)
        self.assertEqual(reg.name_w, name_w)
        self.assertEqual(reg.name_gwk, name_gwk)
        self.assertEqual(reg.name_ds, name_ds)
        self.assertEqual(reg.name_regimes, name_regimes)
    
    def test_regi_err(self):
        #Artficial:
        n = 256
        x1 = np.random.uniform(-10,10,(n,1))
        x2 = np.random.uniform(1,5,(n,1))
        q = x2 + np.random.normal(0,1,(n,1))
        x = np.hstack((x1,x2))
        y = np.dot(np.hstack((np.ones((n,1)),x)),np.array([[1],[0.5],[2]])) + np.random.normal(0,1,(n,1))
        latt = int(np.sqrt(n))
        regi = [0]*(n/2) + [1]*(n/2)
        model = TSLS_Regimes(y, x1, regimes=regi, q=q, yend=x2, regime_err_sep=True, sig2n_k=False)
        model1 = TSLS(y[0:(n/2)].reshape((n/2),1), x1[0:(n/2)], yend=x2[0:(n/2)], q=q[0:(n/2)], sig2n_k=False)
        model2 = TSLS(y[(n/2):n].reshape((n/2),1), x1[(n/2):n], yend=x2[(n/2):n], q=q[(n/2):n], sig2n_k=False)
        tbetas = np.vstack((model1.betas, model2.betas))
        np.testing.assert_array_almost_equal(model.betas,tbetas)
        vm = np.hstack((model1.vm.diagonal(),model2.vm.diagonal()))
        np.testing.assert_array_almost_equal(model.vm.diagonal(), vm, 6)
        #Columbus:
        reg = TSLS_Regimes(self.y, self.x, regimes=self.regimes, yend=self.yd, q=self.q, regime_err_sep=False)
        tbetas = np.array([[ 80.23408166],
       [  5.48218125],
       [ 82.98396737],
       [  0.49775429],
       [ -3.72663211],
       [ -1.27451485],])
        np.testing.assert_array_almost_equal(tbetas, reg.betas)
        vm = np.array([ 495.16048523,   78.89742341,    0.        ,    0.        ,
        -47.12971066,    0.        ])
        np.testing.assert_array_almost_equal(reg.vm[0], vm, 6)
        u_3 = np.array([[ 25.57676372],
       [-17.94922796],
       [-26.71588759]])
        np.testing.assert_array_almost_equal(reg.u[0:3], u_3, 7)
        predy_3 = np.array([[ -9.85078372],
       [ 36.75098196],
       [ 57.34266859]])
        np.testing.assert_array_almost_equal(reg.predy[0:3], predy_3, 7)
        chow_regi = np.array([[ 0.00616179,  0.93743265],
       [ 0.3447218 ,  0.55711631],
       [ 0.37093662,  0.54249417]])
        np.testing.assert_array_almost_equal(reg.chow.regi, chow_regi, 7)
        self.assertAlmostEqual(reg.chow.joint[0], 1.1353790779821029, 7)

if __name__ == '__main__':
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)  
    unittest.main()
    np.set_printoptions(suppress=start_suppress)        
    

########NEW FILE########
__FILENAME__ = test_twosls_sp
import unittest
import numpy as np
import pysal
import pysal.spreg.diagnostics as D
from pysal.spreg.twosls_sp import BaseGM_Lag, GM_Lag

class TestBaseGMLag(unittest.TestCase):
    def setUp(self):
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
        self.db = pysal.open(pysal.examples.get_path("columbus.dbf"), 'r')
        y = np.array(self.db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        
    def test___init__(self):
        w_lags = 2
        X = []
        X.append(self.db.by_col("INC"))
        X.append(self.db.by_col("CRIME"))
        self.X = np.array(X).T
        yd2, q2 = pysal.spreg.utils.set_endog(self.y, self.X, self.w, None, None, w_lags, True)
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        reg = BaseGM_Lag(self.y, self.X, yend=yd2, q=q2, w=self.w.sparse, w_lags=w_lags)
        betas = np.array([[  4.53017056e+01], [  6.20888617e-01], [ -4.80723451e-01], [  2.83622122e-02]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        h_0 = np.array([  1.        ,  19.531     ,  15.72598   ,  18.594     ,
                            24.7142675 ,  13.72216667,  27.82929567])
        np.testing.assert_array_almost_equal(reg.h[0], h_0)
        hth = np.  array([   49.        ,   704.371999  ,  1721.312371  ,   724.7435916 ,
                             1707.35412945,   711.31248483,  1729.63201243])
        np.testing.assert_array_almost_equal(reg.hth[0], hth, 7)
        hthi = np.array([  7.33701328e+00,   2.27764882e-02,   2.18153588e-02,
                           -5.11035447e-02,   1.22515181e-03,  -2.38079378e-01,
                           -1.20149133e-01])
        np.testing.assert_array_almost_equal(reg.hthi[0], hthi, 7)
        self.assertEqual(reg.k, 4)
        self.assertEqual(reg.kstar, 1)
        self.assertAlmostEqual(reg.mean_y, 38.436224469387746, 7)
        self.assertEqual(reg.n, 49)
        pfora1a2 = np.array([ 80.5588479 ,  -1.06625281,  -0.61703759,  -1.10071931]) 
        np.testing.assert_array_almost_equal(reg.pfora1a2[0], pfora1a2, 7)
        predy_5 = np.array([[ 50.87411532],[ 50.76969931],[ 41.77223722],[ 33.44262382],[ 28.77418036]])
        np.testing.assert_array_almost_equal(reg.predy[0:5], predy_5, 7)
        q_5 = np.array([ 18.594     ,  24.7142675 ,  13.72216667,  27.82929567])
        np.testing.assert_array_almost_equal(reg.q[0], q_5)
        self.assertAlmostEqual(reg.sig2n_k, 234.54258763039289, 7)
        self.assertAlmostEqual(reg.sig2n, 215.39625394627919, 7)
        self.assertAlmostEqual(reg.sig2, 215.39625394627919, 7)
        self.assertAlmostEqual(reg.std_y, 18.466069465206047, 7)
        u_5 = np.array( [[ 29.59288768], [ -6.20269831], [-15.42223722], [ -0.24262282], [ -5.54918036]])
        np.testing.assert_array_almost_equal(reg.u[0:5], u_5, 7)
        self.assertAlmostEqual(reg.utu, 10554.41644336768, 7)
        varb = np.array( [[  1.48966377e+00, -2.28698061e-02, -1.20217386e-02, -1.85763498e-02],
                          [ -2.28698061e-02,  1.27893998e-03,  2.74600023e-04, -1.33497705e-04],
                          [ -1.20217386e-02,  2.74600023e-04,  1.54257766e-04,  6.86851184e-05],
                          [ -1.85763498e-02, -1.33497705e-04,  6.86851184e-05,  4.67711582e-04]])
        np.testing.assert_array_almost_equal(reg.varb, varb, 7)
        vm = np.array([[  3.20867996e+02, -4.92607057e+00, -2.58943746e+00, -4.00127615e+00],
                       [ -4.92607057e+00,  2.75478880e-01,  5.91478163e-02, -2.87549056e-02],
                       [ -2.58943746e+00,  5.91478163e-02,  3.32265449e-02,  1.47945172e-02],
                       [ -4.00127615e+00, -2.87549056e-02,  1.47945172e-02,  1.00743323e-01]])
        np.testing.assert_array_almost_equal(reg.vm, vm, 6)
        x_0 = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x[0], x_0, 7)
        y_5 = np.array( [[ 80.467003], [ 44.567001], [ 26.35    ], [ 33.200001], [ 23.225   ]])
        np.testing.assert_array_almost_equal(reg.y[0:5], y_5, 7)
        yend_5 = np.array( [[ 35.4585005 ], [ 46.67233467], [ 45.36475125], [ 32.81675025], [ 30.81785714]])
        np.testing.assert_array_almost_equal(reg.yend[0:5], yend_5, 7)
        z_0 = np.array([  1.       ,  19.531    ,  15.72598  ,  35.4585005]) 
        np.testing.assert_array_almost_equal(reg.z[0], z_0, 7)
        zthhthi = np.array( [[  1.00000000e+00, -2.22044605e-16, -2.22044605e-16 , 2.22044605e-16,
                                4.44089210e-16,  0.00000000e+00, -8.88178420e-16],
                             [  0.00000000e+00,  1.00000000e+00, -3.55271368e-15 , 3.55271368e-15,
                               -7.10542736e-15,  7.10542736e-14,  0.00000000e+00],
                             [  1.81898940e-12,  2.84217094e-14,  1.00000000e+00 , 0.00000000e+00,
                               -2.84217094e-14,  5.68434189e-14,  5.68434189e-14],
                             [ -8.31133940e+00, -3.76104678e-01, -2.07028208e-01 , 1.32618931e+00,
                               -8.04284562e-01,  1.30527047e+00,  1.39136816e+00]])
        np.testing.assert_array_almost_equal(reg.zthhthi, zthhthi, 7)

    def test_init_white_(self):
        w_lags = 2
        X = []
        X.append(self.db.by_col("INC"))
        X.append(self.db.by_col("CRIME"))
        self.X = np.array(X).T
        yd2, q2 = pysal.spreg.utils.set_endog(self.y, self.X, self.w, None, None, w_lags, True)
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        base_gm_lag = BaseGM_Lag(self.y, self.X,  yend=yd2, q=q2, w=self.w.sparse, w_lags=w_lags, robust='white')
        tbetas = np.array([[  4.53017056e+01], [  6.20888617e-01], [ -4.80723451e-01], [  2.83622122e-02]])
        np.testing.assert_array_almost_equal(base_gm_lag.betas, tbetas) 
        dbetas = D.se_betas(base_gm_lag)
        se_betas = np.array([ 20.47077481, 0.50613931, 0.20138425, 0.38028295 ])
        np.testing.assert_array_almost_equal(dbetas, se_betas)

    def test_init_hac_(self):
        w_lags = 2
        X = []
        X.append(self.db.by_col("INC"))
        X.append(self.db.by_col("CRIME"))
        self.X = np.array(X).T
        yd2, q2 = pysal.spreg.utils.set_endog(self.y, self.X, self.w, None, None, w_lags, True)
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        gwk = pysal.kernelW_from_shapefile(pysal.examples.get_path('columbus.shp'),k=15,function='triangular', fixed=False)        
        base_gm_lag = BaseGM_Lag(self.y, self.X, yend=yd2, q=q2, w=self.w.sparse, w_lags=w_lags, robust='hac', gwk=gwk)
        tbetas = np.array([[  4.53017056e+01], [  6.20888617e-01], [ -4.80723451e-01], [  2.83622122e-02]])
        np.testing.assert_array_almost_equal(base_gm_lag.betas, tbetas) 
        dbetas = D.se_betas(base_gm_lag)
        se_betas = np.array([ 19.08513569,   0.51769543,   0.18244862,   0.35460553])
        np.testing.assert_array_almost_equal(dbetas, se_betas)

    def test_init_discbd(self):
        w_lags = 2
        X = np.array(self.db.by_col("INC"))
        self.X = np.reshape(X, (49,1))
        yd = np.array(self.db.by_col("CRIME"))
        yd = np.reshape(yd, (49,1))
        q = np.array(self.db.by_col("DISCBD"))
        q = np.reshape(q, (49,1))
        yd2, q2 = pysal.spreg.utils.set_endog(self.y, self.X, self.w, yd, q, w_lags, True)
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        reg = BaseGM_Lag(self.y, self.X, w=self.w.sparse, yend=yd2, q=q2, w_lags=w_lags)
        tbetas = np.array([[ 100.79359082], [  -0.50215501], [  -1.14881711], [  -0.38235022]])
        np.testing.assert_array_almost_equal(tbetas, reg.betas)
        dbetas = D.se_betas(reg)
        se_betas = np.array([ 53.0829123 ,   1.02511494,   0.57589064,   0.59891744 ])
        np.testing.assert_array_almost_equal(dbetas, se_betas)

    def test_n_k(self):
        w_lags = 2
        X = []
        X.append(self.db.by_col("INC"))
        X.append(self.db.by_col("CRIME"))
        self.X = np.array(X).T
        yd2, q2 = pysal.spreg.utils.set_endog(self.y, self.X, self.w, None, None, w_lags, True)
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        reg = BaseGM_Lag(self.y, self.X, yend=yd2, q=q2, w=self.w.sparse, w_lags=w_lags, sig2n_k=True)
        betas = np.  array([[  4.53017056e+01], [  6.20888617e-01], [ -4.80723451e-01], [  2.83622122e-02]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array( [[  3.49389596e+02, -5.36394351e+00, -2.81960968e+00, -4.35694515e+00],
                         [ -5.36394351e+00,  2.99965892e-01,  6.44054000e-02, -3.13108972e-02],
                         [ -2.81960968e+00,  6.44054000e-02,  3.61800155e-02,  1.61095854e-02],
                         [ -4.35694515e+00, -3.13108972e-02,  1.61095854e-02,  1.09698285e-01]])
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)

    def test_lag_q(self):
        w_lags = 2
        X = np.array(self.db.by_col("INC"))
        self.X = np.reshape(X, (49,1))
        yd = np.array(self.db.by_col("CRIME"))
        yd = np.reshape(yd, (49,1))
        q = np.array(self.db.by_col("DISCBD"))
        q = np.reshape(q, (49,1))
        yd2, q2 = pysal.spreg.utils.set_endog(self.y, self.X, self.w, yd, q, w_lags, False)
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        reg = BaseGM_Lag(self.y, self.X, w=self.w.sparse, yend=yd2, q=q2, w_lags=w_lags, lag_q=False)
        tbetas = np.array( [[ 108.83261383], [  -0.48041099], [  -1.18950006], [  -0.56140186]])
        np.testing.assert_array_almost_equal(tbetas, reg.betas)
        dbetas = D.se_betas(reg)
        se_betas = np.array([ 58.33203837,   1.09100446,   0.62315167,   0.68088777])
        np.testing.assert_array_almost_equal(dbetas, se_betas)



class TestGMLag(unittest.TestCase):
    def setUp(self):
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
        self.db = pysal.open(pysal.examples.get_path("columbus.dbf"), 'r')
        y = np.array(self.db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        
    def test___init__(self):
        X = []
        X.append(self.db.by_col("INC"))
        X.append(self.db.by_col("CRIME"))
        self.X = np.array(X).T
        reg = GM_Lag(self.y, self.X, w=self.w, w_lags=2)
        betas = np.array([[  4.53017056e+01], [  6.20888617e-01], [ -4.80723451e-01], [  2.83622122e-02]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        e_5 = np.array( [[ 29.28976367], [ -6.07439501], [-15.30080685], [ -0.41773375], [ -5.67197968]])
        np.testing.assert_array_almost_equal(reg.e_pred[0:5], e_5, 7)
        h_0 = np.array([  1.        ,  19.531     ,  15.72598   ,  18.594     ,
                            24.7142675 ,  13.72216667,  27.82929567])
        np.testing.assert_array_almost_equal(reg.h[0], h_0)
        hth = np.  array([   49.        ,   704.371999  ,  1721.312371  ,   724.7435916 ,
                             1707.35412945,   711.31248483,  1729.63201243])
        np.testing.assert_array_almost_equal(reg.hth[0], hth, 7)
        hthi = np.array([  7.33701328e+00,   2.27764882e-02,   2.18153588e-02,
                           -5.11035447e-02,   1.22515181e-03,  -2.38079378e-01,
                           -1.20149133e-01])
        np.testing.assert_array_almost_equal(reg.hthi[0], hthi, 7)
        self.assertEqual(reg.k, 4)
        self.assertEqual(reg.kstar, 1)
        self.assertAlmostEqual(reg.mean_y, 38.436224469387746, 7)
        self.assertEqual(reg.n, 49)
        pfora1a2 = np.array([ 80.5588479 ,  -1.06625281,  -0.61703759,  -1.10071931]) 
        self.assertAlmostEqual(reg.pr2, 0.3551928222612527, 7)
        self.assertAlmostEqual(reg.pr2_e, 0.34763857386174174, 7)
        np.testing.assert_array_almost_equal(reg.pfora1a2[0], pfora1a2, 7)
        predy_5 = np.array([[ 50.87411532],[ 50.76969931],[ 41.77223722],[ 33.44262382],[ 28.77418036]])
        np.testing.assert_array_almost_equal(reg.predy[0:5], predy_5, 7)
        predy_e_5 = np.array( [[ 51.17723933], [ 50.64139601], [ 41.65080685], [ 33.61773475], [ 28.89697968]])
        np.testing.assert_array_almost_equal(reg.predy_e[0:5], predy_e_5, 7)
        q_5 = np.array([ 18.594     ,  24.7142675 ,  13.72216667,  27.82929567])
        np.testing.assert_array_almost_equal(reg.q[0], q_5)
        self.assertEqual(reg.robust, 'unadjusted')
        self.assertAlmostEqual(reg.sig2n_k, 234.54258763039289, 7)
        self.assertAlmostEqual(reg.sig2n, 215.39625394627919, 7)
        self.assertAlmostEqual(reg.sig2, 215.39625394627919, 7)
        self.assertAlmostEqual(reg.std_y, 18.466069465206047, 7)
        u_5 = np.array( [[ 29.59288768], [ -6.20269831], [-15.42223722], [ -0.24262282], [ -5.54918036]])
        np.testing.assert_array_almost_equal(reg.u[0:5], u_5, 7)
        self.assertAlmostEqual(reg.utu, 10554.41644336768, 7)
        varb = np.array( [[  1.48966377e+00, -2.28698061e-02, -1.20217386e-02, -1.85763498e-02],
                          [ -2.28698061e-02,  1.27893998e-03,  2.74600023e-04, -1.33497705e-04],
                          [ -1.20217386e-02,  2.74600023e-04,  1.54257766e-04,  6.86851184e-05],
                          [ -1.85763498e-02, -1.33497705e-04,  6.86851184e-05,  4.67711582e-04]])
        np.testing.assert_array_almost_equal(reg.varb, varb, 7)
        vm = np.array([[  3.20867996e+02, -4.92607057e+00, -2.58943746e+00, -4.00127615e+00],
                       [ -4.92607057e+00,  2.75478880e-01,  5.91478163e-02, -2.87549056e-02],
                       [ -2.58943746e+00,  5.91478163e-02,  3.32265449e-02,  1.47945172e-02],
                       [ -4.00127615e+00, -2.87549056e-02,  1.47945172e-02,  1.00743323e-01]])
        np.testing.assert_array_almost_equal(reg.vm, vm, 6)
        x_0 = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x[0], x_0, 7)
        y_5 = np.array( [[ 80.467003], [ 44.567001], [ 26.35    ], [ 33.200001], [ 23.225   ]])
        np.testing.assert_array_almost_equal(reg.y[0:5], y_5, 7)
        yend_5 = np.array( [[ 35.4585005 ], [ 46.67233467], [ 45.36475125], [ 32.81675025], [ 30.81785714]])
        np.testing.assert_array_almost_equal(reg.yend[0:5], yend_5, 7)
        z_0 = np.array([  1.       ,  19.531    ,  15.72598  ,  35.4585005]) 
        np.testing.assert_array_almost_equal(reg.z[0], z_0, 7)
        zthhthi = np.array( [[  1.00000000e+00, -2.22044605e-16, -2.22044605e-16 , 2.22044605e-16,
                                4.44089210e-16,  0.00000000e+00, -8.88178420e-16],
                             [  0.00000000e+00,  1.00000000e+00, -3.55271368e-15 , 3.55271368e-15,
                               -7.10542736e-15,  7.10542736e-14,  0.00000000e+00],
                             [  1.81898940e-12,  2.84217094e-14,  1.00000000e+00 , 0.00000000e+00,
                               -2.84217094e-14,  5.68434189e-14,  5.68434189e-14],
                             [ -8.31133940e+00, -3.76104678e-01, -2.07028208e-01 , 1.32618931e+00,
                               -8.04284562e-01,  1.30527047e+00,  1.39136816e+00]])
        np.testing.assert_array_almost_equal(reg.zthhthi, zthhthi, 7)

    def test_init_white_(self):
        X = []
        X.append(self.db.by_col("INC"))
        X.append(self.db.by_col("CRIME"))
        self.X = np.array(X).T
        base_gm_lag = GM_Lag(self.y, self.X, w=self.w, w_lags=2, robust='white')
        tbetas = np.array([[  4.53017056e+01], [  6.20888617e-01], [ -4.80723451e-01], [  2.83622122e-02]])
        np.testing.assert_array_almost_equal(base_gm_lag.betas, tbetas) 
        dbetas = D.se_betas(base_gm_lag)
        se_betas = np.array([ 20.47077481, 0.50613931, 0.20138425, 0.38028295 ])
        np.testing.assert_array_almost_equal(dbetas, se_betas)

    def test_init_hac_(self):
        X = []
        X.append(self.db.by_col("INC"))
        X.append(self.db.by_col("CRIME"))
        self.X = np.array(X).T
        gwk = pysal.kernelW_from_shapefile(pysal.examples.get_path('columbus.shp'),k=15,function='triangular', fixed=False)        
        base_gm_lag = GM_Lag(self.y, self.X, w=self.w, w_lags=2, robust='hac', gwk=gwk)
        tbetas = np.array([[  4.53017056e+01], [  6.20888617e-01], [ -4.80723451e-01], [  2.83622122e-02]])
        np.testing.assert_array_almost_equal(base_gm_lag.betas, tbetas) 
        dbetas = D.se_betas(base_gm_lag)
        se_betas = np.array([ 19.08513569,   0.51769543,   0.18244862,   0.35460553])
        np.testing.assert_array_almost_equal(dbetas, se_betas)

    def test_init_discbd(self):
        X = np.array(self.db.by_col("INC"))
        X = np.reshape(X, (49,1))
        yd = np.array(self.db.by_col("CRIME"))
        yd = np.reshape(yd, (49,1))
        q = np.array(self.db.by_col("DISCBD"))
        q = np.reshape(q, (49,1))
        reg = GM_Lag(self.y, X, w=self.w, yend=yd, q=q, w_lags=2)
        tbetas = np.array([[ 100.79359082], [  -0.50215501], [  -1.14881711], [  -0.38235022]])
        np.testing.assert_array_almost_equal(tbetas, reg.betas)
        dbetas = D.se_betas(reg)
        se_betas = np.array([ 53.0829123 ,   1.02511494,   0.57589064,   0.59891744 ])
        np.testing.assert_array_almost_equal(dbetas, se_betas)

    def test_n_k(self):
        X = []
        X.append(self.db.by_col("INC"))
        X.append(self.db.by_col("CRIME"))
        self.X = np.array(X).T
        reg = GM_Lag(self.y, self.X, w=self.w, w_lags=2, sig2n_k=True)
        betas = np.  array([[  4.53017056e+01], [  6.20888617e-01], [ -4.80723451e-01], [  2.83622122e-02]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array( [[  3.49389596e+02, -5.36394351e+00, -2.81960968e+00, -4.35694515e+00],
                         [ -5.36394351e+00,  2.99965892e-01,  6.44054000e-02, -3.13108972e-02],
                         [ -2.81960968e+00,  6.44054000e-02,  3.61800155e-02,  1.61095854e-02],
                         [ -4.35694515e+00, -3.13108972e-02,  1.61095854e-02,  1.09698285e-01]])
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)

    def test_lag_q(self):
        X = np.array(self.db.by_col("INC"))
        X = np.reshape(X, (49,1))
        yd = np.array(self.db.by_col("CRIME"))
        yd = np.reshape(yd, (49,1))
        q = np.array(self.db.by_col("DISCBD"))
        q = np.reshape(q, (49,1))
        reg = GM_Lag(self.y, X, w=self.w, yend=yd, q=q, w_lags=2, lag_q=False)
        tbetas = np.array( [[ 108.83261383], [  -0.48041099], [  -1.18950006], [  -0.56140186]])
        np.testing.assert_array_almost_equal(tbetas, reg.betas)
        dbetas = D.se_betas(reg)
        se_betas = np.array([ 58.33203837,   1.09100446,   0.62315167,   0.68088777])
        np.testing.assert_array_almost_equal(dbetas, se_betas)

    def test_spatial(self):
        X = np.array(self.db.by_col("INC"))
        X = np.reshape(X, (49,1))
        yd = np.array(self.db.by_col("CRIME"))
        yd = np.reshape(yd, (49,1))
        q = np.array(self.db.by_col("DISCBD"))
        q = np.reshape(q, (49,1))
        w = pysal.queen_from_shapefile(pysal.examples.get_path('columbus.shp'))
        reg = GM_Lag(self.y, X, yd, q, spat_diag=True, w=w)
        betas = np.array([[  5.46344924e+01], [  4.13301682e-01], [ -5.92637442e-01], [ -7.40490883e-03]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array( [[  4.45202654e+02, -1.50290275e+01, -6.36557072e+00, -5.71403440e-03],
                        [ -1.50290275e+01,  5.93124683e-01,  2.19169508e-01, -6.70675916e-03],
                        [ -6.36557072e+00,  2.19169508e-01,  1.06577542e-01, -2.96533875e-03],
                        [ -5.71403440e-03, -6.70675916e-03, -2.96533875e-03,  1.15655425e-03]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 6)
        ak_test = np.array([ 2.52597326,  0.11198567])
        np.testing.assert_array_almost_equal(reg.ak_test, ak_test, 7)

    def test_names(self):
        X = np.array(self.db.by_col("INC"))
        X = np.reshape(X, (49,1))
        yd = np.array(self.db.by_col("CRIME"))
        yd = np.reshape(yd, (49,1))
        q = np.array(self.db.by_col("DISCBD"))
        q = np.reshape(q, (49,1))
        w = pysal.queen_from_shapefile(pysal.examples.get_path('columbus.shp'))
        gwk = pysal.kernelW_from_shapefile(pysal.examples.get_path('columbus.shp'),k=5,function='triangular', fixed=False)
        name_x = ['inc']
        name_y = 'crime'
        name_yend = ['crime']
        name_q = ['discbd']
        name_w = 'queen'
        name_gwk = 'k=5'
        name_ds = 'columbus'
        reg = GM_Lag(self.y, X, yd, q,
                spat_diag=True, w=w, robust='hac', gwk=gwk,
                name_x=name_x, name_y=name_y, name_q=name_q, name_w=name_w,
                name_yend=name_yend, name_gwk=name_gwk, name_ds=name_ds)
        betas = np.array([[  5.46344924e+01], [  4.13301682e-01], [ -5.92637442e-01], [ -7.40490883e-03]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array( [[  5.70817052e+02, -1.83655385e+01, -8.36602575e+00,  2.37538877e-02],
                        [ -1.85224661e+01,  6.53311383e-01,  2.84209566e-01, -6.47694160e-03],
                        [ -8.31105622e+00,  2.78772694e-01,  1.38144928e-01, -3.98175246e-03],
                        [  2.66662466e-02, -6.23783104e-03, -4.11092891e-03,  1.10936528e-03]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 6)
        self.assertListEqual(reg.name_x, ['CONSTANT']+name_x)
        name_yend.append('W_crime')
        self.assertListEqual(reg.name_yend, name_yend)
        name_q.extend(['W_inc', 'W_discbd'])
        self.assertListEqual(reg.name_q, name_q)
        self.assertEqual(reg.name_y, name_y)
        self.assertEqual(reg.name_w, name_w)
        self.assertEqual(reg.name_gwk, name_gwk)
        self.assertEqual(reg.name_ds, name_ds)



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_twosls_sparse
import unittest
import numpy as np
import pysal
from pysal.spreg.twosls import TSLS, BaseTSLS
from scipy import sparse as SP


class TestBaseTSLS(unittest.TestCase):
    def setUp(self):
        db = pysal.open(pysal.examples.get_path("columbus.dbf"),'r')
        self.y = np.array(db.by_col("CRIME"))
        self.y = np.reshape(self.y, (49,1))
        self.X = []
        self.X.append(db.by_col("INC"))
        self.X = np.array(self.X).T
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.X = SP.csr_matrix(self.X)
        self.yd = []
        self.yd.append(db.by_col("HOVAL"))
        self.yd = np.array(self.yd).T
        self.q = []
        self.q.append(db.by_col("DISCBD"))
        self.q = np.array(self.q).T

    def test_basic(self):
        reg = BaseTSLS(self.y, self.X, self.yd, self.q)
        betas = np.array([[ 88.46579584], [  0.5200379 ], [ -1.58216593]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        h_0 = np.array([  1.   ,  19.531,   5.03 ])
        np.testing.assert_array_almost_equal(reg.h.toarray()[0], h_0)
        hth = np.array([[    49.        ,    704.371999  ,    139.75      ],
                        [   704.371999  ,  11686.67338121,   2246.12800625],
                        [   139.75      ,   2246.12800625,    498.5851    ]])
        np.testing.assert_array_almost_equal(reg.hth, hth, 7)
        hthi = np.array([[ 0.1597275 , -0.00762011, -0.01044191],
                        [-0.00762011,  0.00100135, -0.0023752 ],
                        [-0.01044191, -0.0023752 ,  0.01563276]]) 
        np.testing.assert_array_almost_equal(reg.hthi, hthi, 7)
        self.assertEqual(reg.k, 3)
        self.assertEqual(reg.kstar, 1)
        self.assertAlmostEqual(reg.mean_y, 35.128823897959187, 7)
        self.assertEqual(reg.n, 49)
        pfora1a2 = np.array([[ 9.58156106, -0.22744226, -0.13820537],
                             [ 0.02580142,  0.08226331, -0.03143731],
                             [-3.13896453, -0.33487872,  0.20690965]]) 
        np.testing.assert_array_almost_equal(reg.pfora1a2, pfora1a2, 7)
        predy_5 = np.array([[-28.68949467], [ 28.99484984], [ 55.07344824], [ 38.26609504], [ 57.57145851]]) 
        np.testing.assert_array_almost_equal(reg.predy[0:5], predy_5, 7)
        q_5 = np.array([[ 5.03], [ 4.27], [ 3.89], [ 3.7 ], [ 2.83]])
        np.testing.assert_array_equal(reg.q[0:5], q_5)
        self.assertAlmostEqual(reg.sig2n_k, 587.56797852699822, 7)
        self.assertAlmostEqual(reg.sig2n, 551.5944288212637, 7)
        self.assertAlmostEqual(reg.sig2, 551.5944288212637, 7)
        self.assertAlmostEqual(reg.std_y, 16.732092091229699, 7)
        u_5 = np.array([[ 44.41547467], [-10.19309584], [-24.44666724], [ -5.87833504], [ -6.83994851]]) 
        np.testing.assert_array_almost_equal(reg.u[0:5], u_5, 7)
        self.assertAlmostEqual(reg.utu, 27028.127012241919, 7)
        varb = np.array([[ 0.41526237,  0.01879906, -0.01730372],
                         [ 0.01879906,  0.00362823, -0.00184604],
                         [-0.01730372, -0.00184604,  0.0011406 ]]) 
        np.testing.assert_array_almost_equal(reg.varb, varb, 7)
        vm = np.array([[ 229.05640809,   10.36945783,   -9.54463414],
                       [  10.36945783,    2.0013142 ,   -1.01826408],
                       [  -9.54463414,   -1.01826408,    0.62914915]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)
        x_0 = np.array([  1.   ,  19.531])
        np.testing.assert_array_almost_equal(reg.x.toarray()[0], x_0, 7)
        y_5 = np.array([[ 15.72598 ], [ 18.801754], [ 30.626781], [ 32.38776 ], [ 50.73151 ]]) 
        np.testing.assert_array_almost_equal(reg.y[0:5], y_5, 7)
        yend_5 = np.array([[ 80.467003], [ 44.567001], [ 26.35    ], [ 33.200001], [ 23.225   ]]) 
        np.testing.assert_array_almost_equal(reg.yend[0:5], yend_5, 7)
        z_0 = np.array([  1.      ,  19.531   ,  80.467003]) 
        np.testing.assert_array_almost_equal(reg.z.toarray()[0], z_0, 7)
        zthhthi = np.array([[  1.00000000e+00,  -1.66533454e-16,   4.44089210e-16],
                            [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00],
                            [  1.26978671e+01,   1.05598709e+00,   3.70212359e+00]]) 
        np.testing.assert_array_almost_equal(reg.zthhthi, zthhthi, 7)
        
    def test_n_k(self):
        reg = BaseTSLS(self.y, self.X, self.yd, self.q, sig2n_k=True)
        betas = np.array([[ 88.46579584], [  0.5200379 ], [ -1.58216593]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array([[ 243.99486949,   11.04572682,  -10.16711028],
                       [  11.04572682,    2.13183469,   -1.08467261],
                       [ -10.16711028,   -1.08467261,    0.67018062]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)

    def test_white(self):
        reg = BaseTSLS(self.y, self.X, self.yd, self.q, robust='white')
        betas = np.array([[ 88.46579584], [  0.5200379 ], [ -1.58216593]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array([[ 208.27139316,   15.6687805 ,  -11.53686154],
                       [  15.6687805 ,    2.26882747,   -1.30312033],
                       [ -11.53686154,   -1.30312033,    0.81940656]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)

    def test_hac(self):
        gwk = pysal.kernelW_from_shapefile(pysal.examples.get_path('columbus.shp'),k=15,function='triangular', fixed=False)
        reg = BaseTSLS(self.y, self.X, self.yd, self.q, robust='hac', gwk=gwk)
        betas = np.array([[ 88.46579584], [  0.5200379 ], [ -1.58216593]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array([[ 231.07254978,   15.42050291,  -11.3941033 ],
                       [  15.01376346,    1.92422887,   -1.11865505],
                       [ -11.34381641,   -1.1279227 ,    0.72053806]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)

class TestTSLS(unittest.TestCase):
    def setUp(self):
        db = pysal.open(pysal.examples.get_path("columbus.dbf"),'r')
        self.y = np.array(db.by_col("CRIME"))
        self.y = np.reshape(self.y, (49,1))
        self.X = []
        self.X.append(db.by_col("INC"))
        self.X = np.array(self.X).T
        self.X = SP.csr_matrix(self.X)
        self.yd = []
        self.yd.append(db.by_col("HOVAL"))
        self.yd = np.array(self.yd).T
        self.q = []
        self.q.append(db.by_col("DISCBD"))
        self.q = np.array(self.q).T

    def test_basic(self):
        reg = TSLS(self.y, self.X, self.yd, self.q)
        betas = np.array([[ 88.46579584], [  0.5200379 ], [ -1.58216593]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        h_0 = np.array([  1.   ,  19.531,   5.03 ])
        np.testing.assert_array_almost_equal(reg.h.toarray()[0], h_0)
        hth = np.array([[    49.        ,    704.371999  ,    139.75      ],
                        [   704.371999  ,  11686.67338121,   2246.12800625],
                        [   139.75      ,   2246.12800625,    498.5851    ]])
        np.testing.assert_array_almost_equal(reg.hth, hth, 7)
        hthi = np.array([[ 0.1597275 , -0.00762011, -0.01044191],
                        [-0.00762011,  0.00100135, -0.0023752 ],
                        [-0.01044191, -0.0023752 ,  0.01563276]]) 
        np.testing.assert_array_almost_equal(reg.hthi, hthi, 7)
        self.assertEqual(reg.k, 3)
        self.assertEqual(reg.kstar, 1)
        self.assertAlmostEqual(reg.mean_y, 35.128823897959187, 7)
        self.assertEqual(reg.n, 49)
        pfora1a2 = np.array([[ 9.58156106, -0.22744226, -0.13820537],
                             [ 0.02580142,  0.08226331, -0.03143731],
                             [-3.13896453, -0.33487872,  0.20690965]]) 
        np.testing.assert_array_almost_equal(reg.pfora1a2, pfora1a2, 7)
        predy_5 = np.array([[-28.68949467], [ 28.99484984], [ 55.07344824], [ 38.26609504], [ 57.57145851]]) 
        np.testing.assert_array_almost_equal(reg.predy[0:5], predy_5, 7)
        q_5 = np.array([[ 5.03], [ 4.27], [ 3.89], [ 3.7 ], [ 2.83]])
        np.testing.assert_array_equal(reg.q[0:5], q_5)
        self.assertAlmostEqual(reg.sig2n_k, 587.56797852699822, 7)
        self.assertAlmostEqual(reg.sig2n, 551.5944288212637, 7)
        self.assertAlmostEqual(reg.sig2, 551.5944288212637, 7)
        self.assertAlmostEqual(reg.std_y, 16.732092091229699, 7)
        u_5 = np.array([[ 44.41547467], [-10.19309584], [-24.44666724], [ -5.87833504], [ -6.83994851]]) 
        np.testing.assert_array_almost_equal(reg.u[0:5], u_5, 7)
        self.assertAlmostEqual(reg.utu, 27028.127012241919, 7)
        varb = np.array([[ 0.41526237,  0.01879906, -0.01730372],
                         [ 0.01879906,  0.00362823, -0.00184604],
                         [-0.01730372, -0.00184604,  0.0011406 ]]) 
        np.testing.assert_array_almost_equal(reg.varb, varb, 7)
        vm = np.array([[ 229.05640809,   10.36945783,   -9.54463414],
                       [  10.36945783,    2.0013142 ,   -1.01826408],
                       [  -9.54463414,   -1.01826408,    0.62914915]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)
        x_0 = np.array([  1.   ,  19.531])
        np.testing.assert_array_almost_equal(reg.x.toarray()[0], x_0, 7)
        y_5 = np.array([[ 15.72598 ], [ 18.801754], [ 30.626781], [ 32.38776 ], [ 50.73151 ]]) 
        np.testing.assert_array_almost_equal(reg.y[0:5], y_5, 7)
        yend_5 = np.array([[ 80.467003], [ 44.567001], [ 26.35    ], [ 33.200001], [ 23.225   ]]) 
        np.testing.assert_array_almost_equal(reg.yend[0:5], yend_5, 7)
        z_0 = np.array([  1.      ,  19.531   ,  80.467003]) 
        np.testing.assert_array_almost_equal(reg.z.toarray()[0], z_0, 7)
        zthhthi = np.array([[  1.00000000e+00,  -1.66533454e-16,   4.44089210e-16],
                            [  0.00000000e+00,   1.00000000e+00,   0.00000000e+00],
                            [  1.26978671e+01,   1.05598709e+00,   3.70212359e+00]]) 
        np.testing.assert_array_almost_equal(reg.zthhthi, zthhthi, 7)
        self.assertAlmostEqual(reg.pr2, 0.27936137128173893, 7)
        z_stat = np.array([[  5.84526447e+00,   5.05764078e-09],
                           [  3.67601567e-01,   7.13170346e-01],
                           [ -1.99468913e+00,   4.60767956e-02]])
        np.testing.assert_array_almost_equal(reg.z_stat, z_stat, 7)
        title = 'TWO STAGE LEAST SQUARES'
        self.assertEqual(reg.title, title)
        
    def test_n_k(self):
        reg = TSLS(self.y, self.X, self.yd, self.q, sig2n_k=True)
        betas = np.array([[ 88.46579584], [  0.5200379 ], [ -1.58216593]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array([[ 243.99486949,   11.04572682,  -10.16711028],
                       [  11.04572682,    2.13183469,   -1.08467261],
                       [ -10.16711028,   -1.08467261,    0.67018062]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)

    def test_white(self):
        reg = TSLS(self.y, self.X, self.yd, self.q, robust='white')
        betas = np.array([[ 88.46579584], [  0.5200379 ], [ -1.58216593]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array([[ 208.27139316,   15.6687805 ,  -11.53686154],
                       [  15.6687805 ,    2.26882747,   -1.30312033],
                       [ -11.53686154,   -1.30312033,    0.81940656]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)
        self.assertEqual(reg.robust, 'white')

    def test_hac(self):
        gwk = pysal.kernelW_from_shapefile(pysal.examples.get_path('columbus.shp'),k=5,function='triangular', fixed=False)
        reg = TSLS(self.y, self.X, self.yd, self.q, robust='hac', gwk=gwk)
        betas = np.array([[ 88.46579584], [  0.5200379 ], [ -1.58216593]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array([[ 225.0795089 ,   17.11660041,  -12.22448566],
                       [  17.67097154,    2.47483461,   -1.4183641 ],
                       [ -12.45093722,   -1.40495464,    0.8700441 ]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)
        self.assertEqual(reg.robust, 'hac')

    def test_spatial(self):
        w = pysal.queen_from_shapefile(pysal.examples.get_path('columbus.shp'))
        reg = TSLS(self.y, self.X, self.yd, self.q, spat_diag=True, w=w)
        betas = np.array([[ 88.46579584], [  0.5200379 ], [ -1.58216593]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array([[ 229.05640809,   10.36945783,   -9.54463414],
                       [  10.36945783,    2.0013142 ,   -1.01826408],
                       [  -9.54463414,   -1.01826408,    0.62914915]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)
        ak_test = np.array([ 1.16816972,  0.27977763])
        np.testing.assert_array_almost_equal(reg.ak_test, ak_test, 7)

    def test_names(self):
        w = pysal.queen_from_shapefile(pysal.examples.get_path('columbus.shp'))
        gwk = pysal.kernelW_from_shapefile(pysal.examples.get_path('columbus.shp'),k=5,function='triangular', fixed=False)
        name_x = ['inc']
        name_y = 'crime'
        name_yend = ['hoval']
        name_q = ['discbd']
        name_w = 'queen'
        name_gwk = 'k=5'
        name_ds = 'columbus'
        reg = TSLS(self.y, self.X, self.yd, self.q,
                spat_diag=True, w=w, robust='hac', gwk=gwk,
                name_x=name_x, name_y=name_y, name_q=name_q, name_w=name_w,
                name_yend=name_yend, name_gwk=name_gwk, name_ds=name_ds)
        betas = np.array([[ 88.46579584], [  0.5200379 ], [ -1.58216593]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array([[ 225.0795089 ,   17.11660041,  -12.22448566],
                       [  17.67097154,    2.47483461,   -1.4183641 ],
                       [ -12.45093722,   -1.40495464,    0.8700441 ]])
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)
        self.assertListEqual(reg.name_x, ['CONSTANT']+name_x)
        self.assertListEqual(reg.name_yend, name_yend)
        self.assertListEqual(reg.name_q, name_q)
        self.assertEqual(reg.name_y, name_y)
        self.assertEqual(reg.name_w, name_w)
        self.assertEqual(reg.name_gwk, name_gwk)
        self.assertEqual(reg.name_ds, name_ds)

    


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_twosls_sp_regimes
import unittest
import numpy as np
import pysal
from pysal.spreg.twosls_sp_regimes import GM_Lag_Regimes
from pysal.spreg import utils
from pysal.spreg.twosls_sp import GM_Lag

class TestGMLag_Regimes(unittest.TestCase):
    def setUp(self):
        self.w = pysal.queen_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
        self.db = pysal.open(pysal.examples.get_path("columbus.dbf"), 'r')
        y = np.array(self.db.by_col("CRIME"))
        self.y = np.reshape(y, (49,1))
        self.r_var = 'NSA'
        self.regimes = self.db.by_col(self.r_var)

    def test___init__(self):
        #Matches SpaceStat
        X = []
        X.append(self.db.by_col("INC"))
        X.append(self.db.by_col("HOVAL"))
        self.X = np.array(X).T
        reg = GM_Lag_Regimes(self.y, self.X, self.regimes, w=self.w, sig2n_k=True, regime_lag_sep=False, regime_err_sep=False) 
        betas = np.array([[ 45.14892906],
       [ -1.42593383],
       [ -0.11501037],
       [ 40.99023016],
       [ -0.81498302],
       [ -0.28391409],
       [  0.4736163 ]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        e_5 = np.array([[ -1.47960519],
       [ -7.93748769],
       [ -5.88561835],
       [-13.37941105],
       [  5.2524303 ]])
        np.testing.assert_array_almost_equal(reg.e_pred[0:5], e_5, 7)
        h_0 = np.array([[  0.       ,   0.       ,   0.       ,   1.       ,  19.531    ,
         80.467003 ,   0.       ,   0.       ,  18.594    ,  35.4585005]])
        np.testing.assert_array_almost_equal(reg.h[0]*np.eye(10), h_0)
        self.assertEqual(reg.k, 7)
        self.assertEqual(reg.kstar, 1)
        self.assertAlmostEqual(reg.mean_y, 35.128823897959187, 7)
        self.assertEqual(reg.n, 49)
        self.assertAlmostEqual(reg.pr2, 0.6572182131915739, 7)
        self.assertAlmostEqual(reg.pr2_e, 0.5779687278635434, 7)
        pfora1a2 = np.array([ -2.15017629,  -0.30169328,  -0.07603704, -22.06541809,
         0.45738058,   0.02805828,   0.39073923]) 
        np.testing.assert_array_almost_equal(reg.pfora1a2[0], pfora1a2, 7)
        predy_5 = np.array([[ 13.93216104],
       [ 23.46424269],
       [ 34.43510955],
       [ 44.32473878],
       [ 44.39117516]])
        np.testing.assert_array_almost_equal(reg.predy[0:5], predy_5, 7)
        predy_e_5 = np.array([[ 17.20558519],
       [ 26.73924169],
       [ 36.51239935],
       [ 45.76717105],
       [ 45.4790797 ]])
        np.testing.assert_array_almost_equal(reg.predy_e[0:5], predy_e_5, 7)
        q_5 = np.array([[  0.       ,   0.       ,  18.594    ,  35.4585005]])
        np.testing.assert_array_almost_equal(reg.q[0]*np.eye(4), q_5)
        self.assertEqual(reg.robust, 'unadjusted')
        self.assertAlmostEqual(reg.sig2n_k, 109.76462904625834, 7)
        self.assertAlmostEqual(reg.sig2n, 94.08396775393571, 7)
        self.assertAlmostEqual(reg.sig2, 109.76462904625834, 7)
        self.assertAlmostEqual(reg.std_y, 16.732092091229699, 7)
        u_5 = np.array([[  1.79381896],
       [ -4.66248869],
       [ -3.80832855],
       [-11.93697878],
       [  6.34033484]])
        np.testing.assert_array_almost_equal(reg.u[0:5], u_5, 7)
        self.assertAlmostEqual(reg.utu, 4610.11441994285, 7)
        varb = np.array([  1.23841820e+00,  -3.65620114e-02,  -1.21919663e-03,
         1.00057547e+00,  -2.07403182e-02,  -1.27232693e-03,
        -1.77184084e-02])
        np.testing.assert_array_almost_equal(reg.varb[0], varb, 7)
        vm = np.array([  1.35934514e+02,  -4.01321561e+00,  -1.33824666e-01,
         1.09827796e+02,  -2.27655334e+00,  -1.39656494e-01,
        -1.94485452e+00])
        np.testing.assert_array_almost_equal(reg.vm[0], vm, 6)
        x_0 = np.array([[  0.      ,   0.      ,   0.      ,   1.      ,  19.531   ,
         80.467003]])
        np.testing.assert_array_almost_equal(reg.x[0]*np.eye(6), x_0, 7)
        y_5 = np.array([[ 15.72598 ],
       [ 18.801754],
       [ 30.626781],
       [ 32.38776 ],
       [ 50.73151 ]])
        np.testing.assert_array_almost_equal(reg.y[0:5], y_5, 7)
        yend_5 = np.array([[ 24.7142675 ],
       [ 26.24684033],
       [ 29.411751  ],
       [ 34.64647575],
       [ 40.4653275 ]])
        np.testing.assert_array_almost_equal(reg.yend[0:5]*np.array([[1]]), yend_5, 7)
        z_0 = np.array([[  0.       ,   0.       ,   0.       ,   1.       ,  19.531    ,
         80.467003 ,  24.7142675]]) 
        np.testing.assert_array_almost_equal(reg.z[0]*np.eye(7), z_0, 7)
        zthhthi = np.array([  1.00000000e+00,  -2.35922393e-16,   5.55111512e-17,
         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,
        -4.44089210e-16,   2.22044605e-16,   0.00000000e+00,
         0.00000000e+00])
        np.testing.assert_array_almost_equal(reg.zthhthi[0], zthhthi, 7)
        chow_regi = np.array([[ 0.19692667,  0.65721307],
       [ 0.5666492 ,  0.45159351],
       [ 0.45282066,  0.5009985 ]])
        np.testing.assert_array_almost_equal(reg.chow.regi, chow_regi, 7)
        self.assertAlmostEqual(reg.chow.joint[0], 0.82409867601863462, 7)
    
    def test_init_discbd(self):
        #Matches SpaceStat.
        X = np.array(self.db.by_col("INC"))
        X = np.reshape(X, (49,1))
        yd = np.array(self.db.by_col("HOVAL"))
        yd = np.reshape(yd, (49,1))
        q = np.array(self.db.by_col("DISCBD"))
        q = np.reshape(q, (49,1))
        reg = GM_Lag_Regimes(self.y, X, self.regimes, yend=yd, q=q, lag_q=False, w=self.w, sig2n_k=True, regime_lag_sep=False, regime_err_sep=False) 
        tbetas = np.array([[ 42.7266306 ],
       [ -0.15552345],
       [ 37.70545276],
       [ -0.5341577 ],
       [ -0.68305796],
       [ -0.37106077],
       [  0.55809516]])
        np.testing.assert_array_almost_equal(tbetas, reg.betas)
        vm = np.array([ 270.62979422,    3.62539081,  327.89638627,    6.24949355,
         -5.25333106,   -6.01743515,   -4.19290074])
        np.testing.assert_array_almost_equal(reg.vm[0], vm, 6)
        e_3 = np.array([[-0.33142796],
       [-9.51719607],
       [-7.86272153]])
        np.testing.assert_array_almost_equal(reg.e_pred[0:3], e_3, 7)
        u_3 = np.array([[ 4.51839601],
       [-5.67363147],
       [-5.1927562 ]])
        np.testing.assert_array_almost_equal(reg.u[0:3], u_3, 7)
        predy_3 = np.array([[ 11.20758399],
       [ 24.47538547],
       [ 35.8195372 ]])
        np.testing.assert_array_almost_equal(reg.predy[0:3], predy_3, 7)
        predy_e_3 = np.array([[ 16.05740796],
       [ 28.31895007],
       [ 38.48950253]])
        np.testing.assert_array_almost_equal(reg.predy_e[0:3], predy_e_3, 7)
        chow_regi = np.array([[ 0.13130991,  0.71707772],
       [ 0.04740966,  0.82763357],
       [ 0.15474413,  0.6940423 ]])
        np.testing.assert_array_almost_equal(reg.chow.regi, chow_regi, 7)
        self.assertAlmostEqual(reg.chow.joint[0], 0.31248100032096549, 7)
    
    def test_lag_q(self):
        X = np.array(self.db.by_col("INC"))
        X = np.reshape(X, (49,1))
        yd = np.array(self.db.by_col("HOVAL"))
        yd = np.reshape(yd, (49,1))
        q = np.array(self.db.by_col("DISCBD"))
        q = np.reshape(q, (49,1))
        reg = GM_Lag_Regimes(self.y, X, self.regimes, yend=yd, q=q, w=self.w, sig2n_k=True, regime_lag_sep=False, regime_err_sep=False) 
        tbetas = np.array([[ 37.87698329],
       [ -0.89426982],
       [ 31.4714777 ],
       [ -0.71640525],
       [ -0.28494432],
       [ -0.2294271 ],
       [  0.62996544]])
        np.testing.assert_array_almost_equal(tbetas, reg.betas)
        vm = np.array([ 128.25714554,   -0.38975354,   95.7271044 ,   -1.8429218 ,
         -1.75331978,   -0.18240338,   -1.67767464])
        np.testing.assert_array_almost_equal(reg.vm[0], vm, 6)
        chow_regi = np.array([[ 0.43494049,  0.50957463],
       [ 0.02089281,  0.88507135],
       [ 0.01180501,  0.91347943]])
        np.testing.assert_array_almost_equal(reg.chow.regi, chow_regi, 7)
        self.assertAlmostEqual(reg.chow.joint[0], 0.54288190938307757, 7)
    
    def test_all_regi(self):
        X = np.array(self.db.by_col("INC"))
        X = np.reshape(X, (49,1))
        yd = np.array(self.db.by_col("HOVAL"))
        yd = np.reshape(yd, (49,1))
        q = np.array(self.db.by_col("DISCBD"))
        q = np.reshape(q, (49,1))
        reg = GM_Lag_Regimes(self.y, X, self.regimes, yend=yd, q=q, w=self.w, regime_lag_sep=False, regime_err_sep=True) 
        tbetas = np.array([[ 37.87698329,  -0.89426982,  31.4714777 ,  -0.71640525,
         -0.28494432,  -0.2294271 ,   0.62996544]])
        np.testing.assert_array_almost_equal(tbetas, reg.betas.T)
        vm = np.array([ 70.38291551,  -0.64868787,  49.25453215,  -0.62851534,
        -0.75413453,  -0.12674433,  -0.97179236])
        np.testing.assert_array_almost_equal(reg.vm[0], vm, 6)
        e_3 = np.array([[-2.66997799],
       [-7.69786264],
       [-4.39412782]])
        np.testing.assert_array_almost_equal(reg.e_pred[0:3], e_3, 7)
        u_3 = np.array([[ 1.13879007],
       [-3.76873198],
       [-1.89671717]])
        np.testing.assert_array_almost_equal(reg.u[0:3], u_3, 7)
        predy_3 = np.array([[ 14.58718993],
       [ 22.57048598],
       [ 32.52349817]])
        np.testing.assert_array_almost_equal(reg.predy[0:3], predy_3, 7)
        predy_e_3 = np.array([[ 18.39595799],
       [ 26.49961664],
       [ 35.02090882]])
        np.testing.assert_array_almost_equal(reg.predy_e[0:3], predy_e_3, 7)
        chow_regi = np.array([[ 0.60091096,  0.43823066],
       [ 0.03006744,  0.8623373 ],
       [ 0.01943727,  0.88912016]])
        np.testing.assert_array_almost_equal(reg.chow.regi, chow_regi, 7)
        self.assertAlmostEqual(reg.chow.joint[0], 0.88634854058300516, 7)
    
    def test_all_regi_sig2(self):
        #Artficial:
        n = 256
        x1 = np.random.uniform(-10,10,(n,1))
        x2 = np.random.uniform(1,5,(n,1))
        q = x2 + np.random.normal(0,1,(n,1))
        x = np.hstack((x1,x2))
        y = np.dot(np.hstack((np.ones((n,1)),x)),np.array([[1],[0.5],[2]])) + np.random.normal(0,1,(n,1))
        latt = int(np.sqrt(n))
        w = pysal.lat2W(latt,latt)
        w.transform='r'
        regi = [0]*(n/2) + [1]*(n/2)
        model = GM_Lag_Regimes(y, x1, regi, q=q, yend=x2, w=w, regime_lag_sep=True, regime_err_sep=True)
        w1 = pysal.lat2W(latt/2,latt)
        w1.transform='r'
        model1 = GM_Lag(y[0:(n/2)].reshape((n/2),1), x1[0:(n/2)], yend=x2[0:(n/2)], q=q[0:(n/2)], w=w1)
        model2 = GM_Lag(y[(n/2):n].reshape((n/2),1), x1[(n/2):n], yend=x2[(n/2):n], q=q[(n/2):n], w=w1)
        tbetas = np.vstack((model1.betas, model2.betas))
        np.testing.assert_array_almost_equal(model.betas,tbetas)
        vm = np.hstack((model1.vm.diagonal(),model2.vm.diagonal()))
        np.testing.assert_array_almost_equal(model.vm.diagonal(), vm, 6)
        #Columbus:
        X = np.array(self.db.by_col("INC"))
        X = np.reshape(X, (49,1))
        yd = np.array(self.db.by_col("HOVAL"))
        yd = np.reshape(yd, (49,1))
        q = np.array(self.db.by_col("DISCBD"))
        q = np.reshape(q, (49,1))
        reg = GM_Lag_Regimes(self.y, X, self.regimes, yend=yd, q=q, w=self.w,regime_lag_sep=True, regime_err_sep = True) 
        tbetas = np.array([[ 42.35827477],
       [ -0.09472413],
       [ -0.68794223],
       [  0.54482537],
       [ 32.24228762],
       [ -0.12304063],
       [ -0.46840307],
       [  0.67108156]])
        np.testing.assert_array_almost_equal(tbetas, reg.betas)
        vm = np.array([ 200.92894859,    4.56244927,   -4.85603079,   -2.9755413 ,
          0.        ,    0.        ,    0.        ,    0.        ])
        np.testing.assert_array_almost_equal(reg.vm[0], vm, 6)
        e_3 = np.array([[ -1.32209547],
       [-13.15611199],
       [-11.62357696]])
        np.testing.assert_array_almost_equal(reg.e_pred[0:3], e_3, 7)
        u_3 = np.array([[ 6.99250069],
       [-7.5665856 ],
       [-7.04753328]])
        np.testing.assert_array_almost_equal(reg.u[0:3], u_3, 7)
        predy_3 = np.array([[  8.73347931],
       [ 26.3683396 ],
       [ 37.67431428]])
        np.testing.assert_array_almost_equal(reg.predy[0:3], predy_3, 7)
        predy_e_3 = np.array([[ 17.04807547],
       [ 31.95786599],
       [ 42.25035796]])
        np.testing.assert_array_almost_equal(reg.predy_e[0:3], predy_e_3, 7)
        chow_regi = np.array([[  1.51825373e-01,   6.96797034e-01],
       [  3.20105698e-04,   9.85725412e-01],
       [  8.58836996e-02,   7.69476896e-01],
       [  1.01357290e-01,   7.50206873e-01]])
        np.testing.assert_array_almost_equal(reg.chow.regi, chow_regi, 7)
        self.assertAlmostEqual(reg.chow.joint[0], 0.38417230022512161, 7)

    def test_fixed_const(self):
        X = np.array(self.db.by_col("INC"))
        X = np.reshape(X, (49,1))
        yd = np.array(self.db.by_col("HOVAL"))
        yd = np.reshape(yd, (49,1))
        q = np.array(self.db.by_col("DISCBD"))
        q = np.reshape(q, (49,1))
        reg = GM_Lag_Regimes(self.y, X, self.regimes, yend=yd, q=q, w=self.w, constant_regi='one', regime_lag_sep=False, regime_err_sep=False) 
        tbetas = np.array([[ -0.37658823],
       [ -0.9666079 ],
       [ 35.5445944 ],
       [ -0.45793559],
       [ -0.24216904],
       [  0.62500602]])
        np.testing.assert_array_almost_equal(tbetas, reg.betas)
        vm = np.array([ 1.4183697 , -0.05975784, -0.27161863, -0.62517245,  0.02266177,
        0.00312976])
        np.testing.assert_array_almost_equal(reg.vm[0], vm, 6)
        e_3 = np.array([[ 0.17317815],
       [-5.53766328],
       [-3.82889307]])
        np.testing.assert_array_almost_equal(reg.e_pred[0:3], e_3, 7)
        u_3 = np.array([[ 3.10025518],
       [-1.83150689],
       [-1.49598494]])
        np.testing.assert_array_almost_equal(reg.u[0:3], u_3, 7)
        predy_3 = np.array([[ 12.62572482],
       [ 20.63326089],
       [ 32.12276594]])
        np.testing.assert_array_almost_equal(reg.predy[0:3], predy_3, 7)
        predy_e_3 = np.array([[ 15.55280185],
       [ 24.33941728],
       [ 34.45567407]])
        np.testing.assert_array_almost_equal(reg.predy_e[0:3], predy_e_3, 7)
        chow_regi = np.array([[  1.85767047e-01,   6.66463269e-01],
       [  1.19445012e+01,   5.48089036e-04]])
        np.testing.assert_array_almost_equal(reg.chow.regi, chow_regi, 7)
        self.assertAlmostEqual(reg.chow.joint[0], 12.017256217621382, 7)

    def test_names(self):
        y_var = 'CRIME'
        x_var = ['INC']
        x = np.array([self.db.by_col(name) for name in x_var]).T
        yd_var = ['HOVAL']
        yd = np.array([self.db.by_col(name) for name in yd_var]).T
        q_var = ['DISCBD']
        q = np.array([self.db.by_col(name) for name in q_var]).T
        r_var = 'NSA'
        reg = GM_Lag_Regimes(self.y, x, self.regimes, yend=yd, q=q, w=self.w, name_y=y_var, name_x=x_var, name_yend=yd_var, name_q=q_var, name_regimes=r_var, name_ds='columbus', name_w='columbus.gal', regime_lag_sep=False, regime_err_sep=False)
        betas = np.array([[ 37.87698329],
       [ -0.89426982],
       [ 31.4714777 ],
       [ -0.71640525],
       [ -0.28494432],
       [ -0.2294271 ],
       [  0.62996544]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array([ 109.93469618,   -0.33407447,   82.05180377,   -1.57964725,
         -1.50284553,   -0.15634575,   -1.43800683])
        np.testing.assert_array_almost_equal(reg.vm[0], vm, 6)
        chow_regi = np.array([[ 0.50743058,  0.47625326],
       [ 0.02437494,  0.87593468],
       [ 0.01377251,  0.9065777 ]])
        np.testing.assert_array_almost_equal(reg.chow.regi, chow_regi, 7)
        self.assertAlmostEqual(reg.chow.joint[0], 0.63336222761359162, 7)
        self.assertListEqual(reg.name_x, ['0_CONSTANT', '0_INC', '1_CONSTANT', '1_INC'])
        self.assertListEqual(reg.name_yend, ['0_HOVAL', '1_HOVAL', '_Global_W_CRIME'])
        self.assertListEqual(reg.name_q, ['0_DISCBD', '0_W_INC', '0_W_DISCBD', '1_DISCBD', '1_W_INC', '1_W_DISCBD'])
        self.assertEqual(reg.name_y, y_var)

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_twosls_sp_sparse
import unittest
import numpy as np
import pysal
from pysal.spreg.twosls_sp import BaseGM_Lag, GM_Lag
import pysal.spreg.diagnostics as D
from scipy import sparse as SP


class TestBaseGMLag(unittest.TestCase):
    def setUp(self):
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
        self.db = pysal.open(pysal.examples.get_path("columbus.dbf"), 'r')
        y = np.array(self.db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        
    def test___init__(self):
        X = []
        X.append(self.db.by_col("INC"))
        X.append(self.db.by_col("CRIME"))
        self.X = np.array(X).T
        yd2, q2 = pysal.spreg.utils.set_endog(self.y, self.X, self.w, None, None, 2, True)
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.X = SP.csr_matrix(self.X)
        reg = BaseGM_Lag(self.y, self.X, yend=yd2, q=q2, w=self.w, w_lags=2)
        betas = np.array([[  4.53017056e+01], [  6.20888617e-01], [ -4.80723451e-01], [  2.83622122e-02]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        h_0 = np.array([  1.        ,  19.531     ,  15.72598   ,  18.594     ,
                            24.7142675 ,  13.72216667,  27.82929567])
        np.testing.assert_array_almost_equal(reg.h.toarray()[0], h_0)
        hth = np.array([   49.        ,   704.371999  ,  1721.312371  ,   724.7435916 ,
                             1707.35412945,   711.31248483,  1729.63201243])
        np.testing.assert_array_almost_equal(reg.hth[0], hth, 7)
        hthi = np.array([  7.33701328e+00,   2.27764882e-02,   2.18153588e-02,
                           -5.11035447e-02,   1.22515181e-03,  -2.38079378e-01,
                           -1.20149133e-01])
        np.testing.assert_array_almost_equal(reg.hthi[0], hthi, 7)
        self.assertEqual(reg.k, 4)
        self.assertEqual(reg.kstar, 1)
        self.assertAlmostEqual(reg.mean_y, 38.436224469387746, 7)
        self.assertEqual(reg.n, 49)
        pfora1a2 = np.array([ 80.5588479 ,  -1.06625281,  -0.61703759,  -1.10071931]) 
        np.testing.assert_array_almost_equal(reg.pfora1a2[0], pfora1a2, 7)
        predy_5 = np.array([[ 50.87411532],[ 50.76969931],[ 41.77223722],[ 33.44262382],[ 28.77418036]])
        np.testing.assert_array_almost_equal(reg.predy[0:5], predy_5, 7)
        q_5 = np.array([ 18.594     ,  24.7142675 ,  13.72216667,  27.82929567])
        np.testing.assert_array_almost_equal(reg.q[0], q_5)
        self.assertAlmostEqual(reg.sig2n_k, 234.54258763039289, 7)
        self.assertAlmostEqual(reg.sig2n, 215.39625394627919, 7)
        self.assertAlmostEqual(reg.sig2, 215.39625394627919, 7)
        self.assertAlmostEqual(reg.std_y, 18.466069465206047, 7)
        u_5 = np.array( [[ 29.59288768], [ -6.20269831], [-15.42223722], [ -0.24262282], [ -5.54918036]])
        np.testing.assert_array_almost_equal(reg.u[0:5], u_5, 7)
        self.assertAlmostEqual(reg.utu, 10554.41644336768, 7)
        varb = np.array( [[  1.48966377e+00, -2.28698061e-02, -1.20217386e-02, -1.85763498e-02],
                          [ -2.28698061e-02,  1.27893998e-03,  2.74600023e-04, -1.33497705e-04],
                          [ -1.20217386e-02,  2.74600023e-04,  1.54257766e-04,  6.86851184e-05],
                          [ -1.85763498e-02, -1.33497705e-04,  6.86851184e-05,  4.67711582e-04]])
        np.testing.assert_array_almost_equal(reg.varb, varb, 7)
        vm = np.array([[  3.20867996e+02, -4.92607057e+00, -2.58943746e+00, -4.00127615e+00],
                       [ -4.92607057e+00,  2.75478880e-01,  5.91478163e-02, -2.87549056e-02],
                       [ -2.58943746e+00,  5.91478163e-02,  3.32265449e-02,  1.47945172e-02],
                       [ -4.00127615e+00, -2.87549056e-02,  1.47945172e-02,  1.00743323e-01]])
        np.testing.assert_array_almost_equal(reg.vm, vm, 6)
        x_0 = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x.toarray()[0], x_0, 7)
        y_5 = np.array( [[ 80.467003], [ 44.567001], [ 26.35    ], [ 33.200001], [ 23.225   ]])
        np.testing.assert_array_almost_equal(reg.y[0:5], y_5, 7)
        yend_5 = np.array( [[ 35.4585005 ], [ 46.67233467], [ 45.36475125], [ 32.81675025], [ 30.81785714]])
        np.testing.assert_array_almost_equal(reg.yend[0:5], yend_5, 7)
        z_0 = np.array([  1.       ,  19.531    ,  15.72598  ,  35.4585005]) 
        np.testing.assert_array_almost_equal(reg.z.toarray()[0], z_0, 7)
        zthhthi = np.array( [[  1.00000000e+00, -2.22044605e-16, -2.22044605e-16 , 2.22044605e-16,
                                4.44089210e-16,  0.00000000e+00, -8.88178420e-16],
                             [  0.00000000e+00,  1.00000000e+00, -3.55271368e-15 , 3.55271368e-15,
                               -7.10542736e-15,  7.10542736e-14,  0.00000000e+00],
                             [  1.81898940e-12,  2.84217094e-14,  1.00000000e+00 , 0.00000000e+00,
                               -2.84217094e-14,  5.68434189e-14,  5.68434189e-14],
                             [ -8.31133940e+00, -3.76104678e-01, -2.07028208e-01 , 1.32618931e+00,
                               -8.04284562e-01,  1.30527047e+00,  1.39136816e+00]])
        np.testing.assert_array_almost_equal(reg.zthhthi, zthhthi, 7)

    def test_init_white_(self):
        X = []
        X.append(self.db.by_col("INC"))
        X.append(self.db.by_col("CRIME"))
        self.X = np.array(X).T
        yd2, q2 = pysal.spreg.utils.set_endog(self.y, self.X, self.w, None, None, 2, True)
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.X = SP.csr_matrix(self.X)
        base_gm_lag = BaseGM_Lag(self.y, self.X, yend=yd2, q=q2, w=self.w, w_lags=2, robust='white')
        tbetas = np.array([[  4.53017056e+01], [  6.20888617e-01], [ -4.80723451e-01], [  2.83622122e-02]])
        np.testing.assert_array_almost_equal(base_gm_lag.betas, tbetas) 
        dbetas = D.se_betas(base_gm_lag)
        se_betas = np.array([ 20.47077481, 0.50613931, 0.20138425, 0.38028295 ])
        np.testing.assert_array_almost_equal(dbetas, se_betas)

    def test_init_hac_(self):
        X = []
        X.append(self.db.by_col("INC"))
        X.append(self.db.by_col("CRIME"))
        self.X = np.array(X).T
        yd2, q2 = pysal.spreg.utils.set_endog(self.y, self.X, self.w, None, None, 2, True)
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.X = SP.csr_matrix(self.X)
        gwk = pysal.kernelW_from_shapefile(pysal.examples.get_path('columbus.shp'),k=15,function='triangular', fixed=False)        
        base_gm_lag = BaseGM_Lag(self.y, self.X, yend=yd2, q=q2, w=self.w, w_lags=2, robust='hac', gwk=gwk)
        tbetas = np.array([[  4.53017056e+01], [  6.20888617e-01], [ -4.80723451e-01], [  2.83622122e-02]])
        np.testing.assert_array_almost_equal(base_gm_lag.betas, tbetas) 
        dbetas = D.se_betas(base_gm_lag)
        se_betas = np.array([ 19.08513569,   0.51769543,   0.18244862,   0.35460553])
        np.testing.assert_array_almost_equal(dbetas, se_betas)

    def test_init_discbd(self):
        X = np.array(self.db.by_col("INC"))
        self.X = np.reshape(X, (49,1))
        yd = np.array(self.db.by_col("CRIME"))
        yd = np.reshape(yd, (49,1))
        q = np.array(self.db.by_col("DISCBD"))
        q = np.reshape(q, (49,1))
        yd2, q2 = pysal.spreg.utils.set_endog(self.y, self.X, self.w, yd, q, 2, True)
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.X = SP.csr_matrix(self.X)
        reg = BaseGM_Lag(self.y, self.X, yend=yd2, q=q2, w=self.w, w_lags=2)
        tbetas = np.array([[ 100.79359082], [  -0.50215501], [  -1.14881711], [  -0.38235022]])
        np.testing.assert_array_almost_equal(tbetas, reg.betas)
        dbetas = D.se_betas(reg)
        se_betas = np.array([ 53.0829123 ,   1.02511494,   0.57589064,   0.59891744 ])
        np.testing.assert_array_almost_equal(dbetas, se_betas)

    def test_n_k(self):
        X = []
        X.append(self.db.by_col("INC"))
        X.append(self.db.by_col("CRIME"))
        self.X = np.array(X).T
        yd2, q2 = pysal.spreg.utils.set_endog(self.y, self.X, self.w, None, None, 2, True)
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.X = SP.csr_matrix(self.X)
        reg = BaseGM_Lag(self.y, self.X, yend=yd2, q=q2, w=self.w, w_lags=2, sig2n_k=True)
        betas = np.  array([[  4.53017056e+01], [  6.20888617e-01], [ -4.80723451e-01], [  2.83622122e-02]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array( [[  3.49389596e+02, -5.36394351e+00, -2.81960968e+00, -4.35694515e+00],
                         [ -5.36394351e+00,  2.99965892e-01,  6.44054000e-02, -3.13108972e-02],
                         [ -2.81960968e+00,  6.44054000e-02,  3.61800155e-02,  1.61095854e-02],
                         [ -4.35694515e+00, -3.13108972e-02,  1.61095854e-02,  1.09698285e-01]])
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)

    def test_lag_q(self):
        X = np.array(self.db.by_col("INC"))
        self.X = np.reshape(X, (49,1))
        yd = np.array(self.db.by_col("CRIME"))
        yd = np.reshape(yd, (49,1))
        q = np.array(self.db.by_col("DISCBD"))
        q = np.reshape(q, (49,1))
        yd2, q2 = pysal.spreg.utils.set_endog(self.y, self.X, self.w, yd, q, 2, False)
        self.X = np.hstack((np.ones(self.y.shape),self.X))
        self.X = SP.csr_matrix(self.X)
        reg = BaseGM_Lag(self.y, self.X, yend=yd2, q=q2, w=self.w, w_lags=2, lag_q=False)
        tbetas = np.array( [[ 108.83261383], [  -0.48041099], [  -1.18950006], [  -0.56140186]])
        np.testing.assert_array_almost_equal(tbetas, reg.betas)
        dbetas = D.se_betas(reg)
        se_betas = np.array([ 58.33203837,   1.09100446,   0.62315167,   0.68088777])
        np.testing.assert_array_almost_equal(dbetas, se_betas)



class TestGMLag(unittest.TestCase):
    def setUp(self):
        self.w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.w.transform = 'r'
        self.db = pysal.open(pysal.examples.get_path("columbus.dbf"), 'r')
        y = np.array(self.db.by_col("HOVAL"))
        self.y = np.reshape(y, (49,1))
        
    def test___init__(self):
        X = []
        X.append(self.db.by_col("INC"))
        X.append(self.db.by_col("CRIME"))
        self.X = np.array(X).T
        self.X = SP.csr_matrix(self.X)
        reg = GM_Lag(self.y, self.X, w=self.w, w_lags=2)
        betas = np.array([[  4.53017056e+01], [  6.20888617e-01], [ -4.80723451e-01], [  2.83622122e-02]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        e_5 = np.array( [[ 29.28976367], [ -6.07439501], [-15.30080685], [ -0.41773375], [ -5.67197968]])
        np.testing.assert_array_almost_equal(reg.e_pred[0:5], e_5, 7)
        h_0 = np.array([  1.        ,  19.531     ,  15.72598   ,  18.594     ,
                            24.7142675 ,  13.72216667,  27.82929567])
        np.testing.assert_array_almost_equal(reg.h.toarray()[0], h_0)
        hth = np.  array([   49.        ,   704.371999  ,  1721.312371  ,   724.7435916 ,
                             1707.35412945,   711.31248483,  1729.63201243])
        np.testing.assert_array_almost_equal(reg.hth[0], hth, 7)
        hthi = np.array([  7.33701328e+00,   2.27764882e-02,   2.18153588e-02,
                           -5.11035447e-02,   1.22515181e-03,  -2.38079378e-01,
                           -1.20149133e-01])
        np.testing.assert_array_almost_equal(reg.hthi[0], hthi, 7)
        self.assertEqual(reg.k, 4)
        self.assertEqual(reg.kstar, 1)
        self.assertAlmostEqual(reg.mean_y, 38.436224469387746, 7)
        self.assertEqual(reg.n, 49)
        pfora1a2 = np.array([ 80.5588479 ,  -1.06625281,  -0.61703759,  -1.10071931]) 
        self.assertAlmostEqual(reg.pr2, 0.3551928222612527, 7)
        self.assertAlmostEqual(reg.pr2_e, 0.34763857386174174, 7)
        np.testing.assert_array_almost_equal(reg.pfora1a2[0], pfora1a2, 7)
        predy_5 = np.array([[ 50.87411532],[ 50.76969931],[ 41.77223722],[ 33.44262382],[ 28.77418036]])
        np.testing.assert_array_almost_equal(reg.predy[0:5], predy_5, 7)
        predy_e_5 = np.array( [[ 51.17723933], [ 50.64139601], [ 41.65080685], [ 33.61773475], [ 28.89697968]])
        np.testing.assert_array_almost_equal(reg.predy_e[0:5], predy_e_5, 7)
        q_5 = np.array([ 18.594     ,  24.7142675 ,  13.72216667,  27.82929567])
        np.testing.assert_array_almost_equal(reg.q.toarray()[0], q_5)
        self.assertEqual(reg.robust, 'unadjusted')
        self.assertAlmostEqual(reg.sig2n_k, 234.54258763039289, 7)
        self.assertAlmostEqual(reg.sig2n, 215.39625394627919, 7)
        self.assertAlmostEqual(reg.sig2, 215.39625394627919, 7)
        self.assertAlmostEqual(reg.std_y, 18.466069465206047, 7)
        u_5 = np.array( [[ 29.59288768], [ -6.20269831], [-15.42223722], [ -0.24262282], [ -5.54918036]])
        np.testing.assert_array_almost_equal(reg.u[0:5], u_5, 7)
        self.assertAlmostEqual(reg.utu, 10554.41644336768, 7)
        varb = np.array( [[  1.48966377e+00, -2.28698061e-02, -1.20217386e-02, -1.85763498e-02],
                          [ -2.28698061e-02,  1.27893998e-03,  2.74600023e-04, -1.33497705e-04],
                          [ -1.20217386e-02,  2.74600023e-04,  1.54257766e-04,  6.86851184e-05],
                          [ -1.85763498e-02, -1.33497705e-04,  6.86851184e-05,  4.67711582e-04]])
        np.testing.assert_array_almost_equal(reg.varb, varb, 7)
        vm = np.array([[  3.20867996e+02, -4.92607057e+00, -2.58943746e+00, -4.00127615e+00],
                       [ -4.92607057e+00,  2.75478880e-01,  5.91478163e-02, -2.87549056e-02],
                       [ -2.58943746e+00,  5.91478163e-02,  3.32265449e-02,  1.47945172e-02],
                       [ -4.00127615e+00, -2.87549056e-02,  1.47945172e-02,  1.00743323e-01]])
        np.testing.assert_array_almost_equal(reg.vm, vm, 6)
        x_0 = np.array([  1.     ,  19.531  ,  15.72598])
        np.testing.assert_array_almost_equal(reg.x.toarray()[0], x_0, 7)
        y_5 = np.array( [[ 80.467003], [ 44.567001], [ 26.35    ], [ 33.200001], [ 23.225   ]])
        np.testing.assert_array_almost_equal(reg.y[0:5], y_5, 7)
        yend_5 = np.array( [[ 35.4585005 ], [ 46.67233467], [ 45.36475125], [ 32.81675025], [ 30.81785714]])
        np.testing.assert_array_almost_equal(reg.yend[0:5], yend_5, 7)
        z_0 = np.array([  1.       ,  19.531    ,  15.72598  ,  35.4585005]) 
        np.testing.assert_array_almost_equal(reg.z.toarray()[0], z_0, 7)
        zthhthi = np.array( [[  1.00000000e+00, -2.22044605e-16, -2.22044605e-16 , 2.22044605e-16,
                                4.44089210e-16,  0.00000000e+00, -8.88178420e-16],
                             [  0.00000000e+00,  1.00000000e+00, -3.55271368e-15 , 3.55271368e-15,
                               -7.10542736e-15,  7.10542736e-14,  0.00000000e+00],
                             [  1.81898940e-12,  2.84217094e-14,  1.00000000e+00 , 0.00000000e+00,
                               -2.84217094e-14,  5.68434189e-14,  5.68434189e-14],
                             [ -8.31133940e+00, -3.76104678e-01, -2.07028208e-01 , 1.32618931e+00,
                               -8.04284562e-01,  1.30527047e+00,  1.39136816e+00]])
        np.testing.assert_array_almost_equal(reg.zthhthi, zthhthi, 7)

    def test_init_white_(self):
        X = []
        X.append(self.db.by_col("INC"))
        X.append(self.db.by_col("CRIME"))
        self.X = np.array(X).T
        self.X = SP.csr_matrix(self.X)
        base_gm_lag = GM_Lag(self.y, self.X, w=self.w, w_lags=2, robust='white')
        tbetas = np.array([[  4.53017056e+01], [  6.20888617e-01], [ -4.80723451e-01], [  2.83622122e-02]])
        np.testing.assert_array_almost_equal(base_gm_lag.betas, tbetas) 
        dbetas = D.se_betas(base_gm_lag)
        se_betas = np.array([ 20.47077481, 0.50613931, 0.20138425, 0.38028295 ])
        np.testing.assert_array_almost_equal(dbetas, se_betas)

    def test_init_hac_(self):
        X = []
        X.append(self.db.by_col("INC"))
        X.append(self.db.by_col("CRIME"))
        self.X = np.array(X).T
        self.X = SP.csr_matrix(self.X)
        gwk = pysal.kernelW_from_shapefile(pysal.examples.get_path('columbus.shp'),k=15,function='triangular', fixed=False)        
        base_gm_lag = GM_Lag(self.y, self.X, w=self.w, w_lags=2, robust='hac', gwk=gwk)
        tbetas = np.array([[  4.53017056e+01], [  6.20888617e-01], [ -4.80723451e-01], [  2.83622122e-02]])
        np.testing.assert_array_almost_equal(base_gm_lag.betas, tbetas) 
        dbetas = D.se_betas(base_gm_lag)
        se_betas = np.array([ 19.08513569,   0.51769543,   0.18244862,   0.35460553])
        np.testing.assert_array_almost_equal(dbetas, se_betas)

    def test_init_discbd(self):
        X = np.array(self.db.by_col("INC"))
        X = np.reshape(X, (49,1))
        X = SP.csr_matrix(X)
        yd = np.array(self.db.by_col("CRIME"))
        yd = np.reshape(yd, (49,1))
        q = np.array(self.db.by_col("DISCBD"))
        q = np.reshape(q, (49,1))
        reg = GM_Lag(self.y, X, w=self.w, yend=yd, q=q, w_lags=2)
        tbetas = np.array([[ 100.79359082], [  -0.50215501], [  -1.14881711], [  -0.38235022]])
        np.testing.assert_array_almost_equal(tbetas, reg.betas)
        dbetas = D.se_betas(reg)
        se_betas = np.array([ 53.0829123 ,   1.02511494,   0.57589064,   0.59891744 ])
        np.testing.assert_array_almost_equal(dbetas, se_betas)

    def test_n_k(self):
        X = []
        X.append(self.db.by_col("INC"))
        X.append(self.db.by_col("CRIME"))
        self.X = np.array(X).T
        self.X = SP.csr_matrix(self.X)
        reg = GM_Lag(self.y, self.X, w=self.w, w_lags=2, sig2n_k=True)
        betas = np.  array([[  4.53017056e+01], [  6.20888617e-01], [ -4.80723451e-01], [  2.83622122e-02]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array( [[  3.49389596e+02, -5.36394351e+00, -2.81960968e+00, -4.35694515e+00],
                         [ -5.36394351e+00,  2.99965892e-01,  6.44054000e-02, -3.13108972e-02],
                         [ -2.81960968e+00,  6.44054000e-02,  3.61800155e-02,  1.61095854e-02],
                         [ -4.35694515e+00, -3.13108972e-02,  1.61095854e-02,  1.09698285e-01]])
        np.testing.assert_array_almost_equal(reg.vm, vm, 7)

    def test_lag_q(self):
        X = np.array(self.db.by_col("INC"))
        X = np.reshape(X, (49,1))
        X = SP.csr_matrix(X)
        yd = np.array(self.db.by_col("CRIME"))
        yd = np.reshape(yd, (49,1))
        q = np.array(self.db.by_col("DISCBD"))
        q = np.reshape(q, (49,1))
        reg = GM_Lag(self.y, X, w=self.w, yend=yd, q=q, w_lags=2, lag_q=False)
        tbetas = np.array( [[ 108.83261383], [  -0.48041099], [  -1.18950006], [  -0.56140186]])
        np.testing.assert_array_almost_equal(tbetas, reg.betas)
        dbetas = D.se_betas(reg)
        se_betas = np.array([ 58.33203837,   1.09100446,   0.62315167,   0.68088777])
        np.testing.assert_array_almost_equal(dbetas, se_betas)

    def test_spatial(self):
        X = np.array(self.db.by_col("INC"))
        X = np.reshape(X, (49,1))
        X = SP.csr_matrix(X)
        yd = np.array(self.db.by_col("CRIME"))
        yd = np.reshape(yd, (49,1))
        q = np.array(self.db.by_col("DISCBD"))
        q = np.reshape(q, (49,1))
        w = pysal.queen_from_shapefile(pysal.examples.get_path('columbus.shp'))
        reg = GM_Lag(self.y, X, yd, q, spat_diag=True, w=w)
        betas = np.array([[  5.46344924e+01], [  4.13301682e-01], [ -5.92637442e-01], [ -7.40490883e-03]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array( [[  4.45202654e+02, -1.50290275e+01, -6.36557072e+00, -5.71403440e-03],
                        [ -1.50290275e+01,  5.93124683e-01,  2.19169508e-01, -6.70675916e-03],
                        [ -6.36557072e+00,  2.19169508e-01,  1.06577542e-01, -2.96533875e-03],
                        [ -5.71403440e-03, -6.70675916e-03, -2.96533875e-03,  1.15655425e-03]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 6)
        ak_test = np.array([ 2.52597326,  0.11198567])
        np.testing.assert_array_almost_equal(reg.ak_test, ak_test, 7)

    def test_names(self):
        X = np.array(self.db.by_col("INC"))
        X = np.reshape(X, (49,1))
        X = SP.csr_matrix(X)
        yd = np.array(self.db.by_col("CRIME"))
        yd = np.reshape(yd, (49,1))
        q = np.array(self.db.by_col("DISCBD"))
        q = np.reshape(q, (49,1))
        w = pysal.queen_from_shapefile(pysal.examples.get_path('columbus.shp'))
        gwk = pysal.kernelW_from_shapefile(pysal.examples.get_path('columbus.shp'),k=5,function='triangular', fixed=False)
        name_x = ['inc']
        name_y = 'crime'
        name_yend = ['crime']
        name_q = ['discbd']
        name_w = 'queen'
        name_gwk = 'k=5'
        name_ds = 'columbus'
        reg = GM_Lag(self.y, X, yd, q,
                spat_diag=True, w=w, robust='hac', gwk=gwk,
                name_x=name_x, name_y=name_y, name_q=name_q, name_w=name_w,
                name_yend=name_yend, name_gwk=name_gwk, name_ds=name_ds)
        betas = np.array([[  5.46344924e+01], [  4.13301682e-01], [ -5.92637442e-01], [ -7.40490883e-03]])
        np.testing.assert_array_almost_equal(reg.betas, betas, 7)
        vm = np.array( [[  5.70817052e+02, -1.83655385e+01, -8.36602575e+00,  2.37538877e-02],
                        [ -1.85224661e+01,  6.53311383e-01,  2.84209566e-01, -6.47694160e-03],
                        [ -8.31105622e+00,  2.78772694e-01,  1.38144928e-01, -3.98175246e-03],
                        [  2.66662466e-02, -6.23783104e-03, -4.11092891e-03,  1.10936528e-03]]) 
        np.testing.assert_array_almost_equal(reg.vm, vm, 6)
        self.assertListEqual(reg.name_x, ['CONSTANT']+name_x)
        name_yend.append('W_crime')
        self.assertListEqual(reg.name_yend, name_yend)
        name_q.extend(['W_inc', 'W_discbd'])
        self.assertListEqual(reg.name_q, name_q)
        self.assertEqual(reg.name_y, name_y)
        self.assertEqual(reg.name_w, name_w)
        self.assertEqual(reg.name_gwk, name_gwk)
        self.assertEqual(reg.name_ds, name_ds)



if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = twosls
import numpy as np
import copy
import numpy.linalg as la
import summary_output as SUMMARY
import robust as ROBUST
import user_output as USER
from utils import spdot, sphstack, RegressionPropsY, RegressionPropsVM

__author__ = "Luc Anselin luc.anselin@asu.edu, David C. Folch david.folch@asu.edu, Jing Yao jingyao@asu.edu"
__all__ = ["TSLS"]


class BaseTSLS(RegressionPropsY, RegressionPropsVM):

    """
    Two stage least squares (2SLS) (note: no consistency checks,
    diagnostics or constant added)

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note: 
                   this should not contain any variables from x); cannot be
                   used in combination with h
    h            : array
                   Two dimensional array with n rows and one column for each
                   exogenous variable to use as instruments (note: this 
                   can contain variables from x); cannot be used in 
                   combination with q
    robust       : string
                   If 'white', then a White consistent estimator of the
                   variance-covariance matrix is given.  If 'hac', then a
                   HAC consistent estimator of the variance-covariance
                   matrix is given. Default set to None. 
    gwk          : pysal W object
                   Kernel spatial weights needed for HAC estimation. Note:
                   matrix must have ones along the main diagonal.
    sig2n_k      : boolean
                   If True, then use n-k to estimate sigma^2. If False, use n.


    Attributes
    ----------
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    kstar        : integer
                   Number of endogenous variables. 
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable used as instruments 
    z            : array
                   nxk array of variables (combination of x and yend)
    h            : array
                   nxl array of instruments (combination of x and q)
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    utu          : float
                   Sum of squared residuals
    sig2         : float
                   Sigma squared used in computations
    sig2n        : float
                   Sigma squared (computed with n in the denominator)
    sig2n_k      : float
                   Sigma squared (computed with n-k in the denominator)
    hth          : float
                   H'H
    hthi         : float
                   (H'H)^-1
    varb         : array
                   (Z'H (H'H)^-1 H'Z)^-1
    zthhthi      : array
                   Z'H(H'H)^-1
    pfora1a2     : array
                   n(zthhthi)'varb

    
    Examples
    --------

    >>> import numpy as np
    >>> import pysal
    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),'r')
    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))
    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X = np.array(X).T
    >>> X = np.hstack((np.ones(y.shape),X))
    >>> yd = []
    >>> yd.append(db.by_col("HOVAL"))
    >>> yd = np.array(yd).T
    >>> q = []
    >>> q.append(db.by_col("DISCBD"))
    >>> q = np.array(q).T
    >>> reg = BaseTSLS(y, X, yd, q=q)
    >>> print reg.betas
    [[ 88.46579584]
     [  0.5200379 ]
     [ -1.58216593]]
    >>> reg = BaseTSLS(y, X, yd, q=q, robust="white")
    
    """

    def __init__(self, y, x, yend, q=None, h=None,
                 robust=None, gwk=None, sig2n_k=False):

        if issubclass(type(q), np.ndarray) and issubclass(type(h), np.ndarray):
            raise Exception, "Please do not provide 'q' and 'h' together"
        if q == None and h == None:
            raise Exception, "Please provide either 'q' or 'h'"

        self.y = y
        self.n = y.shape[0]
        self.x = x

        self.kstar = yend.shape[1]
        # including exogenous and endogenous variables
        z = sphstack(self.x, yend)
        if type(h).__name__ not in ['ndarray', 'csr_matrix']:
            # including exogenous variables and instrument
            h = sphstack(self.x, q)
        self.z = z
        self.h = h
        self.q = q
        self.yend = yend
        # k = number of exogenous variables and endogenous variables
        self.k = z.shape[1]
        hth = spdot(h.T, h)
        hthi = la.inv(hth)
        zth = spdot(z.T, h)
        hty = spdot(h.T, y)

        factor_1 = np.dot(zth, hthi)
        factor_2 = np.dot(factor_1, zth.T)
        # this one needs to be in cache to be used in AK
        varb = la.inv(factor_2)
        factor_3 = np.dot(varb, factor_1)
        betas = np.dot(factor_3, hty)
        self.betas = betas
        self.varb = varb
        self.zthhthi = factor_1

        # predicted values
        self.predy = spdot(z, betas)

        # residuals
        u = y - self.predy
        self.u = u

        # attributes used in property
        self.hth = hth     # Required for condition index
        self.hthi = hthi   # Used in error models
        self.htz = zth.T

        if robust:
            self.vm = ROBUST.robust_vm(reg=self, gwk=gwk, sig2n_k=sig2n_k)

        self._cache = {}
        if sig2n_k:
            self.sig2 = self.sig2n_k
        else:
            self.sig2 = self.sig2n

    @property
    def pfora1a2(self):
        if 'pfora1a2' not in self._cache:
            self._cache['pfora1a2'] = self.n * \
                np.dot(self.zthhthi.T, self.varb)
        return self._cache['pfora1a2']

    @property
    def vm(self):
        if 'vm' not in self._cache:
            self._cache['vm'] = np.dot(self.sig2, self.varb)
        return self._cache['vm']


class TSLS(BaseTSLS):

    """
    Two stage least squares with results and diagnostics.

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note: 
                   this should not contain any variables from x)
    w            : pysal W object
                   Spatial weights object (required if running spatial
                   diagnostics)
    robust       : string
                   If 'white', then a White consistent estimator of the
                   variance-covariance matrix is given.  If 'hac', then a
                   HAC consistent estimator of the variance-covariance
                   matrix is given. Default set to None. 
    gwk          : pysal W object
                   Kernel spatial weights needed for HAC estimation. Note:
                   matrix must have ones along the main diagonal.
    sig2n_k      : boolean
                   If True, then use n-k to estimate sigma^2. If False, use n.
    spat_diag    : boolean
                   If True, then compute Anselin-Kelejian test (requires w)
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_yend    : list of strings
                   Names of endogenous variables for use in output
    name_q       : list of strings
                   Names of instruments for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_gwk     : string
                   Name of kernel weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output


    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    kstar        : integer
                   Number of endogenous variables. 
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable used as instruments 
    z            : array
                   nxk array of variables (combination of x and yend)
    h            : array
                   nxl array of instruments (combination of x and q)
    robust       : string
                   Adjustment for robust standard errors
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
    utu          : float
                   Sum of squared residuals
    sig2         : float
                   Sigma squared used in computations
    std_err      : array
                   1xk array of standard errors of the betas    
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
    ak_test      : tuple
                   Anselin-Kelejian test; tuple contains the pair (statistic,
                   p-value)
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_yend    : list of strings
                   Names of endogenous variables for use in output
    name_z       : list of strings
                   Names of exogenous and endogenous variables for use in 
                   output
    name_q       : list of strings
                   Names of external instruments
    name_h       : list of strings
                   Names of all instruments used in ouput
    name_w       : string
                   Name of weights matrix for use in output
    name_gwk     : string
                   Name of kernel weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    title        : string
                   Name of the regression method used
    sig2n        : float
                   Sigma squared (computed with n in the denominator)
    sig2n_k      : float
                   Sigma squared (computed with n-k in the denominator)
    hth          : float
                   H'H
    hthi         : float
                   (H'H)^-1
    varb         : array
                   (Z'H (H'H)^-1 H'Z)^-1
    zthhthi      : array
                   Z'H(H'H)^-1
    pfora1a2     : array
                   n(zthhthi)'varb

    
    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import numpy as np
    >>> import pysal

    Open data on Columbus neighborhood crime (49 areas) using pysal.open().
    This is the DBF associated with the Columbus shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),'r')
    
    Extract the CRIME column (crime rates) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))

    Extract INC (income) vector from the DBF to be used as
    independent variables in the regression.  Note that PySAL requires this to
    be an nxj numpy array, where j is the number of independent variables (not
    including a constant). By default this model adds a vector of ones to the
    independent variables passed in, but this can be overridden by passing
    constant=False.

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X = np.array(X).T

    In this case we consider HOVAL (home value) is an endogenous regressor.
    We tell the model that this is so by passing it in a different parameter
    from the exogenous variables (x).

    >>> yd = []
    >>> yd.append(db.by_col("HOVAL"))
    >>> yd = np.array(yd).T

    Because we have endogenous variables, to obtain a correct estimate of the
    model, we need to instrument for HOVAL. We use DISCBD (distance to the
    CBD) for this and hence put it in the instruments parameter, 'q'.

    >>> q = []
    >>> q.append(db.by_col("DISCBD"))
    >>> q = np.array(q).T

    We are all set with the preliminars, we are good to run the model. In this
    case, we will need the variables (exogenous and endogenous) and the
    instruments. If we want to have the names of the variables printed in the
    output summary, we will have to pass them in as well, although this is optional.

    >>> reg = TSLS(y, X, yd, q, name_x=['inc'], name_y='crime', name_yend=['hoval'], name_q=['discbd'], name_ds='columbus')
    >>> print reg.betas
    [[ 88.46579584]
     [  0.5200379 ]
     [ -1.58216593]]

    """

    def __init__(self, y, x, yend, q,
                 w=None,
                 robust=None, gwk=None, sig2n_k=False,
                 spat_diag=False,
                 vm=False, name_y=None, name_x=None,
                 name_yend=None, name_q=None,
                 name_w=None, name_gwk=None, name_ds=None):

        n = USER.check_arrays(y, x, yend, q)
        USER.check_y(y, n)
        USER.check_weights(w, y)
        USER.check_robust(robust, gwk)
        USER.check_spat_diag(spat_diag, w)
        x_constant = USER.check_constant(x)
        BaseTSLS.__init__(self, y=y, x=x_constant, yend=yend, q=q,
                          robust=robust, gwk=gwk, sig2n_k=sig2n_k)
        self.title = "TWO STAGE LEAST SQUARES"
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_y = USER.set_name_y(name_y)
        self.name_x = USER.set_name_x(name_x, x)
        self.name_yend = USER.set_name_yend(name_yend, yend)
        self.name_z = self.name_x + self.name_yend
        self.name_q = USER.set_name_q(name_q, q)
        self.name_h = USER.set_name_h(self.name_x, self.name_q)
        self.robust = USER.set_robust(robust)
        self.name_w = USER.set_name_w(name_w, w)
        self.name_gwk = USER.set_name_w(name_gwk, gwk)
        SUMMARY.TSLS(reg=self, vm=vm, w=w, spat_diag=spat_diag)


def _test():
    import doctest
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)


if __name__ == '__main__':
    _test()

    import numpy as np
    import pysal
    db = pysal.open(pysal.examples.get_path("columbus.dbf"), 'r')
    y_var = 'CRIME'
    y = np.array([db.by_col(y_var)]).reshape(49, 1)
    x_var = ['INC']
    x = np.array([db.by_col(name) for name in x_var]).T
    yd_var = ['HOVAL']
    yd = np.array([db.by_col(name) for name in yd_var]).T
    q_var = ['DISCBD']
    q = np.array([db.by_col(name) for name in q_var]).T
    w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
    w.transform = 'r'
    tsls = TSLS(y, x, yd, q, w=w, spat_diag=True, name_y=y_var, name_x=x_var,
                name_yend=yd_var, name_q=q_var, name_ds='columbus', name_w='columbus.gal')
    print tsls.summary

########NEW FILE########
__FILENAME__ = twosls_regimes
import numpy as np
import regimes as REGI
import user_output as USER
import multiprocessing as mp
import scipy.sparse as SP
from utils import sphstack, set_warn, RegressionProps_basic, spdot, sphstack
from twosls import BaseTSLS
from robust import hac_multi
import summary_output as SUMMARY
from platform import system

"""
Two-stage Least Squares estimation with regimes.
"""

__author__ = "Luc Anselin luc.anselin@asu.edu, Pedro V. Amaral pedro.amaral@asu.edu, David C. Folch david.folch@asu.edu"


class TSLS_Regimes(BaseTSLS, REGI.Regimes_Frame):

    """
    Two stage least squares (2SLS) with regimes

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note: 
                   this should not contain any variables from x)
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    constant_regi: ['one', 'many']
                   Switcher controlling the constant term setup. It may take
                   the following values:
                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime (default)
    cols2regi    : list, 'all'
                   Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all' (default), all the variables vary by regime.
    regime_err_sep : boolean
                   If True, a separate regression is run for each regime.
    robust       : string
                   If 'white', then a White consistent estimator of the
                   variance-covariance matrix is given.
                   If 'hac', then a HAC consistent estimator of the 
                   variance-covariance matrix is given.
                   If 'ogmm', then Optimal GMM is used to estimate
                   betas and the variance-covariance matrix.
                   Default set to None. 
    gwk          : pysal W object
                   Kernel spatial weights needed for HAC estimation. Note:
                   matrix must have ones along the main diagonal.
    sig2n_k      : boolean
                   If True, then use n-k to estimate sigma^2. If False, use n.
    vm           : boolean
                   If True, include variance-covariance matrix in summary
    cores        : integer
                   Specifies the number of cores to be used in multiprocessing
                   Default: all cores available (specified as cores=None).
                   Note: Multiprocessing currently not available on Windows.
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_yend    : list of strings
                   Names of endogenous variables for use in output
    name_q       : list of strings
                   Names of instruments for use in output
    name_regimes : string
                   Name of regimes variable for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_gwk     : string
                   Name of kernel weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output

    Attributes
    ----------
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable used as instruments 
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    vm           : array
                   Variance covariance matrix (kxk)
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    constant_regi: [False, 'one', 'many']
                   Ignored if regimes=False. Constant option for regimes.
                   Switcher controlling the constant term setup. It may take
                   the following values:
                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime
    cols2regi    : list, 'all'
                   Ignored if regimes=False. Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all', all the variables vary by regime.
    regime_err_sep : boolean
                   If True, a separate regression is run for each regime.
    kr           : int
                   Number of variables/columns to be "regimized" or subject
                   to change by regime. These will result in one parameter
                   estimate by regime for each variable (i.e. nr parameters per
                   variable)
    kf           : int
                   Number of variables/columns to be considered fixed or
                   global across regimes and hence only obtain one parameter
                   estimate
    nr           : int
                   Number of different regimes in the 'regimes' list
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_yend    : list of strings
                   Names of endogenous variables for use in output
    name_q       : list of strings
                   Names of instruments for use in output
    name_regimes : string
                   Name of regimes variable for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_gwk     : string
                   Name of kernel weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    multi        : dictionary
                   Only available when multiple regressions are estimated,
                   i.e. when regime_err_sep=True and no variable is fixed
                   across regimes.
                   Contains all attributes of each individual regression

    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import numpy as np
    >>> import pysal

    Open data on NCOVR US County Homicides (3085 areas) using pysal.open().
    This is the DBF associated with the NAT shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> db = pysal.open(pysal.examples.get_path("NAT.dbf"),'r')
 
    Extract the HR90 column (homicide rates in 1990) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y_var = 'HR90'
    >>> y = np.array([db.by_col(y_var)]).reshape(3085,1)

    Extract UE90 (unemployment rate) and PS90 (population structure) vectors from
    the DBF to be used as independent variables in the regression. Other variables
    can be inserted by adding their names to x_var, such as x_var = ['Var1','Var2','...]
    Note that PySAL requires this to be an nxj numpy array, where j is the
    number of independent variables (not including a constant). By default
    this model adds a vector of ones to the independent variables passed in.

    >>> x_var = ['PS90','UE90']
    >>> x = np.array([db.by_col(name) for name in x_var]).T

    In this case we consider RD90 (resource deprivation) as an endogenous regressor.
    We tell the model that this is so by passing it in a different parameter
    from the exogenous variables (x).

    >>> yd_var = ['RD90']
    >>> yd = np.array([db.by_col(name) for name in yd_var]).T

    Because we have endogenous variables, to obtain a correct estimate of the
    model, we need to instrument for RD90. We use FP89 (families below poverty)
    for this and hence put it in the instruments parameter, 'q'.

    >>> q_var = ['FP89']
    >>> q = np.array([db.by_col(name) for name in q_var]).T

    The different regimes in this data are given according to the North and 
    South dummy (SOUTH).

    >>> r_var = 'SOUTH'
    >>> regimes = db.by_col(r_var)

    Since we want to perform tests for spatial dependence, we need to specify
    the spatial weights matrix that includes the spatial configuration of the
    observations into the error component of the model. To do that, we can open
    an already existing gal file or create a new one. In this case, we will
    create one from ``NAT.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("NAT.shp"))

    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, this allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'

    We can now run the regression and then have a summary of the output
    by typing: model.summary
    Alternatively, we can just check the betas and standard errors of the
    parameters:

    >>> tslsr = TSLS_Regimes(y, x, yd, q, regimes, w=w, constant_regi='many', spat_diag=False, name_y=y_var, name_x=x_var, name_yend=yd_var, name_q=q_var, name_regimes=r_var, name_ds='NAT', name_w='NAT.shp')

    >>> tslsr.betas
    array([[ 3.66973562],
           [ 1.06950466],
           [ 0.14680946],
           [ 2.45864196],
           [ 9.55873243],
           [ 1.94666348],
           [-0.30810214],
           [ 3.68718119]])

    >>> np.sqrt(tslsr.vm.diagonal())
    array([ 0.38389901,  0.09963973,  0.04672091,  0.22725012,  0.49181223,
            0.19630774,  0.07784587,  0.25529011])

    """

    def __init__(self, y, x, yend, q, regimes,
                 w=None, robust=None, gwk=None, sig2n_k=True,
                 spat_diag=False, vm=False, constant_regi='many',
                 cols2regi='all', regime_err_sep=True, name_y=None, name_x=None,
                 cores=None, name_yend=None, name_q=None, name_regimes=None,
                 name_w=None, name_gwk=None, name_ds=None, summ=True):

        n = USER.check_arrays(y, x)
        USER.check_y(y, n)
        USER.check_weights(w, y)
        USER.check_robust(robust, gwk)
        USER.check_spat_diag(spat_diag, w)
        self.constant_regi = constant_regi
        self.cols2regi = cols2regi
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_regimes = USER.set_name_ds(name_regimes)
        self.name_w = USER.set_name_w(name_w, w)
        self.name_gwk = USER.set_name_w(name_gwk, gwk)
        self.name_y = USER.set_name_y(name_y)
        name_yend = USER.set_name_yend(name_yend, yend)
        name_q = USER.set_name_q(name_q, q)
        self.name_x_r = USER.set_name_x(name_x, x) + name_yend
        self.n = n
        cols2regi = REGI.check_cols2regi(
            constant_regi, cols2regi, x, yend=yend, add_cons=False)
        self.regimes_set = REGI._get_regimes_set(regimes)
        self.regimes = regimes
        USER.check_regimes(self.regimes_set, self.n, x.shape[1])
        if regime_err_sep == True and robust == 'hac':
            set_warn(
                self, "Error by regimes is incompatible with HAC estimation for 2SLS models. Hence, the error by regimes has been disabled for this model.")
            regime_err_sep = False
        self.regime_err_sep = regime_err_sep
        if regime_err_sep == True and set(cols2regi) == set([True]) and constant_regi == 'many':
            name_x = USER.set_name_x(name_x, x)
            self.y = y
            regi_ids = dict((r, list(np.where(np.array(regimes) == r)[0]))
                            for r in self.regimes_set)
            self._tsls_regimes_multi(x, yend, q, w, regi_ids, cores,
                                     gwk, sig2n_k, robust, spat_diag, vm, name_x, name_yend, name_q)
        else:
            name_x = USER.set_name_x(name_x, x, constant=True)
            q, self.name_q = REGI.Regimes_Frame.__init__(self, q,
                                                         regimes, constant_regi=None, cols2regi='all', names=name_q)
            x, self.name_x = REGI.Regimes_Frame.__init__(self, x,
                                                         regimes, constant_regi, cols2regi=cols2regi, names=name_x)
            yend, self.name_yend = REGI.Regimes_Frame.__init__(self, yend,
                                                               regimes, constant_regi=None,
                                                               cols2regi=cols2regi, yend=True, names=name_yend)
            if regime_err_sep == True and robust == None:
                robust = 'white'
            BaseTSLS.__init__(self, y=y, x=x, yend=yend, q=q,
                              robust=robust, gwk=gwk, sig2n_k=sig2n_k)
            self.title = "TWO STAGE LEAST SQUARES - REGIMES"
            if robust == 'ogmm':
                _optimal_weight(self, sig2n_k)
            self.name_z = self.name_x + self.name_yend
            self.name_h = USER.set_name_h(self.name_x, self.name_q)
            self.chow = REGI.Chow(self)
            self.robust = USER.set_robust(robust)
            if summ:
                SUMMARY.TSLS(reg=self, vm=vm, w=w,
                             spat_diag=spat_diag, regimes=True)

    def _tsls_regimes_multi(self, x, yend, q, w, regi_ids, cores,
                            gwk, sig2n_k, robust, spat_diag, vm, name_x, name_yend, name_q):
        results_p = {}
        for r in self.regimes_set:
            if system() != 'Windows':
                is_win = True
                results_p[r] = _work(
                    *(self.y, x, w, regi_ids, r, yend, q, robust, sig2n_k, self.name_ds,
                      self.name_y, name_x, name_yend, name_q, self.name_w, self.name_regimes))
            else:
                pool = mp.Pool(cores)
                results_p[r] = pool.apply_async(
                    _work, args=(
                        self.y, x, w, regi_ids, r, yend, q, robust, sig2n_k,
                        self.name_ds, self.name_y, name_x, name_yend, name_q, self.name_w, self.name_regimes))
                is_win = False
        self.kryd = 0
        self.kr = x.shape[1] + yend.shape[1] + 1
        self.kf = 0
        self.nr = len(self.regimes_set)
        self.vm = np.zeros((self.nr * self.kr, self.nr * self.kr), float)
        self.betas = np.zeros((self.nr * self.kr, 1), float)
        self.u = np.zeros((self.n, 1), float)
        self.predy = np.zeros((self.n, 1), float)
        if not is_win:
            pool.close()
            pool.join()
        results = {}
        self.name_y, self.name_x, self.name_yend, self.name_q, self.name_z, self.name_h = [
        ], [], [], [], [], []
        counter = 0
        for r in self.regimes_set:
            if is_win:
                results[r] = results_p[r]
            else:
                results[r] = results_p[r].get()
            self.vm[(counter * self.kr):((counter + 1) * self.kr),
                    (counter * self.kr):((counter + 1) * self.kr)] = results[r].vm
            self.betas[(counter * self.kr):((counter + 1) * self.kr),
                       ] = results[r].betas
            self.u[regi_ids[r], ] = results[r].u
            self.predy[regi_ids[r], ] = results[r].predy
            self.name_y += results[r].name_y
            self.name_x += results[r].name_x
            self.name_yend += results[r].name_yend
            self.name_q += results[r].name_q
            self.name_z += results[r].name_z
            self.name_h += results[r].name_h
            counter += 1
        self.multi = results
        self.hac_var = sphstack(x, q)
        if robust == 'hac':
            hac_multi(self, gwk)
        if robust == 'ogmm':
            set_warn(
                self, "Residuals treated as homoskedastic for the purpose of diagnostics.")
        self.chow = REGI.Chow(self)
        if spat_diag:
            self._get_spat_diag_props(results, regi_ids, x, yend, q)
        SUMMARY.TSLS_multi(reg=self, multireg=self.multi,
                           vm=vm, spat_diag=spat_diag, regimes=True, w=w)

    def _get_spat_diag_props(self, results, regi_ids, x, yend, q):
        self._cache = {}
        x = USER.check_constant(x)
        x = REGI.regimeX_setup(
            x, self.regimes, [True] * x.shape[1], self.regimes_set)
        self.z = sphstack(x, REGI.regimeX_setup(
            yend, self.regimes, [True] * yend.shape[1], self.regimes_set))
        self.h = sphstack(
            x, REGI.regimeX_setup(q, self.regimes, [True] * q.shape[1], self.regimes_set))
        hthi = np.linalg.inv(spdot(self.h.T, self.h))
        zth = spdot(self.z.T, self.h)
        self.varb = np.linalg.inv(spdot(spdot(zth, hthi), zth.T))


def _work(y, x, w, regi_ids, r, yend, q, robust, sig2n_k, name_ds, name_y, name_x, name_yend, name_q, name_w, name_regimes):
    y_r = y[regi_ids[r]]
    x_r = x[regi_ids[r]]
    yend_r = yend[regi_ids[r]]
    q_r = q[regi_ids[r]]
    x_constant = USER.check_constant(x_r)
    if robust == 'hac' or robust == 'ogmm':
        robust2 = None
    else:
        robust2 = robust
    model = BaseTSLS(y_r, x_constant, yend_r, q_r,
                     robust=robust2, sig2n_k=sig2n_k)
    model.title = "TWO STAGE LEAST SQUARES ESTIMATION - REGIME %s" % r
    if robust == 'ogmm':
        _optimal_weight(model, sig2n_k, warn=False)
    model.robust = USER.set_robust(robust)
    model.name_ds = name_ds
    model.name_y = '%s_%s' % (str(r), name_y)
    model.name_x = ['%s_%s' % (str(r), i) for i in name_x]
    model.name_yend = ['%s_%s' % (str(r), i) for i in name_yend]
    model.name_z = model.name_x + model.name_yend
    model.name_q = ['%s_%s' % (str(r), i) for i in name_q]
    model.name_h = model.name_x + model.name_q
    model.name_w = name_w
    model.name_regimes = name_regimes
    if w:
        w_r, warn = REGI.w_regime(w, regi_ids[r], r, transform=True)
        set_warn(model, warn)
        model.w = w_r
    return model


def _optimal_weight(reg, sig2n_k, warn=True):
    try:
        Hu = reg.h.toarray() * reg.u ** 2
    except:
        Hu = reg.h * reg.u ** 2
    if sig2n_k:
        S = spdot(reg.h.T, Hu, array_out=True) / (reg.n - reg.k)
    else:
        S = spdot(reg.h.T, Hu, array_out=True) / reg.n
    Si = np.linalg.inv(S)
    ZtH = spdot(reg.z.T, reg.h)
    ZtHSi = spdot(ZtH, Si)
    fac2 = np.linalg.inv(spdot(ZtHSi, ZtH.T, array_out=True))
    fac3 = spdot(ZtHSi, spdot(reg.h.T, reg.y), array_out=True)
    betas = np.dot(fac2, fac3)
    if sig2n_k:
        vm = fac2 * (reg.n - reg.k)
    else:
        vm = fac2 * reg.n
    RegressionProps_basic(reg, betas=betas, vm=vm, sig2=False)
    reg.title += " (Optimal-Weighted GMM)"
    if warn:
        set_warn(
            reg, "Residuals treated as homoskedastic for the purpose of diagnostics.")
    return


def _test():
    import doctest
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)


if __name__ == '__main__':
    _test()
    import numpy as np
    import pysal
    db = pysal.open(pysal.examples.get_path('NAT.dbf'), 'r')
    y_var = 'HR60'
    y = np.array([db.by_col(y_var)]).T
    x_var = ['PS60', 'DV60', 'RD60']
    x = np.array([db.by_col(name) for name in x_var]).T
    yd_var = ['UE60']
    yd = np.array([db.by_col(name) for name in yd_var]).T
    q_var = ['FP59', 'MA60']
    q = np.array([db.by_col(name) for name in q_var]).T
    r_var = 'SOUTH'
    regimes = db.by_col(r_var)
    tslsr = TSLS_Regimes(
        y, x, yd, q, regimes, constant_regi='many', spat_diag=False, name_y=y_var, name_x=x_var,
        name_yend=yd_var, name_q=q_var, name_regimes=r_var, cols2regi=[
            False, True, True, True],
        sig2n_k=False)
    print tslsr.summary

########NEW FILE########
__FILENAME__ = twosls_sp
'''
Spatial Two Stages Least Squares
'''

__author__ = "Luc Anselin luc.anselin@asu.edu, David C. Folch david.folch@asu.edu"

import copy
import numpy as np
import pysal
import numpy.linalg as la
import twosls as TSLS
import robust as ROBUST
import user_output as USER
import summary_output as SUMMARY
from utils import get_lags, set_endog, sp_att, set_warn

__all__ = ["GM_Lag"]


class BaseGM_Lag(TSLS.BaseTSLS):

    """
    Spatial two stage least squares (S2SLS) (note: no consistency checks,
    diagnostics or constant added); Anselin (1988) [1]_

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note: 
                   this should not contain any variables from x); cannot be
                   used in combination with h
    w            : Sparse matrix
                   Spatial weights sparse matrix 
    w_lags       : integer
                   Orders of W to include as instruments for the spatially
                   lagged dependent variable. For example, w_lags=1, then
                   instruments are WX; if w_lags=2, then WX, WWX; and so on.
    lag_q        : boolean
                   If True, then include spatial lags of the additional 
                   instruments (q).
    robust       : string
                   If 'white', then a White consistent estimator of the
                   variance-covariance matrix is given.  If 'hac', then a
                   HAC consistent estimator of the variance-covariance
                   matrix is given. Default set to None. 
    gwk          : pysal W object
                   Kernel spatial weights needed for HAC estimation. Note:
                   matrix must have ones along the main diagonal.
    sig2n_k      : boolean
                   If True, then use n-k to estimate sigma^2. If False, use n.


    Attributes
    ----------
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    predy        : array
                   nx1 array of predicted y values
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    kstar        : integer
                   Number of endogenous variables. 
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable used as instruments 
    z            : array
                   nxk array of variables (combination of x and yend)
    h            : array
                   nxl array of instruments (combination of x and q)
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    utu          : float
                   Sum of squared residuals
    sig2         : float
                   Sigma squared used in computations
    sig2n        : float
                   Sigma squared (computed with n in the denominator)
    sig2n_k      : float
                   Sigma squared (computed with n-k in the denominator)
    hth          : float
                   H'H
    hthi         : float
                   (H'H)^-1
    varb         : array
                   (Z'H (H'H)^-1 H'Z)^-1
    zthhthi      : array
                   Z'H(H'H)^-1
    pfora1a2     : array
                   n(zthhthi)'varb

    References
    ----------

    .. [1] Anselin, L. (1988) "Spatial Econometrics: Methods and Models".
    Kluwer Academic Publishers. Dordrecht.

    Examples
    --------

    >>> import numpy as np
    >>> import pysal
    >>> import pysal.spreg.diagnostics as D
    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
    >>> w.transform = 'r'
    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),'r')
    >>> y = np.array(db.by_col("HOVAL"))
    >>> y = np.reshape(y, (49,1))
    >>> # no non-spatial endogenous variables
    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("CRIME"))
    >>> X = np.array(X).T
    >>> w_lags = 2
    >>> yd2, q2 = pysal.spreg.utils.set_endog(y, X, w, None, None, w_lags, True)
    >>> X = np.hstack((np.ones(y.shape),X))
    >>> reg=BaseGM_Lag(y, X, yend=yd2, q=q2, w=w.sparse, w_lags=w_lags)
    >>> reg.betas
    array([[ 45.30170561],
           [  0.62088862],
           [ -0.48072345],
           [  0.02836221]])
    >>> D.se_betas(reg)
    array([ 17.91278862,   0.52486082,   0.1822815 ,   0.31740089])
    >>> reg=BaseGM_Lag(y, X, yend=yd2, q=q2, w=w.sparse, w_lags=w_lags, robust='white')
    >>> reg.betas
    array([[ 45.30170561],
           [  0.62088862],
           [ -0.48072345],
           [  0.02836221]])
    >>> D.se_betas(reg)
    array([ 20.47077481,   0.50613931,   0.20138425,   0.38028295])
    >>> # instrument for HOVAL with DISCBD
    >>> X = np.array(db.by_col("INC"))
    >>> X = np.reshape(X, (49,1))
    >>> yd = np.array(db.by_col("CRIME"))
    >>> yd = np.reshape(yd, (49,1))
    >>> q = np.array(db.by_col("DISCBD"))
    >>> q = np.reshape(q, (49,1))
    >>> yd2, q2 = pysal.spreg.utils.set_endog(y, X, w, yd, q, w_lags, True)
    >>> X = np.hstack((np.ones(y.shape),X))
    >>> reg=BaseGM_Lag(y, X, w=w.sparse, yend=yd2, q=q2, w_lags=w_lags)
    >>> reg.betas
    array([[ 100.79359082],
           [  -0.50215501],
           [  -1.14881711],
           [  -0.38235022]])
    >>> D.se_betas(reg)
    array([ 53.0829123 ,   1.02511494,   0.57589064,   0.59891744])

    """

    def __init__(self, y, x, yend=None, q=None,
                 w=None, w_lags=1, lag_q=True,
                 robust=None, gwk=None, sig2n_k=False):

        TSLS.BaseTSLS.__init__(self, y=y, x=x, yend=yend, q=q,
                               robust=robust, gwk=gwk, sig2n_k=sig2n_k)


class GM_Lag(BaseGM_Lag):

    """
    Spatial two stage least squares (S2SLS) with results and diagnostics; 
    Anselin (1988) [1]_

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note: 
                   this should not contain any variables from x); cannot be
                   used in combination with h
    w            : pysal W object
                   Spatial weights object 
    w_lags       : integer
                   Orders of W to include as instruments for the spatially
                   lagged dependent variable. For example, w_lags=1, then
                   instruments are WX; if w_lags=2, then WX, WWX; and so on.
    lag_q        : boolean
                   If True, then include spatial lags of the additional 
                   instruments (q).
    robust       : string
                   If 'white', then a White consistent estimator of the
                   variance-covariance matrix is given.  If 'hac', then a
                   HAC consistent estimator of the variance-covariance
                   matrix is given. Default set to None. 
    gwk          : pysal W object
                   Kernel spatial weights needed for HAC estimation. Note:
                   matrix must have ones along the main diagonal.
    sig2n_k      : boolean
                   If True, then use n-k to estimate sigma^2. If False, use n.
    spat_diag    : boolean
                   If True, then compute Anselin-Kelejian test
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_yend    : list of strings
                   Names of endogenous variables for use in output
    name_q       : list of strings
                   Names of instruments for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_gwk     : string
                   Name of kernel weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_pred       : array
                   nx1 array of residuals (using reduced form)
    predy        : array
                   nx1 array of predicted y values
    predy_e      : array
                   nx1 array of predicted y values (using reduced form)
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
    kstar        : integer
                   Number of endogenous variables. 
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable used as instruments 
    z            : array
                   nxk array of variables (combination of x and yend)
    h            : array
                   nxl array of instruments (combination of x and q)
    robust       : string
                   Adjustment for robust standard errors
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
    pr2_e        : float
                   Pseudo R squared (squared correlation between y and ypred_e
                   (using reduced form))
    utu          : float
                   Sum of squared residuals
    sig2         : float
                   Sigma squared used in computations
    std_err      : array
                   1xk array of standard errors of the betas    
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
    ak_test      : tuple
                   Anselin-Kelejian test; tuple contains the pair (statistic,
                   p-value)
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_yend    : list of strings
                   Names of endogenous variables for use in output
    name_z       : list of strings
                   Names of exogenous and endogenous variables for use in 
                   output
    name_q       : list of strings
                   Names of external instruments
    name_h       : list of strings
                   Names of all instruments used in ouput
    name_w       : string
                   Name of weights matrix for use in output
    name_gwk     : string
                   Name of kernel weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    title        : string
                   Name of the regression method used
    sig2n        : float
                   Sigma squared (computed with n in the denominator)
    sig2n_k      : float
                   Sigma squared (computed with n-k in the denominator)
    hth          : float
                   H'H
    hthi         : float
                   (H'H)^-1
    varb         : array
                   (Z'H (H'H)^-1 H'Z)^-1
    zthhthi      : array
                   Z'H(H'H)^-1
    pfora1a2     : array
                   n(zthhthi)'varb


    References
    ----------

    .. [1] Anselin, L. (1988) "Spatial Econometrics: Methods and Models".
    Kluwer Academic Publishers. Dordrecht.

    
    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis. Since we will need some tests for our
    model, we also import the diagnostics module.

    >>> import numpy as np
    >>> import pysal
    >>> import pysal.spreg.diagnostics as D

    Open data on Columbus neighborhood crime (49 areas) using pysal.open().
    This is the DBF associated with the Columbus shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> db = pysal.open(pysal.examples.get_path("columbus.dbf"),'r')
    
    Extract the HOVAL column (home value) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y = np.array(db.by_col("HOVAL"))
    >>> y = np.reshape(y, (49,1))

    Extract INC (income) and CRIME (crime rates) vectors from the DBF to be used as
    independent variables in the regression.  Note that PySAL requires this to
    be an nxj numpy array, where j is the number of independent variables (not
    including a constant). By default this model adds a vector of ones to the
    independent variables passed in, but this can be overridden by passing
    constant=False.

    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("CRIME"))
    >>> X = np.array(X).T

    Since we want to run a spatial error model, we need to specify the spatial
    weights matrix that includes the spatial configuration of the observations
    into the error component of the model. To do that, we can open an already
    existing gal file or create a new one. In this case, we will create one
    from ``columbus.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))

    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, this allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'
    
    This class runs a lag model, which means that includes the spatial lag of
    the dependent variable on the right-hand side of the equation. If we want
    to have the names of the variables printed in the
    output summary, we will have to pass them in as well, although this is
    optional. The default most basic model to be run would be: 

    >>> reg=GM_Lag(y, X, w=w, w_lags=2, name_x=['inc', 'crime'], name_y='hoval', name_ds='columbus')
    >>> reg.betas
    array([[ 45.30170561],
           [  0.62088862],
           [ -0.48072345],
           [  0.02836221]])

    Once the model is run, we can obtain the standard error of the coefficient
    estimates by calling the diagnostics module:

    >>> D.se_betas(reg)
    array([ 17.91278862,   0.52486082,   0.1822815 ,   0.31740089])

    But we can also run models that incorporates corrected standard errors
    following the White procedure. For that, we will have to include the
    optional parameter ``robust='white'``:

    >>> reg=GM_Lag(y, X, w=w, w_lags=2, robust='white', name_x=['inc', 'crime'], name_y='hoval', name_ds='columbus')
    >>> reg.betas
    array([[ 45.30170561],
           [  0.62088862],
           [ -0.48072345],
           [  0.02836221]])

    And we can access the standard errors from the model object:

    >>> reg.std_err
    array([ 20.47077481,   0.50613931,   0.20138425,   0.38028295])

    The class is flexible enough to accomodate a spatial lag model that,
    besides the spatial lag of the dependent variable, includes other
    non-spatial endogenous regressors. As an example, we will assume that
    CRIME is actually endogenous and we decide to instrument for it with
    DISCBD (distance to the CBD). We reload the X including INC only and
    define CRIME as endogenous and DISCBD as instrument:

    >>> X = np.array(db.by_col("INC"))
    >>> X = np.reshape(X, (49,1))
    >>> yd = np.array(db.by_col("CRIME"))
    >>> yd = np.reshape(yd, (49,1))
    >>> q = np.array(db.by_col("DISCBD"))
    >>> q = np.reshape(q, (49,1))

    And we can run the model again:

    >>> reg=GM_Lag(y, X, w=w, yend=yd, q=q, w_lags=2, name_x=['inc'], name_y='hoval', name_yend=['crime'], name_q=['discbd'], name_ds='columbus')
    >>> reg.betas
    array([[ 100.79359082],
           [  -0.50215501],
           [  -1.14881711],
           [  -0.38235022]])

    Once the model is run, we can obtain the standard error of the coefficient
    estimates by calling the diagnostics module:

    >>> D.se_betas(reg)
    array([ 53.0829123 ,   1.02511494,   0.57589064,   0.59891744])

    """

    def __init__(self, y, x, yend=None, q=None,
                 w=None, w_lags=1, lag_q=True,
                 robust=None, gwk=None, sig2n_k=False,
                 spat_diag=False,
                 vm=False, name_y=None, name_x=None,
                 name_yend=None, name_q=None,
                 name_w=None, name_gwk=None, name_ds=None):

        n = USER.check_arrays(x, yend, q)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        USER.check_robust(robust, gwk)
        yend2, q2 = set_endog(y, x, w, yend, q, w_lags, lag_q)
        x_constant = USER.check_constant(x)
        BaseGM_Lag.__init__(
            self, y=y, x=x_constant, w=w.sparse, yend=yend2, q=q2,
            w_lags=w_lags, robust=robust, gwk=gwk,
            lag_q=lag_q, sig2n_k=sig2n_k)
        self.rho = self.betas[-1]
        self.predy_e, self.e_pred, warn = sp_att(w, self.y, self.predy,
                                                 yend2[:, -1].reshape(self.n, 1), self.rho)
        set_warn(self, warn)
        self.title = "SPATIAL TWO STAGE LEAST SQUARES"
        self.name_ds = USER.set_name_ds(name_ds)
        self.name_y = USER.set_name_y(name_y)
        self.name_x = USER.set_name_x(name_x, x)
        self.name_yend = USER.set_name_yend(name_yend, yend)
        self.name_yend.append(USER.set_name_yend_sp(self.name_y))
        self.name_z = self.name_x + self.name_yend
        self.name_q = USER.set_name_q(name_q, q)
        self.name_q.extend(
            USER.set_name_q_sp(self.name_x, w_lags, self.name_q, lag_q))
        self.name_h = USER.set_name_h(self.name_x, self.name_q)
        self.robust = USER.set_robust(robust)
        self.name_w = USER.set_name_w(name_w, w)
        self.name_gwk = USER.set_name_w(name_gwk, gwk)
        SUMMARY.GM_Lag(reg=self, w=w, vm=vm, spat_diag=spat_diag)


def _test():
    import doctest
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)


if __name__ == '__main__':
    _test()

    import numpy as np
    import pysal
    db = pysal.open(pysal.examples.get_path("columbus.dbf"), 'r')
    y_var = 'CRIME'
    y = np.array([db.by_col(y_var)]).reshape(49, 1)
    x_var = ['INC']
    x = np.array([db.by_col(name) for name in x_var]).T
    yd_var = ['HOVAL']
    yd = np.array([db.by_col(name) for name in yd_var]).T
    q_var = ['DISCBD']
    q = np.array([db.by_col(name) for name in q_var]).T
    w = pysal.rook_from_shapefile(pysal.examples.get_path("columbus.shp"))
    w.transform = 'r'
    model = GM_Lag(
        y, x, yd, q, w=w, spat_diag=True, name_y=y_var, name_x=x_var,
        name_yend=yd_var, name_q=q_var, name_ds='columbus', name_w='columbus.gal')
    print model.summary

########NEW FILE########
__FILENAME__ = twosls_sp_regimes
'''
Spatial Two Stages Least Squares with Regimes
'''

__author__ = "Luc Anselin luc.anselin@asu.edu, Pedro V. Amaral pedro.amaral@asu.edu, David C. Folch david.folch@asu.edu"

import numpy as np
import pysal
import regimes as REGI
import user_output as USER
import summary_output as SUMMARY
import multiprocessing as mp
from twosls_regimes import TSLS_Regimes, _optimal_weight
from twosls import BaseTSLS
from utils import set_endog, set_endog_sparse, sp_att, set_warn, sphstack, spdot
from robust import hac_multi
from platform import system


class GM_Lag_Regimes(TSLS_Regimes, REGI.Regimes_Frame):

    """
    Spatial two stage least squares (S2SLS) with regimes; 
    Anselin (1988) [1]_

    Parameters
    ----------
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, excluding the constant
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable to use as instruments (note: 
                   this should not contain any variables from x); cannot be
                   used in combination with h
    constant_regi: ['one', 'many']
                   Switcher controlling the constant term setup. It may take
                   the following values:
                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime (default)
    cols2regi    : list, 'all'
                   Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all' (default), all the variables vary by regime.
    w            : pysal W object
                   Spatial weights object 
    w_lags       : integer
                   Orders of W to include as instruments for the spatially
                   lagged dependent variable. For example, w_lags=1, then
                   instruments are WX; if w_lags=2, then WX, WWX; and so on.
    lag_q        : boolean
                   If True, then include spatial lags of the additional 
                   instruments (q).
    regime_lag_sep: boolean
                   If True (default), the spatial parameter for spatial lag is also
                   computed according to different regimes. If False, 
                   the spatial parameter is fixed accross regimes.
                   Option valid only when regime_err_sep=True
    regime_err_sep: boolean
                   If True, a separate regression is run for each regime.
    robust       : string
                   If 'white', then a White consistent estimator of the
                   variance-covariance matrix is given.
                   If 'hac', then a HAC consistent estimator of the 
                   variance-covariance matrix is given.
                   If 'ogmm', then Optimal GMM is used to estimate
                   betas and the variance-covariance matrix.
                   Default set to None. 
    gwk          : pysal W object
                   Kernel spatial weights needed for HAC estimation. Note:
                   matrix must have ones along the main diagonal.
    sig2n_k      : boolean
                   If True, then use n-k to estimate sigma^2. If False, use n.
    spat_diag    : boolean
                   If True, then compute Anselin-Kelejian test
    vm           : boolean
                   If True, include variance-covariance matrix in summary
                   results
    cores        : integer
                   Specifies the number of cores to be used in multiprocessing
                   Default: all cores available (specified as cores=None).
                   Note: Multiprocessing currently not available on Windows.
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_yend    : list of strings
                   Names of endogenous variables for use in output
    name_q       : list of strings
                   Names of instruments for use in output
    name_w       : string
                   Name of weights matrix for use in output
    name_gwk     : string
                   Name of kernel weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    name_regimes : string
                   Name of regimes variable for use in output

    Attributes
    ----------
    summary      : string
                   Summary of regression results and diagnostics (note: use in
                   conjunction with the print command)
    betas        : array
                   kx1 array of estimated coefficients
    u            : array
                   nx1 array of residuals
    e_pred       : array
                   nx1 array of residuals (using reduced form)
    predy        : array
                   nx1 array of predicted y values
    predy_e      : array
                   nx1 array of predicted y values (using reduced form)
    n            : integer
                   Number of observations
    k            : integer
                   Number of variables for which coefficients are estimated
                   (including the constant)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    kstar        : integer
                   Number of endogenous variables. 
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    y            : array
                   nx1 array for dependent variable
    x            : array
                   Two dimensional array with n rows and one column for each
                   independent (exogenous) variable, including the constant
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    yend         : array
                   Two dimensional array with n rows and one column for each
                   endogenous variable
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    q            : array
                   Two dimensional array with n rows and one column for each
                   external exogenous variable used as instruments 
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    z            : array
                   nxk array of variables (combination of x and yend)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    h            : array
                   nxl array of instruments (combination of x and q)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    robust       : string
                   Adjustment for robust standard errors
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    mean_y       : float
                   Mean of dependent variable
    std_y        : float
                   Standard deviation of dependent variable
    vm           : array
                   Variance covariance matrix (kxk)
    pr2          : float
                   Pseudo R squared (squared correlation between y and ypred)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    pr2_e        : float
                   Pseudo R squared (squared correlation between y and ypred_e
                   (using reduced form))
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    utu          : float
                   Sum of squared residuals
    sig2         : float
                   Sigma squared used in computations
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    std_err      : array
                   1xk array of standard errors of the betas    
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    z_stat       : list of tuples
                   z statistic; each tuple contains the pair (statistic,
                   p-value), where each is a float
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    ak_test      : tuple
                   Anselin-Kelejian test; tuple contains the pair (statistic,
                   p-value)
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    name_y       : string
                   Name of dependent variable for use in output
    name_x       : list of strings
                   Names of independent variables for use in output
    name_yend    : list of strings
                   Names of endogenous variables for use in output
    name_z       : list of strings
                   Names of exogenous and endogenous variables for use in 
                   output
    name_q       : list of strings
                   Names of external instruments
    name_h       : list of strings
                   Names of all instruments used in ouput
    name_w       : string
                   Name of weights matrix for use in output
    name_gwk     : string
                   Name of kernel weights matrix for use in output
    name_ds      : string
                   Name of dataset for use in output
    name_regimes : string
                   Name of regimes variable for use in output
    title        : string
                   Name of the regression method used
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    sig2n        : float
                   Sigma squared (computed with n in the denominator)
    sig2n_k      : float
                   Sigma squared (computed with n-k in the denominator)
    hth          : float
                   H'H
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    hthi         : float
                   (H'H)^-1
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    varb         : array
                   (Z'H (H'H)^-1 H'Z)^-1
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    zthhthi      : array
                   Z'H(H'H)^-1
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    pfora1a2     : array
                   n(zthhthi)'varb
                   Only available in dictionary 'multi' when multiple regressions
                   (see 'multi' below for details)
    regimes      : list
                   List of n values with the mapping of each
                   observation to a regime. Assumed to be aligned with 'x'.
    constant_regi: ['one', 'many']
                   Ignored if regimes=False. Constant option for regimes.
                   Switcher controlling the constant term setup. It may take
                   the following values:
                     *  'one': a vector of ones is appended to x and held
                               constant across regimes
                     * 'many': a vector of ones is appended to x and considered
                               different per regime
    cols2regi    : list, 'all'
                   Ignored if regimes=False. Argument indicating whether each
                   column of x should be considered as different per regime
                   or held constant across regimes (False).
                   If a list, k booleans indicating for each variable the
                   option (True if one per regime, False to be held constant).
                   If 'all', all the variables vary by regime.
    regime_lag_sep   : boolean
                   If True, the spatial parameter for spatial lag is also
                   computed according to different regimes. If False (default), 
                   the spatial parameter is fixed accross regimes.
    regime_err_sep  : boolean
                   If True, a separate regression is run for each regime.
    kr           : int
                   Number of variables/columns to be "regimized" or subject
                   to change by regime. These will result in one parameter
                   estimate by regime for each variable (i.e. nr parameters per
                   variable)
    kf           : int
                   Number of variables/columns to be considered fixed or
                   global across regimes and hence only obtain one parameter
                   estimate
    nr           : int
                   Number of different regimes in the 'regimes' list
    multi        : dictionary
                   Only available when multiple regressions are estimated,
                   i.e. when regime_err_sep=True and no variable is fixed
                   across regimes.
                   Contains all attributes of each individual regression

    References
    ----------

    .. [1] Anselin, L. (1988) "Spatial Econometrics: Methods and Models".
    Kluwer Academic Publishers. Dordrecht.

    Examples
    --------

    We first need to import the needed modules, namely numpy to convert the
    data we read into arrays that ``spreg`` understands and ``pysal`` to
    perform all the analysis.

    >>> import numpy as np
    >>> import pysal

    Open data on NCOVR US County Homicides (3085 areas) using pysal.open().
    This is the DBF associated with the NAT shapefile.  Note that
    pysal.open() also reads data in CSV format; since the actual class
    requires data to be passed in as numpy arrays, the user can read their
    data in using any method.  

    >>> db = pysal.open(pysal.examples.get_path("NAT.dbf"),'r')
    
    Extract the HR90 column (homicide rates in 1990) from the DBF file and make it the
    dependent variable for the regression. Note that PySAL requires this to be
    an numpy array of shape (n, 1) as opposed to the also common shape of (n, )
    that other packages accept.

    >>> y_var = 'HR90'
    >>> y = np.array([db.by_col(y_var)]).reshape(3085,1)
    
    Extract UE90 (unemployment rate) and PS90 (population structure) vectors from
    the DBF to be used as independent variables in the regression. Other variables
    can be inserted by adding their names to x_var, such as x_var = ['Var1','Var2','...]
    Note that PySAL requires this to be an nxj numpy array, where j is the
    number of independent variables (not including a constant). By default
    this model adds a vector of ones to the independent variables passed in.

    >>> x_var = ['PS90','UE90']
    >>> x = np.array([db.by_col(name) for name in x_var]).T

    The different regimes in this data are given according to the North and 
    South dummy (SOUTH).

    >>> r_var = 'SOUTH'
    >>> regimes = db.by_col(r_var)

    Since we want to run a spatial lag model, we need to specify
    the spatial weights matrix that includes the spatial configuration of the
    observations. To do that, we can open an already existing gal file or 
    create a new one. In this case, we will create one from ``NAT.shp``.

    >>> w = pysal.rook_from_shapefile(pysal.examples.get_path("NAT.shp"))

    Unless there is a good reason not to do it, the weights have to be
    row-standardized so every row of the matrix sums to one. Among other
    things, this allows to interpret the spatial lag of a variable as the
    average value of the neighboring observations. In PySAL, this can be
    easily performed in the following way:

    >>> w.transform = 'r'
    
    This class runs a lag model, which means that includes the spatial lag of
    the dependent variable on the right-hand side of the equation. If we want
    to have the names of the variables printed in the output summary, we will
    have to pass them in as well, although this is optional.

    >>> model=GM_Lag_Regimes(y, x, regimes, w=w, regime_lag_sep=False, regime_err_sep=False, name_y=y_var, name_x=x_var, name_regimes=r_var, name_ds='NAT', name_w='NAT.shp')
    >>> model.betas
    array([[ 1.28897623],
           [ 0.79777722],
           [ 0.56366891],
           [ 8.73327838],
           [ 1.30433406],
           [ 0.62418643],
           [-0.39993716]])

    Once the model is run, we can have a summary of the output by typing:
    model.summary . Alternatively, we can obtain the standard error of 
    the coefficient estimates by calling:

    >>> model.std_err
    array([ 0.44682888,  0.14358192,  0.05655124,  1.06044865,  0.20184548,
            0.06118262,  0.12387232])

    In the example above, all coefficients but the spatial lag vary
    according to the regime. It is also possible to have the spatial lag
    varying according to the regime, which effective will result in an
    independent spatial lag model estimated for each regime. To run these
    models, the argument regime_lag_sep must be set to True:

    >>> model=GM_Lag_Regimes(y, x, regimes, w=w, regime_lag_sep=True, name_y=y_var, name_x=x_var, name_regimes=r_var, name_ds='NAT', name_w='NAT.shp')
    >>> print np.hstack((np.array(model.name_z).reshape(8,1),model.betas,np.sqrt(model.vm.diagonal().reshape(8,1))))
    [['0_CONSTANT' '1.36584769' '0.39854720']
     ['0_PS90' '0.80875730' '0.11324884']
     ['0_UE90' '0.56946813' '0.04625087']
     ['0_W_HR90' '-0.4342438' '0.13350159']
     ['1_CONSTANT' '7.90731073' '1.63601874']
     ['1_PS90' '1.27465703' '0.24709870']
     ['1_UE90' '0.60167693' '0.07993322']
     ['1_W_HR90' '-0.2960338' '0.19934459']]

    Alternatively, we can type: 'model.summary' to see the organized results output.
    The class is flexible enough to accomodate a spatial lag model that,
    besides the spatial lag of the dependent variable, includes other
    non-spatial endogenous regressors. As an example, we will add the endogenous
    variable RD90 (resource deprivation) and we decide to instrument for it with
    FP89 (families below poverty):

    >>> yd_var = ['RD90']
    >>> yd = np.array([db.by_col(name) for name in yd_var]).T
    >>> q_var = ['FP89']
    >>> q = np.array([db.by_col(name) for name in q_var]).T

    And we can run the model again:

    >>> model = GM_Lag_Regimes(y, x, regimes, yend=yd, q=q, w=w, regime_lag_sep=False, regime_err_sep=False, name_y=y_var, name_x=x_var, name_yend=yd_var, name_q=q_var, name_regimes=r_var, name_ds='NAT', name_w='NAT.shp')
    >>> model.betas
    array([[ 3.42195202],
           [ 1.03311878],
           [ 0.14308741],
           [ 8.99740066],
           [ 1.91877758],
           [-0.32084816],
           [ 2.38918212],
           [ 3.67243761],
           [ 0.06959139]])

    Once the model is run, we can obtain the standard error of the coefficient
    estimates. Alternatively, we can have a summary of the output by typing:
    model.summary

    >>> model.std_err
    array([ 0.49163311,  0.12237382,  0.05633464,  0.72555909,  0.17250521,
            0.06749131,  0.27370369,  0.25106224,  0.05804213])
    """

    def __init__(self, y, x, regimes, yend=None, q=None,
                 w=None, w_lags=1, lag_q=True,
                 robust=None, gwk=None, sig2n_k=False,
                 spat_diag=False, constant_regi='many',
                 cols2regi='all', regime_lag_sep=False, regime_err_sep=True,
                 cores=None, vm=False, name_y=None, name_x=None,
                 name_yend=None, name_q=None, name_regimes=None,
                 name_w=None, name_gwk=None, name_ds=None):

        n = USER.check_arrays(y, x)
        USER.check_y(y, n)
        USER.check_weights(w, y, w_required=True)
        USER.check_robust(robust, gwk)
        USER.check_spat_diag(spat_diag, w)
        name_x = USER.set_name_x(name_x, x, constant=True)
        name_y = USER.set_name_y(name_y)
        name_yend = USER.set_name_yend(name_yend, yend)
        name_q = USER.set_name_q(name_q, q)
        name_q.extend(USER.set_name_q_sp(name_x, w_lags, name_q, lag_q,
                      force_all=True))
        self.name_regimes = USER.set_name_ds(name_regimes)
        self.constant_regi = constant_regi
        self.n = n
        cols2regi = REGI.check_cols2regi(
            constant_regi, cols2regi, x, yend=yend, add_cons=False)
        self.cols2regi = cols2regi
        self.regimes_set = REGI._get_regimes_set(regimes)
        self.regimes = regimes
        USER.check_regimes(self.regimes_set, self.n, x.shape[1])
        if regime_err_sep == True and robust == 'hac':
            set_warn(
                self, "Error by regimes is incompatible with HAC estimation for Spatial Lag models. Hence, error and lag by regimes have been disabled for this model.")
            regime_err_sep = False
            regime_lag_sep = False
        self.regime_err_sep = regime_err_sep
        self.regime_lag_sep = regime_lag_sep
        if regime_lag_sep == True:
            if not regime_err_sep:
                raise Exception, "regime_err_sep must be True when regime_lag_sep=True."
            cols2regi += [True]
            w_i, regi_ids, warn = REGI.w_regimes(
                w, regimes, self.regimes_set, transform=True, get_ids=True, min_n=len(cols2regi) + 1)
            set_warn(self, warn)

        else:
            cols2regi += [False]

        if regime_err_sep == True and set(cols2regi) == set([True]) and constant_regi == 'many':
            self.y = y
            self.GM_Lag_Regimes_Multi(y, x, w_i, w, regi_ids,
                                      yend=yend, q=q, w_lags=w_lags, lag_q=lag_q, cores=cores,
                                      robust=robust, gwk=gwk, sig2n_k=sig2n_k, cols2regi=cols2regi,
                                      spat_diag=spat_diag, vm=vm, name_y=name_y, name_x=name_x,
                                      name_yend=name_yend, name_q=name_q, name_regimes=self.name_regimes,
                                      name_w=name_w, name_gwk=name_gwk, name_ds=name_ds)
        else:
            if regime_lag_sep == True:
                w = REGI.w_regimes_union(w, w_i, self.regimes_set)
            yend2, q2 = set_endog(y, x, w, yend, q, w_lags, lag_q)
            name_yend.append(USER.set_name_yend_sp(name_y))
            TSLS_Regimes.__init__(self, y=y, x=x, yend=yend2, q=q2,
                                  regimes=regimes, w=w, robust=robust, gwk=gwk,
                                  sig2n_k=sig2n_k, spat_diag=spat_diag, vm=vm,
                                  constant_regi=constant_regi, cols2regi=cols2regi, regime_err_sep=regime_err_sep,
                                  name_y=name_y, name_x=name_x, name_yend=name_yend, name_q=name_q,
                                  name_regimes=name_regimes, name_w=name_w, name_gwk=name_gwk,
                                  name_ds=name_ds, summ=False)
            if regime_lag_sep:
                self.sp_att_reg(w_i, regi_ids, yend2[:, -1].reshape(self.n, 1))
            else:
                self.rho = self.betas[-1]
                self.predy_e, self.e_pred, warn = sp_att(w, self.y, self.predy,
                                                         yend2[:, -1].reshape(self.n, 1), self.rho)
                set_warn(self, warn)
            self.regime_lag_sep = regime_lag_sep
            self.title = "SPATIAL " + self.title
            SUMMARY.GM_Lag(reg=self, w=w, vm=vm,
                           spat_diag=spat_diag, regimes=True)

    def GM_Lag_Regimes_Multi(self, y, x, w_i, w, regi_ids, cores=None,
                             yend=None, q=None, w_lags=1, lag_q=True,
                             robust=None, gwk=None, sig2n_k=False, cols2regi='all',
                             spat_diag=False, vm=False, name_y=None, name_x=None,
                             name_yend=None, name_q=None, name_regimes=None,
                             name_w=None, name_gwk=None, name_ds=None):
        pool = mp.Pool(cores)
        self.name_ds = USER.set_name_ds(name_ds)
        name_x = USER.set_name_x(name_x, x)
        name_yend.append(USER.set_name_yend_sp(name_y))
        self.name_w = USER.set_name_w(name_w, w_i)
        self.name_gwk = USER.set_name_w(name_gwk, gwk)
        results_p = {}
        for r in self.regimes_set:
            w_r = w_i[r].sparse
            if system() == 'Windows':
                is_win = True
                results_p[r] = _work(
                    *(y, x, regi_ids, r, yend, q, w_r, w_lags, lag_q, robust, sig2n_k,
                      self.name_ds, name_y, name_x, name_yend, name_q, self.name_w, name_regimes))
            else:
                results_p[r] = pool.apply_async(
                    _work, args=(
                        y, x, regi_ids, r, yend, q, w_r, w_lags, lag_q,
                        robust, sig2n_k, self.name_ds, name_y, name_x, name_yend, name_q, self.name_w, name_regimes, ))
                is_win = False
        self.kryd = 0
        self.kr = len(cols2regi) + 1
        self.kf = 0
        self.nr = len(self.regimes_set)
        self.name_x_r = name_x + name_yend
        self.name_regimes = name_regimes
        self.vm = np.zeros((self.nr * self.kr, self.nr * self.kr), float)
        self.betas = np.zeros((self.nr * self.kr, 1), float)
        self.u = np.zeros((self.n, 1), float)
        self.predy = np.zeros((self.n, 1), float)
        self.predy_e = np.zeros((self.n, 1), float)
        self.e_pred = np.zeros((self.n, 1), float)
        if not is_win:
            pool.close()
            pool.join()
        results = {}
        self.name_y, self.name_x, self.name_yend, self.name_q, self.name_z, self.name_h = [
        ], [], [], [], [], []
        counter = 0
        for r in self.regimes_set:
            if is_win:
                results[r] = results_p[r]
            else:
                results[r] = results_p[r].get()
            results[r].predy_e, results[r].e_pred, warn = sp_att(
                w_i[r], results[r].y, results[r].predy, results[r].yend[:, -1].reshape(results[r].n, 1), results[r].rho)
            set_warn(results[r], warn)
            results[r].w = w_i[r]
            self.vm[(counter * self.kr):((counter + 1) * self.kr),
                    (counter * self.kr):((counter + 1) * self.kr)] = results[r].vm
            self.betas[(counter * self.kr):((counter + 1) * self.kr),
                       ] = results[r].betas
            self.u[regi_ids[r], ] = results[r].u
            self.predy[regi_ids[r], ] = results[r].predy
            self.predy_e[regi_ids[r], ] = results[r].predy_e
            self.e_pred[regi_ids[r], ] = results[r].e_pred
            self.name_y += results[r].name_y
            self.name_x += results[r].name_x
            self.name_yend += results[r].name_yend
            self.name_q += results[r].name_q
            self.name_z += results[r].name_z
            self.name_h += results[r].name_h
            if r == self.regimes_set[0]:
                self.hac_var = np.zeros((self.n, results[r].h.shape[1]), float)
            self.hac_var[regi_ids[r], ] = results[r].h
            counter += 1
        self.multi = results
        if robust == 'hac':
            hac_multi(self, gwk, constant=True)
        if robust == 'ogmm':
            set_warn(
                self, "Residuals treated as homoskedastic for the purpose of diagnostics.")
        self.chow = REGI.Chow(self)
        if spat_diag:
            pass
            #self._get_spat_diag_props(y, x, w, yend, q, w_lags, lag_q)
        SUMMARY.GM_Lag_multi(reg=self, multireg=self.multi,
                             vm=vm, spat_diag=spat_diag, regimes=True, w=w)

    def sp_att_reg(self, w_i, regi_ids, wy):
        predy_e_r, e_pred_r = {}, {}
        self.predy_e = np.zeros((self.n, 1), float)
        self.e_pred = np.zeros((self.n, 1), float)
        counter = 1
        for r in self.regimes_set:
            self.rho = self.betas[(self.kr - self.kryd) * self.nr + self.kf -
                                  (self.yend.shape[1] - self.nr * self.kryd) + self.kryd * counter - 1]
            self.predy_e[regi_ids[r], ], self.e_pred[regi_ids[r], ], warn = sp_att(
                w_i[r],
                self.y[regi_ids[r]], self.predy[
                    regi_ids[
                        r]],
                wy[regi_ids[r]], self.rho)
            counter += 1

    def _get_spat_diag_props(self, y, x, w, yend, q, w_lags, lag_q):
        self._cache = {}
        yend, q = set_endog(y, x, w, yend, q, w_lags, lag_q)
        x = USER.check_constant(x)
        x = REGI.regimeX_setup(
            x, self.regimes, [True] * x.shape[1], self.regimes_set)
        self.z = sphstack(x, REGI.regimeX_setup(
            yend, self.regimes, [True] * (yend.shape[1] - 1) + [False], self.regimes_set))
        self.h = sphstack(
            x, REGI.regimeX_setup(q, self.regimes, [True] * q.shape[1], self.regimes_set))
        hthi = np.linalg.inv(spdot(self.h.T, self.h))
        zth = spdot(self.z.T, self.h)
        self.varb = np.linalg.inv(spdot(spdot(zth, hthi), zth.T))


def _work(y, x, regi_ids, r, yend, q, w_r, w_lags, lag_q, robust, sig2n_k, name_ds, name_y, name_x, name_yend, name_q, name_w, name_regimes):
    y_r = y[regi_ids[r]]
    x_r = x[regi_ids[r]]
    if yend != None:
        yend_r = yend[regi_ids[r]]
    else:
        yend_r = yend
    if q != None:
        q_r = q[regi_ids[r]]
    else:
        q_r = q
    yend_r, q_r = set_endog_sparse(y_r, x_r, w_r, yend_r, q_r, w_lags, lag_q)
    x_constant = USER.check_constant(x_r)
    if robust == 'hac' or robust == 'ogmm':
        robust2 = None
    else:
        robust2 = robust
    model = BaseTSLS(y_r, x_constant, yend_r, q_r,
                     robust=robust2, sig2n_k=sig2n_k)
    model.title = "SPATIAL TWO STAGE LEAST SQUARES ESTIMATION - REGIME %s" % r
    if robust == 'ogmm':
        _optimal_weight(model, sig2n_k, warn=False)
    model.rho = model.betas[-1]
    model.robust = USER.set_robust(robust)
    model.name_ds = name_ds
    model.name_y = '%s_%s' % (str(r), name_y)
    model.name_x = ['%s_%s' % (str(r), i) for i in name_x]
    model.name_yend = ['%s_%s' % (str(r), i) for i in name_yend]
    model.name_z = model.name_x + model.name_yend
    model.name_q = ['%s_%s' % (str(r), i) for i in name_q]
    model.name_h = model.name_x + model.name_q
    model.name_w = name_w
    model.name_regimes = name_regimes
    return model


def _test():
    import doctest
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)


if __name__ == '__main__':
    _test()
    import numpy as np
    import pysal
    db = pysal.open(pysal.examples.get_path("columbus.dbf"), 'r')
    y_var = 'CRIME'
    y = np.array([db.by_col(y_var)]).reshape(49, 1)
    x_var = ['INC']
    x = np.array([db.by_col(name) for name in x_var]).T
    yd_var = ['HOVAL']
    yd = np.array([db.by_col(name) for name in yd_var]).T
    q_var = ['DISCBD']
    q = np.array([db.by_col(name) for name in q_var]).T
    r_var = 'NSA'
    regimes = db.by_col(r_var)
    w = pysal.queen_from_shapefile(pysal.examples.get_path("columbus.shp"))
    w.transform = 'r'
    model = GM_Lag_Regimes(
        y, x, regimes, yend=yd, q=q, w=w, constant_regi='many', spat_diag=True, sig2n_k=False, lag_q=True, name_y=y_var,
        name_x=x_var, name_yend=yd_var, name_q=q_var, name_regimes=r_var, name_ds='columbus', name_w='columbus.gal', regime_err_sep=True, robust='white')
    print model.summary

########NEW FILE########
__FILENAME__ = user_output
"""Internal helper files for user output."""

__author__ = "Luc Anselin luc.anselin@asu.edu, David C. Folch david.folch@asu.edu, Jing Yao jingyao@asu.edu"
import textwrap as TW
import numpy as np
import copy as COPY
import diagnostics as diagnostics
import diagnostics_tsls as diagnostics_tsls
import diagnostics_sp as diagnostics_sp
import pysal
import scipy
from scipy.sparse.csr import csr_matrix
from utils import spdot, sphstack

__all__ = []


def set_name_ds(name_ds):
    """Set the dataset name in regression; return generic name if user
    provides no explicit name."

    Parameters
    ----------

    name_ds     : string
                  User provided dataset name.

    Returns
    -------
    
    name_ds     : string
                  
    """
    if not name_ds:
        name_ds = 'unknown'
    return name_ds


def set_name_y(name_y):
    """Set the dataset name in regression; return generic name if user
    provides no explicit name."

    Parameters
    ----------

    name_ds     : string
                  User provided dataset name.

    Returns
    -------
    
    name_ds     : string
                  
    """
    if not name_y:
        name_y = 'dep_var'
    return name_y


def set_name_x(name_x, x, constant=False):
    """Set the independent variable names in regression; return generic name if user
    provides no explicit name."

    Parameters
    ----------

    name_x      : list of string
                  User provided exogenous variable names.

    x           : array
                  User provided exogenous variables.
    constant    : boolean
                  If False (default), constant name not included in name_x list yet
                  Append 'CONSTANT' at the front of the names

    Returns
    -------
    
    name_x      : list of strings
                  
    """
    if not name_x:
        name_x = ['var_' + str(i + 1) for i in range(x.shape[1])]
    else:
        name_x = name_x[:]
    if not constant:
        name_x.insert(0, 'CONSTANT')
    return name_x


def set_name_yend(name_yend, yend):
    """Set the endogenous variable names in regression; return generic name if user
    provides no explicit name."

    Parameters
    ----------

    name_yend   : list of strings
                  User provided exogenous variable names.

    Returns
    -------
    
    name_yend   : list of strings
                  
    """
    if yend != None:
        if not name_yend:
            return ['endogenous_' + str(i + 1) for i in range(len(yend[0]))]
        else:
            return name_yend[:]
    else:
        return []


def set_name_q(name_q, q):
    """Set the external instrument names in regression; return generic name if user
    provides no explicit name."

    Parameters
    ----------

    name_q      : string
                  User provided instrument names.
    q           : array
                  Array of instruments

    Returns
    -------
    
    name_q      : list of strings
                  
    """
    if q != None:
        if not name_q:
            return ['instrument_' + str(i + 1) for i in range(len(q[0]))]
        else:
            return name_q[:]
    else:
        return []


def set_name_yend_sp(name_y):
    """Set the spatial lag name in regression; return generic name if user
    provides no explicit name."

    Parameters
    ----------

    name_y      : string
                  User provided dependent variable name.

    Returns
    -------
    
    name_yend_sp : string
                  
    """
    return 'W_' + name_y


def set_name_q_sp(name_x, w_lags, name_q, lag_q, force_all=False):
    """Set the spatial instrument names in regression; return generic name if user
    provides no explicit name."

    Parameters
    ----------

    name_x      : list of strings
                  User provided exogenous variable names.

    w_lags      : int
                  User provided number of spatial instruments lags

    Returns
    -------
    
    name_q_sp   : list of strings
                  
    """
    if force_all:
        names = name_x
    else:
        names = name_x[1:]  # drop the constant
    if lag_q:
        names = names + name_q
    sp_inst_names = []
    for j in names:
        sp_inst_names.append('W_' + j)
    if w_lags > 1:
        for i in range(2, w_lags + 1):
            for j in names:
                sp_inst_names.append('W' + str(i) + '_' + j)
    return sp_inst_names


def set_name_h(name_x, name_q):
    """Set the full instruments names in regression; return generic name if user
    provides no explicit name."

    Parameters
    ----------

    name_x      : list of strings
                  User provided exogenous variable names.
    name_q      : list of strings
                  User provided instrument variable names.

    Returns
    -------
    
    name_h      : list of strings
                  
    """
    return name_x + name_q


def set_robust(robust):
    """Return generic name if user passes None to the robust parameter in a
    regression. Note: already verified that the name is valid in
    check_robust() if the user passed anything besides None to robust.

    Parameters
    ----------

    robust      : string or None
                  Object passed by the user to a regression class

    Returns
    -------
    
    robust      : string
                  
    """
    if not robust:
        return 'unadjusted'
    return robust


def set_name_w(name_w, w):
    """Return generic name if user passes None to the robust parameter in a
    regression. Note: already verified that the name is valid in
    check_robust() if the user passed anything besides None to robust.

    Parameters
    ----------

    name_w      : string
                  Name passed in by user. Default is None.
    w           : W object
                  pysal W object passed in by user

    Returns
    -------
    
    name_w      : string
                  
    """
    if w != None:
        if name_w != None:
            return name_w
        else:
            return 'unknown'
    return None


def set_name_multi(multireg, multi_set, name_multiID, y, x, name_y, name_x, name_ds, title, name_w, robust, endog=False, sp_lag=False):
    """Returns multiple regression objects with generic names

    Parameters
    ----------

    endog       : tuple
                  If the regression object contains endogenous variables, endog must have the
                  following parameters in the following order: (yend, q, name_yend, name_q)
    sp_lag       : tuple
                  If the regression object contains spatial lag, sp_lag must have the
                  following parameters in the following order: (w_lags, lag_q)

    """
    name_ds = set_name_ds(name_ds)
    name_y = set_name_y(name_y)
    name_x = set_name_x(name_x, x)
    name_multiID = set_name_ds(name_multiID)
    if endog or sp_lag:
        name_yend = set_name_yend(endog[2], endog[0])
        name_q = set_name_q(endog[3], endog[1])
    for r in multi_set:
        multireg[r].title = title + "%s" % r
        multireg[r].name_ds = name_ds
        multireg[r].robust = set_robust(robust)
        multireg[r].name_w = name_w
        multireg[r].name_y = '%s_%s' % (str(r), name_y)
        multireg[r].name_x = ['%s_%s' % (str(r), i) for i in name_x]
        multireg[r].name_multiID = name_multiID
        if endog or sp_lag:
            multireg[r].name_yend = ['%s_%s' % (str(r), i) for i in name_yend]
            multireg[r].name_q = ['%s_%s' % (str(r), i) for i in name_q]
            if sp_lag:
                multireg[r].name_yend.append(
                    set_name_yend_sp(multireg[r].name_y))
                multireg[r].name_q.extend(
                    set_name_q_sp(multireg[r].name_x, sp_lag[0], multireg[r].name_q, sp_lag[1]))
            multireg[r].name_z = multireg[r].name_x + multireg[r].name_yend
            multireg[r].name_h = multireg[r].name_x + multireg[r].name_q
    return multireg


def check_arrays(*arrays):
    """Check if the objects passed by a user to a regression class are
    correctly structured. If the user's data is correctly formed this function
    returns nothing, if not then an exception is raised. Note, this does not 
    check for model setup, simply the shape and types of the objects.

    Parameters
    ----------

    *arrays : anything
              Objects passed by the user to a regression class; any type
              object can be passed and any number of objects can be passed
     
    Returns
    -------

    Returns : int
              number of observations

    Examples
    --------

    >>> import numpy as np
    >>> import pysal
    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    >>> # Extract CRIME column from the dbf file
    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))
    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T
    >>> n = check_arrays(y, X)
    >>> print n
    49

    """
    allowed = ['ndarray', 'csr_matrix']
    rows = []
    for i in arrays:
        if i == None:
            continue
        if i.__class__.__name__ not in allowed:
            raise Exception, "all input data must be either numpy arrays or sparse csr matrices"
        shape = i.shape
        if len(shape) > 2:
            raise Exception, "all input arrays must have exactly two dimensions"
        if len(shape) == 1:
            raise Exception, "all input arrays must have exactly two dimensions"
        if shape[0] < shape[1]:
            raise Exception, "one or more input arrays have more columns than rows"
        rows.append(shape[0])
    if len(set(rows)) > 1:
        raise Exception, "arrays not all of same length"
    return rows[0]


def check_y(y, n):
    """Check if the y object passed by a user to a regression class is
    correctly structured. If the user's data is correctly formed this function
    returns nothing, if not then an exception is raised. Note, this does not 
    check for model setup, simply the shape and types of the objects.

    Parameters
    ----------

    y       : anything
              Object passed by the user to a regression class; any type
              object can be passed

    n       : int
              number of observations
     
    Returns
    -------

    Returns : nothing
              Nothing is returned

    Examples
    --------

    >>> import numpy as np
    >>> import pysal
    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    >>> # Extract CRIME column from the dbf file
    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))
    >>> check_y(y, 49)
    >>> # should not raise an exception

    """
    if y.__class__.__name__ != 'ndarray':
        print y.__class__.__name__
        raise Exception, "y must be a numpy array"
    shape = y.shape
    if len(shape) > 2:
        raise Exception, "all input arrays must have exactly two dimensions"
    if len(shape) == 1:
        raise Exception, "all input arrays must have exactly two dimensions"
    if shape != (n, 1):
        raise Exception, "y must be a single column array matching the length of other arrays"


def check_weights(w, y, w_required=False):
    """Check if the w parameter passed by the user is a pysal.W object and
    check that its dimensionality matches the y parameter.  Note that this
    check is not performed if w set to None.

    Parameters
    ----------

    w       : any python object
              Object passed by the user to a regression class; any type
              object can be passed
    y       : numpy array
              Any shape numpy array can be passed. Note: if y passed
              check_arrays, then it will be valid for this function

    Returns
    -------

    Returns : nothing
              Nothing is returned
              
    Examples
    --------

    >>> import numpy as np
    >>> import pysal
    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    >>> # Extract CRIME column from the dbf file
    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))
    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T
    >>> w = pysal.open(pysal.examples.get_path("columbus.gal"), 'r').read()
    >>> check_weights(w, y)
    >>> # should not raise an exception

    """
    if w_required == True or w != None:
        if w == None:
            raise Exception, "A weights matrix w must be provided to run this method."
        if not isinstance(w, pysal.W):
            raise Exception, "w must be a pysal.W object"
        if w.n != y.shape[0]:
            raise Exception, "y must be nx1, and w must be an nxn PySAL W object"
        diag = w.sparse.diagonal()
        # check to make sure all entries equal 0
        if diag.min() != 0:
            raise Exception, "All entries on diagonal must equal 0."
        if diag.max() != 0:
            raise Exception, "All entries on diagonal must equal 0."


def check_robust(robust, wk):
    """Check if the combination of robust and wk parameters passed by the user
    are valid. Note: this does not check if the W object is a valid adaptive 
    kernel weights matrix needed for the HAC.

    Parameters
    ----------

    robust  : string or None
              Object passed by the user to a regression class
    w       : any python object
              Object passed by the user to a regression class; any type
              object can be passed

    Returns
    -------

    Returns : nothing
              Nothing is returned
              
    Examples
    --------

    >>> import numpy as np
    >>> import pysal
    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    >>> # Extract CRIME column from the dbf file
    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))
    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T
    >>> wk = None
    >>> check_robust('White', wk)
    >>> # should not raise an exception

    """
    if robust:
        if robust.lower() == 'hac':
            if type(wk).__name__ != 'W' and type(wk).__name__ != 'Kernel':
                raise Exception, "HAC requires that wk be a pysal.W object"
            diag = wk.sparse.diagonal()
            # check to make sure all entries equal 1
            if diag.min() < 1.0:
                print diag.min()
                raise Exception, "All entries on diagonal of kernel weights matrix must equal 1."
            if diag.max() > 1.0:
                print diag.max()
                raise Exception, "All entries on diagonal of kernel weights matrix must equal 1."
            # ensure off-diagonal entries are in the set of real numbers [0,1)
            wegt = wk.weights
            for i in wk.id_order:
                vals = wegt[i]
                vmin = min(vals)
                vmax = max(vals)
                if vmin < 0.0:
                    raise Exception, "Off-diagonal entries must be greater than or equal to 0."
                if vmax > 1.0:
                    # NOTE: we are not checking for the case of exactly 1.0 ###
                    raise Exception, "Off-diagonal entries must be less than 1."
        elif robust.lower() == 'white' or robust.lower() == 'ogmm':
            if wk:
                raise Exception, "White requires that wk be set to None"
        else:
            raise Exception, "invalid value passed to robust, see docs for valid options"


def check_spat_diag(spat_diag, w):
    """Check if there is a w parameter passed by the user if the user also
    requests spatial diagnostics.

    Parameters
    ----------

    spat_diag   : boolean
                  Value passed by a used to a regression class
    w           : any python object
                  Object passed by the user to a regression class; any type
                  object can be passed

    Returns
    -------

    Returns : nothing
              Nothing is returned
              
    Examples
    --------

    >>> import numpy as np
    >>> import pysal
    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    >>> # Extract CRIME column from the dbf file
    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))
    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T
    >>> w = pysal.open(pysal.examples.get_path("columbus.gal"), 'r').read()
    >>> check_spat_diag(True, w)
    >>> # should not raise an exception

    """
    if spat_diag:
        if type(w).__name__ != 'W':
            raise Exception, "w must be a pysal.W object to run spatial diagnostics"


def check_regimes(reg_set, N=None, K=None):
    """Check if there are at least two regimes

    Parameters
    ----------

    reg_set     : list
                  List of the regimes IDs

    Returns
    -------

    Returns : nothing
              Nothing is returned
              
    """
    if len(reg_set) < 2:
        raise Exception, "At least 2 regimes are needed to run regimes methods. Please check your regimes variable."
    if 1.0 * N / len(reg_set) < K + 1:
        raise Exception, "There aren't enough observations for the given number of regimes and variables. Please check your regimes variable."


def check_constant(x):
    """Check if the X matrix contains a constant, raise exception if it does
    not

    Parameters
    ----------

    x           : array
                  Value passed by a used to a regression class

    Returns
    -------

    Returns : nothing
              Nothing is returned
              
    Examples
    --------

    >>> import numpy as np
    >>> import pysal
    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    >>> X = []
    >>> X.append(db.by_col("INC"))
    >>> X.append(db.by_col("HOVAL"))
    >>> X = np.array(X).T
    >>> x_constant = check_constant(X)
    >>> x_constant.shape
    (49, 3)

    """
    if not diagnostics.constant_check:
        raise Exception, "x array cannot contain a constant vector; constant will be added automatically"
    else:
        x_constant = COPY.copy(x)
        return sphstack(np.ones((x_constant.shape[0], 1)), x_constant)


def _test():
    import doctest
    doctest.testmod()

if __name__ == '__main__':
    _test()

########NEW FILE########
__FILENAME__ = utils
"""
Tools for different procedure estimations
"""

__author__ = "Luc Anselin luc.anselin@asu.edu, \
        Pedro V. Amaral pedro.amaral@asu.edu, \
        David C. Folch david.folch@asu.edu, \
        Daniel Arribas-Bel darribas@asu.edu"

import numpy as np
from scipy import sparse as SP
import scipy.optimize as op
import numpy.linalg as la
from pysal import lag_spatial
import copy


class RegressionPropsY:

    """
    Helper class that adds common regression properties to any regression
    class that inherits it.  It takes no parameters.  See BaseOLS for example
    usage.

    Parameters
    ----------

    Attributes
    ----------
    mean_y  : float
              Mean of the dependent variable
    std_y   : float
              Standard deviation of the dependent variable
              
    """

    @property
    def mean_y(self):
        if 'mean_y' not in self._cache:
            self._cache['mean_y'] = np.mean(self.y)
        return self._cache['mean_y']

    @property
    def std_y(self):
        if 'std_y' not in self._cache:
            self._cache['std_y'] = np.std(self.y, ddof=1)
        return self._cache['std_y']


class RegressionPropsVM:

    """
    Helper class that adds common regression properties to any regression
    class that inherits it.  It takes no parameters.  See BaseOLS for example
    usage.

    Parameters
    ----------

    Attributes
    ----------
    utu     : float
              Sum of the squared residuals
    sig2n    : float
              Sigma squared with n in the denominator
    sig2n_k : float
              Sigma squared with n-k in the denominator
    vm      : array
              Variance-covariance matrix (kxk)
              
    """

    @property
    def utu(self):
        if 'utu' not in self._cache:
            self._cache['utu'] = np.sum(self.u ** 2)
        return self._cache['utu']

    @property
    def sig2n(self):
        if 'sig2n' not in self._cache:
            self._cache['sig2n'] = self.utu / self.n
        return self._cache['sig2n']

    @property
    def sig2n_k(self):
        if 'sig2n_k' not in self._cache:
            self._cache['sig2n_k'] = self.utu / (self.n - self.k)
        return self._cache['sig2n_k']

    @property
    def vm(self):
        if 'vm' not in self._cache:
            self._cache['vm'] = np.dot(self.sig2, self.xtxi)
        return self._cache['vm']


def get_A1_het(S):
    """
    Builds A1 as in Arraiz et al [1]_

    .. math::

        A_1 = W' W - diag(w'_{.i} w_{.i})

    ...

    Parameters
    ----------

    S               : csr_matrix
                      PySAL W object converted into Scipy sparse matrix

    Returns
    -------

    Implicit        : csr_matrix
                      A1 matrix in scipy sparse format

    References
    ----------

    .. [1] Arraiz, I., Drukker, D. M., Kelejian, H., Prucha, I. R. (2010) "A
    Spatial Cliff-Ord-Type Model with Heteroskedastic Innovations: Small and
    Large Sample Results". Journal of Regional Science, Vol. 60, No. 2, pp.
    592-614.
    """
    StS = S.T * S
    d = SP.spdiags([StS.diagonal()], [0], S.get_shape()[0], S.get_shape()[1])
    d = d.asformat('csr')
    return StS - d


def get_A1_hom(s, scalarKP=False):
    """
    Builds A1 for the spatial error GM estimation with homoscedasticity as in Drukker et al. [1]_ (p. 9).

    .. math::

        A_1 = \{1 + [n^{-1} tr(W'W)]^2\}^{-1} \[W'W - n^{-1} tr(W'W) I\]

    ...

    Parameters
    ----------

    s               : csr_matrix
                      PySAL W object converted into Scipy sparse matrix
    scalarKP        : boolean
                      Flag to include scalar corresponding to the first moment
                      condition as in Drukker et al. [1]_ (Defaults to False)

    Returns
    -------

    Implicit        : csr_matrix
                      A1 matrix in scipy sparse format
    References
    ----------

    .. [1] Drukker, Prucha, I. R., Raciborski, R. (2010) "A command for
    estimating spatial-autoregressive models with spatial-autoregressive
    disturbances and additional endogenous variables". The Stata Journal, 1,
    N. 1, pp. 1-13.      
    """
    n = float(s.shape[0])
    wpw = s.T * s
    twpw = np.sum(wpw.diagonal())
    e = SP.eye(n, n, format='csr')
    e.data = np.ones(n) * (twpw / n)
    num = wpw - e
    if not scalarKP:
        return num
    else:
        den = 1. + (twpw / n) ** 2.
        return num / den


def get_A2_hom(s):
    """
    Builds A2 for the spatial error GM estimation with homoscedasticity as in
    Anselin (2011) [1]_ 

    .. math::

        A_2 = \dfrac{(W + W')}{2}

    ...

    Parameters
    ----------
    s               : csr_matrix
                      PySAL W object converted into Scipy sparse matrix
    Returns
    -------
    Implicit        : csr_matrix
                      A2 matrix in scipy sparse format
    References
    ----------

    .. [1] Anselin (2011) "GMM Estimation of Spatial Error Autocorrelation with and without Heteroskedasticity".
    """
    return (s + s.T) / 2.


def _moments2eqs(A1, s, u):
    '''
    Helper to compute G and g in a system of two equations as in
    the heteroskedastic error models from Drukker et al. [1]_
    ...

    Parameters
    ----------

    A1          : scipy.sparse.csr
                  A1 matrix as in the paper, different deppending on whether
                  it's homocedastic or heteroskedastic model

    s           : W.sparse
                  Sparse representation of spatial weights instance

    u           : array
                  Residuals. nx1 array assumed to be aligned with w
 
    Attributes
    ----------

    moments     : list
                  List of two arrays corresponding to the matrices 'G' and
                  'g', respectively.


    References
    ----------

    .. [1] Drukker, Prucha, I. R., Raciborski, R. (2010) "A command for
    estimating spatial-autoregressive models with spatial-autoregressive
    disturbances and additional endogenous variables". The Stata Journal, 1,
    N. 1, pp. 1-13.
    '''
    n = float(s.shape[0])
    A1u = A1 * u
    wu = s * u
    g1 = np.dot(u.T, A1u)
    g2 = np.dot(u.T, wu)
    g = np.array([[g1][0][0], [g2][0][0]]) / n

    G11 = np.dot(u.T, ((A1 + A1.T) * wu))
    G12 = -np.dot((wu.T * A1), wu)
    G21 = np.dot(u.T, ((s + s.T) * wu))
    G22 = -np.dot(wu.T, (s * wu))
    G = np.array([[G11[0][0], G12[0][0]], [G21[0][0], G22[0][0]]]) / n
    return [G, g]


def optim_moments(moments_in, vcX=np.array([0])):
    """
    Optimization of moments
    ...

    Parameters
    ----------

    moments     : Moments
                  Instance of gmm_utils.moments_het with G and g
    vcX         : array
                  Optional. 2x2 array with the Variance-Covariance matrix to be used as
                  weights in the optimization (applies Cholesky
                  decomposition). Set empty by default.

    Returns
    -------
    x, f, d     : tuple
                  x -- position of the minimum
                  f -- value of func at the minimum
                  d -- dictionary of information from routine
                        d['warnflag'] is
                            0 if converged
                            1 if too many function evaluations
                            2 if stopped for another reason, given in d['task']
                        d['grad'] is the gradient at the minimum (should be 0 ish)
                        d['funcalls'] is the number of function calls made
    """
    moments = copy.deepcopy(moments_in)
    if vcX.any():
        Ec = np.transpose(la.cholesky(la.inv(vcX)))
        moments[0] = np.dot(Ec, moments_in[0])
        moments[1] = np.dot(Ec, moments_in[1])
    scale = np.min([[np.min(moments[0]), np.min(moments[1])]])
    moments[0], moments[1] = moments[0] / scale, moments[1] / scale
    if moments[0].shape[0] == 2:
        optim_par = lambda par: foptim_par(
            np.array([[float(par[0]), float(par[0]) ** 2.]]).T, moments)
        start = [0.0]
        bounds = [(-1.0, 1.0)]
    if moments[0].shape[0] == 3:
        optim_par = lambda par: foptim_par(
            np.array([[float(par[0]), float(par[0]) ** 2., float(par[1])]]).T, moments)
        start = [0.0, 0.0]
        bounds = [(-1.0, 1.0), (0.0, None)]
    lambdaX = op.fmin_l_bfgs_b(
        optim_par, start, approx_grad=True, bounds=bounds)
    return lambdaX[0][0]


def foptim_par(par, moments):
    """ 
    Preparation of the function of moments for minimization
    ...

    Parameters
    ----------

    lambdapar       : float
                      Spatial autoregressive parameter
    moments         : list
                      List of Moments with G (moments[0]) and g (moments[1])

    Returns
    -------

    minimum         : float
                      sum of square residuals (e) of the equation system 
                      moments.g - moments.G * lambdapar = e
    """
    vv = np.dot(moments[0], par)
    vv2 = moments[1] - vv
    return sum(vv2 ** 2)


def get_spFilter(w, lamb, sf):
    '''
    Compute the spatially filtered variables
    
    Parameters
    ----------
    w       : weight
              PySAL weights instance  
    lamb    : double
              spatial autoregressive parameter
    sf      : array
              the variable needed to compute the filter
    Returns
    --------
    rs      : array
              spatially filtered variable
    
    Examples
    --------

    >>> import numpy as np
    >>> import pysal
    >>> db = pysal.open(pysal.examples.get_path('columbus.dbf'),'r')
    >>> y = np.array(db.by_col("CRIME"))
    >>> y = np.reshape(y, (49,1))
    >>> w=pysal.open(pysal.examples.get_path("columbus.gal")).read()        
    >>> solu = get_spFilter(w,0.5,y)
    >>> print solu[0:5]
    [[  -8.9882875]
     [ -20.5685065]
     [ -28.196721 ]
     [ -36.9051915]
     [-111.1298   ]]

    '''
    try:
        result = sf - lamb * (w.sparse * sf)
    except:
        result = sf - lamb * (w * sf)
    return result


def get_lags(w, x, w_lags):
    '''
    Calculates a given order of spatial lags and all the smaller orders

    Parameters
    ----------
    w       : weight
              PySAL weights instance
    x       : array
              nxk arrays with the variables to be lagged  
    w_lags  : integer
              Maximum order of spatial lag

    Returns
    --------
    rs      : array
              nxk*(w_lags+1) array with original and spatially lagged variables

    '''
    lag = lag_spatial(w, x)
    spat_lags = lag
    for i in range(w_lags - 1):
        lag = lag_spatial(w, lag)
        spat_lags = sphstack(spat_lags, lag)
    return spat_lags


def inverse_prod(w, data, scalar, post_multiply=False, inv_method="power_exp", threshold=0.0000000001, max_iterations=None):
    """ 

    Parameters
    ----------

    w               : Pysal W object
                      nxn Pysal spatial weights object 

    data            : Numpy array
                      nx1 vector of data
    
    scalar          : float
                      Scalar value (typically rho or lambda)

    post_multiply   : boolean
                      If True then post-multiplies the data vector by the
                      inverse of the spatial filter, if false then
                      pre-multiplies.
    inv_method      : string
                      If "true_inv" uses the true inverse of W (slow);
                      If "power_exp" uses the power expansion method (default)

    threshold       : float
                      Test value to stop the iterations. Test is against
                      sqrt(increment' * increment), where increment is a
                      vector representing the contribution from each
                      iteration.

    max_iterations  : integer
                      Maximum number of iterations for the expansion.   

    Examples
    --------

    >>> import numpy, pysal
    >>> import numpy.linalg as la
    >>> np.random.seed(10)
    >>> w = pysal.lat2W(5, 5)
    >>> w.transform = 'r'
    >>> data = np.random.randn(w.n)
    >>> data.shape = (w.n, 1)
    >>> rho = 0.4
    >>> inv_pow = inverse_prod(w, data, rho, inv_method="power_exp")
    >>> # true matrix inverse
    >>> inv_reg = inverse_prod(w, data, rho, inv_method="true_inv")
    >>> np.allclose(inv_pow, inv_reg, atol=0.0001)
    True
    >>> # test the transpose version
    >>> inv_pow = inverse_prod(w, data, rho, inv_method="power_exp", post_multiply=True)
    >>> inv_reg = inverse_prod(w, data, rho, inv_method="true_inv", post_multiply=True)
    >>> np.allclose(inv_pow, inv_reg, atol=0.0001)
    True

    """
    if inv_method == "power_exp":
        inv_prod = power_expansion(
            w, data, scalar, post_multiply=post_multiply,
            threshold=threshold, max_iterations=max_iterations)
    elif inv_method == "true_inv":
        try:
            matrix = la.inv(np.eye(w.n) - (scalar * w.full()[0]))
        except:
            matrix = la.inv(np.eye(w.shape[0]) - (scalar * w))
        if post_multiply:
            inv_prod = spdot(data.T, matrix)
        else:
            inv_prod = spdot(matrix, data)
    else:
        raise Exception, "Invalid method selected for inversion."
    return inv_prod


def power_expansion(w, data, scalar, post_multiply=False, threshold=0.0000000001, max_iterations=None):
    """
    Compute the inverse of a matrix using the power expansion (Leontief
    expansion).  General form is:
    
        .. math:: 
            x &= (I - \rho W)^{-1}v = [I + \rho W + \rho^2 WW + \dots]v \\
              &= v + \rho Wv + \rho^2 WWv + \dots

    Examples
    --------
    Tests for this function are in inverse_prod()

    """
    try:
        ws = w.sparse
    except:
        ws = w
    if post_multiply:
        data = data.T
    running_total = copy.copy(data)
    increment = copy.copy(data)
    count = 1
    test = 10000000
    if max_iterations == None:
        max_iterations = 10000000
    while test > threshold and count <= max_iterations:
        if post_multiply:
            increment = increment * ws * scalar
        else:
            increment = ws * increment * scalar
        running_total += increment
        test_old = test
        test = la.norm(increment)
        if test > test_old:
            raise Exception, "power expansion will not converge, check model specification and that weight are less than 1"
        count += 1
    return running_total


def set_endog(y, x, w, yend, q, w_lags, lag_q):
    # Create spatial lag of y
    yl = lag_spatial(w, y)
    # spatial and non-spatial instruments
    if issubclass(type(yend), np.ndarray):
        if lag_q:
            lag_vars = sphstack(x, q)
        else:
            lag_vars = x
        spatial_inst = get_lags(w, lag_vars, w_lags)
        q = sphstack(q, spatial_inst)
        yend = sphstack(yend, yl)
    elif yend == None:  # spatial instruments only
        q = get_lags(w, x, w_lags)
        yend = yl
    else:
        raise Exception, "invalid value passed to yend"
    return yend, q

    lag = lag_spatial(w, x)
    spat_lags = lag
    for i in range(w_lags - 1):
        lag = lag_spatial(w, lag)
        spat_lags = sphstack(spat_lags, lag)
    return spat_lags


def set_endog_sparse(y, x, w, yend, q, w_lags, lag_q):
    """
    Same as set_endog, but with a sparse object passed as weights instead of W object.
    """
    yl = w * y
    # spatial and non-spatial instruments
    if issubclass(type(yend), np.ndarray):
        if lag_q:
            lag_vars = sphstack(x, q)
        else:
            lag_vars = x
        spatial_inst = w * lag_vars
        for i in range(w_lags - 1):
            spatial_inst = sphstack(spatial_inst, w * spatial_inst)
        q = sphstack(q, spatial_inst)
        yend = sphstack(yend, yl)
    elif yend == None:  # spatial instruments only
        q = w * x
        for i in range(w_lags - 1):
            q = sphstack(q, w * q)
        yend = yl
    else:
        raise Exception, "invalid value passed to yend"
    return yend, q


def iter_msg(iteration, max_iter):
    if iteration == max_iter:
        iter_stop = "Maximum number of iterations reached."
    else:
        iter_stop = "Convergence threshold (epsilon) reached."
    return iter_stop


def sp_att(w, y, predy, w_y, rho):
    xb = predy - rho * w_y
    if np.abs(rho) < 1:
        predy_sp = inverse_prod(w, xb, rho)
        warn = None
        # Note 1: Here if omitting pseudo-R2; If not, see Note 2.
        resid_sp = y - predy_sp
    else:
        #warn = "Warning: Estimate for rho is outside the boundary (-1, 1). Computation of true inverse of W was required (slow)."
        #predy_sp = inverse_prod(w, xb, rho, inv_method="true_inv")
        warn = "*** WARNING: Estimate for spatial lag coefficient is outside the boundary (-1, 1). ***"
        predy_sp = np.zeros(y.shape, float)
        resid_sp = np.zeros(y.shape, float)
    # resid_sp = y - predy_sp #Note 2: Here if computing true inverse; If not,
    # see Note 1.
    return predy_sp, resid_sp, warn


def spdot(a, b, array_out=True):
    """
    Matrix multiplication function to deal with sparse and dense objects

    Parameters
    ----------

    a           : array
                  first multiplication factor. Can either be sparse or dense.
    b           : array
                  second multiplication factor. Can either be sparse or dense.
    array_out   : boolean
                  If True (default) the output object is always a np.array

    Returns
    -------

    ab : array
         product of a times b. Sparse if a and b are sparse. Dense otherwise.
    """
    if type(a).__name__ == 'ndarray' and type(b).__name__ == 'ndarray':
        ab = np.dot(a, b)
    elif type(a).__name__ == 'csr_matrix' or type(b).__name__ == 'csr_matrix' \
            or type(a).__name__ == 'csc_matrix' or type(b).__name__ == 'csc_matrix':
        ab = a * b
        if array_out:
            if type(ab).__name__ == 'csc_matrix' or type(ab).__name__ == 'csr_matrix':
                ab = ab.toarray()
    else:
        raise Exception, "Invalid format for 'spdot' argument: %s and %s" % (
            type(a).__name__, type(b).__name__)
    return ab


def spmultiply(a, b, array_out=True):
    """
    Element-wise multiplication function to deal with sparse and dense
    objects. Both objects must be of the same type.

    Parameters
    ----------

    a           : array
                  first multiplication factor. Can either be sparse or dense.
    b           : array
                  second multiplication factor. Can either be sparse or dense.
                  integer.
    array_out   : boolean
                  If True (default) the output object is always a np.array

    Returns
    -------

    ab : array
         elementwise multiplied object. Sparse if a is sparse. Dense otherwise.
    """
    if type(a).__name__ == 'ndarray' and type(b).__name__ == 'ndarray':
        ab = a * b
    elif (type(a).__name__ == 'csr_matrix' or type(a).__name__ == 'csc_matrix') \
            and (type(b).__name__ == 'csr_matrix' or type(b).__name__ == 'csc_matrix'):
        ab = a.multiply(b)
        if array_out:
            if type(ab).__name__ == 'csc_matrix' or type(ab).__name__ == 'csr_matrix':
                ab = ab.toarray()
    else:
        raise Exception, "Invalid format for 'spmultiply' argument: %s and %s" % (
            type(a).__name__, type(b).__name__)
    return ab


def sphstack(a, b, array_out=False):
    """
    Horizontal stacking of vectors (or matrices) to deal with sparse and dense objects

    Parameters
    ----------

    a           : array or sparse matrix
                  First object.
    b           : array or sparse matrix
                  Object to be stacked next to a
    array_out   : boolean
                  If True the output object is a np.array; if False (default)
                  the output object is an np.array if both inputs are
                  arrays or CSR matrix if at least one input is a CSR matrix

    Returns
    -------

    ab          : array or sparse matrix
                  Horizontally stacked objects
    """
    if type(a).__name__ == 'ndarray' and type(b).__name__ == 'ndarray':
        ab = np.hstack((a, b))
    elif type(a).__name__ == 'csr_matrix' or type(b).__name__ == 'csr_matrix':
        ab = SP.hstack((a, b), format='csr')
        if array_out:
            if type(ab).__name__ == 'csr_matrix':
                ab = ab.toarray()
    else:
        raise Exception, "Invalid format for 'sphstack' argument: %s and %s" % (
            type(a).__name__, type(b).__name__)
    return ab


def spbroadcast(a, b, array_out=False):
    """
    Element-wise multiplication of a matrix and vector to deal with sparse 
    and dense objects

    Parameters
    ----------

    a           : array or sparse matrix
                  Object with one or more columns.
    b           : array
                  Object with only one column
    array_out   : boolean
                  If True the output object is a np.array; if False (default)
                  the output object is an np.array if both inputs are
                  arrays or CSR matrix if at least one input is a CSR matrix

    Returns
    -------

    ab          : array or sparse matrix
                  Element-wise multiplication of a and b
    """
    if type(a).__name__ == 'ndarray' and type(b).__name__ == 'ndarray':
        ab = a * b
    elif type(a).__name__ == 'csr_matrix':
        b_mod = SP.lil_matrix((b.shape[0], b.shape[0]))
        b_mod.setdiag(b)
        ab = (a.T * b_mod).T
        if array_out:
            if type(ab).__name__ == 'csr_matrix':
                ab = ab.toarray()
    else:
        raise Exception, "Invalid format for 'spbroadcast' argument: %s and %s" % (
            type(a).__name__, type(b).__name__)
    return ab


def spmin(a):
    """
    Minimum value in a matrix or vector to deal with sparse and dense objects

    Parameters
    ----------

    a           : array or sparse matrix
                  Object with one or more columns.

    Returns
    -------

    min a       : int or float
                  minimum value in a
    """

    if type(a).__name__ == 'ndarray':
        return a.min()
    elif type(a).__name__ == 'csr_matrix' or type(a).__name__ == 'csc_matrix':
        try:
            return min(a.data)
        except:
            if np.sum(a.data) == 0:
                return 0
            else:
                raise Exception, "Error: could not evaluate the minimum value."
    else:
        raise Exception, "Invalid format for 'spmultiply' argument: %s and %s" % (
            type(a).__name__, type(b).__name__)


def spmax(a):
    """
    Maximum value in a matrix or vector to deal with sparse and dense objects

    Parameters
    ----------

    a           : array or sparse matrix
                  Object with one or more columns.

    Returns
    -------

    max a       : int or float
                  maximum value in a
    """
    if type(a).__name__ == 'ndarray':
        return a.max()
    elif type(a).__name__ == 'csr_matrix' or type(a).__name__ == 'csc_matrix':
        try:
            return max(a.data)
        except:
            if np.sum(a.data) == 0:
                return 0
            else:
                raise Exception, "Error: could not evaluate the maximum value."
    else:
        raise Exception, "Invalid format for 'spmultiply' argument: %s and %s" % (
            type(a).__name__, type(b).__name__)


def set_warn(reg, warn):
    ''' Groups warning messages for printout. '''
    if warn:
        try:
            reg.warning += "Warning: " + warn + "\n"
        except:
            reg.warning = "Warning: " + warn + "\n"
    else:
        pass


def RegressionProps_basic(reg, betas=None, predy=None, u=None, sig2=None, sig2n_k=None, vm=None):
    ''' Set props based on arguments passed. '''
    if betas != None:
        reg.betas = betas
    if predy != None:
        reg.predy = predy
    else:
        try:
            reg.predy = spdot(reg.z, reg.betas)
        except:
            reg.predy = spdot(reg.x, reg.betas)
    if u != None:
        reg.u = u
    else:
        reg.u = reg.y - reg.predy
    if sig2 != None:
        reg.sig2 = sig2
    elif sig2n_k:
        reg.sig2 = np.sum(reg.u ** 2) / (reg.n - reg.k)
    else:
        reg.sig2 = np.sum(reg.u ** 2) / reg.n
    if vm != None:
        reg.vm = vm


def _test():
    import doctest
    doctest.testmod()

if __name__ == '__main__':
    _test()

########NEW FILE########
__FILENAME__ = w_utils
import numpy as np
import pysal as ps
import scipy.sparse as SPARSE


def symmetrize(w):
    """Generate symmetric matrix that has same eigenvalues as an asymmetric row
    standardized matrix w

    Parameters
    ----------
    w: weights object that has been row standardized

    Returns
    -------
    a sparse symmetric matrix with same eigenvalues as w
    
    """
    current = w.transform
    w.transform = 'B'
    d = w.sparse.sum(axis=1)  # row sum
    d.shape = (w.n,)
    d = np.sqrt(d)
    Di12 = SPARSE.spdiags(1. / d, [0], w.n, w.n)
    D12 = SPARSE.spdiags(d, [0], w.n, w.n)
    w.transform = 'r'
    return D12 * w.sparse * Di12

########NEW FILE########
__FILENAME__ = test_NameSpace
import os
import unittest
import pysal


class TestNameSpace(unittest.TestCase):
    """
        This test makes sure we don't remove anything from the pysal NameSpace that
        1.0 users might expect to be there.  1.0 Namespace was taken from the 1.1
        Code sprint wave, with special names removes (__all__, etc)
    """
    def test_contents(self):
        namespace_v1_0 = ['Box_Plot', 'DistanceBand', 'Equal_Interval',
                          'Fisher_Jenks', 'Geary', 'Jenks_Caspall',
                          'Jenks_Caspall_Forced', 'Jenks_Caspall_Sampled',
                          'Join_Counts', 'K_classifiers', 'Kernel',
                          'LISA_Markov', 'Markov', 'Max_P_Classifier',
                          'Maximum_Breaks', 'Maxp', 'Maxp_LISA', 'Moran',
                          'Moran_BV', 'Moran_BV_matrix', 'Moran_Local',
                          'Natural_Breaks', 'Percentiles', 'Quantiles',
                          'SpatialTau', 'Spatial_Markov', 'Std_Mean', 'Theil',
                          'TheilD', 'TheilDSim', 'Theta', 'User_Defined', 'W', 'adaptive_kernelW',
                          'adaptive_kernelW_from_shapefile', 'bin', 'bin1d',
                          'binC', 'buildContiguity', 'cg', 'comb', 'common',
                          'core', 'directional', 'ergodic', 'esda', 'full',
                          'gadf', 'higher_order', 'inequality', 'kernelW',
                          'kernelW_from_shapefile', 'knnW', 'knnW_from_array',
                          'knnW_from_shapefile', 'lag_spatial', 'lat2W',
                          'min_threshold_dist_from_shapefile', 'open',
                          'order', 'quantile', 'queen_from_shapefile',
                          'regime_weights', 'region', 'remap_ids',
                          'rook_from_shapefile', 'shimbel', 'spatial_dynamics',
                          'threshold_binaryW_from_array', 'threshold_binaryW_from_shapefile',
                          'threshold_continuousW_from_array', 'threshold_continuousW_from_shapefile',
                          'version', 'w_difference', 'w_intersection', 'w_subset',
                          'w_symmetric_difference', 'w_union', 'weights']

        current_namespace = dir(pysal)
        for item in namespace_v1_0:
            self.assertTrue(item in current_namespace)
        for item in current_namespace:
            if item not in namespace_v1_0 and not item.startswith('__'):
                print item, "added to name space"


suite = unittest.TestLoader().loadTestsFromTestCase(TestNameSpace)

if __name__ == '__main__':
    unittest.main()
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = version
version = "1.8.0dev"

########NEW FILE########
__FILENAME__ = Contiguity
"""
Contiguity based spatial weights
"""

__author__ = "Sergio J. Rey <srey@asu.edu> "
__all__ = ['buildContiguity']

import pysal
from _contW_binning import ContiguityWeights_binning as ContiguityWeights
from _contW_binning import ContiguityWeightsPolygons


WT_TYPE = {'rook': 2, 'queen': 1}  # for _contW_Binning


def buildContiguity(polygons, criterion="rook", ids=None):
    """
    Build contiguity weights from a source

    Parameters
    ----------

    polygons   : an instance of a pysal geo file handler
                 Any thing returned by pysal.open that is explicitly polygons
    criterion  : string
                 contiguity criterion ("rook","queen")
    ids        : list
                 identifiers for i,j

    Returns
    -------

    w         : W instance
                Contiguity weights object

    Examples
    --------

    >>> w = buildContiguity(pysal.open(pysal.examples.get_path('10740.shp'),'r'))
    WARNING: there is one disconnected observation (no neighbors)
    Island id:  [163]
    >>> w[0]
    {1: 1.0, 4: 1.0, 101: 1.0, 85: 1.0, 5: 1.0}
    >>> w = buildContiguity(pysal.open(pysal.examples.get_path('10740.shp'),'r'),criterion='queen')
    WARNING: there is one disconnected observation (no neighbors)
    Island id:  [163]
    >>> w.pct_nonzero
    0.031926364234056544
    >>> w = buildContiguity(pysal.open(pysal.examples.get_path('10740.shp'),'r'),criterion='rook')
    WARNING: there is one disconnected observation (no neighbors)
    Island id:  [163]
    >>> w.pct_nonzero
    0.026351084812623275
    >>> fips = pysal.open(pysal.examples.get_path('10740.dbf')).by_col('STFID')
    >>> w = buildContiguity(pysal.open(pysal.examples.get_path('10740.shp'),'r'),ids=fips)
    WARNING: there is one disconnected observation (no neighbors)
    Island id:  ['35043940300']
    >>> w['35001000107']
    {'35001003805': 1.0, '35001003721': 1.0, '35001000111': 1.0, '35001000112': 1.0, '35001000108': 1.0}

    Notes
    -----

    The types of sources supported will expand over time.

    See Also
    --------
    pysal.weights.W # need to fix sphinx links

    """
    if ids and len(ids) != len(set(ids)):
        raise ValueError("The argument to the ids parameter contains duplicate entries.")

    wt_type = WT_TYPE[criterion.lower()]
    geo = polygons
    if issubclass(type(geo), pysal.open):
        geo.seek(0)  # Make sure we read from the beinging of the file.
        geoObj = geo
    else:
        raise TypeError(
            "Argument must be a FileIO handler or connection string")
    neighbor_data = ContiguityWeights(geoObj, wt_type).w
    neighbors = {}
    #weights={}
    if ids:
        for key in neighbor_data:
            ida = ids[key]
            if ida not in neighbors:
                neighbors[ida] = set()
            neighbors[ida].update([ids[x] for x in neighbor_data[key]])
        for key in neighbors:
            neighbors[key] = list(neighbors[key])
    else:
        for key in neighbor_data:
            neighbors[key] = list(neighbor_data[key])
    return pysal.weights.W(neighbors, id_order=ids)


########NEW FILE########
__FILENAME__ = Distance
"""
Distance based spatial weights
"""

__author__ = "Sergio J. Rey <srey@asu.edu> "

import pysal
import scipy.spatial
from pysal.common import KDTree
from pysal.weights import W
import scipy.stats
import numpy as np

__all__ = ["knnW", "Kernel", "DistanceBand"]


def knnW(data, k=2, p=2, ids=None, pct_unique=0.25):
    """
    Creates nearest neighbor weights matrix based on k nearest
    neighbors.

    Parameters
    ----------

    data       : array (n,k) or KDTree where KDtree.data is array (n,k)
                 n observations on k characteristics used to measure
                 distances between the n objects
    k          : int
                 number of nearest neighbors
    p          : float
                 Minkowski p-norm distance metric parameter:
                 1<=p<=infinity
                 2: Euclidean distance
                 1: Manhattan distance
    ids        : list
                 identifiers to attach to each observation
    pct_unique : float
                 threshold percentage of unique points in data. Below this
                 threshold tree is built on unique values only

    Returns
    -------

    w         : W instance
                Weights object with binary weights

    Examples
    --------

    >>> x,y=np.indices((5,5))
    >>> x.shape=(25,1)
    >>> y.shape=(25,1)
    >>> data=np.hstack([x,y])
    >>> wnn2=knnW(data,k=2)
    >>> wnn4=knnW(data,k=4)
    >>> set([1,5,6,2]) == set(wnn4.neighbors[0])
    True
    >>> set([0,6,10,1]) == set(wnn4.neighbors[5])
    True
    >>> set([1,5]) == set(wnn2.neighbors[0])
    True
    >>> set([0,6]) == set(wnn2.neighbors[5])
    True
    >>> "%.2f"%wnn2.pct_nonzero
    '0.08'
    >>> wnn4.pct_nonzero
    0.16
    >>> wnn3e=knnW(data,p=2,k=3)
    >>> set([1,5,6]) == set(wnn3e.neighbors[0])
    True
    >>> wnn3m=knnW(data,p=1,k=3)
    >>> a = set([1,5,2])
    >>> b = set([1,5,6])
    >>> c = set([1,5,10])
    >>> w0n = set(wnn3m.neighbors[0])
    >>> a==w0n or b==w0n or c==w0n
    True

    ids

    >>> wnn2 = knnW(data,2)
    >>> wnn2[0]
    {1: 1.0, 5: 1.0}
    >>> wnn2[1]
    {0: 1.0, 2: 1.0}

    now with 1 rather than 0 offset

    >>> wnn2 = knnW(data,2, ids = range(1,26))
    >>> wnn2[1]
    {2: 1.0, 6: 1.0}
    >>> wnn2[2]
    {1: 1.0, 3: 1.0}
    >>> 0 in wnn2.neighbors
    False

    Notes
    -----

    Ties between neighbors of equal distance are arbitrarily broken.

    See Also
    --------
    pysal.weights.W

    """

    if issubclass(type(data), scipy.spatial.KDTree):
        kd = data
        data = kd.data
        nnq = kd.query(data, k=k+1, p=p)
        info = nnq[1]
    elif type(data).__name__ == 'ndarray':
        # check if unique points are a small fraction of all points
        ind =  np.lexsort(data.T)
        u = data[np.concatenate(([True],np.any(data[ind[1:]]!=data[ind[:-1]],axis=1)))]
        pct_u = len(u)*1. / len(data)
        if pct_u < pct_unique:
            tree = KDTree(u)
            nnq = tree.query(data, k=k+1, p=p)
            info = nnq[1]
            uid = [np.where((data == ui).all(axis=1))[0][0] for ui in u]
            new_info = np.zeros((len(data), k + 1), 'int')
            for i, row in enumerate(info):
                new_info[i] = [uid[j] for j in row]
            info = new_info
        else:
            kd = KDTree(data)
            # calculate
            nnq = kd.query(data, k=k + 1, p=p)
            info = nnq[1]
    else:
        print 'Unsupported type'
        return None

    neighbors = {}
    for i, row in enumerate(info):
        row = row.tolist()
        if i in row:
            row.remove(i)
            focal = i
        if ids:
            row = [ ids[j] for j in row]
            focal = ids[i]
        neighbors[focal] = row
    return pysal.weights.W(neighbors,  id_order=ids)


class Kernel(W):
    """Spatial weights based on kernel functions

    Parameters
    ----------

    data        : array (n,k) or KDTree where KDtree.data is array (n,k)
                  n observations on k characteristics used to measure
                  distances between the n objects
    bandwidth   : float or array-like (optional)
                  the bandwidth :math:`h_i` for the kernel.
    fixed       : binary
                  If true then :math:`h_i=h \\forall i`. If false then
                  bandwidth is adaptive across observations.
    k           : int
                  the number of nearest neighbors to use for determining
                  bandwidth. For fixed bandwidth, :math:`h_i=max(dknn) \\forall i`
                  where :math:`dknn` is a vector of k-nearest neighbor
                  distances (the distance to the kth nearest neighbor for each
                  observation).  For adaptive bandwidths, :math:`h_i=dknn_i`
    diagonal    : boolean
                  If true, set diagonal weights = 1.0, if false (default),
                  diagonals weights are set to value according to kernel
                  function.
    function    : string {'triangular','uniform','quadratic','quartic','gaussian'}
                  kernel function defined as follows with

                  .. math::

                      z_{i,j} = d_{i,j}/h_i

                  triangular

                  .. math::

                      K(z) = (1 - |z|) \ if |z| \le 1

                  uniform

                  .. math::

                      K(z) = 1/2 \ if |z| \le 1

                  quadratic

                  .. math::

                      K(z) = (3/4)(1-z^2) \ if |z| \le 1

                  quartic

                  .. math::

                      K(z) = (15/16)(1-z^2)^2 \ if |z| \le 1

                  gaussian

                  .. math::

                      K(z) = (2\pi)^{(-1/2)} exp(-z^2 / 2)

    eps         : float
                  adjustment to ensure knn distance range is closed on the
                  knnth observations

    Examples
    --------

    >>> points=[(10, 10), (20, 10), (40, 10), (15, 20), (30, 20), (30, 30)]
    >>> kw=Kernel(points)
    >>> kw.weights[0]
    [1.0, 0.500000049999995, 0.4409830615267465]
    >>> kw.neighbors[0]
    [0, 1, 3]
    >>> kw.bandwidth
    array([[ 20.000002],
           [ 20.000002],
           [ 20.000002],
           [ 20.000002],
           [ 20.000002],
           [ 20.000002]])
    >>> kw15=Kernel(points,bandwidth=15.0)
    >>> kw15[0]
    {0: 1.0, 1: 0.33333333333333337, 3: 0.2546440075000701}
    >>> kw15.neighbors[0]
    [0, 1, 3]
    >>> kw15.bandwidth
    array([[ 15.],
           [ 15.],
           [ 15.],
           [ 15.],
           [ 15.],
           [ 15.]])

    Adaptive bandwidths user specified

    >>> bw=[25.0,15.0,25.0,16.0,14.5,25.0]
    >>> kwa=Kernel(points,bandwidth=bw)
    >>> kwa.weights[0]
    [1.0, 0.6, 0.552786404500042, 0.10557280900008403]
    >>> kwa.neighbors[0]
    [0, 1, 3, 4]
    >>> kwa.bandwidth
    array([[ 25. ],
           [ 15. ],
           [ 25. ],
           [ 16. ],
           [ 14.5],
           [ 25. ]])

    Endogenous adaptive bandwidths

    >>> kwea=Kernel(points,fixed=False)
    >>> kwea.weights[0]
    [1.0, 0.10557289844279438, 9.99999900663795e-08]
    >>> kwea.neighbors[0]
    [0, 1, 3]
    >>> kwea.bandwidth
    array([[ 11.18034101],
           [ 11.18034101],
           [ 20.000002  ],
           [ 11.18034101],
           [ 14.14213704],
           [ 18.02775818]])

    Endogenous adaptive bandwidths with Gaussian kernel

    >>> kweag=Kernel(points,fixed=False,function='gaussian')
    >>> kweag.weights[0]
    [0.3989422804014327, 0.2674190291577696, 0.2419707487162134]
    >>> kweag.bandwidth
    array([[ 11.18034101],
           [ 11.18034101],
           [ 20.000002  ],
           [ 11.18034101],
           [ 14.14213704],
           [ 18.02775818]])

    Diagonals to 1.0

    >>> kq = Kernel(points,function='gaussian')
    >>> kq.weights
    {0: [0.3989422804014327, 0.35206533556593145, 0.3412334260702758], 1: [0.35206533556593145, 0.3989422804014327, 0.2419707487162134, 0.3412334260702758, 0.31069657591175387], 2: [0.2419707487162134, 0.3989422804014327, 0.31069657591175387], 3: [0.3412334260702758, 0.3412334260702758, 0.3989422804014327, 0.3011374490937829, 0.26575287272131043], 4: [0.31069657591175387, 0.31069657591175387, 0.3011374490937829, 0.3989422804014327, 0.35206533556593145], 5: [0.26575287272131043, 0.35206533556593145, 0.3989422804014327]}
    >>> kqd = Kernel(points, function='gaussian', diagonal=True)
    >>> kqd.weights
    {0: [1.0, 0.35206533556593145, 0.3412334260702758], 1: [0.35206533556593145, 1.0, 0.2419707487162134, 0.3412334260702758, 0.31069657591175387], 2: [0.2419707487162134, 1.0, 0.31069657591175387], 3: [0.3412334260702758, 0.3412334260702758, 1.0, 0.3011374490937829, 0.26575287272131043], 4: [0.31069657591175387, 0.31069657591175387, 0.3011374490937829, 1.0, 0.35206533556593145], 5: [0.26575287272131043, 0.35206533556593145, 1.0]}
    """
    def __init__(self, data, bandwidth=None, fixed=True, k=2,
                 function='triangular', eps=1.0000001, ids=None,
                 diagonal=False):
        if issubclass(type(data), scipy.spatial.KDTree):
            self.kdt = data
            self.data = self.kdt.data
            data = self.data
        else:
            self.data = data
            self.kdt = KDTree(self.data)
        self.k = k + 1
        self.function = function.lower()
        self.fixed = fixed
        self.eps = eps
        if bandwidth:
            try:
                bandwidth = np.array(bandwidth)
                bandwidth.shape = (len(bandwidth), 1)
            except:
                bandwidth = np.ones((len(data), 1), 'float') * bandwidth
            self.bandwidth = bandwidth
        else:
            self._set_bw()

        self._eval_kernel()
        neighbors, weights = self._k_to_W(ids)
        if diagonal:
            for i in neighbors:
                weights[i][neighbors[i].index(i)] = 1.0
        W.__init__(self, neighbors, weights, ids)

    def _k_to_W(self, ids=None):
        allneighbors = {}
        weights = {}
        if ids:
            ids = np.array(ids)
        else:
            ids = np.arange(len(self.data))
        for i, neighbors in enumerate(self.kernel):
            if len(self.neigh[i]) == 0:
                allneighbors[ids[i]] = []
                weights[ids[i]] = []
            else:
                allneighbors[ids[i]] = list(ids[self.neigh[i]])
                weights[ids[i]] = self.kernel[i].tolist()
        return allneighbors, weights

    def _set_bw(self):
        dmat, neigh = self.kdt.query(self.data, k=self.k)
        if self.fixed:
            # use max knn distance as bandwidth
            bandwidth = dmat.max() * self.eps
            n = len(dmat)
            self.bandwidth = np.ones((n, 1), 'float') * bandwidth
        else:
            # use local max knn distance
            self.bandwidth = dmat.max(axis=1) * self.eps
            self.bandwidth.shape = (self.bandwidth.size, 1)
            # identify knn neighbors for each point
            nnq = self.kdt.query(self.data, k=self.k)
            self.neigh = nnq[1]

    def _eval_kernel(self):
        # get points within bandwidth distance of each point
        if not hasattr(self, 'neigh'):
            kdtq = self.kdt.query_ball_point
            neighbors = [kdtq(self.data[i], r=bwi[0]) for i,
                         bwi in enumerate(self.bandwidth)]
            self.neigh = neighbors
        # get distances for neighbors
        bw = self.bandwidth

        kdtq = self.kdt.query
        z = []
        for i, nids in enumerate(self.neigh):
            di, ni = kdtq(self.data[i], k=len(nids))
            zi = np.array([dict(zip(ni, di))[nid] for nid in nids]) / bw[i]
            z.append(zi)
        zs = z
        # functions follow Anselin and Rey (2010) table 5.4
        if self.function == 'triangular':
            self.kernel = [1 - zi for zi in zs]  
        elif self.function == 'uniform':
            self.kernel = [np.ones(zi.shape) * 0.5 for zi in zs]
        elif self.function == 'quadratic':
            self.kernel = [(3. / 4) * (1 - zi ** 2) for zi in zs]
        elif self.function == 'quartic':
            self.kernel = [(15. / 16) * (1 - zi ** 2) ** 2 for zi in zs]
        elif self.function == 'gaussian':
            c = np.pi * 2
            c = c ** (-0.5)
            self.kernel = [c * np.exp(-(zi ** 2) / 2.) for zi in zs]
        else:
            print 'Unsupported kernel function', self.function


class DistanceBand(W):
    """Spatial weights based on distance band

    Parameters
    ----------

    data        : array (n,k) or KDTree where KDtree.data is array (n,k)
                  n observations on k characteristics used to measure
                  distances between the n objects
    threshold  : float
                 distance band
    p          : float
                 Minkowski p-norm distance metric parameter:
                 1<=p<=infinity
                 2: Euclidean distance
                 1: Manhattan distance
    binary     : binary
                 If true w_{ij}=1 if d_{i,j}<=threshold, otherwise w_{i,j}=0
                 If false wij=dij^{alpha}
    alpha      : float
                 distance decay parameter for weight (default -1.0)
                 if alpha is positive the weights will not decline with
                 distance. If binary is True, alpha is ignored

    Examples
    --------

    >>> points=[(10, 10), (20, 10), (40, 10), (15, 20), (30, 20), (30, 30)]
    >>> w=DistanceBand(points,threshold=11.2)
    WARNING: there is one disconnected observation (no neighbors)
    Island id:  [2]
    >>> w.weights
    {0: [1, 1], 1: [1, 1], 2: [], 3: [1, 1], 4: [1], 5: [1]}
    >>> w.neighbors
    {0: [1, 3], 1: [0, 3], 2: [], 3: [0, 1], 4: [5], 5: [4]}
    >>> w=DistanceBand(points,threshold=14.2)
    >>> w.weights
    {0: [1, 1], 1: [1, 1, 1], 2: [1], 3: [1, 1], 4: [1, 1, 1], 5: [1]}
    >>> w.neighbors
    {0: [1, 3], 1: [0, 3, 4], 2: [4], 3: [0, 1], 4: [1, 2, 5], 5: [4]}

    inverse distance weights

    >>> w=DistanceBand(points,threshold=11.2,binary=False)
    WARNING: there is one disconnected observation (no neighbors)
    Island id:  [2]
    >>> w.weights[0]
    [0.10000000000000001, 0.089442719099991588]
    >>> w.neighbors[0]
    [1, 3]
    >>>

    gravity weights

    >>> w=DistanceBand(points,threshold=11.2,binary=False,alpha=-2.)
    WARNING: there is one disconnected observation (no neighbors)
    Island id:  [2]
    >>> w.weights[0]
    [0.01, 0.0079999999999999984]

    Notes
    -----

    this was initially implemented running scipy 0.8.0dev (in epd 6.1).
    earlier versions of scipy (0.7.0) have a logic bug in scipy/sparse/dok.py
    so serge changed line 221 of that file on sal-dev to fix the logic bug

    """
    def __init__(self, data, threshold, p=2, alpha=-1.0, binary=True, ids=None):
        """
        Casting to floats is a work around for a bug in scipy.spatial.  See detail in pysal issue #126
        """
        if issubclass(type(data), scipy.spatial.KDTree):
            self.kd = data
            self.data = self.kd.data
        else:
            try:
                data = np.asarray(data)
                if data.dtype.kind != 'f':
                    data = data.astype(float)
                self.data = data
                self.kd = KDTree(self.data)
            except:
                raise ValueError("Could not make array from data")

        self.p = p
        self.threshold = threshold
        self.binary = binary
        self.alpha = alpha
        self._band()
        neighbors, weights = self._distance_to_W(ids)
        W.__init__(self, neighbors, weights, ids)

    def _band(self):
        """
        find all pairs within threshold
        """
        kd = self.kd
        #ns=[kd.query_ball_point(point,self.threshold) for point in self.data]
        ns = kd.query_ball_tree(kd, self.threshold)
        self._nmat = ns

    def _distance_to_W(self, ids=None):
        allneighbors = {}
        weights = {}
        if ids:
            ids = np.array(ids)
        else:
            ids = np.arange(len(self._nmat))
        if self.binary:
            for i, neighbors in enumerate(self._nmat):
                ns = [ni for ni in neighbors if ni != i]
                neigh = list(ids[ns])
                if len(neigh) == 0:
                    allneighbors[ids[i]] = []
                    weights[ids[i]] = []
                else:
                    allneighbors[ids[i]] = neigh
                    weights[ids[i]] = [1] * len(ns)
        else:
            self.dmat = self.kd.sparse_distance_matrix(
                self.kd, max_distance=self.threshold)
            for i, neighbors in enumerate(self._nmat):
                ns = [ni for ni in neighbors if ni != i]
                neigh = list(ids[ns])
                if len(neigh) == 0:
                    allneighbors[ids[i]] = []
                    weights[ids[i]] = []
                else:
                    try:
                        allneighbors[ids[i]] = neigh
                        weights[ids[i]] = [self.dmat[(
                            i, j)] ** self.alpha for j in ns]
                    except ZeroDivisionError, e:
                        print(e, "Cannot compute inverse distance for elements at same location (distance=0).")
        return allneighbors, weights


def _test():
    import doctest
    # the following line could be used to define an alternative to the '<BLANKLINE>' flag
    #doctest.BLANKLINE_MARKER = 'something better than <BLANKLINE>'
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)

if __name__ == '__main__':
    _test()

########NEW FILE########
__FILENAME__ = spatial_lag
"""
spatial lag operations
"""
__authors__ = "Serge Rey <srey@asu.edu>, David C. Folch <david.folch@asu.edu>"
__all__ = ['lag_spatial']


def lag_spatial(w, y):
    """
    Spatial lag operator. If w is row standardized, returns the average of
    each observation's neighbors; if not, returns the weighted sum of each
    observation's neighbors.

    Parameters
    ----------

    w : W
        weights object
    y : array
        numpy array with dimensionality conforming to w (see examples)

    Returns
    -------

    wy : array
         array of numeric values for the spatial lag

    Examples
    --------

    >>> import pysal
    >>> import numpy as np

    Setup a 9x9 binary spatial weights matrix and vector of data; compute the
    spatial lag of the vector.

    >>> w = pysal.lat2W(3, 3)
    >>> y = np.arange(9)
    >>> yl = pysal.lag_spatial(w, y)
    >>> yl
    array([  4.,   6.,   6.,  10.,  16.,  14.,  10.,  18.,  12.])

    Row standardize the weights matrix and recompute the spatial lag

    >>> w.transform = 'r'
    >>> yl = pysal.lag_spatial(w, y)
    >>> yl
    array([ 2.        ,  2.        ,  3.        ,  3.33333333,  4.        ,
            4.66666667,  5.        ,  6.        ,  6.        ])

    Explicitly define data vector as 9x1 and recompute the spatial lag

    >>> y.shape = (9, 1)
    >>> yl = pysal.lag_spatial(w, y)
    >>> yl
    array([[ 2.        ],
           [ 2.        ],
           [ 3.        ],
           [ 3.33333333],
           [ 4.        ],
           [ 4.66666667],
           [ 5.        ],
           [ 6.        ],
           [ 6.        ]])

    Take the spatial lag of a 9x2 data matrix

    >>> yr = np.arange(8, -1, -1)
    >>> yr.shape = (9, 1)
    >>> x = np.hstack((y, yr))
    >>> yl = pysal.lag_spatial(w, x)
    >>> yl
    array([[ 2.        ,  6.        ],
           [ 2.        ,  6.        ],
           [ 3.        ,  5.        ],
           [ 3.33333333,  4.66666667],
           [ 4.        ,  4.        ],
           [ 4.66666667,  3.33333333],
           [ 5.        ,  3.        ],
           [ 6.        ,  2.        ],
           [ 6.        ,  2.        ]])

    """
    return w.sparse * y

########NEW FILE########
__FILENAME__ = test_Contiguity
"""Unit test for Contiguity.py"""
import unittest
import pysal
import numpy as np


class TestContiguity(unittest.TestCase):
    def setUp(self):
        self.polyShp = pysal.examples.get_path('10740.shp')

    def test_buildContiguity(self):
        w = pysal.buildContiguity(pysal.open(self.polyShp, 'r'))
        self.assertEqual(w[0], {1: 1.0, 4: 1.0, 101: 1.0, 85: 1.0, 5: 1.0})
        w = pysal.buildContiguity(
            pysal.open(self.polyShp, 'r'), criterion='queen')
        self.assertEqual(w.pct_nonzero, 0.031926364234056544)
        w = pysal.buildContiguity(
            pysal.open(self.polyShp, 'r'), criterion='rook')
        self.assertEqual(w.pct_nonzero, 0.026351084812623275)
        fips = pysal.open(pysal.examples.get_path('10740.dbf')).by_col('STFID')
        w = pysal.buildContiguity(pysal.open(self.polyShp, 'r'), ids=fips)
        self.assertEqual(w['35001000107'], {'35001003805': 1.0, '35001003721':
                                            1.0, '35001000111': 1.0, '35001000112': 1.0, '35001000108': 1.0})


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_Distance
import os
import unittest
import pysal
import numpy as np


class TestDistanceWeights(unittest.TestCase):
    def setUp(self):
        np.random.seed(1234)
        self.polyShp = pysal.examples.get_path('columbus.shp')
        self.arcShp = pysal.examples.get_path('stl_hom.shp')
        self.points = [(
            10, 10), (20, 10), (40, 10), (15, 20), (30, 20), (30, 30)]

    def test_knnW(self):
        x = np.indices((5, 5))
        x, y = np.indices((5, 5))
        x.shape = (25, 1)
        y.shape = (25, 1)
        data = np.hstack([x, y])
        wnn2 = pysal.knnW(data, k=2)
        wnn4 = pysal.knnW(data, k=4)
        wnn4.neighbors[0]
        self.assertEqual(set(wnn4.neighbors[0]), set([1, 5, 6, 2]))
        self.assertEqual(set(wnn2.neighbors[5]), set([0, 6]))
        self.assertEqual(wnn2.pct_nonzero, 0.080000000000000002)
        wnn3e = pysal.knnW(data, p=2, k=3)
        self.assertEqual(set(wnn3e.neighbors[0]), set([1, 5, 6]))
        wc = pysal.knnW_from_shapefile(self.polyShp)
        self.assertEqual(wc.pct_nonzero, 0.040816326530612242)
        self.assertEqual(set(wc.neighbors[0]), set([2, 1]))
        wc3 = pysal.knnW_from_shapefile(self.polyShp, k=3)
        self.assertEqual(wc3.weights[1], [1, 1, 1])
        self.assertEqual(set(wc3.neighbors[1]), set([0,3,7]))

    def test_knnW_arc(self):
        pts = [x.centroid for x in pysal.open(self.arcShp)]
        dist = pysal.cg.sphere.arcdist  # default radius is Earth KM
        full = np.matrix([[dist(pts[i], pts[j]) for j in xrange(
            len(pts))] for i in xrange(len(pts))])

        kd = pysal.cg.kdtree.KDTree(pts, distance_metric='Arc',
                                    radius=pysal.cg.sphere.RADIUS_EARTH_KM)
        w = pysal.knnW(kd, 4)
        self.assertEqual(set(w.neighbors[4]), set([1,3,9,12]))
        self.assertEqual(set(w.neighbors[40]), set([31,38,45,49]))
        #self.assertTrue((full.argsort()[:, 1:5] == np.array(
        #    [w.neighbors[x] for x in range(len(pts))])).all())

    def test_Kernel(self):
        kw = pysal.Kernel(self.points)
        self.assertEqual(kw.weights[0], [1.0, 0.50000004999999503,
                                         0.44098306152674649])
        kw15 = pysal.Kernel(self.points, bandwidth=15.0)
        self.assertEqual(kw15[0], {0: 1.0, 1: 0.33333333333333337,
                                   3: 0.2546440075000701})
        self.assertEqual(kw15.bandwidth[0], 15.)
        self.assertEqual(kw15.bandwidth[-1], 15.)
        bw = [25.0, 15.0, 25.0, 16.0, 14.5, 25.0]
        kwa = pysal.Kernel(self.points, bandwidth=bw)
        self.assertEqual(kwa.weights[0], [1.0, 0.59999999999999998,
                                          0.55278640450004202,
                                          0.10557280900008403])
        self.assertEqual(kwa.neighbors[0], [0, 1, 3, 4])
        self.assertEqual(kwa.bandwidth[0], 25.)
        self.assertEqual(kwa.bandwidth[1], 15.)
        self.assertEqual(kwa.bandwidth[2], 25.)
        self.assertEqual(kwa.bandwidth[3], 16.)
        self.assertEqual(kwa.bandwidth[4], 14.5)
        self.assertEqual(kwa.bandwidth[5], 25.)
        kwea = pysal.Kernel(self.points, fixed=False)
        self.assertEqual(kwea.weights[0], [1.0, 0.10557289844279438,
                                           9.9999990066379496e-08])
        l = kwea.bandwidth.tolist()
        self.assertEqual(l, [[11.180341005532938], [11.180341005532938],
                             [20.000002000000002], [11.180341005532938],
                             [14.142137037944515], [18.027758180095585]])
        kweag = pysal.Kernel(self.points, fixed=False, function='gaussian')
        self.assertEqual(kweag.weights[0], [0.3989422804014327,
                                            0.26741902915776961,
                                            0.24197074871621341])
        l = kweag.bandwidth.tolist()
        self.assertEqual(l, [[11.180341005532938], [11.180341005532938],
                            [20.000002000000002], [11.180341005532938],
                            [14.142137037944515], [18.027758180095585]])

        kw = pysal.kernelW_from_shapefile(self.polyShp, idVariable='POLYID')
        self.assertEqual(kw.weights[1], [0.2052478782400463,
                                         0.0070787731484506233, 1.0,
                                         0.23051223027663237])
        kwa = pysal.adaptive_kernelW_from_shapefile(self.polyShp)
        self.assertEqual(kwa.weights[0], [1.0, 0.03178906767736345,
                                          9.9999990066379496e-08])

    def test_threshold(self):
        md = pysal.min_threshold_dist_from_shapefile(self.polyShp)
        self.assertEqual(md, 0.61886415807685413)
        wid = pysal.threshold_continuousW_from_array(self.points, 11.2)
        self.assertEqual(wid.weights[0], [0.10000000000000001,
                                          0.089442719099991588])
        wid2 = pysal.threshold_continuousW_from_array(
            self.points, 11.2, alpha=-2.0)
        self.assertEqual(wid2.weights[0], [0.01, 0.0079999999999999984])
        w = pysal.threshold_continuousW_from_shapefile(
            self.polyShp, 0.62, idVariable="POLYID")
        self.assertEqual(w.weights[1], [1.6702346893743334,
                                        1.7250729841938093])

    def test_DistanceBand(self):
        """ see issue #126 """
        w = pysal.rook_from_shapefile(
            pysal.examples.get_path("lattice10x10.shp"))
        polygons = pysal.open(
            pysal.examples.get_path("lattice10x10.shp"), "r").read()
        points1 = [poly.centroid for poly in polygons]
        w1 = pysal.DistanceBand(points1, 1)
        for k in range(w.n):
            self.assertEqual(w[k], w1[k])

    def test_DistanceBand_ints(self):
        """ see issue #126 """
        w = pysal.rook_from_shapefile(
            pysal.examples.get_path("lattice10x10.shp"))
        polygons = pysal.open(
            pysal.examples.get_path("lattice10x10.shp"), "r").read()
        points2 = [tuple(map(int, poly.vertices[0])) for poly in polygons]
        w2 = pysal.DistanceBand(points2, 1)
        for k in range(w.n):
            self.assertEqual(w[k], w2[k])

    def test_DistanceBand_arc(self):
        pts = [x.centroid for x in pysal.open(self.arcShp)]
        dist = pysal.cg.sphere.arcdist  # default radius is Earth KM
        full = np.matrix([[dist(pts[i], pts[j]) for j in xrange(
            len(pts))] for i in xrange(len(pts))])

        kd = pysal.cg.kdtree.KDTree(pts, distance_metric='Arc',
                                    radius=pysal.cg.sphere.RADIUS_EARTH_KM)
        w = pysal.DistanceBand(kd, full.max(), binary=False, alpha=1.0)
        self.assertTrue((w.sparse.todense() == full).all())


suite = unittest.TestLoader().loadTestsFromTestCase(TestDistanceWeights)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_spatial_lag

import os
import unittest
import pysal
import numpy as np


class Testlag_spatial(unittest.TestCase):
    def setUp(self):
        self.neighbors = {'c': ['b'], 'b': ['c', 'a'], 'a': ['b']}
        self.weights = {'c': [1.0], 'b': [1.0, 1.0], 'a': [1.0]}
        self.id_order = ['a', 'b', 'c']
        self.weights = {'c': [1.0], 'b': [1.0, 1.0], 'a': [1.0]}
        self.w = pysal.W(self.neighbors, self.weights, self.id_order)
        self.y = np.array([0, 1, 2])

    def test_lag_spatial(self):
        yl = pysal.lag_spatial(self.w, self.y)
        np.testing.assert_array_almost_equal(yl, [1., 2., 1.])
        self.w.id_order = ['b', 'c', 'a']
        y = np.array([1, 2, 0])
        yl = pysal.lag_spatial(self.w, y)
        np.testing.assert_array_almost_equal(yl, [2., 1., 1.])
        w = pysal.lat2W(3, 3)
        y = np.arange(9)
        yl = pysal.lag_spatial(w, y)
        ylc = np.array([4., 6., 6., 10., 16., 14., 10., 18., 12.])
        np.testing.assert_array_almost_equal(yl, ylc)
        w.transform = 'r'
        yl = pysal.lag_spatial(w, y)
        ylc = np.array(
            [2., 2., 3., 3.33333333, 4.,
             4.66666667, 5., 6., 6.])
        np.testing.assert_array_almost_equal(yl, ylc)


suite = unittest.TestLoader().loadTestsFromTestCase(Testlag_spatial)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_user
import os
import unittest
import pysal
import numpy as np


class Testuser(unittest.TestCase):
    def setUp(self):
        self.wq = pysal.queen_from_shapefile(
            pysal.examples.get_path("columbus.shp"))
        self.wr = pysal.rook_from_shapefile(
            pysal.examples.get_path("columbus.shp"))

    def test_queen_from_shapefile(self):
        self.assertAlmostEquals(self.wq.pct_nonzero, 0.098292378175760101)

    def test_rook_from_shapefile(self):
        self.assertAlmostEquals(self.wr.pct_nonzero, 0.083298625572678045)

    def test_knnW_from_array(self):
        import numpy as np
        x, y = np.indices((5, 5))
        x.shape = (25, 1)
        y.shape = (25, 1)
        data = np.hstack([x, y])
        wnn2 = pysal.knnW_from_array(data, k=2)
        wnn4 = pysal.knnW_from_array(data, k=4)
        self.assertEquals(set(wnn4.neighbors[0]), set([1, 5, 6, 2]))
        self.assertEquals(set(wnn4.neighbors[5]), set([0, 6, 10, 1]))
        self.assertEquals(set(wnn2.neighbors[0]), set([1, 5]))
        self.assertEquals(set(wnn2.neighbors[5]), set([0, 6]))
        self.assertAlmostEquals(wnn2.pct_nonzero, 0.080000000000000002)
        self.assertAlmostEquals(wnn4.pct_nonzero, 0.16)
        wnn4 = pysal.knnW_from_array(data, k=4)
        self.assertEquals(set(wnn4.neighbors[0]), set([1, 5, 6, 2]))
        wnn3e = pysal.knnW(data, p=2, k=3)
        self.assertEquals(set(wnn3e.neighbors[0]),set([1, 5, 6]))
        wnn3m = pysal.knnW(data, p=1, k=3)
        self.assertEquals(set(wnn3m.neighbors[0]), set([1, 5, 2]))

    def test_knnW_from_shapefile(self):
        wc = pysal.knnW_from_shapefile(pysal.examples.get_path("columbus.shp"))
        self.assertAlmostEquals(wc.pct_nonzero, 0.040816326530612242)
        wc3 = pysal.knnW_from_shapefile(pysal.examples.get_path(
            "columbus.shp"), k=3)
        self.assertEquals(wc3.weights[1], [1, 1, 1])
        self.assertEquals(set(wc3.neighbors[1]), set([3, 0, 7]))
        self.assertEquals(set(wc.neighbors[0]), set([2, 1]))
        w = pysal.knnW_from_shapefile(pysal.examples.get_path('juvenile.shp'))
        self.assertAlmostEquals(w.pct_nonzero, 0.011904761904761904)
        w1 = pysal.knnW_from_shapefile(
            pysal.examples.get_path('juvenile.shp'), k=1)
        self.assertAlmostEquals(w1.pct_nonzero, 0.0059523809523809521)

    def test_threshold_binaryW_from_array(self):
        points = [(10, 10), (20, 10), (40, 10), (15, 20), (30, 20), (30, 30)]
        w = pysal.threshold_binaryW_from_array(points, threshold=11.2)
        self.assertEquals(w.weights, {0: [1, 1], 1: [1, 1], 2: [],
                                      3: [1, 1], 4: [1], 5: [1]})
        self.assertEquals(w.neighbors, {0: [1, 3], 1: [0, 3], 2: [
        ], 3: [0, 1], 4: [5], 5: [4]})

    def test_threshold_binaryW_from_shapefile(self):

        w = pysal.threshold_binaryW_from_shapefile(pysal.examples.get_path(
            "columbus.shp"), 0.62, idVariable="POLYID")
        self.assertEquals(w.weights[1], [1, 1])

    def test_threshold_continuousW_from_array(self):
        points = [(10, 10), (20, 10), (40, 10), (15, 20), (30, 20), (30, 30)]
        wid = pysal.threshold_continuousW_from_array(points, 11.2)
        self.assertEquals(wid.weights[0], [0.10000000000000001,
                                           0.089442719099991588])
        wid2 = pysal.threshold_continuousW_from_array(points, 11.2, alpha=-2.0)
        self.assertEquals(wid2.weights[0], [0.01, 0.0079999999999999984])

    def test_threshold_continuousW_from_shapefile(self):
        w = pysal.threshold_continuousW_from_shapefile(pysal.examples.get_path(
            "columbus.shp"), 0.62, idVariable="POLYID")
        self.assertEquals(
            w.weights[1], [1.6702346893743334, 1.7250729841938093])

    def test_kernelW(self):
        points = [(10, 10), (20, 10), (40, 10), (15, 20), (30, 20), (30, 30)]
        kw = pysal.kernelW(points)
        self.assertEquals(kw.weights[0], [1.0, 0.50000004999999503,
                                          0.44098306152674649])
        self.assertEquals(kw.neighbors[0], [0, 1, 3])
        np.testing.assert_array_almost_equal(
            kw.bandwidth, np.array([[20.000002],
                                    [20.000002],
                                    [20.000002],
                                    [20.000002],
                                    [20.000002],
                                    [20.000002]]))

    def test_min_threshold_dist_from_shapefile(self):
        f = pysal.examples.get_path('columbus.shp')
        min_d = pysal.min_threshold_dist_from_shapefile(f)
        self.assertAlmostEquals(min_d, 0.61886415807685413)

    def test_kernelW_from_shapefile(self):
        kw = pysal.kernelW_from_shapefile(pysal.examples.get_path(
            'columbus.shp'), idVariable='POLYID')
        self.assertEquals(kw.weights[1], [0.2052478782400463,
                                          0.0070787731484506233, 1.0,
                                          0.23051223027663237])
        np.testing.assert_array_almost_equal(
            kw.bandwidth[:3], np.array([[0.75333961], [0.75333961],
                                        [0.75333961]]))

    def test_adaptive_kernelW(self):
        points = [(10, 10), (20, 10), (40, 10), (15, 20), (30, 20), (30, 30)]
        bw = [25.0, 15.0, 25.0, 16.0, 14.5, 25.0]
        kwa = pysal.adaptive_kernelW(points, bandwidths=bw)
        self.assertEqual(kwa.weights[0], [1.0, 0.59999999999999998,
                                          0.55278640450004202,
                                          0.10557280900008403])
        self.assertEqual(kwa.neighbors[0], [0, 1, 3, 4])
        np.testing.assert_array_almost_equal(kwa.bandwidth,
                                             np.array([[25.], [15.], [25.],
                                                      [16.], [14.5], [25.]]))

        kweag = pysal.adaptive_kernelW(points, function='gaussian')
        self.assertEqual(
            kweag.weights[0], [0.3989422804014327, 0.26741902915776961,
                               0.24197074871621341])
        np.testing.assert_array_almost_equal(kweag.bandwidth,
                                             np.array([[11.18034101],
                                                       [11.18034101],
                                                       [20.000002],
                                                       [11.18034101],
                                                       [14.14213704],
                                                       [18.02775818]]))

    def test_adaptive_kernelW_from_shapefile(self):
        kwa = pysal.adaptive_kernelW_from_shapefile(
            pysal.examples.get_path('columbus.shp'))
        self.assertEquals(kwa.weights[0], [1.0, 0.03178906767736345,
                                           9.9999990066379496e-08])
        np.testing.assert_array_almost_equal(kwa.bandwidth[:3],
                                             np.array([[0.59871832],
                                                       [0.59871832],
                                                       [0.56095647]]))

    def test_build_lattice_shapefile(self):
        of = "lattice.shp"
        pysal.build_lattice_shapefile(20, 20, of)
        w = pysal.rook_from_shapefile(of)
        self.assertEquals(w.n, 400)
        os.remove('lattice.shp')
        os.remove('lattice.shx')


suite = unittest.TestLoader().loadTestsFromTestCase(Testuser)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_util
"""Unit test for util.py"""
import pysal
from pysal.common import *
import pysal.weights
import numpy as np
from scipy import sparse, float32
from scipy.spatial import KDTree
import os
import gc


class Testutil(unittest.TestCase):
    def setUp(self):
        self.w = pysal.rook_from_shapefile(
            pysal.examples.get_path('10740.shp'))

    def test_lat2W(self):
        w9 = pysal.lat2W(3, 3)
        self.assertEquals(w9.pct_nonzero, 0.29629629629629628)
        self.assertEquals(w9[0], {1: 1.0, 3: 1.0})
        self.assertEquals(w9[3], {0: 1.0, 4: 1.0, 6: 1.0})

    def test_lat2SW(self):
        w9 = pysal.weights.lat2SW(3, 3)
        rows, cols = w9.shape
        n = rows * cols
        pct_nonzero = w9.nnz / float(n)
        self.assertEquals(pct_nonzero, 0.29629629629629628)
        data = w9.todense().tolist()
        self.assertEquals(data[0], [0, 1, 0, 1, 0, 0, 0, 0, 0])
        self.assertEquals(data[1], [1, 0, 1, 0, 1, 0, 0, 0, 0])
        self.assertEquals(data[2], [0, 1, 0, 0, 0, 1, 0, 0, 0])
        self.assertEquals(data[3], [1, 0, 0, 0, 1, 0, 1, 0, 0])
        self.assertEquals(data[4], [0, 1, 0, 1, 0, 1, 0, 1, 0])
        self.assertEquals(data[5], [0, 0, 1, 0, 1, 0, 0, 0, 1])
        self.assertEquals(data[6], [0, 0, 0, 1, 0, 0, 0, 1, 0])
        self.assertEquals(data[7], [0, 0, 0, 0, 1, 0, 1, 0, 1])
        self.assertEquals(data[8], [0, 0, 0, 0, 0, 1, 0, 1, 0])

    def test_regime_weights(self):
        regimes = np.ones(25)
        regimes[range(10, 20)] = 2
        regimes[range(21, 25)] = 3
        regimes = np.array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
                            2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 3., 3.,
                            3., 3.])
        w = pysal.regime_weights(regimes)
        ww0 = [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
        self.assertEquals(w.weights[0], ww0)
        wn0 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 20]
        self.assertEquals(w.neighbors[0], wn0)
        regimes = ['n', 'n', 's', 's', 'e', 'e', 'w', 'w', 'e']
        n = len(regimes)
        w = pysal.regime_weights(regimes)
        wn = {0: [1], 1: [0], 2: [3], 3: [2], 4: [5, 8], 5: [4, 8],
              6: [7], 7: [6], 8: [4, 5]}
        self.assertEquals(w.neighbors, wn)

    def test_comb(self):
        x = range(4)
        l = []
        for i in pysal.comb(x, 2):
            l.append(i)
        lo = [[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]
        self.assertEquals(l, lo)

    def test_order(self):
        w3 = pysal.order(self.w, kmax=3)
        w3105 = [1, -1, 1, 2, 1]
        self.assertEquals(w3105, w3[1][0:5])

    def test_higher_order(self):
        w10 = pysal.lat2W(10, 10)
        w10_2 = pysal.higher_order(w10, 2)
        w10_20 = {2: 1.0, 11: 1.0, 20: 1.0}
        self.assertEquals(w10_20, w10_2[0])
        w5 = pysal.lat2W()
        w50 = {1: 1.0, 5: 1.0}
        self.assertEquals(w50, w5[0])
        w51 = {0: 1.0, 2: 1.0, 6: 1.0}
        self.assertEquals(w51, w5[1])
        w5_2 = pysal.higher_order(w5, 2)
        w5_20 = {2: 1.0, 10: 1.0, 6: 1.0}
        self.assertEquals(w5_20, w5_2[0])

    def test_shimbel(self):
        w5 = pysal.lat2W()
        w5_shimbel = pysal.shimbel(w5)
        w5_shimbel024 = 8
        self.assertEquals(w5_shimbel024, w5_shimbel[0][24])
        w5_shimbel004 = [-1, 1, 2, 3]
        self.assertEquals(w5_shimbel004, w5_shimbel[0][0:4])

    def test_full(self):
        neighbors = {'first': ['second'], 'second': ['first',
                                                     'third'], 'third': ['second']}
        weights = {'first': [1], 'second': [1, 1], 'third': [1]}
        w = pysal.W(neighbors, weights)
        wf, ids = pysal.full(w)
        wfo = np.array([[0., 1., 0.], [1., 0., 1.], [0., 1., 0.]])
        np.testing.assert_array_almost_equal(wfo, wf, decimal=8)
        idso = ['first', 'second', 'third']
        self.assertEquals(idso, ids)

    def test_full2W(self):
        a = np.zeros((4, 4))
        for i in range(len(a)):
            for j in range(len(a[i])):
                if i != j:
                    a[i, j] = np.random.random(1)
        w = pysal.weights.util.full2W(a)
        np.testing.assert_array_equal(w.full()[0], a)
        ids = ['myID0', 'myID1', 'myID2', 'myID3']
        w = pysal.weights.util.full2W(a, ids=ids)
        np.testing.assert_array_equal(w.full()[0], a)
        w.full()[0] == a

    def test_WSP2W(self):
        sp = pysal.weights.lat2SW(2, 5)
        wsp = pysal.weights.WSP(sp)
        w = pysal.weights.WSP2W(wsp)
        self.assertEquals(w.n, 10)
        self.assertEquals(w[0], {1: 1, 5: 1})
        w = pysal.open(pysal.examples.get_path('sids2.gal'), 'r').read()
        wsp = pysal.weights.WSP(w.sparse, w.id_order)
        w = pysal.weights.WSP2W(wsp)
        self.assertEquals(w.n, 100)
        self.assertEquals(w['37135'], {'37001': 1.0, '37033': 1.0,
                                       '37037': 1.0, '37063': 1.0, '37145': 1.0})

    def test_insert_diagonal(self):
        w1 = pysal.weights.insert_diagonal(self.w)
        r1 = {0: 1.0, 1: 1.0, 4: 1.0, 101: 1.0, 85: 1.0, 5: 1.0}
        self.assertEquals(w1[0], r1)
        w1 = pysal.weights.insert_diagonal(self.w, 20)
        r1 = {0: 20, 1: 1.0, 4: 1.0, 101: 1.0, 85: 1.0, 5: 1.0}
        self.assertEquals(w1[0], r1)
        diag = np.arange(100, 100 + self.w.n)
        w1 = pysal.weights.insert_diagonal(self.w, diag)
        r1 = {0: 100, 1: 1.0, 4: 1.0, 101: 1.0, 85: 1.0, 5: 1.0}
        self.assertEquals(w1[0], r1)

    def test_remap_ids(self):
        w = pysal.lat2W(3, 2)
        wid_order = [0, 1, 2, 3, 4, 5]
        self.assertEquals(wid_order, w.id_order)
        wneighbors0 = [2, 1]
        self.assertEquals(wneighbors0, w.neighbors[0])
        old_to_new = {0: 'a', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f'}
        w_new = pysal.remap_ids(w, old_to_new)
        w_newid_order = ['a', 'b', 'c', 'd', 'e', 'f']
        self.assertEquals(w_newid_order, w_new.id_order)
        w_newdneighborsa = ['c', 'b']
        self.assertEquals(w_newdneighborsa, w_new.neighbors['a'])

    def test_get_ids(self):
        polyids = pysal.weights.util.get_ids(
            pysal.examples.get_path('columbus.shp'), "POLYID")
        polyids5 = [1, 2, 3, 4, 5]
        self.assertEquals(polyids5, polyids[:5])

    def test_get_points_array_from_shapefile(self):
        xy = pysal.weights.util.get_points_array_from_shapefile(
            pysal.examples.get_path('juvenile.shp'))
        xy3 = np.array([[94., 93.], [80., 95.], [79., 90.]])
        np.testing.assert_array_almost_equal(xy3, xy[:3], decimal=8)
        xy = pysal.weights.util.get_points_array_from_shapefile(
            pysal.examples.get_path('columbus.shp'))
        xy3 = np.array([[8.82721847, 14.36907602], [8.33265837,
                                                    14.03162401], [9.01226541, 13.81971908]])
        np.testing.assert_array_almost_equal(xy3, xy[:3], decimal=8)

    def test_min_threshold_distance(self):
        x, y = np.indices((5, 5))
        x.shape = (25, 1)
        y.shape = (25, 1)
        data = np.hstack([x, y])
        mint = 1.0
        self.assertEquals(
            mint, pysal.weights.util.min_threshold_distance(data))

suite = unittest.TestLoader().loadTestsFromTestCase(Testutil)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test_weights
import unittest
import pysal
import numpy as np

NPTA3E = np.testing.assert_array_almost_equal


class TestW(unittest.TestCase):
    def setUp(self):
        from pysal import rook_from_shapefile
        self.w = rook_from_shapefile(pysal.examples.get_path('10740.shp'))

        self.neighbors = {0: [3, 1], 1: [0, 4, 2], 2: [1, 5], 3: [0, 6, 4], 4: [1, 3,
                                                                                7, 5], 5: [2, 4, 8], 6: [3, 7], 7: [4, 6, 8], 8: [5, 7]}
        self.weights = {0: [1, 1], 1: [1, 1, 1], 2: [1, 1], 3: [1, 1, 1], 4: [1, 1,
                                                                              1, 1], 5: [1, 1, 1], 6: [1, 1], 7: [1, 1, 1], 8: [1, 1]}

        self.w3x3 = pysal.lat2W(3, 3)

    def test_W(self):
        w = pysal.W(self.neighbors, self.weights)
        self.assertEqual(w.pct_nonzero, 0.29629629629629628)

    def test___getitem__(self):
        self.assertEqual(
            self.w[0], {1: 1.0, 4: 1.0, 101: 1.0, 85: 1.0, 5: 1.0})

    def test___init__(self):
        w = pysal.W(self.neighbors, self.weights)
        self.assertEqual(w.pct_nonzero, 0.29629629629629628)

    def test___iter__(self):
        w = pysal.lat2W(3, 3)
        res = {}
        for i, wi in enumerate(w):
            res[i] = wi
        self.assertEqual(res[0], {1: 1.0, 3: 1.0})
        self.assertEqual(res[8], {5: 1.0, 7: 1.0})

    def test_asymmetries(self):
        w = pysal.lat2W(3, 3)
        w.transform = 'r'
        result = w.asymmetry()
        self.assertEqual(result, [(0, 1), (0, 3), (1, 0), (1, 2), (1, 4), (2, 1), (2, 5), (3, 0), (3, 4), (3, 6), (4, 1), (4, 3), (4, 5), (4, 7), (5, 2), (5, 4), (5, 8), (6, 3), (6, 7), (7, 4), (7, 6), (7, 8), (8, 5), (8, 7)])

    def test_asymmetry(self):
        w = pysal.lat2W(3, 3)
        self.assertEqual(w.asymmetry(), [])
        w.transform = 'r'
        self.assertFalse(w.asymmetry() == [])

    def test_cardinalities(self):
        w = pysal.lat2W(3, 3)
        self.assertEqual(w.cardinalities, {0: 2, 1: 3, 2: 2, 3: 3, 4: 4, 5: 3,
                                           6: 2, 7: 3, 8: 2})

    def test_diagW2(self):
        NPTA3E(self.w3x3.diagW2, np.array([2., 3., 2., 3., 4., 3., 2.,
                                           3., 2.]))

    def test_diagWtW(self):
        NPTA3E(self.w3x3.diagW2, np.array([2., 3., 2., 3., 4., 3., 2.,
                                           3., 2.]))

    def test_diagWtW_WW(self):
        NPTA3E(self.w3x3.diagWtW_WW, np.array([4., 6., 4., 6., 8.,
                                               6., 4., 6., 4.]))

    def test_full(self):
        wf = np.array([[0., 1., 0., 1., 0., 0., 0., 0., 0.],
                       [1., 0., 1., 0., 1., 0., 0., 0., 0.],
                       [0., 1., 0., 0., 0., 1., 0., 0., 0.],
                       [1., 0., 0., 0., 1., 0., 1., 0., 0.],
                       [0., 1., 0., 1., 0., 1., 0., 1., 0.],
                       [0., 0., 1., 0., 1., 0., 0., 0., 1.],
                       [0., 0., 0., 1., 0., 0., 0., 1., 0.],
                       [0., 0., 0., 0., 1., 0., 1., 0., 1.],
                       [0., 0., 0., 0., 0., 1., 0., 1., 0.]])
        ids = range(9)

        wf1, ids1 = self.w3x3.full()
        NPTA3E(wf1, wf)
        self.assertEqual(ids1, ids)

    def test_get_transform(self):
        self.assertEqual(self.w3x3.transform, 'O')
        self.w3x3.transform = 'r'
        self.assertEqual(self.w3x3.transform, 'R')
        self.w3x3.transform = 'b'

    def test_higher_order(self):
        weights = {0: [1.0, 1.0, 1.0], 1: [1.0, 1.0, 1.0], 2: [1.0, 1.0, 1.0], 3: [1.0, 1.0,
                                                                                   1.0], 4: [1.0, 1.0, 1.0, 1.0], 5: [1.0, 1.0, 1.0], 6: [1.0, 1.0, 1.0], 7:
                   [1.0, 1.0, 1.0], 8: [1.0, 1.0, 1.0]}
        neighbors = {0: [2, 4, 6], 1: [3, 5, 7], 2: [0, 4, 8], 3: [1, 5, 7],
                     4: [0, 2, 6, 8], 5: [1, 3, 7], 6: [0, 4, 8], 7: [1, 3, 5], 8:
                     [2, 4, 6]}
        w2 = pysal.higher_order(self.w3x3, 2)
        self.assertEqual(w2.neighbors, neighbors)
        self.assertEqual(w2.weights, weights)

    def test_histogram(self):
        hist = [(0, 1), (1, 1), (2, 4), (3, 20), (4, 57), (5, 44), (6, 36),
                (7, 15), (8, 7), (9, 1), (10, 6), (11, 0), (12, 2), (13, 0),
                (14, 0), (15, 1)]
        self.assertEqual(self.w.histogram, hist)

    def test_id2i(self):
        id2i = {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8}
        self.assertEqual(self.w3x3.id2i, id2i)

    def test_id_order_set(self):
        w = pysal.W(neighbors={'a': ['b'], 'b': ['a', 'c'], 'c': ['b']})
        self.assertFalse(w.id_order_set)

    def test_islands(self):
        w = pysal.W(neighbors={'a': ['b'], 'b': ['a', 'c'], 'c':
                               ['b'], 'd': []})
        self.assertEqual(w.islands, ['d'])
        self.assertEqual(self.w3x3.islands, [])

    def test_max_neighbors(self):
        w = pysal.W(neighbors={'a': ['b'], 'b': ['a', 'c'], 'c':
                               ['b'], 'd': []})
        self.assertEqual(w.max_neighbors, 2)
        self.assertEqual(self.w3x3.max_neighbors, 4)

    def test_mean_neighbors(self):
        w = pysal.lat2W()
        self.assertEqual(w.mean_neighbors, 3.2)

    def test_min_neighbors(self):
        w = pysal.lat2W()
        self.assertEqual(w.min_neighbors, 2)

    def test_n(self):
        w = pysal.lat2W()
        self.assertEqual(w.n, 25)

    def test_neighbor_offsets(self):
        d = {0: [3, 1],
             1: [0, 4, 2],
             2: [1, 5],
             3: [0, 6, 4],
             4: [1, 3, 7, 5],
             5: [2, 4, 8],
             6: [3, 7],
             7: [4, 6, 8],
             8: [5, 7]}

        self.assertEqual(self.w3x3.neighbor_offsets, d)

    def test_nonzero(self):
        self.assertEquals(self.w3x3.nonzero, 24)

    def test_order(self):
        w = pysal.lat2W(3, 3)
        o = {0: [-1, 1, 2, 1, 2, 3, 2, 3, 0],
             1: [1, -1, 1, 2, 1, 2, 3, 2, 3],
             2: [2, 1, -1, 3, 2, 1, 0, 3, 2],
             3: [1, 2, 3, -1, 1, 2, 1, 2, 3],
             4: [2, 1, 2, 1, -1, 1, 2, 1, 2],
             5: [3, 2, 1, 2, 1, -1, 3, 2, 1],
             6: [2, 3, 0, 1, 2, 3, -1, 1, 2],
             7: [3, 2, 3, 2, 1, 2, 1, -1, 1],
             8: [0, 3, 2, 3, 2, 1, 2, 1, -1]}
        self.assertEquals(pysal.order(w), o)

    def test_pct_nonzero(self):
        self.assertEqual(self.w3x3.pct_nonzero, 0.29629629629629628)

    def test_s0(self):
        self.assertEqual(self.w3x3.s0, 24.0)

    def test_s1(self):
        self.assertEqual(self.w3x3.s1, 48.0)

    def test_s2(self):
        self.assertEqual(self.w3x3.s2, 272.0)

    def test_s2array(self):
        s2a = np.array([[16.], [36.], [16.], [36.],
                        [64.], [36.], [16.], [36.], [16.]])
        NPTA3E(self.w3x3.s2array, s2a)

    def test_sd(self):
        self.assertEquals(self.w3x3.sd, 0.66666666666666663)

    def test_set_transform(self):
        w = pysal.lat2W(2, 2)
        self.assertEqual(w.transform, 'O')
        self.assertEquals(w.weights[0], [1.0, 1.0])
        w.transform = 'r'
        self.assertEquals(w.weights[0], [0.5, 0.5])

    def test_shimbel(self):
        d = {0: [-1, 1, 2, 1, 2, 3, 2, 3, 4],
             1: [1, -1, 1, 2, 1, 2, 3, 2, 3],
             2: [2, 1, -1, 3, 2, 1, 4, 3, 2],
             3: [1, 2, 3, -1, 1, 2, 1, 2, 3],
             4: [2, 1, 2, 1, -1, 1, 2, 1, 2],
             5: [3, 2, 1, 2, 1, -1, 3, 2, 1],
             6: [2, 3, 4, 1, 2, 3, -1, 1, 2],
             7: [3, 2, 3, 2, 1, 2, 1, -1, 1],
             8: [4, 3, 2, 3, 2, 1, 2, 1, -1]}
        self.assertEquals(pysal.shimbel(self.w3x3), d)

    def test_sparse(self):
        self.assertEqual(self.w3x3.sparse.nnz, 24)

    def test_trcW2(self):
        self.assertEqual(self.w3x3.trcW2, 24.)

    def test_trcWtW(self):
        self.assertEqual(self.w3x3.trcWtW, 24.)

    def test_trcWtW_WW(self):
        self.assertEqual(self.w3x3.trcWtW_WW, 48.)


class Test_WSP_Back_To_W(unittest.TestCase):
    # Test to make sure we get back to the same W functionality
    def setUp(self):
        from pysal import rook_from_shapefile
        self.w = rook_from_shapefile(pysal.examples.get_path('10740.shp'))
        wsp = pysal.weights.WSP(self.w.sparse, self.w.id_order)
        self.w = pysal.weights.WSP2W(wsp)

        self.neighbors = {0: [3, 1], 1: [0, 4, 2], 2: [1, 5], 3: [0, 6, 4], 4: [1, 3,
                                                                                7, 5], 5: [2, 4, 8], 6: [3, 7], 7: [4, 6, 8], 8: [5, 7]}
        self.weights = {0: [1, 1], 1: [1, 1, 1], 2: [1, 1], 3: [1, 1, 1], 4: [1, 1,
                                                                              1, 1], 5: [1, 1, 1], 6: [1, 1], 7: [1, 1, 1], 8: [1, 1]}

        self.w3x3 = pysal.lat2W(3, 3)
        w3x3 = pysal.weights.WSP(self.w3x3.sparse, self.w3x3.id_order)
        self.w3x3 = pysal.weights.WSP2W(w3x3)

    def test_W(self):
        w = pysal.W(self.neighbors, self.weights)
        self.assertEqual(w.pct_nonzero, 0.29629629629629628)

    def test___getitem__(self):
        self.assertEqual(
            self.w[0], {1: 1.0, 4: 1.0, 101: 1.0, 85: 1.0, 5: 1.0})

    def test___init__(self):
        w = pysal.W(self.neighbors, self.weights)
        self.assertEqual(w.pct_nonzero, 0.29629629629629628)

    def test___iter__(self):
        w = pysal.lat2W(3, 3)
        res = {}
        for i, wi in enumerate(w):
            res[i] = wi
        self.assertEqual(res[0], {1: 1.0, 3: 1.0})
        self.assertEqual(res[8], {5: 1.0, 7: 1.0})

    def test_asymmetries(self):
        w = pysal.lat2W(3, 3)
        w.transform = 'r'
        result = w.asymmetry()
        self.assertEqual(result, [(0, 1), (0, 3), (1, 0), (1, 2), (1, 4), (2, 1), (2, 5), (3, 0), (3, 4), (3, 6), (4, 1), (4, 3), (4, 5), (4, 7), (5, 2), (5, 4), (5, 8), (6, 3), (6, 7), (7, 4), (7, 6), (7, 8), (8, 5), (8, 7)])
    def test_asymmetry(self):
        w = pysal.lat2W(3, 3)
        self.assertEqual(w.asymmetry(), [])
        w.transform = 'r'
        self.assertFalse(w.asymmetry() == [])

    def test_cardinalities(self):
        w = pysal.lat2W(3, 3)
        self.assertEqual(w.cardinalities, {0: 2, 1: 3, 2: 2, 3: 3, 4: 4, 5: 3,
                                           6: 2, 7: 3, 8: 2})

    def test_diagW2(self):
        NPTA3E(self.w3x3.diagW2, np.array([2., 3., 2., 3., 4., 3., 2.,
                                           3., 2.]))

    def test_diagWtW(self):
        NPTA3E(self.w3x3.diagW2, np.array([2., 3., 2., 3., 4., 3., 2.,
                                           3., 2.]))

    def test_diagWtW_WW(self):
        NPTA3E(self.w3x3.diagWtW_WW, np.array([4., 6., 4., 6., 8.,
                                               6., 4., 6., 4.]))

    def test_full(self):
        wf = np.array([[0., 1., 0., 1., 0., 0., 0., 0., 0.],
                       [1., 0., 1., 0., 1., 0., 0., 0., 0.],
                       [0., 1., 0., 0., 0., 1., 0., 0., 0.],
                       [1., 0., 0., 0., 1., 0., 1., 0., 0.],
                       [0., 1., 0., 1., 0., 1., 0., 1., 0.],
                       [0., 0., 1., 0., 1., 0., 0., 0., 1.],
                       [0., 0., 0., 1., 0., 0., 0., 1., 0.],
                       [0., 0., 0., 0., 1., 0., 1., 0., 1.],
                       [0., 0., 0., 0., 0., 1., 0., 1., 0.]])
        ids = range(9)

        wf1, ids1 = self.w3x3.full()
        NPTA3E(wf1, wf)
        self.assertEqual(ids1, ids)

    def test_get_transform(self):
        self.assertEqual(self.w3x3.transform, 'O')
        self.w3x3.transform = 'r'
        self.assertEqual(self.w3x3.transform, 'R')
        self.w3x3.transform = 'b'

    def test_higher_order(self):
        weights = {0: [1.0, 1.0, 1.0], 1: [1.0, 1.0, 1.0], 2: [1.0, 1.0, 1.0], 3: [1.0, 1.0,
                                                                                   1.0], 4: [1.0, 1.0, 1.0, 1.0], 5: [1.0, 1.0, 1.0], 6: [1.0, 1.0, 1.0], 7:
                   [1.0, 1.0, 1.0], 8: [1.0, 1.0, 1.0]}
        neighbors = {0: [2, 4, 6], 1: [3, 5, 7], 2: [0, 4, 8], 3: [1, 5, 7],
                     4: [0, 2, 6, 8], 5: [1, 3, 7], 6: [0, 4, 8], 7: [1, 3, 5], 8:
                     [2, 4, 6]}
        w2 = pysal.higher_order(self.w3x3, 2)
        self.assertEqual(w2.neighbors, neighbors)
        self.assertEqual(w2.weights, weights)

    def test_histogram(self):
        hist = [(0, 1), (1, 1), (2, 4), (3, 20), (4, 57), (5, 44), (6, 36),
                (7, 15), (8, 7), (9, 1), (10, 6), (11, 0), (12, 2), (13, 0),
                (14, 0), (15, 1)]
        self.assertEqual(self.w.histogram, hist)

    def test_id2i(self):
        id2i = {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8}
        self.assertEqual(self.w3x3.id2i, id2i)

    def test_id_order_set(self):
        w = pysal.W(neighbors={'a': ['b'], 'b': ['a', 'c'], 'c': ['b']})
        self.assertFalse(w.id_order_set)

    def test_islands(self):
        w = pysal.W(neighbors={'a': ['b'], 'b': ['a', 'c'], 'c':
                               ['b'], 'd': []})
        self.assertEqual(w.islands, ['d'])
        self.assertEqual(self.w3x3.islands, [])

    def test_max_neighbors(self):
        w = pysal.W(neighbors={'a': ['b'], 'b': ['a', 'c'], 'c':
                               ['b'], 'd': []})
        self.assertEqual(w.max_neighbors, 2)
        self.assertEqual(self.w3x3.max_neighbors, 4)

    def test_mean_neighbors(self):
        w = pysal.lat2W()
        self.assertEqual(w.mean_neighbors, 3.2)

    def test_min_neighbors(self):
        w = pysal.lat2W()
        self.assertEqual(w.min_neighbors, 2)

    def test_n(self):
        w = pysal.lat2W()
        self.assertEqual(w.n, 25)

    def test_nonzero(self):
        self.assertEquals(self.w3x3.nonzero, 24)

    def test_order(self):
        w = pysal.lat2W(3, 3)
        o = {0: [-1, 1, 2, 1, 2, 3, 2, 3, 0],
             1: [1, -1, 1, 2, 1, 2, 3, 2, 3],
             2: [2, 1, -1, 3, 2, 1, 0, 3, 2],
             3: [1, 2, 3, -1, 1, 2, 1, 2, 3],
             4: [2, 1, 2, 1, -1, 1, 2, 1, 2],
             5: [3, 2, 1, 2, 1, -1, 3, 2, 1],
             6: [2, 3, 0, 1, 2, 3, -1, 1, 2],
             7: [3, 2, 3, 2, 1, 2, 1, -1, 1],
             8: [0, 3, 2, 3, 2, 1, 2, 1, -1]}
        self.assertEquals(pysal.order(w), o)

    def test_pct_nonzero(self):
        self.assertEqual(self.w3x3.pct_nonzero, 0.29629629629629628)

    def test_s0(self):
        self.assertEqual(self.w3x3.s0, 24.0)

    def test_s1(self):
        self.assertEqual(self.w3x3.s1, 48.0)

    def test_s2(self):
        self.assertEqual(self.w3x3.s2, 272.0)

    def test_s2array(self):
        s2a = np.array([[16.], [36.], [16.], [36.],
                        [64.], [36.], [16.], [36.], [16.]])
        NPTA3E(self.w3x3.s2array, s2a)

    def test_sd(self):
        self.assertEquals(self.w3x3.sd, 0.66666666666666663)

    def test_set_transform(self):
        w = pysal.lat2W(2, 2)
        self.assertEqual(w.transform, 'O')
        self.assertEquals(w.weights[0], [1.0, 1.0])
        w.transform = 'r'
        self.assertEquals(w.weights[0], [0.5, 0.5])

    def test_shimbel(self):
        d = {0: [-1, 1, 2, 1, 2, 3, 2, 3, 4],
             1: [1, -1, 1, 2, 1, 2, 3, 2, 3],
             2: [2, 1, -1, 3, 2, 1, 4, 3, 2],
             3: [1, 2, 3, -1, 1, 2, 1, 2, 3],
             4: [2, 1, 2, 1, -1, 1, 2, 1, 2],
             5: [3, 2, 1, 2, 1, -1, 3, 2, 1],
             6: [2, 3, 4, 1, 2, 3, -1, 1, 2],
             7: [3, 2, 3, 2, 1, 2, 1, -1, 1],
             8: [4, 3, 2, 3, 2, 1, 2, 1, -1]}
        self.assertEquals(pysal.shimbel(self.w3x3), d)

    def test_sparse(self):
        self.assertEqual(self.w3x3.sparse.nnz, 24)

    def test_trcW2(self):
        self.assertEqual(self.w3x3.trcW2, 24.)

    def test_trcWtW(self):
        self.assertEqual(self.w3x3.trcWtW, 24.)

    def test_trcWtW_WW(self):
        self.assertEqual(self.w3x3.trcWtW_WW, 48.)


class TestWSP(unittest.TestCase):
    def setUp(self):
        from pysal import rook_from_shapefile
        self.w = pysal.open(pysal.examples.get_path("sids2.gal")).read()
        self.wsp = pysal.weights.WSP(self.w.sparse, self.w.id_order)
        w3x3 = pysal.lat2W(3, 3)
        self.w3x3 = pysal.weights.WSP(w3x3.sparse)

    def test_WSP(self):
        self.assertEquals(self.w.id_order, self.wsp.id_order)
        self.assertEquals(self.w.n, self.wsp.n)
        np.testing.assert_array_equal(
            self.w.sparse.todense(), self.wsp.sparse.todense())

    def test_diagWtW_WW(self):
        NPTA3E(self.w3x3.diagWtW_WW, np.array([4., 6., 4., 6., 8.,
                                               6., 4., 6., 4.]))

    def test_trcWtW_WW(self):
        self.assertEqual(self.w3x3.trcWtW_WW, 48.)

    def test_s0(self):
        self.assertEqual(self.w3x3.s0, 24.0)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_Wsets
"""Unit test for Wsets module."""
import unittest
import pysal


class TestWsets(unittest.TestCase):
    """Unit test for Wsets module."""

    def test_w_union(self):
        """Unit test"""
        w1 = pysal.lat2W(4, 4)
        w2 = pysal.lat2W(6, 4)
        w3 = pysal.weights.Wsets.w_union(w1, w2)
        self.assertEqual(w1[0], w3[0])
        self.assertEqual(w1.neighbors[15], [11, 14])
        self.assertEqual(w2.neighbors[15], [11, 14, 19])
        self.assertEqual(w3.neighbors[15], [19, 11, 14])

    def test_w_intersection(self):
        """Unit test"""
        w1 = pysal.lat2W(4, 4)
        w2 = pysal.lat2W(6, 4)
        w3 = pysal.weights.Wsets.w_union(w1, w2)
        self.assertEqual(w1[0], w3[0])
        self.assertEqual(w1.neighbors[15], [11, 14])
        self.assertEqual(w2.neighbors[15], [11, 14, 19])
        self.assertEqual(w3.neighbors[15], [19, 11, 14])

    def test_w_difference(self):
        """Unit test"""
        w1 = pysal.lat2W(4, 4, rook=False)
        w2 = pysal.lat2W(4, 4, rook=True)
        w3 = pysal.weights.Wsets.w_difference(w1, w2, constrained=False)
        self.assertNotEqual(w1[0], w3[0])
        self.assertEqual(w1.neighbors[15], [10, 11, 14])
        self.assertEqual(w2.neighbors[15], [11, 14])
        self.assertEqual(w3.neighbors[15], [10])

    def test_w_symmetric_difference(self):
        """Unit test"""
        w1 = pysal.lat2W(4, 4, rook=False)
        w2 = pysal.lat2W(6, 4, rook=True)
        w3 = pysal.weights.Wsets.w_symmetric_difference(
            w1, w2, constrained=False)
        self.assertNotEqual(w1[0], w3[0])
        self.assertEqual(w1.neighbors[15], [10, 11, 14])
        self.assertEqual(w2.neighbors[15], [11, 14, 19])
        self.assertEqual(w3.neighbors[15], [10, 19])

    def test_w_subset(self):
        """Unit test"""
        w1 = pysal.lat2W(6, 4)
        ids = range(16)
        w2 = pysal.weights.Wsets.w_subset(w1, ids)
        self.assertEqual(w1[0], w2[0])
        self.assertEqual(w1.neighbors[15], [11, 14, 19])
        self.assertEqual(w2.neighbors[15], [11, 14])


suite = unittest.TestLoader().loadTestsFromTestCase(TestWsets)

if __name__ == '__main__':
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = test__contW_binning
import os
import unittest
import pysal
from pysal.weights._contW_binning import ContiguityWeights_binning, QUEEN, ROOK


class TestContiguityWeights(unittest.TestCase):
    def setUp(self):
        """ Setup the binning contiguity weights"""
        shpObj = pysal.open(pysal.examples.get_path('virginia.shp'), 'r')
        self.binningW = ContiguityWeights_binning(shpObj, QUEEN)
        shpObj.close()

    def test_w_type(self):
        self.assert_(isinstance(self.binningW, ContiguityWeights_binning))

    def test_QUEEN(self):
        self.assertEqual(QUEEN, 1)

    def test_ROOK(self):
        self.assertEqual(ROOK, 2)

    def test_ContiguityWeights_binning(self):
        self.assert_(hasattr(self.binningW, 'w'))
        self.assert_(issubclass(dict, type(self.binningW.w)))
        self.assertEqual(len(self.binningW.w), 136)

    def test_nested_polygons(self):
        # load queen gal file created using Open Geoda.
        geodaW = pysal.open(
            pysal.examples.get_path('virginia.gal'), 'r').read()
        # build matching W with pysal
        pysalWb = self.build_W(
            pysal.examples.get_path('virginia.shp'), QUEEN, 'POLY_ID')
        # compare output.
        for key in geodaW.neighbors:
            geoda_neighbors = map(int, geodaW.neighbors[key])
            pysalb_neighbors = pysalWb.neighbors[int(key)]
            geoda_neighbors.sort()
            pysalb_neighbors.sort()
            self.assertEqual(geoda_neighbors, pysalb_neighbors)

    def test_true_rook(self):
        # load queen gal file created using Open Geoda.
        geodaW = pysal.open(pysal.examples.get_path('rook31.gal'), 'r').read()
        # build matching W with pysal
        #pysalW = pysal.rook_from_shapefile(pysal.examples.get_path('rook31.shp'),','POLY_ID')
        pysalWb = self.build_W(
            pysal.examples.get_path('rook31.shp'), ROOK, 'POLY_ID')
        # compare output.
        for key in geodaW.neighbors:
            geoda_neighbors = map(int, geodaW.neighbors[key])
            pysalb_neighbors = pysalWb.neighbors[int(key)]
            geoda_neighbors.sort()
            pysalb_neighbors.sort()
            self.assertEqual(geoda_neighbors, pysalb_neighbors)

    def test_true_rook2(self):
        # load queen gal file created using Open Geoda.
        geodaW = pysal.open(
            pysal.examples.get_path('stl_hom_rook.gal'), 'r').read()
        # build matching W with pysal
        pysalWb = self.build_W(pysal.examples.get_path(
            'stl_hom.shp'), ROOK, 'POLY_ID_OG')
        # compare output.
        for key in geodaW.neighbors:
            geoda_neighbors = map(int, geodaW.neighbors[key])
            pysalb_neighbors = pysalWb.neighbors[int(key)]
            geoda_neighbors.sort()
            pysalb_neighbors.sort()
            self.assertEqual(geoda_neighbors, pysalb_neighbors)

    def test_true_rook3(self):
        # load queen gal file created using Open Geoda.
        geodaW = pysal.open(
            pysal.examples.get_path('sacramentot2.gal'), 'r').read()
        # build matching W with pysal
        pysalWb = self.build_W(pysal.examples.get_path(
            'sacramentot2.shp'), ROOK, 'POLYID')
        # compare output.
        for key in geodaW.neighbors:
            geoda_neighbors = map(int, geodaW.neighbors[key])
            pysalb_neighbors = pysalWb.neighbors[int(key)]
            geoda_neighbors.sort()
            pysalb_neighbors.sort()
            self.assertEqual(geoda_neighbors, pysalb_neighbors)

    def test_true_rook4(self):
        # load queen gal file created using Open Geoda.
        geodaW = pysal.open(
            pysal.examples.get_path('virginia_rook.gal'), 'r').read()
        # build matching W with pysal
        pysalWb = self.build_W(
            pysal.examples.get_path('virginia.shp'), ROOK, 'POLY_ID')
        # compare output.
        for key in geodaW.neighbors:
            geoda_neighbors = map(int, geodaW.neighbors[key])
            pysalb_neighbors = pysalWb.neighbors[int(key)]
            geoda_neighbors.sort()
            pysalb_neighbors.sort()
            self.assertEqual(geoda_neighbors, pysalb_neighbors)

    def build_W(self, shapefile, type, idVariable=None):
        """ Building 2 W's the hard way.  We need to do this so we can test both rtree and binning """
        dbname = os.path.splitext(shapefile)[0] + '.dbf'
        db = pysal.open(dbname)
        shpObj = pysal.open(shapefile)
        neighbor_data = ContiguityWeights_binning(shpObj, type).w
        neighbors = {}
        weights = {}
        if idVariable:
            ids = db.by_col[idVariable]
            self.assertEqual(len(ids), len(set(ids)))
            for key in neighbor_data:
                id = ids[key]
                if id not in neighbors:
                    neighbors[id] = set()
                neighbors[id].update([ids[x] for x in neighbor_data[key]])
            for key in neighbors:
                neighbors[key] = list(neighbors[key])
            binningW = pysal.W(neighbors, id_order=ids)
        else:
            neighbors[key] = list(neighbors[key])
            binningW = pysal.W(neighbors)
        return binningW

#suite = unittest.TestLoader().loadTestsFromTestCase(_TestContiguityWeights)

if __name__ == '__main__':
    #runner = unittest.TextTestRunner()
    #runner.run(suite)
    unittest.main()

########NEW FILE########
__FILENAME__ = test__contW_rtree
import os
import unittest
import pysal
OK_TO_RUN = True
try:
    #import rtree
    from pysal.weights._contW_rtree import ContiguityWeights_rtree, QUEEN, ROOK
except ImportError:
    OK_TO_RUN = False
    print "Cannot test rtree contiguity weights, rtree not installed"


class TestRtreeContiguityWeights(unittest.TestCase):
    def setUp(self):
        """ Setup the rtree contiguity weights"""
        shpObj = pysal.open(pysal.examples.get_path('virginia.shp'), 'r')
        self.rtreeW = ContiguityWeights_rtree(shpObj, QUEEN)
        shpObj.close()

    def test_w_type(self):
        self.assert_(isinstance(self.rtreeW, ContiguityWeights_rtree))

    def test_QUEEN(self):
        self.assertEqual(QUEEN, 1)

    def test_ROOK(self):
        self.assertEqual(ROOK, 2)

    def test_ContiguityWeights_rtree(self):
        self.assert_(hasattr(self.rtreeW, 'w'))
        self.assert_(issubclass(dict, type(self.rtreeW.w)))
        self.assertEqual(len(self.rtreeW.w), 136)

    def test_nested_polygons(self):
        # load queen gal file created using Open Geoda.
        geodaW = pysal.open(
            pysal.examples.get_path('virginia.gal'), 'r').read()
        # build matching W with pysal
        pysalWr = self.build_W(
            pysal.examples.get_path('virginia.shp'), QUEEN, 'POLY_ID')
        # compare output.
        for key in geodaW.neighbors:
            geoda_neighbors = map(int, geodaW.neighbors[key])
            pysalr_neighbors = pysalWr.neighbors[int(key)]
            geoda_neighbors.sort()
            pysalr_neighbors.sort()
            self.assertEqual(geoda_neighbors, pysalr_neighbors)

    def test_true_rook(self):
        # load rook gal file created using Open Geoda.
        geodaW = pysal.open(pysal.examples.get_path('rook31.gal'), 'r').read()
        # build matching W with pysal
        #pysalW = pysal.rook_from_shapefile(pysal.examples.get_path('rook31.shp'),'POLY_ID')
        pysalWr = self.build_W(
            pysal.examples.get_path('rook31.shp'), ROOK, 'POLY_ID')
        # compare output.
        for key in geodaW.neighbors:
            geoda_neighbors = map(int, geodaW.neighbors[key])
            pysalr_neighbors = pysalWr.neighbors[int(key)]
            geoda_neighbors.sort()
            pysalr_neighbors.sort()
            self.assertEqual(geoda_neighbors, pysalr_neighbors)

    def test_true_rook2(self):
        # load rook gal file created using Open Geoda.
        geodaW = pysal.open(
            pysal.examples.get_path('stl_hom_rook.gal'), 'r').read()
        # build matching W with pysal
        pysalWr = self.build_W(pysal.examples.get_path(
            'stl_hom.shp'), ROOK, 'POLY_ID_OG')
        # compare output.
        for key in geodaW.neighbors:
            geoda_neighbors = map(int, geodaW.neighbors[key])
            pysalr_neighbors = pysalWr.neighbors[int(key)]
            geoda_neighbors.sort()
            pysalr_neighbors.sort()
            self.assertEqual(geoda_neighbors, pysalr_neighbors)

    def test_true_rook3(self):
        # load rook gal file created using Open Geoda.
        geodaW = pysal.open(
            pysal.examples.get_path('sacramentot2.gal'), 'r').read()
        # build matching W with pysal
        pysalWr = self.build_W(pysal.examples.get_path(
            'sacramentot2.shp'), ROOK, 'POLYID')
        # compare output.
        for key in geodaW.neighbors:
            geoda_neighbors = map(int, geodaW.neighbors[key])
            pysalr_neighbors = pysalWr.neighbors[int(key)]
            geoda_neighbors.sort()
            pysalr_neighbors.sort()
            self.assertEqual(geoda_neighbors, pysalr_neighbors)

    def test_true_rook4(self):
        # load rook gal file created using Open Geoda.
        geodaW = pysal.open(
            pysal.examples.get_path('virginia_rook.gal'), 'r').read()
        # build matching W with pysal
        pysalWr = self.build_W(
            pysal.examples.get_path('virginia.shp'), ROOK, 'POLY_ID')
        # compare output.
        for key in geodaW.neighbors:
            geoda_neighbors = map(int, geodaW.neighbors[key])
            pysalr_neighbors = pysalWr.neighbors[int(key)]
            geoda_neighbors.sort()
            pysalr_neighbors.sort()
            self.assertEqual(geoda_neighbors, pysalr_neighbors)

    def build_W(self, shapefile, type, idVariable=None):
        """ Building 2 W's the hard way.  We need to do this so we can test both rtree and binning """
        dbname = os.path.splitext(shapefile)[0] + '.dbf'
        db = pysal.open(dbname)
        shpObj = pysal.open(shapefile)
        neighbor_data = ContiguityWeights_rtree(shpObj, type).w
        neighbors = {}
        weights = {}
        if idVariable:
            ids = db.by_col[idVariable]
            self.assertEqual(len(ids), len(set(ids)))
            for key in neighbor_data:
                id = ids[key]
                if id not in neighbors:
                    neighbors[id] = set()
                neighbors[id].update([ids[x] for x in neighbor_data[key]])
            for key in neighbors:
                neighbors[key] = list(neighbors[key])
            rtreeW = pysal.W(neighbors, id_order=ids)
        else:
            neighbors[key] = list(neighbors[key])
            rtreeW = pysal.W(neighbors)
        shpObj.seek(0)
        return rtreeW

suite = unittest.TestLoader().loadTestsFromTestCase(TestRtreeContiguityWeights)

if __name__ == '__main__' and OK_TO_RUN:
    runner = unittest.TextTestRunner()
    runner.run(suite)

########NEW FILE########
__FILENAME__ = user
"""
Convenience functions for the construction of spatial weights based on
contiguity and distance criteria
"""

__author__ = "Sergio J. Rey <srey@asu.edu> "
__all__ = ['queen_from_shapefile', 'rook_from_shapefile', 'knnW_from_array', 'knnW_from_shapefile', 'threshold_binaryW_from_array', 'threshold_binaryW_from_shapefile', 'threshold_continuousW_from_array', 'threshold_continuousW_from_shapefile', 'kernelW', 'kernelW_from_shapefile', 'adaptive_kernelW', 'adaptive_kernelW_from_shapefile', 'min_threshold_dist_from_shapefile', 'build_lattice_shapefile']

import pysal
from Contiguity import buildContiguity
from Distance import knnW, Kernel, DistanceBand
from util import get_ids, get_points_array_from_shapefile, min_threshold_distance
import numpy as np

def queen_from_shapefile(shapefile, idVariable=None, sparse=False):
    """
    Queen contiguity weights from a polygon shapefile

    Parameters
    ----------

    shapefile   : string
                  name of polygon shapefile including suffix.
    idVariable  : string
                  name of a column in the shapefile's DBF to use for ids.
    sparse    : boolean
                If True return WSP instance
                If False return W instance
    Returns
    -------

    w            : W
                   instance of spatial weights

    Examples
    --------
    >>> wq=queen_from_shapefile(pysal.examples.get_path("columbus.shp"))
    >>> "%.3f"%wq.pct_nonzero
    '0.098'
    >>> wq=queen_from_shapefile(pysal.examples.get_path("columbus.shp"),"POLYID")
    >>> "%.3f"%wq.pct_nonzero
    '0.098'
    >>> wq=queen_from_shapefile(pysal.examples.get_path("columbus.shp"), sparse=True)
    >>> pct_sp = wq.sparse.nnz *1. / wq.n**2
    >>> "%.3f"%pct_sp
    '0.098'

    Notes
    -----

    Queen contiguity defines as neighbors any pair of polygons that share at
    least one vertex in their polygon definitions.

    See Also
    --------
    :class:`pysal.weights.W`

    """
    shp = pysal.open(shapefile)
    if idVariable:
        ids = get_ids(shapefile, idVariable)
    else:
        ids = None
    w = buildContiguity(shp, criterion='queen', ids=ids)
    shp.close()
    w.set_shapefile(shapefile, idVariable)

    if sparse:
        w = pysal.weights.WSP(w.sparse, id_order=ids)

    return w


def rook_from_shapefile(shapefile, idVariable=None, sparse=False):
    """
    Rook contiguity weights from a polygon shapefile

    Parameters
    ----------

    shapefile : string
                name of polygon shapefile including suffix.
    sparse    : boolean
                If True return WSP instance
                If False return W instance

    Returns
    -------

    w          : W
                 instance of spatial weights

    Examples
    --------
    >>> wr=rook_from_shapefile(pysal.examples.get_path("columbus.shp"), "POLYID")
    >>> "%.3f"%wr.pct_nonzero
    '0.083'
    >>> wr=rook_from_shapefile(pysal.examples.get_path("columbus.shp"), sparse=True)
    >>> pct_sp = wr.sparse.nnz *1. / wr.n**2
    >>> "%.3f"%pct_sp
    '0.083'

    Notes
    -----

    Rook contiguity defines as neighbors any pair of polygons that share a
    common edge in their polygon definitions.

    See Also
    --------
    :class:`pysal.weights.W`

    """
    shp = pysal.open(shapefile)
    if idVariable:
        ids = get_ids(shapefile, idVariable)
    else:
        ids = None
    w = buildContiguity(shp, criterion='rook', ids=ids)
    shp.close()
    w.set_shapefile(shapefile, idVariable)
    if sparse:
        w = pysal.weights.WSP(w.sparse, id_order=ids)


    return w


def spw_from_gal(galfile):
    """
    Sparse scipy matrix for w from a gal file

    Parameters
    ----------

    galfile: string
             name of gal file including suffix

    Returns
    -------

    spw      : scipy sparse matrix in CSR format

    ids      : array
               identifiers for rows/cols of spw

    Examples
    --------

    >>> spw = pysal.weights.user.spw_from_gal(pysal.examples.get_path("sids2.gal"))
    >>> spw.sparse.nnz
    462
    """

    return pysal.open(galfile, 'r').read(sparse=True)

# Distance based weights


def knnW_from_array(array, k=2, p=2, ids=None, radius=None):
    """
    Nearest neighbor weights from a numpy array

    Parameters
    ----------

    data       : array (n,m)
                 attribute data, n observations on m attributes
    k          : int
                 number of nearest neighbors
    p          : float
                 Minkowski p-norm distance metric parameter:
                 1<=p<=infinity
                 2: Euclidean distance
                 1: Manhattan distance
    ids        : list
                 identifiers to attach to each observation
    radius     : If supplied arc_distances will be calculated
                 based on the given radius. p will be ignored.

    Returns
    -------

    w         : W instance
                Weights object with binary weights

    Examples
    --------
    >>> import numpy as np
    >>> x,y=np.indices((5,5))
    >>> x.shape=(25,1)
    >>> y.shape=(25,1)
    >>> data=np.hstack([x,y])
    >>> wnn2=knnW_from_array(data,k=2)
    >>> wnn4=knnW_from_array(data,k=4)
    >>> set([1, 5, 6, 2]) == set(wnn4.neighbors[0])
    True
    >>> set([0, 1, 10, 6]) == set(wnn4.neighbors[5])
    True
    >>> set([1, 5]) == set(wnn2.neighbors[0])
    True
    >>> set([0,6]) == set(wnn2.neighbors[5])
    True
    >>> "%.2f"%wnn2.pct_nonzero
    '0.08'
    >>> wnn4.pct_nonzero
    0.16
    >>> wnn4=knnW_from_array(data,k=4)
    >>> set([ 1,5,6,2]) == set(wnn4.neighbors[0])
    True
    >>> wnn4=knnW_from_array(data,k=4)
    >>> wnn3e=knnW(data,p=2,k=3)
    >>> set([1,5,6]) == set(wnn3e.neighbors[0])
    True
    >>> wnn3m=knnW(data,p=1,k=3)
    >>> set([1,5,2]) == set(wnn3m.neighbors[0])
    True

    Notes
    -----

    Ties between neighbors of equal distance are arbitrarily broken.

    See Also
    --------
    :class:`pysal.weights.W`

    """
    if radius is not None:
        array = pysal.cg.KDTree(array, distance_metric='Arc', radius=radius)
    return knnW(array, k=k, p=p, ids=ids)


def knnW_from_shapefile(shapefile, k=2, p=2, idVariable=None, radius=None):
    """
    Nearest neighbor weights from a shapefile

    Parameters
    ----------

    shapefile  : string
                 shapefile name with shp suffix
    k          : int
                 number of nearest neighbors
    p          : float
                 Minkowski p-norm distance metric parameter:
                 1<=p<=infinity
                 2: Euclidean distance
                 1: Manhattan distance
    idVariable : string
                 name of a column in the shapefile's DBF to use for ids
    radius     : If supplied arc_distances will be calculated
                 based on the given radius. p will be ignored.

    Returns
    -------

    w         : W instance
                Weights object with binary weights

    Examples
    --------

    Polygon shapefile

    >>> wc=knnW_from_shapefile(pysal.examples.get_path("columbus.shp"))
    >>> "%.4f"%wc.pct_nonzero
    '0.0408'
    >>> set([2,1]) == set(wc.neighbors[0])
    True
    >>> wc3=pysal.knnW_from_shapefile(pysal.examples.get_path("columbus.shp"),k=3)
    >>> set(wc3.neighbors[0]) == set([2,1,3])
    True
    >>> set(wc3.neighbors[2]) == set([4,3,0])
    True

    1 offset rather than 0 offset

    >>> wc3_1=knnW_from_shapefile(pysal.examples.get_path("columbus.shp"),k=3,idVariable="POLYID")
    >>> set([4,3,2]) == set(wc3_1.neighbors[1])
    True
    >>> wc3_1.weights[2]
    [1.0, 1.0, 1.0]
    >>> set([4,1,8]) == set(wc3_1.neighbors[2])
    True
    

    Point shapefile

    >>> w=knnW_from_shapefile(pysal.examples.get_path("juvenile.shp"))
    >>> w.pct_nonzero
    0.011904761904761904
    >>> w1=knnW_from_shapefile(pysal.examples.get_path("juvenile.shp"),k=1)
    >>> "%.3f"%w1.pct_nonzero
    '0.006'
    >>>

    Notes
    -----

    Supports polygon or point shapefiles. For polygon shapefiles, distance is
    based on polygon centroids. Distances are defined using coordinates in
    shapefile which are assumed to be projected and not geographical
    coordinates.

    Ties between neighbors of equal distance are arbitrarily broken.

    See Also
    --------
    :class:`pysal.weights.W`

    """

    data = get_points_array_from_shapefile(shapefile)
    if radius is not None:
        data = pysal.cg.KDTree(data, distance_metric='Arc', radius=radius)
    if idVariable:
        ids = get_ids(shapefile, idVariable)
        return knnW(data, k=k, p=p, ids=ids)
    return knnW(data, k=k, p=p)


def threshold_binaryW_from_array(array, threshold, p=2, radius=None):
    """
    Binary weights based on a distance threshold

    Parameters
    ----------

    array       : array (n,m)
                 attribute data, n observations on m attributes
    threshold  : float
                 distance band
    p          : float
                 Minkowski p-norm distance metric parameter:
                 1<=p<=infinity
                 2: Euclidean distance
                 1: Manhattan distance
    radius     : If supplied arc_distances will be calculated
                 based on the given radius. p will be ignored.

    Returns
    -------

    w         : W instance
                Weights object with binary weights

    Examples
    --------
    >>> points=[(10, 10), (20, 10), (40, 10), (15, 20), (30, 20), (30, 30)]
    >>> w=threshold_binaryW_from_array(points,threshold=11.2)
    WARNING: there is one disconnected observation (no neighbors)
    Island id:  [2]
    >>> w.weights
    {0: [1, 1], 1: [1, 1], 2: [], 3: [1, 1], 4: [1], 5: [1]}
    >>> w.neighbors
    {0: [1, 3], 1: [0, 3], 2: [], 3: [0, 1], 4: [5], 5: [4]}
    >>>
    """
    if radius is not None:
        array = pysal.cg.KDTree(array, distance_metric='Arc', radius=radius)
    return DistanceBand(array, threshold=threshold, p=p)


def threshold_binaryW_from_shapefile(shapefile, threshold, p=2, idVariable=None, radius=None):
    """
    Threshold distance based binary weights from a shapefile

    Parameters
    ----------

    shapefile  : string
                 shapefile name with shp suffix
    threshold  : float
                 distance band
    p          : float
                 Minkowski p-norm distance metric parameter:
                 1<=p<=infinity
                 2: Euclidean distance
                 1: Manhattan distance
    idVariable : string
                 name of a column in the shapefile's DBF to use for ids
    radius     : If supplied arc_distances will be calculated
                 based on the given radius. p will be ignored.

    Returns
    -------

    w         : W instance
                Weights object with binary weights

    Examples
    --------
    >>> w = threshold_binaryW_from_shapefile(pysal.examples.get_path("columbus.shp"),0.62,idVariable="POLYID")
    >>> w.weights[1]
    [1, 1]

    Notes
    -----
    Supports polygon or point shapefiles. For polygon shapefiles, distance is
    based on polygon centroids. Distances are defined using coordinates in
    shapefile which are assumed to be projected and not geographical
    coordinates.

    """
    data = get_points_array_from_shapefile(shapefile)
    if radius is not None:
        data = pysal.cg.KDTree(data, distance_metric='Arc', radius=radius)
    if idVariable:
        ids = get_ids(shapefile, idVariable)
        return DistanceBand(data, threshold=threshold, p=p, ids=ids)
    return threshold_binaryW_from_array(data, threshold, p=p)


def threshold_continuousW_from_array(array, threshold, p=2,
                                     alpha=-1, radius=None):

    """
    Continuous weights based on a distance threshold

    Parameters
    ----------

    array      : array (n,m)
                 attribute data, n observations on m attributes
    threshold  : float
                 distance band
    p          : float
                 Minkowski p-norm distance metric parameter:
                 1<=p<=infinity
                 2: Euclidean distance
                 1: Manhattan distance
    alpha      : float
                 distance decay parameter for weight (default -1.0)
                 if alpha is positive the weights will not decline with
                 distance.
    radius     : If supplied arc_distances will be calculated
                 based on the given radius. p will be ignored.

    Returns
    -------

    w         : W instance
                Weights object with continuous weights

    Examples
    --------

    inverse distance weights

    >>> points=[(10, 10), (20, 10), (40, 10), (15, 20), (30, 20), (30, 30)]
    >>> wid=threshold_continuousW_from_array(points,11.2)
    WARNING: there is one disconnected observation (no neighbors)
    Island id:  [2]
    >>> wid.weights[0]
    [0.10000000000000001, 0.089442719099991588]

    gravity weights

    >>> wid2=threshold_continuousW_from_array(points,11.2,alpha=-2.0)
    WARNING: there is one disconnected observation (no neighbors)
    Island id:  [2]
    >>> wid2.weights[0]
    [0.01, 0.0079999999999999984]

    """
    if radius is not None:
        array = pysal.cg.KDTree(array, distance_metric='Arc', radius=radius)
    w = DistanceBand(
        array, threshold=threshold, p=p, alpha=alpha, binary=False)
    return w


def threshold_continuousW_from_shapefile(shapefile, threshold, p=2,
                                         alpha=-1, idVariable=None, radius=None):
    """
    Threshold distance based continuous weights from a shapefile

    Parameters
    ----------

    shapefile  : string
                 shapefile name with shp suffix
    threshold  : float
                 distance band
    p          : float
                 Minkowski p-norm distance metric parameter:
                 1<=p<=infinity
                 2: Euclidean distance
                 1: Manhattan distance
    alpha      : float
                 distance decay parameter for weight (default -1.0)
                 if alpha is positive the weights will not decline with
                 distance.
    idVariable : string
                 name of a column in the shapefile's DBF to use for ids
    radius     : If supplied arc_distances will be calculated
                 based on the given radius. p will be ignored.

    Returns
    -------

    w         : W instance
                Weights object with continuous weights

    Examples
    --------
    >>> w = threshold_continuousW_from_shapefile(pysal.examples.get_path("columbus.shp"),0.62,idVariable="POLYID")
    >>> w.weights[1]
    [1.6702346893743334, 1.7250729841938093]

    Notes
    -----
    Supports polygon or point shapefiles. For polygon shapefiles, distance is
    based on polygon centroids. Distances are defined using coordinates in
    shapefile which are assumed to be projected and not geographical
    coordinates.

    """
    data = get_points_array_from_shapefile(shapefile)
    if radius is not None:
        data = pysal.cg.KDTree(data, distance_metric='Arc', radius=radius)
    if idVariable:
        ids = get_ids(shapefile, idVariable)
        w = DistanceBand(data, threshold=threshold, p=p, alpha=alpha, binary=False, ids=ids)
    else:
        w =  threshold_continuousW_from_array(data, threshold, p=p, alpha=alpha)
    w.set_shapefile(shapefile,idVariable)
    return w


# Kernel Weights


def kernelW(points, k=2, function='triangular', fixed=True,
        radius=None, diagonal=False):
    """
    Kernel based weights

    Parameters
    ----------

    points      : array (n,k)
                  n observations on k characteristics used to measure
                  distances between the n objects
    k           : int
                  the number of nearest neighbors to use for determining
                  bandwidth. Bandwidth taken as :math:`h_i=max(dknn) \\forall i`
                  where :math:`dknn` is a vector of k-nearest neighbor
                  distances (the distance to the kth nearest neighbor for each
                  observation).
    function    : string {'triangular','uniform','quadratic','epanechnikov',
                  'quartic','bisquare','gaussian'}

                  .. math::

                      z_{i,j} = d_{i,j}/h_i

                  triangular

                  .. math::

                      K(z) = (1 - |z|) \ if |z| \le 1

                  uniform

                  .. math::

                      K(z) = |z| \ if |z| \le 1

                  quadratic

                  .. math::

                      K(z) = (3/4)(1-z^2) \ if |z| \le 1

                  epanechnikov

                  .. math::

                      K(z) = (1-z^2) \ if |z| \le 1

                  quartic

                  .. math::

                      K(z) = (15/16)(1-z^2)^2 \ if |z| \le 1

                  bisquare

                  .. math::

                      K(z) = (1-z^2)^2 \ if |z| \le 1

                  gaussian

                  .. math::

                      K(z) = (2\pi)^{(-1/2)} exp(-z^2 / 2)

    fixed        : binary
                   If true then :math:`h_i=h \\forall i`. If false then
                   bandwidth is adaptive across observations.
    radius     : If supplied arc_distances will be calculated
                 based on the given radius. p will be ignored.
    diagonal   : boolean
                 If true, set diagonal weights = 1.0, if false (default)
                 diagonal weights are set to value according to kernel
                 function

    Returns
    -------

    w            : W
                   instance of spatial weights

    Examples
    --------
    >>> points=[(10, 10), (20, 10), (40, 10), (15, 20), (30, 20), (30, 30)]
    >>> kw=kernelW(points)
    >>> kw.weights[0]
    [1.0, 0.500000049999995, 0.4409830615267465]
    >>> kw.neighbors[0]
    [0, 1, 3]
    >>> kw.bandwidth
    array([[ 20.000002],
           [ 20.000002],
           [ 20.000002],
           [ 20.000002],
           [ 20.000002],
           [ 20.000002]])

    use different k

    >>> kw=kernelW(points,k=3)
    >>> kw.neighbors[0]
    [0, 1, 3, 4]
    >>> kw.bandwidth
    array([[ 22.36068201],
           [ 22.36068201],
           [ 22.36068201],
           [ 22.36068201],
           [ 22.36068201],
           [ 22.36068201]])

    Diagonals to 1.0

    >>> kq = kernelW(points,function='gaussian')
    >>> kq.weights
    {0: [0.3989422804014327, 0.35206533556593145, 0.3412334260702758], 1: [0.35206533556593145, 0.3989422804014327, 0.2419707487162134, 0.3412334260702758, 0.31069657591175387], 2: [0.2419707487162134, 0.3989422804014327, 0.31069657591175387], 3: [0.3412334260702758, 0.3412334260702758, 0.3989422804014327, 0.3011374490937829, 0.26575287272131043], 4: [0.31069657591175387, 0.31069657591175387, 0.3011374490937829, 0.3989422804014327, 0.35206533556593145], 5: [0.26575287272131043, 0.35206533556593145, 0.3989422804014327]}
    >>> kqd = kernelW(points, function='gaussian', diagonal=True)
    >>> kqd.weights
    {0: [1.0, 0.35206533556593145, 0.3412334260702758], 1: [0.35206533556593145, 1.0, 0.2419707487162134, 0.3412334260702758, 0.31069657591175387], 2: [0.2419707487162134, 1.0, 0.31069657591175387], 3: [0.3412334260702758, 0.3412334260702758, 1.0, 0.3011374490937829, 0.26575287272131043], 4: [0.31069657591175387, 0.31069657591175387, 0.3011374490937829, 1.0, 0.35206533556593145], 5: [0.26575287272131043, 0.35206533556593145, 1.0]}

    """
    if radius is not None:
        points = pysal.cg.KDTree(points, distance_metric='Arc', radius=radius)
    return Kernel(points, function=function, k=k, fixed=fixed,
            diagonal=diagonal)


def kernelW_from_shapefile(shapefile, k=2, function='triangular',
        idVariable=None, fixed=True, radius=None, diagonal=False):
    """
    Kernel based weights

    Parameters
    ----------

    shapefile   : string
                  shapefile name with shp suffix
    k           : int
                  the number of nearest neighbors to use for determining
                  bandwidth. Bandwidth taken as :math:`h_i=max(dknn) \\forall i`
                  where :math:`dknn` is a vector of k-nearest neighbor
                  distances (the distance to the kth nearest neighbor for each
                  observation).
    function    : string {'triangular','uniform','quadratic','epanechnikov',
                  'quartic','bisquare','gaussian'}

                  .. math::

                      z_{i,j} = d_{i,j}/h_i

                  triangular

                  .. math::

                      K(z) = (1 - |z|) \ if |z| \le 1

                  uniform

                  .. math::

                      K(z) = |z| \ if |z| \le 1

                  quadratic

                  .. math::

                      K(z) = (3/4)(1-z^2) \ if |z| \le 1

                  epanechnikov

                  .. math::

                      K(z) = (1-z^2) \ if |z| \le 1

                  quartic

                  .. math::

                      K(z) = (15/16)(1-z^2)^2 \ if |z| \le 1

                  bisquare

                  .. math::

                      K(z) = (1-z^2)^2 \ if |z| \le 1

                  gaussian

                  .. math::

                      K(z) = (2\pi)^{(-1/2)} exp(-z^2 / 2)
    idVariable   : string
                   name of a column in the shapefile's DBF to use for ids

    fixed        : binary
                   If true then :math:`h_i=h \\forall i`. If false then
                   bandwidth is adaptive across observations.
    radius     : If supplied arc_distances will be calculated
                 based on the given radius. p will be ignored.
    diagonal   : boolean
                 If true, set diagonal weights = 1.0, if false (default)
                 diagonal weights are set to value according to kernel
                 function

    Returns
    -------

    w            : W
                   instance of spatial weights

    Examples
    --------
    >>> kw = pysal.kernelW_from_shapefile(pysal.examples.get_path("columbus.shp"),idVariable='POLYID', function = 'gaussian')

    >>> kwd = pysal.kernelW_from_shapefile(pysal.examples.get_path("columbus.shp"),idVariable='POLYID', function = 'gaussian', diagonal = True)
    >>> kw.neighbors[1]
    [2, 4, 1, 3]
    >>> kwd.neighbors[1]
    [2, 4, 1, 3]
    >>> kw.weights[1]
    [0.29090631630909874, 0.2436835517263174, 0.3989422804014327, 0.29671172124745776]
    >>> kwd.weights[1]
    [0.29090631630909874, 0.2436835517263174, 1.0, 0.29671172124745776]
    

    Notes
    -----
    Supports polygon or point shapefiles. For polygon shapefiles, distance is
    based on polygon centroids. Distances are defined using coordinates in
    shapefile which are assumed to be projected and not geographical
    coordinates.

    """
    points = get_points_array_from_shapefile(shapefile)
    if radius is not None:
        points = pysal.cg.KDTree(points, distance_metric='Arc', radius=radius)
    if idVariable:
        ids = get_ids(shapefile, idVariable)
        return Kernel(points, function=function, k=k, ids=ids, fixed=fixed,
                diagonal = diagonal)
    return kernelW(points, k=k, function=function, fixed=fixed,
            diagonal=diagonal)


def adaptive_kernelW(points, bandwidths=None, k=2, function='triangular',
        radius=None, diagonal=False):
    """
    Kernel weights with adaptive bandwidths

    Parameters
    ----------

    points      : array (n,k)
                  n observations on k characteristics used to measure
                  distances between the n objects
    bandwidths  : float or array-like (optional)
                  the bandwidth :math:`h_i` for the kernel.
                  if no bandwidth is specified k is used to determine the
                  adaptive bandwidth
    k           : int
                  the number of nearest neighbors to use for determining
                  bandwidth. For fixed bandwidth, :math:`h_i=max(dknn) \\forall i`
                  where :math:`dknn` is a vector of k-nearest neighbor
                  distances (the distance to the kth nearest neighbor for each
                  observation).  For adaptive bandwidths, :math:`h_i=dknn_i`
    function    : string {'triangular','uniform','quadratic','quartic','gaussian'}
                  kernel function defined as follows with

                  .. math::

                      z_{i,j} = d_{i,j}/h_i

                  triangular

                  .. math::

                      K(z) = (1 - |z|) \ if |z| \le 1

                  uniform

                  .. math::

                      K(z) = |z| \ if |z| \le 1

                  quadratic

                  .. math::

                      K(z) = (3/4)(1-z^2) \ if |z| \le 1

                  quartic

                  .. math::

                      K(z) = (15/16)(1-z^2)^2 \ if |z| \le 1

                  gaussian

                  .. math::

                      K(z) = (2\pi)^{(-1/2)} exp(-z^2 / 2)

    radius     : If supplied arc_distances will be calculated
                 based on the given radius. p will be ignored.
    diagonal   : boolean
                 If true, set diagonal weights = 1.0, if false (default)
                 diagonal weights are set to value according to kernel
                 function
    Returns
    -------

    w            : W
                   instance of spatial weights

    Examples
    --------

    User specified bandwidths

    >>> points=[(10, 10), (20, 10), (40, 10), (15, 20), (30, 20), (30, 30)]
    >>> bw=[25.0,15.0,25.0,16.0,14.5,25.0]
    >>> kwa=adaptive_kernelW(points,bandwidths=bw)
    >>> kwa.weights[0]
    [1.0, 0.6, 0.552786404500042, 0.10557280900008403]
    >>> kwa.neighbors[0]
    [0, 1, 3, 4]
    >>> kwa.bandwidth
    array([[ 25. ],
           [ 15. ],
           [ 25. ],
           [ 16. ],
           [ 14.5],
           [ 25. ]])

    Endogenous adaptive bandwidths

    >>> kwea=adaptive_kernelW(points)
    >>> kwea.weights[0]
    [1.0, 0.10557289844279438, 9.99999900663795e-08]
    >>> kwea.neighbors[0]
    [0, 1, 3]
    >>> kwea.bandwidth
    array([[ 11.18034101],
           [ 11.18034101],
           [ 20.000002  ],
           [ 11.18034101],
           [ 14.14213704],
           [ 18.02775818]])

    Endogenous adaptive bandwidths with Gaussian kernel

    >>> kweag=adaptive_kernelW(points,function='gaussian')
    >>> kweag.weights[0]
    [0.3989422804014327, 0.2674190291577696, 0.2419707487162134]
    >>> kweag.bandwidth
    array([[ 11.18034101],
           [ 11.18034101],
           [ 20.000002  ],
           [ 11.18034101],
           [ 14.14213704],
           [ 18.02775818]])

    with diagonal

    >>> kweag = pysal.adaptive_kernelW(points, function='gaussian')
    >>> kweagd = pysal.adaptive_kernelW(points, function='gaussian', diagonal=True)
    >>> kweag.neighbors[0]
    [0, 1, 3]
    >>> kweagd.neighbors[0]
    [0, 1, 3]
    >>> kweag.weights[0]
    [0.3989422804014327, 0.2674190291577696, 0.2419707487162134]
    >>> kweagd.weights[0]
    [1.0, 0.2674190291577696, 0.2419707487162134]

    """
    if radius is not None:
        points = pysal.cg.KDTree(points, distance_metric='Arc', radius=radius)
    return Kernel(points, bandwidth=bandwidths, fixed=False, k=k,
            function=function, diagonal=diagonal)


def adaptive_kernelW_from_shapefile(shapefile, bandwidths=None, k=2, function='triangular',
                                    idVariable=None, radius=None,
                                    diagonal = False):
    """
    Kernel weights with adaptive bandwidths

    Parameters
    ----------

    shapefile   : string
                  shapefile name with shp suffix
    bandwidths  : float or array-like (optional)
                  the bandwidth :math:`h_i` for the kernel.
                  if no bandwidth is specified k is used to determine the
                  adaptive bandwidth
    k           : int
                  the number of nearest neighbors to use for determining
                  bandwidth. For fixed bandwidth, :math:`h_i=max(dknn) \\forall i`
                  where :math:`dknn` is a vector of k-nearest neighbor
                  distances (the distance to the kth nearest neighbor for each
                  observation).  For adaptive bandwidths, :math:`h_i=dknn_i`
    function    : string {'triangular','uniform','quadratic','quartic','gaussian'}
                  kernel function defined as follows with

                  .. math::

                      z_{i,j} = d_{i,j}/h_i

                  triangular

                  .. math::

                      K(z) = (1 - |z|) \ if |z| \le 1

                  uniform

                  .. math::

                      K(z) = |z| \ if |z| \le 1

                  quadratic

                  .. math::

                      K(z) = (3/4)(1-z^2) \ if |z| \le 1

                  quartic

                  .. math::

                      K(z) = (15/16)(1-z^2)^2 \ if |z| \le 1

                  gaussian

                  .. math::

                      K(z) = (2\pi)^{(-1/2)} exp(-z^2 / 2)
    idVariable   : string
                   name of a column in the shapefile's DBF to use for ids
    radius     : If supplied arc_distances will be calculated
                 based on the given radius. p will be ignored.
    diagonal   : boolean
                 If true, set diagonal weights = 1.0, if false (default)
                 diagonal weights are set to value according to kernel
                 function

    Returns
    -------

    w            : W
                   instance of spatial weights

    Examples
    --------
    >>> kwa = pysal.adaptive_kernelW_from_shapefile(pysal.examples.get_path("columbus.shp"), function='gaussian')
    >>> kwad = pysal.adaptive_kernelW_from_shapefile(pysal.examples.get_path("columbus.shp"), function='gaussian', diagonal=True)
    >>> kwa.neighbors[0]
    [0, 2, 1]
    >>> kwad.neighbors[0]
    [0, 2, 1]
    >>> kwa.weights[0]
    [0.3989422804014327, 0.24966013701844503, 0.2419707487162134]
    >>> kwad.weights[0]
    [1.0, 0.24966013701844503, 0.2419707487162134]
    >>>

    Notes
    -----
    Supports polygon or point shapefiles. For polygon shapefiles, distance is
    based on polygon centroids. Distances are defined using coordinates in
    shapefile which are assumed to be projected and not geographical
    coordinates.

    """
    points = get_points_array_from_shapefile(shapefile)
    if radius is not None:
        points = pysal.cg.KDTree(points, distance_metric='Arc', radius=radius)
    if idVariable:
        ids = get_ids(shapefile, idVariable)
        return Kernel(points, bandwidth=bandwidths, fixed=False, k=k,
                function=function, ids=ids, diagonal=diagonal)
    return adaptive_kernelW(points, bandwidths=bandwidths, k=k,
            function=function, diagonal=diagonal)


def min_threshold_dist_from_shapefile(shapefile, radius=None, p=2):
    """
    Kernel weights with adaptive bandwidths

    Parameters
    ----------

    shapefile  : string
                 shapefile name with shp suffix
    radius     : If supplied arc_distances will be calculated
                 based on the given radius. p will be ignored.
    p          : float
                 Minkowski p-norm distance metric parameter:
                 1<=p<=infinity
                 2: Euclidean distance
                 1: Manhattan distance

    Returns
    -------
    d            : float
                   minimum nearest neighbor distance between the n observations

    Examples
    --------
    >>> md = min_threshold_dist_from_shapefile(pysal.examples.get_path("columbus.shp"))
    >>> md
    0.61886415807685413
    >>> min_threshold_dist_from_shapefile(pysal.examples.get_path("stl_hom.shp"), pysal.cg.sphere.RADIUS_EARTH_MILES)
    31.846942936393717

    Notes
    -----
    Supports polygon or point shapefiles. For polygon shapefiles, distance is
    based on polygon centroids. Distances are defined using coordinates in
    shapefile which are assumed to be projected and not geographical
    coordinates.

    """
    points = get_points_array_from_shapefile(shapefile)
    if radius is not None:
        points = pysal.cg.KDTree(points, distance_metric='Arc', radius=radius)
    return min_threshold_distance(points,p)


def build_lattice_shapefile(nrows, ncols, outFileName):
    """
    Build a lattice shapefile with nrows rows and ncols cols

    Parameters
    ----------

    nrows       : int
                  Number of rows
    ncols       : int
                  Number of cols
    outFileName : str
                  shapefile name with shp suffix

    Returns
    -------
    None
    """
    if not outFileName.endswith('.shp'):
        raise ValueError("outFileName must end with .shp")
    o = pysal.open(outFileName, 'w')
    dbf_name = outFileName.split(".")[0] + ".dbf"
    d = pysal.open(dbf_name, 'w')
    d.header = [ 'ID' ]
    d.field_spec = [ ('N', 8, 0) ]
    c = 0
    for i in xrange(nrows):
        for j in xrange(ncols):
            ll = i, j
            ul = i, j + 1
            ur = i + 1, j + 1
            lr = i + 1, j
            o.write(pysal.cg.Polygon([ll, ul, ur, lr, ll]))
            d.write([c])
            c += 1
    d.close()
    o.close()

def _test():
    import doctest
    # the following line could be used to define an alternative to the '<BLANKLINE>' flag
    #doctest.BLANKLINE_MARKER = 'something better than <BLANKLINE>'
    start_suppress = np.get_printoptions()['suppress']
    np.set_printoptions(suppress=True)
    doctest.testmod()
    np.set_printoptions(suppress=start_suppress)    

if __name__ == '__main__':
    _test()



########NEW FILE########
__FILENAME__ = util
import pysal
from pysal.common import *
import pysal.weights
import numpy as np
from scipy import sparse, float32
import scipy.spatial
import os
import operator

__all__ = ['lat2W', 'regime_weights', 'comb', 'order', 'higher_order', 'shimbel',
           'remap_ids', 'full2W', 'full', 'WSP2W', 'insert_diagonal', 'get_ids',
           'get_points_array_from_shapefile', 'min_threshold_distance', 'lat2SW', 'w_local_cluster',
           'higher_order_sp', 'hexLat2W']


def hexLat2W(nrows=5, ncols=5):
    """
    Create a W object for a hexagonal lattice.

    Parameters
    ----------

    nrows   : int
              number of rows
    ncols   : int
              number of columns

    Returns
    -------

    w : W
        instance of spatial weights class W

    Notes
    -----

    Observations are row ordered: first k observations are in row 0, next k in row 1, and so on.

    Construction is based on shifting every other column of a regular lattice
    down 1/2 of a cell.

    Examples
    --------

    >>> import pysal as ps
    >>> w = ps.lat2W()
    >>> w.neighbors[1]
    [0, 6, 2]
    >>> w.neighbors[21]
    [16, 20, 22]
    >>> wh = ps.hexLat2W()
    >>> wh.neighbors[1]
    [0, 6, 2, 5, 7]
    >>> wh.neighbors[21]
    [16, 20, 22]
    >>>
    """

    if nrows == 1 or ncols == 1:
        print "Hexagon lattice requires at least 2 rows and columns"
        print "Returning a linear contiguity structure"
        return lat2W(nrows, ncols)

    n = nrows * ncols
    rid = [i / ncols for i in xrange(n)]
    cid = [i % ncols for i in xrange(n)]
    r1 = nrows - 1
    c1 = ncols - 1

    w = lat2W(nrows, ncols).neighbors
    for i in xrange(n):
        odd = cid[i] % 2
        if odd:
            if rid[i] < r1:  # odd col index above last row
                # new sw neighbor
                if cid[i] > 0:
                    j = i + ncols - 1
                    w[i] = w.get(i, []) + [j]
                # new se neighbor
                if cid[i] < c1:
                    j = i + ncols + 1
                    w[i] = w.get(i, []) + [j]

        else:  # even col
            # nw
            jnw = [i - ncols - 1]
            # ne
            jne = [i - ncols + 1]
            if rid[i] > 0:
                w[i]
                if cid[i] == 0:
                    w[i] = w.get(i, []) + jne
                elif cid[i] == c1:
                    w[i] = w.get(i, []) + jnw
                else:
                    w[i] = w.get(i, []) + jne
                    w[i] = w.get(i, []) + jnw


    return pysal.weights.W(w)


def lat2W(nrows=5, ncols=5, rook=True, id_type='int'):
    """
    Create a W object for a regular lattice.

    Parameters
    ----------

    nrows   : int
              number of rows
    ncols   : int
              number of columns
    rook    : boolean
              type of contiguity. Default is rook. For queen, rook =False
    id_type : string
              string defining the type of IDs to use in the final W object;
              options are 'int' (0, 1, 2 ...; default), 'float' (0.0,
              1.0, 2.0, ...) and 'string' ('id0', 'id1', 'id2', ...)

    Returns
    -------

    w : W
        instance of spatial weights class W

    Notes
    -----

    Observations are row ordered: first k observations are in row 0, next k in row 1, and so on.

    Examples
    --------

    >>> from pysal import lat2W
    >>> w9 = lat2W(3,3)
    >>> "%.3f"%w9.pct_nonzero
    '0.296'
    >>> w9[0]
    {1: 1.0, 3: 1.0}
    >>> w9[3]
    {0: 1.0, 4: 1.0, 6: 1.0}
    >>>
    """
    n = nrows * ncols
    r1 = nrows - 1
    c1 = ncols - 1
    rid = [i / ncols for i in xrange(n)]
    cid = [i % ncols for i in xrange(n)]
    w = {}
    r = below = 0
    for i in xrange(n - 1):
        if rid[i] < r1:
            below = rid[i] + 1
            r = below * ncols + cid[i]
            w[i] = w.get(i, []) + [r]
            w[r] = w.get(r, []) + [i]
        if cid[i] < c1:
            right = cid[i] + 1
            c = rid[i] * ncols + right
            w[i] = w.get(i, []) + [c]
            w[c] = w.get(c, []) + [i]
        if not rook:
            # southeast bishop
            if cid[i] < c1 and rid[i] < r1:
                r = (rid[i] + 1) * ncols + 1 + cid[i]
                w[i] = w.get(i, []) + [r]
                w[r] = w.get(r, []) + [i]
            # southwest bishop
            if cid[i] > 0 and rid[i] < r1:
                r = (rid[i] + 1) * ncols - 1 + cid[i]
                w[i] = w.get(i, []) + [r]
                w[r] = w.get(r, []) + [i]

    neighbors = {}
    weights = {}
    for key in w:
        weights[key] = [1.] * len(w[key])
    ids = range(n)
    if id_type == 'string':
        ids = ['id' + str(i) for i in ids]
    elif id_type == 'float':
        ids = [i * 1. for i in ids]
    if id_type == 'string' or id_type == 'float':
        id_dict = dict(zip(range(n), ids))
        alt_w = {}
        alt_weights = {}
        for i in w:
            values = [id_dict[j] for j in w[i]]
            key = id_dict[i]
            alt_w[key] = values
            alt_weights[key] = weights[i]
        w = alt_w
        weights = alt_weights
    return pysal.weights.W(w, weights, ids)


def regime_weights(regimes):
    """
    Construct spatial weights for regime neighbors.

    Block contiguity structures are relevant when defining neighbor relations
    based on membership in a regime. For example, all counties belonging to
    the same state could be defined as neighbors, in an analysis of all
    counties in the US.

    Parameters
    ----------
    regimes : list or array
           ids of which regime an observation belongs to

    Returns
    -------

    W : spatial weights instance

    Examples
    --------

    >>> from pysal import regime_weights
    >>> import numpy as np
    >>> regimes = np.ones(25)
    >>> regimes[range(10,20)] = 2
    >>> regimes[range(21,25)] = 3
    >>> regimes
    array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  2.,  2.,
            2.,  2.,  2.,  2.,  2.,  2.,  2.,  1.,  3.,  3.,  3.,  3.])
    >>> w = regime_weights(regimes)
    >>> w.weights[0]
    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]
    >>> w.neighbors[0]
    [1, 2, 3, 4, 5, 6, 7, 8, 9, 20]
    >>> regimes = ['n','n','s','s','e','e','w','w','e']
    >>> n = len(regimes)
    >>> w = regime_weights(regimes)
    >>> w.neighbors
    {0: [1], 1: [0], 2: [3], 3: [2], 4: [5, 8], 5: [4, 8], 6: [7], 7: [6], 8: [4, 5]}
    """
    rids = np.unique(regimes)
    neighbors = {}
    NPNZ = np.nonzero
    regimes = np.array(regimes)
    for rid in rids:
        members = NPNZ(regimes == rid)[0]
        for member in members:
            neighbors[member] = members[NPNZ(members != member)[0]].tolist()
    return pysal.weights.W(neighbors)


def comb(items, n=None):
    """
    Combinations of size n taken from items

    Parameters
    ----------

    items : sequence
    n     : integer
            size of combinations to take from items

    Returns
    -------

    implicit : generator
               combinations of size n taken from items

    Examples
    --------
    >>> x = range(4)
    >>> for c in comb(x, 2):
    ...     print c
    ...
    [0, 1]
    [0, 2]
    [0, 3]
    [1, 2]
    [1, 3]
    [2, 3]

    """
    if n is None:
        n = len(items)
    for i in range(len(items)):
        v = items[i:i + 1]
        if n == 1:
            yield v
        else:
            rest = items[i + 1:]
            for c in comb(rest, n - 1):
                yield v + c


def order(w, kmax=3):
    """
    Determine the non-redundant order of contiguity up to a specific
    order.

    Parameters
    ----------

    w       : W
              spatial weights object

    kmax    : int
              maximum order of contiguity

    Returns
    -------

    info : dictionary
           observation id is the key, value is a list of contiguity
           orders with a negative 1 in the ith position

    Notes
    -----
    Implements the algorithm in Anselin and Smirnov (1996) [1]_

    Examples
    --------
    >>> from pysal import rook_from_shapefile as rfs
    >>> w = rfs(pysal.examples.get_path('10740.shp'))
    WARNING: there is one disconnected observation (no neighbors)
    Island id:  [163]
    >>> w3 = order(w, kmax = 3)
    >>> w3[1][0:5]
    [1, -1, 1, 2, 1]

    """
    ids = w.neighbors.keys()
    info = {}
    for id in ids:
        s = [0] * w.n
        s[ids.index(id)] = -1
        for j in w.neighbors[id]:
            s[ids.index(j)] = 1
        k = 1
        while k < kmax:
            knext = k + 1
            if s.count(k):
                # get neighbors of order k
                js = [ids[j] for j, val in enumerate(s) if val == k]
                # get first order neighbors for order k neighbors
                for j in js:
                    next_neighbors = w.neighbors[j]
                    for neighbor in next_neighbors:
                        nid = ids.index(neighbor)
                        if s[nid] == 0:
                            s[nid] = knext
            k = knext
        info[id] = s
    return info


def higher_order(w, k=2):
    """
    Contiguity weights object of order k

    Parameters
    ----------

    w     : W
            spatial weights object
    k     : int
            order of contiguity

    Returns
    -------

    implicit : W
               spatial weights object

    Notes
    -----
    Implements the algorithm in Anselin and Smirnov (1996) [1]_

    Examples
    --------
    >>> from pysal import lat2W, higher_order
    >>> w10 = lat2W(10, 10)
    >>> w10_2 = higher_order(w10, 2)
    >>> w10_2[0]
    {2: 1.0, 11: 1.0, 20: 1.0}
    >>> w5 = lat2W()
    >>> w5[0]
    {1: 1.0, 5: 1.0}
    >>> w5[1]
    {0: 1.0, 2: 1.0, 6: 1.0}
    >>> w5_2 = higher_order(w5,2)
    >>> w5_2[0]
    {2: 1.0, 10: 1.0, 6: 1.0}

    References
    ----------
    .. [1] Anselin, L. and O. Smirnov (1996) "Efficient algorithms for
       constructing proper higher order spatial lag operators. Journal of
       Regional Science, 36, 67-89.
    """
    info = order(w, k)
    ids = info.keys()
    neighbors = {}
    weights = {}
    for id in ids:
        nids = [ids[j] for j, o in enumerate(info[id]) if o == k]
        neighbors[id] = nids
        weights[id] = [1.0] * len(nids)
    return pysal.weights.W(neighbors, weights)


def higher_order_sp(wsp, k=2):
    """
    Contiguity weights for a sparse W for order k

    Parameters
    ==========

    wsp:  WSP instance

    k: Order of contiguity

    Returns
    -------

    wk: WSP instance
        binary sparse contiguity of order k

    Notes
    -----
    Lower order contiguities are removed.

    Examples
    --------

    >>> import pysal
    >>> w25 = pysal.lat2W(5,5)
    >>> w25.n
    25
    >>> ws25 = w25.sparse
    >>> ws25o3 = pysal.weights.higher_order_sp(ws25,3)
    >>> w25o3 = pysal.weights.higher_order(w25,3)
    >>> w25o3[12]
    {1: 1.0, 3: 1.0, 5: 1.0, 9: 1.0, 15: 1.0, 19: 1.0, 21: 1.0, 23: 1.0}
    >>> pysal.weights.WSP2W(ws25o3)[12]
    {1: 1.0, 3: 1.0, 5: 1.0, 9: 1.0, 15: 1.0, 19: 1.0, 21: 1.0, 23: 1.0}
    >>>
    """

    wk = wsp ** k
    rk, ck = wk.nonzero()
    sk = set(zip(rk, ck))
    for j in range(1, k):
        wj = wsp ** j
        rj, cj = wj.nonzero()
        sj = set(zip(rj, cj))
        sk.difference_update(sj)
    d = {}
    for pair in sk:
        k, v = pair
        #if d.has_key(k):
        if k in d:
            d[k].append(v)
        else:
            d[k] = [v]
    return pysal.weights.WSP(pysal.W(neighbors=d).sparse)


def w_local_cluster(w):
    """
    Local clustering coefficients for each unit as a node in a graph. [ws]_

    Parameters
    ----------

    w   : W
          spatial weights object

    Returns
    -------

    c     : array (w.n,1)
            local clustering coefficients

    Notes
    -----

    The local clustering coefficient :math:`c_i` quantifies how close the
    neighbors of observation :math:`i` are to being a clique:

            .. math::

               c_i = | \{w_{j,k}\} |/ (k_i(k_i - 1)): j,k \in N_i

    where :math:`N_i` is the set of neighbors to :math:`i`, :math:`k_i =
    |N_i|` and :math:`\{w_{j,k}\}` is the set of non-zero elements of the
    weights between pairs in :math:`N_i`.

    References
    ----------

    .. [ws] Watts, D.J. and S.H. Strogatz (1988) "Collective dynamics of 'small-world' networks". Nature, 393: 440-442.

    Examples
    --------

    >>> w = pysal.lat2W(3,3, rook=False)
    >>> w_local_cluster(w)
    array([[ 1.        ],
           [ 0.6       ],
           [ 1.        ],
           [ 0.6       ],
           [ 0.42857143],
           [ 0.6       ],
           [ 1.        ],
           [ 0.6       ],
           [ 1.        ]])

    """

    c = np.zeros((w.n, 1), float)
    w.transformation = 'b'
    for i, id in enumerate(w.id_order):
        ki = max(w.cardinalities[id], 1)  # deal with islands
        Ni = w.neighbors[id]
        wi = pysal.w_subset(w, Ni).full()[0]
        c[i] = wi.sum() / (ki * (ki - 1))
    return c


def shimbel(w):
    """
    Find the Shimbel matrix for first order contiguity matrix.

    Parameters
    ----------
    w     : W
            spatial weights object
    Returns
    -------

    info  : list of lists
            one list for each observation which stores the shortest
            order between it and each of the the other observations.

    Examples
    --------
    >>> from pysal import lat2W, shimbel
    >>> w5 = lat2W()
    >>> w5_shimbel = shimbel(w5)
    >>> w5_shimbel[0][24]
    8
    >>> w5_shimbel[0][0:4]
    [-1, 1, 2, 3]
    >>>
    """
    info = {}
    ids = w.id_order
    for id in ids:
        s = [0] * w.n
        s[ids.index(id)] = -1
        for j in w.neighbors[id]:
            s[ids.index(j)] = 1
        k = 1
        flag = s.count(0)
        while flag:
            p = -1
            knext = k + 1
            for j in range(s.count(k)):
                neighbor = s.index(k, p + 1)
                p = neighbor
                next_neighbors = w.neighbors[ids[p]]
                for neighbor in next_neighbors:
                    nid = ids.index(neighbor)
                    if s[nid] == 0:
                        s[nid] = knext
            k = knext
            flag = s.count(0)
        info[id] = s
    return info


def full(w):
    """
    Generate a full numpy array

    Parameters
    ----------
    w        : W
               spatial weights object

    Returns
    -------

    implicit : tuple
               first element being the full numpy array and second element
               keys being the ids associated with each row in the array.

    Examples
    --------
    >>> from pysal import W, full
    >>> neighbors = {'first':['second'],'second':['first','third'],'third':['second']}
    >>> weights = {'first':[1],'second':[1,1],'third':[1]}
    >>> w = W(neighbors, weights)
    >>> wf, ids = full(w)
    >>> wf
    array([[ 0.,  1.,  0.],
           [ 1.,  0.,  1.],
           [ 0.,  1.,  0.]])
    >>> ids
    ['first', 'second', 'third']
    """
    wfull = np.zeros([w.n, w.n], dtype=float)
    keys = w.neighbors.keys()
    if w.id_order:
        keys = w.id_order
    for i, key in enumerate(keys):
        n_i = w.neighbors[key]
        w_i = w.weights[key]
        for j, wij in zip(n_i, w_i):
            c = keys.index(j)
            wfull[i, c] = wij
    return (wfull, keys)


def full2W(m, ids=None):
    '''
    Create a PySAL W object from a full array
    ...

    Parameters
    ----------
    m       : array
              nxn array with the full weights matrix
    ids     : list
              User ids assumed to be aligned with m

    Returns
    -------
    w       : W
              PySAL weights object

    Examples
    --------
    >>> import pysal as ps
    >>> import numpy as np

    Create an array of zeros

    >>> a = np.zeros((4, 4))

    For loop to fill it with random numbers

    >>> for i in range(len(a)):
    ...     for j in range(len(a[i])):
    ...         if i!=j:
    ...             a[i, j] = np.random.random(1)

    Create W object

    >>> w = ps.weights.util.full2W(a)
    >>> w.full()[0] == a
    array([[ True,  True,  True,  True],
           [ True,  True,  True,  True],
           [ True,  True,  True,  True],
           [ True,  True,  True,  True]], dtype=bool)

    Create list of user ids

    >>> ids = ['myID0', 'myID1', 'myID2', 'myID3']
    >>> w = ps.weights.util.full2W(a, ids=ids)
    >>> w.full()[0] == a
    array([[ True,  True,  True,  True],
           [ True,  True,  True,  True],
           [ True,  True,  True,  True],
           [ True,  True,  True,  True]], dtype=bool)
    '''
    if m.shape[0] != m.shape[1]:
        raise ValueError('Your array is not square')
    neighbors, weights = {}, {}
    for i in xrange(m.shape[0]):
    # for i, row in enumerate(m):
        row = m[i]
        if ids:
            i = ids[i]
        ngh = list(row.nonzero()[0])
        weights[i] = list(row[ngh])
        ngh = list(ngh)
        if ids:
            ngh = [ids[j] for j in ngh]
        neighbors[i] = ngh
    return pysal.W(neighbors, weights, id_order=ids)


def WSP2W(wsp, silent_island_warning=False):
    """
    Convert a pysal WSP object (thin weights matrix) to a pysal W object.

    Parameters
    ----------
    wsp                     : WSP
                              PySAL sparse weights object
    silent_island_warning   : boolean
                              Switch to turn off (default on) print statements
                              for every observation with islands

    Returns
    -------
    w       : W
              PySAL weights object

    Examples
    --------
    >>> import pysal

    Build a 10x10 scipy.sparse matrix for a rectangular 2x5 region of cells
    (rook contiguity), then construct a PySAL sparse weights object (wsp).

    >>> sp = pysal.weights.lat2SW(2, 5)
    >>> wsp = pysal.weights.WSP(sp)
    >>> wsp.n
    10
    >>> print wsp.sparse[0].todense()
    [[0 1 0 0 0 1 0 0 0 0]]

    Convert this sparse weights object to a standard PySAL weights object.

    >>> w = pysal.weights.WSP2W(wsp)
    >>> w.n
    10
    >>> print w.full()[0][0]
    [ 0.  1.  0.  0.  0.  1.  0.  0.  0.  0.]

    """
    wsp.sparse
    indices = wsp.sparse.indices
    data = wsp.sparse.data
    indptr = wsp.sparse.indptr
    id_order = wsp.id_order
    if id_order:
        # replace indices with user IDs
        indices = [id_order[i] for i in indices]
    else:
        id_order = range(wsp.n)
    neighbors, weights = {}, {}
    start = indptr[0]
    for i in xrange(wsp.n):
        oid = id_order[i]
        end = indptr[i + 1]
        neighbors[oid] = indices[start:end]
        weights[oid] = data[start:end]
        start = end
    ids = copy.copy(wsp.id_order)
    w = pysal.W(neighbors, weights, ids,
                silent_island_warning=silent_island_warning)
    w._sparse = copy.deepcopy(wsp.sparse)
    w._cache['sparse'] = w._sparse
    return w


def insert_diagonal(w, diagonal=1.0, wsp=False):
    """
    Returns a new weights object with values inserted along the main diagonal.

    Parameters
    ----------
    w        : W
               Spatial weights object

    diagonal : float, int or array
               Defines the value(s) to which the weights matrix diagonal should
               be set. If a constant is passed then each element along the
               diagonal will get this value (default is 1.0). An array of length
               w.n can be passed to set explicit values to each element along
               the diagonal (assumed to be in the same order as w.id_order).

    wsp      : boolean
               If True return a thin weights object of the type WSP, if False
               return the standard W object.

    Returns
    -------
    w        : W
               Spatial weights object

    Examples
    --------
    >>> import pysal
    >>> import numpy as np

    Build a basic rook weights matrix, which has zeros on the diagonal, then
    insert ones along the diagonal.

    >>> w = pysal.lat2W(5, 5, id_type='string')
    >>> w_const = pysal.weights.insert_diagonal(w)
    >>> w['id0']
    {'id5': 1.0, 'id1': 1.0}
    >>> w_const['id0']
    {'id5': 1.0, 'id0': 1.0, 'id1': 1.0}

    Insert different values along the main diagonal.

    >>> diag = np.arange(100, 125)
    >>> w_var = pysal.weights.insert_diagonal(w, diag)
    >>> w_var['id0']
    {'id5': 1.0, 'id0': 100.0, 'id1': 1.0}

    """
    w_new = copy.deepcopy(w.sparse)
    w_new = w_new.tolil()
    if issubclass(type(diagonal), np.ndarray):
        if w.n != diagonal.shape[0]:
            raise Exception("shape of w and diagonal do not match")
        w_new.setdiag(diagonal)
    elif operator.isNumberType(diagonal):
        w_new.setdiag([diagonal] * w.n)
    else:
        raise Exception("Invalid value passed to diagonal")
    w_out = pysal.weights.WSP(w_new, copy.copy(w.id_order))
    if wsp:
        return w_out
    else:
        return WSP2W(w_out)


def remap_ids(w, old2new, id_order=[]):
    """
    Remaps the IDs in a spatial weights object

    Parameters
    ----------
    w        : W
               Spatial weights object

    old2new  : dictionary
               Dictionary where the keys are the IDs in w (i.e. "old IDs") and
               the values are the IDs to replace them (i.e. "new IDs")

    id_order : list
               An ordered list of new IDs, which defines the order of observations when
               iterating over W. If not set then the id_order in w will be
               used.

    Returns
    -------

    implicit : W
               Spatial weights object with new IDs

    Examples
    --------
    >>> from pysal import lat2W, remap_ids
    >>> w = lat2W(3,2)
    >>> w.id_order
    [0, 1, 2, 3, 4, 5]
    >>> w.neighbors[0]
    [2, 1]
    >>> old_to_new = {0:'a', 1:'b', 2:'c', 3:'d', 4:'e', 5:'f'}
    >>> w_new = remap_ids(w, old_to_new)
    >>> w_new.id_order
    ['a', 'b', 'c', 'd', 'e', 'f']
    >>> w_new.neighbors['a']
    ['c', 'b']

    """
    if not isinstance(w, pysal.weights.W):
        raise Exception("w must be a spatial weights object")
    new_neigh = {}
    new_weights = {}
    for key, value in w.neighbors.iteritems():
        new_values = [old2new[i] for i in value]
        new_key = old2new[key]
        new_neigh[new_key] = new_values
        new_weights[new_key] = copy.copy(w.weights[key])
    if id_order:
        return pysal.weights.W(new_neigh, new_weights, id_order)
    else:
        if w.id_order:
            id_order = [old2new[i] for i in w.id_order]
            return pysal.weights.W(new_neigh, new_weights, id_order)
        else:
            return pysal.weights.W(new_neigh, new_weights)


def get_ids(shapefile, idVariable):
    """
    Gets the IDs from the DBF file that moves with a given shape file

    Parameters
    ----------
    shapefile    : string
                   name of a shape file including suffix
    idVariable   : string
                   name of a column in the shapefile's DBF to use for ids

    Returns
    -------
    ids          : list
                   a list of IDs

    Examples
    --------
    >>> from pysal.weights.util import get_ids
    >>> polyids = get_ids(pysal.examples.get_path("columbus.shp"), "POLYID")
    >>> polyids[:5]
    [1, 2, 3, 4, 5]
    """

    try:
        dbname = os.path.splitext(shapefile)[0] + '.dbf'
        db = pysal.open(dbname)
        var = db.by_col[idVariable]
        db.close()
        return var
    except IOError:
        msg = 'The shapefile "%s" appears to be missing its DBF file. The DBF file "%s" could not be found.' % (
            shapefile, dbname)
        raise IOError(msg)
    except AttributeError:
        msg = 'The variable "%s" was not found in the DBF file. The DBF contains the following variables: %s.' % (
            idVariable, ','.join(db.header))
        raise KeyError(msg)


def get_points_array_from_shapefile(shapefile):
    """
    Gets a data array of x and y coordinates from a given shape file

    Parameters
    ----------
    shapefile     : string
                    name of a shape file including suffix

    Returns
    -------
    points        : array (n, 2)
                    a data array of x and y coordinates

    Notes
    -----
    If the given shape file includes polygons,
    this function returns x and y coordinates of the polygons' centroids

    Examples
    --------
    Point shapefile

    >>> from pysal.weights.util import get_points_array_from_shapefile
    >>> xy = get_points_array_from_shapefile(pysal.examples.get_path('juvenile.shp'))
    >>> xy[:3]
    array([[ 94.,  93.],
           [ 80.,  95.],
           [ 79.,  90.]])

    Polygon shapefile

    >>> xy = get_points_array_from_shapefile(pysal.examples.get_path('columbus.shp'))
    >>> xy[:3]
    array([[  8.82721847,  14.36907602],
           [  8.33265837,  14.03162401],
           [  9.01226541,  13.81971908]])
    """

    f = pysal.open(shapefile)
    if f.type.__name__ == 'Polygon':
        data = np.array([shape.centroid for shape in f])
    elif f.type.__name__ == 'Point':
        data = np.array([shape for shape in f])
    f.close()
    return data


def min_threshold_distance(data, p=2):
    """
    Get the maximum nearest neighbor distance

    Parameters
    ----------

    data    : array (n,k) or KDTree where KDtree.data is array (n,k)
              n observations on k attributes
    p       : float
              Minkowski p-norm distance metric parameter:
              1<=p<=infinity
              2: Euclidean distance
              1: Manhattan distance

    Returns
    -------
    nnd    : float
             maximum nearest neighbor distance between the n observations

    Examples
    --------
    >>> from pysal.weights.util import min_threshold_distance
    >>> import numpy as np
    >>> x, y = np.indices((5, 5))
    >>> x.shape = (25, 1)
    >>> y.shape = (25, 1)
    >>> data = np.hstack([x, y])
    >>> min_threshold_distance(data)
    1.0

    """
    if issubclass(type(data), scipy.spatial.KDTree):
        kd = data
        data = kd.data
    else:
        kd = KDTree(data)
    nn = kd.query(data, k=2, p=p)
    nnd = nn[0].max(axis=0)[1]
    return nnd


def lat2SW(nrows=3, ncols=5, criterion="rook", row_st=False):
    """
    Create a sparse W matrix for a regular lattice.

    Parameters
    ----------

    nrows   : int
              number of rows
    ncols   : int
              number of columns
    rook    : "rook", "queen", or "bishop"
              type of contiguity. Default is rook.
    row_st  : boolean
              If True, the created sparse W object is row-standardized so
              every row sums up to one. Defaults to False.

    Returns
    -------

    w : scipy.sparse.dia_matrix
        instance of a scipy sparse matrix

    Notes
    -----

    Observations are row ordered: first k observations are in row 0, next k in row 1, and so on.
    This method directly creates the W matrix using the strucuture of the contiguity type.

    Examples
    --------

    >>> from pysal import weights
    >>> w9 = weights.lat2SW(3,3)
    >>> w9[0,1]
    1
    >>> w9[3,6]
    1
    >>> w9r = weights.lat2SW(3,3, row_st=True)
    >>> w9r[3,6]
    0.33333333333333331
    """
    n = nrows * ncols
    diagonals = []
    offsets = []
    if criterion == "rook" or criterion == "queen":
        d = np.ones((1, n))
        for i in range(ncols - 1, n, ncols):
            d[0, i] = 0
        diagonals.append(d)
        offsets.append(-1)

        d = np.ones((1, n))
        diagonals.append(d)
        offsets.append(-ncols)

    if criterion == "queen" or criterion == "bishop":
        d = np.ones((1, n))
        for i in range(0, n, ncols):
            d[0, i] = 0
        diagonals.append(d)
        offsets.append(-(ncols - 1))

        d = np.ones((1, n))
        for i in range(ncols - 1, n, ncols):
            d[0, i] = 0
        diagonals.append(d)
        offsets.append(-(ncols + 1))
    data = np.concatenate(diagonals)
    offsets = np.array(offsets)
    m = sparse.dia_matrix((data, offsets), shape=(n, n), dtype=np.int8)
    m = m + m.T
    if row_st:
        m = sparse.spdiags(1. / m.sum(1).T, 0, *m.shape) * m
    return m


def write_gal(file, k=10):
    f = open(file, 'w')
    n = k * k
    f.write("0 %d" % n)
    for i in xrange(n):
        row = i / k
        col = i % k
        neighs = [i - i, i + 1, i - k, i + k]
        neighs = [j for j in neighs if j >= 0 and j < n]
        f.write("\n%d %d\n" % (i, len(neighs)))
        f.write(" ".join(map(str, neighs)))
    f.close()

if __name__ == "__main__":
    from pysal import lat2W

    assert (lat2W(5, 5).sparse.todense() == lat2SW(5, 5).todense()).all()
    assert (lat2W(5, 3).sparse.todense() == lat2SW(5, 3).todense()).all()
    assert (lat2W(5, 3, rook=False).sparse.todense() == lat2SW(5, 3,
                                                               'queen').todense()).all()
    assert (lat2W(50, 50, rook=False).sparse.todense() == lat2SW(50,
                                                                 50, 'queen').todense()).all()

########NEW FILE########
__FILENAME__ = weights
__all__ = ['W', 'WSP']
__author__ = "Sergio J. Rey <srey@asu.edu> "

import pysal
import math
import numpy as np
import scipy.sparse
from os.path import basename as BASENAME
from pysal.weights import util


class W(object):
    """
    Spatial weights

    Parameters
    ----------
    neighbors       : dictionary
                      key is region ID, value is a list of neighbor IDS
                      Example:  {'a':['b'],'b':['a','c'],'c':['b']}
    weights = None  : dictionary
                      key is region ID, value is a list of edge weights
                      If not supplied all edge weights are assumed to have a weight of 1.
                      Example: {'a':[0.5],'b':[0.5,1.5],'c':[1.5]}
    id_order = None : list
                      An ordered list of ids, defines the order of
                      observations when iterating over W if not set,
                      lexicographical ordering is used to iterate and the
                      id_order_set property will return False.  This can be
                      set after creation by setting the 'id_order' property.
    silent_island_warning   : boolean
                              Switch to turn off (default on) print statements
                              for every observation with islands

    Attributes
    ----------

    asymmetries
    cardinalities
    diagW2
    diagWtW
    diagWtW_WW
    histogram
    id2i
    id_order
    id_order_set
    islands
    max_neighbors
    mean_neighbors
    min_neighbors
    n
    neighbor_offsets
    nonzero
    pct_nonzero
    s0
    s1
    s2
    s2array
    sd
    sparse
    trcW2
    trcWtW
    trcWtW_WW
    transform

    Examples
    --------
    >>> from pysal import W, lat2W
    >>> neighbors = {0: [3, 1], 1: [0, 4, 2], 2: [1, 5], 3: [0, 6, 4], 4: [1, 3, 7, 5], 5: [2, 4, 8], 6: [3, 7], 7: [4, 6, 8], 8: [5, 7]}
    >>> weights = {0: [1, 1], 1: [1, 1, 1], 2: [1, 1], 3: [1, 1, 1], 4: [1, 1, 1, 1], 5: [1, 1, 1], 6: [1, 1], 7: [1, 1, 1], 8: [1, 1]}
    >>> w = W(neighbors, weights)
    >>> "%.3f"%w.pct_nonzero
    '0.296'

    Read from external gal file

    >>> import pysal
    >>> w = pysal.open(pysal.examples.get_path("stl.gal")).read()
    >>> w.n
    78
    >>> "%.3f"%w.pct_nonzero
    '0.065'

    Set weights implicitly

    >>> neighbors = {0: [3, 1], 1: [0, 4, 2], 2: [1, 5], 3: [0, 6, 4], 4: [1, 3, 7, 5], 5: [2, 4, 8], 6: [3, 7], 7: [4, 6, 8], 8: [5, 7]}
    >>> w = W(neighbors)
    >>> "%.3f"%w.pct_nonzero
    '0.296'
    >>> w = lat2W(100, 100)
    >>> w.trcW2
    39600.0
    >>> w.trcWtW
    39600.0
    >>> w.transform='r'
    >>> w.trcW2
    2530.7222222222586
    >>> w.trcWtW
    2533.6666666666774

    Cardinality Histogram

    >>> w=pysal.rook_from_shapefile(pysal.examples.get_path("sacramentot2.shp"))
    >>> w.histogram
    [(1, 1), (2, 6), (3, 33), (4, 103), (5, 114), (6, 73), (7, 35), (8, 17), (9, 9), (10, 4), (11, 4), (12, 3), (13, 0), (14, 1)]

    Disconnected observations (islands)

    >>> w = pysal.W({1:[0],0:[1],2:[], 3:[]})
    WARNING: there are 2 disconnected observations
    Island ids:  [2, 3]

    """

    def __init__(self, neighbors, weights=None, id_order=None, silent_island_warning=False):
        self.silent_island_warning = silent_island_warning
        self.transformations = {}
        self.neighbors = neighbors
        if not weights:
            weights = {}
            for key in neighbors:
                weights[key] = [1.] * len(neighbors[key])
        self.weights = weights
        self.transformations['O'] = self.weights  # original weights
        self.transform = 'O'
        if id_order is None:
            self._id_order = self.neighbors.keys()
            self._id_order.sort()
            self._id_order_set = False
        else:
            self._id_order = id_order
            self._id_order_set = True
        self._reset()
        self._n = len(self.weights)
        if self.islands and not self.silent_island_warning:
            ni = len(self.islands)
            if ni == 1:
                print "WARNING: there is one disconnected observation (no neighbors)"
                print "Island id: ", self.islands
            else:
                print "WARNING: there are %d disconnected observations" % ni
                print "Island ids: ", self.islands

    def _reset(self):
        """
        Reset properties
        """
        self._cache = {}

    @property
    def sparse(self):
        """
        Sparse matrix object

        For any matrix manipulations required for w, w.sparse should be
        used. This is based on scipy.sparse.
        """
        if 'sparse' not in self._cache:
            self._sparse = self._build_sparse()
            self._cache['sparse'] = self._sparse
        return self._sparse

    def _build_sparse(self):
        """
        construct the sparse attribute
        """

        row = []
        col = []
        data = []
        id2i = self.id2i
        for id_i, neigh_list in self.neighbor_offsets.iteritems():
            card = self.cardinalities[id_i]
            row.extend([id2i[id_i]] * card)
            col.extend(neigh_list)
            data.extend(self.weights[id_i])
        row = np.array(row)
        col = np.array(col)
        data = np.array(data)
        s = scipy.sparse.csr_matrix((data, (row, col)), shape=(self.n, self.n))
        return s

    @property
    def id2i(self):
        """
        Dictionary where the key is an ID and the value is that ID's
        index in W.id_order.
        """
        if 'id2i' not in self._cache:
            self._id2i = {}
            for i, id_i in enumerate(self._id_order):
                self._id2i[id_i] = i
            self._id2i = self._id2i
            self._cache['id2i'] = self._id2i
        return self._id2i

    @property
    def n(self):
        """
        number of units
        """
        if "n" not in self._cache:
            self._n = len(self.neighbors)
            self._cache['n'] = self._n
        return self._n

    @property
    def s0(self):
        """
        float

        .. math::

               s0=\sum_i \sum_j w_{i,j}

        """
        if 's0' not in self._cache:
            self._s0 = self.sparse.sum()
            self._cache['s0'] = self._s0
        return self._s0

    @property
    def s1(self):
        """
        float

        .. math::

               s1=1/2 \sum_i \sum_j (w_{i,j} + w_{j,i})^2

        """
        if 's1' not in self._cache:
            t = self.sparse.transpose()
            t = t + self.sparse
            t2 = t.multiply(t)  # element-wise square
            self._s1 = t2.sum() / 2.
            self._cache['s1'] = self._s1
        return self._s1

    @property
    def s2array(self):
        """
        individual elements comprising s2

        See Also
        --------
        s2

        """
        if 's2array' not in self._cache:
            s = self.sparse
            self._s2array = np.array(s.sum(1) + s.sum(0).transpose()) ** 2
            self._cache['s2array'] = self._s2array
        return self._s2array

    @property
    def s2(self):
        """
        float

        .. math::

                s2=\sum_j (\sum_i w_{i,j} + \sum_i w_{j,i})^2

        """
        if 's2' not in self._cache:
            self._s2 = self.s2array.sum()
            self._cache['s2'] = self._s2
        return self._s2

    @property
    def trcW2(self):
        """
        Trace of :math:`WW`

        See Also
        --------
        diagW2

        """
        if 'trcW2' not in self._cache:
            self._trcW2 = self.diagW2.sum()
            self._cache['trcw2'] = self._trcW2
        return self._trcW2

    @property
    def diagW2(self):
        """
        Diagonal of :math:`WW` : array

        See Also
        --------
        trcW2

        """
        if 'diagw2' not in self._cache:
            self._diagW2 = (self.sparse * self.sparse).diagonal()
            self._cache['diagW2'] = self._diagW2
        return self._diagW2

    @property
    def diagWtW(self):
        """
        Diagonal of :math:`W^{'}W`  : array

        See Also
        --------
        trcWtW

        """
        if 'diagWtW' not in self._cache:
            self._diagWtW = (self.sparse.transpose() * self.sparse).diagonal()
            self._cache['diagWtW'] = self._diagWtW
        return self._diagWtW

    @property
    def trcWtW(self):
        """
        Trace of :math:`W^{'}W`  : float

        See Also
        --------
        diagWtW

        """
        if 'trcWtW' not in self._cache:
            self._trcWtW = self.diagWtW.sum()
            self._cache['trcWtW'] = self._trcWtW
        return self._trcWtW

    @property
    def diagWtW_WW(self):
        """
        diagonal of :math:`W^{'}W + WW`
        """
        if 'diagWtW_WW' not in self._cache:
            wt = self.sparse.transpose()
            w = self.sparse
            self._diagWtW_WW = (wt * w + w * w).diagonal()
            self._cache['diagWtW_WW'] = self._diagWtW_WW
        return self._diagWtW_WW

    @property
    def trcWtW_WW(self):
        """
        trace of :math:`W^{'}W + WW`
        """
        if 'trcWtW_WW' not in self._cache:
            self._trcWtW_WW = self.diagWtW_WW.sum()
            self._cache['trcWtW_WW'] = self._trcWtW_WW
        return self._trcWtW_WW

    @property
    def pct_nonzero(self):
        """
        percentage of nonzero weights
        """
        if 'pct_nonzero' not in self._cache:
            self._pct_nonzero = self.sparse.nnz / (1. * self._n ** 2)
            self._cache['pct_nonzero'] = self._pct_nonzero
        return self._pct_nonzero

    @property
    def cardinalities(self):
        """
        number of neighbors for each observation : dict
        """
        if 'cardinalities' not in self._cache:
            c = {}
            for i in self._id_order:
                c[i] = len(self.neighbors[i])
            self._cardinalities = c
            self._cache['cardinalities'] = self._cardinalities
        return self._cardinalities

    @property
    def max_neighbors(self):
        """
        largest number of neighbors
        """
        if 'max_neighbors' not in self._cache:
            self._max_neighbors = max(self.cardinalities.values())
            self._cache['max_neighbors'] = self._max_neighbors
        return self._max_neighbors

    @property
    def mean_neighbors(self):
        """
        average number of neighbors
        """
        if 'mean_neighbors' not in self._cache:
            self._mean_neighbors = np.mean(self.cardinalities.values())
            self._cache['mean_neighbors'] = self._mean_neighbors
        return self._mean_neighbors

    @property
    def min_neighbors(self):
        """
        minimum number of neighbors
        """
        if 'min_neighbors' not in self._cache:
            self._min_neighbors = min(self.cardinalities.values())
            self._cache['min_neighbors'] = self._min_neighbors
        return self._min_neighbors

    @property
    def nonzero(self):
        """
        number of nonzero weights
        """
        if 'nonzero' not in self._cache:
            self._nonzero = self.sparse.nnz
            self._cache['nonzero'] = self._nonzero
        return self._nonzero

    @property
    def sd(self):
        """
        standard deviation of number of neighbors : float
        """
        if 'sd' not in self._cache:
            self._sd = np.std(self.cardinalities.values())
            self._cache['sd'] = self._sd
        return self._sd

    @property
    def asymmetries(self):
        """
        list of id pairs with asymmetric weights
        """
        if 'asymmetries' not in self._cache:
            self._asymmetries = self.asymmetry()
            self._cache['asymmetries'] = self._asymmetries
        return self._asymmetries

    @property
    def islands(self):
        """
        list of ids without any neighbors
        """
        if 'islands' not in self._cache:
            self._islands = [i for i,
                             c in self.cardinalities.items() if c == 0]
            self._cache['islands'] = self._islands
        return self._islands

    @property
    def histogram(self):
        """
        cardinality histogram as a dictionary, key is the id, value is the
        number of neighbors for that unit
        """
        if 'histogram' not in self._cache:
            ct, bin = np.histogram(self.cardinalities.values(),
                                   range(self.min_neighbors, self.max_neighbors + 2))
            self._histogram = zip(bin, ct)
            self._cache['histogram'] = self._histogram
        return self._histogram

    def __getitem__(self, key):
        """
        Allow a dictionary like interaction with the weights class.

        Examples
        --------
        >>> from pysal import rook_from_shapefile as rfs
        >>> w = rfs(pysal.examples.get_path("10740.shp"))
        WARNING: there is one disconnected observation (no neighbors)
        Island id:  [163]
        >>> w[163]
        {}
        >>> w[0]
        {1: 1.0, 4: 1.0, 101: 1.0, 85: 1.0, 5: 1.0}
        """
        return dict(zip(self.neighbors[key], self.weights[key]))

    def __iter__(self):
        """
        Support iteration over weights

        Examples
        --------
        >>> import pysal
        >>> w=pysal.lat2W(3,3)
        >>> for i,wi in enumerate(w):
        ...     print i,wi
        ...
        0 {1: 1.0, 3: 1.0}
        1 {0: 1.0, 2: 1.0, 4: 1.0}
        2 {1: 1.0, 5: 1.0}
        3 {0: 1.0, 4: 1.0, 6: 1.0}
        4 {1: 1.0, 3: 1.0, 5: 1.0, 7: 1.0}
        5 {8: 1.0, 2: 1.0, 4: 1.0}
        6 {3: 1.0, 7: 1.0}
        7 {8: 1.0, 4: 1.0, 6: 1.0}
        8 {5: 1.0, 7: 1.0}
        >>>
        """
        class _W_iter:

            def __init__(self, w):
                self.w = w
                self.n = len(w._id_order)
                self._idx = 0

            def next(self):
                if self._idx >= self.n:
                    self._idx = 0
                    raise StopIteration
                value = self.w.__getitem__(self.w._id_order[self._idx])
                self._idx += 1
                return value
        return _W_iter(self)

    def __set_id_order(self, ordered_ids):
        """
        Set the iteration order in w.

        W can be iterated over. On construction the iteration order is set to
        the lexicographic order of the keys in the w.weights dictionary. If a specific order
        is required it can be set with this method.

        Parameters
        ----------

        ordered_ids : sequence
                      identifiers for observations in specified order

        Notes
        -----

        ordered_ids is checked against the ids implied by the keys in
        w.weights. If they are not equivalent sets an exception is raised and
        the iteration order is not changed.

        Examples
        --------

        >>> import pysal
        >>> w=pysal.lat2W(3,3)
        >>> for i,wi in enumerate(w):
        ...     print i,wi
        ...
        0 {1: 1.0, 3: 1.0}
        1 {0: 1.0, 2: 1.0, 4: 1.0}
        2 {1: 1.0, 5: 1.0}
        3 {0: 1.0, 4: 1.0, 6: 1.0}
        4 {1: 1.0, 3: 1.0, 5: 1.0, 7: 1.0}
        5 {8: 1.0, 2: 1.0, 4: 1.0}
        6 {3: 1.0, 7: 1.0}
        7 {8: 1.0, 4: 1.0, 6: 1.0}
        8 {5: 1.0, 7: 1.0}

        >>> w.id_order
        [0, 1, 2, 3, 4, 5, 6, 7, 8]
        >>> w.id_order=range(8,-1,-1)
        >>> w.id_order
        [8, 7, 6, 5, 4, 3, 2, 1, 0]
        >>> for i,w_i in enumerate(w):
        ...     print i,w_i
        ...
        0 {5: 1.0, 7: 1.0}
        1 {8: 1.0, 4: 1.0, 6: 1.0}
        2 {3: 1.0, 7: 1.0}
        3 {8: 1.0, 2: 1.0, 4: 1.0}
        4 {1: 1.0, 3: 1.0, 5: 1.0, 7: 1.0}
        5 {0: 1.0, 4: 1.0, 6: 1.0}
        6 {1: 1.0, 5: 1.0}
        7 {0: 1.0, 2: 1.0, 4: 1.0}
        8 {1: 1.0, 3: 1.0}
        >>>

        """
        if set(self._id_order) == set(ordered_ids):
            self._id_order = ordered_ids
            self._idx = 0
            self._id_order_set = True
            self._reset()
        else:
            raise Exception('ordered_ids do not align with W ids')

    def __get_id_order(self):
        """
        Returns the ids for the observations in the order in which they
        would be encountered if iterating over the weights .
        """
        return self._id_order

    id_order = property(__get_id_order, __set_id_order)

    @property
    def id_order_set(self):
        """
        Returns True if user has set id_order, False if not.

        Examples
        --------
        >>> from pysal import lat2W
        >>> w=lat2W()
        >>> w.id_order_set
        True
        """
        return self._id_order_set

    @property
    def neighbor_offsets(self):
        """
        Given the current id_order, neighbor_offsets[id] is the offsets of the
        id's neighbors in id_order

        Examples
        --------
        >>> from pysal import W
        >>> neighbors={'c': ['b'], 'b': ['c', 'a'], 'a': ['b']}
        >>> weights ={'c': [1.0], 'b': [1.0, 1.0], 'a': [1.0]}
        >>> w=W(neighbors,weights)
        >>> w.id_order = ['a','b','c']
        >>> w.neighbor_offsets['b']
        [2, 0]
        >>> w.id_order = ['b','a','c']
        >>> w.neighbor_offsets['b']
        [2, 1]
        """
        if "neighbors_0" not in self._cache:
            self.__neighbors_0 = {}
            id2i = self.id2i
            for id, neigh_list in self.neighbors.iteritems():
                self.__neighbors_0[id] = [id2i[neigh] for neigh in neigh_list]
            self._cache['neighbors_0'] = self.__neighbors_0
        return self.__neighbors_0

    def get_transform(self):
        """
        Getter for transform property

        Returns
        -------
        transformation : string (or none)

        Examples
        --------
        >>> from pysal import lat2W
        >>> w=lat2W()
        >>> w.weights[0]
        [1.0, 1.0]
        >>> w.transform
        'O'
        >>> w.transform='r'
        >>> w.weights[0]
        [0.5, 0.5]
        >>> w.transform='b'
        >>> w.weights[0]
        [1.0, 1.0]
        >>>
        """
        return self._transform

    def set_transform(self, value="B"):
        """
        Transformations of weights.

        Notes
        -----

        Transformations are applied only to the value of the weights at
        instantiation. Chaining of transformations cannot be done on a W
        instance.

        Parameters
        ----------
        transform : string (not case sensitive)

        .. table::

            ================   ======================================================
            transform string   value
            ================   ======================================================
            B                  Binary
            R                  Row-standardization (global sum=n)
            D                  Double-standardization (global sum=1)
            V                  Variance stabilizing
            O                  Restore original transformation (from instantiation)
            ================   ======================================================

        Examples
        --------
        >>> from pysal import lat2W
        >>> w=lat2W()
        >>> w.weights[0]
        [1.0, 1.0]
        >>> w.transform
        'O'
        >>> w.transform='r'
        >>> w.weights[0]
        [0.5, 0.5]
        >>> w.transform='b'
        >>> w.weights[0]
        [1.0, 1.0]
        >>>
        """
        value = value.upper()
        self._transform = value
        if value in self.transformations:
            self.weights = self.transformations[value]
            self._reset()
        else:
            if value == "R":
                # row standardized weights
                weights = {}
                self.weights = self.transformations['O']
                for i in self.weights:
                    wijs = self.weights[i]
                    row_sum = sum(wijs) * 1.0
                    if row_sum == 0.0:
                        if not self.silent_island_warning:
                            print 'WARNING: ', i, ' is an island (no neighbors)'
                    weights[i] = [wij / row_sum for wij in wijs]
                weights = weights
                self.transformations[value] = weights
                self.weights = weights
                self._reset()
            elif value == "D":
                # doubly-standardized weights
                # update current chars before doing global sum
                self._reset()
                s0 = self.s0
                ws = 1.0 / s0
                weights = {}
                self.weights = self.transformations['O']
                for i in self.weights:
                    wijs = self.weights[i]
                    weights[i] = [wij * ws for wij in wijs]
                weights = weights
                self.transformations[value] = weights
                self.weights = weights
                self._reset()
            elif value == "B":
                # binary transformation
                weights = {}
                self.weights = self.transformations['O']
                for i in self.weights:
                    wijs = self.weights[i]
                    weights[i] = [1.0 for wij in wijs]
                weights = weights
                self.transformations[value] = weights
                self.weights = weights
                self._reset()
            elif value == "V":
                # variance stabilizing
                weights = {}
                q = {}
                k = self.cardinalities
                s = {}
                Q = 0.0
                self.weights = self.transformations['O']
                for i in self.weights:
                    wijs = self.weights[i]
                    q[i] = math.sqrt(sum([wij * wij for wij in wijs]))
                    s[i] = [wij / q[i] for wij in wijs]
                    Q += sum([si for si in s[i]])
                nQ = self.n / Q
                for i in self.weights:
                    weights[i] = [w * nQ for w in s[i]]
                weights = weights
                self.transformations[value] = weights
                self.weights = weights
                self._reset()
            elif value == "O":
                # put weights back to original transformation
                weights = {}
                original = self.transformations[value]
                self.weights = original
                self._reset()
            else:
                print 'unsupported weights transformation'

    transform = property(get_transform, set_transform)

    def asymmetry(self, intrinsic=True):
        """
        Asymmetry check

        Parameters
        ----------
        intrinsic: boolean (default=True)
                
                intrinsic symmetry:
                      :math:`w_{i,j} == w_{j,i}`

                if intrisic is False:
                    symmetry is defined as :math:`i \in N_j \ AND \ j \in N_i` where
                    :math:`N_j` is the set of neighbors for j.
            
        Returns
        -------
        asymmetries : list
                      empty if no asymmetries are found
                      if asymmetries, then a list of (i,j) tuples is returned 

        Examples
        --------

        >>> from pysal import lat2W
        >>> w=lat2W(3,3)
        >>> w.asymmetry()
        []
        >>> w.transform='r'
        >>> w.asymmetry()
        [(0, 1), (0, 3), (1, 0), (1, 2), (1, 4), (2, 1), (2, 5), (3, 0), (3, 4), (3, 6), (4, 1), (4, 3), (4, 5), (4, 7), (5, 2), (5, 4), (5, 8), (6, 3), (6, 7), (7, 4), (7, 6), (7, 8), (8, 5), (8, 7)]
        >>> result = w.asymmetry(intrinsic=False)
        >>> result
        []
        >>> neighbors={0:[1,2,3], 1:[1,2,3], 2:[0,1], 3:[0,1]}
        >>> weights={0:[1,1,1], 1:[1,1,1], 2:[1,1], 3:[1,1]}
        >>> w=W(neighbors,weights)
        >>> w.asymmetry()
        [(0, 1), (1, 0)]
        """

        if intrinsic:
            wd = self.sparse.transpose() - self.sparse
        else:
            transform = self.transform
            self.transform = 'b'
            wd = self.sparse.transpose() - self.sparse
            self.transform = transform

        ids = np.nonzero(wd)
        if len(ids[0]) == 0:
            return []
        else:
            ijs = zip(ids[0], ids[1])
            ijs.sort()
            return ijs

    def full(self):
        """
        Generate a full numpy array

        Returns
        -------

        implicit : tuple
                   first element being the full numpy array and second element
                   keys being the ids associated with each row in the array.

        Examples
        --------
        >>> from pysal import W
        >>> neighbors={'first':['second'],'second':['first','third'],'third':['second']}
        >>> weights={'first':[1],'second':[1,1],'third':[1]}
        >>> w=W(neighbors,weights)
        >>> wf,ids=w.full()
        >>> wf
        array([[ 0.,  1.,  0.],
               [ 1.,  0.,  1.],
               [ 0.,  1.,  0.]])
        >>> ids
        ['first', 'second', 'third']

        See also
        --------
        full
        """
        return util.full(self)

    def towsp(self):
        '''
        Generate a WSP object

        Returns
        -------

        implicit : pysal.WSP
                   Thin W class

        Examples
        --------
        >>> import pysal as ps
        >>> from pysal import W
        >>> neighbors={'first':['second'],'second':['first','third'],'third':['second']}
        >>> weights={'first':[1],'second':[1,1],'third':[1]}
        >>> w=W(neighbors,weights)
        >>> wsp=w.towsp()
        >>> isinstance(wsp, ps.weights.weights.WSP)
        True
        >>> wsp.n
        3
        >>> wsp.s0
        4

        See also
        --------
        WSP
        '''
        return WSP(self.sparse, self._id_order)

    def set_shapefile(self, shapefile, idVariable=None, full=False):
        """
        Adding meta data for writing headers of gal and gwt files

        Parameters
        ----------

        shapefile : (string) 
                    shapefile name used to construct weights

        idVariable : (string) 
                    name of attribute in shapefile to associate with ids in the weights

        full : (boolean) 
                True - write out entire path for shapefile, False
                (default) only base of shapefile without extension

        """
        if full:
            self._shpName = shapefile
        else:
            self._shpName = BASENAME(shapefile).split(".")[0]

        self._varName = idVariable


class WSP(object):

    """
    Thin W class for spreg

    Parameters
    ----------

    sparse   : scipy sparse object
               NxN object from scipy.sparse

    id_order : list
               An ordered list of ids, assumed to match the ordering in
               sparse.

    Attributes
    ----------

    n
    s0
    trcWtW_WW

    Examples
    --------

    From GAL information

    >>> import scipy.sparse
    >>> import pysal
    >>> rows = [0, 1, 1, 2, 2, 3]
    >>> cols = [1, 0, 2, 1, 3, 3]
    >>> weights =  [1, 0.75, 0.25, 0.9, 0.1, 1]
    >>> sparse = scipy.sparse.csr_matrix((weights, (rows, cols)), shape=(4,4))
    >>> w = pysal.weights.WSP(sparse)
    >>> w.s0
    4.0
    >>> w.trcWtW_WW
    6.3949999999999996
    >>> w.n
    4

    """

    def __init__(self, sparse, id_order=None):
        if not scipy.sparse.issparse(sparse):
            raise ValueError("must pass a scipy sparse object")
        rows, cols = sparse.shape
        if rows != cols:
            raise ValueError("Weights object must be square")
        self.sparse = sparse.tocsr()
        self.n = sparse.shape[0]
        if id_order:
            if len(id_order) != self.n:
                raise ValueError(
                    "Number of values in id_order must match shape of sparse")
        self.id_order = id_order
        self._cache = {}

    @property
    def s0(self):
        """
        float

        .. math::

               s0=\sum_i \sum_j w_{i,j}

        """
        if 's0' not in self._cache:
            self._s0 = self.sparse.sum()
            self._cache['s0'] = self._s0
        return self._s0

    @property
    def trcWtW_WW(self):
        """
        trace of :math:`W^{'}W + WW`
        """
        if 'trcWtW_WW' not in self._cache:
            self._trcWtW_WW = self.diagWtW_WW.sum()
            self._cache['trcWtW_WW'] = self._trcWtW_WW
        return self._trcWtW_WW

    @property
    def diagWtW_WW(self):
        """
        diagonal of :math:`W^{'}W + WW`
        """
        if 'diagWtW_WW' not in self._cache:
            wt = self.sparse.transpose()
            w = self.sparse
            self._diagWtW_WW = (wt * w + w * w).diagonal()
            self._cache['diagWtW_WW'] = self._diagWtW_WW
        return self._diagWtW_WW

########NEW FILE########
__FILENAME__ = Wsets
"""
Set-like manipulation of weights matrices.
"""

__author__ = "Sergio J. Rey <srey@asu.edu>, Charles Schmidt <schmidtc@gmail.com>, David Folch <david.folch@asu.edu>, Dani Arribas-Bel <darribas@asu.edu>"

import pysal
import copy
from scipy.sparse import isspmatrix_csr
from numpy import ones

__all__ = ['w_union', 'w_intersection', 'w_difference',
           'w_symmetric_difference', 'w_subset', 'w_clip']


def w_union(w1, w2, silent_island_warning=False):
    """Returns a binary weights object, w, that includes all neighbor pairs that
    exist in either w1 or w2.

    Parameters
    ----------

    w1                      : W object
    w2                      : W object
    silent_island_warning   : boolean
                              Switch to turn off (default on) print statements
                              for every observation with islands

    Returns
    -------

    w       : W object

    Notes
    -----
    ID comparisons are performed using ==, therefore the integer ID 2 is
    equivalent to the float ID 2.0. Returns a matrix with all the unique IDs
    from w1 and w2.

    Examples
    --------

    Construct rook weights matrices for two regions, one is 4x4 (16 areas)
    and the other is 6x4 (24 areas). A union of these two weights matrices
    results in the new weights matrix matching the larger one.

    >>> import pysal
    >>> w1 = pysal.lat2W(4,4)
    >>> w2 = pysal.lat2W(6,4)
    >>> w = pysal.weights.w_union(w1, w2)
    >>> w1[0] == w[0]
    True
    >>> w1.neighbors[15]
    [11, 14]
    >>> w2.neighbors[15]
    [11, 14, 19]
    >>> w.neighbors[15]
    [19, 11, 14]
    >>>

    """
    neighbors = dict(w1.neighbors.items())
    for i in w2.neighbors:
        if i in neighbors:
            add_neigh = set(neighbors[i]).union(set(w2.neighbors[i]))
            neighbors[i] = list(add_neigh)
        else:
            neighbors[i] = copy.copy(w2.neighbors[i])
    return pysal.W(neighbors, silent_island_warning=silent_island_warning)


def w_intersection(w1, w2, w_shape='w1', silent_island_warning=False):
    """Returns a binary weights object, w, that includes only those neighbor
    pairs that exist in both w1 and w2.

    Parameters
    ----------

    w1                      : W object
    w2                      : W object
    w_shape                 : string
                              Defines the shape of the returned weights matrix. 'w1' returns a
                              matrix with the same IDs as w1; 'all' returns a matrix with all
                              the unique IDs from w1 and w2; and 'min' returns a matrix with
                              only the IDs occurring in both w1 and w2.
    silent_island_warning   : boolean
                              Switch to turn off (default on) print statements
                              for every observation with islands

    Returns
    -------

    w       : W object

    Notes
    -----
    ID comparisons are performed using ==, therefore the integer ID 2 is
    equivalent to the float ID 2.0.

    Examples
    --------

    Construct rook weights matrices for two regions, one is 4x4 (16 areas)
    and the other is 6x4 (24 areas). An intersection of these two weights
    matrices results in the new weights matrix matching the smaller one.

    >>> import pysal
    >>> w1 = pysal.lat2W(4,4)
    >>> w2 = pysal.lat2W(6,4)
    >>> w = pysal.weights.w_intersection(w1, w2)
    >>> w1[0] == w[0]
    True
    >>> w1.neighbors[15]
    [11, 14]
    >>> w2.neighbors[15]
    [11, 14, 19]
    >>> w.neighbors[15]
    [11, 14]
    >>>

    """
    if w_shape == 'w1':
        neigh_keys = w1.neighbors.keys()
    elif w_shape == 'all':
        neigh_keys = set(w1.neighbors.keys()).union(set(w2.neighbors.keys()))
    elif w_shape == 'min':
        neigh_keys = set(w1.neighbors.keys(
        )).intersection(set(w2.neighbors.keys()))
    else:
        raise Exception("invalid string passed to w_shape")

    neighbors = {}
    for i in neigh_keys:
        if i in w1.neighbors and i in w2.neighbors:
            add_neigh = set(w1.neighbors[i]).intersection(set(w2.neighbors[i]))
            neighbors[i] = list(add_neigh)
        else:
            neighbors[i] = []

    return pysal.W(neighbors, silent_island_warning=silent_island_warning)


def w_difference(w1, w2, w_shape='w1', constrained=True, silent_island_warning=False):
    """Returns a binary weights object, w, that includes only neighbor pairs
    in w1 that are not in w2. The w_shape and constrained parameters
    determine which pairs in w1 that are not in w2 are returned.

    Parameters
    ----------

    w1                      : W object
    w2                      : W object
    w_shape                 : string
                              Defines the shape of the returned weights matrix. 'w1' returns a
                              matrix with the same IDs as w1; 'all' returns a matrix with all
                              the unique IDs from w1 and w2; and 'min' returns a matrix with
                              the IDs occurring in w1 and not in w2.
    constrained             : boolean
                              If False then the full set of neighbor pairs in w1 that are
                              not in w2 are returned. If True then those pairs that would
                              not be possible if w_shape='min' are dropped. Ignored if
                              w_shape is set to 'min'.
    silent_island_warning   : boolean
                              Switch to turn off (default on) print statements
                              for every observation with islands

    Returns
    -------

    w       : W object

    Notes
    -----
    ID comparisons are performed using ==, therefore the integer ID 2 is
    equivalent to the float ID 2.0.

    Examples
    --------

    Construct rook (w2) and queen (w1) weights matrices for two 4x4 regions
    (16 areas). A queen matrix has all the joins a rook matrix does plus joins
    between areas that share a corner. The new matrix formed by the difference
    of rook from queen contains only join at corners (typically called a
    bishop matrix). Note that the difference of queen from rook would result
    in a weights matrix with no joins.

    >>> import pysal
    >>> w1 = pysal.lat2W(4,4,rook=False)
    >>> w2 = pysal.lat2W(4,4,rook=True)
    >>> w = pysal.weights.w_difference(w1, w2, constrained=False)
    >>> w1[0] == w[0]
    False
    >>> w1.neighbors[15]
    [10, 11, 14]
    >>> w2.neighbors[15]
    [11, 14]
    >>> w.neighbors[15]
    [10]
    >>>

    """
    if w_shape == 'w1':
        neigh_keys = w1.neighbors.keys()
    elif w_shape == 'all':
        neigh_keys = set(w1.neighbors.keys()).union(set(w2.neighbors.keys()))
    elif w_shape == 'min':
        neigh_keys = set(
            w1.neighbors.keys()).difference(set(w2.neighbors.keys()))
        if not neigh_keys:
            raise Exception("returned an empty weights matrix")
    else:
        raise Exception("invalid string passed to w_shape")

    neighbors = {}
    for i in neigh_keys:
        if i in w1.neighbors:
            if i in w2.neighbors:
                add_neigh = set(w1.neighbors[i]
                                ).difference(set(w2.neighbors[i]))
                neighbors[i] = list(add_neigh)
            else:
                neighbors[i] = copy.copy(w1.neighbors[i])
        else:
            neighbors[i] = []

    if constrained or w_shape == 'min':
        constrained_keys = set(
            w1.neighbors.keys()).difference(set(w2.neighbors.keys()))
        island_keys = set(neighbors.keys()).difference(constrained_keys)
        for i in island_keys:
            neighbors[i] = []
        for i in constrained_keys:
            neighbors[i] = list(
                set(neighbors[i]).intersection(constrained_keys))

    return pysal.W(neighbors, silent_island_warning=silent_island_warning)


def w_symmetric_difference(w1, w2, w_shape='all', constrained=True, silent_island_warning=False):
    """Returns a binary weights object, w, that includes only neighbor pairs
    that are not shared by w1 and w2. The w_shape and constrained parameters
    determine which pairs that are not shared by w1 and w2 are returned.

    Parameters
    ----------

    w1                      : W object
    w2                      : W object
    w_shape                 : string
                              Defines the shape of the returned weights matrix. 'all' returns a
                              matrix with all the unique IDs from w1 and w2; and 'min' returns
                              a matrix with the IDs not shared by w1 and w2.
    constrained             : boolean
                              If False then the full set of neighbor pairs that are not
                              shared by w1 and w2 are returned. If True then those pairs
                              that would not be possible if w_shape='min' are dropped.
                              Ignored if w_shape is set to 'min'.
    silent_island_warning   : boolean
                              Switch to turn off (default on) print statements
                              for every observation with islands

    Returns
    -------

    w       : W object

    Notes
    -----
    ID comparisons are performed using ==, therefore the integer ID 2 is
    equivalent to the float ID 2.0.

    Examples
    --------

    Construct queen weights matrix for a 4x4 (16 areas) region (w1) and a rook
    matrix for a 6x4 (24 areas) region (w2). The symmetric difference of these
    two matrices (with w_shape set to 'all' and constrained set to False)
    contains the corner joins in the overlap area, all the joins in the
    non-overlap area.

    >>> import pysal
    >>> w1 = pysal.lat2W(4,4,rook=False)
    >>> w2 = pysal.lat2W(6,4,rook=True)
    >>> w = pysal.weights.w_symmetric_difference(w1, w2, constrained=False)
    >>> w1[0] == w[0]
    False
    >>> w1.neighbors[15]
    [10, 11, 14]
    >>> w2.neighbors[15]
    [11, 14, 19]
    >>> w.neighbors[15]
    [10, 19]
    >>>

    """
    if w_shape == 'all':
        neigh_keys = set(w1.neighbors.keys()).union(set(w2.neighbors.keys()))
    elif w_shape == 'min':
        neigh_keys = set(w1.neighbors.keys(
        )).symmetric_difference(set(w2.neighbors.keys()))
    else:
        raise Exception("invalid string passed to w_shape")

    neighbors = {}
    for i in neigh_keys:
        if i in w1.neighbors:
            if i in w2.neighbors:
                add_neigh = set(w1.neighbors[i]).symmetric_difference(
                    set(w2.neighbors[i]))
                neighbors[i] = list(add_neigh)
            else:
                neighbors[i] = copy.copy(w1.neighbors[i])
        elif i in w2.neighbors:
            neighbors[i] = copy.copy(w2.neighbors[i])
        else:
            neighbors[i] = []

    if constrained or w_shape == 'min':
        constrained_keys = set(
            w1.neighbors.keys()).difference(set(w2.neighbors.keys()))
        island_keys = set(neighbors.keys()).difference(constrained_keys)
        for i in island_keys:
            neighbors[i] = []
        for i in constrained_keys:
            neighbors[i] = list(
                set(neighbors[i]).intersection(constrained_keys))

    return pysal.W(neighbors, silent_island_warning=silent_island_warning)


def w_subset(w1, ids, silent_island_warning=False):
    """Returns a binary weights object, w, that includes only those
    observations in ids.

    Parameters
    ----------

    w1                      : W object
    ids                     : list
                              A list containing the IDs to be include in the returned weights
                              object.
    silent_island_warning   : boolean
                              Switch to turn off (default on) print statements
                              for every observation with islands

    Returns
    -------

    w       : W object

    Examples
    --------

    Construct a rook weights matrix for a 6x4 region (24 areas). By default
    PySAL assigns integer IDs to the areas in a region. By passing in a list
    of integers from 0 to 15, the first 16 areas are extracted from the
    previous weights matrix, and only those joins relevant to the new region
    are retained.

    >>> import pysal
    >>> w1 = pysal.lat2W(6,4)
    >>> ids = range(16)
    >>> w = pysal.weights.w_subset(w1, ids)
    >>> w1[0] == w[0]
    True
    >>> w1.neighbors[15]
    [11, 14, 19]
    >>> w.neighbors[15]
    [11, 14]
    >>>

    """
    neighbors = {}
    ids_set = set(ids)
    for i in ids:
        if i in w1.neighbors:
            neigh_add = ids_set.intersection(set(w1.neighbors[i]))
            neighbors[i] = list(neigh_add)
        else:
            neighbors[i] = []

    return pysal.W(neighbors, id_order=ids, silent_island_warning=silent_island_warning)


def w_clip(w1, w2, outSP=True, silent_island_warning=False):
    '''
    Clip a continuous W object (w1) with a different W object (w2) so only cells where
    w2 has a non-zero value remain with non-zero values in w1

    Checks on w1 and w2 are performed to make sure they conform to the
    appropriate format and, if not, they are converted.

    Parameters
    ----------
    w1                      : pysal.W, scipy.sparse.csr.csr_matrix
                              Potentially continuous weights matrix to be clipped. The clipped
                              matrix wc will have at most the same elements as w1.
    w2                      : pysal.W, scipy.sparse.csr.csr_matrix
                              Weights matrix to use as shell to clip w1. Automatically
                              converted to binary format. Only non-zero elements in w2 will be
                              kept non-zero in wc. NOTE: assumed to be of the same shape as w1
    outSP                   : boolean
                              If True (default) return sparse version of the clipped W, if
                              False, return pysal.W object of the clipped matrix
    silent_island_warning   : boolean
                              Switch to turn off (default on) print statements
                              for every observation with islands

    Returns
    -------
    wc      : pysal.W, scipy.sparse.csr.csr_matrix
              Clipped W object (sparse if outSP=Ture). It inherits
              ``id_order`` from w1.

    Examples
    --------
    >>> import pysal as ps

    First create a W object from a lattice using queen contiguity and
    row-standardize it (note that these weights will stay when we clip the
    object, but they will not neccesarily represent a row-standardization
    anymore):

    >>> w1 = ps.lat2W(3, 2, rook=False)
    >>> w1.transform = 'R'

    We will clip that geography assuming observations 0, 2, 3 and 4 belong to
    one group and 1, 5 belong to another group and we don't want both groups
    to interact with each other in our weights (i.e. w_ij = 0 if i and j in
    different groups). For that, we use the following method:

    >>> w2 = ps.regime_weights(['r1', 'r2', 'r1', 'r1', 'r1', 'r2'])

    To illustrate that w2 will only be considered as binary even when the
    object passed is not, we can row-standardize it

    >>> w2.transform = 'R'

    The clipped object ``wc`` will contain only the spatial queen
    relationships that occur within one group ('r1' or 'r2') but will have
    gotten rid of those that happen across groups

    >>> wcs = ps.weights.Wsets.w_clip(w1, w2, outSP=True)

    This will create a sparse object (recommended when n is large).

    >>> wcs.sparse.toarray()
    array([[ 0.        ,  0.        ,  0.33333333,  0.33333333,  0.        ,
             0.        ],
           [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ],
           [ 0.2       ,  0.        ,  0.        ,  0.2       ,  0.2       ,
             0.        ],
           [ 0.2       ,  0.        ,  0.2       ,  0.        ,  0.2       ,
             0.        ],
           [ 0.        ,  0.        ,  0.33333333,  0.33333333,  0.        ,
             0.        ],
           [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ]])

    If we wanted an original W object, we can control that with the argument
    ``outSP``:

    >>> wc = ps.weights.Wsets.w_clip(w1, w2, outSP=False)
    WARNING: there are 2 disconnected observations
    Island ids:  [1, 5]
    >>> wc.full()[0]
    array([[ 0.        ,  0.        ,  0.33333333,  0.33333333,  0.        ,
             0.        ],
           [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ],
           [ 0.2       ,  0.        ,  0.        ,  0.2       ,  0.2       ,
             0.        ],
           [ 0.2       ,  0.        ,  0.2       ,  0.        ,  0.2       ,
             0.        ],
           [ 0.        ,  0.        ,  0.33333333,  0.33333333,  0.        ,
             0.        ],
           [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
             0.        ]])

    You can check they are actually the same:

    >>> wcs.sparse.toarray() == wc.full()[0]
    array([[ True,  True,  True,  True,  True,  True],
           [ True,  True,  True,  True,  True,  True],
           [ True,  True,  True,  True,  True,  True],
           [ True,  True,  True,  True,  True,  True],
           [ True,  True,  True,  True,  True,  True],
           [ True,  True,  True,  True,  True,  True]], dtype=bool)

    '''
    if not w1.id_order:
        w1.id_order = None
    id_order = w1.id_order
    if not isspmatrix_csr(w1):
        w1 = w1.sparse
    if not isspmatrix_csr(w2):
        w2 = w2.sparse
    w2.data = ones(w2.data.shape)
    wc = w1.multiply(w2)
    wc = pysal.weights.WSP(wc, id_order=id_order)
    if not outSP:
        wc = pysal.weights.WSP2W(wc, silent_island_warning=silent_island_warning)
    return wc



########NEW FILE########
__FILENAME__ = _contW_binning
#!/usr/bin/python
#import math
import pysal
from pysal.cg.standalone import get_shared_segments

__author__ = "Sergio J. Rey <srey@asu.edu> "
__all__ = ["QUEEN", "ROOK", "ContiguityWeights_binning",
           "ContiguityWeightsPolygons"]


import time

# delta to get buckets right
DELTA = 0.000001

QUEEN = 1
ROOK = 2

# constants for bucket sizes
BUCK_SM = 8
BUCK_LG = 80
SHP_SMALL = 1000


def bbcommon(bb, bbother):
    """
    Checks for overlaps of bounding boxes. First, east-west, then north-south.
    Element 0 is west, element 2 is east, element 3 is north, element 1 is
    south.
    All four checks must be false for chflag to be true, meaning the two
    bounding boxes do not overlap.
    """
    chflag = 0
    if not ((bbother[2] < bb[0]) or (bbother[0] > bb[2])):
        if not ((bbother[3] < bb[1]) or (bbother[1] > bb[3])):
            chflag = 1
    return chflag


class ContiguityWeights_binning:

    """
    Contiguity using a binning algorithm
    """

    def __init__(self, shpFileObject, wttype):
        self.shpFileObject = shpFileObject
        self.wttype = wttype
        self.do_weights()

    def do_weights(self):
        shpFileObject = self.shpFileObject

        if shpFileObject.type != pysal.cg.Polygon:
            return False

        shapebox = shpFileObject.bbox      # bounding box

        numPoly = len(shpFileObject)
        self.numPoly = numPoly

        # bucket size
        if (numPoly < SHP_SMALL):
            bucketmin = numPoly / BUCK_SM + 2
        else:
            bucketmin = numPoly / BUCK_LG + 2
            # print 'bucketmin: ', bucketmin
        # bucket length
        lengthx = ((shapebox[2] + DELTA) - shapebox[0]) / bucketmin
        lengthy = ((shapebox[3] + DELTA) - shapebox[1]) / bucketmin

        # print lengthx, lengthy

        # initialize buckets
        columns = [set() for i in range(bucketmin)]
        rows = [set() for i in range(bucketmin)]

        minbox = shapebox[:2] * 2             # minx,miny,minx,miny
        binWidth = [lengthx, lengthy] * 2      # lenx,leny,lenx,leny
        bbcache = {}
        poly2Column = [set() for i in range(numPoly)]
        poly2Row = [set() for i in range(numPoly)]
        for i in range(numPoly):
            shpObj = shpFileObject.get(i)
            bbcache[i] = shpObj.bounding_box[:]
            projBBox = [int((shpObj.bounding_box[:][j] -
                             minbox[j]) / binWidth[j]) for j in xrange(4)]
            for j in range(projBBox[0], projBBox[2] + 1):
                columns[j].add(i)
                poly2Column[i].add(j)
            for j in range(projBBox[1], projBBox[3] + 1):
                rows[j].add(i)
                poly2Row[i].add(j)

        w = {}
        if self.wttype == QUEEN:
            # loop over polygons rather than bins
            vertCache = {}
            for polyId in xrange(numPoly):
                if polyId not in vertCache:
                    vertCache[polyId] = set(shpFileObject.get(polyId).vertices)
                idRows = poly2Row[polyId]
                idCols = poly2Column[polyId]
                rowPotentialNeighbors = set()
                colPotentialNeighbors = set()
                for row in idRows:
                    rowPotentialNeighbors = rowPotentialNeighbors.union(
                        rows[row])
                for col in idCols:
                    colPotentialNeighbors = colPotentialNeighbors.union(
                        columns[col])
                potentialNeighbors = rowPotentialNeighbors.intersection(
                    colPotentialNeighbors)
                if polyId not in w:
                    w[polyId] = set()
                for j in potentialNeighbors:
                    if polyId < j:
                        if bbcommon(bbcache[polyId], bbcache[j]):
                            if j not in vertCache:
                                vertCache[j] = set(
                                    shpFileObject.get(j).vertices)
                            common = vertCache[
                                polyId].intersection(vertCache[j])
                            if len(common) > 0:
                                w[polyId].add(j)
                                if j not in w:
                                    w[j] = set()
                                w[j].add(polyId)
        elif self.wttype == ROOK:
            # check for a shared edge
            edgeCache = {}
            # loop over polygons rather than bins
            for polyId in xrange(numPoly):
                if polyId not in edgeCache:
                    iEdges = {}
                    iVerts = shpFileObject.get(polyId).vertices
                    nv = len(iVerts)
                    ne = nv - 1
                    for i in xrange(ne):
                        l = iVerts[i]
                        r = iVerts[i + 1]
                        iEdges[(l, r)] = []
                        iEdges[(r, l)] = []
                    edgeCache[polyId] = iEdges
                iEdgeSet = set(edgeCache[polyId].keys())
                idRows = poly2Row[polyId]
                idCols = poly2Column[polyId]
                rowPotentialNeighbors = set()
                colPotentialNeighbors = set()
                for row in idRows:
                    rowPotentialNeighbors = rowPotentialNeighbors.union(
                        rows[row])
                for col in idCols:
                    colPotentialNeighbors = colPotentialNeighbors.union(
                        columns[col])
                potentialNeighbors = rowPotentialNeighbors.intersection(
                    colPotentialNeighbors)
                if polyId not in w:
                    w[polyId] = set()
                for j in potentialNeighbors:
                    if polyId < j:
                        if bbcommon(bbcache[polyId], bbcache[j]):
                            if j not in edgeCache:
                                jVerts = shpFileObject.get(j).vertices
                                jEdges = {}
                                nv = len(jVerts)
                                ne = nv - 1
                                for e in xrange(ne):
                                    l = jVerts[e]
                                    r = jVerts[e + 1]
                                    jEdges[(l, r)] = []
                                    jEdges[(r, l)] = []
                                edgeCache[j] = jEdges
                            # for edge in edgeCache[j]:
                            if iEdgeSet.intersection(edgeCache[j].keys()):
                                w[polyId].add(j)
                                if j not in w:
                                    w[j] = set()
                                w[j].add(polyId)
                                # break
        else:
            print "Unsupported weight type."

        self.w = w

# Generalize to handle polygon collections - independent of origin file type


class ContiguityWeightsPolygons:

    """
    Contiguity for a collection of polygons using a binning algorithm
    """

    def __init__(self, collection, wttype=1):
        """

        Parameters
        ==========

        collection: PySAL PolygonCollection 

        wttype: int
                1: Queen
                2: Rook
        """

        self.collection = collection
        self.wttype = wttype
        self.do_weights()

    def do_weights(self):
        if self.collection.type != pysal.cg.Polygon:
            return False

        shapebox = self.collection.bbox      # bounding box

        numPoly = self.collection.n
        self.numPoly = numPoly

        # bucket size
        if (numPoly < SHP_SMALL):
            bucketmin = numPoly / BUCK_SM + 2
        else:
            bucketmin = numPoly / BUCK_LG + 2
            # print 'bucketmin: ', bucketmin
        # bucket length
        lengthx = ((shapebox[2] + DELTA) - shapebox[0]) / bucketmin
        lengthy = ((shapebox[3] + DELTA) - shapebox[1]) / bucketmin

        # print lengthx, lengthy

        # initialize buckets
        columns = [set() for i in range(bucketmin)]
        rows = [set() for i in range(bucketmin)]

        minbox = shapebox[:2] * 2             # minx,miny,minx,miny
        binWidth = [lengthx, lengthy] * 2      # lenx,leny,lenx,leny
        bbcache = {}
        poly2Column = [set() for i in range(numPoly)]
        poly2Row = [set() for i in range(numPoly)]
        for i in range(numPoly):
            shpObj = self.collection[i]
            bbcache[i] = shpObj.bbox[:]
            projBBox = [int((shpObj.bbox[:][j] -
                             minbox[j]) / binWidth[j]) for j in xrange(4)]
            for j in range(projBBox[0], projBBox[2] + 1):
                columns[j].add(i)
                poly2Column[i].add(j)
            for j in range(projBBox[1], projBBox[3] + 1):
                rows[j].add(i)
                poly2Row[i].add(j)

        w = {}
        if self.wttype == QUEEN:
            # loop over polygons rather than bins
            vertCache = {}
            for polyId in xrange(numPoly):
                if polyId not in vertCache:
                    vertCache[polyId] = set(self.collection[polyId].vertices)
                idRows = poly2Row[polyId]
                idCols = poly2Column[polyId]
                rowPotentialNeighbors = set()
                colPotentialNeighbors = set()
                for row in idRows:
                    rowPotentialNeighbors = rowPotentialNeighbors.union(
                        rows[row])
                for col in idCols:
                    colPotentialNeighbors = colPotentialNeighbors.union(
                        columns[col])
                potentialNeighbors = rowPotentialNeighbors.intersection(
                    colPotentialNeighbors)
                if polyId not in w:
                    w[polyId] = set()
                for j in potentialNeighbors:
                    if polyId < j:
                        if j not in vertCache:
                            vertCache[j] = set(self.collection[j].vertices)
                        if bbcommon(bbcache[polyId], bbcache[j]):
                            vertCache[j] = set(self.collection[j].vertices)
                            common = vertCache[
                                polyId].intersection(vertCache[j])
                            if len(common) > 0:
                                w[polyId].add(j)
                                if j not in w:
                                    w[j] = set()
                                w[j].add(polyId)
        elif self.wttype == ROOK:
            # check for a shared edge
            edgeCache = {}
            # loop over polygons rather than bins
            for polyId in xrange(numPoly):
                if polyId not in edgeCache:
                    iEdges = {}
                    iVerts = shpFileObject.get(polyId).vertices
                    nv = len(iVerts)
                    ne = nv - 1
                    for i in xrange(ne):
                        l = iVerts[i]
                        r = iVerts[i + 1]
                        iEdges[(l, r)] = []
                        iEdges[(r, l)] = []
                    edgeCache[polyId] = iEdges
                iEdgeSet = set(edgeCache[polyId].keys())
                idRows = poly2Row[polyId]
                idCols = poly2Column[polyId]
                rowPotentialNeighbors = set()
                colPotentialNeighbors = set()
                for row in idRows:
                    rowPotentialNeighbors = rowPotentialNeighbors.union(
                        rows[row])
                for col in idCols:
                    colPotentialNeighbors = colPotentialNeighbors.union(
                        columns[col])
                potentialNeighbors = rowPotentialNeighbors.intersection(
                    colPotentialNeighbors)
                if polyId not in w:
                    w[polyId] = set()
                for j in potentialNeighbors:
                    if polyId < j:
                        if bbcommon(bbcache[polyId], bbcache[j]):
                            if j not in edgeCache:
                                jVerts = shpFileObject.get(j).vertices
                                jEdges = {}
                                nv = len(jVerts)
                                ne = nv - 1
                                for e in xrange(ne):
                                    l = jVerts[e]
                                    r = jVerts[e + 1]
                                    jEdges[(l, r)] = []
                                    jEdges[(r, l)] = []
                                edgeCache[j] = jEdges
                            # for edge in edgeCache[j]:
                            if iEdgeSet.intersection(edgeCache[j].keys()):
                                w[polyId].add(j)
                                if j not in w:
                                    w[j] = set()
                                w[j].add(polyId)
                                # break
        else:
            print "Unsupported weight type."

        self.w = w

if __name__ == "__main__":
    import time
    fname = pysal.examples.get_path('NAT.shp')
    print 'QUEEN binning'
    t0 = time.time()
    qb = ContiguityWeights_binning(pysal.open(fname), QUEEN)
    t1 = time.time()
    print "using " + str(fname)
    print "time elapsed for queen... using bins: " + str(t1 - t0)

    t0 = time.time()
    rb = ContiguityWeights_binning(pysal.open(fname), ROOK)
    t1 = time.time()
    print 'Rook binning'
    print "using " + str(fname)
    print "time elapsed for rook... using bins: " + str(t1 - t0)

    from _contW_rtree import ContiguityWeights_rtree

    t0 = time.time()
    rt = ContiguityWeights_rtree(pysal.open(fname), ROOK)
    t1 = time.time()

    print "time elapsed for rook... using rtree: " + str(t1 - t0)
    print rt.w == rb.w

    print 'QUEEN'
    t0 = time.time()
    qt = ContiguityWeights_rtree(pysal.open(fname), QUEEN)
    t1 = time.time()
    print "using " + str(fname)
    print "time elapsed for queen... using rtree: " + str(t1 - t0)
    print qb.w == qt.w

    print 'knn4'
    t0 = time.time()
    knn = pysal.knnW_from_shapefile(fname, k=4)
    t1 = time.time()
    print t1 - t0

    print 'rook from shapefile'
    t0 = time.time()
    knn = pysal.rook_from_shapefile(fname)
    t1 = time.time()
    print t1 - t0

########NEW FILE########
__FILENAME__ = _contW_rtree
import pysal.cg.rtree as rtree
from pysal.cg.standalone import get_shared_segments
#Order by Degree of connectivity, i.e. rook is more connected then queen.
QUEEN = 1
ROOK = 2

__author__ = "Charles R Schmidt <schmidtc@gmail.com>"
__all__ = ["QUEEN", "ROOK", "ContiguityWeights_rtree"]

Q_TARGET_MEM_SIZE = 250 * 1024 * 1024  # 250mb


class _PolyQ(dict):
    def __init__(self):
        dict.__init__(self)
        self.size = 20  # use the first 20 objects to calculate the average Size.
        self.ids = []

    def __checkSize(self):
        """
        Use the objects in the Q to calculate the average size of the objects
        Adjust Q.size to hold Q_TARGET_MEM_SIZE/avgSize object
        This is as many average size object that fit into Q_TARGET_MEM_SIZE
        """
        if len(self.ids) > 50:
            return True
        return False

    def add(self, poly):
        if poly.id not in self:
            if len(self.ids) >= self.size:
                if self.__checkSize():
                    del self[self.ids.pop(0)]
            self[poly.id] = poly
            self.ids.append(poly.id)


class ContiguityWeights_rtree:
    def __init__(self, geoObj, joinType=ROOK):
        self.index = rtree.Rtree()
        self.geoObj = geoObj
        self.joinType = joinType
        self.w = {}
        self.Q = _PolyQ()
        self.cache_hits = 0
        self.cache_misses = 0
        self.create()
        #print "Misses: ",self.cache_misses
        #print "Hits: ",self.cache_hits

    def create(self):
        for id, poly in enumerate(self.geoObj):
            poly.id = id
            self.append(poly)

    def append(self, poly):
        self.Q.add(poly)
        b = poly.bounding_box
        bbox = [b.left, b.lower, b.right, b.upper]
        for id in self.index.intersection(bbox):
            id = int(id)
            if self.check(id, poly) >= self.joinType:
                self.setW(id, poly.id)
        if poly.id not in self.w:  # add the null cases
            self.w[poly.id] = set()
        self.index.add(poly.id, bbox)

    def setW(self, id0, id1):
        "updates the W matrix seting two polygon's as neighbors"
        w = self.w
        if id0 not in w:
            w[id0] = set()
        if id1 not in w:
            w[id1] = set()
        w[id0].add(id1)
        w[id1].add(id0)

    def check(self, id0, poly1):
        "Check's if two polygon's are neighbors"
        if id0 in self.Q:
            self.cache_hits += 1
            poly0 = self.Q[id0]
        else:
            self.cache_misses += 1
            poly0 = self.geoObj.get(id0)
            poly0.id = id0
            self.Q.add(poly0)
        common = set(poly0.vertices).intersection(set(poly1.vertices))
        if len(common) > 1 and self.joinType == ROOK:
            #double check rook
            if get_shared_segments(poly0, poly1, True):
                return ROOK
            return False
            #for vert in common:
            #    idx = poly0.vertices.index(vert)
            #    IDX = poly1.vertices.index(vert)
            #    try:
            #        if poly0.vertices[idx+1] == poly1.vertices[IDX+1] or poly0.vertices[idx+1] == poly1.vertices[IDX-1]\
            #        or poly0.vertices[idx-1] == poly1.vertices[IDX+1] or poly0.vertices[idx-1] == poly1.vertices[IDX-1]:
            #            return ROOK
            #    except IndexError:
            #        pass
            #return False
        elif len(common) > 0:
            return QUEEN
        else:
            return False

if __name__ == '__main__':
    import pysal
    import time
    t0 = time.time()
    shp = pysal.open(pysal.examples.get_path('10740.shp'), 'r')
    w = ContiguityWeights_rtree(shp, QUEEN)
    t1 = time.time()
    print "Completed in: ", t1 - t0, "seconds using rtree"

########NEW FILE########
__FILENAME__ = cron
#!/usr/bin/python
"""Sets and unsets a lock on the local pysal repository."""
import os, sys

#check lock
if os.path.exists('/tmp/pysal.lock'):
    print "LOCK IN PLACE, another process is running perhaps?"
    sys.exit(1)
else:
    lck = open('/tmp/pysal.lock','w')
    lck.write('%d'%os.getpid())
    lck.close()
    lck = True
    os.system('/Users/stephens/Dropbox/work/Projects/pysal/trunk/tools/test.sh')
    os.remove('/tmp/pysal.lock')


########NEW FILE########
__FILENAME__ = py3tool
#!/usr/bin/env python3
# -*- python -*-
"""
%prog SUBMODULE...

Hack to pipe submodules of Numpy through 2to3 and build them in-place
one-by-one.

Example usage:

    python3 tools/py3tool.py testing distutils core

This will copy files to _py3k/numpy, add a dummy __init__.py and
version.py on the top level, and copy and 2to3 the files of the three
submodules.

When running py3tool again, only changed files are re-processed, which
makes the test-bugfix cycle faster.

"""
from optparse import OptionParser
import shutil
import os
import sys
import re
import subprocess
import fnmatch

if os.environ.get('USE_2TO3CACHE'):
    import lib2to3cache

BASE = os.path.normpath(os.path.join(os.path.dirname(__file__), '..'))
TEMP = os.path.normpath(os.path.join(BASE, '_py3k'))

SCRIPT_2TO3 = os.path.join(BASE, 'tools', '2to3.py')

EXTRA_2TO3_FLAGS = {
    '*/setup.py': '-x import',
    #'numpy/core/code_generators/generate_umath.py': '-x import',
    #'numpy/core/code_generators/generate_numpy_api.py': '-x import',
    #'numpy/core/code_generators/generate_ufunc_api.py': '-x import',
    #'numpy/core/defchararray.py': '-x unicode',
    #'numpy/compat/py3k.py': '-x unicode',
    #'numpy/ma/timer_comparison.py': 'skip',
    #'numpy/distutils/system_info.py': '-x reduce',
    #'numpy/f2py/auxfuncs.py': '-x reduce',
    #'numpy/lib/arrayterator.py': '-x reduce',
    #'numpy/lib/tests/test_arrayterator.py': '-x reduce',
    #'numpy/ma/core.py': '-x reduce',
    #'numpy/ma/tests/test_core.py': '-x reduce',
    #'numpy/ma/tests/test_old_ma.py': '-x reduce',
    #'numpy/ma/timer_comparison.py': '-x reduce',
    #'numpy/oldnumeric/ma.py': '-x reduce',
}

def main():
    p = OptionParser(usage=__doc__.strip())
    p.add_option("--clean", "-c", action="store_true",
                 help="clean source directory")
    options, args = p.parse_args()

    if not args:
        p.error('no submodules given')
    else:
        dirs = ['scipy/%s' % x for x in map(os.path.basename, args)]

    # Prepare
    if not os.path.isdir(TEMP):
        os.makedirs(TEMP)

    # Set up dummy files (for building only submodules)
    dummy_files = {
        '__init__.py': 'from scipy.version import version as __version__',
        'version.py': 'version = "0.8.0.dev"'
    }

    for fn, content in dummy_files.items():
        fn = os.path.join(TEMP, 'scipy', fn)
        if not os.path.isfile(fn):
            try:
                os.makedirs(os.path.dirname(fn))
            except OSError:
                pass
            f = open(fn, 'wb+')
            f.write(content.encode('ascii'))
            f.close()

    # Environment
    pp = [os.path.abspath(TEMP)]
    def getenv():
        env = dict(os.environ)
        env.update({'PYTHONPATH': ':'.join(pp)})
        return env

    # Copy
    for d in dirs:
        src = os.path.join(BASE, d)
        dst = os.path.join(TEMP, d)

        # Run 2to3
        sync_2to3(dst=dst,
                  src=src,
                  patchfile=os.path.join(TEMP, os.path.basename(d) + '.patch'),
                  clean=options.clean)

        # Run setup.py, falling back to Pdb post-mortem on exceptions
        setup_py = os.path.join(dst, 'setup.py')
        if os.path.isfile(setup_py):
            code = """\
import pdb, sys, traceback
p = pdb.Pdb()
try:
    import __main__
    __main__.__dict__.update({
        "__name__": "__main__", "__file__": "setup.py",
        "__builtins__": __builtins__})
    fp = open("setup.py", "rb")
    try:
        exec(compile(fp.read(), "setup.py", 'exec'))
    finally:
        fp.close()
except SystemExit:
    raise
except:
    traceback.print_exc()
    t = sys.exc_info()[2]
    p.interaction(None, t)
"""
            ret = subprocess.call([sys.executable, '-c', code,
                                   'build_ext', '-i'],
                                  cwd=dst,
                                  env=getenv())
            if ret != 0:
                raise RuntimeError("Build failed.")

        # Run nosetests
        subprocess.call(['nosetests3', '-v', d], cwd=TEMP)

def custom_mangling(filename):
    import_mangling = [
        os.path.join('cluster', '__init__.py'),
        os.path.join('cluster', 'hierarchy.py'),
        os.path.join('cluster', 'vq.py'),
        os.path.join('fftpack', 'basic.py'),
        os.path.join('fftpack', 'pseudo_diffs.py'),
        os.path.join('integrate', 'odepack.py'),
        os.path.join('integrate', 'quadpack.py'),
        os.path.join('integrate', '_ode.py'),
        os.path.join('interpolate', 'fitpack.py'),
        os.path.join('interpolate', 'fitpack2.py'),
        os.path.join('interpolate', 'interpolate.py'),
        os.path.join('interpolate', 'interpolate_wrapper.py'),
        os.path.join('interpolate', 'ndgriddata.py'),
        os.path.join('io', 'array_import.py'),
        os.path.join('io', '__init__.py'),
        os.path.join('io', 'matlab', 'miobase.py'),
        os.path.join('io', 'matlab', 'mio4.py'),
        os.path.join('io', 'matlab', 'mio5.py'),
        os.path.join('io', 'matlab', 'mio5_params.py'),
        os.path.join('linalg', 'basic.py'),
        os.path.join('linalg', 'decomp.py'),
        os.path.join('linalg', 'lapack.py'),
        os.path.join('linalg', 'flinalg.py'),
        os.path.join('linalg', 'iterative.py'),
        os.path.join('linalg', 'misc.py'),
        os.path.join('lib', 'blas', '__init__.py'),
        os.path.join('lib', 'lapack', '__init__.py'),
        os.path.join('ndimage', 'filters.py'),
        os.path.join('ndimage', 'fourier.py'),
        os.path.join('ndimage', 'interpolation.py'),
        os.path.join('ndimage', 'measurements.py'),
        os.path.join('ndimage', 'morphology.py'),
        os.path.join('optimize', 'minpack.py'),
        os.path.join('optimize', 'zeros.py'),
        os.path.join('optimize', 'lbfgsb.py'),
        os.path.join('optimize', 'cobyla.py'),
        os.path.join('optimize', 'slsqp.py'),
        os.path.join('optimize', 'nnls.py'),
        os.path.join('signal', '__init__.py'),
        os.path.join('signal', 'bsplines.py'),
        os.path.join('signal', 'signaltools.py'),
        os.path.join('signal', 'fir_filter_design.py'),
        os.path.join('special', '__init__.py'),
        os.path.join('special', 'add_newdocs.py'),
        os.path.join('special', 'basic.py'),
        os.path.join('special', 'orthogonal.py'),
        os.path.join('spatial', '__init__.py'),
        os.path.join('spatial', 'distance.py'),
        os.path.join('sparse', 'linalg', 'isolve', 'iterative.py'),
        os.path.join('sparse', 'linalg', 'dsolve', 'linsolve.py'),
        os.path.join('sparse', 'linalg', 'dsolve', 'umfpack', 'umfpack.py'),
        os.path.join('sparse', 'linalg', 'eigen', 'arpack', 'arpack.py'),
        os.path.join('sparse', 'linalg', 'eigen', 'arpack', 'speigs.py'),
        os.path.join('sparse', 'linalg', 'iterative', 'isolve', 'iterative.py'),
        os.path.join('stats', 'stats.py'),
        os.path.join('stats', 'distributions.py'),
        os.path.join('stats', 'morestats.py'),
        os.path.join('stats', 'kde.py'),
        os.path.join('stats', 'mstats_basic.py'),
    ]

    if any(filename.endswith(x) for x in import_mangling):
        print(filename)
        f = open(filename, 'r', encoding='utf-8')
        text = f.read()
        f.close()
        for mod in ['_vq', '_hierarchy_wrap', '_fftpack', 'convolve',
                    '_flinalg', 'fblas', 'flapack', 'cblas', 'clapack',
                    'calc_lwork', '_cephes', 'specfun', 'orthogonal_eval',
                    'lambertw', 'ckdtree', '_distance_wrap', '_logit',
                    '_minpack', '_zeros', '_lbfgsb', '_cobyla', '_slsqp',
                    '_nnls',
                    'sigtools', 'spline', 'spectral',
                    '_fitpack', 'dfitpack', '_interpolate',
                    '_odepack', '_quadpack', 'vode', '_dop',
                    'vonmises_cython',
                    'futil', 'mvn',
                    '_nd_image',
                    'numpyio',
                    '_superlu', '_arpack', '_iterative', '_umfpack',
                    'interpnd',
                    'mio_utils', 'mio5_utils', 'streams'
                    ]:
            text = re.sub(r'^(\s*)import %s' % mod,
                          r'\1from . import %s' % mod,
                          text, flags=re.M)
            text = re.sub(r'^(\s*)from %s import' % mod,
                          r'\1from .%s import' % mod,
                          text, flags=re.M)
        #text = text.replace('from matrixlib', 'from .matrixlib')
        f = open(filename, 'w', encoding='utf-8')
        f.write(text)
        f.close()

def walk_sync(dir1, dir2, _seen=None):
    if _seen is None:
        seen = {}
    else:
        seen = _seen

    if not dir1.endswith(os.path.sep):
        dir1 = dir1 + os.path.sep

    # Walk through stuff (which we haven't yet gone through) in dir1
    for root, dirs, files in os.walk(dir1):
        sub = root[len(dir1):]
        if sub in seen:
            dirs = [x for x in dirs if x not in seen[sub][0]]
            files = [x for x in files if x not in seen[sub][1]]
            seen[sub][0].extend(dirs)
            seen[sub][1].extend(files)
        else:
            seen[sub] = (dirs, files)
        if not dirs and not files:
            continue
        yield os.path.join(dir1, sub), os.path.join(dir2, sub), dirs, files

    if _seen is None:
        # Walk through stuff (which we haven't yet gone through) in dir2
        for root2, root1, dirs, files in walk_sync(dir2, dir1, _seen=seen):
            yield root1, root2, dirs, files

def sync_2to3(src, dst, patchfile=None, clean=False):
    import lib2to3.main
    from io import StringIO

    to_convert = []

    for src_dir, dst_dir, dirs, files in walk_sync(src, dst):
        for fn in dirs + files:
            src_fn = os.path.join(src_dir, fn)
            dst_fn = os.path.join(dst_dir, fn)

            # skip temporary etc. files
            if fn.startswith('.#') or fn.endswith('~'):
                continue

            # remove non-existing
            if os.path.exists(dst_fn) and not os.path.exists(src_fn):
                if clean:
                    if os.path.isdir(dst_fn):
                        shutil.rmtree(dst_fn)
                    else:
                        os.unlink(dst_fn)
                continue

            # make directories
            if os.path.isdir(src_fn):
                if not os.path.isdir(dst_fn):
                    os.makedirs(dst_fn)
                continue

            dst_dir = os.path.dirname(dst_fn)
            if os.path.isfile(dst_fn) and not os.path.isdir(dst_dir):
                os.makedirs(dst_dir)

            # don't replace up-to-date files
            try:
                if os.path.isfile(dst_fn) and \
                       os.stat(dst_fn).st_mtime >= os.stat(src_fn).st_mtime:
                    continue
            except OSError:
                pass

            # copy file
            shutil.copyfile(src_fn, dst_fn)

            # add .py files to 2to3 list
            if dst_fn.endswith('.py'):
                to_convert.append((src_fn, dst_fn))

    # run 2to3
    flag_sets = {}
    for fn, dst_fn in to_convert:
        flag = ''
        for pat, opt in EXTRA_2TO3_FLAGS.items():
            if fnmatch.fnmatch(fn, pat):
                flag = opt
                break
        flag_sets.setdefault(flag, []).append(dst_fn)

    if patchfile:
        p = open(patchfile, 'wb+')
    else:
        p = open(os.devnull, 'wb')

    for flags, filenames in flag_sets.items():
        if flags == 'skip':
            continue

        _old_stdout = sys.stdout
        try:
            sys.stdout = StringIO()
            lib2to3.main.main("lib2to3.fixes", ['-w', '-n'] + flags.split()+filenames)
        finally:
            sys.stdout = _old_stdout

    for fn, dst_fn in to_convert:
        # perform custom mangling
        custom_mangling(dst_fn)

    p.close()

if __name__ == "__main__":
    main()

########NEW FILE########
