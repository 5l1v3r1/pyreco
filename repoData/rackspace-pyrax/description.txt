# Auto Scale

## Basic Concepts
Auto Scale is a service that enables you to scale your application by adding or removing servers based on monitoring events, a schedule, or arbitrary webhooks.

Please note that _this is a Rackspace-specific service_. It is not available in any other OpenStack cloud, so if you add it to your application, keep the code isolated if you need to run your application on non-Rackspace clouds.

Auto Scale functions by linking three services:

* Monitoring (such as Monitoring as a Service)
* Auto Scale API
* Servers and Load Balancers


## Workflow

A _scaling group_ is monitored by Rackspace Cloud Monitoring. When Monitoring triggers an alarm for high utilization within the scaling group, a webhook is triggered. The webhook calls the Auto Scale service, which consults a policy in accordance with the webhook. The policy determines how many additional Cloud Servers should be added or removed in accordance with the alarm.

Alarms may trigger scaling up or scaling down. Scale-down events always remove the oldest server in the group.

Cooldowns allow you to ensure that you don't scale up or down too fast. When a scaling policy runs, both the scaling policy cooldown and the group cooldown start. Any additional requests to the group are discarded while the group cooldown is active. Any additional requests to the specific policy are discarded when the policy cooldown is active.

It is important to remember that Auto Scale does not configure anything within a server. This means that all images should be self-provisioning. It is up to you to make sure that your services are configured to function properly when the server is started. We recommend using something like Chef, Salt, or Puppet.


## Using Auto Scale in pyrax
Once you have authenticated, you can reference the Auto Scale service via `pyrax.autoscale`. That is a lot to type again and again in your code, so it is easier if you include the following line at the beginning of your code:

    au = pyrax.autoscale

Then you can simply use the alias `au` to reference the service. All of the code samples in this document assume that `au` has been defined this way.


## The Scaling Group
The **scaling group** is the basic unit of Auto Scale. It determines the minimum and maximum number of servers that exist at any time for the group, the cooldown period between scaling events, the configuration for each new server, the load balancer to add these servers to (optional), and any policies that are used for this group.

### Listing Your Scaling Groups
The `list()` method displays all the scaling groups currently defined in your account:

    print au.list()

This returns a list of `ScalingGroup` objects:

    [<ScalingGroup activeCapacity=3, desiredCapacity=3,
    id=2747cd20-39cb-443d-9217-53107775ba37, paused=False,
    pendingCapacity=0, name=FirstTest, cooldown=120, metadata={},
    min_entities=1, max_entities=4>,
    <ScalingGroup activeCapacity=2, desiredCapacity=2,
    id=76ee2de8-4df2-4664-b55c-60d0edd5dff8, paused=False,
    pendingCapacity=0, name=SecondTest, cooldown=90, metadata={},
    min_entities=2, max_entities=5>]

To see the [launch configuration](#launch_configuration) for a group, call the `get_launch_config()` method:

    groups = au.list()
    group = groups[0]
    print group.get_launch_config()

This returns a dict with the current values for its launch configuration:

    {'disk_config': u'AUTO',
     'flavor': 'performance1-2',
     'image': u'7789e8ca-b9df-495f-b47d-736a5f7b885a',
     'load_balancers': [{u'loadBalancerId': 175295, u'port': 80}],
     'metadata': {u'somekey': u'somevalue'},
     'name': u'rrr',
     'networks': [{u'uuid': u'11111111-1111-1111-1111-111111111111'}],
     'personality': [],
     'type': u'launch_server'}

### Getting the Current State of a Scaling Group
It is sometimes helpful to determine what the current state of a scaling group is in terms of whether it is scaling up, scaling down, or stable. To do this, call the scaling group's `get_state()` method, or call the client's `get_state()`, passing in the desired scaling group:

    print sg.get_state()
    # or
    print au.get_state(sg)

This returns a dict with the following structure:

    {'active': [u'21d0bce0-c7b9-46f8-80a2-9575bef8b83a', u'3e2c9eac-d7c2-4999-9a17-a6a7ca5d10e7'],
    'desired_capacity': 2,
    'paused': False,
    'pending_capacity': 0,
    'active_capacity': 2}

The `active` key holds a list of the IDs of the servers created as part of this scaling group. The `paused` key shows whether or not the scaling group's response to alarms is active or not. There are 3 'capacity'-related keys: `active_capacity`, `desired_capacity`, and `pending_capacity`:

Key | Respresents
---- | ----
**active_capacity** | The number of active servers that are part of this scaling group.
**desired_capacity** | The target number of servers for this scaling group, based on the combination of configuration settings and monitoring alarm responses.
**pending_capacity** | The number of servers which are in the process of being created (when positive) or destroyed (when negative).

### Pausing a Scaling Group's Policies
If you wish to pause the execution of a scaling group's policies for any reason, call its `pause()` method, or the `pause()` method of the client:

    sg.pause()
    # or
    au.pause(sg)

There is a corresponding `resume()` method for when you want to re-activate the policies:

    sg.resume()
    # or
    au.resume(sg)

### Creating a Scaling Group
To create a scaling group, you call the `create()` method of the client with the desired parameter values:

    sg = au.create("MyScalingGroup", cooldown=120, min_entities=2,
            max_entities=16, launch_config_type="launch_server",
            server_name="sg_test", flavor=my_flavor_id, image=my_image_id,
            disk_config="AUTO", metadata={"mykey": "myvalue"},
            load_balancers=(1234, 80))

This creates the scaling group with the name "MyScalingGroup", and returns a `ScalingGroup` object representing the new group. Since the `min_entities` is 2, it immediately creates 2 servers for the group, based on the image whose ID is in the variable `my_image_id`. When they are created, they are then added to the load balancer whose ID is `1234`, and receive requests on port 80.

Note that the `server_name` parameter represents a base string to which Auto Scale prepends a 10-character prefix to create a unique name for each server. The prefix always begins with 'as' and is followed by 8 random hex digits and a dash (-). For example, if you set the server_name to 'testgroup', and the scaling group creates 3 servers, their names would look like these:

    as5defddd4-testgroup
    as92e512fe-testgroup
    asedcf7587-testgroup

#### Parameters
Parameter | Required | Default | Notes
---- | ---- | ---- | ----
**name** | yes |  |
**cooldown** | yes |  | Period in seconds after a scaling event in which further events are ignored.
**min_entities** | yes |  |
**max_entities** | yes |  |
**launch_config_type** | yes |  | Only option currently is `launch_server`.
**flavor** | yes |  | ID of the flavor to use for each server that is launched.
**server_name** | yes |  | The base name for servers created by Autoscale.
**image** | yes |  | Either a Cloud Servers Image object, or its ID. This is the image that all new servers are created from.
**disk_config** | no | MANUAL | Determines if the server's disk is partitioned to the full size of the flavor ('AUTO') or just to the size of the image ('MANUAL').
**metadata** | no |  | Arbitrary key-value pairs you want to associate with your servers.
**personality** | no |  | Small text files that are created on the new servers. _Personality_ is discussed in the [Rackspace Cloud Servers documentation](http://docs.rackspace.com/servers/api/v2/cs-devguide/content/Server_Personality-d1e2543.html).
**networks** | no |  | The networks to which you want to attach new servers. See the [Create Servers documentation](http://docs.rackspace.com/servers/api/v2/cs-devguide/content/CreateServers.html) for the required format.
**load_balancers** | no |  | Either a  list of (id, port) tuples or a single such tuple, representing the loadbalancer(s) to add the new servers to.
**scaling_policies** | no |  | You can define the scaling policies when you create the group, or add them later.

### Updating a Scaling Group
You can modify the settings for a scaling group by calling its `update()` method. The available settings you may change are: `name`, `cooldown`, `min_entities`, `max_entities`, and `metadata`. To update a scaling group, pass one or more of these as keyword arguments. For example, to change the cooldown period to 2 minutes and increase the maximum entities to 16, you call:

    sg.update(cooldown=120,  max_entities=16)

where `sg` is a reference to the scaling group. Similarly, you can make the call on the Auto Scale client itself, passing in the reference to the scaling group you wish to update:

    au.update(sg, cooldown=120,  max_entities=16)

**Note**: If you pass any metadata values in this call, it must be the full set of metadata for the scaling group, since the underlying API call **overwrites** any existing metadata. If you simply wish to update an existing metadata key, or add a new key/value pair, you must call the `update_metadata(new_meta)` method instead. This call preserves your existing key/value pairs, and only updates it with your changes.

### Deleting a Scaling Group
To remove a scaling group, call its `delete()` method:

    sg.delete()

You can also call the `delete()` method of the client itself, passing in the scaling group to delete:

    au.delete(sg)

Note: you cannot delete a scaling group that has active servers in it. You must first delete the servers by setting the `min_entities` and `max_entities` to zero:

    sg.update(min_entities=0, max_entities=0)

Once the servers are deleted you can then delete the scaling group.


## Launch Configurations <a id="launch_configuration"></a>
Each scaling group has an associated **launch configuration**. This determines the properties of servers that are created in response to a scaling event.

The `server_name` represents a base string to which Auto Scale prepends a 10-character prefix. The prefix always begins with 'as' and is followed by 8 random hex digits and a dash (-). For example, if you set the `server_name` to 'testgroup', and the scaling group creates 3 servers, their names would look like these:

    as5defddd4-testgroup
    as92e512fe-testgroup
    asedcf7587-testgroup    

To see the launch configuration for a given scaling group, call:

    sg.get_launch_config()
    # or
    au.get_launch_config(sg)

### Updating the Launch Configuration
You can also modify the launch configuration for your scaling group by calling the `update_launch_config()` method. This method lets you update any of the following setttings: `server_name`, `flavor`, `image`, `disk_config`, `metadata`, `personality`, `networks`, `load_balancers`. You may update one or more of these parameters in the call. For example, to change the scaling group to use a different image, call:

    sg.update_launch_config(image=new_image_id)

You may also make the call on the Auto Scale client itself, passing in the scaling group you want to modify:

    au.update_launch_config(sg, image=new_image_id)

**Note**: If you pass any metadata values in this call, it must be the full set of metadata for the launch configuration, since the underlying API call **overwrites** any existing metadata. If you simply wish to update an existing metadata key in your launch configuration, or add a new key/value pair, you must call the `update_launch_metadata()` method instead. This call preserves your existing key/value pairs, and only updates with your changes.


## Policies
When an alarm is triggered in Cloud Monitoring, it calls the webhook associated with a particular policy. The policy is designed to update the scaling group to increase or decrease the number of servers in response to the particular alarm.

To list the policies for a given scaling group, call its `list_policies()` method:

    policies = sg.list_policies()

You can also call this directly on the client, passing in the scaling group for which you want to get a list of its policies:

    policies = au.list_policies(sg)

### Creating a Policy
To add a policy to a scaling group, call the `add_policy()` method:

    policy = sg.add_policy(name, policy_type, cooldown,
            change, is_percent)
    # or
    policy = au.add_policy(sg, name, policy_type, cooldown,
            change, is_percent)

#### Parameters
Parameter | Required | Default | Notes
---- | ---- | ---- | ----
**name** | yes |  |
**policy_type** | yes | | Only available type now is 'webhook'.
**cooldown** | yes |  | Period in seconds after a policy execution in which further events are ignored. This is separate from the overall cooldown for the scaling group.
**change** | yes | | Can be positive or negative, which makes this a scale-up or scale-down policy, respectively.
**is_percent** | no | False | Determines whether the value passed in the `change` parameter is interpreted an absolute number, or a percentage.

### Updating a Policy
You may update a policy at any time, passing in any or all of the above parameters to change that value. For example, to change the cooldown to 60 seconds, and the number of servers to remove to 3, call:

    policy.update(cooldown=60, change=-3)

You may also call the `update_policy()` method of either the scaling group for this policy, or the Auto Scale client itself. Either of the following two calls is equivalent to the call above:

    sg.update_policy(policy, cooldown=60, change=-3)
    # or
    au.update_policy(sg, policy, cooldown=60, change=-3)

### Executing a Policy
You don't need to wait for an alarm to be triggered in Cloud Monitoring in order to execute a particular policy. If desired, you may do so manually by calling the policy's `execute()` method:

    policy.execute()

You can also call the execute_policy() method of either the policy's scaling group, or on the client itself:

    sg.execute_policy(policy)
    # or
    au.execute_policy(sg, policy)

### Deleting a Policy
To remove a policy, call its `delete()` method:

    policy.delete()

You can also call the delete_policy() method of either the policy's scaling group or on the client itself:

    sg.delete_policy(policy)
    # or
    au.delete_policy(sg, policy)


## Webhooks
When an alarm is triggered in Cloud Monitoring, it calls the associated webhook, which in turn causes the policy for that webhook to be executed.

To list the webhooks for a given policy, call its `list_webhooks()` method:

    webhooks = policy.list_webhooks()

You can also call this directly on either the scaling group or the client, passing in the policy for which you want the list of webhooks:

    webhooks = sg.list_webhooks(policy)
    # or
    webhooks = au.list_webhooks(sg, policy)

### Creating a webhook
To add a webhook to a policy, call the `add_webhook()` method:

    webhook = policy.add_webhook(name, metadata)
    # or
    webhook = sg.add_webhook(policy, name, metadata)
    # or
    webhook = au.add_webhook(sg, policy, name, metadata)

The `name` parameter is required; the `metadata` parameter is optional.

### Updating a webhook
You may update a webhook at any time to change either its name or its metadata:

    webhook.update(name="something_new",
            metadata={"owner": "webteam"})

You may also call the `update_webhook()` method of either the policy for this webhook, or the scaling group for that policy, or the Auto Scale client itself. Any of the following calls is equivalent to the call above:

    policy.update_webhook(webhook, name="something_new",
            metadata={"owner": "webteam"})
    # or
    sg.update_webhook(policy, webhook, name="something_new",
            metadata={"owner": "webteam"})
    # or
    au.update_webhook(sg, policy, webhook, name="something_new",
            metadata={"owner": "webteam"})

**Note**: If you pass any metadata values in this call, it must be the full set of metadata for the webhook, since the underlying API call **overwrites** any existing metadata. If you simply wish to update an existing metadata key, or add a new key/value pair, you must call the `webhook.update_metadata(new_meta)` method instead (or the corresponding `au.update_webhook_metadata(sg, policy, webhook, new_meta)`). This call preserves your existing key/value pairs, and only updates it with your changes.

### Deleting a webhook
When you wish to remove a webhook, call its `delete()` method:

    webhook.delete()

You can also call the `delete_webhook()` method of the webhook's policy, or the policy's scaling group, or on the client itself:

    policy.delete_webhook(webhook)
    # or
    sg.delete_webhook(policy, webhook)
    # or
    au.delete_webhook(sg, policy, webhook)


# Cloud Block Storage

## Basic Concepts
Rackspace Cloud Block Storage (**CBS**) is a block level storage solution that allows customers to mount drives or volumes to their Rackspace Next Generation Cloud Servers™. The two primary use cases are (1) to allow customers to scale their storage independently from their compute resources, and (2) to allow customers to utilize high performance storage to serve database or I/O-intensive applications.

CBS is built upon the **OpenStack Cinder** project. See [http://cinder.openstack.org](http://cinder.openstack.org) for complete details about the available features and functionality.


## CBS in pyrax
Once you have authenticated and connected to the block storage service, you can reference the block storage module via `pyrax.cloud_blockstorage`. This provides general information about the volumes for the account as well as the ability to define new block storage volumes. You can also attach and detach volumes from your cloud servers.

All of the code samples in this document assume that you have already imported pyrax, authenticated, and created the name `cbs` at the top of the script, like this:

    import pyrax
    pyrax.set_credential_file("my_cred_file")
    # or
    # pyrax.set_credentials("my_username", "my_api_key")
    cbs = pyrax.cloud_blockstorage


## Block Storage Types
There are two types of block storage: **SATA** and **SSD**. SATA volumes offer lower cost and standard performance, while SSD offers high performance for databases and other I/O-intensive applications, at a higher cost.

To get a list of the available types, run:

    print cbs.list_types()

This results in:

    [<CloudBlockStorageVolumeType extra_specs={}, id=1, name=SATA>,
    <CloudBlockStorageVolumeType extra_specs={}, id=2, name=SSD>]

These volume types are read-only.


## Listing Existing Block Storage Volumes
To get a list of all the block storage volumes in your cloud, run:

    cbs.list()

This returns a list of `CloudBlockStorageVolume` objects. You can then interact with the individual `CloudBlockStorageVolume` objects. Assuming that you are just starting out and do not have any volumes created yet, you get back an empty list.


## Creating a Block Storage Volume
To create a block storage volume, you call the `create()` method, passing in the parameters to match what you need.

Parameter | Description | Required
---- | ---- | ----
**name** | The name to be displayed in volume listings. Default = `""` | No
**size** | The size (in GB) of the volume. The size must be a positive integer between 100 and 1024. | Yes
**volume_type** | The type of volume to create, either SATA or SSD. Default = `SATA` | No
**description** | A description of the volume. Used only for display purposes. Default = `""` | No
**metadata** | A dictionary of key/value metadata to be associated with this volume. Default = `{}` | No
**snapshot_id** | The ID of the snapshot from which to create a volume. Default = `None`| No

When you create a volume from a snapshot, the new volume is a copy of the volume from which the snapshot was created. The new volume must be the same size as the original volume used to create the snapshot. If you create a new volume from scratch, it is the equivalent of an unformatted disk drive.

There are additional parameters in the response that are not described in the request parameter table above. **`status`** is typically available although it can be creating if the volume is from a snapshot operation. **`availability_zone`** is always "nova". **`bootable`** is currently always False. **`source_volid`** is currently not used by CBS.

Here is an example of the call to create a new 500 GB volume that uses SSD for high performance:

    vol = cbs.create(name="my_fast_volume", size=500, volume_type="SSD")
    print "New Volume:", vol

This prints:

    <CloudBlockStorageVolume attachments=[], availability_zone=nova, created_at=2012-11-07T20:28:13.000000, display_description=, display_name=my_fast_volume, id=c1b05ede-54bf-46e0-9bd3-bf1946c5b485, metadata={}, size=500, snapshot_id=None, status=available, volume_type=SSD>


## Attaching a Volume to a Server
To mount your Cloud Block Storage to one of your Cloud Servers, you call the volume's `attach_to_instance()` method, passing in a server reference (either a `CloudServer` instance, or the ID of that server), along with the mount point for the volume on that server. Here is an example:

    server = pyrax.cloudservers.servers.get("MyServerID")
    mountpoint = "/dev/xvdb"
    vol.attach_to_instance(server, mountpoint=mountpoint)
    # or
    cbs.attach_to_instance(vol, server, mountpoint=mountpoint)

Once the call to attach has been made successfully, the process of attaching begins. It may take several minutes until the volume has been attached and is available to the server. If this is a new volume, you have to SSH into the server and format it as you would any new disk. Once the volume is formatted, it can be mounted and used by the server.


## Detaching a Volume from a Server
The call to detach the volume is even simpler:

    vol.detach()
    # or
    cbs.detach(vol)

You do not need to specify the server, since a volume can only be attached to a single server at a time. Nothing happens if the volume is not attached when that call is made.


## Deleting a Volume
To delete a volume you no longer need, call:

    vol.delete()
    # or
    cbs.delete_volume(vol)

A volume that is attached to an instance cannot be deleted. It must be detached first, and have a status of 'available' or 'error' in order to be deleted. You also cannot delete a volume from which a snapshot has been created, as the snapshot is linked to that volume. All snapshots of a volume must be deleted before the volume can be deleted.

If you are *absolutely certain* that you no longer want to keep a volume, you can use the optional `force` parameter to `delete()`. If you call:

    vol.delete(force=True)

the volume is detached from its server (if it is attached), and all snapshots of that volume are deleted. The volume is then deleted, too.


## Working with Snapshots
A `Snapshot` captures the contents of a volume at a point in time. It can be used, for example, as a backup point; and you can later create a volume from the snapshot.

The main use for snapshots is to create new volumes. That is done as noted above in the `cbs.create()` method.


### Creating a Snapshot
You create snapshots by calling the `create_snaphot()` method of a volume object. You have the option of specifying a display name and/or description. Normally the volume should not be attached to a server when the snapshot is created, as that may result in the contents being modified while the snapshot is being generated. You can override that by including `force=True`, which lets you create a snapshot of an attached volume. Always be sure to test any such forced snapshots to ensure that their contents are what you would expect, and that they were not corrupted during the forced snapshot process.

In this example `vol` is a `CloudBlockStorageVolume` object from which the snapshot is created:

    snap = vol.create_snapshot(name="My Snapshot")
    # or
    snap = cbs.create_snapshot(vol, name="My Snapshot")

If you have only the ID of the volume from which you want to create the snapshot, you can call the service's `create_snapshot()` method instead, passing in that ID:

    snap = cbs.create_snapshot("c1b05ede-54bf-46e0-9bd3-bf1946c5b485", name="My Snapshot")

All of these calls do the same thing.


## Listing Snapshots
To get a list of all your snapshots, call the `list_snapshots()` method:

    print cbs.list_snapshots()

This returns a list of `CloudBlockStorageSnapshot` objects:

    [<CloudBlockStorageSnapshot created_at=2012-11-09T17:17:19.000000, display_description=, display_name=Daily Snapshot, id=32af1cce-6b03-4a28-b09d-905844edeecf, size=111, status=creating, volume_id=c1b05ede-54bf-46e0-9bd3-bf1946c5b485>]

To get a listing of all snapshots for a specific volume `vol`, run:

    print vol.list_snapshots()

This also returns a list of `CloudBlockStorageSnapshot` objects, but only those for that volume.



## Deleting Snapshots
There are two ways to delete a snapshot you no longer need. For the example below, assume that `snap` is an instance of `CloudBlockStorageSnapshot`:

    snap.delete()
    # or
    cbs.delete_snapshot(snap)

The snapshot must have a status of 'available' or 'error' in order to be deleted. Also, if there are many snapshots of the same volume, only one of them at a time can be deleted.

If you need to delete all the snapshots for a given volume `vol`, such as before deleting a volume you no longer need, there is a convenience method for that:

    vol.delete_all_snapshots()

# Cloud Databases

## Basic Concepts
This is a standalone, API-based, relational database service built on OpenStack® cloud that allows Rackspace customers to easily provision and manage multiple MySQL database instances. Each database instance is accessible by any of your cloud servers in the same region. You can also expose the instance to the public internet by adding the instance to a Cloud Load Balancer.


## Terminology
One potential source of confusion is the use of 'database' in multiple contexts. To clarify, the overall service is named 'Cloud Databases'. Each deployment is referred to as an 'Instance'. Each Instance can contain many 'databases', which is a grouping of related tables in indexes for storing relational information.

To keep these concepts clear, the term 'database' by itself always refers to the relational entity in an instance. When referring to the overall service, the term 'Cloud Databases' is used.


## Using Cloud Databases in pyrax
Once you have authenticated and connected to the database service, you can reference the database module via `pyrax.cloud_databases`. That is a lot to type over and over in your code, so it is easier if you include the following line at the beginning of your code:

    cdb = pyrax.cloud_databases

Then you can simply use `cdb` to reference the module. All of the code samples in this document assume that `cdb` has been defined this way.

## Listing Database Instances
To get a list of all your instances, just run:

    cdb.list()

This returns a list of `CloudDatabaseInstance` objects. Assuming that you are just starting out and have not yet created any instances, you get back an empty list. A good first step, then, would be to create an instance.


## Create an Instance
To create an instance, you need to specify the flavor and volume size for that instance. 'Flavor' refers to the amount of RAM allocated to your instance. Volume size is the disk space available to your instance for storing its data. The volume size is in GB, and must be a whole number between 1 and 50.


### List Available Flavors
To get a list of all the available flavors, run the following:

    cdb.list_flavors()

You should get back something like this:

    [<CloudDatabaseFlavor id=1, name=512MB Instance, ram=512>,
     <CloudDatabaseFlavor id=2, name=1GB Instance, ram=1024>,
     <CloudDatabaseFlavor id=3, name=2GB Instance, ram=2048>,
     <CloudDatabaseFlavor id=4, name=4GB Instance, ram=4096>]

The RAM available is listed in MB, so the flavor with ram=4096 would create an instance with 4GB of RAM.

Assuming that you want to create an instance using the smallest flavor and 2GB of disk space, run the following code:

    inst = cdb.create("first_instance", flavor=1, volume=2)
    print inst

Assuming that all went well, you should see your new instance:

    <CloudDatabaseInstance hostname=a1d5f7312d85f95071ea658d699962b690bd6b60.rackspaceclouddb.com, id=471eff58-66bb-40af-8030-405451e38c02, links=[{u'href': u'https://localhost:8778/v1.0/728829/instances/471eff58-66bb-40af-8030-405451e38c02', u'rel': u'self'}, {u'href': u'https://localhost:8778/instances/471eff58-66bb-40af-8030-405451e38c02', u'rel': u'bookmark'}], name=first_instance, status=BUILD, volume=<pyrax.clouddatabases.CloudDatabaseVolume object at 0x104f73590>>

If you are planning on using your Cloud Database instance from one of your Cloud Servers, you need the `hostname` attribute of that instance. In this case, it is `a1d5f7312d85f95071ea658d699962b690bd6b60.rackspaceclouddb.com`. Since this host is not publicly accessible, only Cloud Servers and Cloud Load Balancers within the same region can access this instance.


## Resizing an Instance
Resizing an instance refers to changing the amount of RAM allocated to your instance. To do this, call the instance's `resize()` method, passing in the flavor of the desired size. This can be a `CloudDatabaseFlavor` object, the flavor name, flavor ID or RAM size of the new flavor. For example, the following 3 commands all change the instance flavor to the 2GB size:

    # By name
    inst.resize("2GB Instance")
    # By RAM (in MB)
    inst.resize(2048)
    # By ID
    inst.resize(3)


## Resizing a Volume
Resizing a volume refers to increasing the amount of disk space for your instance. To do this, call the instance's `resize_volume()` method, passing in the new volume size. Note that you cannot reduce the volume size. Trying to reduce the size of the volume raises an `InvalidVolumeResize` exception.

    inst.resize_volume(8)


## Create a Database
Once you have an instance, you need to create a database. You must specify a name for the new database, as well as the optional parameters for `character_set` and `collate`. If these are not specified, the defaults of `utf8` and `utf8_general_ci` are used, respectively.

There are two variations: calling the `create_database()` method of a `CloudDatabaseInstance` object, or calling the `create_database()` method of the cloud_databases module itself. With the second version, you must specify the instance in which to create the database. Either a `CloudDatabaseInstance` object or its `id` works. Assuming that `inst` is a reference to the instance you created above, here are both versions:

    db = inst.create_database("db_name")
    print "DB:", db

or:

    db = cdb.create_instance(inst, "db_name")
    print "DB:", db

Both calls return an object representing the newly-created database:

    DB: <CloudDatabaseDatabase name=db_name>


## Create a User
You can create a user on an instance with its own username/password credentials, with access to one or more databases on that instance, and optionally restricted to connecting from particular host addresses. Similar to database creation, you can call `create_user()` either on the instance object, or on the module. To simplify these examples, only the call on the instance is displayed.

Assuming that you have the references `inst` and `db` from the previous examples, you can create a user like this:

    user = inst.create_user(name="groucho", password="top secret", database_names=[db], host="%")
    print "User:", user

This prints out:

    User: <CloudDatabaseUser databases=[{u'name': u'db_name'}], name=groucho, host="%">


## List Databases or Users in an Instance
Instances have a `list_databases()` and a `list_users()` method:

    dbs = inst.list_databases()
    users = inst.list_users()
    print "DBs:", dbs
    print "Users:", users

which outputs:

    DBs: [<CloudDatabaseDatabase name=db_name>]
    Users: [<CloudDatabaseUser databases=[{u'name': u'db_name'}], name=groucho>, host="%"]


## Get a `CloudDatabaseDatabase` or `CloudDatabaseUser` Object
You can get a `CloudDatabaseDatabase` or `CloudDatabaseUser` object from an `CloudDatabaseInstance` object by supplying the name:

    db = inst.get_database("db_name")
    user = inst.get_user("groucho")
    print "DB:", db
    print "User:", user

which outputs:

    DB: <CloudDatabaseDatabase name=db_name>
    User: <CloudDatabaseUser databases=[{u'name': u'db_name'}], name=groucho, host="%">


## Backups
You can create a backup of your instance that can be used to re-create that instance at a later date. The backup is created asynchronously. During the backup process, database writes on MyISAM Databases will be disabled. InnoDB Databases will continue to allow all operations.

While the instance is being backed up you will not be able to add/delete databases, add/delete users, or delete/stop/reboot the instance. Users can only run one backup at a time. Duplicate requests will receive a 422 error.

Backups are not deleted when the instance is deleted. You must remove any backups created if you no longer wish to retain them. See the section below on [Deleting a Backup](#deleting-a-backup).

During backup the files will be streamed to your Cloud Files account. The process creates a container called z_CLOUDDB_BACKUPS and places all the files in it. In order for the restore and deletion of backups to work properly, you should not move, rename, or delete any of the files from this container. You will be charged the normal Cloud Files rate for storage of these files.

In the unlikely event that the backup fails to perform correctly and is in the state FAILED, there may be some files that were placed in the container. Deleting the failed backup will remove any such files. See the section below on [Deleting a Backup](#deleting-a-backup).

### Create a Backup
To create a backup, you must specify the instance and a name for the backup. You may optionally supply a text description. The name is limited to 64 characters. If you have a `CloudDatabaseInstance` object, you can make the call directly to its `create_backup()` method. The calls are:

    backup = cdb.create_backup(instance, name[, description=None])
    # - or -
    backup = instance.create_backup(name[, description=None])

Both of these calls return a `CloudDatabaseBackup` object that represents the backup. You can query its status attribute to determine its progress:

Status | Description
---- | ----
NEW | A new backup task was created.
RUNNING | The backup task is currently running.
COMPLETED | The backup task was successfully completed.
FAILED | The backup task failed to complete successfully.

Like other objects that take time to finish being created, you can also use the `utils.wait_for_build(backup)` method call to either block until the build completes, or pass a callback to be triggered upon completion.


## List Backups
There are two ways to list backups: you can either list all the backups, or list just those backups for a particular instance. To get a list of all your backups, call:

    cdb.list_backups()

To only list the backups for a particular instance, call:

    cdb.list_backups(instance)

If you have a `CloudDatabaseInstance` object, you can also call its `list_backups()` method to get a listing of all backups for that instance:

    instance.list_backups()


## Deleting a Backup
If you no longer wish to keep a backup, you can delete it. This will remove it from your list of available backups, and remove the files from your Cloud Files account. You will need either the backup's ID or the `CloudDatabaseBackup` object for the backup you wish to delete. The call is:

    cdb.delete_backup(backup)
    # - or -
    backup.delete()


## Restoring From a Backup
If you need to create a new instance from one of your backups, you can call the `restore_backup()` method. The call is:

    cdb.restore_backup(backup, name, flavor, volume)

The parameters are the same as the call to `create()`, with the exception of the `backup` parameter, which must be either a `CloudDatabaseBackup` object, or the ID of one of your backups. See the [Create an Instance](#create-an-instance) section above for the specifics of the other parameters.

All users/passwords/access that were on the instance at the time of the backup will be restored along with the databases. You can create new users or databases if you want, but they cannot be the same as the ones from the instance that was backed up.


## Working with `CloudDatabaseDatabase` and `CloudDatabaseUser` Objects
These objects are essentially read-only representations of the underlying MySQL database running in your instance. You cannot update the attributes of these objects and expect them to change anything in the instance. They are useful mostly to determine the state of your database. The one method they have is `delete()`, which causes them to be deleted from their instance.

Note that there is a bug in the underlying Python library for the API that affects user names that contain a period. With such users, the API truncates the name at the first period, and attempt to delete the shortened name. Example: if you have two users with the names `"john.doe"` and `"john"`, and you call:

    inst.delete("john.doe")

the API actually deletes the user `"john"`, and `"john.doe"` is untouched! The best way to avoid this problem is to ensure that you do not use user names that contain periods. If you must include periods, do not use pyrax or any other cloud API-based tool to delete them. Instead, use any one of the many MySQL admin tools available.














# Cloud DNS

## Basic Concepts
The Cloud DNS API allows you to programmatically associate domain names with your devices, and add/edit/remove DNS records for those domains.


## About Domain Names
Rackspace does not handle domain name registration. You need to go to any one of the many existing registrars if you want to register a domain name. Once your domain name is registered, you can then use it with your Rackspace Cloud account.

For the purposes of this document, the domain **example.edu** is used. This is not a real domain; it is a domain name reserved for use in documentation. See this [Wikipedia](http://en.wikipedia.org/wiki/Example.com) article for more information about the use of this name.

**Subdomains** are domains within the parent domain, and are typically used to designate different functions within the domain. Examples of subdomains are `mail.example.edu` (for handling email activity), `www.example.edu` (web traffic), and `ftp.example.edu` (FTP traffic). Subdomains can be further broken down into sub-subdomains to suit the needs of the site, for example `main.ftp.example.edu` and `secondary.ftp.example.edu`.


## Cloud DNS in pyrax
Once you have authenticated and connected to the Cloud DNS service, you can reference the DNS module via `pyrax.cloud_dns`. This module provides methods for managing your DNS entries for the cloud.

All of the code samples in this document assume that you have already imported `pyrax`, authenticated, and created the name `dns` at the top of the script, like this:

    import pyrax
    pyrax.set_credential_file("my_cred_file")
    # or
    # pyrax.set_credentials("my_username", "my_api_key")
    dns = pyrax.cloud_dns


## Response Timeouts
Many API calls in Cloud DNS do not wait to complete before returning; instead, they return immediately with a URI that can be used to get updates on the request. This is done because the API server has to communicate with the DNS servers, and that can take a while sometimes.

This is generally of no concern to a developer, as the calls almost always complete within less than a second, so `pyrax` handles the checking of the status update of a call for you, and when complete, returns the final results. By default, `pyrax` waits for up to 5 seconds for the call to complete; if it has not completed by then, a `DNSCallTimedOut` exception is raised.

If you need to change that timeout period from the default, make the following call:

    dns.set_timeout(new_time)

The value of `new_time` is in seconds. You can tell `pyrax` to wait indefinitely for the call to complete by passing zero to `set_timeout()`:

    dns.set_timeout(0)

By default, `pyrax` pauses for 0.5 seconds in between each check of the async call to see if it has completed. This helps to avoid rate limits by not checking continuously. You can adjust this if you want a different delay interval by calling:

    dns.set_delay(2.2)

The call above causes `pyrax` to wait 2.2 seconds in between calls to check if an async API call has completed.


## Listing Domains
To get a list of all the domains that are manageable by your account, call the `list()` method:

    dns.list()

This returns a list of `CloudDNSDomain` objects, with which you can then interact. It is a flat list: there is no hierarchical nesting of subdomains within their parent domains. Assuming that you have just started, you get back an empty list. The next step is to add your domains.


### Paging
You could have hundreds of domains, and by default the `list()` method returns only the first hundred domains. You can optionally pass in `limit` and `offset` parameters to control the results returned by `dns.list()`.

The `limit` parameter determines how many records are returned, and must be a value between 1 and 100. If no `limit` is passed, a value of 100 is used. The `offset` parameter determines where in the listing to begin when fetching. The value of `offset` defaults to 0 if not specified. Additionally, the value of `offset` must be either zero or a multiple of the limit. Together they enable paging across all of your domains.

To make things easier, `pyrax` offers two convenience methods: `list_previous_page()` and `list_next_page()` for traversing your domain records. After the initial call to `list()`, you can then navigate through the pages of results with these methods. Each raises a `NoMoreResults` exception when there are no additional pages of results to fetch. For example, assume that there are 15 domain records, and you want to show them in pages of up to 4 at a time. This code does that for you:

    domains = dns.list(limit=4)
    print domains
    while True:
        try:
            domains = dns.list_next_page()
            print domains
        except pyrax.exceptions.NoMoreResults:
            break

The same approach of using `offset` and `limit` works with subdomains, records, and PTR records. The methods for the next and previous page of results have similar names, which are noted in their respective sections below.


### Iterators
To make working with large numbers of domains simpler, `pyrax` provides the `get_domain_iterator()` method. This returns an iterable object that handles the paging requests for you, so you can treat them as a single request. For this example, assume that you have 250 domains named from 'example001.edu' to 'example250.edu'. Instead of the multiple commands you would need to use as in the example above, you can iterate through them in a single command:

    for domain in dns.get_domain_iterator():
        print domain.name

This prints out:

    example001.edu
    example002.edu
    ...
    example249.edu
    example250.edu

There is a slight delay when the end of a page of domains is reached and the next page is fetched, but it is no different than when you manually page through your domains.


## Adding Domains
To create a domain, you call the `dns.create()` method, supplying some or all of the following parameters:

Parameter | Description | Required?
---- | ---- | ----
**name** | The fully-qualified domain name (FQDN). | yes
**emailAddress** | The email address of the domain administrator. | yes
**ttl** | The Time To Live (in seconds) for the domain. Default=3600. Minimum=300. | no
**comment** | A brief description of the domain. Maximum length=160 characters. | no
**subdomains** | One or more dicts that represent subdomains of this new domain. The dicts have the same structure as the main domain, with each of these parameters being keys in the subdomain dicts. Including subdomains in the `create()` command is equivalent to creating them separately, but requires only one API call instead of many. | no
**records** | You can optionally add DNS records for the domain, such as `MX`, `A`, and `CNAME` records. The records are dicts with the same structure as for the `add_records()` method, and adding them in the `create()` command is equivalent to adding them separately afterward, but requires only one API call. | no


So the simplest form of the call would be:

    dom = dns.create(name="example.edu", emailAddress="sample@example.edu")

You could also add a TTL setting and a comment when creating a domain:

    dom = dns.create(name="example.edu", emailAddress="sample@example.edu",
            ttl=600, comment="Primary domain for this documentation.")

The `create()` command returns an instance of a `CloudDNSDomain` object:

    <CloudDNSDomain accountId=000000, comment=Primary domain for this documentation., created=2012-12-06T20:45:10.000+0000, emailAddress=sample@example.edu, id=3534921, name=example.edu, nameservers=[{u'name': u'dns1.stabletransit.com'}, {u'name': u'dns2.stabletransit.com'}], ttl=600, updated=2012-12-06T20:45:10.000+0000>


## Subdomains
Subdomains are conceptually the same as primary domains, but are a useful way of addressing multiple related devices without requiring each to have its own domain name. You create a subdomain just like creating a primary domain: by calling `dns.create()`, but with the `name` parameter replaced with the **FQDN** (Fully-Qualified Domain Name) of the subdomain. The same holds true for `update()` and `delete()`.

Subdomains in DNS are managed in separate zone files, so this means that there isn't an explicit linkage between a subdomain and the primary domain. Instead, the relationship is only implied, and is the result of the naming: `a.example.edu` is by definition a subdomain of `example.edu`; likewise, `b.a.example.edu` is a subdomain of `a.example.edu`.

## Listing Subdomains
To get a listing of all the subdomains for a given domain 'dom', call the `list_subdomains()` method:

    subs = dom.list_subdomains()
    # or
    subs = dns.list_subdomains(dom)

Each of the above calls returns the same information.


## DNS Records
Records specify information about the domain to which they belong. Rackspace Cloud DNS supports the following record types:

* A
* CNAME
* MX
* AAAA
* NS
* TXT
* SRV
* PTR


## Listing DNS Records
To get a listing of all the records for a given domain 'dom', call the `list_records()` method:

    recs = dom.list_records()
    # or
    recs = dns.list_records(dom)

    print recs

Each of the above calls returns the same information: a series of `CloudDNSRecord` objects:

    [<CloudDNSRecord created=2012-12-10T21:25:45.000+0000, data=example.edu, domain_id=3539045, id=CNAME-11284972, name=sample001.example.edu, ttl=3600, type=CNAME, updated=2012-12-10T21:25:45.000+0000>,
     <CloudDNSRecord created=2012-12-10T21:25:47.000+0000, data=example.edu, domain_id=3539045, id=CNAME-11284973, name=sample002.example.edu, ttl=3600, type=CNAME, updated=2012-12-10T21:25:47.000+0000>,
     <CloudDNSRecord created=2012-12-10T21:25:49.000+0000, data=example.edu, domain_id=3539045, id=CNAME-11284974, name=sample003.example.edu, ttl=3600, type=CNAME, updated=2012-12-10T21:25:49.000+0000>]


### Paging Subdomains and Records
Each of the subdomain and record listing calls are subject to the same paging parameters as those for domain listing: 100 maximum per request. You can optionally specify the `limit` and `offset` parameters to get different blocks of subdomains, just as with domains. There are also the convenience methods for moving back and forth through the pages of results:

    * list_subdomains_previous_page()
    * list_subdomains_next_page()
    * list_records_previous_page()
    * list_records_next_page()

It should be noted that these methods work with a single domain at a time, so if you were to list subdomains or records of a different domain, the new paging information would override the old.

To avoid that limitation, you can use the iterators:

    sub_iter = dns.get_subdomain_iterator(dom)
    # and
    rec_iter = dns.get_record_iterator(dom)

These return objects you can iterate on to get all the subdomains or records for the specified domain 'dom'. Since each iterator is domain-specific, you don't have to be concerned about requests for different domains resetting the paging information.

Please note that like all iterators, these are single-pass objects. Once a value has been returned, you cannot go "backwards" to get it again. Instead, you must re-create the iterator and start from the beginning again. If you need to have a list of all your domains/subdomains/records so that you can randomly access any of its members, cast the iterator to a list:

    all_records = list(dns.get_record_iterator(dom))



## Adding DNS Records
DNS records are associated with a particular domain, so to add records you call the `add_records()` method of the CloudDNSDomain object for that domain. Alternatively, you can call the module's `add_records()` method, passing in the domain reference as well as the record information.

The record information should be a dict whose keys are the relevant record attributes; which keys are needed depend on the record type. To create multiple records in a single call, pass in a list of these record dicts.

Name | Description | Required
--- | --- | ---
**type** | Specifies the record type to add. | Yes
**name** | Specifies the name for the domain or subdomain. Must be a valid domain name. | Yes
**data** | The data field for PTR, A, and AAAA records must be a valid IPv4 or IPv6 IP address. For MX records it must be the FQDN for the mail server. | Yes
**priority** | Required for MX and SRV records, but forbidden for other record types. If specified, must be an integer from 0 to 65535. | For MX and SRV records only
**ttl** | If specified, must be greater than 300. Defaults to the domain TTL if available, or 3600 if no TTL is specified. | No
**comment** | If included, its length must be less than or equal to 160 characters. | No


Here is an example of adding an **A** and an **MX** record to a `CloudDNSDomain` object 'dom':

    recs = [{
            "type": "A",
            "name": "example.edu",
            "data": "192.168.0.42",
            "ttl": 6000,
            }, {
            "type": "MX",
            "name": "example.edu",
            "data": "mail.example.edu",
            "priority": 50,
            "comment": "Backup mail server"
            }]
    print dom.add_records(recs)
    # or
    print dns.add_records(dom, recs)

If the record addition succeeds, it returns a list of `CloudDNSRecord` objects representing the newly-added records:

    [<CloudDNSRecord created=2012-12-17T21:30:56.000+0000, data=192.168.0.42, domain_id=3539045, id=A-9393844, name=example.edu, ttl=6000, type=A, updated=2012-12-17T21:30:56.000+0000>,
     <CloudDNSRecord comment=Backup mail server, created=2012-12-17T21:30:57.000+0000, data=mail.example.edu, domain_id=3539045, id=MX-4184738, name=example.edu, priority=50, ttl=3600, type=MX, updated=2012-12-17T21:30:57.000+0000>]


## Adding Subdomains
Since a subdomain is really not any different than a primary domain, the command to add  a subdomain is exactly the same:

    subdom1 = dns.create(name="north.example.edu", comment="1st sample subdomain",
            emailAddress="sample@rackspace.edu")


Note that there is no reference to the primary domain. Instead, the relation is simply implied via the FQDN.


## Combining Multiple Actions
The `create()` method allows you to specify subdomains and records to be added to the domain you are creating. The end result is identical, but by combining the actions into one, only one API call is made, making it much more efficient.

Consider a situation where you need to create the `example.edu` domain, along with an `A` and an `MX` record, as well as four subdomains: `north.example.edu`, `west.example.edu`, `southeast.example.edu`, and `southby.southeast.example.edu`. The approach using individual requests is to call `create()` and `add_record()` for each one separately:

    dom = dns.create(name="example.edu", comment="Primary domain",
            emailAddress="sample@rackspace.edu")
    rec1 = dom.add_records({"type": "A", "name": "example.edu",
            "data": "192.168.0.42", "ttl": 6000})
    rec2 = dom.add_records({"type": "MX", "name": "example.edu",
            "data": "mail.example.edu", "priority": 50, "comment":
            "Backup mail server"})
    subdom1 = dns.create(name="north.example.edu", comment="1st sample subdomain",
            emailAddress="sample@rackspace.edu")
    subdom2 = dns.create(name="west.example.edu", comment="2nd sample subdomain",
            emailAddress="sample@rackspace.edu")
    subdom3 = dns.create(name="southeast.example.edu",
            emailAddress="sample@rackspace.edu")
    subdom4 = dns.create(name="southby.southeast.example.edu",
            comment="Final sample subdomain", emailAddress="sample@rackspace.edu")


That's a total of 7 separate calls to the server. Actually, some of these calls are done asynchronously, and require several callbacks to determine if the calls succeeded, which `pyrax` handles for you, so the total number of API calls is actually much higher.

By preparing the record and subdomain information ahead of time, the process can be made much more efficient, by requiring only a single `create()` call:

    subs = [
        {"name" : "north.example.edu",
            "comment" : "1st sample subdomain",
            "emailAddress" : "sample@rackspace.edu"},
        {"name" : "west.example.edu",
            "comment" : "2nd sample subdomain",
            "emailAddress" : "sample@rackspace.edu"},
        {"name" : "southeast.example.edu",
            "emailAddress" : "sample@rackspace.edu"},
        {"name" : "southby.southeast.example.edu",
            "comment" : "Final sample subdomain",
            "emailAddress" : "sample@rackspace.edu"}
        ]

    recs = [{
            "type": "A",
            "name": "example.edu",
            "data": "192.168.0.42",
            "ttl": 6000,
            }, {
            "type": "MX",
            "name": "example.edu",
            "data": "mail.example.edu",
            "priority": 50,
            "comment": "Backup mail server"
            }]
    dom = dns.create(name="example.edu", comment="Primary domain",
            emailAddress="sample@rackspace.edu", subdomains=subs,
            records=recs)


This single call has the exact same result as the 7 separate calls, but is much more efficient. A recent test running this code showed 11.4 seconds for the separate calls, but only 4.6 for the combined call.


## Updating a Domain
You can modify any of the following attributes on an existing domain:

- Contact email address
- TTL
- Comment

To do that, call the domain's `update()` method, or the `dns.update_domain()` method. Given a `CloudDNSDomain` object 'dom', each of the following commands produces the same results:

    dom.update(ttl=1200)
    # or
    dns.update_domain(dom, ttl=1200)


## Deleting a Domain
If you have a `CloudDNSDomain` object 'dom' that you want to delete, you can use either the object-level or module-level command to do so:

    dom.delete()
    # or
    dns.delete(dom)


## Import / Export a Domain
If you have a BIND 9 formatted domain configuration file (for an example, see [this page](http://www.centos.org/docs/2/rhl-rg-en-7.2/s1-bind-configuration.html#BIND-EXAMPLE-ZONE-WHOLE) from the CentOS site) that describes the domain and its information, you can import that directly instead of creating the separate commands that are needed to create and configure the domain from scratch.

    with file("/path/to/bindfile.txt") as bindfile:
        data = bindfile.read()
        dom = dns.import_domain(data)

Similarly, you can export a domain by calling its `export()` method, or the module's `export_domain()` method. Each of the following calls produces the same result:

    exp = dom.export()
    # or
    exp = dns.export_domain(dom)

Each call creates the same output:

    example.edu.        3600    IN    SOA    ns.rackspace.com. sample.rackspace.edu. 1354918038 21600 3600 1814400 500
    example.edu.        6000    IN    A    192.168.0.42
    example.edu.        3600    IN    NS    dns1.stabletransit.com.
    example.edu.        3600    IN    NS    dns2.stabletransit.com.
    example.edu.        3600    IN    MX    50 mail.example.edu.


# Updating a DNS Record
The only attributes that you can modify on a record are the `data`, `priority` (for MX and SRV records), `TTL`, and `comment` attributes. If you have to modify anything else, the only option would be to delete the existing record and then create a new record with the desired settings.

To update a record call its `update()` method, passing in the new values as keyword arguments. Alternatively, you can call the `update_record()` method of either the module or the domain to which the record belongs. As an example, assume you have a `CloudDNSDomain` object 'dom' and a `CloudDNSRecord` object 'rec'. Each of the following statements changes the `TTL` for that record to 600 seconds:

    rec.update(ttl=600)
    # or
    dom.update_record(rec, ttl=600)
    # or
    dns.update_record(dom, rec, ttl=600)


## Deleting a DNS Record
To delete a DNS record, call its `delete()` method, or call the `delete_record()` method of either the domain or the module. Assuming we have the 'dom' and 'rec' objects described above, each of the following commands results in the record being deleted:

    rec.delete()
    # or
    dom.delete_record(rec)
    # or
    dns.delete_record(dom, rec)

Please note that you cannot delete all the records for a domain. There *must* be at least one NS record for every domain.


## Reverse DNS (PTR) Records
In computer networking, reverse DNS lookup or reverse DNS resolution (rDNS) is the determination of a domain name that is associated with a given IP address using the Domain Name Service (DNS) of the Internet. The process of reverse resolving an IP address uses the DNS _pointer_ record type (PTR record). Cloud DNS supports the management of reverse DNS (PTR) records for Rackspace Cloud devices such as Cloud Load Balancers and Cloud Servers™.


## Listing PTR Records
To get the PTR records for a given device, call:

    print dns.list_ptr_records(device)

This returns a list of dicts, with each dict representing the information in a single PTR record.

    <CloudDNSPTRRecord id=PTR-539528, data=1.2.3.4, name=1-2-3-4.abc.example.edu, ttl=7500>


## Adding PTR Records
To add PTR records for a device, you call the `dns.add_ptr_records()` method, passing in the data for each record in dict form. Here is an example of adding reverse DNS for a Cloud Server for both IPv4 and IPv6:

    recs = [{"name": "example.edu",
            "type": "PTR",
            "data": "1.2.3.4",
            "ttl": 7200},
            {"name": "example.edu",
            "type": "PTR",
            "data": "2001:db8::7",
            "ttl": 7200}
            ]
    server = pyrax.cloudservers.servers.get(id_of_server)
    dns.add_ptr_records(server, recs)

The following table lists both the required and optional keys for a PTR record:

Name | Description | Required
---- | ---- | ----
type | Specifies the record type as "PTR". | Yes
name | Specifies the name for the domain or subdomain. Must be a valid domain name. | Yes
data | The data field for PTR records must be a valid IPv4 or IPv6 IP address. | Yes
ttl | If specified, must be greater than 300. Defaults to 3600 if no TTL is specified. | No
comment | If included, its length must be less than or equal to 160 characters. | No


## Updating PTR Records
You can modify the `TTL` or `comment` for an existing PTR record by calling the `update_ptr_record()` method of the module, and passing in a reference to the device and the domain name, along with the updated record values as keyword arguments. You must supply the `domain_name` and `ip_address` parameters, and they must match the values in the existing record. Changing the domain name or IP address is not allowed. If you need to change either of those, you must delete the records and then re-create them with the new domain name.

Name | Description | Required
---- | ---- | ----
device | A reference to the Cloud Server or Cloud Load Balancer object that this PTR record is for. | Yes
domain_name | Specifies the name for the domain or subdomain. Must be a valid domain name. Cannot be modified. | Yes
data | The data field is required for PTR records and must be a valid IPv4 or IPv6 IP address. | Yes
ttl | If specified, must be greater than 300. Defaults to 3600 if no TTL is specified. | No
comment | If included, its length must be less than or equal to 160 characters. | No


The following example shows how to change the TTL of a server whose domain name is "example.edu":

    server = pyrax.cloudservers.servers.get(id_of_server)
    dns.update_ptr_record(server, "example.edu", "1.2.3.4", ttl=9600)


## Deleting PTR Records
You may delete one or all of the PTR records for a given device by calling the `delete_ptr_records()` method and passing in the device reference. All PTR records for a device are deleted if you pass only the device reference. However, if you specify an IP address, only the record for that address is deleted.

    server = pyrax.cloudservers.servers.get(id_of_server)
    # To delete just one PTR record:
    dns.delete_ptr_records(server, ip_address="1.2.3.4")
    # To delete all PTR records for the device:
    dns.delete_ptr_records(server)

# Working with Cloud Files

----

## Basic Concepts
Rackspace Cloud Files allows you to store files in a scalable, redundant manner, and optionally make them available globally using the Akamai CDN network. Unlike a typical computer OS, though, Cloud Files consists of containers, each of which can store millions of objects. But unlike directories on your computer, you cannot nest containers within other containers: they exist only at the root level. However, you can simulate a nested folder structure by naming your objects with names that resemble traditional path notation; for example: "photos/vacations/2012/cancun/beach.jpg". So while all your files are at the base level of their containers, you can retrieve them based on the "path" prefix.

In pyrax, Cloud Files is represented by `Container` and `StorageObject` classes. Once you're authenticated with pyrax, you can interact with Cloud Files via the `pyrax.cloudfiles` object. All of the example code that follows assumes that you have already imported pyrax and authenticated.

All of the code samples in this document assume that you have already imported pyrax, authenticated, and created the name `cf` at the top of the script, like this:

    import pyrax
    pyrax.set_credential_file("my_cred_file")
    # or any other auth method
    cf = pyrax.cloudfiles


## General Account Information
If you want to get an idea of the overall usage for your Cloud Files account, you can run the following:

    cf.get_account_metadata()

This returns a dict that looks something like the following:

    {'x-account-bytes-used': '693966',
     'x-account-container-count': '4',
     'x-account-meta-temp-url-key': 'a3f7d9d89d75385245e13c15490b82cf',
     'x-account-object-count': '148'}


## Setting Account Metadata
Accounts can have metadata (arbitrary key pairs) associated with them; this metadata is entirely for your usage. To add metadata to your account, create a dict with the values you want to add, and run the following code:

    meta = {"color": "blue", "flower": "lily"}
    cf.set_account_metadata(meta)

Note that by default this call adds all of the key pairs to your existing metadata. If you wish to overwrite any existing metadata with the new values, add the parameter `clear=True` to the call:

    meta = {"color": "blue", "flower": "lily"}
    cf.set_account_metadata(meta, clear=True)


## Creating a Container
You must have a container before you can store anything on Cloud Files, so start by creating a container:

    cont = cf.create_container("example")
    print "Name:", cont.name
    print "# of objects:", cont.object_count

And this outputs:

    Name: example
    # of objects: 0

Please note that if you call `create_container()` more than once with the same name, the request is ignored, and a reference to the existing container with that name is returned. This is useful for cases where you want to get a reference to a container, creating it if it does not yet exist.


## Listing All Containers
You can also query `pyrax.cloudfiles` for all the containers on the system. There are two methods for this: `list_containers()` and `get_all_containers()`. The difference between these methods is that `list_containers()` returns a list of the *names* of all containers, while `get_all_containers()` returns a list of `Container` *objects* representing each container:

    print "list_containers:", cf.list_containers()
    print "get_all_containers:", cf.get_all_containers()

This results in:

    list_containers: ['example']
    get_all_containers: [<Container 'example'>]

If you want more information about these containers, you can call `list_containers_info()`. This returns a list of dicts, one for each container. Each dict contains the following keys:

* `name` - the name of the container
* `count` - the number of objects in the container
* `bytes` - the total number of bytes in the container


## Getting a Container Object
Given the name of a container, you can get the corresponding `Container` object easily enough:

    cont = cf.get_container("example")
    print "Container:", cont

This should print:

    Container: <Container 'example'>

Note that if there is no existing container with the name you specify, a `NoSuchContainer` exception is raised. A more robust option is the `create_container()` method, which acts like `get_container()` if the specified container exists, and if not, creates it first and then returns the `Container` object for it.


## [Storing Objects in Cloud Files](id:uploadfiles)
There are two primary options for getting your objects into Cloud Files: passing the content directly, or passing in a file-like object reference. In the latter case, pyrax reads the content to be stored from the object. The two methods for this are `store_object()` and `upload_file()`, respectively.

You also have two options for specifying the container in which the object should be stored. If you already have the `Container` object, you can call either of those methods directly on the `Container`, and the object is stored in the corresponding container. You can also pass the name of the container to pyrax.cloudfiles, and the container with that name is chosen to store the object. If there is no container by that name, a `NoSuchContainer` exception is raised.

Both methods take several optional parameters:

* **`content_type`**: Include this to identify what sort of file the object represents. Examples of `content_type` would be `text/html`, or `audio/mpeg`. If you don't specify `content_type`, Cloud Files tries to determine it for you.
* **`content_encoding`**: If you have a compressed file, setting this value allows you to upload the file in compressed format without losing the identity of the underlying media type of the file.
*  **`ttl`**: If you need to store an object for a limited amount of time, set the `ttl` parameter to the number of seconds that you want the object to exist. After that number of seconds, it is deleted from Cloud Files.

As an example, start with the simplest scenario: storing some text as an object. The example below assumes that the 'example' container we created earlier still exists; if not, make sure you create it before running this code.

The example creates some simple content: a single text sentence stored in the variable name `content`. It then tells `pyrax.cloudfiles` to store that content into the container named `example`, and give that stored object the name `new_object.txt`.

    content = "This is the content of the file."
    obj = cf.store_object("example", "new_object.txt", content)
    print "Stored object:", obj

When an object is successfully created, you receive a `StorageObject` instance representing that object.

One common issue when storing objects is ensuring that the object did not get changed or corrupted in the process. In other words, ensuring that the object that is stored is exactly what you uploaded. `StorageObject` instances have an `etag` attribute that is the MD5 checksum of the file as it exists on Cloud Files. You can run a checksum on your local copy to see if the two values match; if they do, the file was stored intact. However, if you're concerned about integrity, you can compute the MD5 checksum of your file before uploading, and then pass that value in the `etag` parameter of `store_object()` or `upload_file()`, and Cloud Files checks to make sure that its generated checksum matches your supplied etag. If the two don't match, the file is not stored in Cloud Files, and an `UploadFailed` exception is raised.

To make this a simpler process, pyrax includes a utility method for calculating the MD5 checksum; it accepts either raw text or a file-like object. So try this again, this time sending the checksum as the `etag` parameter:

    text = "This is a random collection of words."
    chksum = pyrax.utils.get_checksum(text)
    obj = cf.store_object("example", "new_object.txt",
            text, etag=chksum)
    print "Calculated checksum:", chksum
    print "Stored object etag:", obj.etag

If all went well, the two values are identical. If not, an `UploadFailed` exception would have been raised, and the object would not be stored in Cloud Files.

If you have a `Container` object, you can call `store_object()` directly on it to store an object into that container:

    cont = cf.get_container("example")
    text = "This is a random collection of words."
    chksum = pyrax.utils.get_checksum(text)
    obj = cont.store_object("new_object.txt", text, etag=chksum)
    print "Calculated checksum:", chksum
    print "Stored object etag:", obj.etag

Most of the time, though, you won't have raw text in your code to store; the more likely situation is that you want to store files that exist on your computer into Cloud Files. The way to do that is essentially the same, except that you call `upload_file()`, and pass the full path to the file you want to upload. Additionally, specifying the object's name is optional, since pyrax uses the name of the file as the stored object name by default. `upload_file()` accepts the same `etag` parameter that `store_object()` does, and etag verification works the same way.

    pth = "/home/me/path/to/myfile.txt"
    chksum = pyrax.utils.get_checksum(pth)
    obj = cf.upload_file("example", pth, etag=chksum)
    print "Calculated checksum:", chksum
    print "Stored object etag:", obj.etag

And just as with `store_object()`, you can call `upload_file()` directly on a `Container` object.

Note that (currently) both `store_object()` and `upload_file()` run synchronously, so your code blocks while the transfer occurs. If you plan on building an application that involves significant file transfer, you should plan on making these calls using an asynchronous approach such as the `threading` module, `eventlet`, `twisted`, or another similar approach.


## Retrieving (Downloading) Stored Objects
As with most operations on objects, there are 3 ways to do this. If you have a `StorageObject` reference for the object you want to download, just call its `fetch()` method. If you have the `Container` object that holds the stored object, call its `fetch_object()` method, passing in the name of the object to fetch. Finally, you can call the `pyrax.cloudfiles.fetch_object()` method, passing in the container and object names.

All 3 take the same optional parameters:

* `include_meta` – When True, the methods return a 2-tuple, with the first element containing metadata about the object, and the second a stream of bytes representing the object's contents. When False, just the stream of bytes (i.e., the file's contents) is returned. Defaults to False.
* `chunk_size` – This represents the number of bytes to return from the server at a time. Note that if you specify a chunk size, instead of a stream of bytes, a generator is returned. You must iterate on the generator to retrieve the chunks of bytes. You must fully read the object's contents from the generator before making any other requests, or the results are not defined. Default = None.

Here is some sample code that creates a stored object containing some unicode text, and then retrieves that from Cloud Files using the various parameters:

    text = "This is some text containing unicode like é, ü and ˚¬∆ç"
    obj = cf.store_object("example", "new_object.txt", text)

    # Make sure that the content stored is identical
    print "Using obj.fetch()"
    stored_text = obj.fetch()
    if stored_text == text:
        print "Stored text is identical"
    else:
        print "Difference detected!"
        print "Original:", text
        print "Stored:", stored_text

    # Let's look at the metadata for the stored object
    meta, stored_text = obj.fetch(include_meta=True)
    print
    print "Metadata:", meta

    # Demonstrate chunked retrieval
    print
    print "Using chunked retrieval"
    obj_generator = obj.fetch(chunk_size=12)
    joined_text = "".join(obj_generator)
    if joined_text == text:
        print "Joined text is identical"
    else:
        print "Difference detected!"
        print "Original:", text
        print "Joined:", joined_text

Try running this code; you should find that the retrieved text is identical using both chunked and non-chunked methods. The metadata for the object should look something like:

    Metadata: {'content-length': '62', 'accept-ranges': 'bytes',
        'last-modified': 'Wed, 10 Oct 2012 16:06:25 GMT',
        'etag': '3b3e32a6cd87076997dad4552972194b',
        'x-timestamp': '1349885185.68412',
        'x-trans-id': 'txb57464c49e0345f496a7acf451be77d8',
        'date': 'Wed, 10 Oct 2012 16:06:25 GMT',
        'content-type': 'text/plain'}


## Handling Objects in Nested Folders
Since Cloud Files does not have a hierachical folder structure, you can simulate it be including the full folder path in the object name. E.g., if your folder structure looks like:

* base
    * one
        * two
            * three.txt

…you would typically create a container named 'base', and when uploading the file 'three.txt', you would give it the name 'one/two/three.txt'. There really isn't any such structure inside that container, but it helps you to track the relation of files to their original directory structure.

When you retieve the file from Cloud Files, it's helpful to retain that structure on your disk. Like the other methods, there are three ways to do this. If you have a `StorageObject` reference for the object you want to download, just call its `download()` method. If you have the `Container` object that holds the stored object, call its `downlod_object()` method, passing in the name of the object to fetch. Finally, you can call the `pyrax.cloudfiles.download_object()` method, passing in the container and object names. On all three, you also need to pass in the full path to the local directory in which the file is written. That directory must exist on your disk before you attempt to download files to it.

These commands take an optional parameter named `structure`. When `True` (the default if omitted), the folder structure of your object name is recreated on your disk. If for any reason you don't want this done, simply set this to `False`, and the objects are all stored in the same base directory without any regard to any paths in their names.


## Uploading an Entire Folder to Cloud Files
A very common use case is needing to upload an entire folder, including subfolders, to a Cloud Files container. Because this is so common, pyrax includes an `upload_folder()` method. You pass in the path to the folder you want to upload, and it handles the rest in the background. If you specify the name of a container in your request, the folder contents is uploaded to that container. If you don't specify a container name, a new container with the same name as the folder you are uploading is created, and the objects stored in there.

You can also specify one or more file name patterns to ignore, and pyrax skips any of the files that match any of the patterns. This is useful if there are files that you don't wish to retain, such as .pyc and .pyo files in a Python project. You can pass either a single string pattern, or a list of strings to use.

`upload_folder()` accepts the same optional parameters as `upload_file()`: `content_type`, `content_encoding`, and `ttl`. See the section on [Storing Objects in Cloud Files](#uploadfiles) for an explanation of these parameters. Calling `upload_folder()` returns a 2-tuple: the key for the upload process, and the total bytes to be uploaded. You can use the key to query `pyrax.cloudfiles` for the status of the upload, or to cancel it if necessary.

Here are some examples, using the local folder **"/home/me/projects/cool_project/"**:

    folder = "/home/me/projects/cool_project/"

    # This creates a new container named 'cool_project', and
    # uploads the contents of the target folder to it.
    upload_key, total_bytes = cf.upload_folder(folder)

    # This uploads the contents of the target folder to a container
    # named 'software'. If that container does not exist, it is created.
    upload_key, total_bytes = cf.upload_folder(folder, container="software")

    # This is the same as above, but ignores any files ending in '.pyc'
    upload_key, total_bytes = cf.upload_folder(folder, container="software",
            ignore="*.pyc")

    # Same as above, but skips several different file name patterns
    upload_key, total_bytes = cf.upload_folder(folder, container="software",
            ignore=["*.pyc", "*.tgz", "tmp*"])


### Monitoring Folder Uploads
Since a folder upload can take a while, the uploading happens in a background thread. If you'd like to follow the progress of the upload, you can call `pyrax.cloudfiles.get_uploaded(upload_key)` to get the current number of bytes uploaded for this process. Combined with the total number of bytes returned by the initial call to `upload_folder()`, it is simple to calculate the percentage of the upload that has completed.


### Interrupting Folder Uploads
Sometimes it is necessary to stop a folder upload before it has completed. To do this, call `cloudfiles.cancel_folder_upload(upload_key)`, which causes the background thread to stop uploading.


## Syncing a Local Folder with a Container
Another common use case is to use Cloud Files as a backup of the important files on your local machine. `pyrax` provides the `sync_folder_to_container()` method that makes this straightforward. It takes the following parameters:

Parameter | Required? | Description | Default
---- | ---- | ---- | ---- 
**folder_path** | yes | Full path to the folder on your local machine | n/a
**container** | yes | Either the name of an existing container, or an actual container object | n/a
**include_hidden** | no | When False, files in your folder that begin with a period are ignored | False
**ignore_timestamps** | no | When False, if the local file and remote object differ, the local file is uploaded and overwrites the remote. When True, the local file's modification time is compared with the remote object's last_modified time, and the remote object is only overwritten if the local file is newer. | False

As an example, assume you have a project named 'important' that you want to make sure is always backed up to Cloud Files. You could write a quick script like this, and call it from a cron job.

    local = "/home/myname/projects/important"
    remote = cf.create_container("important_files")
    cf.sync_folder_to_container(local, remote)

This would sync all of the files in that folder, except for hidden files, such as .git subdirectories, or the .swp files that vim creates.


## Listing Objects in a Container
Assuming you have a `Container` object, simply call:

    objects = cont.get_objects()

This returns a list of `StorageObjects` representing the objects in the container. Note that since a container can hold millions of objects, there are several ways of limiting the number of objects returned by this method.

The first limit is the default for Cloud Files: only the first 10,000 objects are returned. If you must have more than that returned in a single call, you can call `cont.get_objects(full_listing=True)`. Be warned that very large containers may take a long time to respond, and connections may time out when waiting for millions of objects to be returned. Conversely, if you have lots of objects and only want to retrieve a much smaller set than 10,000, you can set the `limit` parameter to the maximum number of objects you want returned. If you later on want to get more, such as when paginating your object listings, use the `marker` parameter: setting it to the name of the last object returned from your previous `get_objects()` call causes Cloud Files to return objects starting after the `marker` setting.

There are also two ways to filter your results: the `prefix` and `delimiter` parameters to `get_objects()`. `prefix` works by only returning objects whose names begin with the value you set it to. `delimiter` takes a single character, and excludes any object whose name contains that character.

To illustrate these uses, start by creating a new folder, and populating it with 10 objects. The first 5 have names starting with "series_" followed by an integer between 0 and 4; the second 5 simulate items in a nested folder. They have names that are a single repeated character. The content of the objects is not important, as `get_objects()` works only on the names.

    cont = cf.create_container("my_objects")
    for idx in xrange(5):
    fname = "series_%s" % idx
        cf.store_object(cont, fname, "some text")
    start = ord("a")
    for idx in xrange(start, start+5):
        chars = chr(idx) * 4
        fname = "stuff/%s" % chars
        cont.store_object(fname, "some text")

Start by listing everything:

    objs = cont.get_objects()
    for obj in objs:
        print obj.name

This returns:

    series_0
    series_1
    series_2
    series_3
    series_4
    stuff/aaaa
    stuff/bbbb
    stuff/cccc
    stuff/dddd
    stuff/eeee

Now try paginating the results:

    limit = 4
    marker = ""
    objs = cont.get_objects(limit=limit, marker=marker)
    print "Objects:", [obj.name for obj in objs]    while objs:
        marker = objs[-1].name
        objs = cont.get_objects(limit=limit, marker=marker)
        print "Objects:", [obj.name for obj in objs]

The results show simple pagination in action:

    Objects: ['series_0', 'series_1', 'series_2', 'series_3']
    Objects: ['series_4', 'stuff/aaaa', 'stuff/bbbb', 'stuff/cccc']
    Objects: ['stuff/dddd', 'stuff/eeee']
    Objects: []

You can use the `prefix` parameter to only retrieve objects whose names start with that prefix:

    objs = cont.get_objects(prefix="stuff")
    print "Objects:", [obj.name for obj in objs]

This returns only the 5 "stuff/..." objects:

    Objects: ['stuff/aaaa', 'stuff/bbbb', 'stuff/cccc', 'stuff/dddd', 'stuff/eeee']

The `delimiter` parameter takes a single character and filters out those files containing that character. The most common usage is to use the slash character to skip objects in nested folders within a container.

    objs = cont.get_objects(delimiter="/")
    print "Objects:", [obj.name for obj in objs]

This excludes all the objects in the nested 'stuff' folder:

    Objects: ['series_0', 'series_1', 'series_2', 'series_3', 'series_4']


## Deleting Objects
There are several ways to delete an object from Cloud Files.

If you have the associated `StorageObject` instance for that object, just call its `obj.delete()` method. If you have the `Container` object, you can call its `cont.delete_object(obj_name)` method, passing in the object name. You can also call `pyrax.cloudfiles.delete_object(cont_name, obj_name)`, passing in the container and object names. Finally, if you want to delete all the objects in a container, just call the `container.delete_all_objects()` method.

Note that these methods are asynchronous and return almost immediately. They do not wait until the object has actually been deleted, so there may be a period of several seconds where the object still shows up in the container. Do not interpret the presence of the object in the container soon after deleting it as a sign that the deletion failed.

The following example illustrates object deletion:

    cname = "delete_object_test"
    fname = "soon_to_vanish.txt"
    cont = cf.create_container(cname)
    text = "File Content"
    print "Text size:", len(text)

    # Create a file in the container
    obj = cont.store_object(fname, text)
    # Verify that it's the same size
    print "Object size =", obj.total_bytes

    # Delete it!
    cont.delete_object(fname)
    start = time.time()

    # See if it's still there; if not, this should raise an exception
    # Generally this happens quickly, but an object may appear to remain
    # in a container for a short period of time after calling delete().
    while obj:
        try:
            obj = cont.get_object(fname)
            print "...still there..."
            time.sleep(0.5)
        except exc.NoSuchObject:
            obj = None
            print "Object '%s' has been deleted" % fname
            print "It took %4.2f seconds to appear as deleted." % (time.time() - start)


### Bulk Deletion
The methods above describe how to delete a single object, or every object in a container. But what about the case where you want to delete several objects at once? Sure, you could call `delete()` for each object, but if you had more than a few that would be very inefficient, as each deletion would require a separate API call and response.

For this situation, Cloud Files offers the `bulk_delete()` method. You pass the name of the container along with a list containing the names of all the objects you wish to delete, and they are all deleted with a single API call. The `bulk_delete()` method returns a dictionary (described below) with the results of the process. Please note that while this is faster than many individual calls, it does take some time to complete, depending on how many objects are to be deleted. If you want your program to continue execution without waiting for the bulk deletion to complete, `bulk_delete()` takes an optional third parameter: including `async=True` in the call results in the method returning immediately. Instead of the dictionary that is returned by the synchronous call, it returns an object that can be used to query the status of the bulk deletion by checking its `completed` attribute. When `completed` is True, its `results` attribute contains the same dictionary that is returned from the synchronous call.

The returned dictionary contains the following keys:

Key | Value
---- | ----
**deleted** | the number of objects deleted
**not_found** | the number of objects not found
**status** | the HTTP return status code. '200 OK' indicates success
**errors** | a list of any errors returned by the bulk delete call


### Setting an Object's Expiration
You can mark a storage object for deletion in the future by calling its `delete_in_seconds()` method. This method accepts an integer number of seconds after which you wish the object to be deleted from Cloud Files.

Containers and the main client both have the related `delete_object_in_seconds(container, object, seconds)` method that accomplish the same thing.


## Copying / Moving Objects
Occasionally you may want to copy or move an object from one container to another. You could, of course, upload the object a second time to the new container, but that is inefficient and uses up bandwidth. You can do this all server-side  through the use of the `cloudfiles.copy_object()` and `cloudfiles.move_object()` methods. The methods are similar with the sole difference that `move_object()` deletes the object from the original container, whereas `copy_object()` leaves the original in place.

Both methods take the parameters: `container, obj_name, new_container, new_obj_name=None`. If you omit the `new_obj_name` parameter, the object is moved without renaming.


## Metadata for Containers and Objects
Cloud Files allows you to set and retrieve arbitrary metadata on containers and storage objects. Metadata are simple key/value pairs, with both key and value being strings. Keys are case-insensitive, and are always returned in lowercase. The content of the metadata can be anything that is useful to you. The only requirement is that the keys begin with "X-Container-Meta-" and "X-Object-Meta-", respectively, for containers and storage objects. However, to make things easy for you, pyrax automatically prefixes your metadata headers with those strings if they aren't already present.

    cname = "example"
    cont = cf.create_container(cname)

    # Get the existing metadata, if any
    meta = cf.get_container_metadata(cont)
    print "Initial metadata:", meta

Unless you have explicitly added metadata to this container, you should see it print an empty dict here.

    # Create a dict of metadata. Make one key with the required prefix,
    # and the other without, to illustrate how pyrax 'massages' the keys
    # to include the require prefix.
    new_meta = {"X-Account-Meta-City": "Springfield",
            "Famous_Family": "Simpsons"}
    cf.set_container_metadata(cont, new_meta)

    # Verify that the new metadata has been set for both keys.
    meta = cf.get_container_metadata(cont)
    print "Updated metadata:", meta

After running this, you should see:

    Updated metadata: {'x-container-meta-x-account-meta-city': 'Springfield',
    'x-container-meta-famous-family': 'Simpsons'}

You can update the metadata for a container at any time by calling `cf.set_container_metadata()` again with a dict containing your new key/value pairs. That method takes an additional parameter `clear` which defaults to False; if you pass clear=True, any existing metadata is deleted, and only the metadata you pass in remains. If you leave the default clear=False, the key/value pairs you pass simply update the existing metadata.

To remove a single key from a container's metadata, you can call either `cf.remove_container_metadata_key(cont, key)` or `cont.remove_metadata_key(key)`. Both methods do the same thing.

Metadata for storage objects works exactly the same, using the analogous methods `cf.get_object_metadata(container, obj)`, `cf.set_object_metadata(container, obj, metadata, clear=False)` and `obj.remove_metadata_key(key)`.


## CDN Support
Cloud Files makes it easy to publish your stored objects over the high-speed Akamai CDN. Content is made available at the container level. Individual files within a public container cannot be private. This may affect your storage design, so that only files you wish to have accessible to the public are stored in public containers.


### Publishing a Container to CDN
To publish a container to CDN, simply make the following call:

    cf.make_container_public("example", ttl=900)

This makes the 'example' container public, and sets the `TTL`, or `Time To Live`, to 15 minutes (900 seconds). This is the minimum `TTL` supported.

Once a container is made public, you can access its CDN-related properties. You can see this in action by running the following code:

    cont_name = pyrax.utils.random_name()
    cont = cf.create_container(cont_name)
    print "Before Making Public"
    print "cdn_enabled", cont.cdn_enabled
    print "cdn_ttl", cont.cdn_ttl
    print "cdn_log_retention", cont.cdn_log_retention
    print "cdn_uri", cont.cdn_uri
    print "cdn_ssl_uri", cont.cdn_ssl_uri
    print "cdn_streaming_uri", cont.cdn_streaming_uri
    print "cdn_ios_uri", cont.cdn_ios_uri

    # Make it public
    cont.make_public(ttl=1200)

    # Now re-check the container's attributes
    cont = cf.get_container(cont_name)
    print
    print "After Making Public"
    print "cdn_enabled", cont.cdn_enabled
    print "cdn_ttl", cont.cdn_ttl
    print "cdn_log_retention", cont.cdn_log_retention
    print "cdn_uri", cont.cdn_uri
    print "cdn_ssl_uri", cont.cdn_ssl_uri
    print "cdn_streaming_uri", cont.cdn_streaming_uri
    print "cdn_ios_uri", cont.cdn_ios_uri

    # clean up
    cont.delete()

Running this returns the following:

    Before Making Public
    cdn_enabled False
    cdn_ttl 86400
    cdn_log_retention False
    cdn_uri None
    cdn_ssl_uri None
    cdn_streaming_uri None
    cdn_ios_uri None

    After Making Public
    cdn_enabled True
    cdn_ttl 1200
    cdn_log_retention False
    cdn_uri http://6cface6ba364a8b14147-0a7948bc1fe3dbb60c24b92b61e4818f.r83.cf1.rackcdn.com
    cdn_ssl_uri https://8f03601ca5bfb714a8b2-0a7948bc1fe3dbb60c24b92b61e4818f.ssl.cf1.rackcdn.com
    cdn_streaming_uri http://882ea271eef0a907997b-0a7948bc1fe3dbb60c24b92b61e4818f.r83.stream.cf1.rackcdn.com
    cdn_ios_uri http://797722bf130f8afc2538-5a91ce4f079965bd7f8a88a0a2a855cb.iosr.cf3.rackcdn.com

To remove a container from the public CDN, simply call:

    cont.make_private()

One thing to keep in mind is that even though the container is updated immediately, it remains on the CDN for a period of time, depending on the value of the TTL.


### CDN Log Retention
Setting this to True results in the CDN log files being retained in a container in your Cloud Files account. By default it is turned off on public containers; in order to turn it on, call:

    cf.set_cdn_log_retention(container, True)

Or if you have a `Container` object, just set its property directly:

    cont.cdn_log_retention = True

You can turn off log retention at any time by using the above commands and passing False instead of True.


### Purging CDN Objects
Normally, deleting an object from a public container causes it to be eventually deleted from the CDN network, subject to the container's TTL. In some cases, though, you may need to have an object removed from public access due to personal, business, or security concerns. Currently the CDN limits this to 25 such purge requests per day. More than that requires that you open a ticket with Rackspace to handle the request.

If you wish to purge an object from the CDN network, you need to run:

    cf.purge_cdn_object(container, obj, email_addresses=None)

The `container` and `object` parameters refer to the container and object to be removed, respectively. The optional parameter `email_addresses` takes either a single valid email address or a list of addresses. If the `email_addresses` parameter is provided, an email is sent to each address upon the successful purge of the object from the CDN network.

If you have a `StorageObject` instance, you can call it directly instead:

    obj.purge(email_addresses=None)

You are responsible for deleting the purged object from the container separately, as these calls only affect the object on the CDN network.


## Temporary URLs
The Temporary URL feature (TempURL) allows you to create limited-time Internet addresses which allow you to grant limited access to your Cloud Files account. Using TempURL, you may allow others to retrieve or place objects in your Cloud Files account for as long or as short a time as you wish. Access to the TempURL is independent of whether or not your account is CDN-enabled. And even if you don't CDN-enable a directory, you can still grant temporary public access through a TempURL.

This feature is useful if you want to allow a limited audience to download a file from your Cloud Files account or website. You can give out the TempURL and know that after a specified time, no one will be able to access your object through the address. Or, if you want to allow your audience to upload objects into your Cloud Files account, you can give them a TempURL. After the specified time expires, no one will be able to upload to the address.

Additionally, you need not worry about time running out when someone downloads a large object. If the time expires while a file is being retrieved, the download continues until it is finished. Only the link expires.

To create a Temporary URL, you must first set a key that only you know. This key can be any arbitrary sequence as it is for encoding your account. Once the key is set, you should not change it while you still want others to be able to access your temporary URL. If you change it, the TempURL becomes invalid (within 60 seconds, which is the cache time for a key) and others will not be allowed to access it.

When setting the key, you can either provide your own value, or let `pyrax` create the key for you by not passing in a value.

    cf.set_temp_url_key()
    # - or -
    my_key = "jnRB6#1sduo8YGUF&%7r7guf6f"
    cf.set_temp_url_key(my_key)

In either case, you can retrieve your key by calling:

    key = cf.get_temp_url_key()

Once your key has been set, you can generate the TempURL by passing in the name of the container, the name of the object, the number of seconds you want the URL to be valid, and the method (either 'GET' or 'PUT'; default='GET'). In this example, a TempURL is generated for GETting an object named "photo.jpg" in the "vacation" container. This URL is good for 24 hours.

    secs = 24 * 60 * 60
    url = cf.get_temp_url("vacation", "photo.jpg", seconds=secs, method="GET")

Similarly, if you have a StorageObject reference, you can call its get_temp_url() method; only the duration and method are needed. Likewise for Container objects, except that you need to pass in the object name. Here are two examples:

    # Assume we have a StorageObject 'obj'
    obj.get_temp_url(300)

    # Assume we have a Container 'cont'
    cont.get_temp_url("object_name", 300)

Note that in both of these we omitted the method; this results in the default method of "GET" being used.

# Cloud Load Balancers

## Basic Concepts
Load balancers allow you to distribute workloads among several cloud devices, referred to as 'nodes'. External clients access the services on these nodes via a 'Virtual IP', which is an address on the load balancer for that service.


## Load Balancers in pyrax
Once you have authenticated and connected to the load balancer service, you can reference the load balancer module via `pyrax.cloud_loadbalancers`. This provides general load balancer information for the account, as well as methods for interacting with load balancer instances.

For the sake of brevity and convenience, it is common to define abbreviated aliases for the modules. All the code in the document assumes that at the top of your script, you have added the following lines:

    clb = pyrax.cloud_loadbalancers
    cs = pyrax.cloudservers


## Listing Existing Load Balancers
To get a list of all the load balancers in your cloud, run:

    clb.list()

This returns a list of `LoadBalancer` objects. You can then interact with the individual `LoadBalancer` objects. Assuming that you are just starting out and do not have any load balancers configured yet, you get back an empty list. A good first step, then, would be to create a typical setup: two servers behind a load balancer that distributes web traffic to these two servers.


### Create the Servers
[Working with Cloud Servers](cloud_servers.md) explains how to get the image and flavor IDs necessary to create a server, but for the sake of brevity the code below uses the IDs previously obtained. *Note*: these ID values are not constants, so make sure you get the actual IDs for when your system is running.

    img_id = "5cebb13a-f783-4f8c-8058-c4182c724ccd"
    flavor_id = "performance1-2"

    server1 = cs.servers.create("server1", img_id, flavor_id)
    s1_id = server1.id
    server2 = cs.servers.create("server2", img_id, flavor_id)
    s2_id = server2.id

    # The servers won't have their networks assigned immediately, so
    # wait until they do.
    while not (server1.networks and server2.networks):
        time.sleep(1)
        server1 = cs.servers.get(s1_id)
        server2 = cs.servers.get(s2_id)


### Create the Nodes
Next you need to create the `Nodes` that represent these servers. `Nodes` require that you must specify a `condition`. The value of `condition` must be one of the following:

| Name | Description |
| ------ | ---------- |
| ENABLED | Node is permitted to accept new connections. |
| DISABLED | Node is not permitted to accept any new connections regardless of session persistence configuration. Existing connections are forcibly terminated. |
| DRAINING | Node is allowed to service existing established connections and connections that are being directed to it as a result of the session persistence configuration. |

While you can set an existing `Node` to any of these three conditions, **you can only create new nodes in either 'ENABLED' or 'DISABLED' condition**.

A `Node` is logically linked to the server it represents by the IP address. Since the servers and load balancer are all being created in the same datacenter, you can use the private IP address of the server.

    # Get the private network IPs for the servers
    server1_ip = server1.networks["private"][0]
    server2_ip = server2.networks["private"][0]

    # Use the IPs to create the nodes
    node1 = clb.Node(address=server1_ip, port=80, condition="ENABLED")
    node2 = clb.Node(address=server2_ip, port=80, condition="ENABLED")


### Create the Virtual IP for the Load Balancer
The `VirtualIP` class represents the interface for the `LoadBalancer`. It can be "PUBLIC" or "SERVICENET".

    # Create the Virtual IP
    vip = clb.VirtualIP(type="PUBLIC")


### Create the Load Balancer
Now that you have all the information you need, create the `LoadBalancer` as follows:

    lb = clb.create("example_lb", port=80, protocol="HTTP",
            nodes=[node1, node2], virtual_ips=[vip])


### Re-try the Listing
Now that you have created a `LoadBalancer`, re-run the listing command:

    print [(lb.name, lb.id) for lb in clb.list()]

This time the output should look like:

    [(u'example_lb', 82663)]


## Working with Load Balancers
You can get a list of all your load balancers as above, or you can get a specific load balancer by ID.

    lb = clb.get(82663)

Once you have a `LoadBalancer` object, you can use its attributes to get information about its status, nodes, virtual ips, algorithm, and protocol.

    print "Load Balancer:", lb.name
    print "ID:", lb.id
    print "Status:", lb.status
    print "Nodes:", lb.nodes
    print "Virtual IPs:", lb.virtual_ips
    print "Algorithm:", lb.algorithm
    print "Protocol:", lb.protocol

For the `LoadBalancer` just created, the output of the above is:

    Load Balancer: example_lb
    ID: 78273
    Status: ACTIVE
    Nodes: [<Node type=PRIMARY, condition=ENABLED, id=172949, address=10.177.1.1, port=80>, <Node type=PRIMARY, condition=DISABLED, id=173161, address=10.177.1.2, port=80>]
    Virtual IPs: [<VirtualIP type=PUBLIC, id=1893, address=50.56.167.209>, <VirtualIP type=PUBLIC, id=9070313, address=2001:4800:7901:0000:8ca7:b42c:0000:0001>]
    Algorithm: RANDOM
    Protocol: HTTP


## Load Balancer Algorithms
The load balancer's 'algorithm' refers to the logic that determines how connections are spread across the nodes. You can get the available algorithms by running:

    print clb.algorithms

This prints:

    [u'LEAST_CONNECTIONS', u'RANDOM', u'ROUND_ROBIN', u'WEIGHTED_LEAST_CONNECTIONS', u'WEIGHTED_ROUND_ROBIN']

This table lists the algorithms and how they work:

Algorithm | Description
---- | ----
LEAST_CONNECTIONS | The node with the lowest number of connections receives the requests.
RANDOM | Back-end servers are selected at random.
ROUND_ROBIN | Connections are routed to each of the back-end servers in turn.
WEIGHTED_LEAST_CONNECTIONS | Each request is assigned to a node based on the number of concurrent connections to the node and its weight.
WEIGHTED_ROUND_ROBIN | A round robin algorithm, but with different proportions of traffic being directed to the back-end nodes. Weights must be defined as part of the load balancer's node configuration.


## Load Balancer Protocols
All load balancers must define the protocol of the service which is being load balanced. The protocol selection should be based on the protocol of the back-end nodes. When configuring a load balancer, the default port for the given protocol is selected unless otherwise specified. You can get a list of the available protocols by calling:

    print clb.protocols

This prints out:

    [u'DNS_TCP', u'DNS_UDP', u'FTP', u'HTTP', u'HTTPS', u'IMAPS', u'IMAPv2', u'IMAPv3', u'IMAPv4', u'LDAP', u'LDAPS', u'MYSQL', u'POP3', u'POP3S', u'SFTP', u'SMTP', u'TCP', u'TCP_CLIENT_FIRST', u'UDP', u'UDP_STREAM']

Here is a table of available protocols and their description:

Protocol | Description
---- | ----
DNS_TCP | This protocol works with IPv6 and allows your DNS server to receive traffic using TCP port 53.
DNS_UDP | This protocol works with IPv6 and allows your DNS server to receive traffic using UDP port 53.
FTP | The File Transfer Protocol defines how files are transported over the Internet. It is typically used when downloading or uploading files to or from web servers.
HTTP | The Hypertext Transfer Protocol defines how communications occur on the Internet between clients and web servers. For example, if you request a web page in your browser, HTTP defines how the web server fetches the page and returns it your browser.
HTTPS | The Hypertext Transfer Protocol over Secure Socket Layer (SSL) provides encrypted communication over the Internet. It securely verifies the authenticity of the web server you are communicating with.
IMAPS | The Internet Message Application Protocol over Secure Socket Layer (SSL) defines how an email client, such as Microsoft Outlook, retrieves and transfers email messages with a mail server.
IMAPv2 | Version 2 of IMAPS.
IMAPv3 | Version 3 of IMAPS.
IMAPv4 | Version 4, the current version of IMAPS.
LDAP | The Lightweight Directory Access Protocol provides access to distributed directory information services over the Internet. This protocol is typically used to access a large set of hierarchical records, such as corporate email or a telephone directory.
LDAPS | The Lightweight Directory Access Protocol over Secure Socket Layer (SSL).
MYSQL | This protocol allows communication with MySQL, an open source database management system.
POP3 | The Post Office Protocol is one of the two most common protocols for communciation between email clients and email servers. Version 3 is the current standard of POP.
POP3S | Post Office Protocol over Secure Socket Layer.
SFTP | The SSH File Transfer Protocol is a secure file transfer and management protocol. This protocol assumes the files are using a secure channel, such as SSH, and that the identity of the client is available to the protocol.
SMTP | The Simple Mail Transfer Protocol is used by electronic mail servers to send and receive email messages. Email clients use this protocol to relay messages to another computer or web server, but use IMAP or POP to send and receive messages.
TCP | The Transmission Control Protocol is a part of the Transport Layer protocol and is one of the core protocols of the Internet Protocol Suite. It provides a reliable, ordered delivery of a stream of bytes from one program on a computer to another program on another computer. Applications that require an ordered and reliable delivery of packets use this protocol.
TCP_CLIE (TCP_CLIENT_FIRST) | This protocol is similiar to TCP, but is more efficient when a client is expected to write the data first.
UDP | The User Datagram Protocol provides a datagram service that emphasizes speed over reliability, It works well with applications that provide security through other measures.
UDP_STRE (UDP_STREAM) | This protocol is designed to stream media over networks and is built on top of UDP.


## SSL Termination
The SSL Termination feature allows a load balancer user to terminate SSL traffic at the load balancer layer versus at the web server layer. A user may choose to configure SSL Termination using a key and an SSL certificate or an (Intermediate) SSL certificate.

When SSL Termination is configured on a load balancer, a secure shadow server is created that listens only for secure traffic on a user-specified port. This shadow server is only visible to and manageable by the system. Existing or updated attributes on a load balancer with SSL Termination also apply to its shadow server. For example, if Connection Logging is enabled on an SSL load balancer, it is also enabled on the shadow server and Cloud Files logs contain log files for both.

NOTE: SSL termination should not be used when transferring certain types of Personally Identifiable Information (PII). For the definition of PII, see this [Knowledge Center article](http://www.rackspace.com/knowledge_center/article/definition-of-personally-identifiable-information-pii).

To add SSL Termination to your load balancer (`lb`), call its `add_ssl_termination()` method:

    cert = "-----BEGIN CERTIFICATE-----\nMIIEXTCCA0W … Xy8=\n-----END CERTIFICATE-----"
    pk = "-----BEGIN RSA PRIVATE KEY-----\nMII … s8Q==\n-----END RSA PRIVATE KEY-----'"
    lb.add_ssl_termination(
            securePort=443,
            enabled=True,
            secureTrafficOnly=False,
            certificate=cert,
            privatekey=pk,
            )

Once SSL Termination is configured, you can only update the `securePort`, `secureTrafficOnly`, or `enabled` settings. This is done by passing one or more of these values to `lb.update_ssl_termination()`. You may not add or update the certificates or keys. If you need to change certificates, you must first call `lb.delete_ssl_termination()`, and then add all the info back at once with `lb.add_ssl_termination()`


## Metadata
Each load balancer can have arbitrary key/value pairs associated with it. These keys and values must be valid UTF-8 characters, of 256 characters or less. To see the metadata for a load balancer, call its `get_metadata()` method. This returns a dict that contains the keys and associated values, or an empty dict if the load balancer does not have any metadata.

There are two methods for creating metadata for a load balancer: `set_metadata()` and `update_metadata()`. The difference is that `update_metadata()` only affects the keys in the update, whereas `set_metadata()` deletes any existing metadata first before setting the values passed to it. The following code illustrates the different methods for working with metadata:

    print "Initial metadata:", lb.get_metadata()
    lb.set_metadata({"a": "one", "b": "two", "c": "three"})
    print "New metadata:", lb.get_metadata()
    lb.update_metadata({"d": "four"})
    print "Updated metadata:", lb.get_metadata()
    lb.set_metadata({"e": "five"})
    print "After set_metadata:", lb.get_metadata()
    lb.delete_metadata()
    print "After delete_metadata:", lb.get_metadata()

This results in:

    Initial metadata: {}
    New metadata: {u'a': u'one', u'c': u'three', u'b': u'two'}
    Updated metadata: {u'a': u'one', u'c': u'three', u'b': u'two', u'd': u'four'}
    After set_metadata: {u'e': u'five'}

    After delete_metadata: {}


## Updating the Load Balancer
A Load Balancer has several attributes that can be updated while the load balancer is running:

    *  name
    *  algorithm
    *  protocol
    *  halfClosed
    *  port
    *  timeout

To update any of these, call the `update()` method of the load balancer, and pass in the new values as keyword arguments. For example, to change the timeout to 60 seconds and the algorithm to 'RANDOM' for a given `CloudLoadBalancer` object named `lb`, you would call:

    lb.update(timeout=60, algorithm="RANDOM")

You can also call the module itself, passing in the load balancer reference, which can be either a `CloudLoadBalancer` object, or the ID of the load balancer:

    clb.update(lb, timeout=60, algorithm="RANDOM")


## Managing Nodes

### Adding and Removing Nodes for a Load Balancer
`CloudLoadBalancer` instances have a method `add_nodes()` that accepts either a single `Node` or a list of `Node` objects and adds them to the `LoadBalancer`. To remove a `Node`, though, you must get a reference to that node and then call its `delete()` method.

    lb = clb.list()[0]
    print
    print "Load Balancer:", lb
    print
    print "Current nodes:", lb.nodes

    # You may have to adjust the address of the node to something on
    # the same internal network as your load balancer.
    new_node = clb.Node(address="10.177.1.2", port=80, condition="ENABLED")
    lb.add_nodes([new_node])
    pyrax.utils.wait_until(lb, "status", "ACTIVE", interval=1, attempts=30, verbose=True)

    print
    print "After adding node:", lb.nodes

    # Now remove that node. Note that we can't use the original node instance,
    # as it was created independently, and doesn't have the link to its load
    # balancer. Instead, we'll get the last node from the load balancer.
    added_node = [node for node in lb.nodes
            if node.address == new_node.address][0]
    print
    print "Added Node:", added_node
    added_node.delete()
    pyrax.utils.wait_until(lb, "status", "ACTIVE", interval=1, attempts=30, verbose=True)    print
    print "After removing node:", lb.nodes

Note the `wait_until()` method. After modifying a load balancer, its status is set to `PENDING_UPDATE`. While it is in that status, no further changes can be made. Once the changes have completed, the status is set back to `ACTIVE`. All that `wait_until()` does is loop until the load balancer is ready. It is a convenient routine for processes that require intermediate steps that must complete before the next step is taken.

Running the above code results in:

    Load Balancer: <CloudLoadBalancer algorithm=RANDOM, created={u'time': u'2012-11-12T18:47:14Z'}, id=78273, name=sUwSNqKH, nodeCount=3, port=80, protocol=HTTP, status=ACTIVE, updated={u'time': u'2012-11-16T20:43:10Z'}, virtual_ips=[<VirtualIP type=PUBLIC, id=1893, address=50.56.167.209 version=IPV4>, <VirtualIP type=PUBLIC, id=9070313, address=2001:4800:7901:0000:8ca7:b42c:0000:0001 version=IPV6>]>

    Current nodes: [<Node type=PRIMARY, condition=ENABLED, id=176621, address=10.177.1.42, port=80 weight=1>, <Node type=PRIMARY, condition=ENABLED, id=172949, address=10.177.1.1, port=80 weight=1>, <Node type=PRIMARY, condition=ENABLED, id=176435, address=10.177.1.3, port=80 weight=1>]

    After adding node: [<Node type=PRIMARY, condition=ENABLED, id=176435, address=10.177.1.3, port=80 weight=1>, <Node type=PRIMARY, condition=ENABLED, id=176635, address=10.177.1.2, port=80 weight=1>, <Node type=PRIMARY, condition=ENABLED, id=172949, address=10.177.1.1, port=80 weight=1>, <Node type=PRIMARY, condition=ENABLED, id=176621, address=10.177.1.42, port=80 weight=1>]

    Added Node: <Node type=PRIMARY, condition=ENABLED, id=176635, address=10.177.1.2, port=80 weight=1>

    After removing node: [<Node type=PRIMARY, condition=ENABLED, id=172949, address=10.177.1.1, port=80 weight=1>, <Node type=PRIMARY, condition=ENABLED, id=176435, address=10.177.1.3, port=80 weight=1>, <Node type=PRIMARY, condition=ENABLED, id=176621, address=10.177.1.42, port=80 weight=1>]


### Changing a Node's Condition
`Nodes` can be in one of 3 "conditions": ENABLED, DISABLED, and DRAINING. To change the condition of a `Node`, you change its `condition` attribute, and then call its `update()` method.

    lb = clb.list()[0]
    # Initial state
    print "Initial:", [(node.id, node.condition) for node in lb.nodes]

    # Toggle the first node's condition between ENABLED and DISABLED
    node = lb.nodes[0]
    node.condition = "DISABLED" if node.condition == "ENABLED" else "ENABLED"
    node.update()

    # After toggling
    print "Toggled:", [(node.id, node.condition) for node in lb.nodes]

The above should result in something like:

    Initial: [(247917, u'ENABLED'), (248387, u'ENABLED'), (247919, u'ENABLED')]
    Toggled: [(247917, 'DISABLED'), (248387, u'ENABLED'), (247919, u'ENABLED')]


### Node Metadata
Each node can have metadata associated with it, just as load balancers can. The methods, syntax, and effects are exactly the same as for load balancers. See the section above on Metadata for details on the methods and their effects.


## Usage Data
You can get load balancer usage data for your entire account by calling `clb.get_usage()`. Individual instances of the `CloudLoadBalancer` class also have a `get_usage()` method that returns the usage for just that load balancer. Please note that usage statistics are very fine-grained, with a record for every hour that the load balancer is active. Each record is a dict with the following format:

    {'averageNumConnections': 0.0,
      'averageNumConnectionsSsl': 0.0,
      'endTime': datetime.datetime(2012, 10, 15, 14, 0),
      'id': 5213627,
      'incomingTransfer': 0,
      'incomingTransferSsl': 0,
      'numPolls': 12,
      'numVips': 1,
      'outgoingTransfer': 0,
      'outgoingTransferSsl': 0,
      'sslMode': u'OFF',
      'startTime': datetime.datetime(2012, 10, 15, 13, 0),
      'vipType': u'PUBLIC'}

This output is for a test load balancer that is not getting any traffic. If this had been for an actual load balancer in production use, the values reported would not be all zeroes.

The call to `get_usage()` can return a *lot* of data. Many times you may only be interested in the usage data for a given time period, so the method supports two optional parameters: `start` and `end`. These can be date/time values in one of the following formats:

* A Python datetime.datetime object
* A Python datetime.date object
* A string in the format "YYYY-MM-DD HH:MM:SS"
* A string in the format "YYYY-MM-DD"

When both starting and ending times are specified, the resulting usage data only includes records within that time period. When only the starting time is specified, all records from that point to the present are returned. When only the ending time is specified, all records from the earliest up to the ending time are returned.


## Load Balancer Statistics
To get the statistics for an individual load balancer, call its `get_stats()` method. You get back a dictionary like this:

    {'connectError': 0,
     'connectFailure': 0,
     'connectTimeOut': 2,
     'dataTimedOut': 0,
     'keepAliveTimedOut': 0,
     'maxConn': 14}


## Health Monitors
A health monitor is a configurable feature of each load balancer. It is used to determine whether or not a back-end node is usable for processing a request.

To get the current Health Monitor for a load balancer, run the following code:

    lb = clb.list()[0]
    hm = lb.get_health_monitor()

The call to `get_health_monitor()` returns a dict representing the health monitor for the load balancer. If no monitors have been added, an empty dict is returned.

There are 3 types of Health Monitor probes:

* TCP connect
* HTTP
* HTTPS

Health Monitors have an `attemptsBeforeDeactivation` setting that specifies how many failures for a node are needed before the node is removed from the load balancer's rotation.


### Adding a TCP Connection Health Monitor
This type of monitor simply checks if the load balancer's nodes are available for TCP connections.

    lb = clb.list()[0]
    lb.add_health_monitor(type="CONNECT", delay=10, timeout=10,
            attemptsBeforeDeactivation=3)

Here are the parameters for configuring a TCP Connection health monitor:

Name | Description | Default | Required
---- | ---- | ---- | ----
attemptsBeforeDeactivation | Number of permissible monitor failures before removing a node from rotation. Must be a number between 1 and 10. | 3 | Yes
delay | The minimum number of seconds to wait before executing the health monitor. Must be a number between 1 and 3600. | 10 | Yes
timeout | Maximum number of seconds to wait for a connection to be established before timing out. Must be a number between 1 and 300. | 10 | Yes
type | Type of the health monitor. Must be specified as "CONNECT" to monitor connections. | None | Yes


### Adding a Health Monitor for HTTP(S)
These types of monitors check whether the load balancer's nodes can be reached via standard HTTP or HTTPS ports. Note that the type must match the load balancer protocol: if the load balancer is 'HTTP', you cannot create an 'HTTPS' health monitor. These types of monitors also require several more parameters to be defined for the monitor:

    lb = clb.list()[0]
    lb.add_health_monitor(type="HTTP", delay=10, timeout=10,
            attemptsBeforeDeactivation=3, path="/",
            statusRegex="^[234][0-9][0-9]$",
            bodyRegex=".* testing .*"i,
            hostHeader="example.com")

The `path` parameter indicates the HTTP path for the request; the `statusRegex` parameter is compared against the returned status code, and the `bodyRegex` parameter is compared with the body of the response. If both response patterns match, the node is considered healthy. The `hostHeader` parameter is the only one that is optional. If included, the monitor checks that hostname.


####Health Monitor Parameters
Name | Description | Default | Required
---- | ---- | ---- | ----
attemptsBeforeDeactivation | Number of permissible monitor failures before removing a node from rotation. Must be a number between 1 and 10. | 3 | Yes
bodyRegex | A regular expression that is used to evaluate the contents of the body of the response. | None | Yes
delay | The minimum number of seconds to wait before executing the health monitor. Must be a number betwe en 1 and 3600. | 10 | Yes
hostHeader | The name of a host for which the health monitors check. | None | No
path | The HTTP path that is used in the sample request. | "/" | Yes
statusRegex | A regular expression that is used to evaluate the HTTP status code returned in the res ponse. | None | Yes
timeout | Maximum number of seconds to wait for a connection to be established before timing out. Must be a number between 1 and 300. | 10 | Yes
type | Type of the health monitor. Must be specified as "HTTP" to monitor an HTTP response or "HTTPS" to monitor an HTTPS response. | None | Yes


### Deleting a Health Monitor
To remove a health monitor from a load balancer, run the following:

    lb = clb.list()[0]
    lb.delete_health_monitor()


## Session Persistence
Session persistence is a feature of the load balancing service that forces multiple requests from clients to be directed to the same node. This is common with many web applications that do not inherently share application state between back-end servers. There are two persistence modes:

####Session Persistence Modes

| Name | Description |
| ---- | ----------- |
| HTTP_COOKIE | A session persistence mechanism that inserts an HTTP cookie and is used to determine the destination back-end node. This is supported for HTTP load balancing only. |
| SOURCE_IP | A session persistence mechanism that keeps track of the source IP address that is mapped and is able to determine the destination back-end node. This is supported for HTTPS pass-through and non-HTTP load balancing only. |

To get the session persistence setting for a load balancer, you would run:

    lb = clb.list()[0]
    sp_mgr = lb.session_persistence()
    print sp_mgr.get()

By default, load balancers are not configured for session persistence. You would run the following code to add persistence to your load balancer:

    lb = clb.list()[0]
    sp_mgr = lb.session_persistence()
    sp = sp_mgr.resource(persistenceType="HTTP_COOKIE")
    sp_mgr.add(sp)

Similarly, to remove session persistence from your load balancer, you would run:

    lb = clb.list()[0]
    sp_mgr = lb.session_persistence()
    sp_mgr.delete()


## Connection Logging
The connection logging feature allows logs to be delivered to a Cloud Files account every hour. For HTTP-based protocol traffic, these are Apache-style access logs. For all other traffic, this is connection and transfer logging.

You can retrieve the current state of connection logging for a given load balancer, and also enable/disable connection logging.

    lb = clb.list()[0]
    cl_mgr = lb.connection_logging()
    # Get the current state
    print "Current logging status:", cl_mgr.get()
    # Enable connection logging
    cl_mgr.enable()
    print "Logging status after enable():", cl_mgr.get()
    # Disable connection logging
    cl_mgr.disable()
    print "Logging status after disable():", cl_mgr.get()

After running the above code, you should see output like this:

    Current logging status: False
    Logging status after enable(): True
    Logging status after disable(): False


## Access Lists
The access list management feature allows fine-grained network access controls to be applied to the load balancer's virtual IP address. A single IP address, multiple IP addresses, or entire network subnets can be added as a `networkItem`. Items that are configured with the `ALLOW` type always take precedence over items with the `DENY` type. To reject traffic from all items except for those with the `ALLOW` type, add a `networkItem` with an address of "0.0.0.0/0" and a `DENY` type.

To see the access lists for a load balancer, call the load balancer's `get_access_list()` method:

    lb = clb.list()[0]
    print "Starting:", lb.get_access_list()

Assuming you have not yet set up an access list, this would return an empty list:

    Starting: []

Suppose you wanted to only allow access to this load balancer from the address 10.20.30.40: you would create an 'ALLOW' record for that address, and a 'DENY' record for all others. Each record is a dict with the keys 'address' and 'type':

    network_item1 = dict(address="10.20.30.40", type="ALLOW")
    network_item2 = dict(address="0.0.0.0/0", type="DENY")

Now configure the load balancer by passing a list of these records to its `add_access_list()` method:

    lb.add_access_list([network_item1, network_item2])

Now confirm that the load balancer has been configured:

    print "After:", lb.get_access_list()

This prints:

    After: [{u'address': u'0.0.0.0/0', u'id': 19019, u'type': u'DENY'}, {u'address': u'10.20.30.40', u'id': 19021, u'type': u'ALLOW'}]

You can remove any individual item from an access list by calling the `delete_access_list_items()` method of the load balancer and passing in the ID of the item to remove. You may pass in a single ID, or a list of several IDs. To remove the `ALLOW` item from this load balancer, run the following:

    lb.delete_access_list_items(19021)
    print "After deletion", lb.get_access_list()

This should return:

    After deletion: [{u'address': u'0.0.0.0/0', u'id': 19019, u'type': u'DENY'}]

To delete the entire access list, call the `delete_access_list()` method:

    lb.delete_access_list()


## Error Pages
An error page is the HTML file that is shown to the end user when there is an attempt to access a node that is offline. All load balancers are given a default error page, but you also have the ability to add a custom error page per load balancer. Here are some examples of working with error pages:

    lb = clb.list()[0]
    print lb.get_error_page()

If no custom error page has been set, you should see:

    u'<html><head><meta http-equiv="Content-Type" content="text/html;charset=utf-8"><title>Service Unavailable</title><style type="text/css">body, p, h1 {font-family: Verdana, Arial, Helvetica, sans-serif;}h2 {font-family: Arial, Helvetica, sans-serif;color: #b10b29;}</style></head><body><h2>Service Unavailable</h2><p>The service is temporarily unavailable. Please try again later.</p></body></html>'

To create a custom error page for this load balancer, run the following:

    html = "<html><body>Sorry, something is amiss!</body></html>"
    lb.set_error_page(html)

To remove the custom error page and return to the default, run:

    lb.clear_error_page()


## Content Caching
When content caching is enabled, recently-accessed files are stored on the load balancer for easy retrieval by web clients. Content caching improves the performance of high traffic web sites by temporarily storing data that was recently accessed. While it's cached, requests for that data are served by the load balancer, which in turn reduces load off the back end nodes. The result is improved response times for those requests and less load on the web server.

This is a simple on/off setting on the load balancer object. Assuming that you have a reference `lb` to the load balancer:

    # Turn on caching
    lb.content_caching = True
    # Turn off caching
    lb.content_caching = False

# Cloud Monitoring

## Basic Concepts
Rackspace Cloud Monitoring provides timely and accurate information about how your resources are performing. It supplies you with key information that can help you manage your business by enabling you to keep track of your cloud resources and receive instant notification when a resource needs your attention. You can quickly create multiple monitors with predefined checks, such as PING, HTTPS, SMTP, and many others.

## Monitoring in pyrax
Once you have authenticated, you can reference the monitoring service via `pyrax.cloud_monitoring`. You interact with Cloud Monitoring through this object.

For the sake of brevity and convenience, it is common to define abbreviated aliases for the modules. All the code in this document assumes that you have added the following line at the top of your script:

    cm = pyrax.cloud_monitoring

Note that as of this writing, pyrax only supports **remote monitoring**. There is a second type of monitoring that is currently in Preview mode that uses a *Monitoring Agent* installed on your device.


## Key Terminology
### Entity
In Rackspace Cloud Monitoring, an entity is the object or resource that you want to monitor. It can be any object or device that you want to monitor. It is commonly a web server, but it might also be a website, a web page, or a web service.

When you create an entity, you will specify characteristics that describe what you are monitoring. At a minimum you must specify a name for the entity. The name is a user-friendly label or description that helps you identify the resource. You can also specify other attributes of the entity, such the entity's IP address, and any metadata that you'd like to associate with the entity.

### Check
Once you've created an entity, you can configure one or more checks for it. A check is the building block of the monitoring system, and is always associated with an entity. The check specifies the parts or pieces of the entity that you want to monitor, the monitoring frequency, how many monitoring zones are launching the check, and so on. Basically, it contains the specific details of how you are monitoring the entity.

You can associate one or more checks with an entity. An entity must have at least one check, but by creating multiple checks for an entity, you can monitor several different aspects of a single resource.

For each check you create within the monitoring system, you'll designate a check type. The check type tells the monitoring system which method to use, such as PING, HTTP, SMTP, and so on, when investigating the monitored resource. Rackspace Cloud Monitoring check types are fully described [here](http://docs.rackspace.com/cm/api/v1.0/cm-devguide/content/appendix-check-types.html).

Note that if something happens to your resource, the check does not trigger a notification action. Rather, the alarms that you create separately and associate with the check trigger the notifications.

### Monitoring Zones
When you create a check, you specify which monitoring zone(s) you want to launch the check from. A monitoring zone is the point of origin, or "launch point", of the check. This concept of a monitoring zone is similar to that of a datacenter, however in the monitoring system, you can think of it more as a geographical region.

You can launch checks for a particular entity from multiple monitoring zones. This allows you to observe the performance of an entity from different regions of the world. It is also a way to prevent false alarms. For example, if the check from one monitoring zone reports that an entity is down, a second or third monitoring zone might report that the entity is up and running. This gives you a better picture of an entity's overall health.

### Collectors
A collector collects data from the monitoring zone and is mapped directly to an individual machine or a virtual machine. Monitoring zones contain many collectors, all of which will be within the IP address range listed in the response. Note that there may also be unallocated IP addresses or unrelated machines within that IP address range.

### Monitoring Agent
Note: The Monitoring Agent is a Preview feature.

The agent provides insight into the internals of your servers with checks for information such as load average and network usage. The agent runs as a single small service that runs scheduled checks and pushes metrics to the rest of Cloud Monitoring so the metrics can be analyzed, alerted on, and archived. These metrics are gathered via checks using agent check types, and can be used with the other Cloud Monitoring primitives such as alarms. See Section B.2, “[Agent Check Types](http://docs.rackspace.com/cm/api/v1.0/cm-devguide/content/appendix-check-types-agent.html)” for a list of agent check types.

### Alarms
An alarm contains a set of rules that determine when the monitoring system sends a notification. You can create multiple alarms for the different check types associated with an entity. For example, if your entity is a web server that hosts your company's website, you can create one alarm to monitor the server itself, and another alarm to monitor the website.

The alarms language provides you with scoping parameters that let you pinpoint the value that will trigger the alarm. The scoping parameters are inherently flexible, so that you can set up multiple checks to trigger a single alarm. The alarm language supplies an adaptable triggering system that makes it easy for you to define different formulas for each alarm that monitors an entity's uptime. To learn how to use the alarm language to create robust monitors, see [Alert Triggering and Alarms](http://docs.rackspace.com/cm/api/v1.0/cm-devguide/content/alerts-language.html).

### Notifications
A notification is an informational message that you receive from the monitoring system when an alarm is triggered. You can set up notifications to alert a single individual or an entire team. Rackspace Cloud Monitoring currently supports calling webhooks, sending email, and the PagerDuty web service for notifications.

### Notification Plans
A notification plan contains a set of notification rules to execute when an alarm is triggered. A notification plan can contain multiple notifications for each of the following states:

* Critical
* Warning
* OK


## How Cloud Monitoring Works
Cloud Monitoring helps you keep a keen eye on all of your resources, from web sites to web servers, routers, load balancers, and more. Use the following Monitoring workflow:

* Create an entity to represent the item you want to monitor. For example, the entity might represent a web site.
* Attach a predefined check to the entity. For example, you could use the PING check to monitor your web site's public IP address.
* Run your checks from multiple monitoring zones to provide redundant monitoring as well as voting logic to avoid false alarms.
* Create a notification to define an action that Cloud Monitoring uses to communicate with you when a problem occurs. For example, you might define a notification that specifies an email that Cloud Monitoring will send when a condition is met.
* Create notification plans which allow you to organize a set of several notifications, or actions, that are taken for different severities.
* Define one or more alarms for each check. An alarm lets you specify trigger conditions for the various metrics returned by the check. When a specific condition is met, the alarm is triggered and your notification plan is put into action. For example, your alarm may indicate a PING response time. If this time elapses, the alarm could send you an email.

## Create an Entity
The first step in working with Cloud Monitoring is to create an `entity`, which represents the device to monitor. To do so, you specify the characteristics of the device, which include one or more IP addresses. The parameter `ip_addresses` is a dictionary, with the keys being a string that can be used to identify the address (known as an *alias* to other parts of the API), and the value the IPv4 or IPv6 address for the entity. You can include as many addresses as you need. You can also include optional metadata to help you identify what the entity represents in your system.

    ent = cm.create_entity(name="sample_entity", ip_addresses={"example": "1.2.34"},
            metadata={"description": "Just a test entity"})

## Create a Check
There are numerous types of checks, and each requires its own parameters, and offers its own combination of metrics. The list of all [available check types](http://docs.rackspace.com/cm/api/v1.0/cm-devguide/content/appendix-check-types.html) shows how extensive your monitoring options are.

### Check Types
As an example, create a check on the HTTP server for the entity. This check will try to connect to the server and retrieve the specified URL using the specified method, optionally with the password and user for authentication, using SSL, and checking the body with a regex. This can be used to test that a web application running on a server is responding without generating error messages. It can also test if the SSL certificate is valid.

From the table in the Available Check Types link above, you can find that the ID of the desired check type is `remote.http`. You can also get a list of all check types by calling:

    chk_types = cm.list_check_types()

This returns a list of `CloudMonitorCheckType` objects. Each object has an attribute named `fields` that lists the parameters for that type, with each indicating whether the field is required or optional, its name, and a brief description. As an example, here is the `fields` attribute for the `remote.http` check type:

    [{u'description': u'Target URL',
          u'name': u'url',
          u'optional': False},
     {u'description': u'Body match regular expression (body is limited to 100k)',
          u'name': u'body',
          u'optional': True},
     {u'description': u'Arbitrary headers which are sent with the request.',
          u'name': u'headers',
          u'optional': True},
     {u'description': u'Body match regular expressions (body is limited to 100k, matches are truncated to 80 characters)',
          u'name': u'body_matches',
          u'optional': True},
     {u'description': u'HTTP method (default: GET)',
          u'name': u'method',
          u'optional': True},
     {u'description': u'Optional auth user',
          u'name': u'auth_user',
          u'optional': True},
     {u'description': u'Optional auth password',
          u'name': u'auth_password',
          u'optional': True},
     {u'description': u'Follow redirects (default: true)',
          u'name': u'follow_redirects',
          u'optional': True},
     {u'description': u'Specify a request body (limited to 1024 characters). If following a redirect, payload will only be sent to first location',
          u'name': u'payload',
          u'optional': True}]

Note that most of the parameters are optional; the only required parameter is **url**. If you only include that, the monitor will simply check that a plain GET on that URL gets some sort of response. By adding additional parameters to the check, you can make the tests that the check carries out much more specific.


### Monitoring Zones
To list the available Monitoring Zones, call:

    cm.list_monitoring_zones()

This returns a list of `CloudMonitorZone` objects:

    [<CloudMonitorZone country_code=US, id=mzdfw, label=Dallas Fort Worth (DFW), source_ips=[u'2001:4800:7902:0001::/64', u'50.56.142.128/26']>,
     <CloudMonitorZone country_code=HK, id=mzhkg, label=Hong Kong (HKG), source_ips=[u'180.150.149.64/26', u'2401:1800:7902:1:0:0:0:0/64']>,
     <CloudMonitorZone country_code=US, id=mziad, label=Washington Dulles (IAD), source_ips=[u'2001:4802:7902:0001::/64', u'69.20.52.192/26']>,
     <CloudMonitorZone country_code=GB, id=mzlon, label=London (LON), source_ips=[u'2a00:1a48:7902:0001::/64', u'78.136.44.0/26']>,
     <CloudMonitorZone country_code=US, id=mzord, label=Chicago (ORD), source_ips=[u'2001:4801:7902:0001::/64', u'50.57.61.0/26']>,
     <CloudMonitorZone country_code=AU, id=mzsyd, label=Sydney (SYD), source_ips=[u'119.9.5.0/26', u'2401:1801:7902:1::/64']>]

The most important piece of information in a `CloudMonitorZone` is the `id`, which you pass in the `monitoring_zones_poll` argument of `create_check`.

## Create the Check
To create the check, run the following:

    chk = cm.create_check(ent, label="sample_check", check_type="remote.http",
            details={"url": "http://example.com/some_page"}, period=900,
            timeout=20, monitoring_zones_poll=["mzdfw", "mzlon", "mzsyd"],
            target_hostname="http://example.com")

This will create an HTTP check on the entity `ent` for the page `http://example.com/some_page` that will run every 15 minutes from the Dallas, London, and Sydney monitoring zones.

There are several parameters for `create_check()`:

Parameter | Required? | Default | Description
------ | ------ | ------ | ------ 
**label** | no | -blank- | An optional label for this check
**name** | no | -blank- | Synonym for 'label'
**check_type** | yes | | The type of check to create. Can be either a `CloudMonitorCheckType` instance, or its ID.
**details** | no | None | A dictionary for the parameters needed for this type of check.
**disabled** | no | False | Passing `disabled=True` creates the check, but it will not be run until the check is enabled.
**metadata** | no | None | Arbitrary key/value pairs you can associate with this check
**monitoring_zones_poll** | yes | | Either a list or a single monitoring zone. Can be either `CloudMonitoringZone` instances, or their IDs.
**period** | no | (account setting) | How often to run the check, in seconds. Can range between 30 and 1800.
**timeout** | no | None | How long to wait before failing the check. Must be less than the period.
**target_hostname** | Mutually exclusive with `target_alias` | None | Either the IP address or the fully qualified domain name of the target of the check. 
**target_alias** | Mutually exclusive with `target_hostname` | None | A key in the 'ip_addresses' dictionary of the entity for this check.

Note that you must supply either a `target_hostname` or a `target_alias`, but not both.

## Create a Notification

There are three supported notification types; `email`, `webhook`, and `pagerduty`, by which you can be notified of alarms. To see the details for each type, call:

    `cm.list_notification_types()`

This returns a list of `CloudMonitorNotificationType` objects:

    [<CloudMonitorNotificationType fields=[{u'optional': False, u'name': u'url', u'description': u'An HTTP or HTTPS URL to POST to'}], id=webhook>,
     <CloudMonitorNotificationType fields=[{u'optional': False, u'name': u'address', u'description': u'Email address to send notifications to'}], id=email>,
     <CloudMonitorNotificationType fields=[{u'optional': False, u'name': u'service_key', u'description': u'The PagerDuty service key to use.'}], id=pagerduty>]

The `id` value is then passed in as the `notification_type` parameter to `create_notification()`.


## Create the Notification

To create the notification, run the following:

    email = cm.create_notification("email", label="my_email_notification",
            details={"address": "me@example.com"})

This will create an email notification that will notify the caller at *me@example.com*.

The `create_notification()` method contains several parameters:

Parameter | Required? | Default | Description
------ | ------ | ------ | ------ 
**notification_type** | yes | | A `CloudMonitoringNotificationType`, or a string matching a supported type's `id` attribute
**label** | no | None | Friendly name for the notification
**details** | no | None | A dictionary of details for your `notification_type`

## Create the Notification Plan

Notification Plans outline the specific notifications to contact under three conditions: **OK**, **Warning**, and **Critical** states. The `create_notification_plan()` method contains several parameters:

Parameter | Required? | Default | Description
------ | ------ | ------ | ------
**label** | no | | Text to identify this plan
**name** | no | | Text to name this plan
**critical_state** | no | None | A `CloudMonitorNotification` object to be notified when an alarm reaches the critical state
**ok_state** | no | None | A `CloudMonitorNotification` object to be notified when an alarm reaches the ok state
**warning_state** | no | None | A `CloudMonitorNotification` object to be notified when an alarm reaches the warning state

Using one or more notifications you've created, call:

    plan = cm.create_notification_plan(label="default", ok_state=ok_hook, warning_state=warning_email, critical_state=critical_email)

Once created, a Notification Plan is used to configure an Alarm.

## Create an Alarm

Alarms build on the previously covered topics, and introduce a mini-language used to specify the situation in which a notification plan should be executed. The alarm language is fully explained in [Appendix A](http://docs.rackspace.com/cm/api/v1.0/cm-devguide/content/alerts-language.html) of the Cloud Monitoring documentation.

### Metrics

The mini-language operates on *metrics*, which are values stored relating to a *check* you have created. To see the list of supported metrics for a given check, call:

    check.list_metrics()

If your check has not yet collected any metrics, which is dependent on the *period* you have configured for your check along with when you're making the request, it could be an empty list. If metrics are available, they'll be returned as a list of strings in the format **monitoringZone.metricName**. For a **remote.ping** check, you may receive metrics like the following:

    [u'mzdfw.available', u'mzdfw.average']

Those metrics can then be used to construct the criteria string in ``cm.create_alarm``.

### Create the Alarm

Once you have created an entity, a check, and a notification plan, you can create an alarm using the mini-language. For example:

    alarm = cm.create_alarm(entity, check, np, "if (rate(metric['average']) > 10) { return new AlarmStatus(WARNING); } return new AlarmStatus(OK);")

This alarm will alert the *warning* notification in the notification plan when your check's **average** metric measures over 10. Otherwise, the the alarm will report that your check turned out *OK*.


## List Alarm Changelogs

The monitoring service records changelogs for alarm statuses. By default the last 7 days of changelog information is returned. If you have many devices that are being monitored, this can be a lot of information, so you can optionally specify an entity, and the results are limited to changelogs for that entity's alarms.

To get the changelogs, make the following call:

    chglogs = cm.get_changelogs([entity=my_entity])

If you specify an entity, it can either be an Entity object returned from a previous operation, or the ID of that entity. The result looks like the following, with one element in the `values` list for each changelog returned:

    {
        "values": [
            {
                "id": "4c5e28f0-0b3f-11e1-860d-c55c4705a286",
                "timestamp": 1320890228991,
                "entity_id": "enPhid7noo",
                "alarm_id": "alahf9vuNa",
                "check_id": "chIe7vohba",
                "state": "WARNING",
                "analyzed_by_monitoring_zone_id": "DFW"
            }
        ],
        "metadata": {
            "count": 1,
            "limit": 50,
            "marker": null,
            "next_marker": null,
            "next_href": null
        }
    }

## Get Overview

Views contain a combination of data that usually includes multiple, different objects. The primary purpose of a view is to save API calls and make data retrieval more efficient. The data is returned in a dictionary with a `values` key that holds a list for each entity in your account and each entity's child check and alarm objects. Along with the child check and alarm objects it also includes the latest computed state for each check and alarm pair. If there is no latest state available for a check and alarm pair, it means the alarm hasn't been evaluated yet and the current state for this pair is 'UNKNOWN'.

Please note that this is a convenience method, and returns raw data, and not the `Entity`, `Alarm`, and `Check` objects like the other methods in this module.

The call is:

    view = cm.get_overview([entity=my_entity])

You can optionally list this to return the information for a single entity. If you specify an entity, it can either be an Entity object returned from a previous operation, or the ID of that entity. The result looks like the following, with one element in the `values` list for each entity returned:

    {'metadata':
        {'count': 3,
          'limit': 50,
          'marker': None,
          'next_href': None,
          'next_marker': None},
     'values': [
         {'alarms': [{'check_id': 'chFour',
         'criteria': 'if (metric["size"] >= 200) { return new AlarmStatus(CRITICAL); }',
         'id': 'alThree',
         'notification_plan_id': 'npOne'}],
       'checks': [{'details': {'method': 'GET', 'url': 'http://www.foo.com'},
         'disabled': False,
         'id': 'chFour',
         'label': 'ch a',
         'monitoring_zones_poll': ['mzA'],
         'period': 150,
         'target_alias': 'default',
         'target_hostname': '',
         'target_resolver': '',
         'timeout': 60,
         'type': 'remote.http'}],
       'entity': {'id': 'enBBBBIPV4',
        'ip_addresses': {'default': '127.0.0.1'},
        'label': 'entity b v4',
        'metadata': None},
       'latest_alarm_states': [{'alarm_id': 'alThree',
         'analyzed_by_monitoring_zone_id': None,
         'check_id': 'chFour',
         'entity_id': 'enBBBBIPV4',
         'previous_state': 'WARNING',
         'state': 'OK',
         'status': 'everything is ok',
         'timestamp': 1321898988}]},
      {'alarms': [],
       'checks': [],
       'entity': {'id': 'enCCCCIPV4',
        'ip_addresses': {'default': '127.0.0.1'},
        'label': 'entity c v4',
        'metadata': None},
       'latest_alarm_states': []},
      {'alarms': [{'check_id': 'chOne',
         'criteria': 'if (metric["duration"] >= 2) { return new AlarmStatus(OK); } return new AlarmStatus(CRITICAL);',
         'id': 'alOne',
         'label': 'Alarm 1',
         'notification_plan_id': 'npOne'},
        {'check_id': 'chOne',
         'criteria': 'if (metric["size"] >= 200) { return CRITICAL } return OK',
         'id': 'alTwo',
         'label': 'Alarm 2',
         'notification_plan_id': 'npOne'}],
       'checks': [{'details': {'method': 'GET', 'url': 'http://www.foo.com'},
         'disabled': False,
         'id': 'chOne',
         'label': 'ch a',
         'monitoring_zones_poll': ['mzA'],
         'period': 150,
         'target_alias': 'default',
         'target_hostname': '',
         'target_resolver': '',
         'timeout': 60,
         'type': 'remote.http'},
        {'details': {'method': 'GET', 'url': 'http://www.foo.com'},
         'disabled': False,
         'id': 'chThree',
         'label': 'ch a',
         'monitoring_zones_poll': ['mzA'],
         'period': 150,
         'target_alias': 'default',
         'target_hostname': '',
         'target_resolver': '',
         'timeout': 60,
         'type': 'remote.http'},
        {'details': {'method': 'GET', 'url': 'http://www.foo.com'},
         'disabled': False,
         'id': 'chTwo',
         'label': 'ch a',
         'monitoring_zones_poll': ['mzA'],
         'period': 150,
         'target_alias': 'default',
         'target_hostname': '',
         'target_resolver': '',
         'timeout': 60,
         'type': 'remote.http'}],
       'entity': {'id': 'enAAAAIPV4',
        'ip_addresses': {'default': '127.0.0.1'},
        'label': 'entity a',
        'metadata': None},
       'latest_alarm_states': [{'alarm_id': 'alOne',
         'analyzed_by_monitoring_zone_id': None,
         'check_id': 'chOne',
         'entity_id': 'enAAAAIPV4',
         'previous_state': 'OK',
         'state': 'WARNING',
         'status': 'matched return statement on line 7',
         'timestamp': 1321898988},
        {'alarm_id': 'alOne',
         'analyzed_by_monitoring_zone_id': None,
         'check_id': 'chTwo',
         'entity_id': 'enAAAAIPV4',
         'state': 'CRITICAL',
         'timestamp': 1321898988}]}
        ]
    }

# Cloud Networks

## Basic Concepts
Rackspace Cloud Networks allows you to create virtual isolated networks and associate them with your Cloud Server instances. This allows you to create a typical bastion setup, with servers that are only accessible from the internet through a primary bastion server.

See the [sample code](https://github.com/rackspace/pyrax/tree/master/samples/cloud_networks) for an example of creating a typical bastion server setup.


## Default Networks
By default, servers are created with access to two pseudo-networks: `public` and `private`. The `public` network is the connection to the Internet, while the `private` network is the internal `ServiceNet`, which provides connectivity among devices within a data center. These pseudo-networks have special IDs which are available in `pyrax` as the following constants:

    pyrax.cloud_networks.PUBLIC_NET_ID
    pyrax.cloud_networks.SERVICE_NET_ID


## Using Cloud Networks in pyrax
Once you have authenticated, you can reference the cloud networks module via `pyrax.cloud_networks`. That is a lot to type over and over in your code, so it is easier if you include the following line at the beginning of your code:

    cnw = pyrax.cloud_networks

Then you can simply use the alias `cnw` to reference the module. All of the code samples in this document assume that `cnw` has been defined this way.

One thing to note: most of the other cloud products refer to the user-defined identification of its resources as their `name`, while Cloud Networks refers to this as a network's `label`. Wherever possible pyrax tries to alias the two, so you can use `label` and `name` interchangeably.


## Listing Existing Networks
To get a list of all your networks, call the `list()` method:

    cnw.list()

This returns a list of `CloudNetwork` objects. Assuming that you have not yet defined any isolated networks, this call returns the two pseudo-networks:

    [<CloudNetwork id=00000000-0000-0000-0000-000000000000, label=public>,
    <CloudNetwork id=11111111-1111-1111-1111-111111111111, label=private>]


## Create a Network
To create an isolated network, you must supply a name (label) for this network, as well as the network address range for that network using [**CIDR**](http://en.wikipedia.org/wiki/CIDR_notation) notation. For example, a network in the range of 192.168.0.0 – 192.168.0.255 is represented by the CIDR of `192.168.0.0/24`. 

The method call is:

    network = cnw.create("my_lan", cidr="192.168.0.0/24")
    print "New Cloud Network:", network

This prints:

    New Cloud Network: <CloudNetwork cidr=192.168.0.0/24, id=4c2e1ad5-af48-4039-a80a-15d1083f2a8b, label=my_lan>


## Delete a Network
To delete a network you've created, you use either of the two equivalent commands:

    cnw.delete(network_id_or_object)
    # or, if you have an object reference
    network.delete()

In the first form, you pass either a `CloudNetwork` object or the ID of the network to be deleted to the pyrax.cloud_networks client. However, if you already have a `CloudNetwork` object for the network to be deleted, you can simply call its `delete()` method directly.

Please note that you cannot delete a network that is attached to a server. As there is no way to "unattach" a network from a server, you must first delete the server if you want to delete the network. If you attempt to delete a network that is attached to one or more servers, a **`NetworkInUse`** exception is raised.


## Creating a Server with an Isolated Network
Isolated networks can only be attached to servers when the servers are being created; you cannot attach an isolated network to an existing server.

There is an optional `networks` parameter available in the `create()` command when creating a new server. If you do not specify this, the server is created with the public and private networks by default. If you do include the `networks` parameter, **you must specify all the networks for that server**, including the default networks. The Cloud Servers `create()` command expects the argument for the `networks` parameter to be in a particular format, so `pyrax` makes that easy for you by providing the `get_server_networks()` method on both the `CloudNetwork` and the `CloudNetworksClient` classes.

For the following examples, assume that you have run this block of code to create an isolated network:

    cnw = pyrax.cloud_networks
    cs = pyrax.cloudservers
    isolated = cnw.create("my_net", cidr="192.168.0.0/24")

To create a server that uses *only* this isolated network, call:

    networks = isolated.get_server_networks()
    cs.servers.create("test", img_id, flavor_id,
            nics=networks)

When the server has completed building, you can check its `networks` attribute to verify that this worked as expected. The code above creates a server whose `networks` attribute looks like:

    {u'my_net': [u'192.168.0.1']}

To create a server with the isolated network and the ServiceNet network, use this:

    networks = isolated.get_server_networks(private=True)
    cs.servers.create("test", img_id, flavor_id,
            nics=networks)

This results in a server whose `networks` attribute looks like:

    {u'my_net': [u'192.168.0.2'],
     u'private': [u'10.181.11.14']}

Finally, to create a server with the isolated network as well as the default networks:

    networks = isolated.get_server_networks(public=True, private=True)
    cs.servers.create("test", img_id, flavor_id,
            nics=networks)

This server's `networks` attribute shows all three networks:

    {u'my_net': [u'192.168.0.3'],
     u'private': [u'10.181.11.20'],
     u'public': [u'2001:4800:7810:0512:8ca7:b42c:ff04:93ee', u'64.49.237.239']}


## Limitations of Cloud Networks
Please note that there are several limitations regarding Cloud Networks:

* You can create a maximum of **10** isolated networks. Please create a Rackspace support ticket if you'll need more than 10 networks.
* You can attach an isolated network to a maximum of **60** servers.
* A server instance can have a maximum of **15** virtual interfaces (VIFs).
* You cannot attach an isolated network to an existing server.
    * Workaround: Create an image of the server and build a new server based on that image. When you build the new server, you can attach the isolated network. For information, see [Attach a Cloud Network to an Existing Cloud Server](http://www.rackspace.com/knowledge_center/article/attach-a-cloud-network-to-an-existing-cloud-server). To use the create image API, see [Create Image](http://docs.rackspace.com/servers/api/v2/cs-devguide/content/Create_Image-d1e4655.html).
* You cannot delete an isolated network unless the network is not associated with any server.
* You cannot detach an isolated network from a server.
    * Workaround: To detach an isolated network from a server, you must delete the server.
* You cannot rename an isolated network.
* You cannot renumber the CIDR for an isolated network.


# Working with Cloud Servers

----

*Note: pyrax works with OpenStack-based clouds. Rackspace's "First Generation" servers are based on a different API, and are not supported.*

## Listing Servers
Start by listing all the servers in your account:

    import pyrax
    pyrax.set_credential_file("/path/to/credential/file")
    cs = pyrax.cloudservers
    print cs.servers.list()

If you already have Cloud Servers, you get back a list of `Server` objects. But if you are just getting started with the Rackspace Cloud, and you got back an empty list, creating a new cloud server would be a good first step.

To do that, you'll need to specify the operating system image to use for your new server, as well as the flavor. `Flavor` is a class that represents the combination of RAM and disk size for the new server.

## Listing Images
To get a list of available images, run:

    print cs.images.list()

or the equivalent:

    print cs.list_images()

This returns a list of available images:

    [<Image: Windows Server 2012 (with updates) + SQL Server 2012 Web>, <Image: Windows Server 2012 (with updates) + SQL Server 2012 Standard>, <Image: Windows Server 2012 + SQL Server 2012 Web>, <Image: Windows Server 2012 + SQL Server 2012 Standard>, <Image: Windows Server 2012 (with updates)>, <Image: CentOS 5.8>, <Image: Arch 2012.08>, <Image: Gentoo 12.3>, <Image: Windows Server 2008 R2 SP1 + SharePoint Foundation 2010 SP1 & SQL Server 2008 R2 SP1 Std>, <Image: Windows Server 2008 R2 SP1 + SharePoint Foundation 2010 SP1 & SQL Server 2008 R2 SP1 Express>, <Image: Windows Server 2012>, <Image: Ubuntu 10.04 LTS (Lucid Lynx)>, <Image: Windows Server 2008 R2 SP1 + SQL Server 2008 R2 Standard>, <Image: Windows Server 2008 R2 SP1 (with updates) + SQL Server 2008 R2 SP1 Web>, <Image: Windows Server 2008 R2 SP1 (with updates) + SQL Server 2008 R2 SP1 Standard>, <Image: Windows Server 2008 R2 SP1 (with updates) + SQL Server 2012 Web>, <Image: Windows Server 2008 R2 SP1 + SQL Server 2008 R2 Web>, <Image: Windows Server 2008 R2 SP1 + SQL Server 2012 Web>, <Image: Windows Server 2008 R2 SP1 (with updates) + SQL Server 2012 Standard>, <Image: Windows Server 2008 R2 SP1 + SQL Server 2012 Standard>, <Image: Windows Server 2008 R2 SP1 (with updates)>, <Image: Windows Server 2008 R2 SP1>, <Image: CentOS 6.3>, <Image: FreeBSD 9>, <Image: Red Hat Enterprise Linux 6.1>, <Image: Ubuntu 11.10 (Oneiric Oncelot)>, <Image: Ubuntu 12.04 LTS (Precise Pangolin)>, <Image: Fedora 17 (Beefy Miracle)>, <Image: CentOS 6.2>, <Image: CentOS 6.0>, <Image: CentOS 5.6>, <Image: Ubuntu 11.04 (Natty Narwhal)>, <Image: Red Hat Enterprise Linux 5.5>, <Image: openSUSE 12.1>, <Image: Fedora 16 (Verne)>, <Image: Debian 6 (Squeeze)>]

Note that this is a list of `Image` *objects*, not just a bunch of name strings. You can get the image name and ID as follows:

    imgs = cs.images.list()
    for img in imgs:
        print img.name, "  -- ID:", img.id

This is the output:

    FreeBSD 10.0   -- ID: e5d7ca78-a487-4d7e-9dd1-73165df3f9fd
    Arch 2014.4 (PVHVM)   -- ID: 8a605fb2-57c5-43eb-9bd3-4f990802e478
    Ubuntu 14.04 LTS (Trusty Tahr)   -- ID: 5cc098a5-7286-4b96-b3a2-49f4c4f82537
    Ubuntu 14.04 LTS (Trusty Tahr) (PVHVM)   -- ID: bb02b1a3-bc77-4d17-ab5b-421d89850fca
    Windows Server 2012 + SharePoint 2013 with SQL Server 2012 SP1 Standard   -- ID: 51ddbe81-9e2b-492f-a031-a3577bf1016a
    Windows Server 2008 R2 SP1 + SQL Server 2012 SP1 Web   -- ID: 0d58b7da-0664-4a71-b89f-68115771a948
    Windows Server 2012 + SQL Server 2012 SP1 Standard   -- ID: ecd8f832-299c-4711-9763-06f81cf3058a
    Windows Server 2008 R2 SP1 + SharePoint 2010 Foundation with SQL Server 2008 R2 SP1 Standard   -- ID: fe2162fe-8d3c-4c1d-8b83-71ff09321fb2
    Windows Server 2008 R2 SP1 + SharePoint 2010 Foundation with SQL Server 2008 R2 Express   -- ID: 9f8329b4-1985-4be0-b263-31e728a046fc
    Windows Server 2008 R2 SP1 + SQL Server 2012 SP1 Standard   -- ID: cf44eaf2-844a-4bc5-af91-e7ceb021cac9
    Windows Server 2008 R2 SP1 + SQL Server 2008 R2 SP2 Web   -- ID: d1519521-bb03-42ce-b25c-34b72f10cc26
    Windows Server 2008 R2 SP1 + SQL Server 2008 R2 SP2 Standard   -- ID: 427f2199-c441-4bff-95e0-4cba19b8839d
    Windows Server 2012 + SQL Server 2012 SP1 Web   -- ID: 5906d9c5-7a3a-437f-872a-51b0db539f9b
    Windows Server 2012   -- ID: c50f70fc-79ec-4c71-bd4a-a4ac8f1641e1
    Windows Server 2008 R2 SP1   -- ID: 8f39aeb0-79d6-45eb-9505-fb0bf31556d7
    Gentoo 14.2 (PVHVM)   -- ID: 6c031e2c-0d2c-4fa0-aadb-70265957a37d
    Ubuntu 13.10 (Saucy Salamander)   -- ID: 7b8abc3f-5fd2-4d02-9e9a-16d43fc7128e
    OpenSUSE 13.1 (PVHVM)   -- ID: 6ae8a615-57f1-4a19-83d4-e09f8ae25327
    Scientific Linux 6.5 (PVHVM)   -- ID: dbb59b04-bd50-4eef-b05e-6a6451ef9bf2
    Ubuntu 12.04 LTS (Precise Pangolin)   -- ID: ffa476b1-9b14-46bd-99a8-862d1d94eb7a
    Fedora 20 (Heisenbug) (PVHVM)   -- ID: c21acd84-a6c5-44df-ba7d-2483cd5b3ccb
    CentOS 6.5   -- ID: 042395fc-728c-4763-86f9-9b0cacb00701
    Fedora 19 (Schrodinger's Cat) (PVHVM)   -- ID: 4b81c45a-90d6-4654-972c-73972b1d0aea
    Red Hat Enterprise Linux 6.5 (PVHVM)   -- ID: 271e1090-77ad-4400-a1f7-73a922aca8e2
    Red Hat Enterprise Linux 6.5   -- ID: d1b6eba3-aece-407c-8ef3-4488b452336b
    CentOS 6.5 (PVHVM)   -- ID: 592c879e-f37d-43e6-8b54-8c2d97cf04d4
    Debian 7 (Wheezy) (PVHVM)   -- ID: 77e32de8-3304-44f3-b230-436d95fceb19
    Ubuntu 13.10 (Saucy Salamander) (PVHVM)   -- ID: aca656d3-dd70-4d7e-a9e5-f12182871cde
    Ubuntu 12.04 LTS (Precise Pangolin) (PVHVM)   -- ID: a4286a42-137c-46ce-a796-dbd2b12a078c
    Windows Server 2012 (base install without updates)   -- ID: c30b4407-da75-4291-a1fb-edd8e6e037de
    Windows Server 2008 R2 SP1 (base install without updates)   -- ID: 2d4ee06e-d567-47f8-b745-8d281fe5729a
    CentOS 5.10   -- ID: 9522c27d-51d9-44ee-8eb3-fb7b14fd4042
    Red Hat Enterprise Linux 5.10   -- ID: 56ad2db2-d9cd-462e-a2a4-7f3a4fc91ee8
    Debian 6.06 (Squeeze)   -- ID: 695ca76e-fc0d-4e36-82e0-8ed66480a999
    Ubuntu 10.04 LTS (Lucid Lynx)   -- ID: aab63bcf-89aa-440f-b0c7-c7a1c611914b
    Vyatta Network OS 6.5R2   -- ID: 59b394f6-b2e0-4f11-b7d1-7fea4abc60a0

### Different Image Types
There are two types of images: the base images supplied by your cloud provider, and the images created from your cloud servers (commonly referred to as _snapshots_). You can get a list containing just a single type using one of the following calls:

    cs.list_base_images()
    cs.list_snapshots()


## Listing Flavors
Let's do the same for flavors:

    flvs = cs.list_flavors()
    for flv in flvs:
        print "Name:", flv.name
        print "  ID:", flv.id
        print "  RAM:", flv.ram
        print "  Disk:", flv.disk
        print "  VCPUs:", flv.vcpus

This returns:

    Name: 1 GB Performance
      ID: performance1-1
      RAM: 1024
      Disk: 20
      VCPUs: 1
    Name: 2 GB Performance
      ID: performance1-2
      RAM: 2048
      Disk: 40
      VCPUs: 2
    Name: 4 GB Performance
      ID: performance1-4
      RAM: 4096
      Disk: 40
      VCPUs: 4
    Name: 8 GB Performance
      ID: performance1-8
      RAM: 8192
      Disk: 40
      VCPUs: 8
    Name: 120 GB Performance
      ID: performance2-120
      RAM: 122880
      Disk: 40
      VCPUs: 32
    Name: 15 GB Performance
      ID: performance2-15
      RAM: 15360
      Disk: 40
      VCPUs: 4
    Name: 30 GB Performance
      ID: performance2-30
      RAM: 30720
      Disk: 40
      VCPUs: 8
    Name: 60 GB Performance
      ID: performance2-60
      RAM: 61440
      Disk: 40
      VCPUs: 16
    Name: 90 GB Performance
      ID: performance2-90
      RAM: 92160
      Disk: 40
      VCPUs: 24

So you now have the available images and flavors. Suppose you want to create a **1GB Performance-1 Ubuntu 14.04** server: to do this, you can use the `find()` method and the exact name of the image. This is difficult, since the exact name is 'Ubuntu 14.04 LTS (Trusty Tahr)', which you probably would not have guessed. So the easiest way to do this is to check in a less restrictive manner:

    ubu_image = [img for img in cs.images.list()
            if "Ubuntu 14.04" in img.name
            and "PVHVM" in img.name][0]

You can do something similar to get the 1GB Performance-1 flavor:

    flavor_1GB = [flavor for flavor in cs.flavors.list()
            if flavor.ram == 1024][0]

Note that these calls are somewhat inefficient, so if you are going to be working with images and flavors a lot, it is best to make the listing call once and store the results locally. Images and flavors typically do not change very often.

## SSH Key Authentication
The default method for authenticating to a server is by supplying a username/password combination. However, when using SSH to connect to a Linux server, it is generally preferable to authenticate using an SSH key.

You can store your public key on the API server, giving it a name that can be used to identify it when creating a server. Here is an example of storing your key:

    with open(os.path.expanduser("~/.ssh/id_rsa.pub")) as keyfile:
        cs.keypairs.create("my_key", keyfile.read())

The first line above assumes that your public key is named "**id_rsa.pub**", and is located in the "**.ssh**" folder inside your home directory. This is a typical case; if your file location differs, change the path accordingly. After the above code executes, your key is saved in your account, and you can reference by the name you gave it; in this case, "**my_key**".

## Creating a Server
Now that you have the image and flavor objects you want (actually, it's their `id` attributes you really need), you are ready to create your new cloud server! To do this, call the `create()` method, passing in the name you want to give to the new server, along with the IDs for the desired image and flavor.

    server = cs.servers.create("first_server", ubu_image.id, flavor_1GB.id)

If you wanted to create the server with your SSH key as described in the preceeding section, add it using the `key_name` parameter:

    server = cs.servers.create("first_server", ubu_image.id, flavor_1GB.id,
            key_name="my_key")


This returns a new `Server` object. You can test it out using the following code:

    print "ID:", server.id
    print "Status:", server.status
    print "Admin password:", server.adminPass
    print "Networks:", server.networks

Running this code returns:

    ID: u'afbabf55-a9b5-4e77-82bd-d57ab69f2992'
    Status: u'BUILD'
    Admin password: u'73hgYoBnwoXu'
    Networks: {}

Wait - the server has no network addresses? How useful is that? How are you supposed to work with a cloud server that is not on a network?

If you ran that code, you noticed that it returned almost immediately. What happened is that the cloud API recorded your request, and returned as much information as it could about the server that it was *going to create*. The networking for the server had not yet been created, so it could not provide that information.

One important piece of information is the __adminPass__ – without that, you are not be able to log into your server unless you are using SSH key authentication. It is *only* supplied with the `Server` object returned from the initial `create()` request. After that, future calls to `get()` the server do not return that value. Note: if this does happen, you can call the `change_password()` method of the `Server` object to set a new root password on the server.

This brings up an important point: the `Server` objects you get back are essentially snapshots of that server at the moment you requested the information. Your object's 'BUILD' status won't change no matter how long you wait. You have to refresh it to see any changes. In other words, these objects are not dynamic. Fortunately, refreshing the object is simple enough to do:

    server = cs.servers.get(server.id)

The `get(id)` method takes the ID of the desired server and returns a `Server` object with the current information about that server. Try getting the network addresses again with the newly-fetched object:

    print server.networks

This should return something like:

    {u'private': [u'10.179.xxx.xxx'], u'public': [u'198.101.xxx.xxx', u'2001:4800:780d:0509:8ca7:b42c:xxxx:xxxx']}

### Waiting for Server Completion
Since you can't do anything with your new server until it finishes building, it would be helpful to have a way of determining when the build is complete. So `pyrax` includes the `wait_until()` method in its `utils` module. Here is a typical usage:

    srv = cs.servers.create(…)
    new_srv = pyrax.utils.wait_until(srv, "status", ["ACTIVE", "ERROR"])

When you run the above code, execution blocks until the server's status reaches one of the two values in the list. Note that we just don't want to check for "ACTIVE" status, since server creation can fail, and the `wait_until()` call waits forever.

Another common use case is when you are creating several servers, and you don't want to block your app's execution while each server builds. For this case, you can pass a callback function to `wait_until()`, which creates a separate thread for the wait process, and that callback function is called when `wait_until()` completes.

#### Parameters for wait_until():

| Name | Required? | Description |
| ---- | ---- | ---- |
| obj | Yes | The object to examine. |
| att | Yes | The name of the attribute of the object to examine. |
| desired | Yes | The desired value(s) of the attribute that the method waits for. |
| callback | No | An optional function that is called when the `wait_until()` process completes. Providing the callback makes wait_until() non-blocking. The callback function should accept a single parameter: the updated version of the object. Default = `None` |
| interval | No | How long (in seconds) to wait between polling the API for changes in the target object's attribute. Default = `5` seconds.|
| attempts | No | How many times should wait_until() check the object before giving up? Passing `attempts=0` causes `wait_until()` to loop until the desired attribute value is reached. Default = `0`, meaning that `wait_until()` loops indefinitely until one of the attribute in `att` reaches one of the `desired` values. |
| verbose | No | When True, each attempt prints out the current value of the watched attribute and the time that has elapsed since the original request. Note that if a callback function is specified, the value of `verbose` is ignored; all print output is suppressed. Default = `False` |
| verbose_atts | No | A list of additional attributes whose values are printed out for each attempt. If `verbose=False`, this parameter has no effect. Default = `None` |

### Even Easier: `wait_for_build()`
Since waiting for servers (as well as databases and load balancers) to build is such a common use case, pyrax provides a convenience method that provides the most common default values for `wait_until()`. So assuming that you have a reference `srv` to a newly-created server, you can call:

    wait_for_build(srv)

and this would be equivalent to the call:

    wait_until(srv, "status", ["ACTIVE", "ERROR"], interval=20, callback=None,
            attempts=0, verbose=False, verbose_atts="progress")

You can override any of these defaults in the call to `wait_for_build()`. For example, if you want verbose output, you would call:

    wait_for_build(srv, verbose=True)


### Additional Parameters to Create()
There are several optional parameters that you can include when creating a server. Here are the most common:

`meta` - An arbitrary dict of up to 5 key/value pairs that can be stored with the server. Note that the keys and values must be simple strings, and not numbers, datetimes, tuples, or anything else.

`key_name` – As mentioned above, this is the name you gave to an SSH key that you previously uploaded using `cs.keypairs.create()`. The key is installed in the newly-created server's `/root/.ssh/authorized_keys` file, allowing for key-based authenticating when SSHing into the server.

`files` - A dict of up to 5 files that are written to the server upon creation. The keys for this dict are the absolute file paths on the server where these files are written, and the values are the contents for those files. Values can either be a string, or a file-like object that is read. Total combined size of all files must not exceed 2KB, and binary files are not supported (text only).

Now create a server using the `meta` and `files` options. The setup code is the same as in the previous examples; here is the part that you need to change:

    meta = {"test_key": "test_value",
            "meaning_of_life": "42"}
    content = """This is the contents of the text file.
    It has several lines of text.

    And it even has a blank line."""

    files = {"/root/testfile": content}
    server = cs.servers.create("meta_server", ubuid, flavor_1GB.id,
            meta=meta, files=files)

Run that code, and then wait for the server to finish building. When it does, use the admin password to ssh into the server, and then do a directory listing of the home root directory (`ls /root`). You should see a file named `testfile`; run `cat testfile` to verify that the content of the file matches what you had in the script.

Disconnect from the server, and then use the server's ID to get a `Server` object. Check its `metadata` attribute to verify that it contains the same key/value pairs you specified.

## Deleting a Server
Finally, when you are done with a server, you can easily delete it:

    server.delete()

Note that the server isn't deleted immediately (though it usually does happen pretty quickly). If you try refreshing your server object just after calling `delete()`, the call may succeed. But trying again a few seconds later results in a `NotFound` exception being raised.

## Deleting All Servers
Since each `Server` object has a `delete()` method, it is simple to delete all the servers that you've created:

    for server in cs.servers.list():
        server.delete()


## Creating an Image of a Server
If you have a `Server` object and want to create an image of that server, you can call its `create_image()` method, passing in the name of the image to create, along with any optional metadata for the image.

    cs = pyrax.cloudservers
    server = cs.servers.get(id_of_server)
    server.create_image("my_image_name")

Another option is to use call `pyrax.servers.create_image()`, passing in either the name or the ID of the server from which you want to create the image, along with the image name and optional metadata.

    cs = pyrax.cloudservers
    cs.servers.create_image("my_awesome_server", "my_image_name")

The created image also appears in the list of images with the name you gave it.

    cs.images.list()

You need to wait for the imaging to finish before you are able to clone it.

    image_id = server.create_image("base_image")
    # Unlike the create_server() call, create_image() returns the id
    # rather than an Image object.
    image = cs.images.get(im)
    image = pyrax.wait_until(image, "status", ["ACTIVE", "ERROR"], attempts=0)
    cs.servers.create(name="clone", image=image.id, flavor=my_flavor)


## Resizing a Server
Resizing a server is the process of changing the amount of resources allocated to the server. In Cloud Servers terms, it means changing the Flavor of the server: that is, changing the RAM and disk space allocated to that server.

**NOTE**: Resizing is only available for servers built with older technologies and flavors. To resize any of the current Performance servers, you need to take a snapshot image of the server, and then use that image to create a new server of the desired size. Once you are certain that the new server is working correctly, the old server can be deleted. If you're familiar with the old resizing method, one difference is that the new server has new IP addresses, whereas in the previous process the IP addresses were transferred to the new server.

### Resizing Old Flavors
Resizing is a multi-step process. First, determine the desired `Flavor` to which the server is to be resized. Then call the `resize()` method on the server, passing in the ID of the desired `Flavor`. The server's status is then set to "RESIZE".

    cs = pyrax.cloudservers
    server = cs.servers.get(id_of_server)
    server.resize(new_flavor_ID)

On the host, a new server instance with the new flavor size is created based on your existing server. When it is ready, the ID, name, networking, and so forth for the current server instance is transferred to the new instance. At that point, `get(ID)` returns the new instance, and it has a status of "VERIFY_RESIZE". Now you need to determine if the resize was successful, and that the server is functioning properly. If all is well, call:

    server.confirm_resize()

and the old instance is then deleted, and the new server's status is set to "ACTIVE". However, if there are any problems with the new server, call:

    server.revert_resize()

to restore the original server, and delete the resized version.

## Resetting a Server's State
Occasionally a server gets "stuck" in a particular state when a process fails or otherwise gets interrupted. For example, if you called `server.create_image("image_name")`, but something went wrong during the process, you can delete the bad image, but the server's state is still stuck in `task_state image_snapshot`, and you cannot perform actions with that server, such as creating another image. If you ever find one of your servers in this state, you can use the following command to reset its state back to "ACTIVE":

    cs.servers.reset_state("ACTIVE")

# Context Objects

Context objects represent a single authenticated session. You can work with as many context objects as you like at the same time without worrying about logging in or out of each one to work with the other. Earlier versions of pyrax (before 1.8.0) only allowed you to work with one authenticated session at a time.

This ability is useful if you ever need to work with multiple projects (accounts) at the same time, or with multiple users within the same project.

## Creating a Context Object

To create a context object, you call:

    ctx = pyrax.create_context({id_type})

where `id_type` is either "keystone" or "rackspace". If you've defined the identity type you wish to use in either your `~/.pyrax.cfg` file, or in environment variables, you can omit the id_type from the call:

    ctx = pyrax.create_context()

If you have multiple **[environments](https://github.com/rackspace/pyrax/blob/master/docs/getting_started.md#configuration-environments)** defined, you may specify the environment to use when creating the context by specifying that name in the `env` parameter:

    ctx = pyrax.create_context(env="{my_special_environment}")

If you don't specify an environment, the current environment is used. You may also populate the context object with login credential information, as well as the `verify_ssl` setting. The full call with all available parameters is:

    ctx = pyrax.create_context(id_type=None, env=None, username=None,
            password=None, tenant_id=None, tenant_name=None, api_key=None,
            verify_ssl=None)

At this point you have an unauthenticated context. You must authenticate before you can do anything useful with a context object. To authenticate you need to provide your credentials if you haven't already when you created the object. You can either set them individually:

    ctx.username = {your_user_name}
    ctx.password = {your_password}

...or you can set them together:

    ctx.set_credentials({your_user_name}, {your_password})

...or if you have them stored in a file (see the information on **[credential files](https://github.com/rackspace/pyrax/blob/master/docs/getting_started.md#authenticating)** for how to set one up), you can call:

    ctx.set_credential_file({path/to/your/file})

After you have set your credentials, you call:

    ctx.authenticate()

This submits your credentials, and assuming that they are valid, get back a Service Catalog that contains all of the services and regions that the authenticated user is authorized to use.

If you have your password stored in your system's keychain, you can set your credentials and authenticate with a single call:

    ctx.keyring_auth({username})


## Working With Context Objects

Once you have an authenticated context object, you can work with the various services (e.g., object storage, compute, databases) for each region that your provider offers. Examples of regions for Rackspace are **"DFW"**, **"LON"**, **"SYD"**, while HP offers regions such as **"region-a.geo-1"** and **"region-b.geo-1"**.

Many OpenStack installations only have a single region. In those cases it might seem a little annoying to have to include that.

### Service Names

The various services are known by several types of names: descriptive names, project code names, and vendor product names. For example, the **compute** service is known within OpenStack as project **nova**, at Rackspace as the **Cloud Servers** product, and at HP as the **Cloud Compute** product. The descriptive name is the one that is used in these docs, but it is common for developers to use the other variations. The older versions of pyrax (before 1.8.0) primarily used the Rackspace-branded name for the service.

Pyrax accepts any of these different names as aliases for the services. Here is a partial list of these aliases for many of the current services:

Code Name | Service Name
---- | ----
nova | compute                                                                     
cloudservers | compute                                                             
swift | object_store                                                               
cloudfiles | object_store                                                          
cloud_loadbalancers | load_balancer                                                
trove | database                                                                   
cloud_databases | database                                                         
cinder | volume                                                                    
cloud_blockstorage | volume                                                        
designate | dns                                                                    
cloud_dns | dns                                                                    
neutron | network                                                                  
cloud_networks | network                                                           
glance | image                                                                     
images | image                                                                     
marconi | queues                                                                   
queues | queues                                                                    
cloud_monitoring | monitor                                                         
autoscale | autoscale

### Getting a Client

The simplest way to get a client is to call:

    client = ctx.get_client({service}, {region})

For example, if you want a client to interact with your servers in the ORD region of Rackspace, you call:

    srvr_ord = ctx.get_client("compute", "ORD")
    servers = srvr_ord.list()

The following two statements behave identically, as pyrax maps the code names to the actual service names:

    srvr_ord = ctx.get_client("nova", "ORD")
    srvr_ord = ctx.get_client("cloudservers", "ORD")

Once you have the client, you use it as you would use clients in prior versions of pyrax. As an example, once you have `srvr_ord` defined above, to get a list of your servers in that region, you call:

    servers = srvr_ord.list()

#### Private (Internal) Clients

Some services offer internal URLs; these are typically used for communication between resources within the same datacenter. If you are working within a datacenter, these are typically more efficient and reduce or eliminate bandwidth charges with many commercial providers. A typical use case is storing objects in swift from a compute resource in the same datacenter.

To get a client that works with the internal URL, add the parameter `public=False` to the `get_client()` call:

    storage_ord = ctx.get_client("object_store", "ORD", public=False)

If there is no internal URL defined for the service, a **`NoEndpointForService`** exception is raised.

## Shortcuts

Context objects provide convenient ways to access the clients in order to make working with pyrax simpler. You can use standard dot notation to define your clients:

    srvr_ord = ctx.compute.ORD.client
    # - or -
    srvr_ord = ctx.ORD.compute.client

The ordering of service.region and region.service isn't important; either one resolves to the client for that service/region combination.

Another use of this notation is to get an object with all the services in a given region:

    syd_svcs = ctx.SYD
    server_clt = syd_svcs.compute.client
    db_clt = syd_svcs.database.client

Note that the context object _must_ be authenticated before you attempt to access it using dot notation as described here. If the context has not been authenticated, a **`NotAuthenticated`** exception is raised. Also, regions containing periods in their name will not work as shortcuts, since those periods are interpreted by Python as object dot notation. If you're working with a region such as "**region-a.geo-1**", which is one of HP Cloud's regions, you will have to use standard dict syntax to access it:

    compute_client = ctx.compute["region-a.geo-1"].client

# pyrax – Python Bindings for OpenStack and the Rackspace Cloud

----


## Getting Started With pyrax
**pyrax** is the Python language binding for **OpenStack** and the **Rackspace Cloud**. By installing pyrax, you have the ability to build on any OpenStack cloud using standard Python objects and code. *Note: since pyrax works with the OpenStack API, it does not support Rackspace's "First Generation" Cloud Servers, which are based on a different technology.*


## Prerequisites
You need Python 2.7.x to run pyrax. It may work with earlier versions, but this has not been tested. There are plans to port it to run in both 2.x and 3.x, but that work has not yet been started. But no matter what version you run, if you encounter a problem with pyrax, please report it on [https://github.com/rackspace/pyrax/issues](https://github.com/rackspace/pyrax/issues).

The documentation assumes that you are experienced with programming in Python, and have a basic understanding of cloud computing concepts. If you would like to brush up on cloud computing, you should visit the [Rackspace Knowledge Center](http://www.rackspace.com/knowledge_center/).


## Installing pyrax
You install pyrax like any other third-party Python module. Just run:

    pip install pyrax

You need to do this as root/administrator (that is, using `sudo`), unless you are installing into a [virtualenv](http://www.virtualenv.org/en/latest/). `pip` pulls in all of the other modules and client libraries that pyrax needs.

You can also install directly from GitHub (where the pyrax source code is hosted). To do that, run:

    pip install git+git://github.com/rackspace/pyrax.git

The difference is that using the GitHub installation method installs the current trunk version, which has the latest changes, but may also be less stable.

To upgrade your installation in the future, re-run the same command, but this time add the `--upgrade` option to make sure that pyrax and any dependecies are updated to the newest available version.


## Set up Authentication
You need to submit your username and password in order to authenticate. If you are using the Rackspace Public Cloud, that would be your account username and API key. If you are using another OpenStack cloud, you also need to include your tenant ID, which you should be able to get from your provider.

Please note that **all versions of pyrax beginning with 1.4.0 require that you define what type of authentication system you are working with**. Previous versions only worked with Rackspace authentication, so this was not an issue. To do this, you have three options, listed below. In all cases the examples use `keystone` as the identity_type, but if you're using the Rackspace Cloud, change this to `rackspace`.

1. **Configuration File**: make sure that the line `identity_type = keystone` is in your [configuration file](#pyrax-configuration).
1. **Environment Variable** - If you don't have a configuration file, pyrax checks for the environment variable `CLOUD_ID_TYPE`. Set this by executing `export CLOUD_ID_TYPE=keystone` in a bash shell, or by setting it in the System section of the Control Panel in Windows.
1. **Set in Code** - if you can't do either of the above, change the import statement to add `pyrax.set_setting("identity_type", "keystone")` immediately after the `import pyrax` statement.

Note that if you are using the Rackspace Cloud, you would replace "keystone" in the above examples with "rackspace".


## Authenticating
You can authenticate in any one of three ways:

* explicitly pass your credentials to pyrax
* create a file containing those credentials and pass that file path to pyrax
* add them to your operating system's keychain

The credential file is a standard configuration file, with the following format:

    [keystone]
    username = my_username
    password = top_secret
    tenant_id = 01234567890abcdef

For the Rackspace Public Cloud, the credential file should look like this:

    [rackspace_cloud]
    username = my_username
    api_key = 01234567890abcdef

To use the keychain method, you need to add your password or API key to your operating system's keychain in the `pyrax` namespace. Doing a `pip install pyrax` installs the Python module [`keyring`](http://pypi.python.org/pypi/keyring), which provides ready access to this feature. To configure your keychain credentials, run the following in Python:

    import keyring
    keyring.set_password("pyrax", "my_username",
            "my_password")

To authenticate, run the following code using one of these authentication methods; which method you choose depends on your preference for passing credentials.

    import pyrax

    # Using direct method
    pyrax.set_credentials("my_username", "01234567890abcdef")

    # Using credentials file
    pyrax.set_credential_file("/path/to/credential/file")

    # Using keychain
    pyrax.keyring_auth("my_username")
    # Using keychain with username set in configuration file
    pyrax.keyring_auth()

Note that the `keyring_auth()` command allows you to specify a particular username. This is especially useful if you need to connect to multiple cloud accounts in a particular environment. If you only have a single account, you can specify the username for your account in the config file (explained below), and pyrax uses that by default.

If you are using pyrax in conjunction with other software, you may already have authenticated using that other program. In that case, you can call the `auth_with_token()`, supplying the token along with either your tenant name or tenant ID. For Rackspace authentication, both of these values are your account number.

    # Using an existing token
    pyrax.auth_with_token(my_token, tenant_name="0728829")

If you use this method to authenticate in pyrax, be aware that you are responsible for ensuring that the token is valid. That means that if the token expires while your application is running, you need to catch that error, re-authenticate, and then call `auth_with_token()` again with the new token.

Once you have authenticated, you now have access to Cloud Servers, Cloud Files, Cloud Block Storage, Cloud Databases, Cloud Load Balancers, Cloud DNS, and Cloud Networks using the following references:

    pyrax.cloudservers
    pyrax.cloudfiles
    pyrax.cloud_blockstorage
    pyrax.cloud_databases
    pyrax.cloud_loadbalancers
    pyrax.cloud_dns
    pyrax.cloud_networks

You don't have to authenticate to each service separately; pyrax handles that for you. And since it can be cumbersome to type those names throughout your code, these aliases are commonly set at the beginning of scripts:

    cs = pyrax.cloudservers
    cf = pyrax.cloudfiles
    cbs = pyrax.cloud_blockstorage
    cdb = pyrax.cloud_databases
    clb = pyrax.cloud_loadbalancers
    dns = pyrax.cloud_dns
    cnw = pyrax.cloud_networks

These abbreviated aliases are used throughout much of the documentation and sample code for pyrax.


## Auth Token Expiration
When you authenticate the cloud identity service returns a _token_, which is simply a random set of characters that identifies you as an authenticated user. This token is valid for a set period of time; the exact length of the valid period varies across cloud providers.

Every request needs to have the authentication information included in the headers of the API calls; pyrax handles this automatically for you. In addition, pyrax catches the error when a token is no longer valid, and attempts to re-authenticate in the background. As long as your original credentials are still valid, this happens transparently to your application. This enables you to write a long-running application without worrying about handling token expiration in your own code.

Please note that if you used `auth_with_token()` to authenticate originally, pyrax does not have your credentials, and so cannot automatically re-authenticate you. In this case, your code must handle the authentication failure when the token expires, and re-authenticate using whatever process you used to get your initial token. Once you've done that, you need to call `auth_with_token()` again to add the new token to pyrax.


## Pyrax Configuration
You can control how pyrax behaves through the configuration file. It should be named `~/.pyrax.cfg`. Like the credential file, `~/.pyrax.cfg` is a standard configuration file. Alternatively, you may set these values using environment variables in the OS. Note that the configuration file values take precendence over any environment variables. Environment variables also do not support multiple configurations.

**Do not include login credentials**, such as passwords, in the configuration file. As this is a defined file in a known location, it is a security risk to have your login information stored in clear text. You may use a [credential file](#authenticating) if you wish to store your credentials on disk; this can be named whatever you wish, and placed anywhere on disk, since you pass the full path to the file when authenticating this way. Please note that credentials stored in the configuration file will be ignored, and a warning will be issued if they are found.

**NOTE**: At the very least, you *must* set the `identity_type` setting so that can use the correct identity class. Prior versions only worked with Rackspace identity, but that is no longer the case. If you don't want to use a configuration file or an environment variable, you can do this in code:

    pyrax.set_setting("identity_type", "keystone")

or

    pyrax.set_setting("identity_type", "rackspace")


### Configuration Environments
Pyrax supports multiple configurations, which are referred to as ***environments***. An environment is a separate OpenStack deployment with which you want to interact. A common situation is when you have a private cloud for some of your work, but also have a public cloud account for the rest. Each of these clouds require different authentication endpoints, and may require different settings for other things such as region, identity type, etc.

Each environment is a separate section in the configuration file, and the section name is used as the name of the environment. You can name your environments whatever makes sense to you, but there are two special names: '**default**' and '**settings**'. If a section is named 'default', it is used by pyrax unless you explicitly set a different environment. Also, for backwards compatibility with versions of pyrax before 1.4, a section named 'settings' is interpreted as the default. Those versions only supported a single environment in the configuration file, and used 'settings' as the section name. **NOTE**: if you do not have a section named either 'default' or 'settings', then the first section listed is used as the default environment.

### Changing Environments
If you have multiple environments, you need to set the desired environment before you authenticate and connect to the services. If you want the `default` environment, you don't need to do anything. But if you want to connect to a different provider, you should run the following:

    import pyrax
    pyrax.set_environment("desired_environment")

Note that changing the environment requires that you authenticate against the new environment, and create new connections to the various services. In other words, if you had already authenticated so that a service such as `pyrax.cloudservers` referenced the compute service on that cloud, changing the environment to point to a different cloud discards the previous identity and service connections, so that now `pyrax.cloudservers` is `None`. Once you authenticate in the new environment, `pyrax.cloudservers` references the compute service on the cloud for the new environment.


### Available Configuration Settings
Setting | Affects | Default | Notes | Env. Variable
---- | ---- | ---- | ---- | ----
**identity_type** | The system used for authentication.  | -none- | This should be "rackspace" (for the Rackspace Public Cloud) or "keystone" (for all Keystone-based auth systems). Any other system needs a class defined to handle that auth system, and its script added to the pyrax/identity directory. The entry for such custom classes should be in the format of 'module_name.ClassName'. | CLOUD_ID_TYPE
**auth_endpoint** | The URI of the authentication service | -none- | Not required for the Rackspace Public Cloud, where it can be determined from the region. For everything else it is required. | CLOUD_AUTH_ENDPOINT
**keyring_username** | User name used when fetching password from keyring. | -none- | Without setting this, you need to supply the username every time you use keyring_auth(). | CLOUD_KEYRING_USER
**region** | Regional datacenter to connect to; for instance '**DFW**', '**ORD**', '**IAD**' for Rackspace ([full list](http://www.rackspace.com/about/datacenters/)); typically '**RegionOne**' in Keystone. | Depends on account settings | Required. | CLOUD_REGION
**tenant_id** | The tenant ID used for authentication. | -none- | Not used in the Rackspace Public Cloud. | CLOUD_TENANT_ID
**tenant_name** | The tenant name used for authentication. | -none- | Not used in the Rackspace Public Cloud. | CLOUD_TENANT_NAME
**encoding** | The encoding to use when working with non-ASCII values. Unless you have a specific need, the default should work fine. | utf-8 | | CLOUD_ENCODING
**custom_user_agent** | Customizes the User-agent string sent to the server. | -none- | | CLOUD_USER_AGENT
**debug** | When True, causes all HTTP requests and responses to be output to the console to aid in debugging. | False | Previous versions called this setting 'http_debug'. | CLOUD_DEBUG
**verify_ssl** | Set this to False to bypass SSL certificate verification. | True |  | CLOUD_VERIFY_SSL
**use_servicenet** | By default your connection to Cloud Files uses the public internet. If you're connecting from a cloud server in the same region, though, you have the option of using the internal **Service Net** network connection, which is not only faster, but does not incur bandwidth charges for transfers within the datacenter. | False |  | USE_SERVICENET

Here is a sample:

    [private]
    identity_type = keystone
    region = RegionOne
    custom_user_agent =
    debug = True
    auth_endpoint = http://192.168.0.1:5000/v2.0/
    tenant_name = demo
    tenant_id = abc123456
    keyring_username = demo

    [public]
    identity_type = rackspace
    keyring_username = joeracker
    region = ORD
    custom_user_agent = CrazyApp/2.0
    debug = False
    verify_ssl = False

The above configuration file defines two environments: **private** and **public**. Since there is no 'default' or 'settings' section, the 'private' environment is the default, since it is listed first.

When using the 'private' environment, pyrax uses Keystone authentication with the tenant name of 'demo', the tenant ID of 'abc123456', and the password stored in the keyring for user 'demo'. It also emits debugging messages for all HTTP requests and responses, and each request contains the standard `User-agent` header of 'pyrax/1.4.x'.

If the environment is then changed to 'public', pyrax switches to Rackspace authentication against the ORD region, using the username 'joeracker'. It no longer emits debug messages, does not perform SSL certificate verification, and all requests have the custom `User-agent` header of 'CrazyApp/2.0 pyrax/1.4.x'.


### Accessing Environment Information
Pyrax offers several methods for querying and modifying envrionments and their settings. To start, you can determine the current environment by calling `pyrax.get_environment()`. You can also get a list of all defined envrionments by calling `pyrax.list_environments()`. And as mentioned above, you can switch the current envrionment by calling `pyrax.set_environment(new_env_name)`.

To get the value of a setting, call `pyrax.get_setting(key)`. Normally you do not need to change settings in the middle of a session, but just in case you do, you can use the `pyrax.set_setting(key, val)` method. Both of these methods work on the current environment by default. You can get/set settings in other environments with those calls by passing in the envrionment name as the optional `env` parameter to those methods.


## Debugging HTTP requests
Sometimes when developing an application, the results received from the server are not what were expected. In those cases, it is helpful to be able to see the requests being sent to the API server, along with the responses received from the server. For those situations, there is the pyrax **`http_debug`** setting. There are two ways to enable this behavior globally. First, if you want to track all HTTP activity, you can change the `debug` entry in the configuration file mentioned above to 'True'. This causes all API calls and responses to be printed out to the terminal screen. Alternatively, you can call `pyrax.set_http_debug(True)` to turn on debug output, and `pyrax.set_http_debug(False)` to turn it off. This enables you to fine-tune the logging behavior for only the portion of your application that is of concern. Finally, if you only wish to debug HTTP requests for a single service, you can set the `http_log_debug` attribute of that service to True. For example, if you wanted to only see the HTTP traffic for the block storage service, you would call `pyrax.cloud_blockstorage.http_log_debug = True`.


## Working with Rackspace's Multiple Regions
Rackspace divides its cloud infrastructure into "regions", and some interactions are only possible if the entities share a region. For example, if you wish to access a Cloud Database from a Cloud Server, that is only possible if the two are in the same region. Furthermore, if you connect to a region and call `pyrax.cloudservers.list()`, you only get a list of servers in that region. To get a list of all your servers, you have to query each region separately. This is simple to do in pyrax.

As of this writing, Rackspace has three cloud regions in the US: "DFW" (Dallas-Fort Worth), "ORD" (Chicago), and "IAD" (Virginia). Your US credentials will also work with two international regions: "SYD" (Sydney) and "HKG" (Hong Kong). Rackspace also has one UK region: "LON" (London), which has separate login credentials. To get a list of all your US servers, you can do the following:

    cs_dfw = pyrax.connect_to_cloudservers(region="DFW")
    cs_ord = pyrax.connect_to_cloudservers(region="ORD")
    cs_iad = pyrax.connect_to_cloudservers(region="IAD")
    dfw_servers = cs_dfw.list()
    ord_servers = cs_ord.list()
    iad_servers = cs_iad.list()
    all_servers = dfw_servers + ord_servers + iad_servers

The important point to keep in mind when dealing with multiple regions is that all of pyrax's `connect_to_*` methods take a region parameter, and return a region-specific object. If you do not explicitly include a region, the default region you defined in your config file is used. If you did not define a default region, pyrax defaults to the "DFW" region.

**Update as of Version 1.8.0**: Pyrax's new [*context objects*](https://github.com/rackspace/pyrax/blob/master/docs/context_objects.md) make it even easier to work with multiple regions. See that document for more in-depth information, but here is the code listed above re-written to use context objects to handle multiple regions:

    import pyrax
    ctx = pyrax.create_context()
    ctx.keyring_auth()
    cs_dfw = ctx.DFW.compute.client
    cs_ord = ctx.ORD.compute.client
    cs_iad = ctx.IAD.compute.client
    dfw_servers = cs_dfw.list()
    ord_servers = cs_ord.list()
    iad_servers = cs_iad.list()
    all_servers = dfw_servers + ord_servers + iad_servers


## The `Identity` Class
pyrax has an `Identity` class that is used to handle authentication and cache credentials. You can access it in your code using the reference `pyrax.identity`.  Once authenticated, it stores your credentials and authentication token information. In most cases you do not need to interact with this object directly; pyrax uses it to handle authentication tasks for you. But it is available in case you need more fine-grained control of the authentication process, such as querying endpoints in different regions, or getting a list of user roles.

As of Version 1.8.0 of pyrax, the concept of [**context objects**](https://github.com/rackspace/pyrax/docs/context_objects.md) that encapsulate all of the identity and clients for a given login can be used.

You can check its `authenticated` attribute to determine if authentication was successful; if so, its `token` and `expires` attributes contain the returned authentication information, and its `services` attribute contains a dict with all the service endpoint information. Here is an example of the contents of `services` after authentication (with identifying information obscured):

    {u'access':
            {u'serviceCatalog': [
                {u'endpoints': [{u'publicURL': u'https://ord.loadbalancers.api.rackspacecloud.com/v1.0/000000',
                                  u'region': u'ORD',
                                  u'tenantId': u'000000'},
                                 {u'publicURL': u'https://dfw.loadbalancers.api.rackspacecloud.com/v1.0/000000',
                                  u'region': u'DFW',
                                  u'tenantId': u'000000'}],
                  u'name': u'cloudLoadBalancers',
                  u'type': u'rax:load-balancer'},
                 {u'endpoints': [{u'internalURL': u'https://snet-storage101.dfw1.clouddrive.com/v1/MossoCloudFS_ffffffff-ffff-ffff-ffff-ffffffffffff',
                                  u'publicURL': u'https://storage101.dfw1.clouddrive.com/v1/MossoCloudFS_ffffffff-ffff-ffff-ffff-ffffffffffff',
                                  u'region': u'DFW',
                                  u'tenantId': u'MossoCloudFS_ffffffff-ffff-ffff-ffff-ffffffffffff'},
                                 {u'internalURL': u'https://snet-storage101.ord1.clouddrive.com/v1/MossoCloudFS_ffffffff-ffff-ffff-ffff-ffffffffffff',
                                  u'publicURL': u'https://storage101.ord1.clouddrive.com/v1/MossoCloudFS_ffffffff-ffff-ffff-ffff-ffffffffffff',
                                  u'region': u'ORD',
                                  u'tenantId': u'MossoCloudFS_ffffffff-ffff-ffff-ffff-ffffffffffff'}],
                  u'name': u'cloudFiles',
                  u'type': u'object-store'},
                 {u'endpoints': [{u'publicURL': u'https://dfw.servers.api.rackspacecloud.com/v2/000000',
                                  u'region': u'DFW',
                                  u'tenantId': u'000000',
                                  u'versionId': u'2',
                                  u'versionInfo': u'https://dfw.servers.api.rackspacecloud.com/v2',
                                  u'versionList': u'https://dfw.servers.api.rackspacecloud.com/'},
                                 {u'publicURL': u'https://ord.servers.api.rackspacecloud.com/v2/000000',
                                  u'region': u'ORD',
                                  u'tenantId': u'000000',
                                  u'versionId': u'2',
                                  u'versionInfo': u'https://ord.servers.api.rackspacecloud.com/v2',
                                  u'versionList': u'https://ord.servers.api.rackspacecloud.com/'}],
                  u'name': u'cloudServersOpenStack',
                  u'type': u'compute'},
                 {u'endpoints': [{u'publicURL': u'https://dns.api.rackspacecloud.com/v1.0/000000',
                                  u'tenantId': u'000000'}],
                  u'name': u'cloudDNS',
                  u'type': u'rax:dns'},
                 {u'endpoints': [{u'publicURL': u'https://dfw.databases.api.rackspacecloud.com/v1.0/000000',
                                  u'region': u'DFW',
                                  u'tenantId': u'000000'},
                                 {u'publicURL': u'https://ord.databases.api.rackspacecloud.com/v1.0/000000',
                                  u'region': u'ORD',
                                  u'tenantId': u'000000'}],
                  u'name': u'cloudDatabases',
                  u'type': u'rax:database'},
                 {u'endpoints': [{u'publicURL': u'https://servers.api.rackspacecloud.com/v1.0/000000',
                                  u'tenantId': u'000000',
                                  u'versionId': u'1.0',
                                  u'versionInfo': u'https://servers.api.rackspacecloud.com/v1.0',
                                  u'versionList': u'https://servers.api.rackspacecloud.com/'}],
                  u'name': u'cloudServers',
                  u'type': u'compute'},
                 {u'endpoints': [{u'publicURL': u'https://cdn1.clouddrive.com/v1/MossoCloudFS_ffffffff-ffff-ffff-ffff-ffffffffffff',
                                  u'region': u'DFW',
                                  u'tenantId': u'MossoCloudFS_ffffffff-ffff-ffff-ffff-ffffffffffff'},
                                 {u'publicURL': u'https://cdn2.clouddrive.com/v1/MossoCloudFS_ffffffff-ffff-ffff-ffff-ffffffffffff',
                                  u'region': u'ORD',
                                  u'tenantId': u'MossoCloudFS_ffffffff-ffff-ffff-ffff-ffffffffffff'}],
                  u'name': u'cloudFilesCDN',
                  u'type': u'rax:object-cdn'},
                 {u'endpoints': [{u'publicURL': u'https://monitoring.api.rackspacecloud.com/v1.0/000000',
                                  u'tenantId': u'000000'}],
                  u'name': u'cloudMonitoring',
                  u'type': u'rax:monitor'}],
    u'token': {u'expires': u'2222-02-22T22:22:22.000-02:00',
        u'id': u'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx',
        u'tenant': {u'id': u'000000', u'name': u'000000'}},
    u'user': {u'RAX-AUTH:defaultRegion': u'',
       u'id': u'123456',
       u'name': u'someuser',
       u'roles': [{u'description': u'User Admin Role.',
                   u'id': u'3',
                   u'name': u'identity:user-admin'}]}}}


# Images

## Basic Concepts
The Images API allows you to do much more with your compute resources than simply create cloud servers. You can create snapshots of your servers in a particular state, and then share them with others. You can export images to a Cloud Files container, and then copy the image to another data center, from which you can then import it to create copies of compute resources. You can even copy your images to another OpenStack provider and import them there.

There are two main types of images: **public** and **private**. Public images are those supplied by your cloud provider, and are available to everyone. Private images are those that you create in your own account by creating snapshots of your compute resources; these are only available to your account, or to any account with which you explicitly share them.


## Using Images in pyrax
Once you have authenticated, you can reference the Images service via `pyrax.images`. To make your coding easier, include the following line at the beginning of your code:

    imgs = pyrax.images

Then you can simply use the alias `imgs` to reference the service. All of the code samples in this document assume that `imgs` has been defined this way.


## Listing Images
To get a list of the images you have, run:

    images = imgs.list()
    print images

This returns a list of Image objects that looks something like this:

    [<Image auto_disk_config=False, cache_in_nova=True, checksum=47125fb048f027a813ccb27d39cb3087, container_format=ovf, created_at=2014-03-03T20:13:18Z, disk_format=vhd, id=5efb77d0-738c-43a0-9e6e-0c4229e443f2, image_type=base, min_disk=40, min_ram=2048, name=Windows Server 2008 R2 SP1 + SQL Server 2012 SP1 Standard, os_type=windows, protected=False, size=11063822327, status=active, tags=[], updated_at=2014-03-05T18:39:06Z, visibility=public>,
     <Image auto_disk_config=disabled, cache_in_nova=True, checksum=4bc3581e26af4c05813d4dcf75e6de77, container_format=ovf, created_at=2014-02-25T22:03:09Z, disk_format=vhd, id=65bef64f-02e8-4fd7-8433-d538073c7571, image_type=base, min_disk=20, min_ram=512, name=Red Hat Enterprise Linux 6.5 (PVHVM), os_distro=rhel, os_type=linux, protected=False, size=509451746, status=active, tags=[], updated_at=2014-03-05T23:18:08Z, visibility=public, vm_mode=hvm>,
     <Image auto_disk_config=disabled, cache_in_nova=True, checksum=85285e6f4b4256e244f0ed3e8b4215e2, container_format=ovf, created_at=2014-02-25T04:07:34Z, disk_format=vhd, id=9c1d8506-ffcd-4218-80cb-e8a2a0470131, image_type=base, min_disk=20, min_ram=512, name=Ubuntu 13.10 (Saucy Salamander) (PVHVM), os_distro=ubuntu, os_type=linux, protected=False, size=766370044, status=active, tags=[], updated_at=2014-03-05T23:21:11Z, visibility=public, vm_mode=hvm>]

The list that is returned is subject to the built-in limit of 25 images; you can use the pagination parameters (described below) to get more results, or, if you know you want the full listing, call:

    all_images = imgs.list_all()


### Filtering Image Listings
The call to `list()` takes several optional parameters which are used to return just the images that meet the specified values. Here are the available parameters and their effects:

Parameter | Effect
-------- | -----
**limit** | Restricts the number of Image objects returned. Fewer objects may be returned if there are not as many as the limit.
**marker** | Used in pagination to determine where to start the next listing. It is the value of the `id` of the last image returned from the previous listing.
**name** | Filters images whose name exactly matches this value. No wildcards can be used.
**visibility** | Filters images on whether they are 'public' or 'private'.
**member_status** | Filters images to only those that have members with the specified status; values can be 'accepted', 'pending', or 'rejected'. See the section on Sharing Images below.
**owner** | Filters images to those shared with my account by the specified owner.
**tag** | Filters images that contain the specified tag.
**status** | Filter parameter that species the image status as 'queued', 'saving', 'active', 'killed', 'deleted', or 'pending_delete'.
**size_min** | Filters images whose size **in bytes** is greater than or equal to this value.
**size_max** | Filters images whose size **in bytes** is less than or equal to this value. 
**sort_key** | Images by default are returned in order of their **created_at** value. You can specifiy any other attribute of an image to control the sorting with this parameter.
**sort_dir** | Sort direction. Valid values are 'asc' (ascending) and 'desc' (descending). The default is 'desc'.


## Sharing an Image
You share an image that you own by adding accounts other than your own to the image; these external accounts are referred to as _members_. Members are represented by the account number with which you want to share the image; in OpenStack terms, this is the **project_id** (formerly called _tenant_id_). To add a member whose project_id is '12345abc' to the image 'img', call:

    imgs.add_image_member(img, "12345abc")

In this call, 'img' can be either an Image object, or simply the `id` of the image. If 'img' is an Image object, you can call it directly:

    img.add_member("12345abc")

When you add a member to an image you own, they have access to the image and can create new compute resources from it, but it does not appear in their image listings. The member has a status of 'pending' until they either accept or reject the share. Once they accept, the image appears when they retrieve a list of available images.

Note that the image owner cannot change the status of a member; that member with who the image is being shared must do so from their own account. In the event that someone has shared an image with you, you need to get the image ID from the owner. Once you have that, you then call:

    imgs.update_image_member(img_id, status)

where `img_id` is the ID of the image being shared, and `status` is either 'accepted' or 'rejected', depending on what you want to do. There is a third status option: 'pending' - this returns the image member to the state it was in when first shared.

Once an image is shared with a member, they can create new servers with it, but they cannot update or delete it.

If you no longer wish to share an image with a member, you can remove them by calling:

    imgs.delete_image_member(img, project_id)
    # -or-
    img.delete_member(project_id)
    

## Exporting an Image
Exporting an image allows you to move that image across data centers, or even to other cloud providers. When an image is exported, a copy of that image in VHD format is created in a Cloud Files container of your choosing, and a task is created to monitor the progress of the export. To export an image, call:

    task = imgs.export_task(img, cont)

In this call, `img` is either an Image object, or the ID of the image to export, and 'cont' is either a pyrax.cloudfiles.Container object, or the name of the Cloud Files container into which the exported image is to be placed.

Once you have the task, you may poll its status by calling the following:

    task.reload()
    print task.status

The possible status values are:

Status | Meaning
------ | -------
pending | The task is waiting to begin executing
processing | The task is currently running
success | The task completed successfully
failure | The task failed. When this happens, the `message` attribute of the task contains the reason for the failure.

## Importing an Image
You may import images for your use b first uploading them to a container in your Cloud Files account. Once the image is there, you import it by calling:

    task = imgs.import_task(img, cont[, img_format=None[, img_name=None]])

You must supply the image and container. In this case, `img` is the name of the image file within the container, and `cont` is the container name. You may also specify the format of the image by including the `img_format` parameter, but you do not need to if the image is in VHD format (the default). The imported image is named the same as the file in the container unless you include a value in the `img_name` parameter.

Like exporting, importing an image returns a task which you can poll to see the progress. The same statuses apply to imports as exports.

# Installing pyrax
This document explains how to install pyrax on your system so that you can start creating cloud-based applications in Python.

## Installation with `pip`
This is the preferred method, as `pip` handles all of the dependency requirements for pyrax for you.

If you don't already have `pip` installed, you should follow the [pip installation instructions](http://www.pip-installer.org/en/latest/installing.html).

> For all of the examples below, it is assumed that you are installing into a virtualenv, or on a system where you are logged in as the root/administrator. If that is not the case, then you probably have to run the installation under `sudo` to get administrator privileges.

`pip` installs pyrax and its dependencies from one of two sources: the official packaged releases on the [Python Package Index (PyPI)](http://pypi.python.org/pypi), or from the source code repository on [GitHub](https://github.com/rackspace/pyrax). The only difference between the two is that with PyPI you can only install the latest official release of pyrax, while with GitHub you can install any branch, including the current development trunk version. Bear in mind that this option is only for developers who need the latest changes and are willing to live with occasional bugs as the code gets updated – what is commonly referred to as the "bleeding edge".

To install from PyPI, run the following:

    pip install pyrax

To install the current released version from GitHub, run:

    pip install git+git://github.com/rackspace/pyrax.git@latest-release

To install the development trunk version from GitHub, run:

    pip install git+git://github.com/rackspace/pyrax.git


## Installing From Source
> NOTE: some Python distributions come with versions of `distutils` that do not support the `install_requires` option to `setup.py`. If you have one of those versions, you get errors as you try to install using the steps below. The best option at this point is to use `pip` to install, as described above.

Download the source code for pyrax from GitHub, and install from that. First, grab the source:

    curl -O https://github.com/downloads/rackspace/pyrax/pyrax-<version>.tar.gz

Then from the command line:

    tar zxf pyrax-<version>.tar.gz
    cd pyrax-<version>
    python setup.py install


## Testing the Installed Module
If all goes well, pyrax and all its dependencies should be installed and ready to use. To test, open the interpreter and try it out (assuming that you have a Rackspace account):

    [ed@MGM6AEDV7M ~/projects]$ python
    Python 2.7.2 (default, Jun 20 2012, 16:23:33) 
    [GCC 4.2.1 Compatible Apple Clang 4.0 (tags/Apple/clang-418.0.60)] on darwin
    Type "help", "copyright", "credits" or "license" for more information.
    >>> import pyrax
    >>> pyrax.set_setting("identity_type", "rackspace")
    >>> pyrax.set_credentials("my_username", "my_API_key")
    >>> pyrax.cloudfiles.list_containers()
    ['photos', 'music', 'documents']


## Updating to Future Versions
`pip` makes it simple to update pyrax to the latest released version. All you need to do is add `--upgrade` to the command you used to install pyrax, and pip installs the latest version of pyrax along with the latest version of any dependencies.

For a source install, you have to download the latest source, and then manually check each of the dependencies for updated releases, and if found, install them according to their directions.

# Queues

## Basic Concepts
Queues is an open source, scalable, and highly available message and notifications service, based on the OpenStack Marconi project. Users of this service can create and manage a producer-consumer or a publisher-subscriber model. Unlimited queues and messages give users the flexibility they need to create powerful web applications in the cloud.

It consists of a few basic components: queues, messages, claims, and statistics. In the producer-consumer model, users create queues in which producers, or servers, can post messages. Workers, or consumers, can then claim those messages and delete them after they complete the actions associated with the messages. A single claim can contain multiple messages, and administrators can query claims for status.

In the publisher-subscriber model, messages are posted to a queue as in the producer-consumer model, but messages are never claimed. Instead, subscribers, or watchers, send GET requests to pull all messages that have been posted since their last request. In this model, a message remains in the queue, unclaimed, until the message's time to live (TTL) has expired.

In both of these models, administrators can get queue statistics that display the most recent and oldest messages, the number of unclaimed messages, and more.


## Using Queues in pyrax
Once you have authenticated, you can reference the Queues service via `pyrax.queues`. To make your coding easier, include the following line at the beginning of your code:

    pq = pyrax.queues

Then you can simply use the alias `pq` to reference the service. All of the code samples in this document assume that `pq` has been defined this way.


# Client ID
Cloud Queues requires that every client accessing queues have a unique **Client ID**. This Client ID must be a UUID string in its canonical form. Example: "3381af92-2b9e-11e3-b191-71861300734c".

If you aren't familiar with UUIDs, Python provides a module in the Standard Library for working with them. Here is the code for creating a UUID string compatible with Cloud Queues:

    import uuid
    my_client_id = str(uuid.uuid4())

Once you have your ID, you need to make it available to pyrax. There are two ways: 

1) After authenticating, but before calling any Cloud Queues methods, set it directly:

    pq.client_id = my_client_id

2) Export it to an environment variable named `CLOUD_QUEUES_ID`, either in your .bashrc, or by doing it explicitly:

    export CLOUD_QUEUES_ID='3381af92-2b9e-11e3-b191-71861300734c'

If you try to use any of the Cloud Queues methods without setting this value, a `QueueClientIDNotDefined` exception is raised.


## Creating a Queue
Queues require a unique name. If you try to create a queue with a name that already exists, a `DuplicateQueue` exception is raised. The command to create a queue is:

    queue = pq.create("my_unique_queue")

If you wish to check a given queue already exists, you may do so as follows:

    exists = pq.queue_exists("name_to_check")

This call returns `True` or `False`, depending on the existence of a queue with the given name.

## Listing queues
The code below shows how you can list all the queues in a given region:

    qs = pyrax.queues.list()

## Posting a Message to a Queue
Messages can be any type of data, as long as they do not exceed 256 KB in length. The message body can be simple values, or a chunk of XML, or a list of JSON values, or anything else. pyrax handles the JSON-encoding required to post the message.

You need to specify the queue you wish to post to. This can be either the name of the queue, or a `Queue` object. If you already have a `Queue` object reference, you can call its `post_message()` method directly. The call is:

    msg = pq.post_message(queue, body, ttl)
    # or
    msg = queue.post_message(body, ttl)

You must supply both a body and a value for `ttl`. The value of `ttl` must be between 60 and 1209600 seconds (one minute to 14 days).


## Listing Messages in a Queue
To get a listing of messages in a queue, you need the queue name or a `Queue` object reference. If you have a `Queue` object, you can call its `list()` method directly. The call is:

    msgs = pq.list_messages(queue[, echo=False][, include_claimed=False]
            [, marker=None][, limit=None])
    # or
    msgs = queue.list([echo=False][, include_claimed=False]
            [, marker=None][, limit=None])

The optional parameters and their effects are:

Parameter | Default | Effect
---- | ---- | ----
**echo** | False | When True, your own messages are included.
**include_claimed** | False | By default, only unclaimed messages are returned. Pass this as True to get all messages, claimed or not.
**marker** | None | Used for pagination. Normally this should not be needed, as the `list()` methods handle this for you.
**limit** | 10 | The maximum number of messages to return. Note that you may receive fewer than the specified limit if there aren't that many available messages in the queue.


## Claiming Messages in a Queue
Claiming messages is how workers processing a queue mark messages as being handled by that worker, avoiding having two workers process the same message.

To claim messages you need the queue name or a `Queue` object reference. If you have a `Queue` object, you can call its `claim_messages()` method directly. When claiming messages you must not only specify the queue, but also give a TTL and a Grace Period. You may also specify a limit to the number of messages to claim. The call is:

    queue_claim = pq.claim_messages(queue, ttl, grace[, count])
    
    queue_claim = queue.claim_messages(ttl, grace[, count])

An explanation of the parameters of this call follows:

Parameter | Default | Notes
---- | ---- | ----
**queue** |  | Either the name of the queue to claim messages from, or the corresponding `Queue` object.
**ttl** |  | The ttl attribute specifies how long the server waits before releasing the claim. The ttl value must be between 60 and 43200 seconds (12 hours).
**grace** |  | The grace attribute specifies the message grace period in seconds. The value of the grace period must be between 60 and 43200 seconds (12 hours). To deal with workers that have stopped responding (for up to 1209600 seconds or 14 days, including claim lifetime), the server extends the lifetime of claimed messages to be at least as long as the lifetime of the claim itself, plus the specified grace period. If a claimed message would normally live longer than the grace period, its expiration is not adjusted.
**count** | 10 | The number of messages to claim. The maximum number of messages you may claim at once is 20.

If there are no messages to claim, the method returns `None`. When you create a successful claim, a `QueueClaim` object is returned that has a `messages` attribute. This is a list of `QueueMessage` objects representing the claimed messages. You can iterate through this list to process the messages, and once the message has been processed, call its `delete()` method to remove it from the queue to ensure that it is not processed more than once.


## Renewing a Claim
Once a claim has been made, if the TTL and grace period expire, the claim is automatically released and the messages are made available for others to claim. If you have a long-running process and want to ensure that this does not happen in the middle of the process, you should update the claim with one or both of a TTL or grace period. Updating resets the age of the claim, restarting the TTL for the claim. To update a claim, call:

    pq.update_claim(queue, claim[, ttl=None][, grace=None])
    # or
    queue.update_claim(claim[, ttl=None][, grace=None])


## Refreshing a Claim
If you have a `QueueClaim` object, keep in mind that it is not a live window into the status of the claim; rather, it is a snapshot of the claim at the time the object was created. To refresh it with the latest information, call its `reload()` method. This refreshes all of its attributes with the most current status of the claim.


## Releasing a Claim
If you have a claim on several messages and must abandon processing of those messages for any reason, you should release the claim so that those messages can be processed by other workers as soon as possible, instead of waiting for the claim's TTL to expire. When you release a claim, the claimed messages are immediately made available in the queue for other workers to claim. To release a claim, call:

    pq.release_claim(queue, claim)
    # or
    queue.release_claim(claim)



(Mike - I used markup syntax to show lists, links, and code so that you know what to include in the HTML)


# Rackspace Cloud SDK for Python

The Rackspace Cloud SDK for Python contains a language API, Getting Started Guide, API Reference Manual, Quick Reference, Release Notes, and sample code.

## Minimum requirements

The Rackspace Cloud SDK for Python requires the following:

* A Rackspace Cloud account
	* username
	* API key
* Python 2.7.x
	* Not yet tested yet with other Python versions. Please post feedback about what works or does not work with other versions.


## Installation instructions

The best way to install pyrax is by using [pip](http://www.pip-installer.org/en/latest/) to get the latest official release from the Python Package Index:

	pip install pyrax

You may also use `pip` to install directly from GitHub:

	pip install git+git://github.com/rackspace/pyrax.git

If you are not using [virtualenv](http://pypi.python.org/pypi/virtualenv), you will need to run `pip install` as admin using `sudo`.

Complete instructions can be found in the [Installation Guide](https://github.com/rackspace/pyrax/blob/master/docs/installing_pyrax.md)


### Current version - 1.0.0

[Download the SDK](https://github.com/rackspace/pyrax) – language bindings, sample code, and documentation.

[Getting Started with pyrax](https://github.com/rackspace/pyrax/blob/master/docs/pyrax_doc.md)– information and examples to get you started.

[API Reference Manual](http://docs.rackspace.com/sdks/api/python/) – detailed documentation for the classes and methods.

[pyrax Quick Reference](https://github.com/rackspace/pyrax/tree/master/docs) – there are several quick reference guides for the services available.

[Release Notes](https://github.com/rackspace/pyrax/blob/master/RELEASENOTES.md) – release notes for software versions.

[Samples](https://github.com/rackspace/pyrax/tree/master/samples) – code samples demonstrating how to accomplish common tasks.



pyrax

Python SDK for OpenStack/Rackspace APIs

See the COPYING file for license and copyright information.

pyrax should work with most OpenStack-based cloud deployments, though it specifically targets the Rackspace public cloud. For example, the code for cloudfiles contains the ability to publish your content on Rackspace's CDN network, even though CDN support is not part of OpenStack Swift. But if you don't use any of the CDN-related code, your app will work fine on any standard Swift deployment.

See the Release Notes for what has changed in the latest release


* Getting Started with OpenStack/Rackspace
To sign up for a Rackspace Cloud account, go to http://www.rackspace.com/cloud and follow the prompts.

If you are working with an OpenStack deployment, you can find more information at http://www.openstack.org.


* Requirements
  - A Rackspace Cloud account
  - username
  - API key
  - Python 2.7
      Note: pyrax is not yet tested yet with other Python versions. Please post feedback about what works or does not work with other versions. See the Support and Feedback section below for where to post.


* Installation
The best way to install pyrax is by using pip to get the latest official release:

    pip install pyrax

If you would like to work with the current development state of pyrax, you can install directly from trunk on GitHub:

    pip install git+git://github.com/rackspace/pyrax.git

If you are not using virtualenv, you will need to run pip install as admin using sudo.

You may also download and install from source. The source code for pyrax is available on GitHub.

Once you have the source code, cd to the base directory of the source and run (using sudo, if necessary):

    python setup.py install


* Updates
If you installed pyrax using pip, it is simple to get the latest updates from either PyPI or GitHub:

    # PyPI
    pip install --upgrade pyrax
    # GitHub
    pip install --upgrade git+git://github.com/rackspace/pyrax.git


* Support and Feedback
Your feedback is appreciated! If you have specific issues with the pyrax SDK, developers should file an issue via Github. For general feedback and support requests, send an email to: sdk-support@rackspace.com.

#pyrax
Python SDK for OpenStack/Rackspace APIs

See the COPYING file for license and copyright information.

**pyrax** should work with most OpenStack-based cloud deployments, though it specifically targets the Rackspace public cloud. For example, the code for cloudfiles contains the ability to publish your content on Rackspace's CDN network, even though CDN support is not part of OpenStack Swift. But if you don't use any of the CDN-related code, your app will work fine on any standard Swift deployment.

See the [Release Notes](https://github.com/rackspace/pyrax/tree/master/RELEASENOTES.md) for what has changed in the latest release

[![Build Status](https://travis-ci.org/rackspace/pyrax.png)](https://travis-ci.org/rackspace/pyrax)

##Getting Started with OpenStack/Rackspace
To sign up for a Rackspace Cloud account, go to

[http://www.rackspace.com/cloud](http://www.rackspace.com/cloud)

and follow the prompts.

If you are working with an OpenStack deployment, you can find more information at [http://www.openstack.org](http://www.openstack.org).


## Requirements

* A Rackspace Cloud account
	* username
	* API key
* Python 2.7
	* pyrax is not yet tested yet with other Python versions. Please post feedback about what works or does not work with other versions. See the **Support and Feedback** section below for where to post.


## Installation
The best way to install **pyrax** is by using [pip](http://www.pip-installer.org/en/latest/) to get the latest official release:

	pip install pyrax

If you would like to work with the current development state of pyrax, you can install directly from trunk on GitHub:

	pip install git+git://github.com/rackspace/pyrax.git

If you are not using [virtualenv](http://pypi.python.org/pypi/virtualenv), you will need to run `pip install` as admin using `sudo`.

You may also download and install from source. The source code for **pyrax** is available on [GitHub](https://github.com/rackspace/pyrax/).

Once you have the source code, `cd` to the base directory of the source and run (using `sudo`, if necessary):

	python setup.py install


## Updates
If you installed **pyrax** using pip, it is simple to get the latest updates from either PyPI or GitHub:

	# PyPI
	pip install --upgrade pyrax
	# GitHub
	pip install --upgrade git+git://github.com/rackspace/pyrax.git


## Contributing
Please see the HACKING file for contribution guidelines. Make sure pull requests are on the `working` branch!


## Support and Feedback
You can find documentation for using the **pyrax** SDK at [docs.rackspace.com](http://docs.rackspace.com/sdks/guide/content/python.html).

Your feedback is appreciated! If you have specific issues with the **pyrax** SDK, developers should file an [issue via Github](https://github.com/rackspace/pyrax/issues).

For general feedback and support requests, send an email to: <sdk-support@rackspace.com>.

# Release Notes for pyrax

###2014.05.13 - Version 1.8.1
  - General
    - Restored module-level regions and services attributes. GitHub #371
    - Improved error message when calling get_client when not authenticated.
      GitHub #369

  - Identity
    - Added the ability to request multiple clients. GitHub #370
    - Modified list_tenants() function to take an admin argument. GitHub #352
    - Fixed service catalog parsing. GitHub #361

  - Cloud Files
    - Added aliases to make Cloud Files method names more consistent. GitHub
      #373
    - Added missing limit/marker parameters. GitHub #349
    - Added code to check for CDN before making CDN calls.
    - Made the meta prefixes read-only. GitHub #365
    - Added 'prefix' parameter to get/set metadata commands. GitHub #367
    - Added chunking to put_object()
    - Fixed old cloudfiles reference. GitHub #362
    - Fixed unit tests for CDN changes.


###2014.05.06 - Version 1.8.0
  - Identity
    - Added **Context Objects** as a way to encapsulate an authenticated
      session.
    - Context objects remove the limitation in pyrax of only working with a
      single authenticated session at a time.
    - Improves the ability to work with multiple providers at once, or across
      multiple regions for the same provider.
    - More information in the **context_objects.md** document in the docs/
      folder.

  - Cloud Files
    - Fixed missing URL quoting for bulk deletes. GitHub #350
    - Multiple improvements to sync_folder_to_container in GitHub #355:
      - Added the ability to specify a prefix to be added to the object name
        during checking and uploading during a sync
      - Sped up sync_folder_to_container by having it pull down a list of
        objects all at once to use to compare against instead of checking once
        for each file.
      - Added verbose logging to sync_folder_to_container (Originally requested
        in GitHub #250)

  - General
    - Fixed issue where one bad section in the configuration file caused threw
      an exception that terminated your app. GitHub #346
    - Removed the need to specify a tenant_id when authenticating with a token.
      GitHub #345

  - Block Storage
    - Added missing update methods to Cloud Block Storage.

  - Documentation
    - Updated the queues docs to include listing of queues. GitHub #353


###2014.04.07 - Version 1.7.3
  - Identity
    - Updated the identity module and tests to work with the new http library.
      GitHub #333

  - General
    - Fixed some log debug issues.
    - Removed locale test, as it is unreliable at best.

  - Cloud Files
    - Round up the datetime in seconds in convert_list_last_modified to match
      the behaviour in https://review.openstack.org/#/c/55488/. GitHub #337
    - Fixed ValueError when handling a bulk delete response. GitHub #335


###2014.03.30 - Version 1.7.2
  - General
    - Fixes a bug that doubly-encoded JSON body content. GitHub #333


###2014.03.28 - Version 1.7.1
  - General
    - Added a CONTRIBUTING.rst file, following the suggestion of @justinclift
      in GitHub #327.
    - Removed dependecy on the httplib2 library; pyrax now only relies on the
      'requests' module for HTTP communication.
    - Fixed a bug in folder size calculations. GitHub #302
    - Removed a limit that only handled Rackspace vendor extensions. GitHub #315
    - Updated the setup.py version requirements for the 'requests' and 'six'
      libraries. GitHub #314
    - Updated utility calls to reflect new names. GitHub #312

  - Documentation
    - Minor typo correction. GitHub #326
    - Updated docs for better region coverage. GitHub #324 and #316
    - Updated network limits in docs. GitHub #322

  - Images
    - Sample code for accepting images that were shared add_image_member.py.
      GitHub #318

  - Cloud Files
    - Fixed (yet again) the ability to turn on/off debug output after another
      change in the underlying swiftclient library. GitHub #317


###2014.03.12 - Version 1.7.0
  - New:
    - Added support for **Cloud Images** (Glance).
      - Import/export your compute images across different data centers, or
        even different providers.
      - Share your images with other accounts.
  - Queues:
    - Fixed limit bug for queue messages. GitHub #309
  - General
    - Many Python 3 compatibility improvements.
      - Not fully compatible yet, but getting closer.
    - Fixed config file pathing problem on Windows. GitHub #306
    - Fixed issue where non-401 exceptions were suppressed. GitHub #310

###2014.02.24 - Version 1.6.4
 - Cloud Block Storage:
   - Added support for volume cloning.
 - Cloud Files:
   - Added support for bulk deletes > 10K objects. GitHub #286
   - Fixed edge case with object size == max chunk size. GitHub #287
 - General:
   - Added support for identity modules outside of pyrax. GitHub #292
 - Testing:
   - Moved fakes.py into pyrax module to enable easier testing from other
     projects. GitHub #288
 - Docs:
   - Fixed several typos. GitHub #296 & #296.
 - Autoscale:
   - Removed default of AUTO for diskConfig from Autoscale.

###2014.02.03 - Version 1.6.3
 - Cloud Monitoring:
   - Added back missing error info. GitHub #285
   - Added support for Overviews and Changelogs from Cloud Monitoring. GitHub
     #267
 - Autoscale:
   - Corrected how networks are created when none are specified. GitHub #262
   - Added load balancers to sample code for creating a scaling group.
   - Fixed bug in autoscale group creation. GitHub #249 and #203
 - Queues:
   - Removed default TTL when posting messages to a queue. GitHub #265
 - Cloud Files:
   - Add `use_servicenet` setting for Cloud Files. GitHub #273
   - Fixed bug in passing TTL to `delete_in_seconds()`. GitHub #281
   - Added a fix for GETting 0-byte content with Dynamic Large Objects
     (multipart files). GitHub #258
   - Include container name in `X-Object-Manifest` header when creating DLO.
     GitHub #261
   - Use `X-Object-Manifest` instead of `X-Object-Meta-Manifest` when creating
     DLO. GitHub #260
 - Cloud Load Balancers:
   - Added `httpsRedirect` param for Cloud Load Balancers. GitHub #277
   - Adding an entry for the `id` attribute to the Node's `to_dict()` method.
     GitHub #276
 - Cloud DNS:
   - Handle empty bodies in GET responses from the Cloud DNS API. GitHub #280
 - Cloud Servers:
   - Updated docs and samples to eliminate old flavor references.
 - General:
   - Add requests as installation requirement. GitHub #269

###2013.11.13 - Version 1.6.2
 - Cloud Databases:
   - Added missing 'host' parameter. GitHub #246
 - Cloud Queues:
   - Removed requirement for Client ID for non-message requests. GitHub #244
   - Added support for ServiceNet queues. GitHub #240
   - Added the `claim_id` parameter to message deletion calls. GitHub #243
   - Fixed a bug when parsing message and claim IDs.
   - Made several corrections in the docs. - Cloud DNS:
   - Added handling for an occasional empty body when polling a running request.
    GitHub #237
 - General:
   - Added support for Python Wheel distribution
   - Fixed missing file spec in MANIFEST.in
   - Removed unneeded files

###2013.10.31 - Version 1.6.1
 - Cloud Databases:
    - Added support for Backups. GitHub #216
    - Added ability to specify 'host' parameter for users. GitHub #229
    - Added ability to update users.
 - Queues:
    - Removed default TTL for messages. GitHub #234
 - Cloud Files:
    - Fixed large file upload bug. GitHub #231
    - Fixed file naming bug. GitHub #232

###2013.10.24 - Version 1.6.0
 - New:
    - Added support for **Cloud Queues** (Marconi).
 - Cloud Files:
    - Fixed an issue where the `last_modified` datetime values for Cloud Files
      storage_objects were returned inconsistently.
    - Added ability to cache `temp_url_key`. GitHub #221
    - Added ability to do partial downloads. GitHub #150
    - Fixed an issue where calling `delete_object_in_seconds()` deleted existing
      metadata. GitHub #135
 - Cloud Databases:
    - Added missing pagination parameters to several methods. GitHub #226
 - Cloud DNS:
    - Changed the `findall()` method to be case-insensitive.
    - Fixed some error-handling issues. GitHub #219
 - Auto Scale:
    - Added code to force 'flavor' arguments to `str` type.
    - Fixed creation/retrieval of webhooks with policy ID.
    - Added several replacement methods for configurations.
 - Load Balancers:
    - Removed requirement that nodes be passed when creating a load balancer.
      GitHub #222
 - Testing:
    - Improved the smoketest.py integrated test script by adding more services.
    - Fixed the smoketest to work when running in multiple regions that don't
      all offer the same services.
 - General:
    - Refactored the `_create_body()` method from the `BaseClient` class to the
      `BaseManager` class.

###2013.10.04 - Version 1.5.1
 - Pyrax in general:
     - Moved the `get_limits()` behavior to the base client (Nate House)
     - Added ability to call a full URL from the client.
     - Added HEAD methods to base client and manager classes.
     - Removed unused imports. GitHub #189
     - Improved handling of 400 errors in `identity.create_user()`
     - Fixed issue with password auth failing with Rackspace identity.
        GitHub #190
     - Added utility method for RFC 2822-compliant dates.
     - Refactored the `_create_body()` method into the BaseManager class.
     - Improved handling of default regions in the service catalog.
     - Added support for different auth endpoints for Rackspace auth.
     - Added files to allow creating RPMs. (Greg Swift)
 - Cloud Files:
     - Added the `bulk_delete()` method.
     - Added support for "bare" metadata keys. GitHub #164
     - Added cache override capability. GitHub #191
     - Added copy/move methods to Container and StorageObject classes.
        GitHub #192
     - Added listing of pseudo-subdirectories. GitHub #174
     - Added the `list()` method to generate a list of container objects.
        GitHub #186
 - Autoscale improvements, thanks to Christopher Armstrong:
     - Added additional arguments for launch configurations.
        GitHub #207, #209, #212
     - Added support for group metadata. GitHub #202
     - Added suppport for desired_capacity in policies. GitHub #208
     - Added `args` to expand capabilities in webhook policy updates.
        GitHub #204, #205
 - Monitoring:
     - Workaround the odd requirement to use millisecond timestamps in
        `get_metric_data_points()` GitHub #176
     - Unix timestamps are now supported in addition to Python date/datetimes.
 - Load Balancers:
     - Fixed VirtualIP `to_dict()` method to use the ID if available. (Vaibhav)
     - Add node type to the dict passed to the API (Troy C)
 - DNS:
     - Domains can now be specified by name, not just ID. GitHub #180

###2013.09.04 - Version 1.5.0
- Added support for the Rackspace Cloud Monitoring service
- Added support for the Rackspace Autoscale service
- Fixed an issue where parameters to the manger.create() method were passed
  incorrectly.

###2013.08.21 - Version 1.4.11
- Fixed issue #161: different locales caused date parsing error.
- Fixed issue #166: passwords with non-ASCII characters were causing parsing
  errors.
- Added setting identity_type to the sample code. GitHub #169.
- Fixed the way that default regions are handled. GitHub #165.
- Changed the example code to use only ASCII characters for server names.
  GitHub #162.
- Changed container.get_object() to use the more efficient method in the
  client. GitHub #160.
- Fixed broken internal link. GitHub #159.

###2013.08.06 - Version 1.4.10
- Fixed a performance issue when GETting a single object. GitHub #156.
- Fixed an issue with error response parsing. GitHub #151.
- Fixed trailing slash bug in identity. GitHub #154.
- Fixed a bug noticed in #69 in which the parameters to the swiftclient
  connection object were incorrect.
- Fixed a bug in download_object() that would throw an exception if the target
  directory already exists. GitHub #148.
- Added ability to specify content length when uploading an object to swift.
  GitHub #146.

###2013.07.23 - Version 1.4.9
- Fixed a bug introduced in the last release that prevented progressive
  fetching of objects. GitHub #139
- Fixed an issue where the `verify_ssl` setting was not being passed to the
  Identity instance. GitHub #140
- Added support for returning extra info about API calls to Swift. This
  includes info on the status, reason, and header information for the call.
  GitHub #138

###2013.07.19 - Version 1.4.8
- Added a hack to work around an apparent bug in python-swiftclient that was
    preventing automatic re-authentication after a token expired. This affects
    issues #111, #115, #117, and possibly others.
- Fixed Issue #131 that caused an exception when uploading a binary file.
- Fixed Issue #134: uploading file-like objects
- Fixed auth_with_token() to return the full service catalog. Issue #128.
- Improved the checksum process to be more memory efficient with very large
    files. Issue #122.

###2013.06.28 - Version 1.4.7
- Added the `update()` method to modify existing load balancers.
- The `fetch_object()` method was not raising the correct exception when the
    requested object/container combination does not exist.
- Added support for downloading objects in nested folders. GitHub #104.
- Fixed an issue (#110) that was causing the purge from CDN command to fail.
- Added support for bypassing SSL certificate verification with cloud servers,
    based on PR #96.
- Improved unit test coverage for several modules.
- Add `eq` and `ne` to the `Node` class in cloud load balancers.
- Updated the installation guide with identity_type setting. Issue #105.
- Fixed bug where `tenant_id` was ignored if passed to `set_credentials()`.
- Added `return_none` option to cloud files `store_object()` method.

###2013.06.13 - Version 1.4.6
- Added the ability to authenticate with an existing token.
- Fixed an issue where the default environment was not properly set. Issue #87.
- Modified tests so that they work with PyPy.
- Added better explanation of pyrax's ability to automatically re-authenticate
    when a token expires. Issue #93.
- Fixed a bug resulting from overly-aggressive URL quoting.
- Removed the 'default_identity_type' definition in pyrax/__init__.py, as it
    is no longer needed. Issue #95.

###2013.06.05 - Version 1.4.5
- Fixed a bug that prevented region from being properly set. Issue #86.

###2013.06.04 - Version 1.4.4
- Fixed a bug when using environment variables to set the identity_type. Issue
  #82.

###2013.06.03 - Version 1.4.3
- Added support for having objects automatically deleted from Cloud Files after
    a period of time.

###2013.05.30 - Version 1.4.2
- Fixed several bugs related to the identity and config file changes.

###2013.05.30 - Version 1.4.1
- Added support for new Cloud Database user APIs.
- Fixed a bug in which an exception class was not defined (#77)

###2013.05.29 - Version 1.4.0
- Added support for **all** OpenStack clouds. Previous versions only supported
  Rackspace authentication methods.
- Configuration files now support multiple cloud environments in the same file.
  You can switch between environments by calling
  `pyrax.set_environment("env")`, where `env` is the name of the desired
  environment.
- Configuration settings can now be stored in environment variables. These all
  begin with `CLOUD_`; a full list can be found in the [main pyrax
  documentation](https://github.com/rackspace/pyrax/tree/master/docs/pyrax_doc.md).
- Available regions are now available in the `pyrax.regions` attribute after
  authentication.
- Services that are available for the current cloud provider are now available
  in the `pyrax.services` attribute.
- Fixed an issue in Cloud Databases in which the `volume` attribute was
  sometimes a dict and sometimes an instance of `CloudDatabaseVolume`. Now it
  will always be an instance.
- Added a smoke test script to the integrated tests. It currently covers the
  compute, networking, database, and object_store services.
- Removed unnecessary hack for compute URIs.
- Cleaned up some naming and formatting inconsistencies.


###2013.05.10 - Version 1.3.9
- This fixes two issues: #63 and #67. The first fixes an incorrect path in the
    cloudfiles get_temp_url() function; the second adds the ability to specify
    the content_encoding for an object in cloudfiles.

###2013.04.29 - Version 1.3.8
- Fixed a bug that prevented the Cloud Servers code from running properly
    in the UK.

###2013.04.26 - Version 1.3.7
- Removed a lot of the duplicated identity code from the main client
    class. This is in anticipation of a major re-working of identity
    that will work with non-Rackspace OpenStack deployments.
- Added methods to Cloud Load Balancer code to make it easier to get
    individual device usages and links.
- Added a customizable delay period for Cloud DNS.
- Changed the default behavior of utils.wait_until() to wait forever
    for the desired state to be reached. Previous default was 10 attempts,
    and it seemed that over 90% of use cases required waiting indefinitely,
    so the default was changed.
- Cleaned up some of the documentation style.

###2013.04.03 - Version 1.3.6
- Fixed the auth issues with python-novaclient introduced in the most
    recent release of that library. Thanks to Matt Martz for this fix.

###2013.03.27 - Version 1.3.5
- Updated the Cloud Databases code to work with recent API changes.
- Updated HACKING doc to include specific PEP8 exclusions.
- Cleaned up the code in the tests/ and samples/ directories for PEP8.
- Changed all uses of `file()` to `open()`.
- Added tox support.

###2013.03.26 - Version 1.3.4
- Fixed an ImportError in Cloud DNS. Thanks to Matt Martz for finding this.
- Minor improvements to the travis.ci integration.

###2013.03.21 - Version 1.3.3
- Added support for creating Temporary URLs for Cloud Files.
- Added set_account_metadata() for Cloud Files.
- Added shortcuts to the Cloud Servers client to make it more consistent
    with the rest of pyrax. E.g.:
      cs.images.list() -> cs.list_images()
- Added change_content_type() to the StorageObject class in Cloud Files.
- Cleaned up the __repr__ for some classes.
- Added more customization to the output for utils.wait_until()
- Cleaned up the markdown formatting in RELEASENOTES.md.
- Added travis-ci integration.

###2013.03.06 - Version 1.3.2
- Removed lazy loading of Database Volumes. GitHub #8.
- Fixed the inconsistent naming of the cloud databases module.
- Removed mixed line endings from the docs that my markdown
    editor inserted.
- Added the find_record() method to Cloud DNS to return a single
    domain record. GitHub #24.
- Added the test dependency for the 'mock' package to setup.py

###2013.03.04 - Version 1.3.1
- The merge for 1.3.0 did not grab the newly-created files for that
    version. They are included in this version.

###2013.03.04 - Version 1.3.0
- Added support for Rackspace Cloud Networks.
- Modified attach/detach of CBS volumes so that they both raise
exceptions on failure, and return None otherwise. GitHub #22
- Fixed bug in block storage that could connect to incorrect
datacenter. Github #19
- Added the option of running utils.wait_until() in a background thread.
- Added the HACKING file to help people contribute to pyrax.
- Merged pull request #21 from simonz05: fix name error: global
name `self` does not exist.

###2013.02.18 - Version 1.2.8
- Fixed a bug that created multiple debugging loggers.
- Refactored the utils script to use the match_pattern() method.

###2013.02.15 - Version 1.2.7
- Code formatting cleanup. No logical changes or additional
functionality included.
- Added httplib2 requirement, now that novaclient no longer installs
it. Taken from pull request #18 from Dustin Farris.
- Merge pull request #13 from adregner/container-ints: container
stats should be integers
- Modified the upload_file() process to not return an object
reference when not needed. GitHub issue #11.

###2013.02.05 - Version 1.2.6
- Added the `sync_folder_to_container()` method to cloudfiles to make it
easier to keep a copy of a local folder synced to the cloud.
- Removed the lazy load of volume info for cloud databases. Changed the
'volume' attribute to be an object to allow for dot notation access
to its values.
- Eliminated as many places as possible where use of non-ASCII characters
caused encoding issues. Added a configuration option to allow users to
specify their preferred encoding; default=utf-8.
- Fixed a bug in the `get_object_names()` method of the Cloud Files
container class.

###2013.01.26 - Version 1.2.5
- Fixed an issue preventing existing node objects being created if in
'DRAINING' condition (GitHub #6). Modified the rax_identity to accept
UTC dates returned from the LON datacenter (GitHub #5). Fixed an
issue that prevented HTTP debugging from turning off in swiftclient.


###2013.01.15 - Version 1.2.4
- Added support for keychain storage of credentials for authentication.


###2013.01.10 - Version 1.2.3
- Added the 'halfClosed' parameter to the create() method of load balancers.

###2013.01.03 - Version 1.2.2
- Fixed an issue that was causing calls to cloudservers to needlessly
  re-authenticate.

###2012.12.27 - Version 1.2.1
- Removed old class docs that were no longer needed in this release.

###2012.12.26 - Version 1.2.0
- Added support for Cloud DNS.
- Removed the 'beta' designation.

###2012.12.26 - Version 1.1.7b
- Updated setup.py to use setuptools.
- Fixed a problem with circular imports of the version info.
- Added a requirement for python-novaclient>=2.10.0.

###2012.12.18 - Version 1.1.6b
- Removed the code that controlled when pyrax connected to services.
- Changed the User-agent format to match the other SDKs.

###2012.12.17 - Version 1.1.5b
- Enhanced the ability to debug HTTP traffic.
- Fixed a bug in object naming when uploading an entire folder in Cloud Files.

###2012.12.13 - Version 1.1.4b
- Added the ability to connect to the internal URL for Cloud Files.
- Added limit and marker to the base client/manager classes.
- Added the cloudfiles Container and StorageObject classes to the pyrax
  namespace.

###2012.12.10 - Version 1.1.2b
- Added a test that was missing in the previous release.

###2012.12.07 - Version 1.1.1b
- Added the ability for developers to customize the User-agent string for their
  applications.

###2012.11.26 - Version 1.1.1b
- Added Cloud Block Storage support.
- Added the refactored code for Cloud Load Balancers that removes the
  dependency on the python-cloudlb library.

###2012.11.24 - Version 1.0.4b
- Maintenance fix release.

###2012.11.20
- Improved the handling of CDN calls so they don't fail as often, and are more
  resilient when they do.

###2012.11.06
- Release of the initial beta for pyrax. Supports Cloud Servers, Cloud Files,
  and Cloud Load Balancers.

