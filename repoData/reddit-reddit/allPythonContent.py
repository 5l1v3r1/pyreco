__FILENAME__ = Makefile
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
import os

from r2.lib.translation import I18N_PATH
from r2.lib.plugin import PluginLoader
from r2.lib import js

print 'POTFILE := ' + os.path.join(I18N_PATH, 'r2.pot')

plugins = PluginLoader()
print 'PLUGINS := ' + ' '.join(plugin.name for plugin in plugins
                               if plugin.needs_static_build)

print 'PLUGIN_I18N_PATHS := ' + ','.join(os.path.relpath(plugin.path)
                                         for plugin in plugins
                                         if plugin.needs_translation)

import sys
for plugin in plugins:
    print 'PLUGIN_PATH_%s := %s' % (plugin.name, plugin.path)

js.load_plugin_modules(plugins)
modules = dict((k, m) for k, m in js.module.iteritems())
print 'JS_MODULES := ' + ' '.join(modules.iterkeys())
outputs = []
for name, module in modules.iteritems():
    outputs.extend(module.outputs)
    print 'JS_MODULE_OUTPUTS_%s := %s' % (name, ' '.join(module.outputs))
    print 'JS_MODULE_DEPS_%s := %s' % (name, ' '.join(module.dependencies))

print 'JS_OUTPUTS := ' + ' '.join(outputs)
print 'DEFS_SUCCESS := 1'

########NEW FILE########
__FILENAME__ = commands
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import paste.deploy.config
import paste.fixture
from paste.registry import RegistryManager
from paste.script import command
from paste.deploy import appconfig        
from r2.config.environment import load_environment
from paste.script.pluginlib import find_egg_info_dir
from pylons.wsgiapp import PylonsApp

from r2.config.middleware import RedditApp

#from pylons.commands import ShellCommand, ControllerCommand, \
#     RestControllerCommand

import os, sys
#
# commands that will be available by running paste with this app
#

class RunCommand(command.Command):
    max_args = 2
    min_args = 1

    usage = "CONFIGFILE CMDFILE.py"
    summary = "Executed CMDFILE with pylons support"
    group_name = "Reddit"


    parser = command.Command.standard_parser(verbose=True)
    parser.add_option('-c', '--command',
                      dest='command',
                      help="execute command in module")
    parser.add_option("", "--proctitle",
                      dest="proctitle",
                      help="set the title seen by ps and top")

    def command(self):
        try:
            if self.options.proctitle:
                import setproctitle
                setproctitle.setproctitle("paster " + self.options.proctitle)
        except ImportError:
            pass

        here_dir = os.getcwd()

        is_standalone = self.args[0].lower() == 'standalone'
        if is_standalone:
            load_environment(setup_globals=False)
        else:
            config_name = 'config:%s' % self.args[0]

            conf = appconfig(config_name, relative_to=here_dir)
            conf.global_conf['running_as_script'] = True
            conf.update(dict(app_conf=conf.local_conf,
                             global_conf=conf.global_conf))
            paste.deploy.config.CONFIG.push_thread_config(conf)

            load_environment(conf.global_conf, conf.local_conf)

        # Load locals and populate with objects for use in shell
        sys.path.insert(0, here_dir)

        # Load the wsgi app first so that everything is initialized right
        if not is_standalone:
            wsgiapp = RegistryManager(RedditApp())
        else:
            # in standalone mode we don't have an ini so we can't use
            # RedditApp since it imports all the fancy controllers.
            wsgiapp = RegistryManager(PylonsApp())
        test_app = paste.fixture.TestApp(wsgiapp)

        # Query the test app to setup the environment
        tresponse = test_app.get('/_test_vars')
        request_id = int(tresponse.body)

        # Disable restoration during test_app requests
        test_app.pre_request_hook = lambda self: \
            paste.registry.restorer.restoration_end()
        test_app.post_request_hook = lambda self: \
            paste.registry.restorer.restoration_begin(request_id)

        # Restore the state of the Pylons special objects
        # (StackedObjectProxies)
        paste.registry.restorer.restoration_begin(request_id)

        loaded_namespace = {}

        if self.args[1:]:
            execfile(self.args[1], loaded_namespace)

        if self.options.command:
            exec self.options.command in loaded_namespace

########NEW FILE########
__FILENAME__ = environment
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import os
import mimetypes

from mako.lookup import TemplateLookup
from pylons.error import handle_mako_error
from pylons import config

import r2.config
import r2.lib.helpers
from r2.config import routing
from r2.lib.app_globals import Globals
from r2.lib.configparse import ConfigValue


mimetypes.init()


def load_environment(global_conf={}, app_conf={}, setup_globals=True):
    # Setup our paths
    root_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

    paths = {'root': root_path,
             'controllers': os.path.join(root_path, 'controllers'),
             'templates': [os.path.join(root_path, 'templates')],
             }

    if ConfigValue.bool(global_conf.get('uncompressedJS')):
        paths['static_files'] = os.path.join(root_path, 'public')
    else:
        paths['static_files'] = os.path.join(os.path.dirname(root_path), 'build/public')

    config.init_app(global_conf, app_conf, package='r2',
                    template_engine='mako', paths=paths)

    # don't put action arguments onto c automatically
    config['pylons.c_attach_args'] = False
    # when accessing non-existent attributes on c, return "" instead of dying
    config['pylons.strict_c'] = False

    g = config['pylons.g'] = Globals(global_conf, app_conf, paths)
    if setup_globals:
        config['r2.import_private'] = \
            ConfigValue.bool(global_conf['import_private'])
        g.setup()
        g.plugins.declare_queues(g.queues)
    g.plugins.load_plugins()
    config['r2.plugins'] = g.plugins
    g.startup_timer.intermediate("plugins")

    config['pylons.h'] = r2.lib.helpers
    config['routes.map'] = routing.make_map()

    #override the default response options
    config['pylons.response_options']['headers'] = {}

    # when mako loads a previously compiled template file from its cache, it
    # doesn't check that the original template path matches the current path.
    # in the event that a new plugin defines a template overriding a reddit
    # template, unless the mtime newer, mako doesn't update the compiled
    # template. as a workaround, this makes mako store compiled templates with
    # the original path in the filename, forcing it to update with the path.
    if "cache_dir" in app_conf:
        module_directory = os.path.join(app_conf['cache_dir'], 'templates')

        def mako_module_path(filename, uri):
            filename = filename.lstrip('/').replace('/', '-')
            path = os.path.join(module_directory, filename + ".py")
            return os.path.abspath(path)
    else:
        # we're probably in "paster run standalone" mode. we'll just avoid
        # caching templates since we don't know where they should go.
        module_directory = mako_module_path = None

    # set up the templating system
    config["pylons.g"].mako_lookup = TemplateLookup(
        directories=paths["templates"],
        error_handler=handle_mako_error,
        module_directory=module_directory,
        input_encoding="utf-8",
        default_filters=["mako_websafe"],
        filesystem_checks=getattr(g, "reload_templates", False),
        imports=[
            "from r2.lib.filters import websafe, unsafe, mako_websafe",
            "from pylons import c, g, request",
            "from pylons.i18n import _, ungettext",
        ],
        modulename_callable=mako_module_path,
    )

    if setup_globals:
        g.setup_complete()

########NEW FILE########
__FILENAME__ = extensions
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import c

def api_type(subtype = ''):
    return 'api-' + subtype if subtype else 'api'

def is_api(subtype = ''):
    return c.render_style and c.render_style.startswith(api_type(subtype))

def get_api_subtype():
    if is_api() and c.render_style.startswith('api-'):
        return c.render_style[4:]

extension_mapping = {
    "rss": ("xml", "text/xml; charset=UTF-8"),
    "xml": ("xml", "text/xml; charset=UTF-8"),
    "js": ("js", "text/javascript; charset=UTF-8"),
    "embed": ("htmllite", "text/javascript; charset=UTF-8"),
    "mobile": ("mobile", "text/html; charset=UTF-8"),
    "png": ("png", "image/png"),
    "css": ("css", "text/css"),
    "csv": ("csv", "text/csv; charset=UTF-8"),
    "api": (api_type(), "application/json; charset=UTF-8"),
    "json-html": (api_type("html"), "application/json; charset=UTF-8"),
    "json-compact": (api_type("compact"), "application/json; charset=UTF-8"),
    "compact": ("compact", "text/html; charset=UTF-8"),
    "json": (api_type(), "application/json; charset=UTF-8"),
    "i": ("compact", "text/html; charset=UTF-8"),
}

API_TYPES = ('api', 'json')

def set_extension(environ, ext):
    environ["extension"] = ext
    environ["render_style"], environ["content_type"] = extension_mapping[ext]

########NEW FILE########
__FILENAME__ = middleware
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

"""Pylons middleware initialization"""
import importlib
import re
import urllib
import tempfile
import urlparse
from threading import Lock

from paste.cascade import Cascade
from paste.registry import RegistryManager
from paste.urlparser import StaticURLParser
from paste.deploy.converters import asbool
from paste.request import path_info_split
from pylons import config, response
from pylons.middleware import ErrorDocuments, ErrorHandler
from pylons.wsgiapp import PylonsApp
from routes.middleware import RoutesMiddleware

from r2.config.environment import load_environment
from r2.config.extensions import extension_mapping, set_extension
from r2.lib.utils import is_subdomain


# patch in WebOb support for HTTP 429 "Too Many Requests"
import webob.exc
import webob.util

class HTTPTooManyRequests(webob.exc.HTTPClientError):
    code = 429
    title = 'Too Many Requests'
    explanation = ('The server has received too many requests from the client.')

webob.exc.status_map[429] = HTTPTooManyRequests
webob.util.status_reasons[429] = HTTPTooManyRequests.title

#from pylons.middleware import error_mapper
def error_mapper(code, message, environ, global_conf=None, **kw):
    if environ.get('pylons.error_call'):
        return None
    else:
        environ['pylons.error_call'] = True

    from pylons import c

    # c is not always registered with the paste registry by the time we get to
    # this error_mapper. if it's not, we can safely assume that we didn't use
    # the pagecache. one such case where this happens is the
    # DomainMiddleware-based srname.reddit.com -> reddit.com/r/srname redirect.
    try:
        if c.used_cache:
            return
    except TypeError:
        pass

    if global_conf is None:
        global_conf = {}
    codes = [304, 400, 401, 403, 404, 409, 415, 429, 503]
    if not asbool(global_conf.get('debug')):
        codes.append(500)
    if code in codes:
        # StatusBasedForward expects a relative URL (no SCRIPT_NAME)
        d = dict(code = code, message = message)

        exception = environ.get('r2.controller.exception')
        if exception:
            d['explanation'] = exception.explanation
            error_data = getattr(exception, 'error_data', None)
            if error_data:
                environ['extra_error_data'] = error_data
        
        if environ.get('REDDIT_CNAME'):
            d['cnameframe'] = 1
        if environ.get('REDDIT_NAME'):
            d['srname'] = environ.get('REDDIT_NAME')
        if environ.get('REDDIT_TAKEDOWN'):
            d['takedown'] = environ.get('REDDIT_TAKEDOWN')

        #preserve x-sup-id when 304ing
        if code == 304:
            try:
                # make sure that we're in a context where we can use SOP
                # objects (error page statics appear to not be in this context)
                response.headers
            except TypeError:
                pass
            else:
                if response.headers.has_key('x-sup-id'):
                    d['x-sup-id'] = response.headers['x-sup-id']

        extension = environ.get("extension")
        if extension:
            url = '/error/document/.%s?%s' % (extension, urllib.urlencode(d))
        else:
            url = '/error/document/?%s' % (urllib.urlencode(d))
        return url


class ProfilingMiddleware(object):
    def __init__(self, app, directory):
        self.app = app
        self.directory = directory

    def __call__(self, environ, start_response):
        import cProfile

        try:
            tmpfile = tempfile.NamedTemporaryFile(prefix='profile',
                                                  dir=self.directory,
                                                  delete=False)

            profile = cProfile.Profile()
            result = profile.runcall(self.app, environ, start_response)
            profile.dump_stats(tmpfile.name)

            return result
        finally:
            tmpfile.close()


class DomainMiddleware(object):
    lang_re = re.compile(r"\A\w\w(-\w\w)?\Z")

    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        g = config['pylons.g']
        http_host = environ.get('HTTP_HOST', 'localhost').lower()
        domain, s, port = http_host.partition(':')

        # remember the port
        try:
            environ['request_port'] = int(port)
        except ValueError:
            pass

        # localhost is exempt so paster run/shell will work
        # media_domain doesn't need special processing since it's just ads
        if domain == "localhost" or is_subdomain(domain, g.media_domain):
            return self.app(environ, start_response)

        # tell reddit_base to redirect to the appropriate subreddit for
        # a legacy CNAME
        if not is_subdomain(domain, g.domain):
            environ['legacy-cname'] = domain
            return self.app(environ, start_response)

        # figure out what subdomain we're on if any
        subdomains = domain[:-len(g.domain) - 1].split('.')
        extension_subdomains = dict(m="mobile",
                                    i="compact",
                                    api="api",
                                    rss="rss",
                                    xml="xml",
                                    json="json")

        sr_redirect = None
        for subdomain in subdomains[:]:

            extension = extension_subdomains.get(subdomain)
            if subdomain in g.reserved_subdomains:
                environ['reddit-domain-prefix'] = subdomain
            elif extension:
                environ['reddit-domain-extension'] = extension
            elif self.lang_re.match(subdomain):
                environ['reddit-prefer-lang'] = subdomain
                environ['reddit-domain-prefix'] = subdomain
            else:
                sr_redirect = subdomain
                subdomains.remove(subdomain)

        # if there was a subreddit subdomain, redirect
        if sr_redirect and environ.get("FULLPATH"):
            if not subdomains and g.domain_prefix:
                subdomains.append(g.domain_prefix)
            subdomains.append(g.domain)
            redir = "%s/r/%s/%s" % ('.'.join(subdomains),
                                    sr_redirect, environ['FULLPATH'])
            redir = "http://" + redir.replace('//', '/')

            start_response("301 Moved Permanently", [("Location", redir)])
            return [""]

        return self.app(environ, start_response)


class SubredditMiddleware(object):
    sr_pattern = re.compile(r'^/r/([^/]{2,})')

    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        path = environ['PATH_INFO']
        sr = self.sr_pattern.match(path)
        if sr:
            environ['subreddit'] = sr.groups()[0]
            environ['PATH_INFO'] = self.sr_pattern.sub('', path) or '/'
        elif path.startswith(('/subreddits', '/reddits')):
            environ['subreddit'] = 'r'
        return self.app(environ, start_response)

class DomainListingMiddleware(object):
    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        if not environ.has_key('subreddit'):
            path = environ['PATH_INFO']
            domain, rest = path_info_split(path)
            if domain == "domain" and rest:
                domain, rest = path_info_split(rest)
                environ['domain'] = domain
                environ['PATH_INFO'] = rest or '/'
        return self.app(environ, start_response)

class ExtensionMiddleware(object):
    ext_pattern = re.compile(r'\.([^/]+)\Z')
    
    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        path = environ['PATH_INFO']
        fname, sep, path_ext = path.rpartition('.')
        domain_ext = environ.get('reddit-domain-extension')

        ext = None
        if path_ext in extension_mapping:
            ext = path_ext
            # Strip off the extension.
            environ['PATH_INFO'] = path[:-(len(ext) + 1)]
        elif domain_ext in extension_mapping:
            ext = domain_ext

        if ext:
            set_extension(environ, ext)
        else:
            environ['render_style'] = 'html'
            environ['content_type'] = 'text/html; charset=UTF-8'

        return self.app(environ, start_response)

class FullPathMiddleware(object):
    # Debt: we have a lot of middleware which (unfortunately) modify the
    # global URL PATH_INFO string. To work with the original request URL, we
    # save it to a different location here.
    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        environ['FULLPATH'] = environ.get('PATH_INFO')
        qs = environ.get('QUERY_STRING')
        if qs:
            environ['FULLPATH'] += '?' + qs
        return self.app(environ, start_response)

class StaticTestMiddleware(object):
    def __init__(self, app, static_path, domain):
        self.app = app
        self.static_path = static_path
        self.domain = domain

    def __call__(self, environ, start_response):
        if environ['HTTP_HOST'] == self.domain:
            environ['PATH_INFO'] = self.static_path.rstrip('/') + environ['PATH_INFO']
            return self.app(environ, start_response)
        raise webob.exc.HTTPNotFound()

class LimitUploadSize(object):
    """
    Middleware for restricting the size of uploaded files (such as
    image files for the CSS editing capability).
    """
    def __init__(self, app, max_size=1024*500):
        self.app = app
        self.max_size = max_size

    def __call__(self, environ, start_response):
        cl_key = 'CONTENT_LENGTH'
        is_error = environ.get("pylons.error_call", False)
        if not is_error and environ['REQUEST_METHOD'] == 'POST':
            if cl_key not in environ:
                start_response("411 Length Required", [])
                return ['<html><body>length required</body></html>']

            try:
                cl_int = int(environ[cl_key])
            except ValueError:
                start_response("400 Bad Request", [])
                return ['<html><body>bad request</body></html>']

            if cl_int > self.max_size:
                error_msg = "too big. keep it under %d KiB" % (
                    self.max_size / 1024)
                start_response("413 Too Big", [])
                return ["<html>"
                        "<head>"
                        "<script type='text/javascript'>"
                        "parent.completedUploadImage('failed',"
                        "'',"
                        "'',"
                        "[['BAD_CSS_NAME', ''], ['IMAGE_ERROR', '", error_msg,"']],"
                        "'image-upload');"
                        "</script></head><body>you shouldn\'t be here</body></html>"]

        return self.app(environ, start_response)

# TODO CleanupMiddleware seems to exist because cookie headers are being duplicated
# somewhere in the response processing chain. It should be removed as soon as we
# find the underlying issue.
class CleanupMiddleware(object):
    """
    Put anything here that should be called after every other bit of
    middleware. This currently includes the code for removing
    duplicate headers (such as multiple cookie setting).  The behavior
    here is to disregard all but the last record.
    """
    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        def custom_start_response(status, headers, exc_info = None):
            fixed = []
            seen = set()
            for head, val in reversed(headers):
                head = head.lower()
                key = (head, val.split("=", 1)[0])
                if key not in seen:
                    fixed.insert(0, (head, val))
                    seen.add(key)
            return start_response(status, fixed, exc_info)
        return self.app(environ, custom_start_response)


class SafetyMiddleware(object):
    """Clean up any attempts at response splitting in headers."""

    has_bad_characters = re.compile("[\r\n]")
    sanitizer = re.compile("[\r\n]+[ \t]*")

    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        def safe_start_response(status, headers, exc_info=None):
            sanitized = []
            for name, value in headers:
                if self.has_bad_characters.search(value):
                    value = self.sanitizer.sub("", value)
                sanitized.append((name, value))
            return start_response(status, sanitized, exc_info)
        return self.app(environ, safe_start_response)


class RedditApp(PylonsApp):
    def __init__(self, *args, **kwargs):
        super(RedditApp, self).__init__(*args, **kwargs)
        self._loading_lock = Lock()
        self._controllers = None

    def setup_app_env(self, environ, start_response):
        PylonsApp.setup_app_env(self, environ, start_response)
        self.load_controllers()

    def load_controllers(self):
        if self._controllers:
            return

        with self._loading_lock:
            if self._controllers:
                return

            controllers = importlib.import_module(self.package_name +
                                                  '.controllers')
            controllers.load_controllers()
            config['r2.plugins'].load_controllers()
            self._controllers = controllers

    def find_controller(self, controller_name):
        if controller_name in self.controller_classes:
            return self.controller_classes[controller_name]

        controller_cls = self._controllers.get_controller(controller_name)
        self.controller_classes[controller_name] = controller_cls
        return controller_cls

def make_app(global_conf, full_stack=True, **app_conf):
    """Create a Pylons WSGI application and return it

    `global_conf`
        The inherited configuration for this application. Normally from the
        [DEFAULT] section of the Paste ini file.

    `full_stack`
        Whether or not this application provides a full WSGI stack (by default,
        meaning it handles its own exceptions and errors). Disable full_stack
        when this application is "managed" by another WSGI middleware.

    `app_conf`
        The application's local configuration. Normally specified in the
        [app:<name>] section of the Paste ini file (where <name> defaults to
        main).
    """

    # Configure the Pylons environment
    load_environment(global_conf, app_conf)
    g = config['pylons.g']

    # The Pylons WSGI app
    app = RedditApp()
    app = RoutesMiddleware(app, config["routes.map"])

    # CUSTOM MIDDLEWARE HERE (filtered by the error handling middlewares)

    # last thing first from here down
    app = CleanupMiddleware(app)

    app = LimitUploadSize(app)

    profile_directory = g.config.get('profile_directory')
    if profile_directory:
        app = ProfilingMiddleware(app, profile_directory)

    app = DomainListingMiddleware(app)
    app = SubredditMiddleware(app)
    app = ExtensionMiddleware(app)
    app = DomainMiddleware(app)

    if asbool(full_stack):
        # Handle Python exceptions
        app = ErrorHandler(app, global_conf, **config['pylons.errorware'])

        # Display error documents for 401, 403, 404 status codes (and 500 when
        # debug is disabled)
        app = ErrorDocuments(app, global_conf, mapper=error_mapper, **app_conf)

    # Establish the Registry for this application
    app = RegistryManager(app)

    # Static files
    static_app = StaticURLParser(config['pylons.paths']['static_files'])
    static_cascade = [static_app, app]

    if config['r2.plugins'] and g.config['uncompressedJS']:
        plugin_static_apps = Cascade([StaticURLParser(plugin.static_dir)
                                      for plugin in config['r2.plugins']])
        static_cascade.insert(0, plugin_static_apps)
    app = Cascade(static_cascade)

    app = FullPathMiddleware(app)

    if not g.config['uncompressedJS'] and g.config['debug']:
        static_fallback = StaticTestMiddleware(static_app, g.config['static_path'], g.config['static_domain'])
        app = Cascade([static_fallback, app])

    app = SafetyMiddleware(app)

    return app

########NEW FILE########
__FILENAME__ = queues
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.utils import tup


__all__ = ["MessageQueue", "declare_queues"]


class Queues(dict):
    """A container for queue declarations."""
    def __init__(self, queues):
        dict.__init__(self)
        self.__dict__ = self
        self.bindings = set()
        self.declare(queues)

    def __iter__(self):
        for name, queue in self.iteritems():
            if name != "bindings":
                yield queue

    def declare(self, queues):
        for name, queue in queues.iteritems():
            queue.name = name
            queue.bindings = self.bindings
            if queue.bind_to_self:
                queue._bind(name)
        self.update(queues)


class MessageQueue(object):
    """A representation of an AMQP message queue.

    This class is solely intended for use with the Queues class above.

    """
    def __init__(self, durable=True, exclusive=False,
                 auto_delete=False, bind_to_self=False):
        self.durable = durable
        self.exclusive = exclusive
        self.auto_delete = auto_delete
        self.bind_to_self = bind_to_self

    def _bind(self, routing_key):
        self.bindings.add((self.name, routing_key))

    def __lshift__(self, routing_keys):
        """Register bindings from routing keys to this queue."""
        routing_keys = tup(routing_keys)
        for routing_key in routing_keys:
            self._bind(routing_key)


def declare_queues(g):
    queues = Queues({
        "scraper_q": MessageQueue(bind_to_self=True),
        "newcomments_q": MessageQueue(),
        "commentstree_q": MessageQueue(bind_to_self=True),
        "commentstree_fastlane_q": MessageQueue(bind_to_self=True),
        "vote_link_q": MessageQueue(bind_to_self=True),
        "vote_comment_q": MessageQueue(bind_to_self=True),
        "vote_fastlane_q": MessageQueue(bind_to_self=True),
        "log_q": MessageQueue(bind_to_self=True),
        "cloudsearch_changes": MessageQueue(bind_to_self=True),
        "butler_q": MessageQueue(),
    })

    if g.shard_link_vote_queues:
        sharded_vote_queues = {"vote_link_%d_q" % i :
                               MessageQueue(bind_to_self=True)
                               for i in xrange(10)}
        queues.declare(sharded_vote_queues)

    if g.shard_commentstree_queues:
        sharded_commentstree_queues = {"commentstree_%d_q" % i :
                                       MessageQueue(bind_to_self=True)
                                       for i in xrange(10)}
        queues.declare(sharded_commentstree_queues)

    queues.cloudsearch_changes << "search_changes"
    queues.scraper_q << "new_link"
    queues.newcomments_q << "new_comment"
    queues.butler_q << ("new_comment",
                        "usertext_edited")
    return queues

########NEW FILE########
__FILENAME__ = routing
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

"""
Setup your Routes options here
"""
from routes import Mapper
from pylons import config


def not_in_sr(environ, results):
    return ('subreddit' not in environ and
            'sub_domain' not in environ and
            'domain' not in environ)


# FIXME: submappers with path prefixes are broken in Routes 1.11. Once we
# upgrade, we should be able to replace this ugliness with submappers.
def partial_connect(mc, **override_args):
    def connect(path, **kwargs):
        if 'path_prefix' in override_args:
            path = override_args['path_prefix'] + path
        kwargs.update(override_args)
        mc(path, **kwargs)
    return connect


def make_map():
    map = Mapper()
    mc = map.connect

    for plugin in reversed(config['r2.plugins']):
        plugin.add_routes(mc)

    mc('/admin/', controller='awards')

    mc('/robots.txt', controller='robots', action='robots')

    mc('/login', controller='forms', action='login')
    mc('/register', controller='forms', action='register')
    mc('/logout', controller='forms', action='logout')
    mc('/verify', controller='forms', action='verify')
    mc('/adminon', controller='forms', action='adminon')
    mc('/adminoff', controller='forms', action='adminoff')
    mc('/submit', controller='front', action='submit')

    mc('/over18', controller='post', action='over18')

    mc('/rules', controller='front', action='rules')
    mc('/sup', controller='front', action='sup')
    mc('/traffic', controller='front', action='site_traffic')
    mc('/traffic/languages/:langcode', controller='front',
       action='lang_traffic', langcode='')
    mc('/traffic/adverts/:code', controller='front',
       action='advert_traffic', code='')
    mc('/traffic/subreddits/report', controller='front',
       action='subreddit_traffic_report')
    mc('/account-activity', controller='front', action='account_activity')

    mc('/subreddits/create', controller='front', action='newreddit')
    mc('/subreddits/search', controller='front', action='search_reddits')
    mc('/subreddits/login', controller='forms', action='login')
    mc('/subreddits/:where', controller='reddits', action='listing',
       where='popular', requirements=dict(where="popular|new|banned"))

    mc('/subreddits/mine/:where', controller='myreddits', action='listing',
       where='subscriber',
       requirements=dict(where='subscriber|contributor|moderator'))

    # These routes are kept for backwards-compatibility reasons
    # Using the above /subreddits/ ones instead is preferable
    mc('/reddits/create', controller='front', action='newreddit')
    mc('/reddits/search', controller='front', action='search_reddits')
    mc('/reddits/login', controller='forms', action='login')
    mc('/reddits/:where', controller='reddits', action='listing',
       where='popular', requirements=dict(where="popular|new|banned"))

    mc('/reddits/mine/:where', controller='myreddits', action='listing',
       where='subscriber',
       requirements=dict(where='subscriber|contributor|moderator'))

    mc('/buttons', controller='buttons', action='button_demo_page')

    #/button.js and buttonlite.js - the embeds
    mc('/button', controller='buttons', action='button_embed')
    mc('/buttonlite', controller='buttons', action='button_lite')

    mc('/widget', controller='buttons', action='widget_demo_page')
    mc('/bookmarklets', controller='buttons', action='bookmarklets')

    mc('/awards', controller='front', action='awards')
    mc('/awards/confirm/:code', controller='front',
       action='confirm_award_claim')
    mc('/awards/claim/:code', controller='front', action='claim_award')
    mc('/awards/received', controller='front', action='received_award')

    mc('/i18n', controller='redirect', action='redirect',
       dest='http://www.reddit.com/r/i18n')
    mc('/feedback', controller='redirect', action='redirect',
       dest='/contact')
    mc('/contact', controller='front', action='contact_us')
    mc('/ad_inq', controller='redirect', action='redirect',
       dest='/advertising')
    mc('/advertising', controller='front', action='advertising')

    mc('/admin/awards', controller='awards')
    mc('/admin/awards/:awardcn/:action', controller='awards',
       requirements=dict(action="give|winners"))

    mc('/admin/errors', controller='errorlog')

    mc('/user/:username/about', controller='user', action='about',
       where='overview')
    mc('/user/:username/:where', controller='user', action='listing',
       where='overview')
    mc('/user/:username/saved/:category', controller='user', action='listing',
       where='saved')

    multi_prefixes = (
       partial_connect(mc, path_prefix='/user/:username/m/:multipath'),
       partial_connect(mc, path_prefix='/me/m/:multipath', my_multi=True),
    )

    for connect in multi_prefixes:
       connect('/', controller='hot', action='listing')
       connect('/submit', controller='front', action='submit')
       connect('/:sort', controller='browse', sort='top',
          action='listing', requirements=dict(sort='top|controversial'))
       connect('/:controller', action='listing',
          requirements=dict(controller="hot|new|rising|randomrising|ads"))

    mc('/user/:username/:where/:show', controller='user', action='listing')
    
    mc('/explore', controller='front', action='explore')
    mc('/api/recommend/feedback', controller='api', action='rec_feedback')

    mc('/about/sidebar', controller='front', action='sidebar')
    mc('/about/sticky', controller='front', action='sticky')
    mc('/about/flair', controller='front', action='flairlisting')
    mc('/about', controller='front', action='about')
    for connect in (mc,) + multi_prefixes:
       connect('/about/message/:where', controller='message',
          action='listing')
       connect('/about/log', controller='front', action='moderationlog')
       connect('/about/:location', controller='front',
          action='spamlisting',
          requirements=dict(location='reports|spam|modqueue|unmoderated'))
       connect('/about/:where', controller='userlistlisting',
          requirements=dict(where='contributors|banned|wikibanned|wikicontributors|moderators'),
          action='listing')
       connect('/about/:location', controller='front', action='editreddit',
          location='about')
       connect('/comments', controller='comments', action='listing')
       connect('/comments/gilded', action='listing', controller='gilded')
       connect('/gilded', action='listing', controller='gilded')
       connect('/search', controller='front', action='search')

    mc('/u/:username', controller='redirect', action='user_redirect')
    mc('/u/:username/*rest', controller='redirect', action='user_redirect')

    # preserve timereddit URLs from 4/1/2012
    mc('/t/:timereddit', controller='redirect', action='timereddit_redirect')
    mc('/t/:timereddit/*rest', controller='redirect',
       action='timereddit_redirect')

    # /prefs/friends is also aliased to /api/v1/me/friends
    mc('/prefs/:where', controller='userlistlisting',
        action='user_prefs', requirements=dict(where='blocked|friends'))
    mc('/prefs/:location', controller='forms', action='prefs',
       location='options')

    mc('/info/0:article/*rest', controller='front',
       action='oldinfo', dest='comments', type='ancient')
    mc('/info/:article/:dest/:comment', controller='front',
       action='oldinfo', type='old', dest='comments', comment=None)


    mc('/related/:article/:title', controller='front',
       action='related', title=None)
    mc('/details/:article/:title', controller='front',
       action='details', title=None)
    mc('/traffic/:link/:campaign', controller='front', action='traffic',
       campaign=None)
    mc('/comments/:article/:title/:comment', controller='front',
       action='comments', title=None, comment=None)
    mc('/duplicates/:article/:title', controller='front',
       action='duplicates', title=None)

    mc('/mail/optout', controller='forms', action='optout')
    mc('/mail/optin', controller='forms', action='optin')
    mc('/stylesheet', controller='front', action='stylesheet')
    mc('/frame', controller='front', action='frame')
    mc('/framebuster/:blah', controller='front', action='framebuster')
    mc('/framebuster/:what/:blah',
       controller='front', action='framebuster')

    mc('/promoted/report', controller='promote', action='report')
    mc('/promoted/live_promos/:sr', controller='promotelisting',
       sort='live_promos', action='listing')
    mc('/promoted/:sort', controller='promotelisting', action="listing",
       requirements=dict(sort="future_promos|pending_promos|unpaid_promos|"
                              "rejected_promos|live_promos|underdelivered|"
                              "reported|house|all"))
    mc('/promoted/', controller='promotelisting', action="listing")

    mc('/promoted/edit_promo/:link',
       controller='promote', action='edit_promo')
    mc('/promoted/edit_promo/pc/:campaign', controller='promote',  # admin only
       action='edit_promo_campaign')
    mc('/promoted/pay/:link/:campaign',
       controller='promote', action='pay')
    mc('/promoted/refund/:link/:campaign', controller='promote',
       action='refund')
    mc('/promoted/inventory', controller='promote', action='promote_inventory')
    mc('/promoted/:action', controller='promote',
       requirements=dict(action="edit_promo|new_promo|roadblock"))

    mc('/health', controller='health', action='health')
    mc('/health/ads', controller='health', action='promohealth')
    mc('/health/caches', controller='health', action='cachehealth')

    mc('/', controller='hot', action='listing')

    mc('/:controller', action='listing',
       requirements=dict(controller="hot|new|rising|randomrising|ads"))
    mc('/saved', controller='user', action='saved_redirect')

    mc('/by_id/:names', controller='byId', action='listing')

    mc('/:sort', controller='browse', sort='top', action='listing',
       requirements=dict(sort='top|controversial'))

    mc('/message/compose', controller='message', action='compose')
    mc('/message/messages/:mid', controller='message', action='listing',
       where="messages")
    mc('/message/:where', controller='message', action='listing')
    mc('/message/moderator/:subwhere', controller='message', action='listing',
       where='moderator')

    mc('/thanks', controller='forms', action="claim", secret='')
    mc('/thanks/:secret', controller='forms', action="claim")

    mc('/gold', controller='forms', action="gold")
    mc('/gold/creditgild/:passthrough', controller='forms', action='creditgild')
    mc('/gold/thanks', controller='front', action='goldthanks')
    mc('/gold/subscription', controller='forms', action='subscription')

    mc('/password', controller='forms', action="password")
    mc('/:action', controller='front',
       requirements=dict(action="random|framebuster"))
    mc('/:action', controller='embed',
       requirements=dict(action="blog"))
    mc('/help/gold', controller='redirect', action='redirect',
       dest='/gold/about')

    mc('/help/:page', controller='policies', action='policy_page',
       conditions={'function':not_in_sr},
       requirements={'page':'privacypolicy|useragreement'})

    mc('/wiki/create/*page', controller='wiki', action='wiki_create')
    mc('/wiki/edit/*page', controller='wiki', action='wiki_revise')
    mc('/wiki/revisions', controller='wiki', action='wiki_recent')
    mc('/wiki/revisions/*page', controller='wiki', action='wiki_revisions')
    mc('/wiki/settings/*page', controller='wiki', action='wiki_settings')
    mc('/wiki/discussions/*page', controller='wiki', action='wiki_discussions')
    mc('/wiki/pages', controller='wiki', action='wiki_listing')

    mc('/api/wiki/edit', controller='wikiapi', action='wiki_edit')
    mc('/api/wiki/hide', controller='wikiapi', action='wiki_revision_hide')
    mc('/api/wiki/delete', controller='wikiapi', action='wiki_revision_delete')
    mc('/api/wiki/revert', controller='wikiapi', action='wiki_revision_revert')
    mc('/api/wiki/alloweditor/:act', controller='wikiapi',
       requirements=dict(act="del|add"), action='wiki_allow_editor')

    mc('/wiki/*page', controller='wiki', action='wiki_page')
    mc('/wiki/', controller='wiki', action='wiki_page')

    mc('/:action', controller='wiki', requirements=dict(action="help|faq"))
    mc('/help/*page', controller='wiki', action='wiki_redirect')
    mc('/w/*page', controller='wiki', action='wiki_redirect')

    mc('/goto', controller='toolbar', action='goto')
    mc('/tb/:id', controller='toolbar', action='tb')
    mc('/toolbar/:action', controller='toolbar',
       requirements=dict(action="toolbar|inner|login"))
    mc('/toolbar/comments/:id', controller='toolbar', action='comments')

    mc('/c/:comment_id', controller='front', action='comment_by_id')

    mc('/s/*urloid', controller='toolbar', action='s')
    # additional toolbar-related rules just above the catchall

    mc('/d/:what', controller='api', action='bookmarklet')

    mc('/resetpassword/:key', controller='forms',
       action='resetpassword')
    mc('/verification/:key', controller='forms',
       action='verify_email')
    mc('/resetpassword', controller='forms',
       action='resetpassword')

    mc('/post/:action/:url_user', controller='post',
       requirements=dict(action="login|reg"))
    mc('/post/:action', controller='post',
       requirements=dict(action="options|over18|unlogged_options|optout"
                         "|optin|login|reg|explore_settings"))

    mc('/api', controller='redirect', action='redirect', dest='/dev/api')
    mc('/api/distinguish/:how', controller='api', action="distinguish")
    mc('/api/spendcreddits', controller='ipn', action="spendcreddits")
    mc('/api/stripecharge/gold', controller='stripe', action='goldcharge')
    mc('/api/modify_subscription', controller='stripe',
       action='modify_subscription')
    mc('/api/cancel_subscription', controller='stripe',
       action='cancel_subscription')
    mc('/api/stripewebhook/gold/:secret', controller='stripe',
       action='goldwebhook')
    mc('/api/coinbasewebhook/gold/:secret', controller='coinbase',
       action='goldwebhook')
    mc('/api/rgwebhook/gold/:secret', controller='redditgifts',
       action='goldwebhook')
    mc('/api/ipn/:secret', controller='ipn', action='ipn')
    mc('/ipn/:secret', controller='ipn', action='ipn')
    mc('/api/:action/:url_user', controller='api',
       requirements=dict(action="login|register"))
    mc('/api/gadget/click/:ids', controller='api', action='gadget',
       type='click')
    mc('/api/gadget/:type', controller='api', action='gadget')
    mc('/api/:action', controller='promoteapi',
       requirements=dict(action=("promote|unpromote|edit_promo|link_thumb|"
                                 "freebie|promote_note|update_pay|"
                                 "edit_campaign|delete_campaign|"
                                 "add_roadblock|rm_roadblock|check_inventory|"
                                 "refund_campaign|terminate_campaign")))
    mc('/api/:action', controller='apiminimal',
       requirements=dict(action="new_captcha"))
    mc('/api/:type', controller='api',
       requirements=dict(type='wikibannednote|bannednote'),
       action='relnote')
    mc('/api/:action', controller='api')
    
    mc('/api/recommend/sr/:srnames', controller='api',
       action='subreddit_recommendations')

    mc('/api/server_seconds_visibility', controller='api',
       action='server_seconds_visibility')

    mc("/api/multi/mine", controller="multiapi", action="my_multis")
    mc("/api/multi/copy", controller="multiapi", action="multi_copy")
    mc("/api/multi/rename", controller="multiapi", action="multi_rename")
    mc("/api/multi/*multipath/r/:srname", controller="multiapi", action="multi_subreddit")
    mc("/api/multi/*multipath/description", controller="multiapi", action="multi_description")
    mc("/api/multi/*multipath", controller="multiapi", action="multi")

    mc("/api/v1/:action", controller="oauth2frontend",
       requirements=dict(action="authorize"))
    mc("/api/v1/:action", controller="oauth2access",
       requirements=dict(action="access_token"))
    mc("/api/v1/user/:username/trophies",
       controller="apiv1user", action="usertrophies")
    mc("/api/v1/:action", controller="apiv1user")
    # Same controller/action as /prefs/friends
    mc("/api/v1/me/:where", controller="userlistlisting",
        action="user_prefs", requirements=dict(where="friends"))
    mc("/api/v1/me/:action", controller="apiv1user")
    mc("/api/v1/me/:action/:username", controller="apiv1user")

    mc('/dev', controller='redirect', action='redirect', dest='/dev/api')
    mc('/dev/api', controller='apidocs', action='docs')
    mc('/dev/api/:mode', controller='apidocs', action='docs',
       requirements=dict(mode="oauth"))

    mc("/button_info", controller="api", action="info", limit=1)

    mc('/captcha/:iden', controller='captcha', action='captchaimg')

    mc('/mediaembed/:link/:credentials',
       controller="mediaembed", action="mediaembed", credentials=None)

    mc('/code', controller='redirect', action='redirect',
       dest='http://github.com/reddit/')

    mc('/socialite', controller='redirect', action='redirect',
       dest='https://addons.mozilla.org/firefox/addon/socialite/')

    mc('/mobile', controller='redirect', action='redirect',
       dest='http://m.reddit.com/')

    mc('/authorize_embed', controller='front', action='authorize_embed')

    # Used for showing ads
    mc("/ads/", controller="ad", action="ad")

    mc("/try", controller="forms", action="try_compact")

    mc("/web/log/:level", controller="weblog", action="message",
       requirements=dict(level="error"))

    # Obsolete content redirects
    mc("/selfserviceoatmeal", controller="redirect", dest="/ad_inq")

    # This route handles displaying the error page and
    # graphics used in the 404/500
    # error pages. It should likely stay at the top
    # to ensure that the error page is
    # displayed properly.
    mc('/error/document/:id', controller='error', action="document")

    # these should be near the buttom, because they should only kick
    # in if everything else fails. It's the attempted catch-all
    # reddit.com/http://... and reddit.com/34fr, but these redirect to
    # the less-guessy versions at /s/ and /tb/
    mc('/:linkoid', controller='toolbar', action='linkoid',
       requirements=dict(linkoid='[0-9a-z]{1,6}'))
    mc('/:urloid', controller='toolbar', action='s',
       requirements=dict(urloid=r'(\w+\.\w{2,}|https?).*'))

    mc("/*url", controller='front', action='catchall')

    return map

########NEW FILE########
__FILENAME__ = templates
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.manager import tp_manager
from r2.lib.jsontemplates import *

tpm = tp_manager.tp_manager()

def api(type, cls):
    tpm.add_handler(type, 'api', cls())
    tpm.add_handler(type, 'api-html', cls())
    tpm.add_handler(type, 'api-compact', cls())

# blanket fallback rule
api('templated', NullJsonTemplate)

# class specific overrides
api('link',          LinkJsonTemplate)
api('promotedlink',  PromotedLinkJsonTemplate)
api('comment',       CommentJsonTemplate)
api('message',       MessageJsonTemplate)
api('subreddit',     SubredditJsonTemplate)
api('labeledmulti',  LabeledMultiJsonTemplate)
api('morerecursion', MoreCommentJsonTemplate)
api('morechildren',  MoreCommentJsonTemplate)
api('reddit',        RedditJsonTemplate)
api('panestack',     PanestackJsonTemplate)
api('listing',       ListingJsonTemplate)
api('userlisting',   UserListingJsonTemplate)
api('usertableitem', UserTableItemJsonTemplate)
api('account',       AccountJsonTemplate)

api('reltableitem', RelTableItemJsonTemplate)
api('bannedtableitem', BannedTableItemJsonTemplate)
api('invitedmodtableitem', InvitedModTableItemJsonTemplate)
api('friendtableitem', FriendTableItemJsonTemplate)

api('organiclisting',       OrganicListingJsonTemplate)
api('subreddittraffic', TrafficJsonTemplate)
api('takedownpane', TakedownJsonTemplate)
api('policyview', PolicyViewJsonTemplate)

api('wikibasepage', WikiJsonTemplate)
api('wikipagerevisions', WikiJsonTemplate)
api('wikiview', WikiViewJsonTemplate)
api('wikirevision', WikiRevisionJsonTemplate)

api('wikipagelisting', WikiPageListingJsonTemplate)
api('wikipagediscussions', WikiJsonTemplate)
api('wikipagesettings', WikiSettingsJsonTemplate)

api('flairlist', FlairListJsonTemplate)
api('flaircsv', FlairCsvJsonTemplate)
api('flairselector', FlairSelectorJsonTemplate)

api('subredditstylesheet', StylesheetTemplate)
api('subredditstylesheetsource', StylesheetTemplate)
api('createsubreddit', SubredditSettingsTemplate)

api('modaction', ModActionTemplate)

api('trophy', TrophyJsonTemplate)

tpm.add_handler('usertableitem', 'api-html', UserItemHTMLJsonTemplate())
tpm.add_handler('reltableitem', 'api-html', UserItemHTMLJsonTemplate())
tpm.add_handler('bannedtableitem', 'api-html', UserItemHTMLJsonTemplate())
tpm.add_handler('invitedmodtableitem', 'api-html', UserItemHTMLJsonTemplate())
tpm.add_handler('friendtableitem', 'api-html', UserItemHTMLJsonTemplate())

########NEW FILE########
__FILENAME__ = api
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.controllers.reddit_base import (
    cross_domain,
    MinimalController,
    pagecache_policy,
    PAGECACHE_POLICY,
    paginated_listing,
    RedditController,
    set_user_cookie,
)

from pylons.i18n import _
from pylons import c, request, response

from r2.lib.validator import *

from r2.models import *

from r2.lib import amqp
from r2.lib import recommender
from r2.lib import hooks

from r2.lib.utils import get_title, sanitize_url, timeuntil, set_last_modified
from r2.lib.utils import query_string, timefromnow, randstr
from r2.lib.utils import timeago, tup
from r2.lib.pages import (BoringPage, FormPage, CssError, UploadedImage,
                          ClickGadget, UrlParser, WrappedUser)
from r2.lib.pages import FlairList, FlairCsv, FlairTemplateEditor, \
    FlairSelector
from r2.lib.pages import PrefApps
from r2.lib.pages import (
    BannedTableItem,
    ContributorTableItem,
    FriendTableItem,
    InvitedModTableItem,
    ModTableItem,
    SubredditStylesheet,
    WikiBannedTableItem,
    WikiMayContributeTableItem,
)

from r2.lib.pages.things import (
    default_thing_wrapper,
    hot_links_by_url_listing,
    wrap_links,
)
from r2.models.last_modified import LastModified

from r2.lib.menus import CommentSortMenu
from r2.lib.captcha import get_iden
from r2.lib.strings import strings
from r2.lib.filters import _force_unicode, websafe_json, websafe, spaceCompress
from r2.lib.db import queries
from r2.lib.db.queries import changed
from r2.lib import media
from r2.lib.db import tdb_cassandra
from r2.lib import promote
from r2.lib import tracking, emailer
from r2.lib.subreddit_search import search_reddits
from r2.lib.log import log_text
from r2.lib.filters import safemarkdown
from r2.lib.media import str_to_image
from r2.controllers.api_docs import api_doc, api_section
from r2.lib.search import SearchQuery
from r2.controllers.oauth2 import require_oauth2_scope, allow_oauth2_access
from r2.lib.template_helpers import add_sr, get_domain
from r2.lib.system_messages import notify_user_added
from r2.controllers.ipn import generate_blob
from r2.lib.lock import TimeoutExpired

from r2.models import wiki
from r2.models.recommend import AccountSRFeedback
from r2.lib.merge import ConflictException

import csv
from collections import defaultdict
from datetime import datetime, timedelta
from urlparse import urlparse
import hashlib
import re
import urllib
import urllib2

def reject_vote(thing):
    voteword = request.params.get('dir')

    if voteword == '1':
        voteword = 'upvote'
    elif voteword == '0':
        voteword = '0-vote'
    elif voteword == '-1':
        voteword = 'downvote'

    log_text ("rejected vote", "Rejected %s from %s (%s) on %s %s via %s" %
              (voteword, c.user.name, request.ip, thing.__class__.__name__,
               thing._id36, request.referer), "info")


class ApiminimalController(MinimalController):
    """
    Put API calls in here which don't rely on the user being logged in
    """

    # Since this is only a MinimalController, the
    # @allow_oauth2_access decorator has little effect other than
    # (1) to add the endpoint to /dev/api/oauth, and
    # (2) to future-proof in case the function moves elsewhere
    @allow_oauth2_access
    @validatedForm()
    @api_doc(api_section.captcha)
    def POST_new_captcha(self, form, jquery, *a, **kw):
        """
        Responds with an `iden` of a new CAPTCHA.

        Use this endpoint if a user cannot read a given CAPTCHA,
        and wishes to receive a new CAPTCHA.

        To request the CAPTCHA image for an iden, use
        [/captcha/`iden`](#GET_captcha_{iden}).
        """

        iden = get_iden()
        jquery("body").captcha(iden)
        form._send_data(iden = iden) 


class ApiController(RedditController):
    """
    Controller which deals with almost all AJAX site interaction.  
    """
    @validatedForm()
    def ajax_login_redirect(self, form, jquery, dest):
        form.redirect("/login" + query_string(dict(dest=dest)))

    @pagecache_policy(PAGECACHE_POLICY.NEVER)
    @require_oauth2_scope("read")
    @validate(url=VUrl('url'),
              link=VByName('id'),
              count = VLimit('limit'))
    @api_doc(api_section.links_and_comments, uses_site=True)
    def GET_info(self, url, link, count):
        """Get a link by fullname or a list of links by URL.

        If `id` is provided, the link with the given fullname will be returned.
        If `url` is provided, a list of links with the given URL will be
        returned.

        If both `url` and `id` are provided, `id` will take precedence.

        If a subreddit is provided, only links in that subreddit will be
        returned.

        """

        c.update_last_visit = False

        if link or not url:
            listing = wrap_links(link or [], num=count)
        else:
            listing = hot_links_by_url_listing(url, sr=c.site, num=count)
        return BoringPage(_("API"), content=listing).render()


    @json_validate()
    @api_doc(api_section.account, extensions=["json"])
    def GET_me(self, responder):
        """Get info about the currently authenticated user.

        Response includes a modhash, karma, and new mail status.

        """
        if c.user_is_loggedin:
            return Wrapped(c.user).render()
        else:
            return {}

    @json_validate(user=VUname(("user",)))
    @api_doc(api_section.users, extensions=["json"])
    def GET_username_available(self, responder, user):
        """
        Check whether a username is available for registration.
        """
        if not (responder.has_errors("user", errors.BAD_USERNAME)):
            return bool(user)

    @allow_oauth2_access
    @json_validate()
    @api_doc(api_section.captcha, extensions=["json"])
    def GET_needs_captcha(self, responder):
        """
        Check whether CAPTCHAs are needed for API methods that define the
        "captcha" and "iden" parameters.
        """
        return bool(c.user.needs_captcha())

    @validatedForm(VCaptcha(),
                   name=VRequired('name', errors.NO_NAME),
                   email=ValidEmails('email', num = 1),
                   reason = VOneOf('reason', ('ad_inq', 'feedback')),
                   message=VRequired('text', errors.NO_TEXT),
                   )
    def POST_feedback(self, form, jquery, name, email, reason, message):
        if not (form.has_errors('name',     errors.NO_NAME) or
                form.has_errors('email',    errors.BAD_EMAILS) or
                form.has_errors('text', errors.NO_TEXT) or
                form.has_errors('captcha', errors.BAD_CAPTCHA)):

            if reason == 'ad_inq':
                emailer.ad_inq_email(email, message, name, reply_to = '')
            else:
                emailer.feedback_email(email, message, name, reply_to = '')
            form.set_html(".status", _("thanks for your message! "
                            "you should hear back from us shortly."))
            form.set_inputs(text = "", captcha = "")
            form.find(".spacer").hide()
            form.find(".btn").hide()

    POST_ad_inq = POST_feedback

    @require_oauth2_scope("privatemessages")
    @validatedForm(VCaptcha(),
                   VUser(),
                   VModhash(),
                   ip = ValidIP(),
                   to = VMessageRecipient('to'),
                   subject = VLength('subject', 100, empty_error=errors.NO_SUBJECT),
                   body = VMarkdown(['text', 'message']))
    @api_doc(api_section.messages)
    def POST_compose(self, form, jquery, to, subject, body, ip):
        """
        Handles message composition under /message/compose.
        """
        if not (form.has_errors("to",  errors.USER_DOESNT_EXIST,
                                errors.NO_USER, errors.SUBREDDIT_NOEXIST,
                                errors.USER_BLOCKED) or
                form.has_errors("subject", errors.NO_SUBJECT) or
                form.has_errors("subject", errors.TOO_LONG) or
                form.has_errors("text", errors.NO_TEXT, errors.TOO_LONG) or
                form.has_errors("captcha", errors.BAD_CAPTCHA)):

            m, inbox_rel = Message._new(c.user, to, subject, body, ip)
            form.set_html(".status", _("your message has been delivered"))
            form.set_inputs(to = "", subject = "", text = "", captcha="")

            queries.new_message(m, inbox_rel)

    @require_oauth2_scope("submit")
    @json_validate()
    @api_doc(api_section.subreddits, uses_site=True, extensions=["json"])
    def GET_submit_text(self, responder):
        """Get the submission text for the subreddit.

        This text is set by the subreddit moderators and intended to be
        displayed on the submission form.

        See also: [/api/site_admin](#POST_api_site_admin).

        """
        if c.site.over_18 and not c.over18:
            submit_text = None
            submit_text_html = None
        else:
            submit_text = c.site.submit_text
            submit_text_html = safemarkdown(c.site.submit_text)
        return {'submit_text': submit_text,
                'submit_text_html': submit_text_html}

    @require_oauth2_scope("submit")
    @validatedForm(VUser(),
                   VModhash(),
                   VCaptcha(),
                   VRatelimit(rate_user = True, rate_ip = True,
                              prefix = "rate_submit_"),
                   VShamedDomain('url'),
                   ip = ValidIP(),
                   sr = VSubmitSR('sr', 'kind'),
                   url = VUrl('url'),
                   title = VTitle('title'),
                   save = VBoolean('save'),
                   sendreplies = VBoolean('sendreplies'),
                   selftext = VSelfText('text'),
                   kind = VOneOf('kind', ['link', 'self']),
                   then = VOneOf('then', ('tb', 'comments'),
                                 default='comments'),
                   extension=VLength("extension", 20, docs={"extension":
                       "extension used for redirects"}),
                   resubmit=VBoolean('resubmit'),
                  )
    @api_doc(api_section.links_and_comments)
    def POST_submit(self, form, jquery, url, selftext, kind, title,
                    save, sr, ip, then, extension, sendreplies, resubmit):
        """Submit a link to a subreddit.

        Submit will create a link or self-post in the subreddit `sr` with the
        title `title`. If `kind` is `"link"`, then `url` is expected to be a
        valid URL to link to. Otherwise, `text`, if present, will be the
        body of the self-post.

        If a link with the same URL has already been submitted to the specified
        subreddit an error will be returned unless `resubmit` is true.
        `extension` is used for determining which view-type (e.g. `json`,
        `compact` etc.) to use for the redirect that is generated if the
        `resubmit` error occurs.

        If `save` is true, the link will be implicitly saved after submission
        (see [/api/save](#POST_api_save) for more information).

        """

        from r2.models.admintools import is_banned_domain

        if url:
            if url.lower() == 'self':
                url = kind = 'self'

            # VUrl may have replaced 'url' by adding 'http://'
            form.set_inputs(url = url)

        if not kind or form.has_errors('sr', errors.INVALID_OPTION):
            # this should only happen if somebody is trying to post
            # links in some automated manner outside of the regular
            # submission page, and hasn't updated their script
            return

        if form.has_errors('captcha', errors.BAD_CAPTCHA):
            return

        if (form.has_errors('sr',
                            errors.SUBREDDIT_NOEXIST,
                            errors.SUBREDDIT_NOTALLOWED,
                            errors.SUBREDDIT_REQUIRED,
                            errors.NO_SELFS,
                            errors.NO_LINKS)
            or not sr):
            # checking to get the error set in the form, but we can't
            # check for rate-limiting if there's no subreddit
            return

        if not sr.can_submit_text(c.user) and kind == "self":
            # this could happen if they actually typed "self" into the
            # URL box and we helpfully translated it for them
            c.errors.add(errors.NO_SELFS, field='sr')

            # and trigger that by hand for the form
            form.has_errors('sr', errors.NO_SELFS)

            return

        should_ratelimit = sr.should_ratelimit(c.user, 'link')
        #remove the ratelimit error if the user's karma is high
        if not should_ratelimit:
            c.errors.remove((errors.RATELIMIT, 'ratelimit'))

        ban = None

        if kind == 'link':
            check_domain = True

            # check for no url, or clear that error field on return
            if form.has_errors("url", errors.NO_URL, errors.BAD_URL):
                pass
            elif form.has_errors("url", errors.DOMAIN_BANNED):
                g.stats.simple_event('spam.shame.link')
            elif not resubmit:
                listing = hot_links_by_url_listing(url, sr=sr, num=1)
                links = listing.things
                if links:
                    c.errors.add(errors.ALREADY_SUB, field='url')
                    form.has_errors('url', errors.ALREADY_SUB)
                    check_domain = False
                    u = links[0].already_submitted_link
                    if extension:
                        u = UrlParser(u)
                        u.set_extension(extension)
                        u = u.unparse()
                    form.redirect(u)
            # check for title, otherwise look it up and return it
            elif form.has_errors("title", errors.NO_TEXT):
                pass

            if url is None:
                g.log.warning("%s is trying to submit url=None (title: %r)"
                              % (request.ip, title))
            elif check_domain:
                ban = is_banned_domain(url)
        else:
            form.has_errors('text', errors.TOO_LONG)

        if form.has_errors("title", errors.TOO_LONG, errors.NO_TEXT):
            pass

        if form.has_errors('ratelimit', errors.RATELIMIT):
            pass

        if form.has_error() or not title:
            return

        if should_ratelimit:
            filled_quota = c.user.quota_full('link')
            if filled_quota is not None:
                if c.user._spam:
                    msg = strings.generic_quota_msg
                else:
                    log_text ("over-quota",
                              "%s just went over their per-%s quota" %
                              (c.user.name, filled_quota), "info")

                    verify_link = "/verify?reason=submit"
                    reddiquette_link = "/wiki/reddiquette" 

                    if c.user.email_verified:
                        msg = strings.verified_quota_msg % dict(reddiquette=reddiquette_link)
                    else:
                        msg = strings.unverified_quota_msg % dict(verify=verify_link,
                                                                  reddiquette=reddiquette_link)

                md = safemarkdown(msg)
                form.set_html(".status", md)
                c.errors.add(errors.QUOTA_FILLED)
                form.set_error(errors.QUOTA_FILLED, None)
                return

        if not request.POST.get('sendreplies'):
            sendreplies = kind == 'self'

        # get rid of extraneous whitespace in the title
        cleaned_title = re.sub(r'\s+', ' ', title, flags=re.UNICODE)
        cleaned_title = cleaned_title.strip()

        # well, nothing left to do but submit it
        l = Link._submit(cleaned_title, url if kind == 'link' else 'self',
                         c.user, sr, ip, spam=c.user._spam, sendreplies=sendreplies)

        if ban:
            g.stats.simple_event('spam.domainban.link_url')
            admintools.spam(l, banner = "domain (%s)" % ban.banmsg)
            hooks.get_hook('banned_domain.submit').call(item=l, url=url,
                                                        ban=ban)

        if kind == 'self':
            l.url = l.make_permalink_slow()
            l.is_self = True
            l.selftext = selftext

            l._commit()
            l.set_url_cache()

        queries.queue_vote(c.user, l, True, ip, cheater=c.cheater)
        if save:
            l._save(c.user)

        #set the ratelimiter
        if should_ratelimit:
            c.user.clog_quota('link', l)
            VRatelimit.ratelimit(rate_user=True, rate_ip = True,
                                 prefix = "rate_submit_")

        #update the queries
        queries.new_link(l)
        changed(l)

        if then == 'comments':
            path = add_sr(l.make_permalink_slow())
        elif then == 'tb':
            form.attr('target', '_top')
            path = add_sr('/tb/%s' % l._id36)
        if extension:
            path += ".%s" % extension
        form.redirect(path)
        form._send_data(url=path)
        form._send_data(id=l._id36)
        form._send_data(name=l._fullname)

    @validatedForm(VRatelimit(rate_ip = True,
                              rate_user = True,
                              prefix = 'fetchtitle_'),
                   VUser(),
                   url = VSanitizedUrl('url'))
    def POST_fetch_title(self, form, jquery, url):
        if form.has_errors('ratelimit', errors.RATELIMIT):
            form.set_html(".title-status", "");
            return

        VRatelimit.ratelimit(rate_ip = True, rate_user = True,
                             prefix = 'fetchtitle_', seconds=1)
        if url:
            title = get_title(url)
            if title:
                form.set_inputs(title = title)
                form.set_html(".title-status", "");
            else:
                form.set_html(".title-status", _("no title found"))
        
    def _login(self, responder, user, rem = None):
        """
        AJAX login handler, used by both login and register to set the
        user cookie and send back a redirect.
        """
        self.login(user, rem = rem)

        if request.params.get("hoist") != "cookie":
            responder._send_data(modhash = user.modhash())
            responder._send_data(cookie  = user.make_cookie())

    @validatedForm(VLoggedOut(),
                   user = VThrottledLogin(['user', 'passwd']),
                   rem = VBoolean('rem'))
    def _handle_login(self, form, responder, user, rem):
        exempt_ua = (request.user_agent and
                     any(ua in request.user_agent for ua
                         in g.config.get('exempt_login_user_agents', ())))
        if (errors.LOGGED_IN, None) in c.errors:
            if user == c.user or exempt_ua:
                # Allow funky clients to re-login as the current user.
                c.errors.remove((errors.LOGGED_IN, None))
            else:
                from r2.lib.base import abort
                from r2.lib.errors import reddit_http_error
                abort(reddit_http_error(409, errors.LOGGED_IN))

        if not (responder.has_errors("vdelay", errors.RATELIMIT) or
                responder.has_errors("passwd", errors.WRONG_PASSWORD)):
            self._login(responder, user, rem)

    @cross_domain(allow_credentials=True)
    @api_doc(api_section.account, extends=_handle_login)
    def POST_login(self, *args, **kwargs):
        """Log into an account.

        `rem` specifies whether or not the session cookie returned should last
        beyond the current browser session (that is, if `rem` is `True` the
        cookie will have an explicit expiration far in the future indicating
        that it is not a session cookie).

        """
        return self._handle_login(*args, **kwargs)

    @validatedForm(VCaptcha(),
                   VRatelimit(rate_ip = True, prefix = "rate_register_"),
                   name = VUname(['user']),
                   email=ValidEmails(
                       "email",
                       num=1,
                       docs={
                           "email": "(optional) the user's email address",
                       },
                   ),
                   password = VPassword(['passwd', 'passwd2']),
                   rem = VBoolean('rem'))
    def _handle_register(self, form, responder, name, email,
                      password, rem):
        bad_captcha = responder.has_errors('captcha', errors.BAD_CAPTCHA)
        if not (responder.has_errors("user", errors.BAD_USERNAME,
                                errors.USERNAME_TAKEN_DEL,
                                errors.USERNAME_TAKEN) or
                responder.has_errors("email", errors.BAD_EMAILS) or
                responder.has_errors("passwd", errors.BAD_PASSWORD) or
                responder.has_errors("passwd2", errors.BAD_PASSWORD_MATCH) or
                responder.has_errors('ratelimit', errors.RATELIMIT) or
                (not g.disable_captcha and bad_captcha)):
            
            user = register(name, password, request.ip)
            VRatelimit.ratelimit(rate_ip = True, prefix = "rate_register_")

            #anything else we know (email, languages)?
            if email:
                user.email = email

            user.pref_lang = c.lang
            if c.content_langs == 'all':
                user.pref_content_langs = 'all'
            else:
                langs = list(c.content_langs)
                langs.sort()
                user.pref_content_langs = tuple(langs)

            d = c.user._dirties.copy()
            user._commit()

            amqp.add_item('new_account', user._fullname)

            reject = hooks.get_hook("account.spotcheck").call(account=user)
            if any(reject):
                return

            c.user = user
            self._login(responder, user, rem)

    @cross_domain(allow_credentials=True)
    @api_doc(api_section.account, extends=_handle_register)
    def POST_register(self, *args, **kwargs):
        """Register a new account.

        `rem` specifies whether or not the session cookie returned should last
        beyond the current browser session (that is, if `rem` is `True` the
        cookie will have an explicit expiration far in the future indicating
        that it is not a session cookie).

        """
        return self._handle_register(*args, **kwargs)

    @noresponse(VUser(),
                VModhash(),
                container = VByName('id'))
    @api_doc(api_section.moderation)
    def POST_leavemoderator(self, container):
        """Abdicate moderator status in a subreddit.

        See also: [/api/friend](#POST_api_friend).

        """
        if container and container.is_moderator(c.user):
            container.remove_moderator(c.user)
            ModAction.create(container, c.user, 'removemoderator', target=c.user, 
                             details='remove_self')

    @noresponse(VUser(),
                VModhash(),
                container = VByName('id'))
    @api_doc(api_section.moderation)
    def POST_leavecontributor(self, container):
        """Abdicate approved submitter status in a subreddit.

        See also: [/api/friend](#POST_api_friend).

        """
        if container and container.is_contributor(c.user):
            container.remove_contributor(c.user)


    _sr_friend_types = (
        'moderator',
        'moderator_invite',
        'contributor',
        'banned',
        'wikibanned',
        'wikicontributor',
    )

    _sr_friend_types_with_permissions = (
        'moderator',
        'moderator_invite',
    )

    @noresponse(VUser(),
                VModhash(),
                nuser = VExistingUname('name'),
                iuser = VByName('id'),
                container = nop('container'),
                type = VOneOf('type', ('friend', 'enemy') +
                                      _sr_friend_types))
    @api_doc(api_section.users)
    def POST_unfriend(self, nuser, iuser, container, type):
        """
        Handles removal of a friend (a user-user relation) or removal
        of a user's privileges from a subreddit (a user-subreddit
        relation).  The user can either be passed in by name (nuser)
        or by fullname (iuser).  If type is friend or enemy, 'container'
        will be the current user, otherwise the subreddit must be set.
        """
        if type in self._sr_friend_types:
            container = c.site
            if c.user._spam:
                return
        else:
            container = VByName('container').run(container)
            if not container:
                return

        # The user who made the request must be an admin or a moderator
        # for the privilege change to succeed.
        # (Exception: a user can remove privilege from oneself)
        victim = iuser or nuser
        required_perms = []
        if c.user != victim:
            if type.startswith('wiki'):
                required_perms.append('wiki')
            else:
                required_perms.append('access')
        if (not c.user_is_admin
            and (type in self._sr_friend_types
                 and not container.is_moderator_with_perms(
                     c.user, *required_perms))):
            abort(403, 'forbidden')
        if (type == 'moderator' and not
            (c.user_is_admin or container.can_demod(c.user, victim))):
            abort(403, 'forbidden')
        # if we are (strictly) unfriending, the container had better
        # be the current user.
        if type in ("friend", "enemy") and container != c.user:
            abort(403, 'forbidden')
        fn = getattr(container, 'remove_' + type)
        new = fn(victim)

        # Log this action
        if new and type in self._sr_friend_types:
            action = dict(banned='unbanuser', moderator='removemoderator',
                          moderator_invite='uninvitemoderator',
                          wikicontributor='removewikicontributor',
                          wikibanned='wikiunbanned',
                          contributor='removecontributor').get(type, None)
            ModAction.create(container, c.user, action, target=victim)

        if type == "friend" and c.user.gold:
            c.user.friend_rels_cache(_update=True)

    @validatedForm(VSrModerator(), VModhash(),
                   target=VExistingUname('name'),
                   type_and_permissions=VPermissions('type', 'permissions'))
    @api_doc(api_section.users, uses_site=True)
    def POST_setpermissions(self, form, jquery, target, type_and_permissions):
        if form.has_errors('name', errors.USER_DOESNT_EXIST, errors.NO_USER):
            return
        if form.has_errors('type', errors.INVALID_PERMISSION_TYPE):
            return
        if form.has_errors('permissions', errors.INVALID_PERMISSIONS):
            return

        if c.user._spam:
            return

        type, permissions = type_and_permissions
        update = None

        if type in ("moderator", "moderator_invite"):
            if not c.user_is_admin:
                if type == "moderator" and (
                    c.user == target or not c.site.can_demod(c.user, target)):
                    abort(403, 'forbidden')
                if (type == "moderator_invite"
                    and not c.site.is_unlimited_moderator(c.user)):
                    abort(403, 'forbidden')
            if type == "moderator":
                rel = c.site.get_moderator(target)
            if type == "moderator_invite":
                rel = c.site.get_moderator_invite(target)
            rel.set_permissions(permissions)
            rel._commit()
            update = rel.encoded_permissions
            ModAction.create(c.site, c.user, action='setpermissions',
                             target=target, details='permission_' + type,
                             description=update)

        if update:
            row = form.closest('tr')
            editor = row.find('.permissions').data('PermissionEditor')
            editor.onCommit(update)

    @validatedForm(VUser(),
                   VModhash(),
                   ip = ValidIP(),
                   friend = VExistingUname('name'),
                   container = nop('container'),
                   type = VOneOf('type', ('friend',) + _sr_friend_types),
                   type_and_permissions = VPermissions('type', 'permissions'),
                   note = VLength('note', 300))
    @api_doc(api_section.users)
    def POST_friend(self, form, jquery, ip, friend,
                    container, type, type_and_permissions, note):
        """
        Complement to POST_unfriend: handles friending as well as
        privilege changes on subreddits.
        """
        if type in self._sr_friend_types:
            container = c.site
        else:
            container = VByName('container').run(container)
            if not container:
                return

        # Don't let banned users make subreddit access changes
        if type in self._sr_friend_types and c.user._spam:
            return

        if type == "moderator" and not c.user_is_admin:
            # attempts to add moderators now create moderator invites.
            type = "moderator_invite"

        fn = getattr(container, 'add_' + type)

        # Make sure the user making the request has the correct permissions
        # to be able to make this status change
        if type in self._sr_friend_types:
            if c.user_is_admin:
                has_perms = True
            elif type.startswith('wiki'):
                has_perms = container.is_moderator_with_perms(c.user, 'wiki')
            elif type == 'moderator_invite':
                has_perms = container.is_unlimited_moderator(c.user)
            else:
                has_perms = container.is_moderator_with_perms(c.user, 'access')

            if not has_perms:
                abort(403, 'forbidden')

        if type == 'moderator_invite':
            invites = sum(1 for i in container.each_moderator_invite())
            if invites >= g.sr_invite_limit:
                c.errors.add(errors.SUBREDDIT_RATELIMIT, field="name")
                form.set_error(errors.SUBREDDIT_RATELIMIT, "name")
                return

        if type in self._sr_friend_types and not c.user_is_admin:
            quota_key = "sr%squota-%s" % (str(type), container._id36)
            g.cache.add(quota_key, 0, time=g.sr_quota_time)
            subreddit_quota = g.cache.incr(quota_key)
            quota_limit = getattr(g, "sr_%s_quota" % type)
            if subreddit_quota > quota_limit and container.use_quotas:
                form.set_html(".status", errors.SUBREDDIT_RATELIMIT)
                c.errors.add(errors.SUBREDDIT_RATELIMIT)
                form.set_error(errors.SUBREDDIT_RATELIMIT, None)
                return

        # if we are (strictly) friending, the container
        # had better be the current user.
        if type == "friend" and container != c.user:
            abort(403,'forbidden')

        elif form.has_errors("name", errors.USER_DOESNT_EXIST, errors.NO_USER):
            return
        elif form.has_errors("note", errors.TOO_LONG):
            return

        if type in self._sr_friend_types_with_permissions:
            if form.has_errors('type', errors.INVALID_PERMISSION_TYPE):
                return
            if form.has_errors('permissions', errors.INVALID_PERMISSIONS):
                return
        else:
            permissions = None

        if (type in ("banned", "moderator_invite") and
                container.is_moderator(friend)):
            c.errors.add(errors.ALREADY_MODERATOR, field="name")
            form.set_error(errors.ALREADY_MODERATOR, "name")
            return

        # don't allow increasing privileges of banned users
        unbanned_types = ("moderator", "moderator_invite",
                          "contributor", "wikicontributor")
        if type in unbanned_types and container.is_banned(friend):
            c.errors.add(errors.BANNED_FROM_SUBREDDIT, field="name")
            form.set_error(errors.BANNED_FROM_SUBREDDIT, "name")
            return

        if type == "moderator":
            container.remove_moderator_invite(friend)

        new = fn(friend, permissions=type_and_permissions[1])

        # Log this action
        if new and type in self._sr_friend_types:
            action = dict(banned='banuser',
                          moderator='addmoderator',
                          moderator_invite='invitemoderator',
                          wikicontributor='wikicontributor',
                          contributor='addcontributor',
                          wikibanned='wikibanned').get(type, None)
            ModAction.create(container, c.user, action, target=friend)

        if type == "friend" and c.user.gold:
            # Yes, the order of the next two lines is correct.
            # First you recalculate the rel_ids, then you find
            # the right one and update its data.
            c.user.friend_rels_cache(_update=True)
            c.user.add_friend_note(friend, note or '')
        
        if type in ('banned', 'wikibanned'):
            container.add_rel_note(type, friend, note)

        row_cls = dict(friend=FriendTableItem,
                       moderator=ModTableItem,
                       moderator_invite=InvitedModTableItem,
                       contributor=ContributorTableItem,
                       wikicontributor=WikiMayContributeTableItem,
                       banned=BannedTableItem,
                       wikibanned=WikiBannedTableItem).get(type)

        form.set_inputs(name = "")
        if note:
            form.set_inputs(note = "")
        form.removeClass("edited")

        if new and row_cls:
            new._thing2 = friend
            user_row = row_cls(new)
            form.set_html(".status:first", user_row.executed_message)
            rev_types = ["moderator", "moderator_invite", "friend"]
            index = 0 if user_row.type not in rev_types else -1
            table = jquery("." + type + "-table").show().find("table")
            table.insert_table_rows(user_row, index=index)
            table.find(".notfound").hide()

        if new:
            notify_user_added(type, c.user, friend, container)

    @validatedForm(VGold(),
                   VModhash(),
                   friend = VExistingUname('name'),
                   note = VLength('note', 300))
    def POST_friendnote(self, form, jquery, friend, note):
        if form.has_errors("note", errors.TOO_LONG):
            return
        c.user.add_friend_note(friend, note)
        form.set_html('.status', _("saved"))

    @validatedForm(VModhash(),
                   type = VOneOf('type', ('bannednote', 'wikibannednote')),
                   user = VExistingUname('name'),
                   note = VLength('note', 300))
    def POST_relnote(self, form, jquery, type, user, note):
        perm = 'wiki' if type.startswith('wiki') else 'access'
        if (not c.user_is_admin
            and (not c.site.is_moderator_with_perms(c.user, perm))):
            if c.user._spam:
                return
            else:
                abort(403, 'forbidden')
        if form.has_errors("note", errors.TOO_LONG):
            return
        c.site.add_rel_note(type[:-4], user, note)
        form.set_html('.status', _("saved"))

    @validatedForm(VUser(),
                   VModhash(),
                   ip=ValidIP())
    @api_doc(api_section.moderation, uses_site=True)
    def POST_accept_moderator_invite(self, form, jquery, ip):
        """Accept an invite to moderate the specified subreddit.

        The authenticated user must have been invited to moderate the subreddit
        by one of its current moderators.

        See also: [/api/friend](#POST_api_friend) and
        [/subreddits/mine](#GET_subreddits_mine_{where}).

        """

        rel = c.site.get_moderator_invite(c.user)
        if not c.site.remove_moderator_invite(c.user):
            c.errors.add(errors.NO_INVITE_FOUND)
            form.set_error(errors.NO_INVITE_FOUND, None)
            return

        permissions = rel.get_permissions()
        ModAction.create(c.site, c.user, "acceptmoderatorinvite")
        c.site.add_moderator(c.user, permissions=rel.get_permissions())
        notify_user_added("accept_moderator_invite", c.user, c.user, c.site)
        jquery.refresh()

    @validatedForm(VUser('curpass', default=''),
                   VModhash(),
                   password=VPassword(
                        ['curpass', 'curpass'],
                        docs=dict(curpass="the user's current password")
                   ),
                   dest = VDestination())
    @api_doc(api_section.account)
    def POST_clear_sessions(self, form, jquery, password, dest):
        """Clear all session cookies and replace the current one.

        A valid password (`curpass`) must be supplied.

        """
        # password is required to proceed
        if form.has_errors("curpass", errors.WRONG_PASSWORD):
            return

        form.set_html('.status',
                      _('all other sessions have been logged out'))
        form.set_inputs(curpass = "")

        # deauthorize all access tokens
        OAuth2AccessToken.revoke_all_by_user(c.user)
        OAuth2RefreshToken.revoke_all_by_user(c.user)

        # run the change password command to get a new salt
        change_password(c.user, password)
        # the password salt has changed, so the user's cookie has been
        # invalidated.  drop a new cookie.
        self.login(c.user)

    @validatedForm(
        VUser('curpass', default=''),
        VModhash(),
        email=ValidEmails("email", num=1),
        verify=VBoolean("verify"),
        dest=VDestination(),
    )
    @api_doc(api_section.account)
    def POST_update_email(self, form, jquery, email, verify, dest):
        """Update account email address.

        Called by /prefs/update on the site.

        """

        if form.has_errors("curpass", errors.WRONG_PASSWORD):
            return

        if not form.has_errors("email", errors.BAD_EMAILS) and email:
            if (not hasattr(c.user, 'email') or c.user.email != email):
                if c.user.email:
                    emailer.email_change_email(c.user)

                c.user.email = email
                c.user.email_verified = None
                c.user._commit()
                Award.take_away("verified_email", c.user)

            if verify:
                if dest == '/':
                    dest = None

                emailer.verify_email(c.user, dest=dest)
                form.set_html('.status',
                     _("you should be getting a verification email shortly."))
            else:
                form.set_html('.status', _('your email has been updated'))

        # user is removing their email
        if (not email and c.user.email and 
            (errors.NO_EMAILS, 'email') in c.errors):
            c.errors.remove((errors.NO_EMAILS, 'email'))
            if c.user.email:
                emailer.email_change_email(c.user)
            c.user.email = ''
            c.user.email_verified = None
            c.user._commit()
            Award.take_away("verified_email", c.user)
            form.set_html('.status', _('your email has been updated'))

    @validatedForm(
        VUser('curpass', default=''),
        VModhash(),
        password=VPassword(['newpass', 'verpass']),
    )
    @api_doc(api_section.account)
    def POST_update_password(self, form, jquery, password):
        """Update account password.

        Called by /prefs/update on the site. For frontend form verification
        purposes, `newpass` and `verpass` must be equal for a password change
        to succeed.

        """

        if form.has_errors("curpass", errors.WRONG_PASSWORD):
            return

        if (password and
            not (form.has_errors("newpass", errors.BAD_PASSWORD) or
                 form.has_errors("verpass", errors.BAD_PASSWORD_MATCH))):
            change_password(c.user, password)

            if c.user.email:
                emailer.password_change_email(c.user)

            form.set_html('.status', _('your password has been updated'))
            form.set_inputs(curpass="", newpass="", verpass="")

            # the password has changed, so the user's cookie has been
            # invalidated.  drop a new cookie.
            self.login(c.user)

    @validatedForm(VUser('curpass', default = ''),
                   VModhash(),
                   email = ValidEmails("email", num = 1),
                   password = VPassword(['newpass', 'verpass']),
                   verify = VBoolean("verify"),
                   dest=VDestination())
    @api_doc(api_section.account)
    def POST_update(self, form, jquery, email, password, verify, dest):
        """
        Update account email address and password.

        Called by /prefs/update on the site. For frontend form verification
        purposes, `newpass` and `verpass` must be equal for a password change
        to succeed.
        """
        # password is required to proceed
        if form.has_errors("curpass", errors.WRONG_PASSWORD):
            return
        
        # check if the email is valid.  If one is given and it is
        # different from the current address (or there is not one
        # currently) apply it
        updated = False
        if (not form.has_errors("email", errors.BAD_EMAILS) and
            email):
            if (not hasattr(c.user,'email') or c.user.email != email):
                if c.user.email:
                    emailer.email_change_email(c.user)
                c.user.email = email
                # unverified email for now
                c.user.email_verified = None
                c.user._commit()
                Award.take_away("verified_email", c.user)
                updated = True
            if verify:
                # TODO: rate limit this?
                if dest == '/':
                    dest = None

                emailer.verify_email(c.user, dest=dest)
                form.set_html('.status',
                     _("you should be getting a verification email shortly."))
            else:
                form.set_html('.status', _('your email has been updated'))

        # user is removing their email
        if (not email and c.user.email and 
            (errors.NO_EMAILS, 'email') in c.errors):
            c.errors.remove((errors.NO_EMAILS, 'email'))
            c.user.email = ''
            c.user.email_verified = None
            c.user._commit()
            Award.take_away("verified_email", c.user)
            updated = True
            form.set_html('.status', _('your email has been updated'))

        # change password
        if (password and
            not (form.has_errors("newpass", errors.BAD_PASSWORD) or
                 form.has_errors("verpass", errors.BAD_PASSWORD_MATCH))):
            change_password(c.user, password)
            if c.user.email:
                emailer.password_change_email(c.user)
            if updated:
                form.set_html(".status",
                              _('your email and password have been updated'))
            else:
                form.set_html('.status', 
                              _('your password has been updated'))
            form.set_inputs(curpass = "", newpass = "", verpass = "")
            # the password has changed, so the user's cookie has been
            # invalidated.  drop a new cookie.
            self.login(c.user)

    @validatedForm(VUser(),
                   VModhash(),
                   delete_message = VLength("delete_message", max_length=500),
                   username = VRequired("user", errors.NOT_USER),
                   user = VThrottledLogin(["user", "passwd"]),
                   confirm = VBoolean("confirm"))
    @api_doc(api_section.account)
    def POST_delete_user(self, form, jquery, delete_message, username, user, confirm):
        """Delete the currently logged in account.

        A valid username/password and confirmation must be supplied. An
        optional `delete_message` may be supplied to explain the reason the
        account is to be deleted.

        Called by /prefs/delete on the site.

        """
        if username and username.lower() != c.user.name.lower():
            c.errors.add(errors.NOT_USER, field="user")

        if not confirm:
            c.errors.add(errors.CONFIRM, field="confirm")

        if not (form.has_errors('vdelay', errors.RATELIMIT) or
                form.has_errors("user", errors.NOT_USER) or
                form.has_errors("passwd", errors.WRONG_PASSWORD) or
                form.has_errors("delete_message", errors.TOO_LONG) or
                form.has_errors("confirm", errors.CONFIRM)):
            c.user.delete(delete_message)
            form.redirect("/?deleted=true")

    @require_oauth2_scope("edit")
    @noresponse(VUser(),
                VModhash(),
                thing = VByNameIfAuthor('id'))
    @api_doc(api_section.links_and_comments)
    def POST_del(self, thing):
        """Delete a Link or Comment."""
        if not thing: return
        was_deleted = thing._deleted
        thing._deleted = True
        if (getattr(thing, "promoted", None) is not None and
            not promote.is_promoted(thing)):
            promote.reject_promotion(thing)
        thing._commit()

        # flag search indexer that something has changed
        changed(thing)

        #expire the item from the sr cache
        if isinstance(thing, Link):
            queries.delete(thing)

        #comments have special delete tasks
        elif isinstance(thing, Comment):
            parent_id = getattr(thing, 'parent_id', None)
            link_id = thing.link_id
            recipient = None

            if parent_id:
                parent_comment = Comment._byID(parent_id, data=True)
                recipient = Account._byID(parent_comment.author_id)
            else:
                parent_link = Link._byID(link_id, data=True)
                if parent_link.is_self:
                    recipient = Account._byID(parent_link.author_id)

            if not was_deleted:
                queries.delete_comment(thing)

            if recipient:
                inbox_class = Inbox.rel(Account, Comment)
                d = inbox_class._fast_query(recipient, thing, ("inbox",
                                                               "selfreply",
                                                               "mention"))
                rels = filter(None, d.values()) or None
                queries.new_comment(thing, rels)

            queries.delete(thing)

    @require_oauth2_scope("modposts")
    @noresponse(VUser(),
                VModhash(),
                VSrCanAlter('id'),
                thing = VByName('id'))
    @api_doc(api_section.links_and_comments)
    def POST_marknsfw(self, thing):
        """Mark a link NSFW.

        See also: [/api/unmarknsfw](#POST_api_unmarknsfw).

        """
        thing.over_18 = True
        thing._commit()

        if c.user._id != thing.author_id:
            ModAction.create(thing.subreddit_slow, c.user, target=thing,
                             action='marknsfw')

        # flag search indexer that something has changed
        changed(thing)

    @require_oauth2_scope("modposts")
    @noresponse(VUser(),
                VModhash(),
                VSrCanAlter('id'),
                thing = VByName('id'))
    @api_doc(api_section.links_and_comments)
    def POST_unmarknsfw(self, thing):
        """Remove the NSFW marking from a link.

        See also: [/api/marknsfw](#POST_api_marknsfw).

        """

        if promote.is_promo(thing):
            if c.user_is_sponsor:
                # set the override attribute so this link won't be automatically
                # reset as nsfw by promote.make_daily_promotions
                thing.over_18_override = True
            else:
                abort(403,'forbidden')

        thing.over_18 = False
        thing._commit()

        if c.user._id != thing.author_id:
            ModAction.create(thing.subreddit_slow, c.user, target=thing,
                             action='marknsfw', details='remove')

        # flag search indexer that something has changed
        changed(thing)

    @require_oauth2_scope("edit")
    @noresponse(VUser(),
                VModhash(),
                thing=VByNameIfAuthor('id'),
                state=VBoolean('state'))
    @api_doc(api_section.links_and_comments)
    def POST_sendreplies(self, thing, state):
        """Enable or disable inbox replies for a link.

        `state` is a boolean that indicates whether you are enabling or
        disabling inbox replies - true to enable, false to disable.

        """
        if not isinstance(thing, Link):
            return
        thing.sendreplies = state
        thing._commit()

    @noresponse(VUser(),
                VModhash(),
                VSrCanAlter('id'),
                thing=VByName('id'))
    def POST_rescrape(self, thing):
        """Re-queues the link in the media scraper."""
        if not isinstance(thing, Link):
            return

        # KLUDGE: changing the cache entry to a placeholder for this URL will
        # cause the media scraper to force a rescrape.  This will be fixed
        # when parameters can be passed to the scraper queue.
        media_cache.MediaByURL.add_placeholder(thing.url, autoplay=False)

        amqp.add_item("scraper_q", thing._fullname)

    @require_oauth2_scope("modposts")
    @validatedForm(VUser(),
                   VModhash(),
                   VSrCanBan('id'),
                   thing=VByName('id'),
                   state=VBoolean('state'))
    @api_doc(api_section.links_and_comments)
    def POST_set_contest_mode(self, form, jquery, thing, state):
        """Set or unset "contest mode" for a link's comments.
        
        `state` is a boolean that indicates whether you are enabling or
        disabling contest mode - true to enable, false to disable.

        """
        thing.contest_mode = state
        thing._commit()
        jquery.refresh()

    @require_oauth2_scope("modposts")
    @validatedForm(VUser(),
                   VModhash(),
                   VSrCanBan('id'),
                   thing=VByName('id'),
                   state=VBoolean('state'))
    @api_doc(api_section.links_and_comments)
    def POST_set_subreddit_sticky(self, form, jquery, thing, state):
        """Set or unset a self-post as the sticky post in its subreddit.
        
        `state` is a boolean that indicates whether to sticky or unsticky
        this post - true to sticky, false to unsticky.

        Note that if another post was previously stickied, stickying a new
        one will replace the previous one.
        
        """
        if not isinstance(thing, Link) or not thing.is_self:
            return

        sr = thing.subreddit_slow

        if state:
            sr.sticky_fullname = thing._fullname
            ModAction.create(sr, c.user, 'sticky', target=thing)
        elif not state:
            sr.sticky_fullname = None
            ModAction.create(sr, c.user, 'unsticky', target=thing)

        sr._commit()
        jquery.refresh()

    @require_oauth2_scope("report")
    @noresponse(VUser(), VModhash(),
                thing = VByName('id'))
    @api_doc(api_section.links_and_comments)
    def POST_report(self, thing):
        """Report a link or comment.

        Reporting a thing brings it to the attention of the subreddit's
        moderators. The thing is implicitly hidden as well (see
        [/api/hide](#POST_api_hide) for details).

        """
        if not thing or thing._deleted:
            return

        # if it is a message that is being reported, ban it.
        # every user is admin over their own personal inbox
        if isinstance(thing, Message):
            admintools.spam(thing, False, True, c.user.name)
        # auto-hide links that are reported
        elif isinstance(thing, Link):
            thing._hide(c.user)
        # TODO: be nice to be able to remove comments that are reported
        # from a user's inbox so they don't have to look at them.
        elif isinstance(thing, Comment):
            pass

        hooks.get_hook("thing.report").call(thing=thing)

        sr = getattr(thing, 'subreddit_slow', None)
        if (c.user._spam or
                c.user.ignorereports or
                (sr and sr.is_banned(c.user))):
            return
        Report.new(c.user, thing)
        admintools.report(thing)

    @require_oauth2_scope("privatemessages")
    @noresponse(VUser(), VModhash(),
                thing=VByName('id'))
    @api_doc(api_section.messages)
    def POST_block(self, thing):
        '''For blocking via inbox.'''
        if not thing:
            return

        try:
            sr = Subreddit._byID(thing.sr_id) if thing.sr_id else None
        except NotFound:
            sr = None
        # Users may only block someone who has
        # actively harassed them (i.e., comment/link reply
        # or PM). Check that 'thing' is in the user's inbox somewhere
        if not (sr and sr.is_moderator_with_perms(c.user, 'mail')):
            inbox_cls = Inbox.rel(Account, thing.__class__)
            rels = inbox_cls._fast_query(c.user, thing,
                                        ("inbox", "selfreply", "mention"))
            if not filter(None, rels.values()):
                return

        block_acct = Account._byID(thing.author_id)
        if block_acct.name in g.admins:
            return
        c.user.add_enemy(block_acct)

    @require_oauth2_scope("edit")
    @validatedForm(VUser(),
                   VModhash(),
                   item = VByNameIfAuthor('thing_id'),
                   text = VSelfText('text'))
    @api_doc(api_section.links_and_comments)
    def POST_editusertext(self, form, jquery, item, text):
        """Edit the body text of a comment or self-post."""
        if (not form.has_errors("text",
                                errors.NO_TEXT, errors.TOO_LONG) and
            not form.has_errors("thing_id", errors.NOT_AUTHOR)):

            if isinstance(item, Comment):
                kind = 'comment'
                item.body = text
            elif isinstance(item, Link):
                kind = 'link'
                if not getattr(item, "is_self", False):
                    return abort(403, "forbidden")
                item.selftext = text
            else:
                g.log.warning("%s tried to edit usertext on %r", c.user, item)
                return

            if item._deleted:
                return abort(403, "forbidden")

            if (item._date < timeago('3 minutes')
                or (item._ups + item._downs > 2)):
                item.editted = c.start_time

            item.ignore_reports = False

            item._commit()

            changed(item)

            amqp.add_item('usertext_edited', item._fullname)

            if kind == 'link':
                set_last_modified(item, 'comments')
                LastModified.touch(item._fullname, 'Comments')

            wrapper = default_thing_wrapper(expand_children = True)
            jquery(".content").replace_things(item, True, True, wrap = wrapper)
            jquery(".content .link .rank").hide()

    @require_oauth2_scope("submit")
    @validatedForm(VUser(),
                   VModhash(),
                   VRatelimit(rate_user = True, rate_ip = True,
                              prefix = "rate_comment_"),
                   ip = ValidIP(),
                   parent = VSubmitParent(['thing_id', 'parent']),
                   comment = VMarkdown(['text', 'comment']))
    @api_doc(api_section.links_and_comments)
    def POST_comment(self, commentform, jquery, parent, comment, ip):
        """Submit a new comment or reply to a message.

        `parent` is the fullname of the thing being replied to. Its value
        changes the kind of object created by this request:

        * the fullname of a Link: a top-level comment in that Link's thread.
        * the fullname of a Comment: a comment reply to that comment.
        * the fullname of a Message: a message reply to that message.

        `text` should be the raw markdown body of the comment or message.

        To start a new message thread, use [/api/compose](#POST_api_compose).

        """
        should_ratelimit = True
        #check the parent type here cause we need that for the
        #ratelimit checks
        if isinstance(parent, Message):
            if not getattr(parent, "repliable", True):
                abort(403, 'forbidden')
            if not parent.can_view_slow():
                abort(403, 'forbidden')
            is_message = True
            should_ratelimit = False
        else:
            is_message = False
            if isinstance(parent, Link):
                link = parent
                parent_comment = None
            else:
                link = Link._byID(parent.link_id, data = True)
                parent_comment = parent

            sr = parent.subreddit_slow
            is_author = link.author_id == c.user._id
            if (is_author and (link.is_self or promote.is_promo(link)) or
                    not sr.should_ratelimit(c.user, 'comment')):
                should_ratelimit = False

            if link._age > sr.archive_age:
                c.errors.add(errors.TOO_OLD, field = "parent")

            hooks.get_hook("comment.validate").call(sr=sr, link=link,
                           parent_comment=parent_comment)

        #remove the ratelimit error if the user's karma is high
        if not should_ratelimit:
            c.errors.remove((errors.RATELIMIT, 'ratelimit'))

        if (not commentform.has_errors("text",
                                       errors.NO_TEXT,
                                       errors.TOO_LONG) and
            not commentform.has_errors("ratelimit",
                                       errors.RATELIMIT) and
            not commentform.has_errors("parent",
                                       errors.DELETED_COMMENT,
                                       errors.DELETED_LINK,
                                       errors.TOO_OLD,
                                       errors.USER_BLOCKED)):

            if is_message:
                if parent.from_sr:
                    to = Subreddit._byID(parent.sr_id)
                else:
                    to = Account._byID(parent.author_id)
                subject = parent.subject
                re = "re: "
                if not subject.startswith(re):
                    subject = re + subject
                item, inbox_rel = Message._new(c.user, to, subject,
                                               comment, ip, parent = parent)
                item.parent_id = parent._id
            else:
                item, inbox_rel = Comment._new(c.user, link, parent_comment,
                                               comment, ip)
                queries.queue_vote(c.user, item, True, ip, cheater=c.cheater)

                # adding to comments-tree is done as part of
                # newcomments_q, so if they refresh immediately they
                # won't see their comment

            #update the queries
            if is_message:
                queries.new_message(item, inbox_rel)
            else:
                queries.new_comment(item, inbox_rel)

            #set the ratelimiter
            if should_ratelimit:
                VRatelimit.ratelimit(rate_user=True, rate_ip = True,
                                     prefix = "rate_comment_")

            # clean up the submission form and remove it from the DOM (if reply)
            t = commentform.find("textarea")
            t.attr('rows', 3).html("").val("")
            if isinstance(parent, (Comment, Message)):
                commentform.remove()
                jquery.things(parent._fullname).set_html(".reply-button:first",
                                                         _("replied"))

            # insert the new comment
            jquery.insert_things(item)

            # remove any null listings that may be present
            jquery("#noresults").hide()

    @validatedForm(VUser(),
                   VModhash(),
                   VCaptcha(),
                   VRatelimit(rate_user = True, rate_ip = True,
                              prefix = "rate_share_"),
                   share_from = VLength('share_from', max_length = 100),
                   emails = ValidEmailsOrExistingUnames("share_to"),
                   reply_to = ValidEmails("replyto", num = 1), 
                   message = VLength("message", max_length = 1000), 
                   thing = VByName('parent'),
                   ip = ValidIP())
    def POST_share(self, shareform, jquery, emails, thing, share_from, reply_to,
                   message, ip):
        if not thing:
            abort(404, 'not found')

        # remove the ratelimit error if the user's karma is high
        sr = thing.subreddit_slow
        should_ratelimit = sr.should_ratelimit(c.user, 'link')
        if not should_ratelimit:
            c.errors.remove((errors.RATELIMIT, 'ratelimit'))

        # share_from and messages share a too_long error.
        # finding an error on one necessitates hiding the other error
        if shareform.has_errors("share_from", errors.TOO_LONG):
            shareform.find(".message-errors").children().hide()
        elif shareform.has_errors("message", errors.TOO_LONG):
            shareform.find(".share-form-errors").children().hide()
        # reply_to and share_to also share errors...
        elif shareform.has_errors("share_to", errors.BAD_EMAILS,
                                  errors.NO_EMAILS,
                                  errors.TOO_MANY_EMAILS):
            shareform.find(".reply-to-errors").children().hide()
        elif shareform.has_errors("replyto", errors.BAD_EMAILS,
                                  errors.TOO_MANY_EMAILS):
            shareform.find(".share-to-errors").children().hide()
        # lastly, check the captcha.
        elif shareform.has_errors("captcha", errors.BAD_CAPTCHA):
            pass
        elif shareform.has_errors("ratelimit", errors.RATELIMIT):
            pass
        elif not sr.can_view(c.user):
            return abort(403, 'forbidden')
        else:
            emails, users = emails
            c.user.add_share_emails(emails)
            c.user._commit()
            link = jquery.things(thing._fullname)
            link.set_html(".share", _("shared"))
            shareform.html("<div class='clearleft'></div>"
                           "<p class='error'>%s</p>" % 
                           _("your link has been shared."))
            
            # Set up the parts that are common between e-mail and PMs
            urlparts = (get_domain(cname=c.cname, subreddit=False),
                        thing._id36)
            url = "http://%s/tb/%s" % urlparts
            
            if message:
                message = message + "\n\n"
            else:
                message = ""
            message = message + '\n%s\n\n%s\n\n' % (thing.title,url)
            
            # Deliberately not translating this, as it'd be in the
            # sender's language
            if thing.num_comments:
                count = ("There are currently %(num_comments)s comments on " +
                         "this link.  You can view them here:")
                if thing.num_comments == 1:
                    count = ("There is currently %(num_comments)s comment " +
                             "on this link.  You can view it here:")
                
                numcom = count % {'num_comments':thing.num_comments}
                message = message + "%s\n\n" % numcom
            else:
                message = message + "You can leave a comment here:\n\n"
                
            url = add_sr(thing.make_permalink_slow(), force_hostname=True)
            message = message + url
            
            # E-mail everyone
            emailer.share(thing, emails, from_name = share_from or "",
                          body = message or "", reply_to = reply_to or "")

            # Send the PMs
            subject = "%s has shared a link with you!" % c.user.name
            # Prepend this subject to the message - we're repeating ourselves
            # because it looks very abrupt without it.
            message = "%s\n\n%s" % (subject,message)
            
            for target in users:
                
                m, inbox_rel = Message._new(c.user, target, subject,
                                            message, ip)
                # Queue up this PM
                amqp.add_item('new_message', m._fullname)

                queries.new_message(m, inbox_rel)

            #set the ratelimiter
            if should_ratelimit:
                VRatelimit.ratelimit(rate_user=True, rate_ip = True,
                                     prefix = "rate_share_")


    @require_oauth2_scope("vote")
    @noresponse(VUser(),
                VModhash(),
                vote_info=VVotehash('vh'),
                ip = ValidIP(),
                dir=VInt('dir', min=-1, max=1, docs={"dir":
                    "vote direction. one of (1, 0, -1)"}),
                thing = VByName('id'))
    @api_doc(api_section.links_and_comments)
    def POST_vote(self, dir, thing, ip, vote_info):
        """Cast a vote on a thing.

        `id` should be the fullname of the Link or Comment to vote on.

        `dir` indicates the direction of the vote. Voting `1` is an upvote,
        `-1` is a downvote, and `0` is equivalent to "un-voting" by clicking
        again on a highlighted arrow.

        **Note: votes must be cast by humans.** That is, API clients proxying a
        human's action one-for-one are OK, but bots deciding how to vote on
        content or amplifying a human's vote are not. See [the reddit
        rules](/rules) for more details on what constitutes vote cheating.

        """

        ip = request.ip
        user = c.user
        store = True

        if not thing or thing._deleted:
            return

        hooks.get_hook("vote.validate").call(thing=thing)

        if not isinstance(thing, (Link, Comment)):
            return

        if vote_info == 'rejected':
            reject_vote(thing)
            store = False

        if thing._age > thing.subreddit_slow.archive_age:
            store = False

        if getattr(c.user, "suspicious", False):
            g.log.info("%s cast a %d vote on %s", c.user.name, dir, thing._fullname)

        dir = (True if dir > 0
               else False if dir < 0
               else None)

        queries.queue_vote(user, thing, dir, ip, vote_info=vote_info,
                           store=store,
                           cheater=c.cheater)

    @require_oauth2_scope("modconfig")
    @validatedForm(VUser(),
                   VModhash(),
                   # nop is safe: handled after auth checks below
                   stylesheet_contents=nop('stylesheet_contents',
                       docs={"stylesheet_contents":
                             "the new stylesheet content"}),
                   reason=VPrintable('reason', 256, empty_error=None),
                   op = VOneOf('op',['save','preview']))
    @api_doc(api_section.subreddits, uses_site=True)
    def POST_subreddit_stylesheet(self, form, jquery,
                                  stylesheet_contents = '', prevstyle='',
                                  op='save', reason=None):
        """Update a subreddit's stylesheet.

        `op` should be `save` to update the contents of the stylesheet.

        """
        
        css_errors, parsed = c.site.parse_css(stylesheet_contents)

        if g.css_killswitch:
            return abort(403, 'forbidden')

        if css_errors:
            error_items = [CssError(x).render(style='html') for x in css_errors]
            form.set_html(".status", _('validation errors'))
            form.set_html(".errors ul", ''.join(error_items))
            form.find('.errors').show()
            c.errors.add(errors.BAD_CSS, field="stylesheet_contents")
            form.has_errors("stylesheet_contents", errors.BAD_CSS)
            return
        else:
            form.find('.errors').hide()
            form.set_html(".errors ul", '')

        if op == 'save':
            wr = c.site.change_css(stylesheet_contents, parsed, reason=reason)
            form.find('.errors').hide()
            form.set_html(".status", _('saved'))
            form.set_html(".errors ul", "")
            if wr:
                description = wiki.modactions.get('config/stylesheet')
                ModAction.create(c.site, c.user, 'wikirevise', description)

        parsed_http, parsed_https = parsed
        if c.secure:
            jquery.apply_stylesheet(parsed_https)
        else:
            jquery.apply_stylesheet(parsed_http)

        if op == 'preview':
            # try to find a link to use, otherwise give up and
            # return
            links = SubredditStylesheet.find_preview_links(c.site)
            if links:

                jquery('#preview-table').show()
    
                # do a regular link
                jquery('#preview_link_normal').html(
                    SubredditStylesheet.rendered_link(
                        links, media='off', compress=False))
                # now do one with media
                jquery('#preview_link_media').html(
                    SubredditStylesheet.rendered_link(
                        links, media='on', compress=False))
                # do a compressed link
                jquery('#preview_link_compressed').html(
                    SubredditStylesheet.rendered_link(
                        links, media='off', compress=True))
                # do a stickied link
                jquery('#preview_link_stickied').html(
                    SubredditStylesheet.rendered_link(
                        links, media='off', compress=False, stickied=True))
    
            # and do a comment
            comments = SubredditStylesheet.find_preview_comments(c.site)
            if comments:
                jquery('#preview_comment').html(
                    SubredditStylesheet.rendered_comment(comments))

                jquery('#preview_comment_gilded').html(
                    SubredditStylesheet.rendered_comment(
                        comments, gilded=True))

    @require_oauth2_scope("modconfig")
    @validatedForm(VSrModerator(perms='config'),
                   VModhash(),
                   name = VCssName('img_name'))
    @api_doc(api_section.subreddits, uses_site=True)
    def POST_delete_sr_img(self, form, jquery, name):
        """Remove an image from the subreddit's custom image set.

        The image will no longer count against the subreddit's image limit.
        However, the actual image data may still be accessible for an
        unspecified amount of time. If the image is currently referenced by the
        subreddit's stylesheet, that stylesheet will no longer validate and
        won't be editable until the image reference is removed.

        See also: [/api/upload_sr_img](#POST_api_upload_sr_img).

        """
        # just in case we need to kill this feature from XSS
        if g.css_killswitch:
            return abort(403, 'forbidden')

        if form.has_errors("img_name", errors.BAD_CSS_NAME):
            return

        wiki.ImagesByWikiPage.delete_image(c.site, "config/stylesheet", name)
        ModAction.create(c.site, c.user, action='editsettings', 
                         details='del_image', description=name)

    @require_oauth2_scope("modconfig")
    @validatedForm(VSrModerator(perms='config'),
                   VModhash())
    @api_doc(api_section.subreddits, uses_site=True)
    def POST_delete_sr_header(self, form, jquery):
        """Remove the subreddit's custom header image.

        The sitewide-default header image will be shown again after this call.

        See also: [/api/upload_sr_img](#POST_api_upload_sr_img).

        """
        # just in case we need to kill this feature from XSS
        if g.css_killswitch:
            return abort(403, 'forbidden')
        if c.site.header:
            c.site.header = None
            c.site.header_size = None
            c.site._commit()
            ModAction.create(c.site, c.user, action='editsettings', 
                             details='del_header')

        # hide the button which started this
        form.find('.delete-img').hide()
        # hide the preview box
        form.find('.img-preview-container').hide()
        # reset the status boxes
        form.set_html('.img-status', _("deleted"))
        

    def GET_upload_sr_img(self, *a, **kw):
        """
        Completely unnecessary method which exists because safari can
        be dumb too.  On page reload after an image has been posted in
        safari, the iframe to which the request posted preserves the
        URL of the POST, and safari attempts to execute a GET against
        it.  The iframe is hidden, so what it returns is completely
        irrelevant.
        """
        return "nothing to see here."

    @require_oauth2_scope("modconfig")
    @validate(VSrModerator(perms='config'),
              VModhash(),
              file = VUploadLength('file', max_length=1024*500),
              name = VCssName("name"),
              img_type = VImageType('img_type'),
              form_id = VLength('formid', max_length = 100,
                                docs={"formid": "(optional) can be ignored"}),
              header = VInt('header', max=1, min=0))
    @api_doc(api_section.subreddits, uses_site=True)
    def POST_upload_sr_img(self, file, header, name, form_id, img_type):
        """Add or replace a subreddit image or custom header logo.

        If the `header` value is `0`, an image for use in the subreddit
        stylesheet is uploaded with the name specified in `name`. If the value
        of `header` is `1` then the image uploaded will be the subreddit's new
        logo and `name` will be ignored.

        The `img_type` field specifies whether to store the uploaded image as a
        PNG or JPEG.

        Subreddits have a limited number of images that can be in use at any
        given time. If no image with the specified name already exists, one of
        the slots will be consumed.

        If an image with the specified name already exists, it will be
        replaced.  This does not affect the stylesheet immediately, but will
        take effect the next time the stylesheet is saved.

        See also: [/api/delete_sr_img](#POST_api_delete_sr_img) and
        [/api/delete_sr_header](#POST_api_delete_sr_header).

        """

        # default error list (default values will reset the errors in
        # the response if no error is raised)
        errors = dict(BAD_CSS_NAME = "", IMAGE_ERROR = "")
        add_image_to_sr = False
        size = None
        
        if not header:
            add_image_to_sr = True
            if not name:
                # error if the name wasn't specified and the image was not for a sponsored link or header
                # this may also fail if a sponsored image was added and the user is not an admin
                errors['BAD_CSS_NAME'] = _("bad image name")
        
        if add_image_to_sr:
            image_count = wiki.ImagesByWikiPage.get_image_count(
                c.site, "config/stylesheet")
            if image_count >= g.max_sr_images:
                errors['IMAGE_ERROR'] = _("too many images (you only get %d)") % g.max_sr_images

        if any(errors.values()):
            return UploadedImage("", "", "", errors=errors, form_id=form_id).render()
        else:
            try:
                new_url = media.upload_media(file, file_type="." + img_type)
            except Exception as e:
                g.log.warning("error uploading subreddit image: %s", e)
                errors['IMAGE_ERROR'] = _("Invalid image or general image error")
                return UploadedImage("", "", "", errors=errors, form_id=form_id).render()

            size = str_to_image(file).size
            if header:
                c.site.header = new_url
                c.site.header_size = size
                c.site._commit()
            if add_image_to_sr:
                wiki.ImagesByWikiPage.add_image(c.site, "config/stylesheet",
                                                name, new_url)

            if header:
                kw = dict(details='upload_image_header')
            else:
                kw = dict(details='upload_image', description=name)
            ModAction.create(c.site, c.user, action='editsettings', **kw)

            return UploadedImage(_('saved'), new_url, name, 
                                 errors=errors, form_id=form_id).render()

    @require_oauth2_scope("modconfig")
    @validatedForm(VUser(),
                   VModhash(),
                   VRatelimit(rate_user = True,
                              rate_ip = True,
                              prefix = 'create_reddit_'),
                   sr = VByName('sr'),
                   name = VAvailableSubredditName("name"),
                   title = VLength("title", max_length = 100),
                   header_title = VLength("header-title", max_length = 500),
                   domain = VCnameDomain("domain"),
                   submit_text = VMarkdown("submit_text", max_length=1024),
                   public_description = VMarkdown("public_description", max_length = 500),
                   description = VMarkdown("description", max_length = 5120),
                   lang = VLang("lang"),
                   over_18 = VBoolean('over_18'),
                   allow_top = VBoolean('allow_top'),
                   show_media = VBoolean('show_media'),
                   public_traffic = VBoolean('public_traffic'),
                   exclude_banned_modqueue = VBoolean('exclude_banned_modqueue'),
                   show_cname_sidebar = VBoolean('show_cname_sidebar'),
                   spam_links = VOneOf('spam_links', ('low', 'high', 'all')),
                   spam_selfposts = VOneOf('spam_selfposts', ('low', 'high', 'all')),
                   spam_comments = VOneOf('spam_comments', ('low', 'high', 'all')),
                   type = VOneOf('type', ('public', 'private', 'restricted', 'gold_restricted', 'archived')),
                   link_type = VOneOf('link_type', ('any', 'link', 'self')),
                   submit_link_label=VLength('submit_link_label', max_length=60),
                   submit_text_label=VLength('submit_text_label', max_length=60),
                   comment_score_hide_mins=VInt('comment_score_hide_mins',
                       coerce=False, num_default=0, min=0, max=1440),
                   wikimode = VOneOf('wikimode', ('disabled', 'modonly', 'anyone')),
                   wiki_edit_karma = VInt("wiki_edit_karma", coerce=False, num_default=0, min=0),
                   wiki_edit_age = VInt("wiki_edit_age", coerce=False, num_default=0, min=0),
                   ip = ValidIP(),
                   css_on_cname = VBoolean("css_on_cname"),
                   )
    @api_doc(api_section.subreddits)
    def POST_site_admin(self, form, jquery, name, ip, sr, **kw):
        """Create or configure a subreddit.

        If `sr` is specified, the request will attempt to modify the specified
        subreddit. If not, a subreddit with name `name` will be created.

        This endpoint expects *all* values to be supplied on every request.  If
        modifying a subset of options, it may be useful to get the current
        settings from [/about/edit.json](#GET_r_{subreddit}_about_edit.json)
        first.

        For backwards compatibility, `description` is the sidebar text and
        `public_description` is the publicly visible subreddit description.

        Most of the parameters for this endpoint are identical to options
        visible in the user interface and their meanings are best explained
        there.

        See also: [/about/edit.json](#GET_r_{subreddit}_about_edit.json).

        """
        def apply_wikid_field(sr, form, pagename, value, field):
            try:
                wikipage = wiki.WikiPage.get(sr, pagename)
            except tdb_cassandra.NotFound:
                wikipage = wiki.WikiPage.create(sr, pagename)
            wr = wikipage.revise(value, author=c.user._id36)
            setattr(sr, field, value)
            if wr:
                ModAction.create(sr, c.user, 'wikirevise',
                                 details=wiki.modactions.get(pagename))

        # the status button is outside the form -- have to reset by hand
        form.parent().set_html('.status', "")

        redir = False
        kw = dict((k, v) for k, v in kw.iteritems()
                  if k in ('name', 'title', 'domain', 'description',
                           'show_media', 'exclude_banned_modqueue',
                           'show_cname_sidebar', 'type', 'public_traffic',
                           'link_type',
                           'submit_link_label', 'comment_score_hide_mins',
                           'submit_text_label', 'lang', 'css_on_cname',
                           'header_title', 'over_18', 'wikimode', 'wiki_edit_karma',
                           'wiki_edit_age', 'allow_top', 'public_description',
                           'spam_links', 'spam_selfposts', 'spam_comments',
                           'submit_text'))

        public_description = kw.pop('public_description')
        description = kw.pop('description')
        submit_text = kw.pop('submit_text')

        def update_wiki_text(sr):
            error = False
            apply_wikid_field(
                sr,
                form,
                'config/sidebar',
                description,
                'description',
            )

            apply_wikid_field(
                sr,
                form,
                'config/submit_text',
                submit_text,
                'submit_text',
            )

            apply_wikid_field(
                sr,
                form,
                'config/description',
                public_description,
                'public_description',
            )
        
        #if a user is banned, return rate-limit errors
        if c.user._spam:
            time = timeuntil(datetime.now(g.tz) + timedelta(seconds=600))
            c.errors.add(errors.RATELIMIT, {'time': time})

        domain = kw['domain']
        cname_sr = domain and Subreddit._by_domain(domain)
        if cname_sr and (not sr or sr != cname_sr):
            c.errors.add(errors.USED_CNAME)

        can_set_archived = c.user_is_admin or (sr and sr.type == 'archived')
        if kw['type'] == 'archived' and not can_set_archived:
            c.errors.add(errors.INVALID_OPTION, field='type')

        can_set_gold_restricted = c.user_is_admin or (sr and sr.type == 'gold_restricted')
        if kw['type'] == 'gold_restricted' and not can_set_gold_restricted:
            c.errors.add(errors.INVALID_OPTION, field='type')

        if not sr and form.has_errors("ratelimit", errors.RATELIMIT):
            pass
        elif not sr and form.has_errors("name", errors.SUBREDDIT_EXISTS,
                                        errors.BAD_SR_NAME):
            form.find('#example_name').hide()
        elif form.has_errors('title', errors.NO_TEXT, errors.TOO_LONG):
            form.find('#example_title').hide()
        elif form.has_errors('domain', errors.BAD_CNAME, errors.USED_CNAME):
            form.find('#example_domain').hide()
        elif (form.has_errors(('type', 'link_type', 'wikimode'),
                              errors.INVALID_OPTION) or
              form.has_errors(('public_description',
                               'submit_text',
                               'description'), errors.TOO_LONG)):
            pass
        elif (form.has_errors(('wiki_edit_karma', 'wiki_edit_age'), 
                              errors.BAD_NUMBER)):
            pass
        elif form.has_errors('comment_score_hide_mins', errors.BAD_NUMBER):
            pass
        #creating a new reddit
        elif not sr:
            #sending kw is ok because it was sanitized above
            sr = Subreddit._new(name = name, author_id = c.user._id, ip = ip,
                                **kw)

            update_wiki_text(sr)
            sr._commit()

            Subreddit.subscribe_defaults(c.user)
            # make sure this user is on the admin list of that site!
            if sr.add_subscriber(c.user):
                sr._incr('_ups', 1)
            sr.add_moderator(c.user)
            sr.add_contributor(c.user)
            redir = sr.path + "about/edit/?created=true"
            if not c.user_is_admin:
                VRatelimit.ratelimit(rate_user=True,
                                     rate_ip = True,
                                     prefix = "create_reddit_")

            queries.new_subreddit(sr)
            changed(sr)

        #editting an existing reddit
        elif sr.is_moderator_with_perms(c.user, 'config') or c.user_is_admin:
            #assume sr existed, or was just built
            old_domain = sr.domain

            update_wiki_text(sr)

            if not sr.domain:
                del kw['css_on_cname']
            for k, v in kw.iteritems():
                if getattr(sr, k, None) != v:
                    ModAction.create(sr, c.user, action='editsettings', 
                                     details=k)
                setattr(sr, k, v)
            sr._commit()

            #update the domain cache if the domain changed
            if sr.domain != old_domain:
                Subreddit._by_domain(old_domain, _update = True)
                Subreddit._by_domain(sr.domain, _update = True)

            # flag search indexer that something has changed
            changed(sr)
            form.parent().set_html('.status', _("saved"))

        if form.has_error():
            return

        if redir:
            form.redirect(redir)
        else:
            jquery.refresh()

    @noresponse(q = VPrintable('q', max_length=500),
                sort = VPrintable('sort', max_length=10),
                t = VPrintable('t', max_length=10),
                approval = VBoolean('approval'))
    def POST_searchfeedback(self, q, sort, t, approval):
        timestamp = c.start_time.strftime("%Y/%m/%d-%H:%M:%S")
        if c.user_is_loggedin:
            username = c.user.name
        else:
            username = None
        d = dict(username=username, q=q, sort=sort, t=t)
        hex = hashlib.md5(repr(d)).hexdigest()
        key = "searchfeedback-%s-%s-%s" % (timestamp[:10], request.ip, hex)
        d['timestamp'] = timestamp
        d['approval'] = approval
        g.hardcache.set(key, d, time=86400 * 7)

    @require_oauth2_scope("modposts")
    @noresponse(VUser(), VModhash(),
                VSrCanBan('id'),
                thing = VByName('id'),
                spam = VBoolean('spam', default=True))
    @api_doc(api_section.moderation)
    def POST_remove(self, thing, spam):
        """Remove a link or comment.

        If the thing is a link, it will be removed from all subreddit listings.
        If the thing is a comment, it will be redacted and removed from all
        subreddit comment listings.

        See also: [/api/approve](#POST_api_approve).

        """

        # Don't remove a promoted link
        if getattr(thing, "promoted", None):
            return

        filtered = thing._spam
        kw = {'target': thing}

        if filtered and spam:
            kw['details'] = 'confirm_spam'
            train_spam = False
        elif filtered and not spam:
            kw['details'] = 'remove'
            admintools.unspam(thing, unbanner=c.user.name, insert=False)
            train_spam = False
        elif not filtered and spam:
            kw['details'] = 'spam'
            train_spam = True
        elif not filtered and not spam:
            kw['details'] = 'remove'
            train_spam = False

        admintools.spam(thing, auto=False,
                        moderator_banned=not c.user_is_admin,
                        banner=c.user.name,
                        train_spam=train_spam)

        modified_thing = None
        if isinstance(thing, Link):
            modified_thing = thing
        elif isinstance(thing, Comment):
            modified_thing = Link._byID(thing.link_id)

        if modified_thing:
            set_last_modified(modified_thing, 'comments')
            LastModified.touch(modified_thing._fullname, 'Comments')

        if isinstance(thing, (Link, Comment)):
            sr = thing.subreddit_slow
            action = 'remove' + thing.__class__.__name__.lower()
            ModAction.create(sr, c.user, action, **kw)

    @require_oauth2_scope("modposts")
    @noresponse(VUser(), VModhash(),
                VSrCanBan('id'),
                thing = VByName('id'))
    @api_doc(api_section.moderation)
    def POST_approve(self, thing):
        """Approve a link or comment.

        If the thing was removed, it will be re-inserted into appropriate
        listings. Any reports on the approved thing will be discarded.

        See also: [/api/remove](#POST_api_remove).

        """
        if not thing: return
        if thing._deleted: return
        if c.user._spam: return
        kw = {'target': thing}
        if thing._spam:
            kw['details'] = 'unspam'
            train_spam = True
            insert = True
        else:
            kw['details'] = 'confirm_ham'
            train_spam = False
            insert = False

        admintools.unspam(thing, moderator_unbanned=not c.user_is_admin,
                          unbanner=c.user.name, train_spam=train_spam,
                          insert=insert)

        if isinstance(thing, (Link, Comment)):
            sr = thing.subreddit_slow
            action = 'approve' + thing.__class__.__name__.lower()
            ModAction.create(sr, c.user, action, **kw)

    @require_oauth2_scope("modposts")
    @noresponse(VUser(), VModhash(),
                VSrCanBan('id'),
                thing=VByName('id'))
    @api_doc(api_section.moderation)
    def POST_ignore_reports(self, thing):
        """Prevent future reports on a thing from causing notifications.

        Any reports made about a thing after this flag is set on it will not
        cause notifications or make the thing show up in the various moderation
        listings.

        See also: [/api/unignore_reports](#POST_api_unignore_reports).

        """
        if not thing: return
        if thing._deleted: return
        if thing.ignore_reports: return

        thing.ignore_reports = True
        thing._commit()

        sr = thing.subreddit_slow
        ModAction.create(sr, c.user, 'ignorereports', target=thing)

    @require_oauth2_scope("modposts")
    @noresponse(VUser(), VModhash(),
                VSrCanBan('id'),
                thing=VByName('id'))
    @api_doc(api_section.moderation)
    def POST_unignore_reports(self, thing):
        """Allow future reports on a thing to cause notifications.

        See also: [/api/ignore_reports](#POST_api_ignore_reports).

        """
        if not thing: return
        if thing._deleted: return
        if not thing.ignore_reports: return

        thing.ignore_reports = False
        thing._commit()

        sr = thing.subreddit_slow
        ModAction.create(sr, c.user, 'unignorereports', target=thing)

    @require_oauth2_scope("modposts")
    @validatedForm(VUser(), VModhash(),
                   VCanDistinguish(('id', 'how')),
                   thing = VByName('id'),
                   how = VOneOf('how', ('yes','no','admin','special')))
    @api_doc(api_section.moderation)
    def POST_distinguish(self, form, jquery, thing, how):
        """Distinguish a thing's author with a sigil.

        This can be useful to draw attention to and confirm the identity of the
        user in the context of a link or comment of theirs. The options for
        distinguish are as follows:

        * `yes` - add a moderator distinguish (`[M]`). only if the user is a
                  moderator of the subreddit the thing is in.
        * `no` - remove any distinguishes.
        * `admin` - add an admin distinguish (`[A]`). admin accounts only.
        * `special` - add a user-specific distinguish. depends on user.

        The first time a top-level comment is moderator distinguished, the
        author of the link the comment is in reply to will get a notification
        in their inbox.

        """
        if not thing:return

        log_modaction = True
        log_kw = {}
        send_message = False
        original = getattr(thing, 'distinguished', 'no')
        if how == original: # Distinguish unchanged
            log_modaction = False
        elif how in ('admin', 'special'): # Add admin/special
            log_modaction = False
            send_message = True
        elif (original in ('admin', 'special') and
                how == 'no'): # Remove admin/special
            log_modaction = False
        elif how == 'no': # From yes to no
            log_kw['details'] = 'remove'
        else: # From no to yes
            send_message = True

        # Send a message if this is a top-level comment on a submission that
        # does not have sendreplies set, if it's the first distinguish for this
        # comment, and if the user isn't banned or blocked by the author
        if isinstance(thing, Comment):
            link = Link._byID(thing.link_id, data=True)
            to = Account._byID(link.author_id, data=True)
            if (send_message and
                    thing.parent_id is None and
                    not link.sendreplies and
                    not hasattr(thing, 'distinguished') and
                    not c.user._spam and
                    c.user._id not in to.enemies and
                    to.name != c.user.name):
                inbox_rel = Inbox._add(to, thing, 'selfreply')
                queries.new_comment(thing, inbox_rel)

        thing.distinguished = how
        thing._commit()

        hooks.get_hook("thing.distinguish").call(thing=thing)

        wrapper = default_thing_wrapper(expand_children = True)
        w = wrap_links(thing, wrapper)
        jquery(".content").replace_things(w, True, True)
        jquery(".content .link .rank").hide()
        if log_modaction:
            sr = thing.subreddit_slow
            ModAction.create(sr, c.user, 'distinguish', target=thing, **log_kw)

    @require_oauth2_scope("save")
    @json_validate(VUser())
    @api_doc(api_section.links_and_comments, extensions=["json"])
    def GET_saved_categories(self, responder):
        """Get a list of categories in which things are currently saved.

        See also: [/api/save](#POST_api_save).

        """
        if not c.user.gold:
            abort(403)
        categories = LinkSavesByCategory.get_saved_categories(c.user)
        categories += CommentSavesByCategory.get_saved_categories(c.user)
        categories = sorted(set(categories), key=lambda name: name.lower())
        categories = [dict(category=category) for category in categories]
        return {'categories': categories}

    @require_oauth2_scope("save")
    @noresponse(VUser(),
                VModhash(),
                category = VSavedCategory('category'),
                thing = VByName('id'))
    @api_doc(api_section.links_and_comments)
    def POST_save(self, thing, category):
        """Save a link or comment.

        Saved things are kept in the user's saved listing for later perusal.

        See also: [/api/unsave](#POST_api_unsave).

        """
        if not thing: return
        if category and not c.user.gold:
            category = None
        if ('BAD_SAVE_CATEGORY', 'category') in c.errors:
            abort(403)
        thing._save(c.user, category=category)

    @require_oauth2_scope("save")
    @noresponse(VUser(),
                VModhash(),
                thing = VByName('id'))
    @api_doc(api_section.links_and_comments)
    def POST_unsave(self, thing):
        """Unsave a link or comment.

        This removes the thing from the user's saved listings as well.

        See also: [/api/save](#POST_api_save).

        """
        if not thing: return
        thing._unsave(c.user)

    def collapse_handler(self, things, collapse):
        if not things:
            return
        things = tup(things)
        srs = Subreddit._byID([t.sr_id for t in things if t.sr_id],
                              return_dict = True)
        for t in things:
            if hasattr(t, "to_id") and c.user._id == t.to_id:
                t.to_collapse = collapse
            elif hasattr(t, "author_id") and c.user._id == t.author_id:
                t.author_collapse = collapse
            elif isinstance(t, Message) and t.sr_id:
                if srs[t.sr_id].is_moderator(c.user):
                    t.to_collapse = collapse
            t._commit()

    @noresponse(VUser(),
                VModhash(),
                things = VByName('id', multiple = True))
    def POST_collapse_message(self, things):
        self.collapse_handler(things, True)

    @noresponse(VUser(),
                VModhash(),
                things = VByName('id', multiple = True))
    def POST_uncollapse_message(self, things):
        self.collapse_handler(things, False)

    def unread_handler(self, things, unread):
        if not things:
            if (errors.TOO_MANY_THING_IDS, 'id') in c.errors:
                return abort(413)
            else:
                return abort(400)

        sr_messages = defaultdict(list)
        comments = []
        messages = []
        # Group things by subreddit or type
        for thing in things:
            if isinstance(thing, Message):
                if getattr(thing, 'sr_id', False):
                    sr_messages[thing.sr_id].append(thing)
                else:
                    messages.append(thing)
            else:
                comments.append(thing)

        if sr_messages:
            mod_srs = Subreddit.reverse_moderator_ids(c.user)
            srs = Subreddit._byID(sr_messages.keys())
        else:
            mod_srs = []

        # Batch set items as unread
        for sr_id, things in sr_messages.items():
            # Remove the item(s) from the user's inbox
            queries.set_unread(things, c.user, unread)
            if sr_id in mod_srs:
                # Only moderators can change the read status of that
                # message in the modmail inbox
                sr = srs[sr_id]
                queries.set_unread(things, sr, unread)
        if comments:
            queries.set_unread(comments, c.user, unread)
        if messages:
            queries.set_unread(messages, c.user, unread)


    @require_oauth2_scope("privatemessages")
    @noresponse(VUser(),
                VModhash(),
                things = VByName('id', multiple=True, limit=25))
    @api_doc(api_section.messages)
    def POST_unread_message(self, things):
        self.unread_handler(things, True)

    @require_oauth2_scope("privatemessages")
    @noresponse(VUser(),
                VModhash(),
                things = VByName('id', multiple=True, limit=25))
    @api_doc(api_section.messages)
    def POST_read_message(self, things):
        self.unread_handler(things, False)

    @require_oauth2_scope("report")
    @noresponse(VUser(),
                VModhash(),
                thing = VByName('id', thing_cls=Link))
    @api_doc(api_section.links_and_comments)
    def POST_hide(self, thing):
        """Hide a link.

        This removes it from the user's default view of subreddit listings.

        See also: [/api/unhide](#POST_api_unhide).

        """
        if not thing: return
        thing._hide(c.user)

    @require_oauth2_scope("report")
    @noresponse(VUser(),
                VModhash(),
                thing = VByName('id'))
    @api_doc(api_section.links_and_comments)
    def POST_unhide(self, thing):
        """Unhide a link.

        See also: [/api/hide](#POST_api_hide).

        """
        if not thing: return
        thing._unhide(c.user)


    @validatedForm(VUser(),
                   parent = VByName('parent_id'))
    def POST_moremessages(self, form, jquery, parent):
        if not parent.can_view_slow():
            return abort(403, 'forbidden')

        if parent.sr_id:
            builder = SrMessageBuilder(parent.subreddit_slow,
                                       parent = parent, skip = False)
        else:
            builder = UserMessageBuilder(c.user, parent = parent, skip = False)
        listing = Listing(builder).listing()
        a = []
        for item in listing.things:
            a.append(item)
            for x in item.child.things:
                a.append(x)
        for item in a:
            if hasattr(item, "child"):
                item.child = None
        jquery.things(parent._fullname).parent().replace_things(a, False, True)

    @require_oauth2_scope("read")
    @validatedForm(link = VByName('link_id'),
                   sort = VMenu('where', CommentSortMenu),
                   children = VCommentIDs('children'),
                   pv_hex=VPrintable("pv_hex", 40, docs={"pv_hex":
                       "(optional) a previous-visits token"}),
                   mc_id=nop("id", docs={"id":
                       "(optional) id of the associated MoreChildren object"}),
                  )
    @api_doc(api_section.links_and_comments)
    def POST_morechildren(self, form, jquery, link, sort, children,
                          pv_hex, mc_id):
        """Retrieve additional comments omitted from a base comment tree.

        When a comment tree is rendered, the most relevant comments are
        selected for display first. Remaining comments are stubbed out with
        "MoreComments" links. This API call is used to retrieve the additional
        comments represented by those stubs, up to 20 at a time.

        The two core parameters required are `link` and `children`.  `link` is
        the fullname of the link whose comments are being fetched. `children`
        is a comma-delimited list of comment ID36s that need to be fetched.

        If `id` is passed, it should be the ID of the MoreComments object this
        call is replacing. This is needed only for the HTML UI's purposes and
        is optional otherwise.

        `pv_hex` is part of the reddit gold "previous visits" feature. It is
        optional and deprecated.

        **NOTE:** you may only make one request at a time to this API endpoint.
        Higher concurrency will result in an error being returned.

        """

        CHILD_FETCH_COUNT = 20

        lock = None
        if c.user_is_loggedin:
            lock = g.make_lock("morechildren", "morechildren-" + c.user.name,
                               timeout=0)
            try:
                lock.acquire()
            except TimeoutExpired:
                abort(429)

        try:
            if not link or not link.subreddit_slow.can_view(c.user):
                return abort(403,'forbidden')

            if pv_hex:
                c.previous_visits = g.cache.get(pv_hex)

            if children:
                builder = CommentBuilder(link, CommentSortMenu.operator(sort),
                                         children=children,
                                         num=CHILD_FETCH_COUNT)
                listing = Listing(builder, nextprev = False)
                items = listing.get_items()
                def _children(cur_items):
                    items = []
                    for cm in cur_items:
                        items.append(cm)
                        if hasattr(cm, 'child'):
                            if hasattr(cm.child, 'things'):
                                items.extend(_children(cm.child.things))
                                cm.child = None
                            else:
                                items.append(cm.child)

                    return items
                # assumes there is at least one child
                # a = _children(items[0].child.things)
                a = []
                for item in items:
                    a.append(item)
                    if hasattr(item, 'child'):
                        a.extend(_children(item.child.things))
                        item.child = None

                # the result is not always sufficient to replace the
                # morechildren link
                jquery.things(str(mc_id)).remove()
                jquery.insert_things(a, append = True)

                if pv_hex:
                    jquery.rehighlight_new_comments()
        finally:
            if lock:
                lock.release()


    @validate(uh = nop('uh'), # VModHash() will raise, check manually
              action = VOneOf('what', ('like', 'dislike', 'save')),
              url=VUrl('u'))
    def GET_bookmarklet(self, action, uh, url):
        '''Controller for the functionality of the bookmarklets (not
        the distribution page)'''

        # the redirect handler will clobber the extension if not told otherwise
        c.extension = "png"

        if not c.user_is_loggedin:
            return self.redirect("/static/css_login.png")
        # check the modhash (or force them to get new bookmarlets)
        elif not c.user.valid_hash(uh) or not action:
            return self.redirect("/static/css_update.png")
        # unlike most cases, if not already submitted, error.
        elif url:
            sr = c.site if not isinstance(c.site, FakeSubreddit) else None
            try:
                links_for_url = Link._by_url(url, sr)
            except NotFound:
                links_for_url = []

            # check permissions on those links to make sure votes will count
            Subreddit.load_subreddits(links_for_url, return_dict = False)
            user = c.user if c.user_is_loggedin else None
            links = [link for link in links_for_url
                          if link.subreddit_slow.can_view(user)]

            if links:
                if action in ['like', 'dislike']:
                    #vote up all of the links
                    for link in links:
                        queries.queue_vote(c.user, link,
                                           action == 'like', request.ip,
                                           cheater=c.cheater)
                elif action == 'save':
                    link = max(links, key = lambda x: x._score)
                    link._save(c.user)
                return self.redirect("/static/css_%sd.png" % action)
        return self.redirect("/static/css_submit.png")


    @validatedForm(VUser(),
                   code = VPrintable("code", 30))
    def POST_claimgold(self, form, jquery, code):
        status = ''
        if not code:
            c.errors.add(errors.NO_TEXT, field = "code")
            form.has_errors("code", errors.NO_TEXT)
            return

        rv = claim_gold(code, c.user._id)

        if rv is None:
            c.errors.add(errors.INVALID_CODE, field = "code")
            log_text ("invalid gold claim",
                      "%s just tried to claim %s" % (c.user.name, code),
                      "info")
        elif rv == "already claimed":
            c.errors.add(errors.CLAIMED_CODE, field = "code")
            log_text ("invalid gold reclaim",
                      "%s just tried to reclaim %s" % (c.user.name, code),
                      "info")
        else:
            days, subscr_id = rv
            if days <= 0:
                raise ValueError("days = %r?" % days)

            log_text ("valid gold claim",
                      "%s just claimed %s" % (c.user.name, code),
                      "info")

            if subscr_id:
                c.user.gold_subscr_id = subscr_id

            if code.startswith("cr_"):
                c.user.gold_creddits += int(days / 31)
                c.user._commit()
                status = 'claimed-creddits'
            else:
                admintools.engolden(c.user, days)

                g.cache.set("recent-gold-" + c.user.name, True, 600)
                status = 'claimed-gold'
                jquery(".lounge").show()

        # Activate any errors we just manually set
        if not form.has_errors("code", errors.INVALID_CODE, errors.CLAIMED_CODE,
                               errors.NO_TEXT):
            form.redirect("/gold/thanks?v=%s" % status)

    @validatedForm(
        VRatelimit(rate_ip=True, prefix="rate_password_"),
        user=VUserWithEmail('name'),
    )
    def POST_password(self, form, jquery, user):
        if form.has_errors('name', errors.USER_DOESNT_EXIST):
            return
        elif form.has_errors('name', errors.NO_EMAIL_FOR_USER):
            return
        elif form.has_errors('ratelimit', errors.RATELIMIT):
            return
        else:
            VRatelimit.ratelimit(rate_ip=True, prefix="rate_password_")
            if emailer.password_email(user):
                form.set_html(".status",
                      _("an email will be sent to that account's address shortly"))
            else:
                form.set_html(".status", _("try again tomorrow"))


    @validatedForm(token=VOneTimeToken(PasswordResetToken, "key"),
                   password=VPassword(["passwd", "passwd2"]))
    def POST_resetpassword(self, form, jquery, token, password):
        # was the token invalid or has it expired?
        if not token:
            form.redirect("/password?expired=true")
            return

        # did they fill out the password form correctly?
        form.has_errors("passwd",  errors.BAD_PASSWORD)
        form.has_errors("passwd2", errors.BAD_PASSWORD_MATCH)
        if form.has_error():
            return

        # at this point, we should mark the token used since it's either
        # valid now or will never be valid again.
        token.consume()

        # load up the user and check that things haven't changed
        user = Account._by_fullname(token.user_id)
        if not token.valid_for_user(user):
            form.redirect('/password?expired=true')
            return

        # Prevent banned users from resetting, and thereby logging in
        if user._banned:
            return

        # successfully entered user name and valid new password
        change_password(user, password)
        if user.email:
            emailer.password_change_email(user)
        g.log.warning("%s did a password reset for %s via %s",
                      request.ip, user.name, token._id)

        # if the token is for the current user, their cookies will be
        # invalidated and they'll have to log in again.
        if not c.user_is_loggedin or c.user._fullname == token.user_id:
            jquery.redirect('/login')

        form.set_html(".status", _("password updated"))

    @require_oauth2_scope("subscribe")
    @noresponse(VUser(),
                VModhash(),
                action = VOneOf('action', ('sub', 'unsub')),
                sr = VSubscribeSR('sr', 'sr_name'))
    @api_doc(api_section.subreddits)
    def POST_subscribe(self, action, sr):
        """Subscribe to or unsubscribe from a subreddit.

        To subscribe, `action` should be `sub`. To unsubscribe, `action` should
        be `unsub`. The user must have access to the subreddit to be able to
        subscribe to it.

        See also: [/subreddits/mine/](#GET_subreddits_mine_{where}).

        """

        if not sr:
            return abort(404, 'not found')
        elif action == "sub" and not sr.can_comment(c.user):
            return abort(403, 'permission denied')

        try:
            Subreddit.subscribe_defaults(c.user)

            if action == "sub":
                if sr.add_subscriber(c.user):
                    sr._incr('_ups', 1)
                else:
                    # tried to subscribe but user was already subscribed
                    pass
            else:
                if sr.remove_subscriber(c.user):
                    sr._incr('_ups', -1)
                else:
                    # tried to unsubscribe but user was not subscribed
                    return abort(404, 'not found')
            changed(sr, True)
        except CreationError:
            # This only seems to happen when someone is pounding on the
            # subscribe button or the DBs are really lagged; either way,
            # some other proc has already handled this subscribe request.
            return

    @validatedForm(VAdmin(),
                   VModhash(),
                   hexkey=VLength("hexkey", max_length=32),
                   nickname=VLength("nickname", max_length = 1000),
                   status = VOneOf("status",
                      ("new", "severe", "interesting", "normal", "fixed")))
    def POST_edit_error(self, form, jquery, hexkey, nickname, status):
        if form.has_errors(("hexkey", "nickname", "status"),
                           errors.NO_TEXT, errors.INVALID_OPTION):
            pass

        if form.has_error():
            return

        key = "error_nickname-%s" % str(hexkey)
        g.hardcache.set(key, nickname, 86400 * 365)

        key = "error_status-%s" % str(hexkey)
        g.hardcache.set(key, status, 86400 * 365)

        form.set_html(".status", _('saved'))

    @validatedForm(VAdmin(),
                   VModhash(),
                   award=VByName("fullname"),
                   colliding_award=VAwardByCodename(("codename", "fullname")),
                   codename=VLength("codename", max_length = 100),
                   title=VLength("title", max_length = 100),
                   awardtype=VOneOf("awardtype",
                                    ("regular", "manual", "invisible")),
                   api_ok=VBoolean("api_ok"),
                   imgurl=VLength("imgurl", max_length = 1000))
    def POST_editaward(self, form, jquery, award, colliding_award, codename,
                       title, awardtype, api_ok, imgurl):
        if form.has_errors(("codename", "title", "awardtype", "imgurl"),
                           errors.NO_TEXT):
            pass

        if awardtype is None:
            form.set_html(".status", "bad awardtype")
            return

        if form.has_errors(("codename"), errors.INVALID_OPTION):
            form.set_html(".status", "some other award has that codename")
            pass

        url_ok = True

        if not imgurl.startswith("//"):
            url_ok = False
            form.set_html(".status", "the url must be protocol-relative")

        try:
            imgurl % 1
        except TypeError:
            url_ok = False
            form.set_html(".status", "the url must have a %d for size")

        if not url_ok:
            c.errors.add(errors.BAD_URL, field="imgurl")
            form.has_errors("imgurl", errors.BAD_URL)

        if form.has_error():
            return

        if award is None:
            Award._new(codename, title, awardtype, imgurl, api_ok)
            form.set_html(".status", "saved. reload to see it.")
            return

        award.codename = codename
        award.title = title
        award.awardtype = awardtype
        award.imgurl = imgurl
        award.api_ok = api_ok
        award._commit()
        form.set_html(".status", _('saved'))

    @require_oauth2_scope("modflair")
    @validatedForm(VSrModerator(perms='flair'),
                   VModhash(),
                   user = VFlairAccount("name"),
                   link = VFlairLink('link'),
                   text = VFlairText("text"),
                   css_class = VFlairCss("css_class"))
    @api_doc(api_section.flair, uses_site=True)
    def POST_flair(self, form, jquery, user, link, text, css_class):
        if link:
            flair_type = LINK_FLAIR
            if hasattr(c.site, '_id') and c.site._id == link.sr_id:
                site = c.site
            else:
                site = Subreddit._byID(link.sr_id, data=True)
                # make sure c.user has permission to set flair on this link
                if not (c.user_is_admin 
                        or site.is_moderator_with_perms(c.user, 'flair')):
                    abort(403, 'forbidden')
        else:
            flair_type = USER_FLAIR
            site = c.site
            if form.has_errors('name', errors.BAD_FLAIR_TARGET):
                return

        if form.has_errors('css_class', errors.BAD_CSS_NAME):
            form.set_html(".status:first", _('invalid css class'))
            return
        if form.has_errors('css_class', errors.TOO_MUCH_FLAIR_CSS):
            form.set_html(".status:first", _('too many css classes'))
            return

        if flair_type == LINK_FLAIR:
            if not text and not css_class:
                text = css_class = None
            link.flair_text = text
            link.flair_css_class = css_class
            link._commit()
            changed(link)
            ModAction.create(site, c.user, action='editflair', target=link,
                             details='flair_edit')
        elif flair_type == USER_FLAIR:
            if not text and not css_class:
                # empty text and css is equivalent to unflairing
                text = css_class = None
                c.site.remove_flair(user)
                jquery('#flairrow_%s' % user._id36).hide()
                new = False
            elif not c.site.is_flair(user):
                c.site.add_flair(user)
                new = True
            else:
                new = False

            # Save the flair details in the account data.
            setattr(user, 'flair_%s_text' % c.site._id, text)
            setattr(user, 'flair_%s_css_class' % c.site._id, css_class)
            user._commit()

            if c.user != user:
                ModAction.create(site, c.user, action='editflair',
                                 target=user, details='flair_edit')

            if new:
                jquery.redirect('?name=%s' % user.name)
            else:
                flair = WrappedUser(
                    user, force_show_flair=True,
                    include_flair_selector=True).render(style='html')
                jquery('.tagline .flairselectable.id-%s'
                    % user._fullname).parent().html(flair)
                jquery('input[name="text"]').data('saved', text)
                jquery('input[name="css_class"]').data('saved', css_class)
                form.set_html('.status', _('saved'))

    @require_oauth2_scope("modflair")
    @validatedForm(VSrModerator(perms='flair'),
                   VModhash(),
                   user = VFlairAccount("name"))
    @api_doc(api_section.flair, uses_site=True)
    def POST_deleteflair(self, form, jquery, user):
        # Check validation.
        if form.has_errors('name', errors.USER_DOESNT_EXIST, errors.NO_USER):
            return
        c.site.remove_flair(user)
        setattr(user, 'flair_%s_text' % c.site._id, None)
        setattr(user, 'flair_%s_css_class' % c.site._id, None)
        user._commit()

        ModAction.create(c.site, c.user, action='editflair', target=user,
                         details='flair_delete')

        jquery('#flairrow_%s' % user._id36).remove()
        unflair = WrappedUser(
            user, include_flair_selector=True).render(style='html')
        jquery('.tagline .id-%s' % user._fullname).parent().html(unflair)

    @require_oauth2_scope("modflair")
    @validate(VSrModerator(perms='flair'),
              VModhash(),
              flair_csv = nop('flair_csv'))
    @api_doc(api_section.flair, uses_site=True)
    def POST_flaircsv(self, flair_csv):
        limit = 100  # max of 100 flair settings per call
        results = FlairCsv()
        # encode to UTF-8, since csv module doesn't fully support unicode
        infile = csv.reader(flair_csv.strip().encode('utf-8').split('\n'))
        for i, row in enumerate(infile):
            line_result = results.add_line()
            line_no = i + 1
            if line_no > limit:
                line_result.error('row',
                                  'limit of %d rows per call reached' % limit)
                break

            try:
                name, text, css_class = row
            except ValueError:
                line_result.error('row', 'improperly formatted row, ignoring')
                continue

            user = VFlairAccount('name').run(name)
            if not user:
                line_result.error('user',
                                  "unable to resolve user `%s', ignoring"
                                  % name)
                continue

            if not text and not css_class:
                # this is equivalent to unflairing
                text = None
                css_class = None

            orig_text = text
            text = VFlairText('text').run(orig_text)
            if text and orig_text and len(text) < len(orig_text):
                line_result.warn('text',
                                 'truncating flair text to %d chars'
                                 % len(text))

            if css_class and not VFlairCss('css_class').run(css_class):
                line_result.error('css',
                                  "invalid css class `%s', ignoring"
                                  % css_class)
                continue

            # all validation passed, enflair the user
            if text or css_class:
                mode = 'added'
                c.site.add_flair(user)
            else:
                mode = 'removed'
                c.site.remove_flair(user)
            setattr(user, 'flair_%s_text' % c.site._id, text)
            setattr(user, 'flair_%s_css_class' % c.site._id, css_class)
            user._commit()

            line_result.status = '%s flair for user %s' % (mode, user.name)
            line_result.ok = True

        ModAction.create(c.site, c.user, action='editflair',
                         details='flair_csv')

        return BoringPage(_("API"), content = results).render()

    @require_oauth2_scope("flair")
    @validatedForm(VUser(),
                   VModhash(),
                   flair_enabled = VBoolean("flair_enabled"))
    @api_doc(api_section.flair, uses_site=True)
    def POST_setflairenabled(self, form, jquery, flair_enabled):
        setattr(c.user, 'flair_%s_enabled' % c.site._id, flair_enabled)
        c.user._commit()
        jquery.refresh()

    @require_oauth2_scope("modflair")
    @validatedForm(
        VSrModerator(perms='flair'),
        VModhash(),
        flair_enabled = VBoolean("flair_enabled"),
        flair_position = VOneOf("flair_position", ("left", "right")),
        link_flair_position = VOneOf("link_flair_position",
                                     ("", "left", "right")),
        flair_self_assign_enabled = VBoolean("flair_self_assign_enabled"),
        link_flair_self_assign_enabled =
            VBoolean("link_flair_self_assign_enabled"))
    @api_doc(api_section.flair, uses_site=True)
    def POST_flairconfig(self, form, jquery, flair_enabled, flair_position,
                         link_flair_position, flair_self_assign_enabled,
                         link_flair_self_assign_enabled):
        if c.site.flair_enabled != flair_enabled:
            c.site.flair_enabled = flair_enabled
            ModAction.create(c.site, c.user, action='editflair',
                             details='flair_enabled')
        if c.site.flair_position != flair_position:
            c.site.flair_position = flair_position
            ModAction.create(c.site, c.user, action='editflair',
                             details='flair_position')
        if c.site.link_flair_position != link_flair_position:
            c.site.link_flair_position = link_flair_position
            ModAction.create(c.site, c.user, action='editflair',
                             details='link_flair_position')
        if c.site.flair_self_assign_enabled != flair_self_assign_enabled:
            c.site.flair_self_assign_enabled = flair_self_assign_enabled
            ModAction.create(c.site, c.user, action='editflair',
                             details='flair_self_enabled')
        if (c.site.link_flair_self_assign_enabled
            != link_flair_self_assign_enabled):
            c.site.link_flair_self_assign_enabled = (
                link_flair_self_assign_enabled)
            ModAction.create(c.site, c.user, action='editflair',
                             details='link_flair_self_enabled')
        c.site._commit()
        jquery.refresh()

    @require_oauth2_scope("modflair")
    @paginated_listing(max_page_size=1000)
    @validate(
        VSrModerator(perms='flair'),
        user=VFlairAccount('name'),
    )
    @api_doc(api_section.flair, uses_site=True)
    def GET_flairlist(self, num, after, reverse, count, user):
        flair = FlairList(num, after, reverse, '', user)
        return BoringPage(_("API"), content = flair).render()

    @require_oauth2_scope("modflair")
    @validatedForm(VSrModerator(perms='flair'),
                   VModhash(),
                   flair_template = VFlairTemplateByID('flair_template_id'),
                   text = VFlairText('text'),
                   css_class = VFlairCss('css_class'),
                   text_editable = VBoolean('text_editable'),
                   flair_type = VOneOf('flair_type', (USER_FLAIR, LINK_FLAIR),
                                       default=USER_FLAIR))
    @api_doc(api_section.flair, uses_site=True)
    def POST_flairtemplate(self, form, jquery, flair_template, text,
                           css_class, text_editable, flair_type):
        if text is None:
            text = ''
        if css_class is None:
            css_class = ''

        # Check validation.
        if form.has_errors('css_class', errors.BAD_CSS_NAME):
            form.set_html(".status:first", _('invalid css class'))
            return
        if form.has_errors('css_class', errors.TOO_MUCH_FLAIR_CSS):
            form.set_html(".status:first", _('too many css classes'))
            return

        # Load flair template thing.
        if flair_template:
            flair_template.text = text
            flair_template.css_class = css_class
            flair_template.text_editable = text_editable
            flair_template._commit()
            new = False
        else:
            try:
                flair_template = FlairTemplateBySubredditIndex.create_template(
                    c.site._id, text=text, css_class=css_class,
                    text_editable=text_editable,
                    flair_type=flair_type)
            except OverflowError:
                form.set_html(".status:first", _('max flair templates reached'))
                return

            new = True

        # Push changes back to client.
        if new:
            empty_ids = {
                USER_FLAIR: '#empty-user-flair-template',
                LINK_FLAIR: '#empty-link-flair-template',
            }
            empty_id = empty_ids[flair_type]
            jquery(empty_id).before(
                FlairTemplateEditor(flair_template, flair_type)
                .render(style='html'))
            empty_template = FlairTemplate()
            empty_template._committed = True  # to disable unnecessary warning
            jquery(empty_id).html(
                FlairTemplateEditor(empty_template, flair_type)
                .render(style='html'))
            form.set_html('.status', _('saved'))
        else:
            jquery('#%s' % flair_template._id).html(
                FlairTemplateEditor(flair_template, flair_type)
                .render(style='html'))
            form.set_html('.status', _('saved'))
            jquery('input[name="text"]').data('saved', text)
            jquery('input[name="css_class"]').data('saved', css_class)
        ModAction.create(c.site, c.user, action='editflair',
                             details='flair_template')

    @require_oauth2_scope("modflair")
    @validatedForm(VSrModerator(perms='flair'),
                   VModhash(),
                   flair_template = VFlairTemplateByID('flair_template_id'))
    @api_doc(api_section.flair, uses_site=True)
    def POST_deleteflairtemplate(self, form, jquery, flair_template):
        idx = FlairTemplateBySubredditIndex.by_sr(c.site._id)
        if idx.delete_by_id(flair_template._id):
            jquery('#%s' % flair_template._id).parent().remove()
            ModAction.create(c.site, c.user, action='editflair',
                             details='flair_delete_template')

    @require_oauth2_scope("modflair")
    @validatedForm(VSrModerator(perms='flair'), VModhash(),
                   flair_type = VOneOf('flair_type', (USER_FLAIR, LINK_FLAIR),
                                       default=USER_FLAIR))
    @api_doc(api_section.flair, uses_site=True)
    def POST_clearflairtemplates(self, form, jquery, flair_type):
        FlairTemplateBySubredditIndex.clear(c.site._id, flair_type=flair_type)
        jquery.refresh()
        ModAction.create(c.site, c.user, action='editflair',
                         details='flair_clear_template')

    @require_oauth2_scope("flair")
    @validate(VUser(),
              user = VFlairAccount('name'),
              link = VFlairLink('link'))
    @api_doc(api_section.flair, uses_site=True)
    def POST_flairselector(self, user, link):
        """Return information about a users's flair options.

        If `link` is given, return link flair options.
        Otherwise, return user flair options for this subreddit.

        The logged in user's flair is also returned.
        Subreddit moderators may give a user by `name` to instead
        retrieve that user's flair.

        """
        if link:
            if not (c.user_is_admin or link.can_flair_slow(c.user)):
                abort(403)

            return FlairSelector(link=link, site=link.subreddit_slow).render()

        if user and not (c.user_is_admin
                         or c.site.is_moderator_with_perms(c.user, 'flair')):
            # ignore user parameter if c.user is not mod/admin
            user = None
        return FlairSelector(user=user).render()

    @require_oauth2_scope("flair")
    @validatedForm(VUser(),
                   VModhash(),
                   user = VFlairAccount('name'),
                   link = VFlairLink('link'),
                   flair_template_id = nop('flair_template_id'),
                   text = VFlairText('text'))
    @api_doc(api_section.flair, uses_site=True)
    def POST_selectflair(self, form, jquery, user, link, flair_template_id,
                         text):
        if link:
            flair_type = LINK_FLAIR
            if hasattr(c.site, '_id') and c.site._id == link.sr_id:
                site = c.site
            else:
                site = Subreddit._byID(link.sr_id, data=True)
            self_assign_enabled = site.link_flair_self_assign_enabled
        else:
            flair_type = USER_FLAIR
            site = c.site
            self_assign_enabled = site.flair_self_assign_enabled

        if flair_template_id:
            try:
                flair_template = FlairTemplateBySubredditIndex.get_template(
                    site._id, flair_template_id, flair_type=flair_type)
            except NotFound:
                # TODO: serve error to client
                g.log.debug('invalid flair template for subreddit %s', site._id)
                return
        else:
            flair_template = None
            text = None

        if not (c.user_is_admin
                or site.is_moderator_with_perms(c.user, 'flair')):
            if not self_assign_enabled:
                # TODO: serve error to client
                g.log.debug('flair self-assignment not permitted')
                return

            # Ignore user choice if not an admin or mod.
            user = c.user

            # Ignore given text if user doesn't have permission to customize it.
            if not (flair_template and flair_template.text_editable):
                text = None

        if not text:
            text = flair_template.text if flair_template else None

        css_class = flair_template.css_class if flair_template else None
        text_editable = (
            flair_template.text_editable if flair_template else False)

        if flair_type == USER_FLAIR:
            site.add_flair(user)
            setattr(user, 'flair_%s_text' % site._id, text)
            setattr(user, 'flair_%s_css_class' % site._id, css_class)
            user._commit()

            if ((c.user_is_admin
                 or site.is_moderator_with_perms(c.user, 'flair'))
                and c.user != user):
                ModAction.create(site, c.user, action='editflair',
                                 target=user, details='flair_edit')

            # Push some client-side updates back to the browser.
            u = WrappedUser(user, force_show_flair=True,
                            flair_text_editable=text_editable,
                            include_flair_selector=True)
            flair = u.render(style='html')
            jquery('.tagline .flairselectable.id-%s'
                % user._fullname).parent().html(flair)
            jquery('#flairrow_%s input[name="text"]' % user._id36).data(
                'saved', text).val(text)
            jquery('#flairrow_%s input[name="css_class"]' % user._id36).data(
                'saved', css_class).val(css_class)
        elif flair_type == LINK_FLAIR:
            link.flair_text = text
            link.flair_css_class = css_class
            link._commit()
            changed(link)

            if c.user_is_admin or site.is_moderator_with_perms(c.user, 'flair'):
                ModAction.create(site, c.user, action='editflair',
                                 target=link, details='flair_edit')

            # Push some client-side updates back to the browser.

            jquery('.id-%s .entry .linkflairlabel' % link._fullname).remove()
            title_path = '.id-%s .entry > .title > .title' % link._fullname

            # TODO: move this to a template
            if flair_template:
                flair = '<span class="linkflairlabel %s">%s</span>' % (
                    ' '.join('linkflair-' + c for c in css_class.split()),
                    websafe(text))
                if site.link_flair_position == 'left':
                    jquery(title_path).before(flair)
                elif site.link_flair_position == 'right':
                    jquery(title_path).after(flair)

            # TODO: close the selector popup more gracefully
            jquery('body').click()

    @validatedForm(secret_used=VAdminOrAdminSecret("secret"),
                   award=VByName("fullname"),
                   description=VLength("description", max_length=1000),
                   url=VLength("url", max_length=1000),
                   recipient=VExistingUname("recipient"))
    def POST_givetrophy(self, form, jquery, secret_used, award, description,
                        url, recipient):
        if form.has_errors("recipient", errors.USER_DOESNT_EXIST,
                                        errors.NO_USER):
            pass

        if form.has_errors("fullname", errors.NO_TEXT, errors.NO_THING_ID):
            pass

        if secret_used and not award.api_ok:
            c.errors.add(errors.NO_API, field='secret')
            form.has_errors('secret', errors.NO_API)

        if form.has_error():
            return

        t = Trophy._new(recipient, award, description=description, url=url)

        form.set_html(".status", _('saved'))
        form._send_data(trophy_fn=t._id36)

    @validatedForm(secret_used=VAdminOrAdminSecret("secret"),
                   trophy = VTrophy("trophy_fn"))
    def POST_removetrophy(self, form, jquery, secret_used, trophy):
        if not trophy:
            return self.abort404()
        recipient = trophy._thing1
        award = trophy._thing2
        if secret_used and not award.api_ok:
            c.errors.add(errors.NO_API, field='secret')
            form.has_errors('secret', errors.NO_API)
        
        if form.has_error():
            return

        trophy._delete()
        Trophy.by_account(recipient, _update=True)
        Trophy.by_award(award, _update=True)

    @noresponse(VUser(),
              ui_elem = VOneOf('id', ('organic',)))
    def POST_disable_ui(self, ui_elem):
        if ui_elem:
            pref = "pref_%s" % ui_elem
            if getattr(c.user, pref):
                setattr(c.user, "pref_" + ui_elem, False)
                c.user._commit()

    @validatedForm(type = VOneOf('type', ('click'), default = 'click'),
                   links = VByName('ids', thing_cls = Link, multiple = True))
    def GET_gadget(self, form, jquery, type, links):
        if not links and type == 'click':
            # malformed cookie, clear it out
            set_user_cookie('recentclicks2', '')

        if not links:
            return

        content = ClickGadget(links).make_content()

        jquery('.gadget').show().find('.click-gadget').html(
            spaceCompress(content))

    @noresponse()
    def POST_tb_commentspanel_show(self):
        # this preference is allowed for non-logged-in users
        c.user.pref_frame_commentspanel = True
        c.user._commit()

    @noresponse()
    def POST_tb_commentspanel_hide(self):
        # this preference is allowed for non-logged-in users
        c.user.pref_frame_commentspanel = False
        c.user._commit()

    @require_oauth2_scope("read")
    @json_validate(query=VPrintable('query', max_length=50),
                   include_over_18=VBoolean('include_over_18', default=True))
    @api_doc(api_section.subreddits, extensions=["json"])
    def POST_search_reddit_names(self, responder, query, include_over_18):
        """List subreddit names that begin with a query string.

        Subreddits whose names begin with `query` will be returned. If
        `include_over_18` is false, subreddits with over-18 content
        restrictions will be filtered from the results.

        """
        names = []
        if query:
            names = search_reddits(query, include_over_18)

        return {'names': names}

    @validate(link = VByName('link_id', thing_cls = Link))
    def POST_expando(self, link):
        if not link:
            abort(404, 'not found')

        # pass through wrap_links/IDBuilder to ensure the user can view the link
        listing = wrap_links(link)
        try:
            wrapped_link = listing.things[0]
        except IndexError:
            wrapped_link = None

        if wrapped_link and wrapped_link.link_child:
            content = wrapped_link.link_child.content()
            return websafe(spaceCompress(content))
        else:
            abort(404, 'not found')

    @validatedForm(VUser('password', default=''),
                   VModhash(),
                   VOneTimePassword("otp",
                                    required=not g.disable_require_admin_otp),
                   remember=VBoolean("remember"),
                   dest=VDestination())
    def POST_adminon(self, form, jquery, remember, dest):
        if c.user.name not in g.admins:
            self.abort403()

        if form.has_errors('password', errors.WRONG_PASSWORD):
            return

        if form.has_errors("otp", errors.WRONG_PASSWORD,
                                  errors.NO_OTP_SECRET,
                                  errors.RATELIMIT):
            return

        if remember:
            self.remember_otp(c.user)

        self.enable_admin_mode(c.user)
        form.redirect(dest)

    @validatedForm(VUser("password", default=""),
                   VModhash())
    def POST_generate_otp_secret(self, form, jquery):
        if form.has_errors("password", errors.WRONG_PASSWORD):
            return

        secret = totp.generate_secret()
        g.cache.set('otp_secret_' + c.user._id36, secret, time=300)
        jquery("body").make_totp_qrcode(secret)

    @validatedForm(VUser(),
                   VModhash(),
                   otp=nop("otp"))
    def POST_enable_otp(self, form, jquery, otp):
        if form.has_errors("password", errors.WRONG_PASSWORD):
            return

        secret = g.cache.get("otp_secret_" + c.user._id36)
        if not secret:
            c.errors.add(errors.EXPIRED, field="otp")
            form.has_errors("otp", errors.EXPIRED)
            return

        if not VOneTimePassword.validate_otp(secret, otp):
            c.errors.add(errors.WRONG_PASSWORD, field="otp")
            form.has_errors("otp", errors.WRONG_PASSWORD)
            return

        c.user.otp_secret = secret
        c.user._commit()

        form.redirect("/prefs/otp")

    @validatedForm(VUser("password", default=""),
                   VOneTimePassword("otp", required=True),
                   VModhash())
    def POST_disable_otp(self, form, jquery):
        if form.has_errors("password", errors.WRONG_PASSWORD):
            return

        if form.has_errors("otp", errors.WRONG_PASSWORD,
                                  errors.NO_OTP_SECRET,
                                  errors.RATELIMIT):
            return

        c.user.otp_secret = ""
        c.user._commit()
        form.redirect("/prefs/otp")

    @require_oauth2_scope("read")
    @json_validate(query=VLength("query", max_length=50))
    @api_doc(api_section.subreddits, extensions=["json"])
    def GET_subreddits_by_topic(self, responder, query):
        """Return a list of subreddits that are relevant to a search query."""
        if not g.CLOUDSEARCH_SEARCH_API:
            return []

        query = query and query.strip()
        if not query or len(query) < 2:
            return []

        exclude = Subreddit.default_subreddits()

        faceting = {"reddit":{"sort":"-sum(text_relevance)", "count":20}}
        results = SearchQuery(query, sort="relevance", faceting=faceting,
                              syntax="plain").run()

        sr_results = []
        for sr, count in results.subreddit_facets:
            if (sr._id in exclude or (sr.over_18 and not c.over18)
                  or sr.type == "archived"):
                continue

            sr_results.append({
                "name": sr.name,
            })

        return sr_results

    @noresponse(VUser(),
                VModhash(),
                client=VOAuth2ClientID())
    @api_doc(api_section.apps)
    def POST_revokeapp(self, client):
        if client:
            client.revoke(c.user)

    @validatedForm(VUser(),
                   VModhash(),
                   name=VRequired('name', errors.NO_TEXT,
                                  docs=dict(name="a name for the app")),
                   about_url=VSanitizedUrl('about_url'),
                   icon_url=VSanitizedUrl('icon_url'),
                   redirect_uri=VRedirectUri('redirect_uri'),
                   app_type=VOneOf('app_type', ('web', 'installed', 'script')))
    @api_doc(api_section.apps)
    def POST_updateapp(self, form, jquery, name, about_url, icon_url,
                       redirect_uri, app_type):
        if (form.has_errors('name', errors.NO_TEXT) |
            form.has_errors('redirect_uri', errors.BAD_URL) |
            form.has_errors('redirect_uri', errors.NO_URL) |
            form.has_errors('app_type', errors.INVALID_OPTION)):
            return

        # Web apps should be redirecting to web
        if app_type == 'web':
            parsed = urlparse(redirect_uri)
            if parsed.scheme not in ('http', 'https'):
                c.errors.add(errors.INVALID_SCHEME, field='redirect_uri',
                        msg_params={"schemes": "http, https"})
                form.has_errors('redirect_uri', errors.INVALID_SCHEME)
                return

        description = request.POST.get('description', '')

        client_id = request.POST.get('client_id')
        if client_id:
            # client_id was specified, updating existing OAuth2Client
            client = OAuth2Client.get_token(client_id)
            if app_type != client.app_type:
                # App type cannot be changed after creation
                abort(400, "invalid request")
                return
            if not client:
                form.set_html('.status', _('invalid client id'))
                return
            if getattr(client, 'deleted', False):
                form.set_html('.status', _('cannot update deleted app'))
                return
            if not client.has_developer(c.user):
                form.set_html('.status', _('app does not belong to you'))
                return

            client.name = name
            client.description = description
            client.about_url = about_url or ''
            client.redirect_uri = redirect_uri
            client._commit()
            form.set_html('.status', _('application updated'))
            apps = PrefApps([], [client])
            jquery('#developed-app-%s' % client._id).replaceWith(
                apps.call('developed_app', client, collapsed=False))
        else:
            # client_id was omitted or empty, creating new OAuth2Client
            client = OAuth2Client._new(name=name,
                                       description=description,
                                       about_url=about_url or '',
                                       redirect_uri=redirect_uri,
                                       app_type=app_type)
            client._commit()
            client.add_developer(c.user)
            form.set_html('.status', _('application created'))
            apps = PrefApps([], [client])
            jquery('#developed-apps > h1').show()
            jquery('#developed-apps > ul').append(
                apps.call('developed_app', client, collapsed=False))

    @validatedForm(VUser(),
                   VModhash(),
                   client=VOAuth2ClientDeveloper(),
                   account=VExistingUname('name'))
    @api_doc(api_section.apps)
    def POST_adddeveloper(self, form, jquery, client, account):
        if not client:
            return
        if form.has_errors('name', errors.USER_DOESNT_EXIST, errors.NO_USER):
            return
        if client.has_developer(account):
            c.errors.add(errors.DEVELOPER_ALREADY_ADDED, field='name')
            form.set_error(errors.DEVELOPER_ALREADY_ADDED, 'name')
            return
        try:
            client.add_developer(account)
        except OverflowError:
            c.errors.add(errors.TOO_MANY_DEVELOPERS, field='')
            form.set_error(errors.TOO_MANY_DEVELOPERS, '')
            return

        form.set_html('.status', _('developer added'))
        apps = PrefApps([], [client])
        (jquery('#app-developer-%s input[name="name"]' % client._id).val('')
            .closest('.prefright').find('ul').append(
                apps.call('editable_developer', client, account)))

    @validatedForm(VUser(),
                   VModhash(),
                   client=VOAuth2ClientDeveloper(),
                   account=VExistingUname('name'))
    @api_doc(api_section.apps)
    def POST_removedeveloper(self, form, jquery, client, account):
        if client and account and not form.has_errors('name'):
            client.remove_developer(account)
            if account._id == c.user._id:
                jquery('#developed-app-%s' % client._id).fadeOut()
            else:
                jquery('li#app-dev-%s-%s' % (client._id, account._id)).fadeOut()

    @noresponse(VUser(),
                VModhash(),
                client=VOAuth2ClientDeveloper())
    @api_doc(api_section.apps)
    def POST_deleteapp(self, client):
        if client:
            client.deleted = True
            client._commit()

    @validatedMultipartForm(VUser(),
                            VModhash(),
                            client=VOAuth2ClientDeveloper(),
                            icon_file=VUploadLength(
                                'file', max_length=1024*128,
                                docs=dict(file="an icon (72x72)")))
    @api_doc(api_section.apps)
    def POST_setappicon(self, form, jquery, client, icon_file):
        if not icon_file:
            form.set_error(errors.TOO_LONG, 'file')
        if not form.has_error():
            filename = 'icon-%s' % client._id
            try:
                client.icon_url = media.upload_icon(filename, icon_file,
                                                    (72, 72))
            except IOError, ex:
                c.errors.add(errors.BAD_IMAGE,
                             msg_params=dict(message=ex.message),
                             field='file')
                form.set_error(errors.BAD_IMAGE, 'file')
            else:
                client._commit()
                form.set_html('.status', 'uploaded')
                jquery('#developed-app-%s .app-icon img'
                       % client._id).attr('src', client.icon_url)
                jquery('#developed-app-%s .ajax-upload-form'
                       % client._id).hide()
                jquery('#developed-app-%s .edit-app-icon-button'
                       % client._id).toggleClass('collapsed')

    @json_validate(VUser(),
                   VModhash(),
                   thing=VByName("thing"))
    def POST_generate_payment_blob(self, responder, thing):
        if not thing:
            abort(400, "Bad Request")

        if thing._deleted:
            abort(403, "Forbidden")

        thing_sr = Subreddit._byID(thing.sr_id, data=True)
        if (not thing_sr.can_view(c.user) or
            not thing_sr.allow_gilding):
            abort(403, "Forbidden")

        try:
            recipient = Account._byID(thing.author_id, data=True)
        except NotFound:
            self.abort404()

        if recipient._deleted:
            self.abort404()

        return generate_blob(dict(
            goldtype="gift",
            account_id=c.user._id,
            account_name=c.user.name,
            status="initialized",
            signed=False,
            recipient=recipient.name,
            giftmessage=None,
            thing=thing._fullname,
        ))

    @validate(srnames=VPrintable("srnames", max_length=2100))
    def POST_request_promo(self, srnames):
        if not srnames:
            return

        srnames = srnames.split('+')
        try:
            srnames.remove(Frontpage.name)
            srnames.append('')
        except ValueError:
            pass

        promo_tuples = promote.lottery_promoted_links(srnames, n=10)
        builder = CampaignBuilder(promo_tuples,
                                  wrap=default_thing_wrapper(),
                                  keep_fn=promote.promo_keep_fn,
                                  num=1,
                                  skip=True)
        listing = LinkListing(builder, nextprev=False).listing()
        if listing.things:
            w = listing.things[0]
            w.num = ""
            return spaceCompress(w.render())

    @json_validate(
        VUser(),
        VModhash(),
        collapsed=VBoolean('collapsed'),
    )
    def POST_set_left_bar_collapsed(self, responder, collapsed):
        c.user.pref_collapse_left_bar = collapsed
        c.user._commit()

    @require_oauth2_scope("read")
    @validate(srs=VSRByNames("srnames"),
              to_omit=VSRByNames("omit", required=False))
    @api_doc(api_section.subreddits, uri='/api/recommend/sr/{srnames}')
    def GET_subreddit_recommendations(self, srs, to_omit):
        """Return subreddits recommended for the given subreddit(s).

        Gets a list of subreddits recommended for `srnames`, filtering out any
        that appear in the optional `omit` param.

        """
        omit_id36s = [sr._id36 for sr in to_omit.values()]
        rec_srs = recommender.get_recommendations(srs.values(),
                                                  to_omit=omit_id36s)
        sr_data = [{'sr_name': sr.name} for sr in rec_srs]
        return json.dumps(sr_data)


    @validatedForm(VUser(),
                   VModhash(),
                   action=VOneOf("type", recommend.FEEDBACK_ACTIONS),
                   srs=VSRByNames("srnames"))
    def POST_rec_feedback(self, form, jquery, action, srs):
        if form.has_errors("type", errors.INVALID_OPTION):
            return self.abort404()
        AccountSRFeedback.record_feedback(c.user, srs.values(), action)


    @validatedForm(
        VUser(),
        VModhash(),
        seconds_visibility=VOneOf(
            "seconds_visibility",
            ("public", "private"),
            default="private",
        ),
    )
    def POST_server_seconds_visibility(self, form, jquery, seconds_visibility):
        c.user.pref_public_server_seconds = seconds_visibility == "public"
        c.user._commit()

    @require_oauth2_scope("save")
    @noresponse(VGold(),
                VModhash(),
                links = VByName('links', thing_cls=Link, multiple=True,
                                limit=100))
    @api_doc(api_section.links_and_comments)
    def POST_store_visits(self, links):
        if not c.user.pref_store_visits or not links:
            return

        LinkVisitsByAccount._visit(c.user, links)

    @validatedForm(
        VAdmin(),
        VModhash(),
        system=VLength('system', 1024),
        subject=VLength('subject', 1024),
        note=VLength('note', 10000),
        author=VLength('author', 1024),
    )
    def POST_add_admin_note(self, form, jquery, system, subject, note, author):
        if form.has_errors(('system', 'subject', 'note', 'author'),
                           errors.TOO_LONG):
            return

        if note:
            from r2.models.admin_notes import AdminNotesBySystem
            AdminNotesBySystem.add(system, subject, note, author)
        form.refresh()

########NEW FILE########
__FILENAME__ = user
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
from pylons import c, response
from r2.controllers.api_docs import api_doc, api_section
from r2.controllers.oauth2 import require_oauth2_scope
from r2.controllers.reddit_base import (
    abort_with_error,
    OAuth2ResourceController,
)
from r2.lib.jsontemplates import (
    FriendTableItemJsonTemplate,
    IdentityJsonTemplate,
    KarmaListJsonTemplate,
    PrefsJsonTemplate,
    TrophyListJsonTemplate,
)
from r2.lib.pages import FriendTableItem
from r2.lib.validator import (
    validate,
    VAccountByName,
    VContentLang,
    VFriendOfMine,
    VLength,
    VList,
    VValidatedJSON,
)
from r2.models import Account, Trophy
import r2.lib.errors as errors
import r2.lib.validator.preferences as vprefs


PREFS_JSON_SPEC = VValidatedJSON.PartialObject({
    k[len("pref_"):]: v for k, v in
    vprefs.PREFS_VALIDATORS.iteritems()
    if k in Account._preference_attrs
})

PREFS_JSON_SPEC.spec["content_langs"] = VValidatedJSON.ArrayOf(
    VContentLang("content_langs")
)


class APIv1UserController(OAuth2ResourceController):
    def pre(self):
        OAuth2ResourceController.pre(self)
        self.authenticate_with_token()
        self.run_sitewide_ratelimits()

    def try_pagecache(self):
        pass

    @staticmethod
    def on_validation_error(error):
        abort_with_error(error, error.code or 400)

    @require_oauth2_scope("identity")
    @api_doc(api_section.account)
    def GET_me(self):
        """Returns the identity of the user currently authenticated via OAuth."""
        resp = IdentityJsonTemplate().data(c.oauth_user)
        return self.api_wrapper(resp)

    @require_oauth2_scope("identity")
    @validate(
        fields=VList(
            "fields",
            choices=PREFS_JSON_SPEC.spec.keys(),
            error=errors.errors.NON_PREFERENCE,
        ),
    )
    @api_doc(api_section.account, uri='/api/v1/me/prefs')
    def GET_prefs(self, fields):
        """Return the preference settings of the logged in user"""
        resp = PrefsJsonTemplate(fields).data(c.oauth_user)
        return self.api_wrapper(resp)

    def _get_usertrophies(self, user):
        trophies = Trophy.by_account(user)
        def visible_trophy(trophy):
            return trophy._thing2.awardtype != 'invisible'
        trophies = filter(visible_trophy, trophies)
        resp = TrophyListJsonTemplate().render(trophies)
        return self.api_wrapper(resp.finalize())

    @require_oauth2_scope("read")
    @validate(
        user=VAccountByName('username'),
    )
    @api_doc(
        section=api_section.users,
        uri='/api/v1/user/{username}/trophies',
    )
    def GET_usertrophies(self, user):
        """Return a list of trophies for the a given user."""
        return self._get_usertrophies(user)

    @require_oauth2_scope("identity")
    @api_doc(
        section=api_section.account,
        uri='/api/v1/me/trophies',
    )
    def GET_trophies(self):
        """Return a list of trophies for the current user."""
        return self._get_usertrophies(c.oauth_user)

    @require_oauth2_scope("mysubreddits")
    @api_doc(
        section=api_section.account,
        uri='/api/v1/me/karma',
    )
    def GET_karma(self):
        """Return a breakdown of subreddit karma."""
        karmas = c.oauth_user.all_karmas(include_old=False)
        resp = KarmaListJsonTemplate().render(karmas)
        return self.api_wrapper(resp.finalize())

    PREFS_JSON_VALIDATOR = VValidatedJSON("json", PREFS_JSON_SPEC,
                                          body=True)

    @require_oauth2_scope("account")
    @validate(validated_prefs=PREFS_JSON_VALIDATOR)
    @api_doc(api_section.account, json_model=PREFS_JSON_VALIDATOR,
             uri='/api/v1/me/prefs')
    def PATCH_prefs(self, validated_prefs):
        user_prefs = c.user.preferences()
        for short_name, new_value in validated_prefs.iteritems():
            pref_name = "pref_" + short_name
            if pref_name == "pref_content_langs":
                new_value = vprefs.format_content_lang_pref(new_value)
            user_prefs[pref_name] = new_value
        vprefs.filter_prefs(user_prefs, c.user)
        vprefs.set_prefs(c.user, user_prefs)
        c.user._commit()
        return self.api_wrapper(PrefsJsonTemplate().data(c.user))

    FRIEND_JSON_SPEC = VValidatedJSON.PartialObject({
        "name": VAccountByName("name"),
        "note": VLength("note", 300),
    })
    FRIEND_JSON_VALIDATOR = VValidatedJSON("json", spec=FRIEND_JSON_SPEC,
                                           body=True)
    @require_oauth2_scope('subscribe')
    @validate(
        friend=VAccountByName('username'),
        notes_json=FRIEND_JSON_VALIDATOR,
    )
    @api_doc(api_section.users, json_model=FRIEND_JSON_VALIDATOR,
             uri='/api/v1/me/friends/{username}')
    def PUT_friends(self, friend, notes_json):
        """Create or update a "friend" relationship.

        This operation is idempotent. It can be used to add a new
        friend, or update an existing friend (e.g., add/change the
        note on that friend)

        """
        err = None
        if 'name' in notes_json and notes_json['name'] != friend:
            # The 'name' in the JSON is optional, but if present, must
            # match the username from the URL
            err = errors.RedditError('BAD_USERNAME', fields='name')
        if 'note' in notes_json and not c.user.gold:
            err = errors.RedditError('GOLD_REQUIRED', fields='note')
        if err:
            self.on_validation_error(err)

        # See if the target is already an existing friend.
        # If not, create the friend relationship.
        friend_rel = Account.get_friend(c.user, friend)
        rel_exists = bool(friend_rel)
        if not friend_rel:
            friend_rel = c.user.add_friend(friend)
            response.status = 201

        if 'note' in notes_json:
            note = notes_json['note'] or ''
            if not rel_exists:
                # If this is a newly created friend relationship,
                # the cache needs to be updated before a note can
                # be applied
                c.user.friend_rels_cache(_update=True)
            c.user.add_friend_note(friend, note)
        rel_view = FriendTableItem(friend_rel)
        return self.api_wrapper(FriendTableItemJsonTemplate().data(rel_view))

    @require_oauth2_scope('mysubreddits')
    @validate(
        friend_rel=VFriendOfMine('username'),
    )
    @api_doc(api_section.users, uri='/api/v1/me/friends/{username}')
    def GET_friends(self, friend_rel):
        """Get information about a specific 'friend', such as notes."""
        rel_view = FriendTableItem(friend_rel)
        return self.api_wrapper(FriendTableItemJsonTemplate().data(rel_view))

    @require_oauth2_scope('subscribe')
    @validate(
        friend_rel=VFriendOfMine('username'),
    )
    @api_doc(api_section.users, uri='/api/v1/me/friends/{username}')
    def DELETE_friends(self, friend_rel):
        """Stop being friends with a user."""
        c.user.remove_friend(friend_rel._thing2)
        if c.user.gold:
            c.user.friend_rels_cache(_update=True)
        response.status = 204

########NEW FILE########
__FILENAME__ = api_docs
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import re
from collections import defaultdict
from itertools import chain
import inspect
from os.path import abspath, relpath

from pylons import g
from pylons.i18n import _
from reddit_base import RedditController
from r2.lib.utils import Storage
from r2.lib.pages import BoringPage, ApiHelp
from r2.lib.validator import validate, VOneOf

# API sections displayed in the documentation page.
# Each section can have a title and a markdown-formatted description.
section_info = {
    'account': {
        'title': _('account'),
    },
    'apps': {
        'title': _('apps'),
    },
    'flair': {
        'title': _('flair'),
    },
    'links_and_comments': {
        'title': _('links & comments'),
    },
    'messages': {
        'title': _('private messages'),
    },
    'moderation': {
        'title': _('moderation'),
    },
    'misc': {
        'title': _('misc'),
    },
    'listings': {
        'title': _('listings'),
    },
    'search': {
        'title': _('search'),
    },
    'subreddits': {
        'title': _('subreddits'),
    },
    'multis': {
        'title': _('multis'),
    },
    'users': {
        'title': _('users'),
    },
    'wiki': {
        'title': _('wiki'),
    },
    'captcha': {
        'title': _('captcha'),
    }
}

api_section = Storage((k, k) for k in section_info)

def api_doc(section, uses_site=False, **kwargs):
    """
    Add documentation annotations to the decorated function.

    See ApidocsController.docs_from_controller for a list of annotation fields.
    """
    def add_metadata(api_function):
        doc = api_function._api_doc = getattr(api_function, '_api_doc', {})
        if 'extends' in kwargs:
            kwargs['extends'] = kwargs['extends']._api_doc
        doc.update(kwargs)
        doc['uses_site'] = uses_site
        doc['section'] = section
        doc['lineno'] = api_function.func_code.co_firstlineno

        file_path = abspath(api_function.func_code.co_filename)
        root_dir = g.paths['root']
        if file_path.startswith(root_dir):
            doc['relfilepath'] = relpath(file_path, root_dir)

        return api_function
    return add_metadata

class ApidocsController(RedditController):
    @staticmethod
    def docs_from_controller(controller, url_prefix='/api', oauth_only=False):
        """
        Examines a controller for documentation.  A dictionary index of
        sections containing dictionaries of URLs is returned.  For each URL, a
        dictionary of HTTP methods (GET, POST, etc.) is contained.  For each
        URL/method pair, a dictionary containing the following items is
        available:

        - `doc`: Markdown-formatted docstring.
        - `uri`: Manually-specified URI to list the API method as
        - `uri_variants`: Alternate URIs to access the API method from
        - `extensions`: URI extensions the API method supports
        - `parameters`: Dictionary of possible parameter names and descriptions.
        - `extends`: API method from which to inherit documentation
        - `json_model`: The JSON model used instead of normal POST parameters
        """

        api_docs = defaultdict(lambda: defaultdict(dict))
        for name, func in controller.__dict__.iteritems():
            method, sep, action = name.partition('_')
            if not action:
                continue

            valid_methods = ('GET', 'POST', 'PUT', 'DELETE', 'PATCH')
            api_doc = getattr(func, '_api_doc', None)
            if api_doc and 'section' in api_doc and method in valid_methods:
                docs = {}
                docs['doc'] = inspect.getdoc(func)

                if 'extends' in api_doc:
                    docs.update(api_doc['extends'])
                    # parameters are handled separately.
                    docs['parameters'] = {}
                docs.update(api_doc)

                # append a message to the docstring if supplied
                notes = docs.get("notes")
                if notes:
                    notes = "\n".join(notes)
                    if docs["doc"]:
                        docs["doc"] += "\n\n" + notes
                    else:
                        docs["doc"] = notes

                uri = docs.get('uri') or '/'.join((url_prefix, action))
                if 'extensions' in docs:
                    # if only one extension was specified, add it to the URI.
                    if len(docs['extensions']) == 1:
                        uri += '.' + docs['extensions'][0]
                        del docs['extensions']
                docs['uri'] = uri

                if api_doc['uses_site']:
                    docs["in-subreddit"] = True

                oauth_perms = getattr(func, 'oauth2_perms', {})
                oauth_allowed = oauth_perms.get('oauth2_allowed', False)
                if not oauth_allowed:
                    # Endpoint is not available over OAuth
                    docs['oauth_scopes'] = []
                else:
                    # [None] signifies to the template to state
                    # that the endpoint is accessible to any oauth client
                    docs['oauth_scopes'] = (oauth_perms['required_scopes'] or
                                            [None])

                # add every variant to the index -- the templates will filter
                # out variants in the long-form documentation
                if oauth_only:
                    if not oauth_allowed:
                        continue
                    for scope in docs['oauth_scopes']:
                        for variant in chain([uri],
                                             docs.get('uri_variants', [])):
                            api_docs[scope][variant][method] = docs
                else:
                    for variant in chain([uri], docs.get('uri_variants', [])):
                        api_docs[docs['section']][variant][method] = docs

        return api_docs

    @validate(
        mode=VOneOf('mode', options=('methods', 'oauth'), default='methods'))
    def GET_docs(self, mode):
        # controllers to gather docs from.
        from r2.controllers.api import ApiController, ApiminimalController
        from r2.controllers.apiv1.user import APIv1UserController
        from r2.controllers.captcha import CaptchaController
        from r2.controllers.front import FrontController
        from r2.controllers.wiki import WikiApiController, WikiController
        from r2.controllers.multi import MultiApiController
        from r2.controllers import listingcontroller

        api_controllers = [
            (APIv1UserController, '/api/v1'),
            (ApiController, '/api'),
            (ApiminimalController, '/api'),
            (WikiApiController, '/api/wiki'),
            (WikiController, '/wiki'),
            (MultiApiController, '/api/multi'),
            (CaptchaController, ''),
            (FrontController, ''),
        ]
        for name, value in vars(listingcontroller).iteritems():
            if name.endswith('Controller'):
                api_controllers.append((value, ''))

        # merge documentation info together.
        api_docs = defaultdict(dict)
        oauth_index = defaultdict(set)
        for controller, url_prefix in api_controllers:
            controller_docs = self.docs_from_controller(controller, url_prefix,
                                                        mode == 'oauth')
            for section, contents in controller_docs.iteritems():
                api_docs[section].update(contents)
                for variant, method_dict in contents.iteritems():
                    for method, docs in method_dict.iteritems():
                        for scope in docs['oauth_scopes']:
                            oauth_index[scope].add((section, variant, method))

        return BoringPage(
            _('api documentation'),
            content=ApiHelp(
                api_docs=api_docs,
                oauth_index=oauth_index,
                mode=mode,
            ),
            css_class="api-help",
            show_sidebar=False,
            show_infobar=False
        ).render()

########NEW FILE########
__FILENAME__ = awards
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import request, g
from reddit_base import RedditController
from r2.lib.pages import AdminPage, AdminAwards
from r2.lib.pages import AdminAwardGive, AdminAwardWinners
from r2.lib.validator import *

class AwardsController(RedditController):

    @validate(VAdmin())
    def GET_index(self):
        res = AdminPage(content = AdminAwards(),
                        title = 'awards').render()
        return res

    @validate(VAdmin(),
              award = VAwardByCodename('awardcn'),
              recipient = nop('recipient'),
              desc = nop('desc'),
              url = nop('url'),
              hours = nop('hours'))
    def GET_give(self, award, recipient, desc, url, hours):
        if award is None:
            abort(404, 'page not found')

        res = AdminPage(content = AdminAwardGive(award, recipient, desc,
                                                 url, hours),
                        title='give an award').render()
        return res

    @validate(VAdmin(),
              award = VAwardByCodename('awardcn'))
    def GET_winners(self, award):
        if award is None:
            abort(404, 'page not found')

        res = AdminPage(content = AdminAwardWinners(award),
                        title='award winners').render()
        return res

########NEW FILE########
__FILENAME__ = buttons
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from reddit_base import RedditController, UnloggedUser
from r2.lib.pages import (ButtonLite, ButtonDemoPanel, WidgetDemoPanel,
                          Bookmarklets, BoringPage)
from r2.lib.pages.things import wrap_links
from r2.models import *
from r2.lib.validator import *
from pylons import c, request, response
from pylons.i18n import _

class ButtonsController(RedditController):
    def get_wrapped_link(self, url, link = None, wrapper = None):
        try:
            links = []
            if link:
                links = [link]
            else:
                sr = None if isinstance(c.site, FakeSubreddit) else c.site
                try:
                    links = Link._by_url(url, sr)
                except NotFound:
                    pass

            if links:
                kw = {}
                if wrapper:
                    links = wrap_links(links, wrapper = wrapper)
                else:
                    links = wrap_links(links)
                links = list(links)
                links = max(links, key = lambda x: x._score) if links else None
            if not links and wrapper:
                return wrapper(None)
            return links
            # note: even if _by_url successed or a link was passed in,
            # it is possible link_listing.things is empty if the
            # link(s) is/are members of a private reddit
            # return the link with the highest score (if more than 1)
        except:
            #we don't want to return 500s in other people's pages.
            import traceback
            g.log.debug("FULLPATH: get_link error in buttons code")
            g.log.debug(traceback.format_exc())
            if wrapper:
                return wrapper(None)

    @validate(buttontype = VInt('t', 1, 5))
    def GET_button_embed(self, buttontype):
        if not buttontype:
            abort(404)

        return self.redirect('/static/button/button%s.js' % buttontype,
                             code=301)

    @validate(buttonimage = VInt('i', 0, 14),
              title = nop('title'),
              url = VSanitizedUrl('url'),
              newwindow = VBoolean('newwindow', default = False),
              styled = VBoolean('styled', default=True))
    def GET_button_lite(self, buttonimage, title, url, styled, newwindow):
        c.user = UnloggedUser([c.lang])
        c.user_is_loggedin = False
        c.render_style = 'js'

        if not url:
            url = request.referer
            # we don't want the JS to be cached if the referer was involved.
            c.used_cache = True

        def builder_wrapper(thing = None):
            kw = {}
            if not thing:
                kw['url'] = url
                kw['title'] = title
            return ButtonLite(thing,
                              image = 1 if buttonimage is None else buttonimage,
                              target = "_new" if newwindow else "_parent",
                              styled = styled, **kw)

        bjs = self.get_wrapped_link(url, wrapper = builder_wrapper)
        response.content_type = "text/javascript"
        return bjs.render()

    def GET_button_demo_page(self):
        # no buttons for domain listings -> redirect to top level
        if isinstance(c.site, DomainSR):
            return self.redirect('/buttons')
        return BoringPage(_("reddit buttons"),
                          show_sidebar = False, 
                          content=ButtonDemoPanel()).render()

    def GET_widget_demo_page(self):
        return BoringPage(_("reddit widget"),
                          show_sidebar = False, 
                          content=WidgetDemoPanel()).render()

    def GET_bookmarklets(self):
        return BoringPage(_("bookmarklets"),
                          show_sidebar = False, 
                          content=Bookmarklets()).render()


########NEW FILE########
__FILENAME__ = captcha
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from reddit_base import RedditController
import StringIO
import r2.lib.captcha as captcha
from pylons import c, response

from r2.controllers.api_docs import api_doc, api_section
from r2.controllers.oauth2 import allow_oauth2_access

class CaptchaController(RedditController):
    @allow_oauth2_access
    @api_doc(api_section.captcha, uri='/captcha/{iden}')
    def GET_captchaimg(self, iden):
        """
        Request a CAPTCHA image given an `iden`.

        An iden is given as the `captcha` field with a `BAD_CAPTCHA`
        error, you should use this endpoint if you get a
        `BAD_CAPTCHA` error response.

        Responds with a 120x50 `image/png` which should be displayed
        to the user.

        The user's response to the CAPTCHA should be sent as `captcha`
        along with your request.

        To request a new CAPTCHA,
        use [/api/new_captcha](#POST_api_new_captcha).
        """
        image = captcha.get_image(iden)
        f = StringIO.StringIO()
        image.save(f, "PNG")
        response.content_type = "image/png;"
        return f.getvalue()
    

########NEW FILE########
__FILENAME__ = embed
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.controllers.reddit_base import RedditController
from r2.lib.base import proxyurl
from r2.lib.template_helpers import get_domain
from r2.lib.pages import Embed, BoringPage, HelpPage
from r2.lib.filters import websafe, SC_OFF, SC_ON
from r2.lib.memoize import memoize
from pylons.i18n import _
from pylons import c, g, request

from BeautifulSoup import BeautifulSoup, Tag

from urllib2 import HTTPError

@memoize("renderurl_cached", time=60)
def renderurl_cached(path):
    # Needed so http://reddit.com/help/ works
    fp = path.rstrip("/")
    u = "http://code.reddit.com/wiki" + fp + '?stripped=1'

    g.log.debug("Pulling %s for help" % u)

    try:
        return fp, proxyurl(u)
    except HTTPError, e:
        if e.code != 404:
            print "error %s" % e.code
            print e.fp.read()
        return (None, None)

class EmbedController(RedditController):
    allow_stylesheets = True

    def rendercontent(self, input, fp):
        soup = BeautifulSoup(input)

        output = soup.find("div", { 'class':'wiki', 'id':'content'} )

        # Replace all links to "/wiki/help/..." with "/help/..."
        for link in output.findAll('a'):
            if link.has_key('href') and link['href'].startswith("/wiki/help"):
                link['href'] = link['href'][5:]

        output = SC_OFF + unicode(output) + SC_ON

        return HelpPage(_("help"),
                        content = Embed(content=output),
                        show_sidebar = None).render()

    def renderurl(self, override=None):
        if override:
            path = override
        else:
            path = request.path

        fp, content = renderurl_cached(path)
        if content is None:
            self.abort404()
        return self.rendercontent(content, fp)

    GET_help = POST_help = renderurl

    def GET_blog(self):
        return self.redirect("http://blog.%s/" %
                             get_domain(cname = False, subreddit = False,
                                        no_www = True))

    def GET_faq(self):
        if c.default_sr:
            return self.redirect('/help/faq')
        else:
            return self.renderurl('/help/faqs/' + c.site.name)

########NEW FILE########
__FILENAME__ = error
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import json
import os
import random

import pylons

from webob.exc import HTTPFound, HTTPMovedPermanently
from pylons.i18n import _
from pylons import c, g, request, response

try:
    # place all r2 specific imports in here.  If there is a code error, it'll
    # get caught and the stack trace won't be presented to the user in
    # production
    from r2.config import extensions
    from r2.controllers.reddit_base import RedditController, Cookies
    from r2.lib.errors import ErrorSet
    from r2.lib.filters import websafe_json, websafe
    from r2.lib import log, pages
    from r2.lib.strings import rand_strings
    from r2.lib.template_helpers import static
    from r2.lib.base import abort
    from r2.models.link import Link
    from r2.models.subreddit import DefaultSR, Subreddit
except Exception, e:
    if g.debug:
        # if debug mode, let the error filter up to pylons to be handled
        raise e
    else:
        # production environment: protect the code integrity!
        print "HuffmanEncodingError: make sure your python compiles before deploying, stupid!"
        # kill this app
        os._exit(1)


redditbroke =  \
'''<html>
  <head>
    <title>reddit broke!</title>
  </head>
  <body>
    <div style="margin: auto; text-align: center">
      <p>
        <a href="/">
          <img border="0" src="%s" alt="you broke reddit" />
        </a>
      </p>
      <p>
        %s
      </p>
  </body>
</html>
'''


FAILIEN_COUNT = 3
def make_failien_url():
    failien_number = random.randint(1, FAILIEN_COUNT)
    failien_name = "youbrokeit%d.png" % failien_number
    return static(failien_name)


class ErrorController(RedditController):
    """Generates error documents as and when they are required.

    The ErrorDocuments middleware forwards to ErrorController when error
    related status codes are returned from the application.

    This behaviour can be altered by changing the parameters to the
    ErrorDocuments middleware in your config/middleware.py file.
    """
    def check_for_bearer_token(self):
        pass

    allowed_render_styles = ('html', 'xml', 'js', 'embed', '', "compact", 'api')
    # List of admins to blame (skip the first admin, "reddit")
    # If list is empty, just blame "an admin"
    admins = g.admins[1:] or ["an admin"]
    def __before__(self):
        try:
            c.error_page = True
            RedditController.__before__(self)
        except (HTTPMovedPermanently, HTTPFound):
            # ignore an attempt to redirect from an error page
            pass
        except Exception as e:
            handle_awful_failure("ErrorController.__before__: %r" % e)

        # c.error_page is special-cased in a couple places to bypass
        # c.site checks. We shouldn't allow the user to get here other
        # than through `middleware.py:error_mapper`.
        if not request.environ.get('pylons.error_call'):
            abort(403, "direct access to error controller disallowed")

    def __after__(self): 
        try:
            RedditController.__after__(self)
        except Exception as e:
            handle_awful_failure("ErrorController.__after__: %r" % e)

    def __call__(self, environ, start_response):
        try:
            return RedditController.__call__(self, environ, start_response)
        except Exception as e:
            return handle_awful_failure("ErrorController.__call__: %r" % e)


    def send403(self):
        c.site = DefaultSR()
        if 'usable_error_content' in request.environ:
            return request.environ['usable_error_content']
        else:
            res = pages.RedditError(
                title=_("forbidden (%(domain)s)") % dict(domain=g.domain),
                message=_("you are not allowed to do that"),
                explanation=request.GET.get('explanation'))
            return res.render()

    def send404(self):
        if 'usable_error_content' in request.environ:
            return request.environ['usable_error_content']
        return pages.RedditError(_("page not found"),
                                 _("the page you requested does not exist")).render()

    def send429(self):
        retry_after = request.environ.get("retry_after")
        if retry_after:
            response.headers["Retry-After"] = str(retry_after)
            template_name = '/ratelimit_toofast.html'
        else:
            template_name = '/ratelimit_throttled.html'

        template = g.mako_lookup.get_template(template_name)
        return template.render(logo_url=static(g.default_header_url))

    def send503(self):
        retry_after = request.environ.get("retry_after")
        if retry_after:
            response.headers["Retry-After"] = str(retry_after)
        return request.environ['usable_error_content']

    def GET_document(self):
        try:
            c.errors = c.errors or ErrorSet()
            # clear cookies the old fashioned way 
            c.cookies = Cookies()

            code =  request.GET.get('code', '')
            try:
                code = int(code)
            except ValueError:
                code = 404
            srname = request.GET.get('srname', '')
            takedown = request.GET.get('takedown', "")

            # StatusBasedRedirect will override this anyway, but we need this
            # here for pagecache to see.
            response.status_int = code

            if srname:
                c.site = Subreddit._by_name(srname)

            if code in (204, 304):
                # NEVER return a content body on 204/304 or downstream
                # caches may become very confused.
                if request.GET.has_key('x-sup-id'):
                    x_sup_id = request.GET.get('x-sup-id')
                    if '\r\n' not in x_sup_id:
                        response.headers['x-sup-id'] = x_sup_id
                return ""
            elif c.render_style not in self.allowed_render_styles:
                return str(code)
            elif c.render_style in extensions.API_TYPES:
                data = request.environ.get('extra_error_data', {'error': code})
                return websafe_json(json.dumps(data))
            elif takedown and code == 404:
                link = Link._by_fullname(takedown)
                return pages.TakedownPage(link).render()
            elif code == 403:
                return self.send403()
            elif code == 429:
                return self.send429()
            elif code == 500:
                randmin = {'admin': random.choice(self.admins)}
                failien_url = make_failien_url()
                return redditbroke % (failien_url, rand_strings.sadmessages % randmin)
            elif code == 503:
                return self.send503()
            elif c.site:
                return self.send404()
            else:
                return "page not found"
        except Exception as e:
            return handle_awful_failure("ErrorController.GET_document: %r" % e)

    POST_document = GET_document
    PUT_document = GET_document
    PATCH_document = GET_document
    DELETE_document = GET_document


def handle_awful_failure(fail_text):
    """
    Makes sure that no errors generated in the error handler percolate
    up to the user unless debug is enabled.
    """
    if g.debug:
        import sys
        s = sys.exc_info()
        # reraise the original error with the original stack trace
        raise s[1], None, s[2]
    try:
        # log the traceback, and flag the "path" as the error location
        import traceback
        log.write_error_summary(fail_text)
        for line in traceback.format_exc().splitlines():
            g.log.error(line)
        return redditbroke % (make_failien_url(), websafe(fail_text))
    except:
        # we are doomed.  Admit defeat
        return "This is an error that should never occur.  You win."

########NEW FILE########
__FILENAME__ = errorlog
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import request, g
from reddit_base import RedditController
from r2.lib.pages import AdminPage, AdminErrorLog
from r2.lib.validator import validate, VAdmin

class ErrorlogController(RedditController):
    @validate(VAdmin())
    def GET_index(self):
        res = AdminPage(content = AdminErrorLog(),
                        title = 'error log',
                        show_sidebar = False
                        ).render()
        return res

########NEW FILE########
__FILENAME__ = front
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons.i18n import _, ungettext
from r2.controllers.reddit_base import (
    base_listing,
    pagecache_policy,
    PAGECACHE_POLICY,
    paginated_listing,
    disable_subreddit_css,
    RedditController,
)
from r2 import config
from r2.models import *
from r2.models.recommend import ExploreSettings
from r2.config.extensions import is_api
from r2.lib import recommender
from r2.lib.pages import *
from r2.lib.pages.things import hot_links_by_url_listing
from r2.lib.pages import trafficpages
from r2.lib.menus import *
from r2.lib.admin_utils import check_cheating
from r2.lib.utils import to36, sanitize_url, title_to_url
from r2.lib.utils import query_string, UrlParser, url_links_builder
from r2.lib.template_helpers import get_domain
from r2.lib.filters import unsafe, _force_unicode, _force_utf8
from r2.lib.emailer import Email
from r2.lib.db.operators import desc
from r2.lib.db import queries
from r2.lib.db.tdb_cassandra import MultiColumnQuery
from r2.lib.strings import strings
from r2.lib.search import (SearchQuery, SubredditSearchQuery, SearchException,
                           InvalidQuery)
from r2.lib.validator import *
from r2.lib import jsontemplates
from r2.lib import sup
import r2.lib.db.thing as thing
from r2.lib.errors import errors
from listingcontroller import ListingController
from oauth2 import require_oauth2_scope
from api_docs import api_doc, api_section
from pylons import c, request, response
from r2.models.token import EmailVerificationToken
from r2.controllers.ipn import generate_blob, validate_blob, GoldException

from operator import attrgetter
import string
import random as rand
import re, socket
import time as time_module
from urllib import quote_plus

class FrontController(RedditController):

    allow_stylesheets = True

    @validate(article=VLink('article'),
              comment=VCommentID('comment'))
    def GET_oldinfo(self, article, type, dest, rest=None, comment=''):
        """Legacy: supporting permalink pages from '06,
           and non-search-engine-friendly links"""
        if not (dest in ('comments','related','details')):
                dest = 'comments'
        if type == 'ancient':
            #this could go in config, but it should never change
            max_link_id = 10000000
            new_id = max_link_id - int(article._id)
            return self.redirect('/info/' + to36(new_id) + '/' + rest)
        if type == 'old':
            if not article.subreddit_slow.can_view(c.user):
                self.abort403()

            new_url = "/%s/%s/%s" % \
                      (dest, article._id36,
                       quote_plus(title_to_url(article.title).encode('utf-8')))
            if not c.default_sr:
                new_url = "/r/%s%s" % (c.site.name, new_url)
            if comment:
                new_url = new_url + "/%s" % comment._id36
            if c.extension:
                new_url = new_url + "/.%s" % c.extension

            new_url = new_url + query_string(request.GET)

            # redirect should be smarter and handle extensions, etc.
            return self.redirect(new_url, code=301)

    @require_oauth2_scope("read")
    @api_doc(api_section.listings, uses_site=True)
    def GET_random(self):
        """The Serendipity button"""
        sort = rand.choice(('new','hot'))
        links = c.site.get_links(sort, 'all')
        if isinstance(links, thing.Query):
            links._limit = g.num_serendipity
            links = [x._fullname for x in links]
        else:
            links = list(links)[:g.num_serendipity]

        rand.shuffle(links)

        builder = IDBuilder(links, skip=True,
                            keep_fn=lambda x: x.fresh,
                            num=1)
        links = builder.get_items()[0]

        if links:
            l = links[0]
            return self.redirect(add_sr("/tb/" + l._id36))
        else:
            return self.redirect(add_sr('/'))

    @disable_subreddit_css()
    @validate(VAdmin(),
              thing=VByName('article'),
              oldid36=nop('article'),
              after=nop('after'),
              before=nop('before'),
              count=VCount('count'))
    def GET_details(self, thing, oldid36, after, before, count):
        """The (now deprecated) details page.  Content on this page
        has been subsubmed by the presence of the LinkInfoBar on the
        rightbox, so it is only useful for Admin-only wizardry."""
        if not thing:
            try:
                link = Link._byID36(oldid36)
                return self.redirect('/details/' + link._fullname)
            except (NotFound, ValueError):
                abort(404)

        kw = {'count': count}
        if before:
            kw['after'] = before
            kw['reverse'] = True
        else:
            kw['after'] = after
            kw['reverse'] = False
        return DetailsPage(thing=thing, expand_children=False, **kw).render()

    @validate(VUser())
    def GET_explore(self):
        settings = ExploreSettings.for_user(c.user)
        recs = recommender.get_recommended_content_for_user(c.user,
                                                            settings,
                                                            record_views=True)
        content = ExploreItemListing(recs, settings)
        return BoringPage(_("explore"),
                          show_sidebar=True,
                          show_chooser=True,
                          page_classes=['explore-page'],
                          content=content).render()
 
    @validate(article=VLink('article'))
    def GET_shirt(self, article):
        if not can_view_link_comments(article):
            abort(403, 'forbidden')
        return self.abort404()

    def _comment_visits(self, article, user, new_visit=None):
        timer = g.stats.get_timer("gold.comment_visits")
        timer.start()

        hc_key = "comment_visits-%s-%s" % (user.name, article._id36)
        old_visits = g.hardcache.get(hc_key, [])

        append = False

        if new_visit is None:
            pass
        elif len(old_visits) == 0:
            append = True
        else:
            last_visit = max(old_visits)
            time_since_last = new_visit - last_visit
            if (time_since_last.days > 0
                or time_since_last.seconds > g.comment_visits_period):
                append = True
            else:
                # They were just here a few seconds ago; consider that
                # the same "visit" as right now
                old_visits.pop()

        if append:
            copy = list(old_visits) # make a copy
            copy.append(new_visit)
            if len(copy) > 10:
                copy.pop(0)
            g.hardcache.set(hc_key, copy, 86400 * 2)

        timer.stop()

        return old_visits


    @validate(article=VLink('article'),
              comment=VCommentID('comment'),
              context=VInt('context', min=0, max=8),
              sort=VMenu('controller', CommentSortMenu),
              limit=VInt('limit'),
              depth=VInt('depth'))
    def POST_comments(self, article, comment, context, sort, limit, depth):
        # VMenu validator will save the value of sort before we reach this
        # point. Now just redirect to GET mode.
        return self.redirect(request.fullpath + query_string(dict(sort=sort)))

    @require_oauth2_scope("read")
    @validate(article=VLink('article',
                  docs={"article": "ID36 of a link"}),
              comment=VCommentID('comment',
                  docs={"comment": "(optional) ID36 of a comment"}),
              context=VInt('context', min=0, max=8),
              sort=VMenu('controller', CommentSortMenu),
              limit=VInt('limit',
                  docs={"limit": "(optional) an integer"}),
              depth=VInt('depth',
                  docs={"depth": "(optional) an integer"}),
             )
    @api_doc(api_section.listings,
             uri='/comments/{article}',
             uses_site=True,
             extensions=['json', 'xml'])
    def GET_comments(self, article, comment, context, sort, limit, depth):
        """Get the comment tree for a given Link `article`.

        If supplied, `comment` is the ID36 of a comment in the comment tree for
        `article`. This comment will be the (highlighted) focal point of the
        returned view and `context` will be the number of parents shown.

        `depth` is the maximum depth of subtrees in the thread.

        `limit` is the maximum number of comments to return.

        See also: [/api/morechildren](#POST_api_morechildren) and
        [/api/comment](#POST_api_comment).

        """
        if comment and comment.link_id != article._id:
            return self.abort404()

        sr = Subreddit._byID(article.sr_id, True)

        if sr.name == g.takedown_sr:
            request.environ['REDDIT_TAKEDOWN'] = article._fullname
            return self.abort404()

        if not c.default_sr and c.site._id != sr._id:
            return self.abort404()

        if not can_view_link_comments(article):
            abort(403, 'forbidden')

        #check for 304
        self.check_modified(article, 'comments')

        # If there is a focal comment, communicate down to
        # comment_skeleton.html who that will be. Also, skip
        # comment_visits check
        previous_visits = None
        if comment:
            c.focal_comment = comment._id36
        elif (c.user_is_loggedin and c.user.gold and
              c.user.pref_highlight_new_comments):
            previous_visits = self._comment_visits(article, c.user, c.start_time)

        # check if we just came from the submit page
        infotext = None
        if request.GET.get('already_submitted'):
            infotext = strings.already_submitted % article.resubmit_link()

        check_cheating('comments')

        if not c.user.pref_num_comments:
            num = g.num_comments
        elif c.user.gold:
            num = min(c.user.pref_num_comments, g.max_comments_gold)
        else:
            num = min(c.user.pref_num_comments, g.max_comments)

        kw = {}
        # allow depth to be reset (I suspect I'll turn the VInt into a
        # validator on my next pass of .compact)
        if depth is not None and 0 < depth < MAX_RECURSION:
            kw['max_depth'] = depth
        elif c.render_style == "compact":
            kw['max_depth'] = 5

        displayPane = PaneStack()

        # allow the user's total count preferences to be overwritten
        # (think of .embed as the use case together with depth=1)

        if limit and limit > 0:
            num = limit

        if c.user_is_loggedin and c.user.gold:
            if num > g.max_comments_gold:
                displayPane.append(InfoBar(message =
                                           strings.over_comment_limit_gold
                                           % max(0, g.max_comments_gold)))
                num = g.max_comments_gold
        elif num > g.max_comments:
            if limit:
                displayPane.append(InfoBar(message =
                                       strings.over_comment_limit
                                       % dict(max=max(0, g.max_comments),
                                              goldmax=max(0,
                                                   g.max_comments_gold))))
            num = g.max_comments

        # if permalink page, add that message first to the content
        if comment:
            displayPane.append(PermalinkMessage(article.make_permalink_slow()))

        displayPane.append(LinkCommentSep())

        # insert reply box only for logged in user
        if c.user_is_loggedin and can_comment_link(article) and not is_api():
            #no comment box for permalinks
            display = False
            if not comment and article._age < sr.archive_age:
                display = True

            if article.promoted:
                geotargeted, city_target = promote.is_geotargeted_promo(article)
                if geotargeted:
                    displayPane.append(GeotargetNotice(city_target=city_target))

            displayPane.append(UserText(item=article, creating=True,
                                        post_form='comment',
                                        display=display,
                                        cloneable=True))

        if previous_visits:
            displayPane.append(CommentVisitsBox(previous_visits))
            # Used in later "more comments" renderings
            pv_hex = md5(repr(previous_visits)).hexdigest()
            g.cache.set(pv_hex, previous_visits, time=g.comment_visits_period)
            c.previous_visits_hex = pv_hex

        # Used in template_helpers
        c.previous_visits = previous_visits

        if article.contest_mode:
            sort = "random"

        # finally add the comment listing
        displayPane.append(CommentPane(article, CommentSortMenu.operator(sort),
                                       comment, context, num, **kw))

        subtitle_buttons = []

        if c.focal_comment or context is not None:
            subtitle = None
        elif article.num_comments == 0:
            subtitle = _("no comments (yet)")
        elif article.num_comments <= num:
            subtitle = _("all %d comments") % article.num_comments
        else:
            subtitle = _("top %d comments") % num

            if g.max_comments > num:
                self._add_show_comments_link(subtitle_buttons, article, num,
                                             g.max_comments, gold=False)

            if (c.user_is_loggedin and c.user.gold
                and article.num_comments > g.max_comments):
                self._add_show_comments_link(subtitle_buttons, article, num,
                                             g.max_comments_gold, gold=True)

        res = LinkInfoPage(link=article, comment=comment,
                           content=displayPane,
                           page_classes=['comments-page'],
                           subtitle=subtitle,
                           subtitle_buttons=subtitle_buttons,
                           nav_menus=[CommentSortMenu(default=sort),
                                        LinkCommentsSettings(article)],
                           infotext=infotext).render()
        return res

    def _add_show_comments_link(self, array, article, num, max_comm, gold=False):
        if num == max_comm:
            return
        elif article.num_comments <= max_comm:
            link_text = _("show all %d") % article.num_comments
        else:
            link_text = _("show %d") % max_comm

        limit_param = "?limit=%d" % max_comm

        if gold:
            link_class = "gold"
        else:
            link_class = ""

        more_link = article.make_permalink_slow() + limit_param
        array.append( (link_text, more_link, link_class) )

    @validate(VUser(),
              name=nop('name'))
    def GET_newreddit(self, name):
        """Create a subreddit form"""
        title = _('create a subreddit')
        content=CreateSubreddit(name=name or '')
        res = FormPage(_("create a subreddit"),
                       content=content,
                       ).render()
        return res

    @pagecache_policy(PAGECACHE_POLICY.LOGGEDIN_AND_LOGGEDOUT)
    @require_oauth2_scope("modconfig")
    @api_doc(api_section.moderation, uses_site=True)
    def GET_stylesheet(self):
        """Get the subreddit's current stylesheet.

        This will return either the content of or a redirect to the subreddit's
        current stylesheet if one exists.

        See also: [/api/subreddit_stylesheet](#POST_api_subreddit_stylesheet).

        """
        if g.css_killswitch:
            self.abort404()

        # de-stale the subreddit object so we don't poison nginx's cache
        if not isinstance(c.site, FakeSubreddit):
            c.site = Subreddit._byID(c.site._id, data=True, stale=False)

        if c.site.stylesheet_url_http:
            url = Reddit.get_subreddit_stylesheet_url()
            if url:
                return self.redirect(url)
            else:
                self.abort404()

        if not c.secure:
            stylesheet_contents = c.site.stylesheet_contents
        else:
            stylesheet_contents = c.site.stylesheet_contents_secure

        if stylesheet_contents:
            c.allow_loggedin_cache = True

            if c.site.stylesheet_modified:
                self.abort_if_not_modified(
                    c.site.stylesheet_modified,
                    private=False,
                    max_age=timedelta(days=7),
                    must_revalidate=False,
                )

            response.content_type = 'text/css'
            if c.site.type == 'private':
                response.headers['X-Private-Subreddit'] = 'private'
            return stylesheet_contents
        else:
            return self.abort404()

    def _make_moderationlog(self, srs, num, after, reverse, count, mod=None, action=None):
        query = Subreddit.get_modactions(srs, mod=mod, action=action)
        builder = QueryBuilder(query, num=num, after=after,
                               count=count,
                               reverse=reverse,
                               wrap=default_thing_wrapper())
        listing = ModActionListing(builder)
        pane = listing.listing()
        return pane

    modname_splitter = re.compile('[ ,]+')

    @require_oauth2_scope("modlog")
    @disable_subreddit_css()
    @paginated_listing(max_page_size=500, backend='cassandra')
    @validate(
        mod=nop('mod', docs={"mod": "(optional) a moderator filter"}),
        action=VOneOf('type', ModAction.actions),
    )
    @api_doc(api_section.moderation, uses_site=True,
             uri="/about/log", extensions=["json", "xml"])
    def GET_moderationlog(self, num, after, reverse, count, mod, action):
        """Get a list of recent moderation actions.

        Moderator actions taken within a subreddit are logged. This listing is
        a view of that log with various filters to aid in analyzing the
        information.

        The optional `mod` parameter can be a comma-delimited list of moderator
        names to restrict the results to, or the string `a` to restrict the
        results to admin actions taken within the subreddit.

        The `type` parameter is optional and if sent limits the log entries
        returned to only those of the type specified.

        """
        if not c.user_is_loggedin or not (c.user_is_admin or
                                          c.site.is_moderator(c.user)):
            return self.abort404()

        if mod:
            if mod == 'a':
                modnames = g.admins
            else:
                modnames = self.modname_splitter.split(mod)
            mod = []
            for name in modnames:
                try:
                    mod.append(Account._by_name(name, allow_deleted=True))
                except NotFound:
                    continue
            mod = mod or None

        if isinstance(c.site, (MultiReddit, ModSR)):
            srs = Subreddit._byID(c.site.sr_ids, return_dict=False)

            # grab all moderators
            mod_ids = set(Subreddit.get_all_mod_ids(srs))
            mods = Account._byID(mod_ids, data=True)

            pane = self._make_moderationlog(srs, num, after, reverse, count,
                                            mod=mod, action=action)
        elif isinstance(c.site, FakeSubreddit):
            return self.abort404()
        else:
            mod_ids = c.site.moderators
            mods = Account._byID(mod_ids, data=True)

            pane = self._make_moderationlog(c.site, num, after, reverse, count,
                                            mod=mod, action=action)

        panes = PaneStack()
        panes.append(pane)

        action_buttons = [NavButton(_('all'), None, opt='type', css_class='primary')]
        for a in ModAction.actions:
            action_buttons.append(NavButton(ModAction._menu[a], a, opt='type'))

        mod_buttons = [NavButton(_('all'), None, opt='mod', css_class='primary')]
        for mod_id in mod_ids:
            mod = mods[mod_id]
            mod_buttons.append(NavButton(mod.name, mod.name, opt='mod'))
        mod_buttons.append(NavButton('admins*', 'a', opt='mod'))
        base_path = request.path
        menus = [NavMenu(action_buttons, base_path=base_path,
                         title=_('filter by action'), type='lightdrop', css_class='modaction-drop'),
                NavMenu(mod_buttons, base_path=base_path,
                        title=_('filter by moderator'), type='lightdrop')]
        extension_handling = "private" if c.user.pref_private_feeds else False
        return EditReddit(content=panes,
                          nav_menus=menus,
                          location="log",
                          extension_handling=extension_handling).render()

    def _make_spamlisting(self, location, only, num, after, reverse, count):
        include_links, include_comments = True, True
        if only == 'links':
            include_comments = False
        elif only == 'comments':
            include_links = False

        if location == 'reports':
            query = c.site.get_reported(include_links=include_links,
                                        include_comments=include_comments)
        elif location == 'spam':
            query = c.site.get_spam(include_links=include_links,
                                    include_comments=include_comments)
        elif location == 'modqueue':
            query = c.site.get_modqueue(include_links=include_links,
                                        include_comments=include_comments)
        elif location == 'unmoderated':
            query = c.site.get_unmoderated()
        else:
            raise ValueError

        if isinstance(query, thing.Query):
            builder_cls = QueryBuilder
        elif isinstance (query, list):
            builder_cls = QueryBuilder
        else:
            builder_cls = IDBuilder

        def keep_fn(x):
            # no need to bother mods with banned users, or deleted content
            if x._deleted:
                return False
            if getattr(x,'author',None) == c.user and c.user._spam:
                return False

            if location == "reports":
                return x.reported > 0 and not x._spam
            elif location == "spam":
                return x._spam
            elif location == "modqueue":
                if x.reported > 0 and not x._spam:
                    return True # reported but not banned
                if x.author._spam and x.subreddit.exclude_banned_modqueue:
                    # banned user, don't show if subreddit pref excludes
                    return False

                verdict = getattr(x, "verdict", None)
                if verdict is None:
                    return True # anything without a verdict
                if x._spam and verdict != 'mod-removed':
                    return True # spam, unless banned by a moderator
                return False
            elif location == "unmoderated":
                # banned user, don't show if subreddit pref excludes
                if x.author._spam and x.subreddit.exclude_banned_modqueue:
                    return False
                return not getattr(x, 'verdict', None)
            else:
                raise ValueError

        builder = builder_cls(query,
                              skip=True,
                              num=num, after=after,
                              keep_fn=keep_fn,
                              count=count, reverse=reverse,
                              wrap=ListingController.builder_wrapper,
                              spam_listing=True)
        listing = LinkListing(builder)
        pane = listing.listing()

        # Indicate that the comment tree wasn't built for comments
        for i in pane.things:
            if hasattr(i, 'body'):
                i.child = None

        return pane

    def _edit_normal_reddit(self, location, created):
        if (location == 'edit' and
            c.user_is_loggedin and
            (c.user_is_admin or c.site.is_moderator_with_perms(c.user, 'config'))):
            pane = PaneStack()
            if created == 'true':
                pane.append(InfoBar(message=strings.sr_created))
            c.allow_styles = True
            c.site = Subreddit._byID(c.site._id, data=True, stale=False)
            pane.append(CreateSubreddit(site=c.site))
        elif (location == 'stylesheet'
              and c.site.can_change_stylesheet(c.user)
              and not g.css_killswitch):
            if hasattr(c.site,'stylesheet_contents_user') and c.site.stylesheet_contents_user:
                stylesheet_contents = c.site.stylesheet_contents_user
            elif hasattr(c.site,'stylesheet_contents') and c.site.stylesheet_contents:
                stylesheet_contents = c.site.stylesheet_contents
            else:
                stylesheet_contents = ''
            c.allow_styles = True
            pane = SubredditStylesheet(site=c.site,
                                       stylesheet_contents=stylesheet_contents)
        elif (location == 'stylesheet'
              and c.site.can_view(c.user)
              and not g.css_killswitch):
            stylesheet = (c.site.stylesheet_contents_user or
                          c.site.stylesheet_contents)
            pane = SubredditStylesheetSource(stylesheet_contents=stylesheet)
        elif (location == 'traffic' and
              (c.site.public_traffic or
               (c.user_is_loggedin and
                (c.site.is_moderator(c.user) or c.user.employee)))):
            pane = trafficpages.SubredditTraffic()
        elif (location == "about") and is_api():
            return self.redirect(add_sr('about.json'), code=301)
        else:
            return self.abort404()

        return EditReddit(content=pane,
                          location=location,
                          extension_handling=False).render()

    @base_listing
    @disable_subreddit_css()
    @validate(VSrModerator(perms='posts'),
              location=nop('location'),
              only=VOneOf('only', ('links', 'comments')))
    def GET_spamlisting(self, location, only, num, after, reverse, count):
        c.allow_styles = True
        c.profilepage = True
        pane = self._make_spamlisting(location, only, num, after, reverse,
                                      count)
        extension_handling = "private" if c.user.pref_private_feeds else False

        if location in ('reports', 'spam', 'modqueue'):
            buttons = [NavButton(_('links and comments'), None, opt='only'),
                       NavButton(_('links'), 'links', opt='only'),
                       NavButton(_('comments'), 'comments', opt='only')]
            menus = [NavMenu(buttons, base_path=request.path, title=_('show'),
                             type='lightdrop')]
        else:
            menus = None
        return EditReddit(content=pane,
                          location=location,
                          nav_menus=menus,
                          extension_handling=extension_handling).render()

    @base_listing
    @disable_subreddit_css()
    @validate(VSrModerator(perms='flair'),
              name=nop('name'))
    def GET_flairlisting(self, num, after, reverse, count, name):
        user = None
        if name:
            try:
                user = Account._by_name(name)
            except NotFound:
                c.errors.add(errors.USER_DOESNT_EXIST, field='name')

        c.allow_styles = True
        pane = FlairPane(num, after, reverse, name, user)
        return EditReddit(content=pane, location='flair').render()

    @require_oauth2_scope("modconfig")
    @disable_subreddit_css()
    @validate(location=nop('location'),
              created=VOneOf('created', ('true','false'),
                             default='false'))
    @api_doc(api_section.subreddits, uri="/r/{subreddit}/about/edit",
             extensions=["json"])
    def GET_editreddit(self, location, created):
        """Get the current settings of a subreddit.

        In the API, this returns the current settings of the subreddit as used
        by [/api/site_admin](#POST_api_site_admin).  On the HTML site, it will
        display a form for editing the subreddit.

        """
        c.profilepage = True
        if isinstance(c.site, FakeSubreddit):
            return self.abort404()
        else:
            return self._edit_normal_reddit(location, created)

    @require_oauth2_scope("read")
    @api_doc(api_section.subreddits, uri='/r/{subreddit}/about', extensions=['json'])
    def GET_about(self):
        """Return information about the subreddit.

        Data includes the subscriber count, description, and header image."""
        if not is_api() or isinstance(c.site, FakeSubreddit):
            return self.abort404()
        return Reddit(content=Wrapped(c.site)).render()

    @require_oauth2_scope("read")
    def GET_sidebar(self):
        usertext = UserText(c.site, c.site.description)
        return Reddit(content=usertext).render()

    @require_oauth2_scope("read")
    def GET_sticky(self):
        if c.site.sticky_fullname:
            sticky = Link._by_fullname(c.site.sticky_fullname, data=True)
            self.redirect(sticky.make_permalink_slow())
        else:
            abort(404)

    def GET_awards(self):
        """The awards page."""
        return BoringPage(_("awards"), content=UserAwards()).render()

    # filter for removing punctuation which could be interpreted as search syntax
    related_replace_regex = re.compile(r'[?\\&|!{}+~^()"\':*-]+')
    related_replace_with = ' '

    @base_listing
    @require_oauth2_scope("read")
    @validate(article=VLink('article'))
    def GET_related(self, num, article, after, reverse, count):
        """Related page: performs a search using title of article as
        the search query.

        """
        if not can_view_link_comments(article):
            abort(403, 'forbidden')

        query = self.related_replace_regex.sub(self.related_replace_with,
                                               article.title)
        query = _force_unicode(query)
        query = query[:1024]
        query = u"|".join(query.split())
        query = u"title:'%s'" % query
        rel_range = timedelta(days=3)
        start = int(time_module.mktime((article._date - rel_range).utctimetuple()))
        end = int(time_module.mktime((article._date + rel_range).utctimetuple()))
        nsfw = u"nsfw:0" if not (article.over_18 or article._nsfw.findall(article.title)) else u""
        query = u"(and %s timestamp:%s..%s %s)" % (query, start, end, nsfw)
        q = SearchQuery(query, raw_sort="-text_relevance",
                        syntax="cloudsearch")
        pane = self._search(q, num=num, after=after, reverse=reverse,
                            count=count)[2]

        return LinkInfoPage(link=article, content=pane,
                            page_classes=['related-page'],
                            subtitle=_('related')).render()

    @base_listing
    @require_oauth2_scope("read")
    @validate(article=VLink('article'))
    def GET_duplicates(self, article, num, after, reverse, count):
        if not can_view_link_comments(article):
            abort(403, 'forbidden')

        builder = url_links_builder(article.url, exclude=article._fullname,
                                    num=num, after=after, reverse=reverse,
                                    count=count)
        num_duplicates = len(builder.get_items()[0])
        listing = LinkListing(builder).listing()

        res = LinkInfoPage(link=article,
                           comment=None,
                           num_duplicates=num_duplicates,
                           content=listing,
                           page_classes=['other-discussions-page'],
                           subtitle=_('other discussions')).render()
        return res


    @base_listing
    @require_oauth2_scope("read")
    @validate(query=nop('q', docs={"q": "a search query"}))
    @api_doc(api_section.subreddits, uri='/subreddits/search', extensions=['json', 'xml'])
    def GET_search_reddits(self, query, reverse, after, count, num):
        """Search subreddits by title and description."""
        q = SubredditSearchQuery(query)

        results, etime, spane = self._search(q, num=num, reverse=reverse,
                                             after=after, count=count,
                                             skip_deleted_authors=False)

        res = SubredditsPage(content=spane,
                             prev_search=query,
                             elapsed_time=etime,
                             # update if we ever add sorts
                             search_params={},
                             title=_("search results"),
                             simple=True).render()
        return res

    search_help_page = "/wiki/search"
    verify_langs_regex = re.compile(r"\A[a-z][a-z](,[a-z][a-z])*\Z")
    @base_listing
    @require_oauth2_scope("read")
    @validate(query=VLength('q', max_length=512),
              sort=VMenu('sort', SearchSortMenu, remember=False),
              recent=VMenu('t', TimeMenu, remember=False),
              restrict_sr=VBoolean('restrict_sr', default=False),
              syntax=VOneOf('syntax', options=SearchQuery.known_syntaxes))
    @api_doc(api_section.search, extensions=['json', 'xml'], uses_site=True)
    def GET_search(self, query, num, reverse, after, count, sort, recent,
                   restrict_sr, syntax):
        """Search links page."""
        if query and '.' in query:
            url = sanitize_url(query, require_scheme=True)
            if url:
                return self.redirect("/submit" + query_string({'url':url}))

        if not restrict_sr:
            site = DefaultSR()
        else:
            site = c.site

        if not syntax:
            syntax = SearchQuery.default_syntax

        try:
            cleanup_message = None
            try:
                q = SearchQuery(query, site, sort,
                                recent=recent, syntax=syntax)
                results, etime, spane = self._search(q, num=num, after=after,
                                                     reverse=reverse,
                                                     count=count)
            except InvalidQuery:
                # Clean the search of characters that might be causing the
                # InvalidQuery exception. If the cleaned search boils down
                # to an empty string, the search code is expected to bail
                # out early with an empty result set.
                cleaned = re.sub("[^\w\s]+", " ", query)
                cleaned = cleaned.lower().strip()

                q = SearchQuery(cleaned, site, sort, recent=recent)
                results, etime, spane = self._search(q, num=num,
                                                     after=after,
                                                     reverse=reverse,
                                                     count=count)
                if cleaned:
                    cleanup_message = strings.invalid_search_query % {
                                                        "clean_query": cleaned
                                                                      }
                    cleanup_message += " "
                    cleanup_message += strings.search_help % {
                                          "search_help": self.search_help_page
                                                              }
                else:
                    cleanup_message = strings.completely_invalid_search_query

            check_cheating("search")
            res = SearchPage(_('search results'), query, etime,
                             content=spane,
                             nav_menus=[SearchSortMenu(default=sort),
                                        TimeMenu(default=recent)],
                             search_params=dict(sort=sort, t=recent),
                             infotext=cleanup_message,
                             simple=False, site=c.site,
                             restrict_sr=restrict_sr,
                             syntax=syntax,
                             converted_data=q.converted_data,
                             facets=results.subreddit_facets,
                             sort=sort,
                             recent=recent,
                             ).render()

            return res
        except SearchException + (socket.error,) as e:
            return self.search_fail(e)

    def _search(self, query_obj, num, after, reverse, count=0,
                skip_deleted_authors=True):
        """Helper function for interfacing with search.  Basically a
           thin wrapper for SearchBuilder."""

        builder = SearchBuilder(query_obj,
                                after=after, num=num, reverse=reverse,
                                count=count,
                                wrap=ListingController.builder_wrapper,
                                skip_deleted_authors=skip_deleted_authors)

        listing = LinkListing(builder, show_nums=True)

        # have to do it in two steps since total_num and timing are only
        # computed after fetch_more
        try:
            res = listing.listing()
        except SearchException + (socket.error,) as e:
            return self.search_fail(e)
        timing = time_module.time() - builder.start_time

        return builder.results, timing, res

    @validate(VAdmin(),
              comment=VCommentByID('comment_id'))
    def GET_comment_by_id(self, comment):
        href = comment.make_permalink_slow(context=5, anchor=True)
        return self.redirect(href)

    @validate(url=VRequired('url', None),
              title=VRequired('title', None),
              text=VRequired('text', None),
              selftext=VRequired('selftext', None),
              then=VOneOf('then', ('tb','comments'), default='comments'))
    def GET_submit(self, url, title, text, selftext, then):
        """Submit form."""
        resubmit = request.GET.get('resubmit')
        url = sanitize_url(url)

        if url and not resubmit:
            # check to see if the url has already been submitted
            listing = hot_links_by_url_listing(url, sr=c.site)
            links = listing.things

            if links and len(links) == 1:
                return self.redirect(links[0].already_submitted_link)
            elif links:
                infotext = (strings.multiple_submitted
                            % links[0].resubmit_link())
                res = BoringPage(_("seen it"),
                                 content=listing,
                                 infotext=infotext).render()
                return res

        if not c.user_is_loggedin:
            raise UserRequiredException

        if not (c.default_sr or c.site.can_submit(c.user)):
            abort(403, "forbidden")

        captcha = Captcha() if c.user.needs_captcha() else None

        extra_subreddits = []
        if isinstance(c.site, MultiReddit):
            extra_subreddits.append((
                _('%s subreddits') % c.site.name,
                c.site.srs
            ))

        newlink = NewLink(
            url=url or '',
            title=title or '',
            text=text or '',
            selftext=selftext or '',
            captcha=captcha,
            resubmit=resubmit,
            default_sr=c.site if not c.default_sr else None,
            extra_subreddits=extra_subreddits,
            show_link=c.default_sr or c.site.can_submit_link(c.user),
            show_self=((c.default_sr or c.site.can_submit_text(c.user))
                      and not request.GET.get('no_self')),
            then=then,
        )

        return FormPage(_("submit"),
                        show_sidebar=True,
                        page_classes=['submit-page'],
                        content=newlink).render()

    def GET_frame(self):
        """used for cname support.  makes a frame and
        puts the proper url as the frame source"""
        sub_domain = request.environ.get('sub_domain')
        original_path = request.environ.get('original_path')
        sr = Subreddit._by_domain(sub_domain)
        return Cnameframe(original_path, sr, sub_domain).render()


    def GET_framebuster(self, what=None, blah=None):
        """
        renders the contents of the iframe which, on a cname, checks
        if the user is currently logged into reddit.

        if this page is hit from the primary domain, redirects to the
        cnamed domain version of the site.  If the user is logged in,
        this cnamed version will drop a boolean session cookie on that
        domain so that subsequent page reloads will be caught in
        middleware and a frame will be inserted around the content.

        If the user is not logged in, previous session cookies will be
        emptied so that subsequent refreshes will not be rendered in
        that pesky frame.
        """
        if not c.site.domain:
            return ""
        elif c.cname:
            return FrameBuster(login=(what == "login")).render()
        else:
            path = "/framebuster/"
            if c.user_is_loggedin:
                path += "login/"
            u = UrlParser(path + str(random.random()))
            u.mk_cname(require_frame=False, subreddit=c.site,
                       port=request.port)
            return self.redirect(u.unparse())
        # the user is not logged in or there is no cname.
        return FrameBuster(login=False).render()

    def GET_catchall(self):
        return self.abort404()

    @validate(period=VInt('seconds',
                          min=sup.MIN_PERIOD,
                          max=sup.MAX_PERIOD,
                          default=sup.MIN_PERIOD))
    def GET_sup(self, period):
        #dont cache this, it's memoized elsewhere
        c.used_cache = True
        sup.set_expires_header()

        if c.extension == 'json':
            return sup.sup_json(period)
        else:
            return self.abort404()


    @require_oauth2_scope("modtraffic")
    @validate(VSponsor('link'),
              link=VLink('link'),
              campaign=VPromoCampaign('campaign'),
              before=VDate('before', format='%Y%m%d%H'),
              after=VDate('after', format='%Y%m%d%H'))
    def GET_traffic(self, link, campaign, before, after):
        if link and campaign and link._id != campaign.link_id:
            return self.abort404()

        if c.render_style == 'csv':
            return trafficpages.PromotedLinkTraffic.as_csv(campaign or link)

        content = trafficpages.PromotedLinkTraffic(link, campaign, before,
                                                   after)
        return LinkInfoPage(link=link,
                            page_classes=["promoted-traffic"],
                            show_sidebar=False, comment=None,
                            show_promote_button=True, content=content).render()

    @validate(VEmployee())
    def GET_site_traffic(self):
        return trafficpages.SitewideTrafficPage().render()

    @validate(VEmployee())
    def GET_lang_traffic(self, langcode):
        return trafficpages.LanguageTrafficPage(langcode).render()

    @validate(VEmployee())
    def GET_advert_traffic(self, code):
        return trafficpages.AdvertTrafficPage(code).render()

    @validate(VEmployee())
    def GET_subreddit_traffic_report(self):
        content = trafficpages.SubredditTrafficReport()

        if c.render_style == 'csv':
            return content.as_csv()
        return trafficpages.TrafficPage(content=content).render()

    @validate(VUser())
    def GET_account_activity(self):
        return AccountActivityPage().render()

    def GET_contact_us(self):
        return BoringPage(_("contact us"), show_sidebar=False,
                          content=ContactUs(), page_classes=["contact-us-page"]
                          ).render()

    def GET_advertising(self):
        return AdvertisingPage('advertise',
                        content = SelfServeBlurb(),
                        loginbox = False).render()

    def GET_rules(self):
        return BoringPage(_("rules of reddit"), show_sidebar=False,
                          content=RulesPage(), page_classes=["rulespage-body"]
                          ).render()

    @validate(vendor=VOneOf("v", ("claimed-gold", "claimed-creddits",
                                  "paypal", "coinbase"),
                            default="claimed-gold"))
    def GET_goldthanks(self, vendor):
        vendor_url = None
        lounge_md = None

        if vendor == "claimed-gold":
            claim_msg = _("claimed! enjoy your reddit gold membership.")
            if g.lounge_reddit:
                lounge_md = strings.lounge_msg
        elif vendor == "claimed-creddits":
            claim_msg = _("your gold creddits have been claimed! now go to "
                          "someone's userpage and give them a present!")
        elif vendor == "paypal":
            claim_msg = _("thanks for buying reddit gold! your transaction "
                          "has been completed and emailed to you. you can "
                          "check the details by logging into your account "
                          "at:")
            vendor_url = "https://www.paypal.com/us"
        elif vendor == "coinbase":
            claim_msg = _("thanks for buying reddit gold! your transaction is "
                          "being processed. if you have any questions please "
                          "email us at %(gold_email)s")
            claim_msg = claim_msg % {'gold_email': g.goldthanks_email}
        else:
            abort(404)

        return BoringPage(_("thanks"), show_sidebar=False,
                          content=GoldThanks(claim_msg=claim_msg,
                                             vendor_url=vendor_url,
                                             lounge_md=lounge_md)).render()

    @validate(VUser(),
              token=VOneTimeToken(AwardClaimToken, "code"))
    def GET_confirm_award_claim(self, token):
        if not token:
            abort(403)

        award = Award._by_fullname(token.awardfullname)
        trophy = FakeTrophy(c.user, award, token.description, token.url)
        content = ConfirmAwardClaim(trophy=trophy, user=c.user.name,
                                    token=token)
        return BoringPage(_("claim this award?"), content=content).render()

    @validate(VUser(),
              token=VOneTimeToken(AwardClaimToken, "code"))
    def POST_claim_award(self, token):
        if not token:
            abort(403)

        token.consume()

        award = Award._by_fullname(token.awardfullname)
        trophy, preexisting = Trophy.claim(c.user, token.uid, award,
                                           token.description, token.url)
        redirect = '/awards/received?trophy=' + trophy._id36
        if preexisting:
            redirect += '&duplicate=true'
        self.redirect(redirect)

    @validate(trophy=VTrophy('trophy'),
              preexisting=VBoolean('duplicate'))
    def GET_received_award(self, trophy, preexisting):
        content = AwardReceived(trophy=trophy, preexisting=preexisting)
        return BoringPage(_("award claim"), content=content).render()


class FormsController(RedditController):

    def GET_password(self):
        """The 'what is my password' page"""
        return BoringPage(_("password"), content=Password()).render()

    @validate(VUser(),
              dest=VDestination(),
              reason=nop('reason'))
    def GET_verify(self, dest, reason):
        if c.user.email_verified:
            content = InfoBar(message=strings.email_verified)
            if dest:
                return self.redirect(dest)
        else:
            if reason == "submit":
                infomsg = strings.verify_email_submit
            else:
                infomsg = strings.verify_email

            content = PaneStack(
                [InfoBar(message=infomsg),
                 PrefUpdate(email=True, verify=True,
                            password=False, dest=dest)])
        return BoringPage(_("verify email"), content=content).render()

    @validate(VUser(),
              token=VOneTimeToken(EmailVerificationToken, "key"),
              dest=VDestination(default="/prefs/update?verified=true"))
    def GET_verify_email(self, token, dest):
        fail_msg = None
        if token and token.user_id != c.user._fullname:
            fail_msg = strings.email_verify_wrong_user
        elif c.user.email_verified:
            # they've already verified.
            if token:
                # consume and ignore this token (if not already consumed).
                token.consume()
            return self.redirect(dest)
        elif token and token.valid_for_user(c.user):
            # successful verification!
            token.consume()
            c.user.email_verified = True
            c.user._commit()
            Award.give_if_needed("verified_email", c.user)
            return self.redirect(dest)

        # failure. let 'em know.
        content = PaneStack(
            [InfoBar(message=fail_msg or strings.email_verify_failed),
             PrefUpdate(email=True,
                        verify=True,
                        password=False)])
        return BoringPage(_("verify email"), content=content).render()

    @validate(token=VOneTimeToken(PasswordResetToken, "key"),
              key=nop("key"))
    def GET_resetpassword(self, token, key):
        """page hit once a user has been sent a password reset email
        to verify their identity before allowing them to update their
        password."""

        done = False
        if not key and request.referer:
            referer_path = request.referer.split(g.domain)[-1]
            done = referer_path.startswith(request.fullpath)
        elif not token:
            return self.redirect("/password?expired=true")

        token_user = Account._by_fullname(token.user_id, data=True)

        return BoringPage(
            _("reset password"),
            content=ResetPassword(
                key=key,
                done=done,
                username=token_user.name,
            )
        ).render()

    @disable_subreddit_css()
    @validate(VUser(),
              location=nop("location"),
              verified=VBoolean("verified"))
    def GET_prefs(self, location='', verified=False):
        """Preference page"""
        content = None
        infotext = None
        if not location or location == 'options':
            content = PrefOptions(done=request.GET.get('done'))
        elif location == 'update':
            if verified:
                infotext = strings.email_verified
            content = PrefUpdate()
        elif location == 'apps':
            content = PrefApps(my_apps=OAuth2Client._by_user_grouped(c.user),
                               developed_apps=OAuth2Client._by_developer(c.user))
        elif location == 'feeds' and c.user.pref_private_feeds:
            content = PrefFeeds()
        elif location == 'delete':
            content = PrefDelete()
        elif location == 'otp':
            content = PrefOTP()
        else:
            return self.abort404()

        return PrefsPage(content=content, infotext=infotext).render()


    @validate(dest=VDestination())
    def GET_login(self, dest):
        """The /login form.  No link to this page exists any more on
        the site (all actions invoking it now go through the login
        cover).  However, this page is still used for logging the user
        in during submission or voting from the bookmarklets."""

        if (c.user_is_loggedin and
            not request.environ.get('extension') == 'embed'):
            return self.redirect(dest)
        return LoginPage(dest=dest).render()


    @validate(dest=VDestination())
    def GET_register(self, dest):
        if (c.user_is_loggedin and
            not request.environ.get('extension') == 'embed'):
            return self.redirect(dest)
        return RegisterPage(dest=dest).render()

    @validate(VUser(),
              VModhash(),
              dest=VDestination())
    def GET_logout(self, dest):
        return self.redirect(dest)

    @validate(VUser(),
              VModhash(),
              dest=VDestination())
    def POST_logout(self, dest):
        """wipe login cookie and redirect to referer."""
        self.logout()
        return self.redirect(dest)


    @validate(VUser(),
              dest=VDestination())
    def GET_adminon(self, dest):
        """Enable admin interaction with site"""
        #check like this because c.user_is_admin is still false
        if not c.user.name in g.admins:
            return self.abort404()

        return AdminModeInterstitial(dest=dest).render()

    @validate(VAdmin(),
              dest=VDestination())
    def GET_adminoff(self, dest):
        """disable admin interaction with site."""
        if not c.user.name in g.admins:
            return self.abort404()
        self.disable_admin_mode(c.user)
        return self.redirect(dest)

    def _render_opt_in_out(self, msg_hash, leave):
        """Generates the form for an optin/optout page"""
        email = Email.handler.get_recipient(msg_hash)
        if not email:
            return self.abort404()
        sent = (has_opted_out(email) == leave)
        return BoringPage(_("opt out") if leave else _("welcome back"),
                          content=OptOut(email=email, leave=leave,
                                           sent=sent,
                                           msg_hash=msg_hash)).render()

    @validate(msg_hash=nop('x'))
    def GET_optout(self, msg_hash):
        """handles /mail/optout to add an email to the optout mailing
        list.  The actual email addition comes from the user posting
        the subsequently rendered form and is handled in
        ApiController.POST_optout."""
        return self._render_opt_in_out(msg_hash, True)

    @validate(msg_hash=nop('x'))
    def GET_optin(self, msg_hash):
        """handles /mail/optin to remove an email address from the
        optout list. The actual email removal comes from the user
        posting the subsequently rendered form and is handled in
        ApiController.POST_optin."""
        return self._render_opt_in_out(msg_hash, False)

    @validate(dest=VDestination("dest"))
    def GET_try_compact(self, dest):
        c.render_style = "compact"
        return TryCompact(dest=dest).render()

    @validate(VUser(),
              secret=VPrintable("secret", 50))
    def GET_claim(self, secret):
        """The page to claim reddit gold trophies"""
        return BoringPage(_("thanks"), content=Thanks(secret)).render()

    @validate(VUser(),
              passthrough=nop('passthrough'))
    def GET_creditgild(self, passthrough):
        """Used only for setting up credit card payments for gilding."""
        try:
            payment_blob = validate_blob(passthrough)
        except GoldException:
            self.abort404()

        if c.user != payment_blob['buyer']:
            self.abort404()

        if not payment_blob['goldtype'] == 'gift':
            self.abort404()

        recipient = payment_blob['recipient']
        thing = payment_blob.get('thing')
        if not thing:
            thing = payment_blob['comment']
        if (not thing or
            thing._deleted or
            not thing.subreddit_slow.can_view(c.user)):
            self.abort404()

        if isinstance(thing, Comment):
            summary = strings.gold_summary_gilding_page_comment
        else:
            summary = strings.gold_summary_gilding_page_link
        summary = summary % {'recipient': recipient.name}
        months = 1
        price = g.gold_month_price * months

        if isinstance(thing, Comment):
            desc = thing.body
        else:
            desc = thing.markdown_link_slow()

        content = CreditGild(
            summary=summary,
            price=price,
            months=months,
            stripe_key=g.secrets['stripe_public_key'],
            passthrough=passthrough,
            description=desc,
            period=None,
        )

        return BoringPage(_("reddit gold"),
                          show_sidebar=False,
                          content=content).render()

    @validate(goldtype=VOneOf("goldtype",
                              ("autorenew", "onetime", "creddits", "gift",
                               "code")),
              period=VOneOf("period", ("monthly", "yearly")),
              months=VInt("months"),
              # variables below are just for gifts
              signed=VBoolean("signed"),
              recipient_name=VPrintable("recipient", max_length=50),
              thing=VByName("thing"),
              giftmessage=VLength("giftmessage", 10000),
              email=ValidEmails("email", num=1))
    def GET_gold(self, goldtype, period, months,
                 signed, recipient_name, giftmessage, thing, email):

        if thing:
            thing_sr = Subreddit._byID(thing.sr_id, data=True)
            if (thing._deleted or
                    thing._spam or
                    not thing_sr.can_view(c.user) or
                    not thing_sr.allow_gilding):
                thing = None

        start_over = False
        recipient = None
        if not c.user_is_loggedin:
            if goldtype != "code":
                start_over = True
            elif months is None or months < 1:
                start_over = True
            elif not email:
                start_over = True
        elif goldtype == "autorenew":
            if period is None:
                start_over = True
            elif c.user.has_gold_subscription:
                return self.redirect("/gold/subscription")
        elif goldtype in ("onetime", "creddits", "code"):
            if months is None or months < 1:
                start_over = True
        elif goldtype == "gift":
            if months is None or months < 1:
                start_over = True

            if thing:
                recipient = Account._byID(thing.author_id, data=True)
                if recipient._deleted:
                    thing = None
                    recipient = None
                    start_over = True
            else:
                try:
                    recipient = Account._by_name(recipient_name or "")
                except NotFound:
                    start_over = True
        else:
            goldtype = ""
            start_over = True

        if start_over:
            can_subscribe = (c.user_is_loggedin and
                             not c.user.has_gold_subscription)
            if not can_subscribe and goldtype == "autorenew":
                goldtype = "creddits"
                
            return BoringPage(_("reddit gold"),
                              show_sidebar=False,
                              content=Gold(goldtype, period, months, signed,
                                           recipient, recipient_name,
                                           can_subscribe=can_subscribe)).render()
        else:
            payment_blob = dict(goldtype=goldtype,
                                status="initialized")
            if c.user_is_loggedin:
                payment_blob["account_id"] = c.user._id
                payment_blob["account_name"] = c.user.name
            else:
                payment_blob["email"] = email

            if goldtype == "gift":
                payment_blob["signed"] = signed
                payment_blob["recipient"] = recipient.name
                payment_blob["giftmessage"] = _force_utf8(giftmessage)
                if thing:
                    payment_blob["thing"] = thing._fullname

            passthrough = generate_blob(payment_blob)

            return BoringPage(_("reddit gold"),
                              show_sidebar=False,
                              content=GoldPayment(goldtype, period, months,
                                                  signed, recipient,
                                                  giftmessage, passthrough,
                                                  thing)
                              ).render()

    @validate(VUser())
    def GET_subscription(self):
        user = c.user
        content = GoldSubscription(user)
        return BoringPage(_("reddit gold subscription"), show_sidebar=False,
                          content=content).render()

########NEW FILE########
__FILENAME__ = health
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import json
import os

import pylibmc
from pylons import g, request, response
from pylons.controllers.util import abort

from r2.controllers.reddit_base import MinimalController
from r2.lib import promote, cache


class HealthController(MinimalController):
    def try_pagecache(self):
        pass

    def pre(self):
        pass

    def post(self):
        pass

    def GET_health(self):
        if os.path.exists("/var/opt/reddit/quiesce"):
            request.environ["usable_error_content"] = "No thanks, I'm full."
            abort(503)

        response.content_type = "application/json"
        return json.dumps(g.versions, sort_keys=True, indent=4)

    def GET_promohealth(self):
        response.content_type = "application/json"
        return json.dumps(promote.health_check())

    def GET_cachehealth(self):
        results = {}
        behaviors = {
            # Passed on to poll(2) in milliseconds
            "connect_timeout": 1000,
            # Passed on to setsockopt(2) in microseconds
            "receive_timeout": int(1e6),
            "send_timeout": int(1e6),
        }
        for server in cache._CACHE_SERVERS:
            try:
                if server.startswith("udp:"):
                    # libmemcached doesn't support UDP get/fetch operations
                    continue
                mc = pylibmc.Client([server], behaviors=behaviors)
                mc.get("__health_check_%s__" % server)
                results[server] = "OK"
            except pylibmc.Error as e:
                g.log.warning("Health check for %s FAILED: %s", server, e)
                results[server] = "FAILED %s" % e
        return json.dumps(results)

########NEW FILE########
__FILENAME__ = ipn
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from datetime import datetime, timedelta

import json

from pylons import c, g, request
from pylons.i18n import _
from sqlalchemy.exc import IntegrityError
import stripe

from r2.controllers.reddit_base import RedditController
from r2.lib.base import abort
from r2.lib.emailer import _system_email
from r2.lib.errors import MessageError
from r2.lib.filters import _force_unicode, _force_utf8
from r2.lib.log import log_text
from r2.lib.pages import GoldGiftCodeEmail
from r2.lib.strings import strings
from r2.lib.utils import constant_time_compare, randstr, timeago
from r2.lib.validator import (
    nop,
    textresponse,
    validatedForm,
    VByName,
    VFloat,
    VInt,
    VLength,
    VModhash,
    VOneOf,
    VPrintable,
    VUser,
)
from r2.models import (
    Account,
    account_by_payingid,
    account_from_stripe_customer_id,
    accountid_from_paypalsubscription,
    admintools,
    append_random_bottlecap_phrase,
    cancel_subscription,
    Comment,
    Email,
    create_claimed_gold,
    create_gift_gold,
    create_gold_code,
    get_discounted_price,
    Link,
    make_gold_message,
    NotFound,
    retrieve_gold_transaction,
    send_system_message,
    Thing,
    update_gold_transaction,
    generate_token,
)

stripe.api_key = g.secrets['stripe_secret_key']

def generate_blob(data):
    passthrough = generate_token(15)

    g.hardcache.set("payment_blob-" + passthrough,
                    data, 86400 * 30)
    g.log.info("just set payment_blob-%s", passthrough)
    return passthrough


def get_blob(code):
    key = "payment_blob-" + code
    with g.make_lock("payment_blob", "payment_blob_lock-" + code):
        blob = g.hardcache.get(key)
        if not blob:
            raise NotFound("No payment_blob-" + code)
        if blob.get('status', None) != 'initialized':
            raise ValueError("payment_blob %s has status = %s" %
                             (code, blob.get('status', None)))
        blob['status'] = "locked"
        g.hardcache.set(key, blob, 86400 * 30)
    return key, blob

def has_blob(custom):
    if not custom:
        return False

    blob = g.hardcache.get('payment_blob-%s' % custom)
    return bool(blob)

def dump_parameters(parameters):
    for k, v in parameters.iteritems():
        g.log.info("IPN: %r = %r" % (k, v))

def check_payment_status(payment_status):
    if payment_status is None:
        payment_status = ''

    psl = payment_status.lower()

    if psl == 'completed':
        return (None, psl)
    elif psl == 'refunded':
        log_text("refund", "Just got notice of a refund.", "info")
        # TODO: something useful when this happens -- and don't
        # forget to verify first
        return ("Ok", psl)
    elif psl == 'pending':
        log_text("pending",
                 "Just got notice of a Pending, whatever that is.", "info")
        # TODO: something useful when this happens -- and don't
        # forget to verify first
        return ("Ok", psl)
    elif psl == 'reversed':
        log_text("reversal",
                 "Just got notice of a PayPal reversal.", "info")
        # TODO: something useful when this happens -- and don't
        # forget to verify first
        return ("Ok", psl)
    elif psl == 'canceled_reversal':
        log_text("canceled_reversal",
                 "Just got notice of a PayPal 'canceled reversal'.", "info")
        return ("Ok", psl)
    elif psl == 'failed':
        return ("Ok", psl)
    elif psl == 'denied':
        return ("Ok", psl)
    elif psl == '':
        return (None, psl)
    else:
        raise ValueError("Unknown IPN status: %r" % payment_status)

def check_txn_type(txn_type, psl):
    if txn_type == 'subscr_signup':
        return ("Ok", None)
    elif txn_type == 'subscr_cancel':
        return ("Ok", "cancel")
    elif txn_type == 'subscr_eot':
        return ("Ok", None)
    elif txn_type == 'subscr_failed':
        log_text("failed_subscription",
                 "Just got notice of a failed PayPal resub.", "info")
        return ("Ok", None)
    elif txn_type == 'subscr_modify':
        log_text("modified_subscription",
                 "Just got notice of a modified PayPal sub.", "info")
        return ("Ok", None)
    elif txn_type == 'send_money':
        return ("Ok", None)
    elif txn_type in ('new_case',
        'recurring_payment_suspended_due_to_max_failed_payment'):
        return ("Ok", None)
    elif txn_type == 'subscr_payment' and psl == 'completed':
        return (None, "new")
    elif txn_type == 'web_accept' and psl == 'completed':
        return (None, None)
    elif txn_type == "paypal_here":
        return ("Ok", None)
    else:
        raise ValueError("Unknown IPN txn_type / psl %r" %
                         ((txn_type, psl),))


def existing_subscription(subscr_id, paying_id, custom):
    if subscr_id is None:
        return None

    account_id = accountid_from_paypalsubscription(subscr_id)

    if not account_id and has_blob(custom):
        # New subscription contains the user info in hardcache
        return None

    should_set_subscriber = False
    if account_id is None:
        # Payment from legacy subscription (subscr_id not set), fall back
        # to guessing the user from the paying_id
        account_id = account_by_payingid(paying_id)
        should_set_subscriber = True
        if account_id is None:
            return None

    try:
        account = Account._byID(account_id, data=True)

        if account._deleted:
            g.log.info("IPN renewal for deleted account %d (%s)", account_id,
                       subscr_id)
            return "deleted account"

        if should_set_subscriber:
            if hasattr(account, "gold_subscr_id") and account.gold_subscr_id:
                g.log.warning("Attempted to set subscr_id (%s) for account (%d) "
                              "that already has one." % (subscr_id, account_id))
                return None

            account.gold_subscr_id = subscr_id
            account._commit()
    except NotFound:
        g.log.info("Just got IPN renewal for non-existent account #%d" % account_id)

    return account

def months_and_days_from_pennies(pennies, discount=False):
    if discount:
        year_pennies = get_discounted_price(g.gold_year_price).pennies
        month_pennies = get_discounted_price(g.gold_month_price).pennies
    else:
        year_pennies = g.gold_year_price.pennies
        month_pennies = g.gold_month_price.pennies

    if pennies >= year_pennies:
        years = pennies / year_pennies
        months = 12 * years
        days  = 366 * years
    else:
        months = pennies / month_pennies
        days   = 31 * months
    return (months, days)

def send_gift(buyer, recipient, months, days, signed, giftmessage,
              thing_fullname):
    admintools.engolden(recipient, days)

    if thing_fullname:
        thing = Thing._by_fullname(thing_fullname, data=True)
        thing._gild(buyer)
    else:
        thing = None

    if signed:
        sender = buyer.name
        md_sender = "[%s](/user/%s)" % (sender, sender)
    else:
        sender = _("An anonymous redditor")
        md_sender = _("An anonymous redditor")

    create_gift_gold (buyer._id, recipient._id, days, c.start_time, signed)

    if months == 1:
        amount = "a month"
    else:
        amount = "%d months" % months

    if not thing:
        subject = _('Let there be gold! %s just sent you reddit gold!') % sender
        message = strings.youve_got_gold % dict(sender=md_sender, amount=amount)

        if giftmessage and giftmessage.strip():
            message += "\n\n" + strings.giftgold_note + giftmessage + '\n\n----'
    else:
        url = thing.make_permalink_slow()
        if isinstance(thing, Comment):
            subject = _('Your comment has been gilded!')
            message = strings.youve_been_gilded_comment % {'url': url}
        else:
            subject = _('Your submission has been gilded!')
            message = strings.youve_been_gilded_link % {'url': url}

    message += '\n\n' + strings.gold_benefits_msg
    if g.lounge_reddit:
        message += '\n* ' + strings.lounge_msg
    message = append_random_bottlecap_phrase(message)

    try:
        send_system_message(recipient, subject, message,
                            distinguished='gold-auto')
    except MessageError:
        g.log.error('send_gift: could not send system message')

    g.log.info("%s gifted %s to %s" % (buyer.name, amount, recipient.name))
    return thing


def send_gold_code(buyer, months, days,
                   trans_id=None, payer_email='', pennies=0, buyer_email=None):
    if buyer:
        paying_id = buyer._id
        buyer_name = buyer.name
    else:
        paying_id = buyer_email
        buyer_name = buyer_email
    code = create_gold_code(trans_id, payer_email,
                            paying_id, pennies, days, c.start_time)
    # format the code so it's easier to read (XXXXX-XXXXX)
    split_at = len(code) / 2
    code = code[:split_at] + '-' + code[split_at:]

    if months == 1:
        amount = "a month"
    else:
        amount = "%d months" % months

    subject = _('Your gold gift code has been generated!')
    message = _('Here is your gift code for %(amount)s of reddit gold:\n\n'
                '%(code)s\n\nThe recipient (or you!) can enter it at '
                'http://www.reddit.com/gold or go directly to '
                'http://www.reddit.com/thanks/%(code)s to claim it.'
              ) % {'amount': amount, 'code': code}

    if buyer:
        # bought by a logged-in user, send a reddit PM
        message = append_random_bottlecap_phrase(message)
        send_system_message(buyer, subject, message, distinguished='gold-auto')
    else:
        # bought by a logged-out user, send an email
        contents = GoldGiftCodeEmail(message=message).render(style='email')
        _system_email(buyer_email, contents, Email.Kind.GOLD_GIFT_CODE)
                      
    g.log.info("%s bought a gold code for %s", buyer_name, amount)
    return code


class IpnController(RedditController):
    # Used when buying gold with creddits
    @validatedForm(VUser(),
                   VModhash(),
                   months = VInt("months"),
                   passthrough = VPrintable("passthrough", max_length=50))
    def POST_spendcreddits(self, form, jquery, months, passthrough):
        if months is None or months < 1:
            form.set_html(".status", _("nice try."))
            return

        days = months * 31

        if not passthrough:
            raise ValueError("/spendcreddits got no passthrough?")

        blob_key, payment_blob = get_blob(passthrough)
        if payment_blob["goldtype"] not in ("gift", "code"):
            raise ValueError("/spendcreddits payment_blob %s has goldtype %s" %
                             (passthrough, payment_blob["goldtype"]))

        if payment_blob["account_id"] != c.user._id:
            fmt = ("/spendcreddits payment_blob %s has userid %d " +
                   "but c.user._id is %d")
            raise ValueError(fmt % passthrough,
                             payment_blob["account_id"],
                             c.user._id)

        if payment_blob["goldtype"] == "gift":
            signed = payment_blob["signed"]
            giftmessage = _force_unicode(payment_blob["giftmessage"])
            recipient_name = payment_blob["recipient"]

            try:
                recipient = Account._by_name(recipient_name)
            except NotFound:
                raise ValueError("Invalid username %s in spendcreddits, buyer = %s"
                                 % (recipient_name, c.user.name))

            if recipient._deleted:
                form.set_html(".status", _("that user has deleted their account"))
                return

        if not c.user.employee:
            if months > c.user.gold_creddits:
                raise ValueError("%s is trying to sneak around the creddit check"
                                 % c.user.name)

            c.user.gold_creddits -= months
            c.user.gold_creddit_escrow += months
            c.user._commit()

        if payment_blob["goldtype"] == "gift":
            thing_fullname = payment_blob.get("thing")
            thing = send_gift(c.user, recipient, months, days, signed,
                              giftmessage, thing_fullname)
            form.set_html(".status", _("the gold has been delivered!"))
        else:
            try:
                send_gold_code(c.user, months, days)
            except MessageError:
                form.set_html(".status",
                              _("there was an error creating a gift code. "
                                "please try again later, or contact %(email)s "
                                "for assistance.")
                              % {'email': g.goldthanks_email})
                return
            thing = None
            form.set_html(".status",
                          _("the gift code has been messaged to you!"))

        if not c.user.employee:
            c.user.gold_creddit_escrow -= months
            c.user._commit()
        form.find("button").hide()

        payment_blob["status"] = "processed"
        g.hardcache.set(blob_key, payment_blob, 86400 * 30)

        if thing:
            gilding_message = make_gold_message(thing, user_gilded=True)
            jquery.gild_thing(thing_fullname, gilding_message, thing.gildings)

    @textresponse(paypal_secret = VPrintable('secret', 50),
                  payment_status = VPrintable('payment_status', 20),
                  txn_id = VPrintable('txn_id', 20),
                  paying_id = VPrintable('payer_id', 50),
                  payer_email = VPrintable('payer_email', 250),
                  mc_currency = VPrintable('mc_currency', 20),
                  mc_gross = VFloat('mc_gross'),
                  custom = VPrintable('custom', 50))
    def POST_ipn(self, paypal_secret, payment_status, txn_id, paying_id,
                 payer_email, mc_currency, mc_gross, custom):

        parameters = request.POST.copy()

        # Make sure it's really PayPal
        if not constant_time_compare(paypal_secret,
                                     g.secrets['paypal_webhook']):
            log_text("invalid IPN secret",
                     "%s guessed the wrong IPN secret" % request.ip,
                     "warning")
            raise ValueError

        # Return early if it's an IPN class we don't care about
        response, psl = check_payment_status(payment_status)
        if response:
            return response

        # Return early if it's a txn_type we don't care about
        response, subscription = check_txn_type(parameters['txn_type'], psl)
        if subscription is None:
            subscr_id = None
        elif subscription == "new":
            subscr_id = parameters['subscr_id']
        elif subscription == "cancel":
            cancel_subscription(parameters['subscr_id'])
        else:
            raise ValueError("Weird subscription: %r" % subscription)

        if response:
            return response

        # Check for the debug flag, and if so, dump the IPN dict
        if g.cache.get("ipn-debug"):
            g.cache.delete("ipn-debug")
            dump_parameters(parameters)

        if mc_currency != 'USD':
            raise ValueError("Somehow got non-USD IPN %r" % mc_currency)

        if not (txn_id and paying_id and payer_email and mc_gross):
            dump_parameters(parameters)
            raise ValueError("Got incomplete IPN")

        pennies = int(mc_gross * 100)
        months, days = months_and_days_from_pennies(pennies)

        # Special case: autorenewal payment
        existing = existing_subscription(subscr_id, paying_id, custom)
        if existing:
            if existing != "deleted account":
                try:
                    create_claimed_gold ("P" + txn_id, payer_email, paying_id,
                                         pennies, days, None, existing._id,
                                         c.start_time, subscr_id)
                except IntegrityError:
                    return "Ok"
                admintools.engolden(existing, days)

                g.log.info("Just applied IPN renewal for %s, %d days" %
                           (existing.name, days))
            return "Ok"

        # More sanity checks that all non-autorenewals should pass:

        if not custom:
            dump_parameters(parameters)
            raise ValueError("Got IPN with txn_id=%s and no custom"
                             % txn_id)

        self.finish(parameters, "P" + txn_id,
                    payer_email, paying_id, subscr_id,
                    custom, pennies, months, days)

    def finish(self, parameters, txn_id,
               payer_email, paying_id, subscr_id,
               custom, pennies, months, days):

        try:
            blob_key, payment_blob = get_blob(custom)
        except ValueError:
            g.log.error("whoops, %s was locked", custom)
            return

        buyer = None
        buyer_email = None
        buyer_id = payment_blob.get('account_id', None)
        if buyer_id:
            try:
                buyer = Account._byID(buyer_id, data=True)
            except NotFound:
                dump_parameters(parameters)
                raise ValueError("Invalid buyer_id %d in IPN with custom='%s'"
                                 % (buyer_id, custom))
        else:
            buyer_email = payment_blob.get('email')
            if not buyer_email:
                dump_parameters(parameters)
                error = "No buyer_id or email in IPN with custom='%s'" % custom
                raise ValueError(error)

        if subscr_id:
            buyer.gold_subscr_id = subscr_id

        instagift = False
        if payment_blob['goldtype'] in ('autorenew', 'onetime'):
            admintools.engolden(buyer, days)

            subject = _("Eureka! Thank you for investing in reddit gold!")
            
            message = _("Thank you for buying reddit gold. Your patronage "
                        "supports the site and makes future development "
                        "possible. For example, one month of reddit gold "
                        "pays for 5 instance hours of reddit's servers.")
            message += "\n\n" + strings.gold_benefits_msg
            if g.lounge_reddit:
                message += "\n* " + strings.lounge_msg
        elif payment_blob['goldtype'] == 'creddits':
            buyer._incr("gold_creddits", months)
            buyer._commit()
            subject = _("Eureka! Thank you for investing in reddit gold "
                        "creddits!")

            message = _("Thank you for buying creddits. Your patronage "
                        "supports the site and makes future development "
                        "possible. To spend your creddits and spread reddit "
                        "gold, visit [/gold](/gold) or your favorite "
                        "person's user page.")
            message += "\n\n" + strings.gold_benefits_msg + "\n\n"
            message += _("Thank you again for your support, and have fun "
                         "spreading gold!")
        elif payment_blob['goldtype'] == 'gift':
            recipient_name = payment_blob.get('recipient', None)
            try:
                recipient = Account._by_name(recipient_name)
            except NotFound:
                dump_parameters(parameters)
                raise ValueError("Invalid recipient_name %s in IPN/GC with custom='%s'"
                                 % (recipient_name, custom))
            signed = payment_blob.get("signed", False)
            giftmessage = _force_unicode(payment_blob.get("giftmessage", ""))
            thing_fullname = payment_blob.get("thing")
            send_gift(buyer, recipient, months, days, signed, giftmessage,
                      thing_fullname)
            instagift = True
            subject = _("Thanks for giving the gift of reddit gold!")
            message = _("Your classy gift to %s has been delivered.\n\n"
                        "Thank you for gifting reddit gold. Your patronage "
                        "supports the site and makes future development "
                        "possible.") % recipient.name
            message += "\n\n" + strings.gold_benefits_msg + "\n\n"
            message += _("Thank you again for your support, and have fun "
                         "spreading gold!")
        elif payment_blob['goldtype'] == 'code':
            pass
        else:
            dump_parameters(parameters)
            raise ValueError("Got status '%s' in IPN/GC" % payment_blob['status'])

        if payment_blob['goldtype'] == 'code':
            send_gold_code(buyer, months, days, txn_id, payer_email,
                           pennies, buyer_email)
        else:
            # Reuse the old "secret" column as a place to record the goldtype
            # and "custom", just in case we need to debug it later or something
            secret = payment_blob['goldtype'] + "-" + custom

            if instagift:
                status="instagift"
            else:
                status="processed"

            create_claimed_gold(txn_id, payer_email, paying_id, pennies, days,
                                secret, buyer_id, c.start_time,
                                subscr_id, status=status)

            message = append_random_bottlecap_phrase(message)

            try:
                send_system_message(buyer, subject, message,
                                    distinguished='gold-auto')
            except MessageError:
                g.log.error('finish: could not send system message')

        payment_blob["status"] = "processed"
        g.hardcache.set(blob_key, payment_blob, 86400 * 30)


class Webhook(object):
    def __init__(self, passthrough=None, transaction_id=None, subscr_id=None,
                 pennies=None, months=None, payer_email='', payer_id='',
                 goldtype=None, buyer=None, recipient=None, signed=False,
                 giftmessage=None, thing=None, buyer_email=None):
        self.passthrough = passthrough
        self.transaction_id = transaction_id
        self.subscr_id = subscr_id
        self.pennies = pennies
        self.months = months
        self.payer_email = payer_email
        self.payer_id = payer_id
        self.goldtype = goldtype
        self.buyer = buyer
        self.buyer_email = buyer_email
        self.recipient = recipient
        self.signed = signed
        self.giftmessage = giftmessage
        self.thing = thing

    def load_blob(self):
        payment_blob = validate_blob(self.passthrough)
        self.goldtype = payment_blob['goldtype']
        self.buyer = payment_blob.get('buyer')
        self.buyer_email = payment_blob.get('email')
        self.recipient = payment_blob.get('recipient')
        self.signed = payment_blob.get('signed', False)
        self.giftmessage = payment_blob.get('giftmessage')
        thing = payment_blob.get('thing')
        self.thing = thing._fullname if thing else None

    def __repr__(self):
        return '<%s: transaction %s>' % (self.__class__.__name__, self.transaction_id)


class GoldPaymentController(RedditController):
    name = ''
    webhook_secret = ''
    event_type_mappings = {}

    @textresponse(secret=VPrintable('secret', 50))
    def POST_goldwebhook(self, secret):
        self.validate_secret(secret)
        status, webhook = self.process_response()

        try:
            event_type = self.event_type_mappings[status]
        except KeyError:
            g.log.error('%s %s: unknown status %s' % (self.name,
                                                      webhook,
                                                      status))
            self.abort403()
        self.process_webhook(event_type, webhook)

    def validate_secret(self, secret):
        if not constant_time_compare(secret, self.webhook_secret):
            g.log.error('%s: invalid webhook secret from %s' % (self.name,
                                                                request.ip))
            self.abort403() 

    @classmethod
    def process_response(cls):
        """Extract status and webhook."""
        raise NotImplementedError

    def process_webhook(self, event_type, webhook):
        if event_type == 'noop':
            return

        existing = retrieve_gold_transaction(webhook.transaction_id)
        if not existing and webhook.passthrough:
            try:
                webhook.load_blob()
            except GoldException as e:
                g.log.error('%s: payment_blob %s', webhook.transaction_id, e)
                self.abort403()
        msg = None

        if event_type == 'cancelled':
            subject = _('reddit gold payment cancelled')
            msg = _('Your reddit gold payment has been cancelled, contact '
                    '%(gold_email)s for details') % {'gold_email':
                                                     g.goldthanks_email}
            if existing:
                # note that we don't check status on existing, probably
                # should update gold_table when a cancellation happens
                reverse_gold_purchase(webhook.transaction_id)
        elif event_type == 'succeeded':
            if (existing and
                    existing.status in ('processed', 'unclaimed', 'claimed')):
                g.log.info('POST_goldwebhook skipping %s' % webhook.transaction_id)
                return

            self.complete_gold_purchase(webhook)
        elif event_type == 'failed':
            subject = _('reddit gold payment failed')
            msg = _('Your reddit gold payment has failed, contact '
                    '%(gold_email)s for details') % {'gold_email':
                                                     g.goldthanks_email}
        elif event_type == 'deleted_subscription':
            # the subscription may have been deleted directly by the user using
            # POST_delete_subscription, in which case gold_subscr_id is already
            # unset and we don't need to message them
            if webhook.buyer and webhook.buyer.gold_subscr_id:
                subject = _('reddit gold subscription cancelled')
                msg = _('Your reddit gold subscription has been cancelled '
                        'because your credit card could not be charged. '
                        'Contact %(gold_email)s for details')
                msg %= {'gold_email': g.goldthanks_email}
                webhook.buyer.gold_subscr_id = None
                webhook.buyer._commit()
        elif event_type == 'refunded':
            if not (existing and existing.status == 'processed'):
                return

            subject = _('reddit gold refund')
            msg = _('Your reddit gold payment has been refunded, contact '
                   '%(gold_email)s for details') % {'gold_email':
                                                    g.goldthanks_email}
            reverse_gold_purchase(webhook.transaction_id)

        if msg:
            if existing:
                buyer = Account._byID(int(existing.account_id), data=True)
            elif webhook.buyer:
                buyer = webhook.buyer
            else:
                return

            try:
                send_system_message(buyer, subject, msg)
            except MessageError:
                g.log.error('process_webhook: send_system_message error')

    @classmethod
    def complete_gold_purchase(cls, webhook):
        """After receiving a message from a payment processor, apply gold.

        Shared endpoint for all payment processing systems. Validation of gold
        purchase (sender, recipient, etc.) should happen before hitting this.

        """

        secret = webhook.passthrough
        transaction_id = webhook.transaction_id
        payer_email = webhook.payer_email
        payer_id = webhook.payer_id
        subscr_id = webhook.subscr_id
        pennies = webhook.pennies
        months = webhook.months
        goldtype = webhook.goldtype
        buyer = webhook.buyer
        buyer_email = webhook.buyer_email
        recipient = webhook.recipient
        signed = webhook.signed
        giftmessage = webhook.giftmessage
        thing = webhook.thing

        days = days_from_months(months)

        # locking isn't necessary for code purchases
        if goldtype == 'code':
            send_gold_code(buyer, months, days, transaction_id,
                           payer_email, pennies, buyer_email)
            # the rest of the function isn't needed for a code purchase
            return

        gold_recipient = recipient or buyer
        with gold_lock(gold_recipient):
            gold_recipient._sync_latest()

            if goldtype in ('onetime', 'autorenew'):
                admintools.engolden(buyer, days)
                if goldtype == 'onetime':
                    subject = "thanks for buying reddit gold!"
                    if g.lounge_reddit:
                        message = strings.lounge_msg
                    else:
                        message = ":)"
                else:
                    subject = "your reddit gold has been renewed!"
                    message = ("see the details of your subscription on "
                               "[your userpage](/u/%s)" % buyer.name)

            elif goldtype == 'creddits':
                buyer._incr('gold_creddits', months)
                subject = "thanks for buying creddits!"
                message = ("To spend them, visit http://%s/gold or your "
                           "favorite person's userpage." % (g.domain))

            elif goldtype == 'gift':
                send_gift(buyer, recipient, months, days, signed, giftmessage,
                          thing)
                subject = "thanks for giving reddit gold!"
                message = "Your gift to %s has been delivered." % recipient.name

            status = 'processed'
            secret_pieces = [goldtype]
            if goldtype == 'gift':
                secret_pieces.append(recipient.name)
            secret_pieces.append(secret or transaction_id)
            secret = '-'.join(secret_pieces)

            try:
                create_claimed_gold(transaction_id, payer_email, payer_id,
                                    pennies, days, secret, buyer._id,
                                    c.start_time, subscr_id=subscr_id,
                                    status=status)
            except IntegrityError:
                g.log.error('gold: got duplicate gold transaction')

            try:
                message = append_random_bottlecap_phrase(message)
                send_system_message(buyer, subject, message,
                                    distinguished='gold-auto')
            except MessageError:
                g.log.error('complete_gold_purchase: send_system_message error')


def handle_stripe_error(fn):
    def wrapper(cls, form, *a, **kw):
        try:
            return fn(cls, form, *a, **kw)
        except stripe.CardError as e:
            form.set_html('.status', 
                          _('error: %(error)s') % {'error': e.message})
        except stripe.InvalidRequestError as e:
            form.set_html('.status', _('invalid request'))
        except stripe.APIConnectionError as e:
            form.set_html('.status', _('api error'))
        except stripe.AuthenticationError as e:
            form.set_html('.status', _('connection error'))
        except stripe.StripeError as e:
            form.set_html('.status', _('error'))
            g.log.error('stripe error: %s' % e)
        except:
            raise
        form.find('.stripe-submit').removeAttr('disabled').end()
    return wrapper


class StripeController(GoldPaymentController):
    name = 'stripe'
    webhook_secret = g.secrets['stripe_webhook']
    event_type_mappings = {
        'charge.succeeded': 'succeeded',
        'charge.failed': 'failed',
        'charge.refunded': 'refunded',
        'charge.dispute.created': 'noop',
        'charge.dispute.updated': 'noop',
        'charge.dispute.closed': 'noop',
        'customer.created': 'noop',
        'customer.card.created': 'noop',
        'customer.card.deleted': 'noop',
        'transfer.created': 'noop',
        'transfer.paid': 'noop',
        'balance.available': 'noop',
        'invoice.created': 'noop',
        'invoice.updated': 'noop',
        'invoice.payment_succeeded': 'noop',
        'invoice.payment_failed': 'noop',
        'invoiceitem.deleted': 'noop',
        'customer.subscription.created': 'noop',
        'customer.deleted': 'noop',
        'customer.updated': 'noop',
        'customer.subscription.deleted': 'deleted_subscription',
        'customer.subscription.trial_will_end': 'noop',
        'customer.subscription.updated': 'noop',
        'dummy': 'noop',
    }

    @classmethod
    def process_response(cls):
        event_dict = json.loads(request.body)
        stripe_secret = g.secrets['stripe_secret_key']
        event = stripe.Event.construct_from(event_dict, stripe_secret)
        status = event.type

        if status == 'invoice.created':
            # sent 1 hr before a subscription is charged or immediately for
            # a new subscription
            invoice = event.data.object
            customer_id = invoice.customer
            account = account_from_stripe_customer_id(customer_id)
            # if the charge hasn't been attempted (meaning this is 1 hr before
            # the charge) check that the account can receive the gold
            if (not invoice.attempted and
                (not account or (account and account._banned))):
                # there's no associated account - delete the subscription
                # to cancel the charge
                g.log.error('no account for stripe invoice: %s', invoice)
                try:
                    customer = stripe.Customer.retrieve(customer_id)
                    customer.delete()
                except stripe.InvalidRequestError:
                    pass
        elif status == 'customer.subscription.deleted':
            subscription = event.data.object
            customer_id = subscription.customer
            buyer = account_from_stripe_customer_id(customer_id)
            webhook = Webhook(subscr_id=customer_id, buyer=buyer)
            return status, webhook

        event_type = cls.event_type_mappings.get(status)
        if not event_type:
            raise ValueError('Stripe: unrecognized status %s' % status)
        elif event_type == 'noop':
            return status, None

        charge = event.data.object
        description = charge.description
        invoice_id = charge.invoice
        transaction_id = 'S%s' % charge.id
        pennies = charge.amount
        months, days = months_and_days_from_pennies(pennies)

        if status == 'charge.failed' and invoice_id:
            # we'll get an additional failure notification event of
            # "invoice.payment_failed", don't double notify
            return 'dummy', None
        elif status == 'charge.failed' and not description:
            # create_customer can POST successfully but fail to create a
            # customer because the card is declined. This will trigger a
            # 'charge.failed' notification but without description so we can't
            # do anything with it
            return 'dummy', None
        elif invoice_id:
            # subscription charge - special handling
            customer_id = charge.customer
            buyer = account_from_stripe_customer_id(customer_id)
            if not buyer and status == 'charge.refunded':
                # refund may happen after the subscription has been cancelled
                # and removed from the user's account. the refund process will
                # be able to find the user from the transaction record
                webhook = Webhook(transaction_id=transaction_id)
                return status, webhook
            elif not buyer:
                charge_date = datetime.fromtimestamp(charge.created, tz=g.tz)

                # don't raise exception if charge date is within the past hour
                # db replication lag may cause the account lookup to fail
                if charge_date < timeago('1 hour'):
                    raise ValueError('no buyer for charge: %s' % charge.id)
                else:
                    abort(404, "not found")
            webhook = Webhook(transaction_id=transaction_id,
                              subscr_id=customer_id, pennies=pennies,
                              months=months, goldtype='autorenew',
                              buyer=buyer)
            return status, webhook
        else:
            try:
                passthrough, buyer_name = description.split('-', 1)
            except (AttributeError, ValueError):
                g.log.error('stripe_error on charge: %s', charge)
                raise

            webhook = Webhook(passthrough=passthrough,
                transaction_id=transaction_id, pennies=pennies, months=months)
            return status, webhook

    @classmethod
    @handle_stripe_error
    def create_customer(cls, form, token, description):
        customer = stripe.Customer.create(card=token, description=description)

        if (customer['active_card']['address_line1_check'] == 'fail' or
            customer['active_card']['address_zip_check'] == 'fail'):
            form.set_html('.status',
                          _('error: address verification failed'))
            form.find('.stripe-submit').removeAttr('disabled').end()
            return None
        elif customer['active_card']['cvc_check'] == 'fail':
            form.set_html('.status', _('error: cvc check failed'))
            form.find('.stripe-submit').removeAttr('disabled').end()
            return None
        else:
            return customer

    @classmethod
    @handle_stripe_error
    def charge_customer(cls, form, customer, pennies, passthrough,
                        description):
        charge = stripe.Charge.create(
            amount=pennies,
            currency="usd",
            customer=customer['id'],
            description='%s-%s' % (passthrough, description),
        )
        return charge

    @classmethod
    @handle_stripe_error
    def set_creditcard(cls, form, user, token):
        if not user.has_stripe_subscription:
            return

        customer = stripe.Customer.retrieve(user.gold_subscr_id)
        customer.card = token
        customer.save()
        return customer

    @classmethod
    @handle_stripe_error
    def set_subscription(cls, form, customer, plan_id):
        subscription = customer.update_subscription(plan=plan_id)
        return subscription

    @classmethod
    @handle_stripe_error
    def cancel_subscription(cls, form, user):
        if not user.has_stripe_subscription:
            return

        customer = stripe.Customer.retrieve(user.gold_subscr_id)
        customer.delete()

        user.gold_subscr_id = None
        user._commit()
        subject = _('your gold subscription has been cancelled')
        message = _('if you have any questions please email %(email)s')
        message %= {'email': g.goldthanks_email}
        send_system_message(user, subject, message)
        return customer

    @validatedForm(token=nop('stripeToken'),
                   passthrough=VPrintable("passthrough", max_length=50),
                   pennies=VInt('pennies'),
                   months=VInt("months"),
                   period=VOneOf("period", ("monthly", "yearly")))
    def POST_goldcharge(self, form, jquery, token, passthrough, pennies, months,
                        period):
        """
        Submit charge to stripe.

        Called by GoldPayment form. This submits the charge to stripe, and gold
        will be applied once we receive a webhook from stripe.

        """

        try:
            payment_blob = validate_blob(passthrough)
        except GoldException as e:
            # This should never happen. All fields in the payment_blob
            # are validated on creation
            form.set_html('.status',
                          _('something bad happened, try again later'))
            g.log.debug('POST_goldcharge: %s' % e.message)
            return

        if period:
            plan_id = (g.STRIPE_MONTHLY_GOLD_PLAN if period == 'monthly'
                       else g.STRIPE_YEARLY_GOLD_PLAN)
            if c.user.has_gold_subscription:
                form.set_html('.status',
                              _('your account already has a gold subscription'))
                return
        else:
            plan_id = None
            penny_months, days = months_and_days_from_pennies(pennies)
            if not months or months != penny_months:
                form.set_html('.status', _('stop trying to trick the form'))
                return

        if c.user_is_loggedin:
            description = c.user.name
        else:
            description = payment_blob["email"]
        customer = self.create_customer(form, token, description)
        if not customer:
            return

        if period:
            subscription = self.set_subscription(form, customer, plan_id)
            if not subscription:
                return

            c.user.gold_subscr_id = customer.id
            c.user._commit()

            status = _('subscription created')
            subject = _('reddit gold subscription')
            body = _('Your subscription is being processed and reddit gold '
                     'will be delivered shortly.')
        else:
            charge = self.charge_customer(form, customer, pennies,
                                          passthrough, description)
            if not charge:
                return

            status = _('payment submitted')
            subject = _('reddit gold payment')
            body = _('Your payment is being processed and reddit gold '
                     'will be delivered shortly.')

        form.set_html('.status', status)
        if c.user_is_loggedin:
            body = append_random_bottlecap_phrase(body)
            send_system_message(c.user, subject, body, distinguished='gold-auto')

    @validatedForm(VUser(),
                   VModhash(),
                   token=nop('stripeToken'))
    def POST_modify_subscription(self, form, jquery, token):
        customer = self.set_creditcard(form, c.user, token)
        if not customer:
            return

        form.set_html('.status', _('your payment details have been updated'))

    @validatedForm(VUser(),
                   VModhash(),
                   user=VByName('user'))
    def POST_cancel_subscription(self, form, jquery, user):
        if user != c.user and not c.user_is_admin:
            self.abort403()
        customer = self.cancel_subscription(form, user)
        if not customer:
            return

        form.set_html(".status", _("your subscription has been cancelled"))

class CoinbaseController(GoldPaymentController):
    name = 'coinbase'
    webhook_secret = g.secrets['coinbase_webhook']
    event_type_mappings = {
        'completed': 'succeeded',
        'cancelled': 'cancelled',
        'mispaid': 'noop',
        'expired': 'noop',
    }

    @classmethod
    def process_response(cls):
        event_dict = json.loads(request.body)
        order = event_dict['order']
        transaction_id = 'C%s' % order['id']
        status = order['status']    # new/completed/cancelled
        pennies = int(order['total_native']['cents'])
        months, days = months_and_days_from_pennies(pennies, discount=True)
        passthrough = order['custom']
        webhook = Webhook(passthrough=passthrough,
            transaction_id=transaction_id, pennies=pennies, months=months)
        return status, webhook


class RedditGiftsController(GoldPaymentController):
    """Handle notifications of gold purchases from reddit gifts.

    Payment is handled by reddit gifts. Once an order is complete they can hit
    this route to apply gold to a user's account.

    The post should include data in the form:
    {
        'transaction_id', transaction_id,
        'goldtype': goldtype,
        'buyer': buyer name,
        'pennies': pennies,
        'months': months,
        ['recipient': recipient name,]
        ['giftmessage': message,]
        ['signed': bool,]
    }

    """

    name = 'redditgifts'
    webhook_secret = g.secrets['redditgifts_webhook']
    event_type_mappings = {'succeeded': 'succeeded'}

    def process_response(self):
        data = request.POST

        transaction_id = 'RG%s' % data['transaction_id']
        pennies = int(data['pennies'])
        months = int(data['months'])
        status = 'succeeded'

        goldtype = data['goldtype']
        buyer = Account._by_name(data['buyer'])

        if goldtype == 'gift':
            gift_kw = {
                'recipient': Account._by_name(data['recipient']),
                'giftmessage': _force_utf8(data.get('giftmessage', None)),
                'signed': data.get('signed') == 'True',
            }
        else:
            gift_kw = {}

        webhook = Webhook(transaction_id=transaction_id, pennies=pennies,
                          months=months, goldtype=goldtype, buyer=buyer,
                          **gift_kw)
        return status, webhook


class GoldException(Exception): pass


def validate_blob(custom):
    """Validate payment_blob and return a dict with everything looked up."""
    ret = {}

    if not custom:
        raise GoldException('no custom')

    payment_blob = g.hardcache.get('payment_blob-%s' % str(custom))
    if not payment_blob:
        raise GoldException('no payment_blob')

    if 'account_id' in payment_blob and 'account_name' in payment_blob:
        try:
            buyer = Account._byID(payment_blob['account_id'], data=True)
            ret['buyer'] = buyer
        except NotFound:
            raise GoldException('bad account_id')

        if not buyer.name.lower() == payment_blob['account_name'].lower():
            raise GoldException('buyer mismatch')
    elif 'email' in payment_blob:
        ret['email'] = payment_blob['email']
    else:
        raise GoldException('no account_id or email')

    goldtype = payment_blob['goldtype']
    ret['goldtype'] = goldtype

    if goldtype == 'gift':
        recipient_name = payment_blob.get('recipient', None)
        if not recipient_name:
            raise GoldException('gift missing recpient')
        try:
            recipient = Account._by_name(recipient_name)
            ret['recipient'] = recipient
        except NotFound:
            raise GoldException('bad recipient')
        thing_fullname = payment_blob.get('thing', None)
        if thing_fullname:
            try:
                ret['thing'] = Thing._by_fullname(thing_fullname)
            except NotFound:
                raise GoldException('bad thing')
        ret['signed'] = payment_blob.get('signed', False)
        giftmessage = payment_blob.get('giftmessage')
        giftmessage = _force_unicode(giftmessage) if giftmessage else None
        ret['giftmessage'] = giftmessage
    elif goldtype not in ('onetime', 'autorenew', 'creddits', 'code'):
        raise GoldException('bad goldtype')

    return ret


def gold_lock(user):
    return g.make_lock('gold_purchase', 'gold_%s' % user._id)


def days_from_months(months):
    if months >= 12:
        assert months % 12 == 0
        years = months / 12
        days = years * 366
    else:
        days = months * 31
    return days


def subtract_gold_days(user, days):
    user.gold_expiration -= timedelta(days=days)
    if user.gold_expiration < datetime.now(g.display_tz):
        user.gold = False
    user._commit()


def subtract_gold_creddits(user, num):
    user._incr('gold_creddits', -num)


def reverse_gold_purchase(transaction_id):
    transaction = retrieve_gold_transaction(transaction_id)

    if not transaction:
        raise GoldException('gold_table %s not found' % transaction_id)

    buyer = Account._byID(int(transaction.account_id), data=True)
    recipient = None
    days = transaction.days
    months = days / 31

    secret = transaction.secret
    if '{' in secret:
        secret.strip('{}') # I goofed
        pieces = secret.split(',')
    else:
        pieces = secret.split('-')
    goldtype = pieces[0]
    if goldtype == 'gift':
        recipient_name, secret = pieces[1:]
        recipient = Account._by_name(recipient_name)

    gold_recipient = recipient or buyer
    with gold_lock(gold_recipient):
        gold_recipient._sync_latest()

        if goldtype in ('onetime', 'autorenew'):
            subtract_gold_days(buyer, days)

        elif goldtype == 'creddits':
            subtract_gold_creddits(buyer, months)

        elif goldtype == 'gift':
            subtract_gold_days(recipient, days)
            subject = 'your gifted gold has been reversed'
            message = 'sorry, but the payment was reversed'
            send_system_message(recipient, subject, message)
    update_gold_transaction(transaction_id, 'reversed')

########NEW FILE########
__FILENAME__ = listingcontroller
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import urllib

from oauth2 import require_oauth2_scope
from reddit_base import RedditController, base_listing, paginated_listing

from r2.models import *
from r2.models.query_cache import CachedQuery, MergedCachedQuery
from r2.config.extensions import is_api
from r2.lib.pages import *
from r2.lib.pages.things import wrap_links
from r2.lib.menus import TimeMenu, SortMenu, RecSortMenu, ProfileSortMenu
from r2.lib.menus import ControversyTimeMenu, menu
from r2.lib.rising import get_rising, normalized_rising
from r2.lib.wrapped import Wrapped
from r2.lib.normalized_hot import normalized_hot
from r2.lib.db.thing import Query, Merge, Relations
from r2.lib.db import queries
from r2.lib.strings import Score
import r2.lib.search as search
from r2.lib.template_helpers import add_sr
from r2.lib.admin_utils import check_cheating
from r2.lib.utils import iters, timeago
from r2.lib import organic, sup, trending
from r2.lib.memoize import memoize
from r2.lib.validator import *
from r2.lib.butler import extract_user_mentions
import socket

from api_docs import api_doc, api_section

from pylons.i18n import _

import random
from functools import partial

class ListingController(RedditController):
    """Generalized controller for pages with lists of links."""

    # toggle skipping of links based on the users' save/hide/vote preferences
    skip = True

    # allow stylesheets on listings
    allow_stylesheets = True

    # toggles showing numbers
    show_nums = True

    # any text that should be shown on the top of the page
    infotext = None

    # builder class to use to generate the listing. if none, we'll try
    # to figure it out based on the query type
    builder_cls = None

    # page title
    title_text = ''

    # login box, subreddit box, submit box, etc, visible
    show_sidebar = True
    show_chooser = False

    # class (probably a subclass of Reddit) to use to render the page.
    render_cls = Reddit

    # class for suggestions next to "next/prev" buttons
    next_suggestions_cls = None

    #extra parameters to send to the render_cls constructor
    render_params = {}
    extra_page_classes = ['listing-page']

    @property
    def menus(self):
        """list of menus underneat the header (e.g., sort, time, kind,
        etc) to be displayed on this listing page"""
        return []

    def build_listing(self, num, after, reverse, count, **kwargs):
        """uses the query() method to define the contents of the
        listing and renders the page self.render_cls(..).render() with
        the listing as contents"""
        self.num = num
        self.count = count
        self.after = after
        self.reverse = reverse

        self.query_obj = self.query()
        self.builder_obj = self.builder()
        self.listing_obj = self.listing()

        content = self.content()
        return self.render_cls(content=content,
                               page_classes=self.extra_page_classes,
                               show_sidebar=self.show_sidebar,
                               show_chooser=self.show_chooser,
                               nav_menus=self.menus,
                               title=self.title(),
                               infotext=self.infotext,
                               robots=getattr(self, "robots", None),
                               **self.render_params).render()

    def content(self):
        """Renderable object which will end up as content of the render_cls"""
        return self.listing_obj

    def query(self):
        """Query to execute to generate the listing"""
        raise NotImplementedError

    def builder(self):
        #store the query itself so it can be used elsewhere
        if self.builder_cls:
            builder_cls = self.builder_cls
        elif isinstance(self.query_obj, Query):
            builder_cls = QueryBuilder
        elif isinstance(self.query_obj, search.SearchQuery):
            builder_cls = SearchBuilder
        elif isinstance(self.query_obj, iters):
            builder_cls = IDBuilder
        elif isinstance(self.query_obj, (queries.CachedResults, queries.MergedCachedResults)):
            builder_cls = IDBuilder
        elif isinstance(self.query_obj, (CachedQuery, MergedCachedQuery)):
            builder_cls = IDBuilder

        b = builder_cls(self.query_obj,
                        num = self.num,
                        skip = self.skip,
                        after = self.after,
                        count = self.count,
                        reverse = self.reverse,
                        keep_fn = self.keep_fn(),
                        wrap = self.builder_wrapper)

        return b

    def keep_fn(self):
        def keep(item):
            wouldkeep = item.keep_item(item)
            if getattr(item, "promoted", None) is not None:
                return False
            if item._deleted and not c.user_is_admin:
                return False
            return wouldkeep
        return keep

    def listing(self):
        """Listing to generate from the builder"""
        if (getattr(c.site, "_id", -1) == get_promote_srid() and
            not c.user_is_sponsor):
            abort(403, 'forbidden')
        model = LinkListing(self.builder_obj, show_nums=self.show_nums)
        suggestions = None
        if self.next_suggestions_cls:
            suggestions = self.next_suggestions_cls()
        pane = model.listing(next_suggestions=suggestions)
        # Indicate that the comment tree wasn't built for comments
        for i in pane:
            if hasattr(i, 'full_comment_path'):
                i.child = None
        return pane

    def title(self):
        """Page <title>"""
        return _(self.title_text) + " : " + c.site.name

    def rightbox(self):
        """Contents of the right box when rendering"""
        pass

    builder_wrapper = staticmethod(default_thing_wrapper())

    @require_oauth2_scope("read")
    @base_listing
    def GET_listing(self, **env):
        check_cheating('site')
        return self.build_listing(**env)

listing_api_doc = partial(
    api_doc,
    section=api_section.listings,
    extends=ListingController.GET_listing,
    notes=[paginated_listing.doc_note],
    extensions=["json", "xml"],
)


class ListingWithPromos(ListingController):
    show_organic = False

    def make_requested_ad(self, requested_ad):
        try:
            link = Link._by_fullname(requested_ad, data=True)
        except NotFound:
            self.abort404()

        if not (link.promoted and
                (c.user_is_sponsor or
                 c.user_is_loggedin and link.author_id == c.user._id)):
            self.abort403()

        if not promote.is_live_on_sr(link, c.site):
            self.abort403()

        res = wrap_links([link._fullname], wrapper=self.builder_wrapper,
                         skip=False)
        res.parent_name = "promoted"
        if res.things:
            return res

    def make_single_ad(self):
        srnames = promote.srnames_with_live_promos(c.user, c.site)
        if srnames:
            return SpotlightListing(show_promo=True, srnames=srnames,
                                    navigable=False).listing()

    def make_spotlight(self):
        """Build the Spotlight.

        The frontpage gets a Spotlight box that contains promoted and organic
        links from the user's subscribed subreddits and promoted links targeted
        to the frontpage. If the user has disabled ads promoted links will not
        be shown. Promoted links are requested from the adserver client-side.

        """

        organic_fullnames = organic.organic_links(c.user)
        promoted_links = []

        show_promo = False
        srnames = []
        can_show_promo = c.user.pref_show_sponsors or not c.user.gold
        try_show_promo = ((c.user_is_loggedin and random.random() > 0.5) or
                          not c.user_is_loggedin)

        if can_show_promo and try_show_promo:
            srnames = promote.srnames_with_live_promos(c.user, c.site)
            if srnames:
                show_promo = True

        random.shuffle(organic_fullnames)
        organic_fullnames = organic_fullnames[:10]
        b = IDBuilder(organic_fullnames,
                      wrap=self.builder_wrapper,
                      keep_fn=organic.keep_fresh_links,
                      skip=True)
        organic_links = b.get_items()[0]

        has_subscribed = c.user.has_subscribed
        interestbar_prob = g.live_config['spotlight_interest_sub_p'
                                         if has_subscribed else
                                         'spotlight_interest_nosub_p']
        interestbar = InterestBar(has_subscribed)

        s = SpotlightListing(organic_links=organic_links,
                             interestbar=interestbar,
                             interestbar_prob=interestbar_prob,
                             show_promo=show_promo,
                             srnames=srnames,
                             max_num = self.listing_obj.max_num,
                             max_score = self.listing_obj.max_score).listing()
        return s

    def content(self):
        # only send a spotlight listing for HTML rendering
        if c.render_style == "html":
            spotlight = None
            show_sponsors = not (not c.user.pref_show_sponsors and c.user.gold)
            show_organic = self.show_organic and c.user.pref_organic
            on_frontpage = isinstance(c.site, DefaultSR)
            requested_ad = request.GET.get('ad')

            if on_frontpage:
                self.extra_page_classes = \
                    self.extra_page_classes + ['front-page']

            if requested_ad:
                spotlight = self.make_requested_ad(requested_ad)
            elif on_frontpage and show_organic:
                spotlight = self.make_spotlight()
            elif show_sponsors:
                spotlight = self.make_single_ad()

            self.spotlight = spotlight

            if spotlight:
                return PaneStack([spotlight, self.listing_obj],
                                 css_class='spacer')
        return self.listing_obj


class HotController(ListingWithPromos):
    where = 'hot'
    extra_page_classes = ListingController.extra_page_classes + ['hot-page']
    show_chooser = True
    next_suggestions_cls = ListingSuggestions
    show_organic = True

    def query(self):

        if isinstance(c.site, DefaultSR):
            sr_ids = Subreddit.user_subreddits(c.user)
            return normalized_hot(sr_ids)
        elif isinstance(c.site, MultiReddit):
            return normalized_hot(c.site.kept_sr_ids, obey_age_limit=False)
        else:
            if c.site.sticky_fullname:
                link_list = [c.site.sticky_fullname]
                wrapped = wrap_links(link_list,
                                     wrapper=self.builder_wrapper,
                                     keep_fn=self.keep_fn(),
                                     skip=True)
                # add all other items and decrement count if sticky is visible
                if wrapped.things:
                    link_list += [l for l in c.site.get_links('hot', 'all')
                                    if l != c.site.sticky_fullname]
                    if not self.after:
                        self.count -= 1
                        self.num += 1
                    return link_list
            
            # no sticky or sticky hidden
            return c.site.get_links('hot', 'all')

    @classmethod
    def trending_info(cls):
        if not c.user.pref_show_trending:
            return None

        trending_data = trending.get_trending_subreddits()

        if not trending_data:
            return None

        link = Link._byID(trending_data['link_id'], data=True, stale=True)
        return {
            'subreddit_names': trending_data['subreddit_names'],
            'comment_url': trending_data['permalink'],
            'comment_count': link.num_comments,
        }

    def content(self):
        content = super(HotController, self).content()
        if (c.render_style == "html" and isinstance(c.site, DefaultSR) and
                not self.listing_obj.prev):
            trending_info = self.trending_info()
            if trending_info:
                return PaneStack(filter(None, [
                    self.spotlight,
                    TrendingSubredditsBar(**trending_info),
                    self.listing_obj,
                ]), css_class='spacer')
        return content


    def title(self):
        return c.site.title

    @require_oauth2_scope("read")
    @listing_api_doc(uri='/hot', uses_site=True)
    def GET_listing(self, **env):
        self.infotext = request.GET.get('deleted') and strings.user_deleted
        return ListingController.GET_listing(self, **env)

class NewController(ListingWithPromos):
    where = 'new'
    title_text = _('newest submissions')
    extra_page_classes = ListingController.extra_page_classes + ['new-page']
    show_chooser = True
    next_suggestions_cls = ListingSuggestions

    def keep_fn(self):
        def keep(item):
            if item.promoted is not None:
                return False
            else:
                return item.keep_item(item)
        return keep

    def query(self):
        return c.site.get_links('new', 'all')

    def POST_listing(self, **env):
        # Redirect to GET mode in case of any legacy requests
        return self.redirect(request.fullpath)

    @require_oauth2_scope("read")
    @listing_api_doc(uri='/new', uses_site=True)
    def GET_listing(self, **env):
        if request.params.get('sort') == 'rising':
            return self.redirect(add_sr('/rising'))

        return ListingController.GET_listing(self, **env)

class RisingController(NewController):
    where = 'rising'
    title_text = _('rising submissions')
    extra_page_classes = ListingController.extra_page_classes + ['rising-page']

    def query(self):
        if isinstance(c.site, DefaultSR):
            sr_ids = Subreddit.user_subreddits(c.user)
            return normalized_rising(sr_ids)
        elif isinstance(c.site, MultiReddit):
            return normalized_rising(c.site.kept_sr_ids)

        return get_rising(c.site)

class BrowseController(ListingWithPromos):
    where = 'browse'
    show_chooser = True
    next_suggestions_cls = ListingSuggestions

    def keep_fn(self):
        """For merged time-listings, don't show items that are too old
           (this can happen when mr_top hasn't run in a while)"""
        if self.time != 'all' and c.default_sr:
            oldest = timeago('1 %s' % (str(self.time),))
            def keep(item):
                return item._date > oldest and item.keep_item(item)
            return keep
        else:
            return ListingController.keep_fn(self)

    @property
    def menus(self):
        return [ControversyTimeMenu(default = self.time)]

    def query(self):
        return c.site.get_links(self.sort, self.time)

    @validate(t = VMenu('sort', ControversyTimeMenu))
    def POST_listing(self, sort, t, **env):
        # VMenu validator will save the value of time before we reach this
        # point. Now just redirect to GET mode.
        return self.redirect(
            request.fullpath + query_string(dict(sort=sort, t=t)))

    @require_oauth2_scope("read")
    @validate(t = VMenu('sort', ControversyTimeMenu))
    @listing_api_doc(uri='/{sort}', uri_variants=['/top', '/controversial'],
                     uses_site=True)
    def GET_listing(self, sort, t, **env):
        self.sort = sort
        if sort == 'top':
            self.title_text = _('top scoring links')
            self.extra_page_classes = \
                self.extra_page_classes + ['top-page']
        elif sort == 'controversial':
            self.title_text = _('most controversial links')
            self.extra_page_classes = \
                self.extra_page_classes + ['controversial-page']
        else:
            # 'sort' is forced to top/controversial by routing.py,
            # but in case something has gone wrong...
            abort(404)
        self.time = t
        return ListingController.GET_listing(self, **env)


class AdsController(ListingController):
    builder_cls = CampaignBuilder
    title_text = _('promoted links')

    @property
    def infotext(self):
        infotext = _("want to advertise? [click here!](%(link)s)")
        if c.user.pref_show_promote or c.user_is_sponsor:
            return infotext % {'link': '/promoted'}
        else:
            return infotext % {'link': '/ad_inq'}

    def keep_fn(self):
        def keep(item):
            return item.promoted and not item._deleted
        return keep

    def query(self):
        try:
            return c.site.get_live_promos()
        except NotImplementedError:
            self.abort404()


class RandomrisingController(ListingWithPromos):
    where = 'randomrising'
    title_text = _('you\'re really bored now, eh?')
    next_suggestions_cls = ListingSuggestions

    def query(self):
        links = get_rising(c.site)

        if not links:
            # just pull from the new page if the rising page isn't
            # populated for some reason
            links = c.site.get_links('new', 'all')
            if isinstance(links, Query):
                links._limit = 200
                links = [x._fullname for x in links]

        links = list(links)
        random.shuffle(links)

        return links

class ByIDController(ListingController):
    title_text = _('API')
    skip = False

    def query(self):
        return self.names

    @require_oauth2_scope("read")
    @validate(links=VByName("names", thing_cls=Link,
                            ignore_missing=True, multiple=True))
    @api_doc(api_section.listings, uri='/by_id/{names}')
    def GET_listing(self, links, **env):
        """Get a listing of links by fullname.

        `names` is a list of fullnames for links separated by commas or spaces.

        """
        if not links:
            return self.abort404()
        self.names = [l._fullname for l in links]
        return ListingController.GET_listing(self, **env)


class UserController(ListingController):
    render_cls = ProfilePage
    show_nums = False
    skip = True

    @property
    def menus(self):
        res = []
        if (self.where in ('overview', 'submitted', 'comments')):
            res.append(ProfileSortMenu(default = self.sort))
            if self.sort not in ("hot", "new"):
                res.append(TimeMenu(default = self.time))
        if self.where == 'saved' and c.user.gold:
            srnames = LinkSavesBySubreddit.get_saved_subreddits(self.vuser)
            srnames += CommentSavesBySubreddit.get_saved_subreddits(self.vuser)
            srs = Subreddit._by_name(srnames)
            srnames = [name for name, sr in srs.iteritems()
                            if sr.can_view(c.user)]
            srnames = sorted(set(srnames), key=lambda name: name.lower())
            if len(srnames) > 1:
                sr_buttons = [NavButton(_('all'), None, opt='sr',
                                        css_class='primary')]
                for srname in srnames:
                    sr_buttons.append(NavButton(srname, srname, opt='sr'))
                base_path = '/user/%s/saved' % self.vuser.name
                if self.savedcategory:
                    base_path += '/%s' % urllib.quote(self.savedcategory)
                sr_menu = NavMenu(sr_buttons, base_path=base_path,
                                  title=_('filter by subreddit'),
                                  type='lightdrop')
                res.append(sr_menu)
            categories = LinkSavesByCategory.get_saved_categories(self.vuser)
            categories += CommentSavesByCategory.get_saved_categories(self.vuser)
            categories = sorted(set(categories))
            if len(categories) >= 1:
                cat_buttons = [NavButton(_('all'), '/', css_class='primary')]
                for cat in categories:
                    cat_buttons.append(NavButton(cat,
                                                 urllib.quote(cat),
                                                 use_params=True))
                base_path = '/user/%s/saved/' % self.vuser.name
                cat_menu = NavMenu(cat_buttons, base_path=base_path,
                                   title=_('filter by category'),
                                   type='lightdrop')
                res.append(cat_menu)
        elif (self.where == 'gilded' and
                (c.user == self.vuser or c.user_is_admin)):
            path = '/user/%s/gilded/' % self.vuser.name
            buttons = [NavButton(_("gildings received"), dest='/'),
                       NavButton(_("gildings given"), dest='/given')]
            res.append(NavMenu(buttons, base_path=path, type='flatlist'))

        return res

    def title(self):
        titles = {'overview': _("overview for %(user)s"),
                  'comments': _("comments by %(user)s"),
                  'submitted': _("submitted by %(user)s"),
                  'gilded': _("gilded by %(user)s"),
                  'liked': _("liked by %(user)s"),
                  'disliked': _("disliked by %(user)s"),
                  'saved': _("saved by %(user)s"),
                  'hidden': _("hidden by %(user)s"),
                  'promoted': _("promoted by %(user)s")}
        if self.where == 'gilded' and self.show == 'given':
            return _("gildings given by %(user)s") % {'user': self.vuser.name}

        title = titles.get(self.where, _('profile for %(user)s')) \
            % dict(user = self.vuser.name, site = c.site.name)
        return title

    def keep_fn(self):
        def keep(item):
            if self.where == 'promoted':
                return bool(getattr(item, "promoted", None))

            if item._deleted and not c.user_is_admin:
                return False

            if c.user == self.vuser:
                if not item.likes and self.where == 'liked':
                    return False
                if item.likes is not False and self.where == 'disliked':
                    return False
                if self.where == 'saved' and not item.saved:
                    return False

            if (self.time != 'all' and
                item._date <= utils.timeago('1 %s' % str(self.time))):
                return False

            if self.where == 'gilded' and item.gildings <= 0:
                return False

            if self.where == 'deleted' and not item._deleted:
                return False

            is_promoted = getattr(item, "promoted", None) is not None
            if self.where != 'saved' and is_promoted:
                return False

            return True
        return keep

    def query(self):
        q = None
        if self.where == 'overview':
            self.check_modified(self.vuser, 'overview')
            q = queries.get_overview(self.vuser, self.sort, self.time)

        elif self.where == 'comments':
            sup.set_sup_header(self.vuser, 'commented')
            self.check_modified(self.vuser, 'commented')
            q = queries.get_comments(self.vuser, self.sort, self.time)

        elif self.where == 'submitted':
            sup.set_sup_header(self.vuser, 'submitted')
            self.check_modified(self.vuser, 'submitted')
            q = queries.get_submitted(self.vuser, self.sort, self.time)

        elif self.where == 'gilded':
            sup.set_sup_header(self.vuser, 'gilded')
            self.check_modified(self.vuser, 'gilded')
            if self.show == 'given':
                q = queries.get_user_gildings(self.vuser)
            else:
                q = queries.get_gilded_user(self.vuser)

        elif self.where in ('liked', 'disliked'):
            sup.set_sup_header(self.vuser, self.where)
            self.check_modified(self.vuser, self.where)
            if self.where == 'liked':
                q = queries.get_liked(self.vuser)
            else:
                q = queries.get_disliked(self.vuser)

        elif self.where == 'hidden':
            q = queries.get_hidden(self.vuser)

        elif self.where == 'saved':
            if not self.savedcategory and c.user.gold:
                self.builder_cls = SavedBuilder
            sr_id = self.savedsr._id if self.savedsr else None
            q = queries.get_saved(self.vuser, sr_id,
                                  category=self.savedcategory)
        elif c.user_is_sponsor and self.where == 'promoted':
            q = queries.get_promoted_links(self.vuser._id)

        if q is None:
            return self.abort404()

        return q

    @require_oauth2_scope("history")
    @validate(vuser = VExistingUname('username'),
              sort = VMenu('sort', ProfileSortMenu, remember = False),
              time = VMenu('t', TimeMenu, remember = False),
              show=VOneOf('show', ('given',)))
    @listing_api_doc(section=api_section.users, uri='/user/{username}/{where}',
                     uri_variants=['/user/{username}/' + where for where in [
                                       'overview', 'submitted', 'comments',
                                       'liked', 'disliked', 'hidden', 'saved',
                                       'gilded']])
    def GET_listing(self, where, vuser, sort, time, show, **env):
        self.where = where
        self.sort = sort
        self.time = time
        self.show = show

        # the validator will ensure that vuser is a valid account
        if not vuser:
            return self.abort404()

        if self.sort in  ('hot', 'new'):
            self.time = 'all'


        # hide spammers profile pages
        if (not c.user_is_loggedin or
            (c.user._id != vuser._id and not c.user_is_admin)) \
               and vuser._spam:
            return self.abort404()

        if where in ('liked', 'disliked') and not votes_visible(vuser):
            return self.abort403()

        if ((where in ('saved', 'hidden') or 
                (where == 'gilded' and show == 'given')) and
                not (c.user_is_loggedin and c.user._id == vuser._id) and
                not c.user_is_admin):
            return self.abort403()

        if where == 'saved':
            self.show_chooser = True
            category = VSavedCategory('category').run(env.get('category'))
            srname = request.GET.get('sr')
            if srname and c.user.gold:
                try:
                    sr = Subreddit._by_name(srname)
                except NotFound:
                    sr = None
            else:
                sr = None
            if category and not c.user.gold:
                category = None
            self.savedsr = sr
            self.savedcategory = category

        check_cheating('user')

        self.vuser = vuser
        self.render_params = {'user' : vuser}
        c.profilepage = True

        if vuser.pref_hide_from_robots:
            self.robots = 'noindex,nofollow'

        return ListingController.GET_listing(self, **env)

    @require_oauth2_scope("read")
    @validate(vuser = VExistingUname('username'))
    @api_doc(section=api_section.users, uri='/user/{username}/about',
             extensions=['json'])
    def GET_about(self, vuser):
        """Return information about the user, including karma and gold status."""
        if (not is_api() or
                not vuser or
                (vuser._spam and vuser != c.user)):
            return self.abort404()
        return Reddit(content = Wrapped(vuser)).render()

    def GET_saved_redirect(self):
        if not c.user_is_loggedin:
            abort(404)

        dest = "/".join(("/user", c.user.name, "saved"))
        extension = request.environ.get('extension')
        if extension:
            dest = ".".join((dest, extension))
        query_string = request.environ.get('QUERY_STRING')
        if query_string:
            dest += "?" + query_string
        return self.redirect(dest)

class MessageController(ListingController):
    show_nums = False
    render_cls = MessagePage
    allow_stylesheets = False
    # note: this intentionally replaces the listing-page class which doesn't
    # conceptually fit for styling these pages.
    extra_page_classes = ['messages-page']

    @property
    def show_sidebar(self):
        if c.default_sr and not isinstance(c.site, (ModSR, MultiReddit)):
            return False

        return self.where in ("moderator", "multi")

    @property
    def menus(self):
        if c.default_sr and self.where in ('inbox', 'messages', 'comments',
                          'selfreply', 'unread', 'mentions'):
            buttons = [NavButton(_("all"), "inbox"),
                       NavButton(_("unread"), "unread"),
                       NavButton(plurals.messages, "messages"),
                       NavButton(_("comment replies"), 'comments'),
                       NavButton(_("post replies"), 'selfreply')]

            if c.user.gold:
                buttons += [NavButton(_("username mentions"),
                                      "mentions",
                                      css_class="gold")]

            return [NavMenu(buttons, base_path = '/message/',
                            default = 'inbox', type = "flatlist")]
        elif not c.default_sr or self.where in ('moderator', 'multi'):
            buttons = (NavButton(_("all"), "inbox"),
                       NavButton(_("unread"), "unread"))
            return [NavMenu(buttons, base_path = '/message/moderator/',
                            default = 'inbox', type = "flatlist")]
        return []


    def title(self):
        return _('messages') + ': ' + _(self.where)

    def keep_fn(self):
        def keep(item):
            wouldkeep = item.keep_item(item)

            # TODO: Consider a flag to disable this (and see above plus builder.py)
            if item._deleted and not c.user_is_admin:
                return False
            if (item._spam and
                    item.author_id != c.user._id and
                    not c.user_is_admin):
                return False
            if item.author_id in c.user.enemies:
                return False
            # don't show user their own unread stuff
            if ((self.where == 'unread' or self.subwhere == 'unread')
                and (item.author_id == c.user._id or not item.new)):
                return False

            if (item.message_style == "mention" and
                c.user.name.lower() not in extract_user_mentions(item.body)):
                return False

            return wouldkeep
        return keep

    @staticmethod
    def builder_wrapper(thing):
        if isinstance(thing, Comment):
            f = thing._fullname
            w = Wrapped(thing)
            w.render_class = Message
            w.to_id = c.user._id
            w.was_comment = True
            w._fullname = f
        else:
            w = ListingController.builder_wrapper(thing)

        return w

    def builder(self):
        if (self.where == 'messages' or
            (self.where in ("moderator", "multi") and self.subwhere != "unread")):
            root = c.user
            message_cls = UserMessageBuilder

            if self.where == "multi":
                root = c.site
                message_cls = MultiredditMessageBuilder
            elif not c.default_sr:
                root = c.site
                message_cls = SrMessageBuilder
            elif self.where == 'moderator' and self.subwhere != 'unread':
                message_cls = ModeratorMessageBuilder

            parent = None
            skip = False
            if self.message:
                if self.message.first_message:
                    parent = Message._byID(self.message.first_message,
                                           data=True)
                else:
                    parent = self.message
            elif c.user.pref_threaded_messages:
                skip = (c.render_style == "html")

            if (message_cls is UserMessageBuilder and parent and parent.sr_id
                and not parent.from_sr):
                # Make sure we use the subreddit message builder for modmail,
                # because the per-user cache will be wrong if more than two
                # parties are involved in the thread.
                root = Subreddit._byID(parent.sr_id)
                message_cls = SrMessageBuilder

            return message_cls(root,
                               wrap = self.builder_wrapper,
                               parent = parent,
                               skip = skip,
                               num = self.num,
                               after = self.after,
                               keep_fn = self.keep_fn(),
                               reverse = self.reverse)
        return ListingController.builder(self)

    def listing(self):
        if (self.where == 'messages' and
            (c.user.pref_threaded_messages or self.message)):
            return Listing(self.builder_obj).listing()
        pane = ListingController.listing(self)

        # Indicate that the comment tree wasn't built for comments
        for i in pane.things:
            if i.was_comment:
                i.child = None

        return pane

    def query(self):
        if self.where == 'messages':
            q = queries.get_inbox_messages(c.user)
        elif self.where == 'comments':
            q = queries.get_inbox_comments(c.user)
        elif self.where == 'selfreply':
            q = queries.get_inbox_selfreply(c.user)
        elif self.where == 'mentions':
            q = queries.get_inbox_comment_mentions(c.user)
        elif self.where == 'inbox':
            q = queries.get_inbox(c.user)
        elif self.where == 'unread':
            q = queries.get_unread_inbox(c.user)
        elif self.where == 'sent':
            q = queries.get_sent(c.user)
        elif self.where == 'multi' and self.subwhere == 'unread':
            q = queries.get_unread_subreddit_messages_multi(c.site.kept_sr_ids)
        elif self.where == 'moderator' and self.subwhere == 'unread':
            if c.default_sr:
                srids = Subreddit.reverse_moderator_ids(c.user)
                srs = [sr for sr in Subreddit._byID(srids, data=False,
                                                    return_dict=False)
                       if sr.is_moderator_with_perms(c.user, 'mail')]
                q = queries.get_unread_subreddit_messages_multi(srs)
            else:
                q = queries.get_unread_subreddit_messages(c.site)
        elif self.where in ('moderator', 'multi'):
            if c.have_mod_messages and self.mark != 'false':
                c.have_mod_messages = False
                c.user.modmsgtime = False
                c.user._commit()
            # the query is handled by the builder on the moderator page
            return
        else:
            return self.abort404()
        if self.where != 'sent':
            #reset the inbox
            if c.have_messages and self.mark != 'false':
                c.have_messages = False
                c.user.msgtime = False
                c.user._commit()

        return q

    @require_oauth2_scope("privatemessages")
    @validate(VUser(),
              message = VMessageID('mid'),
              mark = VOneOf('mark',('true','false')))
    @listing_api_doc(section=api_section.messages,
                     uri='/message/{where}',
                     uri_variants=['/message/inbox', '/message/unread', '/message/sent'])
    def GET_listing(self, where, mark, message, subwhere = None, **env):
        if not (c.default_sr
                or c.site.is_moderator_with_perms(c.user, 'mail')
                or c.user_is_admin):
            abort(403, "forbidden")
        if isinstance(c.site, MultiReddit):
            if not (c.user_is_admin or c.site.is_moderator(c.user)):
                self.abort403()
            self.where = "multi"
        elif isinstance(c.site, ModSR) or not c.default_sr:
            self.where = "moderator"
        else:
            self.where = where
        self.subwhere = subwhere
        if mark is not None:
            self.mark = mark
        elif is_api():
            self.mark = 'false'
        elif c.render_style and c.render_style == "xml":
            self.mark = 'false'
        else:
            self.mark = 'true'
        self.message = message
        return ListingController.GET_listing(self, **env)

    @validate(VUser(),
              to = nop('to'),
              subject = nop('subject'),
              message = nop('message'),
              success = nop('success'))
    def GET_compose(self, to, subject, message, success):
        captcha = Captcha() if c.user.needs_captcha() else None
        content = MessageCompose(to = to, subject = subject,
                                 captcha = captcha,
                                 message = message,
                                 success = success)
        return MessagePage(content = content).render()

class RedditsController(ListingController):
    render_cls = SubredditsPage

    def title(self):
        return _('subreddits')

    def keep_fn(self):
        base_keep_fn = ListingController.keep_fn(self)
        def keep(item):
            return base_keep_fn(item) and (c.over18 or not item.over_18)
        return keep

    def query(self):
        if self.where == 'banned' and c.user_is_admin:
            reddits = Subreddit._query(Subreddit.c._spam == True,
                                       sort = desc('_date'),
                                       write_cache = True,
                                       read_cache = True,
                                       cache_time = 5 * 60)
        else:
            reddits = None
            if self.where == 'new':
                reddits = Subreddit._query( write_cache = True,
                                            read_cache = True,
                                            cache_time = 5 * 60)
                reddits._sort = desc('_date')
            else:
                reddits = Subreddit._query( write_cache = True,
                                            read_cache = True,
                                            cache_time = 60 * 60)
                reddits._sort = desc('_downs')
            # Consider resurrecting when it is not the World Cup
            #if c.content_langs != 'all':
            #    reddits._filter(Subreddit.c.lang == c.content_langs)

            if g.domain != 'reddit.com':
                # don't try to render special subreddits (like promos)
                reddits._filter(Subreddit.c.author_id != -1)

        if self.where == 'popular':
            self.render_params = {"show_interestbar": True}

        return reddits

    @require_oauth2_scope("read")
    @listing_api_doc(section=api_section.subreddits,
                     uri='/subreddits/{where}',
                     uri_variants=['/subreddits/popular', '/subreddits/new'])
    def GET_listing(self, where, **env):
        """Get all subreddits.

        The `where` parameter chooses the order in which the subreddits are
        displayed.  `popular` sorts on the activity of the subreddit and the
        position of the subreddits can shift around. `new` sorts the subreddits
        based on their creation date, newest first.

        """
        self.where = where
        return ListingController.GET_listing(self, **env)

class MyredditsController(ListingController):
    render_cls = MySubredditsPage

    @property
    def menus(self):
        buttons = (NavButton(plurals.subscriber,  'subscriber'),
                    NavButton(getattr(plurals, "approved submitter"), 'contributor'),
                    NavButton(plurals.moderator,   'moderator'))

        return [NavMenu(buttons, base_path = '/subreddits/mine/',
                        default = 'subscriber', type = "flatlist")]

    def title(self):
        return _('subreddits: ') + self.where

    def builder_wrapper(self, thing):
        w = ListingController.builder_wrapper(thing)
        if self.where == 'moderator':
            is_moderator = thing.is_moderator(c.user)
            if is_moderator:
                w.mod_permissions = is_moderator.get_permissions()
        return w

    def query(self):
        reddits = SRMember._query(SRMember.c._name == self.where,
                                  SRMember.c._thing2_id == c.user._id,
                                  #hack to prevent the query from
                                  #adding it's own date
                                  sort = (desc('_t1_ups'), desc('_t1_date')),
                                  eager_load = True,
                                  thing_data = True)
        reddits.prewrap_fn = lambda x: x._thing1
        return reddits

    def content(self):
        user = c.user if c.user_is_loggedin else None
        num_subscriptions = len(Subreddit.reverse_subscriber_ids(user))
        if self.where == 'subscriber' and num_subscriptions == 0:
            message = strings.sr_messages['empty']
        else:
            message = strings.sr_messages.get(self.where)

        stack = PaneStack()

        if message:
            stack.append(InfoBar(message=message))

        stack.append(self.listing_obj)

        return stack

    def build_listing(self, after=None, **kwargs):
        if after and isinstance(after, Subreddit):
            after = SRMember._fast_query(after, c.user, self.where,
                                         data=False).values()[0]
        if after and not isinstance(after, SRMember):
            abort(400, 'gimme a srmember')

        return ListingController.build_listing(self, after=after, **kwargs)

    @require_oauth2_scope("mysubreddits")
    @validate(VUser())
    @listing_api_doc(section=api_section.subreddits,
                     uri='/subreddits/mine/{where}',
                     uri_variants=['/subreddits/mine/subscriber', '/subreddits/mine/contributor', '/subreddits/mine/moderator'])
    def GET_listing(self, where='subscriber', **env):
        """Get subreddits the user has a relationship with.

        The `where` parameter chooses which subreddits are returned as follows:

        * `subscriber` - subreddits the user is subscribed to
        * `contributor` - subreddits the user is an approved submitter in
        * `moderator` - subreddits the user is a moderator of

        See also: [/api/subscribe](#POST_api_subscribe),
        [/api/friend](#POST_api_friend), and
        [/api/accept_moderator_invite](#POST_api_accept_moderator_invite).

        """
        self.where = where
        return ListingController.GET_listing(self, **env)

class CommentsController(ListingController):
    title_text = _('comments')

    def keep_fn(self):
        def keep(item):
            can_see_spam = (c.user_is_loggedin and
                            (item.author_id == c.user._id or
                             c.user_is_admin or
                             item.subreddit.is_moderator(c.user)))
            can_see_deleted = c.user_is_loggedin and c.user_is_admin

            return ((not item._spam or can_see_spam) and
                    (not item._deleted or can_see_deleted))

        return keep

    def query(self):
        return c.site.get_all_comments()

    @require_oauth2_scope("read")
    def GET_listing(self, **env):
        c.profilepage = True
        return ListingController.GET_listing(self, **env)

class UserListListingController(ListingController):
    builder_cls = UserListBuilder
    allow_stylesheets = False
    skip = False
    friends_compat = True

    @property
    def infotext(self):
        if self.where == 'friends':
            return strings.friends % Friends.path
        elif self.where == 'blocked':
            return _("To block a user click 'block user'  below a message"
                     " from a user you wish to block from messaging you.")

    @property
    def render_params(self):
        params = {}
        is_wiki_action = self.where in ["wikibanned", "wikicontributors"]
        params["show_wiki_actions"] = is_wiki_action
        return params

    @property
    def render_cls(self):
        if self.where in ["friends", "blocked"]:
            return PrefsPage
        return Reddit

    def moderator_wrap(self, rel, invited=False):
        rel._permission_class = ModeratorPermissionSet
        cls = ModTableItem if not invited else InvitedModTableItem
        return cls(rel, editable=self.editable)

    @property
    def builder_wrapper(self):
        if self.where == 'banned':
            cls = BannedTableItem
        elif self.where == 'moderators':
            return self.moderator_wrap
        elif self.where == 'wikibanned':
            cls = WikiBannedTableItem
        elif self.where == 'contributors':
            cls = ContributorTableItem
        elif self.where == 'wikicontributors':
            cls = WikiMayContributeTableItem
        elif self.where == 'friends':
            cls = FriendTableItem
        elif self.where == 'blocked':
            cls = EnemyTableItem
        return lambda rel : cls(rel, editable=self.editable)

    def title(self):
        return menu[self.where]

    def rel(self):
        if self.where in ['friends', 'blocked']:
            return Friend
        return SRMember

    def name(self):
        return self._names.get(self.where)

    _names = {
              'friends': 'friend',
              'blocked': 'enemy',
              'moderators': 'moderator',
              'contributors': 'contributor',
              'banned': 'banned',
              'wikibanned': 'wikibanned',
              'wikicontributors': 'wikicontributor',
             }

    def query(self):
        rel = self.rel()
        if self.where in ["friends", "blocked"]:
            thing1_id = c.user._id
        else:
            thing1_id = c.site._id
        reversed_types = ["friends", "moderators", "blocked"]
        sort = desc if self.where not in reversed_types else asc
        q = rel._query(rel.c._thing1_id == thing1_id,
                       rel.c._name == self.name(),
                       sort=sort('_date'),
                       data=True)
        if self.jump_to_val:
            thing2_id = self.user._id if self.user else None
            q._filter(rel.c._thing2_id == thing2_id)
        return q

    def listing(self):
        listing = self.listing_cls(self.builder_obj,
                                   addable=self.editable,
                                   show_jump_to=self.show_jump_to,
                                   jump_to_value=self.jump_to_val,
                                   show_not_found=self.show_not_found,
                                   nextprev=self.paginated,
                                   has_add_form=self.editable)
        return listing.listing()

    def invited_mod_listing(self):
        query = SRMember._query(SRMember.c._name == 'moderator_invite',
                                SRMember.c._thing1_id == c.site._id,
                                sort=asc('_date'), data=True)
        wrapper = lambda rel: self.moderator_wrap(rel, invited=True)
        b = self.builder_cls(query,
                             keep_fn=self.keep_fn(),
                             wrap=wrapper,
                             skip=False,
                             num=0)
        return InvitedModListing(b, nextprev=False).listing()

    def content(self):
        is_api = c.render_style in extensions.API_TYPES
        if self.where == 'moderators' and self.editable and not is_api:
            # Do not stack the invited mod list in api mode
            # to allow for api compatibility with older api users.
            content = PaneStack()
            content.append(self.listing_obj)
            content.append(self.invited_mod_listing())
        elif self.where == 'friends' and is_api and self.friends_compat:
            content = PaneStack()
            content.append(self.listing_obj)
            empty_builder = IDBuilder([])
            # Append an empty UserList on the api for backwards
            # compatibility with the old blocked list.
            content.append(UserListing(empty_builder, nextprev=False).listing())
        else:
            content = self.listing_obj
        return content

    @require_oauth2_scope("read")
    @validate(VUser())
    @base_listing
    @listing_api_doc(section=api_section.account,
                     uri='/prefs/{where}',
                     uri_variants=['/prefs/friends', '/prefs/blocked',
                         '/api/v1/me/friends', '/api/v1/me/blocked'])
    def GET_user_prefs(self, where, **kw):
        self.where = where

        self.listing_cls = None
        self.editable = True
        self.paginated = False
        self.jump_to_val = None
        self.show_not_found = False
        self.show_jump_to = False
        # The /prefs/friends version of this endpoint used to contain
        # two lists of users: friends AND blocked users. For backwards
        # compatibility with the old JSON structure, an empty list
        # of "blocked" users is sent.
        # The /api/v1/me/friends version of the friends list does not
        # have this requirement, so it will send just the "friends"
        # data structure.
        self.friends_compat = not request.path.startswith('/api/v1/me/')

        if where == 'friends':
            self.listing_cls = FriendListing
        elif where == 'blocked':
            self.listing_cls = EnemyListing
            self.show_not_found = True
        else:
            abort(404)

        kw['num'] = 0
        check_cheating('site')
        return self.build_listing(**kw)


    @require_oauth2_scope("read")
    @validate(user=VAccountByName('user'))
    @base_listing
    @listing_api_doc(section=api_section.subreddits,
                     uses_site=True,
                     uri='/about/{where}',
                     uri_variants=['/about/' + where for where in [
                        'banned', 'wikibanned', 'contributors',
                        'wikicontributors', 'moderators']])
    def GET_listing(self, where, user=None, **kw):
        if isinstance(c.site, FakeSubreddit):
            return self.abort404()

        self.where = where

        has_mod_access = ((c.user_is_loggedin and
                           c.site.is_moderator_with_perms(c.user, 'access'))
                          or c.user_is_admin)

        if not c.user_is_loggedin and where not in ['contributors', 'moderators']:
            abort(403)

        self.listing_cls = None
        self.editable = True
        self.paginated = True
        self.jump_to_val = request.GET.get('user')
        self.show_not_found = bool(self.jump_to_val)

        if where == 'contributors':
            # On public reddits, only moderators may see the whitelist.
            if c.site.type == 'public' and not has_mod_access:
                abort(403)
            # Used for subreddits like /r/lounge
            if c.site.hide_subscribers:
                abort(403)
            self.listing_cls = ContributorListing
            self.editable = has_mod_access

        elif where == 'banned':
            if not has_mod_access:
                abort(403)
            self.listing_cls = BannedListing

        elif where == 'wikibanned':
            if not c.site.is_moderator_with_perms(c.user, 'wiki'):
                abort(403)
            self.listing_cls = WikiBannedListing

        elif where == 'wikicontributors':
            if not c.site.is_moderator_with_perms(c.user, 'wiki'):
                abort(403)
            self.listing_cls = WikiMayContributeListing

        elif where == 'moderators':
            self.editable = ((c.user_is_loggedin and
                              c.site.is_unlimited_moderator(c.user)) or
                             c.user_is_admin)
            self.listing_cls = ModListing
            self.paginated = False

        if not self.listing_cls:
            abort(404)

        self.user = user
        self.show_jump_to = self.paginated

        if not self.paginated:
            kw['num'] = 0

        check_cheating('site')
        return self.build_listing(**kw)

class GildedController(ListingController):
    title_text = _("gilded")

    def keep_fn(self):
        def keep(item):
            return item.gildings > 0 and not item._deleted and not item._spam
        return keep

    def query(self):
        try:
            return c.site.get_gilded()
        except NotImplementedError:
            abort(404)

    @require_oauth2_scope("read")
    def GET_listing(self, **env):
        c.profilepage = True
        return ListingController.GET_listing(self, **env)

########NEW FILE########
__FILENAME__ = mediaembed
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import hashlib
import hmac

from pylons import request, g, c
from pylons.controllers.util import abort

from r2.controllers.reddit_base import MinimalController
from r2.lib.pages import MediaEmbedBody
from r2.lib.media import get_media_embed
from r2.lib.utils import constant_time_compare
from r2.lib.validator import validate, VLink, nop


class MediaembedController(MinimalController):
    @validate(
        link=VLink('link'),
        credentials=nop('credentials'),
    )
    def GET_mediaembed(self, link, credentials):
        if request.host != g.media_domain:
            # don't serve up untrusted content except on our
            # specifically untrusted domain
            abort(404)

        if link.subreddit_slow.type == "private":
            expected_mac = hmac.new(g.secrets["media_embed"], link._id36,
                                    hashlib.sha1).hexdigest()
            if not constant_time_compare(credentials or "", expected_mac):
                abort(404)

        if not c.secure:
            media_object = link.media_object
        else:
            media_object = link.secure_media_object

        if not media_object:
            abort(404)
        elif isinstance(media_object, dict):
            # otherwise it's the new style, which is a dict(type=type, **args)
            media_embed = get_media_embed(media_object)
            content = media_embed.content

        c.allow_framing = True

        return MediaEmbedBody(body = content).render()


class AdController(MinimalController):
    def try_pagecache(self):
        pass

    def GET_ad(self):
        return "This is a placeholder ad."

########NEW FILE########
__FILENAME__ = multi
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import c, request, response
from pylons.i18n import _

from r2.config.extensions import set_extension
from r2.controllers.api_docs import api_doc, api_section
from r2.controllers.reddit_base import RedditController, abort_with_error
from r2.controllers.oauth2 import require_oauth2_scope
from r2.models.account import Account
from r2.models.subreddit import (
    FakeSubreddit,
    Subreddit,
    LabeledMulti,
    TooManySubredditsError,
)
from r2.lib.db import tdb_cassandra, thing
from r2.lib.wrapped import Wrapped
from r2.lib.validator import (
    validate,
    VUser,
    VModhash,
    VOneOf,
    VSubredditName,
    VSRByName,
    VValidatedJSON,
    VMarkdown,
    VMultiPath,
    VMultiByPath,
)
from r2.lib.pages.things import wrap_things
from r2.lib.jsontemplates import (
    LabeledMultiJsonTemplate,
    LabeledMultiDescriptionJsonTemplate,
)
from r2.lib.errors import errors, RedditError


multi_sr_data_json_spec = VValidatedJSON.Object({
    'name': VSubredditName('name', allow_language_srs=True),
})


multi_json_spec = VValidatedJSON.Object({
    'visibility': VOneOf('visibility', ('private', 'public')),
    'subreddits': VValidatedJSON.ArrayOf(multi_sr_data_json_spec),
})


multi_description_json_spec = VValidatedJSON.Object({
    'body_md': VMarkdown('body_md', empty_error=None),
})


class MultiApiController(RedditController):
    on_validation_error = staticmethod(abort_with_error)

    def pre(self):
        set_extension(request.environ, "json")
        RedditController.pre(self)

    @require_oauth2_scope("read")
    @validate(VUser())
    @api_doc(api_section.multis, uri="/api/multi/mine")
    def GET_my_multis(self):
        """Fetch a list of multis belonging to the current user."""
        multis = LabeledMulti.by_owner(c.user)
        wrapped = wrap_things(*multis)
        resp = [w.render() for w in wrapped]
        return self.api_wrapper(resp)

    def _format_multi(self, multi):
        resp = wrap_things(multi)[0].render()
        return self.api_wrapper(resp)

    @require_oauth2_scope("read")
    @validate(multi=VMultiByPath("multipath", require_view=True))
    @api_doc(
        api_section.multis,
        uri="/api/multi/{multipath}",
    )
    def GET_multi(self, multi):
        """Fetch a multi's data and subreddit list by name."""
        return self._format_multi(multi)

    def _check_new_multi_path(self, path_info):
        if path_info['username'].lower() != c.user.name.lower():
            raise RedditError('MULTI_CANNOT_EDIT', code=403,
                              fields='multipath')

    def _add_multi_srs(self, multi, sr_datas):
        srs = Subreddit._by_name(sr_data['name'] for sr_data in sr_datas)

        for sr in srs.itervalues():
            if isinstance(sr, FakeSubreddit):
                raise RedditError('MULTI_SPECIAL_SUBREDDIT',
                                  msg_params={'path': sr.path},
                                  code=400)

        sr_props = {}
        for sr_data in sr_datas:
            try:
                sr = srs[sr_data['name']]
            except KeyError:
                raise RedditError('SUBREDDIT_NOEXIST', code=400)
            else:
                # name is passed in via the API data format, but should not be
                # stored on the model.
                del sr_data['name']
                sr_props[sr] = sr_data

        try:
            multi.add_srs(sr_props)
        except TooManySubredditsError as e:
            raise RedditError('MULTI_TOO_MANY_SUBREDDITS', code=409)

        return sr_props

    def _write_multi_data(self, multi, data):
        multi.visibility = data['visibility']

        multi.clear_srs()
        try:
            self._add_multi_srs(multi, data['subreddits'])
        except:
            multi._revert()
            raise

        multi._commit()
        return multi

    @require_oauth2_scope("subscribe")
    @validate(
        VUser(),
        VModhash(),
        path_info=VMultiPath("multipath"),
        data=VValidatedJSON("model", multi_json_spec),
    )
    @api_doc(api_section.multis, extends=GET_multi)
    def POST_multi(self, path_info, data):
        """Create a multi. Responds with 409 Conflict if it already exists."""

        self._check_new_multi_path(path_info)

        try:
            LabeledMulti._byID(path_info['path'])
        except tdb_cassandra.NotFound:
            multi = LabeledMulti.create(path_info['path'], c.user)
            response.status = 201
        else:
            raise RedditError('MULTI_EXISTS', code=409, fields='multipath')

        self._write_multi_data(multi, data)
        return self._format_multi(multi)

    @require_oauth2_scope("subscribe")
    @validate(
        VUser(),
        VModhash(),
        path_info=VMultiPath("multipath"),
        data=VValidatedJSON("model", multi_json_spec),
    )
    @api_doc(api_section.multis, extends=GET_multi)
    def PUT_multi(self, path_info, data):
        """Create or update a multi."""

        self._check_new_multi_path(path_info)

        try:
            multi = LabeledMulti._byID(path_info['path'])
        except tdb_cassandra.NotFound:
            multi = LabeledMulti.create(path_info['path'], c.user)
            response.status = 201

        self._write_multi_data(multi, data)
        return self._format_multi(multi)

    @require_oauth2_scope("subscribe")
    @validate(
        VUser(),
        VModhash(),
        multi=VMultiByPath("multipath", require_edit=True),
    )
    @api_doc(api_section.multis, extends=GET_multi)
    def DELETE_multi(self, multi):
        """Delete a multi."""
        multi.delete()

    def _copy_multi(self, from_multi, to_path_info):
        self._check_new_multi_path(to_path_info)

        to_owner = Account._by_name(to_path_info['username'])

        try:
            LabeledMulti._byID(to_path_info['path'])
        except tdb_cassandra.NotFound:
            to_multi = LabeledMulti.copy(to_path_info['path'], from_multi,
                                         owner=to_owner)
        else:
            raise RedditError('MULTI_EXISTS', code=409, fields='multipath')

        return to_multi

    @require_oauth2_scope("subscribe")
    @validate(
        VUser(),
        VModhash(),
        from_multi=VMultiByPath("from", require_view=True),
        to_path_info=VMultiPath("to",
            docs={"to": "destination multireddit url path"},
        ),
    )
    @api_doc(
        api_section.multis,
        uri="/api/multi/{multipath}/copy",
    )
    def POST_multi_copy(self, from_multi, to_path_info):
        """Copy a multi.

        Responds with 409 Conflict if the target already exists.

        A "copied from ..." line will automatically be appended to the
        description.

        """
        to_multi = self._copy_multi(from_multi, to_path_info)
        from_path = from_multi.path
        to_multi.copied_from = from_path
        if to_multi.description_md:
            to_multi.description_md += '\n\n'
        to_multi.description_md += _('copied from %(source)s') % {
            # force markdown linking since /user/foo is not autolinked
            'source': '[%s](%s)' % (from_path, from_path)
        }
        to_multi.visibility = 'private'
        to_multi._commit()
        return self._format_multi(to_multi)

    @require_oauth2_scope("subscribe")
    @validate(
        VUser(),
        VModhash(),
        from_multi=VMultiByPath("from", require_edit=True),
        to_path_info=VMultiPath("to",
            docs={"to": "destination multireddit url path"},
        ),
    )
    @api_doc(
        api_section.multis,
        uri="/api/multi/{multipath}/rename",
    )
    def POST_multi_rename(self, from_multi, to_path_info):
        """Rename a multi."""

        to_multi = self._copy_multi(from_multi, to_path_info)
        from_multi.delete()
        return self._format_multi(to_multi)

    def _get_multi_subreddit(self, multi, sr):
        resp = LabeledMultiJsonTemplate.sr_props(multi, [sr])[0]
        return self.api_wrapper(resp)

    @require_oauth2_scope("read")
    @validate(
        VUser(),
        multi=VMultiByPath("multipath", require_view=True),
        sr=VSRByName('srname'),
    )
    @api_doc(
        api_section.multis,
        uri="/api/multi/{multipath}/r/{srname}",
    )
    def GET_multi_subreddit(self, multi, sr):
        """Get data about a subreddit in a multi."""
        return self._get_multi_subreddit(multi, sr)

    @require_oauth2_scope("subscribe")
    @validate(
        VUser(),
        VModhash(),
        multi=VMultiByPath("multipath", require_edit=True),
        sr_name=VSubredditName('srname', allow_language_srs=True),
        data=VValidatedJSON("model", multi_sr_data_json_spec),
    )
    @api_doc(api_section.multis, extends=GET_multi_subreddit)
    def PUT_multi_subreddit(self, multi, sr_name, data):
        """Add a subreddit to a multi."""

        new = not any(sr.name.lower() == sr_name.lower() for sr in multi.srs)

        data['name'] = sr_name
        sr_props = self._add_multi_srs(multi, [data])
        sr = sr_props.items()[0][0]
        multi._commit()

        if new:
            response.status = 201

        return self._get_multi_subreddit(multi, sr)

    @require_oauth2_scope("subscribe")
    @validate(
        VUser(),
        VModhash(),
        multi=VMultiByPath("multipath", require_edit=True),
        sr=VSRByName('srname'),
    )
    @api_doc(api_section.multis, extends=GET_multi_subreddit)
    def DELETE_multi_subreddit(self, multi, sr):
        """Remove a subreddit from a multi."""
        multi.del_srs(sr)
        multi._commit()

    def _format_multi_description(self, multi):
        resp = LabeledMultiDescriptionJsonTemplate().render(multi).finalize()
        return self.api_wrapper(resp)

    @require_oauth2_scope("read")
    @validate(
        VUser(),
        multi=VMultiByPath("multipath", require_view=True),
    )
    @api_doc(
        api_section.multis,
        uri="/api/multi/{multipath}/description",
    )
    def GET_multi_description(self, multi):
        """Get a multi's description."""
        return self._format_multi_description(multi)

    @require_oauth2_scope("read")
    @validate(
        VUser(),
        VModhash(),
        multi=VMultiByPath("multipath", require_edit=True),
        data=VValidatedJSON('model', multi_description_json_spec),
    )
    @api_doc(api_section.multis, extends=GET_multi_description)
    def PUT_multi_description(self, multi, data):
        """Change a multi's markdown description."""
        multi.description_md = data['body_md']
        multi._commit()
        return self._format_multi_description(multi)

########NEW FILE########
__FILENAME__ = oauth2
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from urllib import urlencode
import base64
import simplejson

from pylons import c, g, request
from pylons.i18n import _
from r2.config.extensions import set_extension
from r2.lib.base import abort
from reddit_base import RedditController, MinimalController, require_https
from r2.lib.db.thing import NotFound
from r2.models import Account
from r2.models.token import (
    OAuth2Client, OAuth2AuthorizationCode, OAuth2AccessToken,
    OAuth2RefreshToken, OAuth2Scope)
from r2.lib.errors import ForbiddenError, errors
from r2.lib.pages import OAuth2AuthorizationPage
from r2.lib.require import RequirementException, require, require_split
from r2.lib.utils import constant_time_compare, parse_http_basic, UrlParser
from r2.lib.validator import (
    nop,
    validate,
    VRequired,
    VThrottledLogin,
    VOneOf,
    VUser,
    VModhash,
    VOAuth2ClientID,
    VOAuth2Scope,
    VOAuth2RefreshToken,
)


def _update_redirect_uri(base_redirect_uri, params):
    parsed = UrlParser(base_redirect_uri)
    parsed.update_query(**params)
    return parsed.unparse()


class OAuth2FrontendController(RedditController):
    def check_for_bearer_token(self):
        pass

    def pre(self):
        RedditController.pre(self)
        require_https()

    def _check_redirect_uri(self, client, redirect_uri):
        if not redirect_uri or not client or redirect_uri != client.redirect_uri:
            abort(ForbiddenError(errors.OAUTH2_INVALID_REDIRECT_URI))

    def _error_response(self, state, redirect_uri):
        """Return an error redirect, but only if client_id and redirect_uri are valid."""

        resp = {"state": state}

        if (errors.OAUTH2_INVALID_CLIENT, "client_id") in c.errors:
            resp["error"] = "unauthorized_client"
        elif (errors.OAUTH2_ACCESS_DENIED, "authorize") in c.errors:
            resp["error"] = "access_denied"
        elif (errors.BAD_HASH, None) in c.errors:
            resp["error"] = "access_denied"
        elif (errors.INVALID_OPTION, "response_type") in c.errors:
            resp["error"] = "unsupported_response_type"
        elif (errors.OAUTH2_INVALID_SCOPE, "scope") in c.errors:
            resp["error"] = "invalid_scope"
        else:
            resp["error"] = "invalid_request"

        final_redirect = _update_redirect_uri(redirect_uri, resp)
        return self.redirect(final_redirect, code=302)

    @validate(VUser(),
              response_type = VOneOf("response_type", ("code",)),
              client = VOAuth2ClientID(),
              redirect_uri = VRequired("redirect_uri", errors.OAUTH2_INVALID_REDIRECT_URI),
              scope = VOAuth2Scope(),
              state = VRequired("state", errors.NO_TEXT),
              duration = VOneOf("duration", ("temporary", "permanent"),
                                default="temporary"))
    def GET_authorize(self, response_type, client, redirect_uri, scope, state,
                      duration):
        """
        First step in [OAuth 2.0](http://oauth.net/2/) authentication.
        End users will be prompted for their credentials (username/password)
        and asked if they wish to authorize the application identified by
        the **client_id** parameter with the permissions specified by the
        **scope** parameter.  They are then redirected to the endpoint on
        the client application's side specified by **redirect_uri**.

        If the user granted permission to the application, the response will
        contain a **code** parameter with a temporary authorization code
        which can be exchanged for an access token at
        [/api/v1/access_token](#api_method_access_token).

        **redirect_uri** must match the URI configured for the client in the
        [app preferences](/prefs/apps).  If **client_id** or **redirect_uri**
        is not valid, or if the call does not take place over SSL, a 403
        error will be returned.  For all other errors, a redirect to
        **redirect_uri** will be returned, with a **error** parameter
        indicating why the request failed.
        """

        self._check_redirect_uri(client, redirect_uri)

        if not c.errors:
            return OAuth2AuthorizationPage(client, redirect_uri, scope, state,
                                           duration).render()
        else:
            return self._error_response(state, redirect_uri)

    @validate(VUser(),
              VModhash(fatal=False),
              client = VOAuth2ClientID(),
              redirect_uri = VRequired("redirect_uri", errors.OAUTH2_INVALID_REDIRECT_URI),
              scope = VOAuth2Scope(),
              state = VRequired("state", errors.NO_TEXT),
              duration = VOneOf("duration", ("temporary", "permanent"),
                                default="temporary"),
              authorize = VRequired("authorize", errors.OAUTH2_ACCESS_DENIED))
    def POST_authorize(self, authorize, client, redirect_uri, scope, state,
                       duration):
        """Endpoint for OAuth2 authorization."""

        self._check_redirect_uri(client, redirect_uri)

        if not c.errors:
            code = OAuth2AuthorizationCode._new(client._id, redirect_uri,
                                                c.user._id36, scope,
                                                duration == "permanent")
            resp = {"code": code._id, "state": state}
            final_redirect = _update_redirect_uri(redirect_uri, resp)
            return self.redirect(final_redirect, code=302)
        else:
            return self._error_response(state, redirect_uri)

class OAuth2AccessController(MinimalController):
    def pre(self):
        set_extension(request.environ, "json")
        MinimalController.pre(self)
        require_https()
        c.oauth2_client = self._get_client_auth()

    def _get_client_auth(self):
        auth = request.headers.get("Authorization")
        try:
            client_id, client_secret = parse_http_basic(auth)
            client = OAuth2Client.get_token(client_id)
            require(client)
            require(constant_time_compare(client.secret, client_secret))
            return client
        except RequirementException:
            abort(401, headers=[("WWW-Authenticate", 'Basic realm="reddit"')])

    @validate(grant_type=VOneOf("grant_type",
                                ("authorization_code",
                                 "refresh_token",
                                 "password")))
    def POST_access_token(self, grant_type):
        """
        Exchange an [OAuth 2.0](http://oauth.net/2/) authorization code
        or refresh token (from [/api/v1/authorize](#api_method_authorize)) for
        an access token.

        On success, returns a URL-encoded dictionary containing
        **access_token**, **token_type**, **expires_in**, and **scope**.
        If an authorization code for a permanent grant was given, a
        **refresh_token** will be included. If there is a problem, an **error**
        parameter will be returned instead.

        Must be called using SSL, and must contain a HTTP `Authorization:`
        header which contains the application's client identifier as the
        username and client secret as the password.  (The client id and secret
        are visible on the [app preferences page](/prefs/apps).)

        Per the OAuth specification, **grant_type** must
        be ``authorization_code`` for the initial access token or
        ``refresh_token`` for renewing the access token.

        **redirect_uri** must exactly match the value that was used in the call
        to [/api/v1/authorize](#api_method_authorize) that created this grant.
        """

        if grant_type == "authorization_code":
            return self._access_token_code()
        elif grant_type == "refresh_token":
            return self._access_token_refresh()
        elif grant_type == "password":
            return self._access_token_password()
        else:
            resp = {"error": "unsupported_grant_type"}
            return self.api_wrapper(resp)

    def _check_for_errors(self):
        resp = {}
        if (errors.INVALID_OPTION, "scope") in c.errors:
            resp["error"] = "invalid_scope"
        else:
            resp["error"] = "invalid_request"
        return resp

    def _make_token_dict(self, access_token, refresh_token=None):
        if not access_token:
            return {"error": "invalid_grant"}
        expires_in = int(access_token._ttl) if access_token._ttl else None
        resp = {
            "access_token": access_token._id,
            "token_type": access_token.token_type,
            "expires_in": expires_in,
            "scope": access_token.scope,
        }
        if refresh_token:
            resp["refresh_token"] = refresh_token._id
        return resp

    @validate(code=nop("code"),
              redirect_uri=VRequired("redirect_uri",
                                     errors.OAUTH2_INVALID_REDIRECT_URI))
    def _access_token_code(self, code, redirect_uri):
        if not code:
            c.errors.add("NO_TEXT", field="code")
        if c.errors:
            return self.api_wrapper(self._check_for_errors())

        access_token = None
        refresh_token = None

        auth_token = OAuth2AuthorizationCode.use_token(
            code, c.oauth2_client._id, redirect_uri)
        if auth_token:
            if auth_token.refreshable:
                refresh_token = OAuth2RefreshToken._new(
                    auth_token.client_id, auth_token.user_id,
                    auth_token.scope)
            access_token = OAuth2AccessToken._new(
                auth_token.client_id, auth_token.user_id,
                auth_token.scope,
                refresh_token._id if refresh_token else None)

        resp = self._make_token_dict(access_token, refresh_token)

        return self.api_wrapper(resp)

    @validate(refresh_token=VOAuth2RefreshToken("refresh_token"))
    def _access_token_refresh(self, refresh_token):
        resp = {}
        access_token = None
        if refresh_token:
            access_token = OAuth2AccessToken._new(
                refresh_token.client_id, refresh_token.user_id,
                refresh_token.scope,
                refresh_token=refresh_token._id)
        else:
            c.errors.add("NO_TEXT", field="refresh_token")

        if c.errors:
            resp = self._check_for_errors()
        else:
            resp = self._make_token_dict(access_token)
        return self.api_wrapper(resp)

    @validate(user=VThrottledLogin(["username", "password"]),
              scope=nop("scope"))
    def _access_token_password(self, user, scope):
        # username:password auth via OAuth is only allowed for
        # private use scripts
        client = c.oauth2_client
        if client.app_type != "script":
            return self.api_wrapper({"error": "unauthorized_client",
                "error_description": "Only script apps may use password auth"})
        dev_ids = client._developer_ids
        if not user or user._id not in dev_ids:
            return self.api_wrapper({"error": "invalid_grant"})
        if c.errors:
            return self.api_wrapper(self._check_for_errors())

        if scope:
            scope = OAuth2Scope(scope)
            if not scope.is_valid():
                c.errors.add(errors.INVALID_OPTION, "scope")
                return self.api_wrapper({"error": "invalid_scope"})
        else:
            scope = OAuth2Scope(OAuth2Scope.FULL_ACCESS)

        access_token = OAuth2AccessToken._new(
                client._id,
                user._id36,
                scope
        )
        resp = self._make_token_dict(access_token)
        return self.api_wrapper(resp)


def require_oauth2_scope(*scopes):
    def oauth2_scope_wrap(fn):
        fn.oauth2_perms = {"required_scopes": scopes, "oauth2_allowed": True}
        return fn
    return oauth2_scope_wrap


def allow_oauth2_access(fn):
    fn.oauth2_perms = {"required_scopes": [], "oauth2_allowed": True}
    return fn

########NEW FILE########
__FILENAME__ = policies
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import g
from pylons.i18n import _
from BeautifulSoup import BeautifulSoup, Tag

from r2.lib.base import abort
from r2.controllers.reddit_base import RedditController
from r2.models.subreddit import Frontpage
from r2.models.wiki import WikiPage, WikiRevision, WikiBadRevision
from r2.lib.db import tdb_cassandra
from r2.lib.filters import unsafe, wikimarkdown, generate_table_of_contents
from r2.lib.validator import validate, nop
from r2.lib.pages import PolicyPage, PolicyView


class PoliciesController(RedditController):
    @validate(requested_rev=nop('v'))
    def GET_policy_page(self, page, requested_rev):
        if page == 'privacypolicy':
            wiki_name = g.wiki_page_privacy_policy
            pagename = _('privacy policy')
        elif page == 'useragreement':
            wiki_name = g.wiki_page_user_agreement
            pagename = _('user agreement')
        else:
            abort(404)

        wp = WikiPage.get(Frontpage, wiki_name)

        revs = list(wp.get_revisions())

        # collapse minor edits into revisions with reasons
        rev_info = []
        last_edit = None
        for rev in revs:
            if rev.is_hidden:
                continue

            if not last_edit:
                last_edit = rev

            if rev._get('reason'):
                rev_info.append({
                    'id': str(last_edit._id),
                    'title': rev._get('reason'),
                })
                last_edit = None

        if requested_rev:
            try:
                display_rev = WikiRevision.get(requested_rev, wp._id)
            except (tdb_cassandra.NotFound, WikiBadRevision):
                abort(404)
        else:
            display_rev = revs[0]

        doc_html = wikimarkdown(display_rev.content, include_toc=False)
        soup = BeautifulSoup(doc_html.decode('utf-8'))
        toc = generate_table_of_contents(soup, prefix='section')
        self._number_sections(soup)
        self._linkify_headings(soup)

        content = PolicyView(
            body_html=unsafe(soup),
            toc_html=unsafe(toc),
            revs=rev_info,
            display_rev=str(display_rev._id),
        )
        return PolicyPage(
            pagename=pagename,
            content=content,
        ).render()

    def _number_sections(self, soup):
        count = 1
        for para in soup.find('div', 'md').findAll(['p'], recursive=False):
            a = Tag(soup, 'a', [
                ('class', 'p-anchor'),
                ('id', 'p_%d' % count),
                ('href', '#p_%d' % count),
            ])
            a.append(str(count))
            para.insert(0, a)
            para.insert(1, ' ')
            count += 1

    def _linkify_headings(self, soup):
        md_el = soup.find('div', 'md')
        for heading in md_el.findAll(['h1', 'h2', 'h3'], recursive=False):
            heading_a = Tag(soup, "a", [('href', '#%s' % heading['id'])])
            heading_a.contents = heading.contents
            heading.contents = []
            heading.append(heading_a)

########NEW FILE########
__FILENAME__ = post
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
from r2.lib.pages import *
from reddit_base import cross_domain
from api import ApiController
from r2.lib.errors import BadRequestError, errors
from r2.lib.utils import Storage, query_string, UrlParser
from r2.lib.emailer import opt_in, opt_out
from r2.lib.validator import *
from r2.lib.validator.preferences import (
    filter_prefs,
    format_content_lang_pref,
    PREFS_VALIDATORS,
    set_prefs,
)
from r2.models.recommend import ExploreSettings
from pylons import request, c, g
from pylons.controllers.util import redirect_to
from pylons.i18n import _
from r2.models import *
import hashlib

class PostController(ApiController):
    def _langs_from_post(self, all_or_some):
        if all_or_some == 'all':
            return 'all'
        langs = []
        for lang in g.all_languages:
            if request.POST.get('lang-' + lang):
                langs.append(str(lang))
        return format_content_lang_pref(langs)

    @validate(pref_lang = VLang('lang'),
              all_langs = VOneOf('all-langs', ('all', 'some'), default='all'))
    def POST_unlogged_options(self, all_langs, pref_lang):
        content_langs = self._langs_from_post(all_langs)
        prefs = {"pref_content_lang": content_langs, "pref_lang": pref_lang}
        set_prefs(c.user, prefs)
        c.user._commit()
        return self.redirect(request.referer)

    @validate(VUser(), VModhash(),
              all_langs=VOneOf('all-langs', ('all', 'some'), default='all'),
              **PREFS_VALIDATORS)
    def POST_options(self, all_langs, **prefs):
        pref_content_langs = self._langs_from_post(all_langs)
        prefs['pref_content_langs'] = pref_content_langs
        filter_prefs(prefs, c.user)
        if c.errors.errors:
            return abort(BadRequestError(errors.INVALID_PREF))
        set_prefs(c.user, prefs)
        c.user._commit()
        u = UrlParser(c.site.path + "prefs")
        u.update_query(done = 'true')
        if c.cname:
            u.put_in_frame()
        return self.redirect(u.unparse())

    def GET_over18(self):
        return BoringPage(_("over 18?"),
                          content = Over18()).render()

    @validate(VModhash(fatal=False),
              over18 = nop('over18'),
              dest = VDestination(default = '/'))
    def POST_over18(self, over18, dest):
        if over18 == 'yes':
            if c.user_is_loggedin and not c.errors:
                c.user.pref_over_18 = True
                c.user._commit()
            else:
                c.cookies.add("over18", "1")
            return self.redirect(dest)
        else:
            return self.redirect('/')


    @validate(msg_hash = nop('x'))
    def POST_optout(self, msg_hash):
        email, sent = opt_out(msg_hash)
        if not email:
            return self.abort404()
        return BoringPage(_("opt out"),
                          content = OptOut(email = email, leave = True,
                                           sent = True,
                                           msg_hash = msg_hash)).render()

    @validate(msg_hash = nop('x'))
    def POST_optin(self, msg_hash):
        email, sent = opt_in(msg_hash)
        if not email:
            return self.abort404()
        return BoringPage(_("welcome back"),
                          content = OptOut(email = email, leave = False,
                                           sent = True,
                                           msg_hash = msg_hash)).render()


    @validate(dest = VDestination(default = "/"))
    def POST_login(self, dest, *a, **kw):
        ApiController._handle_login(self, *a, **kw)
        c.render_style = "html"
        response.content_type = "text/html"

        if c.errors:
            return LoginPage(user_login = request.POST.get('user'),
                             dest = dest).render()

        return self.redirect(dest)

    @validate(dest = VDestination(default = "/"))
    def POST_reg(self, dest, *a, **kw):
        ApiController._handle_register(self, *a, **kw)
        c.render_style = "html"
        response.content_type = "text/html"

        if c.errors:
            return LoginPage(user_reg = request.POST.get('user'),
                             dest = dest).render()

        return self.redirect(dest)

    def GET_login(self, *a, **kw):
        return self.redirect('/login' + query_string(dict(dest="/")))

    @validatedForm(
        VUser(),
        VModhash(),
        personalized=VBoolean('pers', default=False),
        discovery=VBoolean('disc', default=False),
        rising=VBoolean('ris', default=False),
        nsfw=VBoolean('nsfw', default=False),
    )
    def POST_explore_settings(self,
                              form,
                              jquery,
                              personalized,
                              discovery,
                              rising,
                              nsfw):
        ExploreSettings.record_settings(
            c.user,
            personalized=personalized,
            discovery=discovery,
            rising=rising,
            nsfw=nsfw,
        )
        return redirect_to(controller='front', action='explore')

########NEW FILE########
__FILENAME__ = promotecontroller
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
from datetime import datetime, timedelta

from babel.dates import format_date
from babel.numbers import format_number
import json
import urllib

from pylons import c, g, request
from pylons.i18n import _, N_

from r2.controllers.api import ApiController
from r2.controllers.listingcontroller import ListingController
from r2.controllers.reddit_base import RedditController

from r2.lib import inventory, promote
from r2.lib.authorize import get_account_info, edit_profile, PROFILE_LIMIT
from r2.lib.base import abort
from r2.lib.db import queries
from r2.lib.errors import errors
from r2.lib.filters import websafe
from r2.lib.media import force_thumbnail, thumbnail_url
from r2.lib.memoize import memoize
from r2.lib.menus import NamedButton, NavButton, NavMenu
from r2.lib.pages import (
    LinkInfoPage,
    PaymentForm,
    PromoteInventory,
    PromotePage,
    PromoteLinkForm,
    PromoteLinkNew,
    PromoteReport,
    Reddit,
    RefundPage,
    RenderableCampaign,
    Roadblocks,
    UploadedImage,
)
from r2.lib.pages.things import wrap_links
from r2.lib.system_messages import user_added_messages
from r2.lib.utils import make_offset_date, to_date, to36
from r2.lib.validator import (
    json_validate,
    nop,
    noresponse,
    VAccountByName,
    ValidAddress,
    validate,
    validatedForm,
    ValidCard,
    ValidIP,
    VBoolean,
    VByName,
    VDate,
    VDateRange,
    VExistingUname,
    VFloat,
    VImageType,
    VInt,
    VLength,
    VLink,
    VLocation,
    VModhash,
    VOneOf,
    VPriority,
    VPromoCampaign,
    VRatelimit,
    VSelfText,
    VShamedDomain,
    VSponsor,
    VSponsorAdmin,
    VSponsorAdminOrAdminSecret,
    VSubmitSR,
    VTitle,
    VUploadLength,
    VUrl,
)
from r2.models import (
    Account,
    calc_impressions,
    Frontpage,
    get_promote_srid,
    Link,
    Message,
    NotFound,
    PromoCampaign,
    PromotionLog,
    PromotionWeights,
    PromotedLinkRoadblock,
    Subreddit,
)


def campaign_has_oversold_error(form, campaign):
    if campaign.priority.inventory_override:
        return

    target = Subreddit._by_name(campaign.sr_name) if campaign.sr_name else None
    return has_oversold_error(form, campaign, campaign.start_date,
                              campaign.end_date, campaign.bid, campaign.cpm,
                              target, campaign.location)


def has_oversold_error(form, campaign, start, end, bid, cpm, target, location):
    ndays = (to_date(end) - to_date(start)).days
    total_request = calc_impressions(bid, cpm)
    daily_request = int(total_request / ndays)
    oversold = inventory.get_oversold(target or Frontpage, start, end,
                                      daily_request, ignore=campaign,
                                      location=location)

    if oversold:
        min_daily = min(oversold.values())
        available = min_daily * ndays
        msg_params = {
            'available': format_number(available, locale=c.locale),
            'target': target.name if target else 'the frontpage',
            'start': start.strftime('%m/%d/%Y'),
            'end': end.strftime('%m/%d/%Y'),
        }
        c.errors.add(errors.OVERSOLD_DETAIL, field='bid',
                     msg_params=msg_params)
        form.has_errors('bid', errors.OVERSOLD_DETAIL)
        return True


class PromoteController(RedditController):
    @validate(VSponsor())
    def GET_new_promo(self):
        return PromotePage(title=_("create sponsored link"),
                           content=PromoteLinkNew()).render()

    @validate(VSponsor('link'),
              link=VLink('link'))
    def GET_edit_promo(self, link):
        if not link or link.promoted is None:
            return self.abort404()
        rendered = wrap_links(link, skip=False)
        form = PromoteLinkForm(link, rendered)
        page = Reddit(title=_("edit sponsored link"), content=form,
                      show_sidebar=False, extension_handling=False)
        return page.render()

    # admin only because the route might change
    @validate(VSponsorAdmin('campaign'),
              campaign=VPromoCampaign('campaign'))
    def GET_edit_promo_campaign(self, campaign):
        if not campaign:
            return self.abort404()
        link = Link._byID(campaign.link_id)
        return self.redirect(promote.promo_edit_url(link))

    @validate(VSponsorAdmin(),
              link=VLink("link"),
              campaign=VPromoCampaign("campaign"))
    def GET_refund(self, link, campaign):
        if link._id != campaign.link_id:
            return self.abort404()

        content = RefundPage(link, campaign)
        return Reddit("refund", content=content, show_sidebar=False).render()

    @validate(VSponsorAdmin())
    def GET_roadblock(self):
        return PromotePage(title=_("manage roadblocks"),
                           content=Roadblocks()).render()

    @validate(VSponsor("link"),
              link=VLink("link"),
              campaign=VPromoCampaign("campaign"))
    def GET_pay(self, link, campaign):
        if link._id != campaign.link_id:
            return self.abort404()

        # no need for admins to play in the credit card area
        if c.user_is_loggedin and c.user._id != link.author_id:
            return self.abort404()

        if g.authorizenetapi:
            data = get_account_info(c.user)
            content = PaymentForm(link, campaign,
                                  customer_id=data.customerProfileId,
                                  profiles=data.paymentProfiles,
                                  max_profiles=PROFILE_LIMIT)
        else:
            content = None
        res = LinkInfoPage(link=link,
                            content=content,
                            show_sidebar=False)
        return res.render()

    @validate(VSponsorAdminOrAdminSecret('secret'),
              start=VDate('startdate'),
              end=VDate('enddate'),
              link_text=nop('link_text'),
              owner=VAccountByName('owner'))
    def GET_report(self, start, end, link_text=None, owner=None):
        now = datetime.now(g.tz).replace(hour=0, minute=0, second=0,
                                         microsecond=0)
        end = end or now - timedelta(days=1)
        start = start or end - timedelta(days=7)

        links = []
        bad_links = []
        owner_name = owner.name if owner else ''

        if owner:
            promo_weights = PromotionWeights.get_campaigns(start, end,
                                                           author_id=owner._id)
            campaign_ids = [pw.promo_idx for pw in promo_weights]
            campaigns = PromoCampaign._byID(campaign_ids, data=True)
            link_ids = {camp.link_id for camp in campaigns.itervalues()}
            links.extend(Link._byID(link_ids, data=True, return_dict=False))

        if link_text is not None:
            id36s = link_text.replace(',', ' ').split()
            try:
                links_from_text = Link._byID36(id36s, data=True)
            except NotFound:
                links_from_text = {}

            bad_links = [id36 for id36 in id36s if id36 not in links_from_text]
            links.extend(links_from_text.values())

        content = PromoteReport(links, link_text, owner_name, bad_links, start,
                                end)
        if c.render_style == 'csv':
            return content.as_csv()
        else:
            return PromotePage(title=_("sponsored link report"),
                               content=content).render()

    @validate(
        VSponsorAdmin(),
        start=VDate('startdate', reference_date=promote.promo_datetime_now),
        end=VDate('enddate', reference_date=promote.promo_datetime_now),
        sr_name=nop('sr_name'),
    )
    def GET_promote_inventory(self, start, end, sr_name):
        if not start or not end:
            start = promote.promo_datetime_now(offset=1).date()
            end = promote.promo_datetime_now(offset=8).date()
            c.errors.remove((errors.BAD_DATE, 'startdate'))
            c.errors.remove((errors.BAD_DATE, 'enddate'))

        sr = Frontpage
        if sr_name:
            try:
                sr = Subreddit._by_name(sr_name)
            except NotFound:
                c.errors.add(errors.SUBREDDIT_NOEXIST, field='sr_name')

        content = PromoteInventory(start, end, sr)
        return PromotePage(title=_("sponsored link inventory"),
                           content=content).render()


class PromoteListingController(ListingController):
    where = 'promoted'
    render_cls = PromotePage
    titles = {
        'future_promos': N_('unapproved promoted links'),
        'pending_promos': N_('accepted promoted links'),
        'unpaid_promos': N_('unpaid promoted links'),
        'rejected_promos': N_('rejected promoted links'),
        'live_promos': N_('live promoted links'),
        'underdelivered': N_('underdelivered promoted links'),
        'reported': N_('reported promoted links'),
        'house': N_('house promoted links'),
        'all': N_('all promoted links'),
    }

    def title(self):
        return _(self.titles[self.sort])

    @property
    def title_text(self):
        return _('promoted by you')

    @classmethod
    @memoize('live_by_subreddit', time=300)
    def _live_by_subreddit(cls, sr_names):
        promotuples = promote.get_live_promotions(sr_names)
        return [pt.link for pt in promotuples]

    def live_by_subreddit(cls, sr):
        sr_names = [''] if sr == Frontpage else [sr.name]
        return cls._live_by_subreddit(sr_names)

    @classmethod
    @memoize('house_link_names', time=60)
    def get_house_link_names(cls):
        now = promote.promo_datetime_now()
        pws = PromotionWeights.get_campaigns(now)
        campaign_ids = {pw.promo_idx for pw in pws}
        q = PromoCampaign._query(PromoCampaign.c._id.in_(campaign_ids),
                                 PromoCampaign.c.priority_name == 'house',
                                 data=True)
        link_names = {Link._fullname_from_id36(to36(camp.link_id))
                      for camp in q}
        return sorted(link_names, reverse=True)

    @property
    def menus(self):
        filters = [
            NamedButton('all_promos', dest=''),
            NamedButton('future_promos'),
            NamedButton('unpaid_promos'),
            NamedButton('rejected_promos'),
            NamedButton('pending_promos'),
            NamedButton('live_promos'),
        ]
        menus = [NavMenu(filters, base_path='/promoted', title='show',
                        type='lightdrop')]

        if self.sort == 'live_promos' and c.user_is_sponsor:
            srnames = promote.all_live_promo_srnames()
            buttons = [NavButton('all', '')]
            try:
                srnames.remove('')
                frontbutton = NavButton('FRONTPAGE', Frontpage.name,
                                        aliases=['/promoted/live_promos/%s' %
                                                 urllib.quote(Frontpage.name)])
                buttons.append(frontbutton)
            except KeyError:
                pass

            srnames = sorted(srnames, key=lambda name: name.lower())
            buttons.extend([NavButton(name, name) for name in srnames])
            menus.append(NavMenu(buttons, base_path='/promoted/live_promos',
                                 title='subreddit', type='lightdrop'))

        return menus

    def keep_fn(self):
        def keep(item):
            if self.sort == "future_promos":
                # this sort is used to review links that need to be approved
                # skip links that don't have any paid campaigns
                campaigns = list(PromoCampaign._by_link(item._id))
                if not any(promote.authed_or_not_needed(camp)
                           for camp in campaigns):
                    return False

            if item.promoted and not item._deleted:
                return True
            else:
                return False
        return keep

    def query(self):
        if c.user_is_sponsor:
            if self.sort == "future_promos":
                return queries.get_all_unapproved_links()
            elif self.sort == "pending_promos":
                return queries.get_all_accepted_links()
            elif self.sort == "unpaid_promos":
                return queries.get_all_unpaid_links()
            elif self.sort == "rejected_promos":
                return queries.get_all_rejected_links()
            elif self.sort == "live_promos" and self.sr:
                return self.live_by_subreddit(self.sr)
            elif self.sort == 'live_promos':
                return queries.get_all_live_links()
            elif self.sort == 'underdelivered':
                q = queries.get_underdelivered_campaigns()
                campaigns = PromoCampaign._by_fullname(list(q), data=True,
                                                       return_dict=False)
                link_ids = [camp.link_id for camp in campaigns]
                return [Link._fullname_from_id36(to36(id)) for id in link_ids]
            elif self.sort == 'reported':
                return queries.get_reported_links(get_promote_srid())
            elif self.sort == 'house':
                return self.get_house_link_names()
            elif self.sort == 'all':
                return queries.get_all_promoted_links()
        else:
            if self.sort == "future_promos":
                return queries.get_unapproved_links(c.user._id)
            elif self.sort == "pending_promos":
                return queries.get_accepted_links(c.user._id)
            elif self.sort == "unpaid_promos":
                return queries.get_unpaid_links(c.user._id)
            elif self.sort == "rejected_promos":
                return queries.get_rejected_links(c.user._id)
            elif self.sort == "live_promos":
                return queries.get_live_links(c.user._id)
            elif self.sort == "all":
                return queries.get_promoted_links(c.user._id)

    @validate(VSponsor(),
              sr=nop('sr'))
    def GET_listing(self, sr=None, sort="all", **env):
        if not c.user_is_loggedin or not c.user.email_verified:
            # never reached--see MinimalController.on_validation_error
            return self.redirect("/ad_inq")

        if (sort in ('underdelivered', 'reported', 'house') and
            not c.user_is_sponsor):
            self.abort403()

        self.sort = sort
        self.sr = None
        if sr and sr == Frontpage.name:
            self.sr = Frontpage
        elif sr:
            try:
                self.sr = Subreddit._by_name(sr)
            except NotFound:
                pass
        return ListingController.GET_listing(self, **env)


class PromoteApiController(ApiController):
    @json_validate(sr=VSubmitSR('sr', promotion=True),
                   location=VLocation(),
                   start=VDate('startdate'),
                   end=VDate('enddate'))
    def GET_check_inventory(self, responder, sr, location, start, end):
        sr = sr or Frontpage
        if not location or not location.country:
            available = inventory.get_available_pageviews(sr, start, end,
                                                          datestr=True)
        elif sr == Frontpage or c.user_is_sponsor:
            available = inventory.get_available_pageviews_geotargeted(sr,
                            location, start, end, datestr=True)
        else:
            return abort(403, 'forbidden')
        return {'inventory': available}

    @validatedForm(VSponsorAdmin(),
                   VModhash(),
                   link=VLink("link_id36"),
                   campaign=VPromoCampaign("campaign_id36"))
    def POST_freebie(self, form, jquery, link, campaign):
        if not link or not campaign or link._id != campaign.link_id:
            return abort(404, 'not found')

        if campaign_has_oversold_error(form, campaign):
            form.set_html(".freebie", "target oversold, can't freebie")
            return

        if promote.is_promo(link) and campaign:
            promote.free_campaign(link, campaign, c.user)
            form.redirect(promote.promo_edit_url(link))

    @validatedForm(VSponsorAdmin(),
                   VModhash(),
                   link=VByName("link"),
                   note=nop("note"))
    def POST_promote_note(self, form, jquery, link, note):
        if promote.is_promo(link):
            text = PromotionLog.add(link, note)
            form.find(".notes").children(":last").after(
                "<p>" + websafe(text) + "</p>")


    @noresponse(VSponsorAdmin(),
                VModhash(),
                thing=VByName('id'))
    def POST_promote(self, thing):
        if promote.is_promo(thing):
            promote.accept_promotion(thing)

    @noresponse(VSponsorAdmin(),
                VModhash(),
                thing=VByName('id'),
                reason=nop("reason"))
    def POST_unpromote(self, thing, reason):
        if promote.is_promo(thing):
            promote.reject_promotion(thing, reason=reason)

    @validatedForm(VSponsorAdmin(),
                   VModhash(),
                   link=VLink('link'),
                   campaign=VPromoCampaign('campaign'))
    def POST_refund_campaign(self, form, jquery, link, campaign):
        if not link or not campaign or link._id != campaign.link_id:
            return abort(404, 'not found')

        billable_impressions = promote.get_billable_impressions(campaign)
        billable_amount = promote.get_billable_amount(campaign,
                                                      billable_impressions)
        refund_amount = promote.get_refund_amount(campaign, billable_amount)
        if refund_amount > 0:
            promote.refund_campaign(link, campaign, billable_amount,
                                    billable_impressions)
            form.set_html('.status', _('refund succeeded'))
        else:
            form.set_html('.status', _('refund not needed'))

    @validatedForm(VSponsor('link_id36'),
                   VModhash(),
                   VRatelimit(rate_user=True,
                              rate_ip=True,
                              prefix='create_promo_'),
                   VShamedDomain('url'),
                   username=VLength('username', 100, empty_error=None),
                   l=VLink('link_id36'),
                   title=VTitle('title'),
                   url=VUrl('url', allow_self=False),
                   selftext=VSelfText('text'),
                   kind=VOneOf('kind', ['link', 'self']),
                   ip=ValidIP(),
                   disable_comments=VBoolean("disable_comments"),
                   sendreplies=VBoolean("sendreplies"),
                   media_width=VInt("media-width", min=0),
                   media_height=VInt("media-height", min=0),
                   media_embed=VLength("media-embed", 1000),
                   media_override=VBoolean("media-override"),
                   domain_override=VLength("domain", 100)
                   )
    def POST_edit_promo(self, form, jquery, ip, username, l, title, url,
                        selftext, kind, disable_comments, sendreplies, media_height,
                        media_width, media_embed, media_override, domain_override):

        should_ratelimit = False
        if not c.user_is_sponsor:
            should_ratelimit = True

        if not should_ratelimit:
            c.errors.remove((errors.RATELIMIT, 'ratelimit'))

        # check for user override
        if not l and c.user_is_sponsor and username:
            try:
                user = Account._by_name(username)
            except NotFound:
                c.errors.add(errors.USER_DOESNT_EXIST, field="username")
                form.set_error(errors.USER_DOESNT_EXIST, "username")
                return

            if not user.email:
                c.errors.add(errors.NO_EMAIL_FOR_USER, field="username")
                form.set_error(errors.NO_EMAIL_FOR_USER, "username")
                return

            if not user.email_verified:
                c.errors.add(errors.NO_VERIFIED_EMAIL, field="username")
                form.set_error(errors.NO_VERIFIED_EMAIL, "username")
                return
        else:
            user = c.user

        # check for shame banned domains
        if form.has_errors("url", errors.DOMAIN_BANNED):
            g.stats.simple_event('spam.shame.link')
            return

        # demangle URL in canonical way
        if url:
            if isinstance(url, (unicode, str)):
                form.set_inputs(url=url)
            elif isinstance(url, tuple) or isinstance(url[0], Link):
                # there's already one or more links with this URL, but
                # we're allowing mutliple submissions, so we really just
                # want the URL
                url = url[0].url

        if kind == 'link':
            if form.has_errors('url', errors.NO_URL, errors.BAD_URL):
                return

        # users can change the disable_comments on promoted links
        if ((not l or not promote.is_promoted(l)) and
            (form.has_errors('title', errors.NO_TEXT, errors.TOO_LONG) or
             jquery.has_errors('ratelimit', errors.RATELIMIT))):
            return

        if not l:
            l = promote.new_promotion(title, url if kind == 'link' else 'self',
                                      selftext if kind == 'self' else '',
                                      user, ip)

        elif promote.is_promo(l):
            # changing link type is not allowed
            if ((l.is_self and kind == 'link') or
                (not l.is_self and kind == 'self')):
                c.errors.add(errors.NO_CHANGE_KIND, field="kind")
                form.set_error(errors.NO_CHANGE_KIND, "kind")
                return

            changed = False
            # live items can only be changed by a sponsor, and also
            # pay the cost of de-approving the link
            trusted = c.user_is_sponsor or c.user.trusted_sponsor
            if not promote.is_promoted(l) or trusted:
                if title and title != l.title:
                    l.title = title
                    changed = not trusted

                if kind == 'link' and url and url != l.url:
                    l.url = url
                    changed = not trusted

            # only trips if the title and url are changed by a non-sponsor
            if changed:
                promote.unapprove_promotion(l)

            # selftext can be changed at any time
            if kind == 'self':
                l.selftext = selftext

            # comment disabling and sendreplies is free to be changed any time.
            l.disable_comments = disable_comments
            l.sendreplies = sendreplies
            if c.user_is_sponsor or c.user.trusted_sponsor:
                if media_embed and media_width and media_height:
                    l.media_object = dict(height=media_height,
                                          width=media_width,
                                          content=media_embed,
                                          type='custom')
                else:
                    l.media_object = None

                l.media_override = media_override
                if getattr(l, "domain_override", False) or domain_override:
                    l.domain_override = domain_override
            l._commit()

        form.redirect(promote.promo_edit_url(l))

    @validatedForm(VSponsorAdmin(),
                   VModhash(),
                   dates=VDateRange(['startdate', 'enddate'],
                                    reference_date=promote.promo_datetime_now),
                   sr=VSubmitSR('sr', promotion=True))
    def POST_add_roadblock(self, form, jquery, dates, sr):
        if (form.has_errors('startdate', errors.BAD_DATE) or
            form.has_errors('enddate', errors.BAD_DATE, errors.BAD_DATE_RANGE)):
            return
        if form.has_errors('sr', errors.SUBREDDIT_NOEXIST,
                           errors.SUBREDDIT_NOTALLOWED,
                           errors.SUBREDDIT_REQUIRED):
            return
        if dates and sr:
            sd, ed = dates
            PromotedLinkRoadblock.add(sr, sd, ed)
            jquery.refresh()

    @validatedForm(VSponsorAdmin(),
                   VModhash(),
                   dates=VDateRange(['startdate', 'enddate'],
                                    reference_date=promote.promo_datetime_now),
                   sr=VSubmitSR('sr', promotion=True))
    def POST_rm_roadblock(self, form, jquery, dates, sr):
        if dates and sr:
            sd, ed = dates
            PromotedLinkRoadblock.remove(sr, sd, ed)
            jquery.refresh()

    @validatedForm(VSponsor('link_id36'),
                   VModhash(),
                   dates=VDateRange(['startdate', 'enddate'],
                       earliest=timedelta(days=g.min_promote_future),
                       latest=timedelta(days=g.max_promote_future),
                       reference_date=promote.promo_datetime_now,
                       business_days=True,
                       sponsor_override=True),
                   link=VLink('link_id36'),
                   bid=VFloat('bid', coerce=False),
                   sr=VSubmitSR('sr', promotion=True),
                   campaign_id36=nop("campaign_id36"),
                   targeting=VLength("targeting", 10),
                   priority=VPriority("priority"),
                   location=VLocation())
    def POST_edit_campaign(self, form, jquery, link, campaign_id36,
                          dates, bid, sr, targeting, priority, location):
        if not link:
            return

        start, end = dates or (None, None)

        if location and sr and not c.user_is_sponsor:
            # only sponsors can geotarget on subreddits
            location = None

        if location and location.metro:
            cpm = g.cpm_selfserve_geotarget_metro.pennies
        elif location:
            cpm = g.cpm_selfserve_geotarget_country.pennies
        else:
            author = Account._byID(link.author_id, data=True)
            cpm = author.cpm_selfserve_pennies

        if (form.has_errors('startdate', errors.BAD_DATE,
                            errors.DATE_TOO_EARLY, errors.DATE_TOO_LATE) or
            form.has_errors('enddate', errors.BAD_DATE, errors.DATE_TOO_EARLY,
                            errors.DATE_TOO_LATE, errors.BAD_DATE_RANGE)):
            return

        # check that start is not so late that authorization hold will expire
        if not c.user_is_sponsor:
            max_start = promote.get_max_startdate()
            if start > max_start:
                c.errors.add(errors.DATE_TOO_LATE,
                             msg_params={'day': max_start.strftime("%m/%d/%Y")},
                             field='startdate')
                form.has_errors('startdate', errors.DATE_TOO_LATE)
                return

        # Limit the number of PromoCampaigns a Link can have
        # Note that the front end should prevent the user from getting
        # this far
        existing_campaigns = list(PromoCampaign._by_link(link._id))
        if len(existing_campaigns) > g.MAX_CAMPAIGNS_PER_LINK:
            c.errors.add(errors.TOO_MANY_CAMPAIGNS,
                         msg_params={'count': g.MAX_CAMPAIGNS_PER_LINK},
                         field='title')
            form.has_errors('title', errors.TOO_MANY_CAMPAIGNS)
            return

        campaign = None
        if campaign_id36:
            try:
                campaign = PromoCampaign._byID36(campaign_id36)
            except NotFound:
                pass

        if campaign and link._id != campaign.link_id:
            return abort(404, 'not found')

        if priority.cpm:
            min_bid = 0 if c.user_is_sponsor else g.min_promote_bid
            max_bid = None if c.user_is_sponsor else g.max_promote_bid

            if bid is None or bid < min_bid or (max_bid and bid > max_bid):
                c.errors.add(errors.BAD_BID, field='bid',
                             msg_params={'min': min_bid,
                                         'max': max_bid or g.max_promote_bid})
                form.has_errors('bid', errors.BAD_BID)
                return

            # you cannot edit the bid of a live ad unless it's a freebie
            if (campaign and bid != campaign.bid and
                promote.is_live_promo(link, campaign) and
                not campaign.is_freebie()):
                c.errors.add(errors.BID_LIVE, field='bid')
                form.has_errors('bid', errors.BID_LIVE)
                return

        else:
            bid = 0.   # Set bid to 0 as dummy value

        if targeting == 'one':
            if form.has_errors('sr', errors.SUBREDDIT_NOEXIST,
                               errors.SUBREDDIT_NOTALLOWED,
                               errors.SUBREDDIT_REQUIRED):
                # checking to get the error set in the form, but we can't
                # check for rate-limiting if there's no subreddit
                return

            roadblock = PromotedLinkRoadblock.is_roadblocked(sr, start, end)
            if roadblock and not c.user_is_sponsor:
                msg_params = {"start": roadblock[0].strftime('%m/%d/%Y'),
                              "end": roadblock[1].strftime('%m/%d/%Y')}
                c.errors.add(errors.OVERSOLD, field='sr',
                             msg_params=msg_params)
                form.has_errors('sr', errors.OVERSOLD)
                return

        elif targeting == 'none':
            sr = None

        # Check inventory
        campaign = campaign if campaign_id36 else None
        if not priority.inventory_override:
            oversold = has_oversold_error(form, campaign, start, end, bid, cpm,
                                          sr, location)
            if oversold:
                return

        if campaign:
            promote.edit_campaign(link, campaign, dates, bid, cpm, sr, priority,
                                  location)
        else:
            campaign = promote.new_campaign(link, dates, bid, cpm, sr, priority,
                                            location)
        rc = RenderableCampaign.from_campaigns(link, campaign)
        jquery.update_campaign(campaign._fullname, rc.render_html())

    @validatedForm(VSponsor('link_id36'),
                   VModhash(),
                   l=VLink('link_id36'),
                   campaign=VPromoCampaign("campaign_id36"))
    def POST_delete_campaign(self, form, jquery, l, campaign):
        if not campaign or not l or l._id != campaign.link_id:
            return abort(404, 'not found')

        promote.delete_campaign(l, campaign)

    @validatedForm(VSponsorAdmin(),
                   VModhash(),
                   link=VLink('link_id36'),
                   campaign=VPromoCampaign("campaign_id36"))
    def POST_terminate_campaign(self, form, jquery, link, campaign):
        if not link or not campaign or link._id != campaign.link_id:
            return abort(404, 'not found')

        promote.terminate_campaign(link, campaign)
        rc = RenderableCampaign.from_campaigns(link, campaign)
        jquery.update_campaign(campaign._fullname, rc.render_html())

    @validatedForm(VSponsor('link'),
                   VModhash(),
                   link=VByName("link"),
                   campaign=VPromoCampaign("campaign"),
                   customer_id=VInt("customer_id", min=0),
                   pay_id=VInt("account", min=0),
                   edit=VBoolean("edit"),
                   address=ValidAddress(
                    ["firstName", "lastName", "company", "address",
                     "city", "state", "zip", "country", "phoneNumber"]),
                   creditcard=ValidCard(["cardNumber", "expirationDate",
                                           "cardCode"]))
    def POST_update_pay(self, form, jquery, link, campaign, customer_id, pay_id,
                        edit, address, creditcard):
        if not g.authorizenetapi:
            return

        if not link or not campaign or link._id != campaign.link_id:
            return abort(404, 'not found')

        # Check inventory
        if campaign_has_oversold_error(form, campaign):
            return

        # check that start is not so late that authorization hold will expire
        max_start = promote.get_max_startdate()
        if campaign.start_date > max_start:
            msg = _("please change campaign start date to %(date)s or earlier")
            date = format_date(max_start, format="short", locale=c.locale)
            msg %= {'date': date}
            form.set_html(".status", msg)
            return

        address_modified = not pay_id or edit
        if address_modified:
            address_fields = ["firstName", "lastName", "company", "address",
                              "city", "state", "zip", "country", "phoneNumber"]
            card_fields = ["cardNumber", "expirationDate", "cardCode"]

            if (form.has_errors(address_fields, errors.BAD_ADDRESS) or
                    form.has_errors(card_fields, errors.BAD_CARD)):
                return

            pay_id = edit_profile(c.user, address, creditcard, pay_id)

        reason = None
        if pay_id:
            success, reason = promote.auth_campaign(link, campaign, c.user,
                                                    pay_id)

            if success:
                form.redirect(promote.promo_edit_url(link))
                return

        msg = reason or _("failed to authenticate card. sorry.")
        form.set_html(".status", msg)

    @validate(VSponsor("link_name"),
              VModhash(),
              link=VByName('link_name'),
              file=VUploadLength('file', 500*1024),
              img_type=VImageType('img_type'))
    def POST_link_thumb(self, link=None, file=None, img_type='jpg'):
        if link and (not promote.is_promoted(link) or
                     c.user_is_sponsor or c.user.trusted_sponsor):
            errors = dict(BAD_CSS_NAME="", IMAGE_ERROR="")

            # thumnails for promoted links can change and therefore expire
            force_thumbnail(link, file, file_type=".%s" % img_type)

            if any(errors.values()):
                return UploadedImage("", "", "upload", errors=errors,
                                     form_id="image-upload").render()
            else:
                link._commit()
                return UploadedImage(_('saved'), thumbnail_url(link), "",
                                     errors=errors,
                                     form_id="image-upload").render()

########NEW FILE########
__FILENAME__ = reddit_base
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import collections
import json
import re
import simplejson
import socket
import time

from Cookie import CookieError
from copy import copy
from datetime import datetime, timedelta
from functools import wraps
from hashlib import sha1
from urllib import quote, unquote
from urlparse import urlparse

import babel.core
import pylibmc

from mako.filters import url_escape
from pylons import c, g, request, response
from pylons.i18n import _
from pylons.i18n.translation import LanguageError

from r2.config.extensions import is_api, set_extension
from r2.lib import filters, pages, utils, hooks
from r2.lib.authentication import authenticate_user
from r2.lib.base import BaseController, abort
from r2.lib.cache import make_key, MemcachedError
from r2.lib.errors import (
    ErrorSet,
    BadRequestError,
    ForbiddenError,
    errors,
    reddit_http_error,
)
from r2.lib.filters import _force_utf8, _force_unicode
from r2.lib.require import RequirementException, require, require_split
from r2.lib.strings import strings
from r2.lib.template_helpers import add_sr, JSPreload
from r2.lib.tracking import encrypt, decrypt
from r2.lib.translation import set_lang
from r2.lib.utils import (
    Enum,
    SimpleSillyStub,
    UniqueIterator,
    http_utils,
    is_subdomain,
    is_throttled,
    tup,
)
from r2.lib.validator import (
    build_arg_list,
    chksrname,
    fullname_regex,
    valid_jsonp_callback,
    validate,
    VByName,
    VCount,
    VLang,
    VLength,
    VLimit,
    VTarget,
)
from r2.models import (
    Account,
    All,
    AllMinus,
    DefaultSR,
    DomainSR,
    FakeAccount,
    FakeSubreddit,
    Friends,
    Frontpage,
    LabeledMulti,
    Link,
    Mod,
    ModMinus,
    MultiReddit,
    NotFound,
    OAuth2AccessToken,
    OAuth2Client,
    OAuth2Scope,
    Random,
    RandomNSFW,
    RandomSubscription,
    Sub,
    Subreddit,
    valid_admin_cookie,
    valid_feed,
    valid_otp_cookie,
)
from r2.lib.db import tdb_cassandra


NEVER = datetime(2037, 12, 31, 23, 59, 59)
DELETE = datetime(1970, 01, 01, 0, 0, 1)
PAGECACHE_POLICY = Enum(
    # logged in users may use the pagecache as well.
    "LOGGEDIN_AND_LOGGEDOUT",
    # only attempt to use pagecache if the current user is not logged in.
    "LOGGEDOUT_ONLY",
    # do not use pagecache.
    "NEVER",
)


def pagecache_policy(policy):
    """Decorate a controller method to specify desired pagecache behaviour.

    If not specified, the policy will default to LOGGEDOUT_ONLY.

    """

    assert policy in PAGECACHE_POLICY

    def pagecache_decorator(fn):
        fn.pagecache_policy = policy
        return fn
    return pagecache_decorator


cache_affecting_cookies = ('over18', '_options')

class Cookies(dict):
    def add(self, name, value, *k, **kw):
        name = name.encode('utf-8')
        self[name] = Cookie(value, *k, **kw)

class Cookie(object):
    def __init__(self, value, expires=None, domain=None,
                 dirty=True, secure=False, httponly=False):
        self.value = value
        self.expires = expires
        self.dirty = dirty
        self.secure = secure
        self.httponly = httponly
        if domain:
            self.domain = domain
        elif c.authorized_cname and not c.default_sr:
            self.domain = utils.common_subdomain(request.host, c.site.domain)
        else:
            self.domain = g.domain

    @staticmethod
    def classify(cookie_name):
        if cookie_name == g.login_cookie:
            return "session"
        elif cookie_name == g.admin_cookie:
            return "admin"
        elif cookie_name == "reddit_first":
            return "first"
        elif cookie_name == "over18":
            return "over18"
        elif cookie_name.endswith("_last_thing"):
            return "last_thing"
        elif cookie_name.endswith("_options"):
            return "options"
        elif cookie_name.endswith("_recentclicks2"):
            return "clicks"
        elif cookie_name.startswith("__utm"):
            return "ga"
        elif cookie_name.startswith("beta_"):
            return "beta"
        else:
            return "other"

    def __repr__(self):
        return ("Cookie(value=%r, expires=%r, domain=%r, dirty=%r)"
                % (self.value, self.expires, self.domain, self.dirty))

class UnloggedUser(FakeAccount):
    COOKIE_NAME = "_options"
    allowed_prefs = {
        "pref_lang": VLang.validate_lang,
        "pref_content_langs": VLang.validate_content_langs,
        "pref_frame_commentspanel": bool,
    }

    def __init__(self, browser_langs, *a, **kw):
        FakeAccount.__init__(self, *a, **kw)
        if browser_langs:
            lang = browser_langs[0]
            content_langs = list(browser_langs)
            # try to coerce the default language 
            if g.lang not in content_langs:
                content_langs.append(g.lang)
            content_langs.sort()
        else:
            lang = g.lang
            content_langs = 'all'
        self._defaults = self._defaults.copy()
        self._defaults['pref_lang'] = lang
        self._defaults['pref_content_langs'] = content_langs
        self._defaults['pref_frame_commentspanel'] = False
        self._load()

    @property
    def name(self):
        raise NotImplementedError

    def _decode_json(self, json_blob):
        data = json.loads(json_blob)
        validated = {}
        for k, v in data.iteritems():
            validator = self.allowed_prefs.get(k)
            if validator:
                try:
                    validated[k] = validator(v)
                except ValueError:
                    pass  # don't override defaults for bad data
        return validated

    def _from_cookie(self):
        cookie = c.cookies.get(self.COOKIE_NAME)
        if not cookie:
            return {}

        try:
            return self._decode_json(cookie.value)
        except ValueError:
            # old-style _options cookies are encrypted
            try:
                plaintext = decrypt(cookie.value)
                values = self._decode_json(plaintext)
            except (TypeError, ValueError):
                # this cookie is totally invalid, delete it
                c.cookies[self.COOKIE_NAME] = Cookie(value="", expires=DELETE)
                return {}
            else:
                self._to_cookie(values)  # upgrade the cookie
                return values

    def _to_cookie(self, data):
        allowed_data = {k: v for k, v in data.iteritems()
                        if k in self.allowed_prefs}
        jsonified = json.dumps(allowed_data, sort_keys=True)
        c.cookies[self.COOKIE_NAME] = Cookie(value=jsonified)

    def _subscribe(self, sr):
        pass

    def _unsubscribe(self, sr):
        pass

    def valid_hash(self, hash):
        return False

    def _commit(self):
        if self._dirty:
            for k, (oldv, newv) in self._dirties.iteritems():
                self._t[k] = newv
            self._to_cookie(self._t)

    def _load(self):
        self._t.update(self._from_cookie())
        self._loaded = True

def read_user_cookie(name):
    uname = c.user.name if c.user_is_loggedin else ""
    cookie_name = uname + '_' + name
    if cookie_name in c.cookies:
        return c.cookies[cookie_name].value
    else:
        return ''

def set_user_cookie(name, val, **kwargs):
    uname = c.user.name if c.user_is_loggedin else ""
    c.cookies[uname + '_' + name] = Cookie(value=val,
                                           **kwargs)


valid_click_cookie = fullname_regex(Link, True).match
def set_recent_clicks():
    c.recent_clicks = []
    if not c.user_is_loggedin:
        return

    click_cookie = read_user_cookie('recentclicks2')
    if click_cookie:
        if valid_click_cookie(click_cookie):
            names = [ x for x in UniqueIterator(click_cookie.split(',')) if x ]

            if len(names) > 5:
                names = names[:5]
                set_user_cookie('recentclicks2', ','.join(names))
            #eventually this will look at the user preference
            names = names[:5]

            try:
                c.recent_clicks = Link._by_fullname(names, data=True,
                                                    return_dict=False)
            except NotFound:
                # clear their cookie because it's got bad links in it
                set_user_cookie('recentclicks2', '')
        else:
            #if the cookie wasn't valid, clear it
            set_user_cookie('recentclicks2', '')

def delete_obsolete_cookies():
    for cookie_name in c.cookies:
        if cookie_name.endswith(("_last_thing", "_mod")):
            c.cookies[cookie_name] = Cookie("", expires=DELETE)

def over18():
    if c.user_is_loggedin:
        return c.user.pref_over_18 or c.user_is_admin
    else:
        if 'over18' in c.cookies:
            cookie = c.cookies['over18'].value
            if cookie == "1":
                return True
            else:
                c.cookies["over18"] = Cookie(value="", expires=DELETE)

def set_obey_over18():
    "querystring parameter for API to obey over18 filtering rules"
    c.obey_over18 = request.GET.get("obey_over18") == "true"

valid_ascii_domain = re.compile(r'\A(\w[-\w]*\.)+[\w]+\Z')
def set_subreddit():
    #the r parameter gets added by javascript for POST requests so we
    #can reference c.site in api.py
    sr_name = request.environ.get("subreddit", request.POST.get('r'))
    domain = request.environ.get("domain")

    can_stale = request.method.upper() in ('GET', 'HEAD')

    c.site = Frontpage
    if not sr_name:
        #check for cnames
        cname = request.environ.get('legacy-cname')
        if cname:
            sr = Subreddit._by_domain(cname) or Frontpage
            domain = g.domain
            if g.domain_prefix:
                domain = ".".join((g.domain_prefix, domain))
            path = 'http://%s%s' % (domain, sr.path)
            abort(301, location=BaseController.format_output_url(path))
    elif sr_name == 'r':
        #reddits
        c.site = Sub
    elif '+' in sr_name:
        sr_names = sr_name.split('+')
        srs = Subreddit._by_name(sr_names, stale=can_stale).values()
        if All in srs:
            c.site = All
        elif Friends in srs:
            c.site = Friends
        else:
            srs = [sr for sr in srs if not isinstance(sr, FakeSubreddit)]
            multi_path = '/r/' + sr_name
            if not srs:
                c.site = MultiReddit(multi_path, [])
            elif len(srs) == 1:
                c.site = srs[0]
            else:
                c.site = MultiReddit(multi_path, srs)
    elif '-' in sr_name:
        sr_names = sr_name.split('-')
        base_sr_name, exclude_sr_names = sr_names[0], sr_names[1:]
        srs = Subreddit._by_name(sr_names, stale=can_stale)
        base_sr = srs.pop(base_sr_name, None)
        exclude_srs = [sr for sr in srs.itervalues()
                          if not isinstance(sr, FakeSubreddit)]

        if base_sr == All:
            if exclude_srs:
                c.site = AllMinus(exclude_srs)
            else:
                c.site = All
        elif base_sr == Mod:
            if exclude_srs:
                c.site = ModMinus(exclude_srs)
            else:
                c.site = Mod
        else:
            path = "/subreddits/search?q=%s" % sr_name
            abort(302, location=BaseController.format_output_url(path))
    else:
        try:
            c.site = Subreddit._by_name(sr_name, stale=can_stale)
        except NotFound:
            sr_name = chksrname(sr_name)
            if sr_name:
                path = "/subreddits/search?q=%s" % sr_name
                abort(302, location=BaseController.format_output_url(path))
            elif not c.error_page and not request.path.startswith("/api/login/") :
                abort(404)

    #if we didn't find a subreddit, check for a domain listing
    if not sr_name and isinstance(c.site, DefaultSR) and domain:
        # Redirect IDN to their IDNA name if necessary
        try:
            idna = _force_unicode(domain).encode("idna")
            if idna != domain:
                path_info = request.environ["PATH_INFO"]
                path = "/domain/%s%s" % (idna, path_info)
                abort(302, location=BaseController.format_output_url(path))
        except UnicodeError:
            domain = ''  # Ensure valid_ascii_domain fails
        if not c.error_page and not valid_ascii_domain.match(domain):
            abort(404)
        c.site = DomainSR(domain)

    if isinstance(c.site, FakeSubreddit):
        c.default_sr = True


def set_multireddit():
    routes_dict = request.environ["pylons.routes_dict"]
    if "multipath" in routes_dict:
        multipath = routes_dict["multipath"].lower()
        multi_id = None

        if c.user_is_loggedin and routes_dict.get("my_multi"):
            multi_id = "/user/%s/m/%s" % (c.user.name.lower(), multipath)
        elif "username" in routes_dict:
            username = routes_dict["username"].lower()

            if c.user_is_loggedin:
                # redirect /user/foo/m/... to /me/m/... for user foo.
                if username == c.user.name.lower():
                    # trim off multi id
                    url_parts = request.path_qs.split("/")[5:]
                    url_parts.insert(0, "/me/m/%s" % multipath)
                    path = "/".join(url_parts)
                    abort(302, location=BaseController.format_output_url(path))

            multi_id = "/user/%s/m/%s" % (username, multipath)

        if multi_id:
            try:
                c.site = LabeledMulti._byID(multi_id)
            except tdb_cassandra.NotFound:
                abort(404)


def set_content_type():
    e = request.environ
    c.render_style = e['render_style']
    response.content_type = e['content_type']

    if e.has_key('extension'):
        c.extension = ext = e['extension']
        if ext in ('embed', 'widget'):
            wrapper = request.params.get("callback", "document.write")
            wrapper = filters._force_utf8(wrapper)
            if not valid_jsonp_callback(wrapper):
                abort(BadRequestError(errors.BAD_JSONP_CALLBACK))

            # force logged-out state since these can be accessed cross-domain
            c.user = UnloggedUser(get_browser_langs())
            c.user_is_loggedin = False

            def to_js(content):
                return wrapper + "(" + utils.string2js(content) + ");"

            c.response_wrapper = to_js
        if ext in ("rss", "api", "json") and request.method.upper() == "GET":
            user = valid_feed(request.GET.get("user"),
                              request.GET.get("feed"),
                              request.path)
            if user and not g.read_only_mode:
                c.user = user
                c.user_is_loggedin = True
        if ext in ("mobile", "m") and not request.GET.get("keep_extension"):
            try:
                if request.cookies['reddit_mobility'] == "compact":
                    c.extension = "compact"
                    c.render_style = "compact"
            except (ValueError, KeyError):
                c.suggest_compact = True
        if ext in ("mobile", "m", "compact"):
            if request.GET.get("keep_extension"):
                c.cookies['reddit_mobility'] = Cookie(ext, expires=NEVER)
    # allow JSONP requests to generate callbacks, but do not allow
    # the user to be logged in for these 
    callback = request.GET.get("jsonp")
    if is_api() and request.method.upper() == "GET" and callback:
        if not valid_jsonp_callback(callback):
            abort(BadRequestError(errors.BAD_JSONP_CALLBACK))
        c.allowed_callback = callback
        c.user = UnloggedUser(get_browser_langs())
        c.user_is_loggedin = False
        response.content_type = "application/javascript"

def get_browser_langs():
    browser_langs = []
    langs = request.environ.get('HTTP_ACCEPT_LANGUAGE')
    if langs:
        langs = langs.split(',')
        browser_langs = []
        seen_langs = set()
        # extract languages from browser string
        for l in langs:
            if ';' in l:
                l = l.split(';')[0]
            if l not in seen_langs and l in g.languages:
                browser_langs.append(l)
                seen_langs.add(l)
            if '-' in l:
                l = l.split('-')[0]
            if l not in seen_langs and l in g.languages:
                browser_langs.append(l)
                seen_langs.add(l)
    return browser_langs

def set_iface_lang():
    host_lang = request.environ.get('reddit-prefer-lang')
    lang = host_lang or c.user.pref_lang

    if getattr(g, "lang_override") and lang.startswith("en"):
        lang = g.lang_override

    c.lang = lang

    try:
        set_lang(lang, fallback_lang=g.lang)
    except LanguageError:
        lang = g.lang
        set_lang(lang, graceful_fail=True)

    try:
        c.locale = babel.core.Locale.parse(lang, sep='-')
    except (babel.core.UnknownLocaleError, ValueError):
        c.locale = babel.core.Locale.parse(g.lang, sep='-')


def set_content_lang():
    if c.user.pref_content_langs != 'all':
        c.content_langs = list(c.user.pref_content_langs)
        c.content_langs.sort()
    else:
        c.content_langs = c.user.pref_content_langs

def set_cnameframe():
    if (bool(request.params.get(utils.UrlParser.cname_get))
        or not request.host.split(":")[0].endswith(g.domain)):
        c.cname = True
        request.environ['REDDIT_CNAME'] = 1
    c.frameless_cname = request.environ.get('frameless_cname', False)
    if hasattr(c.site, 'domain'):
        c.authorized_cname = request.environ.get('authorized_cname', False)

def set_colors():
    theme_rx = re.compile(r'')
    color_rx = re.compile(r'\A([a-fA-F0-9]){3}(([a-fA-F0-9]){3})?\Z')
    c.theme = None
    if color_rx.match(request.GET.get('bgcolor') or ''):
        c.bgcolor = request.GET.get('bgcolor')
    if color_rx.match(request.GET.get('bordercolor') or ''):
        c.bordercolor = request.GET.get('bordercolor')


def _get_ratelimit_timeslice(slice_seconds):
    slice_start, secs_since = divmod(time.time(), slice_seconds)
    slice_start = time.gmtime(int(slice_start * slice_seconds))
    secs_to_next = slice_seconds - int(secs_since)
    return slice_start, secs_to_next


def ratelimit_agent(agent, limit=10, slice_size=10):
    slice_size = min(slice_size, 60)
    time_slice, retry_after = _get_ratelimit_timeslice(slice_size)
    key = "rate_agent_" + agent + time.strftime("_%S", time_slice)
    g.cache.add(key, 0, time=slice_size + 1)
    if g.cache.incr(key) > limit:
        request.environ['retry_after'] = retry_after
        abort(429)

appengine_re = re.compile(r'AppEngine-Google; \(\+http://code.google.com/appengine; appid: (?:dev|s)~([a-z0-9-]{6,30})\)\Z')
def ratelimit_agents():
    user_agent = request.user_agent

    if not user_agent:
        return

    # parse out the appid for appengine apps
    appengine_match = appengine_re.search(user_agent)
    if appengine_match:
        appid = appengine_match.group(1)
        ratelimit_agent(appid)
        return

    user_agent = user_agent.lower()
    for agent, limit in g.agents.iteritems():
        if agent in user_agent:
            ratelimit_agent(agent, limit)
            return

def ratelimit_throttled():
    ip = request.ip.strip()
    if is_throttled(ip):
        abort(429)


def paginated_listing(default_page_size=25, max_page_size=100, backend='sql'):
    def decorator(fn):
        @validate(num=VLimit('limit', default=default_page_size,
                             max_limit=max_page_size),
                  after=VByName('after', backend=backend),
                  before=VByName('before', backend=backend),
                  count=VCount('count'),
                  target=VTarget("target"),
                  show=VLength('show', 3, empty_error=None,
                               docs={"show": "(optional) the string `all`"}),
        )
        @wraps(fn)
        def new_fn(self, before, **env):
            if c.render_style == "htmllite":
                c.link_target = env.get("target")
            elif "target" in env:
                del env["target"]

            if "show" in env and env['show'] == 'all':
                c.ignore_hide_rules = True
            kw = build_arg_list(fn, env)

            #turn before into after/reverse
            kw['reverse'] = False
            if before:
                kw['after'] = before
                kw['reverse'] = True

            return fn(self, **kw)

        if hasattr(fn, "_api_doc"):
            notes = fn._api_doc["notes"] or []
            if paginated_listing.doc_note not in notes:
                notes.append(paginated_listing.doc_note)
            fn._api_doc["notes"] = notes

        return new_fn
    return decorator

paginated_listing.doc_note = "*This endpoint is [a listing](#listings).*"

#TODO i want to get rid of this function. once the listings in front.py are
#moved into listingcontroller, we shouldn't have a need for this
#anymore
def base_listing(fn):
    return paginated_listing()(fn)

def is_trusted_origin(origin):
    try:
        origin = urlparse(origin)
    except ValueError:
        return False

    return any(is_subdomain(origin.hostname, domain) for domain in g.trusted_domains)

def cross_domain(origin_check=is_trusted_origin, **options):
    """Set up cross domain validation and hoisting for a request handler."""
    def cross_domain_wrap(fn):
        cors_perms = {
            "origin_check": origin_check,
            "allow_credentials": bool(options.get("allow_credentials"))
        }

        @wraps(fn)
        def cross_domain_handler(self, *args, **kwargs):
            if request.params.get("hoist") == "cookie":
                # Cookie polling response
                if cors_perms["origin_check"](g.origin):
                    name = request.environ["pylons.routes_dict"]["action_name"]
                    resp = fn(self, *args, **kwargs)
                    c.cookies.add('hoist_%s' % name, ''.join(tup(resp)))
                    response.content_type = 'text/html'
                    return ""
                else:
                    abort(403)
            else:
                self.check_cors()
                return fn(self, *args, **kwargs)

        cross_domain_handler.cors_perms = cors_perms
        return cross_domain_handler
    return cross_domain_wrap

def require_https():
    if not c.secure:
        abort(ForbiddenError(errors.HTTPS_REQUIRED))


def require_domain(required_domain):
    if not is_subdomain(request.host, required_domain):
        abort(ForbiddenError(errors.WRONG_DOMAIN))


def disable_subreddit_css():
    def wrap(f):
        @wraps(f)
        def no_funny_business(*args, **kwargs):
            c.allow_styles = False
            return f(*args, **kwargs)
        return no_funny_business
    return wrap


def request_timer_name(action):
    return "service_time.web." + action


def flatten_response(content):
    """Convert a content iterable to a string, properly handling unicode."""
    # TODO: it would be nice to replace this with response.body someday
    # once unicode issues are ironed out.
    return "".join(_force_utf8(x) for x in tup(content) if x)


def abort_with_error(error, code=None):
    if not code and not error.code:
        raise ValueError('Error %r missing status code' % error)

    abort(reddit_http_error(
        code=code or error.code,
        error_name=error.name,
        explanation=error.message,
        fields=error.fields,
    ))


class MinimalController(BaseController):

    allow_stylesheets = False
    defer_ratelimiting = False

    def request_key(self):
        # note that this references the cookie at request time, not
        # the current value of it
        try:
            cookies_key = [(x, request.cookies.get(x, ''))
                           for x in cache_affecting_cookies]
        except CookieError:
            cookies_key = ''

        return make_key('request',
                        c.lang,
                        c.content_langs,
                        request.host,
                        c.secure,
                        c.cname,
                        request.fullpath,
                        c.over18,
                        c.extension,
                        c.render_style,
                        cookies_key)

    def cached_response(self):
        return ""

    def run_sitewide_ratelimits(self):
        """Ratelimit users and add ratelimit headers to the response.

        Headers added are:
        X-Ratelimit-Used: Number of requests used in this period
        X-Ratelimit-Remaining: Number of requests left to use
        X-Ratelimit-Reset: Approximate number of seconds to end of period

        This function only has an effect if one of
        g.RL_SITEWIDE_ENABLED or g.RL_OAUTH_SITEWIDE_ENABLED
        are set to 'true' in the app configuration

        If the ratelimit is exceeded, a 429 response will be sent,
        unless the app configuration has g.ENFORCE_RATELIMIT off.
        Headers will be sent even on aborted requests.

        """
        if c.error_page:
            # ErrorController is re-running pre, don't double ratelimit
            return
        if c.cdn_cacheable:
            type_ = "cdn"
        elif not is_api():
            type_ = "web"
        elif c.oauth_user and g.RL_OAUTH_SITEWIDE_ENABLED:
            type_ = "oauth"
            period = g.RL_OAUTH_RESET_SECONDS
            max_reqs = c.oauth_client._max_reqs
            # Convert client_id to ascii str for use as memcache key
            client_id = c.oauth2_access_token.client_id.encode("ascii")
            # OAuth2 ratelimits are per user-app combination
            key = 'siterl-oauth-' + c.user._id36 + ":" + client_id
        elif g.RL_SITEWIDE_ENABLED:
            type_ = "api"
            max_reqs = g.RL_MAX_REQS
            period = g.RL_RESET_SECONDS
            # API (non-oauth) limits are per-ip
            key = 'siterl-api-' + request.ip
        else:
            type_ = "none"

        g.stats.event_count("ratelimit.type", type_, sample_rate=0.01)
        if type_ in ("cdn", "web", "none"):
            # No ratelimiting or headers for:
            # * Web requests (HTML)
            # * CDN requests (logged out via www.reddit.com)
            return

        period_start, retry_after = _get_ratelimit_timeslice(period)
        key += time.strftime("-%H%M%S", period_start)

        try:
            g.ratelimitcache.add(key, 0, time=retry_after + 1)

            try:
                # Increment the key to track the current request
                recent_reqs = g.ratelimitcache.incr(key)
            except pylibmc.NotFound:
                # Previous round of ratelimiting fell out in the
                # time between calling `add` and calling `incr`.
                g.ratelimitcache.add(key, 1, time=retry_after + 1)
                recent_reqs = 1
        except pylibmc.Error as e:
            # Ratelimiting is non-critical; if the caches are
            # having issues, just skip adding the headers
            g.log.info("ratelimitcache error: %s", e)
            return

        reqs_remaining = max(0, max_reqs - recent_reqs)

        c.ratelimit_headers = {
            "X-Ratelimit-Used": str(recent_reqs),
            "X-Ratelimit-Reset": str(retry_after),
            "X-Ratelimit-Remaining": str(reqs_remaining),
        }

        if reqs_remaining <= 0:
            if recent_reqs > (2 * max_reqs):
                g.stats.event_count("ratelimit.exceeded", "hyperbolic")
            else:
                g.stats.event_count("ratelimit.exceeded", "over")
            if g.ENFORCE_RATELIMIT:
                # For non-abort situations, the headers will be added in post(),
                # to avoid including them in a pagecache
                response.headers.update(c.ratelimit_headers)
                abort(429)
        elif reqs_remaining < (0.1 * max_reqs):
            g.stats.event_count("ratelimit.exceeded", "close")

    def pre(self):
        action = request.environ["pylons.routes_dict"].get("action")
        if action:
            if not self._get_action_handler():
                action = 'invalid'
            controller = request.environ["pylons.routes_dict"]["controller"]
            key = "{}.{}".format(controller, action)
            c.request_timer = g.stats.get_timer(request_timer_name(key))
        else:
            c.request_timer = SimpleSillyStub()

        c.response_wrapper = None
        c.start_time = datetime.now(g.tz)
        c.request_timer.start()
        g.reset_caches()

        c.domain_prefix = request.environ.get("reddit-domain-prefix",
                                              g.domain_prefix)
        c.secure = request.environ["wsgi.url_scheme"] == "https"
        c.request_origin = request.host_url

        #check if user-agent needs a dose of rate-limiting
        if not c.error_page:
            ratelimit_throttled()
            ratelimit_agents()

        c.allow_loggedin_cache = False
        c.allow_framing = False

        c.cdn_cacheable = (request.via_cdn and
                           g.login_cookie not in request.cookies)

        c.extension = request.environ.get('extension')
        # the domain has to be set before Cookies get initialized
        set_subreddit()
        c.errors = ErrorSet()
        c.cookies = Cookies()
        # if an rss feed, this will also log the user in if a feed=
        # GET param is included
        set_content_type()

        c.request_timer.intermediate("minimal-pre")
        # True/False forces. None updates for most non-POST requests
        c.update_last_visit = None

        g.stats.count_string('user_agents', request.user_agent)

        if not self.defer_ratelimiting:
            self.run_sitewide_ratelimits()
            c.request_timer.intermediate("minimal-ratelimits")

        hooks.get_hook("reddit.request.minimal_begin").call()

    def can_use_pagecache(self):
        handler = self._get_action_handler()
        policy = getattr(handler, "pagecache_policy",
                         PAGECACHE_POLICY.LOGGEDOUT_ONLY)

        if policy == PAGECACHE_POLICY.LOGGEDIN_AND_LOGGEDOUT:
            return True
        elif policy == PAGECACHE_POLICY.LOGGEDOUT_ONLY:
            return not c.user_is_loggedin

        return False

    def try_pagecache(self):
        c.can_use_pagecache = self.can_use_pagecache()

        if request.method.upper() == 'GET' and c.can_use_pagecache:
            request_key = self.request_key()
            try:
                r = g.pagecache.get(request_key)
            except MemcachedError as e:
                g.log.warning("pagecache error: %s", e)
                return

            if r:
                r, c.cookies = r
                response.headers = r.headers
                response.body = r.body
                response.status_int = r.status_int

                request.environ['pylons.routes_dict']['action'] = 'cached_response'
                c.request_timer.name = request_timer_name("cached_response")

                c.used_cache = True
                # response wrappers have already been applied before cache write
                c.response_wrapper = None

    def post(self):
        c.request_timer.intermediate("action")

        # if the action raised an HTTPException (i.e. it aborted) then pylons
        # will have replaced response with the exception itself.
        c.is_exception_response = getattr(response, "_exception", False)

        if c.response_wrapper and not c.is_exception_response:
            content = flatten_response(response.content)
            wrapped_content = c.response_wrapper(content)
            response.content = wrapped_content

        if c.user_is_loggedin and not c.allow_loggedin_cache:
            response.headers['Cache-Control'] = 'no-cache'
            response.headers['Pragma'] = 'no-cache'

        # pagecache stores headers. we need to not add X-Frame-Options to
        # cached requests (such as media embeds) that intend to allow framing.
        if not c.allow_framing and not c.used_cache:
            response.headers["X-Frame-Options"] = "SAMEORIGIN"

        # set some headers related to client security
        response.headers['X-Content-Type-Options'] = 'nosniff'
        response.headers['X-XSS-Protection'] = '1; mode=block'

        # save the result of this page to the pagecache if possible.  we
        # mustn't cache things that rely on state not tracked by request_key
        # such as If-Modified-Since headers for 304s or requesting IP for 429s.
        if (g.page_cache_time
            and request.method.upper() == 'GET'
            and c.can_use_pagecache
            and not c.used_cache
            and response.status_int not in (304, 429)
            and not response.status.startswith("5")
            and not c.is_exception_response):
            try:
                g.pagecache.set(self.request_key(),
                                (response._current_obj(), c.cookies),
                                g.page_cache_time)
            except MemcachedError as e:
                # this codepath will actually never be hit as long as
                # the pagecache memcached client is in no_reply mode.
                g.log.warning("Ignored exception (%r) on pagecache "
                              "write for %r", e, request.path)

        pragmas = [p.strip() for p in
                   request.headers.get("Pragma", "").split(",")]
        if g.debug or "x-reddit-pagecache" in pragmas:
            if c.can_use_pagecache:
                pagecache_state = "hit" if c.used_cache else "miss"
            else:
                pagecache_state = "disallowed"
            response.headers["X-Reddit-Pagecache"] = pagecache_state

        if c.ratelimit_headers:
            response.headers.update(c.ratelimit_headers)

        # send cookies
        for k, v in c.cookies.iteritems():
            if v.dirty:
                response.set_cookie(key=k,
                                    value=quote(v.value),
                                    domain=v.domain,
                                    expires=v.expires,
                                    secure=getattr(v, 'secure', False),
                                    httponly=getattr(v, 'httponly', False))

        if self.should_update_last_visit():
            c.user.update_last_visit(c.start_time)

        hooks.get_hook("reddit.request.end").call()

        # this thread is probably going to be reused, but it could be
        # a while before it is. So we might as well dump the cache in
        # the mean time so that we don't have dead objects hanging
        # around taking up memory
        g.reset_caches()

        c.request_timer.intermediate("post")

        # push data to statsd
        c.request_timer.stop()
        g.stats.flush()

    def on_validation_error(self, error):
        if error.name == errors.USER_REQUIRED:
            self.intermediate_redirect('/login')
        elif error.name == errors.VERIFIED_USER_REQUIRED:
            self.intermediate_redirect('/verify')

    def abort404(self):
        abort(404, "not found")

    def abort403(self):
        abort(403, "forbidden")

    def check_cors(self):
        origin = request.headers.get("Origin")
        if not origin:
            return

        method = request.method
        if method == 'OPTIONS':
            # preflight request
            method = request.headers.get("Access-Control-Request-Method")
            if not method:
                self.abort403()

        action = request.environ["pylons.routes_dict"]["action_name"]

        handler = self._get_action_handler(action, method)
        cors = handler and getattr(handler, "cors_perms", None)

        if cors and cors["origin_check"](origin):
            response.headers["Access-Control-Allow-Origin"] = origin
            if cors.get("allow_credentials"):
                response.headers["Access-Control-Allow-Credentials"] = "true"

    def OPTIONS(self):
        """Return empty responses for CORS preflight requests"""
        self.check_cors()

    def update_qstring(self, dict):
        merged = copy(request.GET)
        merged.update(dict)
        return request.path + utils.query_string(merged)

    def api_wrapper(self, kw):
        data = simplejson.dumps(kw)
        return filters.websafe_json(data)

    def should_update_last_visit(self):
        if g.disallow_db_writes:
            return False

        if not c.user_is_loggedin:
            return False

        if c.update_last_visit is not None:
            return c.update_last_visit

        return request.method.upper() != "POST"


class OAuth2ResourceController(MinimalController):
    defer_ratelimiting = True

    def authenticate_with_token(self):
        set_extension(request.environ, "json")
        set_content_type()
        require_https()
        require_domain(g.oauth_domain)

        try:
            access_token = OAuth2AccessToken.get_token(self._get_bearer_token())
            require(access_token)
            require(access_token.check_valid())
            c.oauth2_access_token = access_token
            account = Account._byID36(access_token.user_id, data=True)
            require(account)
            require(not account._deleted)
            c.user = c.oauth_user = account
            c.user_is_loggedin = True
            c.oauth_client = OAuth2Client._byID(access_token.client_id)
        except RequirementException:
            self._auth_error(401, "invalid_token")

        handler = self._get_action_handler()
        if handler:
            oauth2_perms = getattr(handler, "oauth2_perms", {})
            if oauth2_perms.get("oauth2_allowed", False):
                grant = OAuth2Scope(access_token.scope)
                required = set(oauth2_perms['required_scopes'])
                if not grant.has_access(c.site.name, required):
                    self._auth_error(403, "insufficient_scope")
                c.oauth_scope = grant
            else:
                self._auth_error(400, "invalid_request")

    def check_for_bearer_token(self):
        if self._get_bearer_token(strict=False):
            self.authenticate_with_token()

    def _auth_error(self, code, error):
        abort(code, headers=[("WWW-Authenticate", 'Bearer realm="reddit", error="%s"' % error)])

    def _get_bearer_token(self, strict=True):
        auth = request.headers.get("Authorization")
        if not auth:
            return None
        try:
            auth_scheme, bearer_token = require_split(auth, 2)
            require(auth_scheme.lower() == "bearer")
            return bearer_token
        except RequirementException:
            if strict:
                self._auth_error(400, "invalid_request")
            else:
                return None


class RedditController(OAuth2ResourceController):

    @staticmethod
    def login(user, rem=False):
        # This can't be handled in post() due to PRG and ErrorController fun.
        user.update_last_visit(c.start_time)
        c.cookies[g.login_cookie] = Cookie(value=user.make_cookie(),
                                           expires=NEVER if rem else None,
                                           httponly=True)

    @staticmethod
    def logout():
        c.cookies[g.login_cookie] = Cookie(value='', expires=DELETE)

    @staticmethod
    def enable_admin_mode(user, first_login=None):
        # no expiration time so the cookie dies with the browser session
        admin_cookie = user.make_admin_cookie(first_login=first_login)
        c.cookies[g.admin_cookie] = Cookie(value=admin_cookie, httponly=True)

    @staticmethod
    def remember_otp(user):
        cookie = user.make_otp_cookie()
        expiration = datetime.utcnow() + timedelta(seconds=g.OTP_COOKIE_TTL)
        set_user_cookie(g.otp_cookie,
                        cookie,
                        secure=True,
                        httponly=True,
                        expires=expiration)

    @staticmethod
    def disable_admin_mode(user):
        c.cookies[g.admin_cookie] = Cookie(value='', expires=DELETE)

    def pre(self):
        record_timings = g.admin_cookie in request.cookies or g.debug
        admin_bar_eligible = response.content_type == 'text/html'
        if admin_bar_eligible and record_timings:
            g.stats.start_logging_timings()

        # set up stuff needed in base templates at error time here.
        c.js_preload = JSPreload()

        MinimalController.pre(self)

        set_cnameframe()

        # populate c.cookies unless we're on the unsafe media_domain
        if request.host != g.media_domain or g.media_domain == g.domain:
            cookie_counts = collections.Counter()
            try:
                for k, v in request.cookies.iteritems():
                    # minimalcontroller can still set cookies
                    if k not in c.cookies:
                        # we can unquote even if it's not quoted
                        c.cookies[k] = Cookie(value=unquote(v), dirty=False)
                        cookie_counts[Cookie.classify(k)] += 1
            except CookieError:
                #pylons or one of the associated retarded libraries
                #can't handle broken cookies
                request.environ['HTTP_COOKIE'] = ''

            for cookietype, count in cookie_counts.iteritems():
                g.stats.simple_event("cookie.%s" % cookietype, count)

        delete_obsolete_cookies()

        # the user could have been logged in via one of the feeds 
        maybe_admin = False
        is_otpcookie_valid = False

        self.check_for_bearer_token()

        # no logins for RSS feed unless valid_feed has already been called
        if not c.user:
            if c.extension != "rss":
                authenticate_user()

                admin_cookie = c.cookies.get(g.admin_cookie)
                if c.user_is_loggedin and admin_cookie:
                    maybe_admin, first_login = valid_admin_cookie(admin_cookie.value)

                    if maybe_admin:
                        self.enable_admin_mode(c.user, first_login=first_login)
                    else:
                        self.disable_admin_mode(c.user)

                otp_cookie = read_user_cookie(g.otp_cookie)
                if c.user_is_loggedin and otp_cookie:
                    is_otpcookie_valid = valid_otp_cookie(otp_cookie)

            if not c.user:
                c.user = UnloggedUser(get_browser_langs())
                # patch for fixing mangled language preferences
                if (not isinstance(c.user.pref_lang, basestring) or
                    not all(isinstance(x, basestring)
                            for x in c.user.pref_content_langs)):
                    c.user.pref_lang = g.lang
                    c.user.pref_content_langs = [g.lang]
                    c.user._commit()
        if c.user_is_loggedin:
            if not c.user._loaded:
                c.user._load()
            c.modhash = c.user.modhash()
            if hasattr(c.user, 'msgtime') and c.user.msgtime:
                c.have_messages = c.user.msgtime
            c.have_mod_messages = bool(c.user.modmsgtime)
            c.user_is_admin = maybe_admin and c.user.name in g.admins
            c.user_special_distinguish = c.user.special_distinguish()
            c.user_is_sponsor = c.user_is_admin or c.user.name in g.sponsors
            c.otp_cached = is_otpcookie_valid
            if not isinstance(c.site, FakeSubreddit) and not g.disallow_db_writes:
                c.user.update_sr_activity(c.site)

        c.request_timer.intermediate("base-auth")

        self.run_sitewide_ratelimits()
        c.request_timer.intermediate("base-ratelimits")

        c.over18 = over18()
        set_obey_over18()

        # looking up the multireddit requires c.user.
        set_multireddit()

        #set_browser_langs()
        set_iface_lang()
        set_content_lang()
        set_recent_clicks()
        # used for HTML-lite templates
        set_colors()

        # set some environmental variables in case we hit an abort
        if not isinstance(c.site, FakeSubreddit):
            request.environ['REDDIT_NAME'] = c.site.name

        # random reddit trickery -- have to do this after the content lang is set
        if c.site == Random:
            c.site = Subreddit.random_reddit(user=c.user)
            site_path = c.site.path.strip('/')
            path = "/" + site_path + request.path_qs
            abort(302, location=self.format_output_url(path))
        elif c.site == RandomSubscription:
            if not c.user.gold:
                abort(302, location=self.format_output_url('/gold/about'))
            c.site = Subreddit.random_subscription(c.user)
            site_path = c.site.path.strip('/')
            path = '/' + site_path + request.path_qs
            abort(302, location=self.format_output_url(path))
        elif c.site == RandomNSFW:
            c.site = Subreddit.random_reddit(over18=True, user=c.user)
            site_path = c.site.path.strip('/')
            path = '/' + site_path + request.path_qs
            abort(302, location=self.format_output_url(path))

        if not request.path.startswith("/api/login/"):
            # is the subreddit banned?
            if c.site.spammy() and not c.user_is_admin and not c.error_page:
                ban_info = getattr(c.site, "ban_info", {})
                if "message" in ban_info:
                    message = ban_info['message']
                else:
                    sitelink = url_escape(add_sr("/"))
                    subject = ("/r/%s has been incorrectly banned" %
                                   c.site.name)
                    link = ("/r/redditrequest/submit?url=%s&title=%s" %
                                (sitelink, subject))
                    message = strings.banned_subreddit_message % dict(
                                                                    link=link)
                errpage = pages.RedditError(strings.banned_subreddit_title,
                                            message,
                                            image="subreddit-banned.png")
                request.environ['usable_error_content'] = errpage.render()
                self.abort404()

            # check if the user has access to this subreddit
            if not c.site.can_view(c.user) and not c.error_page:
                if isinstance(c.site, LabeledMulti):
                    # do not leak the existence of multis via 403.
                    self.abort404()
                else:
                    public_description = c.site.public_description
                    errpage = pages.RedditError(
                        strings.private_subreddit_title,
                        strings.private_subreddit_message,
                        image="subreddit-private.png",
                        sr_description=public_description,
                    )
                    request.environ['usable_error_content'] = errpage.render()
                    self.abort403()

            #check over 18
            if (c.site.over_18 and not c.over18 and
                request.path not in ("/frame", "/over18")
                and c.render_style == 'html'):
                return self.intermediate_redirect("/over18", sr_path=False)

        #check whether to allow custom styles
        c.allow_styles = True
        c.can_apply_styles = self.allow_stylesheets
        if g.css_killswitch:
            c.can_apply_styles = False
        #if the preference is set and we're not at a cname
        elif not c.user.pref_show_stylesheets and not c.cname:
            c.can_apply_styles = False
        #if the site has a cname, but we're not using it
        elif c.site.domain and c.site.css_on_cname and not c.cname:
            c.can_apply_styles = False

        c.bare_content = request.GET.pop('bare', False)

        c.show_admin_bar = admin_bar_eligible and (c.user_is_admin or g.debug)
        if not c.show_admin_bar:
            g.stats.end_logging_timings()

        hooks.get_hook("reddit.request.begin").call()

        c.request_timer.intermediate("base-pre")

    def post(self):
        MinimalController.post(self)
        self._embed_html_timing_data()

        # allow logged-out JSON requests to be read cross-domain
        if (request.method.upper() == "GET" and not c.user_is_loggedin and
            c.render_style == "api"):
            response.headers["Access-Control-Allow-Origin"] = "*"

            request_origin = request.headers.get('Origin')
            if request_origin and request_origin != g.origin:
                g.stats.simple_event('cors.api_request')
                g.stats.count_string('origins', request_origin)

    def _embed_html_timing_data(self):
        timings = g.stats.end_logging_timings()

        if not timings or not c.show_admin_bar or c.is_exception_response:
            return

        timings = [{
            "key": timing.key,
            "start": round(timing.start, 4),
            "end": round(timing.end, 4),
        } for timing in timings]

        content = flatten_response(response.content)
        # inject stats script tag at the end of the <body>
        body_parts = list(content.rpartition("</body>"))
        if body_parts[1]:
            script = ('<script type="text/javascript">'
                      'r.timings = %s'
                      '</script>') % simplejson.dumps(timings)
            body_parts.insert(1, script)
            response.content = "".join(body_parts)

    def check_modified(self, thing, action):
        # this is a legacy shim until the old last_modified system is dead
        last_modified = utils.last_modified_date(thing, action)
        return self.abort_if_not_modified(last_modified)

    def abort_if_not_modified(self, last_modified, private=True,
                              max_age=timedelta(0),
                              must_revalidate=True):
        """Check If-Modified-Since and abort(304) if appropriate."""

        if c.user_is_loggedin and not c.allow_loggedin_cache:
            return

        # HTTP timestamps round to nearest second. truncate this value for
        # comparisons.
        last_modified = last_modified.replace(microsecond=0)

        date_str = http_utils.http_date_str(last_modified)
        response.headers['last-modified'] = date_str

        cache_control = []
        if private:
            cache_control.append('private')
        cache_control.append('max-age=%d' % max_age.total_seconds())
        if must_revalidate:
            cache_control.append('must-revalidate')
        response.headers['cache-control'] = ', '.join(cache_control)

        modified_since = request.if_modified_since
        if modified_since and modified_since >= last_modified:
            abort(304, 'not modified')

    def search_fail(self, exception):
        from r2.lib.search import SearchException
        if isinstance(exception, SearchException + (socket.error,)):
            g.log.error("Search Error: %s" % repr(exception))

        errpage = pages.RedditError(_("search failed"),
                                    strings.search_failed)

        request.environ['usable_error_content'] = errpage.render()
        request.environ['retry_after'] = 60
        abort(503)

########NEW FILE########
__FILENAME__ = redirect
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
from pylons import request, c
from pylons.controllers.util import abort

from r2.lib.base import BaseController
from r2.lib.validator import chkuser, chksrname


class RedirectController(BaseController):
    def pre(self, *k, **kw):
        BaseController.pre(self, *k, **kw)
        c.extension = request.environ.get('extension')

    def GET_redirect(self, dest):
        return self.redirect(str(dest))

    def GET_user_redirect(self, username, rest=None):
        user = chkuser(username)
        if not user:
            abort(400)
        url = "/user/" + user
        if rest:
            url += "/" + rest
        if request.query_string:
            url += "?" + request.query_string
        return self.redirect(str(url), code=301)

    def GET_timereddit_redirect(self, timereddit, rest=None):
        tr_name = chksrname(timereddit)
        if not tr_name:
            abort(400)
        if rest:
            rest = str(rest)
        else:
            rest = ''
        return self.redirect("/r/t:%s/%s" % (tr_name, rest), code=301)

########NEW FILE########
__FILENAME__ = robots
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
from pylons import request, response, g

from r2.controllers.reddit_base import MinimalController
from r2.lib.pages import Robots
from r2.lib import utils


class RobotsController(MinimalController):
    def try_pagecache(self):
        pass

    def pre(self):
        pass

    def post(self):
        pass

    def on_crawlable_domain(self):
        return utils.domain(request.host) == g.domain

    def GET_robots(self):
        response.content_type = "text/plain"
        if self.on_crawlable_domain():
            return Robots().render(style='txt')
        else:
            return "User-Agent: *\nDisallow: /\n"


########NEW FILE########
__FILENAME__ = toolbar
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
import re
import string

from pylons import c

from reddit_base import RedditController
from r2.lib import utils
from r2.lib.filters import spaceCompress, safemarkdown
from r2.lib.menus import CommentSortMenu
from r2.lib.pages import *
from r2.lib.pages.things import hot_links_by_url_listing, wrap_links
from r2.lib.template_helpers import add_sr
from r2.lib.validator import *
from r2.models import *
from r2.models.admintools import is_shamed_domain

# strips /r/foo/, /s/, or both
strip_sr          = re.compile('\A/r/[a-zA-Z0-9_-]+')
strip_s_path      = re.compile('\A/s/')
leading_slash     = re.compile('\A/+')
has_protocol      = re.compile('\A[a-zA-Z_-]+:')
allowed_protocol  = re.compile('\Ahttps?:')
need_insert_slash = re.compile('\Ahttps?:/[^/]')
def demangle_url(path):
    # there's often some URL mangling done by the stack above us, so
    # let's clean up the URL before looking it up
    path = strip_sr.sub('', path)
    path = strip_s_path.sub('', path)
    path = leading_slash.sub("", path)

    if has_protocol.match(path):
        if not allowed_protocol.match(path):
            return None
    else:
        path = 'http://%s' % path

    if need_insert_slash.match(path):
        path = string.replace(path, '/', '//', 1)

    path = utils.sanitize_url(path)

    return path

def match_current_reddit_subdomain(url):
    # due to X-Frame-Options: SAMEORIGIN headers, we can't frame mismatched
    # reddit subdomains
    parsed = UrlParser(url)
    if parsed.is_reddit_url():
        parsed.hostname = request.host
        return parsed.unparse()
    else:
        return url

def force_html():
    """Because we can take URIs like /s/http://.../foo.png, and we can
       guarantee that the toolbar will never be used with a non-HTML
       render style, we don't want to interpret the extension from the
       target URL. So here we rewrite Middleware's interpretation of
       the extension to force it to be HTML
    """

    c.render_style = 'html'
    c.extension = None
    c.content_type = 'text/html; charset=UTF-8'

def auto_expand_panel(link):
    if not link.num_comments or link.is_self:
        return False
    else:
        return c.user.pref_frame_commentspanel

class ToolbarController(RedditController):

    allow_stylesheets = True

    @validate(link1 = VByName('id'),
              link2 = VLink('id', redirect = False))
    def GET_goto(self, link1, link2):
        """Support old /goto?id= urls. deprecated"""
        link = link2 if link2 else link1
        if link:
            return self.redirect(add_sr("/tb/" + link._id36))
        return self.abort404()

    @validate(link = VLink('id'))
    def GET_tb(self, link):
        '''/tb/$id36, show a given link with the toolbar
        If the user doesn't have the toolbar enabled, redirect to comments
        page.
        
        '''
        from r2.lib.media import thumbnail_url
        if not link:
            return self.abort404()
        elif not link.subreddit_slow.can_view(c.user):
            # don't disclose the subreddit/title of a post via the redirect url
            self.abort403()
        elif link.is_self:
            return self.redirect(link.url)
        elif not (c.user_is_loggedin and c.user.pref_frame):
            return self.redirect(link.make_permalink_slow(force_domain=True))
        
        # if the domain is shame-banned, bail out.
        if is_shamed_domain(link.url)[0]:
            self.abort404()

        if link.has_thumbnail:
            thumbnail = thumbnail_url(link)
        else:
            thumbnail = None

        res = Frame(
            title=link.title,
            url=match_current_reddit_subdomain(link.url),
            thumbnail=thumbnail,
            fullname=link._fullname,
        )
        return spaceCompress(res.render())

    @validate(urloid=nop('urloid'))
    def GET_s(self, urloid):
        """/s/http://..., show a given URL with the toolbar. if it's
           submitted, redirect to /tb/$id36"""
        force_html()
        path = demangle_url(request.fullpath)

        if not path:
            # it was malformed
            self.abort404()

        # if the domain is shame-banned, bail out.
        if is_shamed_domain(path)[0]:
            self.abort404()

        listing = hot_links_by_url_listing(path, sr=c.site, num=1)
        link = listing.things[0] if listing.things else None

        if c.cname and not c.authorized_cname:
            # In this case, we make some bad guesses caused by the
            # cname frame on unauthorised cnames. 
            # 1. User types http://foo.com/http://myurl?cheese=brie
            #    (where foo.com is an unauthorised cname)
            # 2. We generate a frame that points to
            #    http://www.reddit.com/r/foo/http://myurl?cnameframe=0.12345&cheese=brie
            # 3. Because we accept everything after the /r/foo/, and
            #    we've now parsed, modified, and reconstituted that
            #    URL to add cnameframe, we really can't make any good
            #    assumptions about what we've done to a potentially
            #    already broken URL, and we can't assume that we've
            #    rebuilt it in the way that it was originally
            #    submitted (if it was)
            # We could try to work around this with more guesses (by
            # having demangle_url try to remove that param, hoping
            # that it's not already a malformed URL, and that we
            # haven't re-ordered the GET params, removed
            # double-slashes, etc), but for now, we'll just refuse to
            # do this operation
            return self.abort404()

        if link:
            # we were able to find it, let's send them to the
            # toolbar (if enabled) or comments (if not)
            return self.redirect(add_sr("/tb/" + link._id36))
        else:
            # It hasn't been submitted yet. Give them a chance to
            qs = utils.query_string({"url": path})
            return self.redirect(add_sr("/submit?" + qs))

    @validate(link = VLink('id'))
    def GET_comments(self, link):
        if not link:
            self.abort404()
        if not link.subreddit_slow.can_view(c.user):
            abort(403, 'forbidden')

        links = list(wrap_links(link))
        if not links:
            # they aren't allowed to see this link
            return abort(403, 'forbidden')
        link = links[0]

        wrapper = make_wrapper(render_class = StarkComment,
                               target = "_top")
        b = TopCommentBuilder(link, CommentSortMenu.operator('confidence'),
                              num=10, wrap=wrapper)

        listing = NestedListing(b, parent_name=link._fullname)

        raw_bar = strings.comments_panel_text % dict(
            fd_link=link.permalink)

        md_bar = safemarkdown(raw_bar, target="_top")

        res = RedditMin(content=CommentsPanel(link=link,
                                              listing=listing.listing(),
                                              expanded=auto_expand_panel(link),
                                              infobar=md_bar))

        return res.render()

    @validate(link = VByName('id'),
              url = nop('url'))
    def GET_toolbar(self, link, url):
        """The visible toolbar, with voting buttons and all"""
        if url:
            url = demangle_url(url)

        if link:
            wrapped = wrap_links(link, wrapper=FrameToolbar, num=1)
        else:
            return self.abort404()

        return spaceCompress(wrapped.render())

    @validate(link = VByName('id'))
    def GET_inner(self, link):
        """The intermediate frame that displays the comments side-bar
           on one side and the link on the other"""
        if not link:
            return self.abort404()

        res = InnerToolbarFrame(
            link=link,
            url=match_current_reddit_subdomain(link.url),
            expanded=auto_expand_panel(link),
        )
        return spaceCompress(res.render())

    @validate(link = VLink('linkoid'))
    def GET_linkoid(self, link):
        if not link:
            return self.abort404()
        return self.redirect(add_sr("/tb/" + link._id36))


########NEW FILE########
__FILENAME__ = web
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import g, c, request
from pylons.i18n import _

from r2.controllers.reddit_base import RedditController, abort_with_error
from r2.lib.base import abort
from r2.lib.validator import (
    validate,
    VOneOf,
    VPrintable,
    VRatelimit,
    VValidatedJSON,
)


class WebLogController(RedditController):
    on_validation_error = staticmethod(abort_with_error)

    @validate(
        VRatelimit(rate_user=False, rate_ip=True, prefix='rate_weblog_'),
        level=VOneOf('level', ('error',)),
        logs=VValidatedJSON('logs',
            VValidatedJSON.ArrayOf(VValidatedJSON.PartialObject({
                'msg': VPrintable('msg', max_length=256),
                'url': VPrintable('url', max_length=256),
                'tag': VPrintable('tag', max_length=32),
            }))
        ),
    )
    def POST_message(self, level, logs):
        # Whitelist tags to keep the frontend from creating too many keys in statsd
        valid_frontend_log_tags = {
            'unknown',
            'jquery-migrate-bad-html',
        }

        # prevent simple CSRF by requiring a custom header
        if not request.headers.get('X-Loggit'):
            abort(403)

        uid = c.user._id if c.user_is_loggedin else '-'

        # only accept a maximum of 3 entries per request
        for log in logs[:3]:
            if 'msg' not in log or 'url' not in log:
                continue

            tag = 'unknown'

            if log.get('tag') in valid_frontend_log_tags:
                tag = log['tag']

            g.stats.simple_event('frontend.error.' + tag)

            g.log.warning('[web frontend] %s: %s | U: %s FP: %s UA: %s',
                          level, log['msg'], uid, log['url'],
                          request.user_agent)

        VRatelimit.ratelimit(rate_user=False, rate_ip=True,
                             prefix="rate_weblog_", seconds=10)

########NEW FILE########
__FILENAME__ = wiki
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import request, g, c
from reddit_base import RedditController
from r2.controllers.oauth2 import require_oauth2_scope
from r2.lib.utils import url_links_builder
from reddit_base import paginated_listing
from r2.models.wiki import (WikiPage, WikiRevision, ContentLengthError,
                            modactions)
from r2.models.subreddit import Subreddit
from r2.models.modaction import ModAction
from r2.models.builder import WikiRevisionBuilder, WikiRecentRevisionBuilder

from r2.lib.template_helpers import join_urls


from r2.lib.validator import (
    nop,
    validate,
    VAdmin,
    VBoolean,
    VExistingUname,
    VInt,
    VMarkdown,
    VModhash,
    VOneOf,
    VPrintable,
    VRatelimit,
)

from r2.lib.validator.wiki import (
    VWikiPage,
    VWikiPageAndVersion,
    VWikiModerator,
    VWikiPageRevise,
    this_may_revise,
    this_may_view,
    VWikiPageName,
)
from r2.controllers.api_docs import api_doc, api_section
from r2.lib.pages.wiki import (WikiPageView, WikiNotFound, WikiRevisions,
                              WikiEdit, WikiSettings, WikiRecent,
                              WikiListing, WikiDiscussions,
                              WikiCreate)

from r2.config.extensions import set_extension
from r2.lib.template_helpers import add_sr
from r2.lib.db import tdb_cassandra
from r2.models.listing import WikiRevisionListing
from r2.lib.pages.things import default_thing_wrapper
from r2.lib.pages import BoringPage, CssError
from reddit_base import base_listing
from r2.models import IDBuilder, LinkListing, DefaultSR
from r2.lib.merge import ConflictException, make_htmldiff
from pylons.i18n import _
from r2.lib.pages import PaneStack
from r2.lib.utils import timesince
from r2.config import extensions
from r2.lib.base import abort
from r2.lib.errors import reddit_http_error

import json

page_descriptions = {'config/stylesheet':_("This page is the subreddit stylesheet, changes here apply to the subreddit css"),
                     'config/submit_text':_("The contents of this page appear on the submit page"),
                     'config/sidebar':_("The contents of this page appear on the subreddit sidebar"),
                     'config/description':_("The contents of this page appear in the public subreddit description")}

ATTRIBUTE_BY_PAGE = {"config/sidebar": "description",
                     "config/submit_text": "submit_text",
                     "config/description": "public_description"}
RENDERERS_BY_PAGE = {"config/sidebar": "reddit",
                     "config/submit_text": "reddit",
                     "config/description": "reddit",
                     "config/stylesheet": "stylesheet"}

class WikiController(RedditController):
    allow_stylesheets = True

    @require_oauth2_scope("wikiread")
    @api_doc(api_section.wiki, uri='/wiki/{page}', uses_site=True)
    @validate(pv=VWikiPageAndVersion(('page', 'v', 'v2'),
                                     required=False,
                                     restricted=False,
                                     allow_hidden_revision=False),
              page_name=VWikiPageName('page',
                                      error_on_name_normalized=True))
    def GET_wiki_page(self, pv, page_name):
        """Return the content of a wiki page

        If `v` is given, show the wiki page as it was at that version
        If both `v` and `v2` are given, show a diff of the two

        """
        message = None

        if c.errors.get(('PAGE_NAME_NORMALIZED', 'page')):
            url = join_urls(c.wiki_base_url, page_name)
            return self.redirect(url)

        page, version, version2 = pv

        if not page:
            is_api = c.render_style in extensions.API_TYPES
            if this_may_revise():
                if is_api:
                    self.handle_error(404, 'PAGE_NOT_CREATED')
                errorpage = WikiNotFound(page=page_name)
                request.environ['usable_error_content'] = errorpage.render()
            elif is_api:
                self.handle_error(404, 'PAGE_NOT_FOUND')
            self.abort404()

        if version:
            edit_by = version.get_author()
            edit_date = version.date
        else:
            edit_by = page.get_author()
            edit_date = page._get('last_edit_date')

        diffcontent = None
        if not version:
            content = page.content
            if c.is_wiki_mod and page.name in page_descriptions:
                message = page_descriptions[page.name]
        else:
            message = _("viewing revision from %s") % timesince(version.date)
            if version2:
                t1 = timesince(version.date)
                t2 = timesince(version2.date)
                timestamp1 = _("%s ago") % t1
                timestamp2 = _("%s ago") % t2
                message = _("comparing revisions from %(date_1)s and %(date_2)s") \
                          % {'date_1': t1, 'date_2': t2}
                diffcontent = make_htmldiff(version.content, version2.content, timestamp1, timestamp2)
                content = version2.content
            else:
                message = _("viewing revision from %s ago") % timesince(version.date)
                content = version.content

        renderer = RENDERERS_BY_PAGE.get(page.name, 'wiki') 

        return WikiPageView(content, alert=message, v=version, diff=diffcontent,
                            may_revise=this_may_revise(page), edit_by=edit_by,
                            edit_date=edit_date, page=page.name,
                            renderer=renderer).render()

    @require_oauth2_scope("wikiread")
    @api_doc(api_section.wiki, uri='/wiki/revisions/{page}', uses_site=True)
    @paginated_listing(max_page_size=100, backend='cassandra')
    @validate(page=VWikiPage(('page'), restricted=False))
    def GET_wiki_revisions(self, num, after, reverse, count, page):
        """Retrieve a list of revisions of this wiki `page`"""
        revisions = page.get_revisions()
        wikiuser = c.user if c.user_is_loggedin else None
        builder = WikiRevisionBuilder(revisions, user=wikiuser, sr=c.site,
                                      num=num, reverse=reverse, count=count,
                                      after=after, skip=not c.is_wiki_mod,
                                      wrap=default_thing_wrapper(),
                                      page=page)
        listing = WikiRevisionListing(builder).listing()
        return WikiRevisions(listing, page=page.name, may_revise=this_may_revise(page)).render()

    @validate(wp=VWikiPageRevise('page'),
              page=VWikiPageName('page'))
    def GET_wiki_create(self, wp, page):
        api = c.render_style in extensions.API_TYPES
        error = c.errors.get(('WIKI_CREATE_ERROR', 'page'))
        if error:
            error = error.msg_params
        if wp[0]:
            return self.redirect(join_urls(c.wiki_base_url, wp[0].name))
        elif api:
            if error:
                self.handle_error(403, **error)
            else:
                self.handle_error(404, 'PAGE_NOT_CREATED')
        elif error:
            error_msg = ''
            if error['reason'] == 'PAGE_NAME_LENGTH':
                error_msg = _("this wiki cannot handle page names of that magnitude!  please select a page name shorter than %d characters") % error['max_length']
            elif error['reason'] == 'PAGE_CREATED_ELSEWHERE':
                error_msg = _("this page is a special page, please go into the subreddit settings and save the field once to create this special page")
            elif error['reason'] == 'PAGE_NAME_MAX_SEPARATORS':
                error_msg = _('a max of %d separators "/" are allowed in a wiki page name.') % error['max_separators']
            return BoringPage(_("Wiki error"), infotext=error_msg).render()
        else:
            return WikiCreate(page=page, may_revise=True).render()

    @validate(wp=VWikiPageRevise('page', restricted=True, required=True))
    def GET_wiki_revise(self, wp, page, message=None, **kw):
        wp = wp[0]
        previous = kw.get('previous', wp._get('revision'))
        content = kw.get('content', wp.content)
        if not message and wp.name in page_descriptions:
            message = page_descriptions[wp.name]
        return WikiEdit(content, previous, alert=message, page=wp.name,
                        may_revise=True).render()

    @require_oauth2_scope("wikiread")
    @api_doc(api_section.wiki, uri='/wiki/revisions', uses_site=True)
    @paginated_listing(max_page_size=100, backend='cassandra')
    def GET_wiki_recent(self, num, after, reverse, count):
        """Retrieve a list of recently changed wiki pages in this subreddit"""
        revisions = WikiRevision.get_recent(c.site)
        wikiuser = c.user if c.user_is_loggedin else None
        builder = WikiRecentRevisionBuilder(revisions,  num=num, count=count,
                                            reverse=reverse, after=after,
                                            wrap=default_thing_wrapper(),
                                            skip=not c.is_wiki_mod,
                                            user=wikiuser, sr=c.site)
        listing = WikiRevisionListing(builder).listing()
        return WikiRecent(listing).render()

    @require_oauth2_scope("wikiread")
    @api_doc(api_section.wiki, uri='/wiki/pages', uses_site=True)
    def GET_wiki_listing(self):
        """Retrieve a list of wiki pages in this subreddit"""
        def check_hidden(page):
            return page.listed and this_may_view(page)
        pages, linear_pages = WikiPage.get_listing(c.site, filter_check=check_hidden)
        return WikiListing(pages, linear_pages).render()

    def GET_wiki_redirect(self, page='index'):
        return self.redirect(str("%s/%s" % (c.wiki_base_url, page)), code=301)

    @require_oauth2_scope("wikiread")
    @api_doc(api_section.wiki, uri='/wiki/discussions/{page}', uses_site=True)
    @base_listing
    @validate(page=VWikiPage('page', restricted=True))
    def GET_wiki_discussions(self, page, num, after, reverse, count):
        """Retrieve a list of discussions about this wiki `page`"""
        page_url = add_sr("%s/%s" % (c.wiki_base_url, page.name))
        builder = url_links_builder(page_url, num=num, after=after,
                                    reverse=reverse, count=count)
        listing = LinkListing(builder).listing()
        return WikiDiscussions(listing, page=page.name,
                               may_revise=this_may_revise(page)).render()

    @require_oauth2_scope("modwiki")
    @api_doc(api_section.wiki, uri='/wiki/settings/{page}', uses_site=True)
    @validate(page=VWikiPage('page', restricted=True, modonly=True))
    def GET_wiki_settings(self, page):
        """Retrieve the current permission settings for `page`"""
        settings = {'permlevel': page._get('permlevel', 0),
                    'listed': page.listed}
        mayedit = page.get_editor_accounts()
        restricted = (not page.special) and page.restricted
        show_editors = not restricted
        return WikiSettings(settings, mayedit, show_settings=not page.special,
                            page=page.name, show_editors=show_editors,
                            restricted=restricted,
                            may_revise=True).render()

    @require_oauth2_scope("modwiki")
    @api_doc(api_section.wiki, uri='/wiki/settings/{page}', uses_site=True)
    @validate(VModhash(),
              page=VWikiPage('page', restricted=True, modonly=True),
              permlevel=VInt('permlevel'),
              listed=VBoolean('listed'))
    def POST_wiki_settings(self, page, permlevel, listed):
        """Update the permissions and visibility of wiki `page`"""
        oldpermlevel = page.permlevel
        try:
            page.change_permlevel(permlevel)
        except ValueError:
            self.handle_error(403, 'INVALID_PERMLEVEL')
        if page.listed != listed:
            page.listed = listed
            page._commit()
            verb = 'Relisted' if listed else 'Delisted'
            description = '%s page %s' % (verb, page.name)
            ModAction.create(c.site, c.user, 'wikipagelisted',
                             description=description)
        if oldpermlevel != permlevel:
            description = 'Page: %s, Changed from %s to %s' % (
                page.name, oldpermlevel, permlevel
            )
            ModAction.create(c.site, c.user, 'wikipermlevel',
                             description=description)
        return self.GET_wiki_settings(page=page.name)

    def on_validation_error(self, error):
        RedditController.on_validation_error(self, error)
        if error.code:
            self.handle_error(error.code, error.name)

    def handle_error(self, code, reason=None, **data):
        abort(reddit_http_error(code, reason, **data))

    def pre(self):
        RedditController.pre(self)
        if g.disable_wiki and not c.user_is_admin:
            self.handle_error(403, 'WIKI_DOWN')
        if not c.site._should_wiki:
            self.handle_error(404, 'NOT_WIKIABLE')  # /r/mod for an example
        frontpage = isinstance(c.site, DefaultSR)
        c.wiki_base_url = join_urls(c.site.path, 'wiki')
        c.wiki_api_url = join_urls(c.site.path, '/api/wiki')
        c.wiki_id = g.default_sr if frontpage else c.site.name
        self.editconflict = False
        c.is_wiki_mod = (
            c.user_is_admin or c.site.is_moderator_with_perms(c.user, 'wiki')
            ) if c.user_is_loggedin else False
        c.wikidisabled = False

        mode = c.site.wikimode
        if not mode or mode == 'disabled':
            if not c.is_wiki_mod:
                self.handle_error(403, 'WIKI_DISABLED')
            else:
                c.wikidisabled = True

    # Redirects from the old wiki
    def GET_faq(self):
        return self.GET_wiki_redirect(page='faq')

    GET_help = GET_wiki_redirect


class WikiApiController(WikiController):
    @require_oauth2_scope("wikiedit")
    @validate(VModhash(),
              pageandprevious=VWikiPageRevise(('page', 'previous'), restricted=True),
              content=nop(('content')),
              page_name=VWikiPageName('page'),
              reason=VPrintable('reason', 256, empty_error=None))
    @api_doc(api_section.wiki, uri='/api/wiki/edit', uses_site=True)
    def POST_wiki_edit(self, pageandprevious, content, page_name, reason):
        """Edit a wiki `page`"""
        page, previous = pageandprevious

        if not page:
            error = c.errors.get(('WIKI_CREATE_ERROR', 'page'))
            if error:
                self.handle_error(403, **(error.msg_params or {}))
            if not c.user._spam:
                page = WikiPage.create(c.site, page_name)
        if c.user._spam:
            error = _("You are doing that too much, please try again later.")
            self.handle_error(415, 'SPECIAL_ERRORS', special_errors=[error])

        renderer = RENDERERS_BY_PAGE.get(page.name, 'wiki')
        if renderer in ('wiki', 'reddit'):
            content = VMarkdown(('content'), renderer=renderer).run(content)

        # Use the raw POST value as we need to tell the difference between
        # None/Undefined and an empty string.  The validators use a default
        # value with both of those cases and would need to be changed.
        # In order to avoid breaking functionality, this was done instead.
        previous = previous._id if previous else request.POST.get('previous')
        try:
            if page.name == 'config/stylesheet':
                css_errors, parsed = c.site.parse_css(content, verify=False)
                if g.css_killswitch:
                    self.handle_error(403, 'STYLESHEET_EDIT_DENIED')
                if css_errors:
                    error_items = [CssError(x).message for x in css_errors]
                    self.handle_error(415, 'SPECIAL_ERRORS', special_errors=error_items)
                c.site.change_css(content, parsed, previous, reason=reason)
            else:
                try:
                    page.revise(content, previous, c.user._id36, reason=reason)
                except ContentLengthError as e:
                    self.handle_error(403, 'CONTENT_LENGTH_ERROR', max_length=e.max_length)

                # continue storing the special pages as data attributes on the subreddit
                # object. TODO: change this to minimize subreddit get sizes.
                if page.special:
                    setattr(c.site, ATTRIBUTE_BY_PAGE[page.name], content)
                    c.site._commit()

                if page.special or c.is_wiki_mod:
                    description = modactions.get(page.name, 'Page %s edited' % page.name)
                    ModAction.create(c.site, c.user, 'wikirevise', details=description)
        except ConflictException as e:
            self.handle_error(409, 'EDIT_CONFLICT', newcontent=e.new, newrevision=page.revision, diffcontent=e.htmldiff)
        return json.dumps({})

    @require_oauth2_scope("modwiki")
    @validate(VModhash(),
              VWikiModerator(),
              page=VWikiPage('page'),
              act=VOneOf('act', ('del', 'add')),
              user=VExistingUname('username'))
    @api_doc(api_section.wiki, uri='/api/wiki/alloweditor/{act}',
             uses_site=True,
             uri_variants=['/api/wiki/alloweditor/%s' % act for act in ('del', 'add')])
    def POST_wiki_allow_editor(self, act, page, user):
        """Allow/deny `username` to edit this wiki `page`"""
        if not user:
            self.handle_error(404, 'UNKNOWN_USER')
        elif act == 'del':
            page.remove_editor(user._id36)
        elif act == 'add':
            page.add_editor(user._id36)
        else:
            self.handle_error(400, 'INVALID_ACTION')
        return json.dumps({})

    @validate(
        VModhash(),
        VAdmin(),
        pv=VWikiPageAndVersion(('page', 'revision')),
        deleted=VBoolean('deleted'),
    )
    def POST_wiki_revision_delete(self, pv, deleted):
        page, revision = pv
        if not revision:
            self.handle_error(400, 'INVALID_REVISION')
        if deleted and page.revision == str(revision._id):
            self.handle_error(400, 'REVISION_IS_CURRENT')
        revision.admin_deleted = deleted
        revision._commit()
        return json.dumps({'status': revision.admin_deleted})

    @require_oauth2_scope("modwiki")
    @validate(VModhash(),
              VWikiModerator(),
              pv=VWikiPageAndVersion(('page', 'revision')))
    @api_doc(api_section.wiki, uri='/api/wiki/hide', uses_site=True)
    def POST_wiki_revision_hide(self, pv):
        """Toggle the public visibility of a wiki page revision"""
        page, revision = pv
        if not revision:
            self.handle_error(400, 'INVALID_REVISION')
        return json.dumps({'status': revision.toggle_hide()})

    @require_oauth2_scope("modwiki")
    @validate(VModhash(),
              VWikiModerator(),
              pv=VWikiPageAndVersion(('page', 'revision')))
    @api_doc(api_section.wiki, uri='/api/wiki/revert', uses_site=True)
    def POST_wiki_revision_revert(self, pv):
        """Revert a wiki `page` to `revision`"""
        page, revision = pv
        if not revision:
            self.handle_error(400, 'INVALID_REVISION')
        content = revision.content
        reason = 'reverted back %s' % timesince(revision.date)
        if page.name == 'config/stylesheet':
            css_errors, parsed = c.site.parse_css(content)
            if css_errors:
                self.handle_error(403, 'INVALID_CSS')
            c.site.change_css(content, parsed, prev=None, reason=reason, force=True)
        else:
            try:
                page.revise(content, author=c.user._id36, reason=reason, force=True)

                # continue storing the special pages as data attributes on the subreddit
                # object. TODO: change this to minimize subreddit get sizes.
                if page.special:
                    setattr(c.site, ATTRIBUTE_BY_PAGE[page.name], content)
                    c.site._commit()
            except ContentLengthError as e:
                self.handle_error(403, 'CONTENT_LENGTH_ERROR', max_length=e.max_length)
        return json.dumps({})

    def pre(self):
        WikiController.pre(self)
        c.render_style = 'api'
        set_extension(request.environ, 'json')


########NEW FILE########
__FILENAME__ = admin_utils
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2013 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import config


def modhash(user, rand=None, test=False):
    return user.name


def valid_hash(user, hash):
    return True


def check_cheating(loc):
    pass


def vote_hash():
    return ''


if config['r2.import_private']:
    from r2admin.lib.admin_utils import *

########NEW FILE########
__FILENAME__ = amqp
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from Queue import Queue
from threading import local, Thread
from datetime import datetime
import os
import sys
import time
import errno
import socket
import itertools
import cPickle as pickle

from amqplib import client_0_8 as amqp

from pylons import g

amqp_host = g.amqp_host
amqp_user = g.amqp_user
amqp_pass = g.amqp_pass
amqp_exchange = 'reddit_exchange'
log = g.log
amqp_virtual_host = g.amqp_virtual_host
amqp_logging = g.amqp_logging
stats = g.stats
queues = g.queues

#there are two ways of interacting with this module: add_item and
#handle_items/consume_items. _add_item (the internal function for
#adding items to amqp that are added using add_item) might block for
#an arbitrary amount of time while trying to get a connection to amqp.

reset_caches = g.reset_caches

class Worker:
    def __init__(self):
        self.q = Queue()
        self.t = Thread(target=self._handle)
        self.t.setDaemon(True)
        self.t.start()

    def _handle(self):
        while True:
            reset_caches()

            fn = self.q.get()
            try:
                fn()
                self.q.task_done()
            except:
                import traceback
                print traceback.format_exc()

    def do(self, fn, *a, **kw):
        fn1 = lambda: fn(*a, **kw)
        self.q.put(fn1)

    def join(self):
        self.q.join()

worker = Worker()

class ConnectionManager(local):
    # There should be only two threads that ever talk to AMQP: the
    # worker thread and the foreground thread (whether consuming queue
    # items or a shell). This class is just a wrapper to make sure
    # that they get separate connections
    def __init__(self):
        self.connection = None
        self.channel = None
        self.have_init = False

    def get_connection(self):
        while not self.connection:
            try:
                self.connection = amqp.Connection(host = amqp_host,
                                                  userid = amqp_user,
                                                  password = amqp_pass,
                                                  virtual_host = amqp_virtual_host,
                                                  insist = False)
            except (socket.error, IOError), e:
                print 'error connecting to amqp %s @ %s (%r)' % (amqp_user, amqp_host, e)
                time.sleep(1)

        # don't run init_queue until someone actually needs it. this
        # allows the app server to start and serve most pages if amqp
        # isn't running
        if not self.have_init:
            self.init_queue()
            self.have_init = True

        return self.connection

    def get_channel(self, reconnect = False):
        # Periodic (and increasing with uptime) errors appearing when
        # connection object is still present, but appears to have been
        # closed.  This checks that the the connection is still open.
        if self.connection and self.connection.channels is None:
            log.error("Error: amqp.py, connection object with no available channels.  Reconnecting...")
            self.connection = None

        if not self.connection or reconnect:
            self.connection = None
            self.channel = None
            self.get_connection()

        if not self.channel:
            self.channel = self.connection.channel()

        return self.channel

    def init_queue(self):
        chan = self.get_channel()
        chan.exchange_declare(exchange=amqp_exchange,
                              type="direct",
                              durable=True,
                              auto_delete=False)

        for queue in queues:
            chan.queue_declare(queue=queue.name,
                               durable=queue.durable,
                               exclusive=queue.exclusive,
                               auto_delete=queue.auto_delete)

        for queue, key in queues.bindings:
            chan.queue_bind(routing_key=key,
                            queue=queue,
                            exchange=amqp_exchange)

connection_manager = ConnectionManager()

DELIVERY_TRANSIENT = 1
DELIVERY_DURABLE = 2

def _add_item(routing_key, body, message_id = None,
              delivery_mode=DELIVERY_DURABLE, headers=None,
              exchange=amqp_exchange):
    """adds an item onto a queue. If the connection to amqp is lost it
    will try to reconnect and then call itself again."""
    if not amqp_host:
        log.error("Ignoring amqp message %r to %r" % (body, routing_key))
        return

    chan = connection_manager.get_channel()
    msg = amqp.Message(body,
                       timestamp = datetime.now(),
                       delivery_mode = delivery_mode)
    if message_id:
        msg.properties['message_id'] = message_id

    if headers:
        msg.properties["application_headers"] = headers

    event_name = 'amqp.%s' % routing_key
    try:
        chan.basic_publish(msg,
                           exchange=exchange,
                           routing_key = routing_key)
    except Exception as e:
        stats.event_count(event_name, 'enqueue_failed')
        if e.errno == errno.EPIPE:
            connection_manager.get_channel(True)
            add_item(routing_key, body, message_id)
        else:
            raise
    else:
        stats.event_count(event_name, 'enqueue')

def add_item(routing_key, body, message_id=None,
             delivery_mode=DELIVERY_DURABLE, headers=None,
             exchange=amqp_exchange):
    if amqp_host and amqp_logging:
        log.debug("amqp: adding item %r to %r" % (body, routing_key))

    worker.do(_add_item, routing_key, body, message_id = message_id,
              delivery_mode=delivery_mode, headers=headers, exchange=exchange)

def add_kw(routing_key, **kw):
    add_item(routing_key, pickle.dumps(kw))

def consume_items(queue, callback, verbose=True):
    """A lighter-weight version of handle_items that uses AMQP's
       basic.consume instead of basic.get. Callback is only passed a
       single items at a time. This is more efficient than
       handle_items when the queue is likely to be occasionally empty
       or if batching the received messages is not necessary."""
    from pylons import c

    chan = connection_manager.get_channel()

    # configure the amount of data rabbit will send down to our buffer before
    # we're ready for it (to reduce network latency). by default, it will send
    # as much as our buffers will allow.
    chan.basic_qos(
        # size in bytes of prefetch window. zero indicates no preference.
        prefetch_size=0,
        # maximum number of prefetched messages.
        prefetch_count=10,
        # if global, applies to the whole connection, else just this channel.
        a_global=False
    )

    def _callback(msg):
        if verbose:
            count_str = ''
            if 'message_count' in msg.delivery_info:
                # the count from the last message, if the count is
                # available
                count_str = '(%d remaining)' % msg.delivery_info['message_count']

            print "%s: 1 item %s" % (queue, count_str)

        g.reset_caches()
        c.use_write_db = {}

        ret = callback(msg)
        msg.channel.basic_ack(msg.delivery_tag)
        sys.stdout.flush()
        return ret

    chan.basic_consume(queue=queue, callback=_callback)

    try:
        while chan.callbacks:
            try:
                chan.wait()
            except KeyboardInterrupt:
                break
    finally:
        worker.join()
        if chan.is_open:
            chan.close()

def handle_items(queue, callback, ack=True, limit=1, min_size=0,
                 drain=False, verbose=True, sleep_time=1):
    """Call callback() on every item in a particular queue. If the
    connection to the queue is lost, it will die. Intended to be
    used as a long-running process."""
    if limit < min_size:
        raise ValueError("min_size must be less than limit")
    from pylons import c

    chan = connection_manager.get_channel()
    countdown = None

    while True:
        # NB: None != 0, so we don't need an "is not None" check here
        if countdown == 0:
            break

        msg = chan.basic_get(queue)
        if not msg and drain:
            return
        elif not msg:
            time.sleep(sleep_time)
            continue

        if countdown is None and drain and 'message_count' in msg.delivery_info:
            countdown = 1 + msg.delivery_info['message_count']

        g.reset_caches()
        c.use_write_db = {}

        items = [msg]

        while countdown != 0:
            if countdown is not None:
                countdown -= 1
            if len(items) >= limit:
                break # the innermost loop only
            msg = chan.basic_get(queue)
            if msg is None:
                if len(items) < min_size:
                    time.sleep(sleep_time)
                else:
                    break
            else:
                items.append(msg)

        try:
            count_str = ''
            if 'message_count' in items[-1].delivery_info:
                # the count from the last message, if the count is
                # available
                count_str = '(%d remaining)' % items[-1].delivery_info['message_count']
            if verbose:
                print "%s: %d items %s" % (queue, len(items), count_str)
            callback(items, chan)

            if ack:
                # ack *all* outstanding messages
                chan.basic_ack(0, multiple=True)

            # flush any log messages printed by the callback
            sys.stdout.flush()
        except:
            for item in items:
                # explicitly reject the items that we've not processed
                chan.basic_reject(item.delivery_tag, requeue = True)
            raise


def empty_queue(queue):
    """debug function to completely erase the contents of a queue"""
    chan = connection_manager.get_channel()
    chan.queue_purge(queue)


def black_hole(queue):
    """continually empty out a queue as new items are created"""
    def _ignore(msg):
        print 'Ignoring msg: %r' % msg.body

    consume_items(queue, _ignore)

def dedup_queue(queue, rk = None, limit=None,
                delivery_mode = DELIVERY_DURABLE):
    """Hackily try to reduce the size of a queue by removing duplicate
       messages. The consumers of the target queue must consider
       identical messages to be idempotent. Preserves only message
       bodies"""
    chan = connection_manager.get_channel()

    if rk is None:
        rk = queue

    bodies = set()

    while True:
        msg = chan.basic_get(queue)

        if msg is None:
            break

        if msg.body not in bodies:
            bodies.add(msg.body)

        if limit is None:
            limit = msg.delivery_info.get('message_count')
            if limit is None:
                default_max = 100*1000
                print ("Message count was unavailable, defaulting to %d"
                       % (default_max,))
                limit = default_max
            else:
                print "Grabbing %d messages" % (limit,)
        else:
            limit -= 1
            if limit <= 0:
                break
            elif limit % 1000 == 0:
                print limit

    print "Grabbed %d unique bodies" % (len(bodies),)

    if bodies:
        for body in bodies:
            _add_item(rk, body, delivery_mode = delivery_mode)

        worker.join()

        chan.basic_ack(0, multiple=True)

########NEW FILE########
__FILENAME__ = app_globals
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from datetime import datetime
from urlparse import urlparse

import base64
import ConfigParser
import locale
import json
import logging
import os
import signal
import site
import socket
import subprocess
import sys

from sqlalchemy import engine, event

import pkg_resources
import pytz

from r2.config import queues
from r2.lib.cache import (
    CacheChain,
    CassandraCache,
    CassandraCacheChain,
    CL_ONE,
    CL_QUORUM,
    CMemcache,
    HardCache,
    HardcacheChain,
    LocalCache,
    MemcacheChain,
    SelfEmptyingCache,
    StaleCacheChain,
)
from r2.lib.configparse import ConfigValue, ConfigValueParser
from r2.lib.contrib import ipaddress
from r2.lib.lock import make_lock_factory
from r2.lib.manager import db_manager
from r2.lib.plugin import PluginLoader
from r2.lib.providers import select_provider
from r2.lib.stats import Stats, CacheStats, StatsCollectingConnectionPool
from r2.lib.translation import get_active_langs, I18N_PATH
from r2.lib.utils import config_gold_price, thread_dump


LIVE_CONFIG_NODE = "/config/live"
SECRETS_NODE = "/config/secrets"


def extract_live_config(config, plugins):
    """Gets live config out of INI file and validates it according to spec."""

    # ConfigParser will include every value in DEFAULT (which paste abuses)
    # if we do this the way we're supposed to. sorry for the horribleness.
    live_config = config._sections["live_config"].copy()
    del live_config["__name__"]  # magic value used by ConfigParser

    # parse the config data including specs from plugins
    parsed = ConfigValueParser(live_config)
    parsed.add_spec(Globals.live_config_spec)
    for plugin in plugins:
        parsed.add_spec(plugin.live_config)

    return parsed


def _decode_secrets(secrets):
    return {key: base64.b64decode(value) for key, value in secrets.iteritems()}


def extract_secrets(config):
    # similarly to the live_config one above, if we just did
    # .options("secrets") we'd get back all the junk from DEFAULT too. bleh.
    secrets = config._sections["secrets"].copy()
    del secrets["__name__"]  # magic value used by ConfigParser
    return _decode_secrets(secrets)


def fetch_secrets(zk_client):
    node_data = zk_client.get(SECRETS_NODE)[0]
    secrets = json.loads(node_data)
    return _decode_secrets(secrets)


class Globals(object):
    spec = {

        ConfigValue.int: [
            'db_pool_size',
            'db_pool_overflow_size',
            'page_cache_time',
            'commentpane_cache_time',
            'num_mc_clients',
            'MAX_CAMPAIGNS_PER_LINK',
            'MIN_DOWN_LINK',
            'MIN_UP_KARMA',
            'MIN_DOWN_KARMA',
            'MIN_RATE_LIMIT_KARMA',
            'MIN_RATE_LIMIT_COMMENT_KARMA',
            'HOT_PAGE_AGE',
            'QUOTA_THRESHOLD',
            'ADMIN_COOKIE_TTL',
            'ADMIN_COOKIE_MAX_IDLE',
            'OTP_COOKIE_TTL',
            'num_comments',
            'max_comments',
            'max_comments_gold',
            'num_default_reddits',
            'max_sr_images',
            'num_serendipity',
            'sr_dropdown_threshold',
            'comment_visits_period',
            'min_membership_create_community',
            'bcrypt_work_factor',
            'cassandra_pool_size',
            'sr_banned_quota',
            'sr_wikibanned_quota',
            'sr_wikicontributor_quota',
            'sr_moderator_invite_quota',
            'sr_contributor_quota',
            'sr_quota_time',
            'sr_invite_limit',
            'wiki_keep_recent_days',
            'wiki_max_page_length_bytes',
            'wiki_max_page_name_length',
            'wiki_max_page_separators',
            'min_promote_future',
            'max_promote_future',
            'RL_RESET_MINUTES',
            'RL_OAUTH_RESET_MINUTES',
        ],

        ConfigValue.float: [
            'default_promote_bid',
            'min_promote_bid',
            'max_promote_bid',
            'statsd_sample_rate',
            'querycache_prune_chance',
            'RL_AVG_REQ_PER_SEC',
            'RL_OAUTH_AVG_REQ_PER_SEC',
        ],

        ConfigValue.bool: [
            'debug',
            'log_start',
            'sqlprinting',
            'template_debug',
            'reload_templates',
            'uncompressedJS',
            'css_killswitch',
            'db_create_tables',
            'disallow_db_writes',
            'disable_ratelimit',
            'amqp_logging',
            'read_only_mode',
            'disable_wiki',
            'heavy_load_mode',
            'disable_captcha',
            'disable_ads',
            'disable_require_admin_otp',
            'static_pre_gzipped',
            'static_secure_pre_gzipped',
            'trust_local_proxies',
            'shard_link_vote_queues',
            'shard_commentstree_queues',
            'subreddit_stylesheets_static',
            'ENFORCE_RATELIMIT',
            'RL_SITEWIDE_ENABLED',
            'RL_OAUTH_SITEWIDE_ENABLED',
        ],

        ConfigValue.tuple: [
            'plugins',
            'stalecaches',
            'memcaches',
            'lockcaches',
            'permacache_memcaches',
            'rendercaches',
            'pagecaches',
            'memoizecaches',
            'srmembercaches',
            'ratelimitcaches',
            'cassandra_seeds',
            'admins',
            'sponsors',
            'employees',
            'automatic_reddits',
            'hardcache_categories',
            'case_sensitive_domains',
            'reserved_subdomains',
            'offsite_subdomains',
            'TRAFFIC_LOG_HOSTS',
            'exempt_login_user_agents',
            'timed_templates',
            'autoexpand_media_types',
        ],

        ConfigValue.dict(ConfigValue.str, ConfigValue.int): [
            'agents',
        ],

        ConfigValue.str: [
            'wiki_page_registration_info',
            'wiki_page_privacy_policy',
            'wiki_page_user_agreement',
            'wiki_page_gold_bottlecaps',
        ],

        ConfigValue.choice: {
             'cassandra_rcl': {
                 'ONE': CL_ONE,
                 'QUORUM': CL_QUORUM
             },
             'cassandra_wcl': {
                 'ONE': CL_ONE,
                 'QUORUM': CL_QUORUM
             },
        },

        ConfigValue.timeinterval: [
            'ARCHIVE_AGE',
        ],

        config_gold_price: [
            'gold_month_price',
            'gold_year_price',
            'cpm_selfserve',
            'cpm_selfserve_geotarget_country',
            'cpm_selfserve_geotarget_metro',
        ],
    }

    live_config_spec = {
        ConfigValue.bool: [
            'frontend_logging',
        ],
        ConfigValue.float: [
            'spotlight_interest_sub_p',
            'spotlight_interest_nosub_p',
            'gold_revenue_goal',
        ],
        ConfigValue.tuple: [
            'fastlane_links',
            'listing_chooser_sample_multis',
            'discovery_srs',
        ],
        ConfigValue.str: [
            'listing_chooser_gold_multi',
            'listing_chooser_explore_sr',
        ],
        ConfigValue.dict(ConfigValue.int, ConfigValue.float): [
            'comment_tree_version_weights',
        ],
        ConfigValue.messages: [
            'welcomebar_messages',
            'sidebar_message',
            'gold_sidebar_message',
        ],
        ConfigValue.dict(ConfigValue.str, ConfigValue.float): [
            'pennies_per_server_second',
        ],
    }

    def __init__(self, global_conf, app_conf, paths, **extra):
        """
        Globals acts as a container for objects available throughout
        the life of the application.

        One instance of Globals is created by Pylons during
        application initialization and is available during requests
        via the 'g' variable.

        ``global_conf``
            The same variable used throughout ``config/middleware.py``
            namely, the variables from the ``[DEFAULT]`` section of the
            configuration file.

        ``app_conf``
            The same ``kw`` dictionary used throughout
            ``config/middleware.py`` namely, the variables from the
            section in the config file for your application.

        ``extra``
            The configuration returned from ``load_config`` in 
            ``config/middleware.py`` which may be of use in the setup of
            your global variables.

        """

        global_conf.setdefault("debug", False)

        # reloading site ensures that we have a fresh sys.path to build our
        # working set off of. this means that forked worker processes won't get
        # the sys.path that was current when the master process was spawned
        # meaning that new plugins will be picked up on regular app reload
        # rather than having to restart the master process as well.
        reload(site)
        self.pkg_resources_working_set = pkg_resources.WorkingSet()

        self.config = ConfigValueParser(global_conf)
        self.config.add_spec(self.spec)
        self.plugins = PluginLoader(self.pkg_resources_working_set,
                                    self.config.get("plugins", []))

        self.stats = Stats(self.config.get('statsd_addr'),
                           self.config.get('statsd_sample_rate'))
        self.startup_timer = self.stats.get_timer("app_startup")
        self.startup_timer.start()

        self.paths = paths

        self.running_as_script = global_conf.get('running_as_script', False)
        
        # turn on for language support
        self.lang = getattr(self, 'site_lang', 'en')
        self.languages, self.lang_name = \
            get_active_langs(default_lang=self.lang)

        all_languages = self.lang_name.keys()
        all_languages.sort()
        self.all_languages = all_languages
        
        # set default time zone if one is not set
        tz = global_conf.get('timezone', 'UTC')
        self.tz = pytz.timezone(tz)
        
        dtz = global_conf.get('display_timezone', tz)
        self.display_tz = pytz.timezone(dtz)

        self.startup_timer.intermediate("init")

    def __getattr__(self, name):
        if not name.startswith('_') and name in self.config:
            return self.config[name]
        else:
            raise AttributeError

    def setup(self):
        self.queues = queues.declare_queues(self)

        ################# PROVIDERS
        self.media_provider = select_provider(
            self.config,
            self.pkg_resources_working_set,
            "r2.provider.media",
            self.media_provider,
        )
        self.startup_timer.intermediate("providers")

        ################# CONFIGURATION
        # AMQP is required
        if not self.amqp_host:
            raise ValueError("amqp_host not set in the .ini")

        if not self.cassandra_seeds:
            raise ValueError("cassandra_seeds not set in the .ini")

        # heavy load mode is read only mode with a different infobar
        if self.heavy_load_mode:
            self.read_only_mode = True

        origin_prefix = self.domain_prefix + "." if self.domain_prefix else ""
        self.origin = "http://" + origin_prefix + self.domain

        self.trusted_domains = set([self.domain])
        if self.https_endpoint:
            https_url = urlparse(self.https_endpoint)
            self.trusted_domains.add(https_url.hostname)

        # load the unique hashed names of files under static
        static_files = os.path.join(self.paths.get('static_files'), 'static')
        names_file_path = os.path.join(static_files, 'names.json')
        if os.path.exists(names_file_path):
            with open(names_file_path) as handle:
                self.static_names = json.load(handle)
        else:
            self.static_names = {}

        # make python warnings go through the logging system
        logging.captureWarnings(capture=True)

        log = logging.getLogger('reddit')

        # when we're a script (paster run) just set up super simple logging
        if self.running_as_script:
            log.setLevel(logging.INFO)
            log.addHandler(logging.StreamHandler())

        # if in debug mode, override the logging level to DEBUG
        if self.debug:
            log.setLevel(logging.DEBUG)

        # attempt to figure out which pool we're in and add that to the
        # LogRecords.
        try:
            with open("/etc/ec2_asg", "r") as f:
                pool = f.read().strip()
            # clean up the pool name since we're putting stuff after "-"
            pool = pool.partition("-")[0]
        except IOError:
            pool = "reddit-app"
        self.log = logging.LoggerAdapter(log, {"pool": pool})

        # set locations
        self.locations = {}

        if not self.media_domain:
            self.media_domain = self.domain
        if self.media_domain == self.domain:
            print >> sys.stderr, ("Warning: g.media_domain == g.domain. " +
                   "This may give untrusted content access to user cookies")

        for arg in sys.argv:
            tokens = arg.split("=")
            if len(tokens) == 2:
                k, v = tokens
                self.log.debug("Overriding g.%s to %s" % (k, v))
                setattr(self, k, v)

        self.reddit_host = socket.gethostname()
        self.reddit_pid  = os.getpid()

        if hasattr(signal, 'SIGUSR1'):
            # not all platforms have user signals
            signal.signal(signal.SIGUSR1, thread_dump)

        locale.setlocale(locale.LC_ALL, self.locale)

        # Pre-calculate ratelimit values
        self.RL_RESET_SECONDS = self.config["RL_RESET_MINUTES"] * 60
        self.RL_MAX_REQS = int(self.config["RL_AVG_REQ_PER_SEC"] *
                                      self.RL_RESET_SECONDS)

        self.RL_OAUTH_RESET_SECONDS = self.config["RL_OAUTH_RESET_MINUTES"] * 60
        self.RL_OAUTH_MAX_REQS = int(self.config["RL_OAUTH_AVG_REQ_PER_SEC"] *
                                     self.RL_OAUTH_RESET_SECONDS)

        self.startup_timer.intermediate("configuration")

        ################# ZOOKEEPER
        # for now, zookeeper will be an optional part of the stack.
        # if it's not configured, we will grab the expected config from the
        # [live_config] section of the ini file
        zk_hosts = self.config.get("zookeeper_connection_string")
        if zk_hosts:
            from r2.lib.zookeeper import (connect_to_zookeeper,
                                          LiveConfig, LiveList)
            zk_username = self.config["zookeeper_username"]
            zk_password = self.config["zookeeper_password"]
            self.zookeeper = connect_to_zookeeper(zk_hosts, (zk_username,
                                                             zk_password))
            self.live_config = LiveConfig(self.zookeeper, LIVE_CONFIG_NODE)
            self.secrets = fetch_secrets(self.zookeeper)
            self.throttles = LiveList(self.zookeeper, "/throttles",
                                      map_fn=ipaddress.ip_network,
                                      reduce_fn=ipaddress.collapse_addresses)
        else:
            self.zookeeper = None
            parser = ConfigParser.RawConfigParser()
            parser.optionxform = str
            parser.read([self.config["__file__"]])
            self.live_config = extract_live_config(parser, self.plugins)
            self.secrets = extract_secrets(parser)
            self.throttles = tuple()  # immutable since it's not real

        self.startup_timer.intermediate("zookeeper")

        ################# MEMCACHE
        num_mc_clients = self.num_mc_clients

        # the main memcache pool. used for most everything.
        self.memcache = CMemcache(
            self.memcaches,
            min_compress_len=50 * 1024,
            num_clients=num_mc_clients,
        )

        # a pool just used for @memoize results
        memoizecaches = CMemcache(
            self.memoizecaches,
            min_compress_len=50 * 1024,
            num_clients=num_mc_clients,
        )

        # a pool just for srmember rels
        srmembercaches = CMemcache(
            self.srmembercaches,
            min_compress_len=96,
            num_clients=num_mc_clients,
        )

        ratelimitcaches = CMemcache(
            self.ratelimitcaches,
            min_compress_len=96,
            num_clients=num_mc_clients,
        )

        # a smaller pool of caches used only for distributed locks.
        # TODO: move this to ZooKeeper
        self.lock_cache = CMemcache(self.lockcaches,
                                    num_clients=num_mc_clients)
        self.make_lock = make_lock_factory(self.lock_cache, self.stats)

        # memcaches used in front of the permacache CF in cassandra.
        # XXX: this is a legacy thing; permacache was made when C* didn't have
        # a row cache.
        if self.permacache_memcaches:
            permacache_memcaches = CMemcache(self.permacache_memcaches,
                                             min_compress_len=50 * 1024,
                                             num_clients=num_mc_clients)
        else:
            permacache_memcaches = None

        # the stalecache is a memcached local to the current app server used
        # for data that's frequently fetched but doesn't need to be fresh.
        if self.stalecaches:
            stalecaches = CMemcache(self.stalecaches,
                                    num_clients=num_mc_clients)
        else:
            stalecaches = None

        # rendercache holds rendered partial templates.
        rendercaches = CMemcache(
            self.rendercaches,
            noreply=True,
            no_block=True,
            num_clients=num_mc_clients,
            min_compress_len=480,
        )

        # pagecaches hold fully rendered pages
        pagecaches = CMemcache(
            self.pagecaches,
            noreply=True,
            no_block=True,
            num_clients=num_mc_clients,
            min_compress_len=1400,
        )

        self.startup_timer.intermediate("memcache")

        ################# CASSANDRA
        keyspace = "reddit"
        self.cassandra_pools = {
            "main":
                StatsCollectingConnectionPool(
                    keyspace,
                    stats=self.stats,
                    logging_name="main",
                    server_list=self.cassandra_seeds,
                    pool_size=self.cassandra_pool_size,
                    timeout=4,
                    max_retries=3,
                    prefill=False
                ),
        }

        permacache_cf = CassandraCache(
            'permacache',
            self.cassandra_pools[self.cassandra_default_pool],
            read_consistency_level=self.cassandra_rcl,
            write_consistency_level=self.cassandra_wcl
        )

        self.startup_timer.intermediate("cassandra")

        ################# POSTGRES
        self.dbm = self.load_db_params()
        self.startup_timer.intermediate("postgres")

        ################# CHAINS
        # initialize caches. Any cache-chains built here must be added
        # to cache_chains (closed around by reset_caches) so that they
        # can properly reset their local components
        cache_chains = {}
        localcache_cls = (SelfEmptyingCache if self.running_as_script
                          else LocalCache)

        if stalecaches:
            self.cache = StaleCacheChain(
                localcache_cls(),
                stalecaches,
                self.memcache,
            )
        else:
            self.cache = MemcacheChain((localcache_cls(), self.memcache))
        cache_chains.update(cache=self.cache)

        if stalecaches:
            self.memoizecache = StaleCacheChain(
                localcache_cls(),
                stalecaches,
                memoizecaches,
            )
        else:
            self.memoizecache = MemcacheChain(
                (localcache_cls(), memoizecaches))
        cache_chains.update(memoizecache=self.memoizecache)

        if stalecaches:
            self.srmembercache = StaleCacheChain(
                localcache_cls(),
                stalecaches,
                srmembercaches,
            )
        else:
            self.srmembercache = MemcacheChain(
                (localcache_cls(), srmembercaches))
        cache_chains.update(srmembercache=self.srmembercache)

        self.ratelimitcache = MemcacheChain(
                (localcache_cls(), ratelimitcaches))
        cache_chains.update(ratelimitcache=self.ratelimitcache)

        self.rendercache = MemcacheChain((
            localcache_cls(),
            rendercaches,
        ))
        cache_chains.update(rendercache=self.rendercache)

        self.pagecache = MemcacheChain((
            localcache_cls(),
            pagecaches,
        ))
        cache_chains.update(pagecache=self.pagecache)

        # the thing_cache is used in tdb_cassandra.
        self.thing_cache = CacheChain((localcache_cls(),))
        cache_chains.update(thing_cache=self.thing_cache)

        self.permacache = CassandraCacheChain(
            localcache_cls(),
            permacache_cf,
            memcache=permacache_memcaches,
            lock_factory=self.make_lock,
        )
        cache_chains.update(permacache=self.permacache)

        # hardcache is used for various things that tend to expire
        # TODO: replace hardcache w/ cassandra stuff
        self.hardcache = HardcacheChain(
            (localcache_cls(), self.memcache, HardCache(self)),
            cache_negative_results=True,
        )
        cache_chains.update(hardcache=self.hardcache)

        # I know this sucks, but we need non-request-threads to be
        # able to reset the caches, so we need them be able to close
        # around 'cache_chains' without being able to call getattr on
        # 'g'
        def reset_caches():
            for name, chain in cache_chains.iteritems():
                chain.reset()
                chain.stats = CacheStats(self.stats, name)
        self.cache_chains = cache_chains

        self.reset_caches = reset_caches
        self.reset_caches()

        self.startup_timer.intermediate("cache_chains")

        # try to set the source control revision numbers
        self.versions = {}
        r2_root = os.path.dirname(os.path.dirname(self.paths["root"]))
        r2_gitdir = os.path.join(r2_root, ".git")
        self.short_version = self.record_repo_version("r2", r2_gitdir)

        if I18N_PATH:
            i18n_git_path = os.path.join(os.path.dirname(I18N_PATH), ".git")
            self.record_repo_version("i18n", i18n_git_path)

        self.startup_timer.intermediate("revisions")

    def setup_complete(self):
        self.startup_timer.stop()
        self.stats.flush()

        if self.log_start:
            self.log.error(
                "%s:%s started %s at %s (took %.02fs)",
                self.reddit_host,
                self.reddit_pid,
                self.short_version,
                datetime.now().strftime("%H:%M:%S"),
                self.startup_timer.elapsed_seconds()
            )

    def record_repo_version(self, repo_name, git_dir):
        """Get the currently checked out git revision for a given repository,
        record it in g.versions, and return the short version of the hash."""
        try:
            subprocess.check_output
        except AttributeError:
            # python 2.6 compat
            pass
        else:
            try:
                revision = subprocess.check_output(["git",
                                                    "--git-dir", git_dir,
                                                    "rev-parse", "HEAD"])
            except subprocess.CalledProcessError, e:
                self.log.warning("Unable to fetch git revision: %r", e)
            else:
                self.versions[repo_name] = revision.rstrip()
                return revision[:7]

        return "(unknown)"

    def load_db_params(self):
        self.databases = tuple(ConfigValue.to_iter(self.config.raw_data['databases']))
        self.db_params = {}
        if not self.databases:
            return

        dbm = db_manager.db_manager()
        db_param_names = ('name', 'db_host', 'db_user', 'db_pass', 'db_port',
                          'pool_size', 'max_overflow')
        for db_name in self.databases:
            conf_params = ConfigValue.to_iter(self.config.raw_data[db_name + '_db'])
            params = dict(zip(db_param_names, conf_params))
            if params['db_user'] == "*":
                params['db_user'] = self.db_user
            if params['db_pass'] == "*":
                params['db_pass'] = self.db_pass
            if params['db_port'] == "*":
                params['db_port'] = self.db_port

            if params['pool_size'] == "*":
                params['pool_size'] = self.db_pool_size
            if params['max_overflow'] == "*":
                params['max_overflow'] = self.db_pool_overflow_size

            dbm.setup_db(db_name, g_override=self, **params)
            self.db_params[db_name] = params

        dbm.type_db = dbm.get_engine(self.config.raw_data['type_db'])
        dbm.relation_type_db = dbm.get_engine(self.config.raw_data['rel_type_db'])

        def split_flags(raw_params):
            params = []
            flags = {}

            for param in raw_params:
                if not param.startswith("!"):
                    params.append(param)
                else:
                    key, sep, value = param[1:].partition("=")
                    if sep:
                        flags[key] = value
                    else:
                        flags[key] = True

            return params, flags

        prefix = 'db_table_'
        self.predefined_type_ids = {}
        for k, v in self.config.raw_data.iteritems():
            if not k.startswith(prefix):
                continue

            params, table_flags = split_flags(ConfigValue.to_iter(v))
            name = k[len(prefix):]
            kind = params[0]
            server_list = self.config.raw_data["db_servers_" + name]
            engines, flags = split_flags(ConfigValue.to_iter(server_list))

            typeid = table_flags.get("typeid")
            if typeid:
                self.predefined_type_ids[name] = int(typeid)

            if kind == 'thing':
                dbm.add_thing(name, dbm.get_engines(engines),
                              **flags)
            elif kind == 'relation':
                dbm.add_relation(name, params[1], params[2],
                                 dbm.get_engines(engines),
                                 **flags)
        return dbm

    def __del__(self):
        """
        Put any cleanup code to be run when the application finally exits 
        here.
        """
        pass

########NEW FILE########
__FILENAME__ = authentication
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Authentication providers for setting c.user on every request.

This system is intended to allow pluggable authentication for intranets etc. It
is not intended to cover all login/logout functionality and in non-cookie
scenarios those are probably nonsensical to allow user control of (i.e.
single-signon on an intranet doesn't generally allow new account creation on a
single website.)
"""

import bcrypt
from pylons import g, c, request
from urllib import unquote

from r2.models import Account, NotFound
from r2.lib.utils import constant_time_compare, parse_http_basic
from r2.lib.require import RequirementException


_AUTHENTICATION_PROVIDERS = {}


def authentication_provider(allow_logout):
    """Register an authentication provider with the framework.

    Authentication providers should return None if authentication failed or an
    Account object if it succeeded.
    """
    def authentication_provider_decorator(fn):
        _AUTHENTICATION_PROVIDERS[fn.__name__] = fn
        fn.allow_logout = allow_logout
        return fn
    return authentication_provider_decorator


@authentication_provider(allow_logout=True)
def cookie():
    """Authenticate the user given a session cookie."""
    session_cookie = request.cookies.get(g.login_cookie)
    if session_cookie:
        session_cookie = unquote(session_cookie)
    else:
        return None

    try:
        uid, timestr, hash = session_cookie.split(",")
        uid = int(uid)
    except:
        return None

    try:
        account = Account._byID(uid, data=True)
    except NotFound:
        return None

    if not constant_time_compare(session_cookie, account.make_cookie(timestr)):
        return None
    return account


@authentication_provider(allow_logout=False)
def http_basic():
    """Authenticate the user based on their HTTP "Authorization" header."""
    import crypt

    try:
        authorization = request.environ.get("HTTP_AUTHORIZATION")
        username, password = parse_http_basic(authorization)
    except RequirementException:
        return None

    try:
        account = Account._by_name(username)
    except NotFound:
        return None

    # not all systems support bcrypt in the standard crypt
    if account.password.startswith("$2a$"):
        expected_hash = bcrypt.hashpw(password, account.password)
    else:
        expected_hash = crypt.crypt(password, account.password)

    if not constant_time_compare(expected_hash, account.password):
        return None
    return account


def _get_authenticator():
    """Return the configured authenticator."""
    return _AUTHENTICATION_PROVIDERS[g.authentication_provider]


def user_can_log_out():
    """Return if the configured authenticator allows users to log out."""
    authenticator = _get_authenticator()
    return authenticator.allow_logout


def authenticate_user():
    """Attempt to authenticate the user using the configured authenticator."""

    if not g.read_only_mode:
        authenticator = _get_authenticator()
        c.user = authenticator()

        if c.user and c.user._deleted:
            c.user = None
    else:
        c.user = None

    c.user_is_loggedin = bool(c.user)

########NEW FILE########
__FILENAME__ = api
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

"""
For talking to authorize.net credit card payments via their XML api.

This file consists mostly of wrapper classes for dealing with their
API, while the actual useful functions live in interaction.py

NOTE: This is using the Customer Information Manager (CIM) API
http://developer.authorize.net/api/cim/
"""

import re
from httplib import HTTPSConnection
from urlparse import urlparse

from BeautifulSoup import BeautifulStoneSoup
from pylons import g
from xml.sax.saxutils import escape

from r2.lib.export import export
from r2.lib.utils import iters, Storage
from r2.models.bidding import CustomerID, PayID, ShippingAddress

__all__ = ["PROFILE_LIMIT"]


# list of the most common errors.
Errors = Storage(TESTMODE="E00009",
                 TRANSACTION_FAIL="E00027",
                 DUPLICATE_RECORD="E00039", 
                 RECORD_NOT_FOUND="E00040",
                 TOO_MANY_PAY_PROFILES="E00042",
                 TOO_MANY_SHIP_ADDRESSES="E00043")

PROFILE_LIMIT = 10 # max payment profiles per user allowed by authorize.net

@export
class AuthorizeNetException(Exception):
    def __init__(self, msg):
        # don't let CC info show up in logs
        msg = re.sub("<cardNumber>\d+(\d{4})</cardNumber>", 
                     "<cardNumber>...\g<1></cardNumber>",
                     msg)
        msg = re.sub("<cardCode>\d+</cardCode>",
                     "<cardCode>omitted</cardCode>",
                     msg)
        super(AuthorizeNetException, self).__init__(msg)



# xml tags whose content shouldn't be escaped 
_no_escape_list = ["extraOptions"]


class SimpleXMLObject(object):
    """
    All API transactions are done with authorize.net using XML, so
    here's a class for generating and extracting structured data from
    XML.
    """
    _keys = []
    def __init__(self, **kw):
        self._used_keys = self._keys if self._keys else kw.keys()
        for k in self._used_keys:
            if not hasattr(self, k):
                setattr(self, k, kw.get(k, ""))

    @staticmethod
    def simple_tag(name, content, **attrs):
        attrs = " ".join('%s="%s"' % (k, v) for k, v in attrs.iteritems())
        if attrs:
            attrs = " " + attrs
        return ("<%(name)s%(attrs)s>%(content)s</%(name)s>" %
                dict(name=name, content=content, attrs=attrs))

    def toXML(self):
        content = []
        def process(k, v):
            if isinstance(v, SimpleXMLObject):
                v = v.toXML()
            elif v is not None:
                v = unicode(v)
                if k not in _no_escape_list:
                    v = escape(v) # escape &, <, and >
            if v is not None:
                content.append(self.simple_tag(k, v))

        for k in self._used_keys:
            v = getattr(self, k)
            if isinstance(v, iters):
                for val in v:
                    process(k, val)
            else:
                process(k, v)
        return self._wrapper("".join(content))

    @classmethod
    def fromXML(cls, data):
        kw = {}
        for k in cls._keys:
            d = data.find(k.lower())
            if d and d.contents:
                kw[k] = unicode(d.contents[0])
        return cls(**kw)


    def __repr__(self):
        return "<%s {%s}>" % (self.__class__.__name__,
                              ",".join("%s=%s" % (k, repr(getattr(self, k)))
                                       for k in self._used_keys))

    def _name(self):
        name = self.__class__.__name__
        return name[0].lower() + name[1:]
    
    def _wrapper(self, content):
        return content


class Auth(SimpleXMLObject):
    _keys = ["name", "transactionKey"]


@export
class Address(SimpleXMLObject):
    _keys = ["firstName", "lastName", "company", "address",
             "city", "state", "zip", "country", "phoneNumber",
             "faxNumber",
             "customerPaymentProfileId",
             "customerAddressId" ]
    def __init__(self, **kw):
        kw['customerPaymentProfileId'] = kw.get("customerPaymentProfileId",
                                                 None)
        kw['customerAddressId'] = kw.get("customerAddressId", None)
        SimpleXMLObject.__init__(self, **kw)


@export
class CreditCard(SimpleXMLObject):
    _keys = ["cardNumber", "expirationDate", "cardCode"]


class Profile(SimpleXMLObject):
    """
    Converts a user into a Profile object.
    """
    _keys = ["merchantCustomerId", "description",
             "email", "customerProfileId", "paymentProfiles", "shipToList",
             "validationMode"]
    def __init__(self, user, paymentProfiles, address,
                 validationMode=None):
        SimpleXMLObject.__init__(self, merchantCustomerId=user._fullname,
                                 description=user.name, email="",
                                 paymentProfiles=paymentProfiles,
                                 shipToList=address,
                                 validationMode=validationMode,
                                 customerProfileId=CustomerID.get_id(user))

class PaymentProfile(SimpleXMLObject):
    _keys = ["billTo", "payment", "customerPaymentProfileId", "validationMode"]
    def __init__(self, billTo, card, paymentId=None,
                 validationMode=None):
        SimpleXMLObject.__init__(self, billTo=billTo,
                                 customerPaymentProfileId=paymentId,
                                 payment=SimpleXMLObject(creditCard=card),
                                 validationMode=validationMode)

    @classmethod
    def fromXML(cls, res):
        payid = int(res.customerpaymentprofileid.contents[0])
        return cls(Address.fromXML(res.billto),
                   CreditCard.fromXML(res.payment), payid)


@export
class Order(SimpleXMLObject):
    _keys = ["invoiceNumber", "description", "purchaseOrderNumber"]


class Transaction(SimpleXMLObject):
    _keys = ["amount", "customerProfileId", "customerPaymentProfileId",
             "transId", "order"]

    def __init__(self, amount, profile_id, pay_id, trans_id=None,
                 order=None):
        SimpleXMLObject.__init__(self, amount=amount,
                                 customerProfileId=profile_id,
                                 customerPaymentProfileId=pay_id,
                                 transId=trans_id,
                                 order=order)

    def _wrapper(self, content):
        return self.simple_tag(self._name(), content)


# authorize and charge
@export
class ProfileTransAuthCapture(Transaction): pass


# only authorize (no charge is made)
@export
class ProfileTransAuthOnly(Transaction): pass


# charge only (requires previous auth_only)
@export
class ProfileTransPriorAuthCapture(Transaction): pass


# stronger than above: charge even on decline (not sure why you would want to)
@export
class ProfileTransCaptureOnly(Transaction): pass


# refund a transaction
@export
class ProfileTransRefund(Transaction): pass


# void a transaction
@export
class ProfileTransVoid(Transaction): pass


#-----
class AuthorizeNetRequest(SimpleXMLObject):
    _keys = ["merchantAuthentication"]

    @property
    def merchantAuthentication(self):
        return Auth(name=g.secrets['authorizenetname'],
                    transactionKey=g.secrets['authorizenetkey'])

    def _wrapper(self, content):
        return ('<?xml version="1.0" encoding="utf-8"?>' +
                self.simple_tag(self._name(), content,
                             xmlns="AnetApi/xml/v1/schema/AnetApiSchema.xsd"))

    def make_request(self):
        u = urlparse(g.authorizenetapi)
        conn = HTTPSConnection(u.hostname, u.port)
        conn.request("POST", u.path, self.toXML().encode('utf-8'),
                     {"Content-type": "text/xml"})
        res = conn.getresponse()
        res = self.handle_response(res.read())
        conn.close()
        return res

    def is_error_code(self, res, code):
        return (res.message.code and res.message.code.contents and
                res.message.code.contents[0] == code)


    def process_error(self, res):
        msg = "Response %r" % res
        raise AuthorizeNetException(msg)

    _autoclose_re = re.compile("<([^/]+)/>")
    def _autoclose_handler(self, m):
        return "<%(m)s></%(m)s>" % dict(m=m.groups()[0])

    def handle_response(self, res):
        res = self._autoclose_re.sub(self._autoclose_handler, res)
        res = BeautifulStoneSoup(res, 
                                 markupMassage=False, 
                                 convertEntities=BeautifulStoneSoup.XML_ENTITIES)
        if res.resultcode.contents[0] == u"Ok":
            return self.process_response(res)
        else:
            return self.process_error(res)

    def process_response(self, res):
        raise NotImplementedError

class CustomerRequest(AuthorizeNetRequest):
    _keys = AuthorizeNetRequest._keys + ["customerProfileId"]
    def __init__(self, user, **kw):
        if isinstance(user, int):
            cust_id = user
            self._user = None
        else:
            cust_id = CustomerID.get_id(user)
            self._user = user
        AuthorizeNetRequest.__init__(self, customerProfileId=cust_id, **kw)

# --- real request classes below


class CreateCustomerProfileRequest(AuthorizeNetRequest):
    """
    Create a new user object on authorize.net and return the new object ID.

    Handles the case of already existing users on either end
    gracefully and will update the Account object accordingly.
    """
    _keys = AuthorizeNetRequest._keys + ["profile", "validationMode"]

    def __init__(self, user, validationMode=None):
        # cache the user object passed in
        self._user = user
        AuthorizeNetRequest.__init__(self,
                                     profile=Profile(user, None, None), 
                                     validationMode=validationMode)

    def process_response(self, res):
        customer_id = int(res.customerprofileid.contents[0])
        CustomerID.set(self._user, customer_id)
        return customer_id

    def make_request(self):
        # don't send a new request if the user already has an id
        return (CustomerID.get_id(self._user) or
                AuthorizeNetRequest.make_request(self))

    re_lost_id = re.compile("A duplicate record with ID (\d+) already exists")
    def process_error(self, res):
        if self.is_error_code(res, Errors.DUPLICATE_RECORD):
            # authorize.net has a record for this customer but we don't. get
            # the correct id from the error message and update our db
            matches = self.re_lost_id.match(res.find("text").contents[0])
            if matches:
                match_groups = matches.groups()
                CustomerID.set(self._user, match_groups[0])
                g.log.debug("Updated missing authorize.net id for user %s" % self._user._id)
            else:
                # could happen if the format of the error message changes.
                msg = ("Failed to fix duplicate authorize.net profile id. "
                       "re_lost_id regexp may need to be updated. Response: %r" 
                       % res)
                raise AuthorizeNetException(msg)
        # otherwise, we might have sent a user that already had a customer ID
        cust_id = CustomerID.get_id(self._user)
        if cust_id:
            return cust_id
        return AuthorizeNetRequest.process_error(self, res)


class CreateCustomerPaymentProfileRequest(CustomerRequest):
    """
    Adds a payment profile to an existing user object.  The profile
    includes a valid address and a credit card number.
    """
    _keys = (CustomerRequest._keys + ["paymentProfile", "validationMode"])

    def __init__(self, user, address, creditcard, validationMode=None):
        CustomerRequest.__init__(self, user,
                                 paymentProfile=PaymentProfile(address,
                                                               creditcard),
                                 validationMode=validationMode)

    def process_response(self, res):
        pay_id = int(res.customerpaymentprofileid.contents[0])
        PayID.add(self._user, pay_id)
        return pay_id

    def process_error(self, res):
        if self.is_error_code(res, Errors.DUPLICATE_RECORD):
            u, data = GetCustomerProfileRequest(self._user).make_request()
            profiles = data.paymentProfiles
            if len(profiles) == 1:
                return profiles[0].customerPaymentProfileId
            return
        return CustomerRequest.process_error(self, res)


class CreateCustomerShippingAddressRequest(CustomerRequest):
    """
    Adds a shipping address.
    """
    _keys = CustomerRequest._keys + ["address"]
    def process_response(self, res):
        pay_id = int(res.customeraddressid.contents[0])
        ShippingAddress.add(self._user, pay_id)
        return pay_id

    def process_error(self, res):
        if self.is_error_code(res, Errors.DUPLICATE_RECORD):
            return
        return CustomerRequest.process_error(self, res)


class GetCustomerPaymentProfileRequest(CustomerRequest):
    _keys = CustomerRequest._keys + ["customerPaymentProfileId"]
    """
    Gets a payment profile by user Account object and authorize.net
    profileid of the payment profile.

    Error handling: make_request returns None if the id generates a
    RECORD_NOT_FOUND error from the server.  The user object is
    cleaned up in either case; if the user object lacked the (valid)
    pay id, it is added to its list, while if the pay id is invalid,
    it is removed from the user object.
    """
    def __init__(self, user, profileid):
        CustomerRequest.__init__(self, user,
                                 customerPaymentProfileId=profileid)
    def process_response(self, res):
        # add the id to the user object in case something has gone wrong
        PayID.add(self._user, self.customerPaymentProfileId)
        return PaymentProfile.fromXML(res.paymentprofile)

    def process_error(self, res):
        if self.is_error_code(res, Errors.RECORD_NOT_FOUND):
            PayID.delete(self._user, self.customerPaymentProfileId)
        return CustomerRequest.process_error(self, res)


class GetCustomerShippingAddressRequest(CustomerRequest):
    """
    Same as GetCustomerPaymentProfileRequest except with shipping addresses.

    Error handling is identical.
    """
    _keys = CustomerRequest._keys + ["customerAddressId"]
    def __init__(self, user, shippingid):
        CustomerRequest.__init__(self, user,
                                 customerAddressId=shippingid)

    def process_response(self, res):
        # add the id to the user object in case something has gone wrong
        ShippingAddress.add(self._user, self.customerAddressId)
        return Address.fromXML(res.address)

    def process_error(self, res):
        if self.is_error_code(res, Errors.RECORD_NOT_FOUND):
            ShippingAddress.delete(self._user, self.customerAddressId)
        return CustomerRequest.process_error(self, res)
 

class GetCustomerProfileIdsRequest(AuthorizeNetRequest):
    """
    Get a list of all customer ids that have been recorded with
    authorize.net
    """
    def process_response(self, res):
        return [int(x.contents[0]) for x in res.ids.findAll('numericstring')]


class GetCustomerProfileRequest(CustomerRequest): 
    """
    Given a user, find their customer information.
    """
    def process_response(self, res):
        from r2.models import Account
        fullname = res.merchantcustomerid.contents[0]
        name = res.description.contents[0]
        customer_id = int(res.customerprofileid.contents[0])
        acct = Account._by_name(name)

        # make sure we are updating the correct account!
        if acct.name == name:
            CustomerID.set(acct, customer_id)
        else:
            raise AuthorizeNetException, \
                  "account name doesn't match authorize.net account"

        # parse the ship-to list, and make sure the Account is up todate
        ship_to = []
        for profile in res.findAll("shiptolist"):
            a = Address.fromXML(profile)
            ShippingAddress.add(acct, a.customerAddressId)
            ship_to.append(a)

        # parse the payment profiles, and ditto
        profiles = []
        for profile in res.findAll("paymentprofiles"):
            a = Address.fromXML(profile)
            cc = CreditCard.fromXML(profile.payment)
            payprof = PaymentProfile(a, cc, int(a.customerPaymentProfileId))
            PayID.add(acct, a.customerPaymentProfileId)
            profiles.append(payprof)

        return acct, Profile(acct, profiles, ship_to)
    
class DeleteCustomerProfileRequest(CustomerRequest):
    """
    Delete a customer shipping address
    """
    def process_response(self, res):
        if self._user:
            CustomerID.delete(self._user)
        return 

    def process_error(self, res):
        if self.is_error_code(res, Errors.RECORD_NOT_FOUND):
            CustomerID.delete(self._user)
        return CustomerRequest.process_error(self, res)


class DeleteCustomerPaymentProfileRequest(GetCustomerPaymentProfileRequest):
    """
    Delete a customer shipping address
    """
    def process_response(self, res):
        PayID.delete(self._user, self.customerPaymentProfileId)
        return True

    def process_error(self, res):
        if self.is_error_code(res, Errors.RECORD_NOT_FOUND):
            PayID.delete(self._user, self.customerPaymentProfileId)
        return GetCustomerPaymentProfileRequest.process_error(self, res)


class DeleteCustomerShippingAddressRequest(GetCustomerShippingAddressRequest):
    """
    Delete a customer shipping address
    """
    def process_response(self, res):
        ShippingAddress.delete(self._user, self.customerAddressId)
        return True

    def process_error(self, res):
        if self.is_error_code(res, Errors.RECORD_NOT_FOUND):
            ShippingAddress.delete(self._user, self.customerAddressId)
        GetCustomerShippingAddressRequest.process_error(self, res)


class UpdateCustomerPaymentProfileRequest(CreateCustomerPaymentProfileRequest):
    """
    For updating the user's payment profile
    """
    def __init__(self, user, paymentid, address, creditcard, 
                 validationMode=None):
        CustomerRequest.__init__(self, user,
                                 paymentProfile=PaymentProfile(address,
                                                               creditcard,
                                                               paymentid),
                                 validationMode=validationMode)

    def process_response(self, res):
        return self.paymentProfile.customerPaymentProfileId


class UpdateCustomerShippingAddressRequest(
    CreateCustomerShippingAddressRequest):
    """
    For updating the user's shipping address
    """
    def __init__(self, user, address_id, address):
        address.customerAddressId = address_id
        CreateCustomerShippingAddressRequest.__init__(self, user,
                                                      address=address)

    def process_response(self, res):
        return True


class CreateCustomerProfileTransactionRequest(AuthorizeNetRequest):
    _keys = AuthorizeNetRequest._keys + ["transaction", "extraOptions"]

    # unlike every other response we get back, this api function
    # returns CSV data of the response with no field labels.  these
    # are used in package_response to zip this data into a usable
    # storage.
    response_keys = ("response_code",
                     "response_subcode",
                     "response_reason_code",
                     "response_reason_text",
                     "authorization_code",
                     "avs_response",
                     "trans_id",
                     "invoice_number",
                     "description",
                     "amount", "method",
                     "transaction_type",
                     "customerID",
                     "firstName", "lastName",
                     "company", "address", "city", "state",
                     "zip", "country", 
                     "phoneNumber", "faxNumber", "email",
                     "shipTo_firstName", "shipTo_lastName",
                     "shipTo_company", "shipTo_address",
                     "shipTo_city", "shipTo_state",
                     "shipTo_zip", "shipTo_country",
                     "tax", "duty", "freight",
                     "tax_exempt", "po_number", "md5",
                     "cav_response")

    # list of casts for the response fields given above
    response_types = dict(response_code=int,
                          response_subcode=int,
                          response_reason_code=int,
                          trans_id=int)

    def __init__(self, **kw):
        from pylons import g
        self._extra = kw.get("extraOptions", {})
        #if g.debug:
        #    self._extra['x_test_request'] = "TRUE"
        AuthorizeNetRequest.__init__(self, **kw)

    @property
    def extraOptions(self):
        return "<![CDATA[%s]]>" % "&".join("%s=%s" % x
                                            for x in self._extra.iteritems())

    def process_response(self, res):
        return (True, self.package_response(res))

    def process_error(self, res):
        if self.is_error_code(res, Errors.TRANSACTION_FAIL):
            return (False, self.package_response(res))
        elif self.is_error_code(res, Errors.TESTMODE):
            return (None, None)
        return AuthorizeNetRequest.process_error(self, res)


    def package_response(self, res):
        content = res.directresponse.contents[0]
        s = Storage(zip(self.response_keys, content.split(',')))
        for name, cast in self.response_types.iteritems():
            try:
                s[name] = cast(s[name])
            except ValueError:
                pass
        return s


class GetSettledBatchListRequest(AuthorizeNetRequest):
    _keys = AuthorizeNetRequest._keys + ["includeStatistics", 
                                         "firstSettlementDate", 
                                         "lastSettlementDate"]
    def __init__(self, start_date, end_date, **kw):
        AuthorizeNetRequest.__init__(self, 
                                     includeStatistics=1,
                                     firstSettlementDate=start_date.isoformat(),
                                     lastSettlementDate=end_date.isoformat(),
                                     **kw)

    def process_response(self, res):
        return res


########NEW FILE########
__FILENAME__ = interaction
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from sqlalchemy.orm.exc import MultipleResultsFound

from pylons import g

from r2.lib.db.thing import NotFound
from r2.lib.utils import Storage
from r2.lib.export import export
from r2.models.bidding import Bid, CustomerID, PayID

from r2.lib.authorize.api import (
    Address,
    AuthorizeNetException,
    CreateCustomerPaymentProfileRequest,
    CreateCustomerProfileRequest,
    CreateCustomerProfileTransactionRequest,
    CreditCard,
    GetCustomerProfileRequest,
    Order,
    ProfileTransAuthOnly,
    ProfileTransPriorAuthCapture,
    ProfileTransRefund,
    ProfileTransVoid,
    UpdateCustomerPaymentProfileRequest,
)

__all__ = ['TRANSACTION_NOT_FOUND']

TRANSACTION_NOT_FOUND = 16

# useful test data:
test_card = dict(AMEX       = ("370000000000002"  , 1234),
                 DISCOVER   = ("6011000000000012" , 123),
                 MASTERCARD = ("5424000000000015" , 123),
                 VISA       = ("4007000000027"    , 123),
                 # visa card which generates error codes based on the amount
                 ERRORCARD  = ("4222222222222"    , 123))

test_card = Storage((k, CreditCard(cardNumber=x,
                                   expirationDate="2011-11",
                                   cardCode=y)) for k, (x, y) in
                    test_card.iteritems())

test_address = Address(firstName="John",
                       lastName="Doe",
                       address="123 Fake St.",
                       city="Anytown",
                       state="MN",
                       zip="12346")


@export
def get_account_info(user, recursed=False): 
    # if we don't have an ID for the user, try to make one
    if not CustomerID.get_id(user):
        cust_id = CreateCustomerProfileRequest(user).make_request()

    # if we do have a customerid, we should be able to fetch it from authorize
    try:
        u, data = GetCustomerProfileRequest(user).make_request()
    except AuthorizeNetException:
        u = None

    # if the user and the returned user don't match, delete the
    # current customer_id and recurse
    if u != user:
        if not recursed:
            CustomerID.delete(user)
            return get_account_info(user, True)
        else:
            raise AuthorizeNetException, "error creating user"
    return data


@export
def edit_profile(user, address, creditcard, pay_id=None):
    if pay_id:
        request = UpdateCustomerPaymentProfileRequest(user, pay_id, address,
                                                      creditcard)
    else:
        request = CreateCustomerPaymentProfileRequest(user, address, creditcard)

    try:
        pay_id = request.make_request()
        return pay_id
    except AuthorizeNetException:
        return None


def _make_transaction(trans_cls, amount, user, pay_id,
                      order=None, trans_id=None, test=None):
    """
    private function for handling transactions (since the data is
    effectively the same regardless of trans_cls)
    """
    # format the amount
    if amount:
        amount = "%.2f" % amount
    # lookup customer ID
    cust_id = CustomerID.get_id(user)
    # create a new transaction
    trans = trans_cls(amount, cust_id, pay_id, trans_id=trans_id,
                      order=order)
    extra = {}
    # the optional test field makes the transaction a test, and will
    # make the response be the error code corresponding to int(test).
    if isinstance(test, int):
        extra = dict(x_test_request="TRUE",
                     x_card_num=test_card.ERRORCARD.cardNumber,
                     x_amount=test)

    # using the transaction, generate a transaction request and make it
    req = CreateCustomerProfileTransactionRequest(transaction=trans,
                                                  extraOptions=extra)
    return req.make_request()


@export
def auth_transaction(amount, user, payid, thing, campaign):
    # use negative pay_ids to identify freebies, coupons, or anything
    # that doesn't require a CC.
    if payid < 0:
        trans_id = -thing._id
        # update previous freebie transactions if we can
        try:
            bid = Bid.one(thing_id=thing._id,
                          transaction=trans_id,
                          campaign=campaign)
            bid.bid = amount
            bid.auth()
        except NotFound:
            bid = Bid._new(trans_id, user, payid, thing._id, amount, campaign)
        return bid.transaction, ""

    elif int(payid) in PayID.get_ids(user):
        order = Order(invoiceNumber="T%dC%d" % (thing._id, campaign))
        success, res = _make_transaction(ProfileTransAuthOnly,
                                         amount, user, payid,
                                         order=order)
        if success:
            Bid._new(res.trans_id, user, payid, thing._id, amount, campaign)
            return res.trans_id, ""

        elif (res.trans_id and
              (res.response_code, res.response_reason_code) == (3, 11)):
            # duplicate transaction, which is bad, but not horrible.  Log
            # the transaction id, creating a new bid if necessary.
            g.log.error("Authorize.net duplicate trans %d on campaign %d" % 
                        (res.trans_id, campaign))
            try:
                Bid.one(res.trans_id, campaign=campaign)
            except NotFound:
                Bid._new(res.trans_id, user, payid, thing._id, amount, campaign)

        return res.trans_id, res.response_reason_text


@export
def void_transaction(user, trans_id, campaign, test=None):
    bid =  Bid.one(transaction=trans_id, campaign=campaign)
    bid.void()
    if trans_id > 0:
        res = _make_transaction(ProfileTransVoid,
                                None, user, None, trans_id=trans_id,
                                test=test)
        return res


@export
def is_charged_transaction(trans_id, campaign):
    if not trans_id: return False # trans_id == 0 means no bid
    try:
        bid = Bid.one(transaction=trans_id, campaign=campaign)
    except NotFound:
        return False
    except MultipleResultsFound:
        g.log.error('Multiple bids for trans_id %s' % trans_id)
        return False

    return bid.is_charged() or bid.is_refund()


@export
def charge_transaction(user, trans_id, campaign, test=None):
    bid = Bid.one(transaction=trans_id, campaign=campaign)
    if bid.is_charged():
        return True

    if trans_id < 0:
        success = True
        response_reason_code = None
    else:
        success, res = _make_transaction(ProfileTransPriorAuthCapture,
                                         bid.bid, user,
                                         bid.pay_id, trans_id=trans_id,
                                         test=test)
        response_reason_code = res.get("response_reason_code")

    if success:
        bid.charged()
    elif response_reason_code == TRANSACTION_NOT_FOUND:
        # authorization hold has expired
        bid.void()

    return success, response_reason_code


@export
def refund_transaction(user, trans_id, campaign_id, amount, test=None):
    # refund will only work if charge has settled
    bid =  Bid.one(transaction=trans_id, campaign=campaign_id)
    if trans_id < 0:
        bid.refund(amount)
        return True
    else:
        success, res = _make_transaction(ProfileTransRefund, amount, user,
                                         bid.pay_id, trans_id=trans_id,
                                         test=test)
        if success:
            bid.refund(amount)
        elif success == False:
            msg = "Refund failed, response: %r" % res
            raise AuthorizeNetException(msg)
        return True

########NEW FILE########
__FILENAME__ = base
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import c, g, request, session, config, response
from pylons.controllers import WSGIController
from pylons.i18n import N_, _, ungettext, get_lang
from webob.exc import HTTPException, status_map
from r2.lib.filters import spaceCompress, _force_unicode
from r2.lib.template_helpers import get_domain
from utils import string2js, read_http_date

import re, hashlib
from urllib import quote
import urllib2
import sys


#TODO hack
import logging
from r2.lib.utils import UrlParser, query_string
logging.getLogger('scgi-wsgi').setLevel(logging.CRITICAL)


def is_local_address(ip):
    # TODO: support the /20 and /24 private networks? make this configurable?
    return ip.startswith('10.') or ip == "127.0.0.1"

def abort(code_or_exception=None, detail="", headers=None, comment=None,
          **kwargs):
    """Raise an HTTPException and save it in environ for use by error pages."""
    # Pylons 0.9.6 makes it really hard to get your raised HTTPException,
    # so this helper implements it manually using a familiar syntax.
    # FIXME: when we upgrade Pylons, we can replace this with raise
    #        and access environ['pylons.controller.exception']
    # NOTE: when we say "upgrade Pylons" we mean to 0.10+
    if isinstance(code_or_exception, HTTPException):
        exc = code_or_exception
    else:
        if type(code_or_exception) is type and issubclass(code_or_exception,
                                                          HTTPException):
            exc_cls = code_or_exception
        else:
            exc_cls = status_map[code_or_exception]
        exc = exc_cls(detail, headers, comment, **kwargs)
    request.environ['r2.controller.exception'] = exc
    raise exc

class BaseController(WSGIController):
    def try_pagecache(self):
        pass

    def __before__(self):
        self.pre()
        self.try_pagecache()

    def __after__(self):
        self.post()

    def __call__(self, environ, start_response):
        # we override this here to ensure that this header, and only this
        # header, is trusted to reduce the number of potential
        # misconfigurations between wsgi application servers (e.g. gunicorn
        # which trusts three different headers out of the box for this) and
        # haproxy (which won't clean out bad headers by default)
        forwarded_proto = environ.get("HTTP_X_FORWARDED_PROTO", "http").lower()
        assert forwarded_proto in ("http", "https")
        request.environ["wsgi.url_scheme"] = forwarded_proto

        true_client_ip = environ.get('HTTP_TRUE_CLIENT_IP')
        ip_hash = environ.get('HTTP_TRUE_CLIENT_IP_HASH')
        forwarded_for = environ.get('HTTP_X_FORWARDED_FOR', ())
        remote_addr = environ.get('REMOTE_ADDR')

        request.via_cdn = False
        if (g.secrets["true_ip"]
            and true_client_ip
            and ip_hash
            and hashlib.md5(true_client_ip + g.secrets["true_ip"]).hexdigest() \
            == ip_hash.lower()):
            request.ip = true_client_ip
            request.via_cdn = True
        elif g.trust_local_proxies and forwarded_for and is_local_address(remote_addr):
            request.ip = forwarded_for.split(',')[-1]
        else:
            request.ip = environ['REMOTE_ADDR']

        #if x-dont-decode is set, pylons won't unicode all the paramters
        if environ.get('HTTP_X_DONT_DECODE'):
            request.charset = None

        request.referer = environ.get('HTTP_REFERER')
        request.user_agent = environ.get('HTTP_USER_AGENT')
        request.fullpath = environ.get('FULLPATH', request.path)
        request.fullurl = request.host_url + request.fullpath
        request.port = environ.get('request_port')
        
        if_modified_since = environ.get('HTTP_IF_MODIFIED_SINCE')
        if if_modified_since:
            request.if_modified_since = read_http_date(if_modified_since)
        else:
            request.if_modified_since = None

        #set the function to be called
        action = request.environ['pylons.routes_dict'].get('action')
        if action:
            meth = request.method.upper()
            if meth == 'HEAD':
                meth = 'GET'

            if meth != 'OPTIONS':
                handler_name = meth + '_' + action
            else:
                handler_name = meth

            request.environ['pylons.routes_dict']['action_name'] = action
            request.environ['pylons.routes_dict']['action'] = handler_name

        return WSGIController.__call__(self, environ, start_response)

    def pre(self): pass
    def post(self): pass

    def _get_action_handler(self, name=None, method=None):
        name = name or request.environ["pylons.routes_dict"]["action_name"]
        method = method or request.method
        action = method + "_" + name
        return getattr(self, action, None)

    @classmethod
    def format_output_url(cls, url, **kw):
        """
        Helper method used during redirect to ensure that the redirect
        url (assisted by frame busting code or javasctipt) will point
        to the correct domain and not have any extra dangling get
        parameters.  The extensions are also made to match and the
        resulting url is utf8 encoded.

        Node: for development purposes, also checks that the port
        matches the request port
        """
        u = UrlParser(url)

        if u.is_reddit_url():
            # make sure to pass the port along if not 80
            if not kw.has_key('port'):
                kw['port'] = request.port

            # disentagle the cname (for urls that would have
            # cnameframe=1 in them)
            u.mk_cname(**kw)

            # make sure the extensions agree with the current page
            if c.extension:
                u.set_extension(c.extension)

        # unparse and encode it un utf8
        rv = _force_unicode(u.unparse()).encode('utf8')
        if "\n" in rv or "\r" in rv:
            abort(400)
        return rv


    @classmethod
    def intermediate_redirect(cls, form_path, sr_path=True):
        """
        Generates a /login or /over18 redirect from the current
        fullpath, after having properly reformated the path via
        format_output_url.  The reformatted original url is encoded
        and added as the "dest" parameter of the new url.
        """
        from r2.lib.template_helpers import add_sr
        params = dict(dest=cls.format_output_url(request.fullurl))
        if c.extension == "widget" and request.GET.get("callback"):
            params['callback'] = request.GET.get("callback")

        path = add_sr(cls.format_output_url(form_path) +
                      query_string(params), sr_path=sr_path)
        abort(302, location=path)

    @classmethod
    def redirect(cls, dest, code = 302):
        """
        Reformats the new Location (dest) using format_output_url and
        sends the user to that location with the provided HTTP code.
        """
        dest = cls.format_output_url(dest or "/")
        response.status_int = code
        response.headers['Location'] = dest

class EmbedHandler(urllib2.BaseHandler, urllib2.HTTPHandler,
                   urllib2.HTTPErrorProcessor, urllib2.HTTPDefaultErrorHandler):

    def http_redirect(self, req, fp, code, msg, hdrs):
        to = hdrs['Location']
        h = urllib2.HTTPRedirectHandler()
        r = h.redirect_request(req, fp, code, msg, hdrs, to)
        return embedopen.open(r)

    http_error_301 = http_redirect
    http_error_302 = http_redirect
    http_error_303 = http_redirect
    http_error_307 = http_redirect

embedopen = urllib2.OpenerDirector()
embedopen.add_handler(EmbedHandler())

def proxyurl(url):
    r = urllib2.Request(url, None, {})
    content = embedopen.open(r).read()
    return content


########NEW FILE########
__FILENAME__ = butler
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import g, c

from r2.lib.db import queries
from r2.lib import amqp
from r2.lib.utils import extract_urls_from_markdown
from r2.lib.validator import chkuser
from r2.models import query_cache, Thing, Comment, Account, Inbox, NotFound


def extract_user_mentions(text):
    for url in extract_urls_from_markdown(text):
        if not url.startswith("/u/"):
            continue

        username = url[len("/u/"):]
        if chkuser(username):
            yield username.lower()


def notify_mention(user, thing):
    inbox_rel = Inbox._add(user, thing, "mention")
    with query_cache.CachedQueryMutator() as m:
        m.insert(queries.get_inbox_comment_mentions(user), [inbox_rel])
        queries.set_unread(thing, user, unread=True, mutator=m)


def monitor_mentions(comment):
    if not isinstance(comment, Comment):
        return

    if comment._spam or comment._deleted:
        return

    sender = comment.author_slow
    if getattr(sender, "butler_ignore", False):
        # this is an account that generates false notifications, e.g.
        # LinkFixer
        return

    subreddit = comment.subreddit_slow
    usernames = list(extract_user_mentions(comment.body))
    inbox_class = Inbox.rel(Account, Comment)

    # don't be a jerk spammer
    if len(usernames) > 3:
        return

    # Subreddit.can_view stupidly requires this.
    c.user_is_loggedin = True

    for username in usernames:
        try:
            account = Account._by_name(username)
        except NotFound:
            continue

        # most people are aware of when they mention themselves.
        if account == sender:
            continue

        # bail out if that user doesn't have gold or has the feature turned off
        if not account.gold or not account.pref_monitor_mentions:
            continue

        # don't notify users of things they can't see
        if not subreddit.can_view(account):
            continue

        # don't notify users when a person they've blocked mentions them
        if account.is_enemy(sender):
            continue

        # ensure this comment isn't already in the user's inbox already
        rels = inbox_class._fast_query(
            account,
            comment,
            ("inbox", "selfreply", "mention"),
        )
        if filter(None, rels.values()):
            continue

        notify_mention(account, comment)


def run():
    @g.stats.amqp_processor("butler_q")
    def process_message(msg):
        fname = msg.body
        item = Thing._by_fullname(fname, data=True)
        monitor_mentions(item)

    amqp.consume_items("butler_q",
                       process_message,
                       verbose=True)

########NEW FILE########
__FILENAME__ = cache
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from threading import local
from hashlib import md5
import cPickle as pickle
from copy import copy

import pylibmc
from _pylibmc import MemcachedError

from pycassa import ColumnFamily
from pycassa.cassandra.ttypes import ConsistencyLevel
from pycassa.cassandra.ttypes import NotFoundException as CassandraNotFound

from r2.lib.utils import in_chunks, prefix_keys, trace
from r2.lib.hardcachebackend import HardCacheBackend

from r2.lib.sgm import sgm # get this into our namespace so that it's
                           # importable from us

# This is for use in the health controller
_CACHE_SERVERS = set()

class NoneResult(object): pass

class CacheUtils(object):
    # Caches that never expire entries should set this to true, so that
    # CacheChain can properly count hits and misses.
    permanent = False

    def incr_multi(self, keys, delta=1, prefix=''):
        for k in keys:
            try:
                self.incr(prefix + k, delta)
            except ValueError:
                pass

    def add_multi(self, keys, prefix='', time=0):
        for k,v in keys.iteritems():
            self.add(prefix+str(k), v, time = time)

    def get_multi(self, keys, prefix='', **kw):
        return prefix_keys(keys, prefix, lambda k: self.simple_get_multi(k, **kw))

class CMemcache(CacheUtils):
    def __init__(self,
                 servers,
                 debug = False,
                 noreply = False,
                 no_block = False,
                 min_compress_len=512 * 1024,
                 num_clients = 10):
        self.servers = servers
        self.clients = pylibmc.ClientPool(n_slots = num_clients)
        for x in xrange(num_clients):
            client = pylibmc.Client(servers, binary=True)
            behaviors = {
                'no_block': no_block, # use async I/O
                'tcp_nodelay': True, # no nagle
                '_noreply': int(noreply),
                'ketama': True, # consistent hashing
                }

            client.behaviors.update(behaviors)
            self.clients.put(client)

        self.min_compress_len = min_compress_len

        _CACHE_SERVERS.update(servers)

    def get(self, key, default = None):
        with self.clients.reserve() as mc:
            ret =  mc.get(key)
            if ret is None:
                return default
            return ret

    def get_multi(self, keys, prefix = ''):
        with self.clients.reserve() as mc:
            return mc.get_multi(keys, key_prefix = prefix)

    # simple_get_multi exists so that a cache chain can
    # single-instance the handling of prefixes for performance, but
    # pylibmc does this in C which is faster anyway, so CMemcache
    # implements get_multi itself. But the CacheChain still wants
    # simple_get_multi to be available for when it's already prefixed
    # them, so here it is
    simple_get_multi = get_multi

    def set(self, key, val, time = 0):
        with self.clients.reserve() as mc:
            return mc.set(key, val, time = time,
                          min_compress_len = self.min_compress_len)

    def set_multi(self, keys, prefix='', time=0):
        new_keys = {}
        for k,v in keys.iteritems():
            new_keys[str(k)] = v
        with self.clients.reserve() as mc:
            return mc.set_multi(new_keys, key_prefix = prefix,
                                time = time,
                                min_compress_len = self.min_compress_len)

    def add_multi(self, keys, prefix='', time=0):
        new_keys = {}
        for k,v in keys.iteritems():
            new_keys[str(k)] = v
        with self.clients.reserve() as mc:
            return mc.add_multi(new_keys, key_prefix = prefix,
                                time = time)

    def incr_multi(self, keys, prefix='', delta=1):
        with self.clients.reserve() as mc:
            return mc.incr_multi(map(str, keys),
                                 key_prefix = prefix,
                                 delta=delta)

    def append(self, key, val, time=0):
        with self.clients.reserve() as mc:
            return mc.append(key, val, time=time)

    def incr(self, key, delta=1, time=0):
        # ignore the time on these
        with self.clients.reserve() as mc:
            return mc.incr(key, delta)

    def add(self, key, val, time=0):
        try:
            with self.clients.reserve() as mc:
                return mc.add(key, val, time=time)
        except pylibmc.DataExists:
            return None

    def delete(self, key, time=0):
        with self.clients.reserve() as mc:
            return mc.delete(key)

    def delete_multi(self, keys, prefix=''):
        with self.clients.reserve() as mc:
            return mc.delete_multi(keys, key_prefix=prefix)

    def __repr__(self):
        return '<%s(%r)>' % (self.__class__.__name__,
                             self.servers)

class HardCache(CacheUtils):
    backend = None
    permanent = True

    def __init__(self, gc):
        self.backend = HardCacheBackend(gc)

    def _split_key(self, key):
        tokens = key.split("-", 1)
        if len(tokens) != 2:
            raise ValueError("key %s has no dash" % key)

        category, ids = tokens
        return category, ids

    def set(self, key, val, time=0):
        if val == NoneResult:
            # NoneResult caching is for other parts of the chain
            return

        category, ids = self._split_key(key)
        self.backend.set(category, ids, val, time)

    def simple_get_multi(self, keys):
        results = {}
        category_bundles = {}
        for key in keys:
            category, ids = self._split_key(key)
            category_bundles.setdefault(category, []).append(ids)

        for category in category_bundles:
            idses = category_bundles[category]
            chunks = in_chunks(idses, size=50)
            for chunk in chunks:
                new_results = self.backend.get_multi(category, chunk)
                results.update(new_results)

        return results

    def set_multi(self, keys, prefix='', time=0):
        for k,v in keys.iteritems():
            if v != NoneResult:
                self.set(prefix+str(k), v, time=time)

    def get(self, key, default=None):
        category, ids = self._split_key(key)
        r = self.backend.get(category, ids)
        if r is None: return default
        return r

    def delete(self, key, time=0):
        # Potential optimization: When on a negative-result caching chain,
        # shove NoneResult throughout the chain when a key is deleted.
        category, ids = self._split_key(key)
        self.backend.delete(category, ids)

    def add(self, key, value, time=0):
        category, ids = self._split_key(key)
        return self.backend.add(category, ids, value, time=time)

    def incr(self, key, delta=1, time=0):
        category, ids = self._split_key(key)
        return self.backend.incr(category, ids, delta=delta, time=time)


class LocalCache(dict, CacheUtils):
    def __init__(self, *a, **kw):
        return dict.__init__(self, *a, **kw)

    def _check_key(self, key):
        if isinstance(key, unicode):
            key = str(key) # try to convert it first
        if not isinstance(key, str):
            raise TypeError('Key is not a string: %r' % (key,))

    def get(self, key, default=None):
        r = dict.get(self, key)
        if r is None: return default
        return r

    def simple_get_multi(self, keys):
        out = {}
        for k in keys:
            if self.has_key(k):
                out[k] = self[k]
        return out

    def set(self, key, val, time = 0):
        # time is ignored on localcache
        self._check_key(key)
        self[key] = val

    def set_multi(self, keys, prefix='', time=0):
        for k,v in keys.iteritems():
            self.set(prefix+str(k), v, time=time)

    def add(self, key, val, time = 0):
        self._check_key(key)
        was = key in self
        self.setdefault(key, val)
        return not was

    def delete(self, key):
        if self.has_key(key):
            del self[key]

    def delete_multi(self, keys):
        for key in keys:
            if self.has_key(key):
                del self[key]

    def incr(self, key, delta=1, time=0):
        if self.has_key(key):
            self[key] = int(self[key]) + delta

    def decr(self, key, amt=1):
        if self.has_key(key):
            self[key] = int(self[key]) - amt

    def append(self, key, val, time = 0):
        if self.has_key(key):
            self[key] = str(self[key]) + val

    def prepend(self, key, val, time = 0):
        if self.has_key(key):
            self[key] = val + str(self[key])

    def replace(self, key, val, time = 0):
        if self.has_key(key):
            self[key] = val

    def flush_all(self):
        self.clear()

    def __repr__(self):
        return "<LocalCache(%d)>" % (len(self),)


class TransitionalCache(CacheUtils):
    """A cache "chain" for moving keys to a new cluster live.

    `original_cache` is the cache chain previously in use (this'll frequently
    be `g.cache` since it's the catch-all for most things) and
    `replacement_cache` is the new place for the keys using this chain to live.

    To use this cache chain, do three separate deployments as follows:

        * start dual-writing to the new pool by putting this chain in place
          with `read_original=True`.
        * cut reads over to the new pool after it is sufficiently heated up by
          deploying `read_original=False`.
        * remove this cache chain entirely and replace it with
          `replacement_cache`.

    This ensures that at any point, all apps regardless of their position in
    the push order will have a consistent view of the data in the cache pool as
    much as is possible.

    """

    def __init__(self, original_cache, replacement_cache, read_original):
        self.original = original_cache
        self.replacement = replacement_cache
        self.read_original = read_original

    @property
    def stats(self):
        if self.read_original:
            return self.original.stats
        else:
            return self.replacement.stats

    def make_get_fn(fn_name):
        def transitional_cache_get_fn(self, *args, **kwargs):
            if self.read_original:
                return getattr(self.original, fn_name)(*args, **kwargs)
            else:
                return getattr(self.replacement, fn_name)(*args, **kwargs)
        return transitional_cache_get_fn

    get = make_get_fn("get")
    get_multi = make_get_fn("get_multi")
    simple_get_multi = make_get_fn("simple_get_multi")

    def make_set_fn(fn_name):
        def transitional_cache_set_fn(self, *args, **kwargs):
            getattr(self.original, fn_name)(*args, **kwargs)
            getattr(self.replacement, fn_name)(*args, **kwargs)
        return transitional_cache_set_fn

    add = make_set_fn("add")
    set = make_set_fn("set")
    append = make_set_fn("append")
    prepend = make_set_fn("prepend")
    replace = make_set_fn("replace")
    set_multi = make_set_fn("set_multi")
    add = make_set_fn("add")
    add_multi = make_set_fn("add_multi")
    incr = make_set_fn("incr")
    incr_multi = make_set_fn("incr_multi")
    decr = make_set_fn("decr")
    delete = make_set_fn("delete")
    delete_multi = make_set_fn("delete_multi")
    flush_all = make_set_fn("flush_all")
    reset = make_set_fn("reset")


class CacheChain(CacheUtils, local):
    def __init__(self, caches, cache_negative_results=False):
        self.caches = caches
        self.cache_negative_results = cache_negative_results
        self.stats = None

    def make_set_fn(fn_name):
        def fn(self, *a, **kw):
            ret = None
            for c in self.caches:
                ret = getattr(c, fn_name)(*a, **kw)
            return ret
        return fn

    # note that because of the naive nature of `add' when used on a
    # cache chain, its return value isn't reliable. if you need to
    # verify its return value you'll either need to make it smarter or
    # use the underlying cache directly
    add = make_set_fn('add')

    set = make_set_fn('set')
    append = make_set_fn('append')
    prepend = make_set_fn('prepend')
    replace = make_set_fn('replace')
    set_multi = make_set_fn('set_multi')
    add = make_set_fn('add')
    add_multi = make_set_fn('add_multi')
    incr = make_set_fn('incr')
    incr_multi = make_set_fn('incr_multi')
    decr = make_set_fn('decr')
    delete = make_set_fn('delete')
    delete_multi = make_set_fn('delete_multi')
    flush_all = make_set_fn('flush_all')
    cache_negative_results = False

    def get(self, key, default = None, allow_local = True, stale=None):
        stat_outcome = False  # assume a miss until a result is found
        try:
            for c in self.caches:
                if not allow_local and isinstance(c,LocalCache):
                    continue

                val = c.get(key)

                if val is not None:
                    if not c.permanent:
                        stat_outcome = True

                    #update other caches
                    for d in self.caches:
                        if c is d:
                            break # so we don't set caches later in the chain
                        d.set(key, val)

                    if val == NoneResult:
                        return default
                    else:
                        return val

            if self.cache_negative_results:
                for c in self.caches[:-1]:
                    c.set(key, NoneResult)

            return default
        finally:
            if self.stats:
                if stat_outcome:
                    self.stats.cache_hit()
                else:
                    self.stats.cache_miss()

    def get_multi(self, keys, prefix='', allow_local = True, **kw):
        l = lambda ks: self.simple_get_multi(ks, allow_local = allow_local, **kw)
        return prefix_keys(keys, prefix, l)

    def simple_get_multi(self, keys, allow_local = True, stale=None):
        out = {}
        need = set(keys)
        hits = 0
        misses = 0
        for c in self.caches:
            if not allow_local and isinstance(c, LocalCache):
                continue

            if c.permanent and not misses:
                # Once we reach a "permanent" cache, we count any outstanding
                # items as misses.
                misses = len(need)

            if len(out) == len(keys):
                # we've found them all
                break
            r = c.simple_get_multi(need)
            #update other caches
            if r:
                if not c.permanent:
                    hits += len(r)
                for d in self.caches:
                    if c is d:
                        break # so we don't set caches later in the chain
                    d.set_multi(r)
                r.update(out)
                out = r
                need = need - set(r.keys())

        if need and self.cache_negative_results:
            d = dict((key, NoneResult) for key in need)
            for c in self.caches[:-1]:
                c.set_multi(d)

        out = dict((k, v)
                   for (k, v) in out.iteritems()
                   if v != NoneResult)

        if self.stats:
            if not misses:
                # If this chain contains no permanent caches, then we need to
                # count the misses here.
                misses = len(need)
            self.stats.cache_hit(hits)
            self.stats.cache_miss(misses)

        return out

    def __repr__(self):
        return '<%s %r>' % (self.__class__.__name__,
                            self.caches)

    def debug(self, key):
        print "Looking up [%r]" % key
        for i, c in enumerate(self.caches):
            print "[%d] %10s has value [%r]" % (i, c.__class__.__name__,
                                                c.get(key))

    def reset(self):
        # the first item in a cache chain is a LocalCache
        self.caches = (self.caches[0].__class__(),) +  self.caches[1:]

class MemcacheChain(CacheChain):
    pass

class HardcacheChain(CacheChain):
    def add(self, key, val, time=0):
        authority = self.caches[-1] # the authority is the hardcache
                                    # itself
        added_val = authority.add(key, val, time=time)
        for cache in self.caches[:-1]:
            # Calling set() rather than add() to ensure that all caches are
            # in sync and that de-syncs repair themselves
            cache.set(key, added_val, time=time)

        return added_val

    def accrue(self, key, time=0, delta=1):
        auth_value = self.caches[-1].get(key)

        if auth_value is None:
            auth_value = 0

        try:
            auth_value = int(auth_value) + delta
        except ValueError:
            raise ValueError("Can't accrue %s; it's a %s (%r)" %
                             (key, auth_value.__class__.__name__, auth_value))

        for c in self.caches:
            c.set(key, auth_value, time=time)

        return auth_value

    @property
    def backend(self):
        # the hardcache is always the last item in a HardCacheChain
        return self.caches[-1].backend

class StaleCacheChain(CacheChain):
    """A cache chain of two cache chains. When allowed by `stale`,
       answers may be returned by a "closer" but potentially older
       cache. Probably doesn't play well with NoneResult cacheing"""
    staleness = 30

    def __init__(self, localcache, stalecache, realcache):
        self.localcache = localcache
        self.stalecache = stalecache
        self.realcache = realcache
        self.caches = (localcache, realcache) # for the other
                                              # CacheChain machinery
        self.stats = None

    def get(self, key, default=None, stale = False, **kw):
        if kw.get('allow_local', True) and key in self.caches[0]:
            return self.caches[0][key]

        if stale:
            stale_value = self._getstale([key]).get(key, None)
            if stale_value is not None:
                return stale_value # never return stale data into the
                                   # LocalCache, or people that didn't
                                   # say they'll take stale data may
                                   # get it

        value = CacheChain.get(self, key, **kw)
        if value is None:
            return default

        if value is not None and stale:
            self.stalecache.set(key, value, time=self.staleness)

        return value

    def simple_get_multi(self, keys, stale = False, **kw):
        if not isinstance(keys, set):
            keys = set(keys)

        ret = {}

        if kw.get('allow_local'):
            for k in list(keys):
                if k in self.localcache:
                    ret[k] = self.localcache[k]
                    keys.remove(k)

        if keys and stale:
            stale_values = self._getstale(keys)
            # never put stale data into the localcache
            for k, v in stale_values.iteritems():
                ret[k] = v
                keys.remove(k)

        if keys:
            values = self.realcache.simple_get_multi(keys)
            if values and stale:
                self.stalecache.set_multi(values, time=self.staleness)
            self.localcache.update(values)
            ret.update(values)

        return ret

    def _getstale(self, keys):
        # this is only in its own function to make tapping it for
        # debugging easier
        return self.stalecache.simple_get_multi(keys)

    def reset(self):
        newcache = self.localcache.__class__()
        self.localcache = newcache
        self.caches = (newcache,) +  self.caches[1:]
        if isinstance(self.realcache, CacheChain):
            assert isinstance(self.realcache.caches[0], LocalCache)
            self.realcache.caches = (newcache,) + self.realcache.caches[1:]

    def __repr__(self):
        return '<%s %r>' % (self.__class__.__name__,
                            (self.localcache, self.stalecache, self.realcache))

CL_ONE = ConsistencyLevel.ONE
CL_QUORUM = ConsistencyLevel.QUORUM
CL_ALL = ConsistencyLevel.ALL

class CassandraCacheChain(CacheChain):
    def __init__(self, localcache, cassa, lock_factory, memcache=None, **kw):
        if memcache:
            caches = (localcache, memcache, cassa)
        else:
            caches = (localcache, cassa)

        self.cassa = cassa
        self.memcache = memcache
        self.make_lock = lock_factory
        CacheChain.__init__(self, caches, **kw)

    def mutate(self, key, mutation_fn, default = None, willread=True):
        """Mutate a Cassandra key as atomically as possible"""
        with self.make_lock("permacache_mutate", 'mutate_%s' % key):
            # we have to do some of the the work of the cache chain
            # here so that we can be sure that if the value isn't in
            # memcached (an atomic store), we fetch it from Cassandra
            # with CL_QUORUM (because otherwise it's not an atomic
            # store). This requires us to know the structure of the
            # chain, which means that changing the chain will probably
            # require changing this function. (This has an edge-case
            # where memcached was populated by a ONE read rather than
            # a QUORUM one just before running this. We could avoid
            # this by not using memcached at all for these mutations,
            # which would require some more row-cache performace
            # testing)
            rcl = wcl = self.cassa.write_consistency_level
            if willread:
                try:
                    value = None
                    if self.memcache:
                        value = self.memcache.get(key)
                    if value is None:
                        value = self.cassa.get(key,
                                               read_consistency_level = rcl)
                except CassandraNotFound:
                    value = default

                # due to an old bug in NoneResult caching, we still
                # have some of these around
                if value == NoneResult:
                    value = default

            else:
                value = None

            # send in a copy in case they mutate it in-place
            new_value = mutation_fn(copy(value))

            if not willread or value != new_value:
                self.cassa.set(key, new_value,
                               write_consistency_level = wcl)
            for ca in self.caches[:-1]:
                # and update the rest of the chain; assumes that
                # Cassandra is always the last entry
                ca.set(key, new_value)
        return new_value

    def bulk_load(self, start='', end='', chunk_size = 100):
        """Try to load everything out of Cassandra and put it into
           memcached"""
        cf = self.cassa.cf
        for rows in in_chunks(cf.get_range(start=start,
                                           finish=end,
                                           columns=['value']),
                              chunk_size):
            print rows[0][0]
            rows = dict((key, pickle.loads(cols['value']))
                        for (key, cols)
                        in rows
                        if (cols
                            # hack
                            and len(key) < 250))
            self.memcache.set_multi(rows)


class CassandraCache(CacheUtils):
    permanent = True

    """A cache that uses a Cassandra ColumnFamily. Uses only the
       column-name 'value'"""
    def __init__(self, column_family, client,
                 read_consistency_level = CL_ONE,
                 write_consistency_level = CL_QUORUM):
        self.column_family = column_family
        self.client = client
        self.read_consistency_level = read_consistency_level
        self.write_consistency_level = write_consistency_level
        self.cf = ColumnFamily(self.client,
                               self.column_family,
                               read_consistency_level = read_consistency_level,
                               write_consistency_level = write_consistency_level)

    def _rcl(self, alternative):
        return (alternative if alternative is not None
                else self.cf.read_consistency_level)

    def _wcl(self, alternative):
        return (alternative if alternative is not None
                else self.cf.write_consistency_level)

    def get(self, key, default = None, read_consistency_level = None):
        try:
            rcl = self._rcl(read_consistency_level)
            row = self.cf.get(key, columns=['value'],
                              read_consistency_level = rcl)
            return pickle.loads(row['value'])
        except (CassandraNotFound, KeyError):
            return default

    def simple_get_multi(self, keys, read_consistency_level = None):
        rcl = self._rcl(read_consistency_level)
        rows = self.cf.multiget(list(keys),
                                columns=['value'],
                                read_consistency_level = rcl)
        return dict((key, pickle.loads(row['value']))
                    for (key, row) in rows.iteritems())

    def set(self, key, val,
            write_consistency_level = None,
            time = None):
        if val == NoneResult:
            # NoneResult caching is for other parts of the chain
            return

        wcl = self._wcl(write_consistency_level)
        ret = self.cf.insert(key, {'value': pickle.dumps(val)},
                              write_consistency_level = wcl,
                             ttl = time)
        self._warm([key])
        return ret

    def set_multi(self, keys, prefix='',
                  write_consistency_level = None,
                  time = None):
        if not isinstance(keys, dict):
            # allow iterables yielding tuples
            keys = dict(keys)

        wcl = self._wcl(write_consistency_level)
        ret = {}

        with self.cf.batch(write_consistency_level = wcl):
            for key, val in keys.iteritems():
                if val != NoneResult:
                    ret[key] = self.cf.insert('%s%s' % (prefix, key),
                                              {'value': pickle.dumps(val)},
                                              ttl = time or None)

        self._warm(keys.keys())

        return ret

    def _warm(self, keys):
        import random
        if False and random.random() > 0.98:
            print 'Warming', keys
            self.cf.multiget(keys)

    def delete(self, key, write_consistency_level = None):
        wcl = self._wcl(write_consistency_level)
        self.cf.remove(key, write_consistency_level = wcl)


def test_cache(cache, prefix=''):
    #basic set/get
    cache.set('%s1' % prefix, 1)
    assert cache.get('%s1' % prefix) == 1

    #python data
    cache.set('%s2' % prefix, [1,2,3])
    assert cache.get('%s2' % prefix) == [1,2,3]

    #set multi, no prefix
    cache.set_multi({'%s3' % prefix:3, '%s4' % prefix: 4})
    assert cache.get_multi(('%s3' % prefix, '%s4' % prefix)) == {'%s3' % prefix: 3, 
                                                                 '%s4' % prefix: 4}

    #set multi, prefix
    cache.set_multi({'3':3, '4': 4}, prefix='%sp_' % prefix)
    assert cache.get_multi(('3', 4), prefix='%sp_' % prefix) == {'3':3, 4: 4}
    assert cache.get_multi(('%sp_3' % prefix, '%sp_4' % prefix)) == {'%sp_3'%prefix: 3,
                                                                     '%sp_4'%prefix: 4}

    # delete
    cache.set('%s1'%prefix, 1)
    assert cache.get('%s1'%prefix) == 1
    cache.delete('%s1'%prefix)
    assert cache.get('%s1'%prefix) is None

    cache.set('%s1'%prefix, 1)
    cache.set('%s2'%prefix, 2)
    cache.set('%s3'%prefix, 3)
    assert cache.get('%s1'%prefix) == 1 and cache.get('%s2'%prefix) == 2
    cache.delete_multi(['%s1'%prefix, '%s2'%prefix])
    assert (cache.get('%s1'%prefix) is None
            and cache.get('%s2'%prefix) is None
            and cache.get('%s3'%prefix) == 3)

    #incr
    cache.set('%s5'%prefix, 1)
    cache.set('%s6'%prefix, 1)
    cache.incr('%s5'%prefix)
    assert cache.get('%s5'%prefix) == 2
    cache.incr('%s5'%prefix,2)
    assert cache.get('%s5'%prefix) == 4
    cache.incr_multi(('%s5'%prefix, '%s6'%prefix), 1)
    assert cache.get('%s5'%prefix) == 5
    assert cache.get('%s6'%prefix) == 2

def test_multi(cache):
    from threading import Thread

    num_threads = 100
    num_per_thread = 1000

    threads = []
    for x in range(num_threads):
        def _fn(prefix):
            def __fn():
                for y in range(num_per_thread):
                    test_cache(cache,prefix=prefix)
            return __fn
        t = Thread(target=_fn(str(x)))
        t.start()
        threads.append(t)

    for thread in threads:
        thread.join()

# a cache that occasionally dumps itself to be used for long-running
# processes
class SelfEmptyingCache(LocalCache):
    def __init__(self, max_size=10*1000):
        self.max_size = max_size

    def maybe_reset(self):
        if len(self) > self.max_size:
            self.clear()

    def set(self, key, val, time=0):
        self.maybe_reset()
        return LocalCache.set(self,key,val,time)

    def add(self, key, val, time=0):
        self.maybe_reset()
        return LocalCache.add(self, key, val)

def make_key(iden, *a, **kw):
    """
    A helper function for making memcached-usable cache keys out of
    arbitrary arguments. Hashes the arguments but leaves the `iden'
    human-readable
    """
    h = md5()

    def _conv(s):
        if isinstance(s, str):
            return s
        elif isinstance(s, unicode):
            return s.encode('utf-8')
        elif isinstance(s, (tuple, list)):
            return ','.join(_conv(x) for x in s)
        elif isinstance(s, dict):
            return ','.join('%s:%s' % (_conv(k), _conv(v))
                            for (k, v) in sorted(s.iteritems()))
        else:
            return str(s)

    iden = _conv(iden)
    h.update(iden)
    h.update(_conv(a))
    h.update(_conv(kw))

    return '%s(%s)' % (iden, h.hexdigest())

def test_stale():
    from pylons import g
    ca = g.cache
    assert isinstance(ca, StaleCacheChain)

    ca.localcache.clear()

    ca.stalecache.set('foo', 'bar', time=ca.staleness)
    assert ca.stalecache.get('foo') == 'bar'
    ca.realcache.set('foo', 'baz')
    assert ca.realcache.get('foo') == 'baz'

    assert ca.get('foo', stale=True) == 'bar'
    ca.localcache.clear()
    assert ca.get('foo', stale=False) == 'baz'
    ca.localcache.clear()

    assert ca.get_multi(['foo'], stale=True) == {'foo': 'bar'}
    assert len(ca.localcache) == 0
    assert ca.get_multi(['foo'], stale=False) == {'foo': 'baz'}
    ca.localcache.clear()

########NEW FILE########
__FILENAME__ = captcha
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import random, string
from pylons import g
from Captcha.Base import randomIdentifier
from Captcha.Visual import Text, Backgrounds, Distortions, ImageCaptcha

from r2.lib.cache import make_key

IDEN_LENGTH = 32
SOL_LENGTH = 6

class RandCaptcha(ImageCaptcha):
    defaultSize = (120, 50)
    fontFactory = Text.FontFactory(18, "vera/VeraBd.ttf")

    def getLayers(self, solution="blah"):
        self.addSolution(solution)
        return ((Backgrounds.Grid(size=8, foreground="white"),
                 Distortions.SineWarp(amplitudeRange=(5,9))),
                (Text.TextLayer(solution,
                               textColor = 'white',
                               fontFactory = self.fontFactory),
                 Distortions.SineWarp()))

def get_iden():
    return randomIdentifier(length=IDEN_LENGTH)

def make_solution():
    return randomIdentifier(alphabet=string.ascii_letters, length = SOL_LENGTH).upper()

def get_image(iden):
    key = make_key(iden)
    solution = g.cache.get(key)
    if not solution:
        solution = make_solution()
        g.cache.set(key, solution, time = 300)
    return RandCaptcha(solution=solution).render()

def valid_solution(iden, solution):
    key = make_key(iden)

    if (not iden
        or not solution
        or len(iden) != IDEN_LENGTH
        or len(solution) != SOL_LENGTH
        or solution.upper() != g.cache.get(key)):
        solution = make_solution()
        g.cache.set(key, solution, time = 300)
        return False
    else:
        g.cache.delete(key)
        return True

########NEW FILE########
__FILENAME__ = cloudsearch
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import collections
import cPickle as pickle
from datetime import datetime, timedelta
import functools
import httplib
import json
from lxml import etree
from pylons import g, c
import re
import time
import urllib

import l2cs

from r2.lib import amqp, filters
from r2.lib.db.operators import desc
from r2.lib.db.sorts import epoch_seconds
import r2.lib.utils as r2utils
from r2.models import (Account, Link, Subreddit, Thing, All, DefaultSR,
                       MultiReddit, DomainSR, Friends, ModContribSR,
                       FakeSubreddit, NotFound)


_CHUNK_SIZE = 4000000 # Approx. 4 MB, to stay under the 5MB limit
_VERSION_OFFSET = 13257906857
ILLEGAL_XML = re.compile(u'[\x00-\x08\x0b\x0c\x0e-\x1F\uD800-\uDFFF\uFFFE\uFFFF]')


def _safe_xml_str(s, use_encoding="utf-8"):
    '''Replace invalid-in-XML unicode control characters with '\uFFFD'.
    Also, coerces result to unicode
    
    '''
    if not isinstance(s, unicode):
        if isinstance(s, str):
            s = unicode(s, use_encoding, errors="replace")
        else:
            # ints will raise TypeError if the "errors" kwarg
            # is passed, but since it's not a str no problem
            s = unicode(s)
    s = ILLEGAL_XML.sub(u"\uFFFD", s)
    return s


def safe_get(get_fn, ids, return_dict=True, **kw):
    items = {}
    for i in ids:
        try:
            item = get_fn(i, **kw)
        except NotFound:
            g.log.info("%s failed for %r", get_fn.__name__, i)
        else:
            items[i] = item
    if return_dict:
        return items
    else:
        return items.values()


class CloudSearchHTTPError(httplib.HTTPException): pass
class InvalidQuery(Exception): pass


Field = collections.namedtuple("Field", "name cloudsearch_type "
                               "lucene_type function")
SAME_AS_CLOUDSEARCH = object()
FIELD_TYPES = (int, str, datetime, SAME_AS_CLOUDSEARCH, "yesno")

def field(name=None, cloudsearch_type=str, lucene_type=SAME_AS_CLOUDSEARCH):
    if lucene_type is SAME_AS_CLOUDSEARCH:
        lucene_type = cloudsearch_type
    if cloudsearch_type not in FIELD_TYPES + (None,):
        raise ValueError("cloudsearch_type %r not in %r" %
                         (cloudsearch_type, FIELD_TYPES))
    if lucene_type not in FIELD_TYPES + (None,):
        raise ValueError("lucene_type %r not in %r" %
                         (lucene_type, FIELD_TYPES))
    if callable(name):
        # Simple case; decorated as '@field'; act as a decorator instead
        # of a decorator factory
        function = name
        name = None
    else:
        function = None

    def field_inner(fn):
        fn.field = Field(name or fn.func_name, cloudsearch_type,
                         lucene_type, fn)
        return fn

    if function:
        return field_inner(function)
    else:
        return field_inner


class FieldsMeta(type):
    def __init__(cls, name, bases, attrs):
        type.__init__(cls, name, bases, attrs)
        fields = []
        for attr in attrs.itervalues():
            if hasattr(attr, "field"):
                fields.append(attr.field)
        cls._fields = tuple(fields)


class FieldsBase(object):
    __metaclass__ = FieldsMeta

    def fields(self):
        data = {}
        for field in self._fields:
            if field.cloudsearch_type is None:
                continue
            val = field.function(self)
            if val is not None:
                data[field.name] = val
        return data

    @classmethod
    def all_fields(cls):
        return cls._fields

    @classmethod
    def cloudsearch_fields(cls, type_=None, types=FIELD_TYPES):
        types = (type_,) if type_ else types
        return [f for f in cls._fields if f.cloudsearch_type in types]

    @classmethod
    def lucene_fields(cls, type_=None, types=FIELD_TYPES):
        types = (type_,) if type_ else types
        return [f for f in cls._fields if f.lucene_type in types]

    @classmethod
    def cloudsearch_fieldnames(cls, type_=None, types=FIELD_TYPES):
        return [f.name for f in cls.cloudsearch_fields(type_=type_,
                                                       types=types)]

    @classmethod
    def lucene_fieldnames(cls, type_=None, types=FIELD_TYPES):
        return [f.name for f in cls.lucene_fields(type_=type_, types=types)]


class LinkFields(FieldsBase):
    def __init__(self, link, author, sr):
        self.link = link
        self.author = author
        self.sr = sr

    @field(cloudsearch_type=int, lucene_type=None)
    def ups(self):
        return max(0, self.link._ups)

    @field(cloudsearch_type=int, lucene_type=None)
    def downs(self):
        return max(0, self.link._downs)

    @field(cloudsearch_type=int, lucene_type=None)
    def num_comments(self):
        return max(0, getattr(self.link, 'num_comments', 0))

    @field
    def fullname(self):
        return self.link._fullname

    @field
    def subreddit(self):
        return self.sr.name

    @field
    def reddit(self):
        return self.sr.name

    @field
    def title(self):
        return self.link.title

    @field(cloudsearch_type=int)
    def sr_id(self):
        return self.link.sr_id

    @field(cloudsearch_type=int, lucene_type=datetime)
    def timestamp(self):
        return int(time.mktime(self.link._date.utctimetuple()))

    @field(cloudsearch_type=int, lucene_type="yesno")
    def over18(self):
        nsfw = (self.sr.over_18 or self.link.over_18 or
                Link._nsfw.findall(self.link.title))
        return (1 if nsfw else 0)

    @field(cloudsearch_type=None, lucene_type="yesno")
    def nsfw(self):
        return NotImplemented

    @field(cloudsearch_type=int, lucene_type="yesno")
    def is_self(self):
        return (1 if self.link.is_self else 0)

    @field(name="self", cloudsearch_type=None, lucene_type="yesno")
    def self_(self):
        return NotImplemented

    @field
    def author_fullname(self):
        return self.author._fullname

    @field(name="author")
    def author_field(self):
        return '[deleted]' if self.author._deleted else self.author.name

    @field(cloudsearch_type=int)
    def type_id(self):
        return self.link._type_id

    @field
    def site(self):
        if self.link.is_self:
            return g.domain
        else:
            url = r2utils.UrlParser(self.link.url)
            try:
                return list(url.domain_permutations())
            except ValueError:
                return None

    @field
    def selftext(self):
        if self.link.is_self and self.link.selftext:
            return self.link.selftext
        else:
            return None

    @field
    def url(self):
        if not self.link.is_self:
            return self.link.url
        else:
            return None

    @field
    def flair_css_class(self):
        return self.link.flair_css_class

    @field
    def flair_text(self):
        return self.link.flair_text

    @field(cloudsearch_type=None, lucene_type=str)
    def flair(self):
        return NotImplemented


class SubredditFields(FieldsBase):
    def __init__(self, sr):
        self.sr = sr

    @field
    def name(self):
        return self.sr.name

    @field
    def title(self):
        return self.sr.title

    @field(name="type")
    def type_(self):
        return self.sr.type

    @field
    def language(self):
        return self.sr.lang

    @field
    def header_title(self):
        return self.sr.header_title

    @field
    def description(self):
        return self.sr.public_description

    @field
    def sidebar(self):
        return self.sr.description

    @field(cloudsearch_type=int)
    def over18(self):
        return 1 if self.sr.over_18 else 0

    @field
    def link_type(self):
        return self.sr.link_type

    @field
    def activity(self):
        return self.sr._downs

    @field
    def subscribers(self):
        return self.sr._ups

    @field
    def type_id(self):
        return self.sr._type_id


class CloudSearchUploader(object):
    use_safe_get = False
    types = ()

    def __init__(self, doc_api, fullnames=None, version_offset=_VERSION_OFFSET):
        self.doc_api = doc_api
        self._version_offset = version_offset
        self.fullnames = fullnames

    @classmethod
    def desired_fullnames(cls, items):
        '''Pull fullnames that represent instances of 'types' out of items'''
        fullnames = set()
        type_ids = [type_._type_id for type_ in cls.types]
        for item in items:
            item_type = r2utils.decompose_fullname(item['fullname'])[1]
            if item_type in type_ids:
                fullnames.add(item['fullname'])
        return fullnames

    def _version_tenths(self):
        '''Cloudsearch documents don't update unless the sent "version" field
        is higher than the one currently indexed. As our documents don't have
        "versions" and could in theory be updated multiple times in one second,
        for now, use "tenths of a second since 12:00:00.00 1/1/2012" as the
        "version" - this will last approximately 13 years until bumping up against
        the version max of 2^32 for cloudsearch docs'''
        return int(time.time() * 10) - self._version_offset

    def _version_seconds(self):
        return int(time.time()) - int(self._version_offset / 10)

    _version = _version_tenths

    def add_xml(self, thing, version):
        add = etree.Element("add", id=thing._fullname, version=str(version),
                            lang="en")

        for field_name, value in self.fields(thing).iteritems():
            field = etree.SubElement(add, "field", name=field_name)
            field.text = _safe_xml_str(value)

        return add

    def delete_xml(self, thing, version=None):
        '''Return the cloudsearch XML representation of
        "delete this from the index"
        
        '''
        version = str(version or self._version())
        delete = etree.Element("delete", id=thing._fullname, version=version)
        return delete

    def delete_ids(self, ids):
        '''Delete documents from the index.
        'ids' should be a list of fullnames
        
        '''
        version = self._version()
        deletes = [etree.Element("delete", id=id_, version=str(version))
                   for id_ in ids]
        batch = etree.Element("batch")
        batch.extend(deletes)
        return self.send_documents(batch)

    def xml_from_things(self):
        '''Generate a <batch> XML tree to send to cloudsearch for
        adding/updating/deleting the given things
        
        '''
        batch = etree.Element("batch")
        self.batch_lookups()
        version = self._version()
        for thing in self.things:
            try:
                if thing._spam or thing._deleted:
                    delete_node = self.delete_xml(thing, version)
                    batch.append(delete_node)
                elif self.should_index(thing):
                    add_node = self.add_xml(thing, version)
                    batch.append(add_node)
            except (AttributeError, KeyError) as e:
                # Problem! Bail out, which means these items won't get
                # "consumed" from the queue. If the problem is from DB
                # lag or a transient issue, then the queue consumer
                # will succeed eventually. If it's something else,
                # then manually run a consumer with 'use_safe_get'
                # on to get past the bad Thing in the queue
                if not self.use_safe_get:
                    raise
                else:
                    g.log.warning("Ignoring problem on thing %r.\n\n%r",
                                  thing, e)
        return batch

    def should_index(self, thing):
        raise NotImplementedError

    def batch_lookups(self):
        try:
            self.things = Thing._by_fullname(self.fullnames, data=True,
                                             return_dict=False)
        except NotFound:
            if self.use_safe_get:
                self.things = safe_get(Thing._by_fullname, self.fullnames,
                                       data=True, return_dict=False)
            else:
                raise

    def fields(self, thing):
        raise NotImplementedError

    def inject(self, quiet=False):
        '''Send things to cloudsearch. Return value is time elapsed, in seconds,
        of the communication with the cloudsearch endpoint
        
        '''
        xml_things = self.xml_from_things()

        if not len(xml_things):
            return 0

        cs_start = datetime.now(g.tz)
        sent = self.send_documents(xml_things)
        cs_time = (datetime.now(g.tz) - cs_start).total_seconds()

        adds, deletes, warnings = 0, 0, []
        for record in sent:
            response = etree.fromstring(record)
            adds += int(response.get("adds", 0))
            deletes += int(response.get("deletes", 0))
            if response.get("warnings"):
                warnings.append(response.get("warnings"))

        g.stats.simple_event("cloudsearch.uploads.adds", delta=adds)
        g.stats.simple_event("cloudsearch.uploads.deletes", delta=deletes)
        g.stats.simple_event("cloudsearch.uploads.warnings",
                delta=len(warnings))

        if not quiet:
            print "%s Changes: +%i -%i" % (self.__class__.__name__,
                                           adds, deletes)
            if len(warnings):
                print "%s Warnings: %s" % (self.__class__.__name__,
                                           "; ".join(warnings))

        return cs_time

    def send_documents(self, docs):
        '''Open a connection to the cloudsearch endpoint, and send the documents
        for indexing. Multiple requests are sent if a large number of documents
        are being sent (see chunk_xml())
        
        Raises CloudSearchHTTPError if the endpoint indicates a failure
        '''
        responses = []
        connection = httplib.HTTPConnection(self.doc_api, 80)
        chunker = chunk_xml(docs)
        try:
            for data in chunker:
                headers = {}
                headers['Content-Type'] = 'application/xml'
                # HTTPLib calculates Content-Length header automatically
                connection.request('POST', "/2011-02-01/documents/batch",
                                   data, headers)
                response = connection.getresponse()
                if 200 <= response.status < 300:
                    responses.append(response.read())
                else:
                    raise CloudSearchHTTPError(response.status,
                                               response.reason,
                                               response.read())
        finally:
            connection.close()
        return responses


class LinkUploader(CloudSearchUploader):
    types = (Link,)

    def __init__(self, doc_api, fullnames=None, version_offset=_VERSION_OFFSET):
        super(LinkUploader, self).__init__(doc_api, fullnames, version_offset)
        self.accounts = {}
        self.srs = {}

    def fields(self, thing):
        '''Return fields relevant to a Link search index'''
        account = self.accounts[thing.author_id]
        sr = self.srs[thing.sr_id]
        return LinkFields(thing, account, sr).fields()

    def batch_lookups(self):
        super(LinkUploader, self).batch_lookups()
        author_ids = [thing.author_id for thing in self.things
                      if hasattr(thing, 'author_id')]
        try:
            self.accounts = Account._byID(author_ids, data=True,
                                          return_dict=True)
        except NotFound:
            if self.use_safe_get:
                self.accounts = safe_get(Account._byID, author_ids, data=True,
                                         return_dict=True)
            else:
                raise

        sr_ids = [thing.sr_id for thing in self.things
                  if hasattr(thing, 'sr_id')]
        try:
            self.srs = Subreddit._byID(sr_ids, data=True, return_dict=True)
        except NotFound:
            if self.use_safe_get:
                self.srs = safe_get(Subreddit._byID, sr_ids, data=True,
                                    return_dict=True)
            else:
                raise

    def should_index(self, thing):
        return (thing.promoted is None and getattr(thing, "sr_id", None) != -1)


class SubredditUploader(CloudSearchUploader):
    types = (Subreddit,)
    _version = CloudSearchUploader._version_seconds

    def fields(self, thing):
        return SubredditFields(thing).fields()

    def should_index(self, thing):
        return getattr(thing, 'author_id', None) != -1


def chunk_xml(xml, depth=0):
    '''Chunk POST data into pieces that are smaller than the 20 MB limit.
    
    Ideally, this never happens (if chunking is necessary, would be better
    to avoid xml'ifying before testing content_length)'''
    data = etree.tostring(xml)
    content_length = len(data)
    if content_length < _CHUNK_SIZE:
        yield data
    else:
        depth += 1
        print "WARNING: Chunking (depth=%s)" % depth
        half = len(xml) / 2
        left_half = xml # for ease of reading
        right_half = etree.Element("batch")
        # etree magic simultaneously removes the elements from one tree
        # when they are appended to a different tree
        right_half.append(xml[half:])
        for chunk in chunk_xml(left_half, depth=depth):
            yield chunk
        for chunk in chunk_xml(right_half, depth=depth):
            yield chunk


@g.stats.amqp_processor('cloudsearch_q')
def _run_changed(msgs, chan):
    '''Consume the cloudsearch_changes queue, and print reporting information
    on how long it took and how many remain
    
    '''
    start = datetime.now(g.tz)

    changed = [pickle.loads(msg.body) for msg in msgs]

    link_fns = LinkUploader.desired_fullnames(changed)
    sr_fns = SubredditUploader.desired_fullnames(changed)

    link_uploader = LinkUploader(g.CLOUDSEARCH_DOC_API, fullnames=link_fns)
    subreddit_uploader = SubredditUploader(g.CLOUDSEARCH_SUBREDDIT_DOC_API,
                                           fullnames=sr_fns)

    link_time = link_uploader.inject()
    subreddit_time = subreddit_uploader.inject()
    cloudsearch_time = link_time + subreddit_time

    totaltime = (datetime.now(g.tz) - start).total_seconds()

    print ("%s: %d messages in %.2fs seconds (%.2fs secs waiting on "
           "cloudsearch); %d duplicates, %s remaining)" %
           (start, len(changed), totaltime, cloudsearch_time,
            len(changed) - len(link_fns | sr_fns),
            msgs[-1].delivery_info.get('message_count', 'unknown')))


def run_changed(drain=False, min_size=500, limit=1000, sleep_time=10,
                use_safe_get=False, verbose=False):
    '''Run by `cron` (through `paster run`) on a schedule to send Things to
        Amazon CloudSearch
    
    '''
    if use_safe_get:
        CloudSearchUploader.use_safe_get = True
    amqp.handle_items('cloudsearch_changes', _run_changed, min_size=min_size,
                      limit=limit, drain=drain, sleep_time=sleep_time,
                      verbose=verbose)


def _progress_key(item):
    return "%s/%s" % (item._id, item._date)


_REBUILD_INDEX_CACHE_KEY = "cloudsearch_cursor_%s"


def rebuild_link_index(start_at=None, sleeptime=1, cls=Link,
                       uploader=LinkUploader, doc_api='CLOUDSEARCH_DOC_API',
                       estimate=50000000, chunk_size=1000):
    cache_key = _REBUILD_INDEX_CACHE_KEY % uploader.__name__.lower()
    doc_api = getattr(g, doc_api)
    uploader = uploader(doc_api)

    if start_at is _REBUILD_INDEX_CACHE_KEY:
        start_at = g.cache.get(cache_key)
        if not start_at:
            raise ValueError("Told me to use '%s' key, but it's not set" %
                             cache_key)

    q = cls._query(cls.c._deleted == (True, False),
                   sort=desc('_date'), data=True)
    if start_at:
        after = cls._by_fullname(start_at)
        assert isinstance(after, cls)
        q._after(after)
    q = r2utils.fetch_things2(q, chunk_size=chunk_size)
    q = r2utils.progress(q, verbosity=1000, estimate=estimate, persec=True,
                         key=_progress_key)
    for chunk in r2utils.in_chunks(q, size=chunk_size):
        uploader.things = chunk
        for x in range(5):
            try:
                uploader.inject()
            except httplib.HTTPException as err:
                print "Got %s, sleeping %s secs" % (err, x)
                time.sleep(x)
                continue
            else:
                break
        else:
            raise err
        last_update = chunk[-1]
        g.cache.set(cache_key, last_update._fullname)
        time.sleep(sleeptime)


rebuild_subreddit_index = functools.partial(rebuild_link_index,
                                            cls=Subreddit,
                                            uploader=SubredditUploader,
                                            doc_api='CLOUDSEARCH_SUBREDDIT_DOC_API',
                                            estimate=200000,
                                            chunk_size=1000)


def test_run_link(start_link, count=1000):
    '''Inject `count` number of links, starting with `start_link`'''
    if isinstance(start_link, basestring):
        start_link = int(start_link, 36)
    links = Link._byID(range(start_link - count, start_link), data=True,
                       return_dict=False)
    uploader = LinkUploader(g.CLOUDSEARCH_DOC_API, things=links)
    return uploader.inject()


def test_run_srs(*sr_names):
    '''Inject Subreddits by name into the index'''
    srs = Subreddit._by_name(sr_names).values()
    uploader = SubredditUploader(g.CLOUDSEARCH_SUBREDDIT_DOC_API, things=srs)
    return uploader.inject()


### Query Code ###
class Results(object):
    def __init__(self, docs, hits, facets):
        self.docs = docs
        self.hits = hits
        self._facets = facets
        self._subreddits = []

    def __repr__(self):
        return '%s(%r, %r, %r)' % (self.__class__.__name__,
                                   self.docs,
                                   self.hits,
                                   self._facets)

    @property
    def subreddit_facets(self):
        '''Filter out subreddits that the user isn't allowed to see'''
        if not self._subreddits and 'reddit' in self._facets:
            sr_facets = [(sr['value'], sr['count']) for sr in
                         self._facets['reddit']]

            # look up subreddits
            srs_by_name = Subreddit._by_name([name for name, count
                                              in sr_facets])

            sr_facets = [(srs_by_name[name], count) for name, count
                         in sr_facets if name in srs_by_name]

            # filter by can_view
            self._subreddits = [(sr, count) for sr, count in sr_facets
                                if sr.can_view(c.user)]

        return self._subreddits


_SEARCH = "/2011-02-01/search?"
INVALID_QUERY_CODES = ('CS-UnknownFieldInMatchExpression',
                       'CS-IncorrectFieldTypeInMatchExpression',
                       'CS-InvalidMatchSetExpression',)
DEFAULT_FACETS = {"reddit": {"count":20}}
def basic_query(query=None, bq=None, faceting=None, size=1000,
                start=0, rank="-relevance", return_fields=None, record_stats=False,
                search_api=None):
    if search_api is None:
        search_api = g.CLOUDSEARCH_SEARCH_API
    if faceting is None:
        faceting = DEFAULT_FACETS
    path = _encode_query(query, bq, faceting, size, start, rank, return_fields)
    timer = None
    if record_stats:
        timer = g.stats.get_timer("cloudsearch_timer")
        timer.start()
    connection = httplib.HTTPConnection(search_api, 80)
    try:
        connection.request('GET', path)
        resp = connection.getresponse()
        response = resp.read()
        if record_stats:
            g.stats.action_count("event.search_query", resp.status)
        if resp.status >= 300:
            try:
                reasons = json.loads(response)
            except ValueError:
                pass
            else:
                messages = reasons.get("messages", [])
                for message in messages:
                    if message['code'] in INVALID_QUERY_CODES:
                        raise InvalidQuery(resp.status, resp.reason, message,
                                           path, reasons)
            raise CloudSearchHTTPError(resp.status, resp.reason, path,
                                       response)
    finally:
        connection.close()
        if timer is not None:
            timer.stop()

    return json.loads(response)


basic_link = functools.partial(basic_query, size=10, start=0,
                               rank="-relevance",
                               return_fields=['title', 'reddit',
                                              'author_fullname'],
                               record_stats=False,
                               search_api=g.CLOUDSEARCH_SEARCH_API)


basic_subreddit = functools.partial(basic_query,
                                    faceting=None,
                                    size=10, start=0,
                                    rank="-activity",
                                    return_fields=['title', 'reddit',
                                                   'author_fullname'],
                                    record_stats=False,
                                    search_api=g.CLOUDSEARCH_SUBREDDIT_SEARCH_API)


def _encode_query(query, bq, faceting, size, start, rank, return_fields):
    if not (query or bq):
        raise ValueError("Need query or bq")
    params = {}
    if bq:
        params["bq"] = bq
    else:
        params["q"] = query
    params["results-type"] = "json"
    params["size"] = size
    params["start"] = start
    params["rank"] = rank
    if faceting:
        params["facet"] = ",".join(faceting.iterkeys())
        for facet, options in faceting.iteritems():
            params["facet-%s-top-n" % facet] = options.get("count", 20)
            if "sort" in options:
                params["facet-%s-sort" % facet] = options["sort"]
    if return_fields:
        params["return-fields"] = ",".join(return_fields)
    encoded_query = urllib.urlencode(params)
    path = _SEARCH + encoded_query
    return path


class CloudSearchQuery(object):
    '''Represents a search query sent to cloudsearch'''
    search_api = None
    sorts = {}
    sorts_menu_mapping = {}
    recents = {None: None}
    known_syntaxes = ("cloudsearch", "lucene", "plain")
    default_syntax = "plain"
    lucene_parser = None

    def __init__(self, query, sr=None, sort=None, syntax=None, raw_sort=None,
                 faceting=None, recent=None):
        if syntax is None:
            syntax = self.default_syntax
        elif syntax not in self.known_syntaxes:
            raise ValueError("Unknown search syntax: %s" % syntax)
        self.query = filters._force_unicode(query or u'')
        self.converted_data = None
        self.syntax = syntax
        self.sr = sr
        self._sort = sort
        if raw_sort:
            self.sort = raw_sort
        else:
            self.sort = self.sorts[sort]
        self._recent = recent
        self.recent = self.recents[recent]
        self.faceting = faceting
        self.bq = u''
        self.results = None

    def run(self, after=None, reverse=False, num=1000, _update=False):
        results = self._run(_update=_update)

        docs, hits, facets = results.docs, results.hits, results._facets

        after_docs = r2utils.get_after(docs, after, num, reverse=reverse)

        self.results = Results(after_docs, hits, facets)
        return self.results

    def _run(self, start=0, num=1000, _update=False):
        '''Run the search against self.query'''
        q = None
        if self.syntax == "cloudsearch":
            self.bq = self.customize_query(self.query)
        elif self.syntax == "lucene":
            bq = l2cs.convert(self.query, self.lucene_parser)
            self.converted_data = {"syntax": "cloudsearch",
                                   "converted": bq}
            self.bq = self.customize_query(bq)
        elif self.syntax == "plain":
            q = self.query.encode('utf-8')
        if g.sqlprinting:
            g.log.info("%s", self)
        return self._run_cached(q, self.bq.encode('utf-8'), self.sort,
                                self.faceting, start=start, num=num,
                                _update=_update)

    def customize_query(self, bq):
        return bq

    def __repr__(self):
        '''Return a string representation of this query'''
        result = ["<", self.__class__.__name__, "> query:",
                  repr(self.query), " "]
        if self.bq:
            result.append(" bq:")
            result.append(repr(self.bq))
            result.append(" ")
        result.append("sort:")
        result.append(self.sort)
        return ''.join(result)

    @classmethod
    def _run_cached(cls, query, bq, sort="relevance", faceting=None, start=0,
                    num=1000, _update=False):
        '''Query the cloudsearch API. _update parameter allows for supposed
        easy memoization at later date.
        
        Example result set:
        
        {u'facets': {u'reddit': {u'constraints':
                                    [{u'count': 114, u'value': u'politics'},
                                    {u'count': 42, u'value': u'atheism'},
                                    {u'count': 27, u'value': u'wtf'},
                                    {u'count': 19, u'value': u'gaming'},
                                    {u'count': 12, u'value': u'bestof'},
                                    {u'count': 12, u'value': u'tf2'},
                                    {u'count': 11, u'value': u'AdviceAnimals'},
                                    {u'count': 9, u'value': u'todayilearned'},
                                    {u'count': 9, u'value': u'pics'},
                                    {u'count': 9, u'value': u'funny'}]}},
         u'hits': {u'found': 399,
                   u'hit': [{u'id': u't3_11111'},
                            {u'id': u't3_22222'},
                            {u'id': u't3_33333'},
                            {u'id': u't3_44444'},
                            ...
                            ],
                   u'start': 0},
         u'info': {u'cpu-time-ms': 10,
                   u'messages': [{u'code': u'CS-InvalidFieldOrRankAliasInRankParameter',
                                  u'message': u"Unable to create score object for rank '-hot'",
                                  u'severity': u'warning'}],
                   u'rid': u'<hash>',
                   u'time-ms': 9},
                   u'match-expr': u"(label 'my query')",
                   u'rank': u'-text_relevance'}
        
        '''
        if not query and not bq:
            return Results([], 0, {})
        response = basic_query(query=query, bq=bq, size=num, start=start,
                               rank=sort, search_api=cls.search_api,
                               faceting=faceting, record_stats=True)

        warnings = response['info'].get('messages', [])
        for warning in warnings:
            g.log.warning("%(code)s (%(severity)s): %(message)s" % warning)

        hits = response['hits']['found']
        docs = [doc['id'] for doc in response['hits']['hit']]
        facets = response.get('facets', {})
        for facet in facets.keys():
            values = facets[facet]['constraints']
            facets[facet] = values

        results = Results(docs, hits, facets)
        return results


class LinkSearchQuery(CloudSearchQuery):
    search_api = g.CLOUDSEARCH_SEARCH_API
    sorts = {'relevance': '-relevance',
             'hot': '-hot2',
             'top': '-top',
             'new': '-timestamp',
             'comments': '-num_comments',
             }
    sorts_menu_mapping = {'relevance': 1,
                          'hot': 2,
                          'new': 3,
                          'top': 4,
                          'comments': 5,
                          }
    recents = {
        'hour': timedelta(hours=1),
        'day': timedelta(days=1),
        'week': timedelta(days=7),
        'month': timedelta(days=31),
        'year': timedelta(days=366),
        'all': None,
        None: None,
    }
    schema = l2cs.make_schema(LinkFields.lucene_fieldnames())
    lucene_parser = l2cs.make_parser(
             int_fields=LinkFields.lucene_fieldnames(type_=int),
             yesno_fields=LinkFields.lucene_fieldnames(type_="yesno"),
             schema=schema)
    known_syntaxes = ("cloudsearch", "lucene", "plain")
    default_syntax = "lucene"

    def customize_query(self, bq):
        queries = [bq]
        subreddit_query = self._get_sr_restriction(self.sr)
        if subreddit_query:
            queries.append(subreddit_query)
        if self.recent:
            recent_query = self._restrict_recent(self.recent)
            queries.append(recent_query)
        return self.create_boolean_query(queries)

    @classmethod
    def create_boolean_query(cls, queries):
        '''Return an AND clause combining all queries'''
        if len(queries) > 1:
            bq = '(and ' + ' '.join(queries) + ')'
        else:
            bq = queries[0]
        return bq

    @staticmethod
    def _restrict_recent(recent):
        now = datetime.now(g.tz)
        since = epoch_seconds(now - recent)
        return 'timestamp:%i..' % since

    @staticmethod
    def _get_sr_restriction(sr):
        '''Return a cloudsearch appropriate query string that restricts
        results to only contain results from self.sr
        
        '''
        bq = []
        if (not sr) or sr == All or isinstance(sr, DefaultSR):
            return None
        elif isinstance(sr, MultiReddit):
            bq = ["(or"]
            for sr_id in sr.sr_ids:
                bq.append("sr_id:%s" % sr_id)
            bq.append(")")
        elif isinstance(sr, DomainSR):
            bq = ["site:'%s'" % sr.domain]
        elif sr == Friends:
            if not c.user_is_loggedin or not c.user.friends:
                return None
            bq = ["(or"]
            # The query limit is roughly 8k bytes. Limit to 200 friends to
            # avoid getting too close to that limit
            friend_ids = c.user.friends[:200]
            friends = ["author_fullname:'%s'" %
                       Account._fullname_from_id36(r2utils.to36(id_))
                       for id_ in friend_ids]
            bq.extend(friends)
            bq.append(")")
        elif isinstance(sr, ModContribSR):
            bq = ["(or"]
            for sr_id in sr.sr_ids:
                bq.append("sr_id:%s" % sr_id)
            bq.append(")")
        elif not isinstance(sr, FakeSubreddit):
            bq = ["sr_id:%s" % sr._id]

        return ' '.join(bq)


class SubredditSearchQuery(CloudSearchQuery):
    search_api = g.CLOUDSEARCH_SUBREDDIT_SEARCH_API
    sorts = {'relevance': '-activity',
             None: '-activity',
             }
    sorts_menu_mapping = {'relevance': 1,
                          }

    known_syntaxes = ("plain",)
    default_syntax = "plain"

########NEW FILE########
__FILENAME__ = comment_tree
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import g, c
from itertools import chain
from r2.lib.utils import SimpleSillyStub, tup, to36
from r2.lib.db.sorts import epoch_seconds
from r2.lib.cache import sgm
from r2.models.comment_tree import CommentTree
from r2.models.link import Comment, Link

MESSAGE_TREE_SIZE_LIMIT = 15000

def comments_key(link_id):
    return 'comments_' + str(link_id)

def lock_key(link_id):
    return 'comment_lock_' + str(link_id)

def parent_comments_key(link_id):
    return 'comments_parents_' + str(link_id)

def sort_comments_key(link_id, sort):
    assert sort.startswith('_')
    return '%s%s' % (to36(link_id), sort)

def _get_sort_value(comment, sort):
    if sort == "_date":
        return epoch_seconds(comment._date)
    return getattr(comment, sort)

def add_comments(comments):
    links = Link._byID([com.link_id for com in tup(comments)], data=True)
    comments = tup(comments)

    link_map = {}
    for com in comments:
        link_map.setdefault(com.link_id, []).append(com)

    for link_id, coms in link_map.iteritems():
        link = links[link_id]
        add_comments = [comment for comment in coms if not comment._deleted]
        delete_comments = (comment for comment in coms if comment._deleted)
        timer = g.stats.get_timer('comment_tree.add.%s'
                                  % link.comment_tree_version)
        timer.start()
        try:
            with CommentTree.mutation_context(link):
                timer.intermediate('lock')
                cache = get_comment_tree(link, timer=timer)
                timer.intermediate('get')
                if add_comments:
                    cache.add_comments(add_comments)
                for comment in delete_comments:
                    cache.delete_comment(comment, link)
                timer.intermediate('update')
        except:
            g.log.exception(
                'add_comments_nolock failed for link %s, recomputing tree',
                link_id)

            # calculate it from scratch
            get_comment_tree(link, _update=True, timer=timer)
        timer.stop()
        update_comment_votes(coms)

def update_comment_votes(comments, write_consistency_level = None):
    from r2.models import CommentSortsCache

    comments = tup(comments)

    link_map = {}
    for com in comments:
        link_map.setdefault(com.link_id, []).append(com)

    for link_id, coms in link_map.iteritems():
        for sort in ("_controversy", "_hot", "_confidence", "_score", "_date"):
            # Cassandra always uses the id36 instead of the integer
            # ID, so we'll map that first before sending it
            c_key = sort_comments_key(link_id, sort)
            c_r = dict((cm._id36, _get_sort_value(cm, sort))
                       for cm in coms)
            CommentSortsCache._set_values(c_key, c_r,
                                          write_consistency_level = write_consistency_level)


def _comment_sorter_from_cids(cids, sort):
    comments = Comment._byID(cids, data = False, return_dict = False)
    return dict((x._id, _get_sort_value(x, sort)) for x in comments)

def _get_comment_sorter(link_id, sort):
    from r2.models import CommentSortsCache
    from r2.lib.db.tdb_cassandra import NotFound

    key = sort_comments_key(link_id, sort)
    try:
        sorter = CommentSortsCache._byID(key)._values()
    except NotFound:
        return {}

    # we store these id36ed, but there are still bits of the code that
    # want to deal in integer IDs
    sorter = dict((int(c_id, 36), val)
                  for (c_id, val) in sorter.iteritems())
    return sorter

def link_comments_and_sort(link, sort):
    from r2.models import CommentSortsCache

    # This has grown sort of organically over time. Right now the
    # cache of the comments tree consists in three keys:
    # 1. The comments_key: A tuple of
    #      (cids, comment_tree, depth)
    #    given:
    #      cids         =:= [comment_id]
    #      comment_tree =:= dict(comment_id -> [comment_id])
    #      depth        =:= dict(comment_id -> int depth)
    # 2. The parent_comments_key =:= dict(comment_id -> parent_id)
    # 3. The comments_sorts keys =:= dict(comment_id36 -> float).
    #    These are represented by a Cassandra model
    #    (CommentSortsCache) rather than a permacache key. One of
    #    these exists for each sort (hot, new, etc)

    timer = g.stats.get_timer('comment_tree.get.%s' % link.comment_tree_version)
    timer.start()

    link_id = link._id
    cache = get_comment_tree(link, timer=timer)
    cids = cache.cids
    tree = cache.tree
    depth = cache.depth
    parents = cache.parents

    # load the sorter
    sorter = _get_comment_sorter(link_id, sort)

    sorter_needed = []
    if cids and not sorter:
        sorter_needed = cids
        g.log.debug("comment_tree.py: sorter (%s) cache miss for Link %s"
                    % (sort, link_id))
        sorter = {}

    sorter_needed = [x for x in cids if x not in sorter]
    if cids and sorter_needed:
        g.log.debug(
            "Error in comment_tree: sorter %r inconsistent (missing %d e.g. %r)"
            % (sort_comments_key(link_id, sort), len(sorter_needed), sorter_needed[:10]))
        if not g.disallow_db_writes:
            update_comment_votes(Comment._byID(sorter_needed, data=True, return_dict=False))

        sorter.update(_comment_sorter_from_cids(sorter_needed, sort))
        timer.intermediate('sort')

    if parents is None:
        g.log.debug("comment_tree.py: parents cache miss for Link %s"
                    % link_id)
        parents = {}
    elif cids and not all(x in parents for x in cids):
        g.log.debug("Error in comment_tree: parents inconsistent for Link %s"
                    % link_id)
        parents = {}

    if not parents and len(cids) > 0:
        with CommentTree.mutation_context(link):
            # reload under lock so the sorter and parents are consistent
            timer.intermediate('lock')
            cache = get_comment_tree(link, timer=timer)
            cache.parents = cache.parent_dict_from_tree(cache.tree)

    timer.stop()

    return (cache.cids, cache.tree, cache.depth, cache.parents, sorter)

def get_comment_tree(link, _update=False, timer=None):
    if timer is None:
        timer = SimpleSillyStub()
    cache = CommentTree.by_link(link)
    timer.intermediate('load')
    if cache and not _update:
        return cache
    with CommentTree.mutation_context(link, timeout=180):
        timer.intermediate('lock')
        cache = CommentTree.rebuild(link)
        timer.intermediate('rebuild')
        # the tree rebuild updated the link's comment count, so schedule it for
        # search reindexing
        from r2.lib.db.queries import changed
        changed([link])
        timer.intermediate('changed')
        return cache

# message conversation functions
def messages_key(user_id):
    return 'message_conversations_' + str(user_id)

def messages_lock_key(user_id):
    return 'message_conversations_lock_' + str(user_id)

def add_message(message):
    # add the message to the author's list and the recipient
    with g.make_lock("message_tree", messages_lock_key(message.author_id)):
        add_message_nolock(message.author_id, message)
    if message.to_id:
        with g.make_lock("message_tree", messages_lock_key(message.to_id)):
            add_message_nolock(message.to_id, message)
    # Messages to a subreddit should end in its inbox. Messages
    # FROM a subreddit (currently, just ban messages) should NOT
    if message.sr_id and not message.from_sr:
        with g.make_lock("modmail_tree", sr_messages_lock_key(message.sr_id)):
            add_sr_message_nolock(message.sr_id, message)


def _add_message_nolock(key, message):
    from r2.models import Account, Message
    trees = g.permacache.get(key)
    if not trees:
        # in case an empty list got written at some point, delete it to
        # force a recompute
        if trees is not None:
            g.permacache.delete(key)
        # no point computing it now.  We'll do it when they go to
        # their message page.
        return

    # if it is a new root message, easy enough
    if message.first_message is None:
        trees.insert(0, (message._id, []))
    else:
        tree_dict = dict(trees)

        # if the tree already has the first message, update the list
        if message.first_message in tree_dict:
            if message._id not in tree_dict[message.first_message]:
                tree_dict[message.first_message].append(message._id)
                tree_dict[message.first_message].sort()
        # we have to regenerate the conversation :/
        else:
            m = Message._query(Message.c.first_message == message.first_message,
                               data = True)
            new_tree = compute_message_trees(m)
            if new_tree:
                trees.append(new_tree[0])
        trees.sort(key = tree_sort_fn, reverse = True)

    # If we have too many messages in the tree, drop the oldest
    # conversation to avoid the permacache size limit
    tree_size = len(trees) + sum(len(convo[1]) for convo in trees)

    if tree_size > MESSAGE_TREE_SIZE_LIMIT:
        del trees[-1]

    # done!
    g.permacache.set(key, trees)


def add_message_nolock(user_id, message):
    return _add_message_nolock(messages_key(user_id), message)

def _conversation(trees, parent):
    from r2.models import Message
    if parent._id in trees:
        convo = trees[parent._id]
        if convo:
            m = Message._byID(convo[0], data = True)
        if not convo or m.first_message == m.parent_id:
            return [(parent._id, convo)]

    # if we get to this point, either we didn't find the conversation,
    # or the first child of the result was not the actual first child.
    # To the database!
    rules = [Message.c.first_message == parent._id]
    if c.user_is_admin:
        rules.append(Message.c._spam == (True, False))
        rules.append(Message.c._deleted == (True, False))
    m = Message._query(*rules, data=True)
    return compute_message_trees([parent] + list(m))

def conversation(user, parent):
    trees = dict(user_messages(user))
    return _conversation(trees, parent)


def user_messages(user, update = False):
    key = messages_key(user._id)
    trees = g.permacache.get(key)
    if not trees or update:
        trees = user_messages_nocache(user)
        g.permacache.set(key, trees)
    return trees

def _process_message_query(inbox):
    if hasattr(inbox, 'prewrap_fn'):
        return [inbox.prewrap_fn(i) for i in inbox]
    return list(inbox)


def _load_messages(mlist):
    from r2.models import Message
    m = {}
    ids = [x for x in mlist if not isinstance(x, Message)]
    if ids:
        m = Message._by_fullname(ids, return_dict = True, data = True)
    messages = [m.get(x, x) for x in mlist]
    return messages

def user_messages_nocache(user):
    """
    Just like user_messages, but avoiding the cache
    """
    from r2.lib.db import queries
    inbox = _process_message_query(queries.get_inbox_messages(user))
    sent = _process_message_query(queries.get_sent(user))
    messages = _load_messages(list(chain(inbox, sent)))
    return compute_message_trees(messages)

def sr_messages_key(sr_id):
    return 'sr_messages_conversation_' + str(sr_id)

def sr_messages_lock_key(sr_id):
    return 'sr_messages_conversation_lock_' + str(sr_id)


def subreddit_messages(sr, update = False):
    key = sr_messages_key(sr._id)
    trees = g.permacache.get(key)
    if not trees or update:
        trees = subreddit_messages_nocache(sr)
        g.permacache.set(key, trees)
    return trees

def moderator_messages(sr_ids):
    from r2.models import Subreddit

    srs = Subreddit._byID(sr_ids)
    sr_ids = [sr_id for sr_id, sr in srs.iteritems()
              if sr.is_moderator_with_perms(c.user, 'mail')]

    def multi_load_tree(sr_ids):
        res = {}
        for sr_id in sr_ids:
            trees = subreddit_messages_nocache(srs[sr_id])
            if trees:
                res[sr_id] = trees
        return res

    res = sgm(g.permacache, sr_ids, miss_fn = multi_load_tree,
              prefix = sr_messages_key(""))

    return sorted(chain(*res.values()), key = tree_sort_fn, reverse = True)

def subreddit_messages_nocache(sr):
    """
    Just like user_messages, but avoiding the cache
    """
    from r2.lib.db import queries
    inbox = _process_message_query(queries.get_subreddit_messages(sr))
    messages = _load_messages(inbox)
    return compute_message_trees(messages)


def add_sr_message_nolock(sr_id, message):
    return _add_message_nolock(sr_messages_key(sr_id), message)

def sr_conversation(sr, parent):
    trees = dict(subreddit_messages(sr))
    return _conversation(trees, parent)


def compute_message_trees(messages):
    from r2.models import Message
    roots = set()
    threads = {}
    mdict = {}
    messages = sorted(messages, key = lambda m: m._date, reverse = True)

    for m in messages:
        if not m._loaded:
            m._load()
        mdict[m._id] = m
        if m.first_message:
            roots.add(m.first_message)
            threads.setdefault(m.first_message, set()).add(m._id)
        else:
            roots.add(m._id)

    # load any top-level messages which are not in the original list
    missing = [m for m in roots if m not in mdict]
    if missing:
        mdict.update(Message._byID(tup(missing),
                                   return_dict = True, data = True))

    # sort threads in chrono order
    for k in threads:
        threads[k] = list(sorted(threads[k]))

    tree = [(root, threads.get(root, [])) for root in roots]
    tree.sort(key = tree_sort_fn, reverse = True)

    return tree

def tree_sort_fn(tree):
    root, threads = tree
    return threads[-1] if threads else root

def _populate(after_id = None, estimate=54301242):
    from r2.models import CommentSortsCache, desc
    from r2.lib.db import tdb_cassandra
    from r2.lib import utils

    # larger has a chance to decrease the number of Cassandra writes,
    # but the probability is low
    chunk_size = 5000

    q = Comment._query(Comment.c._spam==(True,False),
                       Comment.c._deleted==(True,False),
                       sort=desc('_date'))

    if after_id is not None:
        q._after(Comment._byID(after_id))

    q = utils.fetch_things2(q, chunk_size=chunk_size)
    q = utils.progress(q, verbosity=chunk_size, estimate = estimate)

    for chunk in utils.in_chunks(q, chunk_size):
        chunk = filter(lambda x: hasattr(x, 'link_id'), chunk)
        update_comment_votes(chunk, write_consistency_level = tdb_cassandra.CL.ONE)

########NEW FILE########
__FILENAME__ = configparse
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import re


class ConfigValue(object):
    _bool_map = dict(true=True, false=False)

    @staticmethod
    def str(v, key=None, data=None):
        return str(v)

    @staticmethod
    def int(v, key=None, data=None):
        return int(v)

    @staticmethod
    def float(v, key=None, data=None):
        return float(v)

    @staticmethod
    def bool(v, key=None, data=None):
        if v in (True, False, None):
            return bool(v)
        try:
            return ConfigValue._bool_map[v.lower()]
        except KeyError:
            raise ValueError("Unknown value for %r: %r" % (key, v))

    @staticmethod
    def tuple(v, key=None, data=None):
        return tuple(ConfigValue.to_iter(v))

    @staticmethod
    def dict(key_type, value_type):
        def parse(v, key=None, data=None):
            return {key_type(x): value_type(y)
                    for x, y in (
                        i.split(':', 1) for i in ConfigValue.to_iter(v))}
        return parse

    @staticmethod
    def choice(v, key, data):
        if v not in data:
            raise ValueError("Unknown option for %r: %r not in %r" % (key, v, data))
        return data[v]

    @staticmethod
    def to_iter(v, delim = ','):
        return (x.strip() for x in v.split(delim) if x)

    @staticmethod
    def timeinterval(v, key=None, data=None):
        # this import is at function level because it relies on the cythonized
        # modules being present which is a problem for plugin __init__s that
        # use this module since they are imported in the early stages of the
        # makefile
        from r2.lib.utils import timeinterval_fromstr
        return timeinterval_fromstr(v)

    messages_re = re.compile(r'"([^"]+)"')
    @staticmethod
    def messages(v, key=None, data=None):
        return ConfigValue.messages_re.findall(v.decode("string_escape"))


class ConfigValueParser(dict):
    def __init__(self, raw_data):
        dict.__init__(self, raw_data)
        self.config_keys = {}
        self.raw_data = raw_data

    def add_spec(self, spec):
        new_keys = []
        for parser, keys in spec.iteritems():
            # keys can be either a list or a dict
            for key in keys:
                assert key not in self.config_keys
                # if keys is a dict, the value is passed as extra data to the parser.
                extra_data = keys[key] if type(keys) is dict else None
                self.config_keys[key] = (parser, extra_data)
                new_keys.append(key)
        self._update_values(new_keys)

    def _update_values(self, keys):
        for key in keys:
            if key not in self.raw_data:
                continue

            value = self.raw_data[key]
            if key in self.config_keys:
                parser, extra_data = self.config_keys[key]
                value = parser(value, key, extra_data)
            self[key] = value

########NEW FILE########
__FILENAME__ = ipaddress
#!/usr/bin/python3
#
# Copyright 2007 Google Inc.
#  Licensed to PSF under a Contributor Agreement.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied. See the License for the specific language governing
# permissions and limitations under the License.

"""A fast, lightweight IPv4/IPv6 manipulation library in Python.

This library is used to create/poke/manipulate IPv4 and IPv6 addresses
and networks.

"""

__version__ = '1.0'

import struct

IPV4LENGTH = 32
IPV6LENGTH = 128


class AddressValueError(ValueError):
    """A Value Error related to the address."""


class NetmaskValueError(ValueError):
    """A Value Error related to the netmask."""


def ip_address(address, version=None):
    """Take an IP string/int and return an object of the correct type.

    Args:
        address: A string or integer, the IP address.  Either IPv4 or
          IPv6 addresses may be supplied; integers less than 2**32 will
          be considered to be IPv4 by default.
        version: An Integer, 4 or 6. If set, don't try to automatically
          determine what the IP address type is. important for things
          like ip_address(1), which could be IPv4, '192.0.2.1',  or IPv6,
          '2001:db8::1'.

    Returns:
        An IPv4Address or IPv6Address object.

    Raises:
        ValueError: if the string passed isn't either a v4 or a v6
          address.

    """
    if version:
        if version == 4:
            return IPv4Address(address)
        elif version == 6:
            return IPv6Address(address)

    try:
        return IPv4Address(address)
    except (AddressValueError, NetmaskValueError):
        pass

    try:
        return IPv6Address(address)
    except (AddressValueError, NetmaskValueError):
        pass

    raise ValueError('%r does not appear to be an IPv4 or IPv6 address' %
                     address)


def ip_network(address, version=None, strict=True):
    """Take an IP string/int and return an object of the correct type.

    Args:
        address: A string or integer, the IP network.  Either IPv4 or
          IPv6 networks may be supplied; integers less than 2**32 will
          be considered to be IPv4 by default.
        version: An Integer, if set, don't try to automatically
          determine what the IP address type is. important for things
          like ip_network(1), which could be IPv4, '192.0.2.1/32', or IPv6,
          '2001:db8::1/128'.

    Returns:
        An IPv4Network or IPv6Network object.

    Raises:
        ValueError: if the string passed isn't either a v4 or a v6
          address. Or if the network has host bits set.

    """
    if version:
        if version == 4:
            return IPv4Network(address, strict)
        elif version == 6:
            return IPv6Network(address, strict)

    try:
        return IPv4Network(address, strict)
    except (AddressValueError, NetmaskValueError):
        pass

    try:
        return IPv6Network(address, strict)
    except (AddressValueError, NetmaskValueError):
        pass

    raise ValueError('%r does not appear to be an IPv4 or IPv6 network' %
                     address)


def ip_interface(address, version=None):
    """Take an IP string/int and return an object of the correct type.

    Args:
        address: A string or integer, the IP address.  Either IPv4 or
          IPv6 addresses may be supplied; integers less than 2**32 will
          be considered to be IPv4 by default.
        version: An Integer, if set, don't try to automatically
          determine what the IP address type is. important for things
          like ip_network(1), which could be IPv4, '192.0.2.1/32', or IPv6,
          '2001:db8::1/128'.

    Returns:
        An IPv4Network or IPv6Network object.

    Raises:
        ValueError: if the string passed isn't either a v4 or a v6
          address.

    Notes:
        The IPv?Interface classes describe an Address on a particular
        Network, so they're basically a combination of both the Address
        and Network classes.
    """
    if version:
        if version == 4:
            return IPv4Interface(address)
        elif version == 6:
            return IPv6Interface(address)

    try:
        return IPv4Interface(address)
    except (AddressValueError, NetmaskValueError):
        pass

    try:
        return IPv6Interface(address)
    except (AddressValueError, NetmaskValueError):
        pass

    raise ValueError('%r does not appear to be an IPv4 or IPv6 network' %
                     address)


def v4_int_to_packed(address):
    """The binary representation of this address.

    Args:
        address: An integer representation of an IPv4 IP address.

    Returns:
        The binary representation of this address.

    Raises:
        ValueError: If the integer is too large to be an IPv4 IP
          address.
    """
    if address > _BaseV4._ALL_ONES:
        raise ValueError('Address too large for IPv4')
    return struct.pack('!I', address)


def v6_int_to_packed(address):
    """The binary representation of this address.

    Args:
        address: An integer representation of an IPv6 IP address.

    Returns:
        The binary representation of this address.
    """
    return struct.pack('!QQ', address >> 64, address & (2**64 - 1))


def _find_address_range(addresses):
    """Find a sequence of addresses.

    Args:
        addresses: a list of IPv4 or IPv6 addresses.

    Returns:
        A tuple containing the first and last IP addresses in the sequence.

    """
    first = last = addresses[0]
    for ip in addresses[1:]:
        if ip._ip == last._ip + 1:
            last = ip
        else:
            break
    return (first, last)

def _get_prefix_length(number1, number2, bits):
    """Get the number of leading bits that are same for two numbers.

    Args:
        number1: an integer.
        number2: another integer.
        bits: the maximum number of bits to compare.

    Returns:
        The number of leading bits that are the same for two numbers.

    """
    for i in range(bits):
        if number1 >> i == number2 >> i:
            return bits - i
    return 0

def _count_righthand_zero_bits(number, bits):
    """Count the number of zero bits on the right hand side.

    Args:
        number: an integer.
        bits: maximum number of bits to count.

    Returns:
        The number of zero bits on the right hand side of the number.

    """
    if number == 0:
        return bits
    for i in range(bits):
        if (number >> i) % 2:
            return i


def summarize_address_range(first, last):
    """Summarize a network range given the first and last IP addresses.

    Example:
        >>> summarize_address_range(IPv4Address('192.0.2.0'),
            IPv4Address('192.0.2.130'))
        [IPv4Network('192.0.2.0/25'), IPv4Network('192.0.2.128/31'),
        IPv4Network('192.0.2.130/32')]

    Args:
        first: the first IPv4Address or IPv6Address in the range.
        last: the last IPv4Address or IPv6Address in the range.

    Returns:
        An iterator of the summarized IPv(4|6) network objects.

    Raise:
        TypeError:
            If the first and last objects are not IP addresses.
            If the first and last objects are not the same version.
        ValueError:
            If the last object is not greater than the first.
            If the version is not 4 or 6.

    """
    if not (isinstance(first, _BaseAddress) and isinstance(last, _BaseAddress)):
        raise TypeError('first and last must be IP addresses, not networks')
    if first.version != last.version:
        raise TypeError("%s and %s are not of the same version" % (
                str(first), str(last)))
    if first > last:
        raise ValueError('last IP address must be greater than first')

    networks = []

    if first.version == 4:
        ip = IPv4Network
    elif first.version == 6:
        ip = IPv6Network
    else:
        raise ValueError('unknown IP version')

    ip_bits = first._max_prefixlen
    first_int = first._ip
    last_int = last._ip
    while first_int <= last_int:
        nbits = _count_righthand_zero_bits(first_int, ip_bits)
        current = None
        while nbits >= 0:
            addend = 2**nbits - 1
            current = first_int + addend
            nbits -= 1
            if current <= last_int:
                break
        prefix = _get_prefix_length(first_int, current, ip_bits)
        net = ip('%s/%d' % (str(first), prefix))
        yield net
        #networks.append(net)
        if current == ip._ALL_ONES:
            break
        first_int = current + 1
        first = ip_address(first_int, version=first._version)

def _collapse_addresses_recursive(addresses):
    """Loops through the addresses, collapsing concurrent netblocks.

    Example:

        ip1 = IPv4Network('192.0.2.0/26')
        ip2 = IPv4Network('192.0.2.64/26')
        ip3 = IPv4Network('192.0.2.128/26')
        ip4 = IPv4Network('192.0.2.192/26')

        _collapse_addresses_recursive([ip1, ip2, ip3, ip4]) ->
          [IPv4Network('192.0.2.0/24')]

        This shouldn't be called directly; it is called via
          collapse_addresses([]).

    Args:
        addresses: A list of IPv4Network's or IPv6Network's

    Returns:
        A list of IPv4Network's or IPv6Network's depending on what we were
        passed.

    """
    ret_array = []
    optimized = False

    for cur_addr in addresses:
        if not ret_array:
            ret_array.append(cur_addr)
            continue
        if (cur_addr.network_address >= ret_array[-1].network_address and
            cur_addr.broadcast_address <= ret_array[-1].broadcast_address):
            optimized = True
        elif cur_addr == list(ret_array[-1].supernet().subnets())[1]:
            ret_array.append(ret_array.pop().supernet())
            optimized = True
        else:
            ret_array.append(cur_addr)

    if optimized:
        return _collapse_addresses_recursive(ret_array)

    return ret_array


def collapse_addresses(addresses):
    """Collapse a list of IP objects.

    Example:
        collapse_addresses([IPv4Network('192.0.2.0/25'),
                            IPv4Network('192.0.2.128/25')]) ->
                           [IPv4Network('192.0.2.0/24')]

    Args:
        addresses: An iterator of IPv4Network or IPv6Network objects.

    Returns:
        An iterator of the collapsed IPv(4|6)Network objects.

    Raises:
        TypeError: If passed a list of mixed version objects.

    """
    i = 0
    addrs = []
    ips = []
    nets = []

    # split IP addresses and networks
    for ip in addresses:
        if isinstance(ip, _BaseAddress):
            if ips and ips[-1]._version != ip._version:
                raise TypeError("%s and %s are not of the same version" % (
                        str(ip), str(ips[-1])))
            ips.append(ip)
        elif ip._prefixlen == ip._max_prefixlen:
            if ips and ips[-1]._version != ip._version:
                raise TypeError("%s and %s are not of the same version" % (
                        str(ip), str(ips[-1])))
            try:
                ips.append(ip.ip)
            except AttributeError:
                ips.append(ip.network_address)
        else:
            if nets and nets[-1]._version != ip._version:
                raise TypeError("%s and %s are not of the same version" % (
                        str(ip), str(nets[-1])))
            nets.append(ip)

    # sort and dedup
    ips = sorted(set(ips))
    nets = sorted(set(nets))

    while i < len(ips):
        (first, last) = _find_address_range(ips[i:])
        i = ips.index(last) + 1
        addrs.extend(summarize_address_range(first, last))

    return iter(_collapse_addresses_recursive(sorted(
        addrs + nets, key=_BaseNetwork._get_networks_key)))


def get_mixed_type_key(obj):
    """Return a key suitable for sorting between networks and addresses.

    Address and Network objects are not sortable by default; they're
    fundamentally different so the expression

        IPv4Address('192.0.2.0') <= IPv4Network('192.0.2.0/24')

    doesn't make any sense.  There are some times however, where you may wish
    to have ipaddress sort these for you anyway. If you need to do this, you
    can use this function as the key= argument to sorted().

    Args:
      obj: either a Network or Address object.
    Returns:
      appropriate key.

    """
    if isinstance(obj, _BaseNetwork):
        return obj._get_networks_key()
    elif isinstance(obj, _BaseAddress):
        return obj._get_address_key()
    return NotImplemented


class _IPAddressBase(object):

    """The mother class."""

    @property
    def exploded(self):
        """Return the longhand version of the IP address as a string."""
        return self._explode_shorthand_ip_string()

    @property
    def compressed(self):
        """Return the shorthand version of the IP address as a string."""
        return str(self)

    def _ip_int_from_prefix(self, prefixlen=None):
        """Turn the prefix length netmask into a int for comparison.

        Args:
            prefixlen: An integer, the prefix length.

        Returns:
            An integer.

        """
        if not prefixlen and prefixlen != 0:
            prefixlen = self._prefixlen
        return self._ALL_ONES ^ (self._ALL_ONES >> prefixlen)

    def _prefix_from_ip_int(self, ip_int, mask=32):
        """Return prefix length from the decimal netmask.

        Args:
            ip_int: An integer, the IP address.
            mask: The netmask.  Defaults to 32.

        Returns:
            An integer, the prefix length.

        """
        while mask:
            if ip_int & 1 == 1:
                break
            ip_int >>= 1
            mask -= 1

        return mask

    def _ip_string_from_prefix(self, prefixlen=None):
        """Turn a prefix length into a dotted decimal string.

        Args:
            prefixlen: An integer, the netmask prefix length.

        Returns:
            A string, the dotted decimal netmask string.

        """
        if not prefixlen:
            prefixlen = self._prefixlen
        return self._string_from_ip_int(self._ip_int_from_prefix(prefixlen))


class _BaseAddress(_IPAddressBase):

    """A generic IP object.

    This IP class contains the version independent methods which are
    used by single IP addresses.

    """

    def __init__(self, address):
        if (not isinstance(address, bytes)
            and '/' in str(address)):
            raise AddressValueError(address)

    def __index__(self):
        return self._ip

    def __int__(self):
        return self._ip

    def __hex__(self):
        return hex(self._ip)

    def __eq__(self, other):
        try:
            return (self._ip == other._ip
                    and self._version == other._version)
        except AttributeError:
            return NotImplemented

    def __ne__(self, other):
        eq = self.__eq__(other)
        if eq is NotImplemented:
            return NotImplemented
        return not eq

    def __le__(self, other):
        gt = self.__gt__(other)
        if gt is NotImplemented:
            return NotImplemented
        return not gt

    def __ge__(self, other):
        lt = self.__lt__(other)
        if lt is NotImplemented:
            return NotImplemented
        return not lt

    def __lt__(self, other):
        if self._version != other._version:
            raise TypeError('%s and %s are not of the same version' % (
                    str(self), str(other)))
        if not isinstance(other, _BaseAddress):
            raise TypeError('%s and %s are not of the same type' % (
                    str(self), str(other)))
        if self._ip != other._ip:
            return self._ip < other._ip
        return False

    def __gt__(self, other):
        if self._version != other._version:
            raise TypeError('%s and %s are not of the same version' % (
                    str(self), str(other)))
        if not isinstance(other, _BaseAddress):
            raise TypeError('%s and %s are not of the same type' % (
                    str(self), str(other)))
        if self._ip != other._ip:
            return self._ip > other._ip
        return False

    # Shorthand for Integer addition and subtraction. This is not
    # meant to ever support addition/subtraction of addresses.
    def __add__(self, other):
        if not isinstance(other, int):
            return NotImplemented
        return ip_address(int(self) + other, version=self._version)

    def __sub__(self, other):
        if not isinstance(other, int):
            return NotImplemented
        return ip_address(int(self) - other, version=self._version)

    def __repr__(self):
        return '%s(%r)' % (self.__class__.__name__, str(self))

    def __str__(self):
        return  '%s' % self._string_from_ip_int(self._ip)

    def __hash__(self):
        return hash(hex(int(self._ip)))

    def _get_address_key(self):
        return (self._version, self)

    @property
    def version(self):
        raise NotImplementedError('BaseIP has no version')


class _BaseNetwork(_IPAddressBase):

    """A generic IP object.

    This IP class contains the version independent methods which are
    used by networks.

    """

    def __init__(self, address):
        self._cache = {}

    def __index__(self):
        return int(self.network_address) ^ self.prefixlen

    def __int__(self):
        return int(self.network_address)

    def __repr__(self):
        return '%s(%r)' % (self.__class__.__name__, str(self))

    def hosts(self):
        """Generate Iterator over usable hosts in a network.

           This is like __iter__ except it doesn't return the network
           or broadcast addresses.

        """
        cur = int(self.network_address) + 1
        bcast = int(self.broadcast_address) - 1
        while cur <= bcast:
            cur += 1
            yield ip_address(cur - 1, version=self._version)

    def __iter__(self):
        cur = int(self.network_address)
        bcast = int(self.broadcast_address)
        while cur <= bcast:
            cur += 1
            yield ip_address(cur - 1, version=self._version)

    def __getitem__(self, n):
        network = int(self.network_address)
        broadcast = int(self.broadcast_address)
        if n >= 0:
            if network + n > broadcast:
                raise IndexError
            return ip_address(network + n, version=self._version)
        else:
            n += 1
            if broadcast + n < network:
                raise IndexError
            return ip_address(broadcast + n, version=self._version)

    def __lt__(self, other):
        if self._version != other._version:
            raise TypeError('%s and %s are not of the same version' % (
                    str(self), str(other)))
        if not isinstance(other, _BaseNetwork):
            raise TypeError('%s and %s are not of the same type' % (
                    str(self), str(other)))
        if self.network_address != other.network_address:
            return self.network_address < other.network_address
        if self.netmask != other.netmask:
            return self.netmask < other.netmask
        return False

    def __gt__(self, other):
        if self._version != other._version:
            raise TypeError('%s and %s are not of the same version' % (
                    str(self), str(other)))
        if not isinstance(other, _BaseNetwork):
            raise TypeError('%s and %s are not of the same type' % (
                    str(self), str(other)))
        if self.network_address != other.network_address:
            return self.network_address > other.network_address
        if self.netmask != other.netmask:
            return self.netmask > other.netmask
        return False

    def __le__(self, other):
        gt = self.__gt__(other)
        if gt is NotImplemented:
            return NotImplemented
        return not gt

    def __ge__(self, other):
        lt = self.__lt__(other)
        if lt is NotImplemented:
            return NotImplemented
        return not lt

    def __eq__(self, other):
        if not isinstance(other, _BaseNetwork):
            raise TypeError('%s and %s are not of the same type' % (
                    str(self), str(other)))
        return (self._version == other._version and
                self.network_address == other.network_address and
                int(self.netmask) == int(other.netmask))

    def __ne__(self, other):
        eq = self.__eq__(other)
        if eq is NotImplemented:
            return NotImplemented
        return not eq

    def __str__(self):
        return  '%s/%s' % (str(self.ip),
                           str(self._prefixlen))

    def __hash__(self):
        return hash(int(self.network_address) ^ int(self.netmask))

    def __contains__(self, other):
        # always false if one is v4 and the other is v6.
        if self._version != other._version:
          return False
        # dealing with another network.
        if isinstance(other, _BaseNetwork):
            return False
        # dealing with another address
        else:
            # address
            return (int(self.network_address) <= int(other._ip) <=
                    int(self.broadcast_address))

    def overlaps(self, other):
        """Tell if self is partly contained in other."""
        return self.network_address in other or (
            self.broadcast_address in other or (
                other.network_address in self or (
                    other.broadcast_address in self)))

    @property
    def broadcast_address(self):
        x = self._cache.get('broadcast_address')
        if x is None:
            x = ip_address(int(self.network_address) | int(self.hostmask),
                           version=self._version)
            self._cache['broadcast_address'] = x
        return x

    @property
    def hostmask(self):
        x = self._cache.get('hostmask')
        if x is None:
            x = ip_address(int(self.netmask) ^ self._ALL_ONES,
                          version=self._version)
            self._cache['hostmask'] = x
        return x

    @property
    def network(self):
        return ip_network('%s/%d' % (str(self.network_address),
                                     self.prefixlen))

    @property
    def with_prefixlen(self):
        return '%s/%d' % (str(self.ip), self._prefixlen)

    @property
    def with_netmask(self):
        return '%s/%s' % (str(self.ip), str(self.netmask))

    @property
    def with_hostmask(self):
        return '%s/%s' % (str(self.ip), str(self.hostmask))

    @property
    def num_addresses(self):
        """Number of hosts in the current subnet."""
        return int(self.broadcast_address) - int(self.network_address) + 1

    @property
    def version(self):
        raise NotImplementedError('BaseNet has no version')

    @property
    def prefixlen(self):
        return self._prefixlen

    def address_exclude(self, other):
        """Remove an address from a larger block.

        For example:

            addr1 = ip_network('192.0.2.0/28')
            addr2 = ip_network('192.0.2.1/32')
            addr1.address_exclude(addr2) =
                [IPv4Network('192.0.2.0/32'), IPv4Network('192.0.2.2/31'),
                IPv4Network('192.0.2.4/30'), IPv4Network('192.0.2.8/29')]

        or IPv6:

            addr1 = ip_network('2001:db8::1/32')
            addr2 = ip_network('2001:db8::1/128')
            addr1.address_exclude(addr2) =
                [ip_network('2001:db8::1/128'),
                ip_network('2001:db8::2/127'),
                ip_network('2001:db8::4/126'),
                ip_network('2001:db8::8/125'),
                ...
                ip_network('2001:db8:8000::/33')]

        Args:
            other: An IPv4Network or IPv6Network object of the same type.

        Returns:
            An iterator of the the IPv(4|6)Network objects which is self
            minus other.

        Raises:
            TypeError: If self and other are of difffering address
              versions, or if other is not a network object.
            ValueError: If other is not completely contained by self.

        """
        if not self._version == other._version:
            raise TypeError("%s and %s are not of the same version" % (
                str(self), str(other)))

        if not isinstance(other, _BaseNetwork):
            raise TypeError("%s is not a network object" % str(other))

        if not (other.network_address >= self.network_address and
                other.broadcast_address <= self.broadcast_address):
            raise ValueError('%s not contained in %s' % (str(other), str(self)))

        if other == self:
            raise StopIteration

        ret_addrs = []

        # Make sure we're comparing the network of other.
        other = ip_network('%s/%s' % (str(other.network_address),
                                      str(other.prefixlen)),
                           version=other._version)

        s1, s2 = self.subnets()
        while s1 != other and s2 != other:
            if (other.network_address >= s1.network_address and
                other.broadcast_address <= s1.broadcast_address):
                yield s2
                s1, s2 = s1.subnets()
            elif (other.network_address >= s2.network_address and
                  other.broadcast_address <= s2.broadcast_address):
                yield s1
                s1, s2 = s2.subnets()
            else:
                # If we got here, there's a bug somewhere.
                raise AssertionError('Error performing exclusion: '
                                     's1: %s s2: %s other: %s' %
                                     (str(s1), str(s2), str(other)))
        if s1 == other:
            yield s2
        elif s2 == other:
            yield s1
        else:
            # If we got here, there's a bug somewhere.
            raise AssertionError('Error performing exclusion: '
                                 's1: %s s2: %s other: %s' %
                                 (str(s1), str(s2), str(other)))

    def compare_networks(self, other):
        """Compare two IP objects.

        This is only concerned about the comparison of the integer
        representation of the network addresses.  This means that the
        host bits aren't considered at all in this method.  If you want
        to compare host bits, you can easily enough do a
        'HostA._ip < HostB._ip'

        Args:
            other: An IP object.

        Returns:
            If the IP versions of self and other are the same, returns:

            -1 if self < other:
              eg: IPv4Network('192.0.2.0/25') < IPv4Network('192.0.2.128/25')
              IPv6Network('2001:db8::1000/124') <
                  IPv6Network('2001:db8::2000/124')
            0 if self == other
              eg: IPv4Network('192.0.2.0/24') == IPv4Network('192.0.2.0/24')
              IPv6Network('2001:db8::1000/124') ==
                  IPv6Network('2001:db8::1000/124')
            1 if self > other
              eg: IPv4Network('192.0.2.128/25') > IPv4Network('192.0.2.0/25')
                  IPv6Network('2001:db8::2000/124') >
                      IPv6Network('2001:db8::1000/124')

          Raises:
              TypeError if the IP versions are different.

        """
        # does this need to raise a ValueError?
        if self._version != other._version:
            raise TypeError('%s and %s are not of the same type' % (
                    str(self), str(other)))
        # self._version == other._version below here:
        if self.network_address < other.network_address:
            return -1
        if self.network_address > other.network_address:
            return 1
        # self.network_address == other.network_address below here:
        if self.netmask < other.netmask:
            return -1
        if self.netmask > other.netmask:
            return 1
        return 0

    def _get_networks_key(self):
        """Network-only key function.

        Returns an object that identifies this address' network and
        netmask. This function is a suitable "key" argument for sorted()
        and list.sort().

        """
        return (self._version, self.network_address, self.netmask)

    def subnets(self, prefixlen_diff=1, new_prefix=None):
        """The subnets which join to make the current subnet.

        In the case that self contains only one IP
        (self._prefixlen == 32 for IPv4 or self._prefixlen == 128
        for IPv6), yield an iterator with just ourself.

        Args:
            prefixlen_diff: An integer, the amount the prefix length
              should be increased by. This should not be set if
              new_prefix is also set.
            new_prefix: The desired new prefix length. This must be a
              larger number (smaller prefix) than the existing prefix.
              This should not be set if prefixlen_diff is also set.

        Returns:
            An iterator of IPv(4|6) objects.

        Raises:
            ValueError: The prefixlen_diff is too small or too large.
                OR
            prefixlen_diff and new_prefix are both set or new_prefix
              is a smaller number than the current prefix (smaller
              number means a larger network)

        """
        if self._prefixlen == self._max_prefixlen:
            yield self
            return

        if new_prefix is not None:
            if new_prefix < self._prefixlen:
                raise ValueError('new prefix must be longer')
            if prefixlen_diff != 1:
                raise ValueError('cannot set prefixlen_diff and new_prefix')
            prefixlen_diff = new_prefix - self._prefixlen

        if prefixlen_diff < 0:
            raise ValueError('prefix length diff must be > 0')
        new_prefixlen = self._prefixlen + prefixlen_diff

        if not self._is_valid_netmask(str(new_prefixlen)):
            raise ValueError(
                'prefix length diff %d is invalid for netblock %s' % (
                    new_prefixlen, str(self)))

        first = ip_network('%s/%s' % (str(self.network_address),
                                     str(self._prefixlen + prefixlen_diff)),
                         version=self._version)

        yield first
        current = first
        while True:
            broadcast = current.broadcast_address
            if broadcast == self.broadcast_address:
                return
            new_addr = ip_address(int(broadcast) + 1, version=self._version)
            current = ip_network('%s/%s' % (str(new_addr), str(new_prefixlen)),
                                version=self._version)

            yield current

    def masked(self):
        """Return the network object with the host bits masked out."""
        return ip_network('%s/%d' % (self.network_address, self._prefixlen),
                         version=self._version)

    def supernet(self, prefixlen_diff=1, new_prefix=None):
        """The supernet containing the current network.

        Args:
            prefixlen_diff: An integer, the amount the prefix length of
              the network should be decreased by.  For example, given a
              /24 network and a prefixlen_diff of 3, a supernet with a
              /21 netmask is returned.

        Returns:
            An IPv4 network object.

        Raises:
            ValueError: If self.prefixlen - prefixlen_diff < 0. I.e., you have a
              negative prefix length.
                OR
            If prefixlen_diff and new_prefix are both set or new_prefix is a
              larger number than the current prefix (larger number means a
              smaller network)

        """
        if self._prefixlen == 0:
            return self

        if new_prefix is not None:
            if new_prefix > self._prefixlen:
                raise ValueError('new prefix must be shorter')
            if prefixlen_diff != 1:
                raise ValueError('cannot set prefixlen_diff and new_prefix')
            prefixlen_diff = self._prefixlen - new_prefix


        if self.prefixlen - prefixlen_diff < 0:
            raise ValueError(
                'current prefixlen is %d, cannot have a prefixlen_diff of %d' %
                (self.prefixlen, prefixlen_diff))
        # TODO (pmoody): optimize this.
        t = ip_network('%s/%d' % (str(self.network_address),
                                    self.prefixlen - prefixlen_diff),
                         version=self._version, strict=False)
        return ip_network('%s/%d' % (str(t.network_address), t.prefixlen),
                          version=t._version)


class _BaseV4(object):

    """Base IPv4 object.

    The following methods are used by IPv4 objects in both single IP
    addresses and networks.

    """

    # Equivalent to 255.255.255.255 or 32 bits of 1's.
    _ALL_ONES = (2**IPV4LENGTH) - 1
    _DECIMAL_DIGITS = frozenset('0123456789')

    def __init__(self, address):
        self._version = 4
        self._max_prefixlen = IPV4LENGTH

    def _explode_shorthand_ip_string(self):
        return str(self)

    def _ip_int_from_string(self, ip_str):
        """Turn the given IP string into an integer for comparison.

        Args:
            ip_str: A string, the IP ip_str.

        Returns:
            The IP ip_str as an integer.

        Raises:
            AddressValueError: if ip_str isn't a valid IPv4 Address.

        """
        octets = ip_str.split('.')
        if len(octets) != 4:
            raise AddressValueError(ip_str)

        packed_ip = 0
        for oc in octets:
            try:
                packed_ip = (packed_ip << 8) | self._parse_octet(oc)
            except ValueError:
                raise AddressValueError(ip_str)
        return packed_ip

    def _parse_octet(self, octet_str):
        """Convert a decimal octet into an integer.

        Args:
            octet_str: A string, the number to parse.

        Returns:
            The octet as an integer.

        Raises:
            ValueError: if the octet isn't strictly a decimal from [0..255].

        """
        # Whitelist the characters, since int() allows a lot of bizarre stuff.
        if not self._DECIMAL_DIGITS.issuperset(octet_str):
            raise ValueError
        octet_int = int(octet_str, 10)
        # Disallow leading zeroes, because no clear standard exists on
        # whether these should be interpreted as decimal or octal.
        if octet_int > 255 or (octet_str[0] == '0' and len(octet_str) > 1):
            raise ValueError
        return octet_int

    def _string_from_ip_int(self, ip_int):
        """Turns a 32-bit integer into dotted decimal notation.

        Args:
            ip_int: An integer, the IP address.

        Returns:
            The IP address as a string in dotted decimal notation.

        """
        octets = []
        for _ in range(4):
            octets.insert(0, str(ip_int & 0xFF))
            ip_int >>= 8
        return '.'.join(octets)

    @property
    def max_prefixlen(self):
        return self._max_prefixlen

    @property
    def version(self):
        return self._version

    @property
    def is_reserved(self):
       """Test if the address is otherwise IETF reserved.

        Returns:
            A boolean, True if the address is within the
            reserved IPv4 Network range.

       """
       reserved_network = IPv4Network('240.0.0.0/4')
       if isinstance(self, _BaseAddress):
           return self in reserved_network
       return (self.network_address in reserved_network and
               self.broadcast_address in reserved_network)

    @property
    def is_private(self):
        """Test if this address is allocated for private networks.

        Returns:
            A boolean, True if the address is reserved per RFC 1918.

        """
        private_10 = IPv4Network('10.0.0.0/8')
        private_172 = IPv4Network('172.16.0.0/12')
        private_192 = IPv4Network('192.168.0.0/16')
        if isinstance(self, _BaseAddress):
            return (self in private_10 or self in private_172 or
                    self in private_192)
        else:
            return ((self.network_address in private_10 and
                     self.broadcast_address in private_10) or
                    (self.network_address in private_172 and
                     self.broadcast_address in private_172) or
                    (self.network_address in private_192 and
                     self.broadcast_address in private_192))

    @property
    def is_multicast(self):
        """Test if the address is reserved for multicast use.

        Returns:
            A boolean, True if the address is multicast.
            See RFC 3171 for details.

        """
        multicast_network = IPv4Network('224.0.0.0/4')
        if isinstance(self, _BaseAddress):
            return self in IPv4Network('224.0.0.0/4')
        return (self.network_address in multicast_network and
                self.broadcast_address in multicast_network)

    @property
    def is_unspecified(self):
        """Test if the address is unspecified.

        Returns:
            A boolean, True if this is the unspecified address as defined in
            RFC 5735 3.

        """
        unspecified_address = IPv4Address('0.0.0.0')
        if isinstance(self, _BaseAddress):
            return self in unspecified_address
        return (self.network_address == self.broadcast_address ==
                unspecified_address)

    @property
    def is_loopback(self):
        """Test if the address is a loopback address.

        Returns:
            A boolean, True if the address is a loopback per RFC 3330.

        """
        loopback_address = IPv4Network('127.0.0.0/8')
        if isinstance(self, _BaseAddress):
            return self in loopback_address

        return (self.network_address in loopback_address and
                self.broadcast_address in loopback_address)

    @property
    def is_link_local(self):
        """Test if the address is reserved for link-local.

        Returns:
            A boolean, True if the address is link-local per RFC 3927.

        """
        linklocal_network = IPv4Network('169.254.0.0/16')
        if isinstance(self, _BaseAddress):
            return self in linklocal_network
        return (self.network_address in linklocal_network and
                self.broadcast_address in linklocal_network)


class IPv4Address(_BaseV4, _BaseAddress):

    """Represent and manipulate single IPv4 Addresses."""

    def __init__(self, address):

        """
        Args:
            address: A string or integer representing the IP

              Additionally, an integer can be passed, so
              IPv4Address('192.0.2.1') == IPv4Address(3221225985).
              or, more generally
              IPv4Address(int(IPv4Address('192.0.2.1'))) ==
                IPv4Address('192.0.2.1')

        Raises:
            AddressValueError: If ipaddressisn't a valid IPv4 address.

        """
        _BaseAddress.__init__(self, address)
        _BaseV4.__init__(self, address)

        # Efficient constructor from integer.
        if isinstance(address, int):
            self._ip = address
            if address < 0 or address > self._ALL_ONES:
                raise AddressValueError(address)
            return

        # Constructing from a packed address
        if (not isinstance(address, str) and
            isinstance(address, bytes) and len(address) == 4):
            self._ip = struct.unpack('!I', address)[0]
            return

        # Assume input argument to be string or any object representation
        # which converts into a formatted IP string.
        addr_str = str(address)
        self._ip = self._ip_int_from_string(addr_str)

    @property
    def packed(self):
        """The binary representation of this address."""
        return v4_int_to_packed(self._ip)


class IPv4Interface(IPv4Address):

    # the valid octets for host and netmasks. only useful for IPv4.
    _valid_mask_octets = set((255, 254, 252, 248, 240, 224, 192, 128, 0))

    def __init__(self, address):
        if isinstance(address, (bytes, int)):
            IPv4Address.__init__(self, address)
            self.network = IPv4Network(self._ip)
            self._prefixlen = self._max_prefixlen
            return

        addr = str(address).split('/')
        if len(addr) > 2:
            raise AddressValueError(address)
        IPv4Address.__init__(self, addr[0])

        self.network = IPv4Network(address, strict=False)
        self._prefixlen = self.network._prefixlen

        self.netmask = self.network.netmask
        self.hostmask = self.network.hostmask


    def __str__(self):
        return '%s/%d' % (self._string_from_ip_int(self._ip),
                          self.network.prefixlen)

    def __eq__(self, other):
        try:
            return (IPv4Address.__eq__(self, other) and
                    self.network == other.network)
        except AttributeError:
            return NotImplemented

    def __hash__(self):
        return self._ip ^ self._prefixlen ^ int(self.network.network_address)

    def _is_valid_netmask(self, netmask):
        """Verify that the netmask is valid.

        Args:
            netmask: A string, either a prefix or dotted decimal
              netmask.

        Returns:
            A boolean, True if the prefix represents a valid IPv4
            netmask.

        """
        mask = netmask.split('.')
        if len(mask) == 4:
            if [x for x in mask if int(x) not in self._valid_mask_octets]:
                return False
            if [y for idx, y in enumerate(mask) if idx > 0 and
                y > mask[idx - 1]]:
                return False
            return True
        try:
            netmask = int(netmask)
        except ValueError:
            return False
        return 0 <= netmask <= self._max_prefixlen

    def _is_hostmask(self, ip_str):
        """Test if the IP string is a hostmask (rather than a netmask).

        Args:
            ip_str: A string, the potential hostmask.

        Returns:
            A boolean, True if the IP string is a hostmask.

        """
        bits = ip_str.split('.')
        try:
            parts = [int(x) for x in bits if int(x) in self._valid_mask_octets]
        except ValueError:
            return False
        if len(parts) != len(bits):
            return False
        if parts[0] < parts[-1]:
            return True
        return False


    @property
    def prefixlen(self):
        return self._prefixlen

    @property
    def ip(self):
        return IPv4Address(self._ip)

    @property
    def with_prefixlen(self):
        return self

    @property
    def with_netmask(self):
        return '%s/%s' % (self._string_from_ip_int(self._ip),
                          self.netmask)
    @property
    def with_hostmask(self):
        return '%s/%s' % (self._string_from_ip_int(self._ip),
                          self.hostmask)


class IPv4Network(_BaseV4, _BaseNetwork):

    """This class represents and manipulates 32-bit IPv4 network + addresses..

    Attributes: [examples for IPv4Network('192.0.2.0/27')]
        .network_address: IPv4Address('192.0.2.0')
        .hostmask: IPv4Address('0.0.0.31')
        .broadcast_address: IPv4Address('192.0.2.32')
        .netmask: IPv4Address('255.255.255.224')
        .prefixlen: 27

    """

    # the valid octets for host and netmasks. only useful for IPv4.
    _valid_mask_octets = set((255, 254, 252, 248, 240, 224, 192, 128, 0))

    def __init__(self, address, strict=True):

        """Instantiate a new IPv4 network object.

        Args:
            address: A string or integer representing the IP [& network].
              '192.0.2.0/24'
              '192.0.2.0/255.255.255.0'
              '192.0.0.2/0.0.0.255'
              are all functionally the same in IPv4. Similarly,
              '192.0.2.1'
              '192.0.2.1/255.255.255.255'
              '192.0.2.1/32'
              are also functionaly equivalent. That is to say, failing to
              provide a subnetmask will create an object with a mask of /32.

              If the mask (portion after the / in the argument) is given in
              dotted quad form, it is treated as a netmask if it starts with a
              non-zero field (e.g. /255.0.0.0 == /8) and as a hostmask if it
              starts with a zero field (e.g. 0.255.255.255 == /8), with the
              single exception of an all-zero mask which is treated as a
              netmask == /0. If no mask is given, a default of /32 is used.

              Additionally, an integer can be passed, so
              IPv4Network('192.0.2.1') == IPv4Network(3221225985)
              or, more generally
              IPv4Interface(int(IPv4Interface('192.0.2.1'))) ==
                IPv4Interface('192.0.2.1')

        Raises:
            AddressValueError: If ipaddressisn't a valid IPv4 address.
            NetmaskValueError: If the netmask isn't valid for
              an IPv4 address.
            ValueError: If strict was True and a network address was not
              supplied.

        """

        _BaseV4.__init__(self, address)
        _BaseNetwork.__init__(self, address)

        # Constructing from a packed address
        if isinstance(address, bytes) and len(address) == 4:
            self.network_address = IPv4Address(
                struct.unpack('!I', address)[0])
            self._prefixlen = self._max_prefixlen
            self.netmask = IPv4Address(self._ALL_ONES)
            #fixme: address/network test here
            return

        # Efficient constructor from integer.
        if isinstance(address, int):
            self._prefixlen = self._max_prefixlen
            self.netmask = IPv4Address(self._ALL_ONES)
            if address < 0 or address > self._ALL_ONES:
                raise AddressValueError(address)
            self.network_address = IPv4Address(address)
            #fixme: address/network test here.
            return

        # Assume input argument to be string or any object representation
        # which converts into a formatted IP prefix string.
        addr = str(address).split('/')
        self.network_address = IPv4Address(self._ip_int_from_string(addr[0]))

        if len(addr) > 2:
            raise AddressValueError(address)

        if len(addr) == 2:
            mask = addr[1].split('.')

            if len(mask) == 4:
                # We have dotted decimal netmask.
                if self._is_valid_netmask(addr[1]):
                    self.netmask = IPv4Address(self._ip_int_from_string(
                            addr[1]))
                elif self._is_hostmask(addr[1]):
                    self.netmask = IPv4Address(
                        self._ip_int_from_string(addr[1]) ^ self._ALL_ONES)
                else:
                    raise NetmaskValueError('%s is not a valid netmask'
                                                     % addr[1])

                self._prefixlen = self._prefix_from_ip_int(int(self.netmask))
            else:
                # We have a netmask in prefix length form.
                if not self._is_valid_netmask(addr[1]):
                    raise NetmaskValueError(addr[1])
                self._prefixlen = int(addr[1])
                self.netmask = IPv4Address(self._ip_int_from_prefix(
                    self._prefixlen))
        else:
            self._prefixlen = self._max_prefixlen
            self.netmask = IPv4Address(self._ip_int_from_prefix(
                self._prefixlen))

        if strict:
            if (IPv4Address(int(self.network_address) & int(self.netmask)) !=
                self.network_address):
                raise ValueError('%s has host bits set' % self)
        self.network_address = IPv4Address(int(self.network_address) &
                                           int(self.netmask))

        if self._prefixlen == (self._max_prefixlen - 1):
            self.hosts = self.__iter__

    @property
    def packed(self):
        """The binary representation of this address."""
        return v4_int_to_packed(self.network_address)

    def __str__(self):
        return '%s/%d' % (str(self.network_address),
                          self.prefixlen)

    def _is_valid_netmask(self, netmask):
        """Verify that the netmask is valid.

        Args:
            netmask: A string, either a prefix or dotted decimal
              netmask.

        Returns:
            A boolean, True if the prefix represents a valid IPv4
            netmask.

        """
        mask = netmask.split('.')
        if len(mask) == 4:
            if [x for x in mask if int(x) not in self._valid_mask_octets]:
                return False
            if [y for idx, y in enumerate(mask) if idx > 0 and
                y > mask[idx - 1]]:
                return False
            return True
        try:
            netmask = int(netmask)
        except ValueError:
            return False
        return 0 <= netmask <= self._max_prefixlen

    def _is_hostmask(self, ip_str):
        """Test if the IP string is a hostmask (rather than a netmask).

        Args:
            ip_str: A string, the potential hostmask.

        Returns:
            A boolean, True if the IP string is a hostmask.

        """
        bits = ip_str.split('.')
        try:
            parts = [int(x) for x in bits if int(x) in self._valid_mask_octets]
        except ValueError:
            return False
        if len(parts) != len(bits):
            return False
        if parts[0] < parts[-1]:
            return True
        return False

    @property
    def with_prefixlen(self):
        return '%s/%d' % (str(self.network_address), self._prefixlen)

    @property
    def with_netmask(self):
        return '%s/%s' % (str(self.network_address), str(self.netmask))

    @property
    def with_hostmask(self):
        return '%s/%s' % (str(self.network_address), str(self.hostmask))


class _BaseV6(object):

    """Base IPv6 object.

    The following methods are used by IPv6 objects in both single IP
    addresses and networks.

    """

    _ALL_ONES = (2**IPV6LENGTH) - 1
    _HEXTET_COUNT = 8
    _HEX_DIGITS = frozenset('0123456789ABCDEFabcdef')

    def __init__(self, address):
        self._version = 6
        self._max_prefixlen = IPV6LENGTH

    def _ip_int_from_string(self, ip_str):
        """Turn an IPv6 ip_str into an integer.

        Args:
            ip_str: A string, the IPv6 ip_str.

        Returns:
            An int, the IPv6 address

        Raises:
            AddressValueError: if ip_str isn't a valid IPv6 Address.

        """
        parts = ip_str.split(':')

        # An IPv6 address needs at least 2 colons (3 parts).
        if len(parts) < 3:
            raise AddressValueError(ip_str)

        # If the address has an IPv4-style suffix, convert it to hexadecimal.
        if '.' in parts[-1]:
            ipv4_int = IPv4Address(parts.pop())._ip
            parts.append('%x' % ((ipv4_int >> 16) & 0xFFFF))
            parts.append('%x' % (ipv4_int & 0xFFFF))

        # An IPv6 address can't have more than 8 colons (9 parts).
        if len(parts) > self._HEXTET_COUNT + 1:
            raise AddressValueError(ip_str)

        # Disregarding the endpoints, find '::' with nothing in between.
        # This indicates that a run of zeroes has been skipped.
        try:
            skip_index, = (
                [i for i in range(1, len(parts) - 1) if not parts[i]] or
                [None])
        except ValueError:
            # Can't have more than one '::'
            raise AddressValueError(ip_str)

        # parts_hi is the number of parts to copy from above/before the '::'
        # parts_lo is the number of parts to copy from below/after the '::'
        if skip_index is not None:
            # If we found a '::', then check if it also covers the endpoints.
            parts_hi = skip_index
            parts_lo = len(parts) - skip_index - 1
            if not parts[0]:
                parts_hi -= 1
                if parts_hi:
                    raise AddressValueError(ip_str)  # ^: requires ^::
            if not parts[-1]:
                parts_lo -= 1
                if parts_lo:
                    raise AddressValueError(ip_str)  # :$ requires ::$
            parts_skipped = self._HEXTET_COUNT - (parts_hi + parts_lo)
            if parts_skipped < 1:
                raise AddressValueError(ip_str)
        else:
            # Otherwise, allocate the entire address to parts_hi.  The endpoints
            # could still be empty, but _parse_hextet() will check for that.
            if len(parts) != self._HEXTET_COUNT:
                raise AddressValueError(ip_str)
            parts_hi = len(parts)
            parts_lo = 0
            parts_skipped = 0

        try:
            # Now, parse the hextets into a 128-bit integer.
            ip_int = 0
            for i in range(parts_hi):
                ip_int <<= 16
                ip_int |= self._parse_hextet(parts[i])
            ip_int <<= 16 * parts_skipped
            for i in range(-parts_lo, 0):
                ip_int <<= 16
                ip_int |= self._parse_hextet(parts[i])
            return ip_int
        except ValueError:
            raise AddressValueError(ip_str)

    def _parse_hextet(self, hextet_str):
        """Convert an IPv6 hextet string into an integer.

        Args:
            hextet_str: A string, the number to parse.

        Returns:
            The hextet as an integer.

        Raises:
            ValueError: if the input isn't strictly a hex number from [0..FFFF].

        """
        # Whitelist the characters, since int() allows a lot of bizarre stuff.
        if not self._HEX_DIGITS.issuperset(hextet_str):
            raise ValueError
        if len(hextet_str) > 4:
            raise ValueError
        hextet_int = int(hextet_str, 16)
        if hextet_int > 0xFFFF:
            raise ValueError
        return hextet_int

    def _compress_hextets(self, hextets):
        """Compresses a list of hextets.

        Compresses a list of strings, replacing the longest continuous
        sequence of "0" in the list with "" and adding empty strings at
        the beginning or at the end of the string such that subsequently
        calling ":".join(hextets) will produce the compressed version of
        the IPv6 address.

        Args:
            hextets: A list of strings, the hextets to compress.

        Returns:
            A list of strings.

        """
        best_doublecolon_start = -1
        best_doublecolon_len = 0
        doublecolon_start = -1
        doublecolon_len = 0
        for index in range(len(hextets)):
            if hextets[index] == '0':
                doublecolon_len += 1
                if doublecolon_start == -1:
                    # Start of a sequence of zeros.
                    doublecolon_start = index
                if doublecolon_len > best_doublecolon_len:
                    # This is the longest sequence of zeros so far.
                    best_doublecolon_len = doublecolon_len
                    best_doublecolon_start = doublecolon_start
            else:
                doublecolon_len = 0
                doublecolon_start = -1

        if best_doublecolon_len > 1:
            best_doublecolon_end = (best_doublecolon_start +
                                    best_doublecolon_len)
            # For zeros at the end of the address.
            if best_doublecolon_end == len(hextets):
                hextets += ['']
            hextets[best_doublecolon_start:best_doublecolon_end] = ['']
            # For zeros at the beginning of the address.
            if best_doublecolon_start == 0:
                hextets = [''] + hextets

        return hextets

    def _string_from_ip_int(self, ip_int=None):
        """Turns a 128-bit integer into hexadecimal notation.

        Args:
            ip_int: An integer, the IP address.

        Returns:
            A string, the hexadecimal representation of the address.

        Raises:
            ValueError: The address is bigger than 128 bits of all ones.

        """
        if not ip_int and ip_int != 0:
            ip_int = int(self._ip)

        if ip_int > self._ALL_ONES:
            raise ValueError('IPv6 address is too large')

        hex_str = '%032x' % ip_int
        hextets = []
        for x in range(0, 32, 4):
            hextets.append('%x' % int(hex_str[x:x+4], 16))

        hextets = self._compress_hextets(hextets)
        return ':'.join(hextets)

    def _explode_shorthand_ip_string(self):
        """Expand a shortened IPv6 address.

        Args:
            ip_str: A string, the IPv6 address.

        Returns:
            A string, the expanded IPv6 address.

        """
        if isinstance(self, IPv6Network):
            ip_str = str(self.network_address)
        elif isinstance(self, IPv6Interface):
            ip_str = str(self.ip)
        else:
            ip_str = str(self)

        ip_int = self._ip_int_from_string(ip_str)
        parts = []
        for i in range(self._HEXTET_COUNT):
            parts.append('%04x' % (ip_int & 0xFFFF))
            ip_int >>= 16
        parts.reverse()
        if isinstance(self, (_BaseNetwork, IPv6Interface)):
            return '%s/%d' % (':'.join(parts), self.prefixlen)
        return ':'.join(parts)

    @property
    def max_prefixlen(self):
        return self._max_prefixlen

    @property
    def packed(self):
        """The binary representation of this address."""
        return v6_int_to_packed(self._ip)

    @property
    def version(self):
        return self._version

    @property
    def is_multicast(self):
        """Test if the address is reserved for multicast use.

        Returns:
            A boolean, True if the address is a multicast address.
            See RFC 2373 2.7 for details.

        """
        multicast_network = IPv6Network('ff00::/8')
        if isinstance(self, _BaseAddress):
            return self in multicast_network
        return (self.network_address in multicast_network and
                self.broadcast_address in multicast_network)

    @property
    def is_reserved(self):
        """Test if the address is otherwise IETF reserved.

        Returns:
            A boolean, True if the address is within one of the
            reserved IPv6 Network ranges.

        """
        reserved_networks = [IPv6Network('::/8'), IPv6Network('100::/8'),
                             IPv6Network('200::/7'), IPv6Network('400::/6'),
                             IPv6Network('800::/5'), IPv6Network('1000::/4'),
                             IPv6Network('4000::/3'), IPv6Network('6000::/3'),
                             IPv6Network('8000::/3'), IPv6Network('A000::/3'),
                             IPv6Network('C000::/3'), IPv6Network('E000::/4'),
                             IPv6Network('F000::/5'), IPv6Network('F800::/6'),
                             IPv6Network('FE00::/9')]

        if isinstance(self, _BaseAddress):
            return len([x for x in reserved_networks if self in x]) > 0
        return len([x for x in reserved_networks if self.network_address in x
                    and self.broadcast_address in x]) > 0

    @property
    def is_link_local(self):
        """Test if the address is reserved for link-local.

        Returns:
            A boolean, True if the address is reserved per RFC 4291.

        """
        linklocal_network = IPv6Network('fe80::/10')
        if isinstance(self, _BaseAddress):
            return self in linklocal_network
        return (self.network_address in linklocal_network and
                self.broadcast_address in linklocal_network)

    @property
    def is_site_local(self):
        """Test if the address is reserved for site-local.

        Note that the site-local address space has been deprecated by RFC 3879.
        Use is_private to test if this address is in the space of unique local
        addresses as defined by RFC 4193.

        Returns:
            A boolean, True if the address is reserved per RFC 3513 2.5.6.

        """
        sitelocal_network = IPv6Network('fec0::/10')
        if isinstance(self, _BaseAddress):
            return self in sitelocal_network
        return (self.network_address in sitelocal_network and
                self.broadcast_address in sitelocal_network)

    @property
    def is_private(self):
        """Test if this address is allocated for private networks.

        Returns:
            A boolean, True if the address is reserved per RFC 4193.

        """
        private_network = IPv6Network('fc00::/7')
        if isinstance(self, _BaseAddress):
            return self in private_network
        return (self.network_address in private_network and
                self.broadcast_address in private_network)


    @property
    def ipv4_mapped(self):
        """Return the IPv4 mapped address.

        Returns:
            If the IPv6 address is a v4 mapped address, return the
            IPv4 mapped address. Return None otherwise.

        """
        if (self._ip >> 32) != 0xFFFF:
            return None
        return IPv4Address(self._ip & 0xFFFFFFFF)

    @property
    def teredo(self):
        """Tuple of embedded teredo IPs.

        Returns:
            Tuple of the (server, client) IPs or None if the address
            doesn't appear to be a teredo address (doesn't start with
            2001::/32)

        """
        if (self._ip >> 96) != 0x20010000:
            return None
        return (IPv4Address((self._ip >> 64) & 0xFFFFFFFF),
                IPv4Address(~self._ip & 0xFFFFFFFF))

    @property
    def sixtofour(self):
        """Return the IPv4 6to4 embedded address.

        Returns:
            The IPv4 6to4-embedded address if present or None if the
            address doesn't appear to contain a 6to4 embedded address.

        """
        if (self._ip >> 112) != 0x2002:
            return None
        return IPv4Address((self._ip >> 80) & 0xFFFFFFFF)

    @property
    def is_unspecified(self):
        """Test if the address is unspecified.

        Returns:
            A boolean, True if this is the unspecified address as defined in
            RFC 2373 2.5.2.

        """
        if isinstance(self, (IPv6Network, IPv6Interface)):
            return int(self.network_address) == 0 and getattr(
                self, '_prefixlen', 128) == 128
        return self._ip == 0

    @property
    def is_loopback(self):
        """Test if the address is a loopback address.

        Returns:
            A boolean, True if the address is a loopback address as defined in
            RFC 2373 2.5.3.

        """
        if isinstance(self, IPv6Network):
            return int(self.network) == 1 and getattr(
                self, '_prefixlen', 128) == 128
        elif isinstance(self, IPv6Interface):
            return int(self.network.network_address) == 1 and getattr(
                self, '_prefixlen', 128) == 128
        return self._ip == 1


class IPv6Address(_BaseV6, _BaseAddress):

    """Represent and manipulate single IPv6 Addresses.
    """

    def __init__(self, address):
        """Instantiate a new IPv6 address object.

        Args:
            address: A string or integer representing the IP

              Additionally, an integer can be passed, so
              IPv6Address('2001:db8::') ==
                IPv6Address(42540766411282592856903984951653826560)
              or, more generally
              IPv6Address(int(IPv6Address('2001:db8::'))) ==
                IPv6Address('2001:db8::')

        Raises:
            AddressValueError: If address isn't a valid IPv6 address.

        """
        _BaseAddress.__init__(self, address)
        _BaseV6.__init__(self, address)

        # Efficient constructor from integer.
        if isinstance(address, int):
            self._ip = address
            if address < 0 or address > self._ALL_ONES:
                raise AddressValueError(address)
            return

        # Constructing from a packed address
        if (not isinstance(address, str) and
            isinstance(address, bytes) and len(address) == 16):
            tmp = struct.unpack('!QQ', address)
            self._ip = (tmp[0] << 64) | tmp[1]
            return

        # Assume input argument to be string or any object representation
        # which converts into a formatted IP string.
        addr_str = str(address)
        if not addr_str:
            raise AddressValueError('')

        self._ip = self._ip_int_from_string(addr_str)


class IPv6Interface(IPv6Address):

    def __init__(self, address):
        if isinstance(address, (bytes, int)):
            IPv6Address.__init__(self, address)
            self.network = IPv6Network(self._ip)
            self._prefixlen = self._max_prefixlen
            return

        addr = str(address).split('/')
        IPv6Address.__init__(self, addr[0])
        self.network = IPv6Network(address, strict=False)
        self.netmask = self.network.netmask
        self._prefixlen = self.network._prefixlen
        self.hostmask = self.network.hostmask


    def __str__(self):
        return '%s/%d' % (self._string_from_ip_int(self._ip),
                          self.network.prefixlen)

    def __eq__(self, other):
        try:
            return (IPv6Address.__eq__(self, other) and
                    self.network == other.network)
        except AttributeError:
            return NotImplemented

    def __hash__(self):
        return self._ip ^ self._prefixlen ^ int(self.network.network_address)

    @property
    def prefixlen(self):
        return self._prefixlen
    @property
    def ip(self):
        return IPv6Address(self._ip)

    @property
    def with_prefixlen(self):
        return self

    @property
    def with_netmask(self):
        return self.with_prefixlen
    @property
    def with_hostmask(self):
        return '%s/%s' % (self._string_from_ip_int(self._ip),
                          self.hostmask)


class IPv6Network(_BaseV6, _BaseNetwork):

    """This class represents and manipulates 128-bit IPv6 networks.

    Attributes: [examples for IPv6('2001:db8::1000/124')]
        .network_address: IPv6Address('2001:db8::1000')
        .hostmask: IPv6Address('::f')
        .broadcast_address: IPv6Address('2001:db8::100f')
        .netmask: IPv6Address('ffff:ffff:ffff:ffff:ffff:ffff:ffff:fff0')
        .prefixlen: 124

    """

    def __init__(self, address, strict=True):
        """Instantiate a new IPv6 Network object.

        Args:
            address: A string or integer representing the IPv6 network or the IP
              and prefix/netmask.
              '2001:db8::/128'
              '2001:db8:0000:0000:0000:0000:0000:0000/128'
              '2001:db8::'
              are all functionally the same in IPv6.  That is to say,
              failing to provide a subnetmask will create an object with
              a mask of /128.

              Additionally, an integer can be passed, so
              IPv6Network('2001:db8::') ==
                IPv6Network(42540766411282592856903984951653826560)
              or, more generally
              IPv6Network(int(IPv6Network('2001:db8::'))) ==
                IPv6Network('2001:db8::')

            strict: A boolean. If true, ensure that we have been passed
              A true network address, eg, 2001:db8::1000/124 and not an
              IP address on a network, eg, 2001:db8::1/124.

        Raises:
            AddressValueError: If address isn't a valid IPv6 address.
            NetmaskValueError: If the netmask isn't valid for
              an IPv6 address.
            ValueError: If strict was True and a network address was not
              supplied.

        """
        _BaseV6.__init__(self, address)
        _BaseNetwork.__init__(self, address)

        # Efficient constructor from integer.
        if isinstance(address, int):
            if address < 0 or address > self._ALL_ONES:
                raise AddressValueError(address)
            self.network_address = IPv6Address(address)
            self._prefixlen = self._max_prefixlen
            self.netmask = IPv6Address(self._ALL_ONES)
            if strict:
                if (IPv6Address(int(self.network_address) &
                                int(self.netmask)) != self.network_address):
                    raise ValueError('%s has host bits set' % str(self))
            self.network_address = IPv6Address(int(self.network_address) &
                                               int(self.netmask))
            return

        # Constructing from a packed address
        if isinstance(address, bytes) and len(address) == 16:
            tmp = struct.unpack('!QQ', address)
            self.network_address = IPv6Address((tmp[0] << 64) | tmp[1])
            self._prefixlen = self._max_prefixlen
            self.netmask = IPv6Address(self._ALL_ONES)
            if strict:
                if (IPv6Address(int(self.network_address) &
                                int(self.netmask)) != self.network_address):
                    raise ValueError('%s has host bits set' % str(self))
                self.network_address = IPv6Address(int(self.network_address) &
                                                   int(self.netmask))
                return

        # Assume input argument to be string or any object representation
        # which converts into a formatted IP prefix string.
        addr = str(address).split('/')

        if len(addr) > 2:
            raise AddressValueError(address)

        self.network_address = IPv6Address(self._ip_int_from_string(addr[0]))

        if len(addr) == 2:
            if self._is_valid_netmask(addr[1]):
                self._prefixlen = int(addr[1])
            else:
                raise NetmaskValueError(addr[1])
        else:
            self._prefixlen = self._max_prefixlen

        self.netmask = IPv6Address(self._ip_int_from_prefix(self._prefixlen))
        if strict:
            if (IPv6Address(int(self.network_address) & int(self.netmask)) !=
                self.network_address):
                raise ValueError('%s has host bits set' % str(self))
        self.network_address = IPv6Address(int(self.network_address) &
                                           int(self.netmask))

        if self._prefixlen == (self._max_prefixlen - 1):
            self.hosts = self.__iter__

    def __str__(self):
        return '%s/%d' % (str(self.network_address),
                          self.prefixlen)

    def _is_valid_netmask(self, prefixlen):
        """Verify that the netmask/prefixlen is valid.

        Args:
            prefixlen: A string, the netmask in prefix length format.

        Returns:
            A boolean, True if the prefix represents a valid IPv6
            netmask.

        """
        try:
            prefixlen = int(prefixlen)
        except ValueError:
            return False
        return 0 <= prefixlen <= self._max_prefixlen

    @property
    def with_netmask(self):
        return self.with_prefixlen

    @property
    def with_prefixlen(self):
        return '%s/%d' % (str(self.network_address), self._prefixlen)

    @property
    def with_netmask(self):
        return '%s/%s' % (str(self.network_address), str(self.netmask))

    @property
    def with_hostmask(self):
        return '%s/%s' % (str(self.network_address), str(self.hostmask))

########NEW FILE########
__FILENAME__ = rcssmin
#!/usr/bin/env python
# -*- coding: ascii -*-
#
# Copyright 2011, 2012
# Andr\xe9 Malo or his licensors, as applicable
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
r"""
==============
 CSS Minifier
==============

CSS Minifier.

The minifier is based on the semantics of the `YUI compressor`_\, which itself
is based on `the rule list by Isaac Schlueter`_\.

This module is a re-implementation aiming for speed instead of maximum
compression, so it can be used at runtime (rather than during a preprocessing
step). RCSSmin does syntactical compression only (removing spaces, comments
and possibly semicolons). It does not provide semantic compression (like
removing empty blocks, collapsing redundant properties etc). It does, however,
support various CSS hacks (by keeping them working as intended).

Here's a feature list:

- Strings are kept, except that escaped newlines are stripped
- Space/Comments before the very end or before various characters are
  stripped: ``:{});=>+],!`` (The colon (``:``) is a special case, a single
  space is kept if it's outside a ruleset.)
- Space/Comments at the very beginning or after various characters are
  stripped: ``{}(=:>+[,!``
- Optional space after unicode escapes is kept, resp. replaced by a simple
  space
- whitespaces inside ``url()`` definitions are stripped
- Comments starting with an exclamation mark (``!``) can be kept optionally.
- All other comments and/or whitespace characters are replaced by a single
  space.
- Multiple consecutive semicolons are reduced to one
- The last semicolon within a ruleset is stripped
- CSS Hacks supported:

  - IE7 hack (``>/**/``)
  - Mac-IE5 hack (``/*\*/.../**/``)
  - The boxmodelhack is supported naturally because it relies on valid CSS2
    strings
  - Between ``:first-line`` and the following comma or curly brace a space is
    inserted. (apparently it's needed for IE6)
  - Same for ``:first-letter``

rcssmin.c is a reimplementation of rcssmin.py in C and improves runtime up to
factor 50 or so (depending on the input).

Both python 2 (>= 2.4) and python 3 are supported.

.. _YUI compressor: https://github.com/yui/yuicompressor/

.. _the rule list by Isaac Schlueter: https://github.com/isaacs/cssmin/tree/
"""
__author__ = "Andr\xe9 Malo"
__author__ = getattr(__author__, 'decode', lambda x: __author__)('latin-1')
__docformat__ = "restructuredtext en"
__license__ = "Apache License, Version 2.0"
__version__ = '1.0.1'
__all__ = ['cssmin']

import re as _re


def _make_cssmin(python_only=False):
    """
    Generate CSS minifier.

    :Parameters:
      `python_only` : ``bool``
        Use only the python variant. If true, the c extension is not even
        tried to be loaded.

    :Return: Minifier
    :Rtype: ``callable``
    """
    # pylint: disable = W0612
    # ("unused" variables)

    # pylint: disable = R0911, R0912, R0914, R0915
    # (too many anything)

    if not python_only:
        try:
            import _rcssmin
        except ImportError:
            pass
        else:
            return _rcssmin.cssmin

    nl = r'(?:[\n\f]|\r\n?)' # pylint: disable = C0103
    spacechar = r'[\r\n\f\040\t]'

    unicoded = r'[0-9a-fA-F]{1,6}(?:[\040\n\t\f]|\r\n?)?'
    escaped = r'[^\n\r\f0-9a-fA-F]'
    escape = r'(?:\\(?:%(unicoded)s|%(escaped)s))' % locals()

    nmchar = r'[^\000-\054\056\057\072-\100\133-\136\140\173-\177]'
    #nmstart = r'[^\000-\100\133-\136\140\173-\177]'
    #ident = (r'(?:'
    #    r'-?(?:%(nmstart)s|%(escape)s)%(nmchar)s*(?:%(escape)s%(nmchar)s*)*'
    #r')') % locals()

    comment = r'(?:/\*[^*]*\*+(?:[^/*][^*]*\*+)*/)'

    # only for specific purposes. The bang is grouped:
    _bang_comment = r'(?:/\*(!?)[^*]*\*+(?:[^/*][^*]*\*+)*/)'

    string1 = \
        r'(?:\047[^\047\\\r\n\f]*(?:\\[^\r\n\f][^\047\\\r\n\f]*)*\047)'
    string2 = r'(?:"[^"\\\r\n\f]*(?:\\[^\r\n\f][^"\\\r\n\f]*)*")'
    strings = r'(?:%s|%s)' % (string1, string2)

    nl_string1 = \
        r'(?:\047[^\047\\\r\n\f]*(?:\\(?:[^\r]|\r\n?)[^\047\\\r\n\f]*)*\047)'
    nl_string2 = r'(?:"[^"\\\r\n\f]*(?:\\(?:[^\r]|\r\n?)[^"\\\r\n\f]*)*")'
    nl_strings = r'(?:%s|%s)' % (nl_string1, nl_string2)

    uri_nl_string1 = r'(?:\047[^\047\\]*(?:\\(?:[^\r]|\r\n?)[^\047\\]*)*\047)'
    uri_nl_string2 = r'(?:"[^"\\]*(?:\\(?:[^\r]|\r\n?)[^"\\]*)*")'
    uri_nl_strings = r'(?:%s|%s)' % (uri_nl_string1, uri_nl_string2)

    nl_escaped = r'(?:\\%(nl)s)' % locals()

    space = r'(?:%(spacechar)s|%(comment)s)' % locals()

    ie7hack = r'(?:>/\*\*/)'

    uri = (r'(?:'
        r'(?:[^\000-\040"\047()\\\177]*'
            r'(?:%(escape)s[^\000-\040"\047()\\\177]*)*)'
        r'(?:'
            r'(?:%(spacechar)s+|%(nl_escaped)s+)'
            r'(?:'
                r'(?:[^\000-\040"\047()\\\177]|%(escape)s|%(nl_escaped)s)'
                r'[^\000-\040"\047()\\\177]*'
                r'(?:%(escape)s[^\000-\040"\047()\\\177]*)*'
            r')+'
        r')*'
    r')') % locals()

    nl_unesc_sub = _re.compile(nl_escaped).sub

    uri_space_sub = _re.compile((
        r'(%(escape)s+)|%(spacechar)s+|%(nl_escaped)s+'
    ) % locals()).sub
    uri_space_subber = lambda m: m.groups()[0] or ''

    space_sub_simple = _re.compile((
        r'[\r\n\f\040\t;]+|(%(comment)s+)'
    ) % locals()).sub
    space_sub_banged = _re.compile((
        r'[\r\n\f\040\t;]+|(%(_bang_comment)s+)'
    ) % locals()).sub

    post_esc_sub = _re.compile(r'[\r\n\f\t]+').sub

    main_sub = _re.compile((
        r'([^\\"\047u>@\r\n\f\040\t/;:{}]+)'
        r'|(?<=[{}(=:>+[,!])(%(space)s+)'
        r'|^(%(space)s+)'
        r'|(%(space)s+)(?=(([:{});=>+\],!])|$)?)'
        r'|;(%(space)s*(?:;%(space)s*)*)(?=(\})?)'
        r'|(\{)'
        r'|(\})'
        r'|(%(strings)s)'
        r'|(?<!%(nmchar)s)url\(%(spacechar)s*('
                r'%(uri_nl_strings)s'
                r'|%(uri)s'
            r')%(spacechar)s*\)'
        r'|(@[mM][eE][dD][iI][aA])(?!%(nmchar)s)'
        r'|(%(ie7hack)s)(%(space)s*)'
        r'|(:[fF][iI][rR][sS][tT]-[lL]'
            r'(?:[iI][nN][eE]|[eE][tT][tT][eE][rR]))'
            r'(%(space)s*)(?=[{,])'
        r'|(%(nl_strings)s)'
        r'|(%(escape)s[^\\"\047u>@\r\n\f\040\t/;:{}]*)'
    ) % locals()).sub

    #print main_sub.__self__.pattern

    def main_subber(keep_bang_comments):
        """ Make main subber """
        in_macie5, in_rule, at_media = [0], [0], [0]

        if keep_bang_comments:
            space_sub = space_sub_banged
            def space_subber(match):
                """ Space|Comment subber """
                if match.lastindex:
                    group1, group2 = match.group(1, 2)
                    if group2:
                        if group1.endswith(r'\*/'):
                            in_macie5[0] = 1
                        else:
                            in_macie5[0] = 0
                        return group1
                    elif group1:
                        if group1.endswith(r'\*/'):
                            if in_macie5[0]:
                                return ''
                            in_macie5[0] = 1
                            return r'/*\*/'
                        elif in_macie5[0]:
                            in_macie5[0] = 0
                            return '/**/'
                return ''
        else:
            space_sub = space_sub_simple
            def space_subber(match):
                """ Space|Comment subber """
                if match.lastindex:
                    if match.group(1).endswith(r'\*/'):
                        if in_macie5[0]:
                            return ''
                        in_macie5[0] = 1
                        return r'/*\*/'
                    elif in_macie5[0]:
                        in_macie5[0] = 0
                        return '/**/'
                return ''

        def fn_space_post(group):
            """ space with token after """
            if group(5) is None or (
                    group(6) == ':' and not in_rule[0] and not at_media[0]):
                return ' ' + space_sub(space_subber, group(4))
            return space_sub(space_subber, group(4))

        def fn_semicolon(group):
            """ ; handler """
            return ';' + space_sub(space_subber, group(7))

        def fn_semicolon2(group):
            """ ; handler """
            if in_rule[0]:
                return space_sub(space_subber, group(7))
            return ';' + space_sub(space_subber, group(7))

        def fn_open(group):
            """ { handler """
            # pylint: disable = W0613
            if at_media[0]:
                at_media[0] -= 1
            else:
                in_rule[0] = 1
            return '{'

        def fn_close(group):
            """ } handler """
            # pylint: disable = W0613
            in_rule[0] = 0
            return '}'

        def fn_media(group):
            """ @media handler """
            at_media[0] += 1
            return group(13)

        def fn_ie7hack(group):
            """ IE7 Hack handler """
            if not in_rule[0] and not at_media[0]:
                in_macie5[0] = 0
                return group(14) + space_sub(space_subber, group(15))
            return '>' + space_sub(space_subber, group(15))

        table = (
            None,
            None,
            None,
            None,
            fn_space_post,                      # space with token after
            fn_space_post,                      # space with token after
            fn_space_post,                      # space with token after
            fn_semicolon,                       # semicolon
            fn_semicolon2,                      # semicolon
            fn_open,                            # {
            fn_close,                           # }
            lambda g: g(11),                    # string
            lambda g: 'url(%s)' % uri_space_sub(uri_space_subber, g(12)),
                                                # url(...)
            fn_media,                           # @media
            None,
            fn_ie7hack,                         # ie7hack
            None,
            lambda g: g(16) + ' ' + space_sub(space_subber, g(17)),
                                                # :first-line|letter followed
                                                # by [{,] (apparently space
                                                # needed for IE6)
            lambda g: nl_unesc_sub('', g(18)),  # nl_string
            lambda g: post_esc_sub(' ', g(19)), # escape
        )

        def func(match):
            """ Main subber """
            idx, group = match.lastindex, match.group
            if idx > 3:
                return table[idx](group)

            # shortcuts for frequent operations below:
            elif idx == 1:     # not interesting
                return group(1)
            #else: # space with token before or at the beginning
            return space_sub(space_subber, group(idx))

        return func

    def cssmin(style, keep_bang_comments=False): # pylint: disable = W0621
        """
        Minify CSS.

        :Parameters:
          `style` : ``str``
            CSS to minify

          `keep_bang_comments` : ``bool``
            Keep comments starting with an exclamation mark? (``/*!...*/``)

        :Return: Minified style
        :Rtype: ``str``
        """
        return main_sub(main_subber(keep_bang_comments), style)

    return cssmin

cssmin = _make_cssmin()


if __name__ == '__main__':
    def main():
        """ Main """
        import sys as _sys
        keep_bang_comments = (
            '-b' in _sys.argv[1:]
            or '-bp' in _sys.argv[1:]
            or '-pb' in _sys.argv[1:]
        )
        if '-p' in _sys.argv[1:] or '-bp' in _sys.argv[1:] \
                or '-pb' in _sys.argv[1:]:
            global cssmin # pylint: disable = W0603
            cssmin = _make_cssmin(python_only=True)
        _sys.stdout.write(cssmin(
            _sys.stdin.read(), keep_bang_comments=keep_bang_comments
        ))
    main()

########NEW FILE########
__FILENAME__ = count
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.models import Link, Subreddit
from r2.lib import utils
from r2.lib.db.operators import desc
from pylons import g, config

count_period = g.rising_period

#stubs

def incr_counts(wrapped):
    pass

def incr_sr_count(sr):
    pass

def get_link_counts(period = count_period):
    links = Link._query(Link.c._date >= utils.timeago(period),
                        limit=50, data = True)
    return dict((l._fullname, (0, l.sr_id)) for l in links)

def get_sr_counts():
    srs = utils.fetch_things2(Subreddit._query(sort=desc("_date")))

    return dict((sr._fullname, sr._ups) for sr in srs)


if config['r2.import_private']:
    from r2admin.lib.count import *

########NEW FILE########
__FILENAME__ = cssfilter
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Parse and validate a safe subset of CSS.

The goal of this validation is not to ensure functionally correct stylesheets
but rather that the stylesheet is safe to show to downstream users.  This
includes:

    * not generating requests to third party hosts (information leak)
    * xss via strange syntax in buggy browsers

Beyond that, every effort is made to allow the full gamut of modern CSS.

"""

import itertools
import re
import unicodedata

import tinycss2

from pylons.i18n import N_

from r2.lib.contrib import rcssmin
from r2.lib.utils import tup


__all__ = ["validate_css"]


SIMPLE_TOKEN_TYPES = {
    "dimension",
    "hash",
    "ident",
    "literal",
    "number",
    "percentage",
    "string",
    "whitespace",
}


VENDOR_PREFIXES = {
    "-apple-",
    "-khtml-",
    "-moz-",
    "-ms-",
    "-o-",
    "-webkit-",
}
assert all(prefix == prefix.lower() for prefix in VENDOR_PREFIXES)


SAFE_PROPERTIES = {
    "align-content",
    "align-items",
    "align-self",
    "animation",
    "animation-delay",
    "animation-direction",
    "animation-duration",
    "animation-fill-mode",
    "animation-iteration-count",
    "animation-name",
    "animation-play-state",
    "animation-timing-function",
    "backface-visibility",
    "background",
    "background-attachment",
    "background-clip",
    "background-color",
    "background-image",
    "background-origin",
    "background-position",
    "background-repeat",
    "background-size",
    "border",
    "border-bottom",
    "border-bottom-color",
    "border-bottom-left-radius",
    "border-bottom-right-radius",
    "border-bottom-style",
    "border-bottom-width",
    "border-collapse",
    "border-color",
    "border-image",
    "border-image-outset",
    "border-image-repeat",
    "border-image-slice",
    "border-image-source",
    "border-image-width",
    "border-left",
    "border-left-color",
    "border-left-style",
    "border-left-width",
    "border-radius",
    "border-radius-bottomleft",
    "border-radius-bottomright",
    "border-radius-topleft",
    "border-radius-topright",
    "border-right",
    "border-right-color",
    "border-right-style",
    "border-right-width",
    "border-spacing",
    "border-style",
    "border-top",
    "border-top-color",
    "border-top-left-radius",
    "border-top-right-radius",
    "border-top-style",
    "border-top-width",
    "border-width",
    "bottom",
    "box-shadow",
    "box-sizing",
    "caption-side",
    "clear",
    "clip",
    "color",
    "content",
    "counter-increment",
    "counter-reset",
    "cue",
    "cue-after",
    "cue-before",
    "cursor",
    "direction",
    "display",
    "elevation",
    "empty-cells",
    "flex",
    "flex-align",
    "flex-direction",
    "flex-flow",
    "flex-item-align",
    "flex-line-pack",
    "flex-order",
    "flex-pack",
    "flex-wrap",
    "float",
    "font",
    "font-family",
    "font-size",
    "font-style",
    "font-variant",
    "font-weight",
    "grid",
    "grid-area",
    "grid-auto-columns",
    "grid-auto-flow",
    "grid-auto-position",
    "grid-auto-rows",
    "grid-column",
    "grid-column-start",
    "grid-column-end",
    "grid-row",
    "grid-row-start",
    "grid-row-end",
    "grid-template",
    "grid-template-areas",
    "grid-template-rows",
    "grid-template-columns",
    "hanging-punctuation",
    "height",
    "hyphens",
    "image-orientation",
    "image-rendering",
    "image-resolution",
    "left",
    "letter-spacing",
    "line-break",
    "line-height",
    "list-style",
    "list-style-image",
    "list-style-position",
    "list-style-type",
    "margin",
    "margin-bottom",
    "margin-left",
    "margin-right",
    "margin-top",
    "max-height",
    "max-width",
    "min-height",
    "min-width",
    "opacity",
    "orphans",
    "outline",
    "outline-color",
    "outline-offset",
    "outline-style",
    "outline-width",
    "overflow",
    "overflow-wrap",
    "overflow-x",
    "overflow-y",
    "padding",
    "padding-bottom",
    "padding-left",
    "padding-right",
    "padding-top",
    "page-break-after",
    "page-break-before",
    "page-break-inside",
    "pause",
    "pause-after",
    "pause-before",
    "perspective",
    "perspective-origin",
    "pitch",
    "pitch-range",
    "play-during",
    "pointer-events",
    "position",
    "quotes",
    "richness",
    "right",
    "speak",
    "speak-header",
    "speak-numeral",
    "speak-punctuation",
    "speech-rate",
    "stress",
    "table-layout",
    "tab-size",
    "text-align",
    "text-align-last",
    "text-decoration",
    "text-decoration-color",
    "text-decoration-line",
    "text-decoration-skip",
    "text-decoration-style",
    "text-indent",
    "text-justify",
    "text-overflow",
    "text-shadow",
    "text-space-collapse",
    "text-transform",
    "text-underline-position",
    "text-wrap",
    "top",
    "transform",
    "transform-origin",
    "transform-style",
    "transition",
    "transition-delay",
    "transition-duration",
    "transition-property",
    "transition-timing-function",
    "unicode-bidi",
    "vertical-align",
    "visibility",
    "voice-family",
    "volume",
    "white-space",
    "widows",
    "width",
    "word-break",
    "word-spacing",
    "z-index",
}
assert all(property == property.lower() for property in SAFE_PROPERTIES)


SAFE_FUNCTIONS = {
    "attr",
    "calc",
    "counter",
    "counters",
    "hsl",
    "hsla",
    "lang",
    "linear-gradient",
    "matrix",
    "matrix3d",
    "not",
    "nth-child",
    "nth-last-child",
    "nth-last-of-type",
    "nth-of-type",
    "perspective",
    "radial-gradient",
    "rect",
    "repeating-linear-gradient",
    "repeating-radial-gradient",
    "rgb",
    "rgba",
    "rotate",
    "rotate3d",
    "rotatex",
    "rotatey",
    "rotatez",
    "scale",
    "scale3d",
    "scalex",
    "scaley",
    "scalez",
    "skewx",
    "skewy",
    "steps",
    "translate",
    "translate3d",
    "translatex",
    "translatey",
    "translatez",
}
assert all(function == function.lower() for function in SAFE_FUNCTIONS)


ERROR_MESSAGES = {
    "IMAGE_NOT_FOUND": N_('no image found with name "%(name)s"'),
    "NON_PLACEHOLDER_URL": N_("only uploaded images are allowed; reference "
                              "them with the %%%%imagename%%%% system"),
    "SYNTAX_ERROR": N_("syntax error: %(message)s"),
    "UNKNOWN_AT_RULE": N_("@%(keyword)s is not allowed"),
    "UNKNOWN_PROPERTY": N_('unknown property "%(name)s"'),
    "UNKNOWN_FUNCTION": N_('unknown function "%(function)s"'),
    "UNEXPECTED_TOKEN": N_('unexpected token "%(token)s"'),
    "BACKSLASH": N_("backslashes are not allowed"),
    "CONTROL_CHARACTER": N_("control characters are not allowed"),
    "TOO_BIG": N_("the stylesheet is too big. maximum size: %(size)d KiB"),
}


MAX_SIZE_KIB = 100
SUBREDDIT_IMAGE_URL_PLACEHOLDER = re.compile(r"\A%%([a-zA-Z0-9\-]+)%%\Z")


def strip_vendor_prefix(identifier):
    for prefix in VENDOR_PREFIXES:
        if identifier.startswith(prefix):
            return identifier[len(prefix):]
    return identifier


class ValidationError(object):
    def __init__(self, line_number, error_code, message_params=None):
        self.line = line_number
        self.error_code = error_code
        self.message_params = message_params or {}
        # note: _source_lines is added to these objects by the parser

    @property
    def offending_line(self):
        return self._source_lines[self.line - 1]

    @property
    def message_key(self):
        return ERROR_MESSAGES[self.error_code]


class StylesheetValidator(object):
    def __init__(self, images):
        self.images = images

    def validate_url(self, url_node):
        m = SUBREDDIT_IMAGE_URL_PLACEHOLDER.match(url_node.value)
        if not m:
            return ValidationError(url_node.source_line, "NON_PLACEHOLDER_URL")

        image_name = m.group(1)
        if image_name not in self.images:
            return ValidationError(url_node.source_line, "IMAGE_NOT_FOUND",
                                   {"name": image_name})

        # rewrite the url value to the actual url of the image
        url_node.value = self.images[image_name]

    def validate_function(self, function_node):
        function_name = strip_vendor_prefix(function_node.lower_name)

        if function_name not in SAFE_FUNCTIONS:
            return ValidationError(function_node.source_line,
                                   "UNKNOWN_FUNCTION",
                                   {"function": function_node.name})
        # property: attr(something url)
        # https://developer.mozilla.org/en-US/docs/Web/CSS/attr
        elif function_name == "attr":
            for argument in function_node.arguments:
                if argument.type == "ident" and argument.lower_value == "url":
                    return ValidationError(argument.source_line,
                                           "NON_PLACEHOLDER_URL")

        return self.validate_component_values(function_node.arguments)

    def validate_block(self, block):
        return self.validate_component_values(block.content)

    def validate_component_values(self, component_values):
        return self.validate_list(component_values, {
            # {} blocks are technically part of component values but i don't
            # know of any actual valid uses for them in selectors etc. and they
            # can cause issues with e.g.
            # Safari 5: p[foo=bar{}*{background:green}]{background:red}
            "[] block": self.validate_block,
            "() block": self.validate_block,
            "url": self.validate_url,
            "function": self.validate_function,
        }, ignored_types=SIMPLE_TOKEN_TYPES)

    def validate_declaration(self, declaration):
        if strip_vendor_prefix(declaration.lower_name) not in SAFE_PROPERTIES:
            return ValidationError(declaration.source_line, "UNKNOWN_PROPERTY",
                                   {"name": declaration.name})
        return self.validate_component_values(declaration.value)

    def validate_declaration_list(self, declarations):
        return self.validate_list(declarations, {
            "at-rule": self.validate_at_rule,
            "declaration": self.validate_declaration,
        })

    def validate_qualified_rule(self, rule):
        prelude_errors = self.validate_component_values(rule.prelude)
        declarations = tinycss2.parse_declaration_list(rule.content)
        declaration_errors = self.validate_declaration_list(declarations)
        return itertools.chain(prelude_errors, declaration_errors)

    def validate_at_rule(self, rule):
        prelude_errors = self.validate_component_values(rule.prelude)

        keyword = strip_vendor_prefix(rule.lower_at_keyword)

        if keyword in ("media", "keyframes"):
            rules = tinycss2.parse_rule_list(rule.content)
            rule_errors = self.validate_rule_list(rules)
        elif keyword == "page":
            rule_errors = self.validate_qualified_rule(rule)
        else:
            return ValidationError(rule.source_line, "UNKNOWN_AT_RULE",
                                   {"keyword": rule.at_keyword})

        return itertools.chain(prelude_errors, rule_errors)

    def validate_rule_list(self, rules):
        return self.validate_list(rules, {
            "qualified-rule": self.validate_qualified_rule,
            "at-rule": self.validate_at_rule,
        })

    def validate_list(self, nodes, validators_by_type, ignored_types=None):
        for node in nodes:
            if node.type == "error":
                yield ValidationError(node.source_line, "SYNTAX_ERROR",
                                      {"message": node.message})
                continue
            elif node.type == "literal":
                if node.value == ";":
                    # if we're seeing a semicolon as a literal, it's in a place
                    # that doesn't fit naturally in the syntax.
                    # Safari 5 will treat this as two color properties:
                    # color: calc(;color:red;);
                    message = "semicolons are not allowed in this context"
                    yield ValidationError(node.source_line, "SYNTAX_ERROR",
                                          {"message": message})
                    continue

            validator = validators_by_type.get(node.type)

            if validator:
                for error in tup(validator(node)):
                    if error:
                        yield error
            else:
                if not ignored_types or node.type not in ignored_types:
                    yield ValidationError(node.source_line,
                                          "UNEXPECTED_TOKEN",
                                          {"token": node.type})

    def check_for_evil_codepoints(self, source_lines):
        for line_number, line_text in enumerate(source_lines, start=1):
            for codepoint in line_text:
                # IE<8: *{color: expression\28 alert\28 1 \29 \29 }
                if codepoint == "\\":
                    yield ValidationError(line_number, "BACKSLASH")
                    break
                # accept these characters that get classified as control
                elif codepoint in ("\t", "\n", "\r"):
                    continue
                # Safari: *{font-family:'foobar\x03;background:url(evil);';}
                elif unicodedata.category(codepoint).startswith("C"):
                    yield ValidationError(line_number, "CONTROL_CHARACTER")
                    break

    def parse_and_validate(self, stylesheet_source):
        if len(stylesheet_source) > (MAX_SIZE_KIB * 1024):
            return "", [ValidationError(0, "TOO_BIG", {"size": MAX_SIZE_KIB})]

        nodes = tinycss2.parse_stylesheet(stylesheet_source)

        source_lines = stylesheet_source.splitlines()

        backslash_errors = self.check_for_evil_codepoints(source_lines)
        validation_errors = self.validate_rule_list(nodes)

        errors = []
        for error in itertools.chain(backslash_errors, validation_errors):
            error._source_lines = source_lines
            errors.append(error)
        errors.sort(key=lambda e: e.line)

        if not errors:
            serialized = rcssmin.cssmin(tinycss2.serialize(nodes))
        else:
            serialized = ""

        return serialized.encode("utf-8"), errors


def validate_css(stylesheet, images):
    """Validate and re-serialize the user submitted stylesheet.

    images is a mapping of subreddit image names to their URLs.  The
    re-serialized stylesheet will have %%name%% tokens replaced with their
    appropriate URLs.

    The return value is a two-tuple of the re-serialized (and minified)
    stylesheet and a list of errors.  If the list is empty, the stylesheet is
    valid.

    """
    assert isinstance(stylesheet, unicode)
    validator = StylesheetValidator(images)
    return validator.parse_and_validate(stylesheet)

########NEW FILE########
__FILENAME__ = alter_db
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import tdb_sql
import sqlalchemy as sa

def thing_tables():
    for type in tdb_sql.types_id.values():
        yield type.thing_table

    for table in tdb_sql.extra_thing_tables.values():
        yield table

def rel_tables():
    for type in tdb_sql.rel_types_id.values():
        yield type.rel_table[0]

def dtables():
    for type in tdb_sql.types_id.values():
        yield type.data_table[0]

    for type in tdb_sql.rel_types_id.values():
        yield type.rel_table[3]

def exec_all(command, data=False, rel = False, print_only = False):
    if data:
        tables = dtables()
    elif rel:
        tables = rel_tables()
    else:
        tables = thing_tables()

    for tt in tables:
        #print tt
        engine = tt.bind
        if print_only:
            print command % dict(type=tt.name)
        else:
            try:
                engine.execute(command % dict(type=tt.name))
            except:
                print "FAILED!"

"alter table %(type)s add primary key (thing_id, key)"
"drop index idx_thing_id_%(type)s"

"create index concurrently idx_thing1_name_date_%(type)s on %(type)s (thing1_id, name, date);"

########NEW FILE########
__FILENAME__ = operators
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

class BooleanOp(object):
    def __init__(self, *ops):
        self.ops = ops

    def __repr__(self):
        return '<%s_ %s>' % (self.__class__.__name__, str(self.ops))

class or_(BooleanOp): pass
class and_(BooleanOp): pass
class not_(BooleanOp): pass

class op(object):
    def __init__(self, lval, lval_name, rval):
        self.lval = lval
        self.rval = rval
        self.lval_name = lval_name

    def __repr__(self):
        return '<%s: %s, %s>' % (self.__class__.__name__, self.lval, self.rval)

    #sorts in a consistent order, required for Query._iden()
    def __cmp__(self, other):
        return cmp(repr(self), repr(other))

class eq(op): pass
class ne(op): pass
class lt(op): pass
class lte(op): pass
class gt(op): pass
class gte(op): pass
class in_(op): pass

class Slot(object):
    def __init__(self, lval):
        if isinstance(lval, Slot):
            self.name = lval.name
            self.lval = lval
        else:
            self.name = lval

    def __repr__(self):
        return '<%s: %s>' % (self.__class__.__name__, self.name)

    def __eq__(self, other):
        return eq(self, self.name, other)

    def __ne__(self, other):
        return ne(self, self.name, other)

    def __lt__(self, other):
        return lt(self, self.name, other)

    def __le__(self, other):
        return lte(self, self.name, other)

    def __gt__(self, other):
        return gt(self, self.name, other)

    def __ge__(self, other):
        return gte(self, self.name, other)

    def in_(self, other):
        return in_(self, self.name, other)

class Slots(object):
    def __getattr__(self, attr):
        return Slot(attr)

    def __getitem__(self, attr):
        return Slot(attr)
        
def op_iter(ops):
    for o in ops:
        if isinstance(o, op):
            yield o
        elif isinstance(o, BooleanOp):
            for p in op_iter(o.ops):
                yield p

class query_func(Slot): pass
class lower(query_func): pass
class ip_network(query_func): pass
class base_url(query_func): pass
class domain(query_func): pass
class year_func(query_func): pass

class timeago(object):
    def __init__(self, interval):
        self.interval = interval

    def __repr__(self):
        return '<interval: %s>' % self.interval

class sort(object):
    def __init__(self, col):
        self.col = col

    def __repr__(self):
        return '<sort:%s %s>' % (self.__class__.__name__, str(self.col))

    def __eq__(self, other):
        return self.col == other.col

class asc(sort): pass
class desc(sort):pass
class shuffled(desc): pass

########NEW FILE########
__FILENAME__ = queries
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.models import Account, Link, Comment, Vote, Report
from r2.models import Message, Inbox, Subreddit, ModContribSR, ModeratorInbox, MultiReddit
from r2.lib.db.thing import Thing, Merge
from r2.lib.db.operators import asc, desc, timeago
from r2.lib.db.sorts import epoch_seconds
from r2.lib.utils import fetch_things2, tup, UniqueIterator, set_last_modified
from r2.lib import utils
from r2.lib import amqp, sup, filters
from r2.lib.comment_tree import add_comments, update_comment_votes
from r2.models.promo import PROMOTE_STATUS, get_promote_srid, PromotionLog
from r2.models.query_cache import (
    cached_query,
    CachedQuery,
    CachedQueryMutator,
    filter_thing,
    merged_cached_query,
    MergedCachedQuery,
    SubredditQueryCache,
    ThingTupleComparator,
    UserQueryCache,
)
from r2.models.last_modified import LastModified
from r2.lib.utils import SimpleSillyStub

import cPickle as pickle

from datetime import datetime
from time import mktime
import pytz
import itertools
import collections
from copy import deepcopy
from r2.lib.db.operators import and_, or_

from pylons import g
query_cache = g.permacache
log = g.log
make_lock = g.make_lock
worker = amqp.worker
stats = g.stats

precompute_limit = 1000

db_sorts = dict(hot = (desc, '_hot'),
                new = (desc, '_date'),
                top = (desc, '_score'),
                controversial = (desc, '_controversy'))

def db_sort(sort):
    cls, col = db_sorts[sort]
    return cls(col)

db_times = dict(all = None,
                hour = Thing.c._date >= timeago('1 hour'),
                day = Thing.c._date >= timeago('1 day'),
                week = Thing.c._date >= timeago('1 week'),
                month = Thing.c._date >= timeago('1 month'),
                year = Thing.c._date >= timeago('1 year'))

# sorts for which there can be a time filter (by day, by week,
# etc). All of these but 'all' are done in mr_top, who knows about the
# structure of the stored CachedResults (so changes here may warrant
# changes there)
time_filtered_sorts = set(('top', 'controversial'))

#we need to define the filter functions here so cachedresults can be pickled
def filter_identity(x):
    return x

def filter_thing2(x):
    """A filter to apply to the results of a relationship query returns
    the object of the relationship."""
    return x._thing2

class CachedResults(object):
    """Given a query returns a list-like object that will lazily look up
    the query from the persistent cache. """
    def __init__(self, query, filter):
        self.query = query
        self.query._limit = precompute_limit
        self.filter = filter
        self.iden = self.query._iden()
        self.sort_cols = [s.col for s in self.query._sort]
        self.data = []
        self._fetched = False

    @property
    def sort(self):
        return self.query._sort

    def fetch(self, force=False):
        """Loads the query from the cache."""
        self.fetch_multi([self], force=force)

    @classmethod
    def fetch_multi(cls, crs, force=False):
        unfetched = filter(lambda cr: force or not cr._fetched, crs)
        if not unfetched:
            return

        cached = query_cache.get_multi([cr.iden for cr in unfetched],
                                       allow_local = not force)
        for cr in unfetched:
            cr.data = cached.get(cr.iden) or []
            cr._fetched = True

    def make_item_tuple(self, item):
        """Given a single 'item' from the result of a query build the tuple
        that will be stored in the query cache. It is effectively the
        fullname of the item after passing through the filter plus the
        columns of the unfiltered item to sort by."""
        filtered_item = self.filter(item)
        lst = [filtered_item._fullname]
        for col in self.sort_cols:
            #take the property of the original 
            attr = getattr(item, col)
            #convert dates to epochs to take less space
            if isinstance(attr, datetime):
                attr = epoch_seconds(attr)
            lst.append(attr)
        return tuple(lst)

    def can_insert(self):
        """True if a new item can just be inserted rather than
           rerunning the query."""
         # This is only true in some circumstances: queries where
         # eligibility in the list is determined only by its sort
         # value (e.g. hot) and where addition/removal from the list
         # incurs an insertion/deletion event called on the query. So
         # the top hottest items in X some subreddit where the query
         # is notified on every submission/banning/unbanning/deleting
         # will work, but for queries with a time-component or some
         # other eligibility factor, it cannot be inserted this way.
        if self.query._sort in ([desc('_date')],
                                [desc('_hot'), desc('_date')],
                                [desc('_score'), desc('_date')],
                                [desc('_controversy'), desc('_date')]):
            if not any(r for r in self.query._rules
                       if r.lval.name == '_date'):
                # if no time-rule is specified, then it's 'all'
                return True
        return False

    def can_delete(self):
        "True if a item can be removed from the listing, always true for now."
        return True

    def _mutate(self, fn, willread=True):
        self.data = query_cache.mutate(self.iden, fn, default=[], willread=willread)
        self._fetched=True

    def insert(self, items):
        """Inserts the item into the cached data. This only works
           under certain criteria, see can_insert."""
        self._insert_tuples([self.make_item_tuple(item) for item in tup(items)])

    def _insert_tuples(self, tuples):
        def _mutate(data):
            data = data or []
            item_tuples = tuples or []

            existing_fnames = {item[0] for item in data}
            new_fnames = {item[0] for item in item_tuples}

            mutated_length = len(existing_fnames.union(new_fnames))
            would_truncate = mutated_length >= precompute_limit
            if would_truncate:
                # only insert items that are already stored or new items
                # that are large enough that they won't be immediately truncated
                # out of storage
                # item structure is (name, sortval1[, sortval2, ...])
                smallest = data[-1]
                item_tuples = [item for item in item_tuples
                                    if (item[0] in existing_fnames or
                                        item[1:] >= smallest[1:])]

            if not item_tuples:
                return data

            # insert the items, remove the duplicates (keeping the
            # one being inserted over the stored value if applicable),
            # and sort the result
            data = filter(lambda x: x[0] not in new_fnames, data)
            data.extend(item_tuples)
            data.sort(reverse=True, key=lambda x: x[1:])
            if len(data) > precompute_limit:
                data = data[:precompute_limit]
            return data

        self._mutate(_mutate)

    def delete(self, items):
        """Deletes an item from the cached data."""
        fnames = set(self.filter(x)._fullname for x in tup(items))

        def _mutate(data):
            data = data or []
            return filter(lambda x: x[0] not in fnames,
                          data)

        self._mutate(_mutate)

    def _replace(self, tuples):
        """Take pre-rendered tuples from mr_top and replace the
           contents of the query outright. This should be considered a
           private API"""
        def _mutate(data):
            return tuples
        self._mutate(_mutate, willread=False)

    def update(self):
        """Runs the query and stores the result in the cache. This is
           only run by hand."""
        self.data = [self.make_item_tuple(i) for i in self.query]
        self._fetched = True
        query_cache.set(self.iden, self.data)

    def __repr__(self):
        return '<CachedResults %s %s>' % (self.query._rules, self.query._sort)

    def __iter__(self):
        self.fetch()

        for x in self.data:
            yield x[0]

class MergedCachedResults(object):
    """Given two CachedResults, merges their lists based on the sorts
       of their queries."""
    # normally we'd do this by having a superclass of CachedResults,
    # but we have legacy pickled CachedResults that we don't want to
    # break

    def __init__(self, results):
        self.cached_results = results
        CachedResults.fetch_multi([r for r in results
                                   if isinstance(r, CachedResults)])
        CachedQuery._fetch_multi([r for r in results
                                   if isinstance(r, CachedQuery)])
        self._fetched = True

        self.sort = results[0].sort
        comparator = ThingTupleComparator(self.sort)
        # make sure they're all the same
        assert all(r.sort == self.sort for r in results[1:])

        all_items = []
        for cr in results:
            all_items.extend(cr.data)
        all_items.sort(cmp=comparator)
        self.data = all_items


    def __repr__(self):
        return '<MergedCachedResults %r>' % (self.cached_results,)

    def __iter__(self):
        for x in self.data:
            yield x[0]

    def update(self):
        for x in self.cached_results:
            x.update()

def make_results(query, filter = filter_identity):
    return CachedResults(query, filter)

def merge_results(*results):
    if not results:
        return []
    return MergedCachedResults(results)

def migrating_cached_query(model, filter_fn=filter_identity):
    """Returns a CachedResults object that has a new-style cached query
    attached as "new_query". This way, reads will happen from the old
    query cache while writes can be made to go to both caches until a
    backfill migration is complete."""

    decorator = cached_query(model, filter_fn)
    def migrating_cached_query_decorator(fn):
        wrapped = decorator(fn)
        def migrating_cached_query_wrapper(*args):
            new_query = wrapped(*args)
            old_query = make_results(new_query.query, filter_fn)
            old_query.new_query = new_query
            return old_query
        return migrating_cached_query_wrapper
    return migrating_cached_query_decorator


@cached_query(UserQueryCache)
def get_deleted_links(user_id):
    return Link._query(Link.c.author_id == user_id,
                       Link.c._deleted == True,
                       Link.c._spam == (True, False),
                       sort=db_sort('new'))


@cached_query(UserQueryCache)
def get_deleted_comments(user_id):
    return Comment._query(Comment.c.author_id == user_id,
                          Comment.c._deleted == True,
                          Comment.c._spam == (True, False),
                          sort=db_sort('new'))


@merged_cached_query
def get_deleted(user):
    return [get_deleted_links(user),
            get_deleted_comments(user)]


def get_links(sr, sort, time):
    return _get_links(sr._id, sort, time)

def _get_links(sr_id, sort, time):
    """General link query for a subreddit."""
    q = Link._query(Link.c.sr_id == sr_id,
                    sort = db_sort(sort),
                    data = True)

    if time != 'all':
        q._filter(db_times[time])

    res = make_results(q)

    return res

@cached_query(SubredditQueryCache)
def get_spam_links(sr_id):
    return Link._query(Link.c.sr_id == sr_id,
                       Link.c._spam == True,
                       sort = db_sort('new'))

@cached_query(SubredditQueryCache)
def get_spam_comments(sr_id):
    return Comment._query(Comment.c.sr_id == sr_id,
                          Comment.c._spam == True,
                          sort = db_sort('new'))

def moderated_srids(sr, user):
    if isinstance(sr, (ModContribSR, MultiReddit)):
        srs = Subreddit._byID(sr.sr_ids, return_dict=False)
        if user:
            srs = [sr for sr in srs
                   if sr.is_moderator_with_perms(user, 'posts')]
        return [sr._id for sr in srs]
    else:
        return [sr._id]

@merged_cached_query
def get_spam(sr, user=None, include_links=True, include_comments=True):
    sr_ids = moderated_srids(sr, user)
    queries = []

    if include_links:
        queries.append(get_spam_links)
    if include_comments:
        queries.append(get_spam_comments)
    return [query(sr_id) for sr_id, query in itertools.product(sr_ids, queries)]

@cached_query(SubredditQueryCache)
def get_spam_filtered_links(sr_id):
    """ NOTE: This query will never run unless someone does an "update" on it,
        but that will probably timeout. Use insert_spam_filtered_links."""
    return Link._query(Link.c.sr_id == sr_id,
                       Link.c._spam == True,
                       Link.c.verdict != 'mod-removed',
                       sort = db_sort('new'))

@cached_query(SubredditQueryCache)
def get_spam_filtered_comments(sr_id):
    return Comment._query(Comment.c.sr_id == sr_id,
                          Comment.c._spam == True,
                          Comment.c.verdict != 'mod-removed',
                          sort = db_sort('new'))

@merged_cached_query
def get_spam_filtered(sr):
    return [get_spam_filtered_links(sr),
            get_spam_filtered_comments(sr)]

@cached_query(SubredditQueryCache)
def get_reported_links(sr_id):
    return Link._query(Link.c.reported != 0,
                       Link.c.sr_id == sr_id,
                       Link.c._spam == False,
                       sort = db_sort('new'))

@cached_query(SubredditQueryCache)
def get_reported_comments(sr_id):
    return Comment._query(Comment.c.reported != 0,
                          Comment.c.sr_id == sr_id,
                          Comment.c._spam == False,
                          sort = db_sort('new'))

@merged_cached_query
def get_reported(sr, user=None, include_links=True, include_comments=True):
    sr_ids = moderated_srids(sr, user)
    queries = []

    if include_links:
        queries.append(get_reported_links)
    if include_comments:
        queries.append(get_reported_comments)
    return [query(sr_id) for sr_id, query in itertools.product(sr_ids, queries)]

@cached_query(SubredditQueryCache)
def get_unmoderated_links(sr_id):
    q = Link._query(Link.c.sr_id == sr_id,
                    Link.c._spam == (True, False),
                    sort = db_sort('new'))

    # Doesn't really work because will not return Links with no verdict
    q._filter(or_(and_(Link.c._spam == True, Link.c.verdict != 'mod-removed'),
                  and_(Link.c._spam == False, Link.c.verdict != 'mod-approved')))
    return q

@merged_cached_query
def get_modqueue(sr, user=None, include_links=True, include_comments=True):
    sr_ids = moderated_srids(sr, user)
    queries = []

    if include_links:
        queries.append(get_reported_links)
        queries.append(get_spam_filtered_links)
    if include_comments:
        queries.append(get_reported_comments)
        queries.append(get_spam_filtered_comments)
    return [query(sr_id) for sr_id, query in itertools.product(sr_ids, queries)]

@merged_cached_query
def get_unmoderated(sr, user=None):
    sr_ids = moderated_srids(sr, user)
    queries = [get_unmoderated_links]
    return [query(sr_id) for sr_id, query in itertools.product(sr_ids, queries)]

def get_domain_links(domain, sort, time):
    from r2.lib.db import operators
    q = Link._query(operators.domain(Link.c.url) == filters._force_utf8(domain),
                    sort = db_sort(sort),
                    data = True)
    if time != "all":
        q._filter(db_times[time])

    return make_results(q)

def user_query(kind, user_id, sort, time):
    """General profile-page query."""
    q = kind._query(kind.c.author_id == user_id,
                    kind.c._spam == (True, False),
                    sort = db_sort(sort))
    if time != 'all':
        q._filter(db_times[time])
    return make_results(q)

def get_all_comments():
    """the master /comments page"""
    q = Comment._query(sort = desc('_date'))
    return make_results(q)

def get_sr_comments(sr):
    return _get_sr_comments(sr._id)

def _get_sr_comments(sr_id):
    """the subreddit /r/foo/comments page"""
    q = Comment._query(Comment.c.sr_id == sr_id,
                       sort = desc('_date'))
    return make_results(q)

def _get_comments(user_id, sort, time):
    return user_query(Comment, user_id, sort, time)

def get_comments(user, sort, time):
    return _get_comments(user._id, sort, time)

def _get_submitted(user_id, sort, time):
    return user_query(Link, user_id, sort, time)

def get_submitted(user, sort, time):
    return _get_submitted(user._id, sort, time)

def get_overview(user, sort, time):
    return merge_results(get_comments(user, sort, time),
                         get_submitted(user, sort, time))

def rel_query(rel, thing_id, name, filters = []):
    """General relationship query."""

    q = rel._query(rel.c._thing1_id == thing_id,
                   rel.c._t2_deleted == False,
                   rel.c._name == name,
                   sort = desc('_date'),
                   eager_load = True,
                   )
    if filters:
        q._filter(*filters)

    return q

vote_rel = Vote.rel(Account, Link)

cached_userrel_query = cached_query(UserQueryCache, filter_thing2)
cached_srrel_query = cached_query(SubredditQueryCache, filter_thing2)

@cached_userrel_query
def get_liked(user):
    return rel_query(vote_rel, user, '1')

@cached_userrel_query
def get_disliked(user):
    return rel_query(vote_rel, user, '-1')

@cached_query(UserQueryCache, sort=[desc('action_date')])
def get_hidden_links(user_id):
    return

def get_hidden(user):
    return get_hidden_links(user)

@cached_query(UserQueryCache, sort=[desc('action_date')])
def get_categorized_saved_links(user_id, sr_id, category):
    return

@cached_query(UserQueryCache, sort=[desc('action_date')])
def get_categorized_saved_comments(user_id, sr_id, category):
    return

@cached_query(UserQueryCache, sort=[desc('action_date')])
def get_saved_links(user_id, sr_id):
    return

@cached_query(UserQueryCache, sort=[desc('action_date')])
def get_saved_comments(user_id, sr_id):
    return

def get_saved(user, sr_id=None, category=None):
    sr_id = sr_id or 'none'
    if not category:
        queries = [get_saved_links(user, sr_id),
                   get_saved_comments(user, sr_id)]
    else:
        queries = [get_categorized_saved_links(user, sr_id, category),
                   get_categorized_saved_comments(user, sr_id, category)]
    return MergedCachedQuery(queries)

@cached_srrel_query
def get_subreddit_messages(sr):
    return rel_query(ModeratorInbox, sr, 'inbox')

@cached_srrel_query
def get_unread_subreddit_messages(sr):
    return rel_query(ModeratorInbox, sr, 'inbox',
                          filters = [ModeratorInbox.c.new == True])

def get_unread_subreddit_messages_multi(srs):
    if not srs:
        return []
    queries = [get_unread_subreddit_messages(sr) for sr in srs]
    return MergedCachedQuery(queries)

inbox_message_rel = Inbox.rel(Account, Message)
@cached_userrel_query
def get_inbox_messages(user):
    return rel_query(inbox_message_rel, user, 'inbox')

@cached_userrel_query
def get_unread_messages(user):
    return rel_query(inbox_message_rel, user, 'inbox',
                          filters = [inbox_message_rel.c.new == True])

inbox_comment_rel = Inbox.rel(Account, Comment)
@cached_userrel_query
def get_inbox_comments(user):
    return rel_query(inbox_comment_rel, user, 'inbox')

@cached_userrel_query
def get_unread_comments(user):
    return rel_query(inbox_comment_rel, user, 'inbox',
                          filters = [inbox_comment_rel.c.new == True])

@cached_userrel_query
def get_inbox_selfreply(user):
    return rel_query(inbox_comment_rel, user, 'selfreply')

@cached_userrel_query
def get_unread_selfreply(user):
    return rel_query(inbox_comment_rel, user, 'selfreply',
                          filters = [inbox_comment_rel.c.new == True])


@cached_userrel_query
def get_inbox_comment_mentions(user):
    return rel_query(inbox_comment_rel, user, "mention")


@cached_userrel_query
def get_unread_comment_mentions(user):
    return rel_query(inbox_comment_rel, user, "mention",
                     filters=[inbox_comment_rel.c.new == True])


def get_inbox(user):
    return merge_results(get_inbox_comments(user),
                         get_inbox_messages(user),
                         get_inbox_comment_mentions(user),
                         get_inbox_selfreply(user))

@cached_query(UserQueryCache)
def get_sent(user_id):
    return Message._query(Message.c.author_id == user_id,
                          Message.c._spam == (True, False),
                          sort = desc('_date'))

def get_unread_inbox(user):
    return merge_results(get_unread_comments(user),
                         get_unread_messages(user),
                         get_unread_comment_mentions(user),
                         get_unread_selfreply(user))

def _user_reported_query(user_id, thing_cls):
    rel_cls = Report.rel(Account, thing_cls)
    return rel_query(rel_cls, user_id, ('-1', '0', '1'))
    # -1: rejected report
    # 0: unactioned report
    # 1: accepted report

@cached_userrel_query
def get_user_reported_links(user_id):
    return _user_reported_query(user_id, Link)

@cached_userrel_query
def get_user_reported_comments(user_id):
    return _user_reported_query(user_id, Comment)

@cached_userrel_query
def get_user_reported_messages(user_id):
    return _user_reported_query(user_id, Message)

@merged_cached_query
def get_user_reported(user_id):
    return [get_user_reported_links(user_id),
            get_user_reported_comments(user_id),
            get_user_reported_messages(user_id)]


def set_promote_status(link, promote_status):
    all_queries = [promote_query(link.author_id) for promote_query in 
                   (get_unpaid_links, get_unapproved_links, 
                    get_rejected_links, get_live_links, get_accepted_links)]
    all_queries.extend([get_all_unpaid_links(), get_all_unapproved_links(),
                        get_all_rejected_links(), get_all_live_links(),
                        get_all_accepted_links()])

    if promote_status == PROMOTE_STATUS.unpaid:
        inserts = [get_unpaid_links(link.author_id), get_all_unpaid_links()]
    elif promote_status == PROMOTE_STATUS.unseen:
        inserts = [get_unapproved_links(link.author_id),
                   get_all_unapproved_links()]
    elif promote_status == PROMOTE_STATUS.rejected:
        inserts = [get_rejected_links(link.author_id), get_all_rejected_links()]
    elif promote_status == PROMOTE_STATUS.promoted:
        inserts = [get_live_links(link.author_id), get_all_live_links()]
    elif promote_status in (PROMOTE_STATUS.accepted, PROMOTE_STATUS.pending,
                            PROMOTE_STATUS.finished):
        inserts = [get_accepted_links(link.author_id), get_all_accepted_links()]

    deletes = list(set(all_queries) - set(inserts))
    with CachedQueryMutator() as m:
        for q in inserts:
            m.insert(q, [link])
        for q in deletes:
            m.delete(q, [link])

    link.promote_status = promote_status
    link._commit()

    text = "set promote status to '%s'" % PROMOTE_STATUS.name[promote_status]
    PromotionLog.add(link, text)


def _promoted_link_query(user_id, status):
    STATUS_CODES = {'unpaid': PROMOTE_STATUS.unpaid,
                    'unapproved': PROMOTE_STATUS.unseen,
                    'rejected': PROMOTE_STATUS.rejected,
                    'live': PROMOTE_STATUS.promoted,
                    'accepted': (PROMOTE_STATUS.accepted,
                                 PROMOTE_STATUS.pending,
                                 PROMOTE_STATUS.finished)}

    q = Link._query(Link.c.sr_id == get_promote_srid(),
                    Link.c._spam == (True, False),
                    Link.c._deleted == (True, False),
                    Link.c.promote_status == STATUS_CODES[status],
                    sort=db_sort('new'))
    if user_id:
        q._filter(Link.c.author_id == user_id)
    return q


@cached_query(UserQueryCache)
def get_unpaid_links(user_id):
    return _promoted_link_query(user_id, 'unpaid')


@cached_query(UserQueryCache)
def get_all_unpaid_links():
    return _promoted_link_query(None, 'unpaid')


@cached_query(UserQueryCache)
def get_unapproved_links(user_id):
    return _promoted_link_query(user_id, 'unapproved')


@cached_query(UserQueryCache)
def get_all_unapproved_links():
    return _promoted_link_query(None, 'unapproved')


@cached_query(UserQueryCache)
def get_rejected_links(user_id):
    return _promoted_link_query(user_id, 'rejected')


@cached_query(UserQueryCache)
def get_all_rejected_links():
    return _promoted_link_query(None, 'rejected')


@cached_query(UserQueryCache)
def get_live_links(user_id):
    return _promoted_link_query(user_id, 'live')


@cached_query(UserQueryCache)
def get_all_live_links():
    return _promoted_link_query(None, 'live')


@cached_query(UserQueryCache)
def get_accepted_links(user_id):
    return _promoted_link_query(user_id, 'accepted')


@cached_query(UserQueryCache)
def get_all_accepted_links():
    return _promoted_link_query(None, 'accepted')


@cached_query(UserQueryCache, sort=[desc('_date')])
def get_underdelivered_campaigns():
    return


def set_underdelivered_campaigns(campaigns):
    campaigns = tup(campaigns)
    with CachedQueryMutator() as m:
        q = get_underdelivered_campaigns()
        m.insert(q, campaigns)


def unset_underdelivered_campaigns(campaigns):
    campaigns = tup(campaigns)
    with CachedQueryMutator() as m:
        q = get_underdelivered_campaigns()
        m.delete(q, campaigns)


@merged_cached_query
def get_promoted_links(user_id):
    queries = [get_unpaid_links(user_id), get_unapproved_links(user_id),
               get_rejected_links(user_id), get_live_links(user_id),
               get_accepted_links(user_id)]
    return queries


@merged_cached_query
def get_all_promoted_links():
    queries = [get_all_unpaid_links(), get_all_unapproved_links(),
               get_all_rejected_links(), get_all_live_links(),
               get_all_accepted_links()]
    return queries


@cached_query(SubredditQueryCache, sort=[desc("date")], filter_fn=filter_thing)
def get_all_gilded_comments():
    return


@cached_query(SubredditQueryCache, sort=[desc("date")], filter_fn=filter_thing)
def get_all_gilded_links():
    return


@merged_cached_query
def get_all_gilded():
    return [get_all_gilded_comments(), get_all_gilded_links()]


@cached_query(SubredditQueryCache, sort=[desc("date")], filter_fn=filter_thing)
def get_gilded_comments(sr_id):
    return


@cached_query(SubredditQueryCache, sort=[desc("date")], filter_fn=filter_thing)
def get_gilded_links(sr_id):
    return


@merged_cached_query
def get_gilded(sr_ids):
    queries = [get_gilded_links, get_gilded_comments]
    return [query(sr_id)
            for sr_id, query in itertools.product(tup(sr_ids), queries)]


@cached_query(UserQueryCache, sort=[desc("date")], filter_fn=filter_thing)
def get_gilded_user_comments(user_id):
    return


@cached_query(UserQueryCache, sort=[desc("date")], filter_fn=filter_thing)
def get_gilded_user_links(user_id):
    return


@merged_cached_query
def get_gilded_users(user_ids):
    queries = [get_gilded_user_links, get_gilded_user_comments]
    return [query(user_id)
            for user_id, query in itertools.product(tup(user_ids), queries)]


@cached_query(UserQueryCache, sort=[desc("date")], filter_fn=filter_thing)
def get_user_gildings(user_id):
    return


@merged_cached_query
def get_gilded_user(user):
    return [get_gilded_user_comments(user), get_gilded_user_links(user)]


def add_queries(queries, insert_items=None, delete_items=None, foreground=False):
    """Adds multiple queries to the query queue. If insert_items or
       delete_items is specified, the query may not need to be
       recomputed against the database."""
    for q in queries:
        if insert_items and q.can_insert():
            log.debug("Inserting %s into query %s" % (insert_items, q))
            if foreground:
                q.insert(insert_items)
            else:
                worker.do(q.insert, insert_items)
        elif delete_items and q.can_delete():
            log.debug("Deleting %s from query %s" % (delete_items, q))
            if foreground:
                q.delete(delete_items)
            else:
                worker.do(q.delete, delete_items)
        else:
            raise Exception("Cannot update query %r!" % (q,))

    # dual-write any queries that are being migrated to the new query cache
    with CachedQueryMutator() as m:
        new_queries = [getattr(q, 'new_query') for q in queries if hasattr(q, 'new_query')]

        if insert_items:
            for query in new_queries:
                m.insert(query, tup(insert_items))

        if delete_items:
            for query in new_queries:
                m.delete(query, tup(delete_items))

#can be rewritten to be more efficient
def all_queries(fn, obj, *param_lists):
    """Given a fn and a first argument 'obj', calls the fn(obj, *params)
    for every permutation of the parameters in param_lists"""
    results = []
    params = [[obj]]
    for pl in param_lists:
        new_params = []
        for p in pl:
            for c in params:
                new_param = list(c)
                new_param.append(p)
                new_params.append(new_param)
        params = new_params

    results = [fn(*p) for p in params]
    return results

## The following functions should be called after their respective
## actions to update the correct listings.
def new_link(link):
    "Called on the submission and deletion of links"
    sr = Subreddit._byID(link.sr_id)
    author = Account._byID(link.author_id)

    results = [get_links(sr, 'new', 'all')]
    # we don't have to do hot/top/controversy because new_vote will do
    # that

    results.append(get_submitted(author, 'new', 'all'))

    for domain in utils.UrlParser(link.url).domain_permutations():
        results.append(get_domain_links(domain, 'new', "all"))

    with CachedQueryMutator() as m:
        if link._spam:    
            m.insert(get_spam_links(sr), [link])
        if not (sr.exclude_banned_modqueue and author._spam):
            m.insert(get_unmoderated_links(sr), [link])

    add_queries(results, insert_items = link)
    amqp.add_item('new_link', link._fullname)


def add_to_commentstree_q(comment):
    if utils.to36(comment.link_id) in g.live_config["fastlane_links"]:
        amqp.add_item('commentstree_fastlane_q', comment._fullname)
    elif g.shard_commentstree_queues:
        amqp.add_item('commentstree_%d_q' % (comment.link_id % 10),
                      comment._fullname)
    else:
        amqp.add_item('commentstree_q', comment._fullname)


def new_comment(comment, inbox_rels):
    author = Account._byID(comment.author_id)
    job = [get_comments(author, 'new', 'all'),
           get_comments(author, 'top', 'all'),
           get_comments(author, 'controversial', 'all')]

    sr = Subreddit._byID(comment.sr_id)

    with CachedQueryMutator() as m:
        if comment._deleted:
            job_key = "delete_items"
            job.append(get_sr_comments(sr))
            job.append(get_all_comments())
        else:
            job_key = "insert_items"
            if comment._spam:
                m.insert(get_spam_comments(sr), [comment])
            if (was_spam_filtered(comment) and
                    not (sr.exclude_banned_modqueue and author._spam)):
                m.insert(get_spam_filtered_comments(sr), [comment])

            amqp.add_item('new_comment', comment._fullname)
            add_to_commentstree_q(comment)

            if not g.amqp_host:
                add_comments([comment])

        job_dict = { job_key: comment }
        add_queries(job, **job_dict)

        # note that get_all_comments() is updated by the amqp process
        # r2.lib.db.queries.run_new_comments (to minimise lock contention)

        if inbox_rels:
            for inbox_rel in tup(inbox_rels):
                inbox_owner = inbox_rel._thing1
                if inbox_rel._name == "inbox":
                    query = get_inbox_comments(inbox_owner)
                elif inbox_rel._name == "selfreply":
                    query = get_inbox_selfreply(inbox_owner)
                else:
                    raise ValueError("wtf is " + inbox_rel._name)

                # mentions happen in butler_q

                if not comment._deleted:
                    m.insert(query, [inbox_rel])
                else:
                    m.delete(query, [inbox_rel])

                set_unread(comment, inbox_owner,
                           unread=not comment._deleted, mutator=m)


def delete_comment(comment):
    add_to_commentstree_q(comment)


def new_subreddit(sr):
    "no precomputed queries here yet"
    amqp.add_item('new_subreddit', sr._fullname)


def new_vote(vote, foreground=False, timer=None):
    user = vote._thing1
    item = vote._thing2

    if timer is None:
        timer = SimpleSillyStub()

    if vote.valid_thing and not item._spam and not item._deleted:
        sr = item.subreddit_slow
        results = []

        author = Account._byID(item.author_id)
        for sort in ('hot', 'top', 'controversial', 'new'):
            if isinstance(item, Link):
                results.append(get_submitted(author, sort, 'all'))
            if isinstance(item, Comment):
                results.append(get_comments(author, sort, 'all'))

        if isinstance(item, Link):
            # don't do 'new', because that was done by new_link, and
            # the time-filtered versions of top/controversial will be
            # done by mr_top
            results.extend([get_links(sr, 'hot', 'all'),
                            get_links(sr, 'top', 'all'),
                            get_links(sr, 'controversial', 'all'),
                            ])

            parsed = utils.UrlParser(item.url)
            if parsed.hostname and not parsed.hostname.endswith('imgur.com'):
                for domain in parsed.domain_permutations():
                    for sort in ("hot", "top", "controversial"):
                        results.append(get_domain_links(domain, sort, "all"))

        add_queries(results, insert_items = item, foreground=foreground)

    timer.intermediate("permacache")
    
    if isinstance(item, Link):
        # must update both because we don't know if it's a changed
        # vote
        with CachedQueryMutator() as m:
            if vote._name == '1':
                m.insert(get_liked(user), [vote])
                m.delete(get_disliked(user), [vote])
            elif vote._name == '-1':
                m.delete(get_liked(user), [vote])
                m.insert(get_disliked(user), [vote])
            else:
                m.delete(get_liked(user), [vote])
                m.delete(get_disliked(user), [vote])

def new_message(message, inbox_rels):
    from r2.lib.comment_tree import add_message

    from_user = Account._byID(message.author_id)
    for inbox_rel in tup(inbox_rels):
        to = inbox_rel._thing1

        with CachedQueryMutator() as m:
            m.insert(get_sent(from_user), [message])

            # moderator message
            if isinstance(inbox_rel, ModeratorInbox):
                m.insert(get_subreddit_messages(to), [inbox_rel])
            # personal message
            else:
                m.insert(get_inbox_messages(to), [inbox_rel])

            set_unread(message, to, unread=True, mutator=m)

    amqp.add_item('new_message', message._fullname)
    add_message(message)

def set_unread(messages, to, unread, mutator=None):
    # Maintain backwards compatability
    messages = tup(messages)

    if not mutator:
        m = CachedQueryMutator()
    else:
        m = mutator

    if isinstance(to, Subreddit):
        for i in ModeratorInbox.set_unread(messages, unread):
            q = get_unread_subreddit_messages(i._thing1_id)
            if unread:
                m.insert(q, [i])
            else:
                m.delete(q, [i])
    else:
        # All messages should be of the same type
        # (asserted by Inbox.set_unread)
        for i in Inbox.set_unread(messages, unread, to=to):
            query = None
            if isinstance(messages[0], Comment):
                if i._name == "inbox":
                    query = get_unread_comments(i._thing1_id)
                elif i._name == "selfreply":
                    query = get_unread_selfreply(i._thing1_id)
                elif i._name == "mention":
                    query = get_unread_comment_mentions(i._thing1_id)
            elif isinstance(messages[0], Message):
                query = get_unread_messages(i._thing1_id)
            assert query is not None

            if unread:
                m.insert(query, [i])
            else:
                m.delete(query, [i])

    if not mutator:
        m.send()


def changed(things, boost_only=False):
    """Indicate to search that a given item should be updated in the index"""
    for thing in tup(things):
        msg = {'fullname': thing._fullname}
        if boost_only:
            msg['boost_only'] = True

        amqp.add_item('search_changes', pickle.dumps(msg),
                      message_id = thing._fullname,
                      delivery_mode = amqp.DELIVERY_TRANSIENT)

def _by_srid(things, srs=True):
    """Takes a list of things and returns them in a dict separated by
       sr_id, in addition to the looked-up subreddits"""
    ret = {}

    for thing in tup(things):
        if getattr(thing, 'sr_id', None) is not None:
            ret.setdefault(thing.sr_id, []).append(thing)

    if srs:
        _srs = Subreddit._byID(ret.keys(), return_dict=True) if ret else {}
        return ret, _srs
    else:
        return ret


def _by_author(things, authors=True):
    ret = collections.defaultdict(list)

    for thing in tup(things):
        author_id = getattr(thing, 'author_id')
        if author_id:
            ret[author_id].append(thing)

    if authors:
        _authors = Account._byID(ret.keys(), return_dict=True) if ret else {}
        return ret, _authors
    else:
        return ret

def _by_thing1_id(rels):
    ret = {}
    for rel in tup(rels):
        ret.setdefault(rel._thing1_id, []).append(rel)
    return ret


def was_spam_filtered(thing):
    if (thing._spam and not thing._deleted and
        getattr(thing, 'verdict', None) != 'mod-removed'):
        return True
    else:
        return False


def delete(things):
    query_cache_inserts, query_cache_deletes = _common_del_ban(things)
    by_srid, srs = _by_srid(things)
    by_author, authors = _by_author(things)

    for sr_id, sr_things in by_srid.iteritems():
        sr = srs[sr_id]
        links = [x for x in sr_things if isinstance(x, Link)]
        comments = [x for x in sr_things if isinstance(x, Comment)]

        if links:
            query_cache_deletes.append((get_spam_links(sr), links))
            query_cache_deletes.append((get_spam_filtered_links(sr), links))
            query_cache_deletes.append((get_unmoderated_links(sr_id),
                                            links))
        if comments:
            query_cache_deletes.append((get_spam_comments(sr), comments))
            query_cache_deletes.append((get_spam_filtered_comments(sr),
                                        comments))

    for author_id, a_things in by_author.iteritems():
        author = authors[author_id]
        links = [x for x in a_things if isinstance(x, Link)]
        comments = [x for x in a_things if isinstance(x, Comment)]

        if links:
            results = [get_submitted(author, 'hot', 'all'),
                       get_submitted(author, 'new', 'all')]
            for sort in time_filtered_sorts:
                for time in db_times.keys():
                    results.append(get_submitted(author, sort, time))
            add_queries(results, delete_items=links)
            query_cache_inserts.append((get_deleted_links(author_id), links))
        if comments:
            results = [get_comments(author, 'hot', 'all'),
                       get_comments(author, 'new', 'all')]
            for sort in time_filtered_sorts:
                for time in db_times.keys():
                    results.append(get_comments(author, sort, time))
            add_queries(results, delete_items=comments)
            query_cache_inserts.append((get_deleted_comments(author_id),
                                        comments))

    with CachedQueryMutator() as m:
        for q, inserts in query_cache_inserts:
            m.insert(q, inserts)
        for q, deletes in query_cache_deletes:
            m.delete(q, deletes)
    changed(things)


def ban(things, filtered=True):
    query_cache_inserts, query_cache_deletes = _common_del_ban(things)
    by_srid = _by_srid(things, srs=False)

    for sr_id, sr_things in by_srid.iteritems():
        links = []
        modqueue_links = []
        comments = []
        modqueue_comments = []
        for item in sr_things:
            # don't add posts by banned users if subreddit prefs exclude them
            add_to_modqueue = (filtered and
                       not (item.subreddit_slow.exclude_banned_modqueue and
                            item.author_slow._spam))

            if isinstance(item, Link):
                links.append(item)
                if add_to_modqueue:
                    modqueue_links.append(item)
            elif isinstance(item, Comment):
                comments.append(item)
                if add_to_modqueue:
                    modqueue_comments.append(item)

        if links:
            query_cache_inserts.append((get_spam_links(sr_id), links))
            if not filtered:
                query_cache_deletes.append(
                        (get_spam_filtered_links(sr_id), links))
                query_cache_deletes.append(
                        (get_unmoderated_links(sr_id), links))

        if modqueue_links:
            query_cache_inserts.append(
                    (get_spam_filtered_links(sr_id), modqueue_links))

        if comments:
            query_cache_inserts.append((get_spam_comments(sr_id), comments))
            if not filtered:
                query_cache_deletes.append(
                        (get_spam_filtered_comments(sr_id), comments))

        if modqueue_comments:
            query_cache_inserts.append(
                    (get_spam_filtered_comments(sr_id), modqueue_comments))

    with CachedQueryMutator() as m:
        for q, inserts in query_cache_inserts:
            m.insert(q, inserts)
        for q, deletes in query_cache_deletes:
            m.delete(q, deletes)
    changed(things)


def _common_del_ban(things):
    query_cache_inserts = []
    query_cache_deletes = []
    by_srid, srs = _by_srid(things)

    for sr_id, sr_things in by_srid.iteritems():
        sr = srs[sr_id]
        links = [x for x in sr_things if isinstance(x, Link)]
        comments = [x for x in sr_things if isinstance(x, Comment)]

        if links:
            results = [get_links(sr, 'hot', 'all'), get_links(sr, 'new', 'all')]
            for sort in time_filtered_sorts:
                for time in db_times.keys():
                    results.append(get_links(sr, sort, time))
            add_queries(results, delete_items=links)
            query_cache_deletes.append([get_reported_links(sr), links])
        if comments:
            query_cache_deletes.append([get_reported_comments(sr), comments])

    return query_cache_inserts, query_cache_deletes


def unban(things, insert=True):
    query_cache_deletes = []

    by_srid, srs = _by_srid(things)
    if not by_srid:
        return

    for sr_id, things in by_srid.iteritems():
        sr = srs[sr_id]
        links = [x for x in things if isinstance(x, Link)]
        comments = [x for x in things if isinstance(x, Comment)]

        if insert and links:
            # put it back in the listings
            results = [get_links(sr, 'hot', 'all'),
                       get_links(sr, 'top', 'all'),
                       get_links(sr, 'controversial', 'all'),
                       ]
            # the time-filtered listings will have to wait for the
            # next mr_top run
            add_queries(results, insert_items=links)

            # Check if link is being unbanned and should be put in
            # 'new' with current time
            new_links = []
            for l in links:
                ban_info = l.ban_info
                if ban_info.get('reset_used', True) == False and \
                    ban_info.get('auto', False):
                    l_copy = deepcopy(l)
                    l_copy._date = ban_info['unbanned_at']
                    new_links.append(l_copy)
                else:
                    new_links.append(l)
            add_queries([get_links(sr, 'new', 'all')], insert_items=new_links)
            query_cache_deletes.append([get_spam_links(sr), links])

        if insert and comments:
            add_queries([get_all_comments(), get_sr_comments(sr)],
                        insert_items=comments)
            query_cache_deletes.append([get_spam_comments(sr), comments])

        if links:
            query_cache_deletes.append((get_unmoderated_links(sr), links))
            query_cache_deletes.append([get_spam_filtered_links(sr), links])

        if comments:
            query_cache_deletes.append([get_spam_filtered_comments(sr), comments])

    with CachedQueryMutator() as m:
        for q, deletes in query_cache_deletes:
            m.delete(q, deletes)

    changed(things)

def new_report(thing, report_rel):
    reporter_id = report_rel._thing1_id

    with CachedQueryMutator() as m:
        if isinstance(thing, Link):
            m.insert(get_reported_links(thing.sr_id), [thing])
            m.insert(get_user_reported_links(reporter_id), [report_rel])
        elif isinstance(thing, Comment):
            m.insert(get_reported_comments(thing.sr_id), [thing])
            m.insert(get_user_reported_comments(reporter_id), [report_rel])
        elif isinstance(thing, Message):
            m.insert(get_user_reported_messages(reporter_id), [report_rel])


def clear_reports(things, rels):
    query_cache_deletes = []

    by_srid = _by_srid(things, srs=False)

    for sr_id, sr_things in by_srid.iteritems():
        links = [ x for x in sr_things if isinstance(x, Link) ]
        comments = [ x for x in sr_things if isinstance(x, Comment) ]

        if links:
            query_cache_deletes.append([get_reported_links(sr_id), links])
        if comments:
            query_cache_deletes.append([get_reported_comments(sr_id), comments])

    # delete from user_reported if the report was correct
    rels = [r for r in rels if r._name == '1']
    if rels:
        link_rels = [r for r in rels if r._type2 == Link]
        comment_rels = [r for r in rels if r._type2 == Comment]
        message_rels = [r for r in rels if r._type2 == Message]

        rels_to_query = ((link_rels, get_user_reported_links),
                         (comment_rels, get_user_reported_comments),
                         (message_rels, get_user_reported_messages))

        for thing_rels, query in rels_to_query:
            if not thing_rels:
                continue

            by_thing1_id = _by_thing1_id(thing_rels)
            for reporter_id, reporter_rels in by_thing1_id.iteritems():
                query_cache_deletes.append([query(reporter_id), reporter_rels])

    with CachedQueryMutator() as m:
        for q, deletes in query_cache_deletes:
            m.delete(q, deletes)


def add_all_srs():
    """Recalculates every listing query for every subreddit. Very,
       very slow."""
    q = Subreddit._query(sort = asc('_date'))
    for sr in fetch_things2(q):
        for q in all_queries(get_links, sr, ('hot', 'new'), ['all']):
            q.update()
        for q in all_queries(get_links, sr, time_filtered_sorts, db_times.keys()):
            q.update()
        get_spam_links(sr).update()
        get_spam_comments(sr).update()
        get_reported_links(sr).update()
        get_reported_comments(sr).update()

def update_user(user):
    if isinstance(user, str):
        user = Account._by_name(user)
    elif isinstance(user, int):
        user = Account._byID(user)

    results = [get_inbox_messages(user),
               get_inbox_comments(user),
               get_inbox_selfreply(user),
               get_sent(user),
               get_liked(user),
               get_disliked(user),
               get_submitted(user, 'new', 'all'),
               get_comments(user, 'new', 'all')]
    for q in results:
        q.update()

def add_all_users():
    q = Account._query(sort = asc('_date'))
    for user in fetch_things2(q):
        update_user(user)

# amqp queue processing functions

def run_new_comments(limit=1000):
    """Add new incoming comments to the /comments page"""
    # this is done as a queue because otherwise the contention for the
    # lock on the query would be very high

    @g.stats.amqp_processor('newcomments_q')
    def _run_new_comments(msgs, chan):
        fnames = [msg.body for msg in msgs]

        comments = Comment._by_fullname(fnames, data=True, return_dict=False)
        add_queries([get_all_comments()],
                    insert_items=comments)

        bysrid = _by_srid(comments, False)
        for srid, sr_comments in bysrid.iteritems():
            add_queries([_get_sr_comments(srid)],
                        insert_items=sr_comments)

    amqp.handle_items('newcomments_q', _run_new_comments, limit=limit)

def run_commentstree(qname="commentstree_q", limit=100):
    """Add new incoming comments to their respective comments trees"""

    @g.stats.amqp_processor(qname)
    def _run_commentstree(msgs, chan):
        comments = Comment._by_fullname([msg.body for msg in msgs],
                                        data = True, return_dict = False)
        print 'Processing %r' % (comments,)

        # when fastlaning a thread, we may need to have this qproc ignore
        # messages that were put into the non-fastlane queue and are causing
        # both to back up. a full recompute of the old thread will fix these
        # missed messages.
        if qname != "commentstree_fastlane_q":
            fastlaned_links = g.live_config["fastlane_links"]
            links = Link._byID([com.link_id for com in comments], data=True)
            comments = [com for com in comments
                        if utils.to36(com.link_id) not in fastlaned_links and
                           links[com.link_id].skip_commentstree_q != qname]

        if comments:
            add_comments(comments)

    amqp.handle_items(qname, _run_commentstree, limit = limit)

vote_link_q = 'vote_link_q'
vote_comment_q = 'vote_comment_q'
vote_fastlane_q = 'vote_fastlane_q'

def queue_vote(user, thing, dir, ip, vote_info=None,
               cheater = False, store = True):
    # set the vote in memcached so the UI gets updated immediately
    key = prequeued_vote_key(user, thing)
    g.cache.set(key, '1' if dir is True else '0' if dir is None else '-1')
    # queue the vote to be stored unless told not to
    if store:
        if g.amqp_host:
            if isinstance(thing, Link):
                if thing._id36 in g.live_config["fastlane_links"]:
                    qname = vote_fastlane_q
                else:
                    if g.shard_link_vote_queues:
                        qname = "vote_link_%s_q" % str(thing.sr_id)[-1]
                    else:
                        qname = vote_link_q

            elif isinstance(thing, Comment):
                if utils.to36(thing.link_id) in g.live_config["fastlane_links"]:
                    qname = vote_fastlane_q
                else:
                    qname = vote_comment_q
            else:
                log.warning("%s tried to vote on %r. that's not a link or comment!",
                            user, thing)
                return

            amqp.add_item(qname,
                          pickle.dumps((user._id, thing._fullname,
                                        dir, ip, vote_info, cheater)))
        else:
            handle_vote(user, thing, dir, ip, vote_info)

def prequeued_vote_key(user, item):
    return 'registered_vote_%s_%s' % (user._id, item._fullname)

def get_likes(user, items):
    if not user or not items:
        return {}

    res = {}

    # check the prequeued_vote_keys
    keys = {}
    for item in items:
        if (user, item) in res:
            continue

        key = prequeued_vote_key(user, item)
        keys[key] = (user, item)
    if keys:
        r = g.cache.get_multi(keys.keys())
        for key, v in r.iteritems():
            res[keys[key]] = (True if v == '1'
                              else False if v == '-1'
                              else None)

    for item in items:
        # already retrieved above
        if (user, item) in res:
            continue

        # we can only vote on links and comments
        if not isinstance(item, (Link, Comment)):
            res[(user, item)] = None

    likes = Vote.likes(user, [i for i in items if (user, i) not in res])

    res.update(likes)

    return res

def handle_vote(user, thing, dir, ip, vote_info,
                cheater=False, foreground=False, timer=None, date=None):
    if timer is None:
        timer = SimpleSillyStub()

    from r2.lib.db import tdb_sql
    from sqlalchemy.exc import IntegrityError
    try:
        v = Vote.vote(user, thing, dir, ip, vote_info=vote_info,
                      cheater=cheater, timer=timer, date=date)
    except (tdb_sql.CreationError, IntegrityError):
        g.log.error("duplicate vote for: %s" % str((user, thing, dir)))
        return

    new_vote(v, foreground=foreground, timer=timer)

    timestamps = []
    if isinstance(thing, Link):

        #update the modified flags
        if user._id == thing.author_id:
            timestamps.append('Overview')
            timestamps.append('Submitted')
            #update sup listings
            sup.add_update(user, 'submitted')

            #update sup listings
            if dir:
                sup.add_update(user, 'liked')
            elif dir is False:
                sup.add_update(user, 'disliked')

    elif isinstance(thing, Comment):
        #update last modified
        if user._id == thing.author_id:
            timestamps.append('Overview')
            timestamps.append('Commented')
            #update sup listings
            sup.add_update(user, 'commented')

    else:
        raise NotImplementedError

    timer.intermediate("sup")

    for timestamp in timestamps:
        set_last_modified(user, timestamp.lower())
    LastModified.touch(user._fullname, timestamps)
    timer.intermediate("last_modified")


def process_votes(qname, limit=0):
    # limit is taken but ignored for backwards compatibility
    stats_qname = qname
    if stats_qname.startswith("vote_link"):
        stats_qname = "vote_link_q"

    @g.stats.amqp_processor(stats_qname)
    def _handle_vote(msg):
        timer = stats.get_timer("service_time." + stats_qname)
        timer.start()

        #assert(len(msgs) == 1)
        r = pickle.loads(msg.body)

        uid, tid, dir, ip, vote_info, cheater = r
        voter = Account._byID(uid, data=True)
        votee = Thing._by_fullname(tid, data = True)
        timer.intermediate("preamble")

        # Convert the naive timestamp we got from amqplib to a
        # timezone aware one.
        tt = mktime(msg.timestamp.timetuple())
        date = datetime.utcfromtimestamp(tt).replace(tzinfo=pytz.UTC)

        # I don't know how, but somebody is sneaking in votes
        # for subreddits
        if isinstance(votee, (Link, Comment)):
            print (voter, votee, dir, ip, vote_info, cheater)
            handle_vote(voter, votee, dir, ip, vote_info,
                        cheater = cheater, foreground=True, timer=timer,
                        date=date)

        if isinstance(votee, Comment):
            update_comment_votes([votee])
            timer.intermediate("update_comment_votes")

        stats.simple_event('vote.total')
        if cheater:
            stats.simple_event('vote.cheater')
        timer.flush()

    amqp.consume_items(qname, _handle_vote, verbose = False)

########NEW FILE########
__FILENAME__ = sorts
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.db._sorts import epoch_seconds, score, hot, _hot
from r2.lib.db._sorts import controversy, confidence

########NEW FILE########
__FILENAME__ = tdb_cassandra
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import json
import inspect
import pytz
from datetime import datetime
from socket import gethostbyaddr

from pylons import g

from pycassa import ColumnFamily
from pycassa.pool import MaximumRetryException
from pycassa.cassandra.ttypes import ConsistencyLevel, NotFoundException
from pycassa.system_manager import (SystemManager, UTF8_TYPE,
                                    COUNTER_COLUMN_TYPE, TIME_UUID_TYPE,
                                    ASCII_TYPE)
from pycassa.types import DateType, LongType, IntegerType
from r2.lib.utils import tup, Storage
from r2.lib import cache
from uuid import uuid1, UUID
from itertools import chain
import cPickle as pickle
from pycassa.util import OrderedDict
import base64

connection_pools = g.cassandra_pools
default_connection_pool = g.cassandra_default_pool

keyspace = 'reddit'
thing_cache = g.thing_cache
disallow_db_writes = g.disallow_db_writes
tz = g.tz
log = g.log
read_consistency_level = g.cassandra_rcl
write_consistency_level = g.cassandra_wcl
debug = g.debug
make_lock = g.make_lock
db_create_tables = g.db_create_tables

thing_types = {}

TRANSIENT_EXCEPTIONS = (MaximumRetryException,)

# The available consistency levels
CL = Storage(ANY    = ConsistencyLevel.ANY,
             ONE    = ConsistencyLevel.ONE,
             QUORUM = ConsistencyLevel.QUORUM,
             ALL    = ConsistencyLevel.ALL)

# the greatest number of columns that we're willing to accept over the
# wire for a given row (this should be increased if we start working
# with classes with lots of columns, like Account which has lots of
# karma_ rows, or we should not do that)
max_column_count = 10000

# the pycassa date serializer, for use when we can't set the right metadata
# to get pycassa to serialize dates for us
date_serializer = DateType()

class CassandraException(Exception):
    """Base class for Exceptions in tdb_cassandra"""
    pass

class InvariantException(CassandraException):
    """Exceptions that can only be caused by bugs in tdb_cassandra"""
    pass

class ConfigurationException(CassandraException):
    """Exceptions that are caused by incorrect configuration on the
       Cassandra server"""
    pass

class TdbException(CassandraException):
    """Exceptions caused by bugs in our callers or subclasses"""
    pass

class NotFound(CassandraException, NotFoundException):
    """Someone asked us for an ID that isn't stored in the DB at
       all. This is probably an end-user's fault."""
    pass

def will_write(fn):
    """Decorator to indicate that a given function intends to write
       out to Cassandra"""
    def _fn(*a, **kw):
        if disallow_db_writes:
            raise CassandraException("Not so fast! DB writes have been disabled")
        return fn(*a, **kw)
    return _fn

def get_manager(seeds):
    # n.b. does not retry against multiple servers
    server = seeds[0]
    return SystemManager(server)

class ThingMeta(type):
    def __init__(cls, name, bases, dct):
        type.__init__(cls, name, bases, dct)

        if hasattr(cls, '_ttl') and hasattr(cls._ttl, 'total_seconds'):
            cls._ttl = cls._ttl.total_seconds()

        if cls._use_db:
            if cls._type_prefix is None:
                # default to the class name
                cls._type_prefix = name

            if '_' in cls._type_prefix:
                raise TdbException("Cannot have _ in type prefix %r (for %r)"
                                   % (cls._type_prefix, name))

            if cls._type_prefix in thing_types:
                raise InvariantException("Redefining type %r?" % (cls._type_prefix))

            # if we weren't given a specific _cf_name, we can use the
            # classes's name
            cf_name = cls._cf_name or name

            thing_types[cls._type_prefix] = cls

            if not getattr(cls, "_read_consistency_level", None):
                cls._read_consistency_level = read_consistency_level
            if not getattr(cls, "_write_consistency_level", None):
                cls._write_consistency_level = write_consistency_level

            pool_name = getattr(cls, "_connection_pool", default_connection_pool)
            connection_pool = connection_pools[pool_name]
            cassandra_seeds = connection_pool.server_list

            try:
                cls._cf = ColumnFamily(connection_pool,
                                       cf_name,
                                       read_consistency_level = cls._read_consistency_level,
                                       write_consistency_level = cls._write_consistency_level)
            except NotFoundException:
                if not db_create_tables:
                    raise

                manager = get_manager(cassandra_seeds)

                # allow subclasses to add creation args or override base class ones
                extra_creation_arguments = {}
                for c in reversed(inspect.getmro(cls)):
                    creation_args = getattr(c, "_extra_schema_creation_args", {})
                    extra_creation_arguments.update(creation_args)

                log.warning("Creating Cassandra Column Family %s" % (cf_name,))
                with make_lock("cassandra_schema", 'cassandra_schema'):
                    manager.create_column_family(keyspace, cf_name,
                                                 comparator_type = cls._compare_with,
                                                 super=getattr(cls, '_super', False),
                                                 **extra_creation_arguments
                                                 )
                log.warning("Created Cassandra Column Family %s" % (cf_name,))

                # try again to look it up
                cls._cf = ColumnFamily(connection_pool,
                                       cf_name,
                                       read_consistency_level = cls._read_consistency_level,
                                       write_consistency_level = cls._write_consistency_level)

        cls._kind = name

    def __repr__(cls):
        return '<thing: %s>' % cls.__name__

class Counter(object):
    __metaclass__ = ThingMeta

    _use_db = False
    _connection_pool = 'main'
    _extra_schema_creation_args = {
        'default_validation_class': COUNTER_COLUMN_TYPE,
        'replicate_on_write': True
    }

    _type_prefix = None
    _cf_name = None
    _compare_with = UTF8_TYPE

    @classmethod
    def _byID(cls, key):
        return cls._cf.get(key)

    @classmethod
    @will_write
    def _incr(cls, key, column, delta=1, super_column=None):
        cls._cf.add(key, column, delta, super_column)

    @classmethod
    @will_write
    def _incr_multi(cls, key, data):
        with cls._cf.batch() as b:
            b.insert(key, data)


class ThingBase(object):
    # base class for Thing

    __metaclass__ = ThingMeta

    _cf_name = None # the name of the ColumnFamily; defaults to the
                    # name of the class

    # subclasses must replace these

    _type_prefix = None # this must be present for classes with _use_db==True

    _use_db = False

    # the Cassandra column-comparator (internally orders column
    # names). In real life you can't change this without some changes
    # to tdb_cassandra to support other attr types
    _compare_with = UTF8_TYPE

    _value_type = None # if set, overrides all of the _props types
                       # below. Used for Views. One of 'int', 'float',
                       # 'bool', 'pickle', 'date', 'bytes', 'str'

    _int_props = ()
    _float_props = () # note that we can lose resolution on these
    _bool_props = ()
    _pickle_props = ()
    _date_props = () # note that we can lose resolution on these
    _bytes_props = ()
    _str_props = () # at present we never actually read out of here
                    # since it's the default if none of the previous
                    # matches

    # the value that we assume a property to have if it is not found
    # in the DB. Note that we don't do type-checking here, so if you
    # want a default to be a boolean and want it to be storable you'll
    # also have to set it in _bool_props
    _defaults = {}

    # The default TTL in seconds to add to all columns. Note: if an
    # entire object is expected to have a TTL, it should be considered
    # immutable! (You don't want to write out an object with an author
    # and date, then go update author or add a new column, then have
    # the original columns expire. Then when you go to look it up, the
    # inherent properties author and/or date will be gone and only the
    # updated columns will be present.) This is an expected convention
    # and is not enforced.
    _ttl = None
    _warn_on_partial_ttl = True

    # A per-class dictionary of default TTLs that new columns of this
    # class should have
    _default_ttls = {}

    # A per-instance property defining the TTL of individual columns
    # (that must also appear in self._dirties)
    _column_ttls = {}

    # a timestamp property that will automatically be added to newly
    # created Things (disable by setting to None)
    _timestamp_prop = None

    # a per-instance property indicating that this object was
    # partially loaded: i.e. only some properties were requested from
    # the DB
    _partial = None

    # a per-instance property that specifies that the columns backing
    # these attributes are to be removed on _commit()
    _deletes = set()

    # thrift will materialize the entire result set for a slice range
    # in memory, meaning that we need to limit the maximum number of columns
    # we receive in a single get to avoid hurting the server. if this
    # value is true, we will make sure to do extra gets to retrieve all of
    # the columns in a row when there are more than the per-call maximum.
    _fetch_all_columns = False

    def __init__(self, _id = None, _committed = False, _partial = None, **kw):
        # things that have changed
        self._dirties = kw.copy()

        # what the original properties were when we went to Cassandra to
        # get them
        self._orig = {}

        self._defaults = self._defaults.copy()

        # whether this item has ever been created
        self._committed = _committed

        self._partial = None if _partial is None else frozenset(_partial)

        self._deletes = set()
        self._column_ttls = {}

        # our row key
        self._id = _id

        if not self._use_db:
            raise TdbException("Cannot make instances of %r" % (self.__class__,))

    @classmethod
    def _byID(cls, ids, return_dict=True, properties=None):
        ids, is_single = tup(ids, True)

        if properties is not None:
            asked_properties = frozenset(properties)
            willask_properties = set(properties)

        if not len(ids):
            if is_single:
                raise InvariantException("whastis?")
            return {}

        # all keys must be strings or directly convertable to strings
        assert all(isinstance(_id, basestring) or str(_id) for _id in ids)

        def reject_bad_partials(cached, still_need):
            # tell sgm that the match it found in the cache isn't good
            # enough if it's a partial that doesn't include our
            # properties. we still need to look those items up to get
            # the properties that we're after
            stillfind = set()

            for k, v in cached.iteritems():
                if properties is None:
                    if v._partial is not None:
                        # there's a partial in the cache but we're not
                        # looking for partials
                        stillfind.add(k)
                elif v._partial is not None and not asked_properties.issubset(v._partial):
                    # we asked for a partial, and this is a partial,
                    # but it doesn't have all of the properties that
                    # we need
                    stillfind.add(k)

                    # other callers in our request are now expecting
                    # to find the properties that were on that
                    # partial, so we'll have to preserve them
                    for prop in v._partial:
                        willask_properties.add(prop)

            for k in stillfind:
                del cached[k]
                still_need.add(k)

        def lookup(l_ids):
            if properties is None:
                rows = cls._cf.multiget(l_ids, column_count=max_column_count)

                # if we got max_column_count columns back for a row, it was
                # probably clipped. in this case, we should fetch the remaining
                # columns for that row and add them to the result.
                if cls._fetch_all_columns:
                    for key, row in rows.iteritems():
                        if len(row) == max_column_count:
                            last_column_seen = next(reversed(row))
                            cols = cls._cf.xget(key,
                                                column_start=last_column_seen,
                                                buffer_size=max_column_count)
                            row.update(cols)
            else:
                rows = cls._cf.multiget(l_ids, columns = willask_properties)

            l_ret = {}
            for t_id, row in rows.iteritems():
                t = cls._from_serialized_columns(t_id, row)
                if properties is not None:
                    # make sure that the item is marked as a _partial
                    t._partial = willask_properties
                l_ret[t._id] = t

            return l_ret

        ret = cache.sgm(thing_cache, ids, lookup, prefix=cls._cache_prefix(),
                        found_fn=reject_bad_partials)

        if is_single and not ret:
            raise NotFound("<%s %r>" % (cls.__name__,
                                        ids[0]))
        elif is_single:
            assert len(ret) == 1
            return ret.values()[0]
        elif return_dict:
            return ret
        else:
            return filter(None, (ret.get(i) for i in ids))

    @property
    def _fullname(self):
        if self._type_prefix is None:
            raise TdbException("%r has no _type_prefix, so fullnames cannot be generated"
                               % self.__class__)

        return '%s_%s' % (self._type_prefix, self._id)

    @classmethod
    def _by_fullname(cls, fnames, return_dict=True, ignore_missing=False):
        if ignore_missing:
            raise NotImplementedError
        ids, is_single = tup(fnames, True)

        by_cls = {}
        for i in ids:
            typ, underscore, _id = i.partition('_')
            assert underscore == '_'

            by_cls.setdefault(thing_types[typ], []).append(_id)

        items = []
        for typ, ids in by_cls.iteritems():
            items.extend(typ._byID(ids).values())

        if is_single:
            return items[0]
        elif return_dict:
            return dict((x._fullname, x) for x in items)
        else:
            d = dict((x._fullname, x) for x in items)
            return [d[fullname] for fullname in fnames]

    @classmethod
    def _cache_prefix(cls):
        return 'tdbcassandra_' + cls._type_prefix + '_'

    def _cache_key(self):
        if not self._id:
            raise TdbException('no cache key for uncommitted %r' % (self,))

        return self._cache_key_id(self._id)

    @classmethod
    def _cache_key_id(cls, t_id):
        return cls._cache_prefix() + t_id

    @classmethod
    def _wcl(cls, wcl, default = None):
        if wcl is not None:
            return wcl
        elif default is not None:
            return default
        return cls._write_consistency_level

    def _rcl(cls, rcl, default = None):
        if rcl is not None:
            return rcl
        elif default is not None:
            return default
        return cls._read_consistency_level

    @classmethod
    def _get_column_validator(cls, colname):
        return cls._cf.column_validators.get(colname,
                                             cls._cf.default_validation_class)

    @classmethod
    def _deserialize_column(cls, attr, val):
        if attr in cls._int_props or (cls._value_type and cls._value_type == 'int'):
            try:
                return int(val)
            except ValueError:
                return long(val)
        elif attr in cls._float_props or (cls._value_type and cls._value_type == 'float'):
            return float(val)
        elif attr in cls._bool_props or (cls._value_type and cls._value_type == 'bool'):
            # note that only the string "1" is considered true!
            return val == '1'
        elif attr in cls._pickle_props or (cls._value_type and cls._value_type == 'pickle'):
            return pickle.loads(val)
        elif attr in cls._date_props or attr == cls._timestamp_prop or (cls._value_type and cls._value_type == 'date'):
            return cls._deserialize_date(val)
        elif attr in cls._bytes_props or (cls._value_type and cls._value_type == 'bytes'):
            return val

        # otherwise we'll assume that it's a utf-8 string
        return val if isinstance(val, unicode) else val.decode('utf-8')

    @classmethod
    def _serialize_column(cls, attr, val):
        if (attr in chain(cls._int_props, cls._float_props) or
            (cls._value_type and cls._value_type in ('float', 'int'))):
            return str(val)
        elif attr in cls._bool_props or (cls._value_type and cls._value_type == 'bool'):
            # n.b. we "truncate" this to a boolean, so truthy but
            # non-boolean values are discarded
            return '1' if val else '0'
        elif attr in cls._pickle_props or (cls._value_type and cls._value_type == 'pickle'):
            return pickle.dumps(val)
        elif (attr in cls._date_props or attr == cls._timestamp_prop or
              (cls._value_type and cls._value_type == 'date')):
            # the _timestamp_prop is handled in _commit(), not here
            if cls._get_column_validator(attr) == 'DateType':
                # pycassa will take it from here
                return val
            else:
                return cls._serialize_date(val)
        elif attr in cls._bytes_props or (cls._value_type and cls._value_type == 'bytes'):
            return val

        return unicode(val).encode('utf-8')

    @classmethod
    def _serialize_date(cls, date):
        return date_serializer.pack(date)

    @classmethod
    def _deserialize_date(cls, val):
        if isinstance(val, datetime):
            date = val
        elif len(val) == 8: # cassandra uses 8-byte integer format for this
            date = date_serializer.unpack(val)
        else: # it's probably the old-style stringified seconds since epoch
            as_float = float(val)
            date = datetime.utcfromtimestamp(as_float)

        return date.replace(tzinfo=pytz.utc)

    @classmethod
    def _from_serialized_columns(cls, t_id, columns):
        d_columns = dict((attr, cls._deserialize_column(attr, val))
                         for (attr, val)
                         in columns.iteritems())
        return cls._from_columns(t_id, d_columns)

    @classmethod
    def _from_columns(cls, t_id, columns):
        """Given a dictionary of freshly deserialized columns
           construct an instance of cls"""
        t = cls()
        t._orig = columns
        t._id = t_id
        t._committed = True
        return t

    @property
    def _dirty(self):
        return len(self._dirties) or len(self._deletes) or not self._committed

    @will_write
    def _commit(self, write_consistency_level = None):
        if not self._dirty:
            return

        if self._id is None:
            raise TdbException("Can't commit %r without an ID" % (self,))

        if self._committed and self._ttl and self._warn_on_partial_ttl:
            log.warning("Using a full-TTL object %r in a mutable fashion"
                        % (self,))

        if not self._committed:
            # if this has never been committed we should also consider
            # the _orig columns as dirty (but "less dirty" than the
            # _dirties)
            upd = self._orig.copy()
            self._orig.clear()
            upd.update(self._dirties)
            self._dirties = upd

        # Cassandra values are untyped byte arrays, so we need to
        # serialize everything while filtering out anything that's
        # been dirtied but doesn't actually differ from what's already
        # in the DB
        updates = dict((attr, self._serialize_column(attr, val))
                       for (attr, val)
                       in self._dirties.iteritems()
                       if (attr not in self._orig or
                           val != self._orig[attr]))

        # n.b. deleted columns are applied *after* the updates. our
        # __setattr__/__delitem__ tries to make sure that this always
        # works

        if not self._committed and self._timestamp_prop and self._timestamp_prop not in updates:
            # auto-create timestamps on classes that request them

            # this serialize/deserialize is a bit funny: the process
            # of storing and retrieving causes us to lose some
            # resolution because of the floating-point representation,
            # so this is just to make sure that we have the same value
            # that the DB does after writing it out. Note that this is
            # the only property munged this way: other timestamp and
            # floating point properties may lose resolution
            s_now = self._serialize_date(datetime.now(tz))
            now = self._deserialize_date(s_now)

            timestamp_is_typed = self._get_column_validator(self._timestamp_prop) == "DateType"
            updates[self._timestamp_prop] = now if timestamp_is_typed else s_now
            self._dirties[self._timestamp_prop] = now

        if not updates and not self._deletes:
            self._dirties.clear()
            return

        # actually write out the changes to the CF
        wcl = self._wcl(write_consistency_level)
        with self._cf.batch(write_consistency_level = wcl) as b:
            if updates:
                for k, v in updates.iteritems():
                    b.insert(self._id,
                             {k: v},
                             ttl=self._column_ttls.get(k, self._ttl))
            if self._deletes:
                b.remove(self._id, self._deletes)

        self._orig.update(self._dirties)
        self._column_ttls.clear()
        self._dirties.clear()
        for k in self._deletes:
            try:
                del self._orig[k]
            except KeyError:
                pass
        self._deletes.clear()

        if not self._committed:
            self._on_create()
        else:
            self._on_commit()

        self._committed = True

        thing_cache.set(self._cache_key(), self)

    def _revert(self):
        if not self._committed:
            raise TdbException("Revert to what?")

        self._dirties.clear()
        self._deletes.clear()
        self._column_ttls.clear()

    def _destroy(self):
        self._cf.remove(self._id,
                        write_consistency_level=self._write_consistency_level)

    def __getattr__(self, attr):
        if isinstance(attr, basestring) and attr.startswith('_'):
            # TODO: I bet this interferes with Views whose column names can
            # start with a _
            try:
                return self.__dict__[attr]
            except KeyError:
                raise AttributeError, attr

        if attr in self._deletes:
            raise AttributeError("%r has no %r because you deleted it", (self, attr))
        elif attr in self._dirties:
            return self._dirties[attr]
        elif attr in self._orig:
            return self._orig[attr]
        elif attr in self._defaults:
            return self._defaults[attr]
        elif self._partial is not None and attr not in self._partial:
            raise AttributeError("%r has no %r but you didn't request it" % (self, attr))
        else:
            raise AttributeError('%r has no %r' % (self, attr))

    def __setattr__(self, attr, val):
        if attr == '_id' and self._committed:
            raise ValueError('cannot change _id on a committed %r' % (self.__class__))

        if isinstance(attr, basestring) and attr.startswith('_'):
            # TODO: I bet this interferes with Views whose column names can
            # start with a _
            return object.__setattr__(self, attr, val)

        try:
            self._deletes.remove(attr)
        except KeyError:
            pass
        self._dirties[attr] = val
        if attr in self._default_ttls:
            self._column_ttls[attr] = self._default_ttls[attr]

    def __eq__(self, other):
        if self.__class__ != other.__class__:
            return False

        if self._partial or other._partial and self._partial != other._partial:
            raise ValueError("Can't compare incompatible partials")

        return self._id == other._id and self._t == other._t

    def __ne__(self, other):
        return not (self == other)

    @property
    def _t(self):
        """Emulate the _t property from tdb_sql: a dictionary of all
           values that are or will be stored in the database, (not
           including _defaults or unrequested properties on
           partials)"""
        ret = self._orig.copy()
        ret.update(self._dirties)
        for k in self._deletes:
            try:
                del ret[k]
            except KeyError:
                pass
        return ret

    # allow the dictionary mutation syntax; it makes working some some
    # keys a bit easier. Go through our regular
    # __getattr__/__setattr__ functions where all of the appropriate
    # work is done
    def __getitem__(self, key):
        return self.__getattr__(key)

    def __setitem__(self, key, value):
        return self.__setattr__(key, value)

    def __delitem__(self, key):
        try:
            del self._dirties[key]
        except KeyError:
            pass
        try:
            del self._column_ttls[key]
        except KeyError:
            pass
        self._deletes.add(key)

    def _get(self, key, default = None):
        try:
            return self.__getattr__(key)
        except AttributeError:
            if self._partial is not None and key not in self._partial:
                raise AttributeError("_get on unrequested key from partial")
            return default

    def _set_ttl(self, key, ttl):
        assert key in self._dirties
        assert isinstance(ttl, (long, int))
        self._column_ttls[key] = ttl

    def _on_create(self):
        """A hook executed on creation, good for creation of static
           Views. Subclasses should call their parents' hook(s) as
           well"""
        pass

    def _on_commit(self):
        """Executed on _commit other than creation."""
        pass

    @classmethod
    def _all(cls):
        # returns a query object yielding every single item in a
        # column family. it probably shouldn't be used except in
        # debugging
        return Query(cls, limit=None)

    def __repr__(self):
        # it's safe for subclasses to override this to e.g. put a Link
        # title or Account name in the repr(), but they must be
        # careful to check hasattr for the properties that they read
        # out, as __getattr__ itself may call __repr__ in constructing
        # its error messages
        id_str = self._id
        comm_str = '' if self._committed else ' (uncommitted)'
        part_str = '' if self._partial is None else ' (partial)'
        return "<%s %r%s%s>" % (self.__class__.__name__,
                              id_str,
                              comm_str, part_str)

    if debug:
        # we only want this with g.debug because overriding __del__ can play
        # hell with memory leaks
        def __del__(self):
            if not self._committed:
                # normally we'd log this with g.log or something, but we can't
                # guarantee that the thread destructing us has access to g
                print "Warning: discarding uncomitted %r; this is usually a bug" % (self,)
            elif self._dirty:
                print ("Warning: discarding dirty %r; this is usually a bug (_dirties=%r, _deletes=%r)"
                       % (self,self._dirties,self._deletes))

class Thing(ThingBase):
    _timestamp_prop = 'date'

    # alias _date property for consistency with tdb_sql things.
    @property
    def _date(self):
        return self.date

class UuidThing(ThingBase):
    _timestamp_prop = 'date'
    _extra_schema_creation_args = {
        'key_validation_class': TIME_UUID_TYPE
    }

    def __init__(self, **kw):
        ThingBase.__init__(self, _id=uuid1(), **kw)

    @classmethod
    def _byID(cls, ids, **kw):
        ids, is_single = tup(ids, ret_is_single=True)

        #Convert string ids to UUIDs before retrieving
        uuids = [UUID(id) if not isinstance(id, UUID) else id for id in ids]

        if len(uuids) == 0:
            return {}
        elif is_single:
            assert len(uuids) == 1
            uuids = uuids[0]

        return super(UuidThing, cls)._byID(uuids, **kw)

    @classmethod
    def _cache_key_id(cls, t_id):
        return cls._cache_prefix() + str(t_id)


def view_of(cls):
    """Register a class as a view of a Thing.

    When a Thing is created or destroyed the appropriate View method must be
    called to update the View. This can be done using Thing._on_create() for
    general Thing classes or create()/destroy() for DenormalizedRelation
    classes.

    """
    def view_of_decorator(view_cls):
        cls._views.append(view_cls)
        view_cls._view_of = cls
        return view_cls
    return view_of_decorator



class DenormalizedRelation(object):
    """A model of many-to-many relationships, indexed by thing1.

    Each thing1 is represented by a row. The relationships from that thing1 to
    a number of thing2s are represented by columns in that row. To query if
    relationships exist and what its value is ("name" in the PG model), we
    fetch the thing1's row, telling C* we're only interested in the columns
    representing the thing2s we are interested in. This allows negative lookups
    to be very fast because of the row-level bloom filter.

    This data model will generate VERY wide rows. Any column family based on
    it should have its row cache disabled.

    """
    __metaclass__ = ThingMeta
    _use_db = False
    _cf_name = None
    _compare_with = ASCII_TYPE
    _type_prefix = None
    _last_modified_name = None
    _write_last_modified = True
    _extra_schema_creation_args = dict(key_validation_class=ASCII_TYPE,
                                       default_validation_class=UTF8_TYPE)

    @classmethod
    def value_for(cls, thing1, thing2, **kw):
        """Return a value to store for a relationship between thing1/thing2."""
        raise NotImplementedError()

    @classmethod
    def create(cls, thing1, thing2s, **kw):
        """Create a relationship between thing1 and thing2s.

        If there are any other views of this data, they will be updated as
        well.

        Takes kwargs which can be used by views
        or value_for to get additional information.

        """
        thing2s = tup(thing2s)
        values = {thing2._id36 : cls.value_for(thing1, thing2, **kw)
                  for thing2 in thing2s}
        cls._cf.insert(thing1._id36, values)

        for view in cls._views:
            view.create(thing1, thing2s, **kw)

        if cls._write_last_modified:
            from r2.models.last_modified import LastModified
            LastModified.touch(thing1._fullname, cls._last_modified_name)

    @classmethod
    def destroy(cls, thing1, thing2s):
        """Destroy relationships between thing1 and some thing2s."""
        thing2s = tup(thing2s)
        cls._cf.remove(thing1._id36, (thing2._id36 for thing2 in thing2s))

        for view in cls._views:
            view.destroy(thing1, thing2s)

    @classmethod
    def fast_query(cls, thing1, thing2s):
        """Find relationships between thing1 and various thing2s."""
        thing2s, thing2s_is_single = tup(thing2s, ret_is_single=True)

        if not thing1:
            return {}

        # don't bother looking up relationships for items that were created
        # since the last time the thing1 created a relationship of this type
        if cls._last_modified_name:
            from r2.models.last_modified import LastModified
            timestamp = LastModified.get(thing1._fullname,
                                         cls._last_modified_name)
            if timestamp:
                thing2s = [thing2 for thing2 in thing2s
                           if thing2._date <= timestamp]
            else:
                thing2s = []

        if not thing2s:
            return {}

        # fetch the row from cassandra. if it doesn't exist, thing1 has no
        # relation of this type to any thing2!
        try:
            columns = [thing2._id36 for thing2 in thing2s]
            results = cls._cf.get(thing1._id36, columns)
        except NotFoundException:
            results = {}

        # return the data in the expected format
        if not thing2s_is_single:
            # {(thing1, thing2) : value}
            thing2s_by_id = {thing2._id36 : thing2 for thing2 in thing2s}
            return {(thing1, thing2s_by_id[k]) : v
                    for k, v in results.iteritems()}
        else:
            if results:
                assert len(results) == 1
                return results.values()[0]
            else:
                raise NotFound("<%s %r>" % (cls.__name__, (thing1._id36,
                                                           thing2._id36)))


class ColumnQuery(object):
    """
    A query across a row of a CF.
    """
    _chunk_size = 100

    def __init__(self, cls, rowkeys, column_start="", column_finish="",
                 column_count=100, column_reversed=True,
                 column_to_obj=None,
                 obj_to_column=None):
        self.cls = cls
        self.rowkeys = rowkeys
        self.column_start = column_start
        self.column_finish = column_finish
        self._limit = column_count
        self.column_reversed = column_reversed
        self.column_to_obj = column_to_obj or self.default_column_to_obj
        self.obj_to_column = obj_to_column or self.default_obj_to_column
        self._rules = []    # dummy parameter to mimic tdb_sql queries

        # Sorting for TimeUuid objects
        if self.cls._compare_with == TIME_UUID_TYPE:
            def sort_key(i):
                return i.time
        else:
            def sort_key(i):
                return i
        self.sort_key = sort_key

    @staticmethod
    def combine(queries):
        raise NotImplementedError

    @staticmethod
    def default_column_to_obj(columns):
        """
        Mapping from column --> object.

        This default doesn't actually return the underlying object but we don't
        know how to do that without more information.
        """
        return columns

    @staticmethod
    def default_obj_to_column(objs):
        """
        Mapping from object --> column
        """
        objs, is_single = tup(objs, ret_is_single=True)
        columns = [{obj._id: obj._id} for obj in objs]

        if is_single:
            return columns[0]
        else:
            return columns

    def _after(self, thing):
        if thing:
            column_name = self.obj_to_column(thing).keys()[0]
            self.column_start = column_name
        else:
            self.column_start = ""

    def _after_id(self, column_name):
        self.column_start = column_name

    def _reverse(self):
        # Logic of standard reddit query is opposite of cassandra
        self.column_reversed = False

    def __iter__(self, yield_column_names=False):
        retrieved = 0
        column_start = self.column_start
        while retrieved < self._limit:
            try:
                column_count = min(self._chunk_size, self._limit - retrieved)
                if column_start:
                    column_count += 1   # cassandra includes column_start
                r = self.cls._cf.multiget(self.rowkeys,
                                          column_start=column_start,
                                          column_finish=self.column_finish,
                                          column_count=column_count,
                                          column_reversed=self.column_reversed)

                # multiget returns OrderedDict {rowkey: {column_name: column_value}}
                # combine into single OrderedDict of {column_name: column_value}
                nrows = len(r.keys())
                if nrows == 0:
                    return
                elif nrows == 1:
                    columns = r.values()[0]
                else:
                    r_combined = {}
                    for d in r.values():
                        r_combined.update(d)
                    columns = OrderedDict(sorted(r_combined.items(),
                                                 key=lambda t: self.sort_key(t[0]),
                                                 reverse=self.column_reversed))
            except NotFoundException:
                return

            retrieved += self._chunk_size

            if column_start:
                try:
                    del columns[column_start]
                except KeyError:
                    columns.popitem(last=True)  # remove extra column

            if not columns:
                return

            # Convert to list of columns
            l_columns = [{col_name: columns[col_name]} for col_name in columns]

            column_start = l_columns[-1].keys()[0]
            objs = self.column_to_obj(l_columns)

            if yield_column_names:
                column_names = [column.keys()[0] for column in l_columns]
                if len(column_names) == 1:
                    ret = (column_names[0], objs),
                else:
                    ret = zip(column_names, objs)
            else:
                ret = objs

            ret, is_single = tup(ret, ret_is_single=True)
            for r in ret:
                yield r

    def __repr__(self):
        return "<%s(%s-%r)>" % (self.__class__.__name__, self.cls.__name__,
                                self.rowkeys)

class MultiColumnQuery(object):
    def __init__(self, queries, num, sort_key=None):
        self.num = num
        self._queries = queries
        self.sort_key = sort_key    # python doesn't sort UUID1's correctly, need to pass in a sorter
        self._rules = []            # dummy parameter to mimic tdb_sql queries

    def _after(self, thing):
        for q in self._queries:
            q._after(thing)

    def _reverse(self):
        for q in self._queries:
            q._reverse()

    def __setattr__(self, attr, val):
        # Catch _limit to set on all queries
        if attr == '_limit':
             for q in self._queries:
                 q._limit = val
        else:
            object.__setattr__(self, attr, val)

    def __iter__(self):

        if self.sort_key:
            def sort_key(tup):
                # Need to point the supplied sort key at the correct item in
                # the (sortable, item, generator) tuple
                return self.sort_key(tup[0])
        else:
            def sort_key(tup):
                return tup[0]

        top_items = []
        for q in self._queries:
            try:
                gen = q.__iter__(yield_column_names=True)
                column_name, item = gen.next()
                top_items.append((column_name, item, gen))
            except StopIteration:
                pass
        top_items.sort(key=sort_key)

        def _update(top_items):
            # Remove the first item from combined query and update the list
            head = top_items.pop(0)
            item = head[1]
            gen = head[2]

            # Try to get a new item from the query that gave us the current one
            try:
                column_name, item = gen.next()
                top_items.append((column_name, item, gen)) # if multiple queues have the same item value the sort is somewhat undefined
                top_items.sort(key=sort_key)
            except StopIteration:
                pass

        num_ret = 0
        while top_items and num_ret < self.num:
            yield top_items[0][1]
            _update(top_items)
            num_ret += 1

class Query(object):
    """A query across a CF. Note that while you can query rows from a
       CF that has a RandomPartitioner, you won't get them in any sort
       of order"""
    def __init__(self, cls, after=None, properties=None, limit=100,
                 chunk_size=100, _max_column_count = max_column_count):
        self.cls = cls
        self.after = after
        self.properties = properties
        self.limit = limit
        self.chunk_size = chunk_size
        self.max_column_count = _max_column_count

    def __copy__(self):
        return Query(self.cls, after=self.after,
                     properties = self.properties,
                     limit=self.limit,
                     chunk_size=self.chunk_size,
                     _max_column_count = self.max_column_count)
    copy = __copy__

    def _dump(self):
        q = self.copy()
        q.after = q.limit = None

        for row in q:
            print row
            for col, val in row._t.iteritems():
                print '\t%s: %r' % (col, val)

    @will_write
    def _delete_all(self, write_consistency_level = None):
        # uncomment to use on purpose
        raise InvariantException("Nice try, FBI")

        # TODO: this could use cf.truncate instead and be *way*
        # faster, but it wouldn't flush the thing_cache at the same
        # time that way

        q = self.copy()
        q.after = q.limit = None

        # since we're just deleting it, we only need enough columns to
        # avoid reading ghost rows
        q.max_column_count = 1

        # I'm going to guess that if they are trying to flush out an
        # entire CF, they aren't that worried about how long it takes
        # to become consistent. So we'll default to the fastest way
        wcl = q.cls._wcl(write_consistency_level, CL.ONE)

        for row in q:
            print row

            # n.b. we're not calling _on_destroy!
            q.cls._cf.remove(row._id, write_consistency_level = wcl)
            thing_cache.delete(q.cls._cache_key_id(row._id))

    def __iter__(self):
        # n.b.: we aren't caching objects that we find this way in the
        # LocalCache. This may will need to be changed if we ever
        # start using OPP in Cassandra (since otherwise these types of
        # queries aren't useful for anything but debugging anyway)
        after = '' if self.after is None else self.after._id
        limit = self.limit

        if self.properties is None:
            r = self.cls._cf.get_range(start=after, row_count=limit,
                                       column_count = self.max_column_count)
        else:
            r = self.cls._cf.get_range(start=after, row_count=limit,
                                       columns = self.properties)

        for t_id, columns in r:
            if not columns:
                # a ghost row
                continue

            t = self.cls._from_serialized_columns(t_id, columns)
            yield t

class View(ThingBase):
    # Views are Things like any other, but may have special key
    # characteristics. Uses ColumnQuery for queries across a row.

    _timestamp_prop = None
    _value_type = 'str'

    _compare_with = UTF8_TYPE   # Type of the columns - should match _key_validation_class of _view_of class
    _view_of = None
    _write_consistency_level = CL.ONE   # Is this necessary?
    _query_cls = ColumnQuery

    @classmethod
    def _rowkey(cls, obj):
        """Mapping from _view_of object --> view rowkey. No default
        implementation is provided because this is the fundamental aspect of the
        view."""
        raise NotImplementedError

    @classmethod
    def _obj_to_column(cls, objs):
        """Mapping from _view_of object --> view column. Returns a
        single item dict {column name:column value} or list of dicts."""
        objs, is_single = tup(objs, ret_is_single=True)

        columns = [{obj._id: obj._id} for obj in objs]

        if len(columns) == 1:
            return columns[0]
        else:
            return columns

    @classmethod
    def _column_to_obj(cls, columns):
        """Mapping from view column --> _view_of object. Must be complement to
        _obj_to_column()."""
        columns, is_single = tup(columns, ret_is_single=True)

        ids = [column.keys()[0] for column in columns]

        if len(ids) == 1:
            ids = ids[0]
        return cls._view_of._byID(ids, return_dict=False)

    @classmethod
    def add_object(cls, obj, **kw):
        """Add a lookup to the view"""
        rowkey = cls._rowkey(obj)
        column = cls._obj_to_column(obj)
        cls._set_values(rowkey, column, **kw)

    @classmethod
    def query(cls, rowkeys, after=None, reverse=False, count=1000):
        """Return a query to get objects from the underlying _view_of class."""

        column_reversed = not reverse   # Reverse convention for cassandra is opposite

        q = cls._query_cls(cls, rowkeys, column_count=count,
                           column_reversed=column_reversed,
                           column_to_obj=cls._column_to_obj,
                           obj_to_column=cls._obj_to_column)
        q._after(after)
        return q

    def _values(self):
        """Retrieve the entire contents of the view"""
        # TODO: at present this only grabs max_column_count columns
        return self._t

    @classmethod
    def get_time_sorted_columns(cls, rowkey, limit=None):
        q = cls._cf.xget(rowkey, include_timestamp=True)
        r = sorted(q, key=lambda i: i[1][1]) # (col_name, (col_val, timestamp))
        if limit:
            r = r[:limit]
        return OrderedDict([(i[0], i[1][0]) for i in r])

    @classmethod
    @will_write
    def _set_values(cls, row_key, col_values,
                    write_consistency_level = None,
                    ttl=None):
        """Set a set of column values in a row of a view without
           looking up the whole row first"""
        # col_values =:= dict(col_name -> col_value)

        updates = dict((col_name, cls._serialize_column(col_name, col_val))
                       for (col_name, col_val) in col_values.iteritems())

        # if they didn't give us a TTL, use the default TTL for the
        # class. This will be further overwritten below per-column
        # based on the _default_ttls class dict. Note! There is no way
        # to use this API to express that you don't want a TTL if
        # there is a default set on either the row or the column
        default_ttl = ttl or cls._ttl

        with cls._cf.batch(write_consistency_level = cls._wcl(write_consistency_level)) as b:
            # with some quick tweaks we could have a version that
            # operates across multiple row keys, but this is not it
            for k, v in updates.iteritems():
                b.insert(row_key, {k: v},
                         ttl=cls._default_ttls.get(k, default_ttl))

        # can we be smarter here?
        thing_cache.delete(cls._cache_key_id(row_key))

    @classmethod
    @will_write
    def _remove(cls, key, columns):
        cls._cf.remove(key, columns)
        thing_cache.delete(cls._cache_key_id(key))

class DenormalizedView(View):
    """Store the entire underlying object inside the View column."""

    @classmethod
    def is_date_prop(cls, attr):
        view_cls = cls._view_of
        return (view_cls._value_type == 'date' or
                attr in view_cls._date_props or
                view_cls._timestamp_prop and attr == view_cls._timestamp_prop)

    @classmethod
    def _thing_dumper(cls, thing):
        serialize_fn = cls._view_of._serialize_column
        serialized_columns = dict((attr, serialize_fn(attr, val)) for
            (attr, val) in thing._orig.iteritems())

        # Encode date props which may be binary
        for attr, val in serialized_columns.items():
            if cls.is_date_prop(attr):
                serialized_columns[attr] = base64.b64encode(val)

        dump = json.dumps(serialized_columns)
        return dump

    @classmethod
    def _thing_loader(cls, _id, dump):
        serialized_columns = json.loads(dump)

        # Decode date props
        for attr, val in serialized_columns.items():
            if cls.is_date_prop(attr):
                serialized_columns[attr] = base64.b64decode(val)

        obj = cls._view_of._from_serialized_columns(_id, serialized_columns)
        return obj

    @classmethod
    def _obj_to_column(cls, objs):
        objs = tup(objs)
        columns = []
        for o in objs:
            _id = o._id
            dump = cls._thing_dumper(o)
            columns.append({_id: dump})

        if len(columns) == 1:
            return columns[0]
        else:
            return columns

    @classmethod
    def _column_to_obj(cls, columns):
        columns = tup(columns)
        objs = []
        for column in columns:
            _id, dump = column.items()[0]
            obj = cls._thing_loader(_id, dump)
            objs.append(obj)

        if len(objs) == 1:
            return objs[0]
        else:
            return objs

def schema_report():
    manager = get_manager()
    print manager.describe_keyspace(keyspace)

def ring_report():
    # uses a silly algorithm to pick natural endpoints that requires N>=RF+1

    sizes = {}
    nodes = {} # token -> node

    manager = get_manager()

    ring = manager.describe_ring(keyspace)
    ring.sort(key=lambda tr: long(tr.start_token))

    for x, tr in enumerate(ring):
        next = ring[x+1] if x < len(ring)-1 else ring[0]

        # tr = ring1[x]
        # next = ring1[x+1]

        s = long(tr.start_token)
        e = long(tr.end_token)

        if e > s:
            # a regular range
            l = e-s
        else:
            # range that overlaps 0
            l = e+2**127-s

        for ep in tr.endpoints:
            sizes.setdefault(ep, []).append(float(l)/2**127)

        natural = set(tr.endpoints) - set(next.endpoints)
        assert len(natural) == 1
        natural = natural.pop()

        nodes[e] = natural

    totalsize = sum(map(sum, sizes.values()))
    for token, ip in sorted(nodes.items(),
                              key = lambda x: x[0]):
        size = sizes[ip]
        name = gethostbyaddr(ip)[0]

        fmt_perc = lambda num: '%s%2.2f%%' % (' ' if num < 10 else '',
                                              num)
        maxtoklen = len(str(2**127))

        print '%16s\t%s\t%s\t%s' % (ip, name,
                                    fmt_perc(sum(size)/totalsize * 100),
                                    str(token).rjust(maxtoklen))

########NEW FILE########
__FILENAME__ = tdb_lite
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import sqlalchemy as sa
import cPickle as pickle

class tdb_lite(object):
    def __init__(self, gc):
        self.gc = gc

    def make_metadata(self, engine):
        metadata = sa.MetaData(engine)
        metadata.bind.echo = self.gc.sqlprinting
        return metadata

    def index_str(self, table, name, on, where = None):
        index_str = 'create index idx_%s_' % name
        index_str += table.name
        index_str += ' on '+ table.name + ' (%s)' % on
        if where:
            index_str += ' where %s' % where
        return index_str

    def create_table(self, table, index_commands=None):
        t = table
        if self.gc.db_create_tables:
            #@@hackish?
            if not t.bind.has_table(t.name):
                t.create(checkfirst = False)
                if index_commands:
                    for i in index_commands:
                        t.bind.execute(i)

    def py2db(self, val, return_kind=False):
        if isinstance(val, bool):
            val = 't' if val else 'f'
            kind = 'bool'
        elif isinstance(val, (str, unicode)):
            kind = 'str'
        elif isinstance(val, (int, float, long)):
            kind = 'num'
        elif val is None:
            kind = 'none'
        else:
            kind = 'pickle'
            val = pickle.dumps(val)

        if return_kind:
            return (val, kind)
        else:
            return val

    def db2py(self, val, kind):
        if kind == 'bool':
            val = True if val is 't' else False
        elif kind == 'num':
            try:
                val = int(val)
            except ValueError:
                val = float(val)
        elif kind == 'none':
            val = None
        elif kind == 'pickle':
            val = pickle.loads(val)

        return val

########NEW FILE########
__FILENAME__ = tdb_sql
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from copy import deepcopy
from datetime import datetime
import cPickle as pickle
import logging
import operators
import re
import threading

from pylons import g, c, request
import sqlalchemy as sa

from r2.lib import filters
from r2.lib.utils import (
    iters,
    Results,
    simple_traceback,
    storage,
    tup,
)


dbm = g.dbm
predefined_type_ids = g.predefined_type_ids
log_format = logging.Formatter('sql: %(message)s')
max_val_len = 1000


class TransactionSet(threading.local):
    """A manager for SQL transactions.

    This implements a thread local meta-transaction which may span multiple
    databases.  The existing tdb_sql code calls add_engine before executing
    writes.  If thing.py calls begin then these calls will actually kick in
    and start a transaction that must be committed or rolled back by thing.py.

    Because this involves creating transactions at the connection level, this
    system implicitly relies on using the threadlocal strategy for the
    sqlalchemy engines.

    This system is a bit awkward, and should be replaced with something that
    doesn't use module-globals when doing a cleanup of tdb_sql.

    """

    def __init__(self):
        self.transacting_engines = set()
        self.transaction_begun = False

    def begin(self):
        """Indicate that a transaction has begun."""
        self.transaction_begun = True

    def add_engine(self, engine):
        """Add a database connection to the meta-transaction if active."""
        if not self.transaction_begun:
            return

        if engine not in self.transacting_engines:
            engine.begin()
            self.transacting_engines.add(engine)

    def commit(self):
        """Commit the meta-transaction."""
        try:
            for engine in self.transacting_engines:
                engine.commit()
        finally:
            self._clear()

    def rollback(self):
        """Roll back the meta-transaction."""
        try:
            for engine in self.transacting_engines:
                engine.rollback()
        finally:
            self._clear()

    def _clear(self):
        self.transacting_engines.clear()
        self.transaction_begun = False


transactions = TransactionSet()

MAX_THING_ID = 9223372036854775807 # http://www.postgresql.org/docs/8.3/static/datatype-numeric.html

def make_metadata(engine):
    metadata = sa.MetaData(engine)
    metadata.bind.echo = g.sqlprinting
    return metadata

def create_table(table, index_commands=None):
    t = table
    if g.db_create_tables:
        #@@hackish?
        if not t.bind.has_table(t.name):
            t.create(checkfirst = False)
            if index_commands:
                for i in index_commands:
                    t.bind.execute(i)

def index_str(table, name, on, where = None, unique = False):
    if unique:
        index_str = 'create unique index'
    else:
        index_str = 'create index'
    index_str += ' idx_%s_' % name
    index_str += table.name
    index_str += ' on '+ table.name + ' (%s)' % on
    if where:
        index_str += ' where %s' % where
    return index_str


def index_commands(table, type):
    commands = []

    if type == 'thing':
        commands.append(index_str(table, 'id', 'thing_id'))
        commands.append(index_str(table, 'date', 'date'))
        commands.append(index_str(table, 'deleted_spam', 'deleted, spam'))
        commands.append(index_str(table, 'hot', 'hot(ups, downs, date), date'))
        commands.append(index_str(table, 'score', 'score(ups, downs), date'))
        commands.append(index_str(table, 'controversy', 'controversy(ups, downs), date'))
    elif type == 'data':
        commands.append(index_str(table, 'id', 'thing_id'))
        commands.append(index_str(table, 'thing_id', 'thing_id'))
        commands.append(index_str(table, 'key_value', 'key, substring(value, 1, %s)' \
                                  % max_val_len))

        #lower name
        commands.append(index_str(table, 'lower_key_value', 'key, lower(value)',
                                  where = "key = 'name'"))
        #ip
        commands.append(index_str(table, 'ip_network', 'ip_network(value)',
                                  where = "key = 'ip'"))
        #base_url
        commands.append(index_str(table, 'base_url', 'base_url(lower(value))',
                                  where = "key = 'url'"))
    elif type == 'rel':
        commands.append(index_str(table, 'thing1_name_date', 'thing1_id, name, date'))
        commands.append(index_str(table, 'thing2_name_date', 'thing2_id, name, date'))
        commands.append(index_str(table, 'thing1_id', 'thing1_id'))
        commands.append(index_str(table, 'thing2_id', 'thing2_id'))
        commands.append(index_str(table, 'name', 'name'))
        commands.append(index_str(table, 'date', 'date'))
    else:
        print "unknown index_commands() type %s" % type

    return commands

def get_type_table(metadata):
    table = sa.Table(g.db_app_name + '_type', metadata,
                     sa.Column('id', sa.Integer, primary_key = True),
                     sa.Column('name', sa.String, nullable = False))
    return table

def get_rel_type_table(metadata):
    table = sa.Table(g.db_app_name + '_type_rel', metadata,
                     sa.Column('id', sa.Integer, primary_key = True),
                     sa.Column('type1_id', sa.Integer, nullable = False),
                     sa.Column('type2_id', sa.Integer, nullable = False),
                     sa.Column('name', sa.String, nullable = False))
    return table

def get_thing_table(metadata, name):
    table = sa.Table(g.db_app_name + '_thing_' + name, metadata,
                     sa.Column('thing_id', sa.BigInteger, primary_key = True),
                     sa.Column('ups', sa.Integer, default = 0, nullable = False),
                     sa.Column('downs',
                               sa.Integer,
                               default = 0,
                               nullable = False),
                     sa.Column('deleted',
                               sa.Boolean,
                               default = False,
                               nullable = False),
                     sa.Column('spam',
                               sa.Boolean,
                               default = False,
                               nullable = False),
                     sa.Column('date',
                               sa.DateTime(timezone = True),
                               default = sa.func.now(),
                               nullable = False))
    table.thing_name = name
    return table

def get_data_table(metadata, name):
    data_table = sa.Table(g.db_app_name + '_data_' + name, metadata,
                          sa.Column('thing_id', sa.BigInteger, nullable = False,
                                    primary_key = True),
                          sa.Column('key', sa.String, nullable = False,
                                    primary_key = True),
                          sa.Column('value', sa.String),
                          sa.Column('kind', sa.String))
    return data_table

def get_rel_table(metadata, name):
    rel_table = sa.Table(g.db_app_name + '_rel_' + name, metadata,
                         sa.Column('rel_id', sa.BigInteger, primary_key = True),
                         sa.Column('thing1_id', sa.BigInteger, nullable = False),
                         sa.Column('thing2_id', sa.BigInteger, nullable = False),
                         sa.Column('name', sa.String, nullable = False),
                         sa.Column('date', sa.DateTime(timezone = True),
                                   default = sa.func.now(), nullable = False),
                         sa.UniqueConstraint('thing1_id', 'thing2_id', 'name'))
    rel_table.rel_name = name
    return rel_table

#get/create the type tables
def make_type_table():
    metadata = make_metadata(dbm.type_db)
    table = get_type_table(metadata)
    create_table(table)
    return table
type_table = make_type_table()

def make_rel_type_table():
    metadata = make_metadata(dbm.relation_type_db)
    table = get_rel_type_table(metadata)
    create_table(table)
    return table
rel_type_table = make_rel_type_table()

#lookup dicts
types_id = {}
types_name = {}
rel_types_id = {}
rel_types_name = {}

class ConfigurationError(Exception):
    pass

def check_type(table, name, insert_vals):
    # before hitting the db, check if we can get the type id from
    # the ini file
    type_id = predefined_type_ids.get(name)
    if type_id:
        return type_id
    elif len(predefined_type_ids) > 0:
        # flip the hell out if only *some* of the type ids are defined
        raise ConfigurationError("Expected typeid for %s" % name)

    # check for type in type table, create if not existent
    r = table.select(table.c.name == name).execute().fetchone()
    if not r:
        r = table.insert().execute(**insert_vals)
        type_id = r.last_inserted_ids()[0]
    else:
        type_id = r.id
    return type_id

#make the thing tables
def build_thing_tables():
    for name, engines in dbm.things_iter():
        type_id = check_type(type_table,
                             name,
                             dict(name = name))

        tables = []
        for engine in engines:
            metadata = make_metadata(engine)

            #make thing table
            thing_table = get_thing_table(metadata, name)
            create_table(thing_table,
                         index_commands(thing_table, 'thing'))

            #make data tables
            data_table = get_data_table(metadata, name)
            create_table(data_table,
                         index_commands(data_table, 'data'))

            tables.append((thing_table, data_table))

        thing = storage(type_id = type_id,
                        name = name,
                        avoid_master_reads = dbm.avoid_master_reads.get(name),
                        tables = tables)

        types_id[type_id] = thing
        types_name[name] = thing
build_thing_tables()

#make relation tables
def build_rel_tables():
    for name, (type1_name, type2_name, engines) in dbm.rels_iter():
        type1_id = types_name[type1_name].type_id
        type2_id = types_name[type2_name].type_id
        type_id = check_type(rel_type_table,
                             name,
                             dict(name = name,
                                  type1_id = type1_id,
                                  type2_id = type2_id))

        tables = []
        for engine in engines:
            metadata = make_metadata(engine)

            #relation table
            rel_table = get_rel_table(metadata, name)
            create_table(rel_table, index_commands(rel_table, 'rel'))

            #make thing tables
            rel_t1_table = get_thing_table(metadata, type1_name)
            if type1_name == type2_name:
                rel_t2_table = rel_t1_table
            else:
                rel_t2_table = get_thing_table(metadata, type2_name)

            #build the data
            rel_data_table = get_data_table(metadata, 'rel_' + name)
            create_table(rel_data_table,
                         index_commands(rel_data_table, 'data'))

            tables.append((rel_table,
                           rel_t1_table,
                           rel_t2_table,
                           rel_data_table))

        rel = storage(type_id = type_id,
                      type1_id = type1_id,
                      type2_id = type2_id,
                      avoid_master_reads = dbm.avoid_master_reads.get(name),
                      name = name,
                      tables = tables)

        rel_types_id[type_id] = rel
        rel_types_name[name] = rel
build_rel_tables()

def get_write_table(tables):
    if g.disallow_db_writes:
        raise Exception("not so fast! writes are not allowed on this app.")
    else:
        return tables[0]

def add_request_info(select):
    def sanitize(txt):
        return "".join(x if x.isalnum() else "."
                       for x in filters._force_utf8(txt))

    tb = simple_traceback(limit=12)
    try:
        if (hasattr(request, 'path') and
            hasattr(request, 'ip') and
            hasattr(request, 'user_agent')):
            comment = '/*\n%s\n%s\n%s\n*/' % (
                tb or "", 
                sanitize(request.fullpath),
                sanitize(request.ip))
            return select.prefix_with(comment)
    except UnicodeDecodeError:
        pass

    return select


def get_table(kind, action, tables, avoid_master_reads = False):
    if action == 'write':
        #if this is a write, store the kind in the c.use_write_db dict
        #so that all future requests use the write db
        if not isinstance(c.use_write_db, dict):
            c.use_write_db = {}
        c.use_write_db[kind] = True

        return get_write_table(tables)
    elif action == 'read':
        #check to see if we're supposed to use the write db again
        if c.use_write_db and c.use_write_db.has_key(kind):
            return get_write_table(tables)
        else:
            if avoid_master_reads and len(tables) > 1:
                return dbm.get_read_table(tables[1:])
            return dbm.get_read_table(tables)


def get_thing_table(type_id, action = 'read' ):
    return get_table('t' + str(type_id), action,
                     types_id[type_id].tables,
                     avoid_master_reads = types_id[type_id].avoid_master_reads)

def get_rel_table(rel_type_id, action = 'read'):
    return get_table('r' + str(rel_type_id), action,
                     rel_types_id[rel_type_id].tables,
                     avoid_master_reads = rel_types_id[rel_type_id].avoid_master_reads)


#TODO does the type actually exist?
def make_thing(type_id, ups, downs, date, deleted, spam, id=None):
    table = get_thing_table(type_id, action = 'write')[0]

    params = dict(ups = ups, downs = downs,
                  date = date, deleted = deleted, spam = spam)

    if id:
        params['thing_id'] = id

    def do_insert(t):
        transactions.add_engine(t.bind)
        r = t.insert().execute(**params)
        new_id = r.last_inserted_ids()[0]
        new_r = r.last_inserted_params()
        for k, v in params.iteritems():
            if new_r[k] != v:
                raise CreationError, ("There's shit in the plumbing. " +
                                      "expected %s, got %s" % (params,  new_r))
        return new_id

    try:
        id = do_insert(table)
        params['thing_id'] = id
        g.stats.event_count('thing.create', table.thing_name)
        return id
    except sa.exc.DBAPIError, e:
        if not 'IntegrityError' in e.message:
            raise
        # wrap the error to prevent db layer bleeding out
        raise CreationError, "Thing exists (%s)" % str(params)


def set_thing_props(type_id, thing_id, **props):
    table = get_thing_table(type_id, action = 'write')[0]

    if not props:
        return

    #use real columns
    def do_update(t):
        transactions.add_engine(t.bind)
        new_props = dict((t.c[prop], val) for prop, val in props.iteritems())
        u = t.update(t.c.thing_id == thing_id, values = new_props)
        u.execute()

    do_update(table)

def incr_thing_prop(type_id, thing_id, prop, amount):
    table = get_thing_table(type_id, action = 'write')[0]
    
    def do_update(t):
        transactions.add_engine(t.bind)
        u = t.update(t.c.thing_id == thing_id,
                     values={t.c[prop] : t.c[prop] + amount})
        u.execute()

    do_update(table)

class CreationError(Exception): pass

#TODO does the type exist?
#TODO do the things actually exist?
def make_relation(rel_type_id, thing1_id, thing2_id, name, date=None):
    table = get_rel_table(rel_type_id, action = 'write')[0]
    transactions.add_engine(table.bind)
    
    if not date: date = datetime.now(g.tz)
    try:
        r = table.insert().execute(thing1_id = thing1_id,
                                   thing2_id = thing2_id,
                                   name = name, 
                                   date = date)
        g.stats.event_count('rel.create', table.rel_name)
        return r.last_inserted_ids()[0]
    except sa.exc.DBAPIError, e:
        if not 'IntegrityError' in e.message:
            raise
        # wrap the error to prevent db layer bleeding out
        raise CreationError, "Relation exists (%s, %s, %s)" % (name, thing1_id, thing2_id)
        

def set_rel_props(rel_type_id, rel_id, **props):
    t = get_rel_table(rel_type_id, action = 'write')[0]

    if not props:
        return

    #use real columns
    transactions.add_engine(t.bind)
    new_props = dict((t.c[prop], val) for prop, val in props.iteritems())
    u = t.update(t.c.rel_id == rel_id, values = new_props)
    u.execute()


def py2db(val, return_kind=False):
    if isinstance(val, bool):
        val = 't' if val else 'f'
        kind = 'bool'
    elif isinstance(val, (str, unicode)):
        kind = 'str'
    elif isinstance(val, (int, float, long)):
        kind = 'num'
    elif val is None:
        kind = 'none'
    else:
        kind = 'pickle'
        val = pickle.dumps(val)

    if return_kind:
        return (val, kind)
    else:
        return val

def db2py(val, kind):
    if kind == 'bool':
        val = True if val is 't' else False
    elif kind == 'num':
        try:
            val = int(val)
        except ValueError:
            val = float(val)
    elif kind == 'none':
        val = None
    elif kind == 'pickle':
        val = pickle.loads(val)

    return val


def update_data(table, thing_id, **vals):
    transactions.add_engine(table.bind)

    u = table.update(sa.and_(table.c.thing_id == thing_id,
                             table.c.key == sa.bindparam('_key')))

    inserts = []
    for key, val in vals.iteritems():
        val, kind = py2db(val, return_kind=True)

        uresult = u.execute(_key = key, value = val, kind = kind)
        if not uresult.rowcount:
            inserts.append({'key':key, 'value':val, 'kind': kind})

    #do one insert
    if inserts:
        i = table.insert(values = dict(thing_id = thing_id))
        i.execute(*inserts)


def create_data(table, thing_id, **vals):
    transactions.add_engine(table.bind)

    inserts = []
    for key, val in vals.iteritems():
        val, kind = py2db(val, return_kind=True)
        inserts.append(dict(key=key, value=val, kind=kind))

    if inserts:
        i = table.insert(values=dict(thing_id=thing_id))
        i.execute(*inserts)


def incr_data_prop(table, type_id, thing_id, prop, amount):
    t = table
    transactions.add_engine(t.bind)
    u = t.update(sa.and_(t.c.thing_id == thing_id,
                         t.c.key == prop),
                 values={t.c.value : sa.cast(t.c.value, sa.Float) + amount})
    u.execute()

def fetch_query(table, id_col, thing_id):
    """pull the columns from the thing/data tables for a list or single
    thing_id"""
    single = False

    if not isinstance(thing_id, iters):
        single = True
        thing_id = (thing_id,)
    
    s = sa.select([table], id_col.in_(thing_id))

    try:
        r = add_request_info(s).execute().fetchall()
    except Exception, e:
        dbm.mark_dead(table.bind)
        # this thread must die so that others may live
        raise
    return (r, single)

#TODO specify columns to return?
def get_data(table, thing_id):
    r, single = fetch_query(table, table.c.thing_id, thing_id)

    #if single, only return one storage, otherwise make a dict
    res = storage() if single else {}
    for row in r:
        val = db2py(row.value, row.kind)
        stor = res if single else res.setdefault(row.thing_id, storage())
        if single and row.thing_id != thing_id:
            raise ValueError, ("tdb_sql.py: there's shit in the plumbing." 
                               + " got %s, wanted %s" % (row.thing_id,
                                                         thing_id))
        stor[row.key] = val

    return res

def set_thing_data(type_id, thing_id, brand_new_thing, **vals):
    table = get_thing_table(type_id, action = 'write')[1]

    if brand_new_thing:
        return create_data(table, thing_id, **vals)
    else:
        return update_data(table, thing_id, **vals)

def incr_thing_data(type_id, thing_id, prop, amount):
    table = get_thing_table(type_id, action = 'write')[1]
    return incr_data_prop(table, type_id, thing_id, prop, amount)    

def get_thing_data(type_id, thing_id):
    table = get_thing_table(type_id)[1]
    return get_data(table, thing_id)

def get_thing(type_id, thing_id):
    table = get_thing_table(type_id)[0]
    r, single = fetch_query(table, table.c.thing_id, thing_id)

    #if single, only return one storage, otherwise make a dict
    res = {} if not single else None
    for row in r:
        stor = storage(ups = row.ups,
                       downs = row.downs,
                       date = row.date,
                       deleted = row.deleted,
                       spam = row.spam)
        if single:
            res = stor
            # check that we got what we asked for
            if row.thing_id != thing_id:
                raise ValueError, ("tdb_sql.py: there's shit in the plumbing." 
                                    + " got %s, wanted %s" % (row.thing_id,
                                                              thing_id))
        else:
            res[row.thing_id] = stor
    return res

def set_rel_data(rel_type_id, thing_id, brand_new_thing, **vals):
    table = get_rel_table(rel_type_id, action = 'write')[3]

    if brand_new_thing:
        return create_data(table, thing_id, **vals)
    else:
        return update_data(table, thing_id, **vals)

def incr_rel_data(rel_type_id, thing_id, prop, amount):
    table = get_rel_table(rel_type_id, action = 'write')[3]
    return incr_data_prop(table, rel_type_id, thing_id, prop, amount)

def get_rel_data(rel_type_id, rel_id):
    table = get_rel_table(rel_type_id)[3]
    return get_data(table, rel_id)

def get_rel(rel_type_id, rel_id):
    r_table = get_rel_table(rel_type_id)[0]
    r, single = fetch_query(r_table, r_table.c.rel_id, rel_id)
    
    res = {} if not single else None
    for row in r:
        stor = storage(thing1_id = row.thing1_id,
                       thing2_id = row.thing2_id,
                       name = row.name,
                       date = row.date)
        if single:
            res = stor
        else:
            res[row.rel_id] = stor
    return res

def del_rel(rel_type_id, rel_id):
    tables = get_rel_table(rel_type_id, action = 'write')
    table = tables[0]
    data_table = tables[3]

    transactions.add_engine(table.bind)
    transactions.add_engine(data_table.bind)

    table.delete(table.c.rel_id == rel_id).execute()
    data_table.delete(data_table.c.thing_id == rel_id).execute()

def sa_op(op):
    #if BooleanOp
    if isinstance(op, operators.or_):
        return sa.or_(*[sa_op(o) for o in op.ops])
    elif isinstance(op, operators.and_):
        return sa.and_(*[sa_op(o) for o in op.ops])
    elif isinstance(op, operators.not_):
        return sa.not_(*[sa_op(o) for o in op.ops])

    #else, assume op is an instance of op
    if isinstance(op, operators.eq):
        fn = lambda x,y: x == y
    elif isinstance(op, operators.ne):
        fn = lambda x,y: x != y
    elif isinstance(op, operators.gt):
        fn = lambda x,y: x > y
    elif isinstance(op, operators.lt):
        fn = lambda x,y: x < y
    elif isinstance(op, operators.gte):
        fn = lambda x,y: x >= y
    elif isinstance(op, operators.lte):
        fn = lambda x,y: x <= y
    elif isinstance(op, operators.in_):
        return sa.or_(op.lval.in_(op.rval))

    rval = tup(op.rval)

    if not rval:
        return '2+2=5'
    else:
        return sa.or_(*[fn(op.lval, v) for v in rval])

def translate_sort(table, column_name, lval = None, rewrite_name = True):
    if isinstance(lval, operators.query_func):
        fn_name = lval.__class__.__name__
        sa_func = getattr(sa.func, fn_name)
        return sa_func(translate_sort(table,
                                      column_name,
                                      lval.lval,
                                      rewrite_name))

    if rewrite_name:
        if column_name == 'id':
            return table.c.thing_id
        elif column_name == 'hot':
            return sa.func.hot(table.c.ups, table.c.downs, table.c.date)
        elif column_name == 'score':
            return sa.func.score(table.c.ups, table.c.downs)
        elif column_name == 'controversy':
            return sa.func.controversy(table.c.ups, table.c.downs)
    #else
    return table.c[column_name]

#TODO - only works with thing tables
def add_sort(sort, t_table, select):
    sort = tup(sort)

    prefixes = t_table.keys() if isinstance(t_table, dict) else None
    #sort the prefixes so the longest come first
    prefixes.sort(key = lambda x: len(x))
    cols = []

    def make_sa_sort(s):
        orig_col = s.col

        col = orig_col
        if prefixes:
            table = None
            for k in prefixes:
                if k and orig_col.startswith(k):
                    table = t_table[k]
                    col = orig_col[len(k):]
            if table is None:
                table = t_table[None]
        else:
            table = t_table

        real_col = translate_sort(table, col)

        #TODO a way to avoid overlap?
        #add column for the sort parameter using the sorted name
        select.append_column(real_col.label(orig_col))

        #avoids overlap temporarily
        select.use_labels = True

        #keep track of which columns we added so we can add joins later
        cols.append((real_col, table))

        #default to asc
        return (sa.desc(real_col) if isinstance(s, operators.desc)
                else sa.asc(real_col))
        
    sa_sort = [make_sa_sort(s) for s in sort]

    s = select.order_by(*sa_sort)

    return s, cols

def translate_thing_value(rval):
    if isinstance(rval, operators.timeago):
        return sa.text("current_timestamp - interval '%s'" % rval.interval)
    else:
        return rval

#will assume parameters start with a _ for consistency
def find_things(type_id, get_cols, sort, limit, constraints):
    table = get_thing_table(type_id)[0]
    constraints = deepcopy(constraints)

    s = sa.select([table.c.thing_id.label('thing_id')])
    
    for op in operators.op_iter(constraints):
        #assume key starts with _
        #if key.startswith('_'):
        key = op.lval_name
        op.lval = translate_sort(table, key[1:], op.lval)
        op.rval = translate_thing_value(op.rval)

    for op in constraints:
        s.append_whereclause(sa_op(op))

    if sort:
        s, cols = add_sort(sort, {'_': table}, s)

    if limit:
        s = s.limit(limit)

    try:
        r = add_request_info(s).execute()
    except Exception, e:
        dbm.mark_dead(table.bind)
        # this thread must die so that others may live
        raise
    return Results(r, lambda(row): row if get_cols else row.thing_id)

def translate_data_value(alias, op):
    lval = op.lval
    need_substr = False if isinstance(lval, operators.query_func) else True
    lval = translate_sort(alias, 'value', lval, False)

    #add the substring func
    if need_substr:
        lval = sa.func.substring(lval, 1, max_val_len)
    
    op.lval = lval
        
    #convert the rval to db types
    #convert everything to strings for pg8.3
    op.rval = tuple(str(py2db(v)) for v in tup(op.rval))

#TODO sort by data fields
#TODO sort by id wants thing_id
def find_data(type_id, get_cols, sort, limit, constraints):
    t_table, d_table = get_thing_table(type_id)
    constraints = deepcopy(constraints)

    used_first = False
    s = None
    need_join = False
    have_data_rule = False
    first_alias = d_table.alias()
    s = sa.select([first_alias.c.thing_id.label('thing_id')])#, distinct=True)

    for op in operators.op_iter(constraints):
        key = op.lval_name
        vals = tup(op.rval)

        if key == '_id':
            op.lval = first_alias.c.thing_id
        elif key.startswith('_'):
            need_join = True
            op.lval = translate_sort(t_table, key[1:], op.lval)
            op.rval = translate_thing_value(op.rval)
        else:
            have_data_rule = True
            id_col = None
            if not used_first:
                alias = first_alias
                used_first = True
            else:
                alias = d_table.alias()
                id_col = first_alias.c.thing_id

            if id_col is not None:
                s.append_whereclause(id_col == alias.c.thing_id)
            
            s.append_column(alias.c.value.label(key))
            s.append_whereclause(alias.c.key == key)
            
            #add the substring constraint if no other functions are there
            translate_data_value(alias, op)

    for op in constraints:
        s.append_whereclause(sa_op(op))

    if not have_data_rule:
        raise Exception('Data queries must have at least one data rule.')

    #TODO in order to sort by data columns, this is going to need to be smarter
    if sort:
        need_join = True
        s, cols = add_sort(sort, {'_':t_table}, s)
            
    if need_join:
        s.append_whereclause(first_alias.c.thing_id == t_table.c.thing_id)

    if limit:
        s = s.limit(limit)

    try:
        r = add_request_info(s).execute()
    except Exception, e:
        dbm.mark_dead(t_table.bind)
        # this thread must die so that others may live
        raise

    return Results(r, lambda(row): row if get_cols else row.thing_id)


def find_rels(rel_type_id, get_cols, sort, limit, constraints):
    tables = get_rel_table(rel_type_id)
    r_table, t1_table, t2_table, d_table = tables
    constraints = deepcopy(constraints)

    t1_table, t2_table = t1_table.alias(), t2_table.alias()

    s = sa.select([r_table.c.rel_id.label('rel_id')])
    need_join1 = ('thing1_id', t1_table)
    need_join2 = ('thing2_id', t2_table)
    joins_needed = set()

    for op in operators.op_iter(constraints):
        #vals = con.rval
        key = op.lval_name
        prefix = key[:4]
        
        if prefix in ('_t1_', '_t2_'):
            #not a thing attribute
            key = key[4:]

            if prefix == '_t1_':
                join = need_join1
                joins_needed.add(join)
            elif prefix == '_t2_':
                join = need_join2
                joins_needed.add(join)

            table = join[1]
            op.lval = translate_sort(table, key, op.lval)
            op.rval = translate_thing_value(op.rval)
            #ors = [sa_op(con, key, v) for v in vals]
            #s.append_whereclause(sa.or_(*ors))

        elif prefix.startswith('_'):
            op.lval = r_table.c[key[1:]]

        else:
            alias = d_table.alias()
            s.append_whereclause(r_table.c.rel_id == alias.c.thing_id)
            s.append_column(alias.c.value.label(key))
            s.append_whereclause(alias.c.key == key)

            translate_data_value(alias, op)

    for op in constraints:
        s.append_whereclause(sa_op(op))

    if sort:
        s, cols = add_sort(sort,
                           {'_':r_table, '_t1_':t1_table, '_t2_':t2_table},
                           s)
        
        #do we need more joins?
        for (col, table) in cols:
            if table == need_join1[1]:
                joins_needed.add(need_join1)
            elif table == need_join2[1]:
                joins_needed.add(need_join2)
        
    for j in joins_needed:
        col, table = j
        s.append_whereclause(r_table.c[col] == table.c.thing_id)    

    if limit:
        s = s.limit(limit)

    try:
        r = add_request_info(s).execute()
    except Exception, e:
        dbm.mark_dead(r_table.bind)
        # this thread must die so that others may live
        raise
    return Results(r, lambda (row): (row if get_cols else row.rel_id))

if logging.getLogger('sqlalchemy').handlers:
    logging.getLogger('sqlalchemy').handlers[0].formatter = log_format

#inconsitencies:

#relationships assume their thing and data tables are in the same
#database. things don't make that assumption. in practice thing/data
#tables always go together.
#
#we create thing tables for a relationship's things that aren't on the
#same database as the relationship, although they're never used in
#practice. we could remove a healthy chunk of code if we removed that.

########NEW FILE########
__FILENAME__ = thing
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import hashlib
import new
import sys

from copy import copy, deepcopy
from datetime import datetime

from pylons import g

from r2.lib import hooks
from r2.lib.cache import sgm
from r2.lib.db import tdb_sql as tdb, sorts, operators
from r2.lib.utils import Results, tup, to36


class NotFound(Exception): pass
CreationError = tdb.CreationError

thing_types = {}
rel_types = {}

def begin():
    tdb.transactions.begin()

def commit():
    tdb.transactions.commit()

def rollback():
    tdb.transactions.rollback()

def obj_id(things):
    return tuple(t if isinstance(t, (int, long)) else t._id for t in things)

def thing_prefix(cls_name, id=None):
    p = cls_name + '_'
    if id:
        p += str(id)
    return p

class SafeSetAttr:
    def __init__(self, cls):
        self.cls = cls

    def __enter__(self):
        self.cls.__safe__ = True

    def __exit__(self, type, value, tb):
        self.cls.__safe__ = False

class DataThing(object):
    _base_props = ()
    _int_props = ()
    _data_int_props = ()
    _int_prop_suffix = None
    _defaults = {}
    _essentials = ()
    c = operators.Slots()
    __safe__ = False
    _cache = g.cache

    def __init__(self):
        safe_set_attr = SafeSetAttr(self)
        with safe_set_attr:
            self.safe_set_attr = safe_set_attr
            self._dirties = {}
            self._t = {}
            self._created = False
            self._loaded = True

    #TODO some protection here?
    def __setattr__(self, attr, val, make_dirty=True):
        if attr.startswith('__') or self.__safe__:
            object.__setattr__(self, attr, val)
            return 

        if attr.startswith('_'):
            #assume baseprops has the attr
            if make_dirty and hasattr(self, attr):
                old_val = getattr(self, attr)
            object.__setattr__(self, attr, val)
            if not attr in self._base_props:
                return
        else:
            old_val = self._t.get(attr, self._defaults.get(attr))
            self._t[attr] = val
        if make_dirty and val != old_val:
            self._dirties[attr] = (old_val, val)

    def __setstate__(self, state):
        # pylibmc's automatic unpicking will call __setstate__ if it exists.
        # if we don't implement __setstate__ the check for existence will fail
        # in an atypical (and not properly handled) way because we override
        # __getattr__. the implementation provided here is identical to what
        # would happen in the default unimplemented case.
        self.__dict__ = state

    def __getattr__(self, attr):
        try:
            return self._t[attr]
        except KeyError:
            try:
                return self._defaults[attr]
            except KeyError:
                # attr didn't exist--continue on to error recovery below
                pass

        try:
            _id = object.__getattribute__(self, "_id")
        except AttributeError:
            _id = "???"

        try:
            cl = object.__getattribute__(self, "__class__").__name__
        except AttributeError:
            cl = "???"

        if self._loaded:
            nl = "it IS loaded"
        else:
            nl = "it is NOT loaded"

        try:
            id_str = "%d" % _id
        except TypeError:
            id_str = "%r" % _id

        descr = '%s(%s).%s' % (cl, id_str, attr)

        essentials = object.__getattribute__(self, "_essentials")
        deleted = object.__getattribute__(self, "_deleted")

        if deleted:
            nl += " and IS deleted."
        else:
            nl += " and is NOT deleted."

        if attr in essentials and not deleted:
            g.log.error("%s not found; %s forcing reload.", descr, nl)
            self._load()

            try:
                return self._t[attr]
            except KeyError:
                g.log.error("reload of %s didn't help.", descr)

        raise AttributeError, '%s not found; %s' % (descr, nl)

    def _cache_key(self):
        return thing_prefix(self.__class__.__name__, self._id)

    def _other_self(self):
        """Load from the cached version of myself. Skip the local cache."""
        l = self._cache.get(self._cache_key(), allow_local = False)
        if l and l._id != self._id:
            g.log.error("thing.py: Doppleganger on read: got %s for %s",
                        (l, self))
            self._cache.delete(self._cache_key())
            return 
        return l

    def _cache_myself(self):
        ck = self._cache_key()
        self._cache.set(ck, self)

    def _sync_latest(self):
        """Load myself from the cache to and re-apply the .dirties
        list to make sure we don't overwrite a previous commit. """
        other_self = self._other_self()
        if not other_self:
            return self._dirty

        #copy in the cache's version
        for prop in self._base_props:
            self.__setattr__(prop, getattr(other_self, prop), False)

        if other_self._loaded:
            self._t = other_self._t

        #re-apply the .dirties
        old_dirties = self._dirties
        self._dirties = {}
        for k, (old_val, new_val) in old_dirties.iteritems():
            setattr(self, k, new_val)

        #return whether we're still dirty or not
        return self._dirty

    def _commit(self, keys=None):
        lock = None

        try:
            if not self._created:
                begin()
                self._create()
                just_created = True
            else:
                just_created = False

            lock = g.make_lock("thing_commit", 'commit_' + self._fullname)
            lock.acquire()

            if not just_created and not self._sync_latest():
                #sync'd and we have nothing to do now, but we still cache anyway
                self._cache_myself()
                return

            # begin is a no-op if already done, but in the not-just-created
            # case we need to do this here because the else block is not
            # executed when the try block is exited prematurely in any way
            # (including the return in the above branch)
            begin()

            to_set = self._dirties.copy()
            if keys:
                keys = tup(keys)
                for key in to_set.keys():
                    if key not in keys:
                        del to_set[key]

            data_props = {}
            thing_props = {}
            for k, (old_value, new_value) in to_set.iteritems():
                if k.startswith('_'):
                    thing_props[k[1:]] = new_value
                else:
                    data_props[k] = new_value

            if data_props:
                self._set_data(self._type_id,
                               self._id,
                               just_created,
                               **data_props)

            if thing_props:
                self._set_props(self._type_id, self._id, **thing_props)

            if keys:
                for k in keys:
                    if self._dirties.has_key(k):
                        del self._dirties[k]
            else:
                self._dirties.clear()
        except:
            rollback()
            raise
        else:
            commit()
            self._cache_myself()
        finally:
            if lock:
                lock.release()

        hooks.get_hook("thing.commit").call(thing=self, changes=to_set)

    @classmethod
    def _load_multi(cls, need):
        need = tup(need)
        need_ids = [n._id for n in need]
        datas = cls._get_data(cls._type_id, need_ids)
        to_save = {}
        try:
            essentials = object.__getattribute__(cls, "_essentials")
        except AttributeError:
            essentials = ()

        for i in need:
            #if there wasn't any data, keep the empty dict
            i._t.update(datas.get(i._id, i._t))
            i._loaded = True

            for attr in essentials:
                if attr not in i._t:
                    print "Warning: %s is missing %s" % (i._fullname, attr)
            to_save[i._id] = i

        prefix = thing_prefix(cls.__name__)

        #write the data to the cache
        cls._cache.set_multi(to_save, prefix=prefix)

    def _load(self):
        self._load_multi(self)

    def _safe_load(self):
        if not self._loaded:
            self._load()

    def _incr(self, prop, amt = 1):
        if self._dirty:
            raise ValueError, "cannot incr dirty thing"

        #make sure we're incr'ing an _int_prop or _data_int_prop.
        if prop not in self._int_props:
            if (prop in self._data_int_props or
                self._int_prop_suffix and prop.endswith(self._int_prop_suffix)):
                #if we're incr'ing a data_prop, make sure we're loaded
                if not self._loaded:
                    self._load()
            else:
                msg = ("cannot incr non int prop %r on %r -- it's not in %r or %r" %
                       (prop, self, self._int_props, self._data_int_props))
                raise ValueError, msg

        with g.make_lock("thing_commit", 'commit_' + self._fullname):
            self._sync_latest()
            old_val = getattr(self, prop)
            if self._defaults.has_key(prop) and self._defaults[prop] == old_val:
                #potential race condition if the same property gets incr'd
                #from default at the same time
                setattr(self, prop, old_val + amt)
                self._commit(prop)
            else:
                self.__setattr__(prop, old_val + amt, False)
                #db
                if prop.startswith('_'):
                    tdb.incr_thing_prop(self._type_id, self._id, prop[1:], amt)
                else:
                    self._incr_data(self._type_id, self._id, prop, amt)

            self._cache_myself()

    @property
    def _id36(self):
        return to36(self._id)

    @classmethod
    def _fullname_from_id36(cls, id36):
        return cls._type_prefix + to36(cls._type_id) + '_' + id36

    @property
    def _fullname(self):
        return self._fullname_from_id36(self._id36)

    #TODO error when something isn't found?
    @classmethod
    def _byID(cls, ids, data=False, return_dict=True, extra_props=None,
              stale=False, ignore_missing=False):
        ids, single = tup(ids, True)
        prefix = thing_prefix(cls.__name__)

        if not all(x <= tdb.MAX_THING_ID for x in ids):
            raise NotFound('huge thing_id in %r' % ids)

        def count_found(ret, still_need):
            cls._cache.stats.cache_report(
                hits=len(ret), misses=len(still_need),
                cache_name='sgm.%s' % cls.__name__)

        if not cls._cache.stats:
            count_found = None

        def items_db(ids):
            items = cls._get_item(cls._type_id, ids)
            for i in items.keys():
                items[i] = cls._build(i, items[i])

            return items

        bases = sgm(cls._cache, ids, items_db, prefix, stale=stale,
                    found_fn=count_found)

        # Check to see if we found everything we asked for
        missing = []
        for i in ids:
            if i not in bases:
                missing.append(i)
            elif bases[i] and bases[i]._id != i:
                g.log.error("thing.py: Doppleganger on byID: %s got %s for %s" %
                            (cls.__name__, bases[i]._id, i))
                bases[i] = items_db([i]).values()[0]
                bases[i]._cache_myself()
        if missing and not ignore_missing:
            raise NotFound, '%s %s' % (cls.__name__, missing)
        for i in missing:
            ids.remove(i)

        if data:
            need = []
            for v in bases.itervalues():
                if not v._loaded:
                    need.append(v)
            if need:
                cls._load_multi(need)

        if extra_props:
            for _id, props in extra_props.iteritems():
                for k, v in props.iteritems():
                    bases[_id].__setattr__(k, v, False)

        if single:
            return bases[ids[0]] if ids else None
        elif return_dict:
            return bases
        else:
            return filter(None, (bases.get(i) for i in ids))

    @classmethod
    def _byID36(cls, id36s, return_dict = True, **kw):

        id36s, single = tup(id36s, True)

        # will fail if it's not a string
        ids = [ int(x, 36) for x in id36s ]

        things = cls._byID(ids, return_dict=True, **kw)
        things = {thing._id36: thing for thing in things.itervalues()}

        if single:
            return things.values()[0]
        elif return_dict:
            return things
        else:
            return filter(None, (things.get(i) for i in id36s))

    @classmethod
    def _by_fullname(cls, names,
                     return_dict = True, 
                     ignore_missing=False,
                     **kw):
        names, single = tup(names, True)

        table = {}
        lookup = {}
        # build id list by type
        for fullname in names:
            try:
                real_type, thing_id = fullname.split('_')
                #distinguish between things and realtions
                if real_type[0] == 't':
                    type_dict = thing_types
                elif real_type[0] == 'r':
                    type_dict = rel_types
                else:
                    raise NotFound
                real_type = type_dict[int(real_type[1:], 36)]
                thing_id = int(thing_id, 36)
                lookup[fullname] = (real_type, thing_id)
                table.setdefault(real_type, []).append(thing_id)
            except (KeyError, ValueError):
                if single:
                    raise NotFound

        # lookup ids for each type
        identified = {}
        for real_type, thing_ids in table.iteritems():
            i = real_type._byID(thing_ids, ignore_missing=ignore_missing, **kw)
            identified[real_type] = i

        # interleave types in original order of the name
        res = []
        for fullname in names:
            if lookup.has_key(fullname):
                real_type, thing_id = lookup[fullname]
                thing = identified.get(real_type, {}).get(thing_id)
                if not thing and ignore_missing:
                    continue
                res.append((fullname, thing))

        if single:
            return res[0][1] if res else None
        elif return_dict:
            return dict(res)
        else:
            return [x for i, x in res]

    @property
    def _dirty(self):
        return bool(len(self._dirties))

    @classmethod
    def _query(cls, *a, **kw):
        raise NotImplementedError()

    @classmethod
    def _build(*a, **kw):
        raise NotImplementedError()

    def _get_data(*a, **kw):
        raise NotImplementedError()

    def _set_data(*a, **kw):
        raise NotImplementedError()

    def _incr_data(*a, **kw):
        raise NotImplementedError()

    def _get_item(*a, **kw):
        raise NotImplementedError

    def _create(self):
        base_props = (getattr(self, prop) for prop in self._base_props)
        self._id = self._make_fn(self._type_id, *base_props)
        self._created = True

class ThingMeta(type):
    def __init__(cls, name, bases, dct):
        if name == 'Thing' or hasattr(cls, '_nodb') and cls._nodb: return
        #print "checking thing", name

        #TODO exceptions
        cls._type_name = name.lower()
        try:
            cls._type_id = tdb.types_name[cls._type_name].type_id
        except KeyError:
            raise KeyError, 'is the thing database %s defined?' % name

        global thing_types
        thing_types[cls._type_id] = cls

        super(ThingMeta, cls).__init__(name, bases, dct)
    
    def __repr__(cls):
        return '<thing: %s>' % cls._type_name

class Thing(DataThing):
    __metaclass__ = ThingMeta
    _base_props = ('_ups', '_downs', '_date', '_deleted', '_spam')
    _int_props = ('_ups', '_downs')
    _make_fn = staticmethod(tdb.make_thing)
    _set_props = staticmethod(tdb.set_thing_props)
    _get_data = staticmethod(tdb.get_thing_data)
    _set_data = staticmethod(tdb.set_thing_data)
    _get_item = staticmethod(tdb.get_thing)
    _incr_data = staticmethod(tdb.incr_thing_data)
    _type_prefix = 't'

    def __init__(self, ups = 0, downs = 0, date = None, deleted = False,
                 spam = False, id = None, **attrs):
        DataThing.__init__(self)

        with self.safe_set_attr:
            if id:
                self._id = id
                self._created = True
                self._loaded = False

            if not date: date = datetime.now(g.tz)
            
            self._ups = ups
            self._downs = downs
            self._date = date
            self._deleted = deleted
            self._spam = spam

        #new way
        for k, v in attrs.iteritems():
            self.__setattr__(k, v, not self._created)
        
    def __repr__(self):
        return '<%s %s>' % (self.__class__.__name__,
                            self._id if self._created else '[unsaved]')

    def _set_id(self, thing_id):
        if not self._created:
            with self.safe_set_attr:
                self._base_props += ('_thing_id',)
                self._thing_id = thing_id

    @property
    def _age(self):
        return datetime.now(g.tz) - self._date

    @property
    def _hot(self):
        return sorts.hot(self._ups, self._downs, self._date)

    @property
    def _score(self):
        return sorts.score(self._ups, self._downs)

    @property
    def _controversy(self):
        return sorts.controversy(self._ups, self._downs)

    @property
    def _confidence(self):
        return sorts.confidence(self._ups, self._downs)

    @classmethod
    def _build(cls, id, bases):
        return cls(bases.ups, bases.downs, bases.date,
                   bases.deleted, bases.spam, id)

    @classmethod
    def _query(cls, *all_rules, **kw):
        need_deleted = True
        need_spam = True
        #add default spam/deleted
        rules = []
        optimize_rules = kw.pop('optimize_rules', False)
        for r in all_rules:
            if not isinstance(r, operators.op):
                continue
            if r.lval_name == '_deleted':
                need_deleted = False
                # if the caller is explicitly unfiltering based on this column,
                # we don't need this rule at all. taking this out can save us a
                # join that is very expensive on pg9.
                if optimize_rules and r.rval == (True, False):
                    continue
            elif r.lval_name == '_spam':
                need_spam = False
                # see above for explanation
                if optimize_rules and r.rval == (True, False):
                    continue
            rules.append(r)

        if need_deleted:
            rules.append(cls.c._deleted == False)

        if need_spam:
            rules.append(cls.c._spam == False)

        return Things(cls, *rules, **kw)


class RelationMeta(type):
    def __init__(cls, name, bases, dct):
        if name == 'RelationCls': return
        #print "checking relation", name

        cls._type_name = name.lower()
        try:
            cls._type_id = tdb.rel_types_name[cls._type_name].type_id
        except KeyError:
            raise KeyError, 'is the relationship database %s defined?' % name

        global rel_types
        rel_types[cls._type_id] = cls

        super(RelationMeta, cls).__init__(name, bases, dct)

    def __repr__(cls):
        return '<relation: %s>' % cls._type_name

def Relation(type1, type2, denorm1 = None, denorm2 = None):
    class RelationCls(DataThing):
        __metaclass__ = RelationMeta
        if not (issubclass(type1, Thing) and issubclass(type2, Thing)):
                raise TypeError('Relation types must be subclass of %s' % Thing)

        _type1 = type1
        _type2 = type2

        _base_props = ('_thing1_id', '_thing2_id', '_name', '_date')
        _make_fn = staticmethod(tdb.make_relation)
        _set_props = staticmethod(tdb.set_rel_props)
        _get_data = staticmethod(tdb.get_rel_data)
        _set_data = staticmethod(tdb.set_rel_data)
        _get_item = staticmethod(tdb.get_rel)
        _incr_data = staticmethod(tdb.incr_rel_data)
        _type_prefix = Relation._type_prefix
        _eagerly_loaded_data = False

        # data means, do you load the reddit_data_rel_* fields (the data on the
        # rel itself). eager_load means, do you load thing1 and thing2
        # immediately. It calls _byID(xxx, data=thing_data).
        @classmethod
        def _byID_rel(cls, ids, data=False, return_dict=True, extra_props=None,
                      eager_load=False, thing_data=False):

            ids, single = tup(ids, True)

            bases = cls._byID(ids, data=data, return_dict=True,
                              extra_props=extra_props)

            values = bases.values()

            if values and eager_load:
                for base in bases.values():
                    base._eagerly_loaded_data = True
                load_things(values, thing_data)

            if single:
                return bases[ids[0]]
            elif return_dict:
                return bases
            else:
                return filter(None, (bases.get(i) for i in ids))

        def __init__(self, thing1, thing2, name, date = None, id = None, **attrs):
            DataThing.__init__(self)

            def id_and_obj(in_thing):
                if isinstance(in_thing, (int, long)):
                    return in_thing
                else:
                    return in_thing._id

            with self.safe_set_attr:
                if id:
                    self._id = id
                    self._created = True
                    self._loaded = False

                if not date: date = datetime.now(g.tz)


                #store the id, and temporarily store the actual object
                #because we may need it later
                self._thing1_id = id_and_obj(thing1)
                self._thing2_id = id_and_obj(thing2)
                self._name = name
                self._date = date

            for k, v in attrs.iteritems():
                self.__setattr__(k, v, not self._created)

            def denormalize(denorm, src, dest):
                if denorm:
                    setattr(dest, denorm[0], getattr(src, denorm[1]))

            #denormalize
            if not self._created:
                denormalize(denorm1, thing2, thing1)
                denormalize(denorm2, thing1, thing2)

        def __getattr__(self, attr):
            if attr == '_thing1':
                return self._type1._byID(self._thing1_id,
                                         self._eagerly_loaded_data)
            elif attr == '_thing2':
                return self._type2._byID(self._thing2_id,
                                         self._eagerly_loaded_data)
            elif attr.startswith('_t1'):
                return getattr(self._thing1, attr[3:])
            elif attr.startswith('_t2'):
                return getattr(self._thing2, attr[3:])
            else:
                return DataThing.__getattr__(self, attr)

        def __repr__(self):
            return ('<%s %s: <%s %s> - <%s %s> %s>' %
                    (self.__class__.__name__, self._name,
                     self._type1.__name__, self._thing1_id,
                     self._type2.__name__,self._thing2_id,
                     '[unsaved]' if not self._created else '\b'))

        def _commit(self):
            DataThing._commit(self)
            #if i denormalized i need to check here
            if denorm1: self._thing1._commit(denorm1[0])
            if denorm2: self._thing2._commit(denorm2[0])
            #set fast query cache
            self._cache.set(thing_prefix(self.__class__.__name__)
                      + str((self._thing1_id, self._thing2_id, self._name)),
                      self._id)

        def _delete(self):
            tdb.del_rel(self._type_id, self._id)
            
            #clear cache
            prefix = thing_prefix(self.__class__.__name__)
            #TODO - there should be just one cache key for a rel?
            self._cache.delete(prefix + str(self._id))
            #update fast query cache
            self._cache.set(prefix + str((self._thing1_id,
                                    self._thing2_id,
                                    self._name)), None)
            #temporarily set this property so the rest of this request
            #know it's deleted. save -> unsave, hide -> unhide
            self._name = 'un' + self._name

        @classmethod
        def _fast_query(cls, thing1s, thing2s, name, data=True, eager_load=True,
                        thing_data=False):
            """looks up all the relationships between thing1_ids and
               thing2_ids and caches them"""
            prefix = thing_prefix(cls.__name__)

            thing1_dict = dict((t._id, t) for t in tup(thing1s))
            thing2_dict = dict((t._id, t) for t in tup(thing2s))

            thing1_ids = thing1_dict.keys()
            thing2_ids = thing2_dict.keys()

            name = tup(name)

            # permute all of the pairs
            pairs = set((x, y, n)
                        for x in thing1_ids
                        for y in thing2_ids
                        for n in name)

            def lookup_rel_ids(pairs):
                rel_ids = {}

                t1_ids = set()
                t2_ids = set()
                names = set()
                for t1, t2, name in pairs:
                    t1_ids.add(t1)
                    t2_ids.add(t2)
                    names.add(name)

                if t1_ids and t2_ids and names:
                    q = cls._query(cls.c._thing1_id == t1_ids,
                                   cls.c._thing2_id == t2_ids,
                                   cls.c._name == names)
                else:
                    q = []

                for rel in q:
                    rel_ids[(rel._thing1_id, rel._thing2_id, rel._name)] = rel._id

                for p in pairs:
                    if p not in rel_ids:
                        rel_ids[p] = None

                return rel_ids

            # get the relation ids from the cache or query the db
            res = sgm(cls._cache, pairs, lookup_rel_ids, prefix)

            # get the relation objects
            rel_ids = {rel_id for rel_id in res.itervalues()
                              if rel_id is not None}
            rels = cls._byID_rel(rel_ids, data=data, eager_load=eager_load,
                                 thing_data=thing_data)

            res_obj = {}
            for (thing1_id, thing2_id, name), rel_id in res.iteritems():
                pair = (thing1_dict[thing1_id], thing2_dict[thing2_id], name)
                rel = rels[rel_id] if rel_id is not None else None
                res_obj[pair] = rel

            return res_obj

        @classmethod
        def _gay(cls):
            return cls._type1 == cls._type2

        @classmethod
        def _build(cls, id, bases):
            return cls(bases.thing1_id, bases.thing2_id, bases.name, bases.date, id)

        @classmethod
        def _query(cls, *a, **kw):
            return Relations(cls, *a, **kw)


    return RelationCls
Relation._type_prefix = 'r'

class Query(object):
    def __init__(self, kind, *rules, **kw):
        self._rules = []
        self._kind = kind

        self._read_cache = kw.get('read_cache')
        self._write_cache = kw.get('write_cache')
        self._cache_time = kw.get('cache_time', 0)
        self._limit = kw.get('limit')
        self._data = kw.get('data')
        self._sort = kw.get('sort', ())
        self._filter_primary_sort_only = kw.get('filter_primary_sort_only', False)

        self._filter(*rules)
    
    def _setsort(self, sorts):
        sorts = tup(sorts)
        #make sure sorts are wrapped in a Sort obj
        have_date = False
        op_sorts = []
        for s in sorts:
            if not isinstance(s, operators.sort):
                s = operators.asc(s)
            op_sorts.append(s)
            if s.col.endswith('_date'):
                have_date = True
        if op_sorts and not have_date:
            op_sorts.append(operators.desc('_date'))

        self._sort_param = op_sorts
        return self

    def _getsort(self):
        return self._sort_param

    _sort = property(_getsort, _setsort)

    def _reverse(self):
        for s in self._sort:
            if isinstance(s, operators.asc):
                s.__class__ = operators.desc
            else:
                s.__class__ = operators.asc

    def _list(self, data = False):
        if data:
            self._data = data

        return list(self)

    def _dir(self, thing, reverse):
        ors = []

        # this fun hack lets us simplify the query on /r/all 
        # for postgres-9 compatibility. please remove it when
        # /r/all is precomputed.
        sorts = range(len(self._sort))
        if self._filter_primary_sort_only:
            sorts = [0]

        #for each sort add and a comparison operator
        for i in sorts:
            s = self._sort[i]

            if isinstance(s, operators.asc):
                op = operators.gt
            else:
                op = operators.lt

            if reverse:
                op = operators.gt if op == operators.lt else operators.lt

            #remember op takes lval and lval_name
            ands = [op(s.col, s.col, getattr(thing, s.col))]

            #for each sort up to the last add an equals operator
            for j in range(0, i):
                s = self._sort[j]
                ands.append(thing.c[s.col] == getattr(thing, s.col))

            ors.append(operators.and_(*ands))

        return self._filter(operators.or_(*ors))

    def _before(self, thing):
        return self._dir(thing, True)

    def _after(self, thing):
        return self._dir(thing, False)

    def _count(self):
        return self._cursor().rowcount()


    def _filter(*a, **kw):
        raise NotImplementedError

    def _cursor(*a, **kw):
        raise NotImplementedError

    def _iden(self):
        i = str(self._sort) + str(self._kind) + str(self._limit)
        if self._rules:
            rules = copy(self._rules)
            rules.sort()
            for r in rules:
                i += str(r)
        return hashlib.sha1(i).hexdigest()

    def __iter__(self):
        used_cache = False

        def _retrieve():
            return self._cursor().fetchall()

        names = lst = []

        names = g.cache.get(self._iden()) if self._read_cache else None
        if names is None and not self._write_cache:
            # it wasn't in the cache, and we're not going to
            # replace it, so just hit the db
            lst = _retrieve()
        elif names is None and self._write_cache:
            # it's not in the cache, and we have the power to
            # update it, which we should do in a lock to prevent
            # concurrent requests for the same data
            with g.make_lock("thing_query", "lock_%s" % self._iden()):
                # see if it was set while we were waiting for our
                # lock
                names = g.cache.get(self._iden(), allow_local = False) \
                                  if self._read_cache else None
                if names is None:
                    lst = _retrieve()
                    g.cache.set(self._iden(),
                              [ x._fullname for x in lst ],
                              self._cache_time)

        if names and not lst:
            # we got our list of names from the cache, so we need to
            # turn them back into Things
            lst = Thing._by_fullname(names, data = self._data, return_dict = False)

        for item in lst:
            yield item

class Things(Query):
    def __init__(self, kind, *rules, **kw):
        self._use_data = False
        Query.__init__(self, kind, *rules, **kw)

    def _filter(self, *rules):
        for op in operators.op_iter(rules):
            if not op.lval_name.startswith('_'):
                self._use_data = True

        self._rules += rules
        return self


    def _cursor(self):
        #TODO why was this even here?
        #get_cols = bool(self._sort_param)
        get_cols = False
        params = (self._kind._type_id,
                  get_cols,
                  self._sort,
                  self._limit,
                  self._rules)
        if self._use_data:
            c = tdb.find_data(*params)
        else:
            c = tdb.find_things(*params)

        #TODO simplfy this! get_cols is always false?
        #called on a bunch of rows to fetch their properties in batch
        def row_fn(rows):
            #if have a sort, add the sorted column to the results
            if get_cols:
                extra_props = {}
                for r in rows:
                    for sc in (s.col for s in self._sort):
                        #dict of ids to the extra sort params
                        props = extra_props.setdefault(r.thing_id, {})
                        props[sc] = getattr(r, sc)
                _ids = extra_props.keys()
            else:
                _ids = rows
                extra_props = {}
            return self._kind._byID(_ids, self._data, False, extra_props)

        return Results(c, row_fn, True)

def load_things(rels, load_data=False):
    rels = tup(rels)
    kind = rels[0].__class__

    t1_ids = set()
    t2_ids = t1_ids if kind._gay() else set()
    for rel in rels:
        t1_ids.add(rel._thing1_id)
        t2_ids.add(rel._thing2_id)
    kind._type1._byID(t1_ids, data=load_data)
    if not kind._gay():
        t2_items = kind._type2._byID(t2_ids, data=load_data)

class Relations(Query):
    #params are thing1, thing2, name, date
    def __init__(self, kind, *rules, **kw):
        self._eager_load = kw.get('eager_load')
        self._thing_data = kw.get('thing_data')
        Query.__init__(self, kind, *rules, **kw)

    def _filter(self, *rules):
        self._rules += rules
        return self

    def _eager(self, eager, thing_data = False):
        #load the things (id, ups, down, etc.)
        self._eager_load = eager
        #also load the things' data
        self._thing_data = thing_data
        return self

    def _make_rel(self, rows):
        rels = self._kind._byID(rows, self._data, False)
        if rels and self._eager_load:
            for rel in rels:
                rel._eagerly_loaded_data = True
            load_things(rels, self._thing_data)
        return rels

    def _cursor(self):
        c = tdb.find_rels(self._kind._type_id,
                          False,
                          sort = self._sort,
                          limit = self._limit,
                          constraints = self._rules)
        return Results(c, self._make_rel, True)

class MultiCursor(object):
    def __init__(self, *execute_params):
        self._execute_params = execute_params
        self._cursor = None

    def fetchone(self):
        if not self._cursor:
            self._cursor = self._execute(*self._execute_params)
            
        return self._cursor.next()
                
    def fetchall(self):
        if not self._cursor:
            self._cursor = self._execute(*self._execute_params)

        return [i for i in self._cursor]

class MergeCursor(MultiCursor):
    def _execute(self, cursors, sorts):
        #a "pair" is a (cursor, item, done) tuple
        def safe_next(c):
            try:
                #hack to keep searching even if fetching a thing returns notfound
                while True:
                    try:
                        return [c, c.fetchone(), False]
                    except NotFound:
                        #skips the broken item
                        pass
            except StopIteration:
                return c, None, True

        def undone(pairs):
            return [p for p in pairs if not p[2]]

        pairs = undone(safe_next(c) for c in cursors)

        while pairs:
            #only one query left, just dump it
            if len(pairs) == 1:
                c, item, done = pair = pairs[0]
                while not done:
                    yield item
                    c, item, done = safe_next(c)
                    pair[:] = c, item, done
            else:
                #by default, yield the first item
                yield_pair = pairs[0]
                for s in sorts:
                    col = s.col
                    #sort direction?
                    max_fn = min if isinstance(s, operators.asc) else max

                    #find the max (or min) val
                    vals = [(getattr(i[1], col), i) for i in pairs]
                    max_pair = vals[0]
                    all_equal = True
                    for pair in vals[1:]:
                        if all_equal and pair[0] != max_pair[0]:
                            all_equal = False
                        max_pair = max_fn(max_pair, pair, key=lambda x: x[0])

                    if not all_equal:
                        yield_pair = max_pair[1]
                        break

                c, item, done = yield_pair
                yield item
                yield_pair[:] = safe_next(c)

            pairs = undone(pairs)
        raise StopIteration

class MultiQuery(Query):
    def __init__(self, queries, *rules, **kw):
        self._queries = queries
        Query.__init__(self, None, *rules, **kw)

    def _iden(self):
        return ''.join(q._iden() for q in self._queries)

    def _cursor(self):
        raise NotImplementedError()

    def _reverse(self):
        for q in self._queries:
            q._reverse()

    def _setdata(self, data):
        for q in self._queries:
            q._data = data

    def _getdata(self):
        if self._queries:
            return self._queries[0]._data

    _data = property(_getdata, _setdata)

    def _setsort(self, sorts):
        for q in self._queries:
            q._sort = deepcopy(sorts)

    def _getsort(self):
        if self._queries:
            return self._queries[0]._sort

    _sort = property(_getsort, _setsort)

    def _filter(self, *rules):
        for q in self._queries:
            q._filter(*rules)

    def _getrules(self):
        return [q._rules for q in self._queries]

    def _setrules(self, rules):
        for q,r in zip(self._queries, rules):
            q._rules = r

    _rules = property(_getrules, _setrules)

    def _getlimit(self):
        return self._queries[0]._limit

    def _setlimit(self, limit):
        for q in self._queries:
            q._limit = limit

    _limit = property(_getlimit, _setlimit)

class Merge(MultiQuery):
    def _cursor(self):
        if (any(q._sort for q in self._queries) and
            not reduce(lambda x,y: (x == y) and x,
                      (q._sort for q in self._queries))):
            raise "The sorts should be the same"

        return MergeCursor((q._cursor() for q in self._queries),
                           self._sort)

def MultiRelation(name, *relations):
    rels_tmp = {}
    for rel in relations:
        t1, t2 = rel._type1, rel._type2
        clsname = name + '_' + t1.__name__.lower() + '_' + t2.__name__.lower()
        cls = new.classobj(clsname, (rel,), {'__module__':t1.__module__})
        setattr(sys.modules[t1.__module__], clsname, cls)
        rels_tmp[(t1, t2)] = cls

    class MultiRelationCls(object):
        c = operators.Slots()
        rels = rels_tmp

        def __init__(self, thing1, thing2, *a, **kw):
            r = self.rel(thing1, thing2)
            self.__class__ = r
            self.__init__(thing1, thing2, *a, **kw)

        @classmethod
        def rel(cls, thing1, thing2):
            t1 = thing1 if isinstance(thing1, ThingMeta) else thing1.__class__
            t2 = thing2 if isinstance(thing2, ThingMeta) else thing2.__class__
            return cls.rels[(t1, t2)]

        @classmethod
        def _query(cls, *rules, **kw):
            #TODO it should be possible to send the rules and kw to
            #the merge constructor
            queries = [r._query(*rules, **kw) for r in cls.rels.values()]
            if "sort" in kw:
                print "sorting MultiRelations is not supported"
            return Merge(queries)

        @classmethod
        def _fast_query(cls, sub, obj, name, data=True, eager_load=True,
                        thing_data=False):
            #divide into types
            def type_dict(items):
                types = {}
                for i in items:
                    types.setdefault(i.__class__, []).append(i)
                return types

            sub_dict = type_dict(tup(sub))
            obj_dict = type_dict(tup(obj))

            #for each pair of types, see if we have a query to send
            res = {}
            for types, rel in cls.rels.iteritems():
                t1, t2 = types
                if sub_dict.has_key(t1) and obj_dict.has_key(t2):
                    res.update(rel._fast_query(sub_dict[t1], obj_dict[t2], name,
                                               data = data, eager_load=eager_load,
                                               thing_data = thing_data))

            return res

    return MultiRelationCls

########NEW FILE########
__FILENAME__ = userrel
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import functools
import types

from r2.lib.memoize import memoize


class UserRelManager(object):
    """Manages access to a relation between a type of thing and users."""

    def __init__(self, name, relation, permission_class):
        self.name = name
        self.relation = relation
        self.permission_class = permission_class

    def get(self, thing, user):
        if user:
            q = self.relation._fast_query([thing], [user], self.name)
            rel = q.get((thing, user, self.name))
            if rel:
                rel._permission_class = self.permission_class
            return rel

    def add(self, thing, user, permissions=None, **attrs):
        if self.get(thing, user):
            return None
        r = self.relation(thing, user, self.name, **attrs)
        if permissions is not None:
            r.set_permissions(permissions)
        r._commit()
        r._permission_class = self.permission_class
        return r

    def remove(self, thing, user):
        r = self.get(thing, user)
        if r:
            r._delete()
            return True
        return False

    def mutate(self, thing, user, **attrs):
        r = self.get(thing, user)
        if r:
            for k, v in attrs.iteritems():
                setattr(r, k, v)
            r._commit()
            r._permission_class = self.permission_class
            return r
        else:
            return self.add(thing, user, **attrs)

    def ids(self, thing):
        return [r._thing2_id for r in self.by_thing(thing)]

    def reverse_ids(self, user):
        q = self.relation._query(self.relation.c._thing2_id == user._id,
                                 self.relation.c._name == self.name)
        return [r._thing1_id for r in q]

    def by_thing(self, thing, **kw):
        for r in self.relation._query(self.relation.c._thing1_id == thing._id,
                                      self.relation.c._name == self.name,
                                      sort='_date', **kw):
            r._permission_class = self.permission_class
            yield r


class MemoizedUserRelManager(UserRelManager):
    """Memoized manager for a relation to users."""

    def __init__(self, name, relation, permission_class,
                 disable_ids_fn=False, disable_reverse_ids_fn=False):
        super(MemoizedUserRelManager, self).__init__(
            name, relation, permission_class)

        self.disable_ids_fn = disable_ids_fn
        self.disable_reverse_ids_fn = disable_reverse_ids_fn
        self.ids_fn_name = self.name + '_ids'
        self.reverse_ids_fn_name = 'reverse_' + self.name + '_ids'

        sup = super(MemoizedUserRelManager, self)
        self.ids = memoize(self.ids_fn_name)(sup.ids)
        self.reverse_ids = memoize(self.reverse_ids_fn_name)(sup.reverse_ids)
        self.add = self._update_caches_on_success(sup.add)
        self.remove = self._update_caches_on_success(sup.remove)

    def _update_caches(self, thing, user):
        if not self.disable_ids_fn:
            self.ids(thing, _update=True)
        if not self.disable_reverse_ids_fn:
            self.reverse_ids(user, _update=True)

    def _update_caches_on_success(self, method):
        @functools.wraps(method)
        def wrapper(thing, user, *args, **kwargs):
            try:
                result = method(thing, user, *args, **kwargs)
            except:
                raise
            else:
                self._update_caches(thing, user)
            return result
        return wrapper


def UserRel(name, relation, disable_ids_fn=False, disable_reverse_ids_fn=False,
            permission_class=None):
    """Mixin for Thing subclasses for managing a relation to users.

    Provides the following suite of methods for a relation named "<relation>":

      - is_<relation>(self, user) - whether user is related to self
      - add_<relation>(self, user) - relates user to self
      - remove_<relation>(self, user) - dissolves relation of user to self

    This suite also usually includes (unless explicitly disabled):

      - <relation>_ids(self) - list of user IDs related to self
      - (static) reverse_<relation>_ids(user) - list of thing IDs user is
          related to
    """
    mgr = MemoizedUserRelManager(
        name, relation, permission_class,
        disable_ids_fn, disable_reverse_ids_fn)

    class UR:
        @classmethod
        def _bind(cls, fn):
            return types.UnboundMethodType(fn, None, cls)

    setattr(UR, 'is_' + name, UR._bind(mgr.get))
    setattr(UR, 'get_' + name, UR._bind(mgr.get))
    setattr(UR, 'add_' + name, UR._bind(mgr.add))
    setattr(UR, 'remove_' + name, UR._bind(mgr.remove))
    setattr(UR, 'each_' + name, UR._bind(mgr.by_thing))
    setattr(UR, name + '_permission_class', permission_class)
    if not disable_ids_fn:
        setattr(UR, mgr.ids_fn_name, UR._bind(mgr.ids))
    if not disable_reverse_ids_fn:
        setattr(UR, mgr.reverse_ids_fn_name, staticmethod(mgr.reverse_ids))

    return UR

########NEW FILE########
__FILENAME__ = emailer
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from email.MIMEText import MIMEText
from email.errors import HeaderParseError
import datetime
import traceback, sys, smtplib

from pylons import c, g

from r2.lib.utils import timeago
from r2.models import Email, DefaultSR, Account, Award
from r2.models.token import EmailVerificationToken, PasswordResetToken


def _feedback_email(email, body, kind, name='', reply_to = ''):
    """Function for handling feedback and ad_inq emails.  Adds an
    email to the mail queue to the feedback email account."""
    Email.handler.add_to_queue(c.user if c.user_is_loggedin else None,
                               g.feedback_email, name, email,
                               kind, body = body, reply_to = reply_to)

def _system_email(email, body, kind, reply_to = "", thing = None):
    """
    For sending email from the system to a user (reply address will be
    feedback and the name will be reddit.com)
    """
    Email.handler.add_to_queue(c.user if c.user_is_loggedin else None,
                               email, g.domain, g.feedback_email,
                               kind, body = body, reply_to = reply_to,
                               thing = thing)

def _nerds_email(body, from_name, kind):
    """
    For sending email to the nerds who run this joint
    """
    Email.handler.add_to_queue(None, g.nerds_email, from_name, g.nerds_email,
                               kind, body = body)

def _gold_email(body, to_address, from_name, kind):
    """
    For sending email to reddit gold subscribers
    """
    Email.handler.add_to_queue(None, to_address, from_name, g.goldthanks_email,
                               kind, body = body)

def verify_email(user, dest=None):
    """
    For verifying an email address
    """
    from r2.lib.pages import VerifyEmail
    user.email_verified = False
    user._commit()
    Award.take_away("verified_email", user)

    token = EmailVerificationToken._new(user)
    emaillink = 'http://' + g.domain + '/verification/' + token._id
    if dest:
        emaillink += '?dest=%s' % dest
    g.log.debug("Generated email verification link: " + emaillink)

    _system_email(user.email,
                  VerifyEmail(user=user,
                              emaillink = emaillink).render(style='email'),
                  Email.Kind.VERIFY_EMAIL)

def password_email(user):
    """
    For resetting a user's password.
    """
    from r2.lib.pages import PasswordReset

    reset_count_key = "email-reset_count_%s" % user._id
    g.cache.add(reset_count_key, 0, time=3600 * 12)
    if g.cache.incr(reset_count_key) > 3:
        return False

    reset_count_global = "email-reset_count_global"
    g.cache.add(reset_count_global, 0, time=3600)
    if g.cache.incr(reset_count_global) > 1000:
        raise ValueError("Somebody's beating the hell out of the password reset box")

    token = PasswordResetToken._new(user)
    base = g.https_endpoint or g.origin
    passlink = base + '/resetpassword/' + token._id
    g.log.info("Generated password reset link: " + passlink)
    _system_email(user.email,
                  PasswordReset(user=user,
                                passlink=passlink).render(style='email'),
                  Email.Kind.RESET_PASSWORD)
    return True

def password_change_email(user):
    """Queues a system email for a password change notification."""
    from r2.lib.pages import PasswordChangeEmail

    return _system_email(user.email,
                         PasswordChangeEmail(user=user).render(style='email'),
                         Email.Kind.PASSWORD_CHANGE)

def email_change_email(user):
    """Queues a system email for a email change notification."""
    from r2.lib.pages import EmailChangeEmail

    return _system_email(user.email,
                         EmailChangeEmail(user=user).render(style='email'),
                         Email.Kind.EMAIL_CHANGE)

def feedback_email(email, body, name='', reply_to = ''):
    """Queues a feedback email to the feedback account."""
    return _feedback_email(email, body,  Email.Kind.FEEDBACK, name = name,
                           reply_to = reply_to)

def ad_inq_email(email, body, name='', reply_to = ''):
    """Queues a ad_inq email to the feedback account."""
    return _feedback_email(email, body,  Email.Kind.ADVERTISE, name = name,
                           reply_to = reply_to)

def gold_email(body, to_address, from_name=g.domain):
    return _gold_email(body, to_address, from_name, Email.Kind.GOLDMAIL)

def nerds_email(body, from_name=g.domain):
    """Queues a feedback email to the nerds running this site."""
    return _nerds_email(body, from_name, Email.Kind.NERDMAIL)

def share(link, emails, from_name = "", reply_to = "", body = ""):
    """Queues a 'share link' email."""
    now = datetime.datetime.now(g.tz)
    ival = now - timeago(g.new_link_share_delay)
    date = max(now,link._date + ival)
    Email.handler.add_to_queue(c.user, emails, from_name, g.share_reply,
                               Email.Kind.SHARE, date = date,
                               body = body, reply_to = reply_to,
                               thing = link)

def send_queued_mail(test = False):
    """sends mail from the mail queue to smtplib for delivery.  Also,
    on successes, empties the mail queue and adds all emails to the
    sent_mail list."""
    from r2.lib.pages import Share, Mail_Opt
    now = datetime.datetime.now(g.tz)
    if not c.site:
        c.site = DefaultSR()

    clear = False
    if not test:
        session = smtplib.SMTP(g.smtp_server)
    def sendmail(email):
        try:
            mimetext = email.to_MIMEText()
            if mimetext is None:
                print ("Got None mimetext for email from %r and to %r"
                       % (email.fr_addr, email.to_addr))
            if test:
                print mimetext.as_string()
            else:
                session.sendmail(email.fr_addr, email.to_addr,
                                 mimetext.as_string())
                email.set_sent(rejected = False)
        # exception happens only for local recipient that doesn't exist
        except (smtplib.SMTPRecipientsRefused, smtplib.SMTPSenderRefused,
                UnicodeDecodeError, AttributeError, HeaderParseError):
            # handle error and print, but don't stall the rest of the queue
            print "Handled error sending mail (traceback to follow)"
            traceback.print_exc(file = sys.stdout)
            email.set_sent(rejected = True)


    try:
        for email in Email.get_unsent(now):
            clear = True

            should_queue = email.should_queue()
            # check only on sharing that the mail is invalid
            if email.kind == Email.Kind.SHARE:
                if should_queue:
                    email.body = Share(username = email.from_name(),
                                       msg_hash = email.msg_hash,
                                       link = email.thing,
                                       body =email.body).render(style = "email")
                else:
                    email.set_sent(rejected = True)
                    continue
            elif email.kind == Email.Kind.OPTOUT:
                email.body = Mail_Opt(msg_hash = email.msg_hash,
                                      leave = True).render(style = "email")
            elif email.kind == Email.Kind.OPTIN:
                email.body = Mail_Opt(msg_hash = email.msg_hash,
                                      leave = False).render(style = "email")
            # handle unknown types here
            elif not email.body:
                email.set_sent(rejected = True)
                continue
            sendmail(email)

    finally:
        if not test:
            session.quit()

    # clear is true if anything was found and processed above
    if clear:
        Email.handler.clear_queue(now)



def opt_out(msg_hash):
    """Queues an opt-out email (i.e., a confirmation that the email
    address has been opted out of receiving any future mail)"""
    email, added =  Email.handler.opt_out(msg_hash)
    if email and added:
        _system_email(email, "", Email.Kind.OPTOUT)
    return email, added

def opt_in(msg_hash):
    """Queues an opt-in email (i.e., that the email has been removed
    from our opt out list)"""
    email, removed =  Email.handler.opt_in(msg_hash)
    if email and removed:
        _system_email(email, "", Email.Kind.OPTIN)
    return email, removed


def _promo_email(thing, kind, body = "", **kw):
    from r2.lib.pages import Promo_Email
    a = Account._byID(thing.author_id, True)
    body = Promo_Email(link = thing, kind = kind,
                       body = body, **kw).render(style = "email")
    return _system_email(a.email, body, kind, thing = thing,
                         reply_to = "selfservicesupport@reddit.com")


def new_promo(thing):
    return _promo_email(thing, Email.Kind.NEW_PROMO)

def promo_bid(thing, bid, start_date):
    return _promo_email(thing, Email.Kind.BID_PROMO, bid = bid,
                        start_date = start_date)

def accept_promo(thing):
    return _promo_email(thing, Email.Kind.ACCEPT_PROMO)

def reject_promo(thing, reason = ""):
    return _promo_email(thing, Email.Kind.REJECT_PROMO, reason)

def queue_promo(thing, bid, trans_id):
    return _promo_email(thing, Email.Kind.QUEUED_PROMO, bid = bid,
                        trans_id = trans_id)

def live_promo(thing):
    return _promo_email(thing, Email.Kind.LIVE_PROMO)

def finished_promo(thing):
    return _promo_email(thing, Email.Kind.FINISHED_PROMO)


def refunded_promo(thing):
    return _promo_email(thing, Email.Kind.REFUNDED_PROMO)


def void_payment(thing, campaign, reason):
    return _promo_email(thing, Email.Kind.VOID_PAYMENT, campaign=campaign,
                        reason=reason)


def send_html_email(to_addr, from_addr, subject, html, subtype="html"):
    from r2.lib.filters import _force_utf8
    msg = MIMEText(_force_utf8(html), subtype)
    msg["Subject"] = subject
    msg["From"] = from_addr
    msg["To"] = to_addr

    session = smtplib.SMTP(g.smtp_server)
    session.sendmail(from_addr, to_addr, msg.as_string())
    session.quit()

########NEW FILE########
__FILENAME__ = emr_helpers
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from copy import copy

from pylons import g

from r2.lib.memoize import memoize

LIVE_STATES = ['RUNNING', 'STARTING', 'WAITING', 'BOOTSTRAPPING']
COMPLETED = 'COMPLETED'
PENDING = 'PENDING'
NOTFOUND = 'NOTFOUND'


def get_compatible_jobflows(emr_connection, bootstrap_actions=None,
                            setup_steps=None):
    """Return jobflows that have specified bootstrap actions and setup steps.

    Assumes there are no conflicts with bootstrap actions or setup steps:
    a jobflow is compatible if it contains at least the requested
    bootstrap_actions and setup_steps (may contain additional).

    """

    bootstrap_actions = bootstrap_actions or []
    setup_steps = setup_steps or []

    jobflows = emr_connection.describe_jobflows(states=LIVE_STATES)
    if not jobflows:
        return []

    # format of step objects returned from describe_jobflows differs from those
    # created locally, so they must be compared carefully
    def args_tuple_emr(step):
        return tuple(sorted(arg.value for arg in step.args))

    def args_tuple_local(step):
        return tuple(sorted(step.args()))

    required_bootstrap_actions = {(step.name, step.path, args_tuple_local(step))
                                  for step in bootstrap_actions}
    required_setup_steps = {(step.name, step.jar(), args_tuple_local(step))
                            for step in setup_steps}

    if not required_bootstrap_actions and not required_setup_steps:
        return jobflows

    running = []
    for jf in jobflows:
        extant_bootstrap_actions = {(step.name, step.path, args_tuple_emr(step))
                                    for step in jf.bootstrapactions}
        if not (required_bootstrap_actions <= extant_bootstrap_actions):
            continue

        extant_setup_steps = {(step.name, step.jar, args_tuple_emr(step))
                              for step in jf.steps}
        if not (required_setup_steps <= extant_setup_steps):
            continue
        running.append(jf)
    return running


@memoize('get_step_states', time=60, timeout=60)
def get_step_states(emr_connection, jobflowid):
    """Return the names and states of all steps in the jobflow.

    Memoized to prevent ratelimiting.

    """

    jobflow = emr_connection.describe_jobflow(jobflowid)

    if jobflow:
        return [(step.name, step.state) for step in jobflow.steps]
    else:
        return []


def get_step_state(emr_connection, jobflowid, step_name, update=False):
    """Return the state of a step.

    If jobflowid/step_name combination is not unique this will return the state
    of the most recent step.

    """

    g.reset_caches()
    steps = get_step_states(emr_connection, jobflowid, _update=update)

    for name, state in reversed(steps):
        if name == step_name:
            return state
    else:
        return NOTFOUND


def get_jobflow_by_name(emr_connection, jobflow_name):
    """Return the most recent jobflow with specified name."""
    jobflows = emr_connection.describe_jobflows(states=LIVE_STATES)

    for jobflow in jobflows:
        if jobflow.name == jobflow_name:
            return jobflow
    else:
        return None


def terminate_jobflow(emr_connection, jobflow_name):
    jobflow = get_jobflow_by_name(emr_connection, jobflow_name)
    if jobflow:
        emr_connection.terminate_jobflow(jobflow.jobflowid)


def modify_slave_count(emr_connection, jobflow_name, num_slaves=1):
    jobflow = get_jobflow_by_name(emr_connection, jobflow_name)
    if not jobflow:
        return

    slave_instancegroupid = None
    slave_instancerequestcount = 0
    for instance in jobflow.instancegroups:
        if instance.name == 'slave':
            slave_instancegroupid = instance.instancegroupid
            slave_instancerequestcount = instance.instancerequestcount
            break

    if slave_instancegroupid and slave_instancerequestcount != num_slaves:
        print ('Modifying slave instance count of %s (%s -> %s)' %
               (jobflow_name, slave_instancerequestcount, num_slaves))
        emr_connection.modify_instance_groups(slave_instancegroupid,
                                              num_slaves)


class EmrJob(object):
    def __init__(self, emr_connection, name, steps=[], setup_steps=[],
                 bootstrap_actions=[], log_uri=None, keep_alive=True,
                 ec2_keyname=None, hadoop_version='1.0.3',
                 ami_version='latest', master_instance_type='m1.small',
                 slave_instance_type='m1.small', num_slaves=1,
                 visible_to_all_users=True):

        self.jobflowid = None
        self.conn = emr_connection
        self.name = name
        self.steps = steps
        self.setup_steps = setup_steps
        self.bootstrap_actions = bootstrap_actions
        self.log_uri = log_uri
        self.enable_debugging = bool(log_uri)
        self.keep_alive = keep_alive
        self.ec2_keyname = ec2_keyname
        self.hadoop_version = hadoop_version
        self.ami_version = ami_version
        self.master_instance_type = master_instance_type
        self.slave_instance_type = slave_instance_type
        self.num_instances = num_slaves + 1
        self.visible_to_all_users = visible_to_all_users

    def run(self):
        steps = copy(self.setup_steps)
        steps.extend(self.steps)

        job_flow_args = dict(name=self.name,
            steps=steps, bootstrap_actions=self.bootstrap_actions,
            keep_alive=self.keep_alive, ec2_keyname=self.ec2_keyname,
            hadoop_version=self.hadoop_version, ami_version=self.ami_version,
            master_instance_type=self.master_instance_type,
            slave_instance_type=self.slave_instance_type,
            num_instances=self.num_instances,
            enable_debugging=self.enable_debugging,
            log_uri=self.log_uri,
            visible_to_all_users=self.visible_to_all_users)

        self.jobflowid = self.conn.run_jobflow(**job_flow_args)
        return

    @property
    def jobflow_state(self):
        if self.jobflowid:
            return self.conn.describe_jobflow(self.jobflowid).state
        else:
            return NOTFOUND

    def terminate(self):
        terminate_jobflow(self.conn, self.name)

    def modify_slave_count(self, num_slaves=1):
        modify_slave_count(self.conn, self.name, num_slaves)


class EmrException(Exception):
    def __init__(self, msg):
        self.msg = msg

    def __str__(self):
        return self.msg

########NEW FILE########
__FILENAME__ = errors
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from webob.exc import HTTPBadRequest, HTTPForbidden, status_map
from r2.lib.utils import Storage, tup
from pylons import request
from pylons.i18n import _
from copy import copy


error_list = dict((
        ('USER_REQUIRED', _("please login to do that")),
        ('HTTPS_REQUIRED', _("this page must be accessed using https")),
        ('WRONG_DOMAIN', _("you can't do that on this domain")),
        ('VERIFIED_USER_REQUIRED', _("you need to set a valid email address to do that.")),
        ('NO_URL', _('a url is required')),
        ('BAD_URL', _('you should check that url')),
        ('INVALID_SCHEME', _('URI scheme must be one of: %(schemes)s')),
        ('BAD_CAPTCHA', _('care to try these again?')),
        ('BAD_USERNAME', _('invalid user name')),
        ('USERNAME_TAKEN', _('that username is already taken')),
        ('USERNAME_TAKEN_DEL', _('that username is taken by a deleted account')),
        ('USER_BLOCKED', _("you can't send to a user that you have blocked")),
        ('NO_THING_ID', _('id not specified')),
        ('TOO_MANY_THING_IDS', _('you provided too many ids')),
        ('NOT_AUTHOR', _("you can't do that")),
        ('NOT_USER', _("you are not logged in as that user")),
        ('NOT_FRIEND', _("you are not friends with that user")),
        ('LOGGED_IN', _("you are already logged in")),
        ('DELETED_LINK', _('the link you are commenting on has been deleted')),
        ('DELETED_COMMENT', _('that comment has been deleted')),
        ('DELETED_THING', _('that element has been deleted')),
        ('BAD_PASSWORD', _('that password is unacceptable')),
        ('WRONG_PASSWORD', _('invalid password')),
        ('BAD_PASSWORD_MATCH', _('passwords do not match')),
        ('NO_NAME', _('please enter a name')),
        ('NO_EMAIL', _('please enter an email address')),
        ('NO_EMAIL_FOR_USER', _('no email address for that user')),
        ('NO_VERIFIED_EMAIL', _('no verified email address for that user')),
        ('NO_TO_ADDRESS', _('send it to whom?')),
        ('NO_SUBJECT', _('please enter a subject')),
        ('USER_DOESNT_EXIST', _("that user doesn't exist")),
        ('NO_USER', _('please enter a username')),
        ('INVALID_PREF', _("that preference isn't valid")),
        ('NON_PREFERENCE', _("'%(choice)s' is not a user preference field")),
        ('INVALID_LANG', _("that language is not available")),
        ('BAD_NUMBER', _("that number isn't in the right range (%(range)s)")),
        ('BAD_STRING', _("you used a character here that we can't handle")),
        ('BAD_BID', _("your budget must be at least $%(min)d and no more than $%(max)d.")),
        ('ALREADY_SUB', _("that link has already been submitted")),
        ('SUBREDDIT_EXISTS', _('that subreddit already exists')),
        ('SUBREDDIT_NOEXIST', _('that subreddit doesn\'t exist')),
        ('SUBREDDIT_NOTALLOWED', _("you aren't allowed to post there.")),
        ('SUBREDDIT_REQUIRED', _('you must specify a subreddit')),
        ('BAD_SR_NAME', _('that name isn\'t going to work')),
        ('RATELIMIT', _('you are doing that too much. try again in %(time)s.')),
        ('QUOTA_FILLED', _("You've submitted too many links recently. Please try again in an hour.")),
        ('SUBREDDIT_RATELIMIT', _("you are doing that too much. try again later.")),
        ('EXPIRED', _('your session has expired')),
        ('DRACONIAN', _('you must accept the terms first')),
        ('BANNED_IP', "IP banned"),
        ('BAD_CNAME', "that domain isn't going to work"),
        ('USED_CNAME', "that domain is already in use"),
        ('INVALID_OPTION', _('that option is not valid')),
        ('BAD_EMAILS', _('the following emails are invalid: %(emails)s')),
        ('NO_EMAILS', _('please enter at least one email address')),
        ('TOO_MANY_EMAILS', _('please only share to %(num)s emails at a time.')),
        ('OVERSOLD', _('that subreddit has already been oversold on %(start)s to %(end)s. Please pick another subreddit or date.')),
        ('OVERSOLD_DETAIL', _("We have insufficient inventory to fulfill your requested budget, target, and dates. Only %(available)s impressions available on %(target)s from %(start)s to %(end)s.")),
        ('BAD_DATE', _('please provide a date of the form mm/dd/yyyy')),
        ('BAD_DATE_RANGE', _('the dates need to be in order and not identical')),
        ('DATE_RANGE_TOO_LARGE', _('you must choose a date range of less than %(days)s days')),
        ('DATE_TOO_LATE', _('please enter a date %(day)s or earlier')),
        ('DATE_TOO_EARLY', _('please enter a date %(day)s or later')),
        ('BAD_ADDRESS', _('address problem: %(message)s')),
        ('BAD_CARD', _('card problem: %(message)s')),
        ('TOO_LONG', _("this is too long (max: %(max_length)s)")),
        ('NO_TEXT', _('we need something here')),
        ('INVALID_CODE', _("we've never seen that code before")),
        ('CLAIMED_CODE', _("that code has already been claimed -- perhaps by you?")),
        ('NO_SELFS', _("that subreddit doesn't allow text posts")),
        ('NO_LINKS', _("that subreddit only allows text posts")),
        ('TOO_OLD', _("that's a piece of history now; it's too late to reply to it")),
        ('BAD_CSS_NAME', _('invalid css name')),
        ('BAD_CSS', _('invalid css')),
        ('BAD_REVISION', _('invalid revision ID')),
        ('TOO_MUCH_FLAIR_CSS', _('too many flair css classes')),
        ('BAD_FLAIR_TARGET', _('not a valid flair target')),
        ('OAUTH2_INVALID_CLIENT', _('invalid client id')),
        ('OAUTH2_INVALID_REDIRECT_URI', _('invalid redirect_uri parameter')),
        ('OAUTH2_INVALID_SCOPE', _('invalid scope requested')),
        ('OAUTH2_INVALID_REFRESH_TOKEN', _('invalid refresh token')),
        ('OAUTH2_ACCESS_DENIED', _('access denied by the user')),
        ('CONFIRM', _("please confirm the form")),
        ('CONFLICT', _("conflict error while saving")),
        ('NO_API', _('cannot perform this action via the API')),
        ('DOMAIN_BANNED', _('%(domain)s is not allowed on reddit: %(reason)s')),
        ('NO_OTP_SECRET', _('you must enable two-factor authentication')),
        ('BAD_IMAGE', _('image problem')),
        ('DEVELOPER_ALREADY_ADDED', _('already added')),
        ('TOO_MANY_DEVELOPERS', _('too many developers')),
        ('BAD_HASH', _("i don't believe you.")),
        ('ALREADY_MODERATOR', _('that user is already a moderator')),
        ('NO_INVITE_FOUND', _('there is no pending invite for that subreddit')),
        ('BID_LIVE', _('you cannot edit the budget of a live ad')),
        ('TOO_MANY_CAMPAIGNS', _('you have too many campaigns for that promotion')),
        ('BAD_JSONP_CALLBACK', _('that jsonp callback contains invalid characters')),
        ('INVALID_PERMISSION_TYPE', _("permissions don't apply to that type of user")),
        ('INVALID_PERMISSIONS', _('invalid permissions string')),
        ('BAD_MULTI_PATH', _('invalid multi path')),
        ('BAD_MULTI_NAME', _('%(reason)s')),
        ('MULTI_NOT_FOUND', _('that multireddit doesn\'t exist')),
        ('MULTI_EXISTS', _('that multireddit already exists')),
        ('MULTI_CANNOT_EDIT', _('you can\'t change that multireddit')),
        ('MULTI_TOO_MANY_SUBREDDITS', _('no more space for subreddits in that multireddit')),
        ('MULTI_SPECIAL_SUBREDDIT', _("can't add special subreddit %(path)s")),
        ('JSON_PARSE_ERROR', _('unable to parse JSON data')),
        ('JSON_INVALID', _('unexpected JSON structure')),
        ('JSON_MISSING_KEY', _('JSON missing key: "%(key)s"')),
        ('NO_CHANGE_KIND', _("can't change post type")),
        ('INVALID_LOCATION', _("invalid location")),
        ('BANNED_FROM_SUBREDDIT', _('that user is banned from the subreddit')),
        ('GOLD_REQUIRED', _('you must have an active reddit gold subscription to do that')),
    ))

errors = Storage([(e, e) for e in error_list.keys()])


def add_error_codes(new_codes):
    """Add error codes to the error enumeration.

    It is assumed that the incoming messages are marked for translation but not
    yet translated, so they can be declared before pylons.i18n is ready.

    """
    for code, message in new_codes.iteritems():
        error_list[code] = _(message)
        errors[code] = code


class RedditError(Exception):
    name = None
    fields = None
    code = None

    def __init__(self, name=None, msg_params=None, fields=None, code=None):
        Exception.__init__(self)

        if name is not None:
            self.name = name

        self.i18n_message = error_list.get(self.name)
        self.msg_params = msg_params or {}

        if fields is not None:
            # list of fields in the original form that caused the error
            self.fields = tup(fields)

        if code is not None:
            self.code = code

    @property
    def message(self):
        return _(self.i18n_message) % self.msg_params

    def __iter__(self):
        yield ('name', self.name)
        yield ('message', _(self.message))

    def __repr__(self):
        return '<RedditError: %s>' % self.name

    def __str__(self):
        return repr(self)


class ErrorSet(object):
    def __init__(self):
        self.errors = {}

    def __contains__(self, pair):
        """Expects an (error_name, field_name) tuple and checks to
        see if it's in the errors list."""
        return self.errors.has_key(pair)

    def get(self, name, default=None):
        return self.errors.get(name, default)

    def __getitem__(self, name):
        return self.errors[name]

    def __repr__(self):
        return "<ErrorSet %s>" % list(self)

    def __iter__(self):
        for x in self.errors:
            yield x

    def __len__(self):
        return len(self.errors)

    def add(self, error_name, msg_params=None, field=None, code=None):
        for field_name in tup(field):
            e = RedditError(error_name, msg_params, fields=field_name,
                            code=code)
            self.add_error(e)

    def add_error(self, error):
        for field_name in tup(error.fields):
            self.errors[(error.name, field_name)] = error

    def remove(self, pair):
        """Expects an (error_name, field_name) tuple and removes it
        from the errors list."""
        if self.errors.has_key(pair):
            del self.errors[pair]


class ForbiddenError(HTTPForbidden):
    def __init__(self, error_name):
        HTTPForbidden.__init__(self)
        self.explanation = error_list[error_name]


class BadRequestError(HTTPBadRequest):
    def __init__(self, error_name):
        HTTPBadRequest.__init__(self)
        self.error_data = {
            'reason': error_name,
            'explanation': error_list[error_name],
        }


def reddit_http_error(code=400, error_name='UNKNOWN_ERROR', **data):
    exc = status_map[code]()

    data['reason'] = exc.explanation = error_name
    if 'explanation' not in data and error_name in error_list:
        data['explanation'] = exc.explanation = error_list[error_name]

    # omit 'fields' json attribute if it is empty
    if 'fields' in data and not data['fields']:
        del data['fields']

    exc.error_data = data
    return exc


class UserRequiredException(RedditError):
    name = errors.USER_REQUIRED
    code = 403


class VerifiedUserRequiredException(RedditError):
    name = errors.VERIFIED_USER_REQUIRED
    code = 403


class MessageError(Exception): pass

########NEW FILE########
__FILENAME__ = export
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import sys

__all__ = ["export", "ExportError"]


class ExportError(Exception):
    def __init__(self, module):
        msg = "Missing __all__ declaration in module %s.  " \
              "@export cannot be used without declaring __all__ " \
              "in that module." % (module)
        Exception.__init__(self, msg)


def export(exported_entity):
    """Use a decorator to avoid retyping function/class names.
  
    * Based on an idea by Duncan Booth:
    http://groups.google.com/group/comp.lang.python/msg/11cbb03e09611b8a
    * Improved via a suggestion by Dave Angel:
    http://groups.google.com/group/comp.lang.python/msg/3d400fb22d8a42e1
    * Copied from Stack Overflow
    http://stackoverflow.com/questions/6206089/is-it-a-good-practice-to-add-names-to-all-using-a-decorator
    """
    all_var = sys.modules[exported_entity.__module__].__dict__.get('__all__')
    if all_var is None:
        raise ExportError(exported_entity.__module__)
    if exported_entity.__name__ not in all_var:  # Prevent duplicates if run from an IDE.
        all_var.append(exported_entity.__name__)
    return exported_entity

export(export)  # Emulate decorating ourself


########NEW FILE########
__FILENAME__ = filters
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import cgi
import os
import urllib
import re

from collections import Counter

import snudown
from cStringIO import StringIO

from xml.sax.handler import ContentHandler
from lxml.sax import saxify
import lxml.etree
from BeautifulSoup import BeautifulSoup, Tag

from pylons import g, c

from wrapped import Templated, CacheStub

SC_OFF = "<!-- SC_OFF -->"
SC_ON = "<!-- SC_ON -->"

MD_START = '<div class="md">'
MD_END = '</div>'

WIKI_MD_START = '<div class="md wiki">'
WIKI_MD_END = '</div>'

custom_img_url = re.compile(r'\A%%([a-zA-Z0-9\-]+)%%$')

def python_websafe(text):
    return text.replace('&', "&amp;").replace("<", "&lt;").replace(">", "&gt;").replace('"', "&quot;")

def python_websafe_json(text):
    return text.replace('&', "&amp;").replace("<", "&lt;").replace(">", "&gt;")

try:
    from Cfilters import uwebsafe as c_websafe, uspace_compress, \
        uwebsafe_json as c_websafe_json
    def spaceCompress(text):
        try:
            text = unicode(text, 'utf-8')
        except TypeError:
            text = unicode(text)
        return uspace_compress(text)
except ImportError:
    c_websafe      = python_websafe
    c_websafe_json = python_websafe_json
    _between_tags1 = re.compile('> +')
    _between_tags2 = re.compile(' +<')
    _spaces = re.compile('[\s]+')
    _ignore = re.compile('(' + SC_OFF + '|' + SC_ON + ')', re.S | re.I)
    def spaceCompress(content):
        res = ''
        sc = True
        for p in _ignore.split(content):
            if p == SC_ON:
                sc = True
            elif p == SC_OFF:
                sc = False
            elif sc:
                p = _spaces.sub(' ', p)
                p = _between_tags1.sub('>', p)
                p = _between_tags2.sub('<', p)
                res += p
            else:
                res += p

        return res

class _Unsafe(unicode): pass

def _force_unicode(text):
    if text == None:
        return u''

    if isinstance(text, unicode):
        return text

    try:
        text = unicode(text, 'utf-8')
    except UnicodeDecodeError:
        text = unicode(text, 'latin1')
    except TypeError:
        text = unicode(text)
    return text

def _force_utf8(text):
    return str(_force_unicode(text).encode('utf8'))

def unsafe(text=''):
    return _Unsafe(_force_unicode(text))

def websafe_json(text=""):
    return c_websafe_json(_force_unicode(text))

def mako_websafe(text = ''):
    if text.__class__ == _Unsafe:
        return text
    elif isinstance(text, Templated):
        return _Unsafe(text.render())
    elif isinstance(text, CacheStub):
        return _Unsafe(text)
    elif text is None:
        return ""
    elif text.__class__ != unicode:
        text = _force_unicode(text)
    return c_websafe(text)

def websafe(text=''):
    if text.__class__ != unicode:
        text = _force_unicode(text)
    #wrap the response in _Unsafe so make_websafe doesn't unescape it
    return _Unsafe(c_websafe(text))


valid_link_schemes = (
    '/',
    '#',
    'http://',
    'https://',
    'ftp://',
    'mailto:',
    'steam://',
    'irc://',
    'ircs://',
    'news://',
    'mumble://',
    'ssh://',
    'git://',
    'ts3server://',
)

class SouptestSaxHandler(ContentHandler):
    def __init__(self, ok_tags):
        self.ok_tags = ok_tags

    def startElementNS(self, tagname, qname, attrs):
        if qname not in self.ok_tags:
            raise ValueError('HAX: Unknown tag: %r' % qname)

        for (ns, name), val in attrs.items():
            if ns is not None:
                raise ValueError('HAX: Unknown namespace? Seriously? %r' % ns)

            if name not in self.ok_tags[qname]:
                raise ValueError('HAX: Unknown attribute-name %r' % name)

            if qname == 'a' and name == 'href':
                lv = val.lower()
                if not any(lv.startswith(scheme) for scheme in valid_link_schemes):
                    raise ValueError('HAX: Unsupported link scheme %r' % val)

markdown_ok_tags = {
    'div': ('class'),
    'a': set(('href', 'title', 'target', 'nofollow', 'rel')),
    'img': set(('src', 'alt')),
    }

markdown_boring_tags =  ('p', 'em', 'strong', 'br', 'ol', 'ul', 'hr', 'li',
                         'pre', 'code', 'blockquote', 'center',
                          'sup', 'del', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6',)

markdown_user_tags = ('table', 'th', 'tr', 'td', 'tbody',
                     'tbody', 'thead', 'tr', 'tfoot', 'caption')

for bt in markdown_boring_tags:
    markdown_ok_tags[bt] = ('id', 'class')

for bt in markdown_user_tags:
    markdown_ok_tags[bt] = ('colspan', 'rowspan', 'cellspacing', 'cellpadding', 'align', 'scope')

markdown_xhtml_dtd_path = os.path.join(
    os.path.dirname(os.path.abspath(__file__)),
    'contrib/dtds/xhtml.dtd')

markdown_dtd = '<!DOCTYPE div- SYSTEM "file://%s">' % markdown_xhtml_dtd_path

def markdown_souptest(text, nofollow=False, target=None, renderer='reddit'):
    if not text:
        return text
    
    if renderer == 'reddit':
        smd = safemarkdown(text, nofollow=nofollow, target=target)
    elif renderer == 'wiki':
        smd = wikimarkdown(text)

    # Prepend a DTD reference so we can load up definitions of all the standard
    # XHTML entities (&nbsp;, etc.).
    smd_with_dtd = markdown_dtd + smd

    s = StringIO(smd_with_dtd)
    parser = lxml.etree.XMLParser(load_dtd=True)
    tree = lxml.etree.parse(s, parser)
    handler = SouptestSaxHandler(markdown_ok_tags)
    saxify(tree, handler)

    return smd

#TODO markdown should be looked up in batch?
#@memoize('markdown')
def safemarkdown(text, nofollow=False, wrap=True, **kwargs):
    if not text:
        return None

    # this lets us skip the c.cname lookup (which is apparently quite
    # slow) if target was explicitly passed to this function.
    target = kwargs.get("target", None)
    if "target" not in kwargs and c.cname:
        target = "_top"

    text = snudown.markdown(_force_utf8(text), nofollow, target)

    if wrap:
        return SC_OFF + MD_START + text + MD_END + SC_ON
    else:
        return SC_OFF + text + SC_ON

def wikimarkdown(text, include_toc=True, target=None):
    from r2.lib.template_helpers import media_https_if_secure

    # this hard codes the stylesheet page for now, but should be parameterized
    # in the future to allow per-page images.
    from r2.models.wiki import ImagesByWikiPage
    page_images = ImagesByWikiPage.get_images(c.site, "config/stylesheet")
    
    def img_swap(tag):
        name = tag.get('src')
        name = custom_img_url.search(name)
        name = name and name.group(1)
        if name and name in page_images:
            url = page_images[name]
            url = media_https_if_secure(url)
            tag['src'] = url
        else:
            tag.extract()
    
    nofollow = True
    
    text = snudown.markdown(_force_utf8(text), nofollow, target,
                            renderer=snudown.RENDERER_WIKI)
    
    # TODO: We should test how much of a load this adds to the app
    soup = BeautifulSoup(text.decode('utf-8'))
    images = soup.findAll('img')
    
    if images:
        [img_swap(image) for image in images]
    
    if include_toc:
        tocdiv = generate_table_of_contents(soup, prefix="wiki")
        if tocdiv:
            soup.insert(0, tocdiv)
    
    text = str(soup)
    
    return SC_OFF + WIKI_MD_START + text + WIKI_MD_END + SC_ON

title_re = re.compile('[^\w.-]')
header_re = re.compile('^h[1-6]$')
def generate_table_of_contents(soup, prefix):
    header_ids = Counter()
    headers = soup.findAll(header_re)
    if not headers:
        return
    tocdiv = Tag(soup, "div", [("class", "toc")])
    parent = Tag(soup, "ul")
    parent.level = 0
    tocdiv.append(parent)
    level = 0
    previous = 0
    for header in headers:
        contents = u''.join(header.findAll(text=True))
        
        # In the event of an empty header, skip
        if not contents:
            continue
        
        # Convert html entities to avoid ugly header ids
        aid = unicode(BeautifulSoup(contents, convertEntities=BeautifulSoup.XML_ENTITIES))
        # Prefix with PREFIX_ to avoid ID conflict with the rest of the page
        aid = u'%s_%s' % (prefix, aid.replace(" ", "_").lower())
        # Convert down to ascii replacing special characters with hex
        aid = str(title_re.sub(lambda c: '.%X' % ord(c.group()), aid))
        
        # Check to see if a tag with the same ID exists
        id_num = header_ids[aid] + 1
        header_ids[aid] += 1
        # Only start numbering ids with the second instance of an id
        if id_num > 1:
            aid = '%s%d' % (aid, id_num)
        
        header['id'] = aid
        
        li = Tag(soup, "li", [("class", aid)])
        a = Tag(soup, "a", [("href", "#%s" % aid)])
        a.string = contents
        li.append(a)
        
        thislevel = int(header.name[-1])
        
        if previous and thislevel > previous:
            newul = Tag(soup, "ul")
            newul.level = thislevel
            parent.append(newul)
            parent = newul
            level += 1
        elif level and thislevel < previous:
            while level and parent.level > thislevel:
                parent = parent.findParent("ul")
                level -= 1
        
        previous = thislevel
        parent.append(li)
    
    return tocdiv


def keep_space(text):
    text = websafe(text)
    for i in " \n\r\t":
        text=text.replace(i,'&#%02d;' % ord(i))
    return unsafe(text)


def unkeep_space(text):
    return text.replace('&#32;', ' ').replace('&#10;', '\n').replace('&#09;', '\t')

########NEW FILE########
__FILENAME__ = geoip
#!/usr/bin/python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import datetime
import httplib
import json
import os
import socket
import urllib2

from pylons import g

from r2.lib.cache import sgm
from r2.lib.utils import in_chunks, tup

# If the geoip service has nginx in front of it there is a default limit of 8kb:
#   http://wiki.nginx.org/NginxHttpCoreModule#large_client_header_buffers
# >>> len('GET /geoip/' + '+'.join(['255.255.255.255'] * 500) + ' HTTP/1.1')
# 8019
MAX_IPS_PER_GROUP = 500

GEOIP_CACHE_TIME = datetime.timedelta(days=7).total_seconds()

def _location_by_ips(ips):
    if not hasattr(g, 'geoip_location'):
        g.log.warning("g.geoip_location not set. skipping GeoIP lookup.")
        return {}

    ret = {}
    for batch in in_chunks(ips, MAX_IPS_PER_GROUP):
        ip_string = '+'.join(batch)
        url = os.path.join(g.geoip_location, 'geoip', ip_string)

        try:
            response = urllib2.urlopen(url=url, timeout=3)
            json_data = response.read()
        except (urllib2.URLError, httplib.HTTPException, socket.error) as e:
            g.log.warning("Failed to fetch GeoIP information: %r" % e)
            continue

        try:
            ret.update(json.loads(json_data))
        except ValueError, e:
            g.log.warning("Invalid JSON response for GeoIP lookup: %r" % e)
            continue
    return ret


def _organization_by_ips(ips):
    if not hasattr(g, 'geoip_location'):
        g.log.warning("g.geoip_location not set. skipping GeoIP lookup.")
        return {}

    ip_string = '+'.join(set(ips))
    url = os.path.join(g.geoip_location, 'org', ip_string)

    try:
        response = urllib2.urlopen(url=url, timeout=3)
        json_data = response.read()
    except urllib2.URLError, e:
        g.log.warning("Failed to fetch GeoIP information: %r" % e)
        return {}

    try:
        return json.loads(json_data)
    except ValueError, e:
        g.log.warning("Invalid JSON response for GeoIP lookup: %r" % e)
        return {}


def location_by_ips(ips):
    ips, is_single = tup(ips, ret_is_single=True)
    location_by_ip = sgm(g.cache, ips, miss_fn=_location_by_ips,
                         prefix='location_by_ip',
                         time=GEOIP_CACHE_TIME)
    if is_single and location_by_ip:
        return location_by_ip[ips[0]]
    else:
        return location_by_ip


def organization_by_ips(ips):
    ips, is_single = tup(ips, ret_is_single=True)
    organization_by_ip = sgm(g.cache, ips, miss_fn=_organization_by_ips,
                             prefix='organization_by_ip',
                             time=GEOIP_CACHE_TIME)
    if is_single and organization_by_ip:
        return organization_by_ip[ips[0]]
    else:
        return organization_by_ip

########NEW FILE########
__FILENAME__ = gzipper
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import cStringIO
import gzip
import wsgiref.headers

from paste.util.mimeparse import parse_mime_type, desired_matches


ENCODABLE_CONTENT_TYPES = {
    "application/json",
    "application/javascript",
    "application/xml",
    "text/css",
    "text/csv",
    "text/html",
    "text/javascript",
    "text/plain",
    "text/xml",
}


class GzipMiddleware(object):
    """A middleware that transparently compresses content with gzip.

    Note: this middleware deliberately violates PEP-333 in three ways:

        - it disables the use of the "write()" callable.
        - it does content encoding which is a "hop-by-hop" feature.
        - it does not "yield at least one value each time its underlying
          application yields a value".

    None of these are an issue for the reddit application, but use at your
    own risk.

    """

    def __init__(self, app, compression_level, min_size):
        self.app = app
        self.compression_level = compression_level
        self.min_size = min_size

    def _start_response(self, status, response_headers, exc_info=None):
        self.status = status
        self.headers = response_headers
        self.exc_info = exc_info
        return self._write_not_implemented

    @staticmethod
    def _write_not_implemented(*args, **kwargs):
        """Raise an exception.

        This middleware doesn't work with the write callable.

        """
        raise NotImplementedError

    @staticmethod
    def content_length(headers, app_iter):
        """Return the content-length of this response as best as we can tell.

        If the application returned a Content-Length header we will trust it.
        If not, we are allowed by PEP-333 to attempt to determine the length of
        the app's iterable and if it's 1, use the length of the only chunk as
        the content-length.

        """
        content_length_header = headers["Content-Length"]

        if content_length_header:
            return int(content_length_header)

        try:
            app_iter_len = len(app_iter)
        except ValueError:
            return None  # streaming response; we're done here.

        if app_iter_len == 1:
            return len(app_iter[0])
        return None

    def should_gzip_response(self, headers, app_iter):
        # this middleware isn't smart enough to deal with stuff like ETags or
        # content ranges at the moment. let's just bail out. (this prevents
        # issues with pylons/paste's static file middleware)
        if "ETag" in headers:
            return False

        # here we are, violating pep-333 by looking at a hop-by-hop header
        # within the middleware chain. but this will prevent us from overriding
        # encoding done lower down in the app, if present. so it goes.
        if "Content-Encoding" in headers:
            return False

        # bail if we can't figure out how big it is or it's too small
        content_length = self.content_length(headers, app_iter)
        if not content_length or content_length < self.min_size:
            return False

        # make sure this is one of the content-types we're allowed to encode
        content_type = headers["Content-Type"]
        type, subtype, params = parse_mime_type(content_type)
        if "%s/%s" % (type, subtype) not in ENCODABLE_CONTENT_TYPES:
            return False

        return True

    @staticmethod
    def update_vary_header(headers):
        vary_headers = headers.get_all("Vary")
        del headers["Vary"]

        varies = []
        for vary_header in vary_headers:
            varies.extend(field.strip().lower()
                          for field in vary_header.split(","))

        if "*" in varies:
            varies = ["*"]
        elif "accept-encoding" not in varies:
            varies.append("accept-encoding")

        headers["Vary"] = ", ".join(varies)

    @staticmethod
    def request_accepts_gzip(environ):
        accept_encoding = environ.get("HTTP_ACCEPT_ENCODING", "identity")
        return "gzip" in desired_matches(["gzip"], accept_encoding)

    def __call__(self, environ, start_response):
        app_iter = self.app(environ, self._start_response)
        headers = wsgiref.headers.Headers(self.headers)

        response_compressible = self.should_gzip_response(headers, app_iter)
        if response_compressible:
            # this means that the sole factor left in determining whether or
            # not to gzip is the Accept-Encoding header; we should let
            # downstream caches know that this is the case with the Vary header
            self.update_vary_header(headers)

        if response_compressible and self.request_accepts_gzip(environ):
            headers["Content-Encoding"] = "gzip"

            response_buffer = cStringIO.StringIO()
            gzipper = gzip.GzipFile(fileobj=response_buffer, mode="wb",
                                    compresslevel=self.compression_level)
            try:
                for chunk in app_iter:
                    gzipper.write(chunk)
            finally:
                if hasattr(app_iter, "close"):
                    app_iter.close()

            gzipper.close()
            new_response = response_buffer.getvalue()
            encoded_app_iter = [new_response]
            response_buffer.close()

            headers["Content-Length"] = str(len(new_response))
        else:
            encoded_app_iter = app_iter

        # send the response
        start_response(self.status, self.headers, self.exc_info)
        return encoded_app_iter


def make_gzip_middleware(app, global_conf=None, compress_level=9, min_size=0):
    """Return a gzip-compressing middleware."""
    return GzipMiddleware(app, int(compress_level), int(min_size))

########NEW FILE########
__FILENAME__ = hardcachebackend
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import g
from datetime import timedelta as timedelta
from datetime import datetime
import sqlalchemy as sa
from r2.lib.db.tdb_lite import tdb_lite
import pytz
import random

COUNT_CATEGORY = 'hc_count'
ELAPSED_CATEGORY = 'hc_elapsed'
TZ = pytz.timezone("MST")

def expiration_from_time(time):
    if time <= 0:
        raise ValueError ("HardCache items *must* have an expiration time")
    return datetime.now(TZ) + timedelta(0, time)

class HardCacheBackend(object):
    def __init__(self, gc):
        self.tdb = tdb_lite(gc)
        self.profile_categories = {}
        TZ = gc.display_tz

        def _table(metadata):
            return sa.Table(gc.db_app_name + '_hardcache', metadata,
                            sa.Column('category', sa.String, nullable = False,
                                      primary_key = True),
                            sa.Column('ids', sa.String, nullable = False,
                                      primary_key = True),
                            sa.Column('value', sa.String, nullable = False),
                            sa.Column('kind', sa.String, nullable = False),
                            sa.Column('expiration',
                                      sa.DateTime(timezone = True),
                                      nullable = False)
                            )
        enginenames_by_category = {}
        all_enginenames = set()
        for item in gc.hardcache_categories:
            chunks = item.split(":")
            if len(chunks) < 2:
                raise ValueError("Invalid hardcache_overrides")
            category = chunks.pop(0)
            enginenames_by_category[category] = []
            for c in chunks:
                if c == '!profile':
                    self.profile_categories[category] = True
                elif c.startswith("!"):
                    raise ValueError("WTF is [%s] in hardcache_overrides?" % c)
                else:
                    all_enginenames.add(c)
                    enginenames_by_category[category].append(c)

        assert('*' in enginenames_by_category.keys())

        engines_by_enginename = {}
        for enginename in all_enginenames:
            engine = gc.dbm.get_engine(enginename)
            md = self.tdb.make_metadata(engine)
            table = _table(md)
            indstr = self.tdb.index_str(table, 'expiration', 'expiration')
            self.tdb.create_table(table, [ indstr ])
            engines_by_enginename[enginename] = table

        self.mapping = {}
        for category, enginenames in enginenames_by_category.iteritems():
            self.mapping[category] = [ engines_by_enginename[e]
                                       for e in enginenames]

    def engine_by_category(self, category, type="master"):
        if category not in self.mapping:
            category = '*'
        engines = self.mapping[category]
        if type == 'master':
            return engines[0]
        elif type == 'readslave':
            return random.choice(engines[1:])
        else:
            raise ValueError("invalid type %s" % type)

    def profile_start(self, operation, category):
        if category == COUNT_CATEGORY:
            return None

        if category == ELAPSED_CATEGORY:
            return None

        if category in self.mapping:
            effective_category = category
        else:
            effective_category = '*'

        if effective_category not in self.profile_categories:
            return None

        return (datetime.now(TZ), operation, category)

    def profile_stop(self, t):
        if t is None:
            return

        start_time, operation, category = t

        end_time = datetime.now(TZ)

        period = end_time.strftime("%Y/%m/%d_%H:%M")[:-1] + 'x'

        elapsed = end_time - start_time
        msec = elapsed.seconds * 1000 + elapsed.microseconds / 1000

        ids = "-".join((operation, category, period))

        self.add(COUNT_CATEGORY, ids, 0, time=86400)
        self.add(ELAPSED_CATEGORY, ids, 0, time=86400)

        self.incr(COUNT_CATEGORY, ids, time=86400)
        self.incr(ELAPSED_CATEGORY, ids, time=86400, delta=msec)


    def set(self, category, ids, val, time):

        self.delete(category, ids) # delete it if it already exists

        value, kind = self.tdb.py2db(val, True)

        expiration = expiration_from_time(time)

        prof = self.profile_start('set', category)

        engine = self.engine_by_category(category, "master")

        engine.insert().execute(
            category=category,
            ids=ids,
            value=value,
            kind=kind,
            expiration=expiration
            )

        self.profile_stop(prof)

    def add(self, category, ids, val, time=0):
        self.delete_if_expired(category, ids)

        expiration = expiration_from_time(time)

        value, kind = self.tdb.py2db(val, True)

        prof = self.profile_start('add', category)

        engine = self.engine_by_category(category, "master")

        try:
            rp = engine.insert().execute(
                category=category,
                ids=ids,
                value=value,
                kind=kind,
                expiration=expiration
                )
            self.profile_stop(prof)
            return value

        except sa.exc.IntegrityError, e:
            self.profile_stop(prof)
            return self.get(category, ids, force_write_table=True)

    def incr(self, category, ids, time=0, delta=1):
        self.delete_if_expired(category, ids)

        expiration = expiration_from_time(time)

        prof = self.profile_start('incr', category)

        engine = self.engine_by_category(category, "master")

        rp = engine.update(sa.and_(engine.c.category==category,
                                   engine.c.ids==ids,
                                   engine.c.kind=='num'),
                           values = {
                                   engine.c.value:
                                           sa.cast(
                                           sa.cast(engine.c.value, sa.Integer)
                                           + delta, sa.String),
                                   engine.c.expiration: expiration
                                   }
                           ).execute()

        self.profile_stop(prof)

        if rp.rowcount == 1:
            return self.get(category, ids, force_write_table=True)
        elif rp.rowcount == 0:
            existing_value = self.get(category, ids, force_write_table=True)
            if existing_value is None:
                raise ValueError("[%s][%s] can't be incr()ed -- it's not set" %
                                 (category, ids))
            else:
                raise ValueError("[%s][%s] has non-integer value %r" %
                                 (category, ids, existing_value))
        else:
            raise ValueError("Somehow %d rows got updated" % rp.rowcount)

    def get(self, category, ids, force_write_table=False):
        if force_write_table:
            type = "master"
        else:
            type = "readslave"

        engine = self.engine_by_category(category, type)

        prof = self.profile_start('get', category)

        s = sa.select([engine.c.value,
                       engine.c.kind,
                       engine.c.expiration],
                      sa.and_(engine.c.category==category,
                              engine.c.ids==ids),
                      limit = 1)
        rows = s.execute().fetchall()

        self.profile_stop(prof)

        if len(rows) < 1:
            return None
        elif rows[0].expiration < datetime.now(TZ):
            return None
        else:
            return self.tdb.db2py(rows[0].value, rows[0].kind)

    def get_multi(self, category, idses):
        prof = self.profile_start('get_multi', category)

        engine = self.engine_by_category(category, "readslave")

        s = sa.select([engine.c.ids,
                       engine.c.value,
                       engine.c.kind,
                       engine.c.expiration],
                      sa.and_(engine.c.category==category,
                              sa.or_(*[engine.c.ids==ids
                                       for ids in idses])))
        rows = s.execute().fetchall()

        self.profile_stop(prof)

        results = {}

        for row in rows:
          if row.expiration >= datetime.now(TZ):
              k = "%s-%s" % (category, row.ids)
              results[k] = self.tdb.db2py(row.value, row.kind)

        return results

    def delete(self, category, ids):
        prof = self.profile_start('delete', category)
        engine = self.engine_by_category(category, "master")
        engine.delete(
            sa.and_(engine.c.category==category,
                    engine.c.ids==ids)).execute()
        self.profile_stop(prof)

    def ids_by_category(self, category, limit=1000):
        prof = self.profile_start('ids_by_category', category)
        engine = self.engine_by_category(category, "readslave")
        s = sa.select([engine.c.ids],
                      sa.and_(engine.c.category==category,
                              engine.c.expiration > datetime.now(TZ)),
                      limit = limit)
        rows = s.execute().fetchall()
        self.profile_stop(prof)
        return [ r.ids for r in rows ]

    def clause_from_expiration(self, engine, expiration):
        if expiration is None:
            return True
        elif expiration == "now":
            return engine.c.expiration < datetime.now(TZ)
        else:
            return engine.c.expiration < expiration

    def expired(self, engine, expiration_clause, limit=1000):
        s = sa.select([engine.c.category,
                       engine.c.ids,
                       engine.c.expiration],
                      expiration_clause,
                      limit = limit,
                      order_by = engine.c.expiration
                      )
        rows = s.execute().fetchall()
        return [ (r.expiration, r.category, r.ids) for r in rows ]

    def delete_if_expired(self, category, ids, expiration="now"):
        prof = self.profile_start('delete_if_expired', category)
        engine = self.engine_by_category(category, "master")
        expiration_clause = self.clause_from_expiration(engine, expiration)
        engine.delete(sa.and_(engine.c.category==category,
                              engine.c.ids==ids,
                              expiration_clause)).execute()
        self.profile_stop(prof)


def delete_expired(expiration="now", limit=5000):
    hcb = HardCacheBackend(g)

    masters = set()

    for engines in hcb.mapping.values():
        masters.add(engines[0])

    for engine in masters:
        expiration_clause = hcb.clause_from_expiration(engine, expiration)

        # Get all the expired keys
        rows = hcb.expired(engine, expiration_clause, limit)

        if len(rows) == 0:
            continue

        # Delete them from memcache
        mc_keys = [ "%s-%s" % (c, i) for e, c, i in rows ]
        g.memcache.delete_multi(mc_keys)

        # Now delete them from the backend.
        engine.delete(expiration_clause).execute()

        # Note: In between the previous two steps, a key with a
        # near-instantaneous expiration could have been added and expired, and
        # thus it'll be deleted from the backend but not memcache. But that's
        # okay, because it should be expired from memcache anyway by now.

########NEW FILE########
__FILENAME__ = helpers
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

"""
Helper functions

All names available in this module will be available under the Pylons h object.
"""

########NEW FILE########
__FILENAME__ = hooks
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
"""A very simple system for event hooks for plugins etc."""


_HOOKS = {}


def all_hooks():
    """Return all registered hooks."""
    return _HOOKS


class Hook(object):
    """A single hook that can be listened for."""
    def __init__(self):
        self.handlers = []

    def register_handler(self, handler):
        """Register a handler to call from this hook."""
        self.handlers.append(handler)

    def call(self, **kwargs):
        """Call handlers and return their results.

        Handlers will be called in the same order they were registered and
        their results will be returned in the same order as well.

        """
        return [handler(**kwargs) for handler in self.handlers]

    def call_until_return(self, **kwargs):
        """Call handlers until one returns a non-None value.

        As with call, handlers are called in the same order they are
        registered.  Only the return value of the first non-None handler is
        returned.

        """
        for handler in self.handlers:
            ret = handler(**kwargs)
            if ret is not None:
                return ret


def get_hook(name):
    """Return the named hook `name` creating it if necessary."""
    # this should be atomic as long as `name`'s __hash__ isn't python code
    # or for all types after the fixes in python#13521 are merged into 2.7.
    return _HOOKS.setdefault(name, Hook())


class HookRegistrar(object):
    """A registry for deferring module-scope hook registrations.

    This registry allows us to use module-level decorators but not actually
    register with global hooks unless we're told to.

    """
    def __init__(self):
        self.registered = False
        self.connections = []

    def on(self, name):
        """Return a decorator that registers the wrapped function."""

        hook = get_hook(name)

        def hook_decorator(fn):
            if self.registered:
                hook.register_handler(fn)
            else:
                self.connections.append((hook, fn))
            return fn
        return hook_decorator

    def register_all(self):
        """Complete all deferred registrations."""
        for hook, handler in self.connections:
            hook.register_handler(handler)
        self.registered = True

########NEW FILE########
__FILENAME__ = inventory
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################


from collections import defaultdict, OrderedDict
from datetime import datetime, timedelta
import re

from pylons import g
from sqlalchemy import func

from r2.lib.memoize import memoize
from r2.lib.utils import to_date, tup
from r2.models import (
    Bid,
    FakeSubreddit,
    Location,
    NO_TRANSACTION,
    PromoCampaign,
    PromotionWeights,
    Subreddit,
    traffic,
)
from r2.models.promo_metrics import LocationPromoMetrics, PromoMetrics
from r2.models.subreddit import DefaultSR

NDAYS_TO_QUERY = 14  # how much history to use in the estimate
MIN_DAILY_CASS_KEY = 'min_daily_pageviews.GET_listing'
PAGEVIEWS_REGEXP = re.compile('(.*)-GET_listing')
INVENTORY_FACTOR = 1.00
DEFAULT_INVENTORY_FACTOR = 5.00

def get_predicted_by_date(sr_name, start, stop=None):
    """Return dict mapping datetime objects to predicted pageviews."""
    if not sr_name:
        sr_name = DefaultSR.name.lower()
    # lowest pageviews any day the last 2 weeks
    min_daily = PromoMetrics.get(MIN_DAILY_CASS_KEY, sr_name).get(sr_name, 0)
    # expand out to the requested range of dates
    ndays = (stop - start).days if stop else 1  # default is one day
    predicted = OrderedDict()
    for i in range(ndays):
        date = start + timedelta(i)
        predicted[date] = min_daily
    return predicted


def update_prediction_data():
    """Fetch prediction data and write it to cassandra."""
    min_daily_by_sr = _min_daily_pageviews_by_sr(NDAYS_TO_QUERY)
    PromoMetrics.set(MIN_DAILY_CASS_KEY, min_daily_by_sr)


def _min_daily_pageviews_by_sr(ndays=NDAYS_TO_QUERY, end_date=None):
    """Return dict mapping sr_name to min_pageviews over the last ndays."""
    if not end_date:
        last_modified = traffic.get_traffic_last_modified()
        end_date = last_modified - timedelta(days=1)
    stop = end_date
    start = stop - timedelta(ndays)
    time_points = traffic.get_time_points('day', start, stop)
    cls = traffic.PageviewsBySubredditAndPath
    q = (traffic.Session.query(cls.srpath, func.min(cls.pageview_count))
                               .filter(cls.interval == 'day')
                               .filter(cls.date.in_(time_points))
                               .filter(cls.srpath.like('%-GET_listing'))
                               .group_by(cls.srpath))

    # row looks like: ('lightpainting-GET_listing', 16)
    retval = {}
    for row in q:
        m = PAGEVIEWS_REGEXP.match(row[0])
        if m:
            retval[m.group(1)] = row[1]
    return retval


def get_date_range(start, end):
    start, end = map(to_date, [start, end])
    dates = [start + timedelta(i) for i in xrange((end - start).days)]
    return dates


def get_campaigns_by_date(srs, start, end, ignore=None):
    srs, is_single = tup(srs, ret_is_single=True)
    sr_names = ['' if isinstance(sr, DefaultSR) else sr.name for sr in srs]
    dates = set(get_date_range(start, end))
    q = (PromotionWeights.query()
                .filter(PromotionWeights.sr_name.in_(sr_names))
                .filter(PromotionWeights.date.in_(dates)))

    if ignore:
        q = q.filter(PromotionWeights.promo_idx != ignore._id)

    campaign_ids = {pw.promo_idx for pw in q}
    campaigns = PromoCampaign._byID(campaign_ids, data=True, return_dict=False)
    transaction_ids = {camp.trans_id for camp in campaigns
                                     if camp.trans_id != NO_TRANSACTION}

    if transaction_ids:
        transactions = Bid.query().filter(Bid.transaction.in_(transaction_ids))
        transaction_by_id = {bid.transaction: bid for bid in transactions}
    else:
        transaction_by_id = {}

    ret = {sr.name: dict.fromkeys(dates) for sr in srs}
    for srname, date_dict in ret.iteritems():
        for date in date_dict:
            ret[srname][date] = []

    for camp in campaigns:
        if camp.trans_id == NO_TRANSACTION:
            continue

        if camp.impressions <= 0:
            # pre-CPM campaign
            continue

        transaction = transaction_by_id[camp.trans_id]
        if not (transaction.is_auth() or transaction.is_charged()):
            continue

        sr_name = camp.sr_name or DefaultSR.name
        camp_dates = set(get_date_range(camp.start_date, camp.end_date))
        for date in camp_dates.intersection(dates):
            ret[sr_name][date].append(camp)

    if is_single:
        return ret[srs[0].name]
    else:
        return ret


def get_sold_pageviews(srs, start, end, ignore=None):
    srs, is_single = tup(srs, ret_is_single=True)
    campaigns_by_sr_by_date = get_campaigns_by_date(srs, start, end, ignore)

    ret = {}
    for sr_name, campaigns_by_date in campaigns_by_sr_by_date.iteritems():
        ret[sr_name] = defaultdict(int)
        for date, campaigns in campaigns_by_date.iteritems():
            for camp in campaigns:
                daily_impressions = camp.impressions / camp.ndays
                ret[sr_name][date] += daily_impressions

    if is_single:
        return ret[srs[0].name]
    else:
        return ret


def get_predicted_pageviews(srs, start, end):
    srs, is_single = tup(srs, ret_is_single=True)
    sr_names = [sr.name for sr in srs]

    # default subreddits require a different inventory factor
    content_langs = [g.site_lang]
    default_srids = Subreddit.top_lang_srs(content_langs,
                                           limit=g.num_default_reddits,
                                           filter_allow_top=True, over18=False,
                                           ids=True)

    # prediction does not vary by date
    daily_inventory = PromoMetrics.get(MIN_DAILY_CASS_KEY, sr_names=sr_names)
    dates = get_date_range(start, end)
    ret = {}
    for sr in srs:
        if not isinstance(sr, FakeSubreddit) and sr._id in default_srids:
            factor = DEFAULT_INVENTORY_FACTOR
        else:
            factor = INVENTORY_FACTOR
        sr_daily_inventory = daily_inventory.get(sr.name, 0) * factor
        sr_daily_inventory = int(sr_daily_inventory)
        ret[sr.name] = dict.fromkeys(dates, sr_daily_inventory)

    if is_single:
        return ret[srs[0].name]
    else:
        return ret


def get_predicted_geotargeted(sr, location, start, end):
    """
    Predicted geotargeted impressions are estimated as:

    geotargeted impressions = (predicted untargeted impressions) *
                                 (fp impressions for location / fp impressions)

    """

    sr_inventory_by_date = get_predicted_pageviews(sr, start, end)

    no_location = Location(None)
    r = LocationPromoMetrics.get(DefaultSR, [no_location, location])
    ratio = r[(DefaultSR, location)] / float(r[(DefaultSR, no_location)])

    ret = {}
    for date, sr_inventory in sr_inventory_by_date.iteritems():
        ret[date] = int(sr_inventory * ratio)
    return ret


def get_available_pageviews_geotargeted(sr, location, start, end, datestr=False, 
                                        ignore=None):
    """
    Return the available pageviews by date for the subreddit and location.

    Available pageviews depends on all equal and higher level targets:
    A target is: subreddit > country > metro

    e.g. if a campaign is targeting /r/funny in USA/Boston we need to check that
    there's enough inventory in:
    * /r/funny (all campaigns targeting /r/funny regardless of geotargeting)
    * /r/funny + USA (all campaigns targeting /r/funny and USA with or without
      metro level targeting)
    * /r/funny + USA + Boston (all campaigns targeting /r/funny and USA and
      Boston)
    The available inventory is the smallest of these values.

    """

    predicted_by_location = {
        None: get_predicted_pageviews(sr, start, end),
        location: get_predicted_geotargeted(sr, location, start, end),
    }

    if location.metro:
        country_location = Location(country=location.country)
        country_prediction = get_predicted_geotargeted(sr, country_location,
                                                       start, end)
        predicted_by_location[country_location] = country_prediction

    datekey = lambda dt: dt.strftime('%m/%d/%Y') if datestr else dt

    ret = {}
    campaigns_by_date = get_campaigns_by_date(sr, start, end, ignore)
    for date, campaigns in campaigns_by_date.iteritems():

        # calculate sold impressions for each location
        sold_by_location = dict.fromkeys(predicted_by_location.keys(), 0)
        for camp in campaigns:
            daily_impressions = camp.impressions / camp.ndays
            for location in predicted_by_location:
                if not location or location.contains(camp.location):
                    sold_by_location[location] += daily_impressions

        # calculate available impressions for each location
        available_by_location = dict.fromkeys(predicted_by_location.keys(), 0)
        for location, predictions_by_date in predicted_by_location.iteritems():
            predicted = predictions_by_date[date]
            sold = sold_by_location[location]
            available_by_location[location] = predicted - sold

        ret[datekey(date)] = max(0, min(available_by_location.values()))
    return ret


def get_available_pageviews(srs, start, end, datestr=False, ignore=None):
    srs, is_single = tup(srs, ret_is_single=True)
    pageviews_by_sr_by_date = get_predicted_pageviews(srs, start, end)
    sold_by_sr_by_date = get_sold_pageviews(srs, start, end, ignore)

    datekey = lambda dt: dt.strftime('%m/%d/%Y') if datestr else dt

    ret = {}
    dates = get_date_range(start, end)
    for sr in srs:
        sold_by_date = sold_by_sr_by_date[sr.name]
        pageviews_by_date = pageviews_by_sr_by_date[sr.name]
        ret[sr.name] = {}
        for date in dates:
            sold = sold_by_date[date]
            pageviews = pageviews_by_date[date]
            ret[sr.name][datekey(date)] = max(0, pageviews - sold)

    if is_single:
        return ret[srs[0].name]
    else:
        return ret


def get_oversold(sr, start, end, daily_request, ignore=None, location=None):
    if location:
        available_by_date = get_available_pageviews_geotargeted(sr, location,
                                start, end, datestr=True, ignore=ignore)
    else:
        available_by_date = get_available_pageviews(sr, start, end,
                                                    datestr=True, ignore=ignore)
    oversold = {}
    for datestr, available in available_by_date.iteritems():
        if available < daily_request:
            oversold[datestr] = available
    return oversold

########NEW FILE########
__FILENAME__ = js
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import inspect
import sys
import os.path
from subprocess import Popen, PIPE
import re
import json

from r2.lib.translation import (
    extract_javascript_msgids,
    get_catalog,
    iter_langs,
    validate_plural_forms,
)
from r2.lib.plugin import PluginLoader
from r2.lib.permissions import ModeratorPermissionSet


try:
    from pylons import g, c, config
except ImportError:
    STATIC_ROOT = None
else:
    REDDIT_ROOT = config["pylons.paths"]["root"]
    STATIC_ROOT = config["pylons.paths"]["static_files"]

# STATIC_ROOT will be None if pylons is uninitialized
if not STATIC_ROOT:
    REDDIT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    STATIC_ROOT = os.path.join(os.path.dirname(REDDIT_ROOT), "build/public")


script_tag = '<script type="text/javascript" src="{src}"></script>\n'
inline_script_tag = '<script type="text/javascript">{content}</script>'


class ClosureError(Exception): pass


class ClosureCompiler(object):
    def __init__(self, jarpath):
        self.jarpath = jarpath
        self.args = [
            "--jscomp_off=internetExplorerChecks",
        ]

    def _run(self, data, out=PIPE, args=None, expected_code=0):
        args = args or []
        p = Popen(["java", "-jar", self.jarpath] + self.args + args,
                stdin=PIPE, stdout=out, stderr=PIPE)
        out, msg = p.communicate(data)
        if p.returncode != expected_code:
            raise ClosureError(msg)
        else:
            return out, msg

    def compile(self, data, dest, args=None):
        """Run closure compiler on a string of source code `data`, writing the
        result to output file `dest`. A ClosureError exception will be raised if
        the operation is unsuccessful."""
        return self._run(data, dest, args)[0]


class Source(object):
    """An abstract collection of JavaScript code."""
    def get_source(self):
        """Return the full JavaScript source code."""
        raise NotImplementedError

    def use(self):
        """Return HTML to insert the JavaScript source inside a template."""
        raise NotImplementedError

    @property
    def dependencies(self):
        raise NotImplementedError

    @property
    def outputs(self):
        raise NotImplementedError


class FileSource(Source):
    """A JavaScript source file on disk."""
    def __init__(self, name):
        self.name = name

    def get_source(self):
        with open(self.path) as f:
            return f.read()

    @property
    def path(self):
        """The path to the source file on the filesystem."""

        from r2.lib.static import locate_static_file

        try:
            g.plugins
        except TypeError:
            # g.plugins isn't available. this means we're in the build system.
            # we can safely find all files in one place in this case since the
            # build system copies them in from plugins first.
            pass
        else:
            # this is in-request. we should check all the plugin directories
            return locate_static_file(os.path.join("static", "js", self.name))

        return os.path.join(STATIC_ROOT, "static", "js", self.name)

    def use(self):
        from r2.lib.template_helpers import static
        path = [g.static_path, self.name]
        if g.uncompressedJS:
            path.insert(1, "js")
        return script_tag.format(src=static(os.path.join(*path)))

    @property
    def dependencies(self):
        return [self.path]


class Module(Source):
    """A module of JS code consisting of a collection of sources."""
    def __init__(self, name, *sources, **kwargs):
        self.name = name
        self.should_compile = kwargs.get('should_compile', True)
        self.wrap = kwargs.get('wrap')
        self.sources = []
        sources = sources or (name,)
        for source in sources:
            if not isinstance(source, Source):
                if 'prefix' in kwargs:
                    source = os.path.join(kwargs['prefix'], source)
                source = FileSource(source)
            self.sources.append(source)

    def get_source(self):
        return ";".join(s.get_source() for s in self.sources)

    def extend(self, module):
        self.sources.extend(module.sources)

    @property
    def path(self):
        """The destination path of the module file on the filesystem."""
        return os.path.join(STATIC_ROOT, "static", self.name)

    def build(self, closure):
        with open(self.path, "w") as out:
            source = self.get_source()
            if self.wrap:
                source = self.wrap.format(content=source, name=self.name)

            if self.should_compile:
                print >> sys.stderr, "Compiling {0}...".format(self.name),
                closure.compile(source, out)
            else:
                print >> sys.stderr, "Concatenating {0}...".format(self.name),
                out.write(source)
        print >> sys.stderr, " done."

    def use(self):
        from r2.lib.template_helpers import static
        if g.uncompressedJS:
            return "".join(source.use() for source in self.sources)
        else:
            return script_tag.format(src=static(self.name))

    @property
    def dependencies(self):
        deps = []
        for source in self.sources:
            deps.extend(source.dependencies)
        return deps

    @property
    def outputs(self):
        return [self.path]


class DataSource(Source):
    """A generated source consisting of wrapped JSON data."""
    def __init__(self, wrap, data=None):
        self.wrap = wrap
        self.data = data

    def get_content(self):
        return self.data

    def get_source(self):
        content = self.get_content()
        json_data = json.dumps(content)
        return self.wrap.format(content=json_data) + "\n"

    def use(self):
        from r2.lib.filters import SC_OFF, SC_ON, websafe_json
        escaped_json = websafe_json(self.get_source())
        return (SC_OFF + inline_script_tag.format(content=escaped_json) +
                SC_ON + "\n")

    @property
    def dependencies(self):
        return []


class PermissionsDataSource(DataSource):
    """DataSource for PermissionEditor configuration data."""

    def __init__(self, permission_sets):
        self.permission_sets = permission_sets

    @classmethod
    def _make_marked_json(cls, obj):
        """Return serialized psuedo-JSON with translation support.

        Strings are marked for extraction with r.N_. Dictionaries are
        serialized to JSON objects as normal.

        """
        if isinstance(obj, dict):
            props = []
            for key, value in obj.iteritems():
                value_encoded = cls._make_marked_json(value)
                props.append("%s: %s" % (key, value_encoded))
            return "{%s}" % ",".join(props)
        elif isinstance(obj, basestring):
            return "r.N_(%s)" % json.dumps(obj)
        else:
            raise ValueError, "unsupported type"

    def get_source(self):
        permission_set_info = {k: v.info for k, v in
                               self.permission_sets.iteritems()}
        permissions = self._make_marked_json(permission_set_info)
        return "r.permissions = _.extend(r.permissions || {}, %s)" % permissions

    @property
    def dependencies(self):
        dependencies = set(super(PermissionsDataSource, self).dependencies)
        for permission_set in self.permission_sets.itervalues():
            dependencies.add(inspect.getsourcefile(permission_set))
        return list(dependencies)


class TemplateFileSource(DataSource, FileSource):
    """A JavaScript template file on disk."""
    def __init__(self, name, wrap="r.templates.set({content})"):
        DataSource.__init__(self, wrap)
        FileSource.__init__(self, name)
        self.name = name

    def get_content(self):
        from r2.lib.static import locate_static_file
        name, style = os.path.splitext(self.name)
        path = locate_static_file(os.path.join('static/js', self.name))
        with open(path) as f:
            return [{
                "name": name,
                "style": style.lstrip('.'),
                "template": f.read()
            }]


class LocaleSpecificSource(object):
    def get_localized_source(self, lang):
        raise NotImplementedError


class StringsSource(LocaleSpecificSource):
    """Translations sourced from a gettext catalog."""

    def __init__(self, keys):
        self.keys = keys

    invalid_formatting_specifier_re = re.compile(r"(?<!%)%\w|(?<!%)%\(\w+\)[^s]")
    def _check_formatting_specifiers(self, string):
        if not isinstance(string, basestring):
            return

        if self.invalid_formatting_specifier_re.search(string):
            raise ValueError("Invalid string formatting specifier: %r" % string)

    def get_localized_source(self, lang):
        catalog = get_catalog(lang)

        # relies on pyx files, so it can't be imported at global scope
        from r2.lib.utils import tup

        data = {}
        for key in self.keys:
            key = tup(key)[0]  # because the key for plurals is (sing, plur)
            self._check_formatting_specifiers(key)
            msg = catalog[key]

            if not msg or not msg.string:
                continue

            # jed expects to ignore the first value in the translations array
            # so we'll just make it null
            strings = tup(msg.string)
            data[key] = [None] + list(strings)
        return "r.i18n.addMessages(%s)" % json.dumps(data)


class PluralForms(LocaleSpecificSource):
    def get_localized_source(self, lang):
        catalog = get_catalog(lang)
        validate_plural_forms(catalog.plural_expr)
        return "r.i18n.setPluralForms('%s')" % catalog.plural_expr


class LocalizedModule(Module):
    """A module that generates localized code for each language.

    Strings marked for translation with one of the functions in i18n.js (viz.
    r._, r.P_, and r.N_) are extracted from the source and their translations
    are built into the compiled source.

    """

    def __init__(self, *args, **kwargs):
        self.localized_appendices = kwargs.pop("localized_appendices", [])
        Module.__init__(self, *args, **kwargs)

    @staticmethod
    def languagize_path(path, lang):
        path_name, path_ext = os.path.splitext(path)
        return path_name + "." + lang + path_ext

    def build(self, closure):
        Module.build(self, closure)

        with open(self.path) as f:
            reddit_source = f.read()

        localized_appendices = self.localized_appendices
        msgids = extract_javascript_msgids(reddit_source)
        if msgids:
            localized_appendices = localized_appendices + [StringsSource(msgids)]

        print >> sys.stderr, "Creating language-specific files:"
        for lang, unused in iter_langs():
            lang_path = LocalizedModule.languagize_path(self.path, lang)

            # make sure we're not rewriting a different mangled file
            # via symlink
            if os.path.islink(lang_path):
                os.unlink(lang_path)

            with open(lang_path, "w") as out:
                print >> sys.stderr, "  " + lang_path
                out.write(reddit_source)
                for appendix in localized_appendices:
                    out.write(appendix.get_localized_source(lang) + ";")

    def use(self):
        from pylons.i18n import get_lang
        from r2.lib.template_helpers import static
        from r2.lib.filters import SC_OFF, SC_ON

        if g.uncompressedJS:
            if c.lang == "en" or c.lang not in g.all_languages:
                # in this case, the msgids *are* the translated strings and we
                # can save ourselves the pricey step of lexing the js source
                return Module.use(self)

            msgids = extract_javascript_msgids(Module.get_source(self))
            localized_appendices = self.localized_appendices + [StringsSource(msgids)]

            lines = [Module.use(self)]
            for appendix in localized_appendices:
                line = SC_OFF + inline_script_tag.format(
                    content=appendix.get_localized_source(c.lang)) + SC_ON
                lines.append(line)
            return "\n".join(lines)
        else:
            langs = get_lang() or [g.lang]
            url = LocalizedModule.languagize_path(self.name, langs[0])
            return script_tag.format(src=static(url))

    @property
    def outputs(self):
        for lang, unused in iter_langs():
            yield LocalizedModule.languagize_path(self.path, lang)


class JQuery(Module):
    versions = {
        1: "1.11.1",
        2: "2.1.1",
    }

    def __init__(self, cdn_url="http://ajax.googleapis.com/ajax/libs/jquery/{version}/jquery", major_version=1):
        self.jquery_src = FileSource("lib/jquery-{0}.min.js".format(self.versions[major_version]))
        Module.__init__(self, "jquery-{0}.min.js".format(self.versions[major_version]), self.jquery_src, should_compile=False)
        self.cdn_src = cdn_url.format(version=self.versions[major_version])

    def use(self):
        from r2.lib.template_helpers import static
        if c.secure or (c.user and c.user.pref_local_js):
            return Module.use(self)
        else:
            ext = ".js" if g.uncompressedJS else ".min.js"
            return script_tag.format(src=self.cdn_src+ext)


module = {}


module["jquery1x"] = JQuery(major_version=1)
module["jquery2x"] = JQuery(major_version=2)


module["html5shiv"] = Module("html5shiv.js",
    "lib/html5shiv.js",
    should_compile=False
)

catch_errors = "try {{ {content} }} catch (err) {{ r.sendError('Error running module', '{name}', ':', err) }}"

module["reddit-init"] = LocalizedModule("reddit-init.js",
    "lib/es5-shim.js",
    "lib/json2.js",
    "lib/underscore-1.4.4.js",
    "lib/store.js",
    "lib/jed.js",
    "base.js",
    "preload.js",
    "logging.js",
    "client-error-logger.js",
    "jquery.html-patch.js",
    "uibase.js",
    "i18n.js",
    "utils.js",
    "analytics.js",
    "jquery.reddit.js",
    "reddit.js",
    "spotlight.js",
    localized_appendices=[
        PluralForms(),
    ],
    wrap=catch_errors,
)

module["reddit"] = LocalizedModule("reddit.js",
    "lib/jquery.cookie.js",
    "lib/jquery.url.js",
    "lib/backbone-1.0.0.js",
    "templates.js",
    "scrollupdater.js",
    "timetext.js",
    "ui.js",
    "login.js",
    "flair.js",
    "interestbar.js",
    "visited.js",
    "wiki.js",
    "apps.js",
    "gold.js",
    "multi.js",
    "recommender.js",
    "saved.js",
    PermissionsDataSource({
        "moderator": ModeratorPermissionSet,
        "moderator_invite": ModeratorPermissionSet,
    }),
    wrap=catch_errors,
)

module["admin"] = Module("admin.js",
    # include Backbone so it is available early to render admin bar fast.
    "lib/backbone-1.0.0.js",
    "adminbar.js",
)

module["mobile"] = LocalizedModule("mobile.js",
    module["reddit"],
    "lib/jquery.lazyload.js",
    "compact.js"
)


module["button"] = Module("button.js",
    "lib/jquery.cookie.js",
    "jquery.reddit.js",
    "blogbutton.js"
)


module["policies"] = Module("policies.js",
    "policies.js",
)


module["sponsored"] = Module("sponsored.js",
    "lib/ui.core.js",
    "lib/ui.datepicker.js",
    "sponsored.js"
)


module["timeseries"] = Module("timeseries.js",
    "lib/jquery.flot.js",
    "lib/jquery.flot.time.js",
    "timeseries.js",
)


module["timeseries-ie"] = Module("timeseries-ie.js",
    "lib/excanvas.min.js",
    module["timeseries"],
)


module["traffic"] = LocalizedModule("traffic.js",
    "traffic.js",
)


module["qrcode"] = Module("qrcode.js",
    "lib/jquery.qrcode.min.js",
    "qrcode.js",
)


module["highlight"] = Module("highlight.js",
    "lib/highlight.pack.js",
    "highlight.js",
)

module["less"] = Module('less.js',
    'lib/less-1.4.2.js',
    should_compile=False,
)

def use(*names):
    return "\n".join(module[name].use() for name in names)


def load_plugin_modules(plugins=None):
    if not plugins:
        plugins = PluginLoader()
    for plugin in plugins:
        plugin.add_js(module)


commands = {}
def build_command(fn):
    def wrapped(*args):
        load_plugin_modules()
        fn(*args)
    commands[fn.__name__] = wrapped
    return wrapped


@build_command
def enumerate_modules():
    for name, m in module.iteritems():
        print name


@build_command
def dependencies(name):
    for dep in module[name].dependencies:
        print dep


@build_command
def enumerate_outputs(*names):
    if names:
        modules = [module[name] for name in names]
    else:
        modules = module.itervalues()

    for m in modules:
        for output in m.outputs:
            print output


@build_command
def build_module(name):
    closure = ClosureCompiler("r2/lib/contrib/closure_compiler/compiler.jar")
    module[name].build(closure)


if __name__ == "__main__":
    commands[sys.argv[1]](*sys.argv[2:])

########NEW FILE########
__FILENAME__ = jsonresponse
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.config.extensions import get_api_subtype
from r2.lib.utils import tup
from r2.lib.captcha import get_iden
from r2.lib.wrapped import Wrapped, StringTemplate
from r2.lib.filters import websafe_json, spaceCompress
from r2.lib.base import BaseController
from r2.lib.pages.things import wrap_links
from r2.models import IDBuilder, Listing

import simplejson
from pylons import c, g


class JsonResponse(object):
    """
    Simple Api response handler, returning a list of errors generated
    in the api func's validators, as well as blobs of data set by the
    api func.
    """

    content_type = 'application/json'

    def __init__(self):
        self._clear()

    def _clear(self):
        self._errors = set()
        self._new_captcha = False
        self._ratelimit = False
        self._data = {}

    def send_failure(self, error):
        c.errors.add(error)
        self._clear()
        self._errors.add((error, None))

    def __call__(self, *a, **kw):
        return self

    def __getattr__(self, key):
        return self

    def make_response(self):
        res = {}
        if self._data:
            res['data'] = self._data
        if self._new_captcha:
            res['captcha'] = get_iden()
        if self._ratelimit:
            res['ratelimit'] = self._ratelimit
        res['errors'] = [(e[0], c.errors[e].message, e[1]) for e in self._errors]
        return {"json": res}

    def set_error(self, error_name, field_name):
        self._errors.add((error_name, field_name))

    def has_error(self):
        return bool(self._errors)

    def has_errors(self, field_name, *errors, **kw):
        have_error = False
        field_name = tup(field_name)
        for error_name in errors:
            for fname in field_name:
                if (error_name, fname) in c.errors:
                    self.set_error(error_name, fname)
                    have_error = True
        return have_error

    def process_rendered(self, res):
        return res

    def _things(self, things, action, *a, **kw):
        """
        function for inserting/replacing things in listings.
        """
        things = tup(things)
        if not all(isinstance(t, Wrapped) for t in things):
            wrap = kw.pop('wrap', Wrapped)
            things = wrap_links(things, wrapper = wrap)
        data = [self.process_rendered(t.render()) for t in things]

        if kw:
            for d in data:
                if d.has_key('data'):
                    d['data'].update(kw)

        self._data['things'] = data
        return data

    def insert_things(self, things, append = False, **kw):
        return self._things(things, "insert_things", append, **kw)

    def replace_things(self, things, keep_children = False,
                       reveal = False, stubs = False, **kw):
        return self._things(things, "replace_things",
                            keep_children, reveal, stubs, **kw)

    def _send_data(self, **kw):
        self._data.update(kw)

    def new_captcha(self):
        self._new_captcha = True

    def ratelimit(self, seconds):
        self._ratelimit = seconds


class JQueryResponse(JsonResponse):
    """
    class which mimics the jQuery in javascript for allowing Dom
    manipulations on the client side.

    An instantiated JQueryResponse acts just like the "$" function on
    the JS layer with the exception of the ability to run arbitrary
    code on the client.  Selectors and method functions evaluate to
    new JQueryResponse objects, and the transformations are cataloged
    by the original object which can be iterated and sent across the
    wire.
    """
    def __init__(self, top_node = None):
        if top_node:
            self.top_node = top_node
        else:
            self.top_node = self
        JsonResponse.__init__(self)
        self._clear()

    def _clear(self):
        if self.top_node == self:
            self.objs = {self: 0}
            self.ops  = []
        else:
            self.objs = None
            self.ops  = None
        JsonResponse._clear(self)

    def process_rendered(self, res):
        if 'data' in res:
            if 'content' in res['data']:
                res['data']['content'] = spaceCompress(res['data']['content'])
        return res

    def send_failure(self, error):
        c.errors.add(error)
        self._clear()
        self._errors.add((self, error, None))
        self.refresh()

    def __call__(self, *a):
        return self.top_node.transform(self, "call", a)

    def __getattr__(self, key):
        if not key.startswith("__"):
            return self.top_node.transform(self, "attr", key)

    def transform(self, obj, op, args):
        new = self.__class__(self)
        newi = self.objs[new] = len(self.objs)
        self.ops.append([self.objs[obj], newi, op, args])
        return new

    def set_error(self, error_name, field_name):
        #self is the form that had the error checked, but we need to
        #add this error to the top_node of this response and give it a
        #reference to the form.
        self.top_node._errors.add((self, error_name, field_name))

    def has_error(self):
        return bool(self.top_node._errors)

    def make_response(self):
        #add the error messages
        for (form, error_name, field_name) in self._errors:
            selector = ".error." + error_name
            if field_name:
                selector += ".field-" + field_name
            message = c.errors[(error_name, field_name)].message
            form.find(selector).show().text(message).end()
        return {"jquery": self.ops}

    # thing methods
    #--------------

    def _things(self, things, action, *a, **kw):
        data = JsonResponse._things(self, things, action, *a, **kw)
        new = self.__getattr__(action)
        return new(data, *a)

    def insert_table_rows(self, rows, index = -1):
        new = self.__getattr__("insert_table_rows")
        return new([row.render() for row in tup(rows)], index)


    # convenience methods:
    # --------------------
    #def _mark_error(self, e, field):
    #    self.find("." + e).show().html(c.errors[e].message).end()
    #
    #def _unmark_error(self, e):
    #    self.find("." + e).html("").end()

    def new_captcha(self):
        if not self._new_captcha:
            self.captcha(get_iden())
            self._new_captcha = True
        
    def get_input(self, name):
        return self.find("*[name=%s]" % name)

    def set_inputs(self, **kw):
        for k, v in kw.iteritems():
            # Using 'val' instead of setting the 'value' attribute allows this
            # To work for non-textbox inputs, like textareas
            self.get_input(k).val(v).end()
        return self

    def focus_input(self, name):
        return self.get_input(name).focus().end()

    def set_html(self, selector, value):
        if value:
            return self.find(selector).show().html(value).end()
        return self.find(selector).hide().html("").end()


    def set(self, **kw):
        obj = self
        for k, v in kw.iteritems():
            obj = obj.attr(k, v)
        return obj

    def refresh(self):
        return self.top_node.transform(self, "refresh", [])


########NEW FILE########
__FILENAME__ = jsontemplates
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import calendar

from utils import to36, tup, iters
from wrapped import Wrapped, StringTemplate, CacheStub, CachedVariable, Templated
from mako.template import Template
from r2.config.extensions import get_api_subtype
from r2.lib.filters import spaceCompress, safemarkdown
from r2.models import Account
from r2.models.subreddit import SubSR
from r2.models.token import OAuth2Scope, extra_oauth2_scope
import time, pytz
from pylons import c, g
from pylons.i18n import _

from r2.models.wiki import ImagesByWikiPage


def make_typename(typ):
    return 't%s' % to36(typ._type_id)

def make_fullname(typ, _id):
    return '%s_%s' % (make_typename(typ), to36(_id))


class ObjectTemplate(StringTemplate):
    def __init__(self, d):
        self.d = d

    def update(self, kw):
        def _update(obj):
            if isinstance(obj, (str, unicode)):
                return StringTemplate(obj).finalize(kw)
            elif isinstance(obj, dict):
                return dict((k, _update(v)) for k, v in obj.iteritems())
            elif isinstance(obj, (list, tuple)):
                return map(_update, obj)
            elif isinstance(obj, CacheStub) and kw.has_key(obj.name):
                return kw[obj.name]
            else:
                return obj
        res = _update(self.d)
        return ObjectTemplate(res)

    def finalize(self, kw = {}):
        return self.update(kw).d
    
class JsonTemplate(Template):
    def __init__(self): pass

    def render(self, thing = None, *a, **kw):
        return ObjectTemplate({})

class TakedownJsonTemplate(JsonTemplate):
    def render(self, thing = None, *a, **kw):
        return thing.explanation

class TableRowTemplate(JsonTemplate):
    def cells(self, thing):
        raise NotImplementedError
    
    def css_id(self, thing):
        return ""

    def css_class(self, thing):
        return ""

    def render(self, thing = None, *a, **kw):
        return ObjectTemplate(dict(id = self.css_id(thing),
                                   css_class = self.css_class(thing),
                                   cells = self.cells(thing)))

class UserItemHTMLJsonTemplate(TableRowTemplate):
    def cells(self, thing):
        cells = []
        for cell in thing.cells:
            thing.name = cell
            r = thing.part_render('cell_type', style = "html")
            cells.append(spaceCompress(r))
        return cells

    def css_id(self, thing):
        return thing.user._fullname

    def css_class(self, thing):
        return "thing"


class ThingJsonTemplate(JsonTemplate):
    _data_attrs_ = dict(
        created="created",
        created_utc="created_utc",
        id="_id36",
        name="_fullname",
    )

    @classmethod
    def data_attrs(cls, **kw):
        d = cls._data_attrs_.copy()
        d.update(kw)
        return d
    
    def kind(self, wrapped):
        """
        Returns a string literal which identifies the type of this
        thing.  For subclasses of Thing, it will be 't's + kind_id.
        """
        _thing = wrapped.lookups[0] if isinstance(wrapped, Wrapped) else wrapped
        return make_typename(_thing.__class__)

    def rendered_data(self, thing):
        """
        Called only when get_api_type is non-None (i.e., a JSON
        request has been made with partial rendering of the object to
        be returned)

        Canonical Thing data representation for JS, which is currently
        a dictionary of three elements (translated into a JS Object
        when sent out).  The elements are:

         * id : Thing _fullname of thing.
         * content : rendered  representation of the thing by
           calling render on it using the style of get_api_subtype().
        """
        res =  dict(id = thing._fullname,
                    content = thing.render(style=get_api_subtype()))
        return res
        
    def raw_data(self, thing):
        """
        Complement to rendered_data.  Called when a dictionary of
        thing data attributes is to be sent across the wire.
        """
        return dict((k, self.thing_attr(thing, v))
                    for k, v in self._data_attrs_.iteritems())
            
    def thing_attr(self, thing, attr):
        """
        For the benefit of subclasses, to lookup attributes which may
        require more work than a simple getattr (for example, 'author'
        which has to be gotten from the author_id attribute on most
        things).
        """
        if attr == "author":
            if thing.author._deleted:
                return "[deleted]"
            return thing.author.name
        if attr == "author_flair_text":
            if thing.author._deleted:
                return None
            if thing.author.flair_enabled_in_sr(thing.subreddit._id):
                return getattr(thing.author,
                               'flair_%s_text' % (thing.subreddit._id),
                               None)
            else:
                return None
        if attr == "author_flair_css_class":
            if thing.author._deleted:
                return None
            if thing.author.flair_enabled_in_sr(thing.subreddit._id):
                return getattr(thing.author,
                               'flair_%s_css_class' % (thing.subreddit._id),
                               None)
            else:
                return None
        elif attr == "created":
            return time.mktime(thing._date.timetuple())
        elif attr == "created_utc":
            return (time.mktime(thing._date.astimezone(pytz.UTC).timetuple())
                    - time.timezone)
        elif attr == "child":
            return CachedVariable("childlisting")

        if attr == 'distinguished':
            distinguished = getattr(thing, attr, 'no')
            if distinguished == 'no':
                return None
            return distinguished
        
        if attr in ["num_reports", "banned_by", "approved_by"]:
            if c.user_is_loggedin and thing.subreddit.is_moderator(c.user):
                if attr == "num_reports":
                    return thing.reported
                ban_info = getattr(thing, "ban_info", {})
                if attr == "banned_by":
                    banner = (ban_info.get("banner")
                              if ban_info.get('moderator_banned')
                              else True)
                    return banner if thing._spam else None
                elif attr == "approved_by":
                    return ban_info.get("unbanner") if not thing._spam else None

        return getattr(thing, attr, None)

    def data(self, thing):
        if get_api_subtype():
            return self.rendered_data(thing)
        else:
            return self.raw_data(thing)

    def render(self, thing = None, action = None, *a, **kw):
        return ObjectTemplate(dict(kind = self.kind(thing),
                                   data = self.data(thing)))

class SubredditJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = ThingJsonTemplate.data_attrs(
        accounts_active="accounts_active",
        comment_score_hide_mins="comment_score_hide_mins",
        description="description",
        description_html="description_html",
        display_name="name",
        header_img="header",
        header_size="header_size",
        header_title="header_title",
        over18="over_18",
        public_description="public_description",
        public_traffic="public_traffic",
        submission_type="link_type",
        submit_link_label="submit_link_label",
        submit_text_label="submit_text_label",
        submit_text="submit_text",
        submit_text_html="submit_text_html",
        subreddit_type="type",
        subscribers="_ups",
        title="title",
        url="path",
        user_is_banned="is_banned",
        user_is_contributor="is_contributor",
        user_is_moderator="is_moderator",
        user_is_subscriber="is_subscriber",
    )

    # subreddit *attributes* (right side of the equals)
    # that are only accessible if the user can view the subreddit
    _private_attrs = set([
        "accounts_active",
        "comment_score_hide_mins",
        "description",
        "description_html",
        "header",
        "header_size",
        "header_title",
        "submit_link_label",
        "submit_text_label",
    ])

    def raw_data(self, thing):
        data = ThingJsonTemplate.raw_data(self, thing)
        permissions = getattr(thing, 'mod_permissions', None)
        if permissions:
            permissions = [perm for perm, has in permissions.iteritems() if has]
            data['mod_permissions'] = permissions
        return data

    def thing_attr(self, thing, attr):
        if attr in self._private_attrs and not thing.can_view(c.user):
            return None

        if attr == "_ups" and thing.hide_subscribers:
            return 0
        # Don't return accounts_active counts in /subreddits
        elif (attr == "accounts_active" and isinstance(c.site, SubSR)):
            return None
        elif attr == 'description_html':
            return safemarkdown(thing.description)
        elif attr in ('is_banned', 'is_contributor', 'is_moderator',
                      'is_subscriber'):
            if c.user_is_loggedin:
                check_func = getattr(thing, attr)
                return bool(check_func(c.user))
            return None
        elif attr == 'submit_text_html':
            return safemarkdown(thing.submit_text)
        else:
            return ThingJsonTemplate.thing_attr(self, thing, attr)

class LabeledMultiJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = ThingJsonTemplate.data_attrs(
        can_edit="can_edit",
        name="name",
        path="path",
        subreddits="srs",
        visibility="visibility",
    )
    del _data_attrs_["id"]

    def kind(self, wrapped):
        return "LabeledMulti"

    @classmethod
    def sr_props(cls, thing, srs):
        sr_props = thing.sr_props
        return [dict(sr_props[sr._id], name=sr.name) for sr in srs]

    def thing_attr(self, thing, attr):
        if attr == "srs":
            return self.sr_props(thing, thing.srs)
        elif attr == "can_edit":
            return c.user_is_loggedin and thing.can_edit(c.user)
        else:
            return ThingJsonTemplate.thing_attr(self, thing, attr)

class LabeledMultiDescriptionJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = dict(
        body_html="description_html",
        body_md="description_md",
    )

    def kind(self, wrapped):
        return "LabeledMultiDescription"

    def thing_attr(self, thing, attr):
        if attr == "description_html":
            # if safemarkdown is passed a falsy string it returns None :/
            description_html = safemarkdown(thing.description_md) or ''
            return description_html
        else:
            return ThingJsonTemplate.thing_attr(self, thing, attr)

class IdentityJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = ThingJsonTemplate.data_attrs(
        comment_karma="comment_karma",
        has_verified_email="email_verified",
        is_gold="gold",
        is_mod="is_mod",
        link_karma="safe_karma",
        name="name",
    )
    _private_data_attrs = dict(
        over_18="pref_over_18",
        gold_creddits="gold_creddits",
    )

    def raw_data(self, thing):
        attrs = self._data_attrs_.copy()
        if c.user_is_loggedin and thing._id == c.user._id:
            attrs.update(self._private_data_attrs)
        data = {k: self.thing_attr(thing, v) for k, v in attrs.iteritems()}
        try:
            self.add_message_data(data, thing)
        except OAuth2Scope.InsufficientScopeError:
            # No access to privatemessages, but the rest of
            # the identity information is sufficient.
            pass
        return data

    @extra_oauth2_scope("privatemessages")
    def add_message_data(self, data, thing):
        if c.user_is_loggedin and thing._id == c.user._id:
            data['has_mail'] = self.thing_attr(thing, 'has_mail')
            data['has_mod_mail'] = self.thing_attr(thing, 'has_mod_mail')

    def thing_attr(self, thing, attr):
        if attr == "is_mod":
            t = thing.lookups[0] if isinstance(thing, Wrapped) else thing
            return t.is_moderator_somewhere
        elif attr == "has_mail":
            return bool(c.have_messages)
        elif attr == "has_mod_mail":
            return bool(c.have_mod_messages)
        return ThingJsonTemplate.thing_attr(self, thing, attr)


class AccountJsonTemplate(IdentityJsonTemplate):
    _data_attrs_ = IdentityJsonTemplate.data_attrs(is_friend="is_friend")
    _private_data_attrs = dict(
        modhash="modhash",
        **IdentityJsonTemplate._private_data_attrs
    )

    def thing_attr(self, thing, attr):
        if attr == "is_friend":
            return c.user_is_loggedin and thing._id in c.user.friends
        elif attr == "modhash":
            return c.modhash
        return IdentityJsonTemplate.thing_attr(self, thing, attr)



class PrefsJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = dict((k[len("pref_"):], k) for k in
            Account._preference_attrs)

    def __init__(self, fields=None):
        if fields is not None:
            _data_attrs_ = {}
            for field in fields:
                if field not in self._data_attrs_:
                    raise KeyError(field)
                _data_attrs_[field] = self._data_attrs_[field]
            self._data_attrs_ = _data_attrs_

    def thing_attr(self, thing, attr):
        if attr == "pref_clickgadget":
            return bool(thing.pref_clickgadget)
        elif attr == "pref_content_langs":
            return tup(thing.pref_content_langs)
        return ThingJsonTemplate.thing_attr(self, thing, attr)


class LinkJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = ThingJsonTemplate.data_attrs(
        approved_by="approved_by",
        author="author",
        author_flair_css_class="author_flair_css_class",
        author_flair_text="author_flair_text",
        banned_by="banned_by",
        visited="visited",
        clicked="clicked",
        distinguished="distinguished",
        domain="domain",
        downs="downvotes",
        edited="editted",
        gilded="gildings",
        hidden="hidden",
        is_self="is_self",
        likes="likes",
        link_flair_css_class="flair_css_class",
        link_flair_text="flair_text",
        media="media_object",
        media_embed="media_embed",
        num_comments="num_comments",
        num_reports="num_reports",
        over_18="over_18",
        permalink="permalink",
        saved="saved",
        score="score",
        secure_media="secure_media_object",
        secure_media_embed="secure_media_embed",
        selftext="selftext",
        selftext_html="selftext_html",
        stickied="stickied",
        subreddit="subreddit",
        subreddit_id="subreddit_id",
        thumbnail="thumbnail",
        title="title",
        ups="upvotes",
        url="url",
    )

    def thing_attr(self, thing, attr):
        from r2.lib.media import get_media_embed
        if attr in ("media_embed", "secure_media_embed"):
            media_object = getattr(thing, attr.replace("_embed", "_object"))
            if media_object and not isinstance(media_object, basestring):
                media_embed = get_media_embed(media_object)
                if media_embed:
                    return {
                        "scrolling": media_embed.scrolling,
                        "width": media_embed.width,
                        "height": media_embed.height,
                        "content": media_embed.content,
                    }
            return {}
        elif attr == "clicked":
            # this hasn't been used in years.
            return False
        elif attr == "editted" and not isinstance(thing.editted, bool):
            return (time.mktime(thing.editted.astimezone(pytz.UTC).timetuple())
                    - time.timezone)
        elif attr == 'subreddit':
            return thing.subreddit.name
        elif attr == 'subreddit_id':
            return thing.subreddit._fullname
        elif attr == 'selftext':
            if not thing.expunged:
                return thing.selftext
            else:
                return ''
        elif attr == 'selftext_html':
            if not thing.expunged:
                return safemarkdown(thing.selftext)
            else:
                return safemarkdown(_("[removed]"))
        return ThingJsonTemplate.thing_attr(self, thing, attr)

    def rendered_data(self, thing):
        d = ThingJsonTemplate.rendered_data(self, thing)
        d['sr'] = thing.subreddit._fullname
        return d


class PromotedLinkJsonTemplate(LinkJsonTemplate):
    _data_attrs_ = LinkJsonTemplate.data_attrs(
        promoted="promoted",
    )
    del _data_attrs_['author']

class CommentJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = ThingJsonTemplate.data_attrs(
        approved_by="approved_by",
        author="author",
        author_flair_css_class="author_flair_css_class",
        author_flair_text="author_flair_text",
        banned_by="banned_by",
        body="body",
        body_html="body_html",
        distinguished="distinguished",
        downs="downvotes",
        edited="editted",
        gilded="gilded",
        likes="likes",
        link_id="link_id",
        num_reports="num_reports",
        parent_id="parent_id",
        replies="child",
        saved="saved",
        score_hidden="score_hidden",
        subreddit="subreddit",
        subreddit_id="subreddit_id",
        ups="upvotes",
    )

    def thing_attr(self, thing, attr):
        from r2.models import Comment, Link, Subreddit
        if attr == 'link_id':
            return make_fullname(Link, thing.link_id)
        elif attr == "editted" and not isinstance(thing.editted, bool):
            return (time.mktime(thing.editted.astimezone(pytz.UTC).timetuple())
                    - time.timezone)
        elif attr == 'subreddit':
            return thing.subreddit.name
        elif attr == 'subreddit_id':
            return thing.subreddit._fullname
        elif attr == "parent_id":
            if getattr(thing, "parent_id", None):
                return make_fullname(Comment, thing.parent_id)
            else:
                return make_fullname(Link, thing.link_id)
        elif attr == "body_html":
            return spaceCompress(safemarkdown(thing.body))
        elif attr == "gilded":
            return thing.gildings
        return ThingJsonTemplate.thing_attr(self, thing, attr)

    def kind(self, wrapped):
        from r2.models import Comment
        return make_typename(Comment)

    def raw_data(self, thing):
        d = ThingJsonTemplate.raw_data(self, thing)
        if c.profilepage:
            d['link_title'] = thing.link.title
            d['link_author'] = thing.link_author.name
            if thing.link.is_self:
                d['link_url'] = thing.link.make_permalink(thing.subreddit,
                                                          force_domain=True)
            else:
                d['link_url'] = thing.link.url
        return d

    def rendered_data(self, wrapped):
        d = ThingJsonTemplate.rendered_data(self, wrapped)
        d['replies'] = self.thing_attr(wrapped, 'child')
        d['contentText'] = self.thing_attr(wrapped, 'body')
        d['contentHTML'] = self.thing_attr(wrapped, 'body_html')
        d['link'] = self.thing_attr(wrapped, 'link_id')
        d['parent'] = self.thing_attr(wrapped, 'parent_id')
        return d

class MoreCommentJsonTemplate(CommentJsonTemplate):
    _data_attrs_ = dict(
        children="children",
        count="count",
        id="_id36",
        name="_fullname",
        parent_id="parent_id",
    )

    def kind(self, wrapped):
        return "more"

    def thing_attr(self, thing, attr):
        if attr == 'children':
            return [to36(x) for x in thing.children]
        if attr in ('body', 'body_html'):
            return ""
        return CommentJsonTemplate.thing_attr(self, thing, attr)

    def rendered_data(self, wrapped):
        return CommentJsonTemplate.rendered_data(self, wrapped)

class MessageJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = ThingJsonTemplate.data_attrs(
        author="author",
        body="body",
        body_html="body_html",
        context="context",
        created="created",
        dest="dest",
        first_message="first_message",
        first_message_name="first_message_name",
        new="new",
        parent_id="parent_id",
        replies="child",
        subject="subject",
        subreddit="subreddit",
        was_comment="was_comment",
    )

    def thing_attr(self, thing, attr):
        from r2.models import Comment, Link, Message
        if attr == "was_comment":
            return thing.was_comment
        elif attr == "context":
            return ("" if not thing.was_comment
                    else thing.permalink + "?context=3")
        elif attr == "dest":
            if thing.to_id:
                return thing.to.name
            else:
                return "#" + thing.subreddit.name
        elif attr == "subreddit":
            if thing.sr_id:
                return thing.subreddit.name
            return None
        elif attr == "body_html":
            return safemarkdown(thing.body)
        elif attr == "author" and getattr(thing, "hide_author", False):
            return None
        elif attr == "parent_id":
            if thing.was_comment:
                if getattr(thing, "parent_id", None):
                    return make_fullname(Comment, thing.parent_id)
                else:
                    return make_fullname(Link, thing.link_id)
            elif getattr(thing, "parent_id", None):
                return make_fullname(Message, thing.parent_id)
        elif attr == "first_message_name":
            if getattr(thing, "first_message", None):
                return make_fullname(Message, thing.first_message)
        return ThingJsonTemplate.thing_attr(self, thing, attr)

    def raw_data(self, thing):
        d = ThingJsonTemplate.raw_data(self, thing)
        if thing.was_comment:
            d['link_title'] = thing.link_title
            d['likes'] = thing.likes
        return d

    def rendered_data(self, wrapped):
        from r2.models import Message
        parent_id = wrapped.parent_id
        if parent_id:
            parent_id = make_fullname(Message, parent_id)
        d = ThingJsonTemplate.rendered_data(self, wrapped)
        d['parent'] = parent_id
        d['contentText'] = self.thing_attr(wrapped, 'body')
        d['contentHTML'] = self.thing_attr(wrapped, 'body_html')
        return d


class RedditJsonTemplate(JsonTemplate):
    def render(self, thing = None, *a, **kw):
        return ObjectTemplate(thing.content().render() if thing else {})

class PanestackJsonTemplate(JsonTemplate):
    def render(self, thing = None, *a, **kw):
        res = [t.render() for t in thing.stack if t] if thing else []
        res = [x for x in res if x]
        if not res:
            return {}
        return ObjectTemplate(res if len(res) > 1 else res[0] )

class NullJsonTemplate(JsonTemplate):
    def render(self, thing = None, *a, **kw):
        return ""

    def get_def(self, name):
        return self

class ListingJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = dict(
        after="after",
        before="before",
        children="things",
        modhash="modhash",
    )
    
    def thing_attr(self, thing, attr):
        if attr == "modhash":
            return c.modhash
        elif attr == "things":
            res = []
            for a in thing.things:
                a.childlisting = False
                r = a.render()
                res.append(r)
            return res
        return ThingJsonTemplate.thing_attr(self, thing, attr)
        

    def rendered_data(self, thing):
        return self.thing_attr(thing, "things")
    
    def kind(self, wrapped):
        return "Listing"

class UserListingJsonTemplate(ListingJsonTemplate):
    def raw_data(self, thing):
        if not thing.nextprev:
            return {"children": self.rendered_data(thing)}
        return ListingJsonTemplate.raw_data(self, thing)

    def kind(self, wrapped):
        return "Listing" if wrapped.nextprev else "UserList"

class UserListJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = dict(
        children="users",
    )

    def thing_attr(self, thing, attr):
        if attr == "users":
            res = []
            for a in thing.user_rows:
                r = a.render()
                res.append(r)
            return res
        return ThingJsonTemplate.thing_attr(self, thing, attr)

    def rendered_data(self, thing):
        return self.thing_attr(thing, "users")

    def kind(self, wrapped):
        return "UserList"


class UserTableItemJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = dict(
        id="_fullname",
        name="name",
    )

    def thing_attr(self, thing, attr):
        return ThingJsonTemplate.thing_attr(self, thing.user, attr)

    def render(self, thing, *a, **kw):
        return ObjectTemplate(self.data(thing))


class RelTableItemJsonTemplate(UserTableItemJsonTemplate):
    _data_attrs_ = UserTableItemJsonTemplate.data_attrs(
        date="date",
    )

    def thing_attr(self, thing, attr):
        rel_attr, splitter, attr = attr.partition(".")
        if attr == 'note':
            # return empty string instead of None for missing note
            return ThingJsonTemplate.thing_attr(self, thing.rel, attr) or ''
        elif attr:
            return ThingJsonTemplate.thing_attr(self, thing.rel, attr)
        elif rel_attr == 'date':
            # make date UTC
            date = self.thing_attr(thing, 'rel._date')
            date = time.mktime(date.astimezone(pytz.UTC).timetuple())
            return date - time.timezone
        else:
            return UserTableItemJsonTemplate.thing_attr(self, thing, rel_attr)


class FriendTableItemJsonTemplate(RelTableItemJsonTemplate):
    def inject_data(self, thing, d):
        if c.user.gold and thing.type == "friend":
            d["note"] = self.thing_attr(thing, 'rel.note')
        return d

    def rendered_data(self, thing):
        d = RelTableItemJsonTemplate.rendered_data(self, thing)
        return self.inject_data(thing, d)

    def raw_data(self, thing):
        d = RelTableItemJsonTemplate.raw_data(self, thing)
        return self.inject_data(thing, d)


class BannedTableItemJsonTemplate(RelTableItemJsonTemplate):
    _data_attrs_ = RelTableItemJsonTemplate.data_attrs(
        note="rel.note",
    )


class InvitedModTableItemJsonTemplate(RelTableItemJsonTemplate):
    _data_attrs_ = RelTableItemJsonTemplate.data_attrs(
        mod_permissions="permissions",
    )

    def thing_attr(self, thing, attr):
        if attr == 'permissions':
            permissions = thing.permissions.items()
            return [perm for perm, has in permissions if has]
        else:
            return RelTableItemJsonTemplate.thing_attr(self, thing, attr)


class OrganicListingJsonTemplate(ListingJsonTemplate):
    def kind(self, wrapped):
        return "OrganicListing"

class TrafficJsonTemplate(JsonTemplate):
    def render(self, thing, *a, **kw):
        res = {}

        for interval in ("hour", "day", "month"):
            # we don't actually care about the column definitions (used for
            # charting) here, so just pass an empty list.
            interval_data = thing.get_data_for_interval(interval, [])

            # turn the python datetimes into unix timestamps and flatten data
            res[interval] = [(calendar.timegm(date.timetuple()),) + data
                             for date, data in interval_data]

        return ObjectTemplate(res)

class WikiJsonTemplate(JsonTemplate):
    def render(self, thing, *a, **kw):
        try:
            content = thing.content()
        except AttributeError:
            content = thing.listing
        return ObjectTemplate(content.render() if thing else {})

class WikiPageListingJsonTemplate(ThingJsonTemplate):
    def kind(self, thing):
        return "wikipagelisting"
    
    def data(self, thing):
        pages = [p.name for p in thing.linear_pages]
        return pages

class WikiViewJsonTemplate(ThingJsonTemplate):
    def kind(self, thing):
        return "wikipage"
    
    def data(self, thing):
        edit_date = time.mktime(thing.edit_date.timetuple()) if thing.edit_date else None
        edit_by = None
        if thing.edit_by and not thing.edit_by._deleted:
             edit_by = Wrapped(thing.edit_by).render()
        return dict(content_md=thing.page_content_md,
                    content_html=thing.page_content,
                    revision_by=edit_by,
                    revision_date=edit_date,
                    may_revise=thing.may_revise)

class WikiSettingsJsonTemplate(ThingJsonTemplate):
     def kind(self, thing):
         return "wikipagesettings"
    
     def data(self, thing):
         editors = [Wrapped(e).render() for e in thing.mayedit]
         return dict(permlevel=thing.permlevel,
                     listed=thing.listed,
                     editors=editors)

class WikiRevisionJsonTemplate(ThingJsonTemplate):
    def render(self, thing, *a, **kw):
        timestamp = time.mktime(thing.date.timetuple()) if thing.date else None
        author = thing.get_author()
        if author and not author._deleted:
            author = Wrapped(author).render()
        else:
            author = None
        return ObjectTemplate(dict(author=author,
                                   id=str(thing._id),
                                   timestamp=timestamp,
                                   reason=thing._get('reason'),
                                   page=thing.page))

class FlairListJsonTemplate(JsonTemplate):
    def render(self, thing, *a, **kw):
        def row_to_json(row):
            if hasattr(row, 'user'):
              return dict(user=row.user.name, flair_text=row.flair_text,
                          flair_css_class=row.flair_css_class)
            else:
              # prev/next link
              return dict(after=row.after, reverse=row.reverse)

        json_rows = [row_to_json(row) for row in thing.flair]
        result = dict(users=[row for row in json_rows if 'user' in row])
        for row in json_rows:
            if 'after' in row:
                if row['reverse']:
                    result['prev'] = row['after']
                else:
                    result['next'] = row['after']
        return ObjectTemplate(result)

class FlairCsvJsonTemplate(JsonTemplate):
    def render(self, thing, *a, **kw):
        return ObjectTemplate([l.__dict__ for l in thing.results_by_line])


class FlairSelectorJsonTemplate(JsonTemplate):
    def _template_dict(self, flair):
        return {"flair_template_id": flair.flair_template_id,
                "flair_position": flair.flair_position,
                "flair_text": flair.flair_text,
                "flair_css_class": flair.flair_css_class,
                "flair_text_editable": flair.flair_text_editable}

    def render(self, thing, *a, **kw):
        """Render a list of flair choices into JSON

        Sample output:
        {
            "choices": [
                {
                    "flair_css_class": "flair-444",
                    "flair_position": "right",
                    "flair_template_id": "5668d204-9388-11e3-8109-080027a38559",
                    "flair_text": "444",
                    "flair_text_editable": true
                },
                {
                    "flair_css_class": "flair-nouser",
                    "flair_position": "right",
                    "flair_template_id": "58e34d7a-9388-11e3-ab01-080027a38559",
                    "flair_text": "nouser",
                    "flair_text_editable": true
                },
                {
                    "flair_css_class": "flair-bar",
                    "flair_position": "right",
                    "flair_template_id": "fb01cc04-9391-11e3-b1d6-080027a38559",
                    "flair_text": "foooooo",
                    "flair_text_editable": true
                }
            ],
            "current": {
                "flair_css_class": "444",
                "flair_position": "right",
                "flair_template_id": "5668d204-9388-11e3-8109-080027a38559",
                "flair_text": "444"
            }
        }

        """
        choices = [self._template_dict(choice) for choice in thing.choices]

        current_flair = {
            "flair_text": thing.text,
            "flair_css_class": thing.css_class,
            "flair_position": thing.position,
            "flair_template_id": thing.matching_template,
        }
        return ObjectTemplate({"current": current_flair, "choices": choices})


class StylesheetTemplate(ThingJsonTemplate):
    _data_attrs_ = dict(
        images='_images',
        stylesheet='stylesheet_contents',
        subreddit_id='_fullname',
    )

    def kind(self, wrapped):
        return 'stylesheet'

    def images(self):
        sr_images = ImagesByWikiPage.get_images(c.site, "config/stylesheet")
        images = []
        for name, url in sr_images.iteritems():
            images.append({'name': name,
                           'link': 'url(%%%%%s%%%%)' % name,
                           'url': url})
        return images

    def thing_attr(self, thing, attr):
        if attr == '_images':
            return self.images()
        elif attr == '_fullname':
            return c.site._fullname
        return ThingJsonTemplate.thing_attr(self, thing, attr)

class SubredditSettingsTemplate(ThingJsonTemplate):
    _data_attrs_ = dict(
        comment_score_hide_mins='site.comment_score_hide_mins',
        content_options='site.link_type',
        default_set='site.allow_top',
        description='site.description',
        domain='site.domain',
        domain_css='site.css_on_cname',
        domain_sidebar='site.show_cname_sidebar',
        exclude_banned_modqueue='site.exclude_banned_modqueue',
        header_hover_text='site.header_title',
        language='site.lang',
        over_18='site.over_18',
        public_description='site.public_description',
        public_traffic='site.public_traffic',
        show_media='site.show_media',
        submit_link_label='site.submit_link_label',
        submit_text_label='site.submit_text_label',
        submit_text='site.submit_text',
        subreddit_id='site._fullname',
        subreddit_type='site.type',
        title='site.title',
        wiki_edit_age='site.wiki_edit_age',
        wiki_edit_karma='site.wiki_edit_karma',
        wikimode='site.wikimode',
        spam_links='site.spam_links',
        spam_selfposts='site.spam_selfposts',
        spam_comments='site.spam_comments',
    )

    def kind(self, wrapped):
        return 'subreddit_settings'

    def thing_attr(self, thing, attr):
        if attr.startswith('site.') and thing.site:
            return getattr(thing.site, attr[5:])
        return ThingJsonTemplate.thing_attr(self, thing, attr)

class ModActionTemplate(ThingJsonTemplate):
    _data_attrs_ = dict(
        action='action',
        created_utc='date',
        description='description',
        details='details',
        id='_fullname',
        mod='author',
        mod_id36='mod_id36',
        sr_id36='sr_id36',
        subreddit='sr_name',
        target_fullname='target_fullname',
    )

    def thing_attr(self, thing, attr):
        if attr == 'date':
            return (time.mktime(thing.date.astimezone(pytz.UTC).timetuple())
                    - time.timezone)
        return ThingJsonTemplate.thing_attr(self, thing, attr)

    def kind(self, wrapped):
        return 'modaction'


class PolicyViewJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = dict(
        body_html="body_html",
        display_rev="display_rev",
        revs="revs",
        toc_html="toc_html",
    )

    def kind(self, wrapped):
        return "Policy"

class KarmaListJsonTemplate(ThingJsonTemplate):
    def data(self, karmas):
        karmas = [{
            'sr': label,
            'link_karma': lc,
            'comment_karma': cc,
        } for label, title, lc, cc in karmas]
        return karmas

    def kind(self, wrapped):
        return "KarmaList"

class TrophyJsonTemplate(ThingJsonTemplate):
    _data_attrs_ = dict(
        award_id="award._id36",
        description="description",
        name="award.title",
        id="_id36",
        icon_40="icon_40",
        icon_70="icon_70",
        url="trophy_url",
    )

    def thing_attr(self, thing, attr):
        if attr == "icon_40":
            return "https:" + thing._thing2.imgurl % 40
        elif attr == "icon_70":
            return "https:" + thing._thing2.imgurl % 70
        rel_attr, splitter, attr = attr.partition(".")
        if attr:
            return ThingJsonTemplate.thing_attr(self, thing._thing2, attr)
        else:
            return ThingJsonTemplate.thing_attr(self, thing, rel_attr)

    def kind(self, thing):
        return ThingJsonTemplate.kind(self, thing._thing2)

class TrophyListJsonTemplate(ThingJsonTemplate):
    def data(self, trophies):
        trophies = [Wrapped(t).render() for t in trophies]
        return dict(trophies=trophies)

    def kind(self, wrapped):
        return "TrophyList"

########NEW FILE########
__FILENAME__ = lock
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from __future__ import with_statement
from time import sleep
from datetime import datetime
from threading import local
import os
import socket

from r2.lib.utils import simple_traceback

# thread-local storage for detection of recursive locks
locks = local()

reddit_host = socket.gethostname()
reddit_pid  = os.getpid()

class TimeoutExpired(Exception): pass

class MemcacheLock(object):
    """A simple global lock based on the memcache 'add' command. We
    attempt to grab a lock by 'adding' the lock name. If the response
    is True, we have the lock. If it's False, someone else has it."""

    def __init__(self, stats, group, key, cache,
                 time=30, timeout=30, verbose=True):
        # get a thread-local set of locks that we own
        self.locks = locks.locks = getattr(locks, 'locks', set())

        self.stats = stats
        self.group = group
        self.key = key
        self.cache = cache
        self.time = time
        self.timeout = timeout
        self.have_lock = False
        self.verbose = verbose

    def __enter__(self):
        self.acquire()

    def __exit__(self, type, value, tb):
        self.release()

    def acquire(self):
        start = datetime.now()

        my_info = (reddit_host, reddit_pid, simple_traceback(limit=7))

        #if this thread already has this lock, move on
        if self.key in self.locks:
            return

        timer = self.stats.get_timer("lock_wait")
        timer.start()

        #try and fetch the lock, looping until it's available
        while not self.cache.add(self.key, my_info, time = self.time):
            if (datetime.now() - start).seconds > self.timeout:
                if self.verbose:
                    info = self.cache.get(self.key)
                    if info:
                        info = "%s %s\n%s" % info
                    else:
                        info = "(nonexistent)"
                    msg = ("\nSome jerk is hogging %s:\n%s" %
                                         (self.key, info))
                    msg += "^^^ that was the stack trace of the lock hog, not me."
                else:
                    msg = "Timed out waiting for %s" % self.key
                raise TimeoutExpired(msg)

            sleep(.01)

        timer.stop(subname=self.group)

        #tell this thread we have this lock so we can avoid deadlocks
        #of requests for the same lock in the same thread
        self.locks.add(self.key)
        self.have_lock = True

    def release(self):
        #only release the lock if we gained it in the first place
        if self.have_lock:
            self.cache.delete(self.key)
            self.locks.remove(self.key)

def make_lock_factory(cache, stats):
    def factory(group, key, **kw):
        return MemcacheLock(stats, group, key, cache, **kw)
    return factory

########NEW FILE########
__FILENAME__ = log
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import cPickle

from datetime import datetime

from pylons import g, c, request
from weberror.reporter import Reporter


QUEUE_NAME = 'log_q'


def _default_dict():
    return dict(time=datetime.now(g.display_tz),
                host=g.reddit_host,
                port="default",
                pid=g.reddit_pid)


def log_text(classification, text=None, level="info"):
    """Send some log text to log_q for appearance in the streamlog.

    This is deprecated. All logging should be done through python's stdlib
    logging library.

    """

    from r2.lib import amqp
    from r2.lib.filters import _force_utf8

    if text is None:
        text = classification

    if level not in ('debug', 'info', 'warning', 'error'):
        print "What kind of loglevel is %s supposed to be?" % level
        level = 'error'

    d = _default_dict()
    d['type'] = 'text'
    d['level'] = level
    d['text'] = _force_utf8(text)
    d['classification'] = classification

    amqp.add_item(QUEUE_NAME, cPickle.dumps(d))


class LogQueueErrorReporter(Reporter):
    """ErrorMiddleware-compatible reporter that writes exceptions to log_q.

    The log_q queue processor then picks these up, updates the /admin/errors
    overview, and decides whether or not to send out emails about them.

    """

    @staticmethod
    def _operational_exceptions():
        """Get a list of exceptions caused by transient operational stuff.

        These errors aren't terribly useful to track in /admin/errors because
        they aren't directly bugs in the code but rather symptoms of
        operational issues.

        """

        import _pylibmc
        import sqlalchemy.exc
        import pycassa.pool
        import r2.lib.db.thing
        import r2.lib.lock

        return (
            SystemExit,  # gunicorn is shutting us down
            _pylibmc.MemcachedError,
            r2.lib.db.thing.NotFound,
            r2.lib.lock.TimeoutExpired,
            sqlalchemy.exc.OperationalError,
            sqlalchemy.exc.IntegrityError,
            pycassa.pool.AllServersUnavailable,
            pycassa.pool.NoConnectionAvailable,
            pycassa.pool.MaximumRetryException,
        )

    def report(self, exc_data):
        from r2.lib import amqp

        if issubclass(exc_data.exception_type, self._operational_exceptions()):
            return

        d = _default_dict()
        d["type"] = "exception"
        d["exception_type"] = exc_data.exception_type.__name__
        d["exception_desc"] = exc_data.exception_value
        # use the format that log_q expects; same as traceback.extract_tb
        d["traceback"] = [(f.filename, f.lineno, f.name,
                           f.get_source_line().strip())
                          for f in exc_data.frames]

        amqp.add_item(QUEUE_NAME, cPickle.dumps(d))


def write_error_summary(error):
    """Log a single-line summary of the error for easy log grepping."""
    fullpath = request.environ.get('FULLPATH', request.path)
    uid = c.user._id if c.user_is_loggedin else '-'
    g.log.error("E: %s U: %s FP: %s", error, uid, fullpath)


class LoggingErrorReporter(Reporter):
    """ErrorMiddleware-compatible reporter that writes exceptions to g.log."""

    def report(self, exc_data):
        # exception_formatted is the output of traceback.format_exception_only
        exception = exc_data.exception_formatted[-1].strip()

        # First emit a single-line summary.  This is great for grepping the
        # streaming log for errors.
        write_error_summary(exception)

        text, extra = self.format_text(exc_data)
        # TODO: send this all in one burst so that error reports aren't
        # interleaved / individual lines aren't dropped. doing so will take
        # configuration on the syslog side and potentially in apptail as well
        for line in text.splitlines():
            g.log.warning(line)

########NEW FILE########
__FILENAME__ = db_manager
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import logging
import os
import random
import socket
import sqlalchemy
import time
import traceback


logger = logging.getLogger('dm_manager')
logger.addHandler(logging.StreamHandler())
APPLICATION_NAME = "reddit@%s:%d" % (socket.gethostname(), os.getpid())


def get_engine(name, db_host='', db_user='', db_pass='', db_port='5432',
               pool_size=5, max_overflow=5, g_override=None):
    db_port = int(db_port)

    arguments = {
        "dbname": name,
        "host": db_host,
        "port": db_port,
        "application_name": APPLICATION_NAME,
    }
    if db_user:
        arguments["user"] = db_user
    if db_pass:
        arguments["password"] = db_pass
    dsn = "%20".join("%s=%s" % x for x in arguments.iteritems())

    engine = sqlalchemy.create_engine(
        'postgresql:///?dsn=' + dsn,
        strategy='threadlocal',
        pool_size=int(pool_size),
        max_overflow=int(max_overflow),
        # our code isn't ready for unicode to appear
        # in place of strings yet
        use_native_unicode=False,
    )

    if g_override:
        sqlalchemy.event.listens_for(engine, 'before_cursor_execute')(
            g_override.stats.pg_before_cursor_execute)
        sqlalchemy.event.listens_for(engine, 'after_cursor_execute')(
            g_override.stats.pg_after_cursor_execute)

    return engine


class db_manager:
    def __init__(self):
        self.type_db = None
        self.relation_type_db = None
        self._things = {}
        self._relations = {}
        self._engines = {}
        self.avoid_master_reads = {}
        self.dead = {}

    def add_thing(self, name, thing_dbs, avoid_master=False, **kw):
        """thing_dbs is a list of database engines. the first in the
        list is assumed to be the master, the rest are slaves."""
        self._things[name] = thing_dbs
        self.avoid_master_reads[name] = avoid_master

    def add_relation(self, name, type1, type2, relation_dbs,
                     avoid_master=False, **kw):
        self._relations[name] = (type1, type2, relation_dbs)
        self.avoid_master_reads[name] = avoid_master

    def setup_db(self, db_name, g_override=None, **params):
        engine = get_engine(g_override=g_override, **params)
        self._engines[db_name] = engine
        self.test_engine(engine, g_override)

    def things_iter(self):
        for name, engines in self._things.iteritems():
            # ensure we ALWAYS return the actual master as the first,
            # regardless of if we think it's dead or not.
            yield name, [engines[0]] + [e for e in engines[1:]
                                        if e not in self.dead]

    def rels_iter(self):
        for name, (t1_name, t2_name, engines) in self._relations.iteritems():
            engines = [engines[0]] + [e for e in engines[1:]
                                      if e not in self.dead]
            yield name, (t1_name, t2_name, engines)

    def mark_dead(self, engine, g_override=None):
        logger.error("db_manager: marking connection dead: %r", engine)
        self.dead[engine] = time.time()

    def test_engine(self, engine, g_override=None):
        try:
            list(engine.execute("select 1"))
            if engine in self.dead:
                logger.error("db_manager: marking connection alive: %r",
                             engine)
                del self.dead[engine]
            return True
        except Exception:
            logger.error(traceback.format_exc())
            logger.error("connection failure: %r" % engine)
            self.mark_dead(engine, g_override)
            return False

    def get_engine(self, name):
        return self._engines[name]

    def get_engines(self, names):
        return [self._engines[name] for name in names if name in self._engines]

    def get_read_table(self, tables):
        if len(tables) == 1:
            return tables[0]
        return  random.choice(list(tables))

########NEW FILE########
__FILENAME__ = tp_manager
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import g
import hashlib
from mako.template import Template as mTemplate
from mako.exceptions import TemplateLookupException
from r2.lib.filters import websafe, unsafe

from r2.lib.utils import Storage

import inspect, re, os

class tp_manager:
    def __init__(self, template_cls=mTemplate):
        self.templates = {}
        self.Template = template_cls

    def add(self, name, style, file = None):
        key = (name.lower(), style.lower())
        if file is None:
            file = "/%s.%s" % (name, style)
        elif not file.startswith('/'):
            file = '/' + file
        self.templates[key] = file
        return file

    def add_handler(self, name, style, handler):
        key = (name.lower(), style.lower())
        self.templates[key] = handler

    def get(self, thing, style, cache = True):
        if not isinstance(thing, type(object)):
            thing = thing.__class__

        style = style.lower()
        top_key = (thing.__name__.lower(), style)

        template = None
        for cls in inspect.getmro(thing):
            name = cls.__name__.lower()
            key = (name, style)

            template_or_name = self.templates.get(key)
            if not template_or_name:
                template_or_name = self.add(name, style)

            if isinstance(template_or_name, self.Template):
                template = template_or_name
                break
            else:
                try:
                    template = g.mako_lookup.get_template(template_or_name)
                    if cache:
                        self.templates[key] = template
                        # also store a hash for the template
                        if (not hasattr(template, "hash") and
                            hasattr(template, "filename")):
                            with open(template.filename, 'r') as handle:
                                template.hash = hashlib.sha1(handle.read()).hexdigest()
                        # cache also for the base class so
                        # introspection is not required on subsequent passes
                        if key != top_key:
                            self.templates[top_key] = template
                    break
                except TemplateLookupException:
                    pass

        if not template or not isinstance(template, self.Template):
            raise AttributeError, ("template doesn't exist for %s" % str(top_key))
        return template


########NEW FILE########
__FILENAME__ = media
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import base64
import cStringIO
import hashlib
import json
import math
import os
import re
import subprocess
import tempfile
import traceback
import urllib
import urllib2
import urlparse
import gzip

import BeautifulSoup
import Image
import ImageFile
import requests

from pylons import g

from r2.lib import amqp, hooks
from r2.lib.memoize import memoize
from r2.lib.nymph import optimize_png
from r2.lib.utils import TimeoutFunction, TimeoutFunctionException, domain
from r2.models.link import Link
from r2.models.media_cache import (
    ERROR_MEDIA,
    Media,
    MediaByURL,
)
from urllib2 import (
    HTTPError,
    URLError,
)


MEDIA_FILENAME_LENGTH = 12
thumbnail_size = 70, 70

# TODO: replace this with data from the embedly service api when available
_SECURE_SERVICES = [
    "youtube",
    "vimeo",
    "soundcloud",
    "wistia",
    "slideshare",
]


def _image_to_str(image):
    s = cStringIO.StringIO()
    image.save(s, image.format)
    return s.getvalue()


def str_to_image(s):
    s = cStringIO.StringIO(s)
    image = Image.open(s)
    return image


def _image_entropy(img):
    """calculate the entropy of an image"""
    hist = img.histogram()
    hist_size = sum(hist)
    hist = [float(h) / hist_size for h in hist]

    return -sum(p * math.log(p, 2) for p in hist if p != 0)


def _square_image(img):
    """if the image is taller than it is wide, square it off. determine
    which pieces to cut off based on the entropy pieces."""
    x,y = img.size
    while y > x:
        #slice 10px at a time until square
        slice_height = min(y - x, 10)

        bottom = img.crop((0, y - slice_height, x, y))
        top = img.crop((0, 0, x, slice_height))

        #remove the slice with the least entropy
        if _image_entropy(bottom) < _image_entropy(top):
            img = img.crop((0, 0, x, y - slice_height))
        else:
            img = img.crop((0, slice_height, x, y))

        x,y = img.size

    return img


def _prepare_image(image):
    image = _square_image(image)
    image.thumbnail(thumbnail_size, Image.ANTIALIAS)
    return image


def _clean_url(url):
    """url quotes unicode data out of urls"""
    url = url.encode('utf8')
    url = ''.join(urllib.quote(c) if ord(c) >= 127 else c for c in url)
    return url


def _initialize_request(url, referer):
    url = _clean_url(url)

    if not url.startswith(("http://", "https://")):
        return

    req = urllib2.Request(url)
    req.add_header('Accept-Encoding', 'gzip')
    if g.useragent:
        req.add_header('User-Agent', g.useragent)
    if referer:
        req.add_header('Referer', referer)
    return req


def _fetch_url(url, referer=None):
    request = _initialize_request(url, referer=referer)
    if not request:
        return None, None
    response = urllib2.urlopen(request)
    response_data = response.read()
    content_encoding = response.info().get("Content-Encoding")
    if content_encoding and content_encoding.lower() in ["gzip", "x-gzip"]:
        buf = cStringIO.StringIO(response_data)
        f = gzip.GzipFile(fileobj=buf)
        response_data = f.read()
    return response.headers.get("Content-Type"), response_data


@memoize('media.fetch_size', time=3600)
def _fetch_image_size(url, referer):
    """Return the size of an image by URL downloading as little as possible."""

    request = _initialize_request(url, referer)
    if not request:
        return None

    parser = ImageFile.Parser()
    response = None
    try:
        response = urllib2.urlopen(request)

        while True:
            chunk = response.read(1024)
            if not chunk:
                break

            parser.feed(chunk)
            if parser.image:
                return parser.image.size
    except urllib2.URLError:
        return None
    finally:
        if response:
            response.close()


def optimize_jpeg(filename):
    with open(os.path.devnull, 'w') as devnull:
        subprocess.check_call(("/usr/bin/jpegoptim", filename), stdout=devnull)


def thumbnail_url(link):
    """Given a link, returns the url for its thumbnail based on its fullname"""
    if link.has_thumbnail:
        if hasattr(link, "thumbnail_url"):
            return link.thumbnail_url
        else:
            return ''
    else:
        return ''


def _filename_from_content(contents):
    sha = hashlib.sha1(contents).digest()
    return base64.urlsafe_b64encode(sha[0:MEDIA_FILENAME_LENGTH])


def upload_media(image, file_type='.jpg'):
    """Upload an image to the media provider."""
    f = tempfile.NamedTemporaryFile(suffix=file_type, delete=False)
    try:
        img = image
        do_convert = True
        if isinstance(img, basestring):
            img = str_to_image(img)
            if img.format == "PNG" and file_type == ".png":
                img.verify()
                f.write(image)
                f.close()
                do_convert = False

        if do_convert:
            img = img.convert('RGBA')
            if file_type == ".jpg":
                # PIL does not play nice when converting alpha channels to jpg
                background = Image.new('RGBA', img.size, (255, 255, 255))
                background.paste(img, img)
                img = background.convert('RGB')
                img.save(f, quality=85) # Bug in the JPG encoder with the optimize flag, even if set to false
            else:
                img.save(f, optimize=True)

        if file_type == ".png":
            optimize_png(f.name)
        elif file_type == ".jpg":
            optimize_jpeg(f.name)
        contents = open(f.name).read()
        file_name = _filename_from_content(contents) + file_type
        return g.media_provider.put(file_name, contents)
    finally:
        os.unlink(f.name)
    return ""


def upload_stylesheet(content):
    file_name = _filename_from_content(content) + ".css"
    return g.media_provider.put(file_name, content)


def _scrape_media(url, autoplay=False, maxwidth=600, force=False,
                  use_cache=False, max_cache_age=None):
    media = None
    autoplay = bool(autoplay)
    maxwidth = int(maxwidth)

    # Use media from the cache (if available)
    if not force and use_cache:
        mediaByURL = MediaByURL.get(url,
                                    autoplay=autoplay,
                                    maxwidth=maxwidth,
                                    max_cache_age=max_cache_age)
        if mediaByURL:
            media = mediaByURL.media

    # Otherwise, scrape it
    if not media:
        media_object = secure_media_object = None
        thumbnail_image = thumbnail_url = thumbnail_size = None

        scraper = Scraper.for_url(url, autoplay=autoplay)
        try:
            thumbnail_image, media_object, secure_media_object = (
                scraper.scrape())
        except (HTTPError, URLError) as e:
            if use_cache:
                MediaByURL.add_error(url, str(e),
                                     autoplay=autoplay,
                                     maxwidth=maxwidth)
            return None

        # the scraper should be able to make a media embed out of the
        # media object it just gave us. if not, null out the media object
        # to protect downstream code
        if media_object and not scraper.media_embed(media_object):
            print "%s made a bad media obj for url %s" % (scraper, url)
            media_object = None

        if (secure_media_object and
            not scraper.media_embed(secure_media_object)):
            print "%s made a bad secure media obj for url %s" % (scraper, url)
            secure_media_object = None

        if thumbnail_image:
            thumbnail_size = thumbnail_image.size
            thumbnail_url = upload_media(thumbnail_image)

        media = Media(media_object, secure_media_object,
                      thumbnail_url, thumbnail_size)

    # Store the media in the cache (if requested), possibly extending the ttl
    if use_cache and media is not ERROR_MEDIA:
        MediaByURL.add(url,
                       media,
                       autoplay=autoplay,
                       maxwidth=maxwidth)

    return media


def _set_media(link, force=False, **kwargs):
    if link.is_self:
        return
    if not force and link.promoted:
        return
    elif not force and (link.has_thumbnail or link.media_object):
        return

    media = _scrape_media(link.url, force=force, **kwargs)

    if media and not link.promoted:
        link.thumbnail_url = media.thumbnail_url
        link.thumbnail_size = media.thumbnail_size

        link.set_media_object(media.media_object)
        link.set_secure_media_object(media.secure_media_object)

        link._commit()

        hooks.get_hook("scraper.set_media").call(link=link)


def force_thumbnail(link, image_data, file_type=".jpg"):
    image = str_to_image(image_data)
    image = _prepare_image(image)
    thumb_url = upload_media(image, file_type=file_type)

    link.thumbnail_url = thumb_url
    link.thumbnail_size = image.size
    link._commit()


def upload_icon(file_name, image_data, size):
    image = str_to_image(image_data)
    image.format = 'PNG'
    image.thumbnail(size, Image.ANTIALIAS)
    icon_data = _image_to_str(image)
    return g.media_provider.put(file_name + ".png", icon_data)


def _make_custom_media_embed(media_object):
    # this is for promoted links with custom media embeds.
    return MediaEmbed(
        height=media_object.get("height"),
        width=media_object.get("width"),
        content=media_object.get("content"),
    )


def get_media_embed(media_object):
    if not isinstance(media_object, dict):
        return

    embed_hook = hooks.get_hook("scraper.media_embed")
    media_embed = embed_hook.call_until_return(media_object=media_object)
    if media_embed:
        return media_embed

    if media_object.get("type") == "custom":
        return _make_custom_media_embed(media_object)

    if "oembed" in media_object:
        return _EmbedlyScraper.media_embed(media_object)


class MediaEmbed(object):
    width = None
    height = None
    content = None
    scrolling = False

    def __init__(self, height, width, content, scrolling=False):
        self.height = int(height)
        self.width = int(width)
        self.content = content
        self.scrolling = scrolling


def _make_thumbnail_from_url(thumbnail_url, referer):
    if not thumbnail_url:
        return
    content_type, content = _fetch_url(thumbnail_url, referer=referer)
    if not content:
        return
    image = str_to_image(content)
    return _prepare_image(image)


class Scraper(object):
    @classmethod
    def for_url(cls, url, autoplay=False, maxwidth=600):
        scraper = hooks.get_hook("scraper.factory").call_until_return(url=url)
        if scraper:
            return scraper

        embedly_services = _fetch_embedly_services()
        for service_re, service_secure in embedly_services:
            if service_re.match(url):
                return _EmbedlyScraper(url,
                                       service_secure,
                                       autoplay=autoplay,
                                       maxwidth=maxwidth)

        return _ThumbnailOnlyScraper(url)

    def scrape(self):
        # should return a 3-tuple of: thumbnail, media_object, secure_media_obj
        raise NotImplementedError

    @classmethod
    def media_embed(cls, media_object):
        # should take a media object and return an appropriate MediaEmbed
        raise NotImplementedError


class _ThumbnailOnlyScraper(Scraper):
    def __init__(self, url):
        self.url = url

    def scrape(self):
        thumbnail_url = self._find_thumbnail_image()
        thumbnail = _make_thumbnail_from_url(thumbnail_url, referer=self.url)
        return thumbnail, None, None

    def _extract_image_urls(self, soup):
        for img in soup.findAll("img", src=True):
            yield urlparse.urljoin(self.url, img["src"])

    def _find_thumbnail_image(self):
        content_type, content = _fetch_url(self.url)

        # if it's an image. it's pretty easy to guess what we should thumbnail.
        if content_type and "image" in content_type and content:
            return self.url

        if content_type and "html" in content_type and content:
            soup = BeautifulSoup.BeautifulSoup(content)
        else:
            return None

        # allow the content author to specify the thumbnail:
        # <meta property="og:image" content="http://...">
        og_image = (soup.find('meta', property='og:image') or
                    soup.find('meta', attrs={'name': 'og:image'}))
        if og_image and og_image['content']:
            return og_image['content']

        # <link rel="image_src" href="http://...">
        thumbnail_spec = soup.find('link', rel='image_src')
        if thumbnail_spec and thumbnail_spec['href']:
            return thumbnail_spec['href']

        # ok, we have no guidance from the author. look for the largest
        # image on the page with a few caveats. (see below)
        max_area = 0
        max_url = None
        for image_url in self._extract_image_urls(soup):
            size = _fetch_image_size(image_url, referer=self.url)
            if not size:
                continue

            area = size[0] * size[1]

            # ignore little images
            if area < 5000:
                g.log.debug('ignore little %s' % image_url)
                continue

            # ignore excessively long/wide images
            if max(size) / min(size) > 1.5:
                g.log.debug('ignore dimensions %s' % image_url)
                continue

            # penalize images with "sprite" in their name
            if 'sprite' in image_url.lower():
                g.log.debug('penalizing sprite %s' % image_url)
                area /= 10

            if area > max_area:
                max_area = area
                max_url = image_url
        return max_url


class _EmbedlyScraper(Scraper):
    EMBEDLY_API_URL = "https://api.embed.ly/1/oembed"

    def __init__(self, url, can_embed_securely, autoplay=False, maxwidth=600):
        self.url = url
        self.can_embed_securely = can_embed_securely
        self.maxwidth = int(maxwidth)
        self.embedly_params = {}

        if autoplay:
            self.embedly_params["autoplay"] = "true"

    def _fetch_from_embedly(self, secure):
        param_dict = {
            "url": self.url,
            "format": "json",
            "maxwidth": self.maxwidth,
            "key": g.embedly_api_key,
            "secure": "true" if secure else "false",
        }

        param_dict.update(self.embedly_params)
        params = urllib.urlencode(param_dict)
        content = requests.get(self.EMBEDLY_API_URL + "?" + params).content
        return json.loads(content)

    def _make_media_object(self, oembed):
        if oembed.get("type") in ("video", "rich"):
            return {
                "type": domain(self.url),
                "oembed": oembed,
            }
        return None

    def scrape(self):
        oembed = self._fetch_from_embedly(secure=False)
        if not oembed:
            return None, None, None

        if oembed.get("type") == "photo":
            thumbnail_url = oembed.get("url")
        else:
            thumbnail_url = oembed.get("thumbnail_url")
        thumbnail = _make_thumbnail_from_url(thumbnail_url, referer=self.url)

        secure_oembed = {}
        if self.can_embed_securely:
            secure_oembed = self._fetch_from_embedly(secure=True)

        return (
            thumbnail,
            self._make_media_object(oembed),
            self._make_media_object(secure_oembed),
        )

    @classmethod
    def media_embed(cls, media_object):
        oembed = media_object["oembed"]

        html = oembed.get("html")
        width = oembed.get("width")
        height = oembed.get("height")
        if not (html and width and height):
            return

        return MediaEmbed(
            width=width,
            height=height,
            content=html,
        )


@memoize("media.embedly_services2", time=3600)
def _fetch_embedly_service_data():
    return requests.get("https://api.embed.ly/1/services/python").json


def _fetch_embedly_services():
    if not g.embedly_api_key:
        if g.debug:
            g.log.info("No embedly_api_key, using no key while in debug mode.")
        else:
            g.log.warning("No embedly_api_key configured. Will not use "
                          "embed.ly.")
            return []

    service_data = _fetch_embedly_service_data()

    services = []
    for service in service_data:
        services.append((
            re.compile("(?:%s)" % "|".join(service["regex"])),
            service["name"] in _SECURE_SERVICES,
        ))
    return services


def run():
    @g.stats.amqp_processor('scraper_q')
    def process_link(msg):
        fname = msg.body
        link = Link._by_fullname(msg.body, data=True)

        try:
            TimeoutFunction(_set_media, 30)(link, use_cache=True)
        except TimeoutFunctionException:
            print "Timed out on %s" % fname
        except KeyboardInterrupt:
            raise
        except:
            print "Error fetching %s" % fname
            print traceback.format_exc()

    amqp.consume_items('scraper_q', process_link)

########NEW FILE########
__FILENAME__ = memoize
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from hashlib import md5

from r2.lib.filters import _force_utf8
from r2.lib.cache import NoneResult, make_key
from r2.lib.lock import make_lock_factory
from pylons import g

make_lock = g.make_lock
memoizecache = g.memoizecache

def memoize(iden, time = 0, stale=False, timeout=30):
    def memoize_fn(fn):
        from r2.lib.memoize import NoneResult
        def new_fn(*a, **kw):

            #if the keyword param _update == True, the cache will be
            #overwritten no matter what
            update = kw.pop('_update', False)

            key = make_key(iden, *a, **kw)

            res = None if update else memoizecache.get(key, stale=stale)

            if res is None:
                # not cached, we should calculate it.
                with make_lock("memoize", 'memoize_lock(%s)' % key,
                               time=timeout, timeout=timeout):

                    # see if it was completed while we were waiting
                    # for the lock
                    stored = None if update else memoizecache.get(key)
                    if stored is not None:
                        # it was calculated while we were waiting
                        res = stored
                    else:
                        # okay now go and actually calculate it
                        res = fn(*a, **kw)
                        if res is None:
                            res = NoneResult
                        memoizecache.set(key, res, time=time)

            if res == NoneResult:
                res = None

            return res

        new_fn.memoized_fn = fn
        return new_fn
    return memoize_fn

@memoize('test')
def test(x, y):
    import time
    time.sleep(1)
    print 'calculating %d + %d' % (x, y)
    if x + y == 10:
        return None
    else:
        return x + y

########NEW FILE########
__FILENAME__ = menus
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from wrapped import CachedTemplate, Styled
from pylons import c, request, g
from utils import  query_string, timeago
from strings import StringHandler, plurals
from r2.lib.db import operators
import r2.lib.search as search
from r2.lib.filters import _force_unicode
from pylons.i18n import _, N_



class MenuHandler(StringHandler):
    """Bastard child of StringHandler and plurals.  Menus are
    typically a single word (and in some cases, a single plural word
    like 'moderators' or 'contributors' so this class first checks its
    own dictionary of string translations before falling back on the
    plurals list."""
    def __getattr__(self, attr):
        try:
            return StringHandler.__getattr__(self, attr)
        except KeyError:
            return getattr(plurals, attr)

# translation strings for every menu on the site
menu =   MenuHandler(hot          = _('hot'),
                     new          = _('new'),
                     old          = _('old'),
                     ups          = _('ups'),
                     downs        = _('downs'),
                     top          = _('top'),
                     more         = _('more'),
                     relevance    = _('relevance'),
                     controversial  = _('controversial'),
                     gilded       = _('gilded'),
                     confidence   = _('best'),
                     random       = _('random'),
                     saved        = _('saved {toolbar}'),
                     recommended  = _('recommended'),
                     rising       = _('rising'), 
                     admin        = _('admin'), 
                                 
                     # time sort words
                     hour         = _('this hour'),
                     day          = _('today'),
                     week         = _('this week'),
                     month        = _('this month'),
                     year         = _('this year'),
                     all          = _('all time'),
                                  
                     # "kind" words
                     spam         = _("spam"),
                     autobanned   = _("autobanned"),

                     # reddit header strings
                     prefs        = _("preferences"), 
                     submit       = _("submit"),
                     wiki         = _("wiki"),
                     blog         = _("blog"),
                     logout       = _("logout"),
                     
                     #reddit footer strings
                     contact      = _("contact us"),
                     buttons      = _("buttons"),
                     widget       = _("widget"), 
                     code         = _("source code"),
                     mobile       = _("mobile"), 
                     store        = _("store"),  
                     advertising  = _("advertise"),
                     gold         = _('reddit gold'),
                     reddits      = _('subreddits'),
                     team         = _('team'),
                     rules        = _('rules'),
                     jobs         = _('jobs'),

                     #preferences
                     options      = _('options'),
                     apps         = _("apps"),
                     feeds        = _("RSS feeds"),
                     friends      = _("friends"),
                     blocked      = _("blocked"),
                     update       = _("password/email"),
                     delete       = _("delete"),
                     otp          = _("two-factor authentication"),

                     # messages
                     compose      = _("compose"),
                     inbox        = _("inbox"),
                     sent         = _("sent"),

                     # comments
                     comments     = _("comments {toolbar}"),
                     related      = _("related"),
                     details      = _("details"),
                     duplicates   = _("other discussions (%(num)s)"),
                     traffic      = _("traffic stats"),
                     stylesheet   = _("stylesheet"),

                     # reddits
                     home         = _("home"),
                     about        = _("about"),
                     edit_subscriptions = _("edit subscriptions"),
                     community_settings = _("subreddit settings"),
                     moderators   = _("edit moderators"),
                     modmail      = _("moderator mail"),
                     contributors = _("edit approved submitters"),
                     banned       = _("ban users"),
                     banusers     = _("ban users"),
                     flair        = _("edit flair"),
                     log          = _("moderation log"),
                     modqueue     = _("moderation queue"),
                     unmoderated  = _("unmoderated links"),
                     
                     wikibanned        = _("ban wiki contributors"),
                     wikicontributors  = _("add wiki contributors"),
                     
                     wikirecentrevisions = _("recent wiki revisions"),
                     wikipageslist = _("wiki page list"),

                     popular      = _("popular"),
                     create       = _("create"),
                     mine         = _("my subreddits"),

                     i18n         = _("help translate"),
                     errors       = _("errors"),
                     awards       = _("awards"),
                     ads          = _("ads"),
                     promoted     = _("promoted"),
                     reporters    = _("reporters"),
                     reports      = _("reports"),
                     reportedauth = _("reported authors"),
                     info         = _("info"),
                     share        = _("share"),

                     overview     = _("overview"),
                     submitted    = _("submitted"),
                     liked        = _("liked"),
                     disliked     = _("disliked"),
                     hidden       = _("hidden {toolbar}"),
                     deleted      = _("deleted"),
                     reported     = _("reported"),
                     voting       = _("voting"),

                     promote        = _('advertising'),
                     new_promo      = _('create promotion'),
                     my_current_promos = _('my promoted links'),
                     current_promos = _('all promoted links'),
                     all_promos     = _('all'),
                     future_promos  = _('unseen'),
                     roadblock      = _('roadblock'),
                     inventory      = _('inventory'),
                     live_promos    = _('live'),
                     unpaid_promos  = _('unpaid'),
                     pending_promos = _('pending'),
                     rejected_promos = _('rejected'),

                     sitewide = _('sitewide'),
                     languages = _('languages'),
                     adverts = _('adverts'),

                     whitelist = _("whitelist")
                     )

def menu_style(type):
    """Simple manager function for the styled menus.  Returns a
    (style, css_class) pair given a 'type', defaulting to style =
    'dropdown' with no css_class."""
    default = ('dropdown', '')
    d = dict(heavydrop = ('dropdown', 'heavydrop'),
             lightdrop = ('dropdown', 'lightdrop'),
             tabdrop = ('dropdown', 'tabdrop'),
             srdrop = ('dropdown', 'srdrop'),
             flatlist =  ('flatlist', 'flat-list'),
             tabmenu = ('tabmenu', ''),
             formtab = ('tabmenu', 'formtab'),
             flat_vert = ('flatlist', 'flat-vert'),
             )
    return d.get(type, default)

class NavMenu(Styled):
    """generates a navigation menu.  The intention here is that the
    'style' parameter sets what template/layout to use to differentiate, say,
    a dropdown from a flatlist, while the optional _class, and _id attributes
    can be used to set individualized CSS."""

    use_post = False

    def __init__(self, options, default = None, title = '', type = "dropdown",
                 base_path = '', separator = '|', **kw):
        self.options = options
        self.base_path = base_path

        #add the menu style, but preserve existing css_class parameter
        kw['style'], css_class = menu_style(type)
        kw['css_class'] = css_class + ' ' + kw.get('css_class', '')

        #used by flatlist to delimit menu items
        self.separator = separator

        # since the menu contains the path info, it's buttons need a
        # configuration pass to get them pointing to the proper urls
        for opt in self.options:
            opt.build(self.base_path)

        # selected holds the currently selected button defined as the
        # one whose path most specifically matches the current URL
        # (possibly None)
        self.default = default
        self.selected = self.find_selected()

        Styled.__init__(self, title = title, **kw)

    def find_selected(self):
        maybe_selected = [o for o in self.options if o.is_selected()]
        if maybe_selected:
            # pick the button with the most restrictive pathing
            maybe_selected.sort(lambda x, y:
                                len(y.bare_path) - len(x.bare_path))
            return maybe_selected[0]
        elif self.default:
            #lookup the menu with the 'dest' that matches 'default'
            for opt in self.options:
                if opt.dest == self.default:
                    return opt

    def __iter__(self):
        for opt in self.options:
            yield opt

class NavButton(Styled):
    """Smallest unit of site navigation.  A button once constructed
    must also have its build() method called with the current path to
    set self.path.  This step is done automatically if the button is
    passed to a NavMenu instance upon its construction."""
    def __init__(self, title, dest, sr_path = True, 
                 nocname=False, opt = '', aliases = [],
                 target = "", style = "plain", use_params=False, **kw):
        # keep original dest to check against c.location when rendering
        aliases = set(_force_unicode(a.rstrip('/')) for a in aliases)
        if dest:
            aliases.add(_force_unicode(dest.rstrip('/')))

        self.use_params = use_params
        self.request_params = dict(request.GET)
        self.stripped_path = _force_unicode(request.path.rstrip('/').lower())

        Styled.__init__(self, style = style, sr_path = sr_path, 
                        nocname = nocname, target = target,
                        aliases = aliases, dest = dest,
                        selected = False, 
                        title = title, opt = opt, **kw)

    def build(self, base_path = ''):
        '''Generates the href of the button based on the base_path provided.'''

        # append to the path or update the get params dependent on presence
        # of opt 
        if self.opt:
            p = self.request_params.copy()
            if self.dest:
                p[self.opt] = self.dest
            elif self.opt in p:
                del p[self.opt]
        else:
            p = self.request_params.copy() if self.use_params else {}
            base_path = ("%s/%s/" % (base_path, self.dest)).replace('//', '/')

        self.action_params = p

        self.bare_path = _force_unicode(base_path.replace('//', '/')).lower()
        self.bare_path = self.bare_path.rstrip('/')
        self.base_path = base_path
        
        # append the query string
        base_path += query_string(p)
        
        # since we've been sloppy of keeping track of "//", get rid
        # of any that may be present
        self.path = base_path.replace('//', '/')

    def is_selected(self):
        """Given the current request path, would the button be selected."""
        if self.opt:
            if not self.dest and self.opt not in self.request_params:
                return True
            return self.request_params.get(self.opt, '') in self.aliases
        else:
            if self.stripped_path == self.bare_path:
                return True
            site_path = c.site.user_path.lower() + self.bare_path
            if self.sr_path and self.stripped_path == site_path:
                return True
            if self.bare_path and self.stripped_path.startswith(self.bare_path):
                return True
            if self.stripped_path in self.aliases:
                return True

    def selected_title(self):
        """returns the title of the button when selected (for cases
        when it is different from self.title)"""
        return self.title

class ModeratorMailButton(NavButton):
    def is_selected(self):
        if c.default_sr and not self.sr_path:
            return NavButton.is_selected(self)
        elif not c.default_sr and self.sr_path:
            return NavButton.is_selected(self)

class OffsiteButton(NavButton):
    def build(self, base_path = ''):
        self.sr_path = False
        self.path = self.bare_path = self.dest

    def cachable_attrs(self):
        return [('path', self.path), ('title', self.title)]

class SubredditButton(NavButton):
    from r2.models.subreddit import Frontpage, Mod, All, Random, RandomSubscription
    # Translation is deferred (N_); must be done per-request,
    # not at import/class definition time.
    # TRANSLATORS: This refers to /r/mod
    name_overrides = {Mod: N_("mod"),
    # TRANSLATORS: This refers to the user's front page
                      Frontpage: N_("front"),
                      All: N_("all"),
                      Random: N_("random"),
    # TRANSLATORS: Gold feature, "myrandom", a random subreddit from your subscriptions
                      RandomSubscription: N_("myrandom")}

    def __init__(self, sr, **kw):
        self.path = sr.path
        name = self.name_overrides.get(sr)
        # Run the name through deferred translation
        name = _(name) if name else sr.name
        NavButton.__init__(self, name, sr.path, False,
                           isselected = (c.site == sr), **kw)

    def build(self, base_path = ''):
        self.bare_path = ""

    def is_selected(self):
        return self.isselected

    def cachable_attrs(self):
        return [('path', self.path), ('title', self.title),
                ('isselected', self.isselected)]

class NamedButton(NavButton):
    """Convenience class for handling the majority of NavButtons
    whereby the 'title' is just the translation of 'name' and the
    'dest' defaults to the 'name' as well (unless specified
    separately)."""
    
    def __init__(self, name, sr_path = True, nocname=False, dest = None, fmt_args = {}, **kw):
        self.name = name.strip('/')
        menutext = menu[self.name] % fmt_args
        NavButton.__init__(self, menutext, name if dest is None else dest,
                           sr_path = sr_path, nocname=nocname, **kw)

class JsButton(NavButton):
    """A button which fires a JS event and thus has no path and cannot
    be in the 'selected' state"""
    def __init__(self, title, style = 'js', tab_name = None, **kw):
        NavButton.__init__(self, title, '#', style = style, tab_name = tab_name,
                           **kw)

    def build(self, *a, **kw):
        if self.tab_name:
            self.path = '#' + self.tab_name
        else:
            self.path = 'javascript:void(0)'

    def is_selected(self):
        return False

class PageNameNav(Styled):
    """generates the links and/or labels which live in the header
    between the header image and the first nav menu (e.g., the
    subreddit name, the page name, etc.)"""
    pass

class SimplePostMenu(NavMenu):
    """Parent class of menus used for sorting and time sensitivity of
    results. Defines a type of menu that uses hidden forms to POST the user's
    selection to a handler that may commit the user's choice as a preference
    change before redirecting to a URL that also includes the user's choice.
    If other user's load this URL, they won't affect their own preferences, but
    the given choice will apply for that page load.

    The value of the POST/GET parameter must be one of the entries in
    'cls.options'.  This parameter is also used to construct the list
    of NavButtons contained in this Menu instance.  The goal here is
    to have a menu object which 'out of the box' is self validating."""
    options   = []
    hidden_options = []
    name      = ''
    title     = ''
    default = None
    type = 'lightdrop'

    def __init__(self, **kw):
        buttons = []
        for name in self.options:
            css_class = 'hidden' if name in self.hidden_options else ''
            button = NavButton(self.make_title(name), name, opt=self.name,
                               style='post', css_class=css_class)
            buttons.append(button)

        kw['default'] = kw.get('default', self.default)
        kw['base_path'] = kw.get('base_path') or request.path
        NavMenu.__init__(self, buttons, type = self.type, **kw)

    def make_title(self, attr):
        return menu[attr]

    @classmethod
    def operator(self, sort):
        """Converts the opt into a DB-esque operator used for sorting results"""
        return None

class SortMenu(SimplePostMenu):
    """The default sort menu."""
    name      = 'sort'
    default   = 'hot'
    options   = ('hot', 'new', 'top', 'old', 'controversial')

    def __init__(self, **kw):
        kw['title'] = _("sorted by")
        SimplePostMenu.__init__(self, **kw)

    @classmethod
    def operator(self, sort):
        if sort == 'hot':
            return operators.desc('_hot')
        elif sort == 'new':
            return operators.desc('_date')
        elif sort == 'old':
            return operators.asc('_date')
        elif sort == 'top':
            return operators.desc('_score')
        elif sort == 'controversial':
            return operators.desc('_controversy')
        elif sort == 'confidence':
            return operators.desc('_confidence')
        elif sort == 'random':
            return operators.shuffled('_confidence')

class ProfileSortMenu(SortMenu):
    default   = 'new'
    options   = ('hot', 'new', 'top', 'controversial')

class CommentSortMenu(SortMenu):
    """Sort menu for comments pages"""
    default   = 'confidence'
    options   = ('confidence', 'top', 'new', 'hot', 'controversial', 'old',
                 'random')
    hidden_options = ('random',)
    use_post  = True

class SearchSortMenu(SortMenu):
    """Sort menu for search pages."""
    default   = 'relevance'
    mapping   = search.sorts
    options   = mapping.keys()

    @classmethod
    def operator(cls, sort):
        return cls.mapping.get(sort, cls.mapping[cls.default])

class RecSortMenu(SortMenu):
    """Sort menu for recommendation page"""
    default   = 'new'
    options   = ('hot', 'new', 'top', 'controversial', 'relevance')

class KindMenu(SimplePostMenu):
    name    = 'kind'
    default = 'all'
    options = ('links', 'comments', 'messages', 'all')

    def __init__(self, **kw):
        kw['title'] = _("kind")
        SimplePostMenu.__init__(self, **kw)

    def make_title(self, attr):
        if attr == "all":
            return _("all")
        return menu[attr]

class TimeMenu(SimplePostMenu):
    """Menu for setting the time interval of the listing (from 'hour' to 'all')"""
    name      = 't'
    default   = 'all'
    options   = ('hour', 'day', 'week', 'month', 'year', 'all')

    def __init__(self, **kw):
        kw['title'] = _("links from")
        SimplePostMenu.__init__(self, **kw)

    @classmethod
    def operator(self, time):
        from r2.models import Link
        if time != 'all':
            return Link.c._date >= timeago(time)

class ControversyTimeMenu(TimeMenu):
    """time interval for controversial sort.  Make default time 'day' rather than 'all'"""
    default = 'day'
    use_post = True

class SubredditMenu(NavMenu):
    def find_selected(self):
        """Always return False so the title is always displayed"""
        return None

class JsNavMenu(NavMenu):
    def find_selected(self):
        """Always return the first element."""
        return self.options[0]

# --------------------
# TODO: move to admin area
class AdminReporterMenu(SortMenu):
    default = 'top'
    options = ('hot', 'new', 'top')

class AdminKindMenu(KindMenu):
    options = ('all', 'links', 'comments', 'spam', 'autobanned')


class AdminTimeMenu(TimeMenu):
    get_param = 't'
    default   = 'day'
    options   = ('hour', 'day', 'week')



########NEW FILE########
__FILENAME__ = merge
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import subprocess
import tempfile
import difflib
from pylons.i18n import _
from pylons import g

class ConflictException(Exception):
    def __init__(self, new, your, original):
        self.your = your
        self.new = new
        self.original = original
        self.htmldiff = make_htmldiff(new, your, _("current edit"), _("your edit"))
        Exception.__init__(self)


def make_htmldiff(a, b, adesc, bdesc):
    diffcontent = difflib.HtmlDiff(wrapcolumn=60)
    return diffcontent.make_table(a.splitlines(),
                                  b.splitlines(),
                                  fromdesc=adesc,
                                  todesc=bdesc,
                                  context=3)

def threewaymerge(original, a, b):
    temp_dir = g.diff3_temp_location if g.diff3_temp_location else None
    data = [a, original, b]
    files = []
    try:
        for d in data:
            f = tempfile.NamedTemporaryFile(dir=temp_dir)
            f.write(d.encode('utf-8'))
            f.flush()
            files.append(f)
        try:
            final = subprocess.check_output(["diff3", "-a", "--merge"] + [f.name for f in files])
        except subprocess.CalledProcessError:
            raise ConflictException(b, a, original)
    finally:
        for f in files:
            f.close()
    return final.decode('utf-8')

if __name__ == "__main__":
    class test_globals:
        diff3_temp_location = None
    
    g = test_globals()
    
    original = "Hello people of the human rance\n\nHow are you tday"
    a = "Hello people of the human rance\n\nHow are you today"
    b = "Hello people of the human race\n\nHow are you tday"
    
    print threewaymerge(original, a, b)
    
    g.diff3_temp_location = '/dev/shm'
    
    print threewaymerge(original, a, b)

########NEW FILE########
__FILENAME__ = campaigns_to_things
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import sys
from collections import defaultdict
from r2.models import *

def fix_trans_id():
    bad_campaigns = list(PromoCampaign._query(PromoCampaign.c.trans_id == 1, data=True))
    num_bad_campaigns = len(bad_campaigns)

    if not num_bad_campaigns:
        print "No campaigns with trans_id == 1"
        return

    # print some info and prompt user to continue
    print ("Found %d campaigns with trans_id == 1. \n"
           "Campaigns ids: %s \n" 
           "Press 'c' to fix them or any other key to abort." %
           (num_bad_campaigns, [pc._id for pc in bad_campaigns]))
    input_char = sys.stdin.read(1)
    if input_char != 'c' and input_char != 'C':
        print "aborting..."
        return

    # log the ids for reference
    print ("Fixing %d campaigns with bad freebie trans_id: %s" % 
           (num_bad_campaigns, [pc._id for pc in bad_campaigns]))

    # get corresponding links and copy trans_id from link data to campaign thing
    link_ids = set([campaign.link_id for campaign in bad_campaigns])
    print "Fetching associated links: %s" % link_ids
    try:
        links = Link._byID(link_ids, data=True, return_dict=False)
    except NotFound, e:
        print("Invalid data: Some promocampaigns have invalid link_ids. "
              "Please delete these campaigns or fix the data before "
              "continuing. Exception: %s" % e)

    # organize bad campaigns by link_id
    bad_campaigns_by_link = defaultdict(list)
    for c in bad_campaigns:
        bad_campaigns_by_link[c.link_id].append(c)

    # iterate through links and copy trans_id from pickled list on the link to 
    # the campaign thing
    failed = []
    for link in links:
        link_campaigns = getattr(link, "campaigns")
        thing_campaigns = bad_campaigns_by_link[link._id]
        for campaign in thing_campaigns:
            try:
                sd, ed, bid, sr_name, trans_id = link_campaigns[campaign._id]
                if trans_id != campaign.trans_id:
                    campaign.trans_id = trans_id
                    campaign._commit()
            except:
                failed.append({
                    'link_id': link._id,
                    'campaign_id': campaign._id,
                    'exc type': sys.exc_info()[0],
                    'exc msg': sys.exc_info()[1]
                })

    # log the actions for future reference
    msg = ("%d of %d campaigns updated successfully. %d updates failed: %s" %
           (num_bad_campaigns, num_bad_campaigns - len(failed), len(failed), failed))
    print msg

        

########NEW FILE########
__FILENAME__ = comment_sorts
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib import mr_tools
from r2.lib import utils
from r2.lib.utils import to36
from r2.lib.db import sorts

# dumps | sort | join_comments() | combine_links | sort | store_sorts()

# just use the comment-dump query from mr_permacache with the link_id
# data field
def join_comments():
    return mr_tools.join_things(('link_id',))

def combine_links():
    @mr_tools.dataspec_m_thing(('link_id', int))
    def _process(t):
        thing_id = t.thing_id
        id36 = to36(thing_id)

        link_id = t.link_id
        link_id36 = to36(link_id)

        ups, downs, timestamp = t.ups, t.downs, t.timestamp

        yield link_id36+'_controversy', id36, sorts.controversy(ups, downs)
        yield link_id36+'_hot',         id36, sorts._hot(ups, downs, timestamp)
        yield link_id36+'_confidence',  id36, sorts.confidence(ups, downs)
        yield link_id36+'_score',       id36, sorts.score(ups, downs)
        yield link_id36+'_date',        id36, timestamp

    return mr_tools.mr_map(_process)

def store_sorts():
    from r2.models import CommentSortsCache
    from r2.lib.db.tdb_cassandra import CL

    # we're going to do our own Cassandra work here, skipping the
    # tdb_cassandra layer
    cf = CommentSortsCache._cf

    def _process(key, vals):
        vals = dict(vals)

        # this has already been serialised to strings
        cf.insert(key, vals, write_consistency_level = CL.ANY)

        return []

    return mr_tools.mr_reduce(_process)


########NEW FILE########
__FILENAME__ = migrate
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

"""
One-time use functions to migrate from one reddit-version to another
"""
from r2.lib.promote import *

def add_allow_top_to_srs():
    "Add the allow_top property to all stored subreddits"
    from r2.models import Subreddit
    from r2.lib.db.operators import desc
    from r2.lib.utils import fetch_things2

    q = Subreddit._query(Subreddit.c._spam == (True,False),
                         sort = desc('_date'))
    for sr in fetch_things2(q):
        sr.allow_top = True; sr._commit()

def subscribe_to_blog_and_annoucements(filename):
    import re
    from time import sleep
    from r2.models import Account, Subreddit

    r_blog = Subreddit._by_name("blog")
    r_announcements = Subreddit._by_name("announcements")

    contents = file(filename).read()
    numbers = [ int(s) for s in re.findall("\d+", contents) ]

#    d = Account._byID(numbers, data=True)

#   for i, account in enumerate(d.values()):
    for i, account_id in enumerate(numbers):
        account = Account._byID(account_id, data=True)

        for sr in r_blog, r_announcements:
            if sr.add_subscriber(account):
                sr._incr("_ups", 1)
                print ("%d: subscribed %s to %s" % (i, account.name, sr.name))
            else:
                print ("%d: didn't subscribe %s to %s" % (i, account.name, sr.name))


def upgrade_messages(update_comments = True, update_messages = True,
                     update_trees = True):
    from r2.lib.db import queries
    from r2.lib import comment_tree, cache
    from r2.models import Account
    from pylons import g
    accounts = set()

    def batch_fn(items):
        g.reset_caches()
        return items
    
    if update_messages or update_trees:
        q = Message._query(Message.c.new == True,
                           sort = desc("_date"),
                           data = True)
        for m in fetch_things2(q, batch_fn = batch_fn):
            print m,m._date
            if update_messages:
                accounts = accounts | queries.set_unread(m, m.new)
            else:
                accounts.add(m.to_id)
    if update_comments:
        q = Comment._query(Comment.c.new == True,
                           sort = desc("_date"))
        q._filter(Comment.c._id < 26152162676)

        for m in fetch_things2(q, batch_fn = batch_fn):
            print m,m._date
            queries.set_unread(m, True)

    print "Precomputing comment trees for %d accounts" % len(accounts)

    for i, a in enumerate(accounts):
        if not isinstance(a, Account):
            a = Account._byID(a)
        print i, a
        comment_tree.user_messages(a)

def recompute_unread(min_date = None):
    from r2.models import Inbox, Account, Comment, Message
    from r2.lib.db import queries

    def load_accounts(inbox_rel):
        accounts = set()
        q = inbox_rel._query(eager_load = False, data = False,
                             sort = desc("_date"))
        if min_date:
            q._filter(inbox_rel.c._date > min_date)

        for i in fetch_things2(q):
            accounts.add(i._thing1_id)

        return accounts

    accounts_m = load_accounts(Inbox.rel(Account, Message))
    for i, a in enumerate(accounts_m):
        a = Account._byID(a)
        print "%s / %s : %s" % (i, len(accounts_m), a)
        queries.get_unread_messages(a).update()
        queries.get_unread_comments(a).update()
        queries.get_unread_selfreply(a).update()

    accounts = load_accounts(Inbox.rel(Account, Comment)) - accounts_m
    for i, a in enumerate(accounts):
        a = Account._byID(a)
        print "%s / %s : %s" % (i, len(accounts), a)
        queries.get_unread_comments(a).update()
        queries.get_unread_selfreply(a).update()



def pushup_permacache(verbosity=1000):
    """When putting cassandra into the permacache chain, we need to
       push everything up into the rest of the chain, so this is
       everything that uses the permacache, as of that check-in."""
    from pylons import g
    from r2.models import Link, Subreddit, Account
    from r2.lib.db.operators import desc
    from r2.lib.comment_tree import comments_key, messages_key
    from r2.lib.utils import fetch_things2, in_chunks
    from r2.lib.utils import last_modified_key
    from r2.lib.promote import promoted_memo_key
    from r2.lib.subreddit_search import load_all_reddits
    from r2.lib.db import queries
    from r2.lib.cache import CassandraCacheChain

    authority = g.permacache.caches[-1]
    nonauthority = CassandraCacheChain(g.permacache.caches[1:-1])

    def populate(keys):
        vals = authority.simple_get_multi(keys)
        if vals:
            nonauthority.set_multi(vals)

    def gen_keys():
        yield promoted_memo_key

        # just let this one do its own writing
        load_all_reddits()

        yield queries.get_all_comments().iden

        l_q = Link._query(Link.c._spam == (True, False),
                          Link.c._deleted == (True, False),
                          sort=desc('_date'),
                          data=True,
                          )
        for link in fetch_things2(l_q, verbosity):
            yield comments_key(link._id)
            yield last_modified_key(link, 'comments')

        a_q = Account._query(Account.c._spam == (True, False),
                             sort=desc('_date'),
                             )
        for account in fetch_things2(a_q, verbosity):
            yield messages_key(account._id)
            yield last_modified_key(account, 'overview')
            yield last_modified_key(account, 'commented')
            yield last_modified_key(account, 'submitted')
            yield last_modified_key(account, 'liked')
            yield last_modified_key(account, 'disliked')
            yield queries.get_comments(account, 'new', 'all').iden
            yield queries.get_submitted(account, 'new', 'all').iden
            yield queries.get_liked(account).iden
            yield queries.get_disliked(account).iden
            yield queries.get_hidden(account).iden
            yield queries.get_saved(account).iden
            yield queries.get_inbox_messages(account).iden
            yield queries.get_unread_messages(account).iden
            yield queries.get_inbox_comments(account).iden
            yield queries.get_unread_comments(account).iden
            yield queries.get_inbox_selfreply(account).iden
            yield queries.get_unread_selfreply(account).iden
            yield queries.get_sent(account).iden

        sr_q = Subreddit._query(Subreddit.c._spam == (True, False),
                                sort=desc('_date'),
                                )
        for sr in fetch_things2(sr_q, verbosity):
            yield last_modified_key(sr, 'stylesheet_contents')
            yield queries.get_links(sr, 'hot', 'all').iden
            yield queries.get_links(sr, 'new', 'all').iden

            for sort in 'top', 'controversial':
                for time in 'hour', 'day', 'week', 'month', 'year', 'all':
                    yield queries.get_links(sr, sort, time,
                                            merge_batched=False).iden
            yield queries.get_spam_links(sr).iden
            yield queries.get_spam_comments(sr).iden
            yield queries.get_reported_links(sr).iden
            yield queries.get_reported_comments(sr).iden
            yield queries.get_subreddit_messages(sr).iden
            yield queries.get_unread_subreddit_messages(sr).iden

    done = 0
    for keys in in_chunks(gen_keys(), verbosity):
        g.reset_caches()
        done += len(keys)
        print 'Done %d: %r' % (done, keys[-1])
        populate(keys)


def port_cassavotes():
    from r2.models import Vote, Account, Link, Comment
    from r2.models.vote import CassandraVote, CassandraLinkVote, CassandraCommentVote
    from r2.lib.db.tdb_cassandra import CL
    from r2.lib.utils import fetch_things2, to36, progress

    ts = [(Vote.rel(Account, Link), CassandraLinkVote),
          (Vote.rel(Account, Comment), CassandraCommentVote)]

    dataattrs = set(['valid_user', 'valid_thing', 'ip', 'organic'])

    for prel, crel in ts:
        vq = prel._query(sort=desc('_date'),
                         data=True,
                         eager_load=False)
        vq = fetch_things2(vq)
        vq = progress(vq, persec=True)
        for v in vq:
            t1 = to36(v._thing1_id)
            t2 = to36(v._thing2_id)
            cv = crel(thing1_id = t1,
                      thing2_id = t2,
                      date=v._date,
                      name=v._name)
            for dkey, dval in v._t.iteritems():
                if dkey in dataattrs:
                    setattr(cv, dkey, dval)

            cv._commit(write_consistency_level=CL.ONE)


def port_cassaurls(after_id=None, estimate=15231317):
    from r2.models import Link, LinksByUrl
    from r2.lib.db import tdb_cassandra
    from r2.lib.db.operators import desc
    from r2.lib.db.tdb_cassandra import CL
    from r2.lib.utils import fetch_things2, in_chunks, progress

    q = Link._query(Link.c._spam == (True, False),
                    sort=desc('_date'), data=True)
    if after_id:
        q._after(Link._byID(after_id,data=True))
    q = fetch_things2(q, chunk_size=500)
    q = progress(q, estimate=estimate)
    q = (l for l in q
         if getattr(l, 'url', 'self') != 'self'
         and not getattr(l, 'is_self', False))
    chunks = in_chunks(q, 500)

    for chunk in chunks:
        with LinksByUrl._cf.batch(write_consistency_level = CL.ONE) as b:
            for l in chunk:
                k = LinksByUrl._key_from_url(l.url)
                if k:
                    b.insert(k, {l._id36: l._id36})

def port_deleted_links(after_id=None):
    from r2.models import Link
    from r2.lib.db.operators import desc
    from r2.models.query_cache import CachedQueryMutator
    from r2.lib.db.queries import get_deleted_links
    from r2.lib.utils import fetch_things2, in_chunks, progress

    q = Link._query(Link.c._deleted == True,
                    Link.c._spam == (True, False),
                    sort=desc('_date'), data=True)
    q = fetch_things2(q, chunk_size=500)
    q = progress(q, verbosity=1000)

    for chunk in in_chunks(q):
        with CachedQueryMutator() as m:
            for link in chunk:
                query = get_deleted_links(link.author_id)
                m.insert(query, [link])

def convert_query_cache_to_json():
    import cPickle
    from r2.models.query_cache import json, UserQueryCache

    with UserQueryCache._cf.batch() as m:
        for key, columns in UserQueryCache._cf.get_range():
            out = {}
            for ckey, cvalue in columns.iteritems():
                try:
                    raw = cPickle.loads(cvalue)
                except cPickle.UnpicklingError:
                    continue
                out[ckey] = json.dumps(raw)
            m.insert(key, out)

def populate_spam_filtered():
    from r2.lib.db.queries import get_spam_links, get_spam_comments
    from r2.lib.db.queries import get_spam_filtered_links, get_spam_filtered_comments
    from r2.models.query_cache import CachedQueryMutator

    def was_filtered(thing):
        if thing._spam and not thing._deleted and \
           getattr(thing, 'verdict', None) != 'mod-removed':
            return True
        else:
            return False

    q = Subreddit._query(sort = asc('_date'))
    for sr in fetch_things2(q):
        print 'Processing %s' % sr.name
        links = Thing._by_fullname(get_spam_links(sr), data=True,
                                   return_dict=False)
        comments = Thing._by_fullname(get_spam_comments(sr), data=True,
                                      return_dict=False)
        insert_links = [l for l in links if was_filtered(l)]
        insert_comments = [c for c in comments if was_filtered(c)]
        with CachedQueryMutator() as m:
            m.insert(get_spam_filtered_links(sr), insert_links)
            m.insert(get_spam_filtered_comments(sr), insert_comments)

########NEW FILE########
__FILENAME__ = mr_domains
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

"""
Generate the data for the listings for the time-based Subreddit
queries. The format is eventually that of the CachedResults objects
used by r2.lib.db.queries (with some intermediate steps), so changes
there may warrant changes here
"""

# to run:
"""
export LINKDBHOST=prec01
export USER=ri
export INI=production.ini
cd ~/reddit/r2
time psql -F"\t" -A -t -d newreddit -U $USER -h $LINKDBHOST \
     -c "\\copy (select t.thing_id, 'thing', 'link',
                        t.ups, t.downs, t.deleted, t.spam, extract(epoch from t.date)
                   from reddit_thing_link t
                  where not t.spam and not t.deleted
                  )
                  to 'reddit_thing_link.dump'"
time psql -F"\t" -A -t -d newreddit -U $USER -h $LINKDBHOST \
     -c "\\copy (select d.thing_id, 'data', 'link',
                        d.key, d.value
                   from reddit_data_link d
                  where d.key = 'url' ) to 'reddit_data_link.dump'"
cat reddit_data_link.dump reddit_thing_link.dump | sort -T. -S200m | paster --plugin=r2 run $INI r2/lib/migrate/mr_domains.py -c "join_links()" > links.joined
cat links.joined | paster --plugin=r2 run $INI r2/lib/migrate/mr_domains.py -c "time_listings()" | sort -T. -S200m | paster --plugin=r2 run $INI r2/lib/migrate/mr_domains.py -c "write_permacache()"
"""

import sys

from r2.models import Account, Subreddit, Link
from r2.lib.db.sorts import epoch_seconds, score, controversy, _hot
from r2.lib.db import queries
from r2.lib import mr_tools
from r2.lib.utils import timeago, UrlParser
from r2.lib.jsontemplates import make_fullname # what a strange place
                                               # for this function
def join_links():
    mr_tools.join_things(('url',))


def time_listings(times = ('all',)):
    oldests = dict((t, epoch_seconds(timeago('1 %s' % t)))
                   for t in times if t != "all")
    oldests['all'] = epoch_seconds(timeago('10 years'))

    @mr_tools.dataspec_m_thing(("url", str),)
    def process(link):
        assert link.thing_type == 'link'

        timestamp = link.timestamp
        fname = make_fullname(Link, link.thing_id)

        if not link.spam and not link.deleted:
            if link.url:
                domains = UrlParser(link.url).domain_permutations()
            else:
                domains = []
            ups, downs = link.ups, link.downs

            for tkey, oldest in oldests.iteritems():
                if timestamp > oldest:
                    sc = score(ups, downs)
                    contr = controversy(ups, downs)
                    h = _hot(ups, downs, timestamp)
                    for domain in domains:
                        yield ('domain/top/%s/%s' % (tkey, domain),
                               sc, timestamp, fname)
                        yield ('domain/controversial/%s/%s' % (tkey, domain),
                               contr, timestamp, fname)
                        if tkey == "all":
                            yield ('domain/hot/%s/%s' % (tkey, domain),
                                   h, timestamp, fname)
                            yield ('domain/new/%s/%s' % (tkey, domain),
                                   timestamp, timestamp, fname)

    mr_tools.mr_map(process)

def store_keys(key, maxes):
    # we're building queries using queries.py, but we could make the
    # queries ourselves if we wanted to avoid the individual lookups
    # for accounts and subreddits.

    # Note that we're only generating the 'sr-' type queries here, but
    # we're also able to process the other listings generated by the
    # old migrate.mr_permacache for convenience

    userrel_fns = dict(liked = queries.get_liked,
                       disliked = queries.get_disliked,
                       saved = queries.get_saved,
                       hidden = queries.get_hidden)

    if key.startswith('user-'):
        acc_str, keytype, account_id = key.split('-')
        account_id = int(account_id)
        fn = queries.get_submitted if keytype == 'submitted' else queries.get_comments
        q = fn(Account._byID(account_id), 'new', 'all')
        q._insert_tuples([(fname, float(timestamp))
                    for (timestamp, fname)
                    in maxes])

    elif key.startswith('sr-'):
        sr_str, sort, time, sr_id = key.split('-')
        sr_id = int(sr_id)

        if sort == 'controversy':
            # I screwed this up in the mapper and it's too late to fix
            # it
            sort = 'controversial'

        q = queries.get_links(Subreddit._byID(sr_id), sort, time)
        q._insert_tuples([tuple([item[-1]] + map(float, item[:-1]))
                    for item in maxes])
    elif key.startswith('domain/'):
        d_str, sort, time, domain = key.split('/')
        q = queries.get_domain_links(domain, sort, time)
        q._insert_tuples([tuple([item[-1]] + map(float, item[:-1]))
                    for item in maxes])


    elif key.split('-')[0] in userrel_fns:
        key_type, account_id = key.split('-')
        account_id = int(account_id)
        fn = userrel_fns[key_type]
        q = fn(Account._byID(account_id))
        q._insert_tuples([tuple([item[-1]] + map(float, item[:-1]))
                    for item in maxes])


def write_permacache(fd = sys.stdin):
    mr_tools.mr_reduce_max_per_key(lambda x: map(float, x[:-1]), num=1000,
                                   post=store_keys,
                                   fd = fd)

########NEW FILE########
__FILENAME__ = mr_permacache
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

"""
Try to regenerate the permacache items devoted to listings after a
storage failure in Cassandra
"""

"""
cat > mr_permacache <<HERE
#!/bin/sh
cd ~/reddit/r2
paster run staging.ini ./mr_permacache.py -c "\$1"
HERE
chmod u+x mr_permacache

LINKDBHOST=prec01
COMMENTDBHOST=db02s1
VOTEDBHOST=db03s1
SAVEHIDEDBHOST=db01s1

## links
time psql -F"\t" -A -t -d newreddit -U ri -h $LINKDBHOST \
     -c "\\copy (select t.thing_id, 'thing', 'link',
                        t.ups, t.downs, t.deleted, t.spam, extract(epoch from t.date)
                   from reddit_thing_link t) to 'reddit_thing_link.dump'"
time psql -F"\t" -A -t -d newreddit -U ri -h $LINKDBHOST \
     -c "\\copy (select d.thing_id, 'data', 'link',
                        d.key, d.value
                   from reddit_data_link d
                  where d.key = 'author_id' or d.key = 'sr_id') to 'reddit_data_link.dump'"
pv reddit_data_link.dump reddit_thing_link.dump | sort -T. -S200m | ./mr_permacache "join_links()" > links.joined
pv links.joined | ./mr_permacache "link_listings()" | sort -T. -S200m > links.listings

## comments
psql -F"\t" -A -t -d newreddit -U ri -h $COMMENTDBHOST \
     -c "\\copy (select t.thing_id, 'thing', 'comment',
                        t.ups, t.downs, t.deleted, t.spam, extract(epoch from t.date)
                   from reddit_thing_comment t) to 'reddit_thing_comment.dump'"
psql -F"\t" -A -t -d newreddit -U ri -h $COMMENTDBHOST \
     -c "\\copy (select d.thing_id, 'data', 'comment',
                        d.key, d.value
                   from reddit_data_comment d
                  where d.key = 'author_id') to 'reddit_data_comment.dump'"
cat reddit_data_comment.dump reddit_thing_comment.dump | sort -T. -S200m | ./mr_permacache "join_comments()" > comments.joined
cat links.joined | ./mr_permacache "comment_listings()" | sort -T. -S200m > comments.listings

## linkvotes
psql -F"\t" -A -t -d newreddit -U ri -h $VOTEDBHOST \
     -c "\\copy (select r.rel_id, 'vote_account_link',
                        r.thing1_id, r.thing2_id, r.name, extract(epoch from r.date)
                   from reddit_rel_vote_account_link r) to 'reddit_linkvote.dump'"
pv reddit_linkvote.dump | ./mr_permacache "linkvote_listings()" | sort -T. -S200m > linkvotes.listings

#savehide
psql -F"\t" -A -t -d newreddit -U ri -h $SAVEHIDEDBHOST \
     -c "\\copy (select r.rel_id, 'savehide',
                        r.thing1_id, r.thing2_id, r.name, extract(epoch from r.date)
                   from reddit_rel_savehide r) to 'reddit_savehide.dump'"
pv reddit_savehide.dump | ./mr_permacache "savehide_listings()" | sort -T. -S200m > savehide.listings

## load them up
# the individual .listings files are sorted so even if it's not sorted
# overall we don't need to re-sort them
mkdir listings
pv *.listings | ./mr_permacache "top1k_writefiles('listings')"
./mr_permacache "write_permacache_from_dir('$PWD/listings')"

"""

import os, os.path, errno
import sys
import itertools
from hashlib import md5

from r2.lib import mr_tools
from r2.lib.mr_tools import dataspec_m_thing, dataspec_m_rel, join_things


from dateutil.parser import parse as parse_timestamp

from r2.models import *
from r2.lib.db.sorts import epoch_seconds, score, controversy, _hot
from r2.lib.utils import fetch_things2, in_chunks, progress, UniqueIterator, tup
from r2.lib import comment_tree
from r2.lib.db import queries

from r2.lib.jsontemplates import make_fullname # what a strange place
                                               # for this function

def join_links():
    join_things(('author_id', 'sr_id'))

def link_listings():
    @dataspec_m_thing(('author_id', int),
                      ('sr_id', int))
    def process(link):
        assert link.thing_type == 'link'

        author_id = link.author_id
        timestamp = link.timestamp
        fname = make_fullname(Link, link.thing_id)

        yield 'user-submitted-%d' % author_id, timestamp, fname
        if not link.spam:
            sr_id = link.sr_id
            ups, downs = link.ups, link.downs

            yield ('sr-hot-all-%d' % sr_id, _hot(ups, downs, timestamp),
                   timestamp, fname)
            yield 'sr-new-all-%d' % sr_id, timestamp, fname
            yield 'sr-top-all-%d' % sr_id, score(ups, downs), timestamp, fname
            yield ('sr-controversial-all-%d' % sr_id,
                   controversy(ups, downs), timestamp, fname)
            for time in '1 year', '1 month', '1 week', '1 day', '1 hour':
                if timestamp > epoch_seconds(timeago(time)):
                    tkey = time.split(' ')[1]
                    yield ('sr-top-%s-%d' % (tkey, sr_id),
                           score(ups, downs), timestamp, fname)
                    yield ('sr-controversial-%s-%d' % (tkey, sr_id),
                           controversy(ups, downs),
                           timestamp, fname)

    mr_tools.mr_map(process)

def join_comments():
    join_things(('author_id',))

def comment_listings():
    @dataspec_m_thing(('author_id', int),)
    def process(comment):
        assert comment.thing_type == 'comment'

        yield ('user-commented-%d' % comment.author_id,
               comment.timestamp, make_fullname(Comment, comment.thing_id))

    mr_tools.mr_map(process)

def rel_listings(names, thing2_cls = Link):
    # names examples: {'1': 'liked',
    #                  '-1': 'disliked'}
    @dataspec_m_rel()
    def process(rel):
        if rel.name in names:
            yield ('%s-%s' % (names[rel.name], rel.thing1_id), rel.timestamp,
                   make_fullname(thing2_cls, rel.thing2_id))
    mr_tools.mr_map(process)

def linkvote_listings():
    rel_listings({'1': 'liked',
                  '-1': 'disliked'})

def savehide_listings():
    rel_listings({'save': 'saved',
                  'hide': 'hidden'})

def insert_to_query(q, items):
    q._insert_tuples(items)

def store_keys(key, maxes):
    # we're building queries from queries.py, but we could avoid this
    # by making the queries ourselves if we wanted to avoid the
    # individual lookups for accounts and subreddits
    userrel_fns = dict(liked = queries.get_liked,
                       disliked = queries.get_disliked,
                       saved = queries.get_saved,
                       hidden = queries.get_hidden)
    if key.startswith('user-'):
        acc_str, keytype, account_id = key.split('-')
        account_id = int(account_id)
        fn = queries.get_submitted if keytype == 'submitted' else queries.get_comments
        q = fn(Account._byID(account_id), 'new', 'all')
        insert_to_query(q, [(fname, float(timestamp))
                            for (timestamp, fname)
                            in maxes ])
    elif key.startswith('sr-'):
        sr_str, sort, time, sr_id = key.split('-')
        sr_id = int(sr_id)

        if sort == 'controversy':
            # I screwed this up in the mapper and it's too late to fix
            # it
            sort = 'controversial'

        q = queries.get_links(Subreddit._byID(sr_id), sort, time)
        insert_to_query(q, [tuple([item[-1]] + map(float, item[:-1]))
                            for item in maxes])

    elif key.split('-')[0] in userrel_fns:
        key_type, account_id = key.split('-')
        account_id = int(account_id)
        fn = userrel_fns[key_type]
        q = fn(Account._byID(account_id))
        insert_to_query(q, [tuple([item[-1]] + map(float, item[:-1]))
                            for item in maxes])

def top1k_writefiles(dirname):
    """Divide up the top 1k of each key into its own file to make
       restarting after a failure much easier. Pairs with
       write_permacache_from_dir"""
    def hashdir(name, levels = [3]):
        # levels is a list of how long each stage if the hashdirname
        # should be. So [2,2] would make dirs like
        # 'ab/cd/thelisting.txt' (and this function would just return
        # the string 'ab/cd', so that you have the dirname that you
        # can create before os.path.joining to the filename)
        h = md5(name).hexdigest()

        last = 0
        dirs = []
        for l in levels:
            dirs.append(h[last:last+l])
            last += l

        return os.path.join(*dirs)

    def post(key, maxes):
        # we're taking a hash like 12345678901234567890123456789012
        # and making a directory name two deep out of the first half
        # of the characters. We may want to tweak this as the number
        # of listings

        hd = os.path.join(dirname, hashdir(key))
        try:
            os.makedirs(hd)
        except OSError as e:
            if e.errno != errno.EEXIST:
                raise
        filename = os.path.join(hd, key)

        with open(filename, 'w') as f:
            for item in maxes:
                f.write('%s\t' % key)
                f.write('\t'.join(item))
                f.write('\n')
        
    mr_tools.mr_reduce_max_per_key(lambda x: map(float, x[:-1]), num=1000,
                                   post=post)

def top1k_writepermacache(fd = sys.stdin):
    mr_tools.mr_reduce_max_per_key(lambda x: map(float, x[:-1]), num=1000,
                                   post=store_keys,
                                   fd = fd)

def write_permacache_from_dir(dirname):
    # we want the whole list so that we can display accurate progress
    # information. If we're operating on more than tens of millions of
    # files, we should either bail out or tweak this to not need the
    # whole list at once
    allfiles = []
    for root, dirs, files in os.walk(dirname):
        for f in files:
            allfiles.append(os.path.join(root, f))

    for fname in progress(allfiles, persec=True):
        try:
            write_permacache_from_file(fname)
            os.unlink(fname)
        except:
            mr_tools.status('failed on %r' % fname)
            raise

    mr_tools.status('Removing empty directories')
    for root, dirs, files in os.walk(dirname, topdown=False):
        for d in dirs:
            dname = os.path.join(root, d)
            try:
                os.rmdir(dname)
            except OSError as e:
                if e.errno == errno.ENOTEMPTY:
                    mr_tools.status('%s not empty' % (dname,))
                else:
                    raise

def write_permacache_from_file(fname):
    with open(fname) as fd:
        top1k_writepermacache(fd = fd)

########NEW FILE########
__FILENAME__ = mr_tools
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import sys
import multiprocessing

from r2.lib.mr_tools._mr_tools import mr_map, mr_reduce, format_dataspec
from r2.lib.mr_tools._mr_tools import stdin, emit

def join_things(fields, deleted=False, spam=True):
    """A reducer that joins thing table dumps and data table dumps"""
    def process(thing_id, vals):
        data = {}
        thing = None

        for val in vals:
            if val[0] == 'thing':
                thing = format_dataspec(val,
                                        ['data_type', # e.g. 'thing'
                                         'thing_type', # e.g. 'link'
                                         'ups',
                                         'downs',
                                         'deleted',
                                         'spam',
                                         'timestamp'])
            elif val[0] == 'data':
                val = format_dataspec(val,
                                      ['data_type', # e.g. 'data'
                                       'thing_type', # e.g. 'link'
                                       'key', # e.g. 'sr_id'
                                       'value'])
                if val.key in fields:
                    data[val.key] = val.value

        if (
            # silently ignore if we didn't see the 'thing' row
            thing is not None

            # remove spam and deleted as appriopriate
            and (deleted or thing.deleted == 'f')
            and (spam or thing.spam == 'f')

            # and silently ignore items that don't have all of the
            # data that we need
            and all(field in data for field in fields)):

            yield ((thing_id, thing.thing_type, thing.ups, thing.downs,
                    thing.deleted, thing.spam, thing.timestamp)
                   + tuple(data[field] for field in fields))

    mr_reduce(process)

class Mapper(object):
    def __init__(self):
        pass

    def process(self, values):
        raise NotImplemented

    def __call__(self, line):
        line = line.strip('\n')
        vals = line.split('\t')
        return list(self.process(vals)) # a list of tuples

def mr_map_parallel(processor, fd = stdin,
                    workers = multiprocessing.cpu_count(),
                    chunk_size = 1000):
    # `process` must be an instance of Mapper and promise that it is
    # safe to execute in a fork()d process.  Also note that we fuck
    # up the result ordering, but relying on result ordering breaks
    # the mapreduce contract anyway. Note also that like many of the
    # mr_tools functions, we break on newlines in the emitted output

    if workers == 1:
        return mr_map(process, fd=fd)

    pool = multiprocessing.Pool(workers)

    for res in pool.imap_unordered(processor, fd, chunk_size):
        for subres in res:
            emit(subres)

def test():
    from r2.lib.mr_tools._mr_tools import keyiter

    for key, vals in keyiter():
        print key, vals
        for val in vals:
            print '\t', val

class UpperMapper(Mapper):
    def process(self, values):
        yield map(str.upper, values)

def test_parallel():
    return mr_map_parallel(UpperMapper())

########NEW FILE########
__FILENAME__ = mr_top
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

# Known bug: if a given listing hasn't had a submission in the
# allotted time (e.g. the year listing in a subreddit that hasn't had
# a submission in the last year), we won't write out an empty
# list. I'll call it a feature.

import sys

from r2.models import Link, Comment
from r2.lib.db.sorts import epoch_seconds, score, controversy
from r2.lib.db import queries
from r2.lib import mr_tools
from r2.lib.utils import timeago, UrlParser
from r2.lib.jsontemplates import make_fullname # what a strange place
                                               # for this function

thingcls_by_name = {
    "link": Link,
    "comment": Comment,
}


def join_things():
    mr_tools.join_things(('url', 'sr_id', 'author_id'))


def time_listings(intervals):
    cutoff_by_interval = {interval: epoch_seconds(timeago("1 %s" % interval))
                          for interval in intervals}

    @mr_tools.dataspec_m_thing(
        ("url", str),
        ("sr_id", int),
        ("author_id", int),
    )
    def process(thing):
        if thing.deleted:
            return

        thing_cls = thingcls_by_name[thing.thing_type]
        fname = make_fullname(thing_cls, thing.thing_id)
        thing_score = score(thing.ups, thing.downs)
        thing_controversy = controversy(thing.ups, thing.downs)

        for interval, cutoff in cutoff_by_interval.iteritems():
            if thing.timestamp < cutoff:
                continue

            yield ("user/%s/top/%s/%d" % (thing.thing_type, interval, thing.author_id),
                   thing_score, thing.timestamp, fname)
            yield ("user/%s/controversial/%s/%d" % (thing.thing_type, interval, thing.author_id),
                   thing_controversy, thing.timestamp, fname)

            if thing.spam:
                continue

            if thing.thing_type == "link":
                yield ("sr/link/top/%s/%d" % (interval, thing.sr_id),
                       thing_score, thing.timestamp, fname)
                yield ("sr/link/controversial/%s/%d" % (interval, thing.sr_id),
                       thing_controversy, thing.timestamp, fname)

                if thing.url:
                    for domain in UrlParser(thing.url).domain_permutations():
                        yield ("domain/link/top/%s/%s" % (interval, domain),
                               thing_score, thing.timestamp, fname)
                        yield ("domain/link/controversial/%s/%s" % (interval, domain),
                               thing_controversy, thing.timestamp, fname)

    mr_tools.mr_map(process)


def store_keys(key, maxes):
    category, thing_cls, sort, time, id = key.split("/")

    query = None
    if category == "user":
        if thing_cls == "link":
            query = queries._get_submitted(int(id), sort, time)
        elif thing_cls == "comment":
            query = queries._get_comments(int(id), sort, time)
    elif category == "sr":
        if thing_cls == "link":
            query = queries._get_links(int(id), sort, time)
    elif category == "domain":
        if thing_cls == "link":
            query = queries.get_domain_links(id, sort, time)
    assert query

    item_tuples = [tuple([item[-1]] + [float(x) for x in item[:-1]])
                   for item in maxes]
    query._replace(item_tuples)


def write_permacache(fd = sys.stdin):
    mr_tools.mr_reduce_max_per_key(lambda x: map(float, x[:-1]), num=1000,
                                   post=store_keys,
                                   fd = fd)

########NEW FILE########
__FILENAME__ = normalized_hot
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import heapq
import itertools
from datetime import datetime, timedelta

from pylons import g

from r2.lib.cache import sgm
from r2.lib.db.queries import _get_links, CachedResults
from r2.lib.db.sorts import epoch_seconds


MAX_PER_SUBREDDIT = 150
MAX_LINKS = 1000


def get_hot_tuples(sr_ids):
    queries_by_sr_id = {sr_id: _get_links(sr_id, sort='hot', time='all')
                        for sr_id in sr_ids}
    CachedResults.fetch_multi(queries_by_sr_id.values())
    tuples_by_srid = {sr_id: [] for sr_id in sr_ids}

    for sr_id, q in queries_by_sr_id.iteritems():
        if not q.data:
            continue

        link_name, hot, timestamp = q.data[0]
        thot = max(hot, 1.)
        tuples_by_srid[sr_id].append((-1., -hot, link_name, timestamp))

        for link_name, hot, timestamp in q.data[1:MAX_PER_SUBREDDIT]:
            ehot = hot / thot
            # heapq.merge sorts from smallest to largest so we need to flip
            # ehot and hot to get the hottest links first
            tuples_by_srid[sr_id].append((-ehot, -hot, link_name, timestamp))

    return tuples_by_srid


def normalized_hot(sr_ids, obey_age_limit=True):
    timer = g.stats.get_timer("normalized_hot")
    timer.start()

    if not sr_ids:
        return []

    tuples_by_srid = sgm(g.cache, sr_ids, miss_fn=get_hot_tuples,
                         prefix='normalized_hot', time=g.page_cache_time)

    if obey_age_limit:
        cutoff = datetime.now(g.tz) - timedelta(days=g.HOT_PAGE_AGE)
        oldest = epoch_seconds(cutoff)
    else:
        oldest = 0.

    merged = heapq.merge(*tuples_by_srid.values())
    generator = (link_name for ehot, hot, link_name, timestamp in merged
                           if timestamp > oldest)
    ret = list(itertools.islice(generator, MAX_LINKS))
    timer.stop()
    return ret

########NEW FILE########
__FILENAME__ = nymph
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import os
import re
import hashlib
import Image
import subprocess

from r2.lib.static import generate_static_name

SPRITE_PADDING = 6
sprite_line = re.compile(r"background-image: *url\((.*)\) *.*/\* *SPRITE *(stretch-x)? *\*/")


def optimize_png(filename):
    with open(os.path.devnull, 'w') as devnull:
        subprocess.check_call(("/usr/bin/optipng", filename), stdout=devnull)


def _extract_css_info(match):
    image_filename, properties = match.groups('')
    image_filename = image_filename.strip('"\'')
    should_stretch = (properties == 'stretch-x')
    return image_filename, should_stretch


class SpritableImage(object):
    def __init__(self, image, should_stretch=False):
        self.image = image
        self.stretch = should_stretch
        self.filenames = []

    @property
    def width(self):
        return self.image.size[0]

    @property
    def height(self):
        return self.image.size[1]

    def stretch_to_width(self, width):
        self.image = self.image.resize((width, self.height))


class SpriteBin(object):
    def __init__(self, bounding_box):
        # the bounding box is a tuple of
        # top-left-x, top-left-y, bottom-right-x, bottom-right-y
        self.bounding_box = bounding_box
        self.offset = 0
        self.height = bounding_box[3] - bounding_box[1]

    def has_space_for(self, image):
        return (self.offset + image.width <= self.bounding_box[2] and
                self.height >= image.height)

    def add_image(self, image):
        image.sprite_location = (self.offset, self.bounding_box[1])
        self.offset += image.width + SPRITE_PADDING


def _load_spritable_images(css_filename):
    css_location = os.path.dirname(os.path.abspath(css_filename))

    images = {}
    with open(css_filename, 'r') as f:
        for line in f:
            m = sprite_line.search(line)
            if not m:
                continue

            image_filename, should_stretch = _extract_css_info(m)
            image = Image.open(os.path.join(css_location, image_filename))
            image_hash = hashlib.md5(image.convert("RGBA").tostring()).hexdigest()

            if image_hash not in images:
                images[image_hash] = SpritableImage(image, should_stretch)
            else:
                assert images[image_hash].stretch == should_stretch
            images[image_hash].filenames.append(image_filename)

    # Sort images by filename to group the layout by names when possible.
    return sorted(images.values(), key=lambda i: i.filenames[0])


def _generate_sprite(images, sprite_path):
    sprite_width = max(i.width for i in images)
    sprite_height = 0

    # put all the max-width and stretch-x images together at the top
    small_images = []
    for image in images:
        if image.width == sprite_width or image.stretch:
            if image.stretch:
                image.stretch_to_width(sprite_width)
            image.sprite_location = (0, sprite_height)
            sprite_height += image.height + SPRITE_PADDING
        else:
            small_images.append(image)

    # lay out the remaining images -- done with a greedy algorithm
    small_images.sort(key=lambda i: i.height, reverse=True)
    bins = []

    for image in small_images:
        # find a bin to fit in
        for bin in bins:
            if bin.has_space_for(image):
                break
        else:
            # or give up and create a new bin
            bin = SpriteBin((0, sprite_height, sprite_width, sprite_height + image.height))
            sprite_height += image.height + SPRITE_PADDING
            bins.append(bin)

        bin.add_image(image)

    # generate the image
    sprite_dimensions = (sprite_width, sprite_height)
    background_color = (255, 69, 0, 0)  # transparent orangered
    sprite = Image.new('RGBA', sprite_dimensions, background_color)

    for image in images:
        sprite.paste(image.image, image.sprite_location)

    sprite.save(sprite_path, optimize=True)
    optimize_png(sprite_path)

    # give back the mangled name
    sprite_base, sprite_name = os.path.split(sprite_path)
    return generate_static_name(sprite_name, base=sprite_base)


def _rewrite_css(css_filename, sprite_path, images):
    # map filenames to coordinates
    locations = {}
    for image in images:
        for filename in image.filenames:
            locations[filename] = image.sprite_location

    def rewrite_sprite_reference(match):
        image_filename, should_stretch = _extract_css_info(match)
        position = locations[image_filename]

        return ''.join((
            'background-image: url(%s);' % sprite_path,
            'background-position: -%dpx -%dpx;' % position,
            'background-repeat: %s;' % ('repeat' if should_stretch else 'no-repeat'),
        ))

    # read in the css and replace sprite references
    with open(css_filename, 'r') as f:
        css = f.read()
    return sprite_line.sub(rewrite_sprite_reference, css)


def spritify(css_filename, sprite_path):
    images = _load_spritable_images(css_filename)
    sprite_path = _generate_sprite(images, sprite_path)
    return _rewrite_css(css_filename, sprite_path, images)


if __name__ == '__main__':
    import sys
    print spritify(sys.argv[1], sys.argv[2])

########NEW FILE########
__FILENAME__ = organic
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.models import *
from r2.lib.normalized_hot import normalized_hot
from r2.lib import count
from r2.lib.utils import UniqueIterator, timeago

from pylons import c

import random
from time import time

organic_max_length= 50

def keep_fresh_links(item):
    if c.user_is_loggedin and c.user._id == item.author_id:
        return True

    if item._spam or item._deleted:
        return False

    from r2.lib.promote import is_promo
    if is_promo(item):
        return False

    return item.fresh

def cached_organic_links(*sr_ids):
    sr_count = count.get_link_counts()
    #only use links from reddits that you're subscribed to
    link_names = filter(lambda n: sr_count[n][1] in sr_ids, sr_count.keys())
    link_names.sort(key = lambda n: sr_count[n][0])

    if not link_names and g.debug:
        q = All.get_links('new', 'all')
        q._limit = 100 # this decomposes to a _query
        link_names = [x._fullname for x in q if x.promoted is None]
        g.log.debug('Used inorganic links')

    #potentially add an up and coming link
    if random.choice((True, False)) and sr_ids:
        sr_id = random.choice(sr_ids)
        fnames = normalized_hot([sr_id])
        if fnames:
            if len(fnames) == 1:
                new_item = fnames[0]
            else:
                new_item = random.choice(fnames[1:4])
            link_names.insert(0, new_item)

    return link_names

def organic_links(user):
    sr_ids = Subreddit.user_subreddits(user)
    # make sure that these are sorted so the cache keys are constant
    sr_ids.sort()

    # get the default subreddits if the user is not logged in
    user_id = None if isinstance(user, FakeAccount) else user
    sr_ids = Subreddit.user_subreddits(user, True)

    # pass the cached function a sorted list so that we can guarantee
    # cachability
    sr_ids.sort()
    return cached_organic_links(*sr_ids)[:organic_max_length]


########NEW FILE########
__FILENAME__ = admin_pages
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import c, g, config
from pylons.i18n import N_
from r2.lib.wrapped import Templated
from pages   import Reddit
from r2.lib.menus import (
    NamedButton,
    NavButton,
    menu,
    NavMenu,
    OffsiteButton,
)
from r2.lib.utils import timesince

def admin_menu(**kwargs):
    buttons = [
        OffsiteButton("traffic", "/traffic"),
        NavButton(menu.awards, "awards"),
        NavButton(menu.errors, "error log"),
    ]

    admin_menu = NavMenu(buttons, title='admin tools', base_path='/admin',
                         type="lightdrop", **kwargs)
    return admin_menu

class AdminSidebar(Templated):
    def __init__(self, user):
        Templated.__init__(self)
        self.user = user


class SponsorSidebar(Templated):
    def __init__(self, user):
        Templated.__init__(self)
        self.user = user


class Details(Templated):
    def __init__(self, link, *a, **kw):
        Templated.__init__(self, *a, **kw)
        self.link = link


class AdminPage(Reddit):
    create_reddit_box  = False
    submit_box         = False
    extension_handling = False
    show_sidebar = False

    def __init__(self, nav_menus = None, *a, **kw):
        Reddit.__init__(self, nav_menus = nav_menus, *a, **kw)

class AdminProfileMenu(NavMenu):
    def __init__(self, path):
        NavMenu.__init__(self, [], base_path = path,
                         title = 'admin', type="tabdrop")


class AdminLinkMenu(NavMenu):
    def __init__(self, link):
        NavMenu.__init__(self, [], title='admin', type="tabdrop")


class AdminNotesSidebar(Templated):
    EMPTY_MESSAGE = {
        "domain": N_("No notes for this domain"),
        "ip": N_("No notes for this IP address"),
        "subreddit": N_("No notes for this subreddit"),
        "user": N_("No notes for this user"),
    }

    SYSTEMS = {
        "domain": N_("domain"),
        "ip": N_("IP address"),
        "subreddit": N_("subreddit"),
        "user": N_("user"),
    }

    def __init__(self, system, subject):
        from r2.models.admin_notes import AdminNotesBySystem

        self.system = system
        self.subject = subject
        self.author = c.user.name
        self.notes = AdminNotesBySystem.in_display_order(system, subject)
        # Convert timestamps for easier reading/translation
        for note in self.notes:
            note["timesince"] = timesince(note["when"])
        Templated.__init__(self)


if config['r2.import_private']:
    from r2admin.lib.pages import *

########NEW FILE########
__FILENAME__ = pages
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from collections import Counter, OrderedDict

from r2.lib.wrapped import Wrapped, Templated, CachedTemplate
from r2.models import Account, DefaultSR, make_feedurl
from r2.models import FakeSubreddit, Subreddit, SubSR, AllMinus, AllSR
from r2.models import Friends, All, Sub, NotFound, DomainSR, Random, Mod, RandomNSFW, RandomSubscription, MultiReddit, ModSR, Frontpage, LabeledMulti
from r2.models import Link, Printable, Trophy, PromoCampaign, Comment
from r2.models import Flair, FlairTemplate, FlairTemplateBySubredditIndex
from r2.models import USER_FLAIR, LINK_FLAIR
from r2.models.bidding import Bid
from r2.models.gold import (
    gold_payments_by_user,
    gold_received_by_user,
    days_to_pennies,
    gold_goal_on,
    gold_revenue_steady,
    gold_revenue_volatile,
    get_subscription_details,
    TIMEZONE as GOLD_TIMEZONE,
)
from r2.models.promo import (
    NO_TRANSACTION,
    PROMOTE_PRIORITIES,
    PromotedLinkRoadblock,
    PromotionLog,
)
from r2.models.token import OAuth2Client, OAuth2AccessToken
from r2.models import traffic
from r2.models import ModAction
from r2.models import Thing
from r2.models.wiki import WikiPage, ImagesByWikiPage
from r2.lib.db import tdb_cassandra, queries
from r2.config.extensions import is_api
from r2.lib.menus import CommentSortMenu
from pylons.i18n import _, ungettext
from pylons import c, request, g, config
from pylons.controllers.util import abort

from r2.lib import media, inventory
from r2.lib import promote, tracking
from r2.lib.captcha import get_iden
from r2.lib.filters import (
    spaceCompress,
    _force_unicode,
    _force_utf8,
    unsafe,
    websafe,
    SC_ON,
    SC_OFF,
    websafe_json,
    wikimarkdown,
)
from r2.lib.menus import NavButton, NamedButton, NavMenu, PageNameNav, JsButton
from r2.lib.menus import SubredditButton, SubredditMenu, ModeratorMailButton
from r2.lib.menus import OffsiteButton, menu, JsNavMenu
from r2.lib.normalized_hot import normalized_hot
from r2.lib.strings import plurals, rand_strings, strings, Score
from r2.lib.utils import title_to_url, query_string, UrlParser
from r2.lib.utils import url_links_builder, make_offset_date, median, to36
from r2.lib.utils import trunc_time, timesince, timeuntil, weighted_lottery
from r2.lib.template_helpers import (
    add_sr,
    get_domain,
    format_number,
    media_https_if_secure,
    comment_label,
)
from r2.lib.subreddit_search import popular_searches
from r2.lib.log import log_text
from r2.lib.memoize import memoize
from r2.lib.utils import trunc_string as _truncate, to_date
from r2.lib.filters import safemarkdown
from r2.lib.utils import Storage, tup
from r2.lib.utils import precise_format_timedelta

from babel.numbers import format_currency
from babel.dates import format_date
from collections import defaultdict
import csv
import hmac
import hashlib
import cStringIO
import pytz
import sys, random, datetime, calendar, simplejson, re, time
import time
from itertools import chain, product
from urllib import quote, urlencode

# the ip tracking code is currently deeply tied with spam prevention stuff
# this will be open sourced as soon as it can be decoupled
if config['r2.import_private']:
    from r2admin.lib.ip_events import ips_by_account_id
else:
    def ips_by_account_id(account_id):
        return []

from things import wrap_links, wrap_things, default_thing_wrapper

datefmt = _force_utf8(_('%d %b %Y'))

MAX_DESCRIPTION_LENGTH = 150

def get_captcha():
    if not c.user_is_loggedin or c.user.needs_captcha():
        return get_iden()

def responsive(res, space_compress=None):
    """
    Use in places where the template is returned as the result of the
    controller so that it becomes compatible with the page cache.
    """
    if space_compress is None:
        space_compress = not g.template_debug

    if is_api():
        res = websafe_json(simplejson.dumps(res or ''))
        if c.allowed_callback:
            res = "%s(%s)" % (websafe_json(c.allowed_callback), res)
    elif space_compress:
        res = spaceCompress(res)
    return res


class Robots(Templated):
    pass


class Reddit(Templated):
    '''Base class for rendering a page on reddit.  Handles toolbar creation,
    content of the footers, and content of the corner buttons.

    Constructor arguments:

        space_compress -- run r2.lib.filters.spaceCompress on render
        loginbox -- enable/disable rendering of the small login box in the right margin
          (only if no user is logged in; login box will be disabled for a logged in user)
        show_sidebar -- enable/disable content in the right margin

        infotext -- text to display in a <p class="infotext"> above the content
        nav_menus -- list of Menu objects to be shown in the area below the header
        content -- renderable object to fill the main content well in the page.

    settings determined at class-declaration time

      create_reddit_box -- enable/disable display of the "Create a reddit" box
      submit_box        -- enable/disable display of the "Submit" box
      searchbox         -- enable/disable the "search" box in the header
      extension_handling -- enable/disable rendering using non-html templates
                            (e.g. js, xml for rss, etc.)
    '''

    create_reddit_box  = True
    submit_box         = True
    footer             = True
    searchbox          = True
    extension_handling = True
    enable_login_cover = True
    site_tracking      = True
    show_infobar       = True
    content_id         = None
    css_class          = None
    extra_page_classes = None
    extra_stylesheets  = []

    def __init__(self, space_compress=None, nav_menus=None, loginbox=True,
                 infotext='', content=None, short_description='', title='',
                 robots=None, show_sidebar=True, show_chooser=False,
                 footer=True, srbar=True, page_classes=None, short_title=None,
                 show_wiki_actions=False, extra_js_config=None,
                 meta_thumbnail=None,
                 **context):
        Templated.__init__(self, **context)
        self.title = title
        self.short_title = short_title
        self.short_description = short_description
        self.robots = robots
        self.infotext = infotext
        self.extra_js_config = extra_js_config
        self.show_wiki_actions = show_wiki_actions
        self.loginbox = True
        self.show_sidebar = show_sidebar
        self.space_compress = space_compress
        self.meta_thumbnail = meta_thumbnail
        # instantiate a footer
        self.footer = RedditFooter() if footer else None
        self.debug_footer = DebugFooter()
        self.supplied_page_classes = page_classes or []

        #put the sort menus at the top
        self.nav_menu = MenuArea(menus = nav_menus) if nav_menus else None

        #add the infobar
        self.welcomebar = None
        self.infobar = None
        # generate a canonical link for google
        self.canonical_link = request.fullpath
        if c.render_style != "html":
            u = UrlParser(request.fullpath)
            u.set_extension("")
            u.hostname = g.domain
            if g.domain_prefix:
                u.hostname = "%s.%s" % (g.domain_prefix, u.hostname)
            self.canonical_link = u.unparse()

        if self.show_infobar:
            if not infotext:
                if g.heavy_load_mode:
                    # heavy load mode message overrides read only
                    infotext = strings.heavy_load_msg
                elif g.read_only_mode:
                    infotext = strings.read_only_msg
                elif g.live_config.get("announcement_message"):
                    infotext = g.live_config["announcement_message"]

            if infotext:
                self.infobar = InfoBar(message=infotext)
            elif (isinstance(c.site, DomainSR) and
                    c.site.domain.endswith("imgur.com")):
                self.infobar = InfoBar(message=
                    _("imgur.com domain listings (including this one) are "
                      "currently disabled to speed up vote processing.")
                )
            elif isinstance(c.site, AllMinus) and not c.user.gold:
                self.infobar = InfoBar(message=strings.all_minus_gold_only,
                                       extra_class="gold")

            if not c.user_is_loggedin:
                self.welcomebar = WelcomeBar()

        self.srtopbar = None
        if srbar and not c.cname and not is_api():
            self.srtopbar = SubredditTopBar()

        if (c.user_is_loggedin and self.show_sidebar
            and not is_api() and not self.show_wiki_actions):
            # insert some form templates for js to use
            # TODO: move these to client side templates
            gold_link = GoldPayment("gift",
                                    "monthly",
                                    months=1,
                                    signed=False,
                                    recipient="",
                                    giftmessage=None,
                                    passthrough=None,
                                    thing=None,
                                    clone_template=True,
                                    thing_type="link",
                                   )
            gold_comment = GoldPayment("gift",
                                       "monthly",
                                       months=1,
                                       signed=False,
                                       recipient="",
                                       giftmessage=None,
                                       passthrough=None,
                                       thing=None,
                                       clone_template=True,
                                       thing_type="comment",
                                      )
            self._content = PaneStack([ShareLink(), content,
                                       gold_comment, gold_link])
        else:
            self._content = content

        self.show_chooser = (
            show_chooser and
            c.render_style == "html" and
            c.user_is_loggedin and
            (
                isinstance(c.site, (DefaultSR, AllSR, ModSR, LabeledMulti)) or
                c.site.name == g.live_config["listing_chooser_explore_sr"]
            )
        )

        self.toolbars = self.build_toolbars()
        self.subreddit_stylesheet_url = self.get_subreddit_stylesheet_url()

    @staticmethod
    def get_subreddit_stylesheet_url():
        if c.can_apply_styles and c.allow_styles:
            if c.secure:
                if c.site.stylesheet_url_https:
                    return c.site.stylesheet_url_https
                elif c.site.stylesheet_contents_secure:
                    return c.site.stylesheet_url
            else:
                if c.site.stylesheet_url_http:
                    return c.site.stylesheet_url_http
                elif c.site.stylesheet_contents:
                    return c.site.stylesheet_url

    def wiki_actions_menu(self, moderator=False):
        buttons = []

        buttons.append(NamedButton("wikirecentrevisions",
                                   css_class="wikiaction-revisions",
                                   dest="/wiki/revisions"))

        buttons.append(NamedButton("wikipageslist",
                           css_class="wikiaction-pages",
                           dest="/wiki/pages"))
        if moderator:
            buttons += [NamedButton('wikibanned', css_class='reddit-ban',
                                    dest='/about/wikibanned'),
                        NamedButton('wikicontributors',
                                    css_class='reddit-contributors',
                                    dest='/about/wikicontributors')
                        ]

        return SideContentBox(_('wiki tools'),
                      [NavMenu(buttons,
                               type="flat_vert",
                               css_class="icon-menu",
                               separator="")],
                      _id="wikiactions",
                      collapsible=True)

    def sr_admin_menu(self):
        buttons = []
        is_single_subreddit = not isinstance(c.site, (ModSR, MultiReddit))
        is_admin = c.user_is_loggedin and c.user_is_admin
        is_moderator_with_perms = lambda *perms: (
            is_admin or c.site.is_moderator_with_perms(c.user, *perms))

        if is_single_subreddit and is_moderator_with_perms('config'):
            buttons.append(NavButton(menu.community_settings,
                                     css_class="reddit-edit",
                                     dest="edit"))

        if is_moderator_with_perms('mail'):
            buttons.append(NamedButton("modmail",
                                    dest="message/inbox",
                                    css_class="moderator-mail"))

        if is_single_subreddit:
            if is_moderator_with_perms('access'):
                buttons.append(NamedButton("moderators",
                                           css_class="reddit-moderators"))

                if c.site.type != "public":
                    buttons.append(NamedButton("contributors",
                                               css_class="reddit-contributors"))
                else:
                    buttons.append(NavButton(menu.contributors,
                                             "contributors",
                                             css_class="reddit-contributors"))

            buttons.append(NamedButton("traffic", css_class="reddit-traffic"))

        if is_moderator_with_perms('posts'):
            buttons += [NamedButton("modqueue", css_class="reddit-modqueue"),
                        NamedButton("reports", css_class="reddit-reported"),
                        NamedButton("spam", css_class="reddit-spam")]

        if is_single_subreddit:
            if is_moderator_with_perms('access'):
                buttons.append(NamedButton("banned", css_class="reddit-ban"))
            if is_moderator_with_perms('flair'):
                buttons.append(NamedButton("flair", css_class="reddit-flair"))

        buttons.append(NamedButton("log", css_class="reddit-moderationlog"))
        if is_moderator_with_perms('posts'):
            buttons.append(
                NamedButton("unmoderated", css_class="reddit-unmoderated"))

        return SideContentBox(_('moderation tools'),
                              [NavMenu(buttons,
                                       type="flat_vert",
                                       base_path="/about/",
                                       css_class="icon-menu",
                                       separator="")],
                              _id="moderation_tools",
                              collapsible=True)

    def sr_moderators(self):
        accounts = Account._byID(c.site.moderators,
                                 data=True, return_dict=False)
        return [WrappedUser(a) for a in accounts if not a._deleted]

    def rightbox(self):
        """generates content in <div class="rightbox">"""

        ps = PaneStack(css_class='spacer')

        if self.searchbox:
            ps.append(SearchForm())

        sidebar_message = g.live_config.get("sidebar_message")
        if sidebar_message and isinstance(c.site, DefaultSR):
            ps.append(SidebarMessage(sidebar_message[0]))

        gold_sidebar_message = g.live_config.get("gold_sidebar_message")
        if (c.user_is_loggedin and c.user.gold and
                gold_sidebar_message and isinstance(c.site, DefaultSR)):
            ps.append(SidebarMessage(gold_sidebar_message[0],
                                     extra_class="gold"))

        if not c.user_is_loggedin and self.loginbox and not g.read_only_mode:
            ps.append(LoginFormWide())

        if isinstance(c.site, DomainSR) and c.user_is_admin:
            from r2.lib.pages.admin_pages import AdminNotesSidebar
            notebar = AdminNotesSidebar('domain', c.site.domain)
            ps.append(notebar)

        if isinstance(c.site, Subreddit) and c.user_is_admin:
            from r2.lib.pages.admin_pages import AdminNotesSidebar
            notebar = AdminNotesSidebar('subreddit', c.site.name)
            ps.append(notebar)

        if c.user.pref_show_sponsorships or not c.user.gold:
            ps.append(SponsorshipBox())

        if isinstance(c.site, (MultiReddit, ModSR)):
            srs = Subreddit._byID(c.site.sr_ids, data=True,
                                  return_dict=False)

            if (srs and c.user_is_loggedin and
                    (c.user_is_admin or c.site.is_moderator(c.user))):
                ps.append(self.sr_admin_menu())

            if isinstance(c.site, LabeledMulti):
                ps.append(MultiInfoBar(c.site, srs, c.user))
                c.js_preload.set_wrapped(
                    '/api/multi/%s' % c.site.path.lstrip('/'), c.site)
            elif srs:
                if isinstance(c.site, ModSR):
                    box = SubscriptionBox(srs, multi_text=strings.mod_multi)
                else:
                    box = SubscriptionBox(srs)
                ps.append(SideContentBox(_('these subreddits'), [box]))


        if isinstance(c.site, AllSR):
            ps.append(AllInfoBar(c.site, c.user))

        user_banned = c.user_is_loggedin and c.site.is_banned(c.user)

        if (self.submit_box
                and (c.user_is_loggedin or not g.read_only_mode)
                and not user_banned):
            if (not isinstance(c.site, FakeSubreddit)
                    and c.site.type in ("archived",
                                        "restricted",
                                        "gold_restricted")
                    and not (c.user_is_loggedin
                             and c.site.can_submit(c.user))):
                if c.site.type == "archived":
                    subtitle = _('this subreddit is archived '
                                 'and no longer accepting submissions.')
                    ps.append(SideBox(title=_('Submissions disabled'),
                                      css_class="submit",
                                      disabled=True,
                                      subtitles=[subtitle],
                                      show_icon=False))
                else:
                    if c.site.type == 'restricted':
                        subtitle = _('submission in this subreddit '
                                     'is restricted to approved submitters.')
                    elif c.site.type == 'gold_restricted':
                        subtitle = _('submission in this subreddit '
                                     'is restricted to reddit gold members.')
                    ps.append(SideBox(title=_('Submissions restricted'),
                                      css_class="submit",
                                      disabled=True,
                                      subtitles=[subtitle],
                                      show_icon=False))
            else:
                fake_sub = isinstance(c.site, FakeSubreddit)
                is_multi = isinstance(c.site, MultiReddit)

                if isinstance(c.site, FakeSubreddit):
                    submit_buttons = set(("link", "self"))
                else:
                    # we want to show submit buttons for logged-out users too
                    # so we can't just use can_submit_link/text
                    submit_buttons = c.site.allowed_types
                    if c.user_is_loggedin:
                        if c.site.can_submit_link(c.user):
                            submit_buttons.add("link")
                        if c.site.can_submit_text(c.user):
                            submit_buttons.add("self")

                if "link" in submit_buttons:
                    ps.append(SideBox(title=c.site.submit_link_label or
                                            strings.submit_link_label,
                                      css_class="submit submit-link",
                                      link="/submit",
                                      sr_path=not fake_sub or is_multi,
                                      show_cover=True))
                if "self" in submit_buttons:
                    ps.append(SideBox(title=c.site.submit_text_label or
                                            strings.submit_text_label,
                                      css_class="submit submit-text",
                                      link="/submit?selftext=true",
                                      sr_path=not fake_sub or is_multi,
                                      show_cover=True))

        no_ads_yet = True
        show_adbox = (c.user.pref_show_adbox or not c.user.gold) and not g.disable_ads

        # don't show the subreddit info bar on cnames unless the option is set
        if not isinstance(c.site, FakeSubreddit) and (not c.cname or c.site.show_cname_sidebar):
            ps.append(SubredditInfoBar())
            moderator = c.user_is_loggedin and (c.user_is_admin or
                                          c.site.is_moderator(c.user))
            wiki_moderator = c.user_is_loggedin and (
                c.user_is_admin
                or c.site.is_moderator_with_perms(c.user, 'wiki'))
            if self.show_wiki_actions:
                menu = self.wiki_actions_menu(moderator=wiki_moderator)
                ps.append(menu)
            if moderator:
                ps.append(self.sr_admin_menu())
            if show_adbox:
                ps.append(Ads())
            no_ads_yet = False
        elif self.show_wiki_actions:
            ps.append(self.wiki_actions_menu())

        if self.create_reddit_box and c.user_is_loggedin:
            delta = datetime.datetime.now(g.tz) - c.user._date
            if delta.days >= g.min_membership_create_community:
                ps.append(SideBox(_('Create your own subreddit'),
                           '/subreddits/create', 'create',
                           subtitles = rand_strings.get("create_reddit", 2),
                           show_cover = True, nocname=True))

        if not isinstance(c.site, FakeSubreddit) and not c.cname:
            moderators = self.sr_moderators()
            if moderators:
                more_text = mod_href = ""
                sidebar_list_length = 10
                num_not_shown = len(moderators) - sidebar_list_length

                if num_not_shown > 0:
                    more_text = _("...and %d more") % (num_not_shown)
                    mod_href = "http://%s/about/moderators" % get_domain()

                if '/r/%s' % c.site.name == g.admin_message_acct:
                    label = _('message the admins')
                else:
                    label = _('message the moderators')
                helplink = ("/message/compose?to=%%2Fr%%2F%s" % c.site.name,
                            label)
                ps.append(SideContentBox(_('moderators'),
                                         moderators[:sidebar_list_length],
                                         helplink = helplink,
                                         more_href = mod_href,
                                         more_text = more_text))

        if no_ads_yet and show_adbox:
            ps.append(Ads())
            if g.live_config["gold_revenue_goal"]:
                ps.append(Goldvertisement())

        if c.user.pref_clickgadget and c.recent_clicks:
            ps.append(SideContentBox(_("Recently viewed links"),
                                     [ClickGadget(c.recent_clicks)]))

        if c.user_is_loggedin:
            activity_link = AccountActivityBox()
            ps.append(activity_link)

        return ps

    def render(self, *a, **kw):
        """Overrides default Templated.render with two additions
           * support for rendering API requests with proper wrapping
           * support for space compression of the result
        In adition, unlike Templated.render, the result is in the form of a pylons
        Response object with it's content set.
        """
        if c.bare_content:
            res = self.content().render()
        else:
            res = Templated.render(self, *a, **kw)

        return responsive(res, self.space_compress)

    def corner_buttons(self):
        """set up for buttons in upper right corner of main page."""
        buttons = []
        if c.user_is_loggedin:
            if c.user.name in g.admins:
                if c.user_is_admin:
                    buttons += [OffsiteButton(
                        _("turn admin off"),
                        dest="%s/adminoff?dest=%s" %
                            (g.https_endpoint, quote(request.fullpath)),
                        target = "_self",
                    )]
                else:
                    buttons += [OffsiteButton(
                        _("turn admin on"),
                        dest="%s/adminon?dest=%s" %
                            (g.https_endpoint, quote(request.fullpath)),
                        target = "_self",
                    )]
            buttons += [NamedButton("prefs", False,
                                  css_class = "pref-lang")]
        else:
            lang = c.lang.split('-')[0] if c.lang else ''
            lang_name = g.lang_name.get(lang) or [lang, '']
            lang_name = "".join(lang_name)
            buttons += [JsButton(lang_name,
                                 onclick = "return showlang();",
                                 css_class = "pref-lang")]
        return NavMenu(buttons, base_path = "/", type = "flatlist")

    def build_toolbars(self):
        """Sets the layout of the navigation topbar on a Reddit.  The result
        is a list of menus which will be rendered in order and
        displayed at the top of the Reddit."""
        if c.site == Friends:
            main_buttons = [NamedButton('new', dest='', aliases=['/hot']),
                            NamedButton('comments'),
                            NamedButton('gilded'),
                            ]
        else:
            main_buttons = [NamedButton('hot', dest='', aliases=['/hot']),
                            NamedButton('new'),
                            NamedButton('rising'),
                            NamedButton('controversial'),
                            NamedButton('top'),
                            ]

            if not isinstance(c.site, DomainSR):
                main_buttons.append(NamedButton('gilded',
                                                aliases=['/comments/gilded']))

            mod = False
            if c.user_is_loggedin:
                mod = bool(c.user_is_admin
                           or c.site.is_moderator_with_perms(c.user, 'wiki'))
            if c.site._should_wiki and (c.site.wikimode != 'disabled' or mod):
                if not g.disable_wiki:
                    main_buttons.append(NavButton('wiki', 'wiki'))

            if isinstance(c.site, (Subreddit, DefaultSR, MultiReddit)):
                main_buttons.append(NavButton(menu.promoted, 'ads'))

        more_buttons = []

        if c.user_is_loggedin:
            if c.user.pref_show_promote or c.user_is_sponsor:
                more_buttons.append(NavButton(menu.promote, 'promoted', False))

        #if there's only one button in the dropdown, get rid of the dropdown
        if len(more_buttons) == 1:
            main_buttons.append(more_buttons[0])
            more_buttons = []

        toolbar = [NavMenu(main_buttons, type='tabmenu')]
        if more_buttons:
            toolbar.append(NavMenu(more_buttons, title=menu.more, type='tabdrop'))

        if not isinstance(c.site, DefaultSR) and not c.cname:
            func = 'subreddit'
            if isinstance(c.site, DomainSR):
                func = 'domain'
            toolbar.insert(0, PageNameNav(func))

        return toolbar

    def __repr__(self):
        return "<Reddit>"

    @staticmethod
    def content_stack(panes, css_class = None):
        """Helper method for reordering the content stack."""
        return PaneStack(filter(None, panes), css_class = css_class)

    def content(self):
        """returns a Wrapped (or renderable) item for the main content div."""
        return self.content_stack((
            self.welcomebar, self.infobar, self.nav_menu, self._content))

    def page_classes(self):
        classes = set()

        if c.user_is_loggedin:
            classes.add('loggedin')
            if not isinstance(c.site, FakeSubreddit):
                if c.site.is_subscriber(c.user):
                    classes.add('subscriber')
                if c.site.is_contributor(c.user):
                    classes.add('contributor')
            if c.site.is_moderator(c.user):
                classes.add('moderator')
            if c.user.gold:
                classes.add('gold')

        if isinstance(c.site, MultiReddit):
            classes.add('multi-page')

        if self.show_chooser:
            classes.add('with-listing-chooser')
            if c.user.pref_collapse_left_bar:
                classes.add('listing-chooser-collapsed')

        if c.user_is_loggedin and c.user.pref_compress:
            classes.add('compressed-display')

        if self.extra_page_classes:
            classes.update(self.extra_page_classes)
        if self.supplied_page_classes:
            classes.update(self.supplied_page_classes)

        return classes


class DebugFooter(Templated):
    pass


class AccountActivityBox(Templated):
    def __init__(self):
        super(AccountActivityBox, self).__init__()

class RedditHeader(Templated):
    def __init__(self):
        pass

class RedditFooter(CachedTemplate):
    def cachable_attrs(self):
        return [('path', request.path),
                ('buttons', [[(x.title, x.path) for x in y] for y in self.nav])]

    def __init__(self):
        self.nav = [
            NavMenu([
                    NamedButton("blog", False, nocname=True),
                    NamedButton("about", False, nocname=True),
                    NamedButton("team", False, nocname=True, dest="/about/team"),
                    NamedButton("code", False, nocname=True),
                    NamedButton("advertising", False, nocname=True),
                    NamedButton("jobs", False, nocname=True, dest="/r/redditjobs"),
                ],
                title = _("about"),
                type = "flat_vert",
                separator = ""),

            NavMenu([
                    NamedButton("wiki", False, nocname=True),
                    OffsiteButton(_("FAQ"), dest = "/wiki/faq", nocname=True),
                    OffsiteButton(_("reddiquette"), nocname=True, dest = "/wiki/reddiquette"),
                    NamedButton("rules", False, nocname=True),
                    NamedButton("contact", False),
                ],
                title = _("help"),
                type = "flat_vert",
                separator = ""),

            NavMenu([
                    OffsiteButton("mobile", "http://i.reddit.com"),
                    OffsiteButton(_("firefox extension"), "https://addons.mozilla.org/firefox/addon/socialite/"),
                    OffsiteButton(_("chrome extension"), "https://chrome.google.com/webstore/detail/algjnflpgoopkdijmkalfcifomdhmcbe"),
                    NamedButton("buttons", True),
                    NamedButton("widget", True),
                ],
                title = _("tools"),
                type = "flat_vert",
                separator = ""),

            NavMenu([
                    NamedButton("gold", False, nocname=True, dest = "/gold/about", css_class = "buygold"),
                    NamedButton("store", False, nocname=True),
                    OffsiteButton(_("redditgifts"), "http://redditgifts.com"),
                    OffsiteButton(_("reddit.tv"), "http://reddit.tv"),
                    OffsiteButton(_("radio reddit"), "http://radioreddit.com"),
                ],
                title = _("<3"),
                type = "flat_vert",
                separator = "")
        ]
        CachedTemplate.__init__(self)

class ClickGadget(Templated):
    def __init__(self, links, *a, **kw):
        self.links = links
        self.content = ''
        if c.user_is_loggedin and self.links:
            self.content = self.make_content()
        Templated.__init__(self, *a, **kw)

    def make_content(self):
        #this will disable the hardcoded widget styles
        request.GET["style"] = "off"
        wrapper = default_thing_wrapper(embed_voting_style = 'votable',
                                        style = "htmllite")
        content = wrap_links(self.links, wrapper = wrapper)

        return content.render(style = "htmllite")


class RedditMin(Reddit):
    """a version of Reddit that has no sidebar, toolbar, footer,
       etc"""
    footer       = False
    show_sidebar = False
    show_infobar = False

    def page_classes(self):
        return ('min-body',)


class LoginFormWide(CachedTemplate):
    """generates a login form suitable for the 300px rightbox."""
    def __init__(self):
        self.cname = c.cname
        self.auth_cname = c.authorized_cname
        CachedTemplate.__init__(self)

class SubredditInfoBar(CachedTemplate):
    """When not on Default, renders a sidebox which gives info about
    the current reddit, including links to the moderator and
    contributor pages, as well as links to the banning page if the
    current user is a moderator."""

    def __init__(self, site = None):
        site = site or c.site

        # hackity hack. do i need to add all the others props?
        self.sr = list(wrap_links(site))[0]

        # we want to cache on the number of subscribers
        self.subscribers = self.sr._ups

        # so the menus cache properly
        self.path = request.path

        self.accounts_active, self.accounts_active_fuzzed = self.sr.get_accounts_active()

        if c.user_is_loggedin and c.user.pref_show_flair:
            self.flair_prefs = FlairPrefs()
        else:
            self.flair_prefs = None

        CachedTemplate.__init__(self)

    @property
    def creator_text(self):
        if self.sr.author:
            if self.sr.is_moderator(self.sr.author) or self.sr.author._deleted:
                return WrappedUser(self.sr.author).render()
            else:
                return self.sr.author.name
        return None

    def nav(self):
        buttons = [NavButton(plurals.moderators, 'moderators')]
        if self.type != 'public':
            buttons.append(NavButton(getattr(plurals, "approved submitters"), 'contributors'))

        if self.is_moderator or self.is_admin:
            buttons.extend([
                    NamedButton('spam'),
                    NamedButton('reports'),
                    NavButton(menu.banusers, 'banned'),
                    NamedButton('traffic'),
                    NavButton(menu.community_settings, 'edit'),
                    NavButton(menu.flair, 'flair'),
                    NavButton(menu.modactions, 'modactions'),
                    ])
        return [NavMenu(buttons, type = "flat_vert", base_path = "/about/",
                        separator = '')]

class SponsorshipBox(Templated):
    pass

class SideContentBox(Templated):
    def __init__(self, title, content, helplink=None, _id=None, extra_class=None,
                 more_href = None, more_text = "more", collapsible=False):
        Templated.__init__(self, title=title, helplink = helplink,
                           content=content, _id=_id, extra_class=extra_class,
                           more_href = more_href, more_text = more_text,
                           collapsible=collapsible)

class SideBox(CachedTemplate):
    """
    Generic sidebox used to generate the 'submit' and 'create a reddit' boxes.
    """
    def __init__(self, title, link=None, css_class='', subtitles = [],
                 show_cover = False, nocname=False, sr_path = False,
                 disabled=False, show_icon=True, target='_top'):
        CachedTemplate.__init__(self, link = link, target = target,
                           title = title, css_class = css_class,
                           sr_path = sr_path, subtitles = subtitles,
                           show_cover = show_cover, nocname=nocname,
                           disabled=disabled, show_icon=show_icon)


class PrefsPage(Reddit):
    """container for pages accessible via /prefs.  No extension handling."""

    extension_handling = False

    def __init__(self, show_sidebar = False, title=None, *a, **kw):
        title = title or "%s (%s)" % (_("preferences"), c.site.name.strip(' '))
        Reddit.__init__(self, show_sidebar = show_sidebar,
                        title=title,
                        *a, **kw)

    def build_toolbars(self):
        buttons = [NavButton(menu.options, ''),
                   NamedButton('apps')]

        if c.user.pref_private_feeds:
            buttons.append(NamedButton('feeds'))

        buttons.extend([NamedButton('friends'),
                        NamedButton('blocked'),
                        NamedButton('update')])

        if c.user_is_loggedin and c.user.name in g.admins:
            buttons += [NamedButton('otp')]

        #if CustomerID.get_id(user):
        #    buttons += [NamedButton('payment')]
        buttons += [NamedButton('delete')]
        return [PageNameNav('nomenu', title = _("preferences")),
                NavMenu(buttons, base_path = "/prefs", type="tabmenu")]

class PrefOptions(Templated):
    """Preference form for updating language and display options"""
    def __init__(self, done = False):
        Templated.__init__(self, done = done)

class PrefFeeds(Templated):
    pass

class PrefOTP(Templated):
    pass

class PrefUpdate(Templated):
    """Preference form for updating email address and passwords"""
    def __init__(self, email=True, password=True, verify=False, dest=None):
        self.email = email
        self.password = password
        self.verify = verify
        self.dest = dest
        Templated.__init__(self)

class PrefApps(Templated):
    """Preference form for managing authorized third-party applications."""

    def __init__(self, my_apps, developed_apps):
        self.my_apps = my_apps
        self.developed_apps = developed_apps
        super(PrefApps, self).__init__()

class PrefDelete(Templated):
    """Preference form for deleting a user's own account."""
    pass


class MessagePage(Reddit):
    """Defines the content for /message/*"""
    def __init__(self, *a, **kw):
        if not kw.has_key('show_sidebar'):
            kw['show_sidebar'] = False
        Reddit.__init__(self, *a, **kw)
        if is_api():
            self.replybox = None
        else:
            self.replybox = UserText(item = None, creating = True,
                                     post_form = 'comment', display = False,
                                     cloneable = True)


    def content(self):
        return self.content_stack((self.replybox,
                                   self.infobar,
                                   self.nav_menu,
                                   self._content))

    def build_toolbars(self):
        buttons =  [NamedButton('compose', sr_path = False),
                    NamedButton('inbox', aliases = ["/message/comments",
                                                    "/message/uread",
                                                    "/message/messages",
                                                    "/message/selfreply"],
                                sr_path = False),
                    NamedButton('sent', sr_path = False)]
        if c.user_is_loggedin and c.user.is_moderator_somewhere:
            buttons.append(ModeratorMailButton(menu.modmail, "moderator",
                                               sr_path = False))
        if not c.default_sr:
            buttons.append(ModeratorMailButton(
                _("%(site)s mail") % {'site': c.site.name}, "moderator",
                aliases = ["/about/message/inbox",
                           "/about/message/unread"]))
        return [PageNameNav('nomenu', title = _("message")),
                NavMenu(buttons, base_path = "/message", type="tabmenu")]

class MessageCompose(Templated):
    """Compose message form."""
    def __init__(self,to='', subject='', message='', success='',
                 captcha = None):
        from r2.models.admintools import admintools

        Templated.__init__(self, to = to, subject = subject,
                         message = message, success = success,
                         captcha = captcha,
                         admins = admintools.admin_list())


class BoringPage(Reddit):
    """parent class For rendering all sorts of uninteresting,
    sortless, navless form-centric pages.  The top navmenu is
    populated only with the text provided with pagename and the page
    title is 'reddit.com: pagename'"""

    extension_handling= False

    def __init__(self, pagename, css_class=None, **context):
        self.pagename = pagename
        name = c.site.name or g.default_sr
        if css_class:
            self.css_class = css_class
        if "title" not in context:
            context['title'] = "%s: %s" % (name, pagename)

        Reddit.__init__(self, **context)

    def build_toolbars(self):
        if not isinstance(c.site, (DefaultSR, SubSR)) and not c.cname:
            return [PageNameNav('subreddit', title = self.pagename)]
        else:
            return [PageNameNav('nomenu', title = self.pagename)]

class HelpPage(BoringPage):
    def build_toolbars(self):
        return [PageNameNav('help', title = self.pagename)]

class FormPage(BoringPage):
    create_reddit_box  = False
    submit_box         = False
    """intended for rendering forms with no rightbox needed or wanted"""
    def __init__(self, pagename, show_sidebar = False, *a, **kw):
        BoringPage.__init__(self, pagename,  show_sidebar = show_sidebar,
                            *a, **kw)

class AdvertisingPage(FormPage):
    extra_stylesheets  = ['selfserve.less']

class LoginPage(BoringPage):
    enable_login_cover = False
    short_title = "login"

    """a boring page which provides the Login/register form"""
    def __init__(self, **context):
        self.dest = context.get('dest', '')
        context['loginbox'] = False
        context['show_sidebar'] = False
        if c.render_style == "compact":
            title = self.short_title
        else:
            title = _("login or register")
        BoringPage.__init__(self,  title, **context)

        if self.dest:
            u = UrlParser(self.dest)
            # Display a preview message for OAuth2 client authorizations
            if u.path in ['/api/v1/authorize', '/api/v1/authorize.compact']:
                client_id = u.query_dict.get("client_id")
                self.client = client_id and OAuth2Client.get_token(client_id)
                if self.client:
                    self.infobar = ClientInfoBar(self.client,
                                                 strings.oauth_login_msg)
                else:
                    self.infobar = None

    def content(self):
        kw = {}
        for x in ('user_login', 'user_reg'):
            kw[x] = getattr(self, x) if hasattr(self, x) else ''
        login_content = self.login_template(dest = self.dest, **kw)
        return self.content_stack((self.infobar, login_content))

    @classmethod
    def login_template(cls, **kw):
        return Login(**kw)

class RegisterPage(LoginPage):
    short_title = "register"
    @classmethod
    def login_template(cls, **kw):
        return Register(**kw)

class AdminModeInterstitial(BoringPage):
    def __init__(self, dest, *args, **kwargs):
        self.dest = dest
        BoringPage.__init__(self, _("turn admin on"),
                            show_sidebar=False,
                            *args, **kwargs)

    def content(self):
        return PasswordVerificationForm(dest=self.dest)

class PasswordVerificationForm(Templated):
    def __init__(self, dest):
        self.dest = dest
        Templated.__init__(self)

class Login(Templated):
    """The two-unit login and register form."""
    def __init__(self, user_reg = '', user_login = '', dest='', is_popup=False):
        Templated.__init__(self, user_reg = user_reg, user_login = user_login,
                           dest = dest, captcha = Captcha(),
                           is_popup=is_popup,
                           registration_info=RegistrationInfo())

class Register(Login):
    pass


class RegistrationInfo(Templated):
    def __init__(self):
        html = unsafe(self.get_registration_info_html())
        Templated.__init__(self, content_html=html)

    @classmethod
    @memoize('registration_info_html', time=10*60)
    def get_registration_info_html(cls):
        try:
            wp = WikiPage.get(Frontpage, g.wiki_page_registration_info)
        except tdb_cassandra.NotFound:
            return ''
        else:
            return wikimarkdown(wp.content, include_toc=False, target='_blank')


class OAuth2AuthorizationPage(BoringPage):
    def __init__(self, client, redirect_uri, scope, state, duration):
        if duration == "permanent":
            expiration = None
        else:
            expiration = (
                datetime.datetime.now(g.tz)
                + datetime.timedelta(seconds=OAuth2AccessToken._ttl + 1))
        content = OAuth2Authorization(client=client,
                                      redirect_uri=redirect_uri,
                                      scope=scope,
                                      state=state,
                                      duration=duration,
                                      expiration=expiration)
        BoringPage.__init__(self, _("request for permission"),
                            show_sidebar=False, content=content,
                            short_title=_("permission"))

class OAuth2Authorization(Templated):
    pass

class SearchPage(BoringPage):
    """Search results page"""
    searchbox = False
    extra_page_classes = ['search-page']

    def __init__(self, pagename, prev_search, elapsed_time,
                 search_params={},
                 simple=False, restrict_sr=False, site=None,
                 syntax=None, converted_data=None, facets={}, sort=None,
                 recent=None,
                 *a, **kw):
        self.searchbar = SearchBar(prev_search=prev_search,
                                   elapsed_time=elapsed_time,
                                   search_params=search_params,
                                   show_feedback=True, site=site,
                                   simple=simple, restrict_sr=restrict_sr,
                                   syntax=syntax, converted_data=converted_data,
                                   facets=facets, sort=sort, recent=recent)
        BoringPage.__init__(self, pagename, robots='noindex', *a, **kw)

    def content(self):
        return self.content_stack((self.searchbar, self.infobar,
                                   self.nav_menu, self._content))

class TakedownPage(BoringPage):
    def __init__(self, link):
        BoringPage.__init__(self, getattr(link, "takedown_title", _("bummer")),
                            content = TakedownPane(link))

    def render(self, *a, **kw):
        response = BoringPage.render(self, *a, **kw)
        return response


class TakedownPane(Templated):
    def __init__(self, link, *a, **kw):
        self.link = link
        self.explanation = getattr(self.link, "explanation",
                                   _("this page is no longer available due to a copyright claim."))
        Templated.__init__(self, *a, **kw)

class CommentsPanel(Templated):
    """the side-panel on the reddit toolbar frame that shows the top
       comments of a link"""

    def __init__(self, link = None, listing = None, expanded = False, *a, **kw):
        self.link = link
        self.listing = listing
        self.expanded = expanded

        Templated.__init__(self, *a, **kw)

class CommentVisitsBox(Templated):
    def __init__(self, visits, *a, **kw):
        self.visits = []
        for visit in reversed(visits):
            pretty = timesince(visit, precision=60)
            self.visits.append(pretty)
        Templated.__init__(self, *a, **kw)

class LinkInfoPage(Reddit):
    """Renders the varied /info pages for a link.  The Link object is
    passed via the link argument and the content passed to this class
    will be rendered after a one-element listing consisting of that
    link object.

    In addition, the rendering is reordered so that any nav_menus
    passed to this class will also be rendered underneath the rendered
    Link.
    """

    create_reddit_box = False
    extra_page_classes = ['single-page']

    def __init__(self, link = None, comment = None,
                 link_title = '', subtitle = None, num_duplicates = None,
                 show_promote_button=False, *a, **kw):

        c.permalink_page = True
        expand_children = kw.get("expand_children", not bool(comment))

        wrapper = default_thing_wrapper(expand_children=expand_children)

        # link_listing will be the one-element listing at the top
        self.link_listing = wrap_links(link, wrapper = wrapper)

        # link is a wrapped Link object
        self.link = self.link_listing.things[0]

        link_title = ((self.link.title) if hasattr(self.link, 'title') else '')

        # defaults whether or not there is a comment
        params = {'title':_force_unicode(link_title), 'site' : c.site.name}
        title = strings.link_info_title % params
        short_description = None
        if link and link.selftext:
            short_description = _truncate(link.selftext.strip(), MAX_DESCRIPTION_LENGTH)
        # only modify the title if the comment/author are neither deleted nor spam
        if comment and not comment._deleted and not comment._spam:
            author = Account._byID(comment.author_id, data=True)

            if not author._deleted and not author._spam:
                params = {'author' : author.name, 'title' : _force_unicode(link_title)}
                title = strings.permalink_title % params
                short_description = _truncate(comment.body.strip(), MAX_DESCRIPTION_LENGTH) if comment.body else None

        if self.link.has_thumbnail and self.link.thumbnail:
            kw['meta_thumbnail'] = self.link.thumbnail

        self.subtitle = subtitle

        if hasattr(self.link, "shortlink"):
            self.shortlink = self.link.shortlink

        if hasattr(self.link, "dart_keyword"):
            c.custom_dart_keyword = self.link.dart_keyword

        # if we're already looking at the 'duplicates' page, we can
        # avoid doing this lookup twice
        if num_duplicates is None:
            builder = url_links_builder(self.link.url,
                                        exclude=self.link._fullname,
                                        public_srs_only=True)
            self.num_duplicates = len(builder.get_items()[0])
        else:
            self.num_duplicates = num_duplicates

        self.show_promote_button = show_promote_button
        robots = "noindex,nofollow" if link._deleted or link._spam else None
        Reddit.__init__(self, title = title, short_description=short_description, robots=robots, *a, **kw)

    def build_toolbars(self):
        base_path = "/%s/%s/" % (self.link._id36, title_to_url(self.link.title))
        base_path = _force_utf8(base_path)


        def info_button(name, **fmt_args):
            return NamedButton(name, dest = '/%s%s' % (name, base_path),
                               aliases = ['/%s/%s' % (name, self.link._id36)],
                               fmt_args = fmt_args)
        buttons = []
        if not getattr(self.link, "disable_comments", False):
            buttons.extend([info_button('comments'),
                            info_button('related')])

            if self.num_duplicates > 0:
                buttons.append(info_button('duplicates', num=self.num_duplicates))

        if self.show_promote_button:
            buttons.append(NavButton(menu.promote, 'promoted', sr_path=False))

        toolbar = [NavMenu(buttons, base_path = "", type="tabmenu")]

        if not isinstance(c.site, DefaultSR) and not c.cname:
            toolbar.insert(0, PageNameNav('subreddit'))

        if c.user_is_admin:
            from admin_pages import AdminLinkMenu
            toolbar.append(AdminLinkMenu(self.link))

        return toolbar

    def content(self):
        title_buttons = getattr(self, "subtitle_buttons", [])
        return self.content_stack((self.infobar, self.link_listing,
                                   PaneStack([PaneStack((self.nav_menu,
                                                         self._content))],
                                             title = self.subtitle,
                                             title_buttons = title_buttons,
                                             css_class = "commentarea")))

    def rightbox(self):
        rb = Reddit.rightbox(self)
        if not (self.link.promoted and not c.user_is_sponsor):
            rb.insert(1, LinkInfoBar(a = self.link))
        return rb

    def page_classes(self):
        classes = Reddit.page_classes(self)

        if self.link.flair_css_class:
            for css_class in self.link.flair_css_class.split():
                classes.add('post-linkflair-' + css_class)

        if c.user_is_loggedin and self.link.author == c.user:
            classes.add("post-submitter")

        time_ago = datetime.datetime.now(g.tz) - self.link._date
        delta = datetime.timedelta
        steps = [
            delta(minutes=10),
            delta(hours=6),
            delta(hours=24),
        ]
        for step in steps:
            if time_ago < step:
                if step < delta(hours=1):
                    step_str = "%dm" % (step.total_seconds() / 60)
                else:
                    step_str = "%dh" % (step.total_seconds() / (60 * 60))
                classes.add("post-under-%s-old" % step_str)

        return classes

class LinkCommentSep(Templated):
    pass

class CommentPane(Templated):
    def cache_key(self):
        num = self.article.num_comments
        # bit of triage: we don't care about 10% changes in comment
        # trees once they get to a certain length.  The cache is only a few
        # min long anyway.
        if num > 1000:
            num = (num / 100) * 100
        elif num > 100:
            num = (num / 10) * 10
        return "_".join(map(str, ["commentpane", self.article._fullname,
                                  self.article.contest_mode,
                                  num, self.sort, self.num, c.lang,
                                  self.can_reply, c.render_style,
                                  c.domain_prefix, c.secure,
                                  c.user.pref_show_flair,
                                  c.user.pref_show_link_flair,
                                  c.can_save,
                                  self.max_depth]))

    def __init__(self, article, sort, comment, context, num, **kw):
        # keys: lang, num, can_reply, render_style
        # disable: admin

        timer = g.stats.get_timer("service_time.CommentPaneCache")
        timer.start()

        from r2.models import CommentBuilder, NestedListing
        from r2.controllers.reddit_base import UnloggedUser

        self.sort = sort
        self.num = num
        self.article = article

        self.max_depth = kw.get('max_depth')

        # don't cache on permalinks or contexts, and keep it to html
        try_cache = not comment and not context and (c.render_style == "html")
        self.can_reply = False
        if c.user_is_admin:
            try_cache = False

        # don't cache if the current user is the author of the link
        if c.user_is_loggedin and c.user._id == article.author_id:
            try_cache = False

        if try_cache and c.user_is_loggedin:
            sr = article.subreddit_slow
            c.can_reply = self.can_reply = sr.can_comment(c.user)
            c.can_save = True
            # don't cache if the current user can ban comments in the listing
            try_cache = not sr.can_ban(c.user)
            # don't cache for users with custom hide threshholds
            try_cache &= (c.user.pref_min_comment_score ==
                         Account._defaults["pref_min_comment_score"])

        def renderer():
            builder = CommentBuilder(article, sort, comment=comment,
                                     context=context, num=num, **kw)
            listing = NestedListing(builder, parent_name=article._fullname)
            return listing.listing()

        # disable the cache if the user is the author of anything in the
        # thread because of edit buttons etc.
        my_listing = None
        if try_cache and c.user_is_loggedin:
            my_listing = renderer()
            for t in self.listing_iter(my_listing):
                if getattr(t, "is_author", False):
                    try_cache = False
                    break

        timer.intermediate("try_cache")
        cache_hit = False

        if try_cache:
            # try to fetch the comment tree from the cache
            key = self.cache_key()
            self.rendered = g.pagecache.get(key)
            if not self.rendered:
                # spoof an unlogged in user
                user = c.user
                logged_in = c.user_is_loggedin
                try:
                    c.user = UnloggedUser([c.lang])
                    # Preserve the viewing user's flair preferences.
                    c.user.pref_show_flair = user.pref_show_flair
                    c.user.pref_show_link_flair = user.pref_show_link_flair
                    c.user_is_loggedin = False

                    # render as if not logged in (but possibly with reply buttons)
                    self.rendered = renderer().render()
                    g.pagecache.set(
                        key,
                        self.rendered,
                        time=g.commentpane_cache_time
                    )

                finally:
                    # undo the spoofing
                    c.user = user
                    c.user_is_loggedin = logged_in
            else:
                cache_hit = True

            # figure out what needs to be updated on the listing
            if c.user_is_loggedin:
                likes = []
                dislikes = []
                is_friend = set()
                gildings = {}
                saves = set()
                for t in self.listing_iter(my_listing):
                    if not hasattr(t, "likes"):
                        # this is for MoreComments and MoreRecursion
                        continue
                    if getattr(t, "friend", False) and not t.author._deleted:
                        is_friend.add(t.author._fullname)
                    if t.likes:
                        likes.append(t._fullname)
                    if t.likes is False:
                        dislikes.append(t._fullname)
                    if t.user_gilded:
                        gildings[t._fullname] = (t.gilded_message, t.gildings)
                    if t.saved:
                        saves.add(t._fullname)
                self.rendered += ThingUpdater(likes = likes,
                                              dislikes = dislikes,
                                              is_friend = is_friend,
                                              gildings = gildings,
                                              saves = saves).render()
            g.log.debug("using comment page cache")
        else:
            my_listing = my_listing or renderer()
            self.rendered = my_listing.render()

        if try_cache:
            if cache_hit:
                timer.stop("hit")
            else:
                timer.stop("miss")
        else:
            timer.stop("uncached")

    def listing_iter(self, l):
        for t in l:
            yield t
            for x in self.listing_iter(getattr(t, "child", [])):
                yield x

    def render(self, *a, **kw):
        return self.rendered

class ThingUpdater(Templated):
    pass


class LinkInfoBar(Templated):
    """Right box for providing info about a link."""
    def __init__(self, a = None):
        if a:
            a = Wrapped(a)
        Templated.__init__(self, a = a, datefmt = datefmt)

class EditReddit(Reddit):
    """Container for the about page for a reddit"""
    extension_handling= False

    def __init__(self, *a, **kw):
        from r2.lib.menus import menu

        try:
            key = kw.pop("location")
            title = menu[key]
        except KeyError:
            is_moderator = c.user_is_loggedin and \
                c.site.is_moderator(c.user) or c.user_is_admin

            title = (_('subreddit settings') if is_moderator else
                     _('about %(site)s') % dict(site=c.site.name))

        Reddit.__init__(self, title=title, *a, **kw)

    def build_toolbars(self):
        if not c.cname:
            return [PageNameNav('subreddit', title=self.title)]
        else:
            return []

class SubredditsPage(Reddit):
    """container for rendering a list of reddits.  The corner
    searchbox is hidden and its functionality subsumed by an in page
    SearchBar for searching over reddits.  As a result this class
    takes the same arguments as SearchBar, which it uses to construct
    self.searchbar"""
    searchbox    = False
    submit_box   = False
    def __init__(self, prev_search = '', elapsed_time = 0,
                 title = '', loginbox = True, infotext = None, show_interestbar=False,
                 search_params = {}, *a, **kw):
        Reddit.__init__(self, title = title, loginbox = loginbox, infotext = infotext,
                        *a, **kw)
        self.searchbar = SearchBar(prev_search = prev_search,
                                   elapsed_time = elapsed_time,
                                   header = _('search subreddits by name'),
                                   search_params = {},
                                   simple=True,
                                   subreddit_search=True
                                   )
        self.sr_infobar = InfoBar(message = strings.sr_subscribe)

        self.interestbar = InterestBar(True) if show_interestbar else None

    def build_toolbars(self):
        buttons =  [NavButton(menu.popular, ""),
                    NamedButton("new")]
        if c.user_is_admin:
            buttons.append(NamedButton("banned"))

        if c.user_is_loggedin:
            #add the aliases to "my reddits" stays highlighted
            buttons.append(NamedButton("mine",
                                       aliases=['/subreddits/mine/subscriber',
                                                '/subreddits/mine/contributor',
                                                '/subreddits/mine/moderator']))

        return [PageNameNav('subreddits'),
                NavMenu(buttons, base_path = '/subreddits', type="tabmenu")]

    def content(self):
        return self.content_stack((self.interestbar, self.searchbar,
                                   self.nav_menu, self.sr_infobar,
                                   self._content))

    def rightbox(self):
        ps = Reddit.rightbox(self)
        srs = Subreddit.user_subreddits(c.user, ids=False, limit=None)
        srs.sort(key=lambda sr: sr.name.lower())
        subscribe_box = SubscriptionBox(srs,
                                        multi_text=strings.subscribed_multi)
        num_reddits = len(subscribe_box.srs)
        ps.append(SideContentBox(_("your front page subreddits (%s)") %
                                 num_reddits, [subscribe_box]))
        return ps

class MySubredditsPage(SubredditsPage):
    """Same functionality as SubredditsPage, without the search box."""

    def content(self):
        return self.content_stack((self.nav_menu, self.infobar, self._content))


def votes_visible(user):
    """Determines whether to show/hide a user's votes.  They are visible:
     * if the current user is the user in question
     * if the user has a preference showing votes
     * if the current user is an administrator
    """
    return ((c.user_is_loggedin and c.user.name == user.name) or
            user.pref_public_votes or
            c.user_is_admin)


class ProfilePage(Reddit):
    """Container for a user's profile page.  As such, the Account
    object of the user must be passed in as the first argument, along
    with the current sub-page (to determine the title to be rendered
    on the page)"""

    searchbox         = False
    create_reddit_box = False
    submit_box        = False
    extra_page_classes = ['profile-page']

    def __init__(self, user, *a, **kw):
        self.user     = user
        Reddit.__init__(self, *a, **kw)

    def build_toolbars(self):
        path = "/user/%s/" % self.user.name
        main_buttons = [NavButton(menu.overview, '/', aliases = ['/overview']),
                   NamedButton('comments'),
                   NamedButton('submitted'),
                   NamedButton('gilded')]

        if votes_visible(self.user):
            main_buttons += [NamedButton('liked'),
                        NamedButton('disliked'),
                        NamedButton('hidden')]

        if c.user_is_loggedin and (c.user._id == self.user._id or
                                   c.user_is_admin):
            main_buttons += [NamedButton('saved')]

        if c.user_is_sponsor:
            main_buttons += [NamedButton('promoted')]

        toolbar = [PageNameNav('nomenu', title = self.user.name),
                   NavMenu(main_buttons, base_path = path, type="tabmenu")]

        if c.user_is_admin:
            from admin_pages import AdminProfileMenu
            toolbar.append(AdminProfileMenu(path))

        return toolbar


    def rightbox(self):
        rb = Reddit.rightbox(self)

        tc = TrophyCase(self.user)
        helplink = ( "/wiki/awards", _("what's this?") )
        scb = SideContentBox(title=_("trophy case"),
                 helplink=helplink, content=[tc],
                 extra_class="trophy-area")

        rb.push(scb)

        multis = [m for m in LabeledMulti.by_owner(self.user)
                  if m.visibility == "public"]
        if multis:
            scb = SideContentBox(title=_("public multireddits"), content=[
                SidebarMultiList(multis)
            ])
            rb.push(scb)

        if c.user_is_admin:
            from r2.lib.pages.admin_pages import AdminNotesSidebar
            from admin_pages import AdminSidebar

            rb.push(AdminSidebar(self.user))
            rb.push(AdminNotesSidebar('user', self.user.name))
        elif c.user_is_sponsor:
            from admin_pages import SponsorSidebar
            rb.push(SponsorSidebar(self.user))

        mod_sr_ids = Subreddit.reverse_moderator_ids(self.user)
        all_mod_srs = Subreddit._byID(mod_sr_ids, data=True,
                                      return_dict=False)
        mod_srs = [sr for sr in all_mod_srs if sr.can_view(c.user)]
        if mod_srs:
            rb.push(SideContentBox(title=_("moderator of"),
                                   content=[SidebarModList(mod_srs)]))

        if (c.user == self.user or c.user.employee or
            self.user.pref_public_server_seconds):
            seconds_bar = ServerSecondsBar(self.user)
            if seconds_bar.message or seconds_bar.gift_message:
                rb.push(seconds_bar)

        rb.push(ProfileBar(self.user))

        return rb

class TrophyCase(Templated):
    def __init__(self, user):
        self.user = user
        self.trophies = []
        self.invisible_trophies = []
        self.dupe_trophies = []

        award_ids_seen = []

        for trophy in Trophy.by_account(user):
            if trophy._thing2.awardtype == 'invisible':
                self.invisible_trophies.append(trophy)
            elif trophy._thing2_id in award_ids_seen:
                self.dupe_trophies.append(trophy)
            else:
                self.trophies.append(trophy)
                award_ids_seen.append(trophy._thing2_id)

        Templated.__init__(self)


class SidebarMultiList(Templated):
    def __init__(self, multis):
        Templated.__init__(self)
        multis.sort(key=lambda multi: multi.name.lower())
        self.multis = multis


class SidebarModList(Templated):
    def __init__(self, subreddits):
        Templated.__init__(self)
        # primary sort is desc. subscribers, secondary is name
        self.subreddits = sorted(subreddits,
                                 key=lambda sr: (-sr._ups, sr.name.lower()))


class ProfileBar(Templated):
    """Draws a right box for info about the user (karma, etc)"""
    def __init__(self, user):
        Templated.__init__(self, user = user)
        self.is_friend = None
        self.my_fullname = None
        self.gold_remaining = None
        running_out_of_gold = False
        self.gold_creddit_message = None

        if c.user_is_loggedin:
            if ((user._id == c.user._id or c.user_is_admin)
                and getattr(user, "gold", None)):
                self.gold_expiration = getattr(user, "gold_expiration", None)
                if self.gold_expiration is None:
                    self.gold_remaining = _("an unknown amount")
                else:
                    gold_days_left = (self.gold_expiration -
                                      datetime.datetime.now(g.tz)).days
                    if gold_days_left < 7:
                        running_out_of_gold = True

                    if gold_days_left < 1:
                        self.gold_remaining = _("less than a day")
                    else:
                        # Round remaining gold to number of days
                        precision = 60 * 60 * 24
                        self.gold_remaining = timeuntil(self.gold_expiration,
                                                        precision)

                if user.has_paypal_subscription:
                    self.paypal_subscr_id = user.gold_subscr_id
                if user.has_stripe_subscription:
                    self.stripe_customer_id = user.gold_subscr_id

            if ((user._id == c.user._id or c.user_is_admin) and
                user.gold_creddits > 0):
                msg = ungettext("%(creddits)s gold creddit to give",
                                "%(creddits)s gold creddits to give",
                                user.gold_creddits)
                msg = msg % dict(creddits=user.gold_creddits)
                self.gold_creddit_message = msg

            if user._id != c.user._id:
                self.goldlink = "/gold?goldtype=gift&recipient=" + user.name
                self.giftmsg = _("give reddit gold to %(user)s to show "
                                 "your appreciation") % {'user': user.name}
            elif running_out_of_gold:
                self.goldlink = "/gold/about"
                self.giftmsg = _("renew your reddit gold")
            elif not c.user.gold:
                self.goldlink = "/gold/about"
                self.giftmsg = _("get extra features and help support reddit "
                                 "with a reddit gold subscription")

            self.my_fullname = c.user._fullname
            self.is_friend = self.user._id in c.user.friends


class ServerSecondsBar(Templated):
    pennies_per_server_second = {
        datetime.datetime.strptime(datestr, "%Y/%m/%d").date(): v
        for datestr, v in g.live_config['pennies_per_server_second'].iteritems()
    }

    my_message = _("you have helped pay for *%(time)s* of reddit server time.")
    their_message = _("/u/%(user)s has helped pay for *%%(time)s* of reddit server "
                      "time.")

    my_gift_message = _("gifts on your behalf have helped pay for *%(time)s* of "
                        "reddit server time.")
    their_gift_message = _("gifts on behalf of /u/%(user)s have helped pay for "
                           "*%%(time)s* of reddit server time.")

    @classmethod
    def get_rate(cls, dt):
        cutoff_dates = sorted(cls.pennies_per_server_second.keys())
        dt = dt.date()
        key = max(filter(lambda cutoff_date: dt >= cutoff_date, cutoff_dates))
        return cls.pennies_per_server_second[key]

    @classmethod
    def subtract_fees(cls, pennies):
        # for simplicity all payment processor fees are $0.30 + 2.9%
        return pennies * (1 - 0.029) - 30

    @classmethod
    def current_value_of_month(cls):
        price = g.gold_month_price.pennies
        after_fees = cls.subtract_fees(price)
        current_rate = cls.get_rate(datetime.datetime.now(g.display_tz))
        delta = datetime.timedelta(seconds=after_fees / current_rate)
        return precise_format_timedelta(delta, threshold=5, locale=c.locale)

    def make_message(self, seconds, my_message, their_message):
        if not seconds:
            return ''

        delta = datetime.timedelta(seconds=seconds)
        server_time = precise_format_timedelta(delta, threshold=5,
                                                locale=c.locale)
        if c.user == self.user:
            message = my_message
        else:
            message = their_message % {'user': self.user.name}
        return message % {'time': server_time}

    def __init__(self, user):
        Templated.__init__(self)

        self.is_public = user.pref_public_server_seconds
        self.is_user = c.user == user
        self.user = user

        seconds = 0.
        gold_payments = gold_payments_by_user(user)

        for payment in gold_payments:
            rate = self.get_rate(payment.date)
            seconds += self.subtract_fees(payment.pennies) / rate

        try:
            q = (Bid.query().filter(Bid.account_id == user._id)
                    .filter(Bid.status == Bid.STATUS.CHARGE)
                    .filter(Bid.transaction > 0))
            selfserve_payments = list(q)
        except NotFound:
            selfserve_payments = []

        for payment in selfserve_payments:
            rate = self.get_rate(payment.date)
            seconds += self.subtract_fees(payment.charge_amount * 100) / rate
        self.message = self.make_message(seconds, self.my_message,
                                         self.their_message)

        seconds = 0.
        gold_gifts = gold_received_by_user(user)

        for payment in gold_gifts:
            rate = self.get_rate(payment.date)
            pennies = days_to_pennies(payment.days)
            seconds += self.subtract_fees(pennies) / rate
        self.gift_message = self.make_message(seconds, self.my_gift_message,
                                              self.their_gift_message)


class MenuArea(Templated):
    """Draws the gray box at the top of a page for sort menus"""
    def __init__(self, menus = []):
        Templated.__init__(self, menus = menus)

class InfoBar(Templated):
    """Draws the yellow box at the top of a page for info"""
    def __init__(self, message = '', extra_class = ''):
        Templated.__init__(self, message = message, extra_class = extra_class)

class WelcomeBar(InfoBar):
    def __init__(self):
        messages = g.live_config.get("welcomebar_messages")
        if messages:
            message = random.choice(messages).split(" / ")
        else:
            message = (_("reddit is a platform for internet communities"),
                       _("where your votes shape what the world is talking about."))
        InfoBar.__init__(self, message=message)

class ClientInfoBar(InfoBar):
    """Draws the message the top of a login page before OAuth2 authorization"""
    def __init__(self, client, *args, **kwargs):
        kwargs.setdefault("extra_class", "client-info")
        InfoBar.__init__(self, *args, **kwargs)
        self.client = client

class SidebarMessage(Templated):
    """An info message box on the sidebar."""
    def __init__(self, message, extra_class=None):
        Templated.__init__(self, message=message, extra_class=extra_class)

class RedditError(BoringPage):
    site_tracking = False
    def __init__(self, title, message, image=None, sr_description=None,
                 explanation=None):
        BoringPage.__init__(self, title, loginbox=False,
                            show_sidebar = False,
                            content=ErrorPage(title=title,
                                              message=message,
                                              image=image,
                                              sr_description=sr_description,
                                              explanation=explanation))

class ErrorPage(Templated):
    """Wrapper for an error message"""
    def __init__(self, title, message, image=None, explanation=None, **kwargs):
        if not image:
            letter = random.choice(['a', 'b', 'c', 'd', 'e'])
            image = 'reddit404' + letter + '.png'
        # Normalize explanation strings.
        if explanation:
            explanation = explanation.lower().rstrip('.') + '.'
        Templated.__init__(self,
                           title=title,
                           message=message,
                           image_url=image,
                           explanation=explanation,
                           **kwargs)


class Over18(Templated):
    """The creepy 'over 18' check page for nsfw content."""
    pass

class SubredditTopBar(CachedTemplate):

    """The horizontal strip at the top of most pages for navigating
    user-created reddits."""
    def __init__(self):
        self._my_reddits = None
        self._pop_reddits = None
        name = '' if not c.user_is_loggedin else c.user.name
        langs = "" if name else c.content_langs
        # poor man's expiration, with random initial time
        t = int(time.time()) / 3600
        if c.user_is_loggedin:
            t += c.user._id
        CachedTemplate.__init__(self, name = name, langs = langs, t = t,
                               over18 = c.over18)

    @property
    def my_reddits(self):
        if self._my_reddits is None:
            self._my_reddits = Subreddit.user_subreddits(c.user,
                                                         ids=False,
                                                         stale=True)
        return self._my_reddits

    @property
    def pop_reddits(self):
        if self._pop_reddits is None:
            p_srs = Subreddit.default_subreddits(ids = False,
                                                 limit = Subreddit.sr_limit)
            self._pop_reddits = [ sr for sr in p_srs
                                  if sr.name not in g.automatic_reddits ]
        return self._pop_reddits

    @property
    def show_my_reddits_dropdown(self):
        return len(self.my_reddits) > g.sr_dropdown_threshold

    def my_reddits_dropdown(self):
        drop_down_buttons = []
        for sr in sorted(self.my_reddits, key = lambda sr: sr.name.lower()):
            drop_down_buttons.append(SubredditButton(sr))
        drop_down_buttons.append(NavButton(menu.edit_subscriptions,
                                           sr_path = False,
                                           css_class = 'bottom-option',
                                           dest = '/subreddits/'))
        return SubredditMenu(drop_down_buttons,
                             title = _('my subreddits'),
                             type = 'srdrop')

    def subscribed_reddits(self):
        srs = [SubredditButton(sr) for sr in
                        sorted(self.my_reddits,
                               key = lambda sr: sr._downs,
                               reverse=True)
                        if sr.name not in g.automatic_reddits
                        ]
        return NavMenu(srs,
                       type='flatlist', separator = '-',
                       css_class = 'sr-bar')

    def popular_reddits(self, exclude=[]):
        exclusions = set(exclude)
        buttons = [SubredditButton(sr)
                   for sr in self.pop_reddits if sr not in exclusions]

        return NavMenu(buttons,
                       type='flatlist', separator = '-',
                       css_class = 'sr-bar', _id = 'sr-bar')

    def special_reddits(self):
        css_classes = {Random: "random",
                       RandomSubscription: "gold"}
        reddits = [Frontpage, All, Random]
        if getattr(c.site, "over_18", False):
            reddits.append(RandomNSFW)
        if c.user_is_loggedin:
            if c.user.gold:
                reddits.append(RandomSubscription)
            if c.user.friends:
                reddits.append(Friends)
            if c.user.is_moderator_somewhere:
                reddits.append(Mod)
        return NavMenu([SubredditButton(sr, css_class=css_classes.get(sr))
                        for sr in reddits],
                       type = 'flatlist', separator = '-',
                       css_class = 'sr-bar')

    def sr_bar (self):
        sep = '<span class="separator">&nbsp;|&nbsp;</span>'
        menus = []
        menus.append(self.special_reddits())
        menus.append(RawString(sep))


        if not c.user_is_loggedin:
            menus.append(self.popular_reddits())
        else:
            menus.append(self.subscribed_reddits())
            sep = '<span class="separator">&nbsp;&ndash;&nbsp;</span>'
            menus.append(RawString(sep))

            menus.append(self.popular_reddits(exclude=self.my_reddits))

        return menus


class MultiInfoBar(Templated):
    def __init__(self, multi, srs, user):
        Templated.__init__(self)
        self.multi = wrap_things(multi)[0]
        self.can_edit = multi.can_edit(user)
        self.can_copy = c.user_is_loggedin
        self.can_rename = c.user_is_loggedin and multi.owner == c.user
        srs.sort(key=lambda sr: sr.name.lower())
        self.description_md = multi.description_md
        self.srs = srs

        explore_sr = g.live_config["listing_chooser_explore_sr"]
        if explore_sr:
            self.share_url = "/r/%(sr)s/submit?url=%(url)s" % {
                "sr": explore_sr,
                "url": g.origin + self.multi.path,
            }
        else:
            self.share_url = None


class SubscriptionBox(Templated):
    """The list of reddits a user is currently subscribed to to go in
    the right pane."""
    def __init__(self, srs, multi_text=None):
        self.srs = srs
        self.goldlink = None
        self.goldmsg = None
        self.prelink = None
        self.multi_path = None
        self.multi_text = multi_text

        # Construct MultiReddit path
        if multi_text:
            self.multi_path = '/r/' + '+'.join([sr.name for sr in srs])

        if len(srs) > Subreddit.sr_limit and c.user_is_loggedin:
            if not c.user.gold:
                self.goldlink = "/gold"
                self.goldmsg = _("raise it to %s") % Subreddit.gold_limit
                self.prelink = ["/wiki/faq#wiki_how_many_subreddits_can_i_subscribe_to.3F",
                                _("%s visible") % Subreddit.sr_limit]
            else:
                self.goldlink = "/gold/about"
                extra = min(len(srs) - Subreddit.sr_limit,
                            Subreddit.gold_limit - Subreddit.sr_limit)
                visible = min(len(srs), Subreddit.gold_limit)
                bonus = {"bonus": extra}
                self.goldmsg = _("%(bonus)s bonus subreddits") % bonus
                self.prelink = ["/wiki/faq#wiki_how_many_subreddits_can_i_subscribe_to.3F",
                                _("%s visible") % visible]

        Templated.__init__(self, srs=srs, goldlink=self.goldlink,
                           goldmsg=self.goldmsg)

    @property
    def reddits(self):
        return wrap_links(self.srs)


class AllInfoBar(Templated):
    def __init__(self, site, user):
        self.sr = site
        self.allminus_url = None
        self.css_class = None
        if isinstance(site, AllMinus) and c.user.gold:
            self.description = (strings.r_all_minus_description + "\n\n" +
                                " ".join("/r/" + sr.name for sr in site.srs))
            self.css_class = "gold-accent"
        else:
            self.description = strings.r_all_description
            sr_ids = Subreddit.user_subreddits(user)
            srs = Subreddit._byID(sr_ids, data=True, return_dict=False)
            if srs:
                self.allminus_url = '/r/all-' + '-'.join([sr.name for sr in srs])

        self.gilding_listing = False
        if request.path.startswith("/comments/gilded"):
            self.gilding_listing = True

        Templated.__init__(self)


class CreateSubreddit(Templated):
    """reddit creation form."""
    def __init__(self, site = None, name = ''):
        Templated.__init__(self, site = site, name = name)

class SubredditStylesheet(Templated):
    """form for editing or creating subreddit stylesheets"""
    def __init__(self, site = None,
                 stylesheet_contents = ''):
        raw_images = ImagesByWikiPage.get_images(c.site, "config/stylesheet")
        images = {name: media_https_if_secure(url)
                  for name, url in raw_images.iteritems()}

        Templated.__init__(self, site = site, images=images,
                         stylesheet_contents = stylesheet_contents)

    @staticmethod
    def find_preview_comments(sr):
        comments = queries.get_sr_comments(sr)
        comments = list(comments)
        if not comments:
            comments = queries.get_all_comments()
            comments = list(comments)

        return Thing._by_fullname(comments[:25], data=True, return_dict=False)

    @staticmethod
    def find_preview_links(sr):
        # try to find a link to use, otherwise give up and return
        links = normalized_hot([sr._id])
        if not links:
            links = normalized_hot(Subreddit.default_subreddits())

        if links:
            links = links[:25]
            links = Link._by_fullname(links, data=True, return_dict=False)

        return links

    @staticmethod
    def rendered_link(links, media, compress, stickied=False):
        with c.user.safe_set_attr:
            c.user.pref_compress = compress
            c.user.pref_media = media
        links = wrap_links(links, show_nums=True, num=1)
        for wrapped in links:
            wrapped.stickied = stickied
        delattr(c.user, "pref_compress")
        delattr(c.user, "pref_media")
        return links.render(style="html")

    @staticmethod
    def rendered_comment(comments, gilded=False):
        wrapped = wrap_links(comments, num=1)
        if gilded:
            for w in wrapped:
                w.gilded_message = "this comment was fake-gilded"
        return wrapped.render(style="html")

class SubredditStylesheetSource(Templated):
    """A view of the unminified source of a subreddit's stylesheet."""
    def __init__(self, stylesheet_contents):
        Templated.__init__(self, stylesheet_contents=stylesheet_contents)

class CssError(Templated):
    """Rendered error returned to the stylesheet editing page via ajax"""
    def __init__(self, error):
        # error is an instance of cssfilter.py:ValidationError
        Templated.__init__(self, error = error)

    @property
    def message(self):
        return _(self.error.message_key) % self.error.message_params

class UploadedImage(Templated):
    "The page rendered in the iframe during an upload of a header image"
    def __init__(self,status,img_src, name="", errors = {}, form_id = ""):
        self.errors = list(errors.iteritems())
        Templated.__init__(self, status=status, img_src=img_src, name = name,
                           form_id = form_id)

class Thanks(Templated):
    """The page to claim reddit gold trophies"""
    def __init__(self, secret=None):
        if secret and secret.startswith("cr_"):
            status = "creddits"
        elif g.cache.get("recent-gold-" + c.user.name):
            status = "recent"
        elif c.user.gold:
            status = "gold"
        else:
            status = "mundane"

        Templated.__init__(self, status=status, secret=secret)

class GoldThanks(Templated):
    """An actual 'Thanks for buying gold!' landing page"""
    pass

class Gold(Templated):
    def __init__(self, goldtype, period, months, signed,
                 recipient, recipient_name, can_subscribe=True):

        if c.user.employee:
            user_creddits = 50
        else:
            user_creddits = c.user.gold_creddits

        Templated.__init__(self, goldtype = goldtype, period = period,
                           months = months, signed = signed,
                           recipient_name = recipient_name,
                           user_creddits = user_creddits,
                           bad_recipient =
                           bool(recipient_name and not recipient),
                           can_subscribe=can_subscribe)


class GoldPayment(Templated):
    def __init__(self, goldtype, period, months, signed,
                 recipient, giftmessage, passthrough, thing,
                 clone_template=False, thing_type=None):
        pay_from_creddits = False
        desc = None

        if period == "monthly" or 1 <= months < 12:
            unit_price = g.gold_month_price
            if period == 'monthly':
                price = unit_price
            else:
                price = unit_price * months
        else:
            unit_price = g.gold_year_price
            if period == 'yearly':
                price = unit_price
            else:
                years = months / 12
                price = unit_price * years

        if c.user.employee:
            user_creddits = 50
        else:
            user_creddits = c.user.gold_creddits

        if goldtype == "autorenew":
            summary = strings.gold_summary_autorenew % dict(user=c.user.name)
            if period == "monthly":
                paypal_buttonid = g.PAYPAL_BUTTONID_AUTORENEW_BYMONTH
            elif period == "yearly":
                paypal_buttonid = g.PAYPAL_BUTTONID_AUTORENEW_BYYEAR

            quantity = None
            stripe_key = g.secrets['stripe_public_key']
            coinbase_button_id = None

        elif goldtype == "onetime":
            if months < 12:
                paypal_buttonid = g.PAYPAL_BUTTONID_ONETIME_BYMONTH
                quantity = months
                coinbase_name = 'COINBASE_BUTTONID_ONETIME_%sMO' % quantity
                coinbase_button_id = getattr(g, coinbase_name, None)
            else:
                paypal_buttonid = g.PAYPAL_BUTTONID_ONETIME_BYYEAR
                quantity = months / 12
                months = quantity * 12
                coinbase_name = 'COINBASE_BUTTONID_ONETIME_%sYR' % quantity
                coinbase_button_id = getattr(g, coinbase_name, None)

            summary = strings.gold_summary_onetime % dict(user=c.user.name,
                                     amount=Score.somethings(months, "month"))

            stripe_key = g.secrets['stripe_public_key']

        else:
            if months < 12:
                if goldtype == "code":
                    paypal_buttonid = g.PAYPAL_BUTTONID_GIFTCODE_BYMONTH
                else:
                    paypal_buttonid = g.PAYPAL_BUTTONID_CREDDITS_BYMONTH
                quantity = months
                coinbase_name = 'COINBASE_BUTTONID_ONETIME_%sMO' % quantity
                coinbase_button_id = getattr(g, coinbase_name, None)
            else:
                if goldtype == "code":
                    paypal_buttonid = g.PAYPAL_BUTTONID_GIFTCODE_BYYEAR
                else:
                    paypal_buttonid = g.PAYPAL_BUTTONID_CREDDITS_BYYEAR
                quantity = months / 12
                coinbase_name = 'COINBASE_BUTTONID_ONETIME_%sYR' % quantity
                coinbase_button_id = getattr(g, coinbase_name, None)

            if goldtype in ("gift", "code"):
                if months <= user_creddits:
                    pay_from_creddits = True
                elif months >= 12:
                    # If you're not paying with creddits, you have to either
                    # buy by month or spend a multiple of 12 months
                    months = quantity * 12

            if goldtype == "creddits":
                summary = strings.gold_summary_creddits % dict(
                          amount=Score.somethings(months, "month"))
            elif goldtype == "gift":
                if clone_template:
                    if thing_type == "comment":
                        format = strings.gold_summary_gilding_comment
                    elif thing_type == "link":
                        format = strings.gold_summary_gilding_link
                elif thing:
                    if isinstance(thing, Comment):
                        format = strings.gold_summary_gilding_page_comment
                        desc = thing.body
                    else:
                        format = strings.gold_summary_gilding_page_link
                        desc = thing.markdown_link_slow()
                elif signed:
                    format = strings.gold_summary_signed_gift
                else:
                    format = strings.gold_summary_anonymous_gift

                if not clone_template:
                    summary = format % dict(
                        amount=Score.somethings(months, "month"),
                        recipient=recipient and
                                  recipient.name.replace('_', '&#95;'),
                    )
                else:
                    # leave the replacements to javascript
                    summary = format
            elif goldtype == "code":
                summary = strings.gold_summary_gift_code % dict(
                          amount=Score.somethings(months, "month"))
            else:
                raise ValueError("wtf is %r" % goldtype)

            stripe_key = g.secrets['stripe_public_key']

        Templated.__init__(self, goldtype=goldtype, period=period,
                           months=months, quantity=quantity,
                           unit_price=unit_price, price=price,
                           summary=summary, giftmessage=giftmessage,
                           pay_from_creddits=pay_from_creddits,
                           passthrough=passthrough,
                           thing=thing, clone_template=clone_template,
                           description=desc, thing_type=thing_type,
                           paypal_buttonid=paypal_buttonid,
                           stripe_key=stripe_key,
                           coinbase_button_id=coinbase_button_id)


class GoldSubscription(Templated):
    def __init__(self, user):
        if user.has_stripe_subscription:
            details = get_subscription_details(user)
        else:
            details = None

        if details:
            self.has_stripe_subscription = True
            date = details['next_charge_date']
            next_charge_date = format_date(date, format="short",
                                           locale=c.locale)
            credit_card_last4 = details['credit_card_last4']
            amount = format_currency(float(details['pennies']) / 100, 'USD',
                                     locale=c.locale)
            text = _("you have a credit card gold subscription. your card "
                     "(ending in %(last4)s) will be charged %(amount)s on "
                     "%(date)s.")
            self.text = text % dict(last4=credit_card_last4,
                                    amount=amount,
                                    date=next_charge_date)
            self.user_fullname = user._fullname
        else:
            self.has_stripe_subscription = False

        if user.has_paypal_subscription:
            self.has_paypal_subscription = True
            self.paypal_subscr_id = user.gold_subscr_id
            self.paypal_url = "https://www.paypal.com/cgi-bin/webscr?cmd=_subscr-find&alias=%s" % g.goldthanks_email
        else:
            self.has_paypal_subscription = False

        self.stripe_key = g.secrets['stripe_public_key']
        Templated.__init__(self)

class CreditGild(Templated):
    """Page for credit card payments for gilding."""
    pass


class GiftGold(Templated):
    """The page to gift reddit gold trophies"""
    def __init__(self, recipient):
        if c.user.employee:
            gold_creddits = 500
        else:
            gold_creddits = c.user.gold_creddits
        Templated.__init__(self, recipient=recipient, gold_creddits=gold_creddits)

class GoldGiftCodeEmail(Templated):
    """Email sent to a logged-out person that purchases a reddit
    gold gift code."""
    pass

class Password(Templated):
    """Form encountered when 'recover password' is clicked in the LoginFormWide."""
    def __init__(self, success=False):
        Templated.__init__(self, success = success)

class PasswordReset(Templated):
    """Template for generating an email to the user who wishes to
    reset their password (step 2 of password recovery, after they have
    entered their user name in Password.)"""
    pass

class PasswordChangeEmail(Templated):
    """Notification e-mail that a user's password has changed."""
    pass

class EmailChangeEmail(Templated):
    """Notification e-mail that a user's e-mail has changed."""
    pass

class VerifyEmail(Templated):
    pass

class Promo_Email(Templated):
    pass

class ResetPassword(Templated):
    """Form for actually resetting a lost password, after the user has
    clicked on the link provided to them in the Password_Reset email
    (step 3 of password recovery.)"""
    pass


class Captcha(Templated):
    """Container for rendering robot detection device."""
    def __init__(self, error=None):
        self.error = _('try entering those letters again') if error else ""
        self.iden = get_captcha()
        Templated.__init__(self)

class PermalinkMessage(Templated):
    """renders the box on comment pages that state 'you are viewing a
    single comment's thread'"""
    def __init__(self, comments_url):
        Templated.__init__(self, comments_url = comments_url)

class PaneStack(Templated):
    """Utility class for storing and rendering a list of block elements."""

    def __init__(self, panes=[], div_id = None, css_class=None, div=False,
                 title="", title_buttons = []):
        div = div or div_id or css_class or False
        self.div_id    = div_id
        self.css_class = css_class
        self.div       = div
        self.stack     = list(panes)
        self.title = title
        self.title_buttons = title_buttons
        Templated.__init__(self)

    def append(self, item):
        """Appends an element to the end of the current stack"""
        self.stack.append(item)

    def push(self, item):
        """Prepends an element to the top of the current stack"""
        self.stack.insert(0, item)

    def insert(self, *a):
        """inerface to list.insert on the current stack"""
        return self.stack.insert(*a)


class SearchForm(Templated):
    """The simple search form in the header of the page.  prev_search
    is the previous search."""
    def __init__(self, prev_search='', search_params={}, site=None,
                 simple=True, restrict_sr=False, subreddit_search=False,
                 syntax=None):
        Templated.__init__(self, prev_search=prev_search,
                           search_params=search_params, site=site,
                           simple=simple, restrict_sr=restrict_sr,
                           subreddit_search=subreddit_search, syntax=syntax)


class SearchBar(Templated):
    """More detailed search box for /search and /subreddits pages.

    Displays the previous search as well

    """
    def __init__(self, header=None, prev_search='',
                 elapsed_time=0, search_params={}, show_feedback=False,
                 simple=False, restrict_sr=False, site=None, syntax=None,
                 subreddit_search=False, converted_data=None, facets={},
                 sort=None, recent=None, **kw):
        if header is None:
            header = _("previous search")
        self.header = header

        self.prev_search  = prev_search
        self.elapsed_time = elapsed_time
        self.show_feedback = show_feedback

        Templated.__init__(self, search_params=search_params,
                           simple=simple, restrict_sr=restrict_sr,
                           site=site, syntax=syntax,
                           converted_data=converted_data,
                           subreddit_search=subreddit_search, facets=facets,
                           sort=sort, recent=recent)

class Frame(Wrapped):
    """Frameset for the FrameToolbar used when a user hits /tb/. The
    top 30px of the page are dedicated to the toolbar, while the rest
    of the page will show the results of following the link."""
    def __init__(self, url='', title='', fullname=None, thumbnail=None):
        if title:
            title = (_('%(site_title)s via %(domain)s')
                     % dict(site_title = _force_unicode(title),
                            domain     = g.domain))
        else:
            title = g.domain
        Wrapped.__init__(self, url = url, title = title,
                           fullname = fullname, thumbnail = thumbnail)

class FrameToolbar(Wrapped):
    """The reddit voting toolbar used together with Frame."""

    cachable = True
    extension_handling = False
    cache_ignore = Link.cache_ignore
    site_tracking = True
    def __init__(self, link, title = None, url = None, expanded = False, **kw):
        if link:
            self.title = link.title
            self.url = link.url
        else:
            self.title = title
            self.url = url

        self.expanded = expanded
        self.user_is_loggedin = c.user_is_loggedin
        self.have_messages = c.have_messages
        self.user_name = c.user.name if self.user_is_loggedin else ""
        self.cname = c.cname
        self.site_name = c.site.name
        self.site_description = c.site.description
        self.default_sr = c.default_sr

        Wrapped.__init__(self, link)
        if link is None:
            self.add_props(c.user, [self])

    @classmethod
    def add_props(cls, user, wrapped):
        # unlike most wrappers we can guarantee that there is a link
        # that this wrapper is wrapping.
        nonempty = [w for w in wrapped if hasattr(w, "_fullname")]
        Link.add_props(user, nonempty)
        for w in wrapped:
            w.score_fmt = Score.points
            if not hasattr(w, '_fullname'):
                w._fullname = None
                w.tblink = add_sr("/s/"+quote(w.url))
                submit_url_options = dict(url  = _force_unicode(w.url),
                                          then = 'tb')
                if w.title:
                    submit_url_options['title'] = _force_unicode(w.title)
                w.submit_url = add_sr('/submit' +
                                         query_string(submit_url_options))
            else:
                w.tblink = add_sr("/tb/"+w._id36)
                w.upstyle = "mod" if w.likes else ""
                w.downstyle = "mod" if w.likes is False else ""
            if not c.user_is_loggedin:
                w.loginurl = add_sr("/login?dest="+quote(w.tblink))
        # run to set scores with current score format (for example)
        Printable.add_props(user, nonempty)

    def page_classes(self):
        return ("toolbar",)


class NewLink(Templated):
    """Render the link submission form"""
    def __init__(self, captcha=None, url='', title='', text='', selftext='',
                 then='comments', resubmit=False, default_sr=None,
                 extra_subreddits=None, show_link=True, show_self=True):

        self.show_link = show_link
        self.show_self = show_self

        tabs = []
        if show_link:
            tabs.append(('link', ('link-desc', 'url-field')))
        if show_self:
            tabs.append(('text', ('text-desc', 'text-field')))

        if self.show_self and self.show_link:
            all_fields = set(chain(*(parts for (tab, parts) in tabs)))
            buttons = []

            if selftext == 'true' or text != '':
                self.default_tab = tabs[1][0]
            else:
                self.default_tab = tabs[0][0]

            for tab_name, parts in tabs:
                to_show = ','.join('#' + p for p in parts)
                to_hide = ','.join('#' + p for p in all_fields if p not in parts)
                onclick = "return select_form_tab(this, '%s', '%s');"
                onclick = onclick % (to_show, to_hide)
                if tab_name == self.default_tab:
                    self.default_show = to_show
                    self.default_hide = to_hide

                buttons.append(JsButton(tab_name, onclick=onclick, css_class=tab_name + "-button"))

            self.formtabs_menu = JsNavMenu(buttons, type = 'formtab')

        self.resubmit = resubmit
        self.default_sr = default_sr
        self.extra_subreddits = extra_subreddits

        Templated.__init__(self, captcha = captcha, url = url,
                         title = title, text = text, then = then)

class ShareLink(CachedTemplate):
    def __init__(self, link_name = "", emails = None):
        self.captcha = c.user.needs_captcha()
        self.username = c.user.name
        Templated.__init__(self, link_name = link_name,
                           emails = c.user.recent_share_emails())



class Share(Templated):
    pass

class Mail_Opt(Templated):
    pass

class OptOut(Templated):
    pass

class OptIn(Templated):
    pass


class Button(Wrapped):
    cachable = True
    extension_handling = False
    def __init__(self, link, **kw):
        Wrapped.__init__(self, link, **kw)
        if link is None:
            self.title = ""
            self.add_props(c.user, [self])


    @classmethod
    def add_props(cls, user, wrapped):
        # unlike most wrappers we can guarantee that there is a link
        # that this wrapper is wrapping.
        Link.add_props(user, [w for w in wrapped if hasattr(w, "_fullname")])
        for w in wrapped:
            # caching: store the user name since each button has a modhash
            w.user_name = c.user.name if c.user_is_loggedin else ""
            if not hasattr(w, '_fullname'):
                w._fullname = None

    def render(self, *a, **kw):
        res = Wrapped.render(self, *a, **kw)
        return responsive(res, True)

class ButtonLite(Button):
    def render(self, *a, **kw):
        return Wrapped.render(self, *a, **kw)

class ButtonDemoPanel(Templated):
    """The page for showing the different styles of embedable voting buttons"""
    pass

class SelfServeBlurb(Templated):
    pass

class ContactUs(Templated):
    pass

class FeedbackBlurb(Templated):
    pass

class Feedback(Templated):
    """The feedback and ad inquery form(s)"""
    def __init__(self, title, action):
        email = name = ''
        if c.user_is_loggedin:
            email = getattr(c.user, "email", "")
            name = c.user.name

        captcha = None
        if not c.user_is_loggedin or c.user.needs_captcha():
            captcha = Captcha()

        Templated.__init__(self,
                         captcha = captcha,
                         title = title,
                         action = action,
                         email = email,
                         name = name)


class WidgetDemoPanel(Templated):
    """Demo page for the .embed widget."""
    pass

class Bookmarklets(Templated):
    """The bookmarklets page."""
    def __init__(self, buttons=None):
        if buttons is None:
            buttons = ["reddit toolbar", "submit", "serendipity!"]
        Templated.__init__(self, buttons = buttons)


class UserAwards(Templated):
    """For drawing the regular-user awards page."""
    def __init__(self):
        from r2.models import Award, Trophy
        Templated.__init__(self)

        self.regular_winners = []
        self.manuals = []
        self.invisibles = []

        for award in Award._all_awards():
            if award.awardtype == 'regular':
                trophies = Trophy.by_award(award)
                # Don't show awards that nobody's ever won
                # (e.g., "9-Year Club")
                if trophies:
                    winner = trophies[0]._thing1.name
                    self.regular_winners.append( (award, winner, trophies[0]) )
            elif award.awardtype == 'manual':
                self.manuals.append(award)
            elif award.awardtype == 'invisible':
                self.invisibles.append(award)
            else:
                raise NotImplementedError

class AdminErrorLog(Templated):
    """The admin page for viewing the error log"""
    def __init__(self):
        hcb = g.hardcache.backend

        date_groupings = {}
        hexkeys_seen = {}

        idses = hcb.ids_by_category("error", limit=5000)
        errors = g.hardcache.get_multi(prefix="error-", keys=idses)

        for ids in idses:
            date, hexkey = ids.split("-")

            hexkeys_seen[hexkey] = True

            d = errors.get(ids, None)

            if d is None:
                log_text("error=None", "Why is error-%s None?" % ids,
                         "warning")
                continue

            tpl = (d.get('times_seen', 1), hexkey, d)
            date_groupings.setdefault(date, []).append(tpl)

        self.nicknames = {}
        self.statuses = {}

        nicks = g.hardcache.get_multi(prefix="error_nickname-",
                                      keys=hexkeys_seen.keys())
        stati = g.hardcache.get_multi(prefix="error_status-",
                                      keys=hexkeys_seen.keys())

        for hexkey in hexkeys_seen.keys():
            self.nicknames[hexkey] = nicks.get(hexkey, "???")
            self.statuses[hexkey] = stati.get(hexkey, "normal")

        idses = hcb.ids_by_category("logtext")
        texts = g.hardcache.get_multi(prefix="logtext-", keys=idses)

        for ids in idses:
            date, level, classification = ids.split("-", 2)
            textoccs = []
            dicts = texts.get(ids, None)
            if dicts is None:
                log_text("logtext=None", "Why is logtext-%s None?" % ids,
                         "warning")
                continue
            for d in dicts:
                textoccs.append( (d['text'], d['occ'] ) )

            sort_order = {
                'error': -1,
                'warning': -2,
                'info': -3,
                'debug': -4,
                }[level]

            tpl = (sort_order, level, classification, textoccs)
            date_groupings.setdefault(date, []).append(tpl)

        self.date_summaries = []

        for date in sorted(date_groupings.keys(), reverse=True):
            groupings = sorted(date_groupings[date], reverse=True)
            self.date_summaries.append( (date, groupings) )

        Templated.__init__(self)

class AdminAwards(Templated):
    """The admin page for editing awards"""
    def __init__(self):
        from r2.models import Award
        Templated.__init__(self)
        self.awards = Award._all_awards()

class AdminAwardGive(Templated):
    """The interface for giving an award"""
    def __init__(self, award, recipient='', desc='', url='', hours=''):
        now = datetime.datetime.now(g.display_tz)
        if desc:
            self.description = desc
        elif award.awardtype == 'regular':
            self.description = "??? -- " + now.strftime("%Y-%m-%d")
        else:
            self.description = ""
        self.url = url
        self.recipient = recipient
        self.hours = hours

        Templated.__init__(self, award = award)

class AdminAwardWinners(Templated):
    """The list of winners of an award"""
    def __init__(self, award):
        trophies = Trophy.by_award(award)
        Templated.__init__(self, award = award, trophies = trophies)


class Ads(Templated):
    def __init__(self):
        Templated.__init__(self)
        self.ad_url = g.ad_domain + "/ads/"
        self.frame_id = "ad-frame"


class Embed(Templated):
    """wrapper for embedding /help into reddit as if it were not on a separate wiki."""
    def __init__(self,content = ''):
        Templated.__init__(self, content = content)


def wrapped_flair(user, subreddit, force_show_flair):
    if (not hasattr(subreddit, '_id')
        or not (force_show_flair or getattr(subreddit, 'flair_enabled', True))):
        return False, 'right', '', ''

    get_flair_attr = lambda a, default=None: getattr(
        user, 'flair_%s_%s' % (subreddit._id, a), default)

    return (get_flair_attr('enabled', default=True),
            getattr(subreddit, 'flair_position', 'right'),
            get_flair_attr('text'), get_flair_attr('css_class'))

class WrappedUser(CachedTemplate):
    FLAIR_CSS_PREFIX = 'flair-'

    def __init__(self, user, attribs = [], context_thing = None, gray = False,
                 subreddit = None, force_show_flair = None,
                 flair_template = None, flair_text_editable = False,
                 include_flair_selector = False):
        if not subreddit:
            subreddit = c.site

        attribs.sort()
        author_cls = 'author'

        author_title = ''
        if gray:
            author_cls += ' gray'
        for tup in attribs:
            author_cls += " " + tup[2]
            # Hack: '(' should be in tup[3] iff this friend has a note
            if tup[1] == 'F' and '(' in tup[3]:
                author_title = tup[3]

        flair = wrapped_flair(user, subreddit or c.site, force_show_flair)
        flair_enabled, flair_position, flair_text, flair_css_class = flair
        has_flair = bool(
            c.user.pref_show_flair and (flair_text or flair_css_class))

        if flair_template:
            flair_template_id = flair_template._id
            flair_text = flair_template.text
            flair_css_class = flair_template.css_class
            has_flair = True
        else:
            flair_template_id = None

        if flair_css_class:
            # This is actually a list of CSS class *suffixes*. E.g., "a b c"
            # should expand to "flair-a flair-b flair-c".
            flair_css_class = ' '.join(self.FLAIR_CSS_PREFIX + c
                                       for c in flair_css_class.split())

        if include_flair_selector:
            if (not getattr(c.site, 'flair_self_assign_enabled', True)
                and not (c.user_is_admin
                         or c.site.is_moderator_with_perms(c.user, 'flair'))):
                include_flair_selector = False

        target = None
        ip_span = None
        context_deleted = None
        if context_thing:
            target = getattr(context_thing, 'target', None)
            ip_span = getattr(context_thing, 'ip_span', None)
            context_deleted = context_thing.deleted

        karma = ''
        if c.user_is_admin:
            karma = ' (%d)' % user.link_karma
            if user._spam:
                author_cls += " banned-user"

        CachedTemplate.__init__(self,
                                name = user.name,
                                force_show_flair = force_show_flair,
                                has_flair = has_flair,
                                flair_enabled = flair_enabled,
                                flair_position = flair_position,
                                flair_text = flair_text,
                                flair_text_editable = flair_text_editable,
                                flair_css_class = flair_css_class,
                                flair_template_id = flair_template_id,
                                include_flair_selector = include_flair_selector,
                                author_cls = author_cls,
                                author_title = author_title,
                                attribs = attribs,
                                context_thing = context_thing,
                                karma = karma,
                                ip_span = ip_span,
                                context_deleted = context_deleted,
                                fullname = user._fullname,
                                user_deleted = user._deleted)

class UserTableItem(Templated):
    type = ''
    remove_action = 'unfriend'
    cells = ('user', 'age', 'sendmessage', 'remove')

    @property
    def executed_message(self):
        return _("added")

    def __init__(self, user, editable=True, **kw):
        self.user = user
        self.editable = editable
        Templated.__init__(self, **kw)

    def __repr__(self):
        return '<UserTableItem "%s">' % self.user.name

class RelTableItem(UserTableItem):
    def __init__(self, rel, **kw):
        self._id = rel._id
        self.rel = rel
        UserTableItem.__init__(self, rel._thing2, **kw)

    @property
    def container_name(self):
        return c.site._fullname

class FriendTableItem(RelTableItem):
    type = 'friend'

    @property
    def cells(self):
        if c.user.gold:
            return ('user', 'sendmessage', 'note', 'age', 'remove')
        return ('user', 'sendmessage', 'remove')

    @property
    def container_name(self):
        return c.user._fullname

class EnemyTableItem(RelTableItem):
    type = 'enemy'
    cells = ('user', 'age', 'remove')

    @property
    def container_name(self):
        return c.user._fullname

class BannedTableItem(RelTableItem):
    type = 'banned'
    cells = ('user', 'age', 'sendmessage', 'remove', 'note')

    @property
    def executed_message(self):
        return _("banned")

class WikiBannedTableItem(BannedTableItem):
    type = 'wikibanned'

class ContributorTableItem(RelTableItem):
    type = 'contributor'

class WikiMayContributeTableItem(RelTableItem):
    type = 'wikicontributor'

class InvitedModTableItem(RelTableItem):
    type = 'moderator_invite'
    cells = ('user', 'age', 'permissions', 'permissionsctl')

    @property
    def executed_message(self):
        return _("invited")

    def is_editable(self, user):
        if not c.user_is_loggedin:
            return False
        elif c.user_is_admin:
            return True
        return c.site.is_unlimited_moderator(c.user)

    def __init__(self, rel, editable=True, **kw):
        if editable:
            self.cells += ('remove',)
        editable = self.is_editable(rel._thing2)
        self.permissions = ModeratorPermissions(rel._thing2, self.type,
                                                rel.get_permissions(),
                                                editable=editable)
        RelTableItem.__init__(self, rel, editable=editable, **kw)

class ModTableItem(InvitedModTableItem):
    type = 'moderator'

    @property
    def executed_message(self):
        return _("added")

    def is_editable(self, user):
        if not c.user_is_loggedin:
            return False
        elif c.user_is_admin:
            return True
        return c.user != user and c.site.can_demod(c.user, user)

class FlairPane(Templated):
    def __init__(self, num, after, reverse, name, user):
        # Make sure c.site isn't stale before rendering.
        c.site = Subreddit._byID(c.site._id)

        tabs = [
            ('grant', _('grant flair'), FlairList(num, after, reverse, name,
                                                  user)),
            ('templates', _('user flair templates'),
             FlairTemplateList(USER_FLAIR)),
            ('link_templates', _('link flair templates'),
             FlairTemplateList(LINK_FLAIR)),
        ]

        Templated.__init__(
            self,
            tabs=TabbedPane(tabs, linkable=True),
            flair_enabled=c.site.flair_enabled,
            flair_position=c.site.flair_position,
            link_flair_position=c.site.link_flair_position,
            flair_self_assign_enabled=c.site.flair_self_assign_enabled,
            link_flair_self_assign_enabled=
                c.site.link_flair_self_assign_enabled)

class FlairList(Templated):
    """List of users who are tagged with flair within a subreddit."""

    def __init__(self, num, after, reverse, name, user):
        Templated.__init__(self, num=num, after=after, reverse=reverse,
                           name=name, user=user)

    @property
    def flair(self):
        if self.user:
            return [FlairListRow(self.user)]

        if self.name:
            # user lookup was requested, but no user was found, so abort
            return []

        # Fetch one item more than the limit, so we can tell if we need to link
        # to a "next" page.
        query = Flair.flair_id_query(c.site, self.num + 1, self.after,
                                     self.reverse)
        flair_rows = list(query)
        if len(flair_rows) > self.num:
            next_page = flair_rows.pop()
        else:
            next_page = None
        uids = [row._thing2_id for row in flair_rows]
        users = Account._byID(uids, data=True)
        result = [FlairListRow(users[row._thing2_id])
                  for row in flair_rows if row._thing2_id in users]
        links = []
        if self.after:
            links.append(
                FlairNextLink(result[0].user._fullname,
                              reverse=not self.reverse,
                              needs_border=bool(next_page)))
        if next_page:
            links.append(
                FlairNextLink(result[-1].user._fullname, reverse=self.reverse))
        if self.reverse:
            result.reverse()
            links.reverse()
            if len(links) == 2 and links[1].needs_border:
                # if page was rendered after clicking "prev", we need to move
                # the border to the other link.
                links[0].needs_border = True
                links[1].needs_border = False
        return result + links

class FlairListRow(Templated):
    def __init__(self, user):
        get_flair_attr = lambda a: getattr(user,
                                           'flair_%s_%s' % (c.site._id, a), '')
        Templated.__init__(self, user=user,
                           flair_text=get_flair_attr('text'),
                           flair_css_class=get_flair_attr('css_class'))

class FlairNextLink(Templated):
    def __init__(self, after, reverse=False, needs_border=False):
        Templated.__init__(self, after=after, reverse=reverse,
                           needs_border=needs_border)

class FlairCsv(Templated):
    class LineResult:
        def __init__(self):
            self.errors = {}
            self.warnings = {}
            self.status = 'skipped'
            self.ok = False

        def error(self, field, desc):
            self.errors[field] = desc

        def warn(self, field, desc):
            self.warnings[field] = desc

    def __init__(self):
        Templated.__init__(self, results_by_line=[])

    def add_line(self):
        self.results_by_line.append(self.LineResult())
        return self.results_by_line[-1]

class FlairTemplateList(Templated):
    def __init__(self, flair_type):
        Templated.__init__(self, flair_type=flair_type)

    @property
    def templates(self):
        ids = FlairTemplateBySubredditIndex.get_template_ids(
                c.site._id, flair_type=self.flair_type)
        fts = FlairTemplate._byID(ids)
        return [FlairTemplateEditor(fts[i], self.flair_type) for i in ids]

class FlairTemplateEditor(Templated):
    def __init__(self, flair_template, flair_type):
        Templated.__init__(self,
                           id=flair_template._id,
                           text=flair_template.text,
                           css_class=flair_template.css_class,
                           text_editable=flair_template.text_editable,
                           sample=FlairTemplateSample(flair_template,
                                                      flair_type),
                           position=getattr(c.site, 'flair_position', 'right'),
                           flair_type=flair_type)

    def render(self, *a, **kw):
        res = Templated.render(self, *a, **kw)
        if not g.template_debug:
            res = spaceCompress(res)
        return res

class FlairTemplateSample(Templated):
    """Like a read-only version of FlairTemplateEditor."""
    def __init__(self, flair_template, flair_type):
        if flair_type == USER_FLAIR:
            wrapped_user = WrappedUser(c.user, subreddit=c.site,
                                       force_show_flair=True,
                                       flair_template=flair_template)
        else:
            wrapped_user = None
        Templated.__init__(self,
                           flair_template=flair_template,
                           wrapped_user=wrapped_user, flair_type=flair_type)

class FlairPrefs(CachedTemplate):
    def __init__(self):
        sr_flair_enabled = getattr(c.site, 'flair_enabled', False)
        user_flair_enabled = getattr(c.user, 'flair_%s_enabled' % c.site._id,
                                     True)
        sr_flair_self_assign_enabled = getattr(
            c.site, 'flair_self_assign_enabled', True)
        wrapped_user = WrappedUser(c.user, subreddit=c.site,
                                   force_show_flair=True,
                                   include_flair_selector=True)
        CachedTemplate.__init__(
            self,
            sr_flair_enabled=sr_flair_enabled,
            sr_flair_self_assign_enabled=sr_flair_self_assign_enabled,
            user_flair_enabled=user_flair_enabled,
            wrapped_user=wrapped_user)

class FlairSelectorLinkSample(CachedTemplate):
    def __init__(self, link, site, flair_template):
        flair_position = getattr(site, 'link_flair_position', 'right')
        admin = bool(c.user_is_admin
                     or site.is_moderator_with_perms(c.user, 'flair'))
        CachedTemplate.__init__(
            self,
            title=link.title,
            flair_position=flair_position,
            flair_template_id=flair_template._id,
            flair_text=flair_template.text,
            flair_css_class=flair_template.css_class,
            flair_text_editable=admin or flair_template.text_editable,
            )

class FlairSelector(CachedTemplate):
    """Provide user with flair options according to subreddit settings."""
    def __init__(self, user=None, link=None, site=None):
        if user is None:
            user = c.user
        if site is None:
            site = c.site
        admin = bool(c.user_is_admin
                     or site.is_moderator_with_perms(c.user, 'flair'))

        if link:
            flair_type = LINK_FLAIR
            target = link
            target_name = link._fullname
            attr_pattern = 'flair_%s'
            position = getattr(site, 'link_flair_position', 'right')
            target_wrapper = (
                lambda flair_template: FlairSelectorLinkSample(
                    link, site, flair_template))
            self_assign_enabled = (
                c.user._id == link.author_id
                and site.link_flair_self_assign_enabled)
        else:
            flair_type = USER_FLAIR
            target = user
            target_name = user.name
            position = getattr(site, 'flair_position', 'right')
            attr_pattern = 'flair_%s_%%s' % c.site._id
            target_wrapper = (
                lambda flair_template: WrappedUser(
                    user, subreddit=site, force_show_flair=True,
                    flair_template=flair_template,
                    flair_text_editable=admin or template.text_editable))
            self_assign_enabled = site.flair_self_assign_enabled

        text = getattr(target, attr_pattern % 'text', '')
        css_class = getattr(target, attr_pattern % 'css_class', '')
        templates, matching_template = self._get_templates(
                site, flair_type, text, css_class)

        if self_assign_enabled or admin:
            choices = [target_wrapper(template) for template in templates]
        else:
            choices = []

        # If one of the templates is already selected, modify its text to match
        # the user's current flair.
        if matching_template:
            for choice in choices:
                if choice.flair_template_id == matching_template:
                    if choice.flair_text_editable:
                        choice.flair_text = text
                    break

        Templated.__init__(self, text=text, css_class=css_class,
                           position=position, choices=choices,
                           matching_template=matching_template,
                           target_name=target_name)

    def render(self, *a, **kw):
        return responsive(CachedTemplate.render(self, *a, **kw), True)

    def _get_templates(self, site, flair_type, text, css_class):
        ids = FlairTemplateBySubredditIndex.get_template_ids(
            site._id, flair_type)
        template_dict = FlairTemplate._byID(ids)
        templates = [template_dict[i] for i in ids]
        for template in templates:
            if template.covers((text, css_class)):
                matching_template = template._id
                break
        else:
             matching_template = None
        return templates, matching_template


class DetailsPage(LinkInfoPage):
    extension_handling= False

    def __init__(self, thing, *args, **kwargs):
        from admin_pages import Details
        after = kwargs.pop('after', None)
        reverse = kwargs.pop('reverse', False)
        count = kwargs.pop('count', None)

        if isinstance(thing, (Link, Comment)):
            details = Details(thing, after=after, reverse=reverse, count=count)

        if isinstance(thing, Link):
            link = thing
            comment = None
            content = details
        elif isinstance(thing, Comment):
            comment = thing
            link = Link._byID(comment.link_id)
            content = PaneStack()
            content.append(PermalinkMessage(link.make_permalink_slow()))
            content.append(LinkCommentSep())
            content.append(CommentPane(link, CommentSortMenu.operator('new'),
                                   comment, None, 1))
            content.append(details)

        kwargs['content'] = content
        LinkInfoPage.__init__(self, link, comment, *args, **kwargs)

class Cnameframe(Templated):
    """The frame page."""
    def __init__(self, original_path, subreddit, sub_domain):
        Templated.__init__(self, original_path=original_path)
        if sub_domain and subreddit and original_path:
            self.title = "%s - %s" % (subreddit.title, sub_domain)
            u = UrlParser(subreddit.path + original_path)
            u.hostname = get_domain(cname = False, subreddit = False)
            u.update_query(**request.GET.copy())
            u.put_in_frame()
            self.frame_target = u.unparse()
        else:
            self.title = ""
            self.frame_target = None

class FrameBuster(Templated):
    pass

class PromotePage(Reddit):
    create_reddit_box  = False
    submit_box         = False
    extension_handling = False
    searchbox          = False

    def __init__(self, nav_menus = None, *a, **kw):
        buttons = [NamedButton('new_promo')]
        if c.user_is_sponsor:
            buttons.append(NamedButton('roadblock'))
            buttons.append(NamedButton('current_promos', dest = ''))
        else:
            buttons.append(NamedButton('my_current_promos', dest = ''))

        if c.user_is_sponsor:
            buttons.append(NavButton('inventory', 'inventory'))
            buttons.append(NavButton('report', 'report'))
            buttons.append(NavButton('underdelivered', 'underdelivered'))
            buttons.append(NavButton('house ads', 'house'))
            buttons.append(NavButton('reported links', 'reported'))

        menu  = NavMenu(buttons, base_path = '/promoted',
                        type='flatlist')

        if nav_menus:
            nav_menus.insert(0, menu)
        else:
            nav_menus = [menu]

        kw['show_sidebar'] = False
        Reddit.__init__(self, nav_menus = nav_menus, *a, **kw)

class PromoteLinkNew(Templated): pass

class PromoteLinkForm(Templated):
    def __init__(self, link, listing, *a, **kw):
        self.setup(link, listing)
        Templated.__init__(self, *a, **kw)

    def setup(self, link, listing):
        self.bids = []
        self.author = Account._byID(link.author_id, data=True)

        if c.user_is_sponsor:
            try:
                bids = Bid.lookup(thing_id=link._id)
            except NotFound:
                pass
            else:
                bids.sort(key=lambda x: x.date, reverse=True)
                bidders = Account._byID(set(bid.account_id for bid in bids),
                                        data=True, return_dict=True)
                for bid in bids:
                    status = Bid.STATUS.name[bid.status].lower()
                    bidder = bidders[bid.account_id]
                    row = Storage(
                        status=status,
                        bidder=bidder.name,
                        date=bid.date,
                        transaction=bid.transaction,
                        campaign=bid.campaign,
                        pay_id=bid.pay_id,
                        amount_str=format_currency(bid.bid, 'USD',
                                                   locale=c.locale),
                        charge_str=format_currency(bid.charge or bid.bid, 'USD',
                                                   locale=c.locale),
                    )
                    self.bids.append(row)

        # determine date range
        now = promote.promo_datetime_now()

        if c.user_is_sponsor:
            min_start = now
        elif promote.is_accepted(link):
            min_start = make_offset_date(now, 1, business_days=True)
        else:
            min_start = make_offset_date(now, g.min_promote_future,
                                         business_days=True)

        if c.user_is_sponsor:
            max_end = now + datetime.timedelta(days=366)
        else:
            max_end = now + datetime.timedelta(days=g.max_promote_future)

        if c.user_is_sponsor:
            max_start = max_end - datetime.timedelta(days=1)
        else:
            max_start = promote.get_max_startdate()

        self.max_start = max_start.strftime("%m/%d/%Y")
        self.max_end = max_end.strftime("%m/%d/%Y")

        self.min_start = self.default_start = min_start.strftime("%m/%d/%Y")
        default_end = min_start + datetime.timedelta(days=2)
        self.default_end = default_end.strftime("%m/%d/%Y")

        self.subreddit_selector = SubredditSelector()

        self.link = link
        self.listing = listing
        campaigns = list(PromoCampaign._by_link(link._id))
        self.campaigns = RenderableCampaign.from_campaigns(link, campaigns)
        self.promotion_log = PromotionLog.get(link)

        self.min_bid = 0 if c.user_is_sponsor else g.min_promote_bid

        self.priorities = [(p.name, p.text, p.description, p.default, p.inventory_override, p.cpm)
                           for p in sorted(PROMOTE_PRIORITIES.values(), key=lambda p: p.value)]

        # geotargeting
        def location_sort(location_tuple):
            code, name, default = location_tuple
            if code == '':
                return -2
            elif code == 'US':
                return -1
            else:
                return name

        countries = [(code, country['name'], False) for code, country
                                                    in g.locations.iteritems()]
        countries.append(('', _('none'), True))

        self.countries = sorted(countries, key=location_sort)
        self.regions = {}
        self.metros = {}
        for code, country in g.locations.iteritems():
            if 'regions' in country and country['regions']:
                self.regions[code] = [('', _('all'), True)]

                for region_code, region in country['regions'].iteritems():
                    if region['metros']:
                        region_tuple = (region_code, region['name'], False)
                        self.regions[code].append(region_tuple)
                        self.metros[region_code] = []

                        for metro_code, metro in region['metros'].iteritems():
                            metro_tuple = (metro_code, metro['name'], False)
                            self.metros[region_code].append(metro_tuple)
                        self.metros[region_code].sort(key=location_sort)
                self.regions[code].sort(key=location_sort)

        # preload some inventory
        srnames = set()
        for title, names in self.subreddit_selector.subreddit_names:
            srnames.update(names)
        srs = Subreddit._by_name(srnames)
        srs[''] = Frontpage
        inv_start = min_start
        inv_end = min_start + datetime.timedelta(days=14)
        sr_inventory = inventory.get_available_pageviews(
            srs.values(), inv_start, inv_end, datestr=True)

        sr_inventory[''] = sr_inventory[Frontpage.name]
        del sr_inventory[Frontpage.name]
        self.inventory = sr_inventory
        message = _("This dashboard allows you to easily place ads on reddit. "
                    "Have any questions? [Check out the FAQ](%(faq)s).\n\n"
                    "Need some ideas on how to showcase your brand? "
                    "[Here's a slideshow](%(link)s) on ways brands used "
                    "reddit ads last year.")
        message %= {
            'link': 'http://www.slideshare.net/reddit/redditthing',
            'faq': 'http://www.reddit.com/wiki/selfserve'
        }
        self.infobar = InfoBar(message=message)

        if campaigns:
            subreddits = set()
            budget = 0.
            impressions = 0

            for campaign in campaigns:
                subreddits.add(campaign.sr_name)
                budget += campaign.bid
                if hasattr(campaign, 'cpm') and campaign.priority.cpm:
                    impressions += campaign.impressions

            num_srs = len(subreddits)
            summary = ungettext("this promotion has a total budget of "
                                "%(budget)s for %(impressions)s impressions in "
                                "%(num)s subreddit",
                                "this promotion has a total budget of "
                                "%(budget)s for %(impressions)s impressions in "
                                "%(num)s subreddits",
                                num_srs)
            self.summary = summary % {
                'budget': format_currency(budget, 'USD', locale=c.locale),
                'impressions': format_number(impressions),
                'num': num_srs,
            }
        else:
            self.summary = None

class RenderableCampaign(Templated):
    def __init__(self, link, campaign, transaction, is_pending, is_live,
                 is_complete):
        self.link = link
        self.campaign = campaign
        self.spent = promote.get_spent_amount(campaign)
        self.paid = bool(transaction and not transaction.is_void())
        self.free = campaign.is_freebie()
        self.is_pending = is_pending
        self.is_live = is_live
        self.is_complete = is_complete
        self.needs_refund = (is_complete and c.user_is_sponsor and
                             not transaction.is_refund() and
                             self.spent < campaign.bid)
        self.pay_url = promote.pay_url(link, campaign)
        self.view_live_url = promote.view_live_url(link, campaign.sr_name)
        self.refund_url = promote.refund_url(link, campaign)

        if campaign.location:
            self.country = campaign.location.country or ''
            self.region = campaign.location.region or ''
            self.metro = campaign.location.metro or ''
        else:
            self.country, self.region, self.metro = '', '', ''
        self.location_str = campaign.location_str

        Templated.__init__(self)

    @classmethod
    def from_campaigns(cls, link, campaigns):
        campaigns, is_single = tup(campaigns, ret_is_single=True)
        transactions = promote.get_transactions(link, campaigns)
        live_campaigns = promote.live_campaigns_by_link(link)
        today = promote.promo_datetime_now().date()

        ret = []
        for camp in campaigns:
            transaction = transactions.get(camp._id)
            is_pending = today < to_date(camp.start_date)
            is_live = camp in live_campaigns
            is_complete = (transaction and (transaction.is_charged() or
                                            transaction.is_refund()) and
                           not (is_live or is_pending))
            rc = cls(link, camp, transaction, is_pending, is_live, is_complete)
            ret.append(rc)
        if is_single:
            return ret[0]
        else:
            return ret

    def render_html(self):
        return spaceCompress(self.render(style='html'))


class RefundPage(Reddit):
    def __init__(self, link, campaign):
        self.link = link
        self.campaign = campaign
        self.listing = wrap_links(link, skip=False)
        billable_impressions = promote.get_billable_impressions(campaign)
        billable_amount = promote.get_billable_amount(campaign,
                                                      billable_impressions)
        refund_amount = promote.get_refund_amount(campaign, billable_amount)
        self.billable_impressions = billable_impressions
        self.billable_amount = billable_amount
        self.refund_amount = refund_amount
        self.traffic_url = '/traffic/%s/%s' % (link._id36, campaign._id36)
        Reddit.__init__(self, title="refund", show_sidebar=False)


class Roadblocks(Templated):
    def __init__(self):
        self.roadblocks = PromotedLinkRoadblock.get_roadblocks()
        Templated.__init__(self)
        # reference "now" to what we use for promtions
        now = promote.promo_datetime_now()

        startdate = now + datetime.timedelta(1)
        enddate   = startdate + datetime.timedelta(1)

        self.startdate = startdate.strftime("%m/%d/%Y")
        self.enddate   = enddate  .strftime("%m/%d/%Y")


class TabbedPane(Templated):
    def __init__(self, tabs, linkable=False):
        """Renders as tabbed area where you can choose which tab to
        render. Tabs is a list of tuples (tab_name, tab_pane)."""
        buttons = []
        for tab_name, title, pane in tabs:
            onclick = "return select_tab_menu(this, '%s')" % tab_name
            buttons.append(JsButton(title, tab_name=tab_name, onclick=onclick))

        self.tabmenu = JsNavMenu(buttons, type = 'tabmenu')
        self.tabs = tabs

        Templated.__init__(self, linkable=linkable)

class LinkChild(object):
    def __init__(self, link, load = False, expand = False, nofollow = False):
        self.link = link
        self.expand = expand
        self.load = load or expand
        self.nofollow = nofollow

    def content(self):
        return ''

def make_link_child(item):
    link_child = None
    editable = False

    # if the item has a media_object, try to make a MediaEmbed for rendering
    if not c.secure:
        media_object = item.media_object
    else:
        media_object = item.secure_media_object

    if media_object:
        media_embed = None
        expand = False

        if isinstance(media_object, basestring):
            media_embed = media_object
        else:
            expand = (media_object.get('type') in g.autoexpand_media_types and
                      getattr(item, 'expand_children', False))

            try:
                media_embed = media.get_media_embed(media_object)
            except TypeError:
                g.log.warning("link %s has a bad media object" % item)
                media_embed = None

            if media_embed:
                should_authenticate = (item.subreddit.type == "private")
                media_embed =  MediaEmbed(media_domain = g.media_domain,
                                          height = media_embed.height + 10,
                                          width = media_embed.width + 10,
                                          scrolling = media_embed.scrolling,
                                          id36 = item._id36,
                                          authenticated=should_authenticate,
                                        )
            else:
                g.log.debug("media_object without media_embed %s" % item)

        if media_embed:
            link_child = MediaChild(item,
                                    media_embed,
                                    load=True,
                                    expand=expand)

    # if the item is_self, add a selftext child
    elif item.is_self:
        if not item.selftext: item.selftext = u''

        expand = getattr(item, 'expand_children', False)

        editable = (expand and
                    item.author == c.user and
                    not item._deleted)
        link_child = SelfTextChild(item, expand = expand,
                                   nofollow = item.nofollow)

    return link_child, editable

class MediaChild(LinkChild):
    """renders when the user hits the expando button to expand media
       objects, like embedded videos"""
    css_style = "video"
    def __init__(self, link, content, **kw):
        self._content = content
        LinkChild.__init__(self, link, **kw)

    def content(self):
        if isinstance(self._content, basestring):
            return self._content
        return self._content.render()

class MediaEmbed(Templated):
    """The actual rendered iframe for a media child"""

    def __init__(self, *args, **kwargs):
        authenticated = kwargs.pop("authenticated", False)
        if authenticated:
            mac = hmac.new(g.secrets["media_embed"], kwargs["id36"],
                           hashlib.sha1)
            self.credentials = "/" + mac.hexdigest()
        else:
            self.credentials = ""
        Templated.__init__(self, *args, **kwargs)


class SelfTextChild(LinkChild):
    css_style = "selftext"

    def content(self):
        u = UserText(self.link, self.link.selftext,
                     editable = c.user == self.link.author,
                     nofollow = self.nofollow,
                     target="_top" if c.cname else None,
                     expunged=self.link.expunged)
        return u.render()

class UserText(CachedTemplate):
    def __init__(self,
                 item,
                 text = '',
                 have_form = True,
                 editable = False,
                 creating = False,
                 nofollow = False,
                 target = None,
                 display = True,
                 post_form = 'editusertext',
                 cloneable = False,
                 extra_css = '',
                 textarea_class = '',
                 name = "text",
                 expunged=False,
                 include_errors=True):

        css_class = "usertext"
        if cloneable:
            css_class += " cloneable"
        if extra_css:
            css_class += " " + extra_css

        if text is None:
            text = ''

        fullname = ''
        # Do not pass fullname on deleted things, unless we're admin
        if hasattr(item, '_fullname'):
            if not getattr(item, 'deleted', False) or c.user_is_admin:
                fullname = item._fullname

        CachedTemplate.__init__(self,
                                fullname = fullname,
                                text = text,
                                have_form = have_form,
                                editable = editable,
                                creating = creating,
                                nofollow = nofollow,
                                target = target,
                                display = display,
                                post_form = post_form,
                                cloneable = cloneable,
                                css_class = css_class,
                                textarea_class = textarea_class,
                                name = name,
                                expunged=expunged,
                                include_errors=include_errors)

class MediaEmbedBody(CachedTemplate):
    """What's rendered inside the iframe that contains media objects"""
    def render(self, *a, **kw):
        res = CachedTemplate.render(self, *a, **kw)
        return responsive(res, True)


class PaymentForm(Templated):
    def __init__(self, link, campaign, **kw):
        self.link = link
        self.duration = strings.time_label
        self.duration %= {'num': campaign.ndays,
                          'time': ungettext("day", "days", campaign.ndays)}
        self.start_date = campaign.start_date.strftime("%m/%d/%Y")
        self.end_date = campaign.end_date.strftime("%m/%d/%Y")
        self.campaign_id36 = campaign._id36
        self.budget = format_currency(float(campaign.bid), 'USD',
                                      locale=c.locale)
        Templated.__init__(self, **kw)


class PromoteInventory(Templated):
    def __init__(self, start, end, sr):
        Templated.__init__(self)
        self.start = start
        self.end = end
        self.sr = sr
        self.sr_name = '' if isinstance(sr, DefaultSR) else sr.name
        self.setup()

    def setup(self):
        campaigns_by_date = inventory.get_campaigns_by_date(self.sr, self.start,
                                                            self.end)
        link_ids = {camp.link_id for camp
                    in chain.from_iterable(campaigns_by_date.itervalues())}
        links_by_id = Link._byID(link_ids, data=True)
        dates = inventory.get_date_range(self.start, self.end)
        imps_by_link_by_date = defaultdict(lambda: dict.fromkeys(dates, 0))
        total_by_date = dict.fromkeys(dates, 0)
        for date, campaigns in campaigns_by_date.iteritems():
            for camp in campaigns:
                link = links_by_id[camp.link_id]
                daily_impressions = camp.impressions / camp.ndays
                imps_by_link_by_date[link._id][date] += daily_impressions
                total_by_date[date] += daily_impressions

        account_ids = {link.author_id for link in links_by_id.itervalues()}
        accounts_by_id = Account._byID(account_ids, data=True)

        self.header = ['link'] + [date.strftime("%m/%d/%Y") for date in dates]
        rows = []
        for link_id, imps_by_date in imps_by_link_by_date.iteritems():
            link = links_by_id[link_id]
            author = accounts_by_id[link.author_id]
            info = {
                'author': author.name,
                'edit_url': promote.promo_edit_url(link),
            }
            row = Storage(info=info, is_total=False)
            row.columns = [format_number(imps_by_date[date]) for date in dates]
            rows.append(row)
        rows.sort(key=lambda row: row.info['author'].lower())

        total_row = Storage(
            info={'title': 'total'},
            is_total=True,
            columns=[format_number(total_by_date[date]) for date in dates],
        )
        rows.append(total_row)

        predicted_by_date = inventory.get_predicted_pageviews(self.sr,
                                            self.start, self.end)
        predicted_row = Storage(
            info={'title': 'predicted'},
            is_total=True,
            columns=[format_number(predicted_by_date[date]) for date in dates],
        )
        rows.append(predicted_row)

        remaining_by_date = {date: predicted_by_date[date] - total_by_date[date]
                             for date in dates}
        remaining_row = Storage(
            info={'title': 'remaining'},
            is_total=True,
            columns=[format_number(remaining_by_date[date]) for date in dates],
        )
        rows.append(remaining_row)

        self.rows = rows

class PromoteReport(Templated):
    def __init__(self, links, link_text, owner_name, bad_links, start, end):
        self.links = links
        self.start = start
        self.end = end
        if links:
            self.make_reports()
            p = request.GET.copy()
            self.csv_url = '%s.csv?%s' % (request.path, urlencode(p))
        else:
            self.link_report = []
            self.campaign_report = []
            self.csv_url = None

        Templated.__init__(self, link_text=link_text, owner_name=owner_name,
                           bad_links=bad_links)

    def as_csv(self):
        out = cStringIO.StringIO()
        writer = csv.writer(out)

        writer.writerow((_("start date"), self.start.strftime('%m/%d/%Y')))
        writer.writerow((_("end date"), self.end.strftime('%m/%d/%Y')))
        writer.writerow([])
        writer.writerow((_("links"),))
        writer.writerow((
            _("id"),
            _("owner"),
            _("url"),
            _("comments"),
            _("upvotes"),
            _("downvotes"),
            _("clicks"),
            _("impressions"),
        ))
        for row in self.link_report:
            writer.writerow((row['id36'], row['owner'], row['url'],
                             row['comments'], row['upvotes'], row['downvotes'],
                             row['clicks'], row['impressions']))

        writer.writerow([])
        writer.writerow((_("campaigns"),))
        writer.writerow((
            _("link id"),
            _("owner"),
            _("campaign id"),
            _("target"),
            _("bid"),
            _("frontpage clicks"), _("frontpage impressions"),
            _("subreddit clicks"), _("subreddit impressions"),
            _("total clicks"), _("total impressions"),
        ))
        for row in self.campaign_report:
            writer.writerow(
                (row['link'], row['owner'], row['campaign'], row['target'],
                 row['bid'], row['fp_clicks'], row['fp_impressions'],
                 row['sr_clicks'], row['sr_impressions'], row['total_clicks'],
                 row['total_impressions'])
            )
        return out.getvalue()

    def make_reports(self):
        self.make_campaign_report()
        self.make_link_report()

    def make_link_report(self):
        link_report = []
        owners = Account._byID([link.author_id for link in self.links],
                               data=True)

        for link in self.links:
            row = {
                'id36': link._id36,
                'owner': owners[link.author_id].name,
                'comments': link.num_comments,
                'upvotes': link._ups,
                'downvotes': link._downs,
                'clicks': self.clicks_by_link.get(link._id36, 0),
                'impressions': self.impressions_by_link.get(link._id36, 0),
                'url': link.url,
            }
            link_report.append(row)
        self.link_report = link_report

    @classmethod
    def _get_hits(cls, traffic_cls, campaigns, start, end):
        campaigns_by_name = {camp._fullname: camp for camp in campaigns}
        codenames = campaigns_by_name.keys()
        start = (start - promote.timezone_offset).replace(tzinfo=None)
        end = (end - promote.timezone_offset).replace(tzinfo=None)

        # start and end are dates so we need to subtract an hour from end to
        # only include 24 hours per day
        end -= datetime.timedelta(hours=1)

        hits = traffic_cls.campaign_history(codenames, start, end)
        sr_hits = defaultdict(int)
        fp_hits = defaultdict(int)
        for date, codename, sr, (uniques, pageviews) in hits:
            campaign = campaigns_by_name[codename]
            campaign_start = campaign.start_date - promote.timezone_offset
            campaign_end = campaign.end_date - promote.timezone_offset
            date = date.replace(tzinfo=g.tz)
            if not (campaign_start <= date < campaign_end):
                continue
            if sr == '':
                fp_hits[codename] += pageviews
            else:
                sr_hits[codename] += pageviews
        return fp_hits, sr_hits

    @classmethod
    def get_imps(cls, campaigns, start, end):
        return cls._get_hits(traffic.TargetedImpressionsByCodename, campaigns,
                             start, end)

    @classmethod
    def get_clicks(cls, campaigns, start, end):
        return cls._get_hits(traffic.TargetedClickthroughsByCodename, campaigns,
                             start, end)

    def make_campaign_report(self):
        campaigns = PromoCampaign._by_link([link._id for link in self.links])

        def keep_camp(camp):
            return not (camp.start_date.date() >= self.end.date() or
                        camp.end_date.date() <= self.start.date() or
                        not promote.charged_or_not_needed(camp))

        campaigns = [camp for camp in campaigns if keep_camp(camp)]
        fp_imps, sr_imps = self.get_imps(campaigns, self.start, self.end)
        fp_clicks, sr_clicks = self.get_clicks(campaigns, self.start, self.end)
        owners = Account._byID([link.author_id for link in self.links],
                               data=True)
        links_by_id = {link._id: link for link in self.links}
        campaign_report = []
        self.clicks_by_link = Counter()
        self.impressions_by_link = Counter()

        for camp in campaigns:
            link = links_by_id[camp.link_id]
            fullname = camp._fullname
            effective_duration = (min(camp.end_date, self.end)
                                  - max(camp.start_date, self.start)).days
            bid = camp.bid * (float(effective_duration) / camp.ndays)
            row = {
                'link': link._id36,
                'owner': owners[link.author_id].name,
                'campaign': camp._id36,
                'target': camp.sr_name or 'frontpage',
                'bid': format_currency(bid, 'USD', locale=c.locale),
                'fp_impressions': fp_imps[fullname],
                'sr_impressions': sr_imps[fullname],
                'fp_clicks': fp_clicks[fullname],
                'sr_clicks': sr_clicks[fullname],
                'total_impressions': fp_imps[fullname] + sr_imps[fullname],
                'total_clicks': fp_clicks[fullname] + sr_clicks[fullname],
            }
            self.clicks_by_link[link._id36] += row['total_clicks']
            self.impressions_by_link[link._id36] += row['total_impressions']
            campaign_report.append(row)
        self.campaign_report = sorted(campaign_report, key=lambda r: r['link'])

class InnerToolbarFrame(Templated):
    def __init__(self, link, url, expanded=False):
        Templated.__init__(self, link=link, url=url, expanded=expanded)

class RawString(Templated):
   def __init__(self, s):
       self.s = s

   def render(self, *a, **kw):
       return unsafe(self.s)


class TryCompact(Reddit):
    def __init__(self, dest, **kw):
        dest = dest or "/"
        u = UrlParser(dest)
        u.set_extension("compact")
        self.compact = u.unparse()

        u.update_query(keep_extension = True)
        self.like = u.unparse()

        u.set_extension("mobile")
        self.mobile = u.unparse()
        Reddit.__init__(self, **kw)

class AccountActivityPage(BoringPage):
    def __init__(self):
        super(AccountActivityPage, self).__init__(_("account activity"))

    def content(self):
        return UserIPHistory()

class UserIPHistory(Templated):
    def __init__(self):
        self.my_apps = OAuth2Client._by_user_grouped(c.user)
        self.ips = ips_by_account_id(c.user._id)
        super(UserIPHistory, self).__init__()

class ApiHelp(Templated):
    def __init__(self, api_docs, *a, **kw):
        self.api_docs = api_docs
        super(ApiHelp, self).__init__(*a, **kw)

class RulesPage(Templated):
    pass

class AwardReceived(Templated):
    pass

class ConfirmAwardClaim(Templated):
    pass

class TimeSeriesChart(Templated):
    def __init__(self, id, title, interval, columns, rows,
                 latest_available_data=None, classes=[],
                 make_period_link=None):
        self.id = id
        self.title = title
        self.interval = interval
        self.columns = columns
        self.rows = rows
        self.latest_available_data = (latest_available_data or
                                      datetime.datetime.utcnow())
        self.classes = " ".join(classes)
        self.make_period_link = make_period_link

        Templated.__init__(self)

class InterestBar(Templated):
    def __init__(self, has_subscribed):
        self.has_subscribed = has_subscribed
        Templated.__init__(self)

class Goldvertisement(Templated):
    def __init__(self):
        now = datetime.datetime.now(GOLD_TIMEZONE)
        today = now.date()
        tomorrow = today + datetime.timedelta(days=1)
        end_time = datetime.datetime(tomorrow.year,
                                     tomorrow.month,
                                     tomorrow.day,
                                     tzinfo=GOLD_TIMEZONE)
        revenue_today = gold_revenue_volatile(today)
        yesterday = today - datetime.timedelta(days=1)
        revenue_yesterday = gold_revenue_steady(yesterday)
        revenue_goal = float(gold_goal_on(today))
        revenue_goal_yesterday = float(gold_goal_on(yesterday))

        self.percent_filled = int((revenue_today / revenue_goal) * 100)
        self.percent_filled_yesterday = int((revenue_yesterday /
                                             revenue_goal_yesterday) * 100)
        self.hours_paid = ServerSecondsBar.current_value_of_month()
        self.time_left_today = timeuntil(end_time, precision=60)
        if c.user.employee:
            self.goal_today = revenue_goal / 100.0
            self.goal_yesterday = revenue_goal_yesterday / 100.0

        if c.user_is_loggedin:
            self.default_type = "autorenew"
        else:
            self.default_type = "code"

        Templated.__init__(self)

class LinkCommentsSettings(Templated):
    def __init__(self, link):
        Templated.__init__(self)
        sr = link.subreddit_slow
        self.link = link
        self.is_author = c.user_is_loggedin and c.user._id == link.author_id
        self.contest_mode = link.contest_mode
        self.stickied = link._fullname == sr.sticky_fullname
        self.sendreplies = link.sendreplies
        self.can_edit = (c.user_is_loggedin
                           and (c.user_is_admin or
                                sr.is_moderator(c.user)))

class ModeratorPermissions(Templated):
    def __init__(self, user, permissions_type, permissions,
                 editable=False, embedded=False):
        self.user = user
        self.permissions = permissions
        Templated.__init__(self, permissions_type=permissions_type,
                           editable=editable, embedded=embedded)

    def items(self):
        return self.permissions.iteritems()

class ListingChooser(Templated):
    def __init__(self):
        Templated.__init__(self)
        self.sections = defaultdict(list)
        self.add_item("global", _("subscribed"), site=Frontpage,
                      description=_("your front page"))
        self.add_item("global", _("explore"), path="/explore")
        self.add_item("other", _("everything"), site=All,
                      description=_("from all subreddits"))
        if c.user_is_loggedin and c.user.is_moderator_somewhere:
            self.add_item("other", _("moderating"), site=Mod,
                          description=_("subreddits you mod"))

        self.add_item("other", _("saved"), path='/user/%s/saved' % c.user.name)

        gold_multi = g.live_config["listing_chooser_gold_multi"]
        if c.user_is_loggedin and c.user.gold and gold_multi:
            self.add_item("other", name=_("gold perks"), path=gold_multi,
                          extra_class="gold-perks")

        self.show_samples = False
        if c.user_is_loggedin:
            multis = LabeledMulti.by_owner(c.user)
            multis.sort(key=lambda multi: multi.name.lower())
            for multi in multis:
                self.add_item("multi", multi.name, site=multi)

            explore_sr = g.live_config["listing_chooser_explore_sr"]
            if explore_sr:
                self.add_item("multi", name=_("explore multis"),
                              site=Subreddit._by_name(explore_sr))

            self.show_samples = not multis

        if self.show_samples:
            self.add_samples()

        self.selected_item = self.find_selected()
        if self.selected_item:
            self.selected_item["selected"] = True

    def add_item(self, section, name, path=None, site=None, description=None,
                 extra_class=None):
        self.sections[section].append({
            "name": name,
            "description": description,
            "path": path or site.user_path,
            "site": site,
            "selected": False,
            "extra_class": extra_class,
        })

    def add_samples(self):
        for path in g.live_config["listing_chooser_sample_multis"]:
            self.add_item(
                section="sample",
                name=path.rpartition('/')[2],
                path=path,
            )

    def find_selected(self):
        path = request.path
        matching = []
        for item in chain(*self.sections.values()):
            if item["site"]:
                if item["site"] == c.site:
                    matching.append(item)
            elif path.startswith(item["path"]):
                matching.append(item)

        matching.sort(key=lambda item: len(item["path"]), reverse=True)
        return matching[0] if matching else None

class PolicyView(Templated):
    pass


class PolicyPage(BoringPage):
    css_class = 'policy-page'

    def __init__(self, pagename=None, content=None, **kw):
        BoringPage.__init__(self, pagename=pagename, show_sidebar=False,
                            content=content, **kw)
        self.welcomebar = None

    def build_toolbars(self):
        toolbars = BoringPage.build_toolbars(self)
        policies_buttons = [
            NavButton(_('privacy policy'), '/privacypolicy'),
            NavButton(_('user agreement'), '/useragreement'),
        ]
        policies_menu = NavMenu(policies_buttons, type='tabmenu',
                                base_path='/help')
        toolbars.append(policies_menu)
        return toolbars


class SubscribeButton(Templated):
    def __init__(self, sr, bubble_class=None):
        Templated.__init__(self)
        self.sr = sr
        self.data_attrs = {"sr_name": sr.name}
        if bubble_class:
            self.data_attrs["bubble_class"] = bubble_class


class SubredditSelector(Templated):
    def __init__(self, default_sr=None, extra_subreddits=None, required=False,
                 include_searches=True):
        Templated.__init__(self)

        if extra_subreddits:
            self.subreddits = extra_subreddits
        else:
            self.subreddits = []

        self.subreddits.append((
            _('popular choices'),
            Subreddit.user_subreddits(c.user, ids=False)
        ))

        self.default_sr = default_sr
        self.required = required
        if include_searches:
            self.sr_searches = simplejson.dumps(
                popular_searches(include_over_18=c.over18)
            )
        else:
            self.sr_searches = simplejson.dumps({})
        self.include_searches = include_searches

    @property
    def subreddit_names(self):
        groups = []
        for title, subreddits in self.subreddits:
            names = [sr.name for sr in subreddits if sr.can_submit(c.user)]
            names.sort(key=str.lower)
            groups.append((title, names))
        return groups


class ListingSuggestions(Templated):
    def __init__(self):
        Templated.__init__(self)

        self.suggestion_type = None
        if c.default_sr:
            if c.user_is_loggedin and random.randint(0, 1) == 1:
                self.suggestion_type = "explore"
                return

            multis = c.user_is_loggedin and LabeledMulti.by_owner(c.user)

            if multis and c.site in multis:
                multis.remove(c.site)

            if multis:
                self.suggestion_type = "multis"
                if len(multis) <= 3:
                    self.suggestions = multis
                else:
                    self.suggestions = random.sample(multis, 3)
            else:
                self.suggestion_type = "random"


class ExploreItem(Templated):
    """For managing recommended content."""

    def __init__(self, item_type, rec_src, sr, link, comment=None):
        """Constructor.

        item_type - string that helps templates know how to render this item.
        rec_src - code that lets us track where the rec originally came from,
            useful for comparing performance of data sources or algorithms
        sr and link are required
        comment is optional
        
        See r2.lib.recommender for valid values of item_type and rec_src.

        """
        self.sr = sr
        self.link = link
        self.comment = comment
        self.type = item_type
        self.src = rec_src
        Templated.__init__(self)

    def is_over18(self):
        return (self.sr.over_18 or
                self.link.over_18 or
                Link._nsfw.findall(self.link.title))


class ExploreItemListing(Templated):
    def __init__(self, recs, settings):
        self.things = []
        self.settings = settings
        if recs:
            links, srs = zip(*[(rec.link, rec.sr) for rec in recs])
            wrapped_links = {l._id: l for l in wrap_links(links).things}
            wrapped_srs = {sr._id: sr for sr in wrap_things(*srs)}
            for rec in recs:
                if rec.link._id in wrapped_links:
                    rec.link = wrapped_links[rec.link._id]
                    rec.sr = wrapped_srs[rec.sr._id]
                    self.things.append(rec)
        Templated.__init__(self)


class TrendingSubredditsBar(Templated):
    def __init__(self, subreddit_names, comment_url, comment_count):
        Templated.__init__(self)
        self.subreddit_names = subreddit_names
        self.comment_url = comment_url
        self.comment_count = comment_count
        self.comment_label, self.comment_label_cls = \
            comment_label(comment_count)


class GeotargetNotice(Templated):
    def __init__(self, city_target=False):
        self.targeting_level = "city" if city_target else "country"
        if city_target:
            text = _("this promoted link uses city level targeting and may "
                     "have been shown to you because of your location. "
                     "([learn more](%(link)s))")
        else:
            text = _("this promoted link uses country level targeting and may "
                     "have been shown to you because of your location. "
                     "([learn more](%(link)s))")
        more_link = "/wiki/targetingbycountrycity"
        self.text = text % {"link": more_link}
        Templated.__init__(self)

########NEW FILE########
__FILENAME__ = things
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.db.thing import NotFound
from r2.lib.menus import Styled
from r2.lib.wrapped import Wrapped
from r2.models import LinkListing, Link, PromotedLink
from r2.models import make_wrapper, IDBuilder, Thing
from r2.lib.utils import tup
from r2.lib.strings import Score
from r2.lib.promote import *
from datetime import datetime
from pylons import c, g
from pylons.i18n import _, ungettext

class PrintableButtons(Styled):
    def __init__(self, style, thing,
                 show_delete = False, show_report = True,
                 show_distinguish = False, show_marknsfw = False,
                 show_unmarknsfw = False, is_link=False,
                 show_flair=False, show_rescrape=False,
                 show_givegold=False, **kw):
        show_ignore = thing.show_reports
        approval_checkmark = getattr(thing, "approval_checkmark", None)
        show_approve = (thing.show_spam or show_ignore or
                        (is_link and approval_checkmark is None)) and not thing._deleted

        Styled.__init__(self, style = style,
                        thing = thing,
                        fullname = thing._fullname,
                        can_ban = thing.can_ban,
                        show_spam = thing.show_spam,
                        show_reports = thing.show_reports,
                        show_ignore = show_ignore,
                        approval_checkmark = approval_checkmark,
                        show_delete = show_delete,
                        show_approve = show_approve,
                        show_report = show_report,
                        show_distinguish = show_distinguish,
                        show_marknsfw = show_marknsfw,
                        show_unmarknsfw = show_unmarknsfw,
                        show_flair = show_flair,
                        show_rescrape=show_rescrape,
                        show_givegold=show_givegold,
                        **kw)
        
class BanButtons(PrintableButtons):
    def __init__(self, thing,
                 show_delete = False, show_report = True):
        PrintableButtons.__init__(self, "banbuttons", thing)

class LinkButtons(PrintableButtons):
    def __init__(self, thing, comments = True, delete = True, report = True):
        # is the current user the author?
        is_author = (c.user_is_loggedin and thing.author and
                     c.user.name == thing.author.name)
        # do we show the report button?
        show_report = not is_author and report

        if c.user_is_admin and thing.promoted is None:
            show_report = False

        show_marknsfw = show_unmarknsfw = False
        show_rescrape = False
        if thing.can_ban or is_author or (thing.promoted and c.user_is_sponsor):
            if not thing.nsfw:
                show_marknsfw = True
            elif thing.nsfw and not thing.nsfw_str:
                show_unmarknsfw = True

            if (not thing.is_self and
                    not (thing.has_thumbnail or thing.media_object)):
                show_rescrape = True
        show_givegold = thing.can_gild and (c.permalink_page or c.profilepage)

        # do we show the delete button?
        show_delete = is_author and delete and not thing._deleted
        # disable the delete button for live sponsored links
        if (is_promoted(thing) and not c.user_is_sponsor):
            show_delete = False

        # do we show the distinguish button? among other things,
        # we never want it to appear on link listings -- only
        # comments pages
        show_distinguish = (is_author and
                            (thing.can_ban or  # Moderator distinguish
                             c.user.employee or  # Admin distinguish
                             c.user_special_distinguish)
                            and getattr(thing, "expand_children", False))

        kw = {}
        if thing.promoted is not None:
            now = datetime.now(g.tz)
            kw = dict(promo_url = promo_edit_url(thing),
                      promote_status = getattr(thing, "promote_status", 0),
                      user_is_sponsor = c.user_is_sponsor,
                      traffic_url = promo_traffic_url(thing), 
                      is_author = thing.is_author)

        PrintableButtons.__init__(self, 'linkbuttons', thing, 
                                  # user existence and preferences
                                  is_loggedin = c.user_is_loggedin,
                                  # comment link params
                                  comment_label = thing.comment_label,
                                  commentcls = thing.commentcls,
                                  permalink  = thing.permalink,
                                  # button visibility
                                  saved = thing.saved,
                                  editable = thing.editable, 
                                  hidden = thing.hidden, 
                                  ignore_reports = thing.ignore_reports,
                                  show_delete = show_delete,
                                  show_report = show_report and c.user_is_loggedin,
                                  show_distinguish = show_distinguish,
                                  show_marknsfw = show_marknsfw,
                                  show_unmarknsfw = show_unmarknsfw,
                                  show_flair = thing.can_flair,
                                  show_rescrape=show_rescrape,
                                  show_givegold=show_givegold,
                                  show_comments = comments,
                                  # promotion
                                  promoted = thing.promoted,
                                  is_link = True,
                                  **kw)

class CommentButtons(PrintableButtons):
    def __init__(self, thing, delete = True, report = True):
        # is the current user the author?
        is_author = thing.is_author
        # do we show the report button?
        show_report = not is_author and report and thing.can_reply
        # do we show the delete button?
        show_delete = is_author and delete and not thing._deleted

        show_distinguish = (is_author and
                            (thing.can_ban or  # Moderator distinguish
                             c.user.employee or  # Admin distinguish
                             c.user_special_distinguish))

        show_givegold = thing.can_gild

        PrintableButtons.__init__(self, "commentbuttons", thing,
                                  can_save=thing.can_save,
                                  is_author = is_author, 
                                  profilepage = c.profilepage,
                                  permalink = thing.permalink,
                                  saved = thing.saved,
                                  ignore_reports = thing.ignore_reports,
                                  full_comment_path = thing.full_comment_path,
                                  full_comment_count = thing.full_comment_count,
                                  deleted = thing.deleted,
                                  parent_permalink = thing.parent_permalink, 
                                  can_reply = thing.can_reply,
                                  show_report = show_report,
                                  show_distinguish = show_distinguish,
                                  show_delete = show_delete,
                                  show_givegold=show_givegold,
        )

class MessageButtons(PrintableButtons):
    def __init__(self, thing, delete = False, report = True):
        was_comment = getattr(thing, 'was_comment', False)
        permalink = thing.permalink
        # don't allow replying to self unless it's modmail
        valid_recipient = (thing.author_id != c.user._id or
                           thing.sr_id)
        can_reply = (c.user_is_loggedin and
                     getattr(thing, "repliable", True) and
                     valid_recipient)

        PrintableButtons.__init__(self, "messagebuttons", thing,
                                  profilepage = c.profilepage,
                                  permalink = permalink,
                                  was_comment = was_comment,
                                  unread = thing.new,
                                  recipient = thing.recipient,
                                  can_reply = can_reply,
                                  parent_id = getattr(thing, "parent_id", None),
                                  show_report = True,
                                  show_delete = False)

# formerly ListingController.builder_wrapper
def default_thing_wrapper(**params):
    def _default_thing_wrapper(thing):
        w = Wrapped(thing)
        style = params.get('style', c.render_style)
        if isinstance(thing, Link):
            if thing.promoted is not None:
                w.render_class = PromotedLink
                w.rowstyle = 'promoted link'
            elif style == 'htmllite':
                w.score_fmt = Score.points
            w.should_incr_counts = style != 'htmllite'
        return w
    params['parent_wrapper'] = _default_thing_wrapper
    return make_wrapper(**params)

# TODO: move this into lib somewhere?
def wrap_links(links, wrapper = default_thing_wrapper(),
               listing_cls = LinkListing, 
               num = None, show_nums = False, nextprev = False, **kw):
    links = tup(links)
    if not all(isinstance(x, basestring) for x in links):
        links = [x._fullname for x in links]
    b = IDBuilder(links, num = num, wrap = wrapper, **kw)
    l = listing_cls(b, nextprev = nextprev, show_nums = show_nums)
    return l.listing()


def hot_links_by_url_listing(url, sr=None, num=None, **kw):
    try:
        links_for_url = Link._by_url(url, sr)
    except NotFound:
        links_for_url = []

    links_for_url.sort(key=lambda link: link._hot, reverse=True)
    listing = wrap_links(links_for_url, num=num, **kw)
    return listing


def wrap_things(*things):
    """Instantiate Wrapped for each thing, calling add_props if available."""
    if not things:
        return []

    wrapped = [Wrapped(thing) for thing in things]
    if hasattr(things[0], 'add_props'):
        # assume all things are of the same type and use the first thing's
        # add_props to process the list.
        things[0].add_props(c.user, wrapped)
    return wrapped

########NEW FILE########
__FILENAME__ = trafficpages
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
"""View models for the traffic statistic pages on reddit."""

import collections
import datetime
import pytz
import urllib

from pylons.i18n import _
from pylons import g, c, request
import babel.core
from babel.dates import format_datetime
from babel.numbers import format_currency

from r2.lib import promote
from r2.lib.db.sorts import epoch_seconds
from r2.lib.menus import menu
from r2.lib.menus import NavButton, NamedButton, PageNameNav, NavMenu
from r2.lib.pages.pages import Reddit, TimeSeriesChart, TabbedPane
from r2.lib.promote import cost_per_mille, cost_per_click
from r2.lib.template_helpers import format_number
from r2.lib.utils import Storage, to_date, timedelta_by_name
from r2.lib.wrapped import Templated
from r2.models import Thing, Link, PromoCampaign, traffic
from r2.models.subreddit import Subreddit, _DefaultSR


COLORS = Storage(UPVOTE_ORANGE="#ff5700",
                 DOWNVOTE_BLUE="#9494ff",
                 MISCELLANEOUS="#006600")


class TrafficPage(Reddit):
    """Base page template for pages rendering traffic graphs."""

    extension_handling = False
    extra_page_classes = ["traffic"]

    def __init__(self, content):
        Reddit.__init__(self, title=_("traffic stats"), content=content)

    def build_toolbars(self):
        main_buttons = [NavButton(menu.sitewide, "/"),
                        NamedButton("languages"),
                        NamedButton("adverts")]

        toolbar = [PageNameNav("nomenu", title=self.title),
                   NavMenu(main_buttons, base_path="/traffic", type="tabmenu")]

        return toolbar


class SitewideTrafficPage(TrafficPage):
    """Base page for sitewide traffic overview."""

    extra_page_classes = TrafficPage.extra_page_classes + ["traffic-sitewide"]

    def __init__(self):
        TrafficPage.__init__(self, SitewideTraffic())


class LanguageTrafficPage(TrafficPage):
    """Base page for interface language traffic summaries or details."""

    def __init__(self, langcode):
        if langcode:
            content = LanguageTraffic(langcode)
        else:
            content = LanguageTrafficSummary()

        TrafficPage.__init__(self, content)


class AdvertTrafficPage(TrafficPage):
    """Base page for advert traffic summaries or details."""

    def __init__(self, code):
        if code:
            content = AdvertTraffic(code)
        else:
            content = AdvertTrafficSummary()
        TrafficPage.__init__(self, content)


class RedditTraffic(Templated):
    """A generalized content pane for traffic reporting."""

    make_period_link = None

    def __init__(self, place):
        self.place = place

        self.traffic_last_modified = traffic.get_traffic_last_modified()
        self.traffic_lag = (datetime.datetime.utcnow() -
                            self.traffic_last_modified)

        self.make_tables()

        Templated.__init__(self)

    def make_tables(self):
        """Create tables to put in the main table area of the page.

        See the stub implementations below for ways to hook into this process
        without completely overriding this method.

        """

        self.tables = []

        for interval in ("month", "day", "hour"):
            columns = [
                dict(color=COLORS.UPVOTE_ORANGE,
                     title=_("uniques by %s" % interval),
                     shortname=_("uniques")),
                dict(color=COLORS.DOWNVOTE_BLUE,
                     title=_("pageviews by %s" % interval),
                     shortname=_("pageviews")),
            ]

            data = self.get_data_for_interval(interval, columns)

            title = _("traffic by %s" % interval)
            graph = TimeSeriesChart("traffic-" + interval,
                                    title,
                                    interval,
                                    columns,
                                    data,
                                    self.traffic_last_modified,
                                    classes=["traffic-table"],
                                    make_period_link=self.make_period_link,
                                   )
            self.tables.append(graph)

        try:
            self.dow_summary = self.get_dow_summary()
        except NotImplementedError:
            self.dow_summary = None
        else:
            uniques_total = collections.Counter()
            pageviews_total = collections.Counter()
            days_total = collections.Counter()

            # don't include the latest (likely incomplete) day
            for date, (uniques, pageviews) in self.dow_summary[1:]:
                dow = date.weekday()
                uniques_total[dow] += uniques
                pageviews_total[dow] += pageviews
                days_total[dow] += 1

            # make a summary of the averages for each day of the week
            self.dow_summary = []
            for dow in xrange(7):
                day_count = days_total[dow]
                if day_count:
                    avg_uniques = uniques_total[dow] / day_count
                    avg_pageviews = pageviews_total[dow] / day_count
                    self.dow_summary.append((dow,
                                             (avg_uniques, avg_pageviews)))
                else:
                    self.dow_summary.append((dow, (0, 0)))

            # calculate the averages for *any* day of the week
            mean_uniques = sum(r[1][0] for r in self.dow_summary) / 7.0
            mean_pageviews = sum(r[1][1] for r in self.dow_summary) / 7.0
            self.dow_means = (round(mean_uniques), round(mean_pageviews))

    def get_dow_summary(self):
        """Return day-interval data to be aggregated by day of week.

        If implemented, a summary table will be shown on the traffic page
        with the average per day of week over the data interval given.

        """
        raise NotImplementedError()

    def get_data_for_interval(self, interval, columns):
        """Return data for the main overview at the interval given.

        This data will be shown as a set of graphs at the top of the page and a
        table for monthly and daily data (hourly is present but hidden by
        default.)

        """
        raise NotImplementedError()


def make_subreddit_traffic_report(subreddits=None):
    """Return a report of subreddit traffic in the last full month.

    If given a list of subreddits, those subreddits will be put in the report
    otherwise the top subreddits by pageviews will be automatically chosen.

    """
    subreddit_summary = traffic.PageviewsBySubreddit.top_last_month(subreddits)
    report = []
    for srname, data in subreddit_summary:
        if srname == _DefaultSR.name:
            name = _("[frontpage]")
            url = None
        elif srname in Subreddit._specials:
            name = "[%s]" % srname
            url = None
        else:
            name = "/r/%s" % srname
            url = name + "/about/traffic"

        report.append(((name, url), data))
    return report


class SitewideTraffic(RedditTraffic):
    """An overview of all traffic to the site."""
    def __init__(self):
        self.subreddit_summary = make_subreddit_traffic_report()
        RedditTraffic.__init__(self, g.domain)

    def get_dow_summary(self):
        return traffic.SitewidePageviews.history("day")

    def get_data_for_interval(self, interval, columns):
        return traffic.SitewidePageviews.history(interval)


class LanguageTrafficSummary(Templated):
    """An overview of traffic by interface language on the site."""

    def __init__(self):
        # convert language codes to real names
        language_summary = traffic.PageviewsByLanguage.top_last_month()
        locale = c.locale
        self.language_summary = []
        for language_code, data in language_summary:
            name = LanguageTraffic.get_language_name(language_code, locale)
            self.language_summary.append(((language_code, name), data))
        Templated.__init__(self)


class AdvertTrafficSummary(RedditTraffic):
    """An overview of traffic for all adverts on the site."""

    def __init__(self):
        RedditTraffic.__init__(self, _("adverts"))

    def make_tables(self):
        # overall promoted link traffic
        impressions = traffic.AdImpressionsByCodename.historical_totals("day")
        clicks = traffic.ClickthroughsByCodename.historical_totals("day")
        data = traffic.zip_timeseries(impressions, clicks)

        columns = [
            dict(color=COLORS.UPVOTE_ORANGE,
                 title=_("total impressions by day"),
                 shortname=_("impressions")),
            dict(color=COLORS.DOWNVOTE_BLUE,
                 title=_("total clicks by day"),
                 shortname=_("clicks")),
        ]

        self.totals = TimeSeriesChart("traffic-ad-totals",
                                      _("ad totals"),
                                      "day",
                                      columns,
                                      data,
                                      self.traffic_last_modified,
                                      classes=["traffic-table"])

        # get summary of top ads
        advert_summary = traffic.AdImpressionsByCodename.top_last_month()
        things = AdvertTrafficSummary.get_things(ad for ad, data
                                                 in advert_summary)
        self.advert_summary = []
        for id, data in advert_summary:
            name = AdvertTrafficSummary.get_ad_name(id, things=things)
            url = AdvertTrafficSummary.get_ad_url(id, things=things)
            self.advert_summary.append(((name, url), data))

    @staticmethod
    def split_codename(codename):
        """Codenames can be "fullname_campaign". Rend the parts asunder."""
        split_code = codename.split("_")
        fullname = "_".join(split_code[:2])
        campaign = "_".join(split_code[2:])
        return fullname, campaign

    @staticmethod
    def get_things(codes):
        """Fetch relevant things for a list of ad codenames in batch."""
        fullnames = [AdvertTrafficSummary.split_codename(code)[0]
                     for code in codes
                     if code.startswith(Thing._type_prefix)]
        return Thing._by_fullname(fullnames, data=True, return_dict=True)

    @staticmethod
    def get_sr_name(name):
        """Return the display name for a subreddit."""
        if name == g.default_sr:
            return _("frontpage")
        else:
            return "/r/" + name

    @staticmethod
    def get_ad_name(code, things=None):
        """Return a human-readable name for an ad given its codename.

        Optionally, a dictionary of things can be passed in so lookups can
        be done in batch upstream.

        """

        if not things:
            things = AdvertTrafficSummary.get_things([code])

        thing = things.get(code)
        campaign = None

        # if it's not at first a thing, see if it's a thing with campaign
        # appended to it.
        if not thing:
            fullname, campaign = AdvertTrafficSummary.split_codename(code)
            thing = things.get(fullname)

        if not thing:
            if code.startswith("dart_"):
                srname = code.split("_", 1)[1]
                srname = AdvertTrafficSummary.get_sr_name(srname)
                return "DART: " + srname
            else:
                return code
        elif isinstance(thing, Link):
            return "Link: " + thing.title
        elif isinstance(thing, Subreddit):
            srname = AdvertTrafficSummary.get_sr_name(thing.name)
            name = "300x100: " + srname
            if campaign:
                name += " (%s)" % campaign
            return name

    @staticmethod
    def get_ad_url(code, things):
        """Given a codename, return the canonical URL for its traffic page."""
        thing = things.get(code)
        if isinstance(thing, Link):
            return "/traffic/%s" % thing._id36
        return "/traffic/adverts/%s" % code


class LanguageTraffic(RedditTraffic):
    def __init__(self, langcode):
        self.langcode = langcode
        name = LanguageTraffic.get_language_name(langcode)
        RedditTraffic.__init__(self, name)

    def get_data_for_interval(self, interval, columns):
        return traffic.PageviewsByLanguage.history(interval, self.langcode)

    @staticmethod
    def get_language_name(language_code, locale=None):
        if not locale:
            locale = c.locale

        try:
            lang_locale = babel.core.Locale.parse(language_code, sep="-")
        except (babel.core.UnknownLocaleError, ValueError):
            return language_code
        else:
            return lang_locale.get_display_name(locale)


class AdvertTraffic(RedditTraffic):
    def __init__(self, code):
        self.code = code
        name = AdvertTrafficSummary.get_ad_name(code)
        RedditTraffic.__init__(self, name)

    def get_data_for_interval(self, interval, columns):
        columns[1]["title"] = _("impressions by %s" % interval)
        columns[1]["shortname"] = _("impressions")

        columns += [
            dict(shortname=_("unique clicks")),
            dict(color=COLORS.MISCELLANEOUS,
                 title=_("clicks by %s" % interval),
                 shortname=_("total clicks")),
        ]

        imps = traffic.AdImpressionsByCodename.history(interval, self.code)
        clicks = traffic.ClickthroughsByCodename.history(interval, self.code)
        return traffic.zip_timeseries(imps, clicks)


class SubredditTraffic(RedditTraffic):
    def __init__(self):
        RedditTraffic.__init__(self, "/r/" + c.site.name)

        if c.user_is_sponsor:
            fullname = c.site._fullname
            codes = traffic.AdImpressionsByCodename.recent_codenames(fullname)
            self.codenames = [(code,
                               AdvertTrafficSummary.split_codename(code)[1])
                               for code in codes]

    @staticmethod
    def make_period_link(interval, date):
        date = date.replace(tzinfo=g.tz)  # won't be necessary after tz fixup
        if interval == "month":
            if date.month != 12:
                end = date.replace(month=date.month + 1)
            else:
                end = date.replace(month=1, year=date.year + 1)
        else:
            end = date + timedelta_by_name(interval)

        query = urllib.urlencode({
            "syntax": "cloudsearch",
            "restrict_sr": "on",
            "sort": "top",
            "q": "timestamp:{:d}..{:d}".format(int(epoch_seconds(date)),
                                               int(epoch_seconds(end))),
        })
        return "/r/%s/search?%s" % (c.site.name, query)

    def get_dow_summary(self):
        return traffic.PageviewsBySubreddit.history("day", c.site.name)

    def get_data_for_interval(self, interval, columns):
        pageviews = traffic.PageviewsBySubreddit.history(interval, c.site.name)

        if interval == "day":
            columns.append(dict(color=COLORS.MISCELLANEOUS,
                                title=_("subscriptions by day"),
                                shortname=_("subscriptions")))

            sr_name = c.site.name
            subscriptions = traffic.SubscriptionsBySubreddit.history(interval,
                                                                     sr_name)

            return traffic.zip_timeseries(pageviews, subscriptions)
        else:
            return pageviews


def _clickthrough_rate(impressions, clicks):
    """Return the click-through rate percentage."""
    if impressions:
        return (float(clicks) / impressions) * 100.
    else:
        return 0


def _is_promo_preliminary(end_date):
    """Return if results are preliminary for this promotion.

    Results are preliminary until 1 day after the promotion ends.

    """

    now = datetime.datetime.now(g.tz)
    return end_date + datetime.timedelta(days=1) > now


def get_promo_traffic(thing, start, end):
    """Get traffic for a Promoted Link or PromoCampaign"""
    if isinstance(thing, Link):
        imp_fn = traffic.AdImpressionsByCodename.promotion_history
        click_fn = traffic.ClickthroughsByCodename.promotion_history
    elif isinstance(thing, PromoCampaign):
        imp_fn = traffic.TargetedImpressionsByCodename.promotion_history
        click_fn = traffic.TargetedClickthroughsByCodename.promotion_history

    imps = imp_fn(thing._fullname, start.replace(tzinfo=None),
                  end.replace(tzinfo=None))
    clicks = click_fn(thing._fullname, start.replace(tzinfo=None),
                      end.replace(tzinfo=None))

    if imps and not clicks:
        clicks = [(imps[0][0], (0,))]
    elif clicks and not imps:
        imps = [(clicks[0][0], (0,))]

    history = traffic.zip_timeseries(imps, clicks, order="ascending")
    return history


def get_billable_traffic(campaign):
    """Get traffic for dates when PromoCampaign is active."""
    start, end = promote.get_traffic_dates(campaign)
    return get_promo_traffic(campaign, start, end)


def is_early_campaign(campaign):
    # traffic by campaign was only recorded starting 2012/9/12
    return campaign.end_date < datetime.datetime(2012, 9, 12, 0, 0, tzinfo=g.tz)


def is_launched_campaign(campaign):
    now = datetime.datetime.now(g.tz).date()
    return (promote.charged_or_not_needed(campaign) and
            campaign.start_date.date() <= now)


class PromotedLinkTraffic(Templated):
    def __init__(self, thing, campaign, before, after):
        self.thing = thing
        self.campaign = campaign
        self.before = before
        self.after = after
        self.period = datetime.timedelta(days=7)
        self.prev = None
        self.next = None
        self.has_live_campaign = False
        self.has_early_campaign = False
        self.detail_name = ('campaign %s' % campaign._id36 if campaign
                                                           else 'all campaigns')

        editable = c.user_is_sponsor or c.user._id == thing.author_id
        self.traffic_last_modified = traffic.get_traffic_last_modified()
        self.traffic_lag = (datetime.datetime.utcnow() -
                            self.traffic_last_modified)
        self.make_hourly_table(campaign or thing)
        self.make_campaign_table()
        Templated.__init__(self)

    @classmethod
    def make_campaign_table_row(cls, id, start, end, target, location, budget,
                                spent, impressions, clicks, is_live, is_active,
                                url, is_total):

        if impressions:
            cpm = format_currency(promote.cost_per_mille(spent, impressions),
                                  'USD', locale=c.locale)
        else:
            cpm = '---'

        if clicks:
            cpc = format_currency(promote.cost_per_click(spent, clicks), 'USD',
                                  locale=c.locale)
            ctr = format_number(_clickthrough_rate(impressions, clicks))
        else:
            cpc = '---'
            ctr = '---'

        return {
            'id': id,
            'start': start,
            'end': end,
            'target': target,
            'location': location,
            'budget': format_currency(budget, 'USD', locale=c.locale),
            'spent': format_currency(spent, 'USD', locale=c.locale),
            'impressions': format_number(impressions),
            'cpm': cpm,
            'clicks': format_number(clicks),
            'cpc': cpc,
            'ctr': ctr,
            'live': is_live,
            'active': is_active,
            'url': url,
            'csv': url + '.csv',
            'total': is_total,
        }

    def make_campaign_table(self):
        campaigns = PromoCampaign._by_link(self.thing._id)

        total_budget = 0
        total_spent = 0
        total_impressions = 0
        total_clicks = 0

        self.campaign_table = []
        for camp in campaigns:
            if not is_launched_campaign(camp):
                continue

            is_live = camp.is_live_now()
            self.has_early_campaign |= is_early_campaign(camp)
            self.has_live_campaign |= is_live

            history = get_billable_traffic(camp)
            impressions, clicks = 0, 0
            for date, (imp, click) in history:
                impressions += imp
                clicks += click

            start = to_date(camp.start_date).strftime('%Y-%m-%d')
            end = to_date(camp.end_date).strftime('%Y-%m-%d')
            target = camp.sr_name or 'frontpage'
            location = camp.location_str
            spent = promote.get_spent_amount(camp)
            is_active = self.campaign and self.campaign._id36 == camp._id36
            url = '/traffic/%s/%s' % (self.thing._id36, camp._id36)
            is_total = False
            row = self.make_campaign_table_row(camp._id36, start, end, target,
                                               location, camp.bid, spent,
                                               impressions, clicks, is_live,
                                               is_active, url, is_total)
            self.campaign_table.append(row)

            total_budget += camp.bid
            total_spent += spent
            total_impressions += impressions
            total_clicks += clicks

        # total row
        start = '---'
        end = '---'
        target = '---'
        location = '---'
        is_live = False
        is_active = not self.campaign
        url = '/traffic/%s' % self.thing._id36
        is_total = True
        row = self.make_campaign_table_row(_('total'), start, end, target,
                                           location, total_budget, total_spent,
                                           total_impressions, total_clicks,
                                           is_live, is_active, url, is_total)
        self.campaign_table.append(row)

    def check_dates(self, thing):
        """Shorten range for display and add next/prev buttons."""
        start, end = promote.get_traffic_dates(thing)

        # Check date of latest traffic (campaigns can end early).
        history = list(get_promo_traffic(thing, start, end))
        if history:
            end = max(date for date, data in history)
            end = end.replace(tzinfo=g.tz)  # get_promo_traffic returns tz naive
                                            # datetimes but is actually g.tz

        if self.period:
            display_start = self.after
            display_end = self.before

            if not display_start and not display_end:
                display_end = end
                display_start = end - self.period
            elif not display_end:
                display_end = display_start + self.period
            elif not display_start:
                display_start = display_end - self.period

            if display_start > start:
                p = request.GET.copy()
                p.update({
                    'after': None,
                    'before': display_start.strftime('%Y%m%d%H'),
                })
                self.prev = '%s?%s' % (request.path, urllib.urlencode(p))
            else:
                display_start = start

            if display_end < end:
                p = request.GET.copy()
                p.update({
                    'after': display_end.strftime('%Y%m%d%H'),
                    'before': None,
                })
                self.next = '%s?%s' % (request.path, urllib.urlencode(p))
            else:
                display_end = end
        else:
            display_start, display_end = start, end

        return display_start, display_end

    @classmethod
    def get_hourly_traffic(cls, thing, start, end):
        """Retrieve hourly traffic for a Promoted Link or PromoCampaign."""
        history = get_promo_traffic(thing, start, end)
        computed_history = []
        for date, data in history:
            imps, clicks = data
            ctr = _clickthrough_rate(imps, clicks)

            date = date.replace(tzinfo=pytz.utc)
            date = date.astimezone(pytz.timezone("EST"))
            datestr = format_datetime(
                date,
                locale=c.locale,
                format="yyyy-MM-dd HH:mm zzz",
            )
            computed_history.append((date, datestr, data + (ctr,)))
        return computed_history

    def make_hourly_table(self, thing):
        start, end = self.check_dates(thing)
        self.history = self.get_hourly_traffic(thing, start, end)

        self.total_impressions, self.total_clicks = 0, 0
        for date, datestr, data in self.history:
            imps, clicks, ctr = data
            self.total_impressions += imps
            self.total_clicks += clicks
        if self.total_impressions > 0:
            self.total_ctr = _clickthrough_rate(self.total_impressions,
                                                self.total_clicks)
        # XXX: _is_promo_preliminary correctly expects tz-aware datetimes
        # because it's also used with datetimes from promo code. this hack
        # relies on the fact that we're storing UTC w/o timezone info.
        # TODO: remove this when traffic is correctly using timezones.
        end_aware = end.replace(tzinfo=g.tz)
        self.is_preliminary = _is_promo_preliminary(end_aware)

    @classmethod
    def as_csv(cls, thing):
        """Return the traffic data in CSV format for reports."""

        import csv
        import cStringIO

        start, end = promote.get_traffic_dates(thing)
        history = cls.get_hourly_traffic(thing, start, end)

        out = cStringIO.StringIO()
        writer = csv.writer(out)

        writer.writerow((_("date and time (UTC)"),
                         _("impressions"),
                         _("clicks"),
                         _("click-through rate (%)")))
        for date, datestr, values in history:
            # flatten (date, datestr, value-tuple) to (date, value1, value2...)
            writer.writerow((date,) + values)

        return out.getvalue()


class SubredditTrafficReport(Templated):
    def __init__(self):
        self.srs, self.invalid_srs, self.report = [], [], []

        self.textarea = request.params.get("subreddits")
        if self.textarea:
            requested_srs = [srname.strip()
                             for srname in self.textarea.splitlines()]
            subreddits = Subreddit._by_name(requested_srs)

            for srname in requested_srs:
                if srname in subreddits:
                    self.srs.append(srname)
                else:
                    self.invalid_srs.append(srname)

            if subreddits:
                self.report = make_subreddit_traffic_report(subreddits.values())

            param = urllib.quote(self.textarea)
            self.csv_url = "/traffic/subreddits/report.csv?subreddits=" + param

        Templated.__init__(self)

    def as_csv(self):
        """Return the traffic data in CSV format for reports."""

        import csv
        import cStringIO

        out = cStringIO.StringIO()
        writer = csv.writer(out)

        writer.writerow((_("subreddit"),
                         _("uniques"),
                         _("pageviews")))
        for (name, url), (uniques, pageviews) in self.report:
            writer.writerow((name, uniques, pageviews))

        return out.getvalue()

########NEW FILE########
__FILENAME__ = wiki
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.pages.pages import Reddit, SubredditStylesheetSource
from pylons import c
from r2.lib.wrapped import Templated
from r2.lib.menus import PageNameNav
from r2.lib.validator.wiki import this_may_revise
from r2.lib.filters import wikimarkdown, safemarkdown
from pylons.i18n import _

class WikiView(Templated):
    def __init__(self, content, edit_by, edit_date, may_revise=False,
                 page=None, diff=None, renderer='wiki'):
        self.page_content_md = content
        if renderer == 'wiki':
            self.page_content = wikimarkdown(content)
        elif renderer == 'reddit':
            self.page_content = safemarkdown(content)
        elif renderer == 'stylesheet':
            self.page_content = SubredditStylesheetSource(content).render()
        self.renderer = renderer
        self.page = page
        self.diff = diff
        self.edit_by = edit_by
        self.may_revise = may_revise
        self.edit_date = edit_date
        self.base_url = c.wiki_base_url
        Templated.__init__(self)

class WikiPageNotFound(Templated):
    def __init__(self, page):
        self.page = page
        self.base_url = c.wiki_base_url
        Templated.__init__(self)

class WikiPageListing(Templated):
    def __init__(self, pages, linear_pages, page=None):
        self.pages = pages
        self.page = page
        self.linear_pages = linear_pages
        self.base_url = c.wiki_base_url
        Templated.__init__(self)

class WikiEditPage(Templated):
    def __init__(self, page_content='', previous='', page=None):
        self.page_content = page_content
        self.page = page
        self.previous = previous
        self.base_url = c.wiki_base_url
        Templated.__init__(self)

class WikiPageSettings(Templated):
    def __init__(self, settings, mayedit, show_editors=True,
                 show_settings=True, page=None, **context):
        self.permlevel = settings['permlevel']
        self.listed = settings['listed']
        self.show_settings = show_settings
        self.show_editors = show_editors
        self.page = page
        self.base_url = c.wiki_base_url
        self.mayedit = mayedit
        Templated.__init__(self)

class WikiPageRevisions(Templated):
    def __init__(self, revisions, page=None):
        self.listing = revisions
        self.page = page
        Templated.__init__(self)

class WikiPageDiscussions(Templated):
    def __init__(self, listing, page=None):
        self.listing = listing
        self.page = page
        Templated.__init__(self)

class WikiBasePage(Reddit):
    extra_page_classes = ['wiki-page']
    
    def __init__(self, content, page=None, may_revise=False,
                 actionless=False, alert=None, description=None, 
                 showtitle=False, **context):
        pageactions = []
        if not actionless and page:
            pageactions += [(page, _("view"), False)]
            if may_revise:
                pageactions += [('edit', _("edit"), True)]
            pageactions += [('revisions/%s' % page, _("history"), False)]
            pageactions += [('discussions', _("talk"), True)]
            if c.is_wiki_mod and may_revise:
                pageactions += [('settings', _("settings"), True)]

        action = context.get('wikiaction', (page, 'wiki'))
        
        if alert:
            context['infotext'] = alert
        elif c.wikidisabled:
            context['infotext'] = _("this wiki is currently disabled, only mods may interact with this wiki")
        
        self.pageactions = pageactions
        self.page = page
        self.base_url = c.wiki_base_url
        self.action = action
        self.description = description
        
        if showtitle:
            self.pagetitle = action[1]
        else:
            self.pagetitle = None

        page_classes = None

        if page and "title" not in context:
            context["title"] = _("%(page)s - %(site)s") % {
                "site": c.site.name,
                "page": page}
            page_classes = ['wiki-page-%s' % page.replace('/', '-')]

        Reddit.__init__(self, extra_js_config={'wiki_page': page}, 
                        show_wiki_actions=True, page_classes=page_classes,
                        content=content, **context)

    def content(self):
        return self._content

class WikiPageView(WikiBasePage):
    def __init__(self, content, page, diff=None, renderer='wiki', **context):
        may_revise = context.get('may_revise')
        if not content and not context.get('alert'):
            if may_revise:
                context['alert'] = _("this page is empty, edit it to add some content.")
        content = WikiView(content, context.get('edit_by'), context.get('edit_date'), 
                           may_revise=may_revise, page=page, diff=diff, renderer=renderer)
        WikiBasePage.__init__(self, content, page=page, **context)

class WikiNotFound(WikiBasePage):
    def __init__(self, page, **context):
        content = WikiPageNotFound(page)
        context['alert'] = _("page %s does not exist in this subreddit") % page
        context['actionless'] = True
        WikiBasePage.__init__(self, content, page=page, **context)

class WikiCreate(WikiBasePage):
    def __init__(self, page, **context):
        context['alert'] = _("page %s does not exist in this subreddit") % page
        context['actionless'] = True
        content = WikiEditPage(page=page)
        WikiBasePage.__init__(self, content, page, **context)

class WikiEdit(WikiBasePage):
    def __init__(self, content, previous, page, **context):
        content = WikiEditPage(content, previous, page)
        context['wikiaction'] = ('edit', _("editing"))
        WikiBasePage.__init__(self, content, page=page, **context)

class WikiSettings(WikiBasePage):
    def __init__(self, settings, mayedit, page, restricted, **context):
        content = WikiPageSettings(settings, mayedit, page=page, **context)
        if restricted:
            context['alert'] = _("This page is restricted, only moderators may edit it.")
        context['wikiaction'] = ('settings', _("settings"))
        WikiBasePage.__init__(self, content, page=page, **context)

class WikiRevisions(WikiBasePage):
    def __init__(self, revisions, page, **context):
        content = WikiPageRevisions(revisions, page)
        context['wikiaction'] = ('revisions/%s' % page, _("revisions"))
        WikiBasePage.__init__(self, content, page=page, **context)

class WikiRecent(WikiBasePage):
    def __init__(self, revisions, **context):
        content = WikiPageRevisions(revisions)
        context['wikiaction'] = ('revisions', _("Viewing recent revisions for /r/%s") % c.wiki_id)
        WikiBasePage.__init__(self, content, showtitle=True, **context)

class WikiListing(WikiBasePage):
    def __init__(self, pages, linear_pages, **context):
        content = WikiPageListing(pages, linear_pages)
        context['wikiaction'] = ('pages', _("Viewing pages for /r/%s") % c.wiki_id)
        description = [_("Below is a list of pages in this wiki visible to you in this subreddit.")]
        WikiBasePage.__init__(self, content, description=description, showtitle=True, **context)

class WikiDiscussions(WikiBasePage):
    def __init__(self, listing, page, **context):
        content = WikiPageDiscussions(listing, page)
        context['wikiaction'] = ('discussions', _("discussions"))
        description = [_("Discussions are site-wide links to this wiki page."),
                       _("Submit a link to this wiki page or see other discussions about this wiki page.")]
        WikiBasePage.__init__(self, content, page=page, description=description, **context)


########NEW FILE########
__FILENAME__ = permissions
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons.i18n import N_


class PermissionSet(dict):
    ALL = 'all'

    info = None

    def __init__(self, *args, **kwargs):
        super(PermissionSet, self).__init__(*args, **kwargs)

    @classmethod
    def loads(cls, encoded, validate=False):
        if not encoded:
            return cls()
        result = cls(((term[1:], term[0] == '+')
                     for term in encoded.split(',')))
        if result.get(cls.ALL) == False:
            del result[cls.ALL]
        if validate and not result.is_valid():
            raise ValueError
        return result

    def dumps(self):
        if self.is_superuser():
            return '+all'
        return ','.join('-+'[bool(v)] + k for k, v in sorted(self.iteritems()))

    def is_superuser(self):
        return bool(super(PermissionSet, self).get(self.ALL))

    def is_valid(self):
        if not self.info:
            return False
        for k in self:
            if k != self.ALL and k not in self.info:
                return False
        return True

    def get(self, key, default=None):
        if self.info and self.is_superuser():
            return True if key in self.info else default
        return super(PermissionSet, self).get(key, default)

    def __getitem__(self, key):
        if self.info and self.is_superuser():
            return key == self.ALL or key in self.info
        return super(PermissionSet, self).get(key, False)


class ModeratorPermissionSet(PermissionSet):
    info = dict(
        access=dict(
            title=N_('access'),
            description=N_('manage the lists of contributors and banned users'),
        ),
        config=dict(
            title=N_('config'),
            description=N_('edit settings, sidebar, css, and images'),
        ),
        flair=dict(
            title=N_('flair'),
            description=N_('manage user flair, link flair, and flair templates'),
        ),
        mail=dict(
            title=N_('mail'),
            description=N_('read and reply to moderator mail'),
        ),
        posts=dict(
            title=N_('posts'),
            description=N_(
                'use the approve, remove, spam, distinguish, and nsfw buttons'),
        ),
        wiki=dict(
            title=N_('wiki'),
            description=N_('manage the wiki and access to the wiki'),
        ),
    )

    @classmethod
    def loads(cls, encoded, **kwargs):
        if encoded is None:
            return cls(all=True)
        return super(ModeratorPermissionSet, cls).loads(encoded, **kwargs)

########NEW FILE########
__FILENAME__ = plugin
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import sys
import os.path
import pkg_resources
from collections import OrderedDict

from pylons import config


class Plugin(object):
    js = {}
    config = {}
    live_config = {}
    needs_static_build = False
    needs_translation = True
    errors = {}

    def __init__(self, entry_point):
        self.entry_point = entry_point

    @property
    def name(self):
        return self.entry_point.name

    @property
    def path(self):
        module = sys.modules[type(self).__module__]
        return os.path.dirname(module.__file__)

    @property
    def template_dir(self):
        """Add module/templates/ as a template directory."""
        return os.path.join(self.path, 'templates')

    @property
    def static_dir(self):
        return os.path.join(self.path, 'public')

    def on_load(self, g):
        pass

    def add_js(self, module_registry=None):
        if not module_registry:
            from r2.lib import js
            module_registry = js.module

        for name, module in self.js.iteritems():
            if name not in module_registry:
                module_registry[name] = module
            else:
                module_registry[name].extend(module)

    def declare_queues(self, queues):
        pass

    def add_routes(self, mc):
        pass

    def load_controllers(self):
        pass



class PluginLoader(object):
    def __init__(self, working_set=None, plugin_names=None):
        self.working_set = working_set or pkg_resources.WorkingSet()

        if plugin_names is None:
            entry_points = self.available_plugins()
        else:
            entry_points = []
            for name in plugin_names:
                try:
                    entry_point = self.available_plugins(name).next()
                except StopIteration:
                    print >> sys.stderr, ("Unable to locate plugin "
                                          "%s. Skipping." % name)
                    continue
                else:
                    entry_points.append(entry_point)

        self.plugins = OrderedDict()
        for entry_point in entry_points:
            try:
                plugin_cls = entry_point.load()
            except Exception as e:
                if plugin_names:
                    # if this plugin was specifically requested, fail.
                    raise e
                else:
                    print >> sys.stderr, ("Error loading plugin %s (%s)."
                                          " Skipping." % (entry_point.name, e))
                    continue
            self.plugins[entry_point.name] = plugin_cls(entry_point)

    def __len__(self):
        return len(self.plugins)

    def __iter__(self):
        return self.plugins.itervalues()

    def __reversed__(self):
        return reversed(self.plugins.values())

    def __getitem__(self, key):
        return self.plugins[key]

    def available_plugins(self, name=None):
        return self.working_set.iter_entry_points('r2.plugin', name)

    def declare_queues(self, queues):
        for plugin in self:
            plugin.declare_queues(queues)

    def load_plugins(self):
        g = config['pylons.g']
        for plugin in self:
            # Record plugin version
            entry = plugin.entry_point
            git_dir = os.path.join(entry.dist.location, '.git')
            g.record_repo_version(entry.name, git_dir)

            # Load plugin
            g.config.add_spec(plugin.config)
            config['pylons.paths']['templates'].insert(0, plugin.template_dir)
            plugin.add_js()
            plugin.on_load(g)

    def load_controllers(self):
        # this module relies on pylons.i18n._ at import time (for translating
        # messages) which isn't available 'til we're in request context.
        from r2.lib import errors

        for plugin in self:
            errors.add_error_codes(plugin.errors)
            plugin.load_controllers()

########NEW FILE########
__FILENAME__ = promote
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from collections import namedtuple
from datetime import datetime, timedelta
from decimal import Decimal, ROUND_DOWN, ROUND_UP
import itertools
import json
import time

from pylons import g, c
from pylons.i18n import ungettext

from r2.lib import (
    authorize,
    emailer,
    hooks,
)
from r2.lib.db.operators import not_
from r2.lib.db.queries import (
    set_promote_status,
    set_underdelivered_campaigns,
    unset_underdelivered_campaigns,
)
from r2.lib.cache import sgm
from r2.lib.memoize import memoize
from r2.lib.strings import strings
from r2.lib.utils import to_date, weighted_lottery
from r2.models import (
    Account,
    Bid,
    DefaultSR,
    FakeAccount,
    FakeSubreddit,
    get_promote_srid,
    Link,
    MultiReddit,
    NO_TRANSACTION,
    PromoCampaign,
    PROMOTE_STATUS,
    PromotedLink,
    PromotionLog,
    PromotionWeights,
    Subreddit,
    traffic,
)
from r2.models.keyvalue import NamedGlobals

PROMO_HEALTH_KEY = 'promotions_last_updated'

def _mark_promos_updated():
    NamedGlobals.set(PROMO_HEALTH_KEY, time.time())


def health_check():
    """Calculate the number of seconds since promotions were last updated"""
    return time.time() - int(NamedGlobals.get(PROMO_HEALTH_KEY, default=0))


def cost_per_mille(spend, impressions):
    """Return the cost-per-mille given ad spend and impressions."""
    if impressions:
        return 1000. * float(spend) / impressions
    else:
        return 0


def cost_per_click(spend, clicks):
    """Return the cost-per-click given ad spend and clicks."""
    if clicks:
        return float(spend) / clicks
    else:
        return 0


def promo_keep_fn(item):
    return is_promoted(item) and not item.hidden


# attrs

def _base_domain():
    if g.domain_prefix:
        return g.domain_prefix + '.' + g.domain
    else:
        return g.domain

def promo_traffic_url(l): # old traffic url
    return "http://%s/traffic/%s/" % (_base_domain(), l._id36)

def promotraffic_url(l): # new traffic url
    return "http://%s/promoted/traffic/headline/%s" % (_base_domain(), l._id36)

def promo_edit_url(l):
    return "http://%s/promoted/edit_promo/%s" % (_base_domain(), l._id36)

def pay_url(l, campaign):
    return "%spromoted/pay/%s/%s" % (g.payment_domain, l._id36, campaign._id36)

def view_live_url(l, srname):
    domain = _base_domain()
    if srname:
        domain += '/r/%s' % srname
    return 'http://%s/?ad=%s' % (domain, l._fullname)


def refund_url(link, campaign):
    return "%spromoted/refund/%s/%s" % (g.payment_domain, link._id36,
                                        campaign._id36)


# booleans

def is_promo(link):
    return (link and not link._deleted and link.promoted is not None
            and hasattr(link, "promote_status"))

def is_accepted(link):
    return (is_promo(link) and
            link.promote_status != PROMOTE_STATUS.rejected and
            link.promote_status >= PROMOTE_STATUS.accepted)

def is_unpaid(link):
    return is_promo(link) and link.promote_status == PROMOTE_STATUS.unpaid

def is_unapproved(link):
    return is_promo(link) and link.promote_status <= PROMOTE_STATUS.unseen

def is_rejected(link):
    return is_promo(link) and link.promote_status == PROMOTE_STATUS.rejected

def is_promoted(link):
    return is_promo(link) and link.promote_status == PROMOTE_STATUS.promoted

def is_finished(link):
    return is_promo(link) and link.promote_status == PROMOTE_STATUS.finished

def is_live_on_sr(link, sr):
    return bool(live_campaigns_by_link(link, sr=sr))


def update_promote_status(link, status):
    set_promote_status(link, status)
    hooks.get_hook('promote.edit_promotion').call(link=link)


def new_promotion(title, url, selftext, user, ip):
    """
    Creates a new promotion with the provided title, etc, and sets it
    status to be 'unpaid'.
    """
    sr = Subreddit._byID(get_promote_srid())
    l = Link._submit(title, url, user, sr, ip)
    l.promoted = True
    l.disable_comments = False
    l.sendreplies = True
    PromotionLog.add(l, 'promotion created')

    if url == 'self':
        l.url = l.make_permalink_slow()
        l.is_self = True
        l.selftext = selftext

    l._commit()

    update_promote_status(l, PROMOTE_STATUS.unpaid)

    # the user has posted a promotion, so enable the promote menu unless
    # they have already opted out
    if user.pref_show_promote is not False:
        user.pref_show_promote = True
        user._commit()

    # notify of new promo
    emailer.new_promo(l)
    return l


def get_transactions(link, campaigns):
    """Return Bids for specified campaigns on the link.

    A PromoCampaign can have several bids associated with it, but the most
    recent one is recorded on the trans_id attribute. This is the one that will
    be returned.

    """

    campaigns = [c for c in campaigns if (c.trans_id != 0
                                          and c.link_id == link._id)]
    if not campaigns:
        return {}

    bids = Bid.lookup(thing_id=link._id)
    bid_dict = {(b.campaign, b.transaction): b for b in bids}
    bids_by_campaign = {c._id: bid_dict[(c._id, c.trans_id)] for c in campaigns}
    return bids_by_campaign

def new_campaign(link, dates, bid, cpm, sr, priority, location):
    # empty string for sr_name means target to all
    sr_name = sr.name if sr else ""
    campaign = PromoCampaign._new(link, sr_name, bid, cpm, dates[0], dates[1],
                                  priority, location)
    PromotionWeights.add(link, campaign._id, sr_name, dates[0], dates[1], bid)
    PromotionLog.add(link, 'campaign %s created' % campaign._id)

    if campaign.priority.cpm:
        author = Account._byID(link.author_id, data=True)
        if getattr(author, "complimentary_promos", False):
            free_campaign(link, campaign, c.user)

    hooks.get_hook('promote.new_campaign').call(link=link, campaign=campaign)
    return campaign


def free_campaign(link, campaign, user):
    auth_campaign(link, campaign, user, -1)

def edit_campaign(link, campaign, dates, bid, cpm, sr, priority, location):
    sr_name = sr.name if sr else '' # empty string means target to all

    changed = {}
    if bid != campaign.bid:
        changed['bid'] = ("$%0.2f" % campaign.bid, "$%0.2f" % bid)
    if dates[0] != campaign.start_date or dates[1] != campaign.end_date:
        original = '%s to %s' % (campaign.start_date, campaign.end_date)
        edited = '%s to %s' % (dates[0], dates[1])
        changed['dates'] = (original, edited)
    if cpm != campaign.cpm:
        changed['cpm'] = (campaign.cpm, cpm)
    if sr_name != campaign.sr_name:
        format_sr_name = (lambda sr_name: '/r/%s' % sr_name if sr_name
                                                            else '<frontpage>')
        changed['sr_name'] = map(format_sr_name, (campaign.sr_name, sr_name))
    if priority != campaign.priority:
        changed['priority'] = (campaign.priority.name, priority.name)

    change_strs = map(lambda t: '%s: %s -> %s' % (t[0], t[1][0], t[1][1]),
                      changed.iteritems())
    change_text = ', '.join(change_strs)

    # if the bid amount changed, cancel any pending transactions
    if campaign.bid != bid:
        void_campaign(link, campaign, reason='changed_bid')

    # update the schedule
    PromotionWeights.reschedule(link, campaign._id, sr_name,
                                dates[0], dates[1], bid)

    # update values in the db
    campaign.update(dates[0], dates[1], bid, cpm, sr_name,
                    campaign.trans_id, priority, location, commit=True)

    if campaign.priority.cpm:
        # make it a freebie, if applicable
        author = Account._byID(link.author_id, True)
        if getattr(author, "complimentary_promos", False):
            free_campaign(link, campaign, c.user)

    # record the changes
    if change_text:
        PromotionLog.add(link, 'edited %s: %s' % (campaign, change_text))

    hooks.get_hook('promote.edit_campaign').call(link=link, campaign=campaign)


def terminate_campaign(link, campaign):
    if not is_live_promo(link, campaign):
        return

    now = promo_datetime_now()
    original_end = campaign.end_date
    dates = [campaign.start_date, now]
    sr = Subreddit._by_name(campaign.sr_name) if campaign.sr_name else None

    # NOTE: this will delete PromotionWeights after and including now.date()
    edit_campaign(link, campaign, dates, campaign.bid, campaign.cpm, sr,
                  campaign.priority, campaign.location)

    campaigns = list(PromoCampaign._by_link(link._id))
    is_live = any(is_live_promo(link, camp) for camp in campaigns
                                            if camp._id != campaign._id)
    if not is_live:
        update_promote_status(link, PROMOTE_STATUS.finished)
        all_live_promo_srnames(_update=True)

    msg = 'terminated campaign %s (original end %s)' % (campaign._id,
                                                        original_end.date())
    PromotionLog.add(link, msg)


def delete_campaign(link, campaign):
    PromotionWeights.delete_unfinished(link, campaign._id)
    void_campaign(link, campaign, reason='deleted_campaign')
    campaign.delete()
    PromotionLog.add(link, 'deleted campaign %s' % campaign._id)
    hooks.get_hook('promote.delete_campaign').call(link=link, campaign=campaign)


def void_campaign(link, campaign, reason):
    transactions = get_transactions(link, [campaign])
    bid_record = transactions.get(campaign._id)
    if bid_record:
        a = Account._byID(link.author_id)
        authorize.void_transaction(a, bid_record.transaction, campaign._id)
        campaign.trans_id = NO_TRANSACTION
        campaign._commit()
        text = ('voided transaction for %s: (trans_id: %d)'
                % (campaign, bid_record.transaction))
        PromotionLog.add(link, text)

        if bid_record.transaction > 0:
            # notify the user that the transaction was voided if it was not
            # a freebie
            emailer.void_payment(link, campaign, reason)


def auth_campaign(link, campaign, user, pay_id):
    """
    Authorizes (but doesn't charge) a bid with authorize.net.
    Args:
    - link: promoted link
    - campaign: campaign to be authorized
    - user: Account obj of the user doing the auth (usually the currently
        logged in user)
    - pay_id: customer payment profile id to use for this transaction. (One
        user can have more than one payment profile if, for instance, they have
        more than one credit card on file.) Set pay_id to -1 for freebies.

    Returns: (True, "") if successful or (False, error_msg) if not. 
    """
    void_campaign(link, campaign, reason='changed_payment')
    trans_id, reason = authorize.auth_transaction(campaign.bid, user, pay_id,
                                                  link, campaign._id)

    if trans_id and not reason:
        text = ('updated payment and/or bid for campaign %s: '
                'SUCCESS (trans_id: %d, amt: %0.2f)' % (campaign._id, trans_id,
                                                        campaign.bid))
        PromotionLog.add(link, text)
        if trans_id < 0:
            PromotionLog.add(link, 'FREEBIE (campaign: %s)' % campaign._id)

        if trans_id:
            if is_finished(link):
                # When a finished promo gets a new paid campaign it doesn't
                # need to go through approval again and is marked accepted
                new_status = PROMOTE_STATUS.accepted
            else:
                new_status = max(PROMOTE_STATUS.unseen, link.promote_status)
        else:
            new_status = max(PROMOTE_STATUS.unpaid, link.promote_status)
        update_promote_status(link, new_status)

        if user and (user._id == link.author_id) and trans_id > 0:
            emailer.promo_bid(link, campaign.bid, campaign.start_date)

    else:
        text = ("updated payment and/or bid for campaign %s: FAILED ('%s')"
                % (campaign._id, reason))
        PromotionLog.add(link, text)
        trans_id = 0

    campaign.trans_id = trans_id
    campaign._commit()

    return bool(trans_id), reason



# dates are referenced to UTC, while we want promos to change at (roughly)
# midnight eastern-US.
# TODO: make this a config parameter
timezone_offset = -5 # hours
timezone_offset = timedelta(0, timezone_offset * 3600)
def promo_datetime_now(offset=None):
    now = datetime.now(g.tz) + timezone_offset
    if offset is not None:
        now += timedelta(offset)
    return now


def get_max_startdate():
    # authorization hold happens now but expires after 30 days. charge
    # happens 1 day before the campaign launches. the latest a campaign
    # can start is 30 days from now (it will get charged in 29 days).
    return promo_datetime_now() + timedelta(days=30)


def accept_promotion(link):
    update_promote_status(link, PROMOTE_STATUS.accepted)

    if link._spam:
        link._spam = False
        link._commit()

    emailer.accept_promo(link)

    # if the link has campaigns running now charge them and promote the link
    now = promo_datetime_now()
    campaigns = list(PromoCampaign._by_link(link._id))
    is_live = False
    for camp in campaigns:
        if is_accepted_promo(now, link, camp):
            charge_campaign(link, camp)
            if charged_or_not_needed(camp):
                promote_link(link, camp)
                is_live = True

    if is_live:
        all_live_promo_srnames(_update=True)


def reject_promotion(link, reason=None):
    was_live = is_promoted(link)
    update_promote_status(link, PROMOTE_STATUS.rejected)
    if reason:
        PromotionLog.add(link, "rejected: %s" % reason)

    # Send a rejection email (unless the advertiser requested the reject)
    if not c.user or c.user._id != link.author_id:
        emailer.reject_promo(link, reason=reason)

    if was_live:
        all_live_promo_srnames(_update=True)


def unapprove_promotion(link):
    if is_unpaid(link):
        return
    elif is_finished(link):
        # when a finished promo is edited it is bumped down to unpaid so if it
        # eventually gets a paid campaign it can get upgraded to unseen and
        # reviewed
        update_promote_status(link, PROMOTE_STATUS.unpaid)
    else:
        update_promote_status(link, PROMOTE_STATUS.unseen)


def authed_or_not_needed(campaign):
    authed = campaign.trans_id != NO_TRANSACTION
    needs_auth = campaign.priority.cpm
    return authed or not needs_auth


def charged_or_not_needed(campaign):
    # True if a campaign has a charged transaction or doesn't need one
    charged = authorize.is_charged_transaction(campaign.trans_id, campaign._id)
    needs_charge = campaign.priority.cpm
    return charged or not needs_charge


def is_accepted_promo(date, link, campaign):
    return (campaign.start_date <= date < campaign.end_date and
            is_accepted(link) and
            authed_or_not_needed(campaign))


def is_scheduled_promo(date, link, campaign):
    return (is_accepted_promo(date, link, campaign) and 
            charged_or_not_needed(campaign))


def is_live_promo(link, campaign):
    now = promo_datetime_now()
    return is_promoted(link) and is_scheduled_promo(now, link, campaign)


def _is_geotargeted_promo(link):
    campaigns = live_campaigns_by_link(link)
    geotargeted = filter(lambda camp: camp.location, campaigns)
    city_target = any(camp.location.metro for camp in geotargeted)
    return bool(geotargeted), city_target


def is_geotargeted_promo(link):
    key = 'geotargeted_promo_%s' % link._id
    from_cache = g.cache.get(key)
    if not from_cache:
        ret = _is_geotargeted_promo(link)
        g.cache.set(key, ret, time=60)
        return ret
    else:
        return from_cache


def get_promos(date, sr_names=None, link=None):
    pws = PromotionWeights.get_campaigns(date, sr_names=sr_names, link=link)
    campaign_ids = {pw.promo_idx for pw in pws}
    campaigns = PromoCampaign._byID(campaign_ids, data=True, return_dict=False)
    link_ids = {camp.link_id for camp in campaigns}
    links = Link._byID(link_ids, data=True)
    for camp in campaigns:
        yield camp, links[camp.link_id]


def get_accepted_promos(offset=0):
    date = promo_datetime_now(offset=offset)
    for camp, link in get_promos(date):
        if is_accepted_promo(date, link, camp):
            yield camp, link


def get_scheduled_promos(offset=0):
    date = promo_datetime_now(offset=offset)
    for camp, link in get_promos(date):
        if is_scheduled_promo(date, link, camp):
            yield camp, link


def charge_campaign(link, campaign):
    if charged_or_not_needed(campaign):
        return

    user = Account._byID(link.author_id)
    success, reason = authorize.charge_transaction(user, campaign.trans_id,
                                                   campaign._id)

    if not success:
        if reason == authorize.TRANSACTION_NOT_FOUND:
            # authorization hold has expired
            original_trans_id = campaign.trans_id
            campaign.trans_id = NO_TRANSACTION
            campaign._commit()
            text = ('voided expired transaction for %s: (trans_id: %d)'
                    % (campaign, original_trans_id))
            PromotionLog.add(link, text)
        return

    hooks.get_hook('promote.edit_campaign').call(link=link, campaign=campaign)

    if not is_promoted(link):
        update_promote_status(link, PROMOTE_STATUS.pending)

    emailer.queue_promo(link, campaign.bid, campaign.trans_id)
    text = ('auth charge for campaign %s, trans_id: %d' %
            (campaign._id, campaign.trans_id))
    PromotionLog.add(link, text)


def charge_pending(offset=1):
    for camp, link in get_accepted_promos(offset=offset):
        charge_campaign(link, camp)


def live_campaigns_by_link(link, sr=None):
    if not is_promoted(link):
        return []

    if sr:
        sr_names = [''] if isinstance(sr, DefaultSR) else [sr.name]
    else:
        sr_names = None

    now = promo_datetime_now()
    return [camp for camp, link in get_promos(now, sr_names=sr_names,
                                              link=link)
            if is_live_promo(link, camp)]


def promote_link(link, campaign):
    if (not link.over_18 and
        not link.over_18_override and
        campaign.sr_name and Subreddit._by_name(campaign.sr_name).over_18):
        link.over_18 = True
        link._commit()

    if not is_promoted(link):
        update_promote_status(link, PROMOTE_STATUS.promoted)
        emailer.live_promo(link)


def make_daily_promotions():
    # charge campaigns so they can go live
    charge_pending(offset=0)
    charge_pending(offset=1)

    # promote links and record ids of promoted links
    link_ids = set()
    for campaign, link in get_scheduled_promos(offset=0):
        link_ids.add(link._id)
        promote_link(link, campaign)

    # expire finished links
    q = Link._query(Link.c.promote_status == PROMOTE_STATUS.promoted, data=True)
    q = q._filter(not_(Link.c._id.in_(link_ids)))
    for link in q:
        update_promote_status(link, PROMOTE_STATUS.finished)
        emailer.finished_promo(link)

    # update subreddits with promos
    all_live_promo_srnames(_update=True)

    _mark_promos_updated()
    finalize_completed_campaigns(daysago=1)
    hooks.get_hook('promote.make_daily_promotions').call(offset=0)


def finalize_completed_campaigns(daysago=1):
    # PromoCampaign.end_date is utc datetime with year, month, day only
    now = datetime.now(g.tz)
    date = now - timedelta(days=daysago)
    date = date.replace(hour=0, minute=0, second=0, microsecond=0)

    q = PromoCampaign._query(PromoCampaign.c.end_date == date,
                             # exclude no transaction and freebies
                             PromoCampaign.c.trans_id > 0,
                             data=True)
    campaigns = list(q)

    if not campaigns:
        return

    # check that traffic is up to date
    earliest_campaign = min(campaigns, key=lambda camp: camp.start_date)
    start, end = get_total_run(earliest_campaign)
    missing_traffic = traffic.get_missing_traffic(start.replace(tzinfo=None),
                                                  date.replace(tzinfo=None))
    if missing_traffic:
        raise ValueError("Can't finalize campaigns finished on %s."
                         "Missing traffic from %s" % (date, missing_traffic))

    links = Link._byID([camp.link_id for camp in campaigns], data=True)
    underdelivered_campaigns = []

    for camp in campaigns:
        if hasattr(camp, 'refund_amount'):
            continue

        link = links[camp.link_id]
        billable_impressions = get_billable_impressions(camp)
        billable_amount = get_billable_amount(camp, billable_impressions)

        if billable_amount >= camp.bid:
            if hasattr(camp, 'cpm'):
                text = '%s completed with $%s billable (%s impressions @ $%s).'
                text %= (camp, billable_amount, billable_impressions, camp.cpm)
            else:
                text = '%s completed with $%s billable (pre-CPM).'
                text %= (camp, billable_amount) 
            PromotionLog.add(link, text)
            camp.refund_amount = 0.
            camp._commit()
        elif charged_or_not_needed(camp):
            underdelivered_campaigns.append(camp)

        if underdelivered_campaigns:
            set_underdelivered_campaigns(underdelivered_campaigns)


def get_refund_amount(camp, billable):
    existing_refund = getattr(camp, 'refund_amount', 0.)
    charge = camp.bid - existing_refund
    refund_amount = charge - billable
    refund_amount = Decimal(str(refund_amount)).quantize(Decimal('.01'),
                                                    rounding=ROUND_UP)
    return max(float(refund_amount), 0.)


def refund_campaign(link, camp, billable_amount, billable_impressions):
    refund_amount = get_refund_amount(camp, billable_amount)
    if refund_amount <= 0:
        return

    owner = Account._byID(camp.owner_id, data=True)
    try:
        success = authorize.refund_transaction(owner, camp.trans_id,
                                               camp._id, refund_amount)
    except authorize.AuthorizeNetException as e:
        text = ('%s $%s refund failed' % (camp, refund_amount))
        PromotionLog.add(link, text)
        g.log.debug(text + ' (response: %s)' % e)
        return

    text = ('%s completed with $%s billable (%s impressions @ $%s).'
            ' %s refunded.' % (camp, billable_amount,
                               billable_impressions, camp.cpm,
                               refund_amount))
    PromotionLog.add(link, text)
    camp.refund_amount = refund_amount
    camp._commit()
    unset_underdelivered_campaigns(camp)
    emailer.refunded_promo(link)


PromoTuple = namedtuple('PromoTuple', ['link', 'weight', 'campaign'])


@memoize('all_live_promo_srnames')
def all_live_promo_srnames():
    now = promo_datetime_now()
    return {camp.sr_name for camp, link in get_promos(now)
            if is_live_promo(link, camp)}


def srnames_from_site(user, site):
    if not isinstance(site, FakeSubreddit):
        srnames = {site.name}
    elif isinstance(site, MultiReddit):
        srnames = {sr.name for sr in site.srs}
    elif user and not isinstance(user, FakeAccount):
        srnames = {sr.name for sr in Subreddit.user_subreddits(user, ids=False)}
        srnames.add('')
    else:
        srnames = {sr.name for sr in Subreddit.user_subreddits(None, ids=False)}
        srnames.add('')
    return srnames


def srnames_with_live_promos(user, site):
    site_srnames = srnames_from_site(user, site)
    promo_srnames = all_live_promo_srnames()
    return promo_srnames.intersection(site_srnames)


def _get_live_promotions(sr_names):
    now = promo_datetime_now()
    ret = {sr_name: [] for sr_name in sr_names}
    for camp, link in get_promos(now, sr_names=sr_names):
        if is_live_promo(link, camp):
            weight = (camp.bid / camp.ndays)
            pt = PromoTuple(link=link._fullname, weight=weight,
                            campaign=camp._fullname)
            ret[camp.sr_name].append(pt)
    return ret


def get_live_promotions(sr_names):
    promos_by_srname = sgm(g.cache, sr_names, miss_fn=_get_live_promotions,
                           prefix='live_promotions', time=60)
    return itertools.chain.from_iterable(promos_by_srname.itervalues())


def lottery_promoted_links(sr_names, n=10):
    """Run weighted_lottery to order and choose a subset of promoted links."""
    promo_tuples = get_live_promotions(sr_names)

    # house priority campaigns have weight of 0, use some small value
    # so they'll show if there are no other campaigns
    weights = {p: p.weight or 0.001 for p in promo_tuples}
    selected = []
    while weights and len(selected) < n:
        s = weighted_lottery(weights)
        del weights[s]
        selected.append(s)
    return selected


def get_total_run(thing):
    """Return the total time span this link or campaign will run.

    Starts at the start date of the earliest campaign and goes to the end date
    of the latest campaign.

    """

    if isinstance(thing, Link):
        campaigns = PromoCampaign._by_link(thing._id)
    elif isinstance(thing, PromoCampaign):
        campaigns = [thing]

    earliest = None
    latest = None
    for campaign in campaigns:
        if not charged_or_not_needed(campaign):
            continue

        if not earliest or campaign.start_date < earliest:
            earliest = campaign.start_date

        if not latest or campaign.end_date > latest:
            latest = campaign.end_date

    # a manually launched promo (e.g., sr discovery) might not have campaigns.
    if not earliest or not latest:
        latest = datetime.utcnow()
        earliest = latest - timedelta(days=30)  # last month

    # ugh this stuff is a mess. they're stored as "UTC" but actually mean UTC-5.
    earliest = earliest.replace(tzinfo=g.tz) - timezone_offset
    latest = latest.replace(tzinfo=g.tz) - timezone_offset

    return earliest, latest


def get_traffic_dates(thing):
    """Retrieve the start and end of a Promoted Link or PromoCampaign."""
    now = datetime.now(g.tz).replace(minute=0, second=0, microsecond=0)
    start, end = get_total_run(thing)
    end = min(now, end)
    return start, end


def get_billable_impressions(campaign):
    start, end = get_traffic_dates(campaign)
    if start > datetime.now(g.tz):
        return 0

    traffic_lookup = traffic.TargetedImpressionsByCodename.promotion_history
    imps = traffic_lookup(campaign._fullname, start.replace(tzinfo=None),
                          end.replace(tzinfo=None))
    billable_impressions = sum(imp for date, (imp,) in imps)
    return billable_impressions


def get_billable_amount(camp, impressions):
    if hasattr(camp, 'cpm'):
        value_delivered = impressions / 1000. * camp.cpm / 100.
        billable_amount = min(camp.bid, value_delivered)
    else:
        # pre-CPM campaigns are charged in full regardless of impressions
        billable_amount = camp.bid

    billable_amount = Decimal(str(billable_amount)).quantize(Decimal('.01'),
                                                        rounding=ROUND_DOWN)
    return float(billable_amount)


def get_spent_amount(campaign):
    if hasattr(campaign, 'refund_amount'):
        # no need to calculate spend if we've already refunded
        spent = campaign.bid - campaign.refund_amount
    elif not hasattr(campaign, 'cpm'):
        # pre-CPM campaign
        return campaign.bid
    else:
        billable_impressions = get_billable_impressions(campaign)
        spent = get_billable_amount(campaign, billable_impressions)
    return spent


def Run(verbose=True):
    """reddit-job-update_promos: Intended to be run hourly to pull in
    scheduled changes to ads

    """

    if verbose:
        print "promote.py:Run() - make_daily_promotions()"

    make_daily_promotions()

    if verbose:
        print "promote.py:Run() - finished"

########NEW FILE########
__FILENAME__ = filesystem
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import os
import urlparse

from pylons import g

from r2.lib.configparse import ConfigValue
from r2.lib.providers.media import MediaProvider


class FileSystemMediaProvider(MediaProvider):
    """A simple media provider that writes to the filesystem.

    It is assumed that an external HTTP server will take care of serving the
    media objects once written.

    `media_fs_root` is the root directory on the filesystem to write the objects
    into.

    `media_fs_base_url_http` and `media_fs_base_url_https` are the base URLs on
    which to find the media objects. They should be an absolute URL to the root
    directory of the media object server.

    """
    config = {
        ConfigValue.str: [
            "media_fs_root",
            "media_fs_base_url_http",
            "media_fs_base_url_https",
        ],
    }

    def put(self, name, contents):
        assert os.path.dirname(name) == ""
        path = os.path.join(g.media_fs_root, name)
        with open(path, "w") as f:
            f.write(contents)
        return urlparse.urljoin(g.media_fs_base_url_http, name)

    def convert_to_https(self, http_url):
        # http://whatever.com/whatever/filename.jpg -> filename.jpg
        name = http_url[http_url.rfind("/") + 1:]
        return urlparse.urljoin(g.media_fs_base_url_https, name)

########NEW FILE########
__FILENAME__ = s3
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import mimetypes
import os

import boto

from pylons import g

from r2.lib.configparse import ConfigValue
from r2.lib.providers.media import MediaProvider


_NEVER = "Thu, 31 Dec 2037 23:59:59 GMT"
_S3_DOMAIN = "s3.amazonaws.com"


class S3MediaProvider(MediaProvider):
    """A media provider using Amazon S3.

    Credentials for uploading objects can be provided via `S3KEY_ID` and
    `S3SECRET_KEY`. If not provided, boto will search for credentials in
    alternate venues including environment variables and EC2 instance roles if
    on Amazon EC2.

    The `s3_media_direct` option configures how URLs are generated. When true,
    URLs will use Amazon's domain name meaning a zero-DNS configuration. If
    false, the bucket name will be assumed to be a valid domain name that is
    appropriately CNAME'd to S3 and URLs will be generated accordingly.

    If more than one bucket is provided in `s3_media_buckets`, items will be
    sharded out to the various buckets based on their filename. This allows for
    hostname parallelization in the non-direct HTTP case.

    """
    config = {
        ConfigValue.str: [
            "S3KEY_ID",
            "S3SECRET_KEY",
        ],
        ConfigValue.bool: [
            "s3_media_direct",
        ],
        ConfigValue.tuple: [
            "s3_media_buckets",
        ],
    }

    def put(self, name, contents):
        # choose a bucket based on the filename
        name_without_extension = os.path.splitext(name)[0]
        index = ord(name_without_extension[-1]) % len(g.s3_media_buckets)
        bucket_name = g.s3_media_buckets[index]

        # guess the mime type
        mime_type, encoding = mimetypes.guess_type(name)

        # send the key
        s3 = boto.connect_s3(g.S3KEY_ID or None, g.S3SECRET_KEY or None)
        bucket = s3.get_bucket(bucket_name, validate=False)
        key = bucket.new_key(name)
        key.set_contents_from_string(
            contents,
            headers={
                "Content-Type": mime_type,
                "Expires": _NEVER,
            },
            policy="public-read",
            reduced_redundancy=True,
            replace=True,
        )

        if g.s3_media_direct:
            return "http://%s/%s/%s" % (_S3_DOMAIN, bucket_name, name)
        else:
            return "http://%s/%s" % (bucket_name, name)

    def convert_to_https(self, http_url):
        """Convert an HTTP URL on S3 to an HTTPS URL.

        This currently assumes that no HTTPS-configured CDN is present, so
        HTTPS URLs must be direct-S3 URLs so that we can use Amazon's certs.

        """
        if http_url.startswith("http://%s" % _S3_DOMAIN):
            # it's already a direct url, just change scheme
            return http_url.replace("http://", "https://")
        else:
            # an indirect url, put the s3 domain in there too
            return http_url.replace("http://", "https://%s/" % _S3_DOMAIN)

########NEW FILE########
__FILENAME__ = recommender
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from itertools import chain, izip_longest
import math
import random
from collections import defaultdict
from datetime import timedelta
from operator import itemgetter

from r2.lib import rising
from r2.lib.db import operators, tdb_cassandra
from r2.lib.pages import ExploreItem
from r2.lib.normalized_hot import normalized_hot
from r2.lib.utils import roundrobin, tup, to36
from r2.lib.sgm import sgm
from r2.models import Account, Link, Subreddit
from r2.models.builder import CommentBuilder
from r2.models.listing import NestedListing
from r2.models.recommend import (
    AccountSRPrefs,
    AccountSRFeedback,
    ExploreSettings,
)

from pylons import g
from pylons.i18n import _

# recommendation sources
SRC_MULTIREDDITS = 'mr'
SRC_EXPLORE = 'e'  # favors lesser known srs

# explore item types
TYPE_RISING = _("rising")
TYPE_DISCOVERY = _("discovery")
TYPE_HOT = _("hot")
TYPE_COMMENT = _("comment")


def get_recommendations(srs,
                        count=10,
                        source=SRC_MULTIREDDITS,
                        to_omit=None,
                        match_set=True,
                        over18=False):
    """Return subreddits recommended if you like the given subreddits.

    Args:
    - srs is one Subreddit object or a list of Subreddits
    - count is total number of results to return
    - source is a prefix telling which set of recommendations to use
    - to_omit is a single or list of subreddit id36s that should not be
        be included. (Useful for omitting recs that were already rejected.)
    - match_set=True will return recs that are similar to each other, useful
        for matching the "theme" of the original set
    - over18 content is filtered unless over18=True or one of the original srs
        is over18

    """
    srs = tup(srs)
    to_omit = tup(to_omit) if to_omit else []

    # fetch more recs than requested because some might get filtered out
    rec_id36s = SRRecommendation.for_srs([sr._id36 for sr in srs],
                                          to_omit,
                                          count * 2,
                                          source,
                                          match_set=match_set)

    # always check for private subreddits at runtime since type might change
    rec_srs = Subreddit._byID36(rec_id36s, return_dict=False)
    filtered = [sr for sr in rec_srs if is_visible(sr)]

    # don't recommend adult srs unless one of the originals was over_18
    if not over18 and not any(sr.over_18 for sr in srs):
        filtered = [sr for sr in filtered if not sr.over_18]

    return filtered[:count]


def get_recommended_content_for_user(account,
                                     settings,
                                     record_views=False,
                                     src=SRC_EXPLORE):
    """Wrapper around get_recommended_content() that fills in user info.

    If record_views == True, the srs will be noted in the user's preferences
    to keep from showing them again too soon.

    settings is an ExploreSettings object that controls what types of content
    will be included.

    Returns a list of ExploreItems.

    """
    prefs = AccountSRPrefs.for_user(account)
    recs = get_recommended_content(prefs, src, settings)
    if record_views:
        # mark as seen so they won't be shown again too soon
        sr_data = {r.sr: r.src for r in recs}
        AccountSRFeedback.record_views(account, sr_data)
    return recs


def get_recommended_content(prefs, src, settings):
    """Get a mix of content from subreddits recommended for someone with
    the given preferences (likes and dislikes.)

    Returns a list of ExploreItems.

    """
    # numbers chosen empirically to give enough results for explore page
    num_liked = 10  # how many liked srs to use when generating the recs
    num_recs = 20  # how many recommended srs to ask for
    num_discovery = 2  # how many discovery-related subreddits to mix in
    num_rising = 4  # how many rising links to mix in
    num_items = 20  # total items to return
    rising_items = discovery_items = comment_items = hot_items = []

    # make a list of srs that shouldn't be recommended
    default_srid36s = [to36(srid) for srid in Subreddit.default_subreddits()]
    omit_srid36s = list(prefs.likes.union(prefs.dislikes,
                                          prefs.recent_views,
                                          default_srid36s))
    # pick random subset of the user's liked srs
    liked_srid36s = random_sample(prefs.likes, num_liked) if settings.personalized else []
    # pick random subset of discovery srs
    candidates = set(get_discovery_srid36s()).difference(prefs.dislikes)
    discovery_srid36s = random_sample(candidates, num_discovery)
    # multiget subreddits
    to_fetch = liked_srid36s + discovery_srid36s
    srs = Subreddit._byID36(to_fetch)
    liked_srs = [srs[sr_id36] for sr_id36 in liked_srid36s]
    discovery_srs = [srs[sr_id36] for sr_id36 in discovery_srid36s]
    if settings.personalized:
        # generate recs from srs we know the user likes
        recommended_srs = get_recommendations(liked_srs,
                                              count=num_recs,
                                              to_omit=omit_srid36s,
                                              source=src,
                                              match_set=False,
                                              over18=settings.nsfw)
        random.shuffle(recommended_srs)
        # split list of recommended srs in half
        midpoint = len(recommended_srs) / 2
        srs_slice1 = recommended_srs[:midpoint]
        srs_slice2 = recommended_srs[midpoint:]
        # get hot links plus top comments from one half
        comment_items = get_comment_items(srs_slice1, src)
        # just get hot links from the other half
        hot_items = get_hot_items(srs_slice2, TYPE_HOT, src)
    if settings.discovery:
        # get links from subreddits dedicated to discovery
        discovery_items = get_hot_items(discovery_srs, TYPE_DISCOVERY, 'disc')
    if settings.rising:
        # grab some (non-personalized) rising items
        omit_sr_ids = set(int(id36, 36) for id36 in omit_srid36s)
        rising_items = get_rising_items(omit_sr_ids, count=num_rising)
    # combine all items and randomize order to get a mix of types
    all_recs = list(chain(rising_items,
                          comment_items,
                          discovery_items,
                          hot_items))
    random.shuffle(all_recs)
    # make sure subreddits aren't repeated
    seen_srs = set()
    recs = []
    for r in all_recs:
        if not settings.nsfw and r.is_over18():
            continue
        if not is_visible(r.sr):  # could happen in rising items
            continue
        if r.sr._id not in seen_srs:
            recs.append(r)
            seen_srs.add(r.sr._id)
        if len(recs) >= num_items:
            break
    return recs


def get_hot_items(srs, item_type, src):
    """Get hot links from specified srs."""
    hot_srs = {sr._id: sr for sr in srs}  # for looking up sr by id
    hot_link_fullnames = normalized_hot(sr._id for sr in srs)
    hot_links = Link._by_fullname(hot_link_fullnames, return_dict=False)
    hot_items = []
    for l in hot_links:
        hot_items.append(ExploreItem(item_type, src, hot_srs[l.sr_id], l))
    return hot_items


def get_rising_items(omit_sr_ids, count=4):
    """Get links that are rising right now."""
    all_rising = rising.get_all_rising()
    candidate_sr_ids = {sr_id for link, score, sr_id in all_rising}.difference(omit_sr_ids)
    link_fullnames = [link for link, score, sr_id in all_rising if sr_id in candidate_sr_ids]
    link_fullnames_to_show = random_sample(link_fullnames, count)
    rising_links = Link._by_fullname(link_fullnames_to_show,
                                     return_dict=False,
                                     data=True)
    rising_items = [ExploreItem(TYPE_RISING, 'ris', Subreddit._byID(l.sr_id), l)
                   for l in rising_links]
    return rising_items


def get_comment_items(srs, src, count=4):
    """Get hot links from srs, plus top comment from each link."""
    link_fullnames = normalized_hot([sr._id for sr in srs])
    hot_links = Link._by_fullname(link_fullnames[:count], return_dict=False)
    top_comments = []
    for link in hot_links:
        builder = CommentBuilder(link,
                                 operators.desc('_confidence'),
                                 comment=None,
                                 context=None,
                                 num=1,
                                 load_more=False)
        listing = NestedListing(builder, parent_name=link._fullname).listing()
        top_comments.extend(listing.things)
    srs = Subreddit._byID([com.sr_id for com in top_comments])
    links = Link._byID([com.link_id for com in top_comments])
    comment_items = [ExploreItem(TYPE_COMMENT,
                                 src,
                                 srs[com.sr_id],
                                 links[com.link_id],
                                 com) for com in top_comments]
    return comment_items


def get_discovery_srid36s():
    """Get list of srs that help people discover other srs."""
    srs = Subreddit._by_name(g.live_config['discovery_srs'])
    return [sr._id36 for sr in srs.itervalues()]


def random_sample(items, count):
    """Safe random sample that won't choke if len(items) < count."""
    sample_size = min(count, len(items))
    return random.sample(items, sample_size)


def is_visible(sr):
    """True if sr is visible to regular users, false if private or banned."""
    return sr.type != 'private' and not sr._spam


class SRRecommendation(tdb_cassandra.View):
    _use_db = True

    _compare_with = tdb_cassandra.LongType()

    # don't keep these around if a run hasn't happened lately, or if the last
    # N runs didn't generate recommendations for a given subreddit
    _ttl = timedelta(days=7, hours=12)

    # we know that we mess with these but it's okay
    _warn_on_partial_ttl = False

    @classmethod
    def for_srs(cls, srid36, to_omit, count, source, match_set=True):
        # It's usually better to use get_recommendations() than to call this
        # function directly because it does privacy filtering.

        srid36s = tup(srid36)
        to_omit = set(to_omit)
        to_omit.update(srid36s)  # don't show the originals
        rowkeys = ['%s.%s' % (source, srid36) for srid36 in srid36s]

        # fetch multiple sets of recommendations, one for each input srid36
        d = sgm(g.cache, rowkeys, SRRecommendation._byID, prefix='srr.')
        rows = d.values()

        if match_set:
            sorted_recs = SRRecommendation._merge_and_sort_by_count(rows)
            # heuristic: if input set is large, rec should match more than one
            min_count = math.floor(.1 * len(srid36s))
            sorted_recs = (rec[0] for rec in sorted_recs if rec[1] > min_count)
        else:
            sorted_recs = SRRecommendation._merge_roundrobin(rows)
        # remove duplicates and ids listed in to_omit
        filtered = []
        for r in sorted_recs:
            if r not in to_omit:
                filtered.append(r)
                to_omit.add(r)
        return filtered[:count]

    @classmethod
    def _merge_roundrobin(cls, rows):
        """Combine multiple sets of recs, preserving order.

        Picks items equally from each input sr, which can be useful for
        getting a diverse set of recommendations instead of one that matches
        a theme. Preserves ordering, so all rank 1 recs will be listed first,
        then all rank 2, etc.

        Returns a list of id36s.

        """
        return roundrobin(*[row._values().itervalues() for row in rows])

    @classmethod
    def _merge_and_sort_by_count(cls, rows):
        """Combine and sort multiple sets of recs.

        Combines multiple sets of recs and sorts by number of times each rec
        appears, the reasoning being that an item recommended for several of
        the original srs is more likely to match the "theme" of the set.

        """
        # combine recs from all input srs
        rank_id36_pairs = chain.from_iterable(row._values().iteritems()
                                              for row in rows)
        ranks = defaultdict(list)
        for rank, id36 in rank_id36_pairs:
            ranks[id36].append(rank)
        recs = [(id36, len(ranks), max(ranks))
                for id36, ranks in ranks.iteritems()]
        # first, sort ascending by rank
        recs = sorted(recs, key=itemgetter(2))
        # next, sort descending by number of times the rec appeared. since
        # python sort is stable, tied items will still be ordered by rank
        return sorted(recs, key=itemgetter(1), reverse=True)

########NEW FILE########
__FILENAME__ = require
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

class RequirementException(Exception):
    pass

def require(val):
    """A safe version of assert

    Assert can be stripped out if python is run in an optimized
    mode. This function implements assertions in a way that is
    guaranteed to execute.
    """
    if not val:
        raise RequirementException
    return val

def require_split(s, length, sep=None):
    require(s)
    res = s.split(sep)
    require(len(res) == length)
    return res

########NEW FILE########
__FILENAME__ = rising
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from datetime import datetime
import heapq

from pylons import g

from r2.lib import count
from r2.lib.cache import sgm
from r2.models.link import Link


CACHE_KEY = "rising"


def calc_rising():
    link_counts = count.get_link_counts()

    links = Link._by_fullname(link_counts.keys(), data=True)

    def score(link):
        count = link_counts[link._fullname][0]
        return float(link._ups) / max(count, 1)

    # build the rising list, excluding items having 1 or less upvotes
    rising = []
    for link in links.values():
        if link._ups > 1:
            rising.append((link._fullname, score(link), link.sr_id))

    # return rising sorted by score
    return sorted(rising, key=lambda x: x[1], reverse=True)


def set_rising():
    g.cache.set(CACHE_KEY, calc_rising())


def get_all_rising():
    return g.cache.get(CACHE_KEY, [])


def get_rising(sr):
    rising = get_all_rising()
    return [link for link, score, sr_id in rising if sr.keep_for_rising(sr_id)]


def get_rising_tuples(sr_ids):
    rising = get_all_rising()

    tuples_by_srid = {sr_id: [] for sr_id in sr_ids}
    top_rising = {}

    for link, score, sr_id in rising:
        if sr_id not in sr_ids:
            continue

        if sr_id not in top_rising:
            top_rising[sr_id] = score

        norm_score = score / top_rising[sr_id]
        tuples_by_srid[sr_id].append((-norm_score, -score, link))

    return tuples_by_srid


def normalized_rising(sr_ids):
    if not sr_ids:
        return []

    tuples_by_srid = sgm(g.cache, sr_ids, miss_fn=get_rising_tuples,
                         prefix='normalized_rising', time=g.page_cache_time)

    merged = heapq.merge(*tuples_by_srid.values())

    return [link_name for norm_score, score, link_name in merged]

########NEW FILE########
__FILENAME__ = s3_helpers
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import os
import sys

from boto.s3.key import Key

HADOOP_FOLDER_SUFFIX = '_$folder$'


def _to_path(bucket, key):
    if not bucket:
        raise ValueError
    return 's3://%s/%s' % (bucket, key)


def _from_path(path):
    """Return bucket and key names from an s3 path.

    Path of 's3://BUCKET/KEY/NAME' would return 'BUCKET', 'KEY/NAME'.

    """

    if not path.startswith('s3://'):
        raise ValueError('Bad S3 path %s' % path)

    r = path[len('s3://'):].split('/', 1)
    bucket = key = None

    if len(r) == 2:
        bucket, key = r[0], r[1]
    else:
        bucket = r[0]

    if not bucket:
        raise ValueError('Bad S3 path %s' % path)

    return bucket, key


def get_text_from_s3(s3_connection, path):
    """Read a file from S3 and return it as text."""
    bucket_name, key_name = _from_path(path)
    bucket = s3_connection.get_bucket(bucket_name)
    k = Key(bucket)
    k.key = key_name
    txt = k.get_contents_as_string()
    return txt


def mv_file_s3(s3_connection, src_path, dst_path):
    """Move a file within S3."""
    src_bucket_name, src_key_name = _from_path(src_path)
    dst_bucket_name, dst_key_name = _from_path(dst_path)

    src_bucket = s3_connection.get_bucket(src_bucket_name)
    k = Key(src_bucket)
    k.key = src_key_name
    k.copy(dst_bucket_name, dst_key_name)
    k.delete()


def s3_key_exists(s3_connection, path):
    bucket_name, key_name = _from_path(path)
    bucket = s3_connection.get_bucket(bucket_name)
    key = bucket.get_key(key_name)
    return bool(key)


def copy_to_s3(s3_connection, local_path, dst_path, verbose=False):
    def callback(trans, total):
        sys.stdout.write('%s/%s' % trans, total)
        sys.stdout.flush()

    dst_bucket_name, dst_key_name = _from_path(dst_path)
    bucket = s3_connection.get_bucket(dst_bucket_name)

    filename = os.path.basename(local_path)
    if not filename:
        return

    key_name = os.path.join(dst_key_name, filename)
    k = Key(bucket)
    k.key = key_name

    kw = {}
    if verbose:
        print 'Uploading %s to %s' % (local_path, dst_path)
        kw['cb'] = callback

    k.set_contents_from_filename(logfile, **kw)

########NEW FILE########
__FILENAME__ = search
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import r2.lib.cloudsearch as cloudsearch


InvalidQuery = (cloudsearch.InvalidQuery,)
SearchException = (cloudsearch.CloudSearchHTTPError,)

SearchQuery = cloudsearch.LinkSearchQuery
SubredditSearchQuery = cloudsearch.SubredditSearchQuery

sorts = cloudsearch.LinkSearchQuery.sorts_menu_mapping

########NEW FILE########
__FILENAME__ = sr_pops
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.models import Subreddit, SubredditPopularityByLanguage
from r2.lib.db.operators import desc
from r2.lib import count
from r2.lib.utils import fetch_things2, flatten
from r2.lib.memoize import memoize

# the length of the stored per-language list
limit = 2500

def set_downs():
    sr_counts = count.get_sr_counts()
    names = [k for k, v in sr_counts.iteritems() if v != 0]
    srs = Subreddit._by_fullname(names)
    for name in names:
        sr,c = srs[name], sr_counts[name]
        if c != sr._downs and c > 0:
            sr._downs = max(c, 0)
            sr._commit()

def cache_lists():
    def _chop(srs):
        srs.sort(key=lambda s: s._downs, reverse=True)
        return srs[:limit]

    # bylang    =:= dict((lang, over18_state) -> [Subreddit])
    # lang      =:= all | lang()
    # nsfwstate =:= no_over18 | allow_over18 | only_over18
    bylang = {}

    for sr in fetch_things2(Subreddit._query(sort=desc('_date'),
                                             data=True)):
        aid = getattr(sr, 'author_id', None)
        if aid is not None and aid < 0:
            # skip special system reddits like promos
            continue

        type = getattr(sr, 'type', 'private')
        if type not in ('public', 'restricted', 'gold_restricted'):
            # skips reddits that can't appear in the default list
            # because of permissions
            continue

        for lang in 'all', sr.lang:
            over18s = ['allow_over18']
            if sr.over_18:
                over18s.append('only_over18')
            else:
                over18s.append('no_over18')

            for over18 in over18s:
                k = (lang, over18)
                bylang.setdefault(k, []).append(sr)

                # keep the lists small while we work
                if len(bylang[k]) > limit*2:
                    bylang[k] = _chop(bylang[k])

    for (lang, over18), srs in bylang.iteritems():
        srs = _chop(srs)
        sr_tuples = map(lambda sr: (sr._downs, sr.allow_top, sr._id), srs)

        print "For %s/%s setting %s" % (lang, over18,
                                        map(lambda sr: sr.name, srs[:50]))

        SubredditPopularityByLanguage._set_values(lang, {over18: sr_tuples})

def run():
    set_downs()
    cache_lists()

# this relies on c.content_langs being sorted to increase cache hit rate
@memoize('sr_pops.pop_reddits', time=3600, stale=True)
def pop_reddits(langs, over18, over18_only, filter_allow_top = False):
    if not over18:
        over18_state = 'no_over18'
    elif over18_only:
        over18_state = 'only_over18'
    else:
        over18_state = 'allow_over18'

    # we only care about base languages, not subtags here. so en-US -> en
    unique_langs = []
    seen_langs = set()
    for lang in langs:
        if '-' in lang:
            lang = lang.split('-', 1)[0]
        if lang not in seen_langs:
            unique_langs.append(lang)
            seen_langs.add(lang)

    # dict(lang_key -> [(_downs, allow_top, sr_id)])
    bylang = SubredditPopularityByLanguage._byID(unique_langs,
                                                 properties=[over18_state])
    tups = flatten([lang_lists[over18_state] for lang_lists
                                             in bylang.values()])

    if filter_allow_top:
        # remove the folks that have opted out of being on the front
        # page as appropriate
        tups = filter(lambda tpl: tpl[1], tups)

    if len(tups) > 1:
        # if there was only one returned, it's already sorted
        tups.sort(key = lambda tpl: tpl[0], reverse=True)

    return map(lambda tpl: tpl[2], tups)

########NEW FILE########
__FILENAME__ = static
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import sys
import os
import hashlib
import json
import base64
import shutil


def locate_static_file(name):
    from pylons import g
    static_dirs = [plugin.static_dir for plugin in g.plugins]
    static_dirs.insert(0, g.paths['static_files'])

    for static_dir in reversed(static_dirs):
        file_path = os.path.join(static_dir, name.lstrip('/'))
        if os.path.exists(file_path):
            return file_path


def static_mtime(name):
    path = locate_static_file(name)
    if path:
        return os.path.getmtime(path)


def generate_static_name(name, base=None):
    """Generate a unique filename.
    
    Unique filenames are generated by base 64 encoding the first 64 bits of
    the SHA1 hash. This hash string is added to the filename before the extension.
    """
    if base:
        path = os.path.join(base, name)
    else:
        path = name

    sha = hashlib.sha1(open(path).read()).digest()
    shorthash = base64.urlsafe_b64encode(sha[0:8]).rstrip("=")
    name, ext = os.path.splitext(name)
    return name + '.' + shorthash + ext


def update_static_names(names_file, files):
    """Generate a unique file name mapping for ``files`` and write it to a
    JSON file at ``names_file``."""
    if os.path.exists(names_file):
        names = json.load(open(names_file))
    else:
        names = {}

    base = os.path.dirname(names_file)
    for path in files:
        name = os.path.relpath(path, base)
        mangled_name = generate_static_name(name, base)
        names[name] = mangled_name

        if not os.path.islink(path):
            mangled_path = os.path.join(base, mangled_name)
            shutil.move(path, mangled_path)
            # When on NFS, cp has a bad habit of turning our symlinks into
            # hardlinks. shutil.move will then call rename which will noop in
            # the case of hardlinks to the same inode.
            if os.path.exists(path):
                os.unlink(path)
            os.symlink(mangled_name, path)

    json_enc = json.JSONEncoder(indent=2, sort_keys=True)
    open(names_file, "w").write(json_enc.encode(names))

    return names


if __name__ == "__main__":
    update_static_names(sys.argv[1], sys.argv[2:])

########NEW FILE########
__FILENAME__ = stats
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import collections
import functools
import os
import random
import socket
import time
import threading

from pycassa import columnfamily
from pycassa import pool

from r2.lib import cache
from r2.lib import utils

class TimingStatBuffer:
    """Dictionary of keys to cumulative time+count values.

    This provides thread-safe accumulation of pairs of values. Iterating over
    instances of this class yields (key, (total_time, count)) tuples.
    """

    Timing = collections.namedtuple('Timing', ['key', 'start', 'end'])


    def __init__(self):
        # Store data internally as a map of keys to complex values. The real
        # part of the complex value is the total time (in seconds), and the
        # imaginary part is the total count.
        self.data = collections.defaultdict(complex)
        self.log = threading.local()

    def record(self, key, start, end, publish=True):
        if publish:
            # Add to the total time and total count with a single complex value,
            # so as to avoid inconsistency from a poorly timed context switch.
            self.data[key] += (end - start) + 1j

        if getattr(self.log, 'timings', None) is not None:
            self.log.timings.append(self.Timing(key, start, end))

    def flush(self):
        """Yields accumulated timing and counter data and resets the buffer."""
        data, self.data = self.data, collections.defaultdict(complex)
        while True:
            try:
                k, v = data.popitem()
            except KeyError:
                break

            total_time, count = v.real, v.imag
            yield k, str(int(count)) + '|c'
            divisor = count or 1
            mean = total_time / divisor
            yield k, str(mean * 1000) + '|ms'

    def start_logging(self):
        self.log.timings = []

    def end_logging(self):
        timings = getattr(self.log, 'timings', None)
        self.log.timings = None
        return timings


class CountingStatBuffer:
    """Dictionary of keys to cumulative counts."""

    def __init__(self):
        self.data = collections.defaultdict(int)

    def record(self, key, delta):
        self.data[key] += delta

    def flush(self):
        """Yields accumulated counter data and resets the buffer."""
        data, self.data = self.data, collections.defaultdict(int)
        for k, v in data.iteritems():
            yield k, str(v) + '|c'


class StringCountBuffer:
    """Dictionary of keys to counts of various values."""

    def __init__(self):
        self.data = collections.defaultdict(
            functools.partial(collections.defaultdict, int))

    @staticmethod
    def _encode_string(string):
        # escape \ -> \\, | -> \&, : -> \;, and newline -> \n
        return (
            string.replace('\\', '\\\\')
                .replace('\n', '\\n')
                .replace('|', '\\&')
                .replace(':', '\\;'))

    def record(self, key, value, count=1):
        self.data[key][value] += count

    def flush(self):
        new_data = collections.defaultdict(
            functools.partial(collections.defaultdict, int))
        data, self.data = self.data, new_data
        for k, counts in data.iteritems():
            for v, count in counts.iteritems():
                yield k, str(count) + '|s|' + self._encode_string(v)


class StatsdConnection:
    def __init__(self, addr, compress=True):
        if addr:
            self.host, self.port = self._parse_addr(addr)
            self.sock = self._make_socket()
        else:
            self.host = self.port = self.sock = None
        self.compress = compress

    @classmethod
    def _make_socket(cls):
        return socket.socket(socket.AF_INET, socket.SOCK_DGRAM)

    @staticmethod
    def _parse_addr(addr):
        host, port_str = addr.rsplit(':', 1)
        return host, int(port_str)

    @staticmethod
    def _compress(lines):
        compressed_lines = []
        previous = ''
        for line in sorted(lines):
            prefix = os.path.commonprefix([previous, line])
            if len(prefix) > 3:
                prefix_len = len(prefix)
                compressed_lines.append(
                    '^%02x%s' % (prefix_len, line[prefix_len:]))
            else:
                compressed_lines.append(line)
            previous = line
        return compressed_lines

    def send(self, data):
        if self.sock is None:
            return
        data = ('%s:%s' % item for item in data)
        if self.compress:
            data = self._compress(data)
        payload = '\n'.join(data)
        self.sock.sendto(payload, (self.host, self.port))


class StatsdClient:
    _data_iterator = iter
    _make_conn = StatsdConnection

    def __init__(self, addr=None, sample_rate=1.0):
        self.sample_rate = sample_rate
        self.timing_stats = TimingStatBuffer()
        self.counting_stats = CountingStatBuffer()
        self.string_counts = StringCountBuffer()
        self.connect(addr)

    def connect(self, addr):
        self.conn = self._make_conn(addr)

    def disconnect(self):
        self.conn = self._make_conn(None)

    def flush(self):
        data = list(self.timing_stats.flush())
        data.extend(self.counting_stats.flush())
        data.extend(self.string_counts.flush())
        self.conn.send(self._data_iterator(data))


def _get_stat_name(*name_parts):
    def to_str(value):
        if isinstance(value, unicode):
            value = value.encode('utf-8', 'replace')
        return value
    return '.'.join(to_str(x) for x in name_parts if x)


class Counter:
    def __init__(self, client, name):
        self.client = client
        self.name = name

    def _send(self, subname, delta):
        name = _get_stat_name(self.name, subname)
        return self.client.counting_stats.record(name, delta)

    def increment(self, subname=None, delta=1):
        self._send(subname, delta)

    def decrement(self, subname=None, delta=1):
        self._send(subname, -delta)

    def __add__(self, delta):
        self.increment(delta=delta)
        return self

    def __sub__(self, delta):
        self.decrement(delta=delta)
        return self


class Timer:
    _time = time.time

    def __init__(self, client, name, publish=True):
        self.client = client
        self.name = name
        self.publish = publish
        self._start = None
        self._last = None
        self._stop = None
        self._timings = []

    def flush(self):
        for timing in self._timings:
            self.send(*timing)
        self._timings = []

    def elapsed_seconds(self):
        if self._start is None:
            raise AssertionError("timer hasn't been started")
        if self._stop is None:
            raise AssertionError("timer hasn't been stopped")
        return self._stop - self._start

    def send(self, subname, start, end):
        name = _get_stat_name(self.name, subname)
        self.client.timing_stats.record(name, start, end,
                                        publish=self.publish)

    def start(self):
        self._last = self._start = self._time()

    def intermediate(self, subname):
        if self._last is None:
            raise AssertionError("timer hasn't been started")
        if self._stop is not None:
            raise AssertionError("timer is stopped")
        last, self._last = self._last, self._time()
        self._timings.append((subname, last, self._last))

    def stop(self, subname='total'):
        if self._start is None:
            raise AssertionError("timer hasn't been started")
        if self._stop is not None:
            raise AssertionError('timer is already stopped')
        self._stop = self._time()
        self.flush()
        self.send(subname, self._start, self._stop)


class Stats:
    # Sample rate for recording cache hits/misses, relative to the global
    # sample_rate.
    CACHE_SAMPLE_RATE = 0.01

    CASSANDRA_KEY_SUFFIXES = ['error', 'ok']

    def __init__(self, addr, sample_rate):
        self.client = StatsdClient(addr, sample_rate)

    def get_timer(self, name, publish=True):
        return Timer(self.client, name, publish)

    def transact(self, action, start, end):
        timer = self.get_timer('service_time')
        timer.send(action, start, end)

    def get_counter(self, name):
        return Counter(self.client, name)

    def action_count(self, counter_name, name, delta=1):
        counter = self.get_counter(counter_name)
        if counter:
            from pylons import request
            counter.increment('%s.%s' % (request.environ["pylons.routes_dict"]["action"], name), delta=delta)

    def action_event_count(self, event_name, state=None, delta=1, true_name="success", false_name="fail"):
        counter_name = 'event.%s' % event_name
        if state == True:
            self.action_count(counter_name, true_name, delta=delta)
        elif state == False:
            self.action_count(counter_name, false_name, delta=delta)
        self.action_count(counter_name, 'total', delta=delta)

    def simple_event(self, event_name, delta=1):
        parts = event_name.split('.')
        counter = self.get_counter('.'.join(['event'] + parts[:-1]))
        if counter:
            counter.increment(parts[-1], delta=delta)

    def event_count(self, event_name, name, sample_rate=None):
        if sample_rate is None:
            sample_rate = 1.0
        counter = self.get_counter('event.%s' % event_name)
        if counter and random.random() < sample_rate:
            counter.increment(name)
            counter.increment('total')

    def cache_count(self, name, delta=1, sample_rate=None):
        if sample_rate is None:
            sample_rate = self.CACHE_SAMPLE_RATE
        counter = self.get_counter('cache')
        if counter and random.random() < sample_rate:
            counter.increment(name, delta=delta)

    def cache_count_multi(self, data, cache_name=None, sample_rate=None):
        if sample_rate is None:
            sample_rate = self.CACHE_SAMPLE_RATE
        counter = self.get_counter('cache')
        if counter and random.random() < sample_rate:
            for name, delta in data.iteritems():
                counter.increment(name, delta=delta)

    def amqp_processor(self, queue_name):
        """Decorator for recording stats for amqp queue consumers/handlers."""
        def decorator(processor):
            def wrap_processor(msgs, *args):
                # Work the same for amqp.consume_items and amqp.handle_items.
                msg_tup = utils.tup(msgs)

                start = time.time()
                try:
                    return processor(msgs, *args)
                finally:
                    service_time = (time.time() - start) / len(msg_tup)
                    for n, msg in enumerate(msg_tup):
                        fake_start = start + n * service_time
                        fake_end = fake_start + service_time
                        self.transact('amqp.%s' % queue_name,
                                      fake_start, fake_end)
                    self.flush()
            return wrap_processor
        return decorator

    def flush(self):
        self.client.flush()

    def start_logging_timings(self):
        self.client.timing_stats.start_logging()

    def end_logging_timings(self):
        return self.client.timing_stats.end_logging()

    def cassandra_event(self, operation, column_families, success,
                        start, end):
        if not self.client:
            return
        if not isinstance(column_families, list):
            column_families = [column_families]
        for cf in column_families:
            key = '.'.join([
                'cassandra', cf, operation,
                self.CASSANDRA_KEY_SUFFIXES[success]])
            self.client.timing_stats.record(key, start, end)

    def pg_before_cursor_execute(self, conn, cursor, statement, parameters,
                               context, executemany):
        context._query_start_time = time.time()

    def pg_after_cursor_execute(self, conn, cursor, statement, parameters,
                              context, executemany):
        dsn = dict(part.split('=', 1)
                   for part in context.engine.url.query['dsn'].split())
        start = context._query_start_time
        self.pg_event(dsn['host'], dsn['dbname'], start, time.time())

    def pg_event(self, db_server, db_name, start, end):
        if not self.client:
            return
        key = '.'.join(['pg', db_server.replace('.', '-'), db_name])
        self.client.timing_stats.record(key, start, end)

    def count_string(self, key, value, count=1):
        self.client.string_counts.record(key, str(value), count=count)
   

class CacheStats:
    def __init__(self, parent, cache_name):
        self.parent = parent
        self.cache_name = cache_name
        self.hit_stat_name = '%s.hit' % self.cache_name
        self.miss_stat_name = '%s.miss' % self.cache_name
        self.total_stat_name = '%s.total' % self.cache_name

    def cache_hit(self, delta=1):
        if delta:
            self.parent.cache_count(self.hit_stat_name, delta=delta)
            self.parent.cache_count(self.total_stat_name, delta=delta)

    def cache_miss(self, delta=1):
        if delta:
            self.parent.cache_count(self.miss_stat_name, delta=delta)
            self.parent.cache_count(self.total_stat_name, delta=delta)

    def cache_report(self, hits=0, misses=0, cache_name=None, sample_rate=None):
        if hits or misses:
            if not cache_name:
                cache_name = self.cache_name
            hit_stat_name = '%s.hit' % cache_name
            miss_stat_name = '%s.miss' % cache_name
            total_stat_name = '%s.total' % cache_name
            data = {
                hit_stat_name: hits,
                miss_stat_name: misses,
                total_stat_name: hits + misses,
            }
            self.parent.cache_count_multi(data, cache_name=cache_name,
                                          sample_rate=sample_rate)


class StatsCollectingConnectionPool(pool.ConnectionPool):
    def __init__(self, keyspace, stats=None, *args, **kwargs):
        pool.ConnectionPool.__init__(self, keyspace, *args, **kwargs)
        self.stats = stats

    def _get_new_wrapper(self, server):
        host, sep, port = server.partition(':')
        self.stats.event_count('cassandra.connections', host)

        cf_types = (columnfamily.ColumnParent, columnfamily.ColumnPath)

        def get_cf_name_from_args(args, kwargs):
            for v in args:
                if isinstance(v, cf_types):
                    return v.column_family
            for v in kwargs.itervalues():
                if isinstance(v, cf_types):
                    return v.column_family
            return None

        def get_cf_name_from_batch_mutation(args, kwargs):
            cf_names = set()
            mutation_map = args[0]
            for key_mutations in mutation_map.itervalues():
                cf_names.update(key_mutations)
            return list(cf_names)

        instrumented_methods = dict(
            get=get_cf_name_from_args,
            get_slice=get_cf_name_from_args,
            multiget_slice=get_cf_name_from_args,
            get_count=get_cf_name_from_args,
            multiget_count=get_cf_name_from_args,
            get_range_slices=get_cf_name_from_args,
            get_indexed_slices=get_cf_name_from_args,
            insert=get_cf_name_from_args,
            batch_mutate=get_cf_name_from_batch_mutation,
            add=get_cf_name_from_args,
            remove=get_cf_name_from_args,
            remove_counter=get_cf_name_from_args,
            truncate=lambda args, kwargs: args[0],
        )

        def record_error(method_name, cf_name, start, end):
            if cf_name and self.stats:
                self.stats.cassandra_event(method_name, cf_name, False,
                                           start, end)

        def record_success(method_name, cf_name, start, end):
            if cf_name and self.stats:
                self.stats.cassandra_event(method_name, cf_name, True,
                                           start, end)

        def instrument(f, get_cf_name):
            def call_with_instrumentation(*args, **kwargs):
                cf_name = get_cf_name(args, kwargs)
                start = time.time()
                try:
                    result = f(*args, **kwargs)
                except:
                    record_error(f.__name__, cf_name, start, time.time())
                    raise
                else:
                    record_success(f.__name__, cf_name, start, time.time())
                    return result
            return call_with_instrumentation

        wrapper = pool.ConnectionPool._get_new_wrapper(self, server)
        for method_name, get_cf_name in instrumented_methods.iteritems():
            f = getattr(wrapper, method_name)
            setattr(wrapper, method_name, instrument(f, get_cf_name))
        return wrapper


########NEW FILE########
__FILENAME__ = strings
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

"""
Module for maintaining long or commonly used translatable strings,
removing the need to pollute the code with lots of extra _ and
ungettext calls.  Also provides a capacity for generating a list of
random strings which can be different in each language, though the
hooks to the UI are the same.
"""

from pylons import g, c
from pylons.i18n import _, ungettext, get_lang
import random
import babel.numbers

from r2.lib.translation import set_lang

__all__ = ['StringHandler', 'strings', 'PluralManager', 'plurals',
           'Score', 'rand_strings']

# here's where all of the really long site strings (that need to be
# translated) live so as not to clutter up the rest of the code.  This
# dictionary is not used directly but rather is managed by the single
# StringHandler instance strings
string_dict = dict(

    banned_by = "removed by %s",
    banned    = "removed",
    reports   = "reports: %d",
    
    submitting = _("submitting..."),

    # this accomodates asian languages which don't use spaces
    number_label = _("%(num)d %(thing)s"),

    # this accomodates asian languages which don't use spaces
    points_label = _("%(num)d %(point)s"),

    # this accomodates asian languages which don't use spaces
    time_label = _("%(num)d %(time)s"),

    # this accomodates asian languages which don't use spaces
    float_label = _("%(num)5.3f %(thing)s"),

    # this is for Japanese which treats people counts differently
    person_label = _("<span class='number'>%(num)s</span>&#32;<span class='word'>%(persons)s</span>"),

    already_submitted = _("that link has already been submitted, but you can try to [submit it again](%s)."),

    multiple_submitted = _("that link has been submitted to multiple subreddits. you can try to [submit it again](%s)."),

    user_deleted = _("your account has been deleted, but we won't judge you for it."),

    cover_msg      = _("you'll need to login or register to do that"),
    cover_disclaim = _("(don't worry, it only takes a few seconds)"),

    oauth_login_msg = _(
        "Log in or register to connect your reddit account with %(app)s."),

    legal = _("I understand and agree that registration on or use of this site constitutes agreement to its %(user_agreement)s and %(privacy_policy)s."),

    friends = _('to view reddit with only submissions from your friends, use [reddit.com/r/friends](%s)'),

    sr_created = _('your subreddit has been created'),

    more_info_link = _("visit [%(link)s](%(link)s) for more information"),

    sr_messages = dict(
        empty =  _('you have not subscribed to any subreddits.'),
        subscriber =  _('below are the subreddits you have subscribed to'),
        contributor =  _('below are the subreddits that you are an approved submitter on.'),
        moderator = _('below are the subreddits that you have moderator access to.')
        ),

    sr_subscribe =  _('click the `subscribe` or `unsubscribe` buttons to choose which subreddits appear on your front page.'),

    searching_a_reddit = _('you\'re searching within the [%(reddit_name)s](%(reddit_link)s) subreddit. '+
                           'you can also search within [all subreddits](%(all_reddits_link)s)'),

    permalink_title = _("%(author)s comments on %(title)s"),
    link_info_title = _("%(title)s : %(site)s"),
    banned_subreddit_title = _("this subreddit has been banned"),
    banned_subreddit_message = _("most likely this was done automatically by our spam filtering program. the program is still learning, and may even have some bugs, so if you feel the ban was a mistake, please submit a link to our [request a subreddit listing](%(link)s) and be sure to include the **exact name of the subreddit**."),
    private_subreddit_title = _("this subreddit is private"),
    private_subreddit_message = _("the moderators of this subreddit have set it to private. you must be a moderator or approved submitter to view its contents."),
    comments_panel_text = _("""The following is a sample of what Reddit users had to say about this page. The full discussion is available [here](%(fd_link)s); you can also get there by clicking the link's title (in the middle of the toolbar, to the right of the comments button)."""),

    submit_link = _("""You are submitting a link. The key to a successful submission is interesting content and a descriptive title."""),
    submit_text = _("""You are submitting a text-based post. Speak your mind. A title is required, but expanding further in the text field is not. Beginning your title with "vote up if" is violation of intergalactic law."""),
    submit_link_label = _("Submit a new link"),
    submit_text_label = _("Submit a new text post"),
    compact_suggest = _("Looks like you're browsing on a small screen. Would you like to try [reddit's mobile interface](%(url)s)?"),
    verify_email = _("we're going to need to verify your email address for you to proceed."),
    verify_email_submit = _("you'll be able to submit more frequently once you verify your email address"),
    email_verified =  _("your email address has been verified"),
    email_verify_failed = _("Verification failed.  Please try that again"),
    email_verify_wrong_user = _("The email verification link you've followed is for a different user. Please log out and switch to that user or try again below."),
    search_failed = _("Our search machines are under too much load to handle your request right now. :( Sorry for the inconvenience. Try again in a little bit -- but please don't mash reload; that only makes the problem worse."),
    invalid_search_query = _("I couldn't understand your query, so I simplified it and searched for \"%(clean_query)s\" instead."),
    completely_invalid_search_query = _("I couldn't understand your search query. Please try again."),
    search_help = _("You may also want to check the [search help page](%(search_help)s) for more information."),
    formatting_help_info = _('reddit uses a slightly-customized version of [Markdown](http://daringfireball.net/projects/markdown/syntax) for formatting. See below for some basics, or check [the commenting wiki page](/wiki/commenting) for more detailed help and solutions to common issues.'),
    generic_quota_msg = _("You've submitted too many links recently. Please try again in an hour."),
    verified_quota_msg = _("Looks like you're either a brand new user or your posts have not been doing well recently. You may have to wait a bit to post again. In the meantime feel free to [check out the reddiquette](%(reddiquette)s) or join the conversation in a different thread."),
    unverified_quota_msg = _("Looks like you're either a brand new user or your posts have not been doing well recently. You may have to wait a bit to post again. In the meantime feel free to [check out the reddiquette](%(reddiquette)s), join the conversation in a different thread, or [verify your email address](%(verify)s)."),
    read_only_msg = _("reddit is in \"emergency read-only mode\" right now. :( you won't be able to log in. we're sorry, and are working frantically to fix the problem."),
    heavy_load_msg = _("this page is temporarily in read-only mode due to heavy traffic."),
    gold_benefits_msg = _("reddit gold is reddit's premium membership program. Here are the benefits:\n\n* [Extra site features](/gold/about)\n* [Extra perks](/gold/partners)\n* Discuss and get help on the features and perks at /r/goldbenefits"),
    lounge_msg = _("Grab a drink and join us in /r/lounge, the super-secret members-only community that may or may not exist."),
    postcard_msg = _("You sent us a postcard! (Or something similar.) When we run out of room on our refrigerator, we might one day auction off the stuff that people sent in. Is it okay if we include your thing?"),
    over_comment_limit = _("Sorry, the maximum number of comments is %(max)d. (However, if you subscribe to reddit gold, it goes up to %(goldmax)d.)"),
    over_comment_limit_gold = _("Sorry, the maximum number of comments is %d."),
    youve_got_gold = _("%(sender)s just gifted you %(amount)s of reddit gold!"),
    giftgold_note = _("Here's a note that was included:\n\n----\n\n"),
    youve_been_gilded_comment = _("Another user liked [your comment](%(url)s) so much that they gilded it, giving you reddit gold.\n\n"),
    youve_been_gilded_link = _("Another user liked [your submission](%(url)s) so much that they gilded it, giving you reddit gold.\n\n"),
    gold_summary_autorenew = _("You're about to set up an ongoing, autorenewing subscription to reddit gold for yourself (%(user)s)."),
    gold_summary_onetime = _("You're about to make a one-time purchase of %(amount)s of reddit gold for yourself (%(user)s)."),
    gold_summary_creddits = _("You're about to purchase %(amount)s of reddit gold creddits. They work like gift certificates: each creddit you have will allow you to give one month of reddit gold to someone else."),
    gold_summary_gift_code = _("You're about to purchase %(amount)s of reddit gold in the form of a gift code. The recipient (or you) will be able to claim the code to redeem that gold to their account."),
    gold_summary_signed_gift = _("You're about to give %(amount)s of reddit gold to %(recipient)s, who will be told that it came from you."),
    gold_summary_anonymous_gift = _("You're about to give %(amount)s of reddit gold to %(recipient)s. It will be an anonymous gift."),
    gold_summary_gilding_comment = _("Want to say thanks to *%(recipient)s* for this comment? Give them a month of [reddit gold](/gold/about)."),
    gold_summary_gilding_link = _("Want to say thanks to *%(recipient)s* for this submission? Give them a month of [reddit gold](/gold/about)."),
    gold_summary_gilding_page_comment = _("Give *%(recipient)s* a month of [reddit gold](/gold/about) for this comment:"),
    gold_summary_gilding_page_link = _("Give *%(recipient)s* a month of [reddit gold](/gold/about) for this submission:"),
    unvotable_message = _("sorry, this has been archived and can no longer be voted on"),
    account_activity_blurb = _("This page shows a history of recent activity on your account. If you notice unusual activity, you should change your password immediately. Location information is guessed from your computer's IP address and may be wildly wrong, especially for visits from mobile devices. Note: due to a bug, private-use addresses (starting with 10.) sometimes show up erroneously in this list after regular use of the site."),
    your_current_ip_is = _("You are currently accessing reddit from this IP address: %(address)s."),
    account_activity_apps_blurb = _("""
These apps are authorized to access your account. Logging out of all sessions
will revoke access from all apps. You may also revoke access from individual
apps below.
"""),

    traffic_promoted_link_explanation = _("Below you will see your promotion's impression and click traffic per hour of promotion.  Please note that these traffic totals will lag behind by two to three hours, and that daily totals will be preliminary until 24 hours after the link has finished its run."),
    traffic_processing_slow = _("Traffic processing is currently running slow. The latest data available is from %(date)s. This page will be updated as new data becomes available."),
    traffic_processing_normal = _("Traffic processing occurs on an hourly basis. The latest data available is from %(date)s. This page will be updated as new data becomes available."),
    traffic_help_email = _("Questions? Email self serve support: %(email)s"),

    traffic_subreddit_explanation = _("""
Below are the traffic statistics for your subreddit. Each graph represents one of the following over the interval specified.

* **pageviews** are all hits to %(subreddit)s, including both listing pages and comment pages.
* **uniques** are the total number of unique visitors (determined by a combination of their IP address and User Agent string) that generate the above pageviews. This is independent of whether or not they are logged in.
* **subscriptions** is the number of new subscriptions that have been generated in a given day. This number is less accurate than the first two metrics, as, though we can track new subscriptions, we have no way to track unsubscriptions.

Note: there are a couple of places outside of your subreddit where someone can click "subscribe", so it is possible (though unlikely) that the subscription count can exceed the unique count on a given day.
"""),

    subscribed_multi = _("multireddit of your subscriptions"),
    mod_multi = _("multireddit of subreddits you moderate"),

    r_all_description = _("/r/all displays content from all of reddit, including subreddits you aren't subscribed to."),
    r_all_minus_description = _("Displaying content from /r/all of reddit, except the following subreddits:"),
    all_minus_gold_only = _('Filtering /r/all is a feature only available to [reddit gold](/gold/about) subscribers. Displaying unfiltered results from /r/all.'),
)

class StringHandler(object):
    """Class for managing long translatable strings.  Allows accessing
    of strings via both getitem and getattr.  In both cases, the
    string is passed through the gettext _ function before being
    returned."""
    def __init__(self, **sdict):
        self.string_dict = sdict

    def get(self, attr, default=None):
        try:
            return self[attr]
        except KeyError:
            return default

    def __getitem__(self, attr):
        try:
            return self.__getattr__(attr)
        except AttributeError:
            raise KeyError

    def __getattr__(self, attr):
        rval = self.string_dict[attr]
        if isinstance(rval, (str, unicode)):
            return _(rval)
        elif isinstance(rval, dict):
            return StringHandler(**rval)
        else:
            raise AttributeError
    
    def __iter__(self):
        return iter(self.string_dict)

    def keys(self):
        return self.string_dict.keys()

strings = StringHandler(**string_dict)


def P_(x, y):
    """Convenience method for handling pluralizations.  This identity
    function has been added to the list of keyword functions for babel
    in setup.cfg so that the arguments are translated without having
    to resort to ungettext and _ trickery."""
    return (x, y)

class PluralManager(object):
    """String handler for dealing with pluralizable forms.  plurals
    are passed in in pairs (sing, pl) and can be accessed via
    self.sing and self.pl.

    Additionally, calling self.N_sing(n) (or self.N_pl(n)) (where
    'sing' and 'pl' are placeholders for a (sing, pl) pairing) is
    equivalent to ungettext(sing, pl, n)
    """
    def __init__(self, plurals):
        self.string_dict = {}
        for s, p in plurals:
            self.string_dict[s] = self.string_dict[p] = (s, p)

    def __getattr__(self, attr):
        to_func = False
        if attr.startswith("N_"):
            attr = attr[2:]
            to_func = True

        attr = attr.replace("_", " ")
        if to_func:
            rval = self.string_dict[attr]
            return lambda x: ungettext(rval[0], rval[1], x)
        else:
            rval = self.string_dict[attr]
            n = 1 if attr == rval[0] else 5
            return ungettext(rval[0], rval[1], n)

plurals = PluralManager([P_("comment",     "comments"),
                         P_("point",       "points"),

                         # things
                         P_("link",        "links"),
                         P_("comment",     "comments"),
                         P_("message",     "messages"),
                         P_("subreddit",   "subreddits"),
                         P_("creddit",     "creddits"),

                         # people
                         P_("reader",  "readers"),
                         P_("subscriber",  "subscribers"),
                         P_("approved submitter", "approved submitters"),
                         P_("moderator",   "moderators"),
                         P_("user here now",   "users here now"),

                         # time words
                         P_("milliseconds","milliseconds"),
                         P_("second",      "seconds"),
                         P_("minute",      "minutes"),
                         P_("hour",        "hours"),
                         P_("day",         "days"),
                         P_("month",       "months"),
                         P_("year",        "years"),
])


class Score(object):
    """Convienience class for populating '10 points' in a traslatible
    fasion, used primarily by the score() method in printable.html"""
    @staticmethod
    def number_only(x):
        return str(max(x, 0))

    @staticmethod
    def points(x):
        return  strings.points_label % dict(num=x, point=plurals.N_points(x))

    @staticmethod
    def safepoints(x):
        return  strings.points_label % dict(num=max(x,0),
                                            point=plurals.N_points(x))

    @staticmethod
    def _people(x, label, prepend=''):
        num = prepend + babel.numbers.format_number(x, c.locale)
        return strings.person_label % \
            dict(num=num, persons=label(x))

    @staticmethod
    def subscribers(x):
        return Score._people(x, plurals.N_subscribers)

    @staticmethod
    def readers(x):
        return Score._people(x, plurals.N_readers)

    @staticmethod
    def somethings(x, word):
        p = plurals.string_dict[word]
        f = lambda x: ungettext(p[0], p[1], x)
        return strings.number_label % dict(num=x, thing=f(x))

    @staticmethod
    def users_here_now(x, prepend=''):
        return Score._people(x, plurals.N_users_here_now, prepend=prepend)

    @staticmethod
    def none(x):
        return ""


def fallback_trans(x):
    """For translating placeholder strings the user should never see
    in raw form, such as 'funny 500 message'.  If the string does not
    translate in the current language, falls back on the g.lang
    translation that we've hopefully already provided"""
    t = _(x)
    if t == x:
        l = get_lang()
        set_lang(g.lang, graceful_fail = True)
        t = _(x)
        if l and l[0] != g.lang:
            set_lang(l[0])
    return t

class RandomString(object):
    """class for generating a translatable random string that is one
    of n choices.  The 'description' field passed to the constructor
    is only used to generate labels for the translation interface.

    Unlike other translations, this class is accessed directly by the
    translator classes and side-step babel.extract_messages.
    Untranslated, the strings return are of the form 'description n+1'
    for the nth string.  The user-facing versions of these strings are
    therefore completely determined by their translations."""
    def __init__(self, description, num):
        self.desc = description
        self.num = num

    def get(self, quantity = 0):
        """Generates a list of 'quantity' random strings.  If quantity
        < self.num, the entries are guaranteed to be unique."""
        l = []
        possible = []
        for x in range(max(quantity, 1)):
            if not possible:
                possible = range(self.num)
            irand = random.choice(possible)
            possible.remove(irand)
            l.append(fallback_trans(self._trans_string(irand)))

        return l if len(l) > 1 else l[0]

    def _trans_string(self, n):
        """Provides the form of the string that is actually translated by gettext."""
        return "%s %d" % (self.desc, n+1)

    def __iter__(self):
        for i in xrange(self.num):
            yield self._trans_string(i)


class RandomStringManager(object):
    """class for keeping randomized translatable strings organized.
    New strings are added via add, and accessible by either getattr or
    getitem using the short name passed to add."""
    def __init__(self):
        self.strings = {}

    def __getitem__(self, attr):
        return self.strings[attr].get()

    def __getattr__(self, attr):
        try:
            return self[attr]
        except KeyError:
            raise AttributeError

    def get(self, attr, quantity = 0):
        """Convenience method for getting a list of 'quantity' strings
        from the RandomString named 'attr'"""
        return self.strings[attr].get(quantity)

    def add(self, name, description, num):
        """create a new random string accessible by 'name' in the code
        and explained in the translation interface with 'description'."""
        self.strings[name] = RandomString(description, num)

    def __iter__(self):
        """iterator primarily used by r2.lib.translations to fetch the
        list of random strings and to iterate over their names to
        insert them into the resulting .po file for a given language"""
        return self.strings.iteritems()

rand_strings = RandomStringManager()

rand_strings.add('sadmessages',   "Funny 500 page message", 10)
rand_strings.add('create_reddit', "Reason to create a reddit", 20)


def generate_strings():
    """Print out automatically generated strings for translation."""

    # used by error pages and in the sidebar for why to create a subreddit
    for name, rand_string in rand_strings:
        for string in rand_string:
            print "# TRANSLATORS: Do not translate literally. Come up with a funny/relevant phrase (see the English version for ideas)"
            print "print _('" + string + "')"

    # these are used in r2.lib.pages.trafficpages
    INTERVALS = ("hour", "day", "month")
    TYPES = ("uniques", "pageviews", "traffic", "impressions", "clicks")
    for interval in INTERVALS:
        for type in TYPES:
            print "print _('%s by %s')" % (type, interval)

########NEW FILE########
__FILENAME__ = subreddit_search
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.models import Subreddit
from r2.lib.memoize import memoize
from r2.lib.db.operators import desc
from r2.lib import utils
from r2.lib.db import tdb_cassandra
from r2.lib.cache import CL_ONE

class SubredditsByPartialName(tdb_cassandra.View):
    _use_db = True
    _value_type = 'pickle'
    _connection_pool = 'main'
    _read_consistency_level = CL_ONE

def load_all_reddits():
    query_cache = {}

    q = Subreddit._query(Subreddit.c.type == 'public',
                         Subreddit.c._downs > 1,
                         sort = (desc('_downs'), desc('_ups')),
                         data = True)
    for sr in utils.fetch_things2(q):
        name = sr.name.lower()
        for i in xrange(len(name)):
            prefix = name[:i + 1]
            names = query_cache.setdefault(prefix, [])
            if len(names) < 10:
                names.append((sr.name, sr.over_18))

    for name_prefix, subreddits in query_cache.iteritems():
        SubredditsByPartialName._set_values(name_prefix, {'tups': subreddits})

def search_reddits(query, include_over_18=True):
    query = str(query.lower())

    try:
        result = SubredditsByPartialName._byID(query)
        return [name for (name, over_18) in getattr(result, 'tups', [])
                if not over_18 or include_over_18]
    except tdb_cassandra.NotFound:
        return []

@memoize('popular_searches', time = 3600)
def popular_searches(include_over_18=True):
    top_reddits = Subreddit._query(Subreddit.c.type == 'public',
                                   sort = desc('_downs'),
                                   limit = 100,
                                   data = True)
    top_searches = {}
    for sr in top_reddits:
        if sr.over_18 and not include_over_18:
            continue
        name = sr.name.lower()
        for i in xrange(min(len(name), 3)):
            query = name[:i + 1]
            r = search_reddits(query, include_over_18)
            top_searches[query] = r
    return top_searches


########NEW FILE########
__FILENAME__ = sup
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from datetime import datetime
from itertools import ifilter
import time
import hashlib

import simplejson

from r2.lib.utils import rfc3339_date_str, http_date_str, to36
from r2.lib.memoize import memoize
from r2.lib.template_helpers import get_domain
from pylons import g, c, response

PERIODS = [600, 300, 60]
MIN_PERIOD = min(PERIODS)
MAX_PERIOD = max(PERIODS)

def sup_url():
    return 'http://%s/sup.json' % get_domain(subreddit = False)

def period_urls():
    return dict((p, sup_url() + "?seconds=" + str(p)) for p in PERIODS)

def cache_key(ts):
    return 'sup_' + str(ts)

def make_cur_time(period):
    t = int(time.time())
    return t - t % period

def make_last_time(period):
    return make_cur_time(period) - period

def make_sup_id(user, action):
    sup_id = hashlib.md5(user.name + action).hexdigest()
    #cause cool kids only use part of the hash
    return sup_id[:10]

def add_update(user, action):
    update_time = int(time.time())
    sup_id = make_sup_id(user, action)
    supdate = ',%s:%s' % (sup_id, update_time)

    key = cache_key(make_cur_time(MIN_PERIOD))
    g.cache.add(key, '')
    g.cache.append(key, supdate)

@memoize('set_json', time = MAX_PERIOD)
def sup_json_cached(period, last_time):
    #we need to re-add MIN_PERIOD because we moved back that far with
    #the call to make_last_time
    target_time = last_time + MIN_PERIOD - period

    updates = ''
    #loop backwards adding MIN_PERIOD chunks until last_time is as old
    #as target time
    while last_time >= target_time:
        updates += g.cache.get(cache_key(last_time)) or ''
        last_time -= MIN_PERIOD

    supdates = []
    if updates:
        for u in ifilter(None, updates.split(',')):
            sup_id, time = u.split(':')
            time = int(time)
            if time >= target_time:
                supdates.append([sup_id, to36(time)])

    update_time = datetime.utcnow()
    since_time = datetime.utcfromtimestamp(target_time)
    json = simplejson.dumps({'updated_time' : rfc3339_date_str(update_time),
                             'since_time' : rfc3339_date_str(since_time),
                             'period' : period,
                             'available_periods' : period_urls(),
                             'updates' : supdates})

    #undo json escaping
    json = json.replace('\/', '/')
    return json

def sup_json(period):
    return sup_json_cached(period, make_last_time(MIN_PERIOD))

def set_sup_header(user, action):
    sup_id = make_sup_id(user, action)
    response.headers['x-sup-id'] = sup_url() + '#' + sup_id

def set_expires_header():
    seconds = make_cur_time(MIN_PERIOD) + MIN_PERIOD
    expire_time = datetime.fromtimestamp(seconds, g.tz)
    response.headers['expires'] = http_date_str(expire_time)


########NEW FILE########
__FILENAME__ = system_messages
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import request
from pylons.i18n import N_

from r2.models import Account, Message
from r2.lib.db import queries


user_added_messages = {
    "moderator": {
        "pm": {
            "subject": N_("you are a moderator"),
            "msg": N_("you have been added as a moderator to [%(title)s](%(url)s)."),
        },
    },
    "moderator_invite": {
        "pm": {
            "subject": N_("invitation to moderate %(url)s"),
            "msg": N_("**gadzooks! you are invited to become a moderator of [%(title)s](%(url)s)!**\n\n"
                      "*to accept*, visit the [moderators page for %(url)s](%(url)s/about/moderators) and click \"accept\".\n\n"
                      "*otherwise,* if you did not expect to receive this, you can simply ignore this invitation or report it."),
        },
        "modmail": {
            "subject": N_("moderator invited"),
            "msg": N_("%(user)s has been invited by %(author)s to moderate %(url)s."),
        },
    },
    "accept_moderator_invite": {
        "modmail": {
            "subject": N_("moderator added"),
            "msg": N_("%(user)s has accepted an invitation to become moderator of %(url)s."),
        },
    },
    "contributor": {
        "pm": {
            "subject": N_("you are an approved submitter"),
            "msg": N_("you have been added as an approved submitter to [%(title)s](%(url)s)."),
        },
    },
    "banned": {
        "pm": {
            "subject": N_("you've been banned"),
            "msg": N_("you have been banned from posting to [%(title)s](%(url)s)."),
        },
    },
    "traffic": {
        "pm": {
            "subject": N_("you can view traffic on a promoted link"),
            "msg": N_('you have been added to the list of users able to see [traffic for the sponsored link "%(title)s"](%(traffic_url)s).'),
        },
    },
}


def notify_user_added(rel_type, author, user, target):
    msgs = user_added_messages.get(rel_type)
    if not msgs:
        return

    srname = target.path.rstrip("/")
    d = {
        "url": srname,
        "title": "%s: %s" % (srname, target.title),
        "author": "/u/" + author.name,
        "user": "/u/" + user.name,
    }

    if "pm" in msgs and author != user:
        subject = msgs["pm"]["subject"] % d
        msg = msgs["pm"]["msg"] % d

        if rel_type == "banned" and not user.has_interacted_with(target):
            return

        if rel_type in ("banned", "moderator_invite"):
            # send the message from the subreddit
            item, inbox_rel = Message._new(author, user, subject, msg, request.ip,
                                           sr=target, from_sr=True)
        else:
            item, inbox_rel = Message._new(author, user, subject, msg, request.ip)

        queries.new_message(item, inbox_rel)

    if "modmail" in msgs:
        subject = msgs["modmail"]["subject"] % d
        msg = msgs["modmail"]["msg"] % d

        if rel_type == "moderator_invite":
            modmail_author = Account.system_user()
        else:
            modmail_author = author

        item, inbox_rel = Message._new(modmail_author, target, subject, msg,
                                       request.ip, sr=target)
        queries.new_message(item, inbox_rel)

########NEW FILE########
__FILENAME__ = template_helpers
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.models import *
from filters import unsafe, websafe, _force_unicode, _force_utf8
from r2.lib.utils import UrlParser, timeago, timesince, is_subdomain

from r2.lib import hooks
from r2.lib.static import static_mtime
from r2.lib import js

import babel.numbers
import simplejson
import os.path
from copy import copy
import random
import urlparse
import calendar
import math
import time
from pylons import g, c, request
from pylons.i18n import _, ungettext


static_text_extensions = {
    '.js': 'js',
    '.css': 'css',
    '.less': 'css'
}
def static(path, allow_gzip=True):
    """
    Simple static file maintainer which automatically paths and
    versions files being served out of static.

    In the case of JS and CSS where g.uncompressedJS is set, the
    version of the file is set to be random to prevent caching and it
    mangles the path to point to the uncompressed versions.
    """
    dirname, filename = os.path.split(path)
    extension = os.path.splitext(filename)[1]
    is_text = extension in static_text_extensions
    can_gzip = is_text and 'gzip' in request.accept_encoding
    should_gzip = allow_gzip and can_gzip
    should_cache_bust = False

    path_components = []
    actual_filename = None

    if not c.secure and g.static_domain:
        scheme = "http"
        domain = g.static_domain
        suffix = ".gzip" if should_gzip and g.static_pre_gzipped else ""
    elif c.secure and g.static_secure_domain:
        scheme = "https"
        domain = g.static_secure_domain
        suffix = ".gzip" if should_gzip and g.static_secure_pre_gzipped else ""
    else:
        path_components.append(c.site.static_path)

        if g.uncompressedJS:
            # unminified static files are in type-specific subdirectories
            if not dirname and is_text:
                path_components.append(static_text_extensions[extension])

            should_cache_bust = True
            actual_filename = filename

        scheme = None
        domain = None
        suffix = ""

    path_components.append(dirname)
    if not actual_filename:
        actual_filename = g.static_names.get(filename, filename)
    path_components.append(actual_filename + suffix)

    actual_path = os.path.join(*path_components)

    query = None
    if path and should_cache_bust:
        file_id = static_mtime(actual_path) or random.randint(0, 1000000)
        query = 'v=' + str(file_id)

    return urlparse.urlunsplit((
        scheme,
        domain,
        actual_path,
        query,
        None
    ))


def media_https_if_secure(url):
    if not c.secure:
        return url
    return g.media_provider.convert_to_https(url)


def header_url(url):
    if url == g.default_header_url:
        return static(url)
    else:
        return media_https_if_secure(url)


def js_config(extra_config=None):
    logged = c.user_is_loggedin and c.user.name
    gold = bool(logged and c.user.gold)

    config = {
        # is the user logged in?
        "logged": logged,
        # the subreddit's name (for posts)
        "post_site": c.site.name if not c.default_sr else "",
        # the user's voting hash
        "modhash": c.modhash or False,
        # the current rendering style
        "renderstyle": c.render_style,

        # they're welcome to try to override this in the DOM because we just
        # disable the features server-side if applicable
        'store_visits': gold and c.user.pref_store_visits,

        # current domain
        "cur_domain": get_domain(cname=c.frameless_cname, subreddit=False, no_www=True),
        # where do ajax requests go?
        "ajax_domain": get_domain(cname=c.authorized_cname, subreddit=False),
        "extension": c.extension,
        "https_endpoint": is_subdomain(request.host, g.domain) and g.https_endpoint,
        # debugging?
        "debug": g.debug,
        "send_logs": g.live_config["frontend_logging"],
        "server_time": math.floor(time.time()),
        "status_msg": {
          "fetching": _("fetching title..."),
          "submitting": _("submitting..."),
          "loading": _("loading...")
        },
        "is_fake": isinstance(c.site, FakeSubreddit),
        "fetch_trackers_url": g.fetch_trackers_url,
        "adtracker_url": g.adtracker_url,
        "clicktracker_url": g.clicktracker_url,
        "uitracker_url": g.uitracker_url,
        "static_root": static(''),
        "over_18": bool(c.over18),
        "new_window": bool(c.user.pref_newwindow),
        "vote_hash": c.vote_hash,
        "gold": gold,
        "has_subscribed": logged and c.user.has_subscribed,
    }

    if g.uncompressedJS:
        config["uncompressedJS"] = True

    if extra_config:
        config.update(extra_config)

    hooks.get_hook("js_config").call(config=config)

    return config


class JSPreload(js.DataSource):
    def __init__(self, data=None):
        if data is None:
            data = {}
        js.DataSource.__init__(self, "r.preload.set({content})", data)

    def set(self, url, data):
        self.data[url] = data

    def set_wrapped(self, url, wrapped):
        from r2.lib.pages.things import wrap_things
        if not isinstance(wrapped, Wrapped):
            wrapped = wrap_things(wrapped)[0]
        self.data[url] = wrapped.render_nocache('', style='api').finalize()

    def use(self):
        hooks.get_hook("js_preload.use").call(js_preload=self)

        if self.data:
            return js.DataSource.use(self)
        else:
            return ''


def class_dict():
    t_cls = [Link, Comment, Message, Subreddit]
    l_cls = [Listing, OrganicListing]

    classes  = [('%s: %s') % ('t'+ str(cl._type_id), cl.__name__ ) for cl in t_cls] \
             + [('%s: %s') % (cl.__name__, cl._js_cls) for cl in l_cls]

    res = ', '.join(classes)
    return unsafe('{ %s }' % res)

def calc_time_period(comment_time):
    # Set in front.py:GET_comments()
    previous_visits = c.previous_visits

    if not previous_visits:
        return ""

    rv = ""
    for i, visit in enumerate(previous_visits):
        if comment_time > visit:
            rv = "comment-period-%d" % i

    return rv

def comment_label(num_comments=None):
    if not num_comments:
        # generates "comment" the imperative verb
        com_label = _("comment {verb}")
        com_cls = 'comments empty may-blank'
    else:
        # generates "XX comments" as a noun
        com_label = ungettext("comment", "comments", num_comments)
        com_label = strings.number_label % dict(num=num_comments,
                                                thing=com_label)
        com_cls = 'comments may-blank'
    return com_label, com_cls

def replace_render(listing, item, render_func):
    def _replace_render(style = None, display = True):
        """
        A helper function for listings to set uncachable attributes on a
        rendered thing (item) to its proper display values for the current
        context.
        """
        style = style or c.render_style or 'html'
        replacements = {}

        if hasattr(item, 'child'):
            if item.child:
                replacements['childlisting'] = item.child.render(style=style)
            else:
                # Special case for when the comment tree wasn't built which
                # occurs both in the inbox and spam page view of comments.
                replacements['childlisting'] = None
        else:
            replacements['childlisting'] = ''

        #only LinkListing has a show_nums attribute
        if listing and hasattr(listing, "show_nums"):
            replacements["num"] = str(item.num) if listing.show_nums else ""

        if hasattr(item, "num_comments"):
            com_label, com_cls = comment_label(item.num_comments)
            if style == "compact":
                com_label = unicode(item.num_comments)
            replacements['numcomments'] = com_label
            replacements['commentcls'] = com_cls

        replacements['display'] =  "" if display else "style='display:none'"

        if hasattr(item, "render_score"):
            # replace the score stub
            (replacements['scoredislikes'],
             replacements['scoreunvoted'],
             replacements['scorelikes'])  = item.render_score

        # compute the timesince here so we don't end up caching it
        if hasattr(item, "_date"):
            if hasattr(item, "promoted") and item.promoted is not None:
                from r2.lib import promote
                # promoted links are special in their date handling
                replacements['timesince'] = \
                    simplified_timesince(item._date - promote.timezone_offset)
            else:
                replacements['timesince'] = simplified_timesince(item._date)

            replacements['time_period'] = calc_time_period(item._date)

        # compute the last edited time here so we don't end up caching it
        if hasattr(item, "editted") and not isinstance(item.editted, bool):
            replacements['lastedited'] = simplified_timesince(item.editted)

        # Set in front.py:GET_comments()
        replacements['previous_visits_hex'] = c.previous_visits_hex

        renderer = render_func or item.render
        res = renderer(style = style, **replacements)

        if isinstance(res, (str, unicode)):
            rv = unsafe(res)
            if g.debug:
                for leftover in re.findall('<\$>(.+?)(?:<|$)', rv):
                    print "replace_render didn't replace %s" % leftover

            return rv

        return res

    return _replace_render

def get_domain(cname = False, subreddit = True, no_www = False):
    """
    returns the domain on the current subreddit, possibly including
    the subreddit part of the path, suitable for insertion after an
    "http://" and before a fullpath (i.e., something including the
    first '/') in a template.  The domain is updated to include the
    current port (request.port).  The effect of the arguments is:

     * no_www: if the domain ends up being g.domain, the default
       behavior is to prepend "www." to the front of it (for akamai).
       This flag will optionally disable it.

     * cname: whether to respect the value of c.cname and return
       c.site.domain rather than g.domain as the host name.

     * subreddit: if a cname is not used in the resulting path, flags
       whether or not to append to the domain the subreddit path (sans
       the trailing path).

    """
    # locally cache these lookups as this gets run in a loop in add_props
    domain = g.domain
    domain_prefix = c.domain_prefix
    site  = c.site
    ccname = c.cname
    if not no_www and domain_prefix:
        domain = domain_prefix + "." + domain
    if cname and ccname and site.domain:
        domain = site.domain
    if hasattr(request, "port") and request.port:
        domain += ":" + str(request.port)
    if (not ccname or not cname) and subreddit:
        domain += site.path.rstrip('/')
    return domain

def dockletStr(context, type, browser):
    domain      = get_domain()

    # while site_domain will hold the (possibly) cnamed version
    site_domain = get_domain(True)

    if type == "serendipity!":
        return "http://"+site_domain+"/random"
    elif type == "submit":
        return ("javascript:location.href='http://"+site_domain+
               "/submit?url='+encodeURIComponent(location.href)+'&title='+encodeURIComponent(document.title)")
    elif type == "reddit toolbar":
        return ("javascript:%20var%20h%20=%20window.location.href;%20h%20=%20'http://" +
                site_domain + "/s/'%20+%20escape(h);%20window.location%20=%20h;")
    else:
        # these are the linked/disliked buttons, which we have removed
        # from the UI
        return (("javascript:function b(){var u=encodeURIComponent(location.href);"
                 "var i=document.getElementById('redstat')||document.createElement('a');"
                 "var s=i.style;s.position='%(position)s';s.top='0';s.left='0';"
                 "s.zIndex='10002';i.id='redstat';"
                 "i.href='http://%(site_domain)s/submit?url='+u+'&title='+"
                 "encodeURIComponent(document.title);"
                 "var q=i.firstChild||document.createElement('img');"
                 "q.src='http://%(domain)s/d/%(type)s.png?v='+Math.random()+'&uh=%(modhash)s&u='+u;"
                 "i.appendChild(q);document.body.appendChild(i)};b()") %
                dict(position = "absolute" if browser == "ie" else "fixed",
                     domain = domain, site_domain = site_domain, type = type,
                     modhash = c.modhash if c.user else ''))



def add_sr(path, sr_path = True, nocname=False, force_hostname = False, retain_extension=True):
    """
    Given a path (which may be a full-fledged url or a relative path),
    parses the path and updates it to include the subreddit path
    according to the rules set by its arguments:

     * force_hostname: if True, force the url's hostname to be updated
       even if it is already set in the path, and subject to the
       c.cname/nocname combination.  If false, the path will still
       have its domain updated if no hostname is specified in the url.

     * nocname: when updating the hostname, overrides the value of
       c.cname to set the hostname to g.domain.  The default behavior
       is to set the hostname consistent with c.cname.

     * sr_path: if a cname is not used for the domain, updates the
       path to include c.site.path.

    For caching purposes: note that this function uses:
      c.cname, c.render_style, c.site.name
    """
    # don't do anything if it is just an anchor
    if path.startswith(('#', 'javascript:')):
        return path

    u = UrlParser(path)
    if sr_path and (nocname or not c.cname):
        u.path_add_subreddit(c.site)

    if not u.hostname or force_hostname:
        if c.secure:
            u.hostname = request.host
        else:
            u.hostname = get_domain(cname = (c.cname and not nocname),
                                    subreddit = False)

    if c.secure:
        u.scheme = "https"

    if retain_extension:
        if c.render_style == 'mobile':
            u.set_extension('mobile')

        elif c.render_style == 'compact':
            u.set_extension('compact')

    return u.unparse()

def join_urls(*urls):
    """joins a series of urls together without doubles slashes"""
    if not urls:
        return

    url = urls[0]
    for u in urls[1:]:
        if not url.endswith('/'):
            url += '/'
        while u.startswith('/'):
            u = utils.lstrips(u, '/')
        url += u
    return url

def style_line(button_width = None, bgcolor = "", bordercolor = ""):
    style_line = ''
    bordercolor = c.bordercolor or bordercolor
    bgcolor     = c.bgcolor or bgcolor
    if bgcolor:
        style_line += "background-color: #%s;" % bgcolor
    if bordercolor:
        style_line += "border: 1px solid #%s;" % bordercolor
    if button_width:
        style_line += "width: %spx;" % button_width
    return style_line

def choose_width(link, width):
    if width:
        return width - 5
    else:
        if hasattr(link, "_ups"):
            return 100 + (10 * (len(str(link._ups - link._downs))))
        else:
            return 110

# Appends to the list "attrs" a tuple of:
# <priority (higher trumps lower), letter,
#  css class, i18n'ed mouseover label, hyperlink (opt)>
def add_attr(attrs, kind, label=None, link=None, cssclass=None, symbol=None):
    from r2.lib.template_helpers import static

    symbol = symbol or kind

    if kind == 'F':
        priority = 1
        cssclass = 'friend'
        if not label:
            label = _('friend')
        if not link:
            link = '/prefs/friends'
    elif kind == 'S':
        priority = 2
        cssclass = 'submitter'
        if not label:
            label = _('submitter')
        if not link:
            raise ValueError ("Need a link")
    elif kind == 'M':
        priority = 3
        cssclass = 'moderator'
        if not label:
            raise ValueError ("Need a label")
        if not link:
            raise ValueError ("Need a link")
    elif kind == 'A':
        priority = 4
        cssclass = 'admin'
        if not label:
            label = _('reddit admin, speaking officially')
        if not link:
            link = '/about/team'
    elif kind in ('X', '@'):
        priority = 5
        cssclass = 'gray'
        if not label:
            raise ValueError ("Need a label")
    elif kind == 'V':
        priority = 6
        cssclass = 'green'
        if not label:
            raise ValueError ("Need a label")
    elif kind == 'B':
        priority = 7
        cssclass = 'wrong'
        if not label:
            raise ValueError ("Need a label")
    elif kind == 'special':
        priority = 98
    elif kind == "cake":
        priority = 99
        cssclass = "cakeday"
        symbol = "&#x1F370;"
        if not label:
            raise ValueError ("Need a label")
        if not link:
            raise ValueError ("Need a link")
    else:
        raise ValueError ("Got weird kind [%s]" % kind)

    attrs.append( (priority, symbol, cssclass, label, link) )


def search_url(query, subreddit, restrict_sr="off", sort=None, recent=None):
    import urllib
    query = _force_utf8(query)
    url_query = {"q": query}
    if restrict_sr:
        url_query["restrict_sr"] = restrict_sr
    if sort:
        url_query["sort"] = sort
    if recent:
        url_query["t"] = recent
    path = "/r/%s/search?" % subreddit if subreddit else "/search?"
    path += urllib.urlencode(url_query)
    return path


def format_number(number, locale=None):
    if not locale:
        locale = c.locale

    return babel.numbers.format_number(number, locale=locale)


def html_datetime(date):
    # Strip off the microsecond to appease the HTML5 gods, since
    # datetime.isoformat() returns too long of a microsecond value.
    # http://www.whatwg.org/specs/web-apps/current-work/multipage/common-microsyntaxes.html#times
    return date.replace(microsecond=0).isoformat()


def js_timestamp(date):
    return '%d' % (calendar.timegm(date.timetuple()) * 1000)


def simplified_timesince(date, include_tense=True):
    if date > timeago("1 minute"):
        return _("just now")

    since = []
    since.append(timesince(date))
    if include_tense:
        since.append(_("ago"))
    return " ".join(since)

########NEW FILE########
__FILENAME__ = totp
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

"""An implementation of the RFC-6238 Time-Based One Time Password algorithm."""

import time
import hmac
import base64
import struct
import hashlib


PERIOD = 30


def make_hotp(secret, counter):
    """Generate an RFC-4226 HMAC-Based One Time Password."""
    key = base64.b32decode(secret)

    # compute the HMAC digest of the counter with the secret key
    counter_encoded = struct.pack(">q", counter)
    hmac_result = hmac.HMAC(key, counter_encoded, hashlib.sha1).digest()

    # do HOTP dynamic truncation (see RFC4226 5.3)
    offset = ord(hmac_result[-1]) & 0x0f
    truncated_hash = hmac_result[offset:offset + 4]
    code_bits, = struct.unpack(">L", truncated_hash)
    htop = (code_bits & 0x7fffffff) % 1000000

    # pad it out as necessary
    return "%06d" % htop


def make_totp(secret, skew=0, timestamp=None):
    """Generate an RFC-6238 Time-Based One Time Password."""
    timestamp = timestamp or time.time()
    counter = timestamp // PERIOD
    return make_hotp(secret, counter - skew)


def generate_secret():
    """Make a secret key suitable for use in TOTP."""
    from Crypto.Random import get_random_bytes
    bytes = get_random_bytes(20)
    encoded = base64.b32encode(bytes)
    return encoded


if __name__ == "__main__":
    # based on RFC-6238 Appendix B (trimmed to six-digit OTPs)
    secret = base64.b32encode("12345678901234567890")
    assert make_totp(secret, timestamp=59) == "287082"
    assert make_totp(secret, timestamp=1111111109) == "081804"
    assert make_totp(secret, timestamp=1111111111) == "050471"
    assert make_totp(secret, timestamp=1234567890) == "005924"
    assert make_totp(secret, timestamp=2000000000) == "279037"
    assert make_totp(secret, timestamp=20000000000) == "353130"

########NEW FILE########
__FILENAME__ = tracking
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from Crypto.Cipher import AES
from Crypto.Random import get_random_bytes
import base64
import hashlib
import urllib

from pylons import c, g, request

from r2.lib.filters import _force_utf8


KEY_SIZE = 16  # AES-128
SALT_SIZE = KEY_SIZE * 2  # backwards compatibility


def _pad_message(text):
    """Return `text` padded out to a multiple of block_size bytes.

    This uses the PKCS7 padding algorithm. The pad-bytes have a value of N
    where N is the number of bytes of padding added. If the input string is
    already a multiple of the block size, it will be padded with one full extra
    block to make an unambiguous output string.

    """
    block_size = AES.block_size
    padding_size = (block_size - len(text) % block_size) or block_size
    padding = chr(padding_size) * padding_size
    return text + padding


def _unpad_message(text):
    """Return `text` with padding removed. The inverse of _pad_message."""
    if not text:
        return ""

    padding_size = ord(text[-1])
    if padding_size > AES.block_size:
        return ""

    unpadded, padding = text[:-padding_size], text[-padding_size:]
    if any(ord(x) != padding_size for x in padding):
        return ""

    return unpadded


def _make_cipher(initialization_vector, secret):
    """Return a block cipher object for use in `encrypt` and `decrypt`."""
    return AES.new(secret[:KEY_SIZE], AES.MODE_CBC,
                   initialization_vector[:AES.block_size])


def encrypt(plaintext):
    """Return the message `plaintext` encrypted.

    The encrypted message will have its salt prepended and will be URL encoded
    to make it suitable for use in URLs and Cookies.

    NOTE: this function is here for backwards compatibility. Please do not
    use it for new code.

    """

    salt = _make_salt()
    return _encrypt(salt, plaintext, g.tracking_secret)


def _make_salt():
    # we want SALT_SIZE letters of salt text, but we're generating random bytes
    # so we'll calculate how many bytes we need to get SALT_SIZE characters of
    # base64 output. because of padding, this only works for SALT_SIZE % 4 == 0
    assert SALT_SIZE % 4 == 0
    salt_byte_count = (SALT_SIZE / 4) * 3
    salt_bytes = get_random_bytes(salt_byte_count)
    return base64.b64encode(salt_bytes)


def _encrypt(salt, plaintext, secret):
    cipher = _make_cipher(salt, secret)

    padded = _pad_message(plaintext)
    ciphertext = cipher.encrypt(padded)
    encoded = base64.b64encode(ciphertext)

    return urllib.quote_plus(salt + encoded, safe="")


def decrypt(encrypted):
    """Decrypt `encrypted` and return the plaintext.

    NOTE: like `encrypt` above, please do not use this function for new code.

    """

    return _decrypt(encrypted, g.tracking_secret)


def _decrypt(encrypted, secret):
    encrypted = urllib.unquote_plus(encrypted)
    salt, encoded = encrypted[:SALT_SIZE], encrypted[SALT_SIZE:]
    ciphertext = base64.b64decode(encoded)
    cipher = _make_cipher(salt, secret)
    padded = cipher.decrypt(ciphertext)
    return _unpad_message(padded)


def get_site():
    """Return the name of the current "site" (subreddit)."""
    return c.site.analytics_name if c.site else ""


def get_srpath():
    """Return the srpath of the current request.

    The srpath is Subredditname-Action. e.g. sophiepotamus-GET_listing.

    """
    name = get_site()
    action = None
    if c.render_style in ("mobile", "compact"):
        action = c.render_style
    else:
        action = request.environ['pylons.routes_dict'].get('action')

    if not action:
        return name
    return '-'.join((name, action))


def get_pageview_pixel_url():
    """Return a URL to use for tracking pageviews for the current request."""
    data = [
        c.user.name if c.user_is_loggedin else "",
        get_srpath(),
        c.lang or "",
        c.cname,
    ]
    encrypted = encrypt("|".join(_force_utf8(s) for s in data))
    return g.tracker_url + "?v=" + encrypted


def get_impression_pixel_url(codename):
    """Return a URL to use for tracking impressions of the given advert."""
    # TODO: use HMAC here
    mac = codename + hashlib.sha1(codename + g.tracking_secret).hexdigest()
    return g.adframetracker_url + "?" + urllib.urlencode({
        "hash": mac,
        "id": codename,
    })

########NEW FILE########
__FILENAME__ = emr_traffic
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from copy import copy
from pylons import g
import os
from time import time, sleep

from boto.emr.step import InstallPigStep, PigStep
from boto.emr.bootstrap_action import BootstrapAction

from r2.lib.emr_helpers import (
    EmrException,
    EmrJob,
    get_compatible_jobflows,
    get_step_state,
    LIVE_STATES,
    COMPLETED,
    PENDING,
    NOTFOUND,
)


class TrafficBase(EmrJob):

    """Base class for all traffic jobs.

    Includes required bootstrap actions and setup steps.

    """

    BOOTSTRAP_NAME = 'traffic binaries'
    BOOTSTRAP_SCRIPT = os.path.join(g.TRAFFIC_SRC_DIR, 'traffic_bootstrap.sh')
    _defaults = dict(master_instance_type='m1.small',
                     slave_instance_type='m1.xlarge', num_slaves=1)

    def __init__(self, emr_connection, jobflow_name, steps=None, **kw):
        combined_kw = copy(self._defaults)
        combined_kw.update(kw)
        bootstrap_actions = self._bootstrap_actions()
        setup_steps = self._setup_steps()
        steps = steps or []
        EmrJob.__init__(self, emr_connection, jobflow_name,
                        bootstrap_actions=bootstrap_actions,
                        setup_steps=setup_steps,
                        steps=steps,
                        **combined_kw)

    @classmethod
    def _bootstrap_actions(cls):
        name = cls.BOOTSTRAP_NAME
        path = cls.BOOTSTRAP_SCRIPT
        bootstrap_action_args = [g.TRAFFIC_SRC_DIR, g.tracking_secret]
        bootstrap = BootstrapAction(name, path, bootstrap_action_args)
        return [bootstrap]

    @classmethod
    def _setup_steps(self):
        return [InstallPigStep()]


class PigProcessHour(PigStep):
    STEP_NAME = 'pig process hour'
    PIG_FILE = os.path.join(g.TRAFFIC_SRC_DIR, 'mr_process_hour.pig')

    def __init__(self, log_path, output_path):
        self.log_path = log_path
        self.output_path = output_path
        self.name = '%s (%s)' % (self.STEP_NAME, self.log_path)
        pig_args = ['-p', 'OUTPUT=%s' % self.output_path,
                    '-p', 'LOGFILE=%s' % self.log_path]
        PigStep.__init__(self, self.name, self.PIG_FILE, pig_args=pig_args)


class PigAggregate(PigStep):
    STEP_NAME = 'pig aggregate'
    PIG_FILE = os.path.join(g.TRAFFIC_SRC_DIR, 'mr_aggregate.pig')

    def __init__(self, input_path, output_path):
        self.input_path = input_path
        self.output_path = output_path
        self.name = '%s (%s)' % (self.STEP_NAME, self.input_path)
        pig_args = ['-p', 'INPUT=%s' % self.input_path,
                    '-p', 'OUTPUT=%s' % self.output_path]
        PigStep.__init__(self, self.name, self.PIG_FILE, pig_args=pig_args)


class PigCoalesce(PigStep):
    STEP_NAME = 'pig coalesce'
    PIG_FILE = os.path.join(g.TRAFFIC_SRC_DIR, 'mr_coalesce.pig')

    def __init__(self, input_path, output_path):
        self.input_path = input_path
        self.output_path = output_path
        self.name = '%s (%s)' % (self.STEP_NAME, self.input_path)
        pig_args = ['-p', 'INPUT=%s' % self.input_path,
                    '-p', 'OUTPUT=%s' % self.output_path]
        PigStep.__init__(self, self.name, self.PIG_FILE, pig_args=pig_args)


def _add_step(emr_connection, step, jobflow_name, **jobflow_kw):
    """Add step to a running jobflow.

    Append the step onto a compatible jobflow with the specified name if one
    exists, otherwise create a new jobflow and run it. Returns the jobflowid.
    NOTE: jobflow_kw will be used to configure the jobflow ONLY if a new
    jobflow is created.

    """

    running = get_compatible_jobflows(
                emr_connection,
                bootstrap_actions=TrafficBase._bootstrap_actions(),
                setup_steps=TrafficBase._setup_steps())

    for jf in running:
        if jf.name == jobflow_name:
            jobflowid = jf.jobflowid
            emr_connection.add_jobflow_steps(jobflowid, step)
            print 'Added %s to jobflow %s' % (step.name, jobflowid)
            break
    else:
        base = TrafficBase(emr_connection, jobflow_name, steps=[step],
                           **jobflow_kw)
        base.run()
        jobflowid = base.jobflowid
        print 'Added %s to new jobflow %s' % (step.name, jobflowid)

    return jobflowid


def _wait_for_step(emr_connection, step, jobflowid, sleeptime):
    """Poll EMR and wait for a step to finish."""
    sleep(180)
    start = time()
    step_state = get_step_state(emr_connection, jobflowid, step.name,
                                update=True)
    while step_state in LIVE_STATES + [PENDING]:
        sleep(sleeptime)
        step_state = get_step_state(emr_connection, jobflowid, step.name)
    end = time()
    print '%s took %0.2fs (exit: %s)' % (step.name, end - start, step_state)
    return step_state


def run_traffic_step(emr_connection, step, jobflow_name,
                     wait=True, sleeptime=60, retries=1, **jobflow_kw):
    """Run a traffic processing step.

    Helper function to force all steps to be executed by the same jobflow
    (jobflow_name). Also can hold until complete (wait) and retry on
    failure (retries).

    """

    jobflowid = _add_step(emr_connection, step, jobflow_name, **jobflow_kw)

    if not wait:
        return

    attempts = 1
    exit_state = _wait_for_step(emr_connection, step, jobflowid, sleeptime)
    while attempts <= retries and exit_state != COMPLETED:
        jobflowid = _add_step(emr_connection, step, jobflow_name, **jobflow_kw)
        exit_state = _wait_for_step(emr_connection, step, jobflowid, sleeptime)
        attempts += 1

    if exit_state != COMPLETED:
        msg = '%s failed (exit: %s)' % (step.name, exit_state)
        if retries:
            msg += 'retried %s times' % retries
        raise EmrException(msg)


def extract_hour(emr_connection, jobflow_name, log_path, output_path,
                 **jobflow_kw):
    step = PigProcessHour(log_path, output_path)
    run_traffic_step(emr_connection, step, jobflow_name, **jobflow_kw)


def aggregate_interval(emr_connection, jobflow_name, input_path, output_path,
                       **jobflow_kw):
    step = PigAggregate(input_path, output_path)
    run_traffic_step(emr_connection, step, jobflow_name, **jobflow_kw)


def coalesce_interval(emr_connection, jobflow_name, input_path, output_path,
                      **jobflow_kw):
    step = PigCoalesce(input_path, output_path)
    run_traffic_step(emr_connection, step, jobflow_name, **jobflow_kw)

########NEW FILE########
__FILENAME__ = traffic
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import datetime
import calendar
import os
from time import sleep
import urllib

from boto.s3.connection import S3Connection
from boto.emr.connection import EmrConnection
from boto.exception import S3ResponseError
from pylons import g

from r2.lib.emr_helpers import (EmrException, terminate_jobflow,
    modify_slave_count)
from r2.lib.s3_helpers import get_text_from_s3, s3_key_exists, copy_to_s3
from r2.lib.traffic.emr_traffic import (extract_hour, aggregate_interval,
        coalesce_interval)
from r2.lib.utils import tup
from r2.models.traffic import (SitewidePageviews, PageviewsBySubreddit,
        PageviewsBySubredditAndPath, PageviewsByLanguage,
        ClickthroughsByCodename, TargetedClickthroughsByCodename,
        AdImpressionsByCodename, TargetedImpressionsByCodename)


RAW_LOG_DIR = g.RAW_LOG_DIR
PROCESSED_DIR = g.PROCESSED_DIR
AGGREGATE_DIR = g.AGGREGATE_DIR
AWS_LOG_DIR = g.AWS_LOG_DIR

# the "or None" business is so that a blank string becomes None to cause boto
# to look for credentials in other places.
s3_connection = S3Connection(g.TRAFFIC_ACCESS_KEY or None,
                             g.TRAFFIC_SECRET_KEY or None)
emr_connection = EmrConnection(g.TRAFFIC_ACCESS_KEY or None,
                               g.TRAFFIC_SECRET_KEY or None)

traffic_categories = (SitewidePageviews, PageviewsBySubreddit,
                      PageviewsBySubredditAndPath, PageviewsByLanguage,
                      ClickthroughsByCodename, TargetedClickthroughsByCodename,
                      AdImpressionsByCodename, TargetedImpressionsByCodename)

traffic_subdirectories = {
    SitewidePageviews: 'sitewide',
    PageviewsBySubreddit: 'subreddit',
    PageviewsBySubredditAndPath: 'srpath',
    PageviewsByLanguage: 'lang',
    ClickthroughsByCodename: 'clicks',
    TargetedClickthroughsByCodename: 'clicks_targeted',
    AdImpressionsByCodename: 'thing',
    TargetedImpressionsByCodename: 'thingtarget',
}


def _get_processed_path(basedir, interval, category_cls, filename):
    return os.path.join(basedir, interval,
                        traffic_subdirectories[category_cls], filename)


def get_aggregate(interval, category_cls):
    """Return the aggregate output file from S3."""
    part = 0
    data = {}

    while True:
        path = _get_processed_path(AGGREGATE_DIR, interval, category_cls,
                                   'part-r-%05d' % part)
        if not s3_key_exists(s3_connection, path):
            break

        # Sometimes S3 doesn't let us read immediately after key is written
        for i in xrange(5):
            try:
                txt = get_text_from_s3(s3_connection, path)
            except S3ResponseError as e:
                print 'S3ResponseError on %s, retrying' % path
                sleep(300)
            else:
                break
        else:
            print 'Could not retrieve %s' % path
            raise e

        for line in txt.splitlines():
            tuples = line.rstrip('\n').split('\t')
            group, uniques, pageviews = tuples[:-2], tuples[-2], tuples[-1]
            if len(group) > 1:
                group = tuple(group)
            else:
                group = group[0]
            data[group] = (int(uniques), int(pageviews))

        part += 1

    if not data:
        raise ValueError("No data for %s/%s" % (interval,
                                                category_cls.__name__))

    return data


def report_interval(interval, background=True):
    if background:
        from multiprocessing import Process
        p = Process(target=_report_interval, args=(interval,))
        p.start()
    else:
        _report_interval(interval)


def _name_to_kw(category_cls, name):
    """Get the keywords needed to build an instance of traffic data."""
    def target_split(name):
        """Split a name that contains multiple words.

        Name is (link,campaign-subreddit) where link and campaign are
        thing fullnames. campaign and subreddit are each optional, so
        the string could look like any of these:
        (t3_bh,t8_ab-pics), (t3_bh,t8_ab), (t3_bh,-pics), (t3_bh,)
        Also check for the old format (t3_by, pics)

        """

        link_codename, target_info = name
        campaign_codename = None
        if not target_info:
            subreddit = ''
        elif target_info.find('-') != -1:
            campaign_codename, subreddit = target_info.split('-', 1)
        elif target_info.find('_') != -1:
            campaign_codename = target_info
            subreddit = ''
        else:
            subreddit = target_info
        return {'codename': campaign_codename or link_codename,
                'subreddit': subreddit}

    d = {SitewidePageviews: lambda n: {},
         PageviewsBySubreddit: lambda n: {'subreddit': n},
         PageviewsBySubredditAndPath: lambda n: {'srpath': n},
         PageviewsByLanguage: lambda n: {'lang': n},
         ClickthroughsByCodename: lambda n: {'codename': name},
         AdImpressionsByCodename: lambda n: {'codename': name},
         TargetedClickthroughsByCodename: target_split,
         TargetedImpressionsByCodename: target_split}
    return d[category_cls](name)


def _report_interval(interval):
    """Read aggregated traffic from S3 and write to postgres."""
    from sqlalchemy.orm import scoped_session, sessionmaker
    from r2.models.traffic import engine
    Session = scoped_session(sessionmaker(bind=engine))

    # determine interval_type from YYYY-MM[-DD][-HH]
    pieces = interval.split('-')
    pieces = [int(i) for i in pieces]
    if len(pieces) == 4:
        interval_type = 'hour'
    elif len(pieces) == 3:
        interval_type = 'day'
        pieces.append(0)
    elif len(pieces) == 2:
        interval_type = 'month'
        pieces.append(1)
        pieces.append(0)
    else:
        raise

    pg_interval = "%04d-%02d-%02d %02d:00:00" % tuple(pieces)
    print 'reporting interval %s (%s)' % (pg_interval, interval_type)

    # Read aggregates and write to traffic db
    for category_cls in traffic_categories:
        now = datetime.datetime.now()
        print '*** %s - %s - %s' % (category_cls.__name__, interval, now)
        data = get_aggregate(interval, category_cls)
        len_data = len(data)
        step = max(len_data / 5, 100)
        for i, (name, (uniques, pageviews)) in enumerate(data.iteritems()):
            try:
                for n in tup(name):
                    unicode(n)
            except UnicodeDecodeError:
                print '%s - %s - %s - %s' % (category_cls.__name__, name,
                                             uniques, pageviews)
                continue

            if i % step == 0:
                now = datetime.datetime.now()
                print '%s - %s - %s/%s - %s' % (interval, category_cls.__name__,
                                                i, len_data, now)

            kw = {'date': pg_interval, 'interval': interval_type,
                  'unique_count': uniques, 'pageview_count': pageviews}
            kw.update(_name_to_kw(category_cls, name))
            r = category_cls(**kw)
            Session.merge(r)
            Session.commit()
    Session.remove()
    now = datetime.datetime.now()
    print 'finished reporting %s (%s) - %s' % (pg_interval, interval_type, now)


def process_pixel_log(log_path, fast=False):
    """Process an hourly pixel log file.

    Extract data from raw hourly log and aggregate it and report it. Also
    depending on the specific date and options, aggregate and report the day
    and month. Setting fast=True is appropriate for backfilling as it
    eliminates reduntant steps.

    """

    if log_path.endswith('/*'):
        log_dir = log_path[:-len('/*')]
        date_fields = os.path.basename(log_dir).split('.', 1)[0].split('-')
    else:
        date_fields = os.path.basename(log_path).split('.', 1)[0].split('-')
    year, month, day, hour = (int(i) for i in date_fields)
    hour_date = '%s-%02d-%02d-%02d' % (year, month, day, hour)
    day_date = '%s-%02d-%02d' % (year, month, day)
    month_date = '%s-%02d' % (year, month)

    # All logs from this day use the same jobflow
    jobflow_name = 'Traffic Processing (%s)' % day_date

    output_path = os.path.join(PROCESSED_DIR, 'hour', hour_date)
    extract_hour(emr_connection, jobflow_name, log_path, output_path,
                 log_uri=AWS_LOG_DIR)

    input_path = os.path.join(PROCESSED_DIR, 'hour', hour_date)
    output_path = os.path.join(AGGREGATE_DIR, hour_date)
    aggregate_interval(emr_connection, jobflow_name, input_path, output_path,
                       log_uri=AWS_LOG_DIR)
    if not fast:
        report_interval(hour_date)

    if hour == 23 or (not fast and (hour == 0 or hour % 4 == 3)):
        # Don't aggregate and report day on every hour
        input_path = os.path.join(PROCESSED_DIR, 'hour', '%s-*' % day_date)
        output_path = os.path.join(AGGREGATE_DIR, day_date)
        aggregate_interval(emr_connection, jobflow_name, input_path,
                           output_path, log_uri=AWS_LOG_DIR)
        if not fast:
            report_interval(day_date)

    if hour == 23:
        # Special tasks for final hour of the day
        input_path = os.path.join(PROCESSED_DIR, 'hour', '%s-*' % day_date)
        output_path = os.path.join(PROCESSED_DIR, 'day', day_date)
        coalesce_interval(emr_connection, jobflow_name, input_path,
                          output_path, log_uri=AWS_LOG_DIR)
        terminate_jobflow(emr_connection, jobflow_name)

        if not fast:
            aggregate_month(month_date)
            report_interval(month_date)


def aggregate_month(month_date):
    jobflow_name = 'Traffic Processing (%s)' % month_date
    input_path = os.path.join(PROCESSED_DIR, 'day', '%s-*' % month_date)
    output_path = os.path.join(AGGREGATE_DIR, month_date)
    aggregate_interval(emr_connection, jobflow_name, input_path, output_path,
                       log_uri=AWS_LOG_DIR, slave_instance_type='m2.2xlarge')
    terminate_jobflow(emr_connection, jobflow_name)


def process_month_hours(month_date, start_hour=0, days=None):
    """Process hourly logs from entire month.

    Complete monthly backfill requires running [verify_month_inputs,]
    process_month_hours, aggregate_month, [verify_month_outputs,] and
    report_entire_month.

    """

    year, month = month_date.split('-')
    year, month = int(year), int(month)

    days = days or xrange(1, calendar.monthrange(year, month)[1] + 1)
    hours = xrange(start_hour, 24)

    for day in days:
        for hour in hours:
            hour_date = '%04d-%02d-%02d-%02d' % (year, month, day, hour)
            log_path = os.path.join(RAW_LOG_DIR, '%s.log.gz' % hour_date)
            if not s3_key_exists(s3_connection, log_path):
                log_path = os.path.join(RAW_LOG_DIR, '%s.log.bz2' % hour_date)
                if not s3_key_exists(s3_connection, log_path):
                    print 'Missing log for %s' % hour_date
                    continue
            print 'Processing %s' % log_path
            process_pixel_log(log_path, fast=True)
        hours = xrange(24)


def report_entire_month(month_date, start_hour=0, start_day=1):
    """Report all hours and days from month."""
    year, month = month_date.split('-')
    year, month = int(year), int(month)
    hours = xrange(start_hour, 24)

    for day in xrange(start_day, calendar.monthrange(year, month)[1] + 1):
        for hour in hours:
            hour_date = '%04d-%02d-%02d-%02d' % (year, month, day, hour)
            try:
                report_interval(hour_date, background=False)
            except ValueError:
                print 'Failed for %s' % hour_date
                continue
        hours = xrange(24)
        day_date = '%04d-%02d-%02d' % (year, month, day)
        try:
            report_interval(day_date, background=False)
        except ValueError:
            print 'Failed for %s' % day_date
            continue
    report_interval(month_date, background=False)


def verify_month_outputs(month_date):
    """Check existance of all hour, day, month aggregates for month_date."""
    year, month = month_date.split('-')
    year, month = int(year), int(month)
    missing = []

    for day in xrange(1, calendar.monthrange(year, month)[1] + 1):
        for hour in xrange(24):
            hour_date = '%04d-%02d-%02d-%02d' % (year, month, day, hour)
            for category_cls in traffic_categories:
                for d in [AGGREGATE_DIR, os.path.join(PROCESSED_DIR, 'hour')]:
                    path = _get_processed_path(d, hour_date, category_cls,
                                               'part-r-00000')
                    if not s3_key_exists(s3_connection, path):
                        missing.append(hour_date)

        day_date = '%04d-%02d-%02d' % (year, month, day)
        for category_cls in traffic_categories:
            for d in [AGGREGATE_DIR, os.path.join(PROCESSED_DIR, 'day')]:
                path = _get_processed_path(d, day_date, category_cls,
                                           'part-r-00000')
                if not s3_key_exists(s3_connection, path):
                    missing.append(day_date)

    month_date = '%04d-%02d' % (year, month)
    for c in traffic_categories:
        path = _get_processed_path(AGGREGATE_DIR, month_date, category_cls,
                                   'part-r-00000')
        if not s3_key_exists(s3_connection, path):
            missing.append(month_date)

    for d in sorted(list(set(missing))):
        print d


def verify_month_inputs(month_date):
    """Check existance of all hourly traffic logs for month_date."""
    year, month = month_date.split('-')
    year, month = int(year), int(month)
    missing = []

    for day in xrange(1, calendar.monthrange(year, month)[1] + 1):
        for hour in xrange(24):
            hour_date = '%04d-%02d-%02d-%02d' % (year, month, day, hour)
            log_path = os.path.join(RAW_LOG_DIR, '%s.log.gz' % hour_date)
            if not s3_key_exists(s3_connection, log_path):
                log_path = os.path.join(RAW_LOG_DIR, '%s.log.bz2' % hour_date)
                if not s3_key_exists(s3_connection, log_path):
                    missing.append(hour_date)

    for d in missing:
        print d


def process_hour(hour_date):
    """Process hour_date's traffic.

    Can't fire at the very start of an hour because it takes time to bzip and
    upload the file to S3. Check the bucket for the file and sleep if it
    doesn't exist.

    """

    SLEEPTIME = 180

    log_dir = os.path.join(RAW_LOG_DIR, hour_date)
    files_missing = [os.path.join(log_dir, '%s.log.bz2' % h)
                     for h in g.TRAFFIC_LOG_HOSTS]
    files_missing = [f for f in files_missing
                       if not s3_key_exists(s3_connection, f)]

    while files_missing:
        print 'Missing log(s) %s, sleeping' % files_missing
        sleep(SLEEPTIME)
        files_missing = [f for f in files_missing
                           if not s3_key_exists(s3_connection, f)]
    process_pixel_log(os.path.join(log_dir, '*'))

########NEW FILE########
__FILENAME__ = translation
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import json
import os
import token
import tokenize

from babel.messages.extract import extract_javascript
from cStringIO import StringIO

import babel.messages.frontend
import babel.messages.pofile
import pylons

from pylons.i18n.translation import translation, LanguageError, NullTranslations

try:
    import reddit_i18n
except ImportError:
    I18N_PATH = ''
else:
    I18N_PATH = os.path.dirname(reddit_i18n.__file__)

# Different from the default lang (as defined in the ini file)
# Source language is what is in the source code
SOURCE_LANG = 'en'


def _get_translator(lang, graceful_fail=False, **kwargs):
    """Utility method to get a valid translator object from a language name"""
    if not isinstance(lang, list):
        lang = [lang]
    try:
        translator = translation(pylons.config['pylons.package'], I18N_PATH,
                                 languages=lang, **kwargs)
    except IOError, ioe:
        if graceful_fail:
            translator = NullTranslations()
        else:
            raise LanguageError('IOError: %s' % ioe)
    translator.pylons_lang = lang
    return translator


def set_lang(lang, graceful_fail=False, fallback_lang=None, **kwargs):
    """Set the i18n language used"""
    registry = pylons.request.environ['paste.registry']
    if not lang:
        registry.replace(pylons.translator, NullTranslations())
    else:
        translator = _get_translator(lang, graceful_fail = graceful_fail, **kwargs)
        base_lang, is_dialect, dialect = lang.partition("-")
        if is_dialect:
            try:
                base_translator = _get_translator(base_lang)
            except LanguageError:
                pass
            else:
                translator.add_fallback(base_translator)
        if fallback_lang:
            fallback_translator = _get_translator(fallback_lang,
                                                  graceful_fail=True)
            translator.add_fallback(fallback_translator)
        registry.replace(pylons.translator, translator)


def load_data(lang_path, domain=None, extension='data'):
    if domain is None:
        domain = pylons.config['pylons.package']
    filename = os.path.join(lang_path, domain + '.' + extension)
    with open(filename) as datafile:
        data = json.load(datafile)
    return data


def iter_langs(base_path=I18N_PATH):
    if base_path:
        # sorted() so that get_active_langs can check completion
        # data on "base" languages of a dialect
        for lang in sorted(os.listdir(base_path)):
            full_path = os.path.join(base_path, lang, 'LC_MESSAGES')
            if os.path.isdir(full_path):
                yield lang, full_path


def get_active_langs(path=I18N_PATH, default_lang='en'):
    trans = []
    trans_name = {}
    completions = {}
    for lang, lang_path in iter_langs(path):
        data = load_data(lang_path)
        name = [data['name'], '']
        if data['_is_enabled'] and lang != default_lang:
            trans.append(lang)
            completion = float(data['num_completed']) / float(data['num_total'])
            completions[lang] = completion
            # This relies on iter_langs hitting the base_lang first
            base_lang, is_dialect, dialect = lang.partition("-")
            if is_dialect:
                if base_lang == SOURCE_LANG:
                    # Source language has to be 100% complete
                    base_completion = 1.0
                else:
                    base_completion = completions.get(base_lang, 0)
                completion = max(completion, base_completion)
            if completion < .5:
                name[1] = ' (*)'
        trans_name[lang] = name
    trans.sort()
    # insert the default language at the top of the list
    trans.insert(0, default_lang)
    if default_lang not in trans_name:
        trans_name[default_lang] = default_lang
    return trans, trans_name


def get_catalog(lang):
    """Return a Catalog object given the language code."""
    path = os.path.join(I18N_PATH, lang, "LC_MESSAGES", "r2.po")
    with open(path, "r") as f:
        return babel.messages.pofile.read_po(f)


def validate_plural_forms(plural_forms_str):
    """Ensure the gettext plural forms expression supplied is valid."""

    # this code is taken from the python stdlib; gettext.py:c2py
    tokens = tokenize.generate_tokens(StringIO(plural_forms_str).readline)

    try:
        danger = [x for x in tokens if x[0] == token.NAME and x[1] != 'n']
    except tokenize.TokenError:
        raise ValueError, \
              'plural forms expression error, maybe unbalanced parenthesis'
    else:
        if danger:
            raise ValueError, 'plural forms expression could be dangerous'


def extract_javascript_msgids(source):
    """Return message ids of translateable strings in JS source."""

    extracted = extract_javascript(
        fileobj=StringIO(source),
        keywords={
            "_": None,
            "P_": (1, 2),
            "N_": None,
            "NP_": (1, 2),
        },
        comment_tags={},
        options={},
    )

    return [msg_id for line, func, msg_id, comments in extracted]

########NEW FILE########
__FILENAME__ = trending
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import re

from pylons import g

from r2.models.keyvalue import NamedGlobals
from r2.models import NotFound, Subreddit, Thing

_SUBREDDIT_RE = re.compile(r'/r/(\w+)')
TRENDING_SUBREDDITS_KEY = 'trending_subreddits'


def get_trending_subreddits():
    return NamedGlobals.get(TRENDING_SUBREDDITS_KEY, None)


def update_trending_subreddits():
    try:
        trending_sr = Subreddit._by_name(g.config['trending_sr'])
    except NotFound:
        g.log.info("Unknown trending subreddit %r or trending_sr config "
                   "not set. Not updating.", g.config['trending_sr'])
        return

    link = _get_newest_link(trending_sr)
    if not link:
        g.log.info("Unable to find active link in subreddit %r. Not updating.",
                   g.config['trending_sr'])
        return

    subreddit_names = _SUBREDDIT_RE.findall(link.title)
    trending_data = {
        'subreddit_names': subreddit_names,
        'permalink': link.make_permalink(trending_sr),
        'link_id': link._id,
    }
    NamedGlobals.set(TRENDING_SUBREDDITS_KEY, trending_data)
    g.log.debug("Trending subreddit data set to %r", trending_data)


def _get_newest_link(sr):
    for fullname in sr.get_links('new', 'all'):
        link = Thing._by_fullname(fullname, data=True)
        if not link._spam and not link._deleted:
            return link

    return None

########NEW FILE########
__FILENAME__ = http_utils
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import pytz
from datetime import datetime

DATE_RFC822 = '%a, %d %b %Y %H:%M:%S %Z'
DATE_RFC850 = '%A, %d-%b-%y %H:%M:%S %Z'
DATE_RFC3339 = "%Y-%m-%dT%H:%M:%SZ"
DATE_ANSI = '%a %b %d %H:%M:%S %Y'

def read_http_date(date_str):
    try:
        date = datetime.strptime(date_str, DATE_RFC822)
    except ValueError:
        try:
            date = datetime.strptime(date_str, DATE_RFC850)
        except ValueError:
            try:
                date = datetime.strptime(date_str, DATE_ANSI)
            except ValueError:
                return None
    date = date.replace(tzinfo = pytz.timezone('GMT'))
    return date

def http_date_str(date):
    date = date.astimezone(pytz.timezone('GMT'))
    return date.strftime(DATE_RFC822)

def rfc3339_date_str(date):
    return date.strftime(DATE_RFC3339)

########NEW FILE########
__FILENAME__ = thing_utils
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from datetime import datetime
from utils import tup
import pytz

def make_last_modified():
    last_modified = datetime.now(pytz.timezone('GMT'))
    last_modified = last_modified.replace(microsecond = 0)
    return last_modified

def last_modified_key(thing, action):
    return 'last_%s_%s' % (str(action), thing._fullname)

def last_modified_date(thing, action, set_if_empty = True):
    """Returns the date that should be sent as the last-modified header."""
    from pylons import g
    cache = g.permacache

    key = last_modified_key(thing, action)
    last_modified = cache.get(key)
    if not last_modified and set_if_empty:
        #if there is no last_modified, add one
        last_modified = make_last_modified()
        cache.set(key, last_modified)
    return last_modified

def set_last_modified(thing, action):
    from pylons import g
    key = last_modified_key(thing, action)
    g.permacache.set(key, make_last_modified())

def last_modified_multi(things, action):
    from pylons import g
    cache = g.permacache

    things = tup(things)
    keys = dict((last_modified_key(thing, action), thing) for thing in things)

    last_modified = cache.get_multi(keys.keys())
    return dict((keys[k], v) for k, v in last_modified.iteritems())

########NEW FILE########
__FILENAME__ = utils
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import os
import base64
import traceback
import ConfigParser
import codecs
import itertools

from babel.dates import TIMEDELTA_UNITS
from urllib import unquote_plus
from urllib2 import urlopen, Request
from urlparse import urlparse, urlunparse
import signal
from copy import deepcopy
import cPickle as pickle
import re, math, random
import boto
from decimal import Decimal

from BeautifulSoup import BeautifulSoup, SoupStrainer

from time import sleep
from datetime import date, datetime, timedelta
from pylons import c, g
from pylons.i18n import ungettext, _
from r2.lib.filters import _force_unicode, _force_utf8
from mako.filters import url_escape
from r2.lib.contrib import ipaddress
from r2.lib.require import require, require_split, RequirementException
import snudown

from r2.lib.utils._utils import *

iters = (list, tuple, set)

def randstr(length,
            alphabet='abcdefghijklmnopqrstuvwxyz0123456789'):
    """Return a string made up of random chars from alphabet."""
    return ''.join(random.choice(alphabet) for _ in xrange(length))

class Storage(dict):
    """
    A Storage object is like a dictionary except `obj.foo` can be used
    in addition to `obj['foo']`.

        >>> o = storage(a=1)
        >>> o.a
        1
        >>> o['a']
        1
        >>> o.a = 2
        >>> o['a']
        2
        >>> del o.a
        >>> o.a
        Traceback (most recent call last):
            ...
        AttributeError: 'a'

    """
    def __getattr__(self, key):
        try:
            return self[key]
        except KeyError, k:
            raise AttributeError, k

    def __setattr__(self, key, value):
        self[key] = value

    def __delattr__(self, key):
        try:
            del self[key]
        except KeyError, k:
            raise AttributeError, k

    def __repr__(self):
        return '<Storage ' + dict.__repr__(self) + '>'

storage = Storage


class Enum(Storage):
    def __init__(self, *a):
        self.name = tuple(a)
        Storage.__init__(self, ((e, i) for i, e in enumerate(a)))
    def __contains__(self, item):
        if isinstance(item, int):
            return item in self.values()
        else:
            return Storage.__contains__(self, item)


class Results():
    def __init__(self, sa_ResultProxy, build_fn, do_batch=False):
        self.rp = sa_ResultProxy
        self.fn = build_fn
        self.do_batch = do_batch

    @property
    def rowcount(self):
        return self.rp.rowcount

    def _fetch(self, res):
        if self.do_batch:
            return self.fn(res)
        else:
            return [self.fn(row) for row in res]

    def fetchall(self):
        return self._fetch(self.rp.fetchall())

    def fetchmany(self, n):
        rows = self._fetch(self.rp.fetchmany(n))
        if rows:
            return rows
        else:
            raise StopIteration

    def fetchone(self):
        row = self.rp.fetchone()
        if row:
            if self.do_batch:
                row = tup(row)
                return self.fn(row)[0]
            else:
                return self.fn(row)
        else:
            raise StopIteration

def strip_www(domain):
    if domain.count('.') >= 2 and domain.startswith("www."):
        return domain[4:]
    else:
        return domain

def is_subdomain(subdomain, base):
    """Check if a domain is equal to or a subdomain of a base domain."""
    return subdomain == base or (subdomain is not None and subdomain.endswith('.' + base))

r_base_url = re.compile("(?i)(?:.+?://)?(?:www[\d]*\.)?([^#]*[^#/])/?")
def base_url(url):
    res = r_base_url.findall(url)
    return (res and res[0]) or url

r_domain = re.compile("(?i)(?:.+?://)?(?:www[\d]*\.)?([^/:#?]*)")
def domain(s):
    """
        Takes a URL and returns the domain part, minus www., if
        present
    """
    res = r_domain.findall(s)
    domain = (res and res[0]) or s
    return domain.lower()

r_path_component = re.compile(".*?/(.*)")
def path_component(s):
    """
        takes a url http://www.foo.com/i/like/cheese and returns
        i/like/cheese
    """
    res = r_path_component.findall(base_url(s))
    return (res and res[0]) or s

def get_title(url):
    """Fetch the contents of url and try to extract the page's title."""
    if not url or not url.startswith(('http://', 'https://')):
        return None

    try:
        req = Request(url)
        if g.useragent:
            req.add_header('User-Agent', g.useragent)
        opener = urlopen(req, timeout=15)

        # determine the encoding of the response
        for param in opener.info().getplist():
            if param.startswith("charset="):
                param_name, sep, charset = param.partition("=")
                codec = codecs.getreader(charset)
                break
        else:
            codec = codecs.getreader("utf-8")

        with codec(opener, "ignore") as reader:
            # Attempt to find the title in the first 1kb
            data = reader.read(1024)
            title = extract_title(data)

            # Title not found in the first kb, try searching an additional 10kb
            if not title:
                data += reader.read(10240)
                title = extract_title(data)

        return title

    except:
        return None

def extract_title(data):
    """Try to extract the page title from a string of HTML.

    An og:title meta tag is preferred, but will fall back to using
    the <title> tag instead if one is not found. If using <title>,
    also attempts to trim off the site's name from the end.
    """
    bs = BeautifulSoup(data, convertEntities=BeautifulSoup.HTML_ENTITIES)
    if not bs or not bs.html.head:
        return
    head_soup = bs.html.head

    title = None

    # try to find an og:title meta tag to use
    og_title = (head_soup.find("meta", attrs={"property": "og:title"}) or
                head_soup.find("meta", attrs={"name": "og:title"}))
    if og_title:
        title = og_title.get("content")

    # if that failed, look for a <title> tag to use instead
    if not title and head_soup.title and head_soup.title.string:
        title = head_soup.title.string

        # remove end part that's likely to be the site's name
        # looks for last delimiter char between spaces in strings
        # delimiters: |, -, emdash, endash,
        #             left- and right-pointing double angle quotation marks
        reverse_title = title[::-1]
        to_trim = re.search(u'\s[\u00ab\u00bb\u2013\u2014|-]\s',
                            reverse_title,
                            flags=re.UNICODE)

        # only trim if it won't take off over half the title
        if to_trim and to_trim.end() < len(title) / 2:
            title = title[:-(to_trim.end())]

    if not title:
        return

    # get rid of extraneous whitespace in the title
    title = re.sub(r'\s+', ' ', title, flags=re.UNICODE)

    return title.encode('utf-8').strip()

VALID_SCHEMES = ('http', 'https', 'ftp', 'mailto')
valid_dns = re.compile('\A[-a-zA-Z0-9]+\Z')
def sanitize_url(url, require_scheme=False, valid_schemes=VALID_SCHEMES):
    """Validates that the url is of the form

    scheme://domain/path/to/content#anchor?cruft

    using the python built-in urlparse.  If the url fails to validate,
    returns None.  If no scheme is provided and 'require_scheme =
    False' is set, the url is returned with scheme 'http', provided it
    otherwise validates"""

    if not url:
        return None

    url = url.strip()
    if url.lower() == 'self':
        return url

    try:
        u = urlparse(url)
        # first pass: make sure a scheme has been specified
        if not require_scheme and not u.scheme:
            url = 'http://' + url
            u = urlparse(url)
    except ValueError:
        return None

    if not u.scheme:
        return None
    if valid_schemes is not None and u.scheme not in valid_schemes:
        return None

    # if there is a scheme and no hostname, it is a bad url.
    if not u.hostname:
        return None
    if u.username is not None or u.password is not None:
        return None

    try:
        idna_hostname = u.hostname.encode('idna')
    except TypeError as e:
        g.log.warning("Bad hostname given [%r]: %s", u.hostname, e)
        raise
    except UnicodeError:
        return None

    for label in idna_hostname.split('.'):
        if not re.match(valid_dns, label):
            return None

    if idna_hostname != u.hostname:
        url = urlunparse((u[0], idna_hostname, u[2], u[3], u[4], u[5]))
    return url

def trunc_string(text, length):
    return text[0:length]+'...' if len(text)>length else text

# Truncate a time to a certain number of minutes
# e.g, trunc_time(5:52, 30) == 5:30
def trunc_time(time, mins, hours=None):
    if hours is not None:
        if hours < 1 or hours > 60:
            raise ValueError("Hours %d is weird" % mins)
        time = time.replace(hour = hours * (time.hour / hours))

    if mins < 1 or mins > 60:
        raise ValueError("Mins %d is weird" % mins)

    return time.replace(minute = mins * (time.minute / mins),
                        second = 0,
                        microsecond = 0)

def long_datetime(datetime):
    return datetime.astimezone(g.tz).ctime() + " " + str(g.tz)

def median(l):
    if l:
        s = sorted(l)
        i = len(s) / 2
        return s[i]

def query_string(dict):
    pairs = []
    for k,v in dict.iteritems():
        if v is not None:
            try:
                k = url_escape(_force_unicode(k))
                v = url_escape(_force_unicode(v))
                pairs.append(k + '=' + v)
            except UnicodeDecodeError:
                continue
    if pairs:
        return '?' + '&'.join(pairs)
    else:
        return ''

class UrlParser(object):
    """
    Wrapper for urlparse and urlunparse for making changes to urls.
    All attributes present on the tuple-like object returned by
    urlparse are present on this class, and are setable, with the
    exception of netloc, which is instead treated via a getter method
    as a concatenation of hostname and port.

    Unlike urlparse, this class allows the query parameters to be
    converted to a dictionary via the query_dict method (and
    correspondingly updated vi update_query).  The extension of the
    path can also be set and queried.

    The class also contains reddit-specific functions for setting,
    checking, and getting a path's subreddit.  It also can convert
    paths between in-frame and out of frame cname'd forms.

    """

    __slots__ = ['scheme', 'path', 'params', 'query',
                 'fragment', 'username', 'password', 'hostname',
                 'port', '_url_updates', '_orig_url', '_query_dict']

    valid_schemes = ('http', 'https', 'ftp', 'mailto')
    cname_get = "cnameframe"

    def __init__(self, url):
        u = urlparse(url)
        for s in self.__slots__:
            if hasattr(u, s):
                setattr(self, s, getattr(u, s))
        self._url_updates = {}
        self._orig_url    = url
        self._query_dict  = None

    def update_query(self, **updates):
        """
        Can be used instead of self.query_dict.update() to add/change
        query params in situations where the original contents are not
        required.
        """
        self._url_updates.update(updates)

    @property
    def query_dict(self):
        """
        Parses the `params' attribute of the original urlparse and
        generates a dictionary where both the keys and values have
        been url_unescape'd.  Any updates or changes to the resulting
        dict will be reflected in the updated query params
        """
        if self._query_dict is None:
            def _split(param):
                p = param.split('=')
                return (unquote_plus(p[0]),
                        unquote_plus('='.join(p[1:])))
            self._query_dict = dict(_split(p) for p in self.query.split('&')
                                    if p)
        return self._query_dict

    def path_extension(self):
        """
        Fetches the current extension of the path.
        """
        return self.path.split('/')[-1].split('.')[-1]

    def set_extension(self, extension):
        """
        Changes the extension of the path to the provided value (the
        "." should not be included in the extension as a "." is
        provided)
        """
        pieces = self.path.split('/')
        dirs = pieces[:-1]
        base = pieces[-1].split('.')
        base = '.'.join(base[:-1] if len(base) > 1 else base)
        if extension:
            base += '.' + extension
        dirs.append(base)
        self.path =  '/'.join(dirs)
        return self


    def unparse(self):
        """
        Converts the url back to a string, applying all updates made
        to the feilds thereof.

        Note: if a host name has been added and none was present
        before, will enforce scheme -> "http" unless otherwise
        specified.  Double-slashes are removed from the resultant
        path, and the query string is reconstructed only if the
        query_dict has been modified/updated.
        """
        # only parse the query params if there is an update dict
        q = self.query
        if self._url_updates or self._query_dict is not None:
            q = self._query_dict or self.query_dict
            q.update(self._url_updates)
            q = query_string(q).lstrip('?')

        # make sure the port is not doubly specified
        if getattr(self, 'port', None) and ":" in self.hostname:
            self.hostname = self.hostname.split(':')[0]

        # if there is a netloc, there had better be a scheme
        if self.netloc and not self.scheme:
            self.scheme = "http"

        return urlunparse((self.scheme, self.netloc,
                           self.path.replace('//', '/'),
                           self.params, q, self.fragment))

    def path_has_subreddit(self):
        """
        utility method for checking if the path starts with a
        subreddit specifier (namely /r/ or /subreddits/).
        """
        return self.path.startswith(('/r/', '/subreddits/', '/reddits/'))

    def get_subreddit(self):
        """checks if the current url refers to a subreddit and returns
        that subreddit object.  The cases here are:

          * the hostname is unset or is g.domain, in which case it
            looks for /r/XXXX or /subreddits.  The default in this case
            is Default.
          * the hostname is a cname to a known subreddit.

        On failure to find a subreddit, returns None.
        """
        from pylons import g
        from r2.models import Subreddit, Sub, NotFound, DefaultSR
        try:
            if (not self.hostname or
                    is_subdomain(self.hostname, g.domain) or
                    self.hostname.startswith(g.domain)):
                if self.path.startswith('/r/'):
                    return Subreddit._by_name(self.path.split('/')[2])
                elif self.path.startswith(('/subreddits/', '/reddits/')):
                    return Sub
                else:
                    return DefaultSR()
            elif self.hostname:
                return Subreddit._by_domain(self.hostname)
        except NotFound:
            pass
        return None

    def is_reddit_url(self, subreddit = None):
        """utility method for seeing if the url is associated with
        reddit as we don't necessarily want to mangle non-reddit
        domains

        returns true only if hostname is nonexistant, a subdomain of
        g.domain, or a subdomain of the provided subreddit's cname.
        """
        from pylons import g
        subdomain = (
            not self.hostname or
            is_subdomain(self.hostname, g.domain) or
            (subreddit and subreddit.domain and
                is_subdomain(self.hostname, subreddit.domain))
        )
        if not subdomain or not self.hostname or not g.offsite_subdomains:
            return subdomain
        return not any(
            self.hostname.startswith(subdomain + '.')
            for subdomain in g.offsite_subdomains
        )

    def path_add_subreddit(self, subreddit):
        """
        Adds the subreddit's path to the path if another subreddit's
        prefix is not already present.
        """
        if not (self.path_has_subreddit()
                or self.path.startswith(subreddit.user_path)):
            self.path = (subreddit.user_path + self.path)
        return self

    @property
    def netloc(self):
        """
        Getter method which returns the hostname:port, or empty string
        if no hostname is present.
        """
        if not self.hostname:
            return ""
        elif getattr(self, "port", None):
            return self.hostname + ":" + str(self.port)
        return self.hostname

    def mk_cname(self, require_frame = True, subreddit = None, port = None):
        """
        Converts a ?cnameframe url into the corresponding cnamed
        domain if applicable.  Useful for frame-busting on redirect.
        """

        # make sure the url is indeed in a frame
        if require_frame and not self.query_dict.has_key(self.cname_get):
            return self

        # fetch the subreddit and make sure it
        subreddit = subreddit or self.get_subreddit()
        if subreddit and subreddit.domain:

            # no guarantee there was a scheme
            self.scheme = self.scheme or "http"

            # update the domain (preserving the port)
            self.hostname = subreddit.domain
            self.port = getattr(self, 'port', None) or port

            # and remove any cnameframe GET parameters
            if self.query_dict.has_key(self.cname_get):
                del self._query_dict[self.cname_get]

            # remove the subreddit reference
            self.path = lstrips(self.path, subreddit.path)
            if not self.path.startswith('/'):
                self.path = '/' + self.path

        return self

    def is_in_frame(self):
        """
        Checks if the url is in a frame by determining if
        cls.cname_get is present.
        """
        return self.query_dict.has_key(self.cname_get)

    def put_in_frame(self):
        """
        Adds the cls.cname_get get parameter to the query string.
        """
        self.update_query(**{self.cname_get:random.random()})

    def __repr__(self):
        return "<URL %s>" % repr(self.unparse())

    def domain_permutations(self, fragments=False, subdomains=True):
        """
          Takes a domain like `www.reddit.com`, and returns a list of ways
          that a user might search for it, like:
          * www
          * reddit
          * com
          * www.reddit.com
          * reddit.com
          * com
        """
        ret = set()
        if self.hostname:
            r = self.hostname.split('.')

            if subdomains:
                for x in xrange(len(r)-1):
                    ret.add('.'.join(r[x:len(r)]))

            if fragments:
                for x in r:
                    ret.add(x)

        return ret

    @classmethod
    def base_url(cls, url):
        u = cls(url)

        # strip off any www and lowercase the hostname:
        netloc = strip_www(u.netloc.lower())

        # http://code.google.com/web/ajaxcrawling/docs/specification.html
        fragment = u.fragment if u.fragment.startswith("!") else ""

        return urlunparse((u.scheme.lower(), netloc,
                           u.path, u.params, u.query, fragment))


def pload(fname, default = None):
    "Load a pickled object from a file"
    try:
        f = file(fname, 'r')
        d = pickle.load(f)
    except IOError:
        d = default
    else:
        f.close()
    return d

def psave(fname, d):
    "Save a pickled object into a file"
    f = file(fname, 'w')
    pickle.dump(d, f)
    f.close()

def unicode_safe(res):
    try:
        return str(res)
    except UnicodeEncodeError:
        try:
            return unicode(res).encode('utf-8')
        except UnicodeEncodeError:
            return res.decode('utf-8').encode('utf-8')

def decompose_fullname(fullname):
    """
        decompose_fullname("t3_e4fa") ->
            (Thing, 3, 658918)
    """
    from r2.lib.db.thing import Thing,Relation
    if fullname[0] == 't':
        type_class = Thing
    elif fullname[0] == 'r':
        type_class = Relation

    type_id36, thing_id36 = fullname[1:].split('_')

    type_id = int(type_id36,36)
    id      = int(thing_id36,36)

    return (type_class, type_id, id)

def cols(lst, ncols):
    """divides a list into columns, and returns the
    rows. e.g. cols('abcdef', 2) returns (('a', 'd'), ('b', 'e'), ('c',
    'f'))"""
    nrows = int(math.ceil(1.*len(lst) / ncols))
    lst = lst + [None for i in range(len(lst), nrows*ncols)]
    cols = [lst[i:i+nrows] for i in range(0, nrows*ncols, nrows)]
    rows = zip(*cols)
    rows = [filter(lambda x: x is not None, r) for r in rows]
    return rows

def fetch_things(t_class,since,until,batch_fn=None,
                 *query_params, **extra_query_dict):
    """
        Simple utility function to fetch all Things of class t_class
        (spam or not, but not deleted) that were created from 'since'
        to 'until'
    """

    from r2.lib.db.operators import asc

    if not batch_fn:
        batch_fn = lambda x: x

    query_params = ([t_class.c._date >= since,
                     t_class.c._date <  until,
                     t_class.c._spam == (True,False)]
                    + list(query_params))
    query_dict   = {'sort':  asc('_date'),
                    'limit': 100,
                    'data':  True}
    query_dict.update(extra_query_dict)

    q = t_class._query(*query_params,
                        **query_dict)

    orig_rules = deepcopy(q._rules)

    things = list(q)
    while things:
        things = batch_fn(things)
        for t in things:
            yield t
        q._rules = deepcopy(orig_rules)
        q._after(t)
        things = list(q)

def fetch_things2(query, chunk_size = 100, batch_fn = None, chunks = False):
    """Incrementally run query with a limit of chunk_size until there are
    no results left. batch_fn transforms the results for each chunk
    before returning."""

    assert query._sort, "you must specify the sort order in your query!"

    orig_rules = deepcopy(query._rules)
    query._limit = chunk_size
    items = list(query)
    done = False
    while items and not done:
        #don't need to query again at the bottom if we didn't get enough
        if len(items) < chunk_size:
            done = True

        after = items[-1]

        if batch_fn:
            items = batch_fn(items)

        if chunks:
            yield items
        else:
            for i in items:
                yield i

        if not done:
            query._rules = deepcopy(orig_rules)
            query._after(after)
            items = list(query)

def fix_if_broken(thing, delete = True, fudge_links = False):
    from r2.models import Link, Comment, Subreddit, Message

    # the minimum set of attributes that are required
    attrs = dict((cls, cls._essentials)
                 for cls
                 in (Link, Comment, Subreddit, Message))

    if thing.__class__ not in attrs:
        raise TypeError

    tried_loading = False
    for attr in attrs[thing.__class__]:
        try:
            # try to retrieve the attribute
            getattr(thing, attr)
        except AttributeError:
            # that failed; let's explicitly load it and try again

            if not tried_loading:
                tried_loading = True
                thing._load()

            try:
                getattr(thing, attr)
            except AttributeError:
                if not delete:
                    raise
                if isinstance(thing, Link) and fudge_links:
                    if attr == "sr_id":
                        thing.sr_id = 6
                        print "Fudging %s.sr_id to %d" % (thing._fullname,
                                                          thing.sr_id)
                    elif attr == "author_id":
                        thing.author_id = 8244672
                        print "Fudging %s.author_id to %d" % (thing._fullname,
                                                              thing.author_id)
                    else:
                        print "Got weird attr %s; can't fudge" % attr

                if not thing._deleted:
                    print "%s is missing %r, deleting" % (thing._fullname, attr)
                    thing._deleted = True

                thing._commit()

                if not fudge_links:
                    break


def find_recent_broken_things(from_time = None, to_time = None,
                              delete = False):
    """
        Occasionally (usually during app-server crashes), Things will
        be partially written out to the database. Things missing data
        attributes break the contract for these things, which often
        breaks various pages. This function hunts for and destroys
        them as appropriate.
    """
    from r2.models import Link, Comment
    from r2.lib.db.operators import desc
    from pylons import g

    from_time = from_time or timeago('1 hour')
    to_time = to_time or datetime.now(g.tz)

    for cls in (Link, Comment):
        q = cls._query(cls.c._date > from_time,
                       cls.c._date < to_time,
                       data=True,
                       sort=desc('_date'))
        for thing in fetch_things2(q):
            fix_if_broken(thing, delete = delete)


def timeit(func):
    "Run some function, and return (RunTimeInSeconds,Result)"
    before=time.time()
    res=func()
    return (time.time()-before,res)
def lineno():
    "Returns the current line number in our program."
    import inspect
    print "%s\t%s" % (datetime.now(),inspect.currentframe().f_back.f_lineno)

def IteratorFilter(iterator, fn):
    for x in iterator:
        if fn(x):
            yield x

def UniqueIterator(iterator, key = lambda x: x):
    """
    Takes an iterator and returns an iterator that returns only the
    first occurence of each entry
    """
    so_far = set()
    def no_dups(x):
        k = key(x)
        if k in so_far:
            return False
        else:
            so_far.add(k)
            return True

    return IteratorFilter(iterator, no_dups)

def safe_eval_str(unsafe_str):
    return unsafe_str.replace('\\x3d', '=').replace('\\x26', '&')

rx_whitespace = re.compile('\s+', re.UNICODE)
rx_notsafe = re.compile('\W+', re.UNICODE)
rx_underscore = re.compile('_+', re.UNICODE)
def title_to_url(title, max_length = 50):
    """Takes a string and makes it suitable for use in URLs"""
    title = _force_unicode(title)           #make sure the title is unicode
    title = rx_whitespace.sub('_', title)   #remove whitespace
    title = rx_notsafe.sub('', title)       #remove non-printables
    title = rx_underscore.sub('_', title)   #remove double underscores
    title = title.strip('_')                #remove trailing underscores
    title = title.lower()                   #lowercase the title

    if len(title) > max_length:
        #truncate to nearest word
        title = title[:max_length]
        last_word = title.rfind('_')
        if (last_word > 0):
            title = title[:last_word]
    return title or "_"

def dbg(s):
    import sys
    sys.stderr.write('%s\n' % (s,))

def trace(fn):
    def new_fn(*a,**kw):
        ret = fn(*a,**kw)
        dbg("Fn: %s; a=%s; kw=%s\nRet: %s"
            % (fn,a,kw,ret))
        return ret
    return new_fn

def common_subdomain(domain1, domain2):
    if not domain1 or not domain2:
        return ""
    domain1 = domain1.split(":")[0]
    domain2 = domain2.split(":")[0]
    if len(domain1) > len(domain2):
        domain1, domain2 = domain2, domain1

    if domain1 == domain2:
        return domain1
    else:
        dom = domain1.split(".")
        for i in range(len(dom), 1, -1):
            d = '.'.join(dom[-i:])
            if domain2.endswith(d):
                return d
    return ""


def url_links_builder(url, exclude=None, num=None, after=None, reverse=None,
                      count=None, public_srs_only=False):
    from r2.lib.template_helpers import add_sr
    from r2.models import IDBuilder, Link, NotFound, Subreddit
    from operator import attrgetter

    if url.startswith('/'):
        url = add_sr(url, force_hostname=True)

    try:
        links = Link._by_url(url, None)
    except NotFound:
        links = []

    links = [ link for link in links
                   if link._fullname != exclude ]

    if public_srs_only and not c.user_is_admin:
        subreddits = Subreddit._byID([link.sr_id for link in links], data=True)
        links = [link for link in links
                 if subreddits[link.sr_id].type != "private"]

    links.sort(key=attrgetter('num_comments'), reverse=True)

    # don't show removed links in duplicates unless admin or mod
    # or unless it's your own post
    def include_link(link):
        return (not link._spam or
                (c.user_is_loggedin and
                    (link.author_id == c.user._id or
                        c.user_is_admin or
                        link.subreddit.is_moderator(c.user))))

    builder = IDBuilder([link._fullname for link in links], skip=True,
                        keep_fn=include_link, num=num, after=after,
                        reverse=reverse, count=count)

    return builder

class TimeoutFunctionException(Exception):
    pass

class TimeoutFunction:
    """Force an operation to timeout after N seconds. Works with POSIX
       signals, so it's not safe to use in a multi-treaded environment"""
    def __init__(self, function, timeout):
        self.timeout = timeout
        self.function = function

    def handle_timeout(self, signum, frame):
        raise TimeoutFunctionException()

    def __call__(self, *args, **kwargs):
        # can only be called from the main thread
        old = signal.signal(signal.SIGALRM, self.handle_timeout)
        signal.alarm(self.timeout)
        try:
            result = self.function(*args, **kwargs)
        finally:
            signal.alarm(0)
            signal.signal(signal.SIGALRM, old)
        return result

def make_offset_date(start_date, interval, future = True,
                     business_days = False):
    """
    Generates a date in the future or past "interval" days from start_date.

    Can optionally give weekends no weight in the calculation if
    "business_days" is set to true.
    """
    if interval is not None:
        interval = int(interval)
        if business_days:
            weeks = interval / 7
            dow = start_date.weekday()
            if future:
                future_dow = (dow + interval) % 7
                if dow > future_dow or future_dow > 4:
                    weeks += 1
            else:
                future_dow = (dow - interval) % 7
                if dow < future_dow or future_dow > 4:
                    weeks += 1
            interval += 2 * weeks;
        if future:
            return start_date + timedelta(interval)
        return start_date - timedelta(interval)
    return start_date

def to_date(d):
    if isinstance(d, datetime):
        return d.date()
    return d

def to_datetime(d):
    if type(d) == date:
        return datetime(d.year, d.month, d.day)
    return d

def in_chunks(it, size=25):
    chunk = []
    it = iter(it)
    try:
        while True:
            chunk.append(it.next())
            if len(chunk) >= size:
                yield chunk
                chunk = []
    except StopIteration:
        if chunk:
            yield chunk

def spaceout(items, targetseconds,
             minsleep = 0, die = False,
             estimate = None):
    """Given a list of items and a function to apply to them, space
       the execution out over the target number of seconds and
       optionally stop when we're out of time"""
    targetseconds = float(targetseconds)
    state = [1.0]

    if estimate is None:
        try:
            estimate = len(items)
        except TypeError:
            # if we can't come up with an estimate, the best we can do
            # is just enforce the minimum sleep time (and the max
            # targetseconds if die==True)
            pass

    mean = lambda lst: sum(float(x) for x in lst)/float(len(lst))
    beginning = datetime.now()

    for item in items:
        start = datetime.now()
        yield item
        end = datetime.now()

        took_delta = end - start
        took = (took_delta.days * 60 * 24
                + took_delta.seconds
                + took_delta.microseconds/1000000.0)
        state.append(took)
        if len(state) > 10:
            del state[0]

        if die and end > beginning + timedelta(seconds=targetseconds):
            # we ran out of time, ignore the rest of the iterator
            break

        if estimate is None:
            if minsleep:
                # we have no idea how many items we're going to get
                sleep(minsleep)
        else:
            sleeptime = max((targetseconds / estimate) - mean(state),
                            minsleep)
            if sleeptime > 0:
                sleep(sleeptime)

def progress(it, verbosity=100, key=repr, estimate=None, persec=True):
    """An iterator that yields everything from `it', but prints progress
       information along the way, including time-estimates if
       possible"""
    from itertools import islice
    from datetime import datetime
    import sys

    now = start = datetime.now()
    elapsed = start - start

    # try to guess at the estimate if we can
    if estimate is None:
        try:
            estimate = len(it)
        except:
            pass

    def timedelta_to_seconds(td):
        return td.days * (24*60*60) + td.seconds + (float(td.microseconds) / 1000000)
    def format_timedelta(td, sep=''):
        ret = []
        s = timedelta_to_seconds(td)
        if s < 0:
            neg = True
            s *= -1
        else:
            neg = False

        if s >= (24*60*60):
            days = int(s//(24*60*60))
            ret.append('%dd' % days)
            s -= days*(24*60*60)
        if s >= 60*60:
            hours = int(s//(60*60))
            ret.append('%dh' % hours)
            s -= hours*(60*60)
        if s >= 60:
            minutes = int(s//60)
            ret.append('%dm' % minutes)
            s -= minutes*60
        if s >= 1:
            seconds = int(s)
            ret.append('%ds' % seconds)
            s -= seconds

        if not ret:
            return '0s'

        return ('-' if neg else '') + sep.join(ret)
    def format_datetime(dt, show_date=False):
        if show_date:
            return dt.strftime('%Y-%m-%d %H:%M')
        else:
            return dt.strftime('%H:%M:%S')
    def deq(dt1, dt2):
        "Indicates whether the two datetimes' dates describe the same (day,month,year)"
        d1, d2 = dt1.date(), dt2.date()
        return (    d1.day   == d2.day
                and d1.month == d2.month
                and d1.year  == d2.year)

    sys.stderr.write('Starting at %s\n' % (start,))

    # we're going to islice it so we need to start an iterator
    it = iter(it)

    seen = 0
    while True:
        this_chunk = 0
        thischunk_started = datetime.now()

        # the simple bit: just iterate and yield
        for item in islice(it, verbosity):
            this_chunk += 1
            seen += 1
            yield item

        if this_chunk < verbosity:
            # we're done, the iterator is empty
            break

        now = datetime.now()
        elapsed = now - start
        thischunk_seconds = timedelta_to_seconds(now - thischunk_started)

        if estimate:
            # the estimate is based on the total number of items that
            # we've processed in the total amount of time that's
            # passed, so it should smooth over momentary spikes in
            # speed (but will take a while to adjust to long-term
            # changes in speed)
            remaining = ((elapsed/seen)*estimate)-elapsed
            completion = now + remaining
            count_str = ('%d/%d %.2f%%'
                         % (seen, estimate, float(seen)/estimate*100))
            completion_str = format_datetime(completion, not deq(completion,now))
            estimate_str = (' (%s remaining; completion %s)'
                            % (format_timedelta(remaining),
                               completion_str))
        else:
            count_str = '%d' % seen
            estimate_str = ''

        if key:
            key_str = ': %s' % key(item)
        else:
            key_str = ''

        # unlike the estimate, the persec count is the number per
        # second for *this* batch only, without smoothing
        if persec and thischunk_seconds > 0:
            persec_str = ' (%.1f/s)' % (float(this_chunk)/thischunk_seconds,)
        else:
            persec_str = ''

        sys.stderr.write('%s%s, %s%s%s\n'
                         % (count_str, persec_str,
                            format_timedelta(elapsed), estimate_str, key_str))

    now = datetime.now()
    elapsed = now - start
    elapsed_seconds = timedelta_to_seconds(elapsed)
    if persec and seen > 0 and elapsed_seconds > 0:
        persec_str = ' (@%.1f/sec)' % (float(seen)/elapsed_seconds)
    else:
        persec_str = ''
    sys.stderr.write('Processed %d%s items in %s..%s (%s)\n'
                     % (seen,
                        persec_str,
                        format_datetime(start, not deq(start, now)),
                        format_datetime(now, not deq(start, now)),
                        format_timedelta(elapsed)))

class Hell(object):
    def __str__(self):
        return "boom!"

class Bomb(object):
    @classmethod
    def __getattr__(cls, key):
        raise Hell()

    @classmethod
    def __setattr__(cls, key, val):
        raise Hell()

    @classmethod
    def __repr__(cls):
        raise Hell()

class SimpleSillyStub(object):
    """A simple stub object that does nothing when you call its methods."""
    def __nonzero__(self):
        return False

    def __getattr__(self, name):
        return self.stub

    def stub(self, *args, **kwargs):
        pass

def strordict_fullname(item, key='fullname'):
    """Sometimes we migrate AMQP queues from simple strings to pickled
    dictionaries. During the migratory period there may be items in
    the queue of both types, so this function tries to detect which
    the item is. It shouldn't really be used on a given queue for more
    than a few hours or days"""
    try:
        d = pickle.loads(item)
    except:
        d = {key: item}

    if (not isinstance(d, dict)
        or key not in d
        or not isinstance(d[key], str)):
        raise ValueError('Error trying to migrate %r (%r)'
                         % (item, d))

    return d

def thread_dump(*a):
    import sys, traceback
    from datetime import datetime

    sys.stderr.write('%(t)s Thread Dump @%(d)s %(t)s\n' % dict(t='*'*15,
                                                               d=datetime.now()))

    for thread_id, stack in sys._current_frames().items():
        sys.stderr.write('\t-- Thread ID: %s--\n' %  (thread_id,))

        for filename, lineno, fnname, line in traceback.extract_stack(stack):
            sys.stderr.write('\t\t%(filename)s(%(lineno)d): %(fnname)s\n'
                             % dict(filename=filename, lineno=lineno, fnname=fnname))
            sys.stderr.write('\t\t\t%(line)s\n' % dict(line=line))


def constant_time_compare(actual, expected):
    """
    Returns True if the two strings are equal, False otherwise

    The time taken is dependent on the number of characters provided
    instead of the number of characters that match.
    """
    actual_len   = len(actual)
    expected_len = len(expected)
    result = actual_len ^ expected_len
    if expected_len > 0:
        for i in xrange(actual_len):
            result |= ord(actual[i]) ^ ord(expected[i % expected_len])
    return result == 0


def extract_urls_from_markdown(md):
    "Extract URLs that will be hot links from a piece of raw Markdown."

    html = snudown.markdown(_force_utf8(md))
    links = SoupStrainer("a")

    for link in BeautifulSoup(html, parseOnlyThese=links):
        url = link.get('href')
        if url:
            yield url


def summarize_markdown(md):
    """Get the first paragraph of some Markdown text, potentially truncated."""

    first_graf, sep, rest = md.partition("\n\n")
    return first_graf[:500]


def find_containing_network(ip_ranges, address):
    """Find an IP network that contains the given address."""
    addr = ipaddress.ip_address(address)
    for network in ip_ranges:
        if addr in network:
            return network
    return None


def is_throttled(address):
    """Determine if an IP address is in a throttled range."""
    return bool(find_containing_network(g.throttles, address))


def parse_http_basic(authorization_header):
    """Parse the username/credentials out of an HTTP Basic Auth header.

    Raises RequirementException if anything is uncool.
    """
    auth_scheme, auth_token = require_split(authorization_header, 2)
    require(auth_scheme.lower() == "basic")
    try:
        auth_data = base64.b64decode(auth_token)
    except TypeError:
        raise RequirementException
    return require_split(auth_data, 2, ":")


def simple_traceback(limit):
    """Generate a pared-down traceback that's human readable but small.

    `limit` is how many frames of the stack to put in the traceback.

    """

    stack_trace = traceback.extract_stack(limit=limit)[:-2]
    return "\n".join(":".join((os.path.basename(filename),
                               function_name,
                               str(line_number),
                              ))
                     for filename, line_number, function_name, text
                     in stack_trace)


def weighted_lottery(weights, _random=random.random):
    """Randomly choose a key from a dict where values are weights.

    Weights should be non-negative numbers, and at least one weight must be
    non-zero. The probability that a key will be selected is proportional to
    its weight relative to the sum of all weights. Keys with zero weight will
    be ignored.

    Raises ValueError if weights is empty or contains a negative weight.
    """

    total = sum(weights.itervalues())
    if total <= 0:
        raise ValueError("total weight must be positive")

    r = _random() * total
    t = 0
    for key, weight in weights.iteritems():
        if weight < 0:
            raise ValueError("weight for %r must be non-negative" % key)
        t += weight
        if t > r:
            return key

    # this point should never be reached
    raise ValueError(
        "weighted_lottery messed up: r=%r, t=%r, total=%r" % (r, t, total))


def read_static_file_config(config_file):
    with open(config_file) as f:
        parser = parse_ini_file(f)
    config = dict(parser.items("static_files"))

    s3 = boto.connect_s3(config["aws_access_key_id"],
                         config["aws_secret_access_key"])
    bucket = s3.get_bucket(config["bucket"])

    return bucket, config


class GoldPrice(object):
    """Simple price math / formatting type.

    Prices are assumed to be USD at the moment.

    """
    def __init__(self, decimal):
        self.decimal = Decimal(decimal)

    def __mul__(self, other):
        return type(self)(self.decimal * other)

    def __div__(self, other):
        return type(self)(self.decimal / other)

    def __str__(self):
        return "$%s" % self.decimal.quantize(Decimal("1.00"))

    def __repr__(self):
        return "%s(%s)" % (type(self).__name__, self)

    @property
    def pennies(self):
        return int(self.decimal * 100)


def config_gold_price(v, key=None, data=None):
    return GoldPrice(v)


def canonicalize_email(email):
    """Return the given email address without various localpart manglings.

    a.s.d.f+something@gmail.com --> asdf@gmail.com

    This is not at all RFC-compliant or correct. It's only intended to be a
    quick heuristic to remove commonly used mangling techniques.

    """

    if not email:
        return ""

    email = _force_utf8(email.lower())

    localpart, at, domain = email.partition("@")
    if not at or "@" in domain:
        return ""

    localpart = localpart.replace(".", "")
    localpart = localpart.partition("+")[0]

    return localpart + "@" + domain


def precise_format_timedelta(delta, locale, threshold=.85, decimals=2):
    """Like babel.dates.format_datetime but with adjustable precision"""
    seconds = delta.total_seconds()

    for unit, secs_per_unit in TIMEDELTA_UNITS:
        value = abs(seconds) / secs_per_unit
        if value >= threshold:
            plural_form = locale.plural_form(value)
            pattern = None
            for choice in (unit + ':medium', unit):
                patterns = locale._data['unit_patterns'].get(choice)
                if patterns is not None:
                    pattern = patterns[plural_form]
                    break
            if pattern is None:
                return u''
            decimals = int(decimals)
            format_string = "%." + str(decimals) + "f"
            return pattern.replace('{0}', format_string % value)
    return u''


def parse_ini_file(config_file):
    """Given an open file, read and parse it like an ini file."""

    parser = ConfigParser.RawConfigParser()
    parser.optionxform = str  # ensure keys are case-sensitive as expected
    parser.readfp(config_file)
    return parser

def fuzz_activity(count):
    """Add some jitter to an activity metric to maintain privacy."""
    # decay constant is e**(-x / 60)
    decay = math.exp(float(-count) / 60)
    jitter = round(5 * decay)
    return count + random.randint(0, jitter)

# http://docs.python.org/2/library/itertools.html#recipes
def roundrobin(*iterables):
    "roundrobin('ABC', 'D', 'EF') --> A D E B F C"
    # Recipe credited to George Sakkis
    pending = len(iterables)
    nexts = itertools.cycle(iter(it).next for it in iterables)
    while pending:
        try:
            for next in nexts:
                yield next()
        except StopIteration:
            pending -= 1
            nexts = itertools.cycle(itertools.islice(nexts, pending))

########NEW FILE########
__FILENAME__ = preferences
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
from pylons import g
from r2.lib.validator.validator import (
    VBoolean,
    VInt,
    VLang,
    VOneOf,
)

# Validators that map directly to Account._preference_attrs
# The key MUST be the same string as the value in _preference_attrs
# Non-preference validators should be added to to the controller
# method directly (see PostController.POST_options)
PREFS_VALIDATORS = dict(
    pref_frame=VBoolean('frame'),
    pref_clickgadget=VBoolean('clickgadget'),
    pref_organic=VBoolean('organic'),
    pref_newwindow=VBoolean('newwindow'),
    pref_public_votes=VBoolean('public_votes'),
    pref_hide_from_robots=VBoolean('hide_from_robots'),
    pref_hide_ups=VBoolean('hide_ups'),
    pref_hide_downs=VBoolean('hide_downs'),
    pref_over_18=VBoolean('over_18'),
    pref_research=VBoolean('research'),
    pref_numsites=VInt('numsites', 1, 100),
    pref_lang=VLang('lang'),
    pref_media=VOneOf('media', ('on', 'off', 'subreddit')),
    pref_compress=VBoolean('compress'),
    pref_domain_details=VBoolean('domain_details'),
    pref_min_link_score=VInt('min_link_score', -100, 100),
    pref_min_comment_score=VInt('min_comment_score', -100, 100),
    pref_num_comments=VInt('num_comments', 1, g.max_comments,
                           default=g.num_comments),
    pref_show_stylesheets=VBoolean('show_stylesheets'),
    pref_show_flair=VBoolean('show_flair'),
    pref_show_link_flair=VBoolean('show_link_flair'),
    pref_no_profanity=VBoolean('no_profanity'),
    pref_label_nsfw=VBoolean('label_nsfw'),
    pref_show_promote=VBoolean('show_promote'),
    pref_mark_messages_read=VBoolean("mark_messages_read"),
    pref_threaded_messages=VBoolean("threaded_messages"),
    pref_collapse_read_messages=VBoolean("collapse_read_messages"),
    pref_private_feeds=VBoolean("private_feeds"),
    pref_local_js=VBoolean('local_js'),
    pref_store_visits=VBoolean('store_visits'),
    pref_show_adbox=VBoolean("show_adbox"),
    pref_show_sponsors=VBoolean("show_sponsors"),
    pref_show_sponsorships=VBoolean("show_sponsorships"),
    pref_show_trending=VBoolean("show_trending"),
    pref_highlight_new_comments=VBoolean("highlight_new_comments"),
    pref_monitor_mentions=VBoolean("monitor_mentions"),
)


def format_content_lang_pref(content_langs):
    pref_content_langs = []
    for lang in content_langs:
        if lang in g.all_languages:
            pref_content_langs.append(lang)
    pref_content_langs = tuple(sorted(pref_content_langs))

    if not pref_content_langs or "all" in pref_content_langs:
        return "all"
    else:
        return pref_content_langs


def set_prefs(user, prefs):
    for k, v in prefs.iteritems():
        setattr(user, k, v)


def filter_prefs(prefs, user):
    for pref_key in prefs.keys():
        if pref_key not in user._preference_attrs:
            del prefs[pref_key]

    #temporary. eventually we'll change pref_clickgadget to an
    #integer preference
    prefs['pref_clickgadget'] = 5 if prefs['pref_clickgadget'] else 0
    if user.pref_show_promote is None:
        prefs['pref_show_promote'] = None
    elif not prefs.get('pref_show_promote'):
        prefs['pref_show_promote'] = False

    if not prefs.get("pref_over_18") or not user.pref_over_18:
        prefs['pref_no_profanity'] = True

    if prefs.get("pref_no_profanity") or user.pref_no_profanity:
        prefs['pref_label_nsfw'] = True

    # default all the gold options to on if they don't have gold
    if not user.gold:
        for pref in ('pref_show_adbox',
                     'pref_show_sponsors',
                     'pref_show_sponsorships',
                     'pref_highlight_new_comments',
                     'pref_monitor_mentions'):
            prefs[pref] = True

########NEW FILE########
__FILENAME__ = validator
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import cgi
import json

from pylons import c, g, request, response
from pylons.i18n import _
from pylons.controllers.util import abort
from r2.config.extensions import api_type
from r2.lib import utils, captcha, promote, totp
from r2.lib.filters import unkeep_space, websafe, _force_unicode
from r2.lib.filters import markdown_souptest
from r2.lib.db import tdb_cassandra
from r2.lib.db.operators import asc, desc
from r2.lib.template_helpers import add_sr
from r2.lib.jsonresponse import JQueryResponse, JsonResponse
from r2.lib.log import log_text
from r2.lib.permissions import ModeratorPermissionSet
from r2.models import *
from r2.models.promo import Location
from r2.lib.authorize import Address, CreditCard
from r2.lib.utils import constant_time_compare, make_offset_date
from r2.lib.require import require, require_split, RequirementException

from r2.lib.errors import errors, RedditError, UserRequiredException
from r2.lib.errors import VerifiedUserRequiredException

from copy import copy
from datetime import datetime, timedelta
from curses.ascii import isprint
import re, inspect
from itertools import chain
from functools import wraps

def visible_promo(article):
    is_promo = getattr(article, "promoted", None) is not None
    is_author = (c.user_is_loggedin and
                 c.user._id == article.author_id)

    # promos are visible only if comments are not disabled and the
    # user is either the author or the link is live/previously live.
    if is_promo:
        return (c.user_is_sponsor or
                is_author or
                (not article.disable_comments and
                 article.promote_status >= PROMOTE_STATUS.promoted))
    # not a promo, therefore it is visible
    return True

def can_view_link_comments(article):
    return (article.subreddit_slow.can_view(c.user) and
            visible_promo(article))

def can_comment_link(article):
    return (article.subreddit_slow.can_comment(c.user) and
            visible_promo(article))

class Validator(object):
    notes = None
    default_param = None
    def __init__(self, param=None, default=None, post=True, get=True, url=True,
                 body=False, docs=None):
        if param:
            self.param = param
        else:
            self.param = self.default_param

        self.default = default
        self.post, self.get, self.url, self.docs = post, get, url, docs
        self.body = body
        self.has_errors = False

    def set_error(self, error, msg_params={}, field=False, code=None):
        """
        Adds the provided error to c.errors and flags that it is come
        from the validator's param
        """
        if field is False:
            field = self.param

        c.errors.add(error, msg_params=msg_params, field=field, code=code)
        self.has_errors = True

    def param_docs(self):
        param_info = {}
        for param in filter(None, tup(self.param)):
            param_info[param] = None
        return param_info

    def __call__(self, url):
        self.has_errors = False
        a = []
        if self.param:
            for p in utils.tup(self.param):
                # cgi.FieldStorage is falsy even if it has a filled value
                # property. :(
                post_val = request.POST.get(p)
                if self.post and (post_val or
                                  isinstance(post_val, cgi.FieldStorage)):
                    val = request.POST[p]
                elif self.get and request.GET.get(p):
                    val = request.GET[p]
                elif self.url and url.get(p):
                    val = url[p]
                elif self.body:
                    val = request.body
                else:
                    val = self.default
                a.append(val)
        try:
            return self.run(*a)
        except TypeError, e:
            if str(e).startswith('run() takes'):
                # Prepend our class name so we know *which* run()
                raise TypeError('%s.%s' % (type(self).__name__, str(e)))
            else:
                raise


def build_arg_list(fn, env):
    """given a fn and and environment the builds a keyword argument list
    for fn"""
    kw = {}
    argspec = inspect.getargspec(fn)

    # if there is a **kw argument in the fn definition,
    # just pass along the environment
    if argspec[2]:
        kw = env
    #else for each entry in the arglist set the value from the environment
    else:
        #skip self
        argnames = argspec[0][1:]
        for name in argnames:
            if name in env:
                kw[name] = env[name]
    return kw

def _make_validated_kw(fn, simple_vals, param_vals, env):
    for validator in simple_vals:
        validator(env)
    kw = build_arg_list(fn, env)
    for var, validator in param_vals.iteritems():
        kw[var] = validator(env)
    return kw

def set_api_docs(fn, simple_vals, param_vals, extra_vals=None):
    doc = fn._api_doc = getattr(fn, '_api_doc', {})
    param_info = doc.get('parameters', {})
    notes = doc.get('notes', [])
    for validator in chain(simple_vals, param_vals.itervalues()):
        param_docs = validator.param_docs()
        if validator.docs:
            param_docs.update(validator.docs)
        param_info.update(param_docs)
        if validator.notes:
            notes.append(validator.notes)
    if extra_vals:
        param_info.update(extra_vals)
    doc['parameters'] = param_info
    doc['notes'] = notes


def validate(*simple_vals, **param_vals):
    """Validation decorator that delegates error handling to the controller.

    Runs the validators specified and calls self.on_validation_error to
    process each error. This allows controllers to define their own fatal
    error processing logic.
    """
    def val(fn):
        @wraps(fn)
        def newfn(self, *a, **env):
            try:
                kw = _make_validated_kw(fn, simple_vals, param_vals, env)
            except RedditError as err:
                self.on_validation_error(err)

            for err in c.errors:
                self.on_validation_error(c.errors[err])

            try:
                return fn(self, *a, **kw)
            except RedditError as err:
                self.on_validation_error(err)

        set_api_docs(newfn, simple_vals, param_vals)
        return newfn
    return val


def api_validate(response_type=None, add_api_type_doc=False):
    """
    Factory for making validators for API calls, since API calls come
    in two flavors: responsive and unresponsive.  The machinary
    associated with both is similar, and the error handling identical,
    so this function abstracts away the kw validation and creation of
    a Json-y responder object.
    """
    def wrap(response_function):
        def _api_validate(*simple_vals, **param_vals):
            def val(fn):
                @wraps(fn)
                def newfn(self, *a, **env):
                    renderstyle = request.params.get("renderstyle")
                    if renderstyle:
                        c.render_style = api_type(renderstyle)
                    elif not c.extension:
                        # if the request URL included an extension, don't
                        # touch the render_style, since it was already set by
                        # set_extension. if no extension was provided, default
                        # to response_type.
                        c.render_style = api_type(response_type)

                    # generate a response object
                    if response_type == "html" and not request.params.get('api_type') == "json":
                        responder = JQueryResponse()
                    else:
                        responder = JsonResponse()

                    response.content_type = responder.content_type

                    try:
                        kw = _make_validated_kw(fn, simple_vals, param_vals, env)
                        return response_function(self, fn, responder,
                                                 simple_vals, param_vals, *a, **kw)
                    except UserRequiredException:
                        responder.send_failure(errors.USER_REQUIRED)
                        return self.api_wrapper(responder.make_response())
                    except VerifiedUserRequiredException:
                        responder.send_failure(errors.VERIFIED_USER_REQUIRED)
                        return self.api_wrapper(responder.make_response())

                extra_param_vals = {}
                if add_api_type_doc:
                    extra_param_vals = {
                        "api_type": "the string `json`",
                    }

                set_api_docs(newfn, simple_vals, param_vals, extra_param_vals)
                return newfn
            return val
        return _api_validate
    return wrap


@api_validate("html")
def noresponse(self, self_method, responder, simple_vals, param_vals, *a, **kw):
    self_method(self, *a, **kw)
    return self.api_wrapper({})

@api_validate("html")
def textresponse(self, self_method, responder, simple_vals, param_vals, *a, **kw):
    return self_method(self, *a, **kw)

@api_validate()
def json_validate(self, self_method, responder, simple_vals, param_vals, *a, **kw):
    if c.extension != 'json':
        abort(404)

    val = self_method(self, responder, *a, **kw)
    if val is None:
        val = responder.make_response()
    return self.api_wrapper(val)

def _validatedForm(self, self_method, responder, simple_vals, param_vals,
                  *a, **kw):
    # generate a form object
    form = responder(request.POST.get('id', "body"))

    # clear out the status line as a courtesy
    form.set_html(".status", "")

    # do the actual work
    val = self_method(self, form, responder, *a, **kw)

    # add data to the output on some errors
    for validator in chain(simple_vals, param_vals.values()):
        if (isinstance(validator, VCaptcha) and
            (form.has_errors('captcha', errors.BAD_CAPTCHA) or
             (form.has_error() and c.user.needs_captcha()))):
            form.new_captcha()
        elif (isinstance(validator, VRatelimit) and
              form.has_errors('ratelimit', errors.RATELIMIT)):
            form.ratelimit(validator.seconds)
        elif (isinstance(validator, VThrottledLogin) and
                form.has_errors('vdelay', errors.RATELIMIT)):
            form.ratelimit(validator.vdelay.seconds)
    if val:
        return val
    else:
        return self.api_wrapper(responder.make_response())

@api_validate("html", add_api_type_doc=True)
def validatedForm(self, self_method, responder, simple_vals, param_vals,
                  *a, **kw):
    return _validatedForm(self, self_method, responder, simple_vals, param_vals,
                          *a, **kw)

@api_validate("html", add_api_type_doc=True)
def validatedMultipartForm(self, self_method, responder, simple_vals,
                           param_vals, *a, **kw):
    def wrapped_self_method(*a, **kw):
        val = self_method(*a, **kw)
        if val:
            return val
        else:
            data = json.dumps(responder.make_response())
            response.content_type = "text/html"
            return ('<html><head><script type="text/javascript">\n'
                    'parent.$.handleResponse().call('
                    'parent.$("#" + window.frameElement.id).parent(), %s)\n'
                    '</script></head></html>') % filters.websafe_json(data)
    return _validatedForm(self, wrapped_self_method, responder, simple_vals,
                          param_vals, *a, **kw)


jsonp_callback_rx = re.compile(r"""\A[\w$\."'[\]]+\Z""")
def valid_jsonp_callback(callback):
    return jsonp_callback_rx.match(callback)


#### validators ####
class nop(Validator):
    def run(self, x):
        return x

class VLang(Validator):
    @staticmethod
    def validate_lang(lang, strict=False):
        if lang in g.all_languages:
            return lang
        else:
            if not strict:
                return g.lang
            else:
                raise ValueError("invalid language %r" % lang)

    @staticmethod
    def validate_content_langs(langs):
        if langs == "all":
            return langs

        validated = []
        for lang in langs:
            try:
                validated.append(VLang.validate_lang(lang, strict=True))
            except ValueError:
                pass

        if not validated:
            raise ValueError("no valid languages")

        return validated

    def run(self, lang):
        return VLang.validate_lang(lang)

    def param_docs(self):
        return {
            self.param: "a valid IETF language tag (underscore separated)",
        }


class VContentLang(VLang):
    def run(self, lang):
        if lang == "all":
            return lang
        try:
            return VLang.validate_lang(lang, strict=True)
        except ValueError:
            self.set_error(errors.INVALID_LANG)


class VRequired(Validator):
    def __init__(self, param, error, *a, **kw):
        Validator.__init__(self, param, *a, **kw)
        self._error = error

    def error(self, e = None):
        if not e: e = self._error
        if e:
            self.set_error(e)

    def run(self, item):
        if not item:
            self.error()
        else:
            return item

class VThing(Validator):
    def __init__(self, param, thingclass, redirect = True, *a, **kw):
        Validator.__init__(self, param, *a, **kw)
        self.thingclass = thingclass
        self.redirect = redirect

    def run(self, thing_id):
        if thing_id:
            try:
                tid = int(thing_id, 36)
                thing = self.thingclass._byID(tid, True)
                if thing.__class__ != self.thingclass:
                    raise TypeError("Expected %s, got %s" %
                                    (self.thingclass, thing.__class__))
                return thing
            except (NotFound, ValueError):
                if self.redirect:
                    abort(404, 'page not found')
                else:
                    return None

class VLink(VThing):
    def __init__(self, param, redirect = True, *a, **kw):
        VThing.__init__(self, param, Link, redirect=redirect, *a, **kw)

class VPromoCampaign(VThing):
    def __init__(self, param, redirect = True, *a, **kw):
        VThing.__init__(self, param, PromoCampaign, *a, **kw)

class VCommentByID(VThing):
    def __init__(self, param, redirect = True, *a, **kw):
        VThing.__init__(self, param, Comment, redirect=redirect, *a, **kw)


class VAward(VThing):
    def __init__(self, param, redirect = True, *a, **kw):
        VThing.__init__(self, param, Award, redirect=redirect, *a, **kw)

class VAwardByCodename(Validator):
    def run(self, codename, required_fullname=None):
        if not codename:
            return self.set_error(errors.NO_TEXT)

        try:
            a = Award._by_codename(codename)
        except NotFound:
            a = None

        if a and required_fullname and a._fullname != required_fullname:
            return self.set_error(errors.INVALID_OPTION)
        else:
            return a

class VTrophy(VThing):
    def __init__(self, param, redirect = True, *a, **kw):
        VThing.__init__(self, param, Trophy, redirect=redirect, *a, **kw)

class VMessage(Validator):
    def run(self, message_id):
        if message_id:
            try:
                aid = int(message_id, 36)
                return Message._byID(aid, True)
            except (NotFound, ValueError):
                abort(404, 'page not found')


class VCommentID(Validator):
    def run(self, cid):
        if cid:
            try:
                cid = int(cid, 36)
                return Comment._byID(cid, True)
            except (NotFound, ValueError):
                pass

class VMessageID(Validator):
    def run(self, cid):
        if cid:
            try:
                cid = int(cid, 36)
                m = Message._byID(cid, True)
                if not m.can_view_slow():
                    abort(403, 'forbidden')
                return m
            except (NotFound, ValueError):
                pass

class VCount(Validator):
    def run(self, count):
        if count is None:
            count = 0
        try:
            return max(int(count), 0)
        except ValueError:
            return 0

    def param_docs(self):
        return {
            self.param: "a positive integer (default: 0)",
        }


class VLimit(Validator):
    def __init__(self, param, default=25, max_limit=100, **kw):
        self.default_limit = default
        self.max_limit = max_limit
        Validator.__init__(self, param, **kw)

    def run(self, limit):
        default = c.user.pref_numsites
        if not default or c.render_style in ("compact", api_type("compact")):
            default = self.default_limit  # TODO: ini param?

        if limit is None:
            return default

        try:
            i = int(limit)
        except ValueError:
            return default

        return min(max(i, 1), self.max_limit)

    def param_docs(self):
        return {
            self.param: "the maximum number of items desired "
                        "(default: %d, maximum: %d)" % (self.default_limit,
                                                        self.max_limit),
        }

class VCssMeasure(Validator):
    measure = re.compile(r"\A\s*[\d\.]+\w{0,3}\s*\Z")
    def run(self, value):
        return value if value and self.measure.match(value) else ''

subreddit_rx = re.compile(r"\A[A-Za-z0-9][A-Za-z0-9_]{2,20}\Z")
language_subreddit_rx = re.compile(r"\A[a-z]{2}\Z")

def chksrname(x, allow_language_srs=False):
    if not x:
        return None

    #notice the space before reddit.com
    if x in ('friends', 'all', ' reddit.com'):
        return False

    try:
        valid = subreddit_rx.match(x)
        if allow_language_srs:
            valid = valid or language_subreddit_rx.match(x)

        return str(x) if valid else None
    except UnicodeEncodeError:
        return None


class VLength(Validator):
    only_whitespace = re.compile(r"\A\s*\Z", re.UNICODE)

    def __init__(self, param, max_length,
                 empty_error = errors.NO_TEXT,
                 length_error = errors.TOO_LONG,
                 **kw):
        Validator.__init__(self, param, **kw)
        self.max_length = max_length
        self.length_error = length_error
        self.empty_error = empty_error

    def run(self, text, text2 = ''):
        text = text or text2
        if self.empty_error and (not text or self.only_whitespace.match(text)):
            self.set_error(self.empty_error, code=400)
        elif len(text) > self.max_length:
            self.set_error(self.length_error, {'max_length': self.max_length}, code=400)
        else:
            return text

    def param_docs(self):
        return {
            self.param:
                "a string no longer than %d characters" % self.max_length,
        }

class VUploadLength(VLength):
    def run(self, upload, text2=''):
        # upload is expected to be a FieldStorage object
        if isinstance(upload, cgi.FieldStorage):
            return VLength.run(self, upload.value, text2)
        else:
            self.set_error(self.empty_error, code=400)

    def param_docs(self):
        kibibytes = self.max_length / 1024
        return {
            self.param:
                "file upload with maximum size of %d KiB" % kibibytes,
        }

class VPrintable(VLength):
    def run(self, text, text2 = ''):
        text = VLength.run(self, text, text2)

        if text is None:
            return None

        try:
            if all(isprint(str(x)) for x in text):
                return str(text)
        except UnicodeEncodeError:
            pass

        self.set_error(errors.BAD_STRING, code=400)
        return None

    def param_docs(self):
        return {
            self.param: "a string up to %d characters long,"
                        " consisting of printable characters."
                            % self.max_length,
        }

class VTitle(VLength):
    def __init__(self, param, max_length = 300, **kw):
        VLength.__init__(self, param, max_length, **kw)

    def param_docs(self):
        return {
            self.param: "title of the submission. "
                        "up to %d characters long" % self.max_length,
        }

class VMarkdown(VLength):
    def __init__(self, param, max_length = 10000, renderer='reddit', **kw):
        VLength.__init__(self, param, max_length, **kw)
        self.renderer = renderer

    def run(self, text, text2 = ''):
        text = text or text2
        VLength.run(self, text)
        try:
            markdown_souptest(text, renderer=self.renderer)
            return text
        except ValueError:
            import sys
            user = "???"
            if c.user_is_loggedin:
                user = c.user.name
            g.log.error("HAX by %s: %s" % (user, text))
            s = sys.exc_info()
            # reraise the original error with the original stack trace
            raise s[1], None, s[2]

    def param_docs(self):
        return {
            tup(self.param)[0]: "raw markdown text",
        }

class VSelfText(VMarkdown):

    def set_max_length(self, val):
        self._max_length = val

    def get_max_length(self):
        if c.site.link_type == "self":
            return self._max_length * 4
        return self._max_length * 1.5

    max_length = property(get_max_length, set_max_length)


class VSavedCategory(Validator):
    savedcategory_rx = re.compile(r"\A[a-z0-9 _]{1,20}\Z")

    def run(self, name):
        if not name:
            return
        name = name.lower()
        valid = self.savedcategory_rx.match(name)
        if not valid:
            self.set_error('BAD_SAVE_CATEGORY')
            return
        return name

    def param_docs(self):
        return {
            self.param: "a category name",
        }


class VSubredditName(VRequired):
    def __init__(self, item, allow_language_srs=False, *a, **kw):
        VRequired.__init__(self, item, errors.BAD_SR_NAME, *a, **kw)
        self.allow_language_srs = allow_language_srs

    def run(self, name):
        name = chksrname(name, self.allow_language_srs)
        if not name:
            self.set_error(self._error, code=400)
        return name

    def param_docs(self):
        return {
            self.param: "subreddit name",
        }


class VAvailableSubredditName(VSubredditName):
    def run(self, name):
        name = VSubredditName.run(self, name)
        if name:
            try:
                a = Subreddit._by_name(name)
                return self.error(errors.SUBREDDIT_EXISTS)
            except NotFound:
                return name


class VSRByName(Validator):
    def run(self, sr_name):
        if not sr_name:
            self.set_error(errors.BAD_SR_NAME, code=400)
        else:
            try:
                sr = Subreddit._by_name(sr_name)
                return sr
            except NotFound:
                self.set_error(errors.SUBREDDIT_NOEXIST, code=400)

    def param_docs(self):
        return {
            self.param: "subreddit name",
        }


class VSRByNames(Validator):
    """Returns a dict mapping subreddit names to subreddit objects.

    sr_names_csv - a comma delimited string of subreddit names
    required - if true (default) an empty subreddit name list is an error

    """
    def __init__(self, sr_names_csv, required=True):
        self.required = required
        Validator.__init__(self, sr_names_csv)

    def run(self, sr_names_csv):
        if sr_names_csv:
            sr_names = [s.strip() for s in sr_names_csv.split(',')]
            return Subreddit._by_name(sr_names)
        elif self.required:
            self.set_error(errors.BAD_SR_NAME, code=400)
        return {}

    def param_docs(self):
        return {
            self.param: "comma-delimited list of subreddit names",
        }


class VSubredditTitle(Validator):
    def run(self, title):
        if not title:
            self.set_error(errors.NO_TITLE)
        elif len(title) > 100:
            self.set_error(errors.TITLE_TOO_LONG)
        else:
            return title

class VSubredditDesc(Validator):
    def run(self, description):
        if description and len(description) > 500:
            self.set_error(errors.DESC_TOO_LONG)
        return unkeep_space(description or '')

class VAccountByName(VRequired):
    def __init__(self, param, error = errors.USER_DOESNT_EXIST, *a, **kw):
        VRequired.__init__(self, param, error, *a, **kw)

    def run(self, name):
        if name:
            try:
                return Account._by_name(name)
            except NotFound: pass
        return self.error()

    def param_docs(self):
        return {self.param: "A valid, existing reddit username"}


class VFriendOfMine(VAccountByName):
    def run(self, name):
        # Must be logged in
        VUser().run()
        maybe_friend = VAccountByName.run(self, name)
        if maybe_friend:
            friend_rel = Account.get_friend(c.user, maybe_friend)
            if friend_rel:
                return friend_rel
            else:
                self.error(errors.NOT_FRIEND)
        return None


def fullname_regex(thing_cls = None, multiple = False):
    pattern = "[%s%s]" % (Relation._type_prefix, Thing._type_prefix)
    if thing_cls:
        pattern += utils.to36(thing_cls._type_id)
    else:
        pattern += r"[0-9a-z]+"
    pattern += r"_[0-9a-z]+"
    if multiple:
        pattern = r"(%s *,? *)+" % pattern
    return re.compile(r"\A" + pattern + r"\Z")

class VByName(Validator):
    # Lookup tdb_sql.Thing or tdb_cassandra.Thing objects by fullname.
    splitter = re.compile('[ ,]+')
    def __init__(self, param, thing_cls=None, multiple=False, limit=None,
                 error=errors.NO_THING_ID, ignore_missing=False,
                 backend='sql', **kw):
        # Limit param only applies when multiple is True
        if not multiple and limit is not None:
            raise TypeError('multiple must be True when limit is set')
        self.thing_cls = thing_cls
        self.re = fullname_regex(thing_cls)
        self.multiple = multiple
        self.limit = limit
        self._error = error
        self.ignore_missing = ignore_missing
        self.backend = backend

        Validator.__init__(self, param, **kw)

    def run(self, items):
        if self.backend == 'cassandra':
            # tdb_cassandra.Thing objects can't use the regex
            if items and self.multiple:
                items = [item for item in self.splitter.split(items)]
                if self.limit and len(items) > self.limit:
                    return self.set_error(errors.TOO_MANY_THING_IDS)
            if items:
                try:
                    return tdb_cassandra.Thing._by_fullname(
                        items,
                        ignore_missing=self.ignore_missing,
                        return_dict=False,
                    )
                except NotFound:
                    pass
        else:
            if items and self.multiple:
                items = [item for item in self.splitter.split(items)
                         if item and self.re.match(item)]
                if self.limit and len(items) > self.limit:
                    return self.set_error(errors.TOO_MANY_THING_IDS)
            if items and (self.multiple or self.re.match(items)):
                try:
                    return Thing._by_fullname(
                        items,
                        return_dict=False,
                        ignore_missing=self.ignore_missing,
                        data=True,
                    )
                except NotFound:
                    pass

        return self.set_error(self._error)

    def param_docs(self):
        thingtype = (self.thing_cls or Thing).__name__.lower()
        if self.multiple:
            return {
                self.param: ("A comma-separated list of %s [fullnames]"
                             "(#fullnames)" % thingtype)
            }
        else:
            return {
                self.param: "[fullname](#fullnames) of a %s" % thingtype,
            }

class VByNameIfAuthor(VByName):
    def run(self, fullname):
        thing = VByName.run(self, fullname)
        if thing:
            if not thing._loaded: thing._load()
            if c.user_is_loggedin and thing.author_id == c.user._id:
                return thing
        return self.set_error(errors.NOT_AUTHOR)

    def param_docs(self):
        return {
            self.param: "[fullname](#fullnames) of a thing created by the user",
        }

class VCaptcha(Validator):
    default_param = ('iden', 'captcha')

    def run(self, iden, solution):
        if c.user.needs_captcha():
            valid_captcha = captcha.valid_solution(iden, solution)
            if not valid_captcha:
                self.set_error(errors.BAD_CAPTCHA)
            g.stats.action_event_count("captcha", valid_captcha)

    def param_docs(self):
        return {
            self.param[0]: "the identifier of the CAPTCHA challenge",
            self.param[1]: "the user's response to the CAPTCHA challenge",
        }

class VUser(Validator):
    def run(self, password = None):
        if not c.user_is_loggedin:
            raise UserRequiredException

        if (password is not None) and not valid_password(c.user, password):
            self.set_error(errors.WRONG_PASSWORD)

class VModhash(Validator):
    default_param = 'uh'
    def __init__(self, param=None, fatal=True, *a, **kw):
        Validator.__init__(self, param, *a, **kw)
        self.fatal = fatal

    def run(self, uh):
        if uh is None:
            uh = request.headers.get('X-Modhash')

        if not c.user_is_loggedin or uh != c.user.name:
            if self.fatal:
                abort(403)
            self.set_error('INVALID_MODHASH')

    def param_docs(self):
        return {
            '%s / X-Modhash header' % self.param: 'a [modhash](#modhashes)',
        }

class VVotehash(Validator):
    def run(self, vh):
        return None

    def param_docs(self):
        return {
            self.param[0]: "ignored",
        }

class VAdmin(Validator):
    def run(self):
        if not c.user_is_admin:
            abort(404, "page not found")

def make_or_admin_secret_cls(base_cls):
    class VOrAdminSecret(base_cls):
        def run(self, secret=None):
            '''If validation succeeds, return True if the secret was used,
            False otherwise'''
            if secret and constant_time_compare(secret,
                                                g.secrets["ADMINSECRET"]):
                return True
            super(VOrAdminSecret, self).run()

            # import here so that we don't close around VModhash
            # before r2admin can override
            if request.method.upper() != "GET":
                from r2.lib.validator import VModhash
                VModhash(fatal=True).run(request.POST.get("uh"))

            return False
    return VOrAdminSecret

VAdminOrAdminSecret = make_or_admin_secret_cls(VAdmin)

class VVerifiedUser(VUser):
    def run(self):
        VUser.run(self)
        if not c.user.email_verified:
            raise VerifiedUserRequiredException

class VGold(VUser):
    notes = "*Requires a subscription to [reddit gold](/gold/about)*"
    def run(self):
        VUser.run(self)
        if not c.user.gold:
            abort(403, 'forbidden')

class VSponsorAdmin(VVerifiedUser):
    """
    Validator which checks c.user_is_sponsor
    """
    def user_test(self, thing):
        return (thing.author_id == c.user._id)

    def run(self, link_id = None):
        VVerifiedUser.run(self)
        if c.user_is_sponsor:
            return
        abort(403, 'forbidden')

VSponsorAdminOrAdminSecret = make_or_admin_secret_cls(VSponsorAdmin)

class VSponsor(VVerifiedUser):
    """
    Not intended to be used as a check for c.user_is_sponsor, but
    rather is the user allowed to use the sponsored link system.
    If a link or campaign is passed in, it also checks whether the user is
    allowed to edit that particular sponsored link.
    """
    def user_test(self, thing):
        return (thing.author_id == c.user._id)

    def run(self, link_id=None, campaign_id=None):
        assert not (link_id and campaign_id), 'Pass link or campaign, not both'

        VVerifiedUser.run(self)
        if c.user_is_sponsor:
            return
        elif campaign_id:
            pc = None
            try:
                if '_' in campaign_id:
                    pc = PromoCampaign._by_fullname(campaign_id, data=True)
                else:
                    pc = PromoCampaign._byID36(campaign_id, data=True)
            except (NotFound, ValueError):
                pass
            if pc:
                link_id = pc.link_id
        if link_id:
            try:
                if '_' in link_id:
                    t = Link._by_fullname(link_id, True)
                else:
                    aid = int(link_id, 36)
                    t = Link._byID(aid, True)
                if self.user_test(t):
                    return
            except (NotFound, ValueError):
                pass
            abort(403, 'forbidden')


class VEmployee(VVerifiedUser):
    """Validate that user is an employee."""
    def run(self):
        if not c.user.employee:
            abort(403, 'forbidden')
        VVerifiedUser.run(self)


class VSrModerator(Validator):
    def __init__(self, fatal=True, perms=(), *a, **kw):
        # If True, abort rather than setting an error
        self.fatal = fatal
        self.perms = utils.tup(perms)
        super(VSrModerator, self).__init__(*a, **kw)

    def run(self):
        if not (c.user_is_loggedin
                and c.site.is_moderator_with_perms(c.user, *self.perms)
                or c.user_is_admin):
            if self.fatal:
                abort(403, "forbidden")
            return self.set_error('MODERATOR_REQUIRED', code=403)

class VCanDistinguish(VByName):
    def run(self, thing_name, how):
        if c.user_is_loggedin:
            item = VByName.run(self, thing_name)
            if item.author_id == c.user._id:
                # will throw a legitimate 500 if this isn't a link or
                # comment, because this should only be used on links and
                # comments
                subreddit = item.subreddit_slow
                if how in ("yes", "no") and subreddit.can_distinguish(c.user):
                    return True
                elif how in ("special", "no") and c.user_special_distinguish:
                    return True
                elif how in ("admin", "no") and c.user.employee:
                    return True

        abort(403,'forbidden')

    def param_docs(self):
        return {}

class VSrCanAlter(VByName):
    def run(self, thing_name):
        if c.user_is_admin:
            return True
        elif c.user_is_loggedin:
            item = VByName.run(self, thing_name)
            if item.author_id == c.user._id:
                return True
            elif item.promoted and c.user_is_sponsor:
                return True
            else:
                # will throw a legitimate 500 if this isn't a link or
                # comment, because this should only be used on links and
                # comments
                subreddit = item.subreddit_slow
                if subreddit.can_distinguish(c.user):
                    return True
        abort(403,'forbidden')

class VSrCanBan(VByName):
    def run(self, thing_name):
        if c.user_is_admin:
            return True
        elif c.user_is_loggedin:
            item = VByName.run(self, thing_name)
            # will throw a legitimate 500 if this isn't a link or
            # comment, because this should only be used on links and
            # comments
            subreddit = item.subreddit_slow
            if subreddit.is_moderator_with_perms(c.user, 'posts'):
                return True
        abort(403,'forbidden')

class VSrSpecial(VByName):
    def run(self, thing_name):
        if c.user_is_admin:
            return True
        elif c.user_is_loggedin:
            item = VByName.run(self, thing_name)
            # will throw a legitimate 500 if this isn't a link or
            # comment, because this should only be used on links and
            # comments
            subreddit = item.subreddit_slow
            if subreddit.is_special(c.user):
                return True
        abort(403,'forbidden')


class VSubmitParent(VByName):
    def run(self, fullname, fullname2):
        #for backwards compatability (with iphone app)
        fullname = fullname or fullname2
        if fullname:
            parent = VByName.run(self, fullname)
            if not isinstance(parent, (Comment, Link, Message)):
                abort(403, "forbidden")

            if parent:
                if c.user_is_loggedin and parent.author_id in c.user.enemies:
                    self.set_error(errors.USER_BLOCKED)
                if parent._deleted:
                    if isinstance(parent, Link):
                        self.set_error(errors.DELETED_LINK)
                    else:
                        self.set_error(errors.DELETED_COMMENT)
                if parent._spam and isinstance(parent, Comment):
                    # Only author, mod or admin can reply to removed comments
                    can_reply = (c.user_is_loggedin and
                                 (parent.author_id == c.user._id or
                                  c.user_is_admin or
                                  parent.subreddit_slow.is_moderator(c.user)))
                    if not can_reply:
                        self.set_error(errors.DELETED_COMMENT)
            if isinstance(parent, Message):
                return parent
            else:
                link = parent
                if isinstance(parent, Comment):
                    link = Link._byID(parent.link_id, data=True)
                if link and c.user_is_loggedin and can_comment_link(link):
                    return parent
        #else
        abort(403, "forbidden")

    def param_docs(self):
        return {
            self.param[0]: "[fullname](#fullnames) of parent thing",
        }

class VSubmitSR(Validator):
    def __init__(self, srname_param, linktype_param=None, promotion=False):
        self.require_linktype = False
        self.promotion = promotion

        if linktype_param:
            self.require_linktype = True
            Validator.__init__(self, (srname_param, linktype_param))
        else:
            Validator.__init__(self, srname_param)

    def run(self, sr_name, link_type = None):
        if not sr_name:
            self.set_error(errors.SUBREDDIT_REQUIRED)
            return None

        try:
            sr = Subreddit._by_name(str(sr_name).strip())
        except (NotFound, AttributeError, UnicodeEncodeError):
            self.set_error(errors.SUBREDDIT_NOEXIST)
            return

        if not c.user_is_loggedin or not sr.can_submit(c.user, self.promotion):
            self.set_error(errors.SUBREDDIT_NOTALLOWED)
            return

        if self.require_linktype:
            if link_type not in ('link', 'self'):
                self.set_error(errors.INVALID_OPTION)
                return
            elif link_type == "link" and not sr.can_submit_link(c.user):
                self.set_error(errors.NO_LINKS)
                return
            elif link_type == "self" and not sr.can_submit_text(c.user):
                self.set_error(errors.NO_SELFS)
                return

        return sr

    def param_docs(self):
        return {
            self.param[0]: "name of a subreddit",
        }

class VSubscribeSR(VByName):
    def __init__(self, srid_param, srname_param):
        VByName.__init__(self, (srid_param, srname_param))

    def run(self, sr_id, sr_name):
        if sr_id:
            return VByName.run(self, sr_id)
        elif not sr_name:
            return

        try:
            sr = Subreddit._by_name(str(sr_name).strip())
        except (NotFound, AttributeError, UnicodeEncodeError):
            self.set_error(errors.SUBREDDIT_NOEXIST)
            return

        return sr

    def param_docs(self):
        return {
            self.param[0]: "the [fullname](#fullname) of a subreddit",
        }

MIN_PASSWORD_LENGTH = 3

class VPassword(Validator):
    def run(self, password, verify):
        if not (password and len(password) >= MIN_PASSWORD_LENGTH):
            self.set_error(errors.BAD_PASSWORD)
        elif verify != password:
            self.set_error(errors.BAD_PASSWORD_MATCH)
        else:
            return password.encode('utf8')

    def param_docs(self):
        return {
            self.param[0]: "the new password",
            self.param[1]: "the password again (for verification)",
        }

user_rx = re.compile(r"\A[\w-]{3,20}\Z", re.UNICODE)

def chkuser(x):
    if x is None:
        return None
    try:
        if any(ch.isspace() for ch in x):
            return None
        return str(x) if user_rx.match(x) else None
    except TypeError:
        return None
    except UnicodeEncodeError:
        return None

class VUname(VRequired):
    def __init__(self, item, *a, **kw):
        VRequired.__init__(self, item, errors.BAD_USERNAME, *a, **kw)
    def run(self, user_name):
        user_name = chkuser(user_name)
        if not user_name:
            return self.error(errors.BAD_USERNAME)
        else:
            try:
                a = Account._by_name(user_name, True)
                if a._deleted:
                   return self.error(errors.USERNAME_TAKEN_DEL)
                else:
                   return self.error(errors.USERNAME_TAKEN)
            except NotFound:
                return user_name

    def param_docs(self):
        return {
            self.param[0]: "a valid, unused, username",
        }

class VLoggedOut(Validator):
    def run(self):
        if c.user_is_loggedin:
            self.set_error(errors.LOGGED_IN)

class VLogin(VRequired):
    def __init__(self, item, *a, **kw):
        VRequired.__init__(self, item, errors.WRONG_PASSWORD, *a, **kw)

    def run(self, user_name, password):
        user_name = chkuser(user_name)
        user = None
        if user_name:
            try:
                str(password)
            except UnicodeEncodeError:
                password = password.encode('utf8')
            user = valid_login(user_name, password)
        if not user:
            self.error()
            return False
        return user

class VThrottledLogin(VLogin):
    def __init__(self, *args, **kwargs):
        VLogin.__init__(self, *args, **kwargs)
        self.vdelay = VDelay("login")
        self.vlength = VLength("user", max_length=100)

    def run(self, username, password):
        if username:
            username = username.strip()
        username = self.vlength.run(username)

        self.vdelay.run()
        if (errors.RATELIMIT, "vdelay") in c.errors:
            return False

        user = VLogin.run(self, username, password)
        if not user:
            VDelay.record_violation("login", seconds=1, growfast=True)
            c.errors.add(errors.WRONG_PASSWORD, field=self.param[1])
        else:
            return user

    def param_docs(self):
        return {
            self.param[0]: "a username",
            self.param[1]: "the user's password",
        }

class VSanitizedUrl(Validator):
    def run(self, url):
        return utils.sanitize_url(url)

    def param_docs(self):
        return {self.param: "a valid URL"}


class VUrl(VRequired):
    def __init__(self, item, allow_self=True, require_scheme=False,
                 valid_schemes=utils.VALID_SCHEMES, *a, **kw):
        self.allow_self = allow_self
        self.require_scheme = require_scheme
        self.valid_schemes = valid_schemes
        VRequired.__init__(self, item, errors.NO_URL, *a, **kw)

    def run(self, url):
        if not url:
            return self.error(errors.NO_URL)

        url = utils.sanitize_url(url, require_scheme=self.require_scheme,
                                 valid_schemes=self.valid_schemes)
        if not url:
            return self.error(errors.BAD_URL)

        if url == 'self':
            if self.allow_self:
                return url
            else:
                self.error(errors.BAD_URL)
        else:
            return url

    def param_docs(self):
        return {self.param: "a valid URL"}


class VRedirectUri(VUrl):
    def __init__(self, item, valid_schemes=None, *a, **kw):
        VUrl.__init__(self, item, allow_self=False, require_scheme=True,
                      valid_schemes=valid_schemes, *a, **kw)

    def param_docs(self):
        doc = "a valid URI"
        if self.valid_schemes:
            doc += " with one of the following schemes: "
            doc += ", ".join(self.valid_schemes)
        return {self.param: doc}


class VShamedDomain(Validator):
    def run(self, url):
        if not url:
            return

        is_shamed, domain, reason = is_shamed_domain(url)

        if is_shamed:
            self.set_error(errors.DOMAIN_BANNED, dict(domain=domain,
                                                      reason=reason))

class VExistingUname(VRequired):
    def __init__(self, item, *a, **kw):
        VRequired.__init__(self, item, errors.NO_USER, *a, **kw)

    def run(self, name):
        if name and name.startswith('~') and c.user_is_admin:
            try:
                user_id = int(name[1:])
                return Account._byID(user_id, True)
            except (NotFound, ValueError):
                self.error(errors.USER_DOESNT_EXIST)

        # make sure the name satisfies our user name regexp before
        # bothering to look it up.
        name = chkuser(name)
        if name:
            try:
                return Account._by_name(name)
            except NotFound:
                self.error(errors.USER_DOESNT_EXIST)
        else:
            self.error()

    def param_docs(self):
        return {
            self.param: 'the name of an existing user'
        }

class VMessageRecipient(VExistingUname):
    def run(self, name):
        if not name:
            return self.error()
        is_subreddit = False
        if name.startswith('/r/'):
            name = name[3:]
            is_subreddit = True
        elif name.startswith('#'):
            name = name[1:]
            is_subreddit = True
        if is_subreddit:
            try:
                s = Subreddit._by_name(name)
                if isinstance(s, FakeSubreddit):
                    raise NotFound, "fake subreddit"
                if s._spam:
                    raise NotFound, "banned subreddit"
                return s
            except NotFound:
                self.set_error(errors.SUBREDDIT_NOEXIST)
        else:
            account = VExistingUname.run(self, name)
            if account and account._id in c.user.enemies:
                self.set_error(errors.USER_BLOCKED)
            else:
                return account

class VUserWithEmail(VExistingUname):
    def run(self, name):
        user = VExistingUname.run(self, name)
        if not user or not hasattr(user, 'email') or not user.email:
            return self.error(errors.NO_EMAIL_FOR_USER)
        return user


class VBoolean(Validator):
    def run(self, val):
        if val is True or val is False:
            # val is already a bool object, no processing needed
            return val
        lv = str(val).lower()
        if lv == 'off' or lv == '' or lv[0] in ("f", "n"):
            return False
        return bool(val)

    def param_docs(self):
        return {
            self.param: 'boolean value',
        }

class VNumber(Validator):
    def __init__(self, param, min=None, max=None, coerce = True,
                 error=errors.BAD_NUMBER, num_default=None,
                 *a, **kw):
        self.min = self.cast(min) if min is not None else None
        self.max = self.cast(max) if max is not None else None
        self.coerce = coerce
        self.error = error
        self.num_default = num_default
        Validator.__init__(self, param, *a, **kw)

    def cast(self, val):
        raise NotImplementedError

    def _set_error(self):
        if self.max is None and self.min is None:
            range = ""
        elif self.max is None:
            range = _("%(min)d to any") % dict(min=self.min)
        elif self.min is None:
            range = _("any to %(max)d") % dict(max=self.max)
        else:
            range = _("%(min)d to %(max)d") % dict(min=self.min, max=self.max)
        self.set_error(self.error, msg_params=dict(range=range))

    def run(self, val):
        if not val:
            return self.num_default
        try:
            val = self.cast(val)
            if self.min is not None and val < self.min:
                if self.coerce:
                    val = self.min
                else:
                    raise ValueError, ""
            elif self.max is not None and val > self.max:
                if self.coerce:
                    val = self.max
                else:
                    raise ValueError, ""
            return val
        except ValueError:
            self._set_error()

class VInt(VNumber):
    def cast(self, val):
        return int(val)

    def param_docs(self):
        if self.min is not None and self.max is not None:
            description = "an integer between %d and %d" % (self.min, self.max)
        elif self.min is not None:
            description = "an integer greater than %d" % self.min
        elif self.max is not None:
            description = "an integer less than %d" % self.max
        else:
            description = "an integer"

        if self.num_default is not None:
            description += " (default: %d)" % self.num_default

        return {self.param: description}

class VFloat(VNumber):
    def cast(self, val):
        return float(val)

class VCssName(Validator):
    """
    returns a name iff it consists of alphanumeric characters and
    possibly "-", and is below the length limit.
    """

    r_css_name = re.compile(r"\A[a-zA-Z0-9\-]{1,100}\Z")

    def run(self, name):
        if name:
            if self.r_css_name.match(name):
                return name
            else:
                self.set_error(errors.BAD_CSS_NAME)
        return ''

    def param_docs(self):
        return {
            self.param: "a valid subreddit image name",
        }


class VMenu(Validator):

    def __init__(self, param, menu_cls, remember = True, **kw):
        self.nav = menu_cls
        self.remember = remember
        param = (menu_cls.name, param)
        Validator.__init__(self, param, **kw)

    def run(self, sort, where):
        if self.remember:
            pref = "%s_%s" % (where, self.nav.name)
            user_prefs = copy(c.user.sort_options) if c.user else {}
            user_pref = user_prefs.get(pref)

            # check to see if a default param has been set
            if not sort:
                sort = user_pref

        # validate the sort
        if sort not in self.nav.options:
            sort = self.nav.default

        # commit the sort if changed and if this is a POST request
        if (self.remember and c.user_is_loggedin and sort != user_pref
            and request.method.upper() == 'POST'):
            user_prefs[pref] = sort
            c.user.sort_options = user_prefs
            user = c.user
            user._commit()

        return sort

    def param_docs(self):
        return {
            self.param[0]: 'one of (%s)' % ', '.join("`%s`" % s
                                                  for s in self.nav.options),
        }


class VRatelimit(Validator):
    def __init__(self, rate_user = False, rate_ip = False,
                 prefix = 'rate_', error = errors.RATELIMIT, *a, **kw):
        self.rate_user = rate_user
        self.rate_ip = rate_ip
        self.prefix = prefix
        self.error = error
        self.seconds = None
        Validator.__init__(self, *a, **kw)

    def run (self):
        from r2.models.admintools import admin_ratelimit

        if g.disable_ratelimit:
            return

        if c.user_is_loggedin and not admin_ratelimit(c.user):
            return

        to_check = []
        if self.rate_user and c.user_is_loggedin:
            to_check.append('user' + str(c.user._id36))
        if self.rate_ip:
            to_check.append('ip' + str(request.ip))

        r = g.cache.get_multi(to_check, self.prefix)
        if r:
            expire_time = max(r.values())
            time = utils.timeuntil(expire_time)

            g.log.debug("rate-limiting %s from %s" % (self.prefix, r.keys()))

            # when errors have associated field parameters, we'll need
            # to add that here
            if self.error == errors.RATELIMIT:
                from datetime import datetime
                delta = expire_time - datetime.now(g.tz)
                self.seconds = delta.total_seconds()
                if self.seconds < 3:  # Don't ratelimit within three seconds
                    return
                self.set_error(errors.RATELIMIT, {'time': time},
                               field='ratelimit', code=429)
            else:
                self.set_error(self.error)

    @classmethod
    def ratelimit(self, rate_user = False, rate_ip = False, prefix = "rate_",
                  seconds = None):
        to_set = {}
        if seconds is None:
            seconds = g.RL_RESET_SECONDS
        expire_time = datetime.now(g.tz) + timedelta(seconds = seconds)
        if rate_user and c.user_is_loggedin:
            to_set['user' + str(c.user._id36)] = expire_time
        if rate_ip:
            to_set['ip' + str(request.ip)] = expire_time
        g.cache.set_multi(to_set, prefix = prefix, time = seconds)

class VDelay(Validator):
    def __init__(self, category, *a, **kw):
        self.category = category
        self.seconds = None
        Validator.__init__(self, *a, **kw)

    def run (self):
        if g.disable_ratelimit:
            return
        key = "VDelay-%s-%s" % (self.category, request.ip)
        prev_violations = g.cache.get(key)
        if prev_violations:
            time = utils.timeuntil(prev_violations["expire_time"])
            remaining = prev_violations["expire_time"] - datetime.now(g.tz)
            self.seconds = remaining.total_seconds()
            if self.seconds >= 3:
                self.set_error(errors.RATELIMIT, {'time': time},
                               field='vdelay', code=429)

    @classmethod
    def record_violation(self, category, seconds = None, growfast=False):
        if seconds is None:
            seconds = g.RL_RESET_SECONDS

        key = "VDelay-%s-%s" % (category, request.ip)
        prev_violations = g.memcache.get(key)
        if prev_violations is None:
            prev_violations = dict(count=0)

        num_violations = prev_violations["count"]

        if growfast:
            multiplier = 3 ** num_violations
        else:
            multiplier = 1

        max_duration = 8 * 3600
        duration = min(seconds * multiplier, max_duration)

        expire_time = (datetime.now(g.tz) +
                       timedelta(seconds = duration))

        prev_violations["expire_time"] = expire_time
        prev_violations["duration"] = duration
        prev_violations["count"] += 1

        with g.make_lock("record_violation", "lock-" + key, timeout=5, verbose=False):
            existing = g.memcache.get(key)
            if existing and existing["count"] > prev_violations["count"]:
                g.log.warning("Tried to set %s to count=%d, but found existing=%d"
                             % (key, prev_violations["count"], existing["count"]))
            else:
                g.cache.set(key, prev_violations, max_duration)

class VCommentIDs(Validator):
    def run(self, id_str):
        if id_str:
            cids = [int(i, 36) for i in id_str.split(',')]
            comments = Comment._byID(cids, data=True, return_dict = False)
            return comments
        return []

    def param_docs(self):
        return {
            self.param: "a comma-delimited list of comment ID36s",
        }


class CachedUser(object):
    def __init__(self, cache_prefix, user, key):
        self.cache_prefix = cache_prefix
        self.user = user
        self.key = key

    def clear(self):
        if self.key and self.cache_prefix:
            g.cache.delete(str(self.cache_prefix + "_" + self.key))

class VOneTimeToken(Validator):
    def __init__(self, model, param, *args, **kwargs):
        self.model = model
        Validator.__init__(self, param, *args, **kwargs)

    def run(self, key):
        token = self.model.get_token(key)

        if token:
            return token
        else:
            self.set_error(errors.EXPIRED)
            return None

class VOneOf(Validator):
    def __init__(self, param, options = (), *a, **kw):
        Validator.__init__(self, param, *a, **kw)
        self.options = options

    def run(self, val):
        if self.options and val not in self.options:
            self.set_error(errors.INVALID_OPTION, code=400)
            return self.default
        else:
            return val

    def param_docs(self):
        return {
            self.param: 'one of (%s)' % ', '.join("`%s`" % s
                                                  for s in self.options),
        }


class VList(Validator):
    def __init__(self, param, separator=",", choices=None,
                 error=errors.INVALID_OPTION, *a, **kw):
        Validator.__init__(self, param, *a, **kw)
        self.separator = separator
        self.choices = choices
        self.error = error

    def run(self, items):
        if not items:
            return None
        all_values = items.split(self.separator)
        if self.choices is None:
            return all_values

        values = []
        for val in all_values:
            if val in self.choices:
                values.append(val)
            else:
                msg_params = {"choice": val}
                self.set_error(self.error, msg_params=msg_params,
                               code=400)
        return values

    # Not i18n'able, but param_docs are not currently i18n'ed
    NICE_SEP = {",": "comma"}
    def param_docs(self):
        if self.choices:
            msg = ("A %(separator)s-separated list of items from "
                   "this set:\n\n%(choices)s")
            choices = "`" + "`  \n`".join(self.choices) + "`"
        else:
            msg = "A %(separator)s-separated list of items"
            choices = None

        sep = self.NICE_SEP.get(self.separator, self.separator)
        docs = msg % {"separator": sep, "choices": choices}
        return {self.param: docs}


class VPriority(Validator):
    def run(self, val):
        if c.user_is_sponsor:
            return PROMOTE_PRIORITIES.get(val, PROMOTE_DEFAULT_PRIORITY)
        else:
            return PROMOTE_DEFAULT_PRIORITY


class VLocation(Validator):
    default_param = ("country", "region", "metro")

    def run(self, country, region, metro):
        # some browsers are sending "null" rather than omitting the input when
        # the select is disabled
        country, region, metro = map(lambda val: None if val == "null" else val,
                                     [country, region, metro])

        if not (country or region or metro):
            return None

        if not (country and not (region or metro) or
                (country and region and metro)):
            # can target just country or country, region, and metro
            self.set_error(errors.INVALID_LOCATION, code=400)
        elif (country not in g.locations or
              region and region not in g.locations[country]['regions'] or
              metro and metro not in g.locations[country]['regions'][region]['metros']):
            self.set_error(errors.INVALID_LOCATION, code=400)
        else:
            return Location(country, region, metro)


class VImageType(Validator):
    def run(self, img_type):
        if not img_type in ('png', 'jpg'):
            return 'png'
        return img_type

    def param_docs(self):
        return {
            self.param: "one of `png` or `jpg` (default: `png`)",
        }


class ValidEmails(Validator):
    """Validates a list of email addresses passed in as a string and
    delineated by whitespace, ',' or ';'.  Also validates quantity of
    provided emails.  Returns a list of valid email addresses on
    success"""

    separator = re.compile(r'[^\s,;]+')
    email_re  = re.compile(r'.+@.+\..+')

    def __init__(self, param, num = 20, **kw):
        self.num = num
        Validator.__init__(self, param = param, **kw)

    def run(self, emails0):
        emails = set(self.separator.findall(emails0) if emails0 else [])
        failures = set(e for e in emails if not self.email_re.match(e))
        emails = emails - failures

        # make sure the number of addresses does not exceed the max
        if self.num > 0 and len(emails) + len(failures) > self.num:
            # special case for 1: there should be no delineators at all, so
            # send back original string to the user
            if self.num == 1:
                self.set_error(errors.BAD_EMAILS,
                             {'emails': '"%s"' % emails0})
            # else report the number expected
            else:
                self.set_error(errors.TOO_MANY_EMAILS,
                             {'num': self.num})
        # correct number, but invalid formatting
        elif failures:
            self.set_error(errors.BAD_EMAILS,
                         {'emails': ', '.join(failures)})
        # no emails
        elif not emails:
            self.set_error(errors.NO_EMAILS)
        else:
            # return single email if one is expected, list otherwise
            return list(emails)[0] if self.num == 1 else emails

class ValidEmailsOrExistingUnames(Validator):
    """Validates a list of mixed email addresses and usernames passed in
    as a string, delineated by whitespace, ',' or ';'.  Validates total
    quantity too while we're at it.  Returns a tuple of the form
    (e-mail addresses, user account objects)"""

    def __init__(self, param, num=20, **kw):
        self.num = num
        Validator.__init__(self, param=param, **kw)

    def run(self, items):
        # Use ValidEmails separator to break the list up
        everything = set(ValidEmails.separator.findall(items) if items else [])

        # Use ValidEmails regex to divide the list into e-mail and other
        emails = set(e for e in everything if ValidEmails.email_re.match(e))
        failures = everything - emails

        # Run the rest of the validator against the e-mails list
        ve = ValidEmails(self.param, self.num)
        if len(emails) > 0:
            ve.run(", ".join(emails))

        # ValidEmails will add to c.errors for us, so do nothing if that fails
        # Elsewise, on with the users
        if not ve.has_errors:
            users = set()  # set of accounts
            validusers = set()  # set of usernames to subtract from failures

            # Now steal from VExistingUname:
            for uname in failures:
                check = uname
                if re.match('/u/', uname):
                    check = check[3:]
                veu = VExistingUname(check)
                account = veu.run(check)
                if account:
                    validusers.add(uname)
                    users.add(account)

            # We're fine if all our failures turned out to be valid users
            if len(users) == len(failures):
                # ValidEmails checked to see if there were too many addresses,
                # check to see if there's enough left-over space for users
                remaining = self.num - len(emails)
                if len(users) > remaining:
                    if self.num == 1:
                        # We only wanted one, and we got it as an e-mail,
                        # so complain.
                        self.set_error(errors.BAD_EMAILS,
                                       {"emails": '"%s"' % items})
                    else:
                        # Too many total
                        self.set_error(errors.TOO_MANY_EMAILS,
                                       {"num": self.num})
                elif len(users) + len(emails) == 0:
                    self.set_error(errors.NO_EMAILS)
                else:
                    # It's all good!
                    return (emails, users)
            else:
                failures = failures - validusers
                self.set_error(errors.BAD_EMAILS,
                               {'emails': ', '.join(failures)})

class VCnameDomain(Validator):
    domain_re  = re.compile(r'\A([\w\-_]+\.)+[\w]+\Z')

    def run(self, domain):
        if (domain
            and (not self.domain_re.match(domain)
                 or domain.endswith('.' + g.domain)
                 or domain.endswith('.' + g.media_domain)
                 or len(domain) > 300)):
            self.set_error(errors.BAD_CNAME)
        elif domain:
            try:
                return str(domain).lower()
            except UnicodeEncodeError:
                self.set_error(errors.BAD_CNAME)

    def param_docs(self):
        # cnames are dead; don't advertise this.
        return {}


# NOTE: make sure *never* to have res check these are present
# otherwise, the response could contain reference to these errors...!
class ValidIP(Validator):
    def run(self):
        if is_banned_IP(request.ip):
            self.set_error(errors.BANNED_IP)
        return request.ip

class VDate(Validator):
    """
    Date checker that accepts string inputs.

    Optional parameters 'earliest' and 'latest' specify the acceptable timedelta
    offsets (offsets are inclusive).

    Error conditions:
       * BAD_DATE on mal-formed date strings (strptime parse failure)
       * DATE_TOO_EARLY and DATE_TOO_LATE on range errors.

    """
    def __init__(self, param, earliest=None, latest=None,
                 sponsor_override = False,
                 reference_date = lambda : datetime.now(g.tz),
                 business_days = False,
                 format = "%m/%d/%Y"):
        self.earliest = earliest
        self.latest = latest

        # are weekends to be exluded from the interval?
        self.business_days = business_days

        self.format = format

        # function for generating "now"
        self.reference_date = reference_date

        # do we let admins and sponsors override date range checking?
        self.override = sponsor_override
        Validator.__init__(self, param)

    def run(self, date):
        now = self.reference_date()
        earliest = latest = None
        if self.earliest:
            earliest = make_offset_date(now, self.earliest.days,
                                        business_days=self.business_days)
        if self.latest:
            latest = make_offset_date(now, self.latest.days,
                                      business_days=self.business_days)
        override = c.user_is_sponsor and self.override
        try:
            date = datetime.strptime(date, self.format)
            if not override:
                if earliest and not date.date() >= earliest.date():
                    self.set_error(errors.DATE_TOO_EARLY,
                                   {'day': earliest.strftime(self.format)})

                if latest and not date.date() <= latest.date():
                    self.set_error(errors.DATE_TOO_LATE,
                                   {'day': latest.strftime(self.format)})
            return date.replace(tzinfo=g.tz)
        except (ValueError, TypeError):
            self.set_error(errors.BAD_DATE)

class VDateRange(Validator):
    """
    Adds range validation to VDate.  In addition to satisfying
    future/past requirements in VDate, two date fields must be
    provided and they must be in order.

    If required is False, then the dates may be omitted without
    causing an error (but if a start date is provided, an end
    date MUST be provided as well).

    Additional Error conditions:
      * BAD_DATE_RANGE if start_date is not less than end_date
    """
    def __init__(self, param, max_range=None, required=True, **kw):
        self.max_range = max_range
        self.required = required
        self.vstart = VDate(param[0], **kw)
        self.vend = VDate(param[1], **kw)
        Validator.__init__(self, param)

    def run(self, start, end):
        try:
            start_date = self.vstart.run(start)
            end_date = self.vend.run(end)
            # If either date is missing and dates are "required",
            # it's a bad range. Additionally, if one date is missing,
            # but the other is provided, it's always an error.
            if not start_date or not end_date:
                if self.required or (not start_date and not end_date):
                    self.set_error(errors.BAD_DATE_RANGE)
                return (start_date, end_date)
            elif end_date < start_date:
                self.set_error(errors.BAD_DATE_RANGE)
            elif self.max_range and end_date - start_date > self.max_range:
                self.set_error(errors.DATE_RANGE_TOO_LARGE,
                               {'days': self.max_range})
            return (start_date, end_date)
        except ValueError:
            # insufficient number of arguments provided (expect 2)
            self.set_error(errors.BAD_DATE_RANGE)


class VDestination(Validator):
    def __init__(self, param = 'dest', default = "", **kw):
        Validator.__init__(self, param, default, **kw)

    def run(self, dest):
        if not dest:
            dest = self.default or add_sr("/")

        ld = dest.lower()
        if ld.startswith(('/', 'http://', 'https://')):
            u = UrlParser(dest)

            if u.is_reddit_url(c.site):
                return dest

        ip = getattr(request, "ip", "[unknown]")
        fp = getattr(request, "fullpath", "[unknown]")
        dm = c.domain or "[unknown]"
        cn = c.cname or "[unknown]"

        log_text("invalid redirect",
                 "%s attempted to redirect from %s to %s with domain %s and cname %s"
                      % (ip, fp, dest, dm, cn),
                 "info")

        return "/"

    def param_docs(self):
        return {
            self.param: 'destination url (must be same-domain)',
        }

class ValidAddress(Validator):
    def set_error(self, msg, field):
        Validator.set_error(self, errors.BAD_ADDRESS,
                            dict(message=msg), field = field)

    def run(self, firstName, lastName, company, address,
            city, state, zipCode, country, phoneNumber):
        if not firstName:
            self.set_error(_("please provide a first name"), "firstName")
        elif not lastName:
            self.set_error(_("please provide a last name"), "lastName")
        elif not address:
            self.set_error(_("please provide an address"), "address")
        elif not city:
            self.set_error(_("please provide your city"), "city")
        elif not state:
            self.set_error(_("please provide your state"), "state")
        elif not zipCode:
            self.set_error(_("please provide your zip or post code"), "zip")
        elif not country:
            self.set_error(_("please provide your country"), "country")

        # Make sure values don't exceed max length defined in the authorize.net
        # xml schema: https://api.authorize.net/xml/v1/schema/AnetApiSchema.xsd
        max_lengths = [
            (firstName, 50, 'firstName'), # (argument, max len, form field name)
            (lastName, 50, 'lastName'),
            (company, 50, 'company'),
            (address, 60, 'address'),
            (city, 40, 'city'),
            (state, 40, 'state'),
            (zipCode, 20, 'zip'),
            (country, 60, 'country'),
            (phoneNumber, 255, 'phoneNumber')
        ]
        for (arg, max_length, form_field_name) in max_lengths:
            if arg and len(arg) > max_length:
                self.set_error(_("max length %d characters" % max_length), form_field_name)

        if not self.has_errors:
            return Address(firstName = firstName,
                           lastName = lastName,
                           company = company or "",
                           address = address,
                           city = city, state = state,
                           zip = zipCode, country = country,
                           phoneNumber = phoneNumber or "")

class ValidCard(Validator):
    valid_date = re.compile(r"\d\d\d\d-\d\d")
    def set_error(self, msg, field):
        Validator.set_error(self, errors.BAD_CARD,
                            dict(message=msg), field = field)

    def run(self, cardNumber, expirationDate, cardCode):
        has_errors = False

        cardNumber = cardNumber or ""
        if not (cardNumber.isdigit() and 13 <= len(cardNumber) <= 16):
            self.set_error(_("credit card numbers should be 13 to 16 digits"),
                           "cardNumber")
            has_errors = True

        if not self.valid_date.match(expirationDate or ""):
            self.set_error(_("dates should be YYYY-MM"), "expirationDate")
            has_errors = True
        else:
            now = datetime.now(g.tz)
            yyyy, mm = expirationDate.split("-")
            year = int(yyyy)
            month = int(mm)
            if month < 1 or month > 12:
                self.set_error(_("month must be in the range 01..12"), "expirationDate")
                has_errors = True
            elif datetime(year, month, 1) < datetime(now.year, now.month, 1):
                self.set_error(_("expiration date must be in the future"), "expirationDate")
                has_errors = True

        cardCode = cardCode or ""
        if not (cardCode.isdigit() and 3 <= len(cardCode) <= 4):
            self.set_error(_("card verification codes should be 3 or 4 digits"),
                           "cardCode")
            has_errors = True

        if not has_errors:
            return CreditCard(cardNumber = cardNumber,
                              expirationDate = expirationDate,
                              cardCode = cardCode)

class VTarget(Validator):
    target_re = re.compile("\A[\w_-]{3,20}\Z")
    def run(self, name):
        if name and self.target_re.match(name):
            return name

    def param_docs(self):
        # this is just for htmllite and of no interest to api consumers
        return {}

class VFlairAccount(VRequired):
    def __init__(self, item, *a, **kw):
        VRequired.__init__(self, item, errors.BAD_FLAIR_TARGET, *a, **kw)

    def _lookup(self, name, allow_deleted):
        try:
            return Account._by_name(name, allow_deleted=allow_deleted)
        except NotFound:
            return None

    def run(self, name):
        if not name:
            return self.error()
        return (
            self._lookup(name, False)
            or self._lookup(name, True)
            or self.error())

    def param_docs(self):
        return {self.param: _("a user by name")}

class VFlairLink(VRequired):
    def __init__(self, item, *a, **kw):
        VRequired.__init__(self, item, errors.BAD_FLAIR_TARGET, *a, **kw)

    def run(self, name):
        if not name:
            return self.error()
        try:
            return Link._by_fullname(name, data=True)
        except NotFound:
            return self.error()

    def param_docs(self):
        return {self.param: _("a [fullname](#fullname) of a link")}

class VFlairCss(VCssName):
    def __init__(self, param, max_css_classes=10, **kw):
        self.max_css_classes = max_css_classes
        VCssName.__init__(self, param, **kw)

    def run(self, css):
        if not css:
            return css

        names = css.split()
        if len(names) > self.max_css_classes:
            self.set_error(errors.TOO_MUCH_FLAIR_CSS)
            return ''

        for name in names:
            if not self.r_css_name.match(name):
                self.set_error(errors.BAD_CSS_NAME)
                return ''

        return css

class VFlairText(VLength):
    def __init__(self, param, max_length=64, **kw):
        VLength.__init__(self, param, max_length, **kw)

class VFlairTemplateByID(VRequired):
    def __init__(self, param, **kw):
        VRequired.__init__(self, param, None, **kw)

    def run(self, flair_template_id):
        try:
            return FlairTemplateBySubredditIndex.get_template(
                c.site._id, flair_template_id)
        except tdb_cassandra.NotFound:
            return None

class VOneTimePassword(Validator):
    allowed_skew = [-1, 0, 1]  # allow a period of skew on either side of now
    ratelimit = 3  # maximum number of tries per period

    def __init__(self, param, required):
        self.required = required
        Validator.__init__(self, param)

    @classmethod
    def validate_otp(cls, secret, password):
        # is the password a valid format and has it been used?
        try:
            key = "otp-%s-%d" % (c.user._id36, int(password))
        except (TypeError, ValueError):
            valid_and_unused = False
        else:
            # leave this key around for one more time period than the maximum
            # number of time periods we'll check for valid passwords
            key_ttl = totp.PERIOD * (len(cls.allowed_skew) + 1)
            valid_and_unused = g.cache.add(key, True, time=key_ttl)

        # check the password (allowing for some clock-skew as 2FA-users
        # frequently travel at relativistic velocities)
        if valid_and_unused:
            for skew in cls.allowed_skew:
                expected_otp = totp.make_totp(secret, skew=skew)
                if constant_time_compare(password, expected_otp):
                    return True

        return False

    def run(self, password):
        # does the user have 2FA configured?
        secret = c.user.otp_secret
        if not secret:
            if self.required:
                self.set_error(errors.NO_OTP_SECRET)
            return

        # do they have the otp cookie instead?
        if c.otp_cached:
            return

        # make sure they're not trying this too much
        if not g.disable_ratelimit:
            current_password = totp.make_totp(secret)
            key = "otp-tries-" + current_password
            g.cache.add(key, 0)
            recent_attempts = g.cache.incr(key)
            if recent_attempts > self.ratelimit:
                self.set_error(errors.RATELIMIT, dict(time="30 seconds"))
                return

        # check the password
        if self.validate_otp(secret, password):
            return

        # if we got this far, their password was wrong, invalid or already used
        self.set_error(errors.WRONG_PASSWORD)

class VOAuth2ClientID(VRequired):
    default_param = "client_id"
    default_param_doc = _("an app")
    def __init__(self, param=None, *a, **kw):
        VRequired.__init__(self, param, errors.OAUTH2_INVALID_CLIENT, *a, **kw)

    def run(self, client_id):
        client_id = VRequired.run(self, client_id)
        if client_id:
            client = OAuth2Client.get_token(client_id)
            if client and not getattr(client, 'deleted', False):
                return client
            else:
                self.error()

    def param_docs(self):
        return {self.default_param: self.default_param_doc}

class VOAuth2ClientDeveloper(VOAuth2ClientID):
    default_param_doc = _("an app developed by the user")

    def run(self, client_id):
        client = super(VOAuth2ClientDeveloper, self).run(client_id)
        if not client or not client.has_developer(c.user):
            return self.error()
        return client

class VOAuth2Scope(VRequired):
    default_param = "scope"
    def __init__(self, param=None, *a, **kw):
        VRequired.__init__(self, param, errors.OAUTH2_INVALID_SCOPE, *a, **kw)

    def run(self, scope):
        scope = VRequired.run(self, scope)
        if scope:
            parsed_scope = OAuth2Scope(scope)
            if parsed_scope.is_valid():
                return parsed_scope
            else:
                self.error()

class VOAuth2RefreshToken(Validator):
    def __init__(self, param, *a, **kw):
        Validator.__init__(self, param, None, *a, **kw)

    def run(self, refresh_token_id):
        if refresh_token_id:
            try:
                token = OAuth2RefreshToken._byID(refresh_token_id)
            except tdb_cassandra.NotFound:
                self.set_error(errors.OAUTH2_INVALID_REFRESH_TOKEN)
                return None
            if not token.check_valid():
                self.set_error(errors.OAUTH2_INVALID_REFRESH_TOKEN)
                return None
            return token
        else:
            return None

class VPermissions(Validator):
    types = dict(
        moderator=ModeratorPermissionSet,
        moderator_invite=ModeratorPermissionSet,
    )

    def __init__(self, type_param, permissions_param, *a, **kw):
        Validator.__init__(self, (type_param, permissions_param), *a, **kw)

    def run(self, type, permissions):
        permission_class = self.types.get(type)
        if not permission_class:
            self.set_error(errors.INVALID_PERMISSION_TYPE, field=self.param[0])
            return (None, None)
        try:
            perm_set = permission_class.loads(permissions, validate=True)
        except ValueError:
            self.set_error(errors.INVALID_PERMISSIONS, field=self.param[1])
            return (None, None)
        return type, perm_set


class VJSON(Validator):
    def run(self, json_str):
        if not json_str:
            return self.set_error('JSON_PARSE_ERROR', code=400)
        else:
            try:
                return json.loads(json_str)
            except ValueError:
                return self.set_error('JSON_PARSE_ERROR', code=400)

    def param_docs(self):
        return {
            self.param: "JSON data",
        }


class VValidatedJSON(VJSON):
    """Apply validators to the values of JSON formatted data."""
    class ArrayOf(object):
        """A JSON array of objects with the specified schema."""
        def __init__(self, spec):
            self.spec = spec

        def run(self, data):
            if not isinstance(data, list):
                raise RedditError('JSON_INVALID', code=400)

            validated_data = []
            for item in data:
                validated_data.append(self.spec.run(item))
            return validated_data

        def spec_docs(self):
            spec_lines = []
            spec_lines.append('[')
            if hasattr(self.spec, 'spec_docs'):
                array_docs = self.spec.spec_docs()
            else:
                array_docs = self.spec.param_docs()[self.spec.param]
            for line in array_docs.split('\n'):
                spec_lines.append('  ' + line)
            spec_lines[-1] += ','
            spec_lines.append('  ...')
            spec_lines.append(']')
            return '\n'.join(spec_lines)


    class Object(object):
        """A JSON object with validators for specified fields."""
        def __init__(self, spec):
            self.spec = spec

        def run(self, data, ignore_missing=False):
            if not isinstance(data, dict):
                raise RedditError('JSON_INVALID', code=400)

            validated_data = {}
            for key, validator in self.spec.iteritems():
                try:
                    validated_data[key] = validator.run(data[key])
                except KeyError:
                    if ignore_missing:
                        continue
                    raise RedditError('JSON_MISSING_KEY', code=400,
                                      msg_params={'key': key})
            return validated_data

        def spec_docs(self):
            spec_docs = {}
            for key, validator in self.spec.iteritems():
                if hasattr(validator, 'spec_docs'):
                    spec_docs[key] = validator.spec_docs()
                elif hasattr(validator, 'param_docs'):
                    spec_docs.update(validator.param_docs())
                    if validator.docs:
                        spec_docs.update(validator.docs)

            # generate markdown json schema docs
            spec_lines = []
            spec_lines.append('{')
            for key in sorted(spec_docs.keys()):
                key_docs = spec_docs[key]
                # indent any new lines
                key_docs = key_docs.replace('\n', '\n  ')
                spec_lines.append('  "%s": %s,' % (key, key_docs))
            spec_lines.append('}')
            return '\n'.join(spec_lines)

    class PartialObject(Object):
        def run(self, data):
            super_ = super(VValidatedJSON.PartialObject, self)
            return super_.run(data, ignore_missing=True)

    def __init__(self, param, spec, **kw):
        VJSON.__init__(self, param, **kw)
        self.spec = spec

    def run(self, json_str):
        data = VJSON.run(self, json_str)
        if self.has_errors:
            return

        # Note: this relies on the fact that all validator errors are dumped
        # into a global (c.errors) and then checked by @validate.
        return self.spec.run(data)

    def docs_model(self):
        spec_md = self.spec.spec_docs()

        # indent for code formatting
        spec_md = '\n'.join(
            '    ' + line for line in spec_md.split('\n')
        )
        return spec_md

    def param_docs(self):
        return {
            self.param: 'json data:\n\n' + self.docs_model(),
        }


multi_name_rx = re.compile(r"\A[A-Za-z0-9][A-Za-z0-9_]{1,20}\Z")
multi_name_chars_rx = re.compile(r"[^A-Za-z0-9_]")

class VMultiPath(Validator):
    @classmethod
    def normalize(self, path):
        if path[0] != '/':
            path = '/' + path
        path = path.lower().rstrip('/')
        return path

    def run(self, path):
        try:
            require(path)
            path = self.normalize(path)
            require(path.startswith('/user/'))
            user, username, m, name = require_split(path, 5, sep='/')[1:]
            require(m == 'm')
            username = chkuser(username)
            require(username)
        except RequirementException:
            self.set_error('BAD_MULTI_PATH', code=400)
            return

        try:
            require(multi_name_rx.match(name))
        except RequirementException:
            invalid_char = multi_name_chars_rx.search(name)
            if invalid_char:
                char = invalid_char.group()
                if char == ' ':
                    reason = _('no spaces allowed')
                else:
                    reason = _("invalid character: '%s'") % char
            elif name[0] == '_':
                reason = _("can't start with a '_'")
            elif len(name) < 2:
                reason = _('that name is too short')
            elif len(name) > 21:
                reason = _('that name is too long')
            else:
                reason = _("that name isn't going to work")

            self.set_error('BAD_MULTI_NAME', {'reason': reason}, code=400)
            return

        return {'path': path, 'username': username, 'name': name}

    def param_docs(self):
        return {
            self.param: "multireddit url path",
        }


class VMultiByPath(Validator):
    def __init__(self, param, require_view=True, require_edit=False):
        Validator.__init__(self, param)
        self.require_view = require_view
        self.require_edit = require_edit

    def run(self, path):
        path = VMultiPath.normalize(path)
        try:
            multi = LabeledMulti._byID(path)
        except tdb_cassandra.NotFound:
            return self.set_error('MULTI_NOT_FOUND', code=404)

        if not multi or (self.require_view and not multi.can_view(c.user)):
            return self.set_error('MULTI_NOT_FOUND', code=404)
        if self.require_edit and not multi.can_edit(c.user):
            return self.set_error('MULTI_CANNOT_EDIT', code=403)

        return multi

    def param_docs(self):
        return {
            self.param: "multireddit url path",
        }

########NEW FILE########
__FILENAME__ = wiki
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from os.path import normpath
from functools import wraps
import datetime
import re

from pylons.i18n import _

from pylons import c, g, request

from r2.models.wiki import WikiPage, WikiRevision, WikiBadRevision
from r2.lib.validator import (
    Validator,
    VSrModerator,
    set_api_docs,
)
from r2.lib.db import tdb_cassandra

MAX_PAGE_NAME_LENGTH = g.wiki_max_page_name_length

MAX_SEPARATORS = g.wiki_max_page_separators

def this_may_revise(page=None):
    if not c.user_is_loggedin:
        return False
    
    if c.user_is_admin:
        return True
    
    return may_revise(c.site, c.user, page)

def this_may_view(page):
    user = c.user if c.user_is_loggedin else None
    if user and c.user_is_admin:
        return True
    return may_view(c.site, user, page)

def may_revise(sr, user, page=None):    
    if sr.is_moderator_with_perms(user, 'wiki'):
        # Mods may always contribute to non-config pages
        if not page or not page.special:
            return True
    
    if page and page.restricted and not page.special:
        # People may not contribute to restricted pages
        # (Except for special pages)
        return False

    if sr.is_wikibanned(user):
        # Users who are wiki banned in the subreddit may not contribute
        return False
    
    if sr.is_banned(user):
        # If the user is banned from the subreddit, do not allow them to contribute
        return False
    
    if page and not may_view(sr, user, page):
        # Users who are not allowed to view the page may not contribute to the page
        return False
    
    if user.wiki_override == False:
        # global ban
        return False
    
    if page and page.has_editor(user._id36):
        # If the user is an editor on the page, they may edit
        return True

    if (page and page.special and
            sr.is_moderator_with_perms(user, 'config')):
        return True

    if page and page.special:
        # If this is a special page
        # (and the user is not a mod or page editor)
        # They should not be allowed to revise
        return False
    
    if page and page.permlevel > 0:
        # If the page is beyond "anyone may contribute"
        # A normal user should not be allowed to revise
        return False
    
    if sr.is_wikicontributor(user):
        # If the user is a wiki contributor, they may revise
        return True
    
    if sr.wikimode != 'anyone':
        # If the user is not a page editor or wiki contributor,
        # and the mode is not everyone,
        # the user may not edit.
        return False
    
    if not sr.wiki_can_submit(user):
        # If the user can not submit to the subreddit
        # They should not be able to contribute
        return False

    # Use global karma for the frontpage wiki
    karma_sr = sr if sr.wiki_use_subreddit_karma else None

    # Use link or comment karma, whichever is greater
    karma = max(user.karma('link', karma_sr), user.karma('comment', karma_sr))

    if karma < (sr.wiki_edit_karma or 0):
        # If the user has too few karma, they should not contribute
        return False
    
    age = (datetime.datetime.now(g.tz) - user._date).days
    if age < (sr.wiki_edit_age or 0):
        # If they user's account is too young
        # They should not contribute
        return False
    
    # Otherwise, allow them to contribute
    return True

def may_view(sr, user, page):
    # User being None means not logged in
    mod = sr.is_moderator_with_perms(user, 'wiki') if user else False
    
    if mod:
        # Mods may always view
        return True
    
    if page.special:
        # Special pages may always be viewed
        # (Permission level ignored)
        return True
    
    level = page.permlevel
    
    if level < 2:
        # Everyone may view in levels below 2
        return True
    
    if level == 2:
        # Only mods may view in level 2
        return mod
    
    # In any other obscure level,
    # (This should not happen but just in case)
    # nobody may view.
    return False

def normalize_page(page):
    # Ensure there is no side effect if page is None
    page = page or ""
    
    # Replace spaces with underscores
    page = page.replace(' ', '_')
    
    # Case insensitive page names
    page = page.lower()
    
    # Normalize path (And avoid normalizing empty to ".")
    if page:
        page = normpath(page)
    
    # Chop off initial "/", just in case it exists
    page = page.lstrip('/')
    
    return page

class AbortWikiError(Exception):
    pass

page_match_regex = re.compile(r'^[\w_\-/]+\Z')

class VWikiModerator(VSrModerator):
    def __init__(self, fatal=False, *a, **kw):
        VSrModerator.__init__(self, param='page', fatal=fatal, *a, **kw)

    def run(self, page):
        self.perms = ['wiki']
        if page and WikiPage.is_special(page):
            self.perms += ['config']
        VSrModerator.run(self)

class VWikiPageName(Validator):
    def __init__(self, param, error_on_name_normalized=False, *a, **kw):
        self.error_on_name_normalized = error_on_name_normalized
        Validator.__init__(self, param, *a, **kw)
    
    def run(self, page):
        original_page = page
        
        try:
            page = str(page) if page else ""
        except UnicodeEncodeError:
            return self.set_error('INVALID_PAGE_NAME', code=400)
        
        page = normalize_page(page)
        
        if page and not page_match_regex.match(page):
            return self.set_error('INVALID_PAGE_NAME', code=400)
        
        # If no page is specified, give the index page
        page = page or "index"
        
        if WikiPage.is_impossible(page):
            return self.set_error('INVALID_PAGE_NAME', code=400)
        
        if self.error_on_name_normalized and page != original_page:
            self.set_error('PAGE_NAME_NORMALIZED')
        
        return page

class VWikiPage(VWikiPageName):
    def __init__(self, param, required=True, restricted=True, modonly=False,
                 allow_hidden_revision=True, **kw):
        self.restricted = restricted
        self.modonly = modonly
        self.allow_hidden_revision = allow_hidden_revision
        self.required = required
        VWikiPageName.__init__(self, param, **kw)
    
    def run(self, page):
        page = VWikiPageName.run(self, page)
        
        if self.has_errors:
            return
        
        if (not c.is_wiki_mod) and self.modonly:
            return self.set_error('MOD_REQUIRED', code=403)
        
        try:
            wp = self.validpage(page)
        except AbortWikiError:
            return

        return wp
    
    def validpage(self, page):
        try:
            wp = WikiPage.get(c.site, page)
            if self.restricted and wp.restricted:
                if not (c.is_wiki_mod or wp.special):
                    self.set_error('RESTRICTED_PAGE', code=403)
                    raise AbortWikiError
            if not this_may_view(wp):
                self.set_error('MAY_NOT_VIEW', code=403)
                raise AbortWikiError
            return wp
        except tdb_cassandra.NotFound:
            if self.required:
                self.set_error('PAGE_NOT_FOUND', code=404)
                raise AbortWikiError
            return None
    
    def validversion(self, version, pageid=None):
        if not version:
            return
        try:
            r = WikiRevision.get(version, pageid)
            if r.admin_deleted and not c.user_is_admin:
                self.set_error('INVALID_REVISION', code=404)
                raise AbortWikiError
            if not self.allow_hidden_revision and (r.is_hidden and not c.is_wiki_mod):
                self.set_error('HIDDEN_REVISION', code=403)
                raise AbortWikiError
            return r
        except (tdb_cassandra.NotFound, WikiBadRevision):
            self.set_error('INVALID_REVISION', code=404)
            raise AbortWikiError

    def param_docs(self, param=None):
        return {param or self.param: _('the name of an existing wiki page')}

class VWikiPageAndVersion(VWikiPage):    
    def run(self, page, *versions):
        wp = VWikiPage.run(self, page)
        if self.has_errors:
            return
        validated = []
        for v in versions:
            try:
                validated += [self.validversion(v, wp._id) if v and wp else None]
            except AbortWikiError:
                return
        return tuple([wp] + validated)
    
    def param_docs(self):
        doc = dict.fromkeys(self.param, _('a wiki revision ID'))
        doc.update(VWikiPage.param_docs(self, self.param[0]))
        return doc

class VWikiPageRevise(VWikiPage):
    def __init__(self, param, required=False, *k, **kw):
        VWikiPage.__init__(self, param, required=required, *k, **kw)
    
    def may_not_create(self, page):
        if not page:
            # Should not happen, but just in case
            self.set_error('EMPTY_PAGE_NAME', 403)
            return
        
        page = normalize_page(page)
        
        if c.is_wiki_mod and WikiPage.is_special(page):
            return {'reason': 'PAGE_CREATED_ELSEWHERE'}
        elif (not c.user_is_admin) and WikiPage.is_restricted(page):
            self.set_error('RESTRICTED_PAGE', code=403)
            return
        elif page.count('/') > MAX_SEPARATORS:
            return {'reason': 'PAGE_NAME_MAX_SEPARATORS', 'max_separators': MAX_SEPARATORS}
        elif len(page) > MAX_PAGE_NAME_LENGTH:
            return {'reason': 'PAGE_NAME_LENGTH', 'max_length': MAX_PAGE_NAME_LENGTH}
    
    def run(self, page, previous=None):
        wp = VWikiPage.run(self, page)
        if self.has_errors:
            return
        if not this_may_revise(wp):
            if not wp:
                return self.set_error('PAGE_NOT_FOUND', code=404)
            return self.set_error('MAY_NOT_REVISE', code=403)
        if not wp:
            # No abort code on purpose, controller will handle
            error = self.may_not_create(page)
            if error:
                self.set_error('WIKI_CREATE_ERROR', msg_params=error)
            return (None, None)
        if previous:
            try:
                prev = self.validversion(previous, wp._id)
            except AbortWikiError:
                return
            return (wp, prev)
        return (wp, None)
    
    def param_docs(self):
        docs = {self.param[0]: _('the name of an existing page or a new page to create')}
        if 'previous' in self.param:
            docs['previous'] = _('the starting point revision for this edit')
        return docs

########NEW FILE########
__FILENAME__ = websockets
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Utilities for interfacing with the WebSocket server Sutro."""

import hashlib
import hmac
import json
import time
import urllib
import urlparse

from pylons import g

from r2.lib import amqp
from r2.lib.filters import websafe_json


_WEBSOCKET_EXCHANGE = "sutro"


def send_broadcast(namespace, type, payload):
    """Broadcast an object to all WebSocket listeners in a namespace.

    The message type is used to differentiate between different kinds of
    payloads that may be sent. The payload will be encoded as a JSON object
    before being sent to the client.

    """
    frame = {
        "type": type,
        "payload": payload,
    }
    amqp.add_item(routing_key=namespace, body=json.dumps(frame),
                  exchange=_WEBSOCKET_EXCHANGE)


def make_url(namespace, max_age):
    """Return a signed URL for the client to use for websockets.

    The namespace determines which messages the client receives and max_age is
    the number of seconds the URL is valid for.

    """

    expires = str(int(time.time() + max_age))
    mac = hmac.new(g.secrets["websocket"], expires + namespace,
                   hashlib.sha1).hexdigest()

    query_string = urllib.urlencode({
        "h": mac,
        "e": expires,
    })

    return urlparse.urlunparse(("wss", g.websocket_host, namespace,
                               None, query_string, None))

########NEW FILE########
__FILENAME__ = zookeeper
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import os
import json
import urllib
import functools
from collections import MutableMapping

from kazoo.client import KazooClient
from kazoo.security import make_digest_acl
from kazoo.exceptions import NoNodeException
from pylons import g


def connect_to_zookeeper(hostlist, credentials):
    """Create a connection to the ZooKeeper ensemble.

    If authentication credentials are provided (as a two-tuple: username,
    password), we will ensure that they are provided to the server whenever we
    establish a connection.

    """

    client = KazooClient(hostlist,
                         timeout=5,
                         max_retries=3,
                         auth_data=[("digest", ":".join(credentials))])

    # convenient helper function for making credentials
    client.make_acl = functools.partial(make_digest_acl, *credentials)

    client.start()
    return client


class LiveConfig(object):
    """A read-only dictionary view of configuration retrieved from ZooKeeper.

    The data will be parsed using the given configuration specs, exactly like
    the ini file based configuration. When data is changed in ZooKeeper, the
    data in this view will automatically update.

    """
    def __init__(self, client, key):
        self.data = {}

        @client.DataWatch(key)
        def watcher(data, stat):
            self.data = json.loads(data)

    def __getitem__(self, key):
        return self.data[key]

    def get(self, key, default=None):
        return self.data.get(key, default)

    def __repr__(self):
        return "<LiveConfig %r>" % self.data


class LiveList(object):
    """A mutable set shared by all apps and backed by ZooKeeper."""
    def __init__(self, client, root, map_fn=None, reduce_fn=lambda L: L,
                 watch=True):
        self.client = client
        self.root = root
        self.map_fn = map_fn
        self.reduce_fn = reduce_fn
        self.is_watching = watch

        acl = [self.client.make_acl(read=True, create=True, delete=True)]
        self.client.ensure_path(self.root, acl)

        if watch:
            self.data = []

            @client.ChildrenWatch(root)
            def watcher(children):
                self.data = self._normalize_children(children, reduce=True)

    def _nodepath(self, item):
        escaped = urllib.quote(str(item), safe=":")
        return os.path.join(self.root, escaped)

    def _normalize_children(self, children, reduce):
        unquoted = (urllib.unquote(c) for c in children)
        mapped = map(self.map_fn, unquoted)

        if reduce:
            return list(self.reduce_fn(mapped))
        else:
            return list(mapped)

    def add(self, item):
        path = self._nodepath(item)
        self.client.ensure_path(path)

    def remove(self, item):
        path = self._nodepath(item)

        try:
            self.client.delete(path)
        except NoNodeException:
            raise ValueError("not in list")

    def get(self, reduce=True):
        children = self.client.get_children(self.root)
        return self._normalize_children(children, reduce)

    def __iter__(self):
        if not self.is_watching:
            raise NotImplementedError()
        return iter(self.data)

    def __len__(self):
        if not self.is_watching:
            raise NotImplementedError()
        return len(self.data)

    def __repr__(self):
        return "<LiveList %r (%s)>" % (self.data,
                                       "push" if self.is_watching else "pull")


class LiveDict(MutableMapping):
    """Zookeeper-backed dictionary - similar to LiveList in that it can be
    shared by all apps.
    """

    def __init__(self, client, path, watch=True):
        self.client = client
        self.path = path
        self.is_watching = watch
        self.lock_group = "LiveDict"

        acl = [self.client.make_acl(read=True, write=True)]
        self.client.ensure_path(self.path, acl)

        if watch:
            self._data = {}

            @client.DataWatch(path)
            def watcher(data, stat):
                self._set_data(data)

    def fetch_data(self):
        self._refresh()
        return self._data

    def _refresh(self):
        if not self.is_watching:
            self._set_data(self.client.get(self.path)[0])

    def _set_data(self, json_string):
        self._data = json.loads(json_string or "{}")

    def __getitem__(self, key):
        self._refresh()
        return self._data[key]

    def __setitem__(self, key, value):
        with g.make_lock(self.lock_group, self.path):
            self._refresh()
            self._data[key] = value
            json_data = json.dumps(self._data)
            self.client.set(self.path, json_data)

    def __delitem__(self, key):
        with g.make_lock(self.lock_group, self.path):
            self._refresh()
            del self._data[key]
            json_data = json.dumps(self._data)
            self.client.set(self.path, json_data)

    def __repr__(self):
        self._refresh()
        return "<LiveDict {}>".format(self._data)

    def __iter__(self):
        self._refresh()
        return iter(self._data)

    def items(self):
        self._refresh()
        return self._data.items()

    def iteritems(self):
        self._refresh()
        return self._data.iteritems()

    def __len__(self):
        self._refresh()
        return len(self._data)

########NEW FILE########
__FILENAME__ = account
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.db.thing     import Thing, Relation, NotFound
from r2.lib.db.operators import lower
from r2.lib.db.userrel   import UserRel
from r2.lib.db           import tdb_cassandra
from r2.lib.memoize      import memoize
from r2.lib.admin_utils  import modhash, valid_hash
from r2.lib.utils        import randstr, timefromnow
from r2.lib.utils        import UrlParser
from r2.lib.utils        import constant_time_compare, canonicalize_email
from r2.lib.cache        import sgm
from r2.lib import filters, hooks
from r2.lib.log import log_text
from r2.models.last_modified import LastModified
from r2.models.trylater import TryLater

from pylons import c, g, request
from pylons.i18n import _
import time
import hashlib
from copy import copy
from datetime import datetime, timedelta
import bcrypt
import hmac
import hashlib
import itertools
from pycassa.system_manager import ASCII_TYPE


trylater_hooks = hooks.HookRegistrar()
COOKIE_TIMESTAMP_FORMAT = '%Y-%m-%dT%H:%M:%S'


class AccountExists(Exception): pass

class Account(Thing):
    _data_int_props = Thing._data_int_props + ('link_karma', 'comment_karma',
                                               'report_made', 'report_correct',
                                               'report_ignored', 'spammer',
                                               'reported', 'gold_creddits', )
    _int_prop_suffix = '_karma'
    _essentials = ('name', )
    _defaults = dict(pref_numsites = 25,
                     pref_frame = False,
                     pref_frame_commentspanel = False,
                     pref_newwindow = False,
                     pref_clickgadget = 5,
                     pref_store_visits = False,
                     pref_public_votes = False,
                     pref_hide_from_robots = False,
                     pref_research = False,
                     pref_hide_ups = False,
                     pref_hide_downs = False,
                     pref_min_link_score = -4,
                     pref_min_comment_score = -4,
                     pref_num_comments = g.num_comments,
                     pref_lang = g.lang,
                     pref_content_langs = (g.lang,),
                     pref_over_18 = False,
                     pref_compress = False,
                     pref_domain_details = False,
                     pref_organic = True,
                     pref_no_profanity = True,
                     pref_label_nsfw = True,
                     pref_show_stylesheets = True,
                     pref_show_flair = True,
                     pref_show_link_flair = True,
                     pref_mark_messages_read = True,
                     pref_threaded_messages = True,
                     pref_collapse_read_messages = False,
                     pref_private_feeds = True,
                     pref_local_js = False,
                     pref_show_adbox = True,
                     pref_show_sponsors = True, # sponsored links
                     pref_show_sponsorships = True,
                     pref_show_trending=True,
                     pref_highlight_new_comments = True,
                     pref_monitor_mentions=True,
                     pref_collapse_left_bar=False,
                     pref_public_server_seconds=False,
                     mobile_compress = False,
                     mobile_thumbnail = True,
                     trusted_sponsor = False,
                     reported = 0,
                     report_made = 0,
                     report_correct = 0,
                     report_ignored = 0,
                     spammer = 0,
                     sort_options = {},
                     has_subscribed = False,
                     pref_media = 'subreddit',
                     share = {},
                     wiki_override = None,
                     email = "",
                     email_verified = False,
                     ignorereports = False,
                     pref_show_promote = None,
                     gold = False,
                     gold_charter = False,
                     gold_creddits = 0,
                     gold_creddit_escrow = 0,
                     cake_expiration=None,
                     otp_secret=None,
                     state=0,
                     modmsgtime=None,
                     )
    _preference_attrs = tuple(k for k in _defaults.keys()
                              if k.startswith("pref_"))

    def preferences(self):
        return {pref: getattr(self, pref) for pref in self._preference_attrs}

    def __eq__(self, other):
        if type(self) != type(other):
            return False

        return self._id == other._id

    def __ne__(self, other):
        return not self.__eq__(other)

    def has_interacted_with(self, sr):
        if not sr:
            return False

        for type in ('link', 'comment'):
            if hasattr(self, "%s_%s_karma" % (sr.name, type)):
                return True

        if sr.is_subscriber(self):
            return True

        return False

    def karma(self, kind, sr = None):
        suffix = '_' + kind + '_karma'

        #if no sr, return the sum
        if sr is None:
            total = 0
            for k, v in self._t.iteritems():
                if k.endswith(suffix):
                    total += v
            return total
        else:
            try:
                return getattr(self, sr.name + suffix)
            except AttributeError:
                #if positive karma elsewhere, you get min_up_karma
                if self.karma(kind) > 0:
                    return g.MIN_UP_KARMA
                else:
                    return 0

    def incr_karma(self, kind, sr, amt):
        if sr.name.startswith('_'):
            g.log.info("Ignoring karma increase for subreddit %r" % (sr.name,))
            return

        prop = '%s_%s_karma' % (sr.name, kind)
        if hasattr(self, prop):
            return self._incr(prop, amt)
        else:
            default_val = self.karma(kind, sr)
            setattr(self, prop, default_val + amt)
            self._commit()

    @property
    def link_karma(self):
        return self.karma('link')

    @property
    def comment_karma(self):
        return self.karma('comment')

    @property
    def safe_karma(self):
        karma = self.link_karma
        return max(karma, 1) if karma > -1000 else karma

    def all_karmas(self, include_old=True):
        """returns a list of tuples in the form (name, hover-text, link_karma,
        comment_karma)"""
        link_suffix = '_link_karma'
        comment_suffix = '_comment_karma'
        karmas = []
        sr_names = set()
        for k in self._t.keys():
            if k.endswith(link_suffix):
                sr_names.add(k[:-len(link_suffix)])
            elif k.endswith(comment_suffix):
                sr_names.add(k[:-len(comment_suffix)])
        for sr_name in sr_names:
            karmas.append((sr_name, None,
                           self._t.get(sr_name + link_suffix, 0),
                           self._t.get(sr_name + comment_suffix, 0)))

        karmas.sort(key = lambda x: x[2] + x[3], reverse=True)

        old_link_karma = self._t.get('link_karma', 0)
        old_comment_karma = self._t.get('comment_karma', 0)
        if include_old and (old_link_karma or old_comment_karma):
            karmas.append((_('ancient history'),
                           _('really obscure karma from before it was cool to track per-subreddit'),
                           old_link_karma, old_comment_karma))

        return karmas

    def update_last_visit(self, current_time):
        from admintools import apply_updates

        apply_updates(self)

        prev_visit = LastModified.get(self._fullname, "Visit")
        if prev_visit and current_time - prev_visit < timedelta(days=1):
            return

        g.log.debug ("Updating last visit for %s from %s to %s" %
                    (self.name, prev_visit, current_time))

        LastModified.touch(self._fullname, "Visit")

        self.last_visit = int(time.time())
        self._commit()

    def make_cookie(self, timestr=None):
        if not self._loaded:
            self._load()
        timestr = timestr or time.strftime(COOKIE_TIMESTAMP_FORMAT)
        id_time = str(self._id) + ',' + timestr
        to_hash = ','.join((id_time, self.password, g.secrets["SECRET"]))
        return id_time + ',' + hashlib.sha1(to_hash).hexdigest()

    def make_admin_cookie(self, first_login=None, last_request=None):
        if not self._loaded:
            self._load()
        first_login = first_login or datetime.utcnow().strftime(COOKIE_TIMESTAMP_FORMAT)
        last_request = last_request or datetime.utcnow().strftime(COOKIE_TIMESTAMP_FORMAT)
        hashable = ','.join((first_login, last_request, request.ip, request.user_agent, self.password))
        mac = hmac.new(g.secrets["SECRET"], hashable, hashlib.sha1).hexdigest()
        return ','.join((first_login, last_request, mac))

    def make_otp_cookie(self, timestamp=None):
        if not self._loaded:
            self._load()

        timestamp = timestamp or datetime.utcnow().strftime(COOKIE_TIMESTAMP_FORMAT)
        secrets = [request.user_agent, self.otp_secret, self.password]
        signature = hmac.new(g.secrets["SECRET"], ','.join([timestamp] + secrets), hashlib.sha1).hexdigest()

        return ",".join((timestamp, signature))

    def needs_captcha(self):
        return not g.disable_captcha and self.link_karma < 1

    def modhash(self, rand=None, test=False):
        if c.oauth_user:
            # OAuth clients should never receive a modhash of any kind
            # as they could use it in a CSRF attack to bypass their
            # permitted OAuth scopes.
            return None
        return modhash(self, rand = rand, test = test)
    
    def valid_hash(self, hash):
        if self == c.oauth_user:
            # OAuth authenticated requests do not require CSRF protection.
            return True
        else:
            return valid_hash(self, hash)

    @classmethod
    @memoize('account._by_name')
    def _by_name_cache(cls, name, allow_deleted = False):
        #relower name here, just in case
        deleted = (True, False) if allow_deleted else False
        q = cls._query(lower(Account.c.name) == name.lower(),
                       Account.c._spam == (True, False),
                       Account.c._deleted == deleted)

        q._limit = 1
        l = list(q)
        if l:
            return l[0]._id

    @classmethod
    def _by_name(cls, name, allow_deleted = False, _update = False):
        #lower name here so there is only one cache
        uid = cls._by_name_cache(name.lower(), allow_deleted, _update = _update)
        if uid:
            return cls._byID(uid, True)
        else:
            raise NotFound, 'Account %s' % name

    # Admins only, since it's not memoized
    @classmethod
    def _by_name_multiple(cls, name):
        q = cls._query(lower(Account.c.name) == name.lower(),
                       Account.c._spam == (True, False),
                       Account.c._deleted == (True, False))
        return list(q)

    @property
    def friends(self):
        return self.friend_ids()

    @property
    def enemies(self):
        return self.enemy_ids()

    @property
    def is_moderator_somewhere(self):
        # modmsgtime can be:
        #   - a date: the user is a mod somewhere and has unread modmail
        #   - False: the user is a mod somewhere and has no unread modmail
        #   - None: (the default) the user is not a mod anywhere
        return self.modmsgtime is not None

    # Used on the goldmember version of /prefs/friends
    @memoize('account.friend_rels')
    def friend_rels_cache(self):
        q = Friend._query(Friend.c._thing1_id == self._id,
                          Friend.c._name == 'friend')
        return list(f._id for f in q)

    def friend_rels(self, _update = False):
        rel_ids = self.friend_rels_cache(_update=_update)
        try:
            rels = Friend._byID_rel(rel_ids, return_dict=False,
                                    eager_load = True, data = True,
                                    thing_data = True)
            rels = list(rels)
        except NotFound:
            if _update:
                raise
            else:
                log_text("friend-rels-bandaid 1",
                         "Had to recalc friend_rels (1) for %s" % self.name,
                         "warning")
                return self.friend_rels(_update=True)

        if not _update:
            sorted_1 = sorted([r._thing2_id for r in rels])
            sorted_2 = sorted(list(self.friends))
            if sorted_1 != sorted_2:
                g.log.error("FR1: %r" % sorted_1)
                g.log.error("FR2: %r" % sorted_2)
                log_text("friend-rels-bandaid 2",
                         "Had to recalc friend_rels (2) for %s" % self.name,
                         "warning")
                self.friend_ids(_update=True)
                return self.friend_rels(_update=True)
        return dict((r._thing2_id, r) for r in rels)

    def add_friend_note(self, friend, note):
        rels = self.friend_rels()
        rel = rels[friend._id]
        rel.note = note
        rel._commit()

    def delete(self, delete_message=None):
        self.delete_message = delete_message
        self.delete_time = datetime.now(g.tz)
        self._deleted = True
        self._commit()

        #update caches
        Account._by_name(self.name, allow_deleted = True, _update = True)
        #we need to catch an exception here since it will have been
        #recently deleted
        try:
            Account._by_name(self.name, _update = True)
        except NotFound:
            pass
        
        #remove from friends lists
        q = Friend._query(Friend.c._thing2_id == self._id,
                          Friend.c._name == 'friend',
                          eager_load = True)
        for f in q:
            f._thing1.remove_friend(f._thing2)

        q = Friend._query(Friend.c._thing2_id == self._id,
                          Friend.c._name == 'enemy',
                          eager_load=True)
        for f in q:
            f._thing1.remove_enemy(f._thing2)

        # wipe out stored password data after a recovery period
        TryLater.schedule("account_deletion", self._id36,
                          delay=timedelta(days=90))

        # Remove OAuth2Client developer permissions.  This will delete any
        # clients for which this account is the sole developer.
        from r2.models.token import OAuth2Client
        for client in OAuth2Client._by_developer(self):
            client.remove_developer(self)

    # 'State' bitfield properties
    @property
    def _banned(self):
        return self.state & 1

    @_banned.setter
    def _banned(self, value):
        if value and not self._banned:
            self.state |= 1
            # Invalidate all cookies by changing the password
            # First back up the password so we can reverse this
            self.backup_password = self.password
            # New PW doesn't matter, they can't log in with it anyway.
            # Even if their PW /was/ 'banned' for some reason, this
            # will change the salt and thus invalidate the cookies
            change_password(self, 'banned') 

            # deauthorize all access tokens
            from r2.models.token import OAuth2AccessToken
            from r2.models.token import OAuth2RefreshToken

            OAuth2AccessToken.revoke_all_by_user(self)
            OAuth2RefreshToken.revoke_all_by_user(self)
        elif not value and self._banned:
            self.state &= ~1

            # Undo the password thing so they can log in
            self.password = self.backup_password

            # They're on their own for OAuth tokens, though.

        self._commit()

    @property
    def subreddits(self):
        from subreddit import Subreddit
        return Subreddit.user_subreddits(self)

    def recent_share_emails(self):
        return self.share.get('recent', set([]))

    def add_share_emails(self, emails):
        if not emails:
            return
        
        if not isinstance(emails, set):
            emails = set(emails)

        self.share.setdefault('emails', {})
        share = self.share.copy()

        share_emails = share['emails']
        for e in emails:
            share_emails[e] = share_emails.get(e, 0) +1

        share['recent'] = emails

        self.share = share

    def special_distinguish(self):
        if self._t.get("special_distinguish_name"):
            return dict((k, self._t.get("special_distinguish_"+k, None))
                        for k in ("name", "kind", "symbol", "cssclass", "label", "link"))
        else:
            return None

    def quota_key(self, kind):
        return "user_%s_quotas-%s" % (kind, self.name)

    def clog_quota(self, kind, item):
        key = self.quota_key(kind)
        fnames = g.hardcache.get(key, [])
        fnames.append(item._fullname)
        g.hardcache.set(key, fnames, 86400 * 30)

    def quota_baskets(self, kind):
        from r2.models.admintools import filter_quotas
        key = self.quota_key(kind)
        fnames = g.hardcache.get(key)

        if not fnames:
            return None

        unfiltered = Thing._by_fullname(fnames, data=True, return_dict=False)

        baskets, new_quotas = filter_quotas(unfiltered)

        if new_quotas is None:
            pass
        elif new_quotas == []:
            g.hardcache.delete(key)
        else:
            g.hardcache.set(key, new_quotas, 86400 * 30)

        return baskets

    # Needs to take the *canonicalized* version of each email
    # When true, returns the reason
    @classmethod
    def which_emails_are_banned(cls, canons):
        banned = hooks.get_hook('email.get_banned').call(canons=canons)

        # Create a dictionary like:
        # d["abc.def.com"] = [ "bob@abc.def.com", "sue@abc.def.com" ]
        rv = {}
        canons_by_domain = {}

        # email.get_banned will return a list of lists (one layer from the
        # hooks system, the second from the function itself); chain them
        # together for easy processing
        for canon in itertools.chain(*banned):
            rv[canon] = None

            at_sign = canon.find("@")
            domain = canon[at_sign+1:]
            canons_by_domain.setdefault(domain, [])
            canons_by_domain[domain].append(canon)

        # Hand off to the domain ban system; it knows in the case of
        # abc@foo.bar.com to check foo.bar.com, bar.com, and .com
        from r2.models.admintools import bans_for_domain_parts

        for domain, canons in canons_by_domain.iteritems():
            for d in bans_for_domain_parts(domain):
                if d.no_email:
                    rv[canon] = "domain"

        return rv

    def has_banned_email(self):
        canon = self.canonical_email()
        which = self.which_emails_are_banned((canon,))
        return which.get(canon, None)

    def canonical_email(self):
        return canonicalize_email(self.email)

    def cromulent(self):
        """Return whether the user has validated their email address and
           passes some rudimentary 'not evil' checks."""

        if not self.email_verified:
            return False

        if self.has_banned_email():
            return False

        # Otherwise, congratulations; you're cromulent!
        return True

    def quota_limits(self, kind):
        if kind != 'link':
            raise NotImplementedError

        if self.cromulent():
            return dict(hour=3, day=10, week=50, month=150)
        else:
            return dict(hour=1,  day=3,  week=5,   month=5)

    def quota_full(self, kind):
        limits = self.quota_limits(kind)
        baskets = self.quota_baskets(kind)

        if baskets is None:
            return None

        total = 0
        filled_quota = None
        for key in ('hour', 'day', 'week', 'month'):
            total += len(baskets[key])
            if total >= limits[key]:
                filled_quota = key

        return filled_quota

    @classmethod
    def system_user(cls):
        try:
            return cls._by_name(g.system_user)
        except (NotFound, AttributeError):
            return None

    def flair_enabled_in_sr(self, sr_id):
        return getattr(self, 'flair_%s_enabled' % sr_id, True)

    def update_sr_activity(self, sr):
        if not self._spam:
            AccountsActiveBySR.touch(self, sr)

    def get_trophy_id(self, uid):
        '''Return the ID of the Trophy associated with the given "uid"

        `uid` - The unique identifier for the Trophy to look up

        '''
        return getattr(self, 'received_trophy_%s' % uid, None)

    def set_trophy_id(self, uid, trophy_id):
        '''Recored that a user has received a Trophy with "uid"

        `uid` - The trophy "type" that the user should only have one of
        `trophy_id` - The ID of the corresponding Trophy object

        '''
        return setattr(self, 'received_trophy_%s' % uid, trophy_id)

    @property
    def employee(self):
        """Return if the user is an employee.

        Being an employee grants them various special privileges.

        """
        return (hasattr(self, 'name') and
                (self.name in g.admins or
                 self.name in g.sponsors or
                 self.name in g.employees))

    @property
    def cpm_selfserve_pennies(self):
        override_price = getattr(self, 'cpm_selfserve_pennies_override', None)
        if override_price is not None:
            return override_price
        else:
            return g.cpm_selfserve.pennies

    @property
    def has_gold_subscription(self):
        return bool(getattr(self, 'gold_subscr_id', None))

    @property
    def has_paypal_subscription(self):
        return (self.has_gold_subscription and
                not self.gold_subscr_id.startswith('cus_'))

    @property
    def has_stripe_subscription(self):
        return (self.has_gold_subscription and
                self.gold_subscr_id.startswith('cus_'))


class FakeAccount(Account):
    _nodb = True
    pref_no_profanity = True

    def __eq__(self, other):
        return self is other

def valid_admin_cookie(cookie):
    if g.read_only_mode:
        return (False, None)

    # parse the cookie
    try:
        first_login, last_request, hash = cookie.split(',')
    except ValueError:
        return (False, None)

    # make sure it's a recent cookie
    try:
        first_login_time = datetime.strptime(first_login, COOKIE_TIMESTAMP_FORMAT)
        last_request_time = datetime.strptime(last_request, COOKIE_TIMESTAMP_FORMAT)
    except ValueError:
        return (False, None)

    cookie_age = datetime.utcnow() - first_login_time
    if cookie_age.total_seconds() > g.ADMIN_COOKIE_TTL:
        return (False, None)

    idle_time = datetime.utcnow() - last_request_time
    if idle_time.total_seconds() > g.ADMIN_COOKIE_MAX_IDLE:
        return (False, None)

    # validate
    expected_cookie = c.user.make_admin_cookie(first_login, last_request)
    return (constant_time_compare(cookie, expected_cookie),
            first_login)


def valid_otp_cookie(cookie):
    if g.read_only_mode:
        return False

    # parse the cookie
    try:
        remembered_at, signature = cookie.split(",")
    except ValueError:
        return False

    # make sure it hasn't expired
    try:
        remembered_at_time = datetime.strptime(remembered_at, COOKIE_TIMESTAMP_FORMAT)
    except ValueError:
        return False

    age = datetime.utcnow() - remembered_at_time
    if age.total_seconds() > g.OTP_COOKIE_TTL:
        return False

    # validate
    expected_cookie = c.user.make_otp_cookie(remembered_at)
    return constant_time_compare(cookie, expected_cookie)


def valid_feed(name, feedhash, path):
    if name and feedhash and path:
        from r2.lib.template_helpers import add_sr
        path = add_sr(path)
        try:
            user = Account._by_name(name)
            if (user.pref_private_feeds and
                constant_time_compare(feedhash, make_feedhash(user, path))):
                return user
        except NotFound:
            pass

def make_feedhash(user, path):
    return hashlib.sha1("".join([user.name, user.password,
                                 g.secrets["FEEDSECRET"]])
                   ).hexdigest()

def make_feedurl(user, path, ext = "rss"):
    u = UrlParser(path)
    u.update_query(user = user.name,
                   feed = make_feedhash(user, path))
    u.set_extension(ext)
    return u.unparse()

def valid_login(name, password):
    try:
        a = Account._by_name(name)
    except NotFound:
        return False

    if not a._loaded: a._load()

    hooks.get_hook("account.spotcheck").call(account=a)

    if a._banned:
        return False
    return valid_password(a, password)

def valid_password(a, password):
    # bail out early if the account or password's invalid
    if not hasattr(a, 'name') or not hasattr(a, 'password') or not password:
        return False

    # standardize on utf-8 encoding
    password = filters._force_utf8(password)

    if a.password.startswith('$2a$'):
        # it's bcrypt.
        expected_hash = bcrypt.hashpw(password, a.password)
        if not constant_time_compare(a.password, expected_hash):
            return False

        # if it's using the current work factor, we're done, but if it's not
        # we'll have to rehash.
        # the format is $2a$workfactor$salt+hash
        work_factor = int(a.password.split("$")[2])
        if work_factor == g.bcrypt_work_factor:
            return a
    else:
        # alright, so it's not bcrypt. how old is it?
        # if the length of the stored hash is 43 bytes, the sha-1 hash has a salt
        # otherwise it's sha-1 with no salt.
        salt = ''
        if len(a.password) == 43:
            salt = a.password[:3]
        expected_hash = passhash(a.name, password, salt)

        if not constant_time_compare(a.password, expected_hash):
            return False

    # since we got this far, it's a valid password but in an old format
    # let's upgrade it
    a.password = bcrypt_password(password)
    a._commit()
    return a

def bcrypt_password(password):
    salt = bcrypt.gensalt(log_rounds=g.bcrypt_work_factor)
    return bcrypt.hashpw(password, salt)

def passhash(username, password, salt = ''):
    if salt is True:
        salt = randstr(3)
    tohash = '%s%s %s' % (salt, username, password)
    return salt + hashlib.sha1(tohash).hexdigest()

def change_password(user, newpassword):
    user.password = bcrypt_password(newpassword)
    user._commit()
    LastModified.touch(user._fullname, 'Password')
    return True

#TODO reset the cache
def register(name, password, registration_ip):
    try:
        a = Account._by_name(name)
        raise AccountExists
    except NotFound:
        a = Account(name = name,
                    password = bcrypt_password(password))
        # new accounts keep the profanity filter settings until opting out
        a.pref_no_profanity = True
        a.registration_ip = registration_ip
        a._commit()

        #clear the caches
        Account._by_name(name, _update = True)
        Account._by_name(name, allow_deleted = True, _update = True)
        return a

class Friend(Relation(Account, Account)): pass

Account.__bases__ += (UserRel('friend', Friend, disable_reverse_ids_fn=True),
                      UserRel('enemy', Friend, disable_reverse_ids_fn=False))

class DeletedUser(FakeAccount):
    @property
    def name(self):
        return '[deleted]'

    @property
    def _deleted(self):
        return True

    def _fullname(self):
        raise NotImplementedError

    def _id(self):
        raise NotImplementedError

    def __setattr__(self, attr, val):
        if attr == '_deleted':
            pass
        else:
            object.__setattr__(self, attr, val)

class AccountsActiveBySR(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _ttl = timedelta(minutes=15)

    _extra_schema_creation_args = dict(key_validation_class=ASCII_TYPE)

    _read_consistency_level  = tdb_cassandra.CL.ONE
    _write_consistency_level = tdb_cassandra.CL.ANY

    @classmethod
    def touch(cls, account, sr):
        cls._set_values(sr._id36,
                        {account._id36: ''})

    @classmethod
    def get_count(cls, sr, cached=True):
        return cls.get_count_cached(sr._id36, _update=not cached)

    @classmethod
    @memoize('accounts_active', time=60)
    def get_count_cached(cls, sr_id):
        return cls._cf.get_count(sr_id)


@trylater_hooks.on("trylater.account_deletion")
def on_account_deletion(mature_items):
    for account_id36 in mature_items.itervalues():
        account = Account._byID36(account_id36, data=True)

        if not account._deleted:
            continue

        account.password = ""
        account._commit()

########NEW FILE########
__FILENAME__ = admintools
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.errors import MessageError
from r2.lib.utils import tup, fetch_things2
from r2.lib.filters import websafe
from r2.lib.log import log_text
from r2.models import Account, Message, Report, Subreddit
from r2.models.award import Award
from r2.models.gold import append_random_bottlecap_phrase
from r2.models.token import AwardClaimToken

from _pylibmc import MemcachedError
from pylons import g, config
from pylons.i18n import _

from datetime import datetime, timedelta
from copy import copy

class AdminTools(object):

    def spam(self, things, auto=True, moderator_banned=False,
             banner=None, date=None, train_spam=True, **kw):
        from r2.lib.db import queries

        all_things = tup(things)
        new_things = [x for x in all_things if not x._spam]

        Report.accept(all_things, True)

        for t in all_things:
            if getattr(t, "promoted", None) is not None:
                g.log.debug("Refusing to mark promotion %r as spam" % t)
                continue

            if not t._spam and train_spam:
                note = 'spam'
            elif not t._spam and not train_spam:
                note = 'remove not spam'
            elif t._spam and not train_spam:
                note = 'confirm spam'
            elif t._spam and train_spam:
                note = 'reinforce spam'

            t._spam = True

            if moderator_banned:
                t.verdict = 'mod-removed'
            elif not auto:
                t.verdict = 'admin-removed'

            ban_info = copy(getattr(t, 'ban_info', {}))
            if isinstance(banner, dict):
                ban_info['banner'] = banner[t._fullname]
            else:
                ban_info['banner'] = banner
            ban_info.update(auto=auto,
                            moderator_banned=moderator_banned,
                            banned_at=date or datetime.now(g.tz),
                            **kw)
            ban_info['note'] = note

            t.ban_info = ban_info
            t._commit()

        if not auto:
            self.author_spammer(new_things, True)
            self.set_last_sr_ban(new_things)

        queries.ban(all_things, filtered=auto)

    def unspam(self, things, moderator_unbanned=True, unbanner=None,
               train_spam=True, insert=True):
        from r2.lib.db import queries

        things = tup(things)

        # We want to make unban-all moderately efficient, so when
        # mass-unbanning, we're going to skip the code below on links that
        # are already not banned.  However, when someone manually clicks
        # "approve" on an unbanned link, and there's just one, we want do
        # want to run the code below. That way, the little green checkmark
        # will have the right mouseover details, the reports will be
        # cleared, etc.

        if len(things) > 1:
            things = [x for x in things if x._spam]

        Report.accept(things, False)
        for t in things:
            ban_info = copy(getattr(t, 'ban_info', {}))
            ban_info['unbanned_at'] = datetime.now(g.tz)
            if unbanner:
                ban_info['unbanner'] = unbanner
            if ban_info.get('reset_used', None) == None:
                ban_info['reset_used'] = False
            else:
                ban_info['reset_used'] = True
            t.ban_info = ban_info
            t._spam = False
            if moderator_unbanned:
                t.verdict = 'mod-approved'
            else:
                t.verdict = 'admin-approved'
            t._commit()

        self.author_spammer(things, False)
        self.set_last_sr_ban(things)
        queries.unban(things, insert)
    
    def report(self, thing):
        pass

    def author_spammer(self, things, spam):
        """incr/decr the 'spammer' field for the author of every
           passed thing"""
        by_aid = {}
        for thing in things:
            if (hasattr(thing, 'author_id')
                and not getattr(thing, 'ban_info', {}).get('auto',True)):
                # only decrement 'spammer' for items that were not
                # autobanned
                by_aid.setdefault(thing.author_id, []).append(thing)

        if by_aid:
            authors = Account._byID(by_aid.keys(), data=True, return_dict=True)

            for aid, author_things in by_aid.iteritems():
                author = authors[aid]
                author._incr('spammer', len(author_things) if spam else -len(author_things))

    def set_last_sr_ban(self, things):
        by_srid = {}
        for thing in things:
            if getattr(thing, 'sr_id', None) is not None:
                by_srid.setdefault(thing.sr_id, []).append(thing)

        if by_srid:
            srs = Subreddit._byID(by_srid.keys(), data=True, return_dict=True)
            for sr_id, sr_things in by_srid.iteritems():
                sr = srs[sr_id]

                sr.last_mod_action = datetime.now(g.tz)
                sr._commit()
                sr._incr('mod_actions', len(sr_things))

    def engolden(self, account, days):
        account.gold = True

        now = datetime.now(g.display_tz)

        existing_expiration = getattr(account, "gold_expiration", None)
        if existing_expiration is None or existing_expiration < now:
            existing_expiration = now
        account.gold_expiration = existing_expiration + timedelta(days)

        description = "Since " + now.strftime("%B %Y")
        trophy = Award.give_if_needed("reddit_gold", account,
                                     description=description,
                                     url="/gold/about")
        if trophy and trophy.description.endswith("Member Emeritus"):
            trophy.description = description
            trophy._commit()
        account._commit()

        account.friend_rels_cache(_update=True)

    def degolden(self, account, severe=False):

        if severe:
            account.gold_charter = False
            Award.take_away("charter_subscriber", account)

        Award.take_away("reddit_gold", account)
        account.gold = False
        account._commit()

    def admin_list(self):
        return list(g.admins)

    def create_award_claim_code(self, unique_award_id, award_codename,
                                description, url):
        '''Create a one-time-use claim URL for a user to claim a trophy.

        `unique_award_id` - A string that uniquely identifies the kind of
                            Trophy the user would be claiming.
                            See: token.py:AwardClaimToken.uid
        `award_codename` - The codename of the Award the user will claim
        `description` - The description the Trophy will receive
        `url` - The URL the Trophy will receive

        '''
        award = Award._by_codename(award_codename)
        token = AwardClaimToken._new(unique_award_id, award, description, url)
        return token.confirm_url()

admintools = AdminTools()

def cancel_subscription(subscr_id):
    q = Account._query(Account.c.gold_subscr_id == subscr_id, data=True)
    l = list(q)
    if len(l) != 1:
        g.log.warning("Found %d matches for canceled subscription %s"
                      % (len(l), subscr_id))
    for account in l:
        account.gold_subscr_id = None
        account._commit()
        g.log.info("%s canceled their recurring subscription %s" %
                   (account.name, subscr_id))

def all_gold_users():
    q = Account._query(Account.c.gold == True, Account.c._spam == (True, False),
                       data=True, sort="_id")
    return fetch_things2(q)

def accountid_from_paypalsubscription(subscr_id):
    if subscr_id is None:
        return None

    q = Account._query(Account.c.gold_subscr_id == subscr_id,
                       Account.c._spam == (True, False),
                       Account.c._deleted == (True, False), data=False)
    l = list(q)
    if l:
        return l[0]._id
    else:
        return None

def update_gold_users(verbose=False):
    now = datetime.now(g.display_tz)
    minimum = None
    count = 0
    expiration_dates = {}

    renew_msg = _("[Click here for details on how to set up an "
                  "automatically-renewing subscription or to renew.]"
                  "(/gold) If you have any thoughts, complaints, "
                  "rants, suggestions about reddit gold, please write "
                  "to us at %(gold_email)s. Your feedback would be "
                  "much appreciated.\n\nThank you for your past "
                  "patronage.") % {'gold_email': g.goldthanks_email}

    for account in all_gold_users():
        if not hasattr(account, "gold_expiration"):
            g.log.error("%s has no gold_expiration" % account.name)
            continue

        delta = account.gold_expiration - now
        days_left = delta.days

        hc_key = "gold_expiration_notice-" + account.name

        if days_left < 0:
            if verbose:
                print "%s just expired" % account.name
            admintools.degolden(account)
            subject = _("Your reddit gold subscription has expired.")
            message = _("Your subscription to reddit gold has expired.")
            message += "\n\n" + renew_msg
            message = append_random_bottlecap_phrase(message)

            send_system_message(account, subject, message,
                                distinguished='gold-auto')
            continue

        count += 1

        if verbose:
            exp_date = account.gold_expiration.strftime('%Y-%m-%d')
            expiration_dates.setdefault(exp_date, 0)
            expiration_dates[exp_date] += 1

#           print "%s expires in %d days" % (account.name, days_left)
            if minimum is None or delta < minimum[0]:
                minimum = (delta, account)

        if days_left <= 3 and not g.hardcache.get(hc_key):
            if verbose:
                print "%s expires soon: %s days" % (account.name, days_left)
            if account.has_gold_subscription:
                if verbose:
                    print "Not sending notice to %s (%s)" % (account.name,
                                                     account.gold_subscr_id)
            else:
                if verbose:
                    print "Sending notice to %s" % account.name
                g.hardcache.set(hc_key, True, 86400 * 10)
                subject = _("Your reddit gold subscription is about to "
                            "expire!")
                message = _("Your subscription to reddit gold will be "
                            "expiring soon.")
                message += "\n\n" + renew_msg
                message = append_random_bottlecap_phrase(message)

                send_system_message(account, subject, message,
                                    distinguished='gold-auto')

    if verbose:
        for exp_date in sorted(expiration_dates.keys()):
            num_expiring = expiration_dates[exp_date]
            print '%s %3d %s' % (exp_date, num_expiring, '*' * num_expiring)
        print "%s goldmembers" % count
        if minimum is None:
            print "Nobody found."
        else:
            delta, account = minimum
            print "Next expiration is %s, in %d days" % (account.name, delta.days)

def admin_ratelimit(user):
    return True

def is_banned_IP(ip):
    return False

def is_banned_domain(dom):
    return None

def is_shamed_domain(dom):
    return False, None, None

def bans_for_domain_parts(dom):
    return []

def valid_thing(v, karma, *a, **kw):
    return not v._thing1._spam

def valid_user(v, sr, karma, *a, **kw):
    return True

def apply_updates(user):
    pass

def update_score(obj, up_change, down_change, vote, old_valid_thing):
     obj._incr('_ups',   up_change)
     obj._incr('_downs', down_change)

def compute_votes(wrapper, item):
    wrapper.upvotes   = item._ups
    wrapper.downvotes = item._downs

def ip_span(ip):
    ip = websafe(ip)
    return '<!-- %s -->' % ip

def filter_quotas(unfiltered):
    now = datetime.now(g.tz)

    baskets = {
        'hour':  [],
        'day':   [],
        'week':  [],
        'month': [],
        }

    new_quotas = []
    quotas_changed = False

    for item in unfiltered:
        delta = now - item._date

        age = delta.days * 86400 + delta.seconds

        # First, select a basket or abort if item is too old
        if age < 3600:
            basket = 'hour'
        elif age < 86400:
            basket = 'day'
        elif age < 7 * 86400:
            basket = 'week'
        elif age < 30 * 86400:
            basket = 'month'
        else:
            quotas_changed = True
            continue

        verdict = getattr(item, "verdict", None)
        approved = verdict and verdict in (
            'admin-approved', 'mod-approved')

        # Then, make sure it's worthy of quota-clogging
        if item._spam:
            pass
        elif item._deleted:
            pass
        elif item._score <= 0:
            pass
        elif age < 86400 and item._score <= g.QUOTA_THRESHOLD and not approved:
            pass
        else:
            quotas_changed = True
            continue

        baskets[basket].append(item)
        new_quotas.append(item._fullname)

    if quotas_changed:
        return baskets, new_quotas
    else:
        return baskets, None


def send_system_message(user, subject, body, system_user=None,
                        distinguished='admin', repliable=False):
    from r2.lib.db import queries

    if system_user is None:
        system_user = Account.system_user()
    if not system_user:
        g.log.warning("Can't send system message "
                      "- invalid system_user or g.system_user setting")
        return

    item, inbox_rel = Message._new(system_user, user, subject, body,
                                   ip='0.0.0.0')
    item.distinguished = distinguished
    item.repliable = repliable
    item._commit()

    try:
        queries.new_message(item, inbox_rel)
    except MemcachedError:
        raise MessageError('reddit_inbox')


if config['r2.import_private']:
    from r2admin.models.admintools import *

########NEW FILE########
__FILENAME__ = admin_notes
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import json
from datetime import datetime

from pycassa.system_manager import UTF8_TYPE, TIME_UUID_TYPE
from pycassa.util import convert_uuid_to_time
from pylons import g

from r2.lib.db import tdb_cassandra


class AdminNotesBySystem(tdb_cassandra.View):
    _use_db = True
    _read_consistency_level = tdb_cassandra.CL.QUORUM
    _write_consistency_level = tdb_cassandra.CL.ONE
    _compare_with = TIME_UUID_TYPE
    _extra_schema_creation_args = {
        "key_validation_class": UTF8_TYPE,
        "default_validation_class": UTF8_TYPE,
    }

    @classmethod
    def add(cls, system_name, subject, note, author, when=None):
        if not when:
            when = datetime.now(g.tz)
        jsonpacked = json.dumps({"note": note, "author": author})
        updatedict = {when: jsonpacked}
        key = cls._rowkey(system_name, subject)
        cls._set_values(key, updatedict)

    @classmethod
    def in_display_order(cls, system_name, subject):
        key = cls._rowkey(system_name, subject)
        try:
            query = cls._cf.get(key, column_reversed=True)
        except tdb_cassandra.NotFoundException:
            return []
        result = []
        for uuid, json_blob in query.iteritems():
            when = datetime.fromtimestamp(convert_uuid_to_time(uuid), tz=g.tz)
            payload = json.loads(json_blob)
            payload['when'] = when
            result.append(payload)
        return result

    @classmethod
    def _rowkey(cls, system_name, subject):
        return "%s:%s" % (system_name, subject)

########NEW FILE########
__FILENAME__ = award
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.db.thing import Thing, Relation, NotFound
from r2.lib.db.userrel import UserRel
from r2.lib.db.operators import asc, desc, lower
from r2.lib.memoize import memoize
from r2.models import Account
from pylons import c, g, request

class Award (Thing):
    _defaults = dict(
        awardtype = 'regular',
        api_ok = False
        )

    @classmethod
    @memoize('award.all_awards')
    def _all_awards_cache(cls):
        return [ a._id for a in Award._query(sort=asc('_date'), limit=100) ]

    @classmethod
    def _all_awards(cls, _update=False):
        all = Award._all_awards_cache(_update=_update)
        # Can't just return Award._byID() results because
        # the ordering will be lost
        d = Award._byID(all, data=True)
        return [ d[id] for id in all ]

    @classmethod
    def _new(cls, codename, title, awardtype, imgurl, api_ok):
        a = Award(codename=codename, title=title, awardtype=awardtype,
                  imgurl=imgurl, api_ok=api_ok)
        a._commit()
        Award._all_awards_cache(_update=True)

    @classmethod
    def _by_codename(cls, codename):
        q = cls._query(lower(Award.c.codename) == codename.lower())
        q._limit = 1
        award = list(q)

        if award:
            return cls._byID(award[0]._id, True)
        else:
            raise NotFound, 'Award %s' % codename

    @classmethod
    def give_if_needed(cls, codename, user,
                       description=None, url=None):
        """Give an award to a user, unless they already have it.
           Returns the trophy. Does nothing and prints nothing
           (except for g.log.debug) if the award doesn't exist."""

        try:
            award = Award._by_codename(codename)
        except NotFound:
            g.log.debug("No award named '%s'" % codename)
            return None

        trophies = Trophy.by_account(user)

        for trophy in trophies:
            if trophy._thing2.codename == codename:
                g.log.debug("%s already has %s" % (user, codename))
                return trophy

        g.log.debug("Gave %s to %s" % (codename, user))
        return Trophy._new(user, award, description=description,
                        url=url)

    @classmethod
    def take_away(cls, codename, user):
        """Takes an award out of a user's trophy case.  Returns silently
           (except for g.log.debug) if there's no such award."""

        found = False

        try:
            award = Award._by_codename(codename)
        except NotFound:
            g.log.debug("No award named '%s'" % codename)
            return

        trophies = Trophy.by_account(user)

        for trophy in trophies:
            if trophy._thing2.codename == codename:
                if found:
                    g.log.debug("%s had multiple %s awards!" % (user, codename))
                trophy._delete()
                Trophy.by_account(user, _update=True)
                Trophy.by_award(award, _update=True)
                found = True

        if found:
            g.log.debug("Took %s from %s" % (codename, user))
        else:
            g.log.debug("%s didn't have %s" % (user, codename))

class FakeTrophy(object):
    def __init__(self, recipient, award, description=None, url=None):
        self._thing2 = award
        self._thing1 = recipient
        self.description = description
        self.url = url
        self.trophy_url = getattr(self, "url",
                                  getattr(self._thing2, "url", None))
        self._id = self._id36 = None

class Trophy(Relation(Account, Award)):
    @classmethod
    def _new(cls, recipient, award, description = None,
             url = None):

        # The "name" column of the relation can't be a constant or else a
        # given account would not be allowed to win a given award more than
        # once. So we're setting it to the string form of the timestamp.
        # Still, we won't have that date just yet, so for a moment we're
        # setting it to "trophy".

        t = Trophy(recipient, award, "trophy")

        t._name = str(t._date)

        if description:
            t.description = description

        if url:
            t.url = url

        t._commit()
        t.update_caches()
        return t
    
    def update_caches(self):
        self.by_account(self._thing1, _update=True)
        self.by_award(self._thing2, _update=True)

    @classmethod
    @memoize('trophy.by_account2')
    def by_account_cache(cls, account_id):
        q = Trophy._query(Trophy.c._thing1_id == account_id,
                          sort = desc('_date'))
        q._limit = 500
        return [ t._id for t in q ]

    @classmethod
    def by_account(cls, account, _update=False):
        rel_ids = cls.by_account_cache(account._id, _update=_update)
        trophies = Trophy._byID_rel(rel_ids, data=True, eager_load=True,
                                    thing_data=True, return_dict = False)
        return trophies

    @classmethod
    @memoize('trophy.by_award2')
    def by_award_cache(cls, award_id):
        q = Trophy._query(Trophy.c._thing2_id == award_id,
                          sort = desc('_date'))
        q._limit = 50
        return [ t._id for t in q ]

    @classmethod
    def by_award(cls, award, _update=False):
        rel_ids = cls.by_award_cache(award._id, _update=_update)
        trophies = Trophy._byID_rel(rel_ids, data=True, eager_load=True,
                                    thing_data=True, return_dict = False)
        return trophies

    @classmethod
    def claim(cls, user, uid, award, description, url):
        with g.make_lock("claim_award", str("%s_%s" % (user.name, uid))):
            existing_trophy_id = user.get_trophy_id(uid)
            if existing_trophy_id:
                trophy = cls._byID(existing_trophy_id)
                preexisting = True
            else:
                preexisting = False
                trophy = cls._new(user, award, description=description,
                                  url=url)
                user.set_trophy_id(uid, trophy._id)
                user._commit()
        return trophy, preexisting

    @property
    def trophy_url(self):
        return getattr(self, "url", getattr(self._thing2, "url", None))

########NEW FILE########
__FILENAME__ = bidding
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import datetime

from pylons import g, request
from sqlalchemy import (
    and_,
    Boolean,
    BigInteger,
    Column,
    DateTime,
    Date,
    Float,
    func as safunc,
    Integer,
    String,
)
from sqlalchemy.dialects.postgresql.base import PGInet as Inet
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from sqlalchemy.orm.exc import NoResultFound

from r2.lib.db.thing import Thing, NotFound
from r2.lib.memoize import memoize
from r2.lib.utils import Enum, to_date
from r2.models.account import Account
from r2.models import Link


engine = g.dbm.get_engine('authorize')
# Allocate a session maker for communicating object changes with the back end  
Session = sessionmaker(autocommit = True, autoflush = True, bind = engine)
# allocate a SQLalchemy base class for auto-creation of tables based
# on class fields.  
# NB: any class that inherits from this class will result in a table
# being created, and subclassing doesn't work, hence the
# object-inheriting interface classes.
Base = declarative_base(bind = engine)

class Sessionized(object):
    """
    Interface class for wrapping up the "session" in the 0.5 ORM
    required for all database communication.  This allows subclasses
    to have a "query" and "commit" method that doesn't require
    managing of the session.
    """
    session = Session()

    def __init__(self, *a, **kw):
        """
        Common init used by all other classes in this file.  Allows
        for object-creation based on the __table__ field which is
        created by Base (further explained in _disambiguate_args).
        """
        for k, v in self._disambiguate_args(None, *a, **kw):
            setattr(self, k.name, v)
    
    @classmethod
    def _new(cls, *a, **kw):
        """
        Just like __init__, except the new object is committed to the
        db before being returned.
        """
        obj = cls(*a, **kw)
        obj._commit()
        return obj

    def _commit(self):
        """
        Commits current object to the db.
        """
        with self.session.begin():
            self.session.add(self)

    def _delete(self):
        """
        Deletes current object from the db. 
        """
        with self.session.begin():
            self.session.delete(self)

    @classmethod
    def query(cls, **kw):
        """
        Ubiquitous class-level query function. 
        """
        q = cls.session.query(cls)
        if kw:
            q = q.filter_by(**kw)
        return q

    @classmethod
    def _disambiguate_args(cls, filter_fn, *a, **kw):
        """
        Used in _lookup and __init__ to interpret *a as being a list
        of args to match columns in the same order as __table__.c

        For example, if a class Foo has fields a and b, this function
        allows the two to work identically:
        
        >>> foo = Foo(a = 'arg1', b = 'arg2')
        >>> foo = Foo('arg1', 'arg2')

        Additionally, this function invokes _make_storable on each of
        the values in the arg list (including *a as well as
        kw.values())

        """
        args = []
        if filter_fn is None:
            cols = cls.__table__.c
        else:
            cols = filter(filter_fn, cls.__table__.c)
        for k, v in zip(cols, a):
            if not kw.has_key(k.name):
                args.append((k, cls._make_storable(v)))
            else:
                raise TypeError,\
                      "got multiple arguments for '%s'" % k.name

        cols = dict((x.name, x) for x in cls.__table__.c)
        for k, v in kw.iteritems():
            if cols.has_key(k):
                args.append((cols[k], cls._make_storable(v)))
        return args

    @classmethod
    def _make_storable(self, val):
        if isinstance(val, Account):
            return val._id
        elif isinstance(val, Thing):
            return val._fullname
        else:
            return val

    @classmethod
    def _lookup(cls, multiple, *a, **kw):
        """
        Generates an executes a query where it matches *a to the
        primary keys of the current class's table.

        The primary key nature can be overridden by providing an
        explicit list of columns to search.

        This function is only a convenience function, and is called
        only by one() and lookup().
        """
        args = cls._disambiguate_args(lambda x: x.primary_key, *a, **kw)
        res = cls.query().filter(and_(*[k == v for k, v in args]))
        try:
            res = res.all() if multiple else res.one()
            # res.one() will raise NoResultFound, while all() will
            # return an empty list.  This will make the response
            # uniform
            if not res:
                raise NoResultFound
        except NoResultFound: 
            raise NotFound, "%s with %s" % \
                (cls.__name__,
                 ",".join("%s=%s" % x for x in args))
        return res

    @classmethod
    def lookup(cls, *a, **kw):
        """
        Returns all objects which match the kw list, or primary keys
        that match the *a.
        """
        return cls._lookup(True, *a, **kw)

    @classmethod
    def one(cls, *a, **kw):
        """
        Same as lookup, but returns only one argument. 
        """
        return cls._lookup(False, *a, **kw)

    @classmethod
    def add(cls, key, *a):
        try:
            cls.one(key, *a)
        except NotFound:
            cls(key, *a)._commit()
    
    @classmethod
    def delete(cls, key, *a):
        try:
            cls.one(key, *a)._delete()
        except NotFound:
            pass
    
    @classmethod
    def get(cls, key):
        try:
            return cls.lookup(key)
        except NotFound:
            return []

class CustomerID(Sessionized, Base):
    __tablename__  = "authorize_account_id"

    account_id    = Column(BigInteger, primary_key = True,
                           autoincrement = False)
    authorize_id  = Column(BigInteger)

    def __repr__(self):
        return "<AuthNetID(%s)>" % self.authorize_id

    @classmethod
    def set(cls, user, _id):
        try:
            existing = cls.one(user)
            existing.authorize_id = _id
            existing._commit()
        except NotFound:
            cls(user, _id)._commit()
    
    @classmethod
    def get_id(cls, user):
        try:
            return cls.one(user).authorize_id
        except NotFound:
            return

class PayID(Sessionized, Base):
    __tablename__ = "authorize_pay_id"

    account_id    = Column(BigInteger, primary_key = True,
                           autoincrement = False)
    pay_id        = Column(BigInteger, primary_key = True,
                           autoincrement = False)

    def __repr__(self):
        return "<%s(%d)>" % (self.__class__.__name__, self.authorize_id)

    @classmethod
    def get_ids(cls, key):
        return [int(x.pay_id) for x in cls.get(key)]

class ShippingAddress(Sessionized, Base):
    __tablename__ = "authorize_ship_id"

    account_id    = Column(BigInteger, primary_key = True,
                           autoincrement = False)
    ship_id       = Column(BigInteger, primary_key = True,
                           autoincrement = False)

    def __repr__(self):
        return "<%s(%d)>" % (self.__class__.__name__, self.authorize_id)

class Bid(Sessionized, Base):
    __tablename__ = "bids"

    STATUS        = Enum("AUTH", "CHARGE", "REFUND", "VOID")

    # will be unique from authorize
    transaction   = Column(BigInteger, primary_key = True,
                           autoincrement = False)

    # identifying characteristics
    account_id    = Column(BigInteger, index = True, nullable = False)
    pay_id        = Column(BigInteger, index = True, nullable = False)
    thing_id      = Column(BigInteger, index = True, nullable = False)

    # breadcrumbs
    ip            = Column(Inet)
    date          = Column(DateTime(timezone = True), default = safunc.now(),
                           nullable = False)

    # bid information:
    bid           = Column(Float, nullable = False)
    charge        = Column(Float)

    status        = Column(Integer, nullable = False,
                           default = STATUS.AUTH)

    # make this a primary key as well so that we can have more than
    # one freebie per campaign
    campaign      = Column(Integer, default = 0, primary_key = True)

    @classmethod
    def _new(cls, trans_id, user, pay_id, thing_id, bid, campaign = 0):
        bid = Bid(trans_id, user, pay_id, 
                  thing_id, getattr(request, 'ip', '0.0.0.0'), bid = bid,
                  campaign = campaign)
        bid._commit()
        return bid

#    @classmethod
#    def for_transactions(cls, transids):
#        transids = filter(lambda x: x != 0, transids)
#        if transids:
#            q = cls.query()
#            q = q.filter(or_(*[cls.transaction == i for i in transids]))
#            return dict((p.transaction, p) for p in q)
#        return {}

    def set_status(self, status):
        if self.status != status:
            self.status = status
            self._commit()

    def auth(self):
        self.set_status(self.STATUS.AUTH)

    def is_auth(self):
        return (self.status == self.STATUS.AUTH)

    def void(self):
        self.set_status(self.STATUS.VOID)

    def is_void(self):
        return (self.status == self.STATUS.VOID)

    def charged(self):
        self.charge = self.bid
        self.set_status(self.STATUS.CHARGE)
        self._commit()

    def is_charged(self):
        """
        Returns True if transaction has been charged with authorize.net or is
        a freebie with "charged" status.
        """
        return (self.status == self.STATUS.CHARGE)

    def refund(self, amount):
        current_charge = self.charge or self.bid    # needed if charged() not
                                                    # setting charge attr
        self.charge = current_charge - amount
        self.set_status(self.STATUS.REFUND)
        self._commit()

    def is_refund(self):
        return (self.status == self.STATUS.REFUND)

    @property
    def charge_amount(self):
        return self.charge or self.bid


class PromotionWeights(Sessionized, Base):
    __tablename__ = "promotion_weight"

    thing_name = Column(String, primary_key = True,
                        nullable = False, index = True)

    promo_idx    = Column(BigInteger, index = True, autoincrement = False,
                          primary_key = True)

    sr_name    = Column(String, primary_key = True,
                        nullable = True,  index = True)
    date       = Column(Date(), primary_key = True,
                        nullable = False, index = True)

    # because we might want to search by account
    account_id   = Column(BigInteger, index = True, autoincrement = False)

    # bid and weight should always be the same, but they don't have to be
    bid        = Column(Float, nullable = False)
    weight     = Column(Float, nullable = False)

    finished   = Column(Boolean)

    @classmethod
    def reschedule(cls, thing, idx, sr, start_date, end_date, total_weight,
                   finished = False):
        cls.delete_unfinished(thing, idx)
        cls.add(thing, idx, sr, start_date, end_date, total_weight,
                finished = finished)

    @classmethod
    def add(cls, thing, idx, sr, start_date, end_date, total_weight,
            finished = False):
        start_date = to_date(start_date)
        end_date   = to_date(end_date)

        # anything set by the user will be uniform weighting
        duration = max((end_date - start_date).days, 1)
        weight = total_weight / duration

        d = start_date
        while d < end_date:
            cls._new(thing, idx, sr, d,
                     thing.author_id, weight, weight, finished = finished)
            d += datetime.timedelta(1)

    @classmethod
    def delete_unfinished(cls, thing, idx):
        #TODO: do this the right (fast) way before release.  I don't
        #have the inclination to figure out the proper delete method
        #now
        for item in cls.query(thing_name = thing._fullname,
                              promo_idx = idx,
                              finished = False):
            item._delete()

    @classmethod
    def get_campaigns(cls, start, end=None, link=None, author_id=None,
                      sr_names=None):
        start = to_date(start)
        q = cls.query()
        if end:
            end = to_date(end)
            q = q.filter(and_(cls.date >= start, cls.date < end))
        else:
            q = q.filter(cls.date == start)

        if link:
            q = q.filter(cls.thing_name == link._fullname)

        if author_id:
            q = q.filter(cls.account_id == author_id)

        if sr_names:
            q = q.filter(cls.sr_name.in_(sr_names))

        return list(q)


# do all the leg work of creating/connecting to tables
if g.db_create_tables:
    Base.metadata.create_all()


########NEW FILE########
__FILENAME__ = builder
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from collections import defaultdict
from copy import deepcopy
import datetime
import heapq
from random import shuffle
import time

from pylons import c, g, request
from pylons.i18n import _

from r2.lib.comment_tree import (
    conversation,
    link_comments_and_sort,
    moderator_messages,
    sr_conversation,
    subreddit_messages,
    tree_sort_fn,
    user_messages,
)
from r2.lib.wrapped import Wrapped
from r2.lib.db import operators, tdb_cassandra
from r2.lib.filters import _force_unicode
from r2.lib.utils import Storage, timesince, tup
from r2.lib.utils.comment_tree_utils import get_num_children

from r2.models import (
    Account,
    Comment,
    CommentSavesByAccount,
    Link,
    LinkSavesByAccount,
    Message,
    MoreChildren,
    MoreMessages,
    MoreRecursion,
    Subreddit,
    Thing,
    wiki,
)
from r2.models.admintools import compute_votes, ip_span
from r2.models.listing import Listing


EXTRA_FACTOR = 1.5
MAX_RECURSION = 10

class Builder(object):
    def __init__(self, wrap=Wrapped, keep_fn=None, stale=True,
                 spam_listing=False):
        self.stale = stale
        self.wrap = wrap
        self.keep_fn = keep_fn
        self.spam_listing = spam_listing

    def keep_item(self, item):
        if self.keep_fn:
            return self.keep_fn(item)
        else:
            return item.keep_item(item)

    def wrap_items(self, items):
        from r2.lib.db import queries
        from r2.lib.template_helpers import add_attr

        user = c.user if c.user_is_loggedin else None
        aids = set(l.author_id for l in items if hasattr(l, 'author_id')
                   and l.author_id is not None)

        authors = Account._byID(aids, data=True, stale=self.stale)
        now = datetime.datetime.now(g.tz)
        cakes = {a._id for a in authors.itervalues()
                       if a.cake_expiration and a.cake_expiration >= now}
        friend_rels = user.friend_rels() if user and user.gold else {}

        subreddits = Subreddit.load_subreddits(items, stale=self.stale)
        can_ban_set = set()
        can_flair_set = set()
        can_own_flair_set = set()
        if user:
            for sr_id, sr in subreddits.iteritems():
                if sr.can_ban(user):
                    can_ban_set.add(sr_id)
                if sr.is_moderator_with_perms(user, 'flair'):
                    can_flair_set.add(sr_id)
                if sr.link_flair_self_assign_enabled:
                    can_own_flair_set.add(sr_id)

        #get likes/dislikes
        try:
            likes = queries.get_likes(user, items)
        except tdb_cassandra.TRANSIENT_EXCEPTIONS as e:
            g.log.warning("Cassandra vote lookup failed: %r", e)
            likes = {}

        types = {}
        wrapped = []

        modlink = {}
        modlabel = {}
        for s in subreddits.values():
            modlink[s._id] = '/r/%s/about/moderators' % s.name
            modlabel[s._id] = (_('moderator of /r/%(reddit)s, '
                                 'speaking officially') % {'reddit': s.name})

        for item in items:
            w = self.wrap(item)
            wrapped.append(w)
            # add for caching (plus it should be bad form to use _
            # variables in templates)
            w.fullname = item._fullname
            types.setdefault(w.render_class, []).append(w)

            w.author = None
            w.friend = False

            # List of tuples (see add_attr() for details)
            w.attribs = []

            w.distinguished = None
            if hasattr(item, "distinguished"):
                if item.distinguished == 'yes':
                    w.distinguished = 'moderator'
                elif item.distinguished in ('admin', 'special',
                                            'gold', 'gold-auto'):
                    w.distinguished = item.distinguished

            try:
                w.author = authors.get(item.author_id)
                if user and item.author_id in user.friends:
                    # deprecated old way:
                    w.friend = True

                    # new way:
                    label = None
                    if friend_rels:
                        rel = friend_rels[item.author_id]
                        note = getattr(rel, "note", None)
                        if note:
                            label = u"%s (%s)" % (_("friend"), 
                                                  _force_unicode(note))
                    add_attr(w.attribs, 'F', label)

            except AttributeError:
                pass

            if (w.distinguished == 'admin' and w.author):
                add_attr(w.attribs, 'A')

            if w.distinguished == 'moderator':
                add_attr(w.attribs, 'M', label=modlabel[item.sr_id],
                         link=modlink[item.sr_id])
            
            if w.distinguished == 'special':
                args = w.author.special_distinguish()
                args.pop('name')
                if not args.get('kind'):
                    args['kind'] = 'special'
                add_attr(w.attribs, **args)

            if w.author and w.author._id in cakes and not c.profilepage:
                add_attr(
                    w.attribs,
                    kind="cake",
                    label=(_("%(user)s just celebrated a reddit birthday!") %
                           {"user": w.author.name}),
                    link="/user/%s" % w.author.name,
                )

            if hasattr(item, "sr_id") and item.sr_id is not None:
                w.subreddit = subreddits[item.sr_id]

            w.likes = likes.get((user, item))

            # update vote tallies
            compute_votes(w, item)

            w.score = w.upvotes - w.downvotes

            if w.likes:
                base_score = w.score - 1
            elif w.likes is None:
                base_score = w.score
            else:
                base_score = w.score + 1

            # store the set of available scores based on the vote
            # for ease of i18n when there is a label
            w.voting_score = [(base_score + x - 1) for x in range(3)]

            w.deleted = item._deleted

            w.link_notes = []

            if c.user_is_admin:
                if item._deleted:
                    w.link_notes.append("deleted link")
                if getattr(item, "verdict", None):
                    if not item.verdict.endswith("-approved"):
                        w.link_notes.append(w.verdict)

            if c.user_is_admin and getattr(item, 'ip', None):
                w.ip_span = ip_span(item.ip)
            else:
                w.ip_span = ""

            # if the user can ban things on a given subreddit, or an
            # admin, then allow them to see that the item is spam, and
            # add the other spam-related display attributes
            w.show_reports = False
            w.show_spam    = False
            w.can_ban      = False
            w.can_flair    = False
            w.use_big_modbuttons = self.spam_listing

            if (c.user_is_admin
                or (user
                    and hasattr(item,'sr_id')
                    and item.sr_id in can_ban_set)):
                if getattr(item, "promoted", None) is None:
                    w.can_ban = True

                ban_info = getattr(item, 'ban_info', {})
                w.unbanner = ban_info.get('unbanner')

                if item._spam:
                    w.show_spam = True
                    w.moderator_banned = ban_info.get('moderator_banned', False)
                    w.autobanned = ban_info.get('auto', False)
                    w.banner = ban_info.get('banner')
                    if ban_info.get('note', None) and w.banner:
                        w.banner += ' (%s)' % ban_info['note']
                    w.use_big_modbuttons = True
                    if getattr(w, "author", None) and w.author._spam:
                        w.show_spam = "author"

                    if c.user == w.author and c.user._spam:
                        w.show_spam = False
                        w._spam = False
                        w.use_big_modbuttons = False

                elif (getattr(item, 'reported', 0) > 0
                      and (not getattr(item, 'ignore_reports', False) or
                           c.user_is_admin)):
                    w.show_reports = True
                    w.use_big_modbuttons = True

            if (c.user_is_admin
                or (user and hasattr(item, 'sr_id')
                    and (item.sr_id in can_flair_set
                         or (w.author and w.author._id == user._id
                             and item.sr_id in can_own_flair_set)))):
                w.can_flair = True

            w.approval_checkmark = None
            if w.can_ban:
                verdict = getattr(w, "verdict", None)
                if verdict in ('admin-approved', 'mod-approved'):
                    approver = None
                    approval_time = None
                    baninfo = getattr(w, "ban_info", None)
                    if baninfo:
                        approver = baninfo.get("unbanner", None)
                        approval_time = baninfo.get("unbanned_at", None)

                    approver = approver or _("a moderator")

                    if approval_time:
                        text = _("approved by %(who)s %(when)s ago") % {
                                    "who": approver,
                                    "when": timesince(approval_time)}
                    else:
                        text = _("approved by %s") % approver
                    w.approval_checkmark = text

        # recache the user object: it may be None if user is not logged in,
        # whereas now we are happy to have the UnloggedUser object
        user = c.user
        for cls in types.keys():
            cls.add_props(user, types[cls])

        return wrapped

    def get_items(self):
        raise NotImplementedError

    def item_iter(self, a):
        """Iterates over the items returned by get_items"""
        raise NotImplementedError

    def must_skip(self, item):
        """whether or not to skip any item regardless of whether the builder
        was contructed with skip=true"""
        user = c.user if c.user_is_loggedin else None

        if hasattr(item, "promoted") and item.promoted is not None:
            return False

        # can_view_slow only exists for Messages, but checking was_comment
        # is also necessary because items may also be comments that are being
        # viewed from the inbox page where their render class is overridden.
        # This check needs to be done before looking at whether they can view
        # the subreddit, or modmail to/from private subreddits that the user
        # doesn't have access to will be skipped.
        if hasattr(item, 'can_view_slow') and not item.was_comment:
            return not item.can_view_slow()

        if hasattr(item, 'subreddit') and not item.subreddit.can_view(user):
            return True

class QueryBuilder(Builder):
    def __init__(self, query, wrap=Wrapped, keep_fn=None, skip=False,
                 spam_listing=False, **kw):
        Builder.__init__(self, wrap=wrap, keep_fn=keep_fn,
                         spam_listing=spam_listing)
        self.query = query
        self.skip = skip
        self.num = kw.get('num')
        self.start_count = kw.get('count', 0) or 0
        self.after = kw.get('after')
        self.reverse = kw.get('reverse')
        self.prewrap_fn = getattr(query, 'prewrap_fn', None)

    def __repr__(self):
        return "<%s(%r)>" % (self.__class__.__name__, self.query)

    def item_iter(self, a):
        """Iterates over the items returned by get_items"""
        for i in a[0]:
            yield i

    def init_query(self):
        q = self.query

        if self.reverse:
            q._reverse()

        q._data = True
        self.orig_rules = deepcopy(q._rules)
        if self.after:
            q._after(self.after)

    def fetch_more(self, last_item, num_have):
        done = False
        q = self.query
        if self.num:
            num_need = self.num - num_have
            if num_need <= 0:
                #will cause the loop below to break
                return True, None
            else:
                #q = self.query
                #check last_item if we have a num because we may need to iterate
                if last_item:
                    q._rules = deepcopy(self.orig_rules)
                    q._after(last_item)
                    last_item = None
                q._limit = max(int(num_need * EXTRA_FACTOR), 1)
        else:
            done = True
        new_items = list(q)

        return done, new_items

    def get_items(self):
        self.init_query()

        num_have = 0
        done = False
        items = []
        count = self.start_count
        first_item = None
        last_item = None
        have_next = True
        loopcount = 0

        while not done:
            done, new_items = self.fetch_more(last_item, num_have)

            #log loop
            loopcount += 1
            if loopcount == 20:
                done = True

            #no results, we're done
            if not new_items:
                break

            #if fewer results than we wanted, we're done
            elif self.num and len(new_items) < self.num - num_have:
                done = True
                have_next = False

            if not first_item and self.start_count > 0:
                first_item = new_items[0]

            if self.prewrap_fn:
                orig_items = {}
                new_items2 = []
                for i in new_items:
                    new = self.prewrap_fn(i)
                    orig_items[new._id] = i
                    new_items2.append(new)
                new_items = new_items2
            else:
                orig_items = dict((i._id, i) for i in new_items)

            if self.wrap:
                new_items = self.wrap_items(new_items)

            #skip and count
            while new_items and (not self.num or num_have < self.num):
                i = new_items.pop(0)

                if not (self.must_skip(i) or
                        self.skip and not self.keep_item(i)):
                    items.append(i)
                    num_have += 1
                    count = count - 1 if self.reverse else count + 1
                    if self.wrap:
                        i.num = count
                last_item = i
        
            # get original version of last item
            if last_item and (self.prewrap_fn or self.wrap):
                last_item = orig_items[last_item._id]

        if self.reverse:
            items.reverse()
            last_item, first_item = first_item, have_next and last_item
            before_count = count
            after_count = self.start_count - 1
        else:
            last_item = have_next and last_item
            before_count = self.start_count + 1
            after_count = count

        #listing is expecting (things, prev, next, bcount, acount)
        return (items,
                first_item,
                last_item,
                before_count,
                after_count)

class IDBuilder(QueryBuilder):
    def thing_lookup(self, names):
        return Thing._by_fullname(names, data=True, return_dict=False,
                                  stale=self.stale)

    def init_query(self):
        names = list(tup(self.query))

        after = self.after._fullname if self.after else None

        self.names = self._get_after(names,
                                     after,
                                     self.reverse)

    @staticmethod
    def _get_after(l, after, reverse):
        names = list(l)

        if reverse:
            names.reverse()

        if after:
            try:
                i = names.index(after)
            except ValueError:
                names = ()
            else:
                names = names[i + 1:]

        return names

    def fetch_more(self, last_item, num_have):
        done = False
        names = self.names
        if self.num:
            num_need = self.num - num_have
            if num_need <= 0:
                return True, None
            else:
                if last_item:
                    last_item = None
                slice_size = max(int(num_need * EXTRA_FACTOR), 1)
        else:
            slice_size = len(names)
            done = True

        self.names, new_names = names[slice_size:], names[:slice_size]
        new_items = self.thing_lookup(new_names)
        return done, new_items


class CampaignBuilder(IDBuilder):
    """Build on a list of PromoTuples."""

    def __init__(self, query, wrap=Wrapped, keep_fn=None, prewrap_fn=None,
                 skip=False, num=None, after=None, reverse=False, count=0):
        Builder.__init__(self, wrap=wrap, keep_fn=keep_fn)
        self.query = query
        self.skip = skip
        self.num = num
        self.start_count = count
        self.after = after
        self.reverse = reverse
        self.prewrap_fn = prewrap_fn

    @staticmethod
    def _get_after(promo_tuples, after, reverse):
        promo_tuples = list(promo_tuples)

        if not after:
            return promo_tuples

        if reverse:
            promo_tuples.reverse()

        fullname_to_index = {pt.link: i for i, pt in enumerate(promo_tuples)}
        try:
            i = fullname_to_index[after]
        except KeyError:
            promo_tuples = ()
        else:
            promo_tuples = promo_tuples[i + 1:]

        return promo_tuples

    def thing_lookup(self, tuples):
        links = Link._by_fullname([t.link for t in tuples], data=True,
                                  return_dict=True, stale=self.stale)

        return [Storage({'thing': links[t.link],
                         '_id': links[t.link]._id,
                         '_fullname': links[t.link]._fullname,
                         'weight': t.weight,
                         'campaign': t.campaign}) for t in tuples]

    def wrap_items(self, items):
        links = [i.thing for i in items]
        wrapped = IDBuilder.wrap_items(self, links)
        by_link = defaultdict(list)
        for w in wrapped:
            by_link[w._fullname].append(w)

        ret = []
        for i in items:
            w = by_link[i.thing._fullname].pop()
            w.campaign = i.campaign
            w.weight = i.weight
            ret.append(w)

        return ret


class SimpleBuilder(IDBuilder):
    def thing_lookup(self, names):
        return names

    def init_query(self):
        items = list(tup(self.query))

        if self.reverse:
            items.reverse()

        if self.after:
            for i, item in enumerate(items):
                if item._id == self.after:
                    self.names = items[i + 1:]
                    break
            else:
                self.names = ()
        else:
            self.names = items

    def get_items(self):
        items, prev_item, next_item, bcount, acount = IDBuilder.get_items(self)
        prev_item_id = prev_item._id if prev_item else None
        next_item_id = next_item._id if next_item else None
        return (items, prev_item_id, next_item_id, bcount, acount)


class SearchBuilder(IDBuilder):
    def __init__(self, query, wrap=Wrapped, keep_fn=None, skip=False,
                 skip_deleted_authors=True, **kw):
        IDBuilder.__init__(self, query, wrap, keep_fn, skip, **kw)
        self.skip_deleted_authors = skip_deleted_authors

    def init_query(self):
        self.skip = True

        self.start_time = time.time()

        self.results = self.query.run()
        names = list(self.results.docs)
        self.total_num = self.results.hits

        after = self.after._fullname if self.after else None

        self.names = self._get_after(names,
                                     after,
                                     self.reverse)

    def keep_item(self, item):
        # doesn't use the default keep_item because we want to keep
        # things that were voted on, even if they've chosen to hide
        # them in normal listings
        if item._spam or item._deleted:
            return False
        # If checking (wrapped) links, filter out banned subreddits
        elif hasattr(item, 'subreddit') and item.subreddit.spammy():
            return False
        elif (self.skip_deleted_authors and
              getattr(item, "author", None) and item.author._deleted):
            return False
        else:
            return True

class WikiRevisionBuilder(QueryBuilder):
    show_extended = True
    
    def __init__(self, revisions, page=None, **kw):
        self.user = kw.pop('user', None)
        self.sr = kw.pop('sr', None)
        self.page = page
        QueryBuilder.__init__(self, revisions, **kw)
    
    def wrap_items(self, items):
        from r2.lib.validator.wiki import this_may_revise
        types = {}
        wrapped = []
        extended = self.show_extended and c.is_wiki_mod
        extended = extended and this_may_revise(self.page)
        for item in items:
            w = self.wrap(item)
            w.show_extended = extended
            w.show_compare = self.show_extended
            types.setdefault(w.render_class, []).append(w)
            wrapped.append(w)
        
        user = c.user
        for cls in types.keys():
            cls.add_props(user, types[cls])

        return wrapped

    def must_skip(self, item):
        return item.admin_deleted and not c.user_is_admin

    def keep_item(self, item):
        from r2.lib.validator.wiki import may_view
        return ((not item.is_hidden) and
                may_view(self.sr, self.user, item.wikipage))

class WikiRecentRevisionBuilder(WikiRevisionBuilder):
    show_extended = False

    def must_skip(self, item):
        if WikiRevisionBuilder.must_skip(self, item):
            return True
        item_age = datetime.datetime.now(g.tz) - item.date
        return item_age.days >= wiki.WIKI_RECENT_DAYS


def empty_listing(*things):
    parent_name = None
    for t in things:
        try:
            parent_name = t.parent_name
            break
        except AttributeError:
            continue
    l = Listing(None, None, parent_name = parent_name)
    l.things = list(things)
    return Wrapped(l)

def make_wrapper(parent_wrapper = Wrapped, **params):
    def wrapper_fn(thing):
        w = parent_wrapper(thing)
        for k, v in params.iteritems():
            setattr(w, k, v)
        return w
    return wrapper_fn


class CommentBuilder(Builder):
    def __init__(self, link, sort, comment=None, children=None, context=None,
                 load_more=True, continue_this_thread=True,
                 max_depth=MAX_RECURSION, num=None, **kw):
        Builder.__init__(self, **kw)
        self.link = link
        self.comment = comment
        self.children = children
        self.context = context or 0
        self.load_more = load_more
        self.max_depth = max_depth
        self.num = num
        self.continue_this_thread = continue_this_thread
        self.sort = sort
        self.rev_sort = isinstance(sort, operators.desc)

    def update_candidates(self, candidates, sorter, to_add=None):
        for comment in (comment for comment in tup(to_add)
                                if comment in sorter):
            sort_val = -sorter[comment] if self.rev_sort else sorter[comment]
            heapq.heappush(candidates, (sort_val, comment))

    def get_items(self):
        timer = g.stats.get_timer("CommentBuilder.get_items")
        timer.start()
        r = link_comments_and_sort(self.link, self.sort.col)
        cids, cid_tree, depth, parents, sorter = r
        timer.intermediate("load_storage")

        if self.comment and not self.comment._id in depth:
            g.log.error("Hack - self.comment (%d) not in depth. Defocusing..."
                        % self.comment._id)
            self.comment = None

        more_recursions = {}
        dont_collapse = []
        candidates = []
        offset_depth = 0

        if self.children:
            # requested specific child comments
            children = [child._id for child in self.children
                                  if child._id in cids]
            self.update_candidates(candidates, sorter, children)
            dont_collapse.extend(comment for sort_val, comment in candidates)

        elif self.comment:
            # requested the tree from a specific comment

            # construct path back to top level from this comment, a maximum of
            # `context` levels
            comment = self.comment._id
            path = []
            while comment and len(path) <= self.context:
                path.append(comment)
                comment = parents[comment]

            dont_collapse.extend(path)

            # rewrite cid_tree so the parents lead only to the requested comment
            for comment in path:
                parent = parents[comment]
                cid_tree[parent] = [comment]

            # start building comment tree from earliest comment
            self.update_candidates(candidates, sorter, path[-1])

            # set offset_depth because we may not be at the top level and can
            # show deeper levels
            offset_depth = depth.get(path[-1], 0)

        else:
            # full tree requested, start with the top level comments
            top_level_comments = cid_tree.get(None, ())
            self.update_candidates(candidates, sorter, top_level_comments)

        timer.intermediate("pick_candidates")

        if not candidates:
            timer.stop()
            return []

        # choose which comments to show
        items = []
        while (self.num is None or len(items) < self.num) and candidates:
            sort_val, comment_id = heapq.heappop(candidates)
            if comment_id not in cids:
                continue

            comment_depth = depth[comment_id] - offset_depth
            if comment_depth < self.max_depth:
                items.append(comment_id)

                # add children
                if comment_id in cid_tree:
                    children = cid_tree[comment_id]
                    self.update_candidates(candidates, sorter, children)

            elif (self.continue_this_thread and
                  parents.get(comment_id) is not None):
                # the comment is too deep to add, so add a MoreRecursion for
                # its parent
                parent_id = parents[comment_id]
                if parent_id not in more_recursions:
                    w = Wrapped(MoreRecursion(self.link, depth=0,
                                              parent_id=parent_id))
                else:
                    w = more_recursions[parent_id]
                w.children.append(comment_id)
                more_recursions[parent_id] = w

        timer.intermediate("pick_comments")

        # retrieve num_children for the visible comments
        top_level_candidates = [comment for sort_val, comment in candidates
                                        if depth.get(comment, 0) == 0]
        needs_num_children = items + top_level_candidates
        num_children = get_num_children(needs_num_children, cid_tree)
        timer.intermediate("calc_num_children")

        comments = Comment._byID(items, data=True, return_dict=False,
                                 stale=self.stale)
        timer.intermediate("lookup_comments")
        wrapped = self.wrap_items(comments)
        timer.intermediate("wrap_comments")
        wrapped_by_id = {comment._id: comment for comment in wrapped}
        final = []

        for comment in wrapped:
            # skip deleted comments with no children
            if (comment.deleted and not cid_tree.has_key(comment._id)
                and not c.user_is_admin):
                continue

            comment.num_children = num_children[comment._id]

            if comment.collapsed and comment._id in dont_collapse:
                comment.collapsed = False

            # add the comment as a child of its parent or to the top level of
            # the tree if it has no parent
            parent = wrapped_by_id.get(comment.parent_id)
            if parent:
                if not hasattr(parent, 'child'):
                    parent.child = empty_listing()
                if not parent.deleted:
                    parent.child.parent_name = parent._fullname
                parent.child.things.append(comment)
            else:
                final.append(comment)

        for parent_id, more_recursion in more_recursions.iteritems():
            if parent_id not in wrapped_by_id:
                continue

            parent = wrapped_by_id[parent_id]
            parent.child = empty_listing(more_recursion)
            if not parent.deleted:
                parent.child.parent_name = parent._fullname

        timer.intermediate("build_comments")

        if not self.load_more:
            timer.stop()
            return final

        # build MoreChildren for visible comments
        visible_comments = wrapped_by_id.keys()
        for visible_id in visible_comments:
            if visible_id in more_recursions:
                # don't add a MoreChildren if we already have a MoreRecursion
                continue

            children = cid_tree.get(visible_id, ())
            missing_children = [child for child in children
                                      if child not in visible_comments]
            if missing_children:
                visible_children = (child for child in children
                                          if child in visible_comments)
                visible_count = sum(1 + num_children[child]
                                    for child in visible_children)
                missing_count = num_children[visible_id] - visible_count
                missing_depth = depth.get(visible_id, 0) + 1 - offset_depth

                if missing_depth < self.max_depth:
                    mc = MoreChildren(self.link, depth=missing_depth,
                                      parent_id=visible_id)
                    mc.children.extend(missing_children)
                    w = Wrapped(mc)
                    w.count = missing_count
                else:
                    mr = MoreRecursion(self.link, depth=missing_depth,
                                       parent_id=visible_id)
                    w = Wrapped(mr)

                # attach the MoreChildren
                parent = wrapped_by_id[visible_id]
                if hasattr(parent, 'child'):
                    parent.child.things.append(w)
                else:
                    parent.child = empty_listing(w)
                    if not parent.deleted:
                        parent.child.parent_name = parent._fullname

        # build MoreChildren for missing root level comments
        if top_level_candidates:
            mc = MoreChildren(self.link, depth=0, parent_id=None)
            mc.children.extend(top_level_candidates)
            w = Wrapped(mc)
            w.count = sum(1 + num_children[comment]
                          for comment in top_level_candidates)
            final.append(w)

        if isinstance(self.sort, operators.shuffled):
            shuffle(final)

        timer.intermediate("build_morechildren")
        timer.stop()
        return final

    def item_iter(self, a):
        for i in a:
            yield i
            if hasattr(i, 'child'):
                for j in self.item_iter(i.child.things):
                    yield j


class MessageBuilder(Builder):
    def __init__(self, parent = None, focal = None,
                 skip = True, **kw):

        self.num = kw.pop('num', None)
        self.focal = focal
        self.parent = parent
        self.skip = skip

        self.after = kw.pop('after', None)
        self.reverse = kw.pop('reverse', None)

        Builder.__init__(self, **kw)

    def get_tree(self):
        raise NotImplementedError, "get_tree"

    def _tree_filter_reverse(self, x):
        return tree_sort_fn(x) >= self.after._id

    def _tree_filter(self, x):
        return tree_sort_fn(x) < self.after._id

    def _viewable_message(self, m):
        if (c.user_is_admin or
                getattr(m, "author_id", 0) == c.user._id or
                getattr(m, "to_id", 0) == c.user._id):
            return True

        # m is wrapped at this time, so it should have an SR
        subreddit = getattr(m, "subreddit", None)
        if subreddit and subreddit.is_moderator_with_perms(c.user, 'mail'):
            return True

        return False

    def get_items(self):
        tree = self.get_tree()

        prev_item = next_item = None
        if not self.parent:
            if self.num is not None:
                if self.after:
                    if self.reverse:
                        tree = filter(
                            self._tree_filter_reverse,
                            tree)
                        next_item = self.after._id
                        if len(tree) > self.num:
                            first = tree[-(self.num+1)]
                            prev_item = first[1][-1] if first[1] else first[0]
                            tree = tree[-self.num:]
                    else:
                        prev_item = self.after._id
                        tree = filter(
                            self._tree_filter,
                            tree)
                if len(tree) > self.num:
                    tree = tree[:self.num]
                    last = tree[-1]
                    next_item = last[1][-1] if last[1] else last[0]

        # generate the set of ids to look up and look them up
        message_ids = []
        for root, thread in tree:
            message_ids.append(root)
            message_ids.extend(thread)
        if prev_item:
            message_ids.append(prev_item)

        messages = Message._byID(message_ids, data = True, return_dict = False)
        wrapped = {}
        for m in self.wrap_items(messages):
            if not self._viewable_message(m):
                g.log.warning("%r is not viewable by %s; path is %s" %
                                 (m, c.user.name, request.fullpath))
                continue
            wrapped[m._id] = m

        if prev_item:
            prev_item = wrapped[prev_item]
        if next_item:
            next_item = wrapped[next_item]

        final = []
        for parent, children in tree:
            if parent not in wrapped:
                continue
            parent = wrapped[parent]
            if children:
                # if no parent is specified, check if any of the messages are
                # uncollapsed, and truncate the thread
                children = [wrapped[child] for child in children
                                           if child in wrapped]
                parent.child = empty_listing()
                # if the parent is new, uncollapsed, or focal we don't
                # want it to become a moremessages wrapper.
                if (self.skip and 
                    not self.parent and not parent.new and parent.is_collapsed 
                    and not (self.focal and self.focal._id == parent._id)):
                    for i, child in enumerate(children):
                        if (child.new or not child.is_collapsed or
                            (self.focal and self.focal._id == child._id)):
                            break
                    else:
                        i = -1
                    parent = Wrapped(MoreMessages(parent, empty_listing()))
                    children = children[i:]

                parent.child.parent_name = parent._fullname
                parent.child.things = []

                for child in children:
                    child.is_child = True
                    if self.focal and child._id == self.focal._id:
                        # focal message is never collapsed
                        child.collapsed = False
                        child.focal = True
                    else:
                        child.collapsed = child.is_collapsed

                    parent.child.things.append(child)
            parent.is_parent = True
            # the parent might be the focal message on a permalink page
            if self.focal and parent._id == self.focal._id:
                parent.collapsed = False
                parent.focal = True
            else:
                parent.collapsed = parent.is_collapsed
            final.append(parent)

        return (final, prev_item, next_item, len(final), len(final))

    def item_iter(self, a):
        for i in a[0]:
            yield i
            if hasattr(i, 'child'):
                for j in i.child.things:
                    yield j


class ModeratorMessageBuilder(MessageBuilder):
    def __init__(self, user, **kw):
        self.user = user
        MessageBuilder.__init__(self, **kw)

    def get_tree(self):
        if self.parent:
            return conversation(self.user, self.parent)
        sr_ids = Subreddit.reverse_moderator_ids(self.user)
        return moderator_messages(sr_ids)

class MultiredditMessageBuilder(MessageBuilder):
    def __init__(self, user, **kw):
        self.user = user
        MessageBuilder.__init__(self, **kw)

    def get_tree(self):
        if self.parent:
            return conversation(self.user, self.parent)
        return moderator_messages(c.site.sr_ids)

class TopCommentBuilder(CommentBuilder):
    """A comment builder to fetch only the top-level, non-spam,
       non-deleted comments"""
    def __init__(self, link, sort, num=None, wrap=Wrapped):
        CommentBuilder.__init__(self, link, sort,
                                load_more = False,
                                continue_this_thread = False,
                                max_depth=1, wrap=wrap, num=num)

    def get_items(self):
        final = CommentBuilder.get_items(self)
        return [ cm for cm in final if not cm.deleted ]

class SrMessageBuilder(MessageBuilder):
    def __init__(self, sr, **kw):
        self.sr = sr
        MessageBuilder.__init__(self, **kw)

    def get_tree(self):
        if self.parent:
            return sr_conversation(self.sr, self.parent)
        return subreddit_messages(self.sr)

class UserMessageBuilder(MessageBuilder):
    def __init__(self, user, **kw):
        self.user = user
        MessageBuilder.__init__(self, **kw)

    def get_tree(self):
        if self.parent:
            return conversation(self.user, self.parent)
        return user_messages(self.user)

class UserListBuilder(QueryBuilder):
    def thing_lookup(self, rels):
        accounts = Account._byID([rel._thing2_id for rel in rels], data=True)
        for rel in rels:
            rel._thing2 = accounts.get(rel._thing2_id)
        return rels

    def must_skip(self, item):
        return item.user._deleted

    def wrap_items(self, rels):
        return [self.wrap(rel) for rel in rels]

class SavedBuilder(IDBuilder):
    def wrap_items(self, items):
        from r2.lib.template_helpers import add_attr
        categories = LinkSavesByAccount.fast_query(c.user, items).items()
        categories += CommentSavesByAccount.fast_query(c.user, items).items()
        categories = {item[1]._id: category for item, category in categories if category}
        wrapped = QueryBuilder.wrap_items(self, items)
        for w in wrapped:
            category = categories.get(w._id, '')
            w.savedcategory = category
        return wrapped

########NEW FILE########
__FILENAME__ = comment_tree
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.db import tdb_cassandra
from r2.lib import utils
from r2.models.last_modified import LastModified
from r2.models.link import Comment

from pycassa import batch, types
from pycassa.cassandra import ttypes
from pycassa.system_manager import ASCII_TYPE, COUNTER_COLUMN_TYPE

from pylons import g


class CommentTreeStorageBase(object):
    class NoOpContext:
        def __enter__(self):
            pass

        def __exit__(self, exc_type, exc_val, exc_tb):
            pass

    @classmethod
    def mutation_context(cls, link, timeout=None):
        return cls.NoOpContext()

    @classmethod
    def by_link(cls, link):
        raise NotImplementedError

    @classmethod
    def rebuild(cls, tree, comments):
        return cls.add_comments(tree, comments)

    @classmethod
    def add_comments(cls, tree, comments):
        cids = tree.cids
        depth = tree.depth

        new_parents = {}
        for comment in comments:
            cid = comment._id
            p_id = comment.parent_id

            #make sure we haven't already done this before (which would happen
            #if the tree isn't cached when you add a comment)
            if cid in cids:
                continue

            #add to comment list
            cids.append(cid)

            #add to tree
            tree.tree.setdefault(p_id, []).append(cid)

            #add to depth
            depth[cid] = depth[p_id] + 1 if p_id else 0

            #if this comment had a parent, find the parent's parents
            if p_id:
                new_parents[cid] = p_id

        # update our cache of children -> parents as well:
        if not tree.parents:
            tree.parents = tree.parent_dict_from_tree(tree.tree)

        parents = tree.parents

        for cid, p_id in new_parents.iteritems():
            parents[cid] = p_id

        for comment in comments:
            cid = comment._id
            if cid not in new_parents:
                parents[cid] = None

    @classmethod
    def delete_comment(cls, tree, comment):
        # only remove leaf comments from the tree
        if comment._id not in tree.tree:
            if comment._id in tree.cids:
                tree.cids.remove(comment._id)
            if comment._id in tree.depth:
                del tree.depth[comment._id]


class CommentTreeStorageV3(CommentTreeStorageBase):
    """Cassandra column-based storage for comment trees.

    Under this implementation, each column in a link's row corresponds to a
    comment on that link. The column name is an encoding of the tuple of
    (depth, comment.parent_id, comment._id), and the value is not used.

    Key features:
        - does not use permacache!
        - does not require locking for updates
    """

    __metaclass__ = tdb_cassandra.ThingMeta
    _connection_pool = 'main'
    _use_db = True

    _type_prefix = None
    _cf_name = 'CommentTreeStorage'

    # column names are tuples of (depth, parent_id, comment_id)
    _compare_with = types.CompositeType(
        types.LongType(),
        types.LongType(),
        types.LongType())

    COLUMN_READ_BATCH_SIZE = tdb_cassandra.max_column_count
    COLUMN_WRITE_BATCH_SIZE = 1000

    # special value for parent_id when the comment has no parent
    NO_PARENT = -1

    @staticmethod
    def _key(link):
        return utils.to36(link._id)

    @classmethod
    def by_link(cls, link):
        try:
            row = cls.get_row(cls._key(link))
        except ttypes.NotFoundException:
            row = {}
        return cls._from_row(row)

    @classmethod
    def get_row(cls, key):
        return cls._cf.xget(key, buffer_size=cls.COLUMN_READ_BATCH_SIZE)

    @classmethod
    def _from_row(cls, row):
        # row is an iterable of (depth, parent_id, comment_id), '')
        cids = []
        tree = {}
        depth = {}
        parents = {}
        for (d, pid, cid), val in row:
            if pid == cls.NO_PARENT:
                pid = None

            cids.append(cid)
            tree.setdefault(pid, []).append(cid)
            depth[cid] = d
            parents[cid] = pid
        return dict(cids=cids, tree=tree, depth=depth, parents=parents)

    @classmethod
    @tdb_cassandra.will_write
    def rebuild(cls, tree, comments):
        with batch.Mutator(g.cassandra_pools[cls._connection_pool]) as m:
            g.log.debug('removing tree from %s', cls._key(tree.link))
            m.remove(cls._cf, cls._key(tree.link))

        return cls.add_comments(tree, comments)

    @classmethod
    @tdb_cassandra.will_write
    def add_comments(cls, tree, comments):
        CommentTreeStorageBase.add_comments(tree, comments)
        updates = {}
        for comment in comments:
            parent_id = comment.parent_id or cls.NO_PARENT
            depth = tree.depth.get(parent_id, -1) + 1
            updates[(depth, parent_id, comment._id)] = ''

        cols = updates.keys()
        for i in xrange(0, len(updates), cls.COLUMN_WRITE_BATCH_SIZE):
            update_batch = {c: updates[c]
                            for c in cols[i:i + cls.COLUMN_WRITE_BATCH_SIZE]}
            with batch.Mutator(g.cassandra_pools[cls._connection_pool]) as m:
                m.insert(cls._cf, cls._key(tree.link), update_batch)

    @classmethod
    @tdb_cassandra.will_write
    def upgrade(cls, tree, link):
        cids = []
        for parent, children in tree.tree.iteritems():
            cids.extend(children)

        comments = {}
        for i in xrange(0, len(cids), 100):
            g.log.debug('  loading comments %d..%d', i, i + 100)
            comments.update(Comment._byID(cids[i:i + 100], data=True))

        cls.add_comments(tree, comments.values())


class CommentTreeStorageV2(CommentTreeStorageBase):
    """Cassandra column-based storage for comment trees.

    Under this implementation, each column in a link's row corresponds to a
    comment on that link. The column name is an encoding of the tuple of
    (comment.parent_id, comment._id), and the value is a counter giving the
    size of the subtree rooted at the comment.

    Key features:
        - does not use permacache!
        - does not require locking for updates
    """

    __metaclass__ = tdb_cassandra.ThingMeta
    _connection_pool = 'main'
    _use_db = True

    _type_prefix = None
    _cf_name = 'CommentTree'

    # column keys are tuples of (depth, parent_id, comment_id)
    _compare_with = types.CompositeType(
        types.LongType(),
        types.LongType(),
        types.LongType())

    # column values are counters
    _extra_schema_creation_args = {
        'default_validation_class': COUNTER_COLUMN_TYPE,
        'replicate_on_write': True,
    }

    COLUMN_READ_BATCH_SIZE = tdb_cassandra.max_column_count
    COLUMN_WRITE_BATCH_SIZE = 1000

    @staticmethod
    def _key(link):
        revision = getattr(link, 'comment_tree_id', 0)
        if revision:
            return '%s:%s' % (utils.to36(link._id), utils.to36(revision))
        else:
            return utils.to36(link._id)

    @staticmethod
    def _column_to_obj(cols):
        for col in cols:
            for (depth, pid, cid), val in col.iteritems():
                yield (depth, None if pid == -1 else pid, cid), val

    @classmethod
    def by_link(cls, link):
        try:
            row = cls.get_row(cls._key(link))
        except ttypes.NotFoundException:
            row = {}
        return cls._from_row(row)

    @classmethod
    def get_row(cls, key):
        return cls._cf.xget(key, buffer_size=cls.COLUMN_READ_BATCH_SIZE)

    @classmethod
    def _from_row(cls, row):
        # row is an iterable of ((depth, parent_id, comment_id), subtree_size)
        cids = []
        tree = {}
        depth = {}
        parents = {}
        for (d, pid, cid), val in row:
            if cid == -1:
                continue
            if pid == -1:
                pid = None
            cids.append(cid)
            tree.setdefault(pid, []).append(cid)
            depth[cid] = d
            parents[cid] = pid
        return dict(cids=cids, tree=tree, depth=depth, parents=parents)

    @classmethod
    @tdb_cassandra.will_write
    def rebuild(cls, tree, comments):
        with batch.Mutator(g.cassandra_pools[cls._connection_pool]) as m:
            g.log.debug('removing tree from %s', cls._key(tree.link))
            m.remove(cls._cf, cls._key(tree.link))
        tree.link._incr('comment_tree_id')
        g.log.debug('link %s comment tree revision bumped up to %s',
                    tree.link._fullname, tree.link.comment_tree_id)

        # make sure all comments have parents attribute filled in
        parents = {c._id: c.parent_id for c in comments}
        for c in comments:
            if c.parent_id and c.parents is None:
                path = []
                pid = c.parent_id
                while pid:
                    path.insert(0, pid)
                    pid = parents[pid]
                c.parents = ':'.join(utils.to36(i) for i in path)
                c._commit()

        return cls.add_comments(tree, comments)

    @classmethod
    @tdb_cassandra.will_write
    def add_comments(cls, tree, comments):
        CommentTreeStorageBase.add_comments(tree, comments)
        g.log.debug('building updates dict')
        updates = {}
        for c in comments:
            pids = c.parent_path()
            pids.append(c._id)
            for d, (pid, cid) in enumerate(zip(pids, pids[1:])):
                k = (d, pid, cid)
                updates[k] = updates.get(k, 0) + 1

        g.log.debug('writing %d updates to %s',
                    len(updates), cls._key(tree.link))
        # increment counters in slices of 100
        cols = updates.keys()
        for i in xrange(0, len(updates), cls.COLUMN_WRITE_BATCH_SIZE):
            g.log.debug(
                'adding updates %d..%d', i, i + cls.COLUMN_WRITE_BATCH_SIZE)
            update_batch = {c: updates[c]
                            for c in cols[i:i + cls.COLUMN_WRITE_BATCH_SIZE]}
            with batch.Mutator(g.cassandra_pools[cls._connection_pool]) as m:
                m.insert(cls._cf, cls._key(tree.link), update_batch)
        g.log.debug('added %d comments with %d updates',
                    len(comments), len(updates))

    @classmethod
    @tdb_cassandra.will_write
    def delete_comment(cls, tree, comment):
        CommentTreeStorageBase.delete_comment(tree, comment)
        pids = comment.parent_path()
        pids.append(comment._id)
        updates = {}
        for d, (pid, cid) in enumerate(zip(pids, pids[1:])):
            updates[(d, pid, cid)] = -1
        with batch.Mutator(g.cassandra_pools[cls._connection_pool]) as m:
            m.insert(cls._cf, cls._key(tree.link), updates)

    @classmethod
    @tdb_cassandra.will_write
    def upgrade(cls, tree, link):
        cids = []
        for parent, children in tree.tree.iteritems():
            cids.extend(children)

        comments = {}
        for i in xrange(0, len(cids), 100):
            g.log.debug('  loading comments %d..%d', i, i + 100)
            comments.update(Comment._byID(cids[i:i + 100], data=True))

        # need to fill in parents attr for each comment
        modified = []
        stack = [None]
        while stack:
            pid = stack.pop()
            if pid is None:
                parents = ''
            else:
                parents = comments[pid].parents + ':' + comments[pid]._id36
            children = tree.tree.get(pid, [])
            stack.extend(children)
            for cid in children:
                if comments[cid].parents != parents:
                    comments[cid].parents = parents
                    modified.append(comments[cid])

        for i, comment in enumerate(modified):
            comment._commit()

        cls.add_comments(tree, comments.values())


class CommentTreeStorageV1(CommentTreeStorageBase):
    """Cassandra storage of comment trees, using permacache."""

    @staticmethod
    def _comments_key(link_id):
        return 'comments_' + str(link_id)

    @staticmethod
    def _parent_comments_key(link_id):
        return 'comments_parents_' + str(link_id)

    @staticmethod
    def _lock_key(link_id):
        return 'comment_lock_' + str(link_id)

    @classmethod
    def mutation_context(cls, link, timeout=None):
        return g.make_lock("comment_tree", cls._lock_key(link._id),
                           timeout=timeout)

    @classmethod
    def by_link(cls, link):
        key = cls._comments_key(link._id)
        p_key = cls._parent_comments_key(link._id)
        # prefetch both values, they'll be locally cached
        g.permacache.get_multi([key, p_key])

        r = g.permacache.get(key)
        if not r:
            return None

        try:
            cids, cid_tree, depth = r
        except ValueError:
            # We got the old version that includes num_children
            cids, cid_tree, depth, num_children = r

        parents = g.permacache.get(p_key)
        if parents is None:
            parents = {}
        return dict(cids=cids, tree=cid_tree, depth=depth, parents=parents)

    @classmethod
    def add_comments(cls, tree, comments):
        with cls.mutation_context(tree.link):
            CommentTreeStorageBase.add_comments(tree, comments)
            # for read safety write parents first
            g.permacache.set(cls._parent_comments_key(tree.link_id),
                             tree.parents)
            g.permacache.set(cls._comments_key(tree.link_id),
                             (tree.cids, tree.tree, tree.depth))


class CommentTree:
    """Storage for pre-computed relationships between a link's comments.

    An instance of this class serves as a snapshot of a single link's comment
    tree. The actual storage implementation is separated to allow for different
    schemes for different links.

    Attrs:
      - cids: list of ints; link's comment IDs
      - tree: dict of int to list of ints; each non-leaf entry in cids has a
          key in this dict, and the corresponding value is the list of IDs for
          that comment's immediate children
      - depth: dict of int to int; each entry in cids has a key in this dict,
          and the corresponding value is that comment's depth in the tree
          (with a value of 0 for top-level comments)
      - parents: dict of int to int; each entry in cids has a key in this dict,
          and the corresponding value is the ID of that comment's parent (or
          None in the case of top-level comments)
    """

    IMPLEMENTATIONS = {
        1: CommentTreeStorageV1,
        2: CommentTreeStorageV2,
        3: CommentTreeStorageV3,
    }

    DEFAULT_IMPLEMENTATION = 3

    def __init__(self, link, **kw):
        self.link = link
        self.link_id = link._id
        self.__dict__.update(kw)

    @classmethod
    def mutation_context(cls, link, timeout=None):
        impl = cls.IMPLEMENTATIONS[link.comment_tree_version]
        return impl.mutation_context(link, timeout=timeout)

    @classmethod
    def by_link(cls, link):
        impl = cls.IMPLEMENTATIONS[link.comment_tree_version]
        data = impl.by_link(link)
        if data is None:
            return None
        else:
            return cls(link, **data)

    def add_comments(self, comments):
        impl = self.IMPLEMENTATIONS[self.link.comment_tree_version]
        impl.add_comments(self, comments)
        utils.set_last_modified(self.link, 'comments')
        LastModified.touch(self.link._fullname, 'Comments')

    def add_comment(self, comment):
        return self.add_comments([comment])

    def delete_comment(self, comment, link):
        impl = self.IMPLEMENTATIONS[link.comment_tree_version]
        impl.delete_comment(self, comment)
        self.link._incr('num_comments', -1)

    @classmethod
    def rebuild(cls, link):
        # fetch all comments and sort by parent_id, so parents are added to the
        # tree before their children
        q = Comment._query(Comment.c.link_id == link._id,
                           Comment.c._deleted == (True, False),
                           Comment.c._spam == (True, False),
                           optimize_rules=True,
                           data=True)
        comments = sorted(q, key=lambda c: c.parent_id)

        # build tree from scratch (for V2 results in double-counting in cass)
        tree = cls(link, cids=[], tree={}, depth={}, parents={})
        impl = cls.IMPLEMENTATIONS[link.comment_tree_version]
        impl.rebuild(tree, comments)

        link.num_comments = sum(1 for c in comments if not c._deleted)
        link._commit()

        return tree

    @classmethod
    def upgrade(cls, link, to_version=None):
        if to_version is None:
            to_version = cls.DEFAULT_IMPLEMENTATION

        tree = cls.by_link(link)
        new_impl = cls.IMPLEMENTATIONS[to_version]
        new_impl.upgrade(tree, link)
        link.comment_tree_version = to_version
        link._commit()

    @staticmethod
    def parent_dict_from_tree(tree):
        parents = {}
        for parent, children in tree.iteritems():
            for child in children:
                parents[child] = parent
        return parents

########NEW FILE########
__FILENAME__ = flair
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import uuid

from pylons import g

from r2.lib.db.operators import asc, desc
from r2.lib.db.thing import Relation, Thing
from r2.lib.db import tdb_cassandra
from r2.lib.db.userrel import UserRel
from r2.lib.memoize import memoize
from r2.lib.utils import to36
from account import Account
from subreddit import Subreddit

USER_FLAIR = 'USER_FLAIR'
LINK_FLAIR = 'LINK_FLAIR'

class Flair(Relation(Subreddit, Account)):
    @classmethod
    def store(cls, sr, account, text = None, css_class = None):
        flair = cls(sr, account, 'flair', text = text, css_class = css_class)
        flair._commit()

        setattr(account, 'flair_%s_text' % sr._id, text)
        setattr(account, 'flair_%s_css_class' % sr._id, css_class)
        account._commit()

    @classmethod
    @memoize('flair.all_flair_by_sr')
    def all_flair_by_sr_cache(cls, sr_id):
        q = cls._query(cls.c._thing1_id == sr_id)
        return [t._id for t in q]

    @classmethod
    def all_flair_by_sr(cls, sr_id, _update=False):
        relids = cls.all_flair_by_sr_cache(sr_id, _update=_update)
        return cls._byID(relids).itervalues()

    @classmethod
    def flair_id_query(cls, sr, limit, after, reverse=False):
        extra_rules = [
            cls.c._thing1_id == sr._id,
            cls.c._name == 'flair',
          ]
        if after:
            if reverse:
                extra_rules.append(cls.c._thing2_id < after._id)
            else:
                extra_rules.append(cls.c._thing2_id > after._id)
        sort = (desc if reverse else asc)('_thing2_id')
        return cls._query(*extra_rules, sort=sort, limit=limit)

Subreddit.__bases__ += (UserRel('flair', Flair,
                                disable_ids_fn = True,
                                disable_reverse_ids_fn = True),)


class FlairTemplate(tdb_cassandra.Thing):
    """A template for some flair."""
    _defaults = dict(text='',
                     css_class='',
                     text_editable=False,
                    )

    _bool_props = ('text_editable',)

    _use_db = True
    _connection_pool = 'main'

    @classmethod
    def _new(cls, text='', css_class='', text_editable=False):
        if text is None:
            text = ''
        if css_class is None:
            css_class = ''
        ft = cls(text=text, css_class=css_class, text_editable=text_editable)
        ft._commit()
        return ft

    def _commit(self, *a, **kw):
        # Make sure an _id is always assigned before committing.
        if not self._id:
            self._id = str(uuid.uuid1())
        return tdb_cassandra.Thing._commit(self, *a, **kw)

    def covers(self, other_template):
        """Returns true if other_template is a subset of this one.

        The value for other_template may be another FlairTemplate, or a tuple
        of (text, css_class). The latter case is treated like a FlairTemplate
        that doesn't permit editable text.

        For example, if self permits editable text, then this method will return
        True as long as just the css_classes match. On the other hand, if self
        doesn't permit editable text but other_template does, this method will
        return False.
        """
        if isinstance(other_template, FlairTemplate):
            text_editable = other_template.text_editable
            text, css_class = other_template.text, other_template.css_class
        else:
            text_editable = False
            text, css_class = other_template

        if self.css_class != css_class:
            return False
        return self.text_editable or (not text_editable and self.text == text)


class FlairTemplateBySubredditIndex(tdb_cassandra.Thing):
    """Lists of FlairTemplate IDs for a subreddit.

    The FlairTemplate references are stored as an arbitrary number of attrs.
    The lexicographical ordering of these attr names gives the ordering for
    flair templates within the subreddit.
    """

    MAX_FLAIR_TEMPLATES = 350

    _int_props = ('sr_id',)
    _use_db = True
    _connection_pool = 'main'

    _key_prefixes = {
        USER_FLAIR: 'ft_',
        LINK_FLAIR: 'link_ft_',
    }

    @classmethod
    def _new(cls, sr_id, flair_type=USER_FLAIR):
        idx = cls(_id=to36(sr_id), sr_id=sr_id)
        idx._commit()
        return idx

    @classmethod
    def by_sr(cls, sr_id, create=False):
        try:
            return cls._byID(to36(sr_id))
        except tdb_cassandra.NotFound:
            if create:
                return cls._new(sr_id)
            raise

    @classmethod
    def create_template(cls, sr_id, text='', css_class='', text_editable=False,
                        flair_type=USER_FLAIR):
        idx = cls.by_sr(sr_id, create=True)

        if len(idx._index_keys(flair_type)) >= cls.MAX_FLAIR_TEMPLATES:
            raise OverflowError

        ft = FlairTemplate._new(text=text, css_class=css_class,
                                text_editable=text_editable)
        idx.insert(ft._id, flair_type=flair_type)
        return ft

    @classmethod
    def get_template_ids(cls, sr_id, flair_type=USER_FLAIR):
        try:
            return list(cls.by_sr(sr_id).iter_template_ids(flair_type))
        except tdb_cassandra.NotFound:
            return []

    @classmethod
    def get_template(cls, sr_id, ft_id, flair_type=None):
        if flair_type:
            flair_types = [flair_type]
        else:
            flair_types = [USER_FLAIR, LINK_FLAIR]
        for flair_type in flair_types:
            if ft_id in cls.get_template_ids(sr_id, flair_type=flair_type):
                return FlairTemplate._byID(ft_id)
        return None

    @classmethod
    def clear(cls, sr_id, flair_type=USER_FLAIR):
        try:
            idx = cls.by_sr(sr_id)
        except tdb_cassandra.NotFound:
            # Everything went better than expected.
            return

        for k in idx._index_keys(flair_type):
            del idx[k]
            # TODO: delete the orphaned FlairTemplate row

        idx._commit()

    def _index_keys(self, flair_type):
        keys = set(self._dirties.iterkeys())
        keys |= frozenset(self._orig.iterkeys())
        keys -= self._deletes
        key_prefix = self._key_prefixes[flair_type]
        return [k for k in keys if k.startswith(key_prefix)]

    @classmethod
    def _make_index_key(cls, position, flair_type):
        return '%s%08d' % (cls._key_prefixes[flair_type], position)

    def iter_template_ids(self, flair_type):
        return (getattr(self, key)
                for key in sorted(self._index_keys(flair_type)))

    def insert(self, ft_id, position=None, flair_type=USER_FLAIR):
        """Insert template reference into index at position.

        A position value of None means to simply append.
        """
        ft_ids = list(self.iter_template_ids(flair_type))
        if position is None:
            position = len(ft_ids)
        if position < 0 or position > len(ft_ids):
            raise IndexError(position)
        ft_ids.insert(position, ft_id)

        # Rewrite ALL the things.
        for k in self._index_keys(flair_type):
            del self[k]
        for i, ft_id in enumerate(ft_ids):
            setattr(self, self._make_index_key(i, flair_type), ft_id)
        self._commit()

    def delete_by_id(self, ft_id, flair_type=None):
        if flair_type:
            flair_types = [flair_type]
        else:
            flair_types = [USER_FLAIR, LINK_FLAIR]
        for flair_type in flair_types:
            if self._delete_by_id(ft_id, flair_type):
                return True
        g.log.debug("couldn't find %s to delete", ft_id)
        return False

    def _delete_by_id(self, ft_id, flair_type):
        for key in self._index_keys(flair_type):
            ft = getattr(self, key)
            if ft == ft_id:
                # TODO: delete the orphaned FlairTemplate row
                g.log.debug('deleting ft %s (%s)', ft, key)
                del self[key]
                self._commit()
                return True
        return False

########NEW FILE########
__FILENAME__ = gold
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.db.tdb_sql import make_metadata, index_str, create_table

import json
import pytz
import uuid

from pycassa import NotFoundException
from pycassa.system_manager import INT_TYPE, UTF8_TYPE
from pycassa.util import convert_uuid_to_time
from pylons import g, c
from pylons.i18n import _, ungettext
from datetime import datetime
import sqlalchemy as sa
from sqlalchemy.exc import IntegrityError, OperationalError
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import scoped_session, sessionmaker
from sqlalchemy.sql.expression import select
from sqlalchemy.sql.functions import sum as sa_sum

from r2.lib.utils import GoldPrice, randstr
import re
from random import choice
from time import time

from r2.lib.db import tdb_cassandra
from r2.lib.db.tdb_cassandra import NotFound, view_of
from r2.models import Account
from r2.models.subreddit import Frontpage
from r2.models.wiki import WikiPage
from r2.lib.memoize import memoize

import stripe

gold_bonus_cutoff = datetime(2010,7,27,0,0,0,0,g.tz)
gold_static_goal_cutoff = datetime(2013, 11, 7, tzinfo=g.display_tz)

ENGINE_NAME = 'authorize'

ENGINE = g.dbm.get_engine(ENGINE_NAME)
METADATA = make_metadata(ENGINE)
TIMEZONE = pytz.timezone("America/Los_Angeles")

Session = scoped_session(sessionmaker(bind=ENGINE))
Base = declarative_base(bind=ENGINE)

gold_table = sa.Table('reddit_gold', METADATA,
                      sa.Column('trans_id', sa.String, nullable = False,
                                primary_key = True),
                      # status can be: invalid, unclaimed, claimed
                      sa.Column('status', sa.String, nullable = False),
                      sa.Column('date', sa.DateTime(timezone=True),
                                nullable = False,
                                default = sa.func.now()),
                      sa.Column('payer_email', sa.String, nullable = False),
                      sa.Column('paying_id', sa.String, nullable = False),
                      sa.Column('pennies', sa.Integer, nullable = False),
                      sa.Column('secret', sa.String, nullable = True),
                      sa.Column('account_id', sa.String, nullable = True),
                      sa.Column('days', sa.Integer, nullable = True),
                      sa.Column('subscr_id', sa.String, nullable = True))

indices = [index_str(gold_table, 'status', 'status'),
           index_str(gold_table, 'date', 'date'),
           index_str(gold_table, 'account_id', 'account_id'),
           index_str(gold_table, 'secret', 'secret'),
           index_str(gold_table, 'payer_email', 'payer_email'),
           index_str(gold_table, 'subscr_id', 'subscr_id')]
create_table(gold_table, indices)


class GoldRevenueGoalByDate(object):
    __metaclass__ = tdb_cassandra.ThingMeta

    _use_db = True
    _cf_name = "GoldRevenueGoalByDate"
    _read_consistency_level = tdb_cassandra.CL.ONE
    _write_consistency_level = tdb_cassandra.CL.ALL
    _extra_schema_creation_args = {
        "column_name_class": UTF8_TYPE,
        "default_validation_class": INT_TYPE,
    }
    _compare_with = UTF8_TYPE
    _type_prefix = None

    ROWKEY = '1'

    @staticmethod
    def _colkey(date):
        return date.strftime("%Y-%m-%d")

    @classmethod
    def set(cls, date, goal):
        cls._cf.insert(cls.ROWKEY, {cls._colkey(date): int(goal)})

    @classmethod
    def get(cls, date):
        """Gets the goal for a date, or the nearest previous goal."""
        try:
            colkey = cls._colkey(date)
            col = cls._cf.get(
                cls.ROWKEY,
                column_reversed=True,
                column_start=colkey,
                column_count=1,
            )
            return col.values()[0]
        except NotFoundException:
            return None


class GildedCommentsByAccount(tdb_cassandra.DenormalizedRelation):
    _use_db = True
    _last_modified_name = 'Gilding'
    _views = []

    @classmethod
    def value_for(cls, thing1, thing2):
        return ''

    @classmethod
    def gild(cls, user, thing):
        cls.create(user, [thing])


class GildedLinksByAccount(tdb_cassandra.DenormalizedRelation):
    _use_db = True
    _last_modified_name = 'Gilding'
    _views = []

    @classmethod
    def value_for(cls, thing1, thing2):
        return ''

    @classmethod
    def gild(cls, user, thing):
        cls.create(user, [thing])


@view_of(GildedCommentsByAccount)
@view_of(GildedLinksByAccount)
class GildingsByThing(tdb_cassandra.View):
    _use_db = True
    _extra_schema_creation_args = {
        "key_validation_class": tdb_cassandra.UTF8_TYPE,
        "column_name_class": tdb_cassandra.UTF8_TYPE,
    }

    @classmethod
    def get_gilder_ids(cls, thing):
        columns = cls.get_time_sorted_columns(thing._fullname)
        return [int(account_id, 36) for account_id in columns.iterkeys()]

    @classmethod
    def create(cls, user, things):
        for thing in things:
            cls._set_values(thing._fullname, {user._id36: ""})

    @classmethod
    def delete(cls, user, things):
        # gildings cannot be undone
        raise NotImplementedError()


@view_of(GildedCommentsByAccount)
@view_of(GildedLinksByAccount)
class GildingsByDay(tdb_cassandra.View):
    _use_db = True
    _compare_with = tdb_cassandra.TIME_UUID_TYPE
    _extra_schema_creation_args = {
        "key_validation_class": tdb_cassandra.ASCII_TYPE,
        "column_name_class": tdb_cassandra.TIME_UUID_TYPE,
        "default_validation_class": tdb_cassandra.UTF8_TYPE,
    }

    @staticmethod
    def _rowkey(date):
        return date.strftime("%Y-%m-%d")

    @classmethod
    def get_gildings(cls, date):
        key = cls._rowkey(date)
        columns = cls.get_time_sorted_columns(key)
        gildings = []
        for name, json_blob in columns.iteritems():
            timestamp = convert_uuid_to_time(name)
            date = datetime.utcfromtimestamp(timestamp).replace(tzinfo=g.tz)

            gilding = json.loads(json_blob)
            gilding["date"] = date
            gilding["user"] = int(gilding["user"], 36)
            gildings.append(gilding)
        return gildings

    @classmethod
    def create(cls, user, things):
        key = cls._rowkey(datetime.now(g.tz))

        columns = {}
        for thing in things:
            columns[uuid.uuid1()] = json.dumps({
                "user": user._id36,
                "thing": thing._fullname,
            })
        cls._set_values(key, columns)

    @classmethod
    def delete(cls, user, things):
        # gildings cannot be undone
        raise NotImplementedError()


def create_unclaimed_gold (trans_id, payer_email, paying_id,
                           pennies, days, secret, date,
                           subscr_id = None):

    try:
        gold_table.insert().execute(trans_id=str(trans_id),
                                    subscr_id=subscr_id,
                                    status="unclaimed",
                                    payer_email=payer_email,
                                    paying_id=paying_id,
                                    pennies=pennies,
                                    days=days,
                                    secret=str(secret),
                                    date=date
                                    )
    except IntegrityError:
        rp = gold_table.update(
            sa.and_(gold_table.c.status == 'uncharged',
                    gold_table.c.trans_id == str(trans_id)),
            values = {
                gold_table.c.status: "unclaimed",
                gold_table.c.payer_email: payer_email,
                gold_table.c.paying_id: paying_id,
                gold_table.c.pennies: pennies,
                gold_table.c.days: days,
                gold_table.c.secret:secret,
                gold_table.c.subscr_id : subscr_id
                },
            ).execute()


def create_claimed_gold (trans_id, payer_email, paying_id,
                         pennies, days, secret, account_id, date,
                         subscr_id = None, status="claimed"):
    gold_table.insert().execute(trans_id=trans_id,
                                subscr_id=subscr_id,
                                status=status,
                                payer_email=payer_email,
                                paying_id=paying_id,
                                pennies=pennies,
                                days=days,
                                secret=secret,
                                account_id=account_id,
                                date=date)

def create_gift_gold (giver_id, recipient_id, days, date, signed):
    trans_id = "X%d%s-%s" % (int(time()), randstr(2), 'S' if signed else 'A')

    gold_table.insert().execute(trans_id=trans_id,
                                status="gift",
                                paying_id=giver_id,
                                payer_email='',
                                pennies=0,
                                days=days,
                                account_id=recipient_id,
                                date=date)


def create_gold_code(trans_id, payer_email, paying_id, pennies, days, date):
    if not trans_id:
        trans_id = "GC%d%s" % (int(time()), randstr(2))

    valid_chars = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
    # keep picking new codes until we find an unused one
    while True:
        code = randstr(10, alphabet=valid_chars)

        s = sa.select([gold_table],
                      sa.and_(gold_table.c.secret == code.lower(),
                              gold_table.c.status == 'unclaimed'))
        res = s.execute().fetchall()
        if not res:
            gold_table.insert().execute(
                trans_id=trans_id,
                status='unclaimed',
                payer_email=payer_email,
                paying_id=paying_id,
                pennies=pennies,
                days=days,
                secret=code.lower(),
                date=date)
            return code

                
def account_by_payingid(paying_id):
    s = sa.select([sa.distinct(gold_table.c.account_id)],
                  gold_table.c.paying_id == paying_id)
    res = s.execute().fetchall()

    if len(res) != 1:
        return None

    return int(res[0][0])

# returns None if the ID was never valid
# returns "already claimed" if it's already been claimed
# Otherwise, it's valid and the function claims it, returning a tuple with:
#   * the number of days
#   * the subscr_id, if any
def claim_gold(secret, account_id):
    if not secret:
        return None

    # The donation email has the code at the end of the sentence,
    # so they might get sloppy and catch the period or some whitespace.
    secret = secret.strip(". ")
    secret = secret.replace("-", "").lower()

    rp = gold_table.update(sa.and_(gold_table.c.status == 'unclaimed',
                                   gold_table.c.secret == secret),
                           values = {
                                      gold_table.c.status: 'claimed',
                                      gold_table.c.account_id: account_id,
                                    },
                           ).execute()
    if rp.rowcount == 0:
        just_claimed = False
    elif rp.rowcount == 1:
        just_claimed = True
    else:
        raise ValueError("rowcount == %d?" % rp.rowcount)

    s = sa.select([gold_table.c.days, gold_table.c.subscr_id],
                  gold_table.c.secret == secret,
                  limit = 1)
    rows = s.execute().fetchall()

    if not rows:
        return None
    elif just_claimed:
        return (rows[0].days, rows[0].subscr_id)
    else:
        return "already claimed"

def check_by_email(email):
    s = sa.select([gold_table.c.status,
                           gold_table.c.secret,
                           gold_table.c.days,
                           gold_table.c.account_id],
                          gold_table.c.payer_email == email)
    return s.execute().fetchall()


def retrieve_gold_transaction(transaction_id):
    s = sa.select([gold_table], gold_table.c.trans_id == transaction_id)
    res = s.execute().fetchall()
    if res:
        return res[0]   # single row per transaction_id


def update_gold_transaction(transaction_id, status):
    rp = gold_table.update(gold_table.c.trans_id == str(transaction_id),
                           values={gold_table.c.status: status}).execute()


def transactions_by_user(user):
    s = sa.select([gold_table], gold_table.c.account_id == str(user._id))
    res = s.execute().fetchall()
    return res


def gold_payments_by_user(user):
    transactions = transactions_by_user(user)

    # filter out received gifts
    transactions = [trans for trans in transactions
                          if not trans.trans_id.startswith(('X', 'M'))]

    return transactions


def gold_received_by_user(user):
    transactions = transactions_by_user(user)
    transactions = [trans for trans in transactions
                          if trans.trans_id.startswith('X')]
    return transactions


def days_to_pennies(days):
    if days < 366:
        months = days / 31
        return months * g.gold_month_price.pennies
    else:
        years = days / 366
        return years * g.gold_year_price.pennies


def append_random_bottlecap_phrase(message):
    """Appends a random "bottlecap" phrase from the wiki page.

    The wiki page should be an unordered list with each item a separate
    bottlecap.
    """

    bottlecap = None
    try:
        wp = WikiPage.get(Frontpage, g.wiki_page_gold_bottlecaps)

        split_list = re.split('^[*-] ', wp.content, flags=re.MULTILINE)
        choices = [item.strip() for item in split_list if item.strip()]
        if len(choices):
            bottlecap = choice(choices)
    except NotFound:
        pass

    if bottlecap:
        message += '\n\n> ' + bottlecap
    return message


def gold_revenue_multi(dates):
    NON_REVENUE_STATUSES = ("declined", "chargeback", "fudge")
    date_expr = sa.func.date_trunc('day',
                    sa.func.timezone(TIMEZONE.zone, gold_table.c.date))
    query = (select([date_expr, sa_sum(gold_table.c.pennies)])
                .where(~ gold_table.c.status.in_(NON_REVENUE_STATUSES))
                .where(date_expr.in_(dates))
                .group_by(date_expr)
            )
    return {truncated_time.date(): pennies
                for truncated_time, pennies in ENGINE.execute(query)}


@memoize("gold-revenue-volatile", time=600)
def gold_revenue_volatile(date):
    return gold_revenue_multi([date]).get(date, 0)


@memoize("gold-revenue-steady")
def gold_revenue_steady(date):
    return gold_revenue_multi([date]).get(date, 0)


@memoize("gold-goal")
def gold_goal_on(date):
    """Returns the gold revenue goal (in pennies) for a given date."""
    return GoldRevenueGoalByDate.get(date)


def account_from_stripe_customer_id(stripe_customer_id):
    q = Account._query(Account.c.gold_subscr_id == stripe_customer_id,
                       Account.c._spam == (True, False), data=True)
    return next(iter(q), None)


@memoize("subscription-details", time=60)
def _get_subscription_details(stripe_customer_id):
    stripe.api_key = g.secrets['stripe_secret_key']
    customer = stripe.Customer.retrieve(stripe_customer_id)

    if getattr(customer, 'deleted', False):
        return {}

    subscription = customer.subscription
    card = customer.active_card
    end = datetime.fromtimestamp(subscription.current_period_end).date()
    last4 = card.last4
    pennies = subscription.plan.amount

    return {
        'next_charge_date': end,
        'credit_card_last4': last4,
        'pennies': pennies,
    }


def get_subscription_details(user):
    if not getattr(user, 'gold_subscr_id', None):
        return

    return _get_subscription_details(user.gold_subscr_id)


def get_discounted_price(gold_price):
    discount = float(getattr(g, 'BTC_DISCOUNT', '0'))
    price = (gold_price.pennies * (1 - discount)) / 100.
    return GoldPrice("%.2f" % price)


def make_gold_message(thing, user_gilded):
    from r2.models import Comment

    if thing.gildings == 0 or thing._spam or thing._deleted:
        return None

    author = Account._byID(thing.author_id, data=True)
    if not author._deleted:
        author_name = author.name
    else:
        author_name = _("[deleted]")

    if c.user_is_loggedin and thing.author_id == c.user._id:
        if isinstance(thing, Comment):
            gilded_message = ungettext(
                "a redditor gifted you a month of reddit gold for this "
                "comment.",
                "redditors have gifted you %(months)d months of reddit gold "
                "for this comment.",
                thing.gildings
            )
        else:
            gilded_message = ungettext(
                "a redditor gifted you a month of reddit gold for this "
                "submission.",
                "redditors have gifted you %(months)d months of reddit gold "
                "for this submission.",
                thing.gildings
            )
    elif user_gilded:
        if isinstance(thing, Comment):
            gilded_message = ungettext(
                "you have gifted reddit gold to %(recipient)s for this "
                "comment.",
                "you and other redditors have gifted %(months)d months of "
                "reddit gold to %(recipient)s for this comment.",
                thing.gildings
            )
        else:
            gilded_message = ungettext(
                "you have gifted reddit gold to %(recipient)s for this "
                "submission.",
                "you and other redditors have gifted %(months)d months of "
                "reddit gold to %(recipient)s for this submission.",
                thing.gildings
            )
    else:
        if isinstance(thing, Comment):
            gilded_message = ungettext(
                "a redditor has gifted reddit gold to %(recipient)s for this "
                "comment.",
                "redditors have gifted %(months)d months of reddit gold to "
                "%(recipient)s for this comment.",
                thing.gildings
            )
        else:
            gilded_message = ungettext(
                "a redditor has gifted reddit gold to %(recipient)s for this "
                "submission.",
                "redditors have gifted %(months)d months of reddit gold to "
                "%(recipient)s for this submission.",
                thing.gildings
            )

    return gilded_message % dict(
        recipient=author_name,
        months=thing.gildings,
    )

########NEW FILE########
__FILENAME__ = keyvalue
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import json

from pycassa import NotFoundException
from pycassa.system_manager import UTF8_TYPE

from r2.lib.db.tdb_cassandra import ThingMeta

NoDefault = object()

class KeyValueStore(object):
    __metaclass__ = ThingMeta

    _use_db = False
    _cf_name = None
    _type_prefix = None
    _compare_with = UTF8_TYPE

    _extra_schema_creation_args = dict(
        key_validation_class=UTF8_TYPE,
        default_validation_class=UTF8_TYPE,
    )

    @classmethod
    def get(cls, key, default=NoDefault):
        try:
            return json.loads(cls._cf.get(key)["data"])
        except NotFoundException:
            if default is not NoDefault:
                return default
            else:
                raise

    @classmethod
    def set(cls, key, data):
        cls._cf.insert(key, {"data": json.dumps(data)})


class NamedGlobals(KeyValueStore):
    _use_db = True


########NEW FILE########
__FILENAME__ = last_modified
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import datetime

from pylons import g
from pycassa.system_manager import ASCII_TYPE, DATE_TYPE

from r2.lib.db import tdb_cassandra
from r2.lib.utils import tup


class LastModified(tdb_cassandra.View):
    _use_db = True
    _value_type = "date"
    _connection_pool = "main"
    _read_consistency_level = tdb_cassandra.CL.ONE
    _extra_schema_creation_args = dict(key_validation_class=ASCII_TYPE,
                                       default_validation_class=DATE_TYPE)

    @classmethod
    def touch(cls, fullname, names):
        names = tup(names)
        now = datetime.datetime.now(g.tz)
        values = dict.fromkeys(names, now)
        cls._set_values(fullname, values)
        return now

    @classmethod
    def get(cls, fullname, name, touch_if_not_set=False):
        try:
            obj = cls._byID(fullname)
        except tdb_cassandra.NotFound:
            if touch_if_not_set:
                time = cls.touch(fullname, name)
                return time
            else:
                return None

        return getattr(obj, name, None)

    @classmethod
    def get_multi(cls, fullnames, name):
        res = cls._byID(fullnames, return_dict=True)

        return dict((k, getattr(v, name, None))
                    for k, v in res.iteritems())

########NEW FILE########
__FILENAME__ = link
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.db.thing import (
    Thing, Relation, NotFound, MultiRelation, CreationError)
from r2.lib.db.operators import desc
from r2.lib.utils import (
    base_url,
    domain,
    strip_www,
    timesince,
    title_to_url,
    tup,
    UrlParser,
)
from account import Account, DeletedUser
from subreddit import DefaultSR, DomainSR, Subreddit
from printable import Printable
from r2.config import extensions
from r2.lib.memoize import memoize
from r2.lib.filters import _force_utf8, _force_unicode
from r2.lib import hooks, utils
from r2.lib.log import log_text
from mako.filters import url_escape
from r2.lib.strings import strings, Score
from r2.lib.db import tdb_cassandra
from r2.lib.db.tdb_cassandra import NotFoundException, view_of
from r2.lib.utils import sanitize_url
from r2.models.gold import (
    GildedCommentsByAccount,
    GildedLinksByAccount,
    make_gold_message,
)
from r2.models.subreddit import MultiReddit
from r2.models.query_cache import CachedQueryMutator
from r2.models.promo import PROMOTE_STATUS, get_promote_srid

from pylons import c, g, request
from pylons.i18n import _
from datetime import datetime, timedelta
from hashlib import md5

import random, re

class LinkExists(Exception): pass

# defining types
class Link(Thing, Printable):
    _data_int_props = Thing._data_int_props + (
        'num_comments', 'reported', 'comment_tree_id', 'gildings')
    _defaults = dict(is_self=False,
                     over_18=False,
                     over_18_override=False,
                     nsfw_str=False,
                     reported=0, num_comments=0,
                     moderator_banned=False,
                     banned_before_moderator=False,
                     media_object=None,
                     secure_media_object=None,
                     promoted=None,
                     pending=False,
                     disable_comments=False,
                     selftext='',
                     sendreplies=True,
                     ip='0.0.0.0',
                     flair_text=None,
                     flair_css_class=None,
                     comment_tree_version=1,
                     comment_tree_id=0,
                     contest_mode=False,
                     skip_commentstree_q="",
                     ignore_reports=False,
                     gildings=0,
                     )
    _essentials = ('sr_id', 'author_id')
    _nsfw = re.compile(r"\bnsfw\b", re.I)

    def __init__(self, *a, **kw):
        Thing.__init__(self, *a, **kw)

    @property
    def has_thumbnail(self):
        return self._t.get('has_thumbnail', hasattr(self, 'thumbnail_url'))

    @classmethod
    def _by_url(cls, url, sr):
        from subreddit import FakeSubreddit
        if isinstance(sr, FakeSubreddit):
            sr = None

        try:
            lbu = LinksByUrl._byID(LinksByUrl._key_from_url(url))
        except tdb_cassandra.NotFound:
            # translate the tdb_cassandra.NotFound into the NotFound
            # the caller is expecting
            raise NotFound('Link "%s"' % url)

        link_id36s = lbu._values()

        links = Link._byID36(link_id36s, data=True, return_dict=False)
        links = [l for l in links if not l._deleted]
        if sr:
            links = [link for link in links if link.sr_id == sr._id]

        if links:
            return links

        raise NotFound('Link "%s"' % url)

    def set_url_cache(self):
        if self.url != 'self':
            LinksByUrl._set_values(LinksByUrl._key_from_url(self.url),
                                   {self._id36: ''})

    @property
    def already_submitted_link(self):
        return self.make_permalink_slow() + '?already_submitted=true'

    def resubmit_link(self, sr_url=False):
        submit_url = self.subreddit_slow.path if sr_url else '/'
        submit_url += 'submit?resubmit=true&url='
        submit_url += url_escape(_force_unicode(self.url))
        return submit_url

    @classmethod
    def _choose_comment_tree_version(cls):
        try:
            weights = g.live_config['comment_tree_version_weights']
        except KeyError:
            return cls._defaults['comment_tree_version']
        try:
            return int(utils.weighted_lottery(weights))
        except ValueError, ex:
            g.log.error("error choosing comment tree version: %s", ex.message)
            return cls._defaults['comment_tree_version']

    @classmethod
    def _submit(cls, title, url, author, sr, ip, spam=False, sendreplies=True):
        from r2.models import admintools

        l = cls(_ups=1,
                title=title,
                url=url,
                _spam=spam,
                author_id=author._id,
                sendreplies=sendreplies,
                sr_id=sr._id,
                lang=sr.lang,
                ip=ip,
                comment_tree_version=cls._choose_comment_tree_version())
        l._commit()
        l.set_url_cache()
        LinksByAccount.add_link(author, l)
        if author._spam:
            g.stats.simple_event('spam.autoremove.link')
            admintools.spam(l, banner='banned user')
        return l

    def _save(self, user, category=None):
        LinkSavesByAccount._save(user, self, category)

    def _unsave(self, user):
        LinkSavesByAccount._unsave(user, self)

    def _hide(self, user):
        LinkHidesByAccount._hide(user, self)

    def _unhide(self, user):
        LinkHidesByAccount._unhide(user, self)

    def link_domain(self):
        if self.is_self:
            return 'self'
        else:
            return domain(self.url)

    def keep_item(self, wrapped):
        user = c.user if c.user_is_loggedin else None

        if not (c.user_is_admin or (isinstance(c.site, DomainSR) and
                                    wrapped.subreddit.is_moderator(user))):
            if self._spam and (not user or
                               (user and self.author_id != user._id)):
                return False

            #author_karma = wrapped.author.link_karma
            #if author_karma <= 0 and random.randint(author_karma, 0) != 0:
                #return False

        if user and not c.ignore_hide_rules:
            if wrapped.hidden:
                return False

            # never automatically hide user's own posts or stickies
            allow_auto_hide = (not wrapped.stickied and
                               self.author_id != user._id)
            if (allow_auto_hide and
                    ((user.pref_hide_ups and wrapped.likes == True) or
                     (user.pref_hide_downs and wrapped.likes == False) or
                     wrapped._score < user.pref_min_link_score)):
                return False

        # Always show NSFW to API users unless obey_over18=true in querystring
        is_api = c.render_style in extensions.API_TYPES
        if is_api and not c.obey_over18:
            return True

        # hide NSFW links from non-logged users and under 18 logged users
        # if they're not explicitly visiting an NSFW subreddit or a multireddit
        if (((not c.user_is_loggedin and c.site != wrapped.subreddit)
            or (c.user_is_loggedin and not c.over18))
            and not (isinstance(c.site, MultiReddit) and c.over18)):
            is_nsfw = bool(wrapped.over_18)
            is_from_nsfw_sr = bool(wrapped.subreddit.over_18)

            if is_nsfw or is_from_nsfw_sr:
                return False

        return True

    # none of these things will change over a link's lifetime
    cache_ignore = set(['subreddit', 'num_comments', 'link_child']
                       ).union(Printable.cache_ignore)
    @staticmethod
    def wrapped_cache_key(wrapped, style):
        s = Printable.wrapped_cache_key(wrapped, style)
        if wrapped.promoted is not None:
            s.extend([getattr(wrapped, "promote_status", -1),
                      getattr(wrapped, "disable_comments", False),
                      getattr(wrapped, "media_override", False),
                      wrapped._date,
                      c.user_is_sponsor,
                      wrapped.url, repr(wrapped.title)])
        if style == "htmllite":
             s.extend([request.GET.has_key('twocolumn'),
                       c.link_target])
        elif style == "xml":
            s.append(request.GET.has_key("nothumbs"))
        elif style == "compact":
            s.append(c.permalink_page)
        s.append(getattr(wrapped, 'media_object', {}))
        s.append(wrapped.flair_text)
        s.append(wrapped.flair_css_class)
        s.append(wrapped.ignore_reports)

        # if browsing a single subreddit, incorporate link flair position
        # in the key so 'flair' buttons show up appropriately for mods
        if hasattr(c.site, '_id'):
            s.append(c.site.link_flair_position)

        return s

    def make_permalink(self, sr, force_domain=False):
        from r2.lib.template_helpers import get_domain
        p = "comments/%s/%s/" % (self._id36, title_to_url(self.title))
        # promoted links belong to a separate subreddit and shouldn't
        # include that in the path
        if self.promoted is not None:
            if force_domain:
                res = "http://%s/%s" % (get_domain(cname=False,
                                                   subreddit=False), p)
            else:
                res = "/%s" % p
        elif not c.cname and not force_domain:
            res = "/r/%s/%s" % (sr.name, p)
        elif sr != c.site or force_domain:
            if(c.cname and sr == c.site):
                res = "http://%s/%s" % (get_domain(cname=True,
                                                    subreddit=False), p)
            else:
                res = "http://%s/r/%s/%s" % (get_domain(cname=False,
                                                    subreddit=False), sr.name, p)
        else:
            res = "/%s" % p

        # WARNING: If we ever decide to add any ?foo=bar&blah parameters
        # here, Comment.make_permalink will need to be updated or else
        # it will fail.

        return res

    def make_permalink_slow(self, force_domain=False):
        return self.make_permalink(self.subreddit_slow,
                                   force_domain=force_domain)

    def markdown_link_slow(self):
        return "[%s](%s)" % (self.title.decode('utf-8'),
                             self.make_permalink_slow())

    def _gild(self, user):
        now = datetime.now(g.tz)

        self._incr("gildings")

        GildedLinksByAccount.gild(user, self)

        from r2.lib.db import queries
        with CachedQueryMutator() as m:
            gilding = utils.Storage(thing=self, date=now)
            m.insert(queries.get_all_gilded_links(), [gilding])
            m.insert(queries.get_gilded_links(self.sr_id), [gilding])
            m.insert(queries.get_gilded_user_links(self.author_id),
                     [gilding])
            m.insert(queries.get_user_gildings(user), [gilding])

        hooks.get_hook('link.gild').call(link=self, gilder=user)

    @staticmethod
    def _should_expunge_selftext(link):
        verdict = getattr(link, "verdict", "")
        if verdict not in ("admin-removed", "mod-removed"):
            return False
        if not c.user_is_loggedin:
            return True
        if c.user_is_admin:
            return False
        if c.user == link.author:
            return False
        if link.can_ban:
            return False
        return True

    @classmethod
    def add_props(cls, user, wrapped):
        from r2.lib.pages import make_link_child
        from r2.lib.count import incr_counts
        from r2.lib import media
        from r2.lib.utils import timeago
        from r2.lib.template_helpers import get_domain
        from r2.models.subreddit import FakeSubreddit
        from r2.lib.wrapped import CachedVariable

        # referencing c's getattr is cheap, but not as cheap when it
        # is in a loop that calls it 30 times on 25-200 things.
        user_is_admin = c.user_is_admin
        user_is_loggedin = c.user_is_loggedin
        pref_media = user.pref_media
        pref_frame = user.pref_frame
        cname = c.cname
        site = c.site

        saved = hidden = visited = {}

        if user_is_admin:
            # Checking if a domain's banned isn't even cheap
            urls = [item.url for item in wrapped if hasattr(item, 'url')]
            # bans_for_domain_parts is just a generator; convert to a set for
            # easy use of 'intersection'
            from r2.models.admintools import bans_for_domain_parts
            banned_domains = {ban.domain
                              for ban in bans_for_domain_parts(urls)}

        if user_is_loggedin:
            gilded = [thing for thing in wrapped if thing.gildings > 0]
            try:
                user_gildings = GildedLinksByAccount.fast_query(user, gilded)
            except tdb_cassandra.TRANSIENT_EXCEPTIONS as e:
                g.log.warning("Cassandra gilding lookup failed: %r", e)
                user_gildings = {}

            try:
                saved = LinkSavesByAccount.fast_query(user, wrapped)
                hidden = LinkHidesByAccount.fast_query(user, wrapped)

                if user.gold and user.pref_store_visits:
                    visited = LinkVisitsByAccount.fast_query(user, wrapped)

            except tdb_cassandra.TRANSIENT_EXCEPTIONS as e:
                # saved or hidden or may have been done properly, so go ahead
                # with what we do have
                g.log.warning("Cassandra save/hide/visited lookup failed: %r", e)

        for item in wrapped:
            show_media = False
            if not hasattr(item, "score_fmt"):
                item.score_fmt = Score.number_only
            if c.render_style == 'compact':
                item.score_fmt = Score.points
            item.pref_compress = user.pref_compress
            if user.pref_compress:
                item.extra_css_class = "compressed"
                item.score_fmt = Score.points
            elif pref_media == 'on' and not user.pref_compress:
                show_media = True
            elif pref_media == 'subreddit' and item.subreddit.show_media:
                show_media = True
            elif item.promoted and item.has_thumbnail:
                if user_is_loggedin and item.author_id == user._id:
                    show_media = True
                elif pref_media != 'off' and not user.pref_compress:
                    show_media = True

            item.nsfw_str = item._nsfw.findall(item.title)
            item.over_18 = bool(item.over_18 or item.subreddit.over_18 or
                                item.nsfw_str)
            item.nsfw = item.over_18 and user.pref_label_nsfw

            item.is_author = (user == item.author)

            item.thumbnail_sprited = False
            # always show a promo author their own thumbnail
            if item.promoted and (user_is_admin or item.is_author) and item.has_thumbnail:
                item.thumbnail = media.thumbnail_url(item)
            elif user.pref_no_profanity and item.over_18 and not c.site.over_18:
                if show_media:
                    item.thumbnail = "nsfw"
                    item.thumbnail_sprited = True
                else:
                    item.thumbnail = ""
            elif not show_media:
                item.thumbnail = ""
            elif (item._deleted or
                  item._spam and item._date < timeago("6 hours")):
                item.thumbnail = "default"
                item.thumbnail_sprited = True
            elif item.has_thumbnail:
                item.thumbnail = media.thumbnail_url(item)
            elif item.is_self:
                item.thumbnail = "self"
                item.thumbnail_sprited = True
            else:
                item.thumbnail = "default"
                item.thumbnail_sprited = True

            item.score = max(0, item.score)

            if getattr(item, "domain_override", None):
                item.domain = item.domain_override
            else:
                item.domain = (domain(item.url) if not item.is_self
                               else 'self.' + item.subreddit.name)
            item.urlprefix = ''

            if user_is_loggedin:
                item.user_gilded = (user, item) in user_gildings
                item.saved = (user, item) in saved
                item.hidden = (user, item) in hidden
                item.visited = (user, item) in visited

            else:
                item.user_gilded = False
                item.saved = item.hidden = item.visited = False

            item.gilded_message = make_gold_message(item, item.user_gilded)
            item.can_gild = (
                c.user_is_loggedin and
                # you can't gild your own submission
                not (item.author and
                     item.author._id == user._id) and
                # no point in showing the button for things you've already gilded
                not item.user_gilded and
                # ick, if the author deleted their account we shouldn't waste gold
                not item.author._deleted and
                # some subreddits can have gilding disabled
                item.subreddit.allow_gilding
            )

            item.num = None
            item.permalink = item.make_permalink(item.subreddit)
            if item.is_self:
                item.url = item.make_permalink(item.subreddit,
                                               force_domain=True)

            if g.shortdomain:
                item.shortlink = g.shortdomain + '/' + item._id36

            item.domain_str = None
            if c.user.pref_domain_details:
                urlparser = UrlParser(item.url)
                if not item.is_self and urlparser.is_reddit_url():
                    url_subreddit = urlparser.get_subreddit()
                    if (url_subreddit and
                            not isinstance(url_subreddit, DefaultSR)):
                        item.domain_str = ('{0}/r/{1}'
                                           .format(item.domain,
                                                   url_subreddit.name))
                elif isinstance(item.media_object, dict):
                    try:
                        author_url = item.media_object['oembed']['author_url']
                        if domain(author_url) == item.domain:
                            urlparser = UrlParser(author_url)
                            item.domain_str = strip_www(urlparser.hostname)
                            item.domain_str += urlparser.path
                    except KeyError:
                        pass

                    if not item.domain_str:
                        try:
                            author = item.media_object['oembed']['author_name']
                            author = _force_unicode(author)
                            item.domain_str = (_force_unicode('{0}: {1}')
                                               .format(item.domain, author))
                        except KeyError:
                            pass

            if not item.domain_str:
                item.domain_str = item.domain

            # do we hide the score?
            if user_is_admin:
                item.hide_score = False
            elif item.promoted and item.score <= 0:
                item.hide_score = True
            elif user == item.author:
                item.hide_score = False
            elif item._date > timeago("2 hours"):
                item.hide_score = True
            else:
                item.hide_score = False

            # store user preferences locally for caching
            item.pref_frame = pref_frame
            # is this link a member of a different (non-c.site) subreddit?
            item.different_sr = (isinstance(site, FakeSubreddit) or
                                 site.name != item.subreddit.name)
            item.stickied = (not item.different_sr and
                             site.sticky_fullname == item._fullname)

            if user_is_loggedin and item.author_id == user._id:
                item.nofollow = False
            elif item.score <= 1 or item._spam or item.author._spam:
                item.nofollow = True
            else:
                item.nofollow = False

            item.subreddit_path = item.subreddit.path
            if cname:
                item.subreddit_path = ("http://" +
                     get_domain(cname=(site == item.subreddit),
                                subreddit=False))
                if site != item.subreddit:
                    item.subreddit_path += item.subreddit.path
            item.domain_path = "/domain/%s/" % item.domain
            if item.is_self:
                item.domain_path = item.subreddit_path

            # attach video or selftext as needed
            item.link_child, item.editable = make_link_child(item)

            item.tblink = "http://%s/tb/%s" % (
                get_domain(cname=cname, subreddit=False),
                item._id36)

            if item.is_self and not item.promoted:
                item.href_url = item.permalink
            else:
                item.href_url = item.url

            # show the toolbar if the preference is set and the link
            # is neither a promoted link nor a self post
            if pref_frame and not item.is_self and not item.promoted:
                item.mousedown_url = item.tblink
            else:
                item.mousedown_url = None

            item.fresh = not any((item.likes != None,
                                  item.saved,
                                  item.visited,
                                  item.hidden,
                                  item._deleted,
                                  item._spam))

            # bits that we will render stubs (to make the cached
            # version more flexible)
            item.num = CachedVariable("num")
            item.commentcls = CachedVariable("commentcls")
            item.comment_label = CachedVariable("numcomments")
            item.lastedited = CachedVariable("lastedited")

            item.as_deleted = False
            if item.deleted and not c.user_is_admin:
                item.author = DeletedUser()
                item.as_deleted = True

            item.votable = item._age < item.subreddit.archive_age

            item.expunged = False
            if item.is_self:
                item.expunged = Link._should_expunge_selftext(item)

            item.editted = getattr(item, "editted", False)

            taglinetext = ''
            if item.different_sr:
                author_text = (" <span>" + _("by %(author)s to %(reddit)s") +
                               "</span>")
            else:
                author_text = " <span>" + _("by %(author)s") + "</span>"
            if item.editted:
                if item.score_fmt == Score.points:
                    taglinetext = ("<span>" +
                                   _("%(score)s submitted %(when)s "
                                     "%(lastedited)s") +
                                   "</span>")
                    taglinetext += author_text
                elif item.different_sr:
                    taglinetext = _("submitted %(when)s %(lastedited)s "
                                    "by %(author)s to %(reddit)s")
                else:
                    taglinetext = _("submitted %(when)s %(lastedited)s "
                                    "by %(author)s")
            else:
                if item.score_fmt == Score.points:
                    taglinetext = ("<span>" +
                                   _("%(score)s submitted %(when)s") +
                                   "</span>")
                    taglinetext += author_text
                elif item.different_sr:
                    taglinetext = _("submitted %(when)s by %(author)s "
                                    "to %(reddit)s")
                else:
                    taglinetext = _("submitted %(when)s by %(author)s")
            item.taglinetext = taglinetext

            if item.is_author:
                item.should_incr_counts = False

            if user_is_admin:
                # Link notes
                url = getattr(item, 'url')
                # Pull just the relevant portions out of the url
                urlf = sanitize_url(_force_unicode(url))
                if urlf:
                    urlp = UrlParser(urlf)
                    hostname = urlp.hostname
                    if hostname:
                        parts = (hostname.encode("utf-8").rstrip(".").
                            split("."))
                        subparts = {".".join(parts[y:])
                                    for y in xrange(len(parts))}
                        if subparts.intersection(banned_domains):
                            item.link_notes.append('banned domain')

        if user_is_loggedin:
            incr_counts(wrapped)

        # Run this last
        Printable.add_props(user, wrapped)

    @property
    def subreddit_slow(self):
        """Returns the link's subreddit."""
        # The subreddit is often already on the wrapped link as .subreddit
        # If available, that should be used instead of calling this
        return Subreddit._byID(self.sr_id, data=True, return_dict=False)

    @property
    def author_slow(self):
        """Returns the link's author."""
        # The author is often already on the wrapped link as .author
        # If available, that should be used instead of calling this
        return Account._byID(self.author_id, data=True, return_dict=False)

    def can_flair_slow(self, user):
        """Returns whether the specified user can flair this link"""
        site = self.subreddit_slow
        can_assign_own = (self.author_id == user._id and
                          site.link_flair_self_assign_enabled)

        return site.is_moderator_with_perms(user, 'flair') or can_assign_own

    @classmethod
    def _utf8_encode(cls, value):
        """
        Returns a deep copy of the parameter, UTF-8-encoding any strings
        encountered.
        """
        if isinstance(value, dict):
            return {cls._utf8_encode(key): cls._utf8_encode(value)
                    for key, value in value.iteritems()}
        elif isinstance(value, list):
            return [cls._utf8_encode(item)
                    for item in value]
        elif isinstance(value, unicode):
            return value.encode('utf-8')
        else:
            return value

    # There's an issue where pickling fails for collections with string values
    # that have unicode codepoints between 128 and 256.  Encoding the strings
    # as UTF-8 before storing them works around this.
    def set_media_object(self, value):
        self.media_object = Link._utf8_encode(value)

    def set_secure_media_object(self, value):
        self.secure_media_object = Link._utf8_encode(value)


class LinksByUrl(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _read_consistency_level = tdb_cassandra.CL.ONE

    @classmethod
    def _key_from_url(cls, url):
        if not utils.domain(url) in g.case_sensitive_domains:
            keyurl = _force_utf8(UrlParser.base_url(url.lower()))
        else:
            # Convert only hostname to lowercase
            up = UrlParser(url)
            up.hostname = up.hostname.lower()
            keyurl = _force_utf8(UrlParser.base_url(up.unparse()))
        return keyurl

# Note that there are no instances of PromotedLink or LinkCompressed,
# so overriding their methods here will not change their behaviour
# (except for add_props). These classes are used to override the
# render_class on a Wrapped to change the template used for rendering

class PromotedLink(Link):
    _nodb = True

    @classmethod
    def add_props(cls, user, wrapped):
        Link.add_props(user, wrapped)
        user_is_sponsor = c.user_is_sponsor

        status_dict = dict((v, k) for k, v in PROMOTE_STATUS.iteritems())
        for item in wrapped:
            # these are potentially paid for placement
            item.nofollow = True
            item.user_is_sponsor = user_is_sponsor
            status = getattr(item, "promote_status", -1)
            if item.is_author or c.user_is_sponsor:
                item.rowstyle = "link " + PROMOTE_STATUS.name[status].lower()
            else:
                item.rowstyle = "link promoted"
        # Run this last
        Printable.add_props(user, wrapped)


class Comment(Thing, Printable):
    _data_int_props = Thing._data_int_props + ('reported', 'gildings')
    _defaults = dict(reported=0,
                     parent_id=None,
                     moderator_banned=False,
                     new=False,
                     gildings=0,
                     banned_before_moderator=False,
                     parents=None,
                     ignore_reports=False,
                     )
    _essentials = ('link_id', 'author_id')

    def _markdown(self):
        pass

    @classmethod
    def _new(cls, author, link, parent, body, ip):
        from r2.lib.db.queries import changed

        kw = {}
        if link.comment_tree_version > 1:
            # for top-level comments, parents is an empty string
            # for all others, it looks like "<id36>:<id36>:...".
            if parent:
                if parent.parent_id:
                    if parent.parents is None:
                        parent._fill_in_parents()
                    kw['parents'] = parent.parents + ':' + parent._id36
                else:
                    kw['parents'] = parent._id36

        c = Comment(_ups=1,
                    body=body,
                    link_id=link._id,
                    sr_id=link.sr_id,
                    author_id=author._id,
                    ip=ip,
                    **kw)

        # whitelist promoters commenting on their own promoted links
        from r2.lib import promote
        if promote.is_promo(link) and link.author_id == author._id:
            c._spam = False
        else:
            c._spam = author._spam

        if author._spam:
            g.stats.simple_event('spam.autoremove.comment')

        #these props aren't relations
        if parent:
            c.parent_id = parent._id

        link._incr('num_comments', 1)

        to = None
        name = 'inbox'
        if parent:
            to = Account._byID(parent.author_id, True)
        elif link.sendreplies:
            to = Account._byID(link.author_id, True)
            name = 'selfreply'

        c._commit()

        changed(link, True)  # link's number of comments changed

        CommentsByAccount.add_comment(author, c)

        inbox_rel = None
        # only global admins can be message spammed.
        # Don't send the message if the recipient has blocked
        # the author
        if to and ((not c._spam and author._id not in to.enemies)
            or to.name in g.admins):
            # When replying to your own comment, record the inbox
            # relation, but don't give yourself an orangered
            orangered = (to.name != author.name)
            inbox_rel = Inbox._add(to, c, name, orangered=orangered)

        hooks.get_hook('comment.new').call(comment=c)

        return (c, inbox_rel)

    def _save(self, user, category=None):
        CommentSavesByAccount._save(user, self, category)

    def _unsave(self, user):
        CommentSavesByAccount._unsave(user, self)

    @property
    def subreddit_slow(self):
        from subreddit import Subreddit
        """return's a comments's subreddit. in most case the subreddit is already
        on the wrapped link (as .subreddit), and that should be used
        when possible. if sr_id does not exist, then use the parent link's"""
        self._safe_load()

        if hasattr(self, 'sr_id'):
            sr_id = self.sr_id
        else:
            l = Link._byID(self.link_id, True)
            sr_id = l.sr_id
        return Subreddit._byID(sr_id, True, return_dict=False)

    @property
    def author_slow(self):
        """Returns the comment's author."""
        # The author is often already on the wrapped comment as .author
        # If available, that should be used instead of calling this
        return Account._byID(self.author_id, data=True, return_dict=False)

    def keep_item(self, wrapped):
        return True

    cache_ignore = set(["subreddit", "link", "to"]
                       ).union(Printable.cache_ignore)
    @staticmethod
    def wrapped_cache_key(wrapped, style):
        s = Printable.wrapped_cache_key(wrapped, style)
        s.extend([wrapped.body])
        s.extend([hasattr(wrapped, "link") and wrapped.link.contest_mode])
        return s

    def make_permalink(self, link, sr=None, context=None, anchor=False):
        url = link.make_permalink(sr) + self._id36
        if context:
            url += "?context=%d" % context
        if anchor:
            url += "#%s" % self._id36
        return url

    def make_permalink_slow(self, context=None, anchor=False):
        l = Link._byID(self.link_id, data=True)
        return self.make_permalink(l, l.subreddit_slow,
                                   context=context, anchor=anchor)

    def _gild(self, user):
        now = datetime.now(g.tz)

        self._incr("gildings")

        GildedCommentsByAccount.gild(user, self)

        from r2.lib.db import queries
        with CachedQueryMutator() as m:
            gilding = utils.Storage(thing=self, date=now)
            m.insert(queries.get_all_gilded_comments(), [gilding])
            m.insert(queries.get_gilded_comments(self.sr_id), [gilding])
            m.insert(queries.get_gilded_user_comments(self.author_id),
                     [gilding])
            m.insert(queries.get_user_gildings(user), [gilding])

        hooks.get_hook('comment.gild').call(comment=self, gilder=user)

    def _fill_in_parents(self):
        if not self.parent_id:
            self.parents = ''
            self._commit()
            return
        parent = Comment._byID(self.parent_id)
        if parent.parent_id:
            if parent.parents is None:
                parent._fill_in_parents()
            self.parents = parent.parents + ':' + parent._id36
        else:
            self.parents = parent._id36
        self._commit()

    def parent_path(self):
        """Returns path of comment in tree as list of comment ids.

        The returned list will always begin with -1, followed by comment ids in
        path order. The return value for top-level comments will always be [-1].
        """
        if self.parent_id and self.parents is None:
            self._fill_in_parents()

        if self.parents is None:
            return [-1]

        # eliminate any leading colons from the path and parse
        pids = [long(pid_str, 36) if pid_str else -1
                for pid_str in self.parents.lstrip(':').split(':')]

        # ensure path starts with -1
        if pids[0] != -1:
            pids.insert(0, -1)

        return pids

    @classmethod
    def add_props(cls, user, wrapped):
        from r2.lib.template_helpers import add_attr, get_domain
        from r2.lib.utils import timeago
        from r2.lib.wrapped import CachedVariable
        from r2.lib.pages import WrappedUser

        #fetch parent links
        links = Link._byID(set(l.link_id for l in wrapped), data=True,
                           return_dict=True, stale=True)

        # fetch authors
        authors = Account._byID(set(l.author_id for l in links.values()), data=True,
                                return_dict=True, stale=True)

        #get srs for comments that don't have them (old comments)
        for cm in wrapped:
            if not hasattr(cm, 'sr_id'):
                cm.sr_id = links[cm.link_id].sr_id

        subreddits = Subreddit._byID(set(cm.sr_id for cm in wrapped),
                                     data=True, return_dict=False, stale=True)
        cids = dict((w._id, w) for w in wrapped)
        parent_ids = set(cm.parent_id for cm in wrapped
                         if getattr(cm, 'parent_id', None)
                         and cm.parent_id not in cids)
        parents = {}
        if parent_ids:
            parents = Comment._byID(parent_ids, data=True, stale=True)

        can_reply_srs = set(s._id for s in subreddits if s.can_comment(user)) \
                        if c.user_is_loggedin else set()
        can_reply_srs.add(get_promote_srid())

        min_score = user.pref_min_comment_score

        profilepage = c.profilepage
        user_is_admin = c.user_is_admin
        user_is_loggedin = c.user_is_loggedin
        focal_comment = c.focal_comment
        cname = c.cname
        site = c.site

        if user_is_loggedin:
            gilded = [thing for thing in wrapped if thing.gildings > 0]
            try:
                user_gildings = GildedCommentsByAccount.fast_query(user, gilded)
            except tdb_cassandra.TRANSIENT_EXCEPTIONS as e:
                g.log.warning("Cassandra gilding lookup failed: %r", e)
                user_gildings = {}

            try:
                saved = CommentSavesByAccount.fast_query(user, wrapped)
            except tdb_cassandra.TRANSIENT_EXCEPTIONS as e:
                g.log.warning("Cassandra comment save lookup failed: %r", e)
                saved = {}
        else:
            user_gildings = {}
            saved = {}

        for item in wrapped:
            # for caching:
            item.profilepage = c.profilepage
            item.link = links.get(item.link_id)

            if (item.link._score <= 1 or item.score < 3 or
                item.link._spam or item._spam or item.author._spam):
                item.nofollow = True
            else:
                item.nofollow = False

            if not hasattr(item, 'subreddit'):
                item.subreddit = item.subreddit_slow
            if item.author_id == item.link.author_id and not item.link._deleted:
                add_attr(item.attribs, 'S',
                         link=item.link.make_permalink(item.subreddit))
            if not hasattr(item, 'target'):
                item.target = "_top" if cname else None
            if item.parent_id:
                if item.parent_id in parents:
                    parent = parents[item.parent_id]
                else:
                    parent = cids[item.parent_id]
                if not parent.deleted:
                    if item.parent_id in cids:
                        item.parent_permalink = '#' + utils.to36(item.parent_id)
                    else:
                        item.parent_permalink = parent.make_permalink(item.link, item.subreddit)
                else:
                    item.parent_permalink = None
            else:
                item.parent_permalink = None

            item.can_reply = False
            if c.can_reply or (item.sr_id in can_reply_srs):
                if item.link._age < item.subreddit.archive_age:
                    item.can_reply = True

            item.can_save = c.can_save or False

            if user_is_loggedin:
                item.user_gilded = (user, item) in user_gildings
                item.saved = (user, item) in saved
                item.can_save = True
            else:
                item.user_gilded = False
                item.saved = False
            item.gilded_message = make_gold_message(item, item.user_gilded)

            item.can_gild = (
                # this is a way of checking if the user is logged in that works
                # both within CommentPane instances and without.  e.g. CommentPane
                # explicitly sets user_is_loggedin = False but can_reply is
                # correct.  while on user overviews, you can't reply but will get
                # the correct value for user_is_loggedin
                (c.user_is_loggedin or getattr(item, "can_reply", True)) and
                # you can't gild your own comment
                not (c.user_is_loggedin and
                     item.author and
                     item.author._id == user._id) and
                # no point in showing the button for things you've already gilded
                not item.user_gilded and
                # ick, if the author deleted their account we shouldn't waste gold
                not item.author._deleted and
                # some subreddits can have gilding disabled
                item.subreddit.allow_gilding
            )

            # not deleted on profile pages,
            # deleted if spam and not author or admin
            item.deleted = (not profilepage and
                           (item._deleted or
                            (item._spam and
                             item.author != user and
                             not item.show_spam)))

            item.have_form = not item.deleted

            extra_css = ''
            if item.deleted:
                extra_css += "grayed"
                if not user_is_admin:
                    item.author = DeletedUser()
                    item.body = '[deleted]'
                    item.gildings = 0
                    item.distinguished = None

            if focal_comment == item._id36:
                extra_css += " border"

            if profilepage:
                link_author = authors[item.link.author_id]
                if ((item.link._deleted or link_author._deleted) and
                        not user_is_admin):
                    link_author = DeletedUser()
                item.link_author = WrappedUser(link_author)

            item.subreddit_path = item.subreddit.path
            if cname:
                item.subreddit_path = ("http://" +
                     get_domain(cname=(site == item.subreddit),
                                subreddit=False))
                if site != item.subreddit:
                    item.subreddit_path += item.subreddit.path

            item.full_comment_path = item.link.make_permalink(item.subreddit)
            item.full_comment_count = item.link.num_comments

            # don't collapse for admins, on profile pages, or if deleted
            item.collapsed = False
            if ((item.score < min_score) and not (profilepage or
                item.deleted or user_is_admin)):
                item.collapsed = True
                item.collapsed_reason = _("comment score below threshold")
            if user_is_loggedin and item.author_id in c.user.enemies:
                if "grayed" not in extra_css:
                    extra_css += " grayed"
                item.collapsed = True
                item.collapsed_reason = _("blocked user")

            item.editted = getattr(item, "editted", False)

            item.render_css_class = "comment %s" % CachedVariable("time_period")

            #will get updated in builder
            item.num_children = 0
            item.score_fmt = Score.points
            item.permalink = item.make_permalink(item.link, item.subreddit)

            item.is_author = (user == item.author)
            item.is_focal = (focal_comment == item._id36)

            item.votable = item._age < item.subreddit.archive_age

            hide_period = ('{0} minutes'
                          .format(item.subreddit.comment_score_hide_mins))

            if item.link.contest_mode:
                item.score_hidden = True
            elif item._date > timeago(hide_period):
                item.score_hidden = not item.is_author
            else:
                item.score_hidden = False

            if item.score_hidden and c.user_is_loggedin:
                if c.user_is_admin or item.subreddit.is_moderator(c.user):
                    item.score_hidden = False

            if item.score_hidden:
                item.upvotes = 1
                item.downvotes = 0
                item.score = 1
                item.voting_score = [1, 1, 1]
                item.render_css_class += " score-hidden"

            #will seem less horrible when add_props is in pages.py
            from r2.lib.pages import UserText
            item.usertext = UserText(item, item.body,
                                     editable=item.is_author,
                                     nofollow=item.nofollow,
                                     target=item.target,
                                     extra_css=extra_css,
                                     have_form=item.have_form)

            item.lastedited = CachedVariable("lastedited")

        # Run this last
        Printable.add_props(user, wrapped)

class CommentSortsCache(tdb_cassandra.View):
    """A cache of the sort-values of comments to avoid looking up all
       of the comments in a big tree at render-time just to determine
       the candidate order"""
    _use_db = True
    _value_type = 'float'
    _connection_pool = 'main'
    _read_consistency_level = tdb_cassandra.CL.ONE
    _fetch_all_columns = True

class StarkComment(Comment):
    """Render class for the comments in the top-comments display in
       the reddit toolbar"""
    _nodb = True

class MoreMessages(Printable):
    cachable = False
    display = ""
    new = False
    was_comment = False
    is_collapsed = True

    def __init__(self, parent, child):
        self.parent = parent
        self.child = child

    @staticmethod
    def wrapped_cache_key(item, style):
        return False

    @property
    def _fullname(self):
        return self.parent._fullname

    @property
    def _id36(self):
        return self.parent._id36

    @property
    def subject(self):
        return self.parent.subject

    @property
    def childlisting(self):
        return self.child

    @property
    def to(self):
        return self.parent.to

    @property
    def author(self):
        return self.parent.author

    @property
    def recipient(self):
        return self.parent.recipient

    @property
    def sr_id(self):
        return self.parent.sr_id

    @property
    def subreddit(self):
        return self.parent.subreddit


class MoreComments(Printable):
    cachable = False
    display = ""

    @staticmethod
    def wrapped_cache_key(item, style):
        return False

    def __init__(self, link, depth, parent_id=None):
        from r2.lib.wrapped import CachedVariable

        if parent_id is not None:
            id36 = utils.to36(parent_id)
            self.parent_id = parent_id
            self.parent_name = "t%s_%s" % (utils.to36(Comment._type_id), id36)
            self.parent_permalink = link.make_permalink_slow() + id36
        self.link_name = link._fullname
        self.link_id = link._id
        self.depth = depth
        self.children = []
        self.count = 0
        self.previous_visits_hex = CachedVariable("previous_visits_hex")

    @property
    def _fullname(self):
        return "t%s_%s" % (utils.to36(Comment._type_id), self._id36)

    @property
    def _id36(self):
        return utils.to36(self.children[0]) if self.children else '_'


class MoreRecursion(MoreComments):
    pass

class MoreChildren(MoreComments):
    pass

class Message(Thing, Printable):
    _defaults = dict(reported=0,
                     was_comment=False,
                     parent_id=None,
                     new=False,
                     first_message=None,
                     to_id=None,
                     sr_id=None,
                     to_collapse=None,
                     author_collapse=None,
                     from_sr=False)
    _data_int_props = Thing._data_int_props + ('reported',)
    _essentials = ('author_id',)
    cache_ignore = set(["to", "subreddit"]).union(Printable.cache_ignore)

    @classmethod
    def _new(cls, author, to, subject, body, ip, parent=None, sr=None,
             from_sr=False):
        m = Message(subject=subject, body=body, author_id=author._id, new=True,
                    ip=ip, from_sr=from_sr)
        m._spam = author._spam

        if author._spam:
            g.stats.simple_event('spam.autoremove.message')

        sr_id = None
        # check to see if the recipient is a subreddit and swap args accordingly
        if to and isinstance(to, Subreddit):
            if from_sr:
                raise CreationError("Cannot send from SR to SR")
            to_subreddit = True
            to, sr = None, to
        else:
            to_subreddit = False

        if sr:
            sr_id = sr._id
        if parent:
            m.parent_id = parent._id
            if parent.first_message:
                m.first_message = parent.first_message
            else:
                m.first_message = parent._id
            if parent.sr_id:
                sr_id = parent.sr_id

        if not to and not sr_id:
            raise CreationError("Message created with neither to nor sr_id")
        if from_sr and not sr_id:
            raise CreationError("Message sent from_sr without setting sr")

        m.to_id = to._id if to else None
        if sr_id is not None:
            m.sr_id = sr_id

        m._commit()

        MessagesByAccount.add_message(author, m)

        if sr_id and not sr:
            sr = Subreddit._byID(sr_id)

        inbox_rel = []
        if sr_id:
            # if there is a subreddit id, and it's either a reply or
            # an initial message to an SR, add to the moderator inbox
            # (i.e., don't do it for automated messages from the SR)
            if parent or to_subreddit and not from_sr:
                inbox_rel.append(ModeratorInbox._add(sr, m, 'inbox'))
            if sr.is_moderator(author):
                m.distinguished = 'yes'
                m._commit()

        if author.name in g.admins:
            m.distinguished = 'admin'
            m._commit()

        # if there is a "to" we may have to create an inbox relation as well
        # also, only global admins can be message spammed.
        if to and (not m._spam or to.name in g.admins):
            # if the current "to" is not a sr moderator,
            # they need to be notified
            if not sr_id or not sr.is_moderator(to):
                # Record the inbox relation, but don't give the user
                # an orangered, if they PM themselves.
                # Don't notify on PMs from blocked users, either
                orangered = (to.name != author.name and
                             author._id not in to.enemies)
                inbox_rel.append(Inbox._add(to, m, 'inbox',
                                            orangered=orangered))
            # find the message originator
            elif sr_id and m.first_message:
                first = Message._byID(m.first_message, True)
                orig = Account._byID(first.author_id, True)
                # Only inbox if the origin account does not have
                # modmail access.
                if (orig._id != author._id and
                        not sr.is_moderator_with_perms(orig, 'mail')):
                    inbox_rel.append(Inbox._add(orig, m, 'inbox'))
        return (m, inbox_rel)

    @property
    def permalink(self):
        return "/message/messages/%s" % self._id36

    def can_view_slow(self):
        if c.user_is_loggedin:
            # simple case from before:
            if (c.user_is_admin or
                c.user._id in (self.author_id, self.to_id)):
                return True
            elif self.sr_id:
                sr = Subreddit._byID(self.sr_id)
                is_moderator = sr.is_moderator_with_perms(c.user, 'mail')
                # moderators can view messages on subreddits they moderate
                if is_moderator:
                    return True
                elif self.first_message:
                    first = Message._byID(self.first_message, True)
                    return (first.author_id == c.user._id)


    @classmethod
    def add_props(cls, user, wrapped):
        from r2.lib.db import queries
        #TODO global-ish functions that shouldn't be here?
        #reset msgtime after this request
        msgtime = c.have_messages

        # make sure there is a sr_id set:
        for w in wrapped:
            if not hasattr(w, "sr_id"):
                w.sr_id = None

        # load the to fields if one exists
        to_ids = set(w.to_id for w in wrapped if w.to_id is not None)
        tos = Account._byID(to_ids, True) if to_ids else {}

        # load the subreddit field if one exists:
        sr_ids = set(w.sr_id for w in wrapped if w.sr_id is not None)
        m_subreddits = Subreddit._byID(sr_ids, data=True, return_dict=True)

        # load the links and their subreddits (if comment-as-message)
        links = Link._byID(set(l.link_id for l in wrapped if l.was_comment),
                           data=True,
                           return_dict=True)
        # subreddits of the links (for comment-as-message)
        l_subreddits = Subreddit._byID(set(l.sr_id for l in links.values()),
                                       data=True, return_dict=True)

        parents = Comment._byID(set(l.parent_id for l in wrapped
                                  if l.parent_id and l.was_comment),
                                data=True, return_dict=True)

        # load the unread list to determine message newness
        unread = set(queries.get_unread_inbox(user))

        msg_srs = set(m_subreddits[x.sr_id]
                      for x in wrapped if x.sr_id is not None
                      and isinstance(x.lookups[0], Message))
        # load the unread mod list for the same reason
        mod_unread = set(queries.get_unread_subreddit_messages_multi(msg_srs))

        for item in wrapped:
            item.to = tos.get(item.to_id)
            if item.sr_id:
                item.recipient = (item.author_id != c.user._id)
            else:
                item.recipient = (item.to_id == c.user._id)

            if item.author_id == c.user._id:
                item.new = False
            elif item._fullname in unread:
                item.new = True
                # wipe new messages if preferences say so, and this isn't a feed
                # and it is in the user's personal inbox
                if (item.new and c.user.pref_mark_messages_read
                    and c.extension not in ("rss", "xml", "api", "json")):
                    queries.set_unread(item.lookups[0],
                                       c.user, False)
            else:
                item.new = item._fullname in mod_unread

            item.score_fmt = Score.none

            item.message_style = ""
            # comment as message:
            if item.was_comment:
                link = links[item.link_id]
                sr = l_subreddits[link.sr_id]
                item.to_collapse = False
                item.author_collapse = False
                item.link_title = link.title
                item.permalink = item.lookups[0].make_permalink(link, sr=sr)
                item.link_permalink = link.make_permalink(sr)
                if item.parent_id:
                    parent = parents[item.parent_id]
                    item.parent = parent._fullname
                    item.parent_permalink = parent.make_permalink(link, sr)

                    if parent.author_id == c.user._id:
                        item.subject = _('comment reply')
                        item.message_style = "comment-reply"
                    else:
                        item.subject = _('username mention')
                        item.message_style = "mention"
                else:
                    if link.author_id == c.user._id:
                        item.subject = _('post reply')
                        item.message_style = "post-reply"
                    else:
                        item.subject = _('username mention')
                        item.message_style = "mention"
            elif item.sr_id is not None:
                item.subreddit = m_subreddits[item.sr_id]

            item.hide_author = False
            if getattr(item, "from_sr", False):
                if not (item.subreddit.is_moderator(c.user) or
                        c.user_is_admin):
                    item.author = item.subreddit
                    item.hide_author = True

            item.is_collapsed = None
            if not item.new:
                if item.recipient:
                    item.is_collapsed = item.to_collapse
                if item.author_id == c.user._id:
                    item.is_collapsed = item.author_collapse
                if c.user.pref_collapse_read_messages:
                    item.is_collapsed = (item.is_collapsed is not False)
            if item.author_id in c.user.enemies and not item.was_comment:
                item.is_collapsed = True
                if not c.user_is_admin:
                    item.subject = _('[message from blocked user]')
                    item.body = _('[unblock user to see this message]')
            taglinetext = ''
            if item.hide_author:
                taglinetext = _("subreddit message %(author)s sent %(when)s")
            elif item.author_id == c.user._id:
                taglinetext = _("to %(dest)s sent %(when)s")
            elif item.to_id == c.user._id or item.to_id is None:
                taglinetext = _("from %(author)s sent %(when)s")
            else:
                taglinetext = _("to %(dest)s from %(author)s sent %(when)s")
            item.taglinetext = taglinetext
            if item.to:
                if item.to._deleted:
                    item.dest = "[deleted]"
                else:
                    item.dest = item.to.name
            else:
                item.dest = ""
            if item.sr_id:
                if item.hide_author:
                    item.updated_author = _("via %(subreddit)s")
                else:
                    item.updated_author = _("%(author)s via %(subreddit)s")
            else:
                item.updated_author = ''


        # Run this last
        Printable.add_props(user, wrapped)

    @property
    def subreddit_slow(self):
        from subreddit import Subreddit
        if self.sr_id:
            return Subreddit._byID(self.sr_id)

    @property
    def author_slow(self):
        """Returns the message's author."""
        # The author is often already on the wrapped message as .author
        # If available, that should be used instead of calling this
        return Account._byID(self.author_id, data=True, return_dict=False)

    @staticmethod
    def wrapped_cache_key(wrapped, style):
        s = Printable.wrapped_cache_key(wrapped, style)
        s.extend([wrapped.new, wrapped.collapsed])
        return s

    def keep_item(self, wrapped):
        return True


class _SaveHideByAccount(tdb_cassandra.DenormalizedRelation):
    @classmethod
    def value_for(cls, thing1, thing2):
        return ''

    @classmethod
    def _cached_queries(cls, user, thing):
        return []

    @classmethod
    def _savehide(cls, user, things, **kw):
        things = tup(things)
        now = datetime.now(g.tz)
        with CachedQueryMutator() as m:
            for thing in things:
                # action_date is only used by the cached queries as the sort
                # value, we don't want to write it. Report.new(link) needs to
                # incr link.reported but will fail if the link is dirty.
                thing.__setattr__('action_date', now, make_dirty=False)
                for q in cls._cached_queries(user, thing, **kw):
                    m.insert(q, [thing])
        cls.create(user, things, **kw)

    @classmethod
    def destroy(cls, user, things, **kw):
        things = tup(things)
        cls._cf.remove(user._id36, (things._id36 for things in things))

        for view in cls._views:
            view.destroy(user, things, **kw)

    @classmethod
    def _unsavehide(cls, user, things, **kw):
        things = tup(things)
        with CachedQueryMutator() as m:
            for thing in things:
                for q in cls._cached_queries(user, thing, **kw):
                    m.delete(q, [thing])
        cls.destroy(user, things, **kw)


class _ThingSavesByAccount(_SaveHideByAccount):
    _read_consistency_level = tdb_cassandra.CL.QUORUM
    _write_consistency_level = tdb_cassandra.CL.QUORUM

    @classmethod
    def value_for(cls, thing1, thing2, category=None):
        return category or ''
    
    @classmethod
    def _remove_from_category_listings(cls, user, things, category):
        things = tup(things)
        oldcategories = cls.fast_query(user, things)
        changedthings = []
        for thing in things:
            oldcategory = oldcategories.get((user, thing)) or None
            if oldcategory != category:
                changedthings.append(thing)
        cls._unsavehide(user, changedthings, categories=oldcategories)

    @classmethod
    def _save(cls, user, things, category=None):
        category = category.lower() if category else None
        cls._remove_from_category_listings(user, things, category=category)
        cls._savehide(user, things, category=category)

    @classmethod
    def _unsave(cls, user, things):
        # Ensure we delete from existing category cached queries
        categories = cls.fast_query(user, tup(things))
        cls._unsavehide(user, things, categories=categories)

    @classmethod
    def _unsavehide(cls, user, things, categories=None):
        things = tup(things)
        with CachedQueryMutator() as m:
            for thing in things:
                category = categories.get((user, thing)) if categories else None
                for q in cls._cached_queries(user, thing, category=category):
                    m.delete(q, [thing])
        cls.destroy(user, things, categories=categories)

    @classmethod
    def _cached_queries_category(cls, user, thing,
                                 querycatfn, queryfn,
                                 category=None, only_category=False):
        from r2.lib.db import queries
        cached_queries = []
        if not only_category:
            cached_queries = [queryfn(user, 'none'), queryfn(user, thing.sr_id)]
        if category:
            cached_queries.append(querycatfn(user, 'none', category))
            cached_queries.append(querycatfn(user, thing.sr_id, category))
        return cached_queries

class LinkSavesByAccount(_ThingSavesByAccount):
    _use_db = True
    _last_modified_name = 'Save'
    _views = []

    @classmethod
    def _cached_queries(cls, user, thing, **kw):
        from r2.lib.db import queries
        return cls._cached_queries_category(
            user,
            thing,
            queries.get_categorized_saved_links,
            queries.get_saved_links,
            **kw)

class CommentSavesByAccount(_ThingSavesByAccount):
    _use_db = True
    _last_modified_name = 'CommentSave'
    _views = []

    @classmethod
    def _cached_queries(cls, user, thing, **kw):
        from r2.lib.db import queries
        return cls._cached_queries_category(
            user,
            thing,
            queries.get_categorized_saved_comments,
            queries.get_saved_comments,
            **kw)

class _ThingHidesByAccount(_SaveHideByAccount):
    @classmethod
    def _hide(cls, user, things):
        cls._savehide(user, things)

    @classmethod
    def _unhide(cls, user, things):
        cls._unsavehide(user, things)


class LinkHidesByAccount(_ThingHidesByAccount):
    _use_db = True
    _last_modified_name = 'Hide'
    _views = []

    @classmethod
    def _cached_queries(cls, user, thing):
        from r2.lib.db import queries
        return [queries.get_hidden_links(user)]

class LinkVisitsByAccount(_SaveHideByAccount):
    _use_db = True
    _last_modified_name = 'Visit'
    _views = []
    _ttl = timedelta(days=7)
    _write_consistency_level = tdb_cassandra.CL.ONE

    @classmethod
    def _visit(cls, user, things):
        cls._savehide(user, things)

    @classmethod
    def _unvisit(cls, user, things):
        cls._unsavehide(user, things)

class _ThingSavesBySubreddit(tdb_cassandra.View):
    @classmethod
    def _rowkey(cls, user, thing):
        return user._id36

    @classmethod
    def _column(cls, user, thing):
        return {utils.to36(thing.sr_id): ''}

    @classmethod
    def get_saved_values(cls, user):
        rowkey = cls._rowkey(user, None)
        try:
            columns = cls._cf.get(rowkey)
        except NotFoundException:
            return []

        return columns.keys()

    @classmethod
    def get_saved_subreddits(cls, user):
        sr_id36s = cls.get_saved_values(user)
        srs = Subreddit._byID36(sr_id36s, return_dict=False, data=True)
        return sorted([sr.name for sr in srs])

    @classmethod
    def create(cls, user, things, **kw):
        for thing in things:
            rowkey = cls._rowkey(user, thing)
            column = cls._column(user, thing)
            cls._set_values(rowkey, column)

    @classmethod
    def _check_empty(cls, user, sr_id):
        return False

    @classmethod
    def destroy(cls, user, things, **kw):
        # See if thing's sr is present anymore
        sr_ids = set([thing.sr_id for thing in things])
        for sr_id in set(sr_ids):
            if cls._check_empty(user, sr_id):
                cls._cf.remove(user._id36, [utils.to36(sr_id)])

class _ThingSavesByCategory(_ThingSavesBySubreddit):
    @classmethod
    def create(cls, user, things, category=None):
        if not category:
            return
        for thing in things:
            rowkey = cls._rowkey(user, thing)
            column = {category: None}
            cls._set_values(rowkey, column)

    @classmethod
    def _get_query_fn():
        raise NotImplementedError 

    @classmethod
    def _check_empty(cls, user, category):
        from r2.lib.db import queries
        q = cls._get_query_fn()(user, 'none', category)
        q.fetch()
        return not q.data

    @classmethod
    def get_saved_categories(cls, user):
        return cls.get_saved_values(user)

    @classmethod
    def destroy(cls, user, things, categories=None):
        if not categories:
            return
        for category in set(categories.values()):
            if not category or not cls._check_empty(user, category):
                continue
            cls._cf.remove(user._id36, [category])

@view_of(LinkSavesByAccount)
class LinkSavesByCategory(_ThingSavesByCategory):
    _use_db = True

    @classmethod
    def _get_query_fn(cls):
        from r2.lib.db import queries
        return queries.get_categorized_saved_links

@view_of(LinkSavesByAccount)
class LinkSavesBySubreddit(_ThingSavesBySubreddit):
    _use_db = True

    @classmethod
    def _check_empty(cls, user, sr_id):
        from r2.lib.db import queries
        q = queries.get_saved_links(user, sr_id)
        q.fetch()
        return not q.data


@view_of(CommentSavesByAccount)
class CommentSavesBySubreddit(_ThingSavesBySubreddit):
    _use_db = True

    @classmethod
    def _check_empty(cls, user, sr_id):
        from r2.lib.db import queries
        q = queries.get_saved_comments(user, sr_id)
        q.fetch()
        return not q.data

@view_of(CommentSavesByAccount)
class CommentSavesByCategory(_ThingSavesByCategory):
    _use_db = True

    @classmethod
    def _get_query_fn(cls):
        from r2.lib.db import queries
        return queries.get_categorized_saved_comments

class Inbox(MultiRelation('inbox',
                          Relation(Account, Comment),
                          Relation(Account, Message))):
    @classmethod
    def _add(cls, to, obj, *a, **kw):
        orangered = kw.pop("orangered", True)
        i = Inbox(to, obj, *a, **kw)
        i.new = True
        i._commit()

        if not to._loaded:
            to._load()

        #if there is not msgtime, or it's false, set it
        if orangered and (not hasattr(to, 'msgtime') or not to.msgtime):
            to.msgtime = obj._date
            to._commit()

        return i

    @classmethod
    def set_unread(cls, things, unread, to=None):
        things = tup(things)
        if len(set(type(x) for x in things)) != 1:
            raise TypeError('things must only be of a single type')
        thing_ids = [x._id for x in things]
        inbox_rel = cls.rel(Account, things[0].__class__)
        if to:
            inbox = inbox_rel._query(inbox_rel.c._thing2_id == thing_ids,
                                     inbox_rel.c._thing1_id == to._id,
                                     data=True)
        else:
            inbox = inbox_rel._query(inbox_rel.c._thing2_id == thing_ids,
                                     data=True)
        res = []
        for i in inbox:
            if getattr(i, "new", False) != unread:
                i.new = unread
                i._commit()
            res.append(i)
        return res


class ModeratorInbox(Relation(Subreddit, Message)):
    #TODO: shouldn't dupe this
    @classmethod
    def _add(cls, sr, obj, *a, **kw):
        i = ModeratorInbox(sr, obj, *a, **kw)
        i.new = True
        i._commit()

        if not sr._loaded:
            sr._load()

        mod_perms = sr.moderators_with_perms()
        mod_ids = set(mod_id for mod_id, perms in mod_perms.iteritems()
                      if perms.get('mail', False))
        moderators = Account._byID(mod_ids, data=True, return_dict=False)
        for m in moderators:
            if obj.author_id != m._id and not m.modmsgtime:
                m.modmsgtime = obj._date
                m._commit()

        return i

    @classmethod
    def set_unread(cls, things, unread):
        things = tup(things)
        thing_ids = [x._id for x in things]
        inbox = cls._query(cls.c._thing2_id == thing_ids, data=True)
        res = []
        for i in inbox:
            if getattr(i, "new", False) != unread:
                i.new = unread
                i._commit()
            res.append(i)
        return res

class CommentsByAccount(tdb_cassandra.DenormalizedRelation):
    _use_db = True
    _write_last_modified = False
    _views = []

    @classmethod
    def value_for(cls, thing1, thing2):
        return ''

    @classmethod
    def add_comment(cls, account, comment):
        cls.create(account, [comment])


class LinksByAccount(tdb_cassandra.DenormalizedRelation):
    _use_db = True
    _write_last_modified = False
    _views = []

    @classmethod
    def value_for(cls, thing1, thing2):
        return ''

    @classmethod
    def add_link(cls, account, link):
        cls.create(account, [link])


class MessagesByAccount(tdb_cassandra.DenormalizedRelation):
    _use_db = True
    _write_last_modified = False
    _views = []

    @classmethod
    def value_for(cls, thing1, thing2):
        return ''

    @classmethod
    def add_message(cls, account, message):
        cls.create(account, [message])

########NEW FILE########
__FILENAME__ = listing
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from account import *
from link import *
from vote import *
from report import *
from subreddit import DefaultSR, AllSR, Frontpage
from pylons import i18n, request, g
from pylons.i18n import _

from r2.lib.wrapped import Wrapped
from r2.lib import utils
from r2.lib.db import operators
from r2.lib.cache import sgm

from collections import namedtuple
from copy import deepcopy, copy

class Listing(object):
    # class used in Javascript to manage these objects
    _js_cls = "Listing"

    def __init__(self, builder, nextprev = True, next_link = True,
                 prev_link = True, **kw):
        self.builder = builder
        self.nextprev = nextprev
        self.next_link = True
        self.prev_link = True
        self.next = None
        self.prev = None
        self._max_num = 1

    @property
    def max_score(self):
        scores = [x.score for x in self.things if hasattr(x, 'score')]
        return max(scores) if scores else 0

    @property
    def max_num(self):
        return self._max_num

    def get_items(self, *a, **kw):
        """Wrapper around builder's get_items that caches the rendering."""
        from r2.lib.template_helpers import replace_render
        builder_items = self.builder.get_items(*a, **kw)
        for item in self.builder.item_iter(builder_items):
            # rewrite the render method
            if not hasattr(item, "render_replaced"):
                item.render = replace_render(self, item, item.render)
                item.render_replaced = True
        return builder_items

    def listing(self, next_suggestions=None):
        self.things, prev, next, bcount, acount = self.get_items()

        self.next_suggestions = next_suggestions
        self._max_num = max(acount, bcount)
        self.after = None
        self.before = None

        if self.nextprev and self.prev_link and prev and bcount > 1:
            p = request.GET.copy()
            p.update({'after':None, 'before':prev._fullname, 'count':bcount})
            self.before = prev._fullname
            self.prev = (request.path + utils.query_string(p))
            p_first = request.GET.copy()
            p_first.update({'after':None, 'before':None, 'count':None})
            self.first = (request.path + utils.query_string(p_first))
        if self.nextprev and self.next_link and next:
            p = request.GET.copy()
            p.update({'after':next._fullname, 'before':None, 'count':acount})
            self.after = next._fullname
            self.next = (request.path + utils.query_string(p))

        for count, thing in enumerate(self.things):
            thing.rowstyle = getattr(thing, 'rowstyle', "")
            thing.rowstyle += ' ' + ('even' if (count % 2) else 'odd')

        #TODO: need name for template -- must be better way
        return Wrapped(self)

    def __iter__(self):
        return iter(self.things)

class TableListing(Listing): pass

class ModActionListing(TableListing): pass

class WikiRevisionListing(TableListing): pass

class UserListing(TableListing):
    type = ''
    _class = ''
    title = ''
    form_title = ''
    destination = 'friend'
    has_add_form = True
    headers = None
    permissions_form = None

    def __init__(self,
                 builder,
                 show_jump_to=False,
                 show_not_found=False,
                 jump_to_value=None,
                 addable=True, **kw):
        self.addable = addable
        self.show_not_found = show_not_found
        self.show_jump_to = show_jump_to
        self.jump_to_value = jump_to_value
        TableListing.__init__(self, builder, **kw)

    @property
    def container_name(self):
        return c.site._fullname

class FriendListing(UserListing):
    type = 'friend'

    @property
    def _class(self):
        return '' if not c.user.gold else 'gold-accent rounded'

    @property
    def headers(self):
        if c.user.gold:
            return (_('user'), '', _('note'), _('friendship'), '')

    @property
    def form_title(self):
        return _('add a friend')

    @property
    def container_name(self):
        return c.user._fullname


class EnemyListing(UserListing):
    type = 'enemy'
    has_add_form = False

    @property
    def title(self):
        return _('blocked users')

    @property
    def container_name(self):
        return c.user._fullname

class BannedListing(UserListing):
    type = 'banned'

    @property
    def form_title(self):
        return _("ban users")

    @property
    def title(self):
        return _("users banned from"
                 " /r/%(subreddit)s") % dict(subreddit=c.site.name)

class WikiBannedListing(BannedListing):
    type = 'wikibanned'

    @property
    def form_title(self):
        return _("ban wiki contibutors")

    @property
    def title(self):
        return _("wiki contibutors banned from"
                 " /r/%(subreddit)s") % dict(subreddit=c.site.name)

class ContributorListing(UserListing):
    type = 'contributor'

    @property
    def title(self):
        return _("approved submitters for"
                 " /r/%(subreddit)s") % dict(subreddit=c.site.name)

    @property
    def form_title(self):
        return _("add approved submitter")

class WikiMayContributeListing(ContributorListing):
    type = 'wikicontributor'

    @property
    def title(self):
        return _("approved wiki contributors"
                 " for /r/%(subreddit)s") % dict(subreddit=c.site.name)

    @property
    def form_title(self):
        return _("add approved wiki contributor")

class InvitedModListing(UserListing):
    type = 'moderator_invite'
    form_title = _('invite moderator')
    remove_self_title = _('you are a moderator of this subreddit. %(action)s')

    @property
    def permissions_form(self):
        from r2.lib.permissions import ModeratorPermissionSet
        from r2.lib.pages import ModeratorPermissions
        return ModeratorPermissions(
            user=None,
            permissions_type=self.type,
            permissions=ModeratorPermissionSet(all=True),
            editable=True,
            embedded=True,
        )

    @property
    def title(self):
        return _("invited moderators for"
                 " %(subreddit)s") % dict(subreddit=c.site.name)

class ModListing(InvitedModListing):
    type = 'moderator'
    form_title = _('force add moderator')

    @property
    def has_add_form(self):
        return c.user_is_admin

    @property
    def can_remove_self(self):
        return c.user_is_loggedin and c.site.is_moderator(c.user)

    @property
    def has_invite(self):
        return c.user_is_loggedin and c.site.is_moderator_invite(c.user)

    @property
    def title(self):
        return _("moderators of /r/%(subreddit)s") % dict(subreddit=c.site.name)

class LinkListing(Listing):
    def __init__(self, *a, **kw):
        Listing.__init__(self, *a, **kw)

        self.show_nums = kw.get('show_nums', False)

    def listing(self, *args, **kwargs):
        wrapped = Listing.listing(self, *args, **kwargs)
        self.rank_width = len(str(self.max_num)) * 1.1
        self.midcol_width = max(len(str(self.max_score)), 2) + 1.1
        return wrapped


class NestedListing(Listing):
    def __init__(self, *a, **kw):
        Listing.__init__(self, *a, **kw)

        self.num = kw.get('num', g.num_comments)
        self.parent_name = kw.get('parent_name')

    def listing(self):
        ##TODO use the local builder with the render cache. this may
        ##require separating the builder's get_items and tree-building
        ##functionality
        wrapped_items = self.get_items()

        self.things = wrapped_items

        #make into a tree thing
        return Wrapped(self)

SpotlightTuple = namedtuple('SpotlightTuple',
                            ['link', 'is_promo', 'campaign', 'weight'])

class SpotlightListing(Listing):
    # class used in Javascript to manage these objects
    _js_cls = "OrganicListing"

    def __init__(self, *a, **kw):
        self.nextprev   = False
        self.show_nums  = True
        self._parent_max_num   = kw.get('max_num', 0)
        self._parent_max_score = kw.get('max_score', 0)
        self.interestbar = kw.get('interestbar')
        self.interestbar_prob = kw.get('interestbar_prob', 0.)
        self.show_promo = kw.get('show_promo', False)
        srnames = kw.get('srnames', [])
        self.srnames = '+'.join([srname if srname else Frontpage.name
                                 for srname in srnames])
        self.navigable = kw.get('navigable', True)
        self.things = kw.get('organic_links', [])
        self.show_placeholder = isinstance(c.site, (DefaultSR, AllSR))

    def get_items(self):
        from r2.lib.template_helpers import replace_render
        things = self.things
        for t in things:
            if not hasattr(t, "render_replaced"):
                t.render = replace_render(self, t, t.render)
                t.render_replaced = True
        return things, None, None, 0, 0

    def listing(self):
        res = Listing.listing(self)
        for t in res.things:
            t.num = ""
        return Wrapped(self)

########NEW FILE########
__FILENAME__ = mail_queue
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import datetime
import hashlib
from email.MIMEText import MIMEText
from email.errors import HeaderParseError

import sqlalchemy as sa
from sqlalchemy.dialects.postgresql.base import PGInet

from r2.lib.db.tdb_sql import make_metadata, index_str, create_table
from r2.lib.utils import Enum, tup
from r2.lib.memoize import memoize
from pylons import g, request
from pylons.i18n import _

def mail_queue(metadata):
    return sa.Table(g.db_app_name + '_mail_queue', metadata,
                    sa.Column("uid", sa.Integer,
                              sa.Sequence('queue_id_seq'), primary_key=True),

                    # unique hash of the message to carry around
                    sa.Column("msg_hash", sa.String),

                    # the id of the account who started it
                    sa.Column('account_id', sa.BigInteger),

                    # the name (not email) for the from
                    sa.Column('from_name', sa.String),

                    # the "To" address of the email
                    sa.Column('to_addr', sa.String),

                    # the "From" address of the email
                    sa.Column('fr_addr', sa.String),

                    # the "Reply-To" address of the email
                    sa.Column('reply_to', sa.String),

                    # fullname of the thing
                    sa.Column('fullname', sa.String),

                    # when added to the queue
                    sa.Column('date',
                              sa.DateTime(timezone = True),
                              nullable = False),

                    # IP of original request
                    sa.Column('ip', PGInet),

                    # enum of kind of event
                    sa.Column('kind', sa.Integer),

                    # any message that may have been included
                    sa.Column('body', sa.String),

                    )

def sent_mail_table(metadata, name = 'sent_mail'):
    return sa.Table(g.db_app_name + '_' + name, metadata,
                    # tracking hash of the email
                    sa.Column('msg_hash', sa.String, primary_key=True),

                    # the account who started it
                    sa.Column('account_id', sa.BigInteger),

                    # the "To" address of the email
                    sa.Column('to_addr', sa.String),

                    # the "From" address of the email
                    sa.Column('fr_addr', sa.String),

                    # the "reply-to" address of the email
                    sa.Column('reply_to', sa.String),

                    # IP of original request
                    sa.Column('ip', PGInet),

                    # fullname of the reference thing
                    sa.Column('fullname', sa.String),

                    # send date
                    sa.Column('date',
                              sa.DateTime(timezone = True),
                              default = sa.func.now(),
                              nullable = False),

                    # enum of kind of event
                    sa.Column('kind', sa.Integer),

                    )


def opt_out(metadata):
    return sa.Table(g.db_app_name + '_opt_out', metadata,
                    sa.Column('email', sa.String, primary_key = True),
                    # when added to the list
                    sa.Column('date',
                              sa.DateTime(timezone = True),
                              default = sa.func.now(),
                              nullable = False),
                    # why did they do it!?
                    sa.Column('msg_hash', sa.String),
                    )

class EmailHandler(object):
    def __init__(self, force = False):
        engine = g.dbm.get_engine('email')
        self.metadata = make_metadata(engine)
        self.queue_table = mail_queue(self.metadata)
        indices = [index_str(self.queue_table, "date", "date"),
                   index_str(self.queue_table, 'kind', 'kind')]
        create_table(self.queue_table, indices)

        self.opt_table = opt_out(self.metadata)
        indices = [index_str(self.opt_table, 'email', 'email')]
        create_table(self.opt_table, indices)

        self.track_table = sent_mail_table(self.metadata)
        self.reject_table = sent_mail_table(self.metadata, name = "reject_mail")

        def sent_indices(tab):
            indices = [index_str(tab, 'to_addr', 'to_addr'),
                       index_str(tab, 'date', 'date'),
                       index_str(tab, 'ip', 'ip'),
                       index_str(tab, 'kind', 'kind'),
                       index_str(tab, 'fullname', 'fullname'),
                       index_str(tab, 'account_id', 'account_id'),
                       index_str(tab, 'msg_hash', 'msg_hash'),
                       ]

        create_table(self.track_table, sent_indices(self.track_table))
        create_table(self.reject_table, sent_indices(self.reject_table))

    def __repr__(self):
        return "<email-handler>"

    def has_opted_out(self, email):
        o = self.opt_table
        s = sa.select([o.c.email], o.c.email == email, limit = 1)
        res = s.execute()
        return bool(res.fetchall())


    def opt_out(self, msg_hash):
        """Adds the recipient of the email to the opt-out list and returns
        that address."""
        email = self.get_recipient(msg_hash)
        if email:
            o = self.opt_table
            try:
                o.insert().values({o.c.email: email,
                                   o.c.msg_hash: msg_hash}).execute()

                #clear caches
                has_opted_out(email, _update = True)
                opt_count(_update = True)
                return (email, True)
            except sa.exc.DBAPIError:
                return (email, False)
        return (None, False)

    def opt_in(self, msg_hash):
        """Removes recipient of the email from the opt-out list"""
        email = self.get_recipient(msg_hash)
        if email:
            o = self.opt_table
            if self.has_opted_out(email):
                sa.delete(o, o.c.email == email).execute()

                #clear caches
                has_opted_out(email, _update = True)
                opt_count(_update = True)
                return (email, True)
            else:
                return (email, False)
        return (None, False)

    def get_recipient(self, msg_hash):
        t = self.track_table
        s = sa.select([t.c.to_addr], t.c.msg_hash == msg_hash).execute()
        res = s.fetchall()
        return res[0][0] if res and res[:1] else None


    def add_to_queue(self, user, emails, from_name, fr_addr, kind,
                     date = None, ip = None,
                     body = "", reply_to = "", thing = None):
        s = self.queue_table
        hashes = []
        if not date:
            date = datetime.datetime.now(g.tz)
        if not ip:
            ip = getattr(request, "ip", "127.0.0.1")
        for email in tup(emails):
            uid = user._id if user else 0
            tid = thing._fullname if thing else ""
            key = hashlib.sha1(str((email, from_name, uid, tid, ip, kind, body,
                               datetime.datetime.now(g.tz)))).hexdigest()
            s.insert().values({s.c.to_addr : email,
                               s.c.account_id : uid,
                               s.c.from_name : from_name,
                               s.c.fr_addr : fr_addr,
                               s.c.reply_to : reply_to,
                               s.c.fullname: tid,
                               s.c.ip : ip,
                               s.c.kind: kind,
                               s.c.body: body,
                               s.c.date : date,
                               s.c.msg_hash : key}).execute()
            hashes.append(key)
        return hashes


    def from_queue(self, max_date, batch_limit = 50, kind = None):
        from r2.models import is_banned_IP, Account, Thing
        keep_trying = True
        min_id = None
        s = self.queue_table
        while keep_trying:
            where = [s.c.date < max_date]
            if min_id:
                where.append(s.c.uid > min_id)
            if kind:
                where.append(s.c.kind == kind)

            res = sa.select([s.c.to_addr, s.c.account_id,
                             s.c.from_name, s.c.fullname, s.c.body,
                             s.c.kind, s.c.ip, s.c.date, s.c.uid,
                             s.c.msg_hash, s.c.fr_addr, s.c.reply_to],
                            sa.and_(*where),
                            order_by = s.c.uid, limit = batch_limit).execute()
            res = res.fetchall()

            if not res: break

            # batch load user accounts
            aids = [x[1] for x in res if x[1] > 0]
            accts = Account._byID(aids, data = True,
                                  return_dict = True) if aids else {}

            # batch load things
            tids = [x[3] for x in res if x[3]]
            things = Thing._by_fullname(tids, data = True,
                                        return_dict = True) if tids else {}

            # make sure no IPs have been banned in the mean time
            ips = set(x[6] for x in res)
            ips = dict((ip, is_banned_IP(ip)) for ip in ips)

            # get the lower bound date for next iteration
            min_id = max(x[8] for x in res)

            # did we not fetch them all?
            keep_trying = (len(res) == batch_limit)

            for (addr, acct, fname, fulln, body, kind, ip, date, uid,
                 msg_hash, fr_addr, reply_to) in res:
                yield (accts.get(acct), things.get(fulln), addr,
                       fname, date, ip, ips[ip], kind, msg_hash, body,
                       fr_addr, reply_to)

    def clear_queue(self, max_date, kind = None):
        s = self.queue_table
        where = [s.c.date < max_date]
        if kind:
            where.append([s.c.kind == kind])
        sa.delete(s, sa.and_(*where)).execute()


class Email(object):
    handler = EmailHandler()

    Kind = Enum("SHARE", "FEEDBACK", "ADVERTISE", "OPTOUT", "OPTIN",
                "VERIFY_EMAIL", "RESET_PASSWORD",
                "BID_PROMO",
                "ACCEPT_PROMO",
                "REJECT_PROMO",
                "QUEUED_PROMO",
                "LIVE_PROMO",
                "FINISHED_PROMO",
                "NEW_PROMO",
                "NERDMAIL",
                "GOLDMAIL",
                "PASSWORD_CHANGE",
                "EMAIL_CHANGE",
                "REFUNDED_PROMO",
                "VOID_PAYMENT",
                "GOLD_GIFT_CODE",
                )

    subjects = {
        Kind.SHARE : _("[reddit] %(user)s has shared a link with you"),
        Kind.FEEDBACK : _("[feedback] feedback from '%(user)s'"),
        Kind.ADVERTISE :  _("[ad_inq] feedback from '%(user)s'"),
        Kind.OPTOUT : _("[reddit] email removal notice"),
        Kind.OPTIN  : _("[reddit] email addition notice"),
        Kind.RESET_PASSWORD : _("[reddit] reset your password"),
        Kind.VERIFY_EMAIL : _("[reddit] verify your email address"),
        Kind.BID_PROMO : _("[reddit] your bid has been accepted"),
        Kind.ACCEPT_PROMO : _("[reddit] your promotion has been accepted"),
        Kind.REJECT_PROMO : _("[reddit] your promotion has been rejected"),
        Kind.QUEUED_PROMO : _("[reddit] your promotion has been charged"),
        Kind.LIVE_PROMO   : _("[reddit] your promotion is now live"),
        Kind.FINISHED_PROMO : _("[reddit] your promotion has finished"),
        Kind.NEW_PROMO : _("[reddit] your promotion has been created"),
        Kind.NERDMAIL : _("[reddit] hey, nerd!"),
        Kind.GOLDMAIL : _("[reddit] reddit gold activation link"),
        Kind.PASSWORD_CHANGE : _("[reddit] your password has been changed"),
        Kind.EMAIL_CHANGE : _("[reddit] your email address has been changed"),
        Kind.REFUNDED_PROMO: _("[reddit] your campaign didn't get enough impressions"),
        Kind.VOID_PAYMENT: _("[reddit] your payment has been voided"),
        Kind.GOLD_GIFT_CODE: _("[reddit] your reddit gold gift code"),
        }

    def __init__(self, user, thing, email, from_name, date, ip, banned_ip,
                 kind, msg_hash, body = '', from_addr = '',
                 reply_to = ''):
        self.user = user
        self.thing = thing
        self.to_addr = email
        self.fr_addr = from_addr
        self._from_name = from_name
        self.date = date
        self.ip = ip
        self.banned_ip = banned_ip
        self.kind = kind
        self.sent = False
        self.body = body
        self.msg_hash = msg_hash
        self.reply_to = reply_to
        self.subject = self.subjects.get(kind, "")
        try:
            self.subject = self.subject % dict(user = self.from_name())
        except UnicodeDecodeError:
            self.subject = self.subject % dict(user = "a user")


    def from_name(self):
        if not self.user:
            name = "%(name)s"
        elif self._from_name != self.user.name:
            name = "%(name)s (%(uname)s)"
        else:
            name = "%(uname)s"
        return name % dict(name = self._from_name,
                           uname = self.user.name if self.user else '')

    @classmethod
    def get_unsent(cls, max_date, batch_limit = 50, kind = None):
        for e in cls.handler.from_queue(max_date, batch_limit = batch_limit,
                                        kind = kind):
            yield cls(*e)

    def should_queue(self):
        return (not self.user  or not self.user._spam) and \
               (not self.thing or not self.thing._spam) and \
               not self.banned_ip and \
               (self.kind == self.Kind.OPTOUT or
                not has_opted_out(self.to_addr))

    def set_sent(self, date = None, rejected = False):
        if not self.sent:
            self.date = date or datetime.datetime.now(g.tz)
            t = self.handler.reject_table if rejected else self.handler.track_table
            try:
                t.insert().values({t.c.account_id:
                                       self.user._id if self.user else 0,
                                   t.c.to_addr :   self.to_addr,
                                   t.c.fr_addr :   self.fr_addr,
                                   t.c.reply_to :  self.reply_to,
                                   t.c.ip :        self.ip,
                                   t.c.fullname:
                                       self.thing._fullname if self.thing else "",
                                   t.c.date:       self.date,
                                   t.c.kind :      self.kind,
                                   t.c.msg_hash :  self.msg_hash,
                                   }).execute()
            except:
                print "failed to send message"

            self.sent = True

    def to_MIMEText(self):
        def utf8(s, reject_newlines=True):
            if reject_newlines and '\n' in s:
                raise HeaderParseError(
                    'header value contains unexpected newline: {!r}'.format(s))
            return s.encode('utf8') if isinstance(s, unicode) else s

        fr = '"%s" <%s>' % (
            self.from_name().replace('"', ''),
            self.fr_addr.replace('>', ''),
        )

        if not fr.startswith('-') and not self.to_addr.startswith('-'): # security
            msg = MIMEText(utf8(self.body, reject_newlines=False))
            msg.set_charset('utf8')
            msg['To']      = utf8(self.to_addr)
            msg['From']    = utf8(fr)
            msg['Subject'] = utf8(self.subject)
            if self.user:
                msg['X-Reddit-username'] = utf8(self.user.name)
            msg['X-Reddit-ID'] = self.msg_hash
            if self.reply_to:
                msg['Reply-To'] = utf8(self.reply_to)
            return msg
        return None

@memoize('r2.models.mail_queue.has_opted_out')
def has_opted_out(email):
    o = Email.handler.opt_table
    s = sa.select([o.c.email], o.c.email == email, limit = 1)
    res = s.execute()
    return bool(res.fetchall())


@memoize('r2.models.mail_queue.opt_count')
def opt_count():
    o = Email.handler.opt_table
    s = sa.select([sa.func.count(o.c.email)])
    res = s.execute().fetchone()
    return int(res[0])

########NEW FILE########
__FILENAME__ = media_cache
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2013-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import collections
import json

from datetime import (
    datetime,
    timedelta,
)
from pycassa.system_manager import ASCII_TYPE, UTF8_TYPE
from r2.lib.db import tdb_cassandra


Media = collections.namedtuple('_Media', ("media_object",
                                          "secure_media_object",
                                          "thumbnail_url",
                                          "thumbnail_size"))

ERROR_MEDIA = Media(None, None, None, None)


class MediaByURL(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _ttl = timedelta(minutes=720)

    _read_consistency_level = tdb_cassandra.CL.QUORUM
    _write_consistency_level = tdb_cassandra.CL.QUORUM
    _int_props = {"thumbnail_width", "thumbnail_height"}
    _date_props = {"last_modified"}
    _extra_schema_creation_args = {
        "key_validation_class": ASCII_TYPE,
        "column_name_class": UTF8_TYPE,
    }

    _defaults = {
        "state": "enqueued",
        "error": "",
        "thumbnail_url": "",
        "thumbnail_width": 0,
        "thumbnail_height": 0,
        "media_object": "",
        "secure_media_object": "",
        "last_modified": datetime.utcfromtimestamp(0),
    }

    @classmethod
    def _rowkey(cls, url, **kwargs):
        return (
            url +
            # pipe is not allowed in URLs, so use it as a delimiter
            "|" +

            # append the extra cache keys in kwargs as a canonical JSON string
            json.dumps(
                kwargs,
                ensure_ascii=True,
                encoding="ascii",
                indent=None,
                separators=(",", ":"),
                sort_keys=True,
            )
        )

    @classmethod
    def add_placeholder(cls, url, **kwargs):
        rowkey = cls._rowkey(url, **kwargs)
        cls._set_values(rowkey, {
            "state": "enqueued",
            "error": "",
            "last_modified": datetime.utcnow(),
        })

    @classmethod
    def add(cls, url, media, **kwargs):
        rowkey = cls._rowkey(url, **kwargs)
        columns = cls._defaults.copy()

        columns.update({
            "state": "processed",
            "error": "",
            "last_modified": datetime.utcnow(),
        })

        if media.thumbnail_url and media.thumbnail_size:
            columns.update({
                "thumbnail_url": media.thumbnail_url,
                "thumbnail_width": media.thumbnail_size[0],
                "thumbnail_height": media.thumbnail_size[1],
            })

        if media.media_object:
            columns.update({
                "media_object": json.dumps(media.media_object),
            })

        if media.secure_media_object:
            columns.update({
                "secure_media_object": (json.
                                        dumps(media.secure_media_object)),
            })

        cls._set_values(rowkey, columns)

    @classmethod
    def add_error(cls, url, error, **kwargs):
        rowkey = cls._rowkey(url, **kwargs)
        columns = {
            "error": error,
            "state": "processed",
            "last_modified": datetime.utcnow(),
        }
        cls._set_values(rowkey, columns)

    @classmethod
    def get(cls, url, max_cache_age=None, **kwargs):
        rowkey = cls._rowkey(url, **kwargs)
        try:
            temp = cls._byID(rowkey)

            # Return None if this cache entry is too old
            if (max_cache_age is not None and
                datetime.datetime.utcnow() - temp.last_modified >
                max_cache_age):
                return None
            else:
                return temp
        except tdb_cassandra.NotFound:
            return None

    @property
    def media(self):
        if self.state == "processed":
            if not self.error:
                media_object = secure_media_object = None
                thumbnail_url = thumbnail_size = None

                if (self.thumbnail_width and self.thumbnail_height and
                    self.thumbnail_url):
                    thumbnail_url = self.thumbnail_url
                    thumbnail_size = (self.thumbnail_width,
                                      self.thumbnail_height)

                if self.media_object:
                    media_object = json.loads(self.media_object)

                if self.secure_media_object:
                    secure_media_object = json.loads(self.secure_media_object)

                return Media(media_object, secure_media_object,
                             thumbnail_url, thumbnail_size)
            else:
                return ERROR_MEDIA
        else:
            return None

########NEW FILE########
__FILENAME__ = modaction
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from datetime import timedelta
import itertools

from r2.lib.db import tdb_cassandra
from r2.lib.utils import tup
from r2.models import Account, Subreddit, Link, Comment, Printable
from r2.models.subreddit import DefaultSR
from pycassa.system_manager import TIME_UUID_TYPE
from uuid import UUID
from pylons.i18n import _
from pylons import request

class ModAction(tdb_cassandra.UuidThing, Printable):
    """
    Columns:
    sr_id - Subreddit id36
    mod_id - Account id36 of moderator
    action - specific name of action, must be in ModAction.actions
    target_fullname - optional fullname of the target of the action
    details - subcategory available for some actions, must show up in 
    description - optional user
    """

    _read_consistency_level = tdb_cassandra.CL.ONE
    _use_db = True
    _connection_pool = 'main'
    _ttl = timedelta(days=120)
    _str_props = ('sr_id36', 'mod_id36', 'target_fullname', 'action', 'details', 
                  'description')
    _defaults = {}

    actions = ('banuser', 'unbanuser', 'removelink', 'approvelink', 
               'removecomment', 'approvecomment', 'addmoderator',
               'invitemoderator', 'uninvitemoderator', 'acceptmoderatorinvite',
               'removemoderator', 'addcontributor', 'removecontributor',
               'editsettings', 'editflair', 'distinguish', 'marknsfw', 
               'wikibanned', 'wikicontributor', 'wikiunbanned', 'wikipagelisted',
               'removewikicontributor', 'wikirevise', 'wikipermlevel',
               'ignorereports', 'unignorereports', 'setpermissions', 'sticky',
               'unsticky')

    _menu = {'banuser': _('ban user'),
             'unbanuser': _('unban user'),
             'removelink': _('remove post'),
             'approvelink': _('approve post'),
             'removecomment': _('remove comment'),
             'approvecomment': _('approve comment'),
             'addmoderator': _('add moderator'),
             'removemoderator': _('remove moderator'),
             'invitemoderator': _('invite moderator'),
             'uninvitemoderator': _('uninvite moderator'),
             'acceptmoderatorinvite': _('accept moderator invite'),
             'addcontributor': _('add contributor'),
             'removecontributor': _('remove contributor'),
             'editsettings': _('edit settings'),
             'editflair': _('edit flair'),
             'distinguish': _('distinguish'),
             'marknsfw': _('mark nsfw'),
             'wikibanned': _('ban from wiki'),
             'wikiunbanned': _('unban from wiki'),
             'wikicontributor': _('add wiki contributor'),
             'wikipagelisted': _('delist/relist wiki pages'),
             'removewikicontributor': _('remove wiki contributor'),
             'wikirevise': _('wiki revise page'),
             'wikipermlevel': _('wiki page permissions'),
             'ignorereports': _('ignore reports'),
             'unignorereports': _('unignore reports'),
             'setpermissions': _('permissions'),
             'sticky': _('sticky post'),
             'unsticky': _('unsticky post'),
            }

    _text = {'banuser': _('banned'),
             'wikibanned': _('wiki banned'),
             'wikiunbanned': _('unbanned from wiki'),
             'wikicontributor': _('added wiki contributor'),
             'removewikicontributor': _('removed wiki contributor'),
             'unbanuser': _('unbanned'),
             'removelink': _('removed'),
             'approvelink': _('approved'),
             'removecomment': _('removed'),
             'approvecomment': _('approved'),
             'addmoderator': _('added moderator'),
             'removemoderator': _('removed moderator'),
             'invitemoderator': _('invited moderator'),
             'uninvitemoderator': _('uninvited moderator'),
             'acceptmoderatorinvite': _('accepted moderator invitation'),
             'addcontributor': _('added approved contributor'),
             'removecontributor': _('removed approved contributor'),
             'editsettings': _('edited settings'),
             'editflair': _('edited flair'),
             'wikirevise': _('edited wiki page'),
             'wikipermlevel': _('changed wiki page permission level'),
             'wikipagelisted': _('changed wiki page listing preference'),
             'distinguish': _('distinguished'),
             'marknsfw': _('marked nsfw'),
             'ignorereports': _('ignored reports'),
             'unignorereports': _('unignored reports'),
             'setpermissions': _('changed permissions on'),
             'sticky': _('stickied'),
             'unsticky': _('unstickied'),
            }

    _details_text = {# approve comment/link
                     'unspam': _('unspam'),
                     'confirm_ham': _('approved'),
                     # remove comment/link
                     'confirm_spam': _('confirmed spam'),
                     'remove': _('removed not spam'),
                     'spam': _('removed spam'),
                     # removemoderator
                     'remove_self': _('removed self'),
                     # editsettings
                     'title': _('title'),
                     'public_description': _('description'),
                     'description': _('sidebar'),
                     'lang': _('language'),
                     'type': _('type'),
                     'link_type': _('link type'),
                     'submit_link_label': _('submit link button label'),
                     'submit_text_label': _('submit text post button label'),
                     'comment_score_hide_mins': _('comment score hide period'),
                     'over_18': _('toggle viewers must be over 18'),
                     'allow_top': _('toggle allow in default/trending lists'),
                     'show_media': _('toggle show thumbnail images of content'),
                     'public_traffic': _('toggle public traffic stats page'),
                     'exclude_banned_modqueue': _('toggle exclude banned users\' posts from modqueue'),
                     'domain': _('domain'),
                     'show_cname_sidebar': _('toggle show sidebar from cname'),
                     'css_on_cname': _('toggle custom CSS from cname'),
                     'header_title': _('header title'),
                     'stylesheet': _('stylesheet'),
                     'del_header': _('delete header image'),
                     'del_image': _('delete image'),
                     'upload_image_header': _('upload header image'),
                     'upload_image': _('upload image'),
                     # editflair
                     'flair_edit': _('add/edit flair'),
                     'flair_delete': _('delete flair'),
                     'flair_csv': _('edit by csv'),
                     'flair_enabled': _('toggle flair enabled'),
                     'flair_position': _('toggle user flair position'),
                     'link_flair_position': _('toggle link flair position'),
                     'flair_self_enabled': _('toggle user assigned flair enabled'),
                     'link_flair_self_enabled': _('toggle submitter assigned link flair enabled'),
                     'flair_template': _('add/edit flair templates'),
                     'flair_delete_template': _('delete flair template'),
                     'flair_clear_template': _('clear flair templates'),
                     # distinguish/nsfw
                     'remove': _('remove'),
                     'ignore_reports': _('ignore reports'),
                     # permissions
                     'permission_moderator': _('set permissions on moderator'),
                     'permission_moderator_invite': _('set permissions on moderator invitation')}

    # This stuff won't change
    cache_ignore = set(['subreddit', 'target', 'mod', 'button']).union(Printable.cache_ignore)

    # Thing properties for Printable
    @property
    def author_id(self):
        return int(self.mod_id36, 36)

    @property
    def sr_id(self):
        return int(self.sr_id36, 36)

    @property
    def _ups(self):
        return 0

    @property
    def _downs(self):
        return 0

    @property
    def _deleted(self):
        return False

    @property
    def _spam(self):
        return False

    @property
    def reported(self):
        return False

    @classmethod
    def create(cls, sr, mod, action, details=None, target=None, description=None):
        # Split this off into separate function to check for valid actions?
        if not action in cls.actions:
            raise ValueError("Invalid ModAction: %s" % action)
        
        # Front page should insert modactions into the base sr
        sr = sr._base if isinstance(sr, DefaultSR) else sr
        
        kw = dict(sr_id36=sr._id36, mod_id36=mod._id36, action=action)

        if target:
            kw['target_fullname'] = target._fullname
        if details:
            kw['details'] = details
        if description:
            kw['description'] = description

        ma = cls(**kw)
        ma._commit()
        return ma

    def _on_create(self):
        """
        Update all Views.
        """

        views = (ModActionBySR, ModActionBySRMod, ModActionBySRAction, 
                 ModActionBySRActionMod)

        for v in views:
            v.add_object(self)

    @classmethod
    def get_actions(cls, srs, mod=None, action=None, after=None, reverse=False, count=1000):
        """
        Get a ColumnQuery that yields ModAction objects according to
        specified criteria.
        """
        if after and isinstance(after, basestring):
            after = cls._byID(UUID(after))
        elif after and isinstance(after, UUID):
            after = cls._byID(after)

        if not isinstance(after, cls):
            after = None

        srs = tup(srs)

        if not mod and not action:
            rowkeys = [sr._id36 for sr in srs]
            q = ModActionBySR.query(rowkeys, after=after, reverse=reverse, count=count)
        elif mod:
            mods = tup(mod)
            key = '%s_%s' if not action else '%%s_%%s_%s' % action
            rowkeys = itertools.product([sr._id36 for sr in srs],
                [mod._id36 for mod in mods])
            rowkeys = [key % (sr, mod) for sr, mod in rowkeys]
            view = ModActionBySRActionMod if action else ModActionBySRMod
            q = view.query(rowkeys, after=after, reverse=reverse, count=count)
        else:
            rowkeys = ['%s_%s' % (sr._id36, action) for sr in srs]
            q = ModActionBySRAction.query(rowkeys, after=after, reverse=reverse, count=count)

        return q

    def get_extra_text(self):
        text = ''
        if hasattr(self, 'details') and not self.details == None:
            text += self._details_text.get(self.details, self.details)
        if hasattr(self, 'description') and not self.description == None:
            text += ' %s' % self.description
        return text

    @staticmethod
    def get_rgb(i, fade=0.8):
        r = int(256 - (hash(str(i)) % 256)*(1-fade))
        g = int(256 - (hash(str(i) + ' ') % 256)*(1-fade))
        b = int(256 - (hash(str(i) + '  ') % 256)*(1-fade))
        return (r, g, b)

    @classmethod
    def add_props(cls, user, wrapped):

        from r2.lib.menus import NavButton
        from r2.lib.db.thing import Thing
        from r2.lib.pages import WrappedUser
        from r2.lib.filters import _force_unicode

        TITLE_MAX_WIDTH = 50

        request_path = request.path

        target_fullnames = [item.target_fullname for item in wrapped if hasattr(item, 'target_fullname')]
        targets = Thing._by_fullname(target_fullnames, data=True)
        authors = Account._byID([t.author_id for t in targets.values() if hasattr(t, 'author_id')], data=True)
        links = Link._byID([t.link_id for t in targets.values() if hasattr(t, 'link_id')], data=True)

        sr_ids = set([t.sr_id for t in targets.itervalues() if hasattr(t, 'sr_id')] +
                     [w.sr_id for w in wrapped])
        subreddits = Subreddit._byID(sr_ids, data=True)

        # Assemble target links
        target_links = {}
        target_accounts = {}
        for fullname, target in targets.iteritems():
            if isinstance(target, Link):
                author = authors[target.author_id]
                title = _force_unicode(target.title)
                if len(title) > TITLE_MAX_WIDTH:
                    short_title = title[:TITLE_MAX_WIDTH] + '...'
                else:
                    short_title = title
                text = '%(link)s "%(title)s" %(by)s %(author)s' % {
                        'link': _('link'),
                        'title': short_title, 
                        'by': _('by'),
                        'author': author.name}
                path = target.make_permalink(subreddits[target.sr_id])
                target_links[fullname] = (text, path, title)
            elif isinstance(target, Comment):
                author = authors[target.author_id]
                link = links[target.link_id]
                title = _force_unicode(link.title)
                if len(title) > TITLE_MAX_WIDTH:
                    short_title = title[:TITLE_MAX_WIDTH] + '...'
                else:
                    short_title = title
                text = '%(comment)s %(by)s %(author)s %(on)s "%(title)s"' % {
                        'comment': _('comment'),
                        'by': _('by'),
                        'author': author.name,
                        'on': _('on'),
                        'title': short_title}
                path = target.make_permalink(link, subreddits[link.sr_id])
                target_links[fullname] = (text, path, title)
            elif isinstance(target, Account):
                target_accounts[fullname] = WrappedUser(target)

        for item in wrapped:
            # Can I move these buttons somewhere else? Not great to have request stuff in here
            css_class = 'modactions %s' % item.action
            item.button = NavButton('', item.action, opt='type', css_class=css_class)
            item.button.build(base_path=request_path)

            mod_name = item.author.name
            item.mod = NavButton(mod_name, mod_name, opt='mod')
            item.mod.build(base_path=request_path)
            item.text = ModAction._text.get(item.action, '')
            item.details = item.get_extra_text()

            if hasattr(item, 'target_fullname') and item.target_fullname:
                target = targets[item.target_fullname]
                if isinstance(target, Account):
                    item.target_wrapped_user = target_accounts[item.target_fullname]
                elif isinstance(target, Link) or isinstance(target, Comment):
                    item.target_text, item.target_path, item.target_title = target_links[item.target_fullname]

            item.bgcolor = ModAction.get_rgb(item.sr_id)
            item.sr_name = subreddits[item.sr_id].name
            item.sr_path = subreddits[item.sr_id].path

        Printable.add_props(user, wrapped)

class ModActionBySR(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _compare_with = TIME_UUID_TYPE
    _view_of = ModAction
    _ttl = timedelta(days=90)
    _read_consistency_level = tdb_cassandra.CL.ONE

    @classmethod
    def _rowkey(cls, ma):
        return ma.sr_id36

class ModActionBySRMod(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _compare_with = TIME_UUID_TYPE
    _view_of = ModAction
    _ttl = timedelta(days=90)
    _read_consistency_level = tdb_cassandra.CL.ONE

    @classmethod
    def _rowkey(cls, ma):
        return '%s_%s' % (ma.sr_id36, ma.mod_id36)

class ModActionBySRActionMod(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _compare_with = TIME_UUID_TYPE
    _view_of = ModAction
    _ttl = timedelta(days=90)
    _read_consistency_level = tdb_cassandra.CL.ONE

    @classmethod
    def _rowkey(cls, ma):
        return '%s_%s_%s' % (ma.sr_id36, ma.mod_id36, ma.action)

class ModActionBySRAction(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _compare_with = TIME_UUID_TYPE
    _view_of = ModAction
    _ttl = timedelta(days=90)
    _read_consistency_level = tdb_cassandra.CL.ONE

    @classmethod
    def _rowkey(cls, ma):
        return '%s_%s' % (ma.sr_id36, ma.action)

########NEW FILE########
__FILENAME__ = populatedb
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.models import *
from r2.lib.utils import fetch_things2
from pylons import g
from r2.lib.db import queries


import string
import random

def random_word(min, max):
    return ''.join([ random.choice(string.letters)
                         for x
                         in range(random.randint(min, max)) ])

def populate(num_srs = 10, num_users = 1000, num_links = 100, num_comments = 20, num_votes = 50):
    try:
        a = Account._by_name(g.system_user)
    except NotFound:
        a = register(g.system_user, "password", "127.0.0.1")

    srs = []
    for i in range(num_srs):
        name = "reddit_test%d" % i
        try:
            sr = Subreddit._new(name = name, title = "everything about #%d"%i,
                                ip = '0.0.0.0', author_id = a._id)
            sr._downs = 10
            sr.lang = "en"
            sr._commit()
        except SubredditExists:
            sr = Subreddit._by_name(name)
        srs.append(sr)

    accounts = []
    for i in range(num_users):
        name_ext = ''.join([ random.choice(string.letters)
                             for x
                             in range(int(random.uniform(1, 10))) ])
        name = 'test_' + name_ext
        try:
            a = register(name, name, "127.0.0.1")
        except AccountExists:
            a = Account._by_name(name)
        accounts.append(a)

    for i in range(num_links):
        id = random.uniform(1,100)
        title = url = 'http://google.com/?q=' + str(id)
        user = random.choice(accounts)
        sr = random.choice(srs)
        l = Link._submit(title, url, user, sr, '127.0.0.1')
        queries.new_link(l)

        comments = [ None ]
        for i in range(int(random.betavariate(2, 8) * 5 * num_comments)):
            user = random.choice(accounts)
            body = ' '.join([ random_word(1, 10)
                              for x
                              in range(int(200 * random.betavariate(2, 6))) ])
            parent = random.choice(comments)
            (c, inbox_rel) = Comment._new(user, l, parent, body, '127.0.0.1')
            queries.new_comment(c, inbox_rel)
            comments.append(c)
            for i in range(int(random.betavariate(2, 8) * 10)):
                another_user = random.choice(accounts)
                v = Vote.vote(another_user, c, True, '127.0.0.1')
                queries.new_vote(v)

        like = random.randint(50,100)
        for i in range(int(random.betavariate(2, 8) * 5 * num_votes)):
           user = random.choice(accounts)
           v = Vote.vote(user, l, random.randint(0, 100) <= like, '127.0.0.1')
           queries.new_vote(v)

    queries.worker.join()


def by_url_cache():
    q = Link._query(Link.c._spam == (True,False),
                    data = True,
                    sort = desc('_date'))
    for i, link in enumerate(fetch_things2(q)):
        if i % 100 == 0:
            print "%s..." % i
        link.set_url_cache()

########NEW FILE########
__FILENAME__ = printable
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import c, request
from r2.lib.strings import Score
from r2.lib import hooks


class Printable(object):
    show_spam = False
    show_reports = False
    is_special = False
    can_ban = False
    deleted = False
    rowstyle = ''
    collapsed = False
    author = None
    margin = 0
    is_focal = False
    childlisting = None
    cache_ignore = set(['c', 'author', 'score_fmt', 'child',
                        # displayed score is cachable, so remove score
                        # related fields.
                        'voting_score', 'display_score',
                        'render_score', 'score', '_score', 
                        'upvotes', '_ups',
                        'downvotes', '_downs',
                        'subreddit_slow', '_deleted', '_spam',
                        'cachable', 'make_permalink', 'permalink',
                        'timesince',
                        ])

    @classmethod
    def add_props(cls, user, wrapped):
        from r2.lib.wrapped import CachedVariable
        for item in wrapped:
            # insert replacement variable for timesince to allow for
            # caching of thing templates
            item.display = CachedVariable("display")
            item.timesince = CachedVariable("timesince")
            item.childlisting = CachedVariable("childlisting")

            score_fmt = getattr(item, "score_fmt", Score.number_only)
            item.display_score = map(score_fmt, item.voting_score)

            if item.cachable:
                item.render_score  = item.display_score
                item.display_score = map(CachedVariable,
                                         ["scoredislikes", "scoreunvoted",
                                          "scorelikes"])

        hooks.get_hook("add_props").call(items=wrapped)

    @property
    def permalink(self, *a, **kw):
        raise NotImplementedError

    def keep_item(self, wrapped):
        return True

    @staticmethod
    def wrapped_cache_key(wrapped, style):
        s = [wrapped._fullname, wrapped._spam, wrapped.reported]

        if style == 'htmllite':
            s.extend([c.bgcolor, c.bordercolor, 
                      request.GET.has_key('style'),
                      request.GET.get("expanded"),
                      getattr(wrapped, 'embed_voting_style', None)])
        return s

########NEW FILE########
__FILENAME__ = promo
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from datetime import datetime, timedelta
from uuid import uuid1

from pycassa.types import CompositeType
from pylons import g, c
from pylons.i18n import _, N_

from r2.lib import filters
from r2.lib.cache import sgm
from r2.lib.db import tdb_cassandra
from r2.lib.db.thing import Thing, NotFound
from r2.lib.memoize import memoize
from r2.lib.utils import Enum, to_datetime, to_date
from r2.models.subreddit import Subreddit


PROMOTE_STATUS = Enum("unpaid", "unseen", "accepted", "rejected",
                      "pending", "promoted", "finished")

class PriorityLevel(object):
    name = ''
    _text = N_('')
    _description = N_('')
    value = 1   # Values are from 1 (highest) to 100 (lowest)
    default = False
    inventory_override = False
    cpm = True  # Non-cpm is percentage, will fill unsold impressions

    def __repr__(self):
        return "<PriorityLevel %s: %s>" % (self.name, self.value)

    @property
    def text(self):
        return _(self._text) if self._text else ''

    @property
    def description(self):
        return _(self._description) if self._description else ''


class HighPriority(PriorityLevel):
    name = 'high'
    _text = N_('highest')
    value = 5


class MediumPriority(PriorityLevel):
    name = 'standard'
    _text = N_('standard')
    value = 10
    default = True


class RemnantPriority(PriorityLevel):
    name = 'remnant'
    _text = N_('remnant')
    _description = N_('lower priority, impressions are not guaranteed')
    value = 20
    inventory_override = True


class HousePriority(PriorityLevel):
    name = 'house'
    _text = N_('house')
    _description = N_('non-CPM, displays in all unsold impressions')
    value = 30
    inventory_override = True
    cpm = False


HIGH, MEDIUM, REMNANT, HOUSE = HighPriority(), MediumPriority(), RemnantPriority(), HousePriority()
PROMOTE_PRIORITIES = {p.name: p for p in (HIGH, MEDIUM, REMNANT, HOUSE)}
PROMOTE_DEFAULT_PRIORITY = MEDIUM


class Location(object):
    DELIMITER = '-'
    def __init__(self, country, region=None, metro=None):
        self.country = country or None
        self.region = region or None
        self.metro = metro or None

    def __repr__(self):
        return '<%s (%s/%s/%s)>' % (self.__class__.__name__, self.country,
                                    self.region, self.metro)

    def to_code(self):
        fields = [self.country, self.region, self.metro]
        return self.DELIMITER.join(i or '' for i in fields)

    @classmethod
    def from_code(cls, code):
        country, region, metro = [i or None for i in code.split(cls.DELIMITER)]
        return cls(country, region, metro)

    def contains(self, other):
        if not self.country:
            # self is set of all countries, it includes all possible
            # values of other.country
            return True
        elif not other or not other.country:
            # self is more specific than other
            return False
        else:
            # both self and other specify a country
            if self.country != other.country:
                # countries don't match
                return False
            else:
                # countries match
                if not self.metro:
                    # self.metro is set of all metros within country, it
                    # includes all possible values of other.metro
                    return True
                elif not other.metro:
                    # self is more specific than other
                    return False
                else:
                    return self.metro == other.metro


@memoize("get_promote_srid")
def get_promote_srid(name = 'promos'):
    try:
        sr = Subreddit._by_name(name, stale=True)
    except NotFound:
        sr = Subreddit._new(name = name,
                            title = "promoted links",
                            # negative author_ids make this unlisable
                            author_id = -1,
                            type = "public", 
                            ip = '0.0.0.0')
    return sr._id


def calc_impressions(bid, cpm_pennies):
    # bid is in dollars, cpm_pennies is pennies
    # CPM is cost per 1000 impressions
    return int(bid / cpm_pennies * 1000 * 100)


NO_TRANSACTION = 0

class PromoCampaign(Thing):
    _defaults = dict(
        priority_name=PROMOTE_DEFAULT_PRIORITY.name,
        trans_id=NO_TRANSACTION,
        location_code=None,
    )

    def __getattr__(self, attr):
        val = Thing.__getattr__(self, attr)
        if attr in ('start_date', 'end_date'):
            val = to_datetime(val)
            if not val.tzinfo:
                val = val.replace(tzinfo=g.tz)
        return val

    @classmethod
    def get_priority_name(cls, priority):
        if not priority in PROMOTE_PRIORITIES.values():
            raise ValueError("%s is not a valid priority" % val)
        return priority.name

    @classmethod 
    def _new(cls, link, sr_name, bid, cpm, start_date, end_date, priority,
             location):
        location_code = location.to_code() if location else None
        pc = PromoCampaign(link_id=link._id,
                           sr_name=sr_name,
                           bid=bid,
                           cpm=cpm,
                           start_date=start_date,
                           end_date=end_date,
                           trans_id=NO_TRANSACTION,
                           owner_id=link.author_id,
                           priority_name=cls.get_priority_name(priority),
                           location_code=location_code)
        pc._commit()
        return pc

    @classmethod
    def _by_link(cls, link_id):
        '''
        Returns an iterable of campaigns associated with link_id or an empty
        list if there are none.
        '''
        return cls._query(PromoCampaign.c.link_id == link_id, data=True)


    @classmethod
    def _by_user(cls, account_id):
        '''
        Returns an iterable of all campaigns owned by account_id or an empty 
        list if there are none.
        '''
        return cls._query(PromoCampaign.c.owner_id == account_id, data=True)

    @property
    def ndays(self):
        return (self.end_date - self.start_date).days

    @property
    def impressions(self):
        # deal with pre-CPM PromoCampaigns
        if not hasattr(self, 'cpm'):
            return -1
        elif not self.priority.cpm:
            return -1
        return calc_impressions(self.bid, self.cpm)

    @property
    def priority(self):
        return PROMOTE_PRIORITIES[self.priority_name]

    @property
    def location(self):
        if self.location_code is not None:
            return Location.from_code(self.location_code)
        else:
            return None

    @property
    def location_str(self):
        if not self.location:
            return ''
        elif self.location.metro:
            country = self.location.country
            region = self.location.region
            metro_str = (g.locations[country]['regions'][region]
                         ['metros'][self.location.metro]['name'])
            return '/'.join([country, region, metro_str])
        else:
            return g.locations[self.location.country]['name']

    def is_freebie(self):
        return self.trans_id < 0

    def is_live_now(self):
        now = datetime.now(g.tz)
        return self.start_date < now and self.end_date > now

    def update(self, start_date, end_date, bid, cpm, sr_name, trans_id,
               priority, location, commit=True):
        self.start_date = start_date
        self.end_date = end_date
        self.bid = bid
        self.cpm = cpm
        self.sr_name = sr_name
        self.trans_id = trans_id
        self.priority_name = self.get_priority_name(priority)
        self.location_code = location.to_code() if location else None
        if commit:
            self._commit()

    def delete(self):
        self._deleted = True
        self._commit()

class PromotionLog(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _compare_with = tdb_cassandra.TIME_UUID_TYPE

    @classmethod
    def _rowkey(cls, link):
        return link._fullname

    @classmethod
    def add(cls, link, text):
        name = c.user.name if c.user_is_loggedin else "<AUTOMATED>"
        now = datetime.now(g.tz).strftime("%Y-%m-%d %H:%M:%S")
        text = "[%s: %s] %s" % (name, now, text)
        rowkey = cls._rowkey(link)
        column = {uuid1(): filters._force_utf8(text)}
        cls._set_values(rowkey, column)
        return text

    @classmethod
    def get(cls, link):
        rowkey = cls._rowkey(link)
        try:
            row = cls._byID(rowkey)
        except tdb_cassandra.NotFound:
            return []
        tuples = sorted(row._values().items(), key=lambda t: t[0].time)
        return [t[1] for t in tuples]


class PromotedLinkRoadblock(tdb_cassandra.View):
    _use_db = True
    _connection_pool = 'main'
    _read_consistency_level = tdb_cassandra.CL.ONE
    _write_consistency_level = tdb_cassandra.CL.QUORUM
    _compare_with = CompositeType(
        tdb_cassandra.DateType(),
        tdb_cassandra.DateType(),
    )

    @classmethod
    def _column(cls, start, end):
        start, end = map(to_datetime, [start, end])
        return {(start, end): ''}

    @classmethod
    def _dates_from_key(cls, key):
        start, end = map(to_date, key)
        return start, end

    @classmethod
    def add(cls, sr, start, end):
        rowkey = sr._id36
        column = cls._column(start, end)
        now = datetime.now(g.tz).date()
        ndays = (to_date(end) - now).days + 7
        ttl = timedelta(days=ndays).total_seconds()
        cls._set_values(rowkey, column, ttl=ttl)

    @classmethod
    def remove(cls, sr, start, end):
        rowkey = sr._id36
        column = cls._column(start, end)
        cls._remove(rowkey, column)

    @classmethod
    def is_roadblocked(cls, sr, start, end):
        rowkey = sr._id36
        start, end = map(to_date, [start, end])

        # retrieve columns for roadblocks starting before end
        try:
            columns = cls._cf.get(rowkey, column_finish=(to_datetime(end),),
                                  column_count=tdb_cassandra.max_column_count)
        except tdb_cassandra.NotFoundException:
            return False

        for key in columns.iterkeys():
            rb_start, rb_end = cls._dates_from_key(key)

            # check for overlap, end dates not inclusive
            if (start < rb_end) and (rb_start < end):
                return (rb_start, rb_end)
        return False

    @classmethod
    def get_roadblocks(cls):
        ret = []
        q = cls._cf.get_range()
        rows = list(q)
        srs = Subreddit._byID36([id36 for id36, columns in rows], data=True)
        for id36, columns in rows:
            sr = srs[id36]
            for key in columns.iterkeys():
                start, end = cls._dates_from_key(key)
                ret.append((sr.name, start, end))
        return ret

########NEW FILE########
__FILENAME__ = promo_metrics
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from itertools import product

from r2.lib.db import tdb_cassandra
from r2.lib.utils import tup
from r2.models.subreddit import DefaultSR


class PromoMetrics(tdb_cassandra.View):
    '''
    Cassandra data store for promotion metrics. Used for inventory prediction.

    Usage:
      # set metric value for many subreddits at once
      > PromoMetrics.set('min_daily_pageviews.GET_listing',
                          {'funny': 63432, 'pics': 48829, 'books': 4})

      # get metric value for one subreddit
      > res = PromoMetrics.get('min_daily_pageviews.GET_listing', 'funny')
      {'funny': 1234}

      # get metric value for many subreddits
      > res = PromoMetrics.get('min_daily_pageviews.GET_listing',
                               ['funny', 'pics'])
      {'funny':1234, 'pics':4321}

      # get metric values for all subreddits
      > res = PromoMetrics.get('min_daily_pageviews.GET_listing')
    '''
    _use_db = True
    _value_type = 'int'
    _fetch_all_columns = True

    @classmethod
    def get(cls, metric_name, sr_names=None):
        sr_names = tup(sr_names)
        try:
            metric = cls._byID(metric_name, properties=sr_names)
            return metric._values()  # might have additional values
        except tdb_cassandra.NotFound:
            return {}

    @classmethod
    def set(cls, metric_name, values_by_sr):
        if '' in values_by_sr:  # combine front page values
            fp = DefaultSR.name.lower()
            values_by_sr[fp] = values_by_sr.get(fp, 0) + values_by_sr['']
            del(values_by_sr[''])
        cls._set_values(metric_name, values_by_sr)


class LocationPromoMetrics(tdb_cassandra.View):
    _use_db = True
    _write_consistency_level = tdb_cassandra.CL.QUORUM
    _read_consistency_level = tdb_cassandra.CL.ONE
    _extra_schema_creation_args = {
        "default_validation_class": tdb_cassandra.IntegerType(),
    }

    @classmethod
    def _rowkey(cls, location):
        fields = [location.country, location.region, location.metro]
        return '-'.join(map(lambda field: field or '', fields))

    @classmethod
    def _column_name(cls, sr):
        return sr.name

    @classmethod
    def get(cls, srs, locations):
        srs, srs_is_single = tup(srs, ret_is_single=True)
        locations, locations_is_single = tup(locations, ret_is_single=True)
        is_single = srs_is_single and locations_is_single

        rowkeys = {location: cls._rowkey(location) for location in locations}
        columns = {sr: cls._column_name(sr) for sr in srs}
        rcl = cls._read_consistency_level
        metrics = cls._cf.multiget(rowkeys.values(), columns.values(),
                                   read_consistency_level=rcl)
        ret = {}

        for sr, location in product(srs, locations):
            rowkey = rowkeys[location]
            column = columns[sr]
            impressions = metrics.get(rowkey, {}).get(column, 0)
            ret[(sr, location)] = impressions

        if is_single:
            return ret.values()[0]
        else:
            return ret

    @classmethod
    def set(cls, metrics):
        wcl = cls._write_consistency_level
        with cls._cf.batch(write_consistency_level=wcl) as b:
            for location, sr, impressions in metrics:
                rowkey = cls._rowkey(location)
                column = {cls._column_name(sr): impressions}
                b.insert(rowkey, column)

########NEW FILE########
__FILENAME__ = query_cache
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
"""
This module provides a Cassandra-backed lockless query cache.  Rather than
doing complicated queries on the fly to populate listings, a list of items that
would be in that listing are maintained in Cassandra for fast lookup.  The
result can then be fed to IDBuilder to generate a final result.

Whenever an operation occurs that would modify the contents of the listing, the
listing should be updated somehow.  In some cases, this can be done by directly
mutating the listing and in others it must be done offline in batch processing
jobs.

"""

import json
import random
import datetime
import collections

from pylons import g
from pycassa.system_manager import ASCII_TYPE, UTF8_TYPE
from pycassa.batch import Mutator

from r2.models import Thing
from r2.lib.db import tdb_cassandra
from r2.lib.db.operators import asc, desc, BooleanOp
from r2.lib.db.sorts import epoch_seconds
from r2.lib.utils import flatten, to36


CONNECTION_POOL = g.cassandra_pools['main']
PRUNE_CHANCE = g.querycache_prune_chance
MAX_CACHED_ITEMS = 1000
LOG = g.log


class ThingTupleComparator(object):
    """A callable usable for comparing sort-data in a cached query.

    The query cache stores minimal sort data on each thing to be able to order
    the items in a cached query.  This class provides the ordering for those
    thing tuples.

    """

    def __init__(self, sorts):
        self.sorts = sorts

    def __call__(self, t1, t2):
        for i, s in enumerate(self.sorts):
            # t1 and t2 are tuples of (fullname, *sort_cols), so we
            # can get the value to compare right out of the tuple
            v1, v2 = t1[i + 1], t2[i + 1]
            if v1 != v2:
                return cmp(v1, v2) if isinstance(s, asc) else cmp(v2, v1)
        #they're equal
        return 0


class _CachedQueryBase(object):
    def __init__(self, sort):
        self.sort = sort
        self.sort_cols = [s.col for s in self.sort]
        self.data = []
        self._fetched = False

    def fetch(self, force=False):
        """Fill the cached query's sorted item list from Cassandra.

        If the query has already been fetched, this method is a no-op unless
        force=True.

        """
        if not force and self._fetched:
            return

        self._fetch()
        self._sort_data()
        self._fetched = True

    def _fetch(self):
        raise NotImplementedError()

    def _sort_data(self):
        comparator = ThingTupleComparator(self.sort_cols)
        self.data.sort(cmp=comparator)

    def __iter__(self):
        self.fetch()

        for x in self.data[:MAX_CACHED_ITEMS]:
            yield x[0]


class CachedQuery(_CachedQueryBase):
    """A materialized view of a complex query.

    Complicated queries can take way too long to sort in the databases.  This
    class provides a fast-access view of a given listing's items.  The cache
    stores each item's ID and a minimal subset of its data as required for
    sorting.

    Each time the listing is fetched, it is sorted. Because of this, we need to
    ensure the listing does not grow too large.  On each insert, a "pruning"
    can occur (with a configurable probability) which will remove excess items
    from the end of the listing.

    Use CachedQueryMutator to make changes to the cached query's item list.

    """

    def __init__(self, model, key, sort, filter_fn, is_precomputed):
        self.model = model
        self.key = key
        self.filter = filter_fn
        self.timestamps = None  # column timestamps, for safe pruning
        self.is_precomputed = is_precomputed
        super(CachedQuery, self).__init__(sort)

    def _make_item_tuple(self, item):
        """Return an item tuple from the result of a query.

        The item tuple is used to sort the items in a query without having to
        look them up.

        """
        filtered_item = self.filter(item)
        lst = [filtered_item._fullname]
        for col in self.sort_cols:
            # take the property of the original
            attr = getattr(item, col)
            # convert dates to epochs to take less space
            if isinstance(attr, datetime.datetime):
                attr = epoch_seconds(attr)
            lst.append(attr)
        return tuple(lst)

    def _fetch(self):
        self._fetch_multi([self])

    @classmethod
    def _fetch_multi(self, queries):
        """Fetch the unsorted query results for multiple queries at once.

        In the case of precomputed queries, do an extra lookup first to
        determine which row key to find the latest precomputed values for the
        query in.

        """

        by_model = collections.defaultdict(list)
        for q in queries:
            by_model[q.model].append(q)

        cached_queries = {}
        for model, queries in by_model.iteritems():
            pure, need_mangling = [], []
            for q in queries:
                if not q.is_precomputed:
                    pure.append(q.key)
                else:
                    need_mangling.append(q.key)
            mangled = model.index_mangle_keys(need_mangling)
            fetched = model.get(pure + mangled)
            cached_queries.update(fetched)

        for q in queries:
            cached_query = cached_queries.get(q.key)
            if cached_query:
                q.data, q.timestamps = cached_query

    def _insert(self, mutator, things):
        if not things:
            return

        values = {}
        for thing in things:
            t = self._make_item_tuple(thing)
            values[t[0]] = tuple(t[1:])

        self.model.insert(mutator, self.key, values)

    def _delete(self, mutator, things):
        if not things:
            return

        fullnames = [self.filter(x)._fullname for x in things]
        self.model.remove(mutator, self.key, fullnames)

    def _prune(self, mutator):
        extraneous_ids = [t[0] for t in self.data[MAX_CACHED_ITEMS:]]

        if extraneous_ids:
            # if something has gone wrong with previous prunings, there may be
            # a lot of extraneous items.  we'll limit this pruning to the
            # oldest N items to avoid a dangerously large operation.
            # N = the average number of items to prune (doubled for safety)
            prune_size = int(MAX_CACHED_ITEMS * PRUNE_CHANCE) * 2
            extraneous_ids = extraneous_ids[-prune_size:]

            self.model.remove_if_unchanged(mutator, self.key,
                                           extraneous_ids, self.timestamps)

            cf_name = self.model.__name__
            query_name = self.key.split('.')[0]
            counter_key = "cache.%s.%s" % (cf_name, query_name)
            counter = g.stats.get_counter(counter_key)
            if counter:
                counter.increment('pruned', delta=len(extraneous_ids))

    @classmethod
    def _prune_multi(cls, queries):
        cls._fetch_multi(queries)

        with Mutator(CONNECTION_POOL) as m:
            for q in queries:
                q._sort_data()
                q._prune(m)

    def __hash__(self):
        return hash(self.key)

    def __eq__(self, other):
        return self.key == other.key

    def __ne__(self, other):
        return not self.__eq__(other)

    def __repr__(self):
        return "%s(%s, %r)" % (self.__class__.__name__,
                               self.model.__name__, self.key)


class MergedCachedQuery(_CachedQueryBase):
    """A cached query built by merging multiple sub-queries.

    Merged queries can be read, but cannot be modified as it is not easy to
    determine from a given item which sub-query should get modified.

    """

    def __init__(self, queries):
        self.queries = queries

        if queries:
            sort = queries[0].sort
            assert all(sort == q.sort for q in queries)
        else:
            sort = []
        super(MergedCachedQuery, self).__init__(sort)

    def _fetch(self):
        CachedQuery._fetch_multi(self.queries)
        self.data = flatten([q.data for q in self.queries])


class CachedQueryMutator(object):
    """Utility to manipulate cached queries with batching.

    This implements the context manager protocol so it can be used with the
    with statement for clean batches.

    """

    def __init__(self):
        self.mutator = Mutator(CONNECTION_POOL)
        self.to_prune = set()

    def __enter__(self):
        return self

    def __exit__(self, type, value, traceback):
        self.send()

    def insert(self, query, things):
        """Insert items into the given cached query.

        If the items are already in the query, they will have their sorts
        updated.

        This will sometimes trigger pruning with a configurable probability
        (see g.querycache_prune_chance).

        """
        if not things:
            return

        LOG.debug("Inserting %r into query %r", things, query)

        assert not query.is_precomputed
        query._insert(self.mutator, things)

        if (random.random() / len(things)) < PRUNE_CHANCE:
            self.to_prune.add(query)

    def delete(self, query, things):
        """Remove things from the query."""
        if not things:
            return

        LOG.debug("Deleting %r from query %r", things, query)

        query._delete(self.mutator, things)

    def send(self):
        """Commit the mutations batched up so far and potentially do pruning.

        This is automatically called by __exit__ when used as a context
        manager.

        """
        self.mutator.send()

        if self.to_prune:
            LOG.debug("Pruning queries %r", self.to_prune)
            CachedQuery._prune_multi(self.to_prune)


def filter_identity(x):
    """Return the same thing given.

    Use this as the filter_fn of simple Thing-based cached queries so that
    the enumerated things will be returned for rendering.

    """
    return x


def filter_thing2(x):
    """Return the thing2 of a given relationship.

    Use this as the filter_fn of a cached Relation query so that the related
    things will be returned for rendering.

    """
    return x._thing2


def filter_thing(x):
    """Return "thing" from a proxy object.

    Use this as the filter_fn when some object that's not a Thing or Relation
    is used as the basis of a cached query.

    """
    return x.thing


def _is_query_precomputed(query):
    """Return if this query must be updated offline in a batch job.

    Simple queries can be modified in place in the query cache, but ones
    with more complicated eligibility criteria, such as a time limit ("top
    this month") cannot be modified this way and must instead be
    recalculated periodically.  Rather than replacing a single row
    repeatedly, the precomputer stores in a new row every time it runs and
    updates an index of the latest run.

    """

    # visit all the nodes in the rule tree to see if there are time limitations
    # if we find one, this query is one that must be precomputed
    rules = list(query._rules)
    while rules:
        rule = rules.pop()

        if isinstance(rule, BooleanOp):
            rules.extend(rule.ops)
            continue

        if rule.lval.name == "_date":
            return True
    return False


def cached_query(model, filter_fn=filter_identity, sort=None):
    """Decorate a function describing a cached query.

    The decorated function is expected to follow the naming convention common
    in queries.py -- "get_something".  The cached query's key will be generated
    from the combination of the function name and its arguments separated by
    periods.

    There are currently two types of cached queries: SQL-backed and
    pure-Cassandra.  In the prior case, sort should be None and the decorated
    function should return a Things/Relations query that would be used in the
    absence of the query cache.  In the pure-Cassandra case, the sort field is
    used for ranking the returned items should be provided in sort and the
    return value of the decorated function is ignored.

    """
    def cached_query_decorator(fn):
        def cached_query_wrapper(*args):
            # build the row key from the function name and arguments
            assert fn.__name__.startswith("get_")
            row_key_components = [fn.__name__[len('get_'):]]

            if len(args) > 0:
                # we want to accept either a Thing or a thing's ID at this
                # layer, but the query itself should always get just an ID
                if isinstance(args[0], Thing):
                    args = list(args)
                    args[0] = args[0]._id

                thing_id = to36(args[0])
                row_key_components.append(thing_id)

            row_key_components.extend(str(x) for x in args[1:])
            row_key = '.'.join(row_key_components)

            query = fn(*args)

            if query:
                # sql-backed query
                query_sort = query._sort
                is_precomputed = _is_query_precomputed(query)
            else:
                # pure-cassandra query
                assert sort
                query_sort = sort
                is_precomputed = False

            return CachedQuery(model, row_key, query_sort, filter_fn,
                               is_precomputed)
        return cached_query_wrapper
    return cached_query_decorator


def merged_cached_query(fn):
    """Decorate a function describing a cached query made up of others.

    The decorated function should return a sequence of cached queries whose
    results will be merged together into a final listing.

    """
    def merge_wrapper(*args, **kwargs):
        queries = fn(*args, **kwargs)
        return MergedCachedQuery(queries)
    return merge_wrapper


class _BaseQueryCache(object):
    """The model through which cached queries to interact with Cassandra.

    Each cached query is stored as a distinct row in Cassandra.  The row key is
    given by higher level code (see the cached_query decorator above).  Each
    item in the materialized result of the query is stored as a separate
    column.  Each column name is the fullname of the item, while each value is
    the stuff CachedQuery needs to be able to sort the items (see
    CachedQuery._make_item_tuple).

    """

    __metaclass__ = tdb_cassandra.ThingMeta
    _connection_pool = 'main'
    _extra_schema_creation_args = dict(key_validation_class=ASCII_TYPE,
                                       default_validation_class=UTF8_TYPE)
    _compare_with = ASCII_TYPE
    _use_db = False
    _type_prefix = None
    _cf_name = None

    @classmethod
    def get(cls, keys):
        """Retrieve the items in a set of cached queries.

        For each cached query, this returns the thing tuples and the column
        timestamps for them.  The latter is useful for conditional removal
        during pruning.

        """
        rows = cls._cf.multiget(keys, include_timestamp=True,
                                column_count=tdb_cassandra.max_column_count)

        res = {}
        for row, columns in rows.iteritems():
            data = []
            timestamps = []

            for (key, (value, timestamp)) in columns.iteritems():
                value = json.loads(value)
                data.append((key,) + tuple(value))
                timestamps.append((key, timestamp))

            res[row] = (data, dict(timestamps))

        return res

    @classmethod
    def index_mangle_keys(cls, keys):
        if not keys:
            return []

        index_keys = ["/".join((key, "index")) for key in keys]
        rows = cls._cf.multiget(index_keys,
                                column_reversed=True,
                                column_count=1)

        res = []
        for key, columns in rows.iteritems():
            index_component = columns.keys()[0]
            res.append("/".join((key, index_component)))
        return res

    @classmethod
    @tdb_cassandra.will_write
    def insert(cls, mutator, key, columns):
        """Insert things into the cached query.

        This works as an upsert; if the thing already exists, it is updated. If
        not, it is actually inserted.

        """
        updates = dict((key, json.dumps(value))
                       for key, value in columns.iteritems())
        mutator.insert(cls._cf, key, updates)

    @classmethod
    @tdb_cassandra.will_write
    def remove(cls, mutator, key, columns):
        """Unconditionally remove things from the cached query."""
        mutator.remove(cls._cf, key, columns=columns)

    @classmethod
    @tdb_cassandra.will_write
    def remove_if_unchanged(cls, mutator, key, columns, timestamps):
        """Remove things from the cached query if unchanged.

        If the things have been changed since the specified timestamps, they
        will not be removed.  This is useful for avoiding race conditions while
        pruning.

        """
        for col in columns:
            mutator.remove(cls._cf, key, columns=[col],
                           timestamp=timestamps.get(col))


class UserQueryCache(_BaseQueryCache):
    """A query cache column family for user-keyed queries."""
    _use_db = True


class SubredditQueryCache(_BaseQueryCache):
    """A query cache column family for subreddit-keyed queries."""
    _use_db = True

########NEW FILE########
__FILENAME__ = recommend
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import pycassa
import time

from collections import defaultdict
from datetime import datetime, timedelta
from itertools import chain
from pylons import g

from r2.lib.db import tdb_cassandra
from r2.lib.db.tdb_cassandra import max_column_count
from r2.lib.utils import utils, tup
from r2.models import Account, LabeledMulti, Subreddit
from r2.lib.pages import ExploreItem

VIEW = 'imp'
CLICK = 'clk'
DISMISS = 'dis'
FEEDBACK_ACTIONS = [VIEW, CLICK, DISMISS]

# how long to keep each type of feedback
FEEDBACK_TTL = {VIEW: timedelta(hours=6).total_seconds(),  # link lifetime
                CLICK: timedelta(minutes=30).total_seconds(),  # one session
                DISMISS: timedelta(days=60).total_seconds()}  # two months


class AccountSRPrefs(object):
    """Class for managing user recommendation preferences.

    Builds a user profile on-the-fly based on the user's subscriptions,
    multireddits, and recent interactions with the recommender UI.

    Likes are used to generate recommendations, dislikes to filter out
    unwanted results, and recent views to make sure the same subreddits aren't
    recommended too often.

    """

    def __init__(self):
        self.likes = set()
        self.dislikes = set()
        self.recent_views = set()

    @classmethod
    def for_user(cls, account):
        """Return a new AccountSRPrefs obj populated with user's data."""
        prefs = cls()
        multis = LabeledMulti.by_owner(account)
        multi_srs = set(chain.from_iterable(multi.srs for multi in multis))
        feedback = AccountSRFeedback.for_user(account)
        # subscriptions and srs in the user's multis become likes
        subscriptions = Subreddit.user_subreddits(account, limit=None)
        prefs.likes.update(utils.to36(sr_id) for sr_id in subscriptions)
        prefs.likes.update(sr._id36 for sr in multi_srs)
        # recent clicks on explore tab items are also treated as likes
        prefs.likes.update(feedback[CLICK])
        # dismissed recommendations become dislikes
        prefs.dislikes.update(feedback[DISMISS])
        # dislikes take precedence over likes
        prefs.likes = prefs.likes.difference(prefs.dislikes)
        # recently recommended items won't be shown again right away
        prefs.recent_views.update(feedback[VIEW])
        return prefs


class AccountSRFeedback(tdb_cassandra.DenormalizedRelation):
    """Column family for storing users' recommendation feedback."""

    _use_db = True
    _views = []
    _write_last_modified = False
    _read_consistency_level = tdb_cassandra.CL.QUORUM
    _write_consistency_level = tdb_cassandra.CL.QUORUM

    @classmethod
    def for_user(cls, account):
        """Return dict mapping each feedback type to a set of sr id36s."""

        feedback = defaultdict(set)
        try:
            row = AccountSRFeedback._cf.get(account._id36,
                                            column_count=max_column_count)
        except pycassa.NotFoundException:
            return feedback
        for colkey, colval in row.iteritems():
            action, sr_id36 = colkey.split('.')
            feedback[action].add(sr_id36)
        return feedback

    @classmethod
    def record_feedback(cls, account, srs, action):
        if action not in FEEDBACK_ACTIONS:
            g.log.error('Unrecognized feedback: %s' % action)
            return
        srs = tup(srs)
        # update user feedback record, setting appropriate ttls
        fb_rowkey = account._id36
        fb_colkeys = ['%s.%s' % (action, sr._id36) for sr in srs]
        col_data = {col: '' for col in fb_colkeys}
        ttl = FEEDBACK_TTL.get(action, 0)
        if ttl > 0:
            AccountSRFeedback._cf.insert(fb_rowkey, col_data, ttl=ttl)
        else:
            AccountSRFeedback._cf.insert(fb_rowkey, col_data)

    @classmethod
    def record_views(cls, account, srs):
        cls.record_feedback(account, srs, VIEW)


class ExploreSettings(tdb_cassandra.Thing):
    """Column family for storing users' view prefs for the /explore page."""
    _use_db = True
    _bool_props = ('personalized', 'discovery', 'rising', 'nsfw')

    @classmethod
    def for_user(cls, account):
        """Return user's prefs or default prefs if user has none."""
        try:
            return cls._byID(account._id36)
        except tdb_cassandra.NotFound:
            return DefaultExploreSettings()

    @classmethod
    def record_settings(cls,
                        user,
                        personalized=False,
                        discovery=False,
                        rising=False,
                        nsfw=False):
        """Update or create settings for user."""
        try:
            settings = cls._byID(user._id36)
        except tdb_cassandra.NotFound:
            settings = ExploreSettings(
                _id=user._id36,
                personalized=personalized,
                discovery=discovery,
                rising=rising,
                nsfw=nsfw,
            )
        else:
            settings.personalized = personalized
            settings.discovery = discovery
            settings.rising = rising
            settings.nsfw = nsfw
        settings._commit()


class DefaultExploreSettings(object):
    """Default values to use when no settings have been saved for the user."""
    def __init__(self):
        self.personalized = True
        self.discovery = True
        self.rising = True
        self.nsfw = False

########NEW FILE########
__FILENAME__ = report
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.lib.db.thing import Thing, Relation, MultiRelation, thing_prefix
from r2.lib.utils import tup
from r2.lib.memoize import memoize
from r2.models import Link, Comment, Message, Subreddit, Account
from r2.models.vote import score_changes
from datetime import datetime

from pylons import g

class Report(MultiRelation('report',
                           Relation(Account, Link),
                           Relation(Account, Comment),
                           Relation(Account, Subreddit),
                           Relation(Account, Message)
                           )):

    _field = 'reported'

    @classmethod
    def new(cls, user, thing):
        from r2.lib.db import queries

        # check if this report exists already!
        rel = cls.rel(user, thing)
        q = rel._fast_query(user, thing, ['-1', '0', '1'])
        q = [ report for (tupl, report) in q.iteritems() if report ]
        if q:
            # stop if we've seen this before, so that we never get the
            # same report from the same user twice
            oldreport = q[0]
            g.log.debug("Ignoring duplicate report %s" % oldreport)
            return oldreport

        r = Report(user, thing, '0')
        if not thing._loaded:
            thing._load()

        # mark item as reported
        try:
            thing._incr(cls._field)
        except (ValueError, TypeError):
            g.log.error("%r has bad field %r = %r" % (thing, cls._field,
                         getattr(thing, cls._field, "(nonexistent)")))
            raise

        r._commit()

        if hasattr(thing, 'author_id'):
            author = Account._byID(thing.author_id, data=True)
            author._incr('reported')

        if not getattr(thing, "ignore_reports", False):
            # update the reports queue if it exists
            queries.new_report(thing, r)

            # if the thing is already marked as spam, accept the report
            if thing._spam:
                cls.accept(thing)

        return r

    @classmethod
    def for_thing(cls, thing):
        rel = cls.rel(Account, thing.__class__)
        rels = rel._query(rel.c._thing2_id == thing._id)

        return list(rels)

    @classmethod
    def accept(cls, things, correct = True):
        from r2.lib.db import queries

        things = tup(things)

        things_by_cls = {}
        for thing in things:
            things_by_cls.setdefault(thing.__class__, []).append(thing)

        to_clear = []

        for thing_cls, cls_things in things_by_cls.iteritems():
            # look up all of the reports for each thing
            rel_cls = cls.rel(Account, thing_cls)
            thing_ids = [t._id for t in cls_things]
            rels = rel_cls._query(rel_cls.c._thing2_id == thing_ids)
            for r in rels:
                if r._name == '0':
                    r._name = '1' if correct else '-1'
                    r._commit()

            for thing in cls_things:
                if thing.reported > 0:
                    thing.reported = 0
                    thing._commit()
                    to_clear.append(thing)

        queries.clear_reports(to_clear, rels)


########NEW FILE########
__FILENAME__ = subreddit
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from __future__ import with_statement

import base64
import collections
import datetime
import hashlib
import itertools
import json

from pylons import c, g
from pylons.i18n import _

from r2.lib.db.thing import Thing, Relation, NotFound
from account import Account, AccountsActiveBySR
from printable import Printable
from r2.lib.db.userrel import UserRel
from r2.lib.db.operators import lower, or_, and_, desc
from r2.lib.errors import UserRequiredException
from r2.lib.memoize import memoize
from r2.lib.permissions import ModeratorPermissionSet
from r2.lib.utils import tup, last_modified_multi, fuzz_activity
from r2.lib.utils import timeago, summarize_markdown
from r2.lib.cache import sgm, TransitionalCache
from r2.lib.strings import strings, Score
from r2.lib.filters import _force_unicode
from r2.lib.db import tdb_cassandra
from r2.models.wiki import WikiPage, ImagesByWikiPage

from r2.lib.merge import ConflictException
from r2.lib.cache import CL_ONE
from r2.lib import hooks
from r2.models.query_cache import MergedCachedQuery
import pycassa

from r2.lib.utils import set_last_modified
from r2.models.wiki import WikiPage
import os.path
import random


def get_links_sr_ids(sr_ids, sort, time):
    from r2.lib.db import queries

    if not sr_ids:
        return []
    else:
        srs = Subreddit._byID(sr_ids, data=True, return_dict = False)

    results = [queries.get_links(sr, sort, time)
               for sr in srs]
    return queries.merge_results(*results)


class BaseSite(object):
    _defaults = dict(
        static_path=g.static_path,
        stylesheet=None,
        stylesheet_hash='',
        header=None,
        header_title='',
    )

    def __getattr__(self, name):
        if name in self._defaults:
            return self._defaults[name]
        raise AttributeError

    @property
    def path(self):
        return "/r/%s/" % self.name

    @property
    def user_path(self):
        return self.path

    @property
    def analytics_name(self):
        return self.name

    def is_moderator_with_perms(self, user, *perms):
        rel = self.is_moderator(user)
        if rel:
            return all(rel.has_permission(perm) for perm in perms)

    def is_limited_moderator(self, user):
        rel = self.is_moderator(user)
        return bool(rel and not rel.is_superuser())

    def is_unlimited_moderator(self, user):
        rel = self.is_moderator(user)
        return bool(rel and rel.is_superuser())

    def get_links(self, sort, time):
        from r2.lib.db import queries
        return queries.get_links(self, sort, time)

    def get_spam(self, include_links=True, include_comments=True):
        from r2.lib.db import queries
        return queries.get_spam(self, user=c.user, include_links=include_links,
                                include_comments=include_comments)

    def get_reported(self, include_links=True, include_comments=True):
        from r2.lib.db import queries
        return queries.get_reported(self, user=c.user,
                                    include_links=include_links,
                                    include_comments=include_comments)

    def get_modqueue(self, include_links=True, include_comments=True):
        from r2.lib.db import queries
        return queries.get_modqueue(self, user=c.user,
                                    include_links=include_links,
                                    include_comments=include_comments)

    def get_unmoderated(self):
        from r2.lib.db import queries
        return queries.get_unmoderated(self, user=c.user)

    def get_all_comments(self):
        from r2.lib.db import queries
        return queries.get_sr_comments(self)

    def get_gilded(self):
        from r2.lib.db import queries
        return queries.get_gilded(self._id)

    @classmethod
    def get_modactions(cls, srs, mod=None, action=None):
        # Get a query that will yield ModAction objects with mod and action
        from r2.models import ModAction
        return ModAction.get_actions(srs, mod=mod, action=action)

    def get_live_promos(self):
        raise NotImplementedError

    @property
    def stylesheet_url(self):
        from r2.lib.template_helpers import get_domain
        return "//%s/stylesheet.css?v=%s" % (get_domain(cname=False,
                                                        subreddit=True),
                                             self.stylesheet_hash)


class SubredditExists(Exception): pass


class Subreddit(Thing, Printable, BaseSite):
    # Note: As of 2010/03/18, nothing actually overrides the static_path
    # attribute, even on a cname. So c.site.static_path should always be
    # the same as g.static_path.
    _defaults = dict(BaseSite._defaults,
        stylesheet_contents='',
        stylesheet_contents_secure='',
        stylesheet_modified=None,
        stylesheet_url_http="",
        stylesheet_url_https="",
        header_size=None,
        allow_top=False, # overridden in "_new"
        reported=0,
        valid_votes=0,
        show_media=False,
        show_cname_sidebar=False,
        css_on_cname=True,
        domain=None,
        wikimode="disabled",
        wiki_edit_karma=100,
        wiki_edit_age=0,
        over_18=False,
        exclude_banned_modqueue=False,
        mod_actions=0,
        # do we allow self-posts, links only, or any?
        link_type='any', # one of ('link', 'self', 'any')
        sticky_fullname=None,
        submit_link_label='',
        submit_text_label='',
        comment_score_hide_mins=0,
        flair_enabled=True,
        flair_position='right', # one of ('left', 'right')
        link_flair_position='', # one of ('', 'left', 'right')
        flair_self_assign_enabled=False,
        link_flair_self_assign_enabled=False,
        use_quotas=True,
        description="",
        public_description="",
        submit_text="",
        allow_gilding=True,
        hide_subscribers=False,
        public_traffic=False,
        spam_links='high',
        spam_selfposts='high',
        spam_comments='low',
        archive_age=g.ARCHIVE_AGE,
    )
    _essentials = ('type', 'name', 'lang')
    _data_int_props = Thing._data_int_props + ('mod_actions', 'reported',
                                               'wiki_edit_karma', 'wiki_edit_age')

    sr_limit = 50
    gold_limit = 100
    DEFAULT_LIMIT = object()

    MAX_SRNAME_LENGTH = 200 # must be less than max memcached key length

    # note: for purposely unrenderable reddits (like promos) set author_id = -1
    @classmethod
    def _new(cls, name, title, author_id, ip, lang = g.lang, type = 'public',
             over_18 = False, **kw):
        with g.make_lock("create_sr", 'create_sr_' + name.lower()):
            try:
                sr = Subreddit._by_name(name)
                raise SubredditExists
            except NotFound:
                if "allow_top" not in kw:
                    kw['allow_top'] = True
                sr = Subreddit(name = name,
                               title = title,
                               lang = lang,
                               type = type,
                               over_18 = over_18,
                               author_id = author_id,
                               ip = ip,
                               **kw)
                sr._commit()

                #clear cache
                Subreddit._by_name(name, _update = True)
                return sr


    _specials = {}

    @classmethod
    def _by_name(cls, names, stale=False, _update = False):
        '''
        Usages: 
        1. Subreddit._by_name('funny') # single sr name
        Searches for a single subreddit. Returns a single Subreddit object or 
        raises NotFound if the subreddit doesn't exist.
        2. Subreddit._by_name(['aww','iama']) # list of sr names
        Searches for a list of subreddits. Returns a dict mapping srnames to 
        Subreddit objects. Items that were not found are ommitted from the dict.
        If no items are found, an empty dict is returned.
        '''
        #lower name here so there is only one cache
        names, single = tup(names, True)

        to_fetch = {}
        ret = {}

        for name in names:
            ascii_only = str(name.decode("ascii", errors="ignore"))
            lname = ascii_only.lower()

            if lname in cls._specials:
                ret[name] = cls._specials[lname]
            elif len(lname) > Subreddit.MAX_SRNAME_LENGTH:
                g.log.debug("Subreddit._by_name() ignoring invalid srname (too long): %s", lname)
            else:
                to_fetch[lname] = name

        if to_fetch:
            def _fetch(lnames):
                q = cls._query(lower(cls.c.name) == lnames,
                               cls.c._spam == (True, False),
                               limit = len(lnames),
                               data=True)
                try:
                    srs = list(q)
                except UnicodeEncodeError:
                    print "Error looking up SRs %r" % (lnames,)
                    raise

                return dict((sr.name.lower(), sr._id)
                            for sr in srs)

            srs = {}
            srids = sgm(g.cache, to_fetch.keys(), _fetch, prefix='subreddit.byname', stale=stale)
            if srids:
                srs = cls._byID(srids.values(), data=True, return_dict=False, stale=stale)

            for sr in srs:
                ret[to_fetch[sr.name.lower()]] = sr

        if ret and single:
            return ret.values()[0]
        elif not ret and single:
            raise NotFound, 'Subreddit %s' % name
        else:
            return ret

    @classmethod
    @memoize('subreddit._by_domain')
    def _by_domain_cache(cls, name):
        q = cls._query(cls.c.domain == name,
                       limit = 1)
        l = list(q)
        if l:
            return l[0]._id

    @classmethod
    def _by_domain(cls, domain, _update = False):
        sr_id = cls._by_domain_cache(_force_unicode(domain).lower(),
                                     _update = _update)
        if sr_id:
            return cls._byID(sr_id, True)
        else:
            return None

    @property
    def allowed_types(self):
        if self.link_type == "any":
            return set(("link", "self"))
        return set((self.link_type,))

    def add_moderator(self, user, **kwargs):
        if not user.modmsgtime:
            user.modmsgtime = False
            user._commit()
        return super(Subreddit, self).add_moderator(user, **kwargs)

    def remove_moderator(self, user, **kwargs):
        ret = super(Subreddit, self).remove_moderator(user, **kwargs)

        is_mod_somewhere = bool(Subreddit.reverse_moderator_ids(user))
        if not is_mod_somewhere:
            user.modmsgtime = None
            user._commit()

        return ret

    @property
    def moderators(self):
        return self.moderator_ids()

    def moderators_with_perms(self):
        return collections.OrderedDict(
            (r._thing2_id, r.get_permissions())
            for r in self.each_moderator())

    def moderator_invites_with_perms(self):
        return collections.OrderedDict(
            (r._thing2_id, r.get_permissions())
            for r in self.each_moderator_invite())

    @property
    def stylesheet_contents_user(self):
        try:
            return WikiPage.get(self, 'config/stylesheet')._get('content','')
        except tdb_cassandra.NotFound:
           return  self._t.get('stylesheet_contents_user')

    @property
    def prev_stylesheet(self):
        try:
            return WikiPage.get(self, 'config/stylesheet')._get('revision','')
        except tdb_cassandra.NotFound:
            return ''

    @property
    def contributors(self):
        return self.contributor_ids()

    @property
    def banned(self):
        return self.banned_ids()
    
    @property
    def wikibanned(self):
        return self.wikibanned_ids()
    
    @property
    def wikicontributor(self):
        return self.wikicontributor_ids()
    
    @property
    def _should_wiki(self):
        return True

    @property
    def subscribers(self):
        return self.subscriber_ids()

    @property
    def flair(self):
        return self.flair_ids()

    @property
    def accounts_active(self):
        return self.get_accounts_active()[0]

    @property
    def wiki_use_subreddit_karma(self):
        return True

    def get_accounts_active(self):
        fuzzed = False
        count = AccountsActiveBySR.get_count(self)
        key = 'get_accounts_active-' + self._id36

        # Fuzz counts having low values, for privacy reasons
        if count < 100 and not c.user_is_admin:
            fuzzed = True
            cached_count = g.cache.get(key)
            if not cached_count:
                count = fuzz_activity(count)
                g.cache.set(key, count, time=5*60)
            else:
                count = cached_count
        return count, fuzzed

    def spammy(self):
        return self._spam

    def is_contributor(self, user):
        if self.name.lower() == g.lounge_reddit.lower():
            return user.gold or user.gold_charter
        else:
            return super(Subreddit, self).is_contributor(user)

    def can_comment(self, user):
        if c.user_is_admin:
            return True

        override = hooks.get_hook("subreddit.can_comment").call_until_return(
                                                            sr=self, user=user)

        if override is not None:
            return override
        elif self.is_banned(user):
            return False
        elif self.type == 'gold_restricted' and user.gold:
            return True
        elif self.type in ('public','restricted'):
            return True
        elif self.is_moderator(user) or self.is_contributor(user):
            #private requires contributorship
            return True
        else:
            return False

    def wiki_can_submit(self, user):
        return self.can_submit(user)

    def can_submit(self, user, promotion=False):
        if c.user_is_admin:
            return True
        elif self.is_banned(user) and not promotion:
            return False
        elif self.spammy():
            return False
        elif self.type == 'public':
            return True
        elif self.is_moderator(user) or self.is_contributor(user):
            #restricted/private require contributorship
            return True
        elif self.type == 'gold_restricted' and user.gold:
            return True
        elif self.type == 'restricted' and promotion:
            return True
        else:
            return False

    def can_submit_link(self, user):
        if c.user_is_admin or self.is_moderator_with_perms(user, "posts"):
            return True
        return "link" in self.allowed_types

    def can_submit_text(self, user):
        if c.user_is_admin or self.is_moderator_with_perms(user, "posts"):
            return True
        return "self" in self.allowed_types

    def can_ban(self, user):
        return (user
                and (c.user_is_admin
                     or self.is_moderator_with_perms(user, 'posts')))

    def can_distinguish(self,user):
        return (user
                and (c.user_is_admin
                     or self.is_moderator_with_perms(user, 'posts')))

    def can_change_stylesheet(self, user):
        if c.user_is_loggedin:
            return (
                c.user_is_admin or self.is_moderator_with_perms(user, 'config'))
        else:
            return False
    
    def parse_css(self, content, verify=True):
        from r2.lib import cssfilter
        if g.css_killswitch or (verify and not self.can_change_stylesheet(c.user)):
            return (None, None)

        if not content:
            return ([], ("", ""))

        # parse in regular old http mode
        http_images = ImagesByWikiPage.get_images(self, "config/stylesheet")
        parsed_http, errors_http = cssfilter.validate_css(
            content,
            http_images,
        )

        # parse and resolve images with https-safe urls
        https_images = {name: g.media_provider.convert_to_https(url)
                        for name, url in http_images.iteritems()}
        parsed_https, errors_https = cssfilter.validate_css(
            content,
            https_images,
        )

        # the two reports should be identical so we'll just return the http one
        return (errors_http, (parsed_http, parsed_https))

    def change_css(self, content, parsed, prev=None, reason=None, author=None, force=False):
        from r2.models import ModAction
        from r2.lib.media import upload_stylesheet

        author = author if author else c.user._id36
        if content is None:
            content = ''
        try:
            wiki = WikiPage.get(self, 'config/stylesheet')
        except tdb_cassandra.NotFound:
            wiki = WikiPage.create(self, 'config/stylesheet')
        wr = wiki.revise(content, previous=prev, author=author, reason=reason, force=force)

        minified_http, minified_https = parsed
        if minified_http or minified_https:
            if g.subreddit_stylesheets_static:
                self.stylesheet_url_http = upload_stylesheet(minified_http)
                self.stylesheet_url_https = g.media_provider.convert_to_https(
                                             upload_stylesheet(minified_https))
                self.stylesheet_hash = ""
                self.stylesheet_contents = ""
                self.stylesheet_contents_secure = ""
                self.stylesheet_modified = None
            else:
                self.stylesheet_url_http = ""
                self.stylesheet_url_https = ""
                self.stylesheet_hash = hashlib.md5(minified_https).hexdigest()
                self.stylesheet_contents = minified_http
                self.stylesheet_contents_secure = minified_https
                self.stylesheet_modified = datetime.datetime.now(g.tz)
        else:
            self.stylesheet_url_http = ""
            self.stylesheet_url_https = ""
            self.stylesheet_contents = ""
            self.stylesheet_contents_secure = ""
            self.stylesheet_hash = ""
            self.stylesheet_modified = datetime.datetime.now(g.tz)
        self.stylesheet_contents_user = ""  # reads from wiki; ensure pg clean
        self._commit()

        ModAction.create(self, c.user, action='wikirevise', details='Updated subreddit stylesheet')
        return wr

    def is_special(self, user):
        return (user
                and (c.user_is_admin
                     or self.is_moderator(user)
                     or self.is_contributor(user)))

    def can_give_karma(self, user):
        return self.is_special(user)

    def should_ratelimit(self, user, kind):
        from r2.models.admintools import admin_ratelimit
        if (c.user_is_admin or
            self.is_special(user) or
            not admin_ratelimit(user)):
            return False

        if kind == 'comment':
            rl_karma = g.MIN_RATE_LIMIT_COMMENT_KARMA
        else:
            rl_karma = g.MIN_RATE_LIMIT_KARMA

        return user.karma(kind, self) < rl_karma

    def can_view(self, user):
        if c.user_is_admin:
            return True
        
        if self.spammy():
            return False
        elif self.type in ('public', 'restricted',
                           'gold_restricted', 'archived'):
            return True
        elif c.user_is_loggedin:
            return (self.is_contributor(user) or
                    self.is_moderator(user) or
                    self.is_moderator_invite(user))

    def can_demod(self, bully, victim):
        bully_rel = self.get_moderator(bully)
        if bully_rel is not None and bully == victim:
            # mods can always demod themselves
            return True
        victim_rel = self.get_moderator(victim)
        return (
            bully_rel is not None
            and victim_rel is not None
            and bully_rel.is_superuser()  # limited mods can't demod
            and bully_rel._date <= victim_rel._date)

    @classmethod
    def load_subreddits(cls, links, return_dict = True, stale=False):
        """returns the subreddits for a list of links. it also preloads the
        permissions for the current user."""
        srids = set(l.sr_id for l in links
                    if getattr(l, "sr_id", None) is not None)
        subreddits = {}
        if srids:
            subreddits = cls._byID(srids, data=True, stale=stale)

        if subreddits and c.user_is_loggedin:
            # dict( {Subreddit,Account,name} -> Relationship )
            SRMember._fast_query(subreddits.values(), (c.user,), ('moderator',),
                                 data=True)

        return subreddits if return_dict else subreddits.values()

    def keep_for_rising(self, sr_id):
        """Return whether or not to keep a thing in rising for this SR."""
        return sr_id == self._id

    @classmethod
    def add_props(cls, user, wrapped):
        names = ('subscriber', 'moderator', 'contributor')
        rels = (SRMember._fast_query(wrapped, [user], names) if c.user_is_loggedin else {})
        defaults = Subreddit.default_subreddits()
        target = "_top" if c.cname else None
        for item in wrapped:
            if not user or not user.has_subscribed:
                item.subscriber = item._id in defaults
            else:
                item.subscriber = bool(rels.get((item, user, 'subscriber')))
            item.moderator = bool(rels.get((item, user, 'moderator')))
            item.contributor = bool(item.type != 'public' and
                                    (item.moderator or
                                     rels.get((item, user, 'contributor'))))

            if item.hide_subscribers and not c.user_is_admin:
                item._ups = 0

            item.score = item._ups

            # override "voting" score behavior (it will override the use of
            # item.score in builder.py to be ups-downs)
            item.likes = item.subscriber or None
            base_score = item.score - (1 if item.likes else 0)
            item.voting_score = [(base_score + x - 1) for x in range(3)]
            item.score_fmt = Score.subscribers

            #will seem less horrible when add_props is in pages.py
            from r2.lib.pages import UserText
            item.description_usertext = UserText(item, item.description, target=target)
            if item.public_description or item.description:
                text = (item.public_description or
                        summarize_markdown(item.description))
                item.public_description_usertext = UserText(item,
                                                            text,
                                                            target=target)
            else:
                item.public_description_usertext = None


        Printable.add_props(user, wrapped)
    #TODO: make this work
    cache_ignore = set(["subscribers"]).union(Printable.cache_ignore)
    @staticmethod
    def wrapped_cache_key(wrapped, style):
        s = Printable.wrapped_cache_key(wrapped, style)
        s.extend([wrapped._spam])
        return s

    @classmethod
    def top_lang_srs(cls, lang, limit, filter_allow_top = False, over18 = True,
                     over18_only = False, ids=False, stale=False):
        from r2.lib import sr_pops
        lang = tup(lang)

        sr_ids = sr_pops.pop_reddits(lang, over18, over18_only, filter_allow_top = filter_allow_top)
        sr_ids = sr_ids[:limit]

        return (sr_ids if ids
                else Subreddit._byID(sr_ids, data=True, return_dict=False, stale=stale))

    @classmethod
    def default_subreddits(cls, ids = True, over18 = False, limit = g.num_default_reddits,
                           stale=True):
        """
        Generates a list of the subreddits any user with the current
        set of language preferences and no subscriptions would see.

        An optional kw argument 'limit' is defaulted to g.num_default_reddits
        """
        langs = c.content_langs if c.content_langs else g.site_lang

        # we'll let these be unordered for now
        auto_srs = []
        if g.automatic_reddits:
            auto_srs = map(lambda sr: sr._id,
                           Subreddit._by_name(g.automatic_reddits, stale=stale).values())

        srs = cls.top_lang_srs(langs, limit + len(auto_srs),
                               filter_allow_top = True,
                               over18 = over18, ids = True,
                               stale=stale)

        rv = []
        for sr in srs:
            if len(rv) >= limit:
                break
            if sr in auto_srs:
                continue
            rv.append(sr)

        rv = auto_srs + rv

        return rv if ids else Subreddit._byID(rv, data=True, return_dict=False, stale=stale)

    @classmethod
    @memoize('random_reddits', time = 1800)
    def random_reddits_cached(cls, user_name, sr_ids, limit):
        return random.sample(sr_ids, limit)

    @classmethod
    def random_reddits(cls, user_name, sr_ids, limit):
        """Select a random subset from sr_ids.

        Used for limiting the number of subscribed subreddits shown on a user's
        front page. Subreddits that are automatically subscribed aren't counted
        against the limit. Selection is cached for a while so the front page
        doesn't jump around.

        """

        if not limit:
            return sr_ids

        if g.automatic_reddits and len(sr_ids) > limit:
            automatics = Subreddit._by_name(g.automatic_reddits).values()
            automatic_ids = [sr._id for sr in automatics]
            for sr_id in automatic_ids:
                try:
                    sr_ids.remove(sr_id)
                except ValueError:
                    automatic_ids.remove(sr_id)
        else:
            automatic_ids = []

        if len(sr_ids) > limit:
            sr_ids = sorted(sr_ids)
            sr_ids = cls.random_reddits_cached(user_name, sr_ids, limit)

        return sr_ids + automatic_ids

    @classmethod
    def random_reddit(cls, limit=2500, over18=False, user=None):
        srs = cls.top_lang_srs(c.content_langs, limit,
                               filter_allow_top = False,
                               over18 = over18,
                               over18_only = over18,
                               ids=True)
        if user:
            excludes = cls.user_subreddits(user, over18=over18, limit=None)
            srs = list(set(srs) - set(excludes))
        return (Subreddit._byID(random.choice(srs))
                if srs else Subreddit._by_name(g.default_sr))

    @classmethod
    def random_subscription(cls, user):
        srs = Subreddit.reverse_subscriber_ids(user)
        return (Subreddit._byID(random.choice(srs))
                if srs else Subreddit._by_name(g.default_sr))

    @classmethod
    def user_subreddits(cls, user, ids=True, over18=False, limit=DEFAULT_LIMIT,
                        stale=False):
        """
        subreddits that appear in a user's listings. If the user has
        subscribed, returns the stored set of subscriptions.
        
        limit - if it's Subreddit.DEFAULT_LIMIT, limits to 50 subs
                (100 for gold users)
                if it's None, no limit is used
                if it's an integer, then that many subs will be returned

        Otherwise, return the default set.
        """
        # Limit the number of subs returned based on user status,
        # if no explicit limit was passed
        if limit is Subreddit.DEFAULT_LIMIT:
            if user and user.gold:
                # Goldies get extra subreddits
                limit = Subreddit.gold_limit
            else:
                limit = Subreddit.sr_limit
        
        # note: for user not logged in, the fake user account has
        # has_subscribed == False by default.
        if user and user.has_subscribed:
            sr_ids = Subreddit.reverse_subscriber_ids(user)
            sr_ids = cls.random_reddits(user.name, sr_ids, limit)

            return sr_ids if ids else Subreddit._byID(sr_ids,
                                                      data=True,
                                                      return_dict=False,
                                                      stale=stale)
        else:
            return cls.default_subreddits(ids = ids, over18=over18,
                                          limit=g.num_default_reddits,
                                          stale=stale)


    # Used to pull all of the SRs a given user moderates or is a contributor
    # to (which one is controlled by query_param)
    @classmethod
    def special_reddits(cls, user, query_param):
        lookup = getattr(cls, 'reverse_%s_ids' % query_param)
        return lookup(user)

    def is_subscriber_defaults(self, user):
        if user.has_subscribed:
            return self.is_subscriber(user)
        else:
            return self in self.default_subreddits(ids = False)

    @classmethod
    def subscribe_defaults(cls, user):
        if not user.has_subscribed:
            for sr in cls.user_subreddits(None, False,
                                          limit = g.num_default_reddits):
                #this will call reverse_subscriber_ids after every
                #addition. if it becomes a problem we should make an
                #add_multiple_subscriber fn
                if sr.add_subscriber(user):
                    sr._incr('_ups', 1)
            user.has_subscribed = True
            user._commit()

    def keep_item(self, wrapped):
        if c.user_is_admin:
            return True

        user = c.user if c.user_is_loggedin else None
        return self.can_view(user)

    def __eq__(self, other):
        if type(self) != type(other):
            return False

        if isinstance(self, FakeSubreddit):
            return self is other

        return self._id == other._id

    def __ne__(self, other):
        return not self.__eq__(other)

    @staticmethod
    def get_all_mod_ids(srs):
        from r2.lib.db.thing import Merge
        srs = tup(srs)
        queries = [SRMember._query(SRMember.c._thing1_id == sr._id,
                                   SRMember.c._name == 'moderator') for sr in srs]
        merged = Merge(queries)
        # sr_ids = [sr._id for sr in srs]
        # query = SRMember._query(SRMember.c._thing1_id == sr_ids, ...)
        # is really slow
        return [rel._thing2_id for rel in list(merged)]

    def update_moderator_permissions(self, user, **kwargs):
        """Grants or denies permissions to this moderator.

        Does nothing if the given user is not a moderator. Args are named
        parameters with bool or None values (use None to all back to the default
        for a permission).
        """
        rel = self.get_moderator(user)
        if rel:
            rel.update_permissions(**kwargs)
            rel._commit()

    def add_rel_note(self, type, user, note):
        rel = getattr(self, "get_%s" % type)(user)
        if not rel:
            raise ValueError("User is not %s." % type)
        rel.note = note
        rel._commit()

    def get_live_promos(self):
        from r2.lib import promote
        return promote.get_live_promotions([self.name])


class FakeSubreddit(BaseSite):
    _defaults = dict(Subreddit._defaults,
        link_flair_position='right',
    )

    def __init__(self):
        BaseSite.__init__(self)

    def keep_for_rising(self, sr_id):
        return False

    @property
    def _should_wiki(self):
        return False

    def is_moderator(self, user):
        if c.user_is_loggedin and c.user_is_admin:
            return FakeSRMember(ModeratorPermissionSet)

    def can_view(self, user):
        return True

    def can_comment(self, user):
        return False

    def can_submit(self, user, promotion=False):
        return False

    def can_change_stylesheet(self, user):
        return False

    def is_banned(self, user):
        return False

    def get_all_comments(self):
        from r2.lib.db import queries
        return queries.get_all_comments()

    def get_gilded(self):
        raise NotImplementedError()

    def spammy(self):
        return False

class FriendsSR(FakeSubreddit):
    name = 'friends'
    title = 'friends'

    @classmethod
    @memoize("get_important_friends", 5*60)
    def get_important_friends(cls, user_id, max_lookup = 500, limit = 100):
        a = Account._byID(user_id, data = True)
        # friends are returned chronologically by date, so pick the end of the list
        # for the most recent additions
        friends = Account._byID(a.friends[-max_lookup:], return_dict = False,
                                data = True)

        # only include friends that have ever interacted with the site
        last_activity = last_modified_multi(friends, "overview")
        friends = [x for x in friends if x in last_activity]

        # sort friends by most recent interactions
        friends.sort(key = lambda x: last_activity[x], reverse = True)
        return [x._id for x in friends[:limit]]

    def get_links(self, sort, time):
        from r2.lib.db import queries

        if not c.user_is_loggedin:
            raise UserRequiredException

        friends = self.get_important_friends(c.user._id)

        if not friends:
            return []

        # with the precomputer enabled, this Subreddit only supports
        # being sorted by 'new'. it would be nice to have a
        # cleaner UI than just blatantly ignoring their sort,
        # though
        sort = 'new'
        time = 'all'

        friends = Account._byID(friends, return_dict=False)

        crs = [queries.get_submitted(friend, sort, time)
               for friend in friends]
        return queries.MergedCachedResults(crs)

    def get_all_comments(self):
        from r2.lib.db import queries

        if not c.user_is_loggedin:
            raise UserRequiredException

        friends = self.get_important_friends(c.user._id)

        if not friends:
            return []

        # with the precomputer enabled, this Subreddit only supports
        # being sorted by 'new'. it would be nice to have a
        # cleaner UI than just blatantly ignoring their sort,
        # though
        sort = 'new'
        time = 'all'

        friends = Account._byID(friends,
                                return_dict=False)

        crs = [queries.get_comments(friend, sort, time)
               for friend in friends]
        return queries.MergedCachedResults(crs)

    def get_gilded(self):
        from r2.lib.db.queries import get_gilded_users
        if not c.user_is_loggedin:
            raise UserRequiredException

        friends = self.get_important_friends(c.user._id)
        if not friends:
            return []

        return get_gilded_users(friends)


class AllSR(FakeSubreddit):
    name = 'all'
    title = 'all subreddits'

    def keep_for_rising(self, sr_id):
        return True

    def get_links(self, sort, time):
        from r2.models import Link
        from r2.lib.db import queries
        q = Link._query(
            sort=queries.db_sort(sort),
            read_cache=True,
            write_cache=True,
            cache_time=60,
            data=True,
            filter_primary_sort_only=True,
        )
        if time != 'all':
            q._filter(queries.db_times[time])
        return q

    def get_all_comments(self):
        from r2.lib.db import queries
        return queries.get_all_comments()

    def get_gilded(self):
        from r2.lib.db import queries
        return queries.get_all_gilded()


class AllMinus(AllSR):
    name = _("%s (filtered)") % "all"

    def __init__(self, srs):
        AllSR.__init__(self)
        self.srs = srs
        self.sr_ids = [sr._id for sr in srs]

    def keep_for_rising(self, sr_id):
        return sr_id not in self.sr_ids

    @property
    def title(self):
        return 'all subreddits except ' + ', '.join(sr.name for sr in self.srs)

    @property
    def path(self):
        return '/r/all-' + '-'.join(sr.name for sr in self.srs)

    def get_links(self, sort, time):
        from r2.models import Link
        from r2.lib.db.operators import not_
        q = AllSR.get_links(self, sort, time)
        if c.user.gold:
            q._filter(not_(Link.c.sr_id.in_(self.sr_ids)))
        return q

class _DefaultSR(FakeSubreddit):
    #notice the space before reddit.com
    name = ' reddit.com'
    path = '/'
    header = g.default_header_url

    def _get_sr_ids(self):
        if not c.defaultsr_cached_sr_ids:
            user = c.user if c.user_is_loggedin else None
            c.defaultsr_cached_sr_ids = Subreddit.user_subreddits(user)
        return c.defaultsr_cached_sr_ids

    def keep_for_rising(self, sr_id):
        return sr_id in self._get_sr_ids()

    def is_moderator(self, user):
        return False

    def get_links(self, sort, time):
        sr_ids = self._get_sr_ids()
        return get_links_sr_ids(sr_ids, sort, time)

    @property
    def title(self):
        return _(g.short_description)

# This is the base class for the instantiated front page reddit
class DefaultSR(_DefaultSR):
    @property
    def _base(self):
        try:
            return Subreddit._by_name(g.default_sr, stale=True)
        except NotFound:
            return None

    def wiki_can_submit(self, user):
        return True

    @property
    def wiki_use_subreddit_karma(self):
        return False

    @property
    def _should_wiki(self):
        return True
    
    @property
    def wikimode(self):
        return self._base.wikimode if self._base else "disabled"
    
    @property
    def wiki_edit_karma(self):
        return self._base.wiki_edit_karma

    @property
    def wiki_edit_age(self):
        return self._base.wiki_edit_age

    def is_wikicontributor(self, user):
        return self._base.is_wikicontributor(user)
    
    def is_wikibanned(self, user):
        return self._base.is_wikibanned(user)
    
    def is_wikicreate(self, user):
        return self._base.is_wikicreate(user)
    
    @property
    def _fullname(self):
        return "t5_6"
    
    @property
    def _id36(self):
        return self._base._id36

    @property
    def type(self):
        return self._base.type if self._base else "public"

    @property
    def header(self):
        return (self._base and self._base.header) or _DefaultSR.header

    @property
    def header_title(self):
        return (self._base and self._base.header_title) or ""

    @property
    def header_size(self):
        return (self._base and self._base.header_size) or None

    @property
    def stylesheet_contents(self):
        return self._base.stylesheet_contents if self._base else ""

    @property
    def stylesheet_contents_secure(self):
        return self._base.stylesheet_contents_secure if self._base else ""

    @property
    def stylesheet_url_http(self):
        return self._base.stylesheet_url_http if self._base else ""

    @property
    def stylesheet_url_https(self):
        return self._base.stylesheet_url_https if self._base else ""

    @property
    def stylesheet_hash(self):
        return self._base.stylesheet_hash if self._base else ""

    def get_all_comments(self):
        from r2.lib.db.queries import get_sr_comments, merge_results
        srs = Subreddit.user_subreddits(c.user, ids=False)
        results = [get_sr_comments(sr) for sr in srs]
        return merge_results(*results)

    def get_gilded(self):
        from r2.lib.db.queries import get_gilded
        return get_gilded(Subreddit.user_subreddits(c.user))

    def get_live_promos(self):
        from r2.lib import promote
        srs = Subreddit.user_subreddits(c.user, ids=False)
        # '' is for promos targeted to the frontpage
        sr_names = [''] + [sr.name for sr in srs]
        return promote.get_live_promotions(sr_names)


class MultiReddit(FakeSubreddit):
    name = 'multi'
    header = ""

    def __init__(self, path=None, srs=None):
        FakeSubreddit.__init__(self)
        if path is not None:
            self._path = path
        self._srs = srs or []

    @property
    def srs(self):
        return self._srs

    @property
    def sr_ids(self):
        return [sr._id for sr in self.srs]

    @property
    def kept_sr_ids(self):
        return [sr._id for sr in self.srs if not sr._spam]

    @property
    def banned_sr_ids(self):
        return [sr._id for sr in self.srs if sr._spam]

    def keep_for_rising(self, sr_id):
        return sr_id in self.kept_sr_ids

    def is_moderator(self, user):
        if not user:
            return False

        # Get moderator SRMember relations for all in srs
        # if a relation doesn't exist there will be a None entry in the
        # returned dict
        mod_rels = SRMember._fast_query(self.srs, user,
                                        'moderator', data=False)
        if None in mod_rels.values():
            return False
        else:
            return FakeSRMember(ModeratorPermissionSet)

    @property
    def title(self):
        return _('posts from %s') % ', '.join(sr.name for sr in self.srs)

    @property
    def path(self):
        return self._path

    @property
    def over_18(self):
        return any(sr.over_18 for sr in self.srs)

    def get_links(self, sort, time):
        return get_links_sr_ids(self.kept_sr_ids, sort, time)

    def get_all_comments(self):
        from r2.lib.db.queries import get_sr_comments, merge_results
        srs = Subreddit._byID(self.kept_sr_ids, return_dict=False)
        results = [get_sr_comments(sr) for sr in srs]
        return merge_results(*results)

    def get_gilded(self):
        from r2.lib.db.queries import get_gilded
        return get_gilded(self.kept_sr_ids)

    def get_live_promos(self):
        from r2.lib import promote
        srs = Subreddit._byID(self.kept_sr_ids, return_dict=False)
        sr_names = [sr.name for sr in srs]
        return promote.get_live_promotions(sr_names)


class TooManySubredditsError(Exception):
    pass


class LabeledMulti(tdb_cassandra.Thing, MultiReddit):
    """Thing with special columns that hold Subreddit ids and properties."""
    _use_db = True
    _views = []
    _defaults = dict(MultiReddit._defaults,
        visibility='private',
        description_md='',
        copied_from=None,  # for internal analysis/bookkeeping purposes
    )
    _extra_schema_creation_args = {
        "key_validation_class": tdb_cassandra.UTF8_TYPE,
        "column_name_class": tdb_cassandra.UTF8_TYPE,
        "default_validation_class": tdb_cassandra.UTF8_TYPE,
        "column_validation_classes": {
            "date": pycassa.system_manager.DATE_TYPE,
        },
    }
    _compare_with = tdb_cassandra.UTF8_TYPE
    _read_consistency_level = tdb_cassandra.CL.ONE
    _write_consistency_level = tdb_cassandra.CL.QUORUM

    SR_PREFIX = 'SR_'
    MAX_SR_COUNT = 100

    def __init__(self, _id=None, *args, **kwargs):
        tdb_cassandra.Thing.__init__(self, _id, *args, **kwargs)
        MultiReddit.__init__(self)
        self._owner = None

    @classmethod
    def _byID(cls, ids, return_dict=True, properties=None):
        ret = super(cls, cls)._byID(ids, return_dict=False,
                                    properties=properties)
        if not ret:
            return
        ret = cls._load(ret)
        if isinstance(ret, cls):
            return ret
        elif return_dict:
            return {thing._id: thing for thing in ret}
        else:
            return ret

    @classmethod
    def _load_no_lookup(cls, things, srs_dict, owners_dict):
        things, single = tup(things, ret_is_single=True)
        for thing in things:
            thing._srs = [srs_dict[sr_id] for sr_id in thing.sr_ids]
            thing._owner = owners_dict[thing.owner_fullname]
        return things[0] if single else things

    @classmethod
    def _load(cls, things):
        things, single = tup(things, ret_is_single=True)
        sr_ids = set(itertools.chain(*[thing.sr_ids for thing in things]))
        owner_fullnames = set((thing.owner_fullname for thing in things))

        srs = Subreddit._byID(sr_ids, data=True, return_dict=True)
        owners = Thing._by_fullname(owner_fullnames, data=True, return_dict=True)
        ret = cls._load_no_lookup(things, srs, owners)
        return ret[0] if single else things

    @property
    def sr_ids(self):
        return self.sr_props.keys()

    @property
    def srs(self):
        return self._srs

    @property
    def owner(self):
        return self._owner

    @property
    def sr_columns(self):
        # limit to max subreddit count, allowing a little fudge room for
        # cassandra inconsistency
        remaining = self.MAX_SR_COUNT + 10
        sr_columns = {}
        for k, v in self._t.iteritems():
            if not k.startswith(self.SR_PREFIX):
                continue

            sr_columns[k] = v

            remaining -= 1
            if remaining <= 0:
                break
        return sr_columns

    @property
    def sr_props(self):
        return self.columns_to_sr_props(self.sr_columns)

    @property
    def path(self):
        if isinstance(self.owner, Account):
            return '/user/%(username)s/m/%(multiname)s' % {
                'username': self.owner.name,
                'multiname': self.name,
            }

    @property
    def user_path(self):
        if self.owner == c.user:
            return '/me/m/%s' % self.name
        else:
            return self.path

    @property
    def name(self):
        return self._id.split('/')[-1]

    @property
    def analytics_name(self):
        # classify as "multi" (as for unnamed multis) until our traffic system
        # is smarter
        return 'multi'

    @property
    def title(self):
        if isinstance(self.owner, Account):
            return _('%s subreddits curated by /u/%s') % (self.name, self.owner.name)
        return _('%s subreddits') % self.name

    def can_view(self, user):
        if c.user_is_admin:
            return True

        return user == self.owner or self.visibility == 'public'

    def can_edit(self, user):
        if c.user_is_admin and self.owner == Account.system_user():
            return True

        return user == self.owner

    @classmethod
    def by_owner(cls, owner):
        return list(LabeledMultiByOwner.query([owner._fullname]))

    @classmethod
    def create(cls, path, owner):
        obj = cls(_id=path, owner_fullname=owner._fullname)
        obj._commit()
        obj._owner = owner
        return obj

    @classmethod
    def copy(cls, path, multi, owner):
        obj = cls(_id=path, **multi._t)
        obj.owner_fullname = owner._fullname
        obj._commit()
        obj._owner = owner
        return obj

    @classmethod
    def sr_props_to_columns(cls, sr_props):
        columns = {}
        sr_ids = []
        for sr_id, props in sr_props.iteritems():
            if isinstance(sr_id, BaseSite):
                sr_id = sr_id._id
            sr_ids.append(sr_id)
            columns[cls.SR_PREFIX + str(sr_id)] = json.dumps(props)
        return sr_ids, columns

    @classmethod
    def columns_to_sr_props(cls, columns):
        ret = {}
        for s, sr_prop_dump in columns.iteritems():
            sr_id = long(s.strip(cls.SR_PREFIX))
            sr_props = json.loads(sr_prop_dump)
            ret[sr_id] = sr_props
        return ret

    def _on_create(self):
        for view in self._views:
            view.add_object(self)

    def add_srs(self, sr_props):
        """Add/overwrite subreddit(s)."""
        sr_ids, sr_columns = self.sr_props_to_columns(sr_props)

        if len(set(sr_columns) | set(self.sr_columns)) > self.MAX_SR_COUNT:
            raise TooManySubredditsError

        new_sr_ids = set(sr_ids) - set(self.sr_ids)
        new_srs = Subreddit._byID(new_sr_ids, data=True, return_dict=False)
        self._srs.extend(new_srs)

        for attr, val in sr_columns.iteritems():
            self.__setattr__(attr, val)

    def del_srs(self, sr_ids):
        """Delete subreddit(s)."""
        sr_props = dict.fromkeys(tup(sr_ids), {})
        sr_ids, sr_columns = self.sr_props_to_columns(sr_props)

        for key in sr_columns.iterkeys():
            self.__delitem__(key)

        self._srs = [sr for sr in self._srs if sr._id not in sr_ids]

    def clear_srs(self):
        self.del_srs(self.sr_ids)

    def delete(self):
        # Do we want to actually delete objects?
        self._destroy()
        for view in self._views:
            rowkey = view._rowkey(self)
            column = view._obj_to_column(self)
            view._remove(rowkey, column)


@tdb_cassandra.view_of(LabeledMulti)
class LabeledMultiByOwner(tdb_cassandra.View):
    _use_db = True

    @classmethod
    def _rowkey(cls, lm):
        return lm.owner_fullname


class RandomReddit(FakeSubreddit):
    name = 'random'
    header = ""

class RandomNSFWReddit(FakeSubreddit):
    name = 'randnsfw'
    header = ""

class RandomSubscriptionReddit(FakeSubreddit):
    name = 'myrandom'
    header = ""

class ModContribSR(MultiReddit):
    name  = None
    title = None
    query_param = None

    def __init__(self):
        # Can't lookup srs right now, c.user not set
        MultiReddit.__init__(self)

    @property
    def sr_ids(self):
        if c.user_is_loggedin:
            return Subreddit.special_reddits(c.user, self.query_param)
        else:
            return []

    @property
    def srs(self):
        return Subreddit._byID(self.sr_ids, data=True, return_dict=False)

class ModSR(ModContribSR):
    name  = "subreddits you moderate"
    title = "subreddits you moderate"
    query_param = "moderator"
    path = "/r/mod"

    def is_moderator(self, user):
        return FakeSRMember(ModeratorPermissionSet)


class ModMinus(ModSR):
    def __init__(self, exclude_srs):
        ModSR.__init__(self)
        self.exclude_srs = exclude_srs
        self.exclude_sr_ids = [sr._id for sr in exclude_srs]

    @property
    def sr_ids(self):
        sr_ids = super(ModMinus, self).sr_ids
        return [sr_id for sr_id in sr_ids if not sr_id in self.exclude_sr_ids]

    @property
    def name(self):
        exclude_text = ', '.join(sr.name for sr in self.exclude_srs)
        return 'subreddits you moderate except ' + exclude_text

    @property
    def title(self):
        return self.name

    @property
    def path(self):
        return '/r/mod-' + '-'.join(sr.name for sr in self.exclude_srs)


class ContribSR(ModContribSR):
    name  = "contrib"
    title = "communities you're approved on"
    query_param = "contributor"
    path = "/r/contrib"

class SubSR(FakeSubreddit):
    stylesheet = 'subreddit.css'
    #this will make the javascript not send an SR parameter
    name = ''
    title = ''

    def can_view(self, user):
        return True

    def can_comment(self, user):
        return False

    def can_submit(self, user, promotion=False):
        return True

    @property
    def path(self):
        return "/subreddits/"

class DomainSR(FakeSubreddit):
    @property
    def path(self):
        return '/domain/' + self.domain

    def __init__(self, domain):
        FakeSubreddit.__init__(self)
        self.domain = domain
        self.name = domain 
        self.title = _("%(domain)s on %(reddit.com)s") % {
            "domain": domain, "reddit.com": g.domain}
        idn = domain.decode('idna')
        if idn != domain:
            self.idn = idn

    def get_links(self, sort, time):
        from r2.lib.db import queries
        return queries.get_domain_links(self.domain, sort, time)

Frontpage = DefaultSR()
Sub = SubSR()
Friends = FriendsSR()
Mod = ModSR()
Contrib = ContribSR()
All = AllSR()
Random = RandomReddit()
RandomNSFW = RandomNSFWReddit()
RandomSubscription = RandomSubscriptionReddit()

Subreddit._specials.update(dict(friends = Friends,
                                randnsfw = RandomNSFW,
                                myrandom = RandomSubscription,
                                random = Random,
                                mod = Mod,
                                contrib = Contrib,
                                all = All))

class SRMember(Relation(Subreddit, Account)):
    _defaults = dict(encoded_permissions=None)
    _permission_class = None
    _cache = g.srmembercache

    def has_permission(self, perm):
        """Returns whether this member has explicitly been granted a permission.
        """
        return self.get_permissions().get(perm, False)

    def get_permissions(self):
        """Returns permission set for this member (or None if N/A)."""
        if not self._permission_class:
            raise NotImplementedError
        return self._permission_class.loads(self.encoded_permissions)

    def update_permissions(self, **kwargs):
        """Grants or denies permissions to this member.

        Args are named parameters with bool or None values (use None to disable
        granting or denying the permission). After calling this method,
        the relation will be _dirty until _commit is called.
        """
        if not self._permission_class:
            raise NotImplementedError
        perm_set = self._permission_class.loads(self.encoded_permissions)
        if perm_set is None:
            perm_set = self._permission_class()
        for k, v in kwargs.iteritems():
            if v is None:
                if k in perm_set:
                    del perm_set[k]
            else:
                perm_set[k] = v
        self.encoded_permissions = perm_set.dumps()

    def set_permissions(self, perm_set):
        """Assigns a permission set to this relation."""
        self.encoded_permissions = perm_set.dumps()

    def is_superuser(self):
        return self.get_permissions().is_superuser()


class FakeSRMember:
    """All-permission granting stub for SRMember, used by FakeSubreddits."""
    def __init__(self, permission_class):
        self.permission_class = permission_class

    def has_permission(self, perm):
        return True

    def get_permissions(self):
        return self.permission_class(all=True)

    def is_superuser(self):
        return True


Subreddit.__bases__ += (
    UserRel('moderator', SRMember,
            permission_class=ModeratorPermissionSet),
    UserRel('moderator_invite', SRMember,
            permission_class=ModeratorPermissionSet),
    UserRel('contributor', SRMember),
    UserRel('subscriber', SRMember, disable_ids_fn=True),
    UserRel('banned', SRMember),
    UserRel('wikibanned', SRMember),
    UserRel('wikicontributor', SRMember),
)


class SubredditPopularityByLanguage(tdb_cassandra.View):
    _use_db = True
    _value_type = 'pickle'
    _connection_pool = 'main'
    _read_consistency_level = CL_ONE

########NEW FILE########
__FILENAME__ = token
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import datetime
import functools
from os import urandom
from base64 import urlsafe_b64encode

from pycassa.system_manager import ASCII_TYPE, DATE_TYPE, UTF8_TYPE

from pylons import g, c
from pylons.i18n import _

from r2.lib.db import tdb_cassandra
from r2.lib.db.thing import NotFound
from r2.models.account import Account

def generate_token(size):
    return urlsafe_b64encode(urandom(size)).rstrip("=")


class Token(tdb_cassandra.Thing):
    """A unique randomly-generated token used for authentication."""

    _extra_schema_creation_args = dict(
        key_validation_class=ASCII_TYPE,
        default_validation_class=UTF8_TYPE,
        column_validation_classes=dict(
            date=DATE_TYPE,
            used=ASCII_TYPE
        )
    )

    @classmethod
    def _new(cls, **kwargs):
        if "_id" not in kwargs:
            kwargs["_id"] = cls._generate_unique_token()

        token = cls(**kwargs)
        token._commit()
        return token

    @classmethod
    def _generate_unique_token(cls):
        for i in range(3):
            token = generate_token(cls.token_size)
            try:
                cls._byID(token)
            except tdb_cassandra.NotFound:
                return token
            else:
                continue
        raise ValueError

    @classmethod
    def get_token(cls, _id):
        if _id is None:
            return None
        try:
            return cls._byID(_id)
        except tdb_cassandra.NotFound:
            return None


class ConsumableToken(Token):
    _defaults = dict(used=False)
    _bool_props = ("used",)
    _warn_on_partial_ttl = False

    @classmethod
    def get_token(cls, _id):
        token = super(ConsumableToken, cls).get_token(_id)
        if token and not token.used:
            return token
        else:
            return None

    def consume(self):
        self.used = True
        self._commit()


class OAuth2Scope:
    scope_info = {
        None: {
            "id": None,
            "name": _("Any Scope"),
            "description": _("Endpoint is accessible with any combination "
                "of other OAuth 2 scopes."),
        },
        "account": {
            "id": "account",
            "name": _("Update account information"),
            "description": _("Update preferences and related account "
                "information. Will not have access to your email or "
                "password."),
        },
        "edit": {
            "id": "edit",
            "name": _("Edit Posts"),
            "description": _("Edit and delete my comments and submissions."),
        },
        "flair": {
            "id": "flair",
            "name": _("Manage My Flair"),
            "description": _("Select my subreddit flair. "
                             "Change link flair on my submissions."),
        },
        "history": {
            "id": "history",
            "name": _("History"),
            "description": _(
                "Access my voting history and comments or submissions I've"
                " saved or hidden."),
        },
        "identity": {
            "id": "identity",
            "name": _("My Identity"),
            "description": _("Access my reddit username and signup date."),
        },
        "modflair": {
            "id": "modflair",
            "name": _("Moderate Flair"),
            "description": _(
                "Manage and assign flair in subreddits I moderate."),
        },
        "modposts": {
            "id": "modposts",
            "name": _("Moderate Posts"),
            "description": _(
                "Approve, remove, mark nsfw, and distinguish content"
                " in subreddits I moderate."),
        },
        "modconfig": {
            "id": "modconfig",
            "name": _("Moderate Subreddit Configuration"),
            "description": _(
                "Manage the configuration, sidebar, and CSS"
                " of subreddits I moderate."),
        },
        "modlog": {
            "id": "modlog",
            "name": _("Moderation Log"),
            "description": _(
                "Access the moderation log in subreddits I moderate."),
        },
        "modtraffic": {
            "id": "modtraffic",
            "name": _("Subreddit Traffic"),
            "description": _("Access traffic stats in subreddits I moderate."),
        },
        "modwiki": {
            "id": "modwiki",
            "name": _("Moderate Wiki"),
            "description": _(
                "Change editors and visibility of wiki pages"
                " in subreddits I moderate."),
        },
        "mysubreddits": {
            "id": "mysubreddits",
            "name": _("My Subreddits"),
            "description": _(
                "Access the list of subreddits I moderate, contribute to,"
                " and subscribe to."),
        },
        "privatemessages": {
            "id": "privatemessages",
            "name": _("Private Messages"),
            "description": _(
                "Access my inbox and send private messages to other users."),
        },
        "read": {
            "id": "read",
            "name": _("Read Content"),
            "description": _("Access posts and comments through my account."),
        },
        "report": {
            "id": "report",
            "name": _("Report content"),
            "description": _("Report content for rules violations. "
                             "Hide & show individual submissions."),
        },
        "save": {
            "id": "save",
            "name": _("Save Content"),
            "description": _("Save and unsave comments and submissions."),
        },
        "submit": {
            "id": "submit",
            "name": _("Submit Content"),
            "description": _("Submit links and comments from my account."),
        },
        "subscribe": {
            "id": "subscribe",
            "name": _("Edit My Subscriptions"),
            "description": _('Manage my subreddit subscriptions. Manage '
                '"friends" - users whose content I follow.'),
        },
        "vote": {
            "id": "vote",
            "name": _("Vote"),
            "description":
                _("Submit and change my votes on comments and submissions."),
        },
        "wikiedit": {
            "id": "wiki",
            "name": _("Wiki Editing"),
            "description": _("Edit wiki pages on my behalf"),
        },
        "wikiread": {
            "id": "wikiread",
            "name": _("Read Wiki Pages"),
            "description": _("Read wiki pages through my account"),
        },
    }

    # Special scope, granted implicitly to clients with app_type == "script"
    FULL_ACCESS = "*"

    class InsufficientScopeError(StandardError):
        pass

    def __init__(self, scope_str=None, subreddits=None, scopes=None):
        if scope_str:
            self._parse_scope_str(scope_str)
        elif subreddits is not None or scopes is not None:
            self.subreddit_only = bool(subreddits)
            self.subreddits = subreddits
            self.scopes = scopes
        else:
            self.subreddit_only = False
            self.subreddits = set()
            self.scopes = set()

    def _parse_scope_str(self, scope_str):
        srs, sep, scopes = scope_str.rpartition(':')
        if sep:
            self.subreddit_only = True
            self.subreddits = set(srs.split('+'))
        else:
            self.subreddit_only = False
            self.subreddits = set()
        self.scopes = set(scopes.split(','))

    def __str__(self):
        if self.subreddit_only:
            sr_part = '+'.join(sorted(self.subreddits)) + ':'
        else:
            sr_part = ''
        return sr_part + ','.join(sorted(self.scopes))

    def has_access(self, subreddit, required_scopes):
        if self.FULL_ACCESS in self.scopes:
            return True
        if self.subreddit_only and subreddit not in self.subreddits:
            return False
        return (self.scopes >= required_scopes)

    def is_valid(self):
        return all(scope in self.scope_info for scope in self.scopes)

    def details(self):
        if self.FULL_ACCESS in self.scopes:
            scopes = self.scope_info.keys()
        else:
            scopes = self.scopes
        return [(scope, self.scope_info[scope]) for scope in scopes]

    @classmethod
    def merge_scopes(cls, scopes):
        """Return a by-subreddit dict representing merged OAuth2Scopes.

        Takes an iterable of OAuth2Scopes. For each of those,
        if it defines scopes on multiple subreddits, it is split
        into one OAuth2Scope per subreddit. If multiple passed in
        OAuth2Scopes reference the same scopes, they'll be combined.

        """
        merged = {}
        for scope in scopes:
            srs = scope.subreddits if scope.subreddit_only else (None,)
            for sr in srs:
                if sr in merged:
                    merged[sr].scopes.update(scope.scopes)
                else:
                    new_scope = cls()
                    new_scope.subreddits = {sr}
                    new_scope.scopes = scope.scopes
                    if sr is not None:
                        new_scope.subreddit_only = True
                    merged[sr] = new_scope
        return merged


def extra_oauth2_scope(*scopes):
    """Wrap a function so that it only returns data if user has all `scopes`

    When not in an OAuth2 context, function returns normally.
    In an OAuth2 context, the function will not be run unless the user
    has granted all scopes required of this function. Instead, the function
    will raise an OAuth2Scope.InsufficientScopeError.

    """
    def extra_oauth2_wrapper(fn):
        @functools.wraps(fn)
        def wrapper_fn(*a, **kw):
            if not c.oauth_user:
                # Not in an OAuth2 context, run function normally
                return fn(*a, **kw)
            elif c.oauth_scope.has_access(c.site.name, set(scopes)):
                # In an OAuth2 context, and have scope for this function
                return fn(*a, **kw)
            else:
                # In an OAuth2 context, but don't have scope
                raise OAuth2Scope.InsufficientScopeError(scopes)
        return wrapper_fn
    return extra_oauth2_wrapper


class OAuth2Client(Token):
    """A client registered for OAuth2 access"""
    max_developers = 20
    token_size = 10
    client_secret_size = 20
    _float_props = (
        "max_reqs_sec",
    )
    _defaults = dict(name="",
                     description="",
                     about_url="",
                     icon_url="",
                     secret="",
                     redirect_uri="",
                     app_type="web",
                     max_reqs_sec=g.RL_OAUTH_AVG_REQ_PER_SEC,
                    )
    _use_db = True
    _connection_pool = "main"

    _developer_colname_prefix = 'has_developer_'

    @classmethod
    def _new(cls, **kwargs):
        if "secret" not in kwargs:
            kwargs["secret"] = generate_token(cls.client_secret_size)
        return super(OAuth2Client, cls)._new(**kwargs)

    @property
    def _developer_ids(self):
        for k, v in self._t.iteritems():
            if k.startswith(self._developer_colname_prefix) and v:
                try:
                    yield int(k[len(self._developer_colname_prefix):], 36)
                except ValueError:
                    pass

    @property
    def _max_reqs(self):
        return self.max_reqs_sec * g.RL_OAUTH_RESET_SECONDS

    @property
    def _developers(self):
        """Returns a list of users who are developers of this client."""

        devs = Account._byID(list(self._developer_ids))
        return [dev for dev in devs.itervalues()
                if not (dev._deleted or dev._spam)]

    def _developer_colname(self, account):
        """Developer access is granted by way of adding a column with the
        account's ID36 to the client object.  This function returns the
        column name for a given Account.
        """

        return ''.join((self._developer_colname_prefix, account._id36))

    def has_developer(self, account):
        """Returns a boolean indicating whether or not the supplied Account is a developer of this application."""

        if account._deleted or account._spam:
            return False
        else:
            return getattr(self, self._developer_colname(account), False)

    def add_developer(self, account, force=False):
        """Grants developer access to the supplied Account."""

        dev_ids = set(self._developer_ids)
        if account._id not in dev_ids:
            if not force and len(dev_ids) >= self.max_developers:
                raise OverflowError('max developers reached')
            setattr(self, self._developer_colname(account), True)
            self._commit()

        # Also update index
        OAuth2ClientsByDeveloper._set_values(account._id36, {self._id: ''})

    def remove_developer(self, account):
        """Revokes the supplied Account's developer access."""

        if hasattr(self, self._developer_colname(account)):
            del self[self._developer_colname(account)]
            if not len(self._developers):
                # No developers remain, delete the client
                self.deleted = True
            self._commit()

        # Also update index
        try:
            cba = OAuth2ClientsByDeveloper._byID(account._id36)
            del cba[self._id]
        except (tdb_cassandra.NotFound, KeyError):
            pass
        else:
            cba._commit()

    @classmethod
    def _by_developer(cls, account):
        """Returns a (possibly empty) list of clients for which Account is a developer."""

        if account._deleted or account._spam:
            return []

        try:
            cba = OAuth2ClientsByDeveloper._byID(account._id36)
        except tdb_cassandra.NotFound:
            return []

        clients = cls._byID(cba._values().keys())
        return [client for client in clients.itervalues()
                if not getattr(client, 'deleted', False)
                    and client.has_developer(account)]

    @classmethod
    def _by_user(cls, account):
        """Returns a (possibly empty) list of client-scope-expiration triples for which Account has outstanding access tokens."""

        refresh_tokens = {
            token._id: token for token in OAuth2RefreshToken._by_user(account)
            if token.check_valid()}
        access_tokens = [token for token in OAuth2AccessToken._by_user(account)
                         if token.check_valid()]

        tokens = refresh_tokens.values()
        tokens.extend(token for token in access_tokens
                      if token.refresh_token not in refresh_tokens)

        clients = cls._byID([token.client_id for token in tokens])
        return [(clients[token.client_id], OAuth2Scope(token.scope),
                 token.date + datetime.timedelta(seconds=token._ttl)
                     if token._ttl else None)
                for token in tokens]

    @classmethod
    def _by_user_grouped(cls, account):
        token_tuples = cls._by_user(account)
        clients = {}
        for client, scope, expiration in token_tuples:
            if client._id in clients:
                client_data = clients[client._id]
                client_data['scopes'].append(scope)
            else:
                client_data = {'scopes': [scope], 'access_tokens': 0,
                               'refresh_tokens': 0, 'client': client}
                clients[client._id] = client_data
            if expiration:
                client_data['access_tokens'] += 1
            else:
                client_data['refresh_tokens'] += 1

        for client_data in clients.itervalues():
            client_data['scopes'] = OAuth2Scope.merge_scopes(client_data['scopes'])

        return clients

    def revoke(self, account):
        """Revoke all of the outstanding OAuth2AccessTokens associated with this client and user Account."""

        for token in OAuth2RefreshToken._by_user(account):
            if token.client_id == self._id:
                token.revoke()
        for token in OAuth2AccessToken._by_user(account):
            if token.client_id == self._id:
                token.revoke()

class OAuth2ClientsByDeveloper(tdb_cassandra.View):
    """Index providing access to the list of OAuth2Clients of which an Account is a developer."""

    _use_db = True
    _type_prefix = 'OAuth2ClientsByDeveloper'
    _view_of = OAuth2Client
    _connection_pool = 'main'


class OAuth2AuthorizationCode(ConsumableToken):
    """An OAuth2 authorization code for completing authorization flow"""
    token_size = 20
    _ttl = datetime.timedelta(minutes=10)
    _defaults = dict(ConsumableToken._defaults.items() + [
                         ("client_id", ""),
                         ("redirect_uri", ""),
                         ("scope", ""),
                         ("refreshable", False)])
    _bool_props = ConsumableToken._bool_props + ("refreshable",)
    _warn_on_partial_ttl = False
    _use_db = True
    _connection_pool = "main"

    @classmethod
    def _new(cls, client_id, redirect_uri, user_id, scope, refreshable):
        return super(OAuth2AuthorizationCode, cls)._new(
                client_id=client_id,
                redirect_uri=redirect_uri,
                user_id=user_id,
                scope=str(scope),
                refreshable=refreshable)

    @classmethod
    def use_token(cls, _id, client_id, redirect_uri):
        token = cls.get_token(_id)
        if token and (token.client_id == client_id and
                      token.redirect_uri == redirect_uri):
            token.consume()
            return token
        else:
            return None


class OAuth2AccessToken(Token):
    """An OAuth2 access token for accessing protected resources"""
    token_size = 20
    _ttl = datetime.timedelta(minutes=60)
    _defaults = dict(scope="",
                     token_type="bearer",
                     refresh_token=None,
                    )
    _use_db = True
    _connection_pool = "main"

    @classmethod
    def _new(cls, client_id, user_id, scope, refresh_token=None):
        return super(OAuth2AccessToken, cls)._new(
                     client_id=client_id,
                     user_id=user_id,
                     scope=str(scope),
                     refresh_token=refresh_token)

    @classmethod
    def _by_user_view(cls):
        return OAuth2AccessTokensByUser

    def _on_create(self):
        """Updates the by-user view upon creation."""

        self._by_user_view()._set_values(str(self.user_id), {self._id: ''})
        return super(OAuth2AccessToken, self)._on_create()

    def check_valid(self):
        """Returns boolean indicating whether or not this access token is still valid."""

        # Has the token been revoked?
        if getattr(self, 'revoked', False):
            return False

        # Is the OAuth2Client still valid?
        try:
            client = OAuth2Client._byID(self.client_id)
            if getattr(client, 'deleted', False):
                raise NotFound
        except NotFound:
            return False

        # Is the user account still valid?
        try:
            account = Account._byID36(self.user_id)
            if account._deleted:
                raise NotFound
        except NotFound:
            return False

        return True

    def revoke(self):
        """Revokes (invalidates) this access token."""

        self.revoked = True
        self._commit()

        try:
            tba = self._by_user_view()._byID(self.user_id)
            del tba[self._id]
        except (tdb_cassandra.NotFound, KeyError):
            # Not fatal, since self.check_valid() will still be False.
            pass
        else:
            tba._commit()

    @classmethod
    def revoke_all_by_user(cls, account):
        """Revokes all access tokens for a given user Account."""
        tokens = cls._by_user(account)
        for token in tokens:
            token.revoke()

    @classmethod
    def _by_user(cls, account):
        """Returns a (possibly empty) list of valid access tokens for a given user Account."""

        try:
            tba = cls._by_user_view()._byID(account._id36)
        except tdb_cassandra.NotFound:
            return []

        tokens = cls._byID(tba._values().keys())
        return [token for token in tokens.itervalues() if token.check_valid()]

class OAuth2AccessTokensByUser(tdb_cassandra.View):
    """Index listing the outstanding access tokens for an account."""

    _use_db = True
    _ttl = OAuth2AccessToken._ttl
    _type_prefix = 'OAuth2AccessTokensByUser'
    _view_of = OAuth2AccessToken
    _connection_pool = 'main'


class OAuth2RefreshToken(OAuth2AccessToken):
    """A refresh token for obtaining new access tokens for the same grant."""

    _type_prefix = None
    _ttl = None

    @classmethod
    def _by_user_view(cls):
        return OAuth2RefreshTokensByUser

class OAuth2RefreshTokensByUser(tdb_cassandra.View):
    """Index listing the outstanding refresh tokens for an account."""

    _use_db = True
    _ttl = OAuth2RefreshToken._ttl
    _type_prefix = 'OAuth2RefreshTokensByUser'
    _view_of = OAuth2RefreshToken
    _connection_pool = 'main'


class EmailVerificationToken(ConsumableToken):
    _use_db = True
    _connection_pool = "main"
    _ttl = datetime.timedelta(hours=12)
    token_size = 20

    @classmethod
    def _new(cls, user):
        return super(EmailVerificationToken, cls)._new(user_id=user._fullname,
                                                       email=user.email)

    def valid_for_user(self, user):
        return self.email == user.email


class PasswordResetToken(ConsumableToken):
    _use_db = True
    _connection_pool = "main"
    _ttl = datetime.timedelta(hours=12)
    token_size = 20

    @classmethod
    def _new(cls, user):
        return super(PasswordResetToken, cls)._new(user_id=user._fullname,
                                                   email_address=user.email,
                                                   password=user.password)

    def valid_for_user(self, user):
        return (self.email_address == user.email and
                self.password == user.password)


class AwardClaimToken(ConsumableToken):
    token_size = 20
    _ttl = datetime.timedelta(days=30)
    _defaults = dict(ConsumableToken._defaults.items() + [
                         ("awardfullname", ""),
                         ("description", ""),
                         ("url", ""),
                         ("uid", "")])
    _use_db = True
    _connection_pool = "main"

    @classmethod
    def _new(cls, uid, award, description, url):
        '''Create an AwardClaimToken with the given parameters

        `uid` - A string that uniquely identifies the kind of
                Trophy the user would be claiming.*
        `award_codename` - The codename of the Award the user will claim
        `description` - The description the Trophy will receive
        `url` - The URL the Trophy will receive

        *Note that this differs from Award codenames, because it may be
        desirable to allow users to have multiple copies of the same Award,
        but restrict another aspect of the Trophy. For example, users
        are allowed to have multiple Translator awards, but should only get
        one for each language, so the `unique_award_id`s for those would be
        of the form "i18n_%(language)s"

        '''
        return super(AwardClaimToken, cls)._new(
            awardfullname=award._fullname,
            description=description or "",
            url=url or "",
            uid=uid,
        )

    def post_url(self):
        # Relative URL; should be used on an on-site form
        return "/awards/claim/%s" % self._id

    def confirm_url(self):
        # Full URL; for emailing, PM'ing, etc.
        return "http://%s/awards/confirm/%s" % (g.domain, self._id)

########NEW FILE########
__FILENAME__ = traffic
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
"""
These models represent the traffic statistics stored for subreddits and
promoted links.  They are written to by Pig-based MapReduce jobs and read from
various places in the UI.

All traffic statistics are divided up into three "intervals" of granularity,
hourly, daily, and monthly.  Individual hits are tracked as pageviews /
impressions, and can be safely summed.  Unique hits are tracked as well, but
cannot be summed safely because there's no way to know overlap at this point in
the data pipeline.

"""

import datetime

from pylons import g
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import scoped_session, sessionmaker
from sqlalchemy.orm.exc import NoResultFound
from sqlalchemy.schema import Column
from sqlalchemy.sql.expression import desc, distinct
from sqlalchemy.sql.functions import sum as sa_sum
from sqlalchemy.types import (
    BigInteger,
    DateTime,
    Integer,
    String,
    TypeDecorator,
)

from r2.lib.memoize import memoize
from r2.lib.utils import timedelta_by_name, tup
from r2.models.link import Link


engine = g.dbm.get_engine("traffic")
Session = scoped_session(sessionmaker(bind=engine, autocommit=True))
Base = declarative_base(bind=engine)


def memoize_traffic(**memoize_kwargs):
    """Wrap the memoize decorator and automatically determine memoize key.

    The memoize key is based off the full name (including class name) of the
    method being memoized.

    """
    def memoize_traffic_decorator(fn):
        def memoize_traffic_wrapper(cls, *args, **kwargs):
            method = ".".join((cls.__name__, fn.__name__))
            actual_memoize_decorator = memoize(method, **memoize_kwargs)
            actual_memoize_wrapper = actual_memoize_decorator(fn)
            return actual_memoize_wrapper(cls, *args, **kwargs)
        return memoize_traffic_wrapper
    return memoize_traffic_decorator


class PeekableIterator(object):
    """Iterator that supports peeking at the next item in the iterable."""

    def __init__(self, iterable):
        self.iterator = iter(iterable)
        self.item = None

    def peek(self):
        """Get the next item in the iterable without advancing our position."""
        if not self.item:
            try:
                self.item = self.iterator.next()
            except StopIteration:
                return None
        return self.item

    def next(self):
        """Get the next item in the iterable and advance our position."""
        item = self.peek()
        self.item = None
        return item


def zip_timeseries(*series, **kwargs):
    """Zip timeseries data while gracefully handling gaps in the data.

    Timeseries data is expected to be a sequence of two-tuples (date, values).
    Values is expected itself to be a tuple. The width of the values tuples
    should be the same across all elements in a timeseries sequence. The result
    will be a single sequence in timeseries format.

    Gaps in sequences are filled with an appropriate number of zeros based on
    the size of the first value-tuple of that sequence.

    """

    next_slice = (max if kwargs.get("order", "descending") == "descending"
                  else min)
    iterators = [PeekableIterator(s) for s in series]
    widths = []
    for w in iterators:
        r = w.peek()
        if r:
            date, values = r
            widths.append(len(values))
        else:
            widths.append(0)

    while True:
        items = [it.peek() for it in iterators]
        if not any(items):
            return

        current_slice = next_slice(item[0] for item in items if item)

        data = []
        for i, item in enumerate(items):
            # each item is (date, data)
            if item and item[0] == current_slice:
                data.extend(item[1])
                iterators[i].next()
            else:
                data.extend([0] * widths[i])

        yield current_slice, tuple(data)


def decrement_month(date):
    """Given a truncated datetime, return a new one one month in the past."""

    if date.day != 1:
        raise ValueError("Input must be truncated to the 1st of the month.")

    date -= datetime.timedelta(days=1)
    return date.replace(day=1)


def fill_gaps_generator(time_points, query, *columns):
    """Generate a timeseries sequence with a value for every sample expected.

    Iterate over specified time points and pull the columns listed out of
    query. If the query doesn't have data for a time point, fill the gap with
    an appropriate number of zeroes.

    """

    iterator = PeekableIterator(query)
    for t in time_points:
        row = iterator.peek()

        if row and row.date == t:
            yield t, tuple(getattr(row, c) for c in columns)
            iterator.next()
        else:
            yield t, tuple(0 for c in columns)


def fill_gaps(*args, **kwargs):
    """Listify the generator returned by fill_gaps_generator for `memoize`."""
    generator = fill_gaps_generator(*args, **kwargs)
    return list(generator)


time_range_by_interval = dict(hour=datetime.timedelta(days=4),
                              day=datetime.timedelta(weeks=8),
                              month=datetime.timedelta(weeks=52))


def get_time_points(interval, start_time=None, stop_time=None):
    """Return time points for given interval type.

    Time points are in reverse chronological order to match the sort of
    queries this will be used with. If start_time and stop_time are not
    specified they will be picked based on the interval.

    """

    def truncate_datetime(dt):
        dt = dt.replace(minute=0, second=0, microsecond=0)
        if interval in ("day", "month"):
            dt = dt.replace(hour=0)
        if interval == "month":
            dt = dt.replace(day=1)
        return dt

    if start_time and stop_time:
        start_time, stop_time = sorted([start_time, stop_time])
        # truncate stop_time to an actual traffic time point
        stop_time = truncate_datetime(stop_time)
    else:
        # the stop time is the most recent slice-time; get this by truncating
        # the appropriate amount from the current time
        stop_time = datetime.datetime.utcnow()
        stop_time = truncate_datetime(stop_time)

        # then the start time is easy to work out
        range = time_range_by_interval[interval]
        start_time = stop_time - range

    step = timedelta_by_name(interval)
    current_time = stop_time
    time_points = []

    while current_time >= start_time:
        time_points.append(current_time)
        if interval != 'month':
            current_time -= step
        else:
            current_time = decrement_month(current_time)
    return time_points


def points_for_interval(interval):
    """Calculate the number of data points to render for a given interval."""
    range = time_range_by_interval[interval]
    interval = timedelta_by_name(interval)
    return range.total_seconds() / interval.total_seconds()


def make_history_query(cls, interval):
    """Build a generic query showing the history of a given aggregate."""

    time_points = get_time_points(interval)
    q = (Session.query(cls)
                .filter(cls.date.in_(time_points)))

    # subscription stats doesn't have an interval (it's only daily)
    if hasattr(cls, "interval"):
        q = q.filter(cls.interval == interval)

    q = q.order_by(desc(cls.date))

    return time_points, q


def top_last_month(cls, key, ids=None):
    """Aggregate a listing of the top items (by pageviews) last month.

    We use the last month because it's guaranteed to be fully computed and
    therefore will be more meaningful.

    """

    cur_month = datetime.date.today().replace(day=1)
    last_month = decrement_month(cur_month)

    q = (Session.query(cls)
                .filter(cls.date == last_month)
                .filter(cls.interval == "month")
                .order_by(desc(cls.date), desc(cls.pageview_count)))

    if ids:
        q = q.filter(getattr(cls, key).in_(ids))
    else:
        q = q.limit(55)

    return [(getattr(r, key), (r.unique_count, r.pageview_count))
            for r in q.all()]


class CoerceToLong(TypeDecorator):
    # source:
    # https://groups.google.com/forum/?fromgroups=#!topic/sqlalchemy/3fipkThttQA

    impl = BigInteger

    def process_result_value(self, value, dialect):
        if value is not None:
            value = long(value)
        return value


def sum(column):
    """Wrapper around sqlalchemy.sql.functions.sum to handle BigInteger.

    sqlalchemy returns a Decimal for sum over BigInteger values. Detect the
    column type and coerce to long if it's a BigInteger.

    """

    if isinstance(column.property.columns[0].type, BigInteger):
        return sa_sum(column, type_=CoerceToLong)
    else:
        return sa_sum(column)


def totals(cls, interval):
    """Aggregate sitewide totals for self-serve promotion traffic.

    We only aggregate codenames that start with a link type prefix which
    effectively filters out all DART / 300x100 etc. traffic numbers.

    """

    time_points = get_time_points(interval)

    q = (Session.query(cls.date, sum(cls.pageview_count).label("sum"))
                .filter(cls.interval == interval)
                .filter(cls.date.in_(time_points))
                .filter(cls.codename.startswith(Link._type_prefix))
                .group_by(cls.date)
                .order_by(desc(cls.date)))
    return fill_gaps(time_points, q, "sum")


def total_by_codename(cls, codenames):
    """Return total lifetime pageviews (or clicks) for given codename(s)."""
    codenames = tup(codenames)
    # uses hour totals to get the most up-to-date count
    q = (Session.query(cls.codename, sum(cls.pageview_count))
                       .filter(cls.interval == "hour")
                       .filter(cls.codename.in_(codenames))
                       .group_by(cls.codename))
    return list(q)


def promotion_history(cls, codename, start, stop):
    """Get hourly traffic for a self-serve promotion.

    Traffic stats are summed over all targets for classes that include a target.

    """

    time_points = get_time_points('hour', start, stop)
    q = (Session.query(cls.date, sum(cls.pageview_count))
                .filter(cls.interval == "hour")
                .filter(cls.codename == codename)
                .filter(cls.date.in_(time_points))
                .group_by(cls.date)
                .order_by(cls.date))
    return [(r[0], (r[1],)) for r in q.all()]


def campaign_history(cls, codenames, start, stop):
    """Get hourly traffic for given campaigns."""
    time_points = get_time_points('hour', start, stop)
    q = (Session.query(cls)
                .filter(cls.interval == "hour")
                .filter(cls.codename.in_(codenames))
                .filter(cls.date.in_(time_points))
                .order_by(cls.date))
    return [(r.date, r.codename, r.subreddit, (r.unique_count,
                                               r.pageview_count))
            for r in q.all()]


@memoize("traffic_last_modified", time=60 * 10)
def get_traffic_last_modified():
    """Guess how far behind the traffic processing system is."""
    try:
        return (Session.query(SitewidePageviews.date)
                   .order_by(desc(SitewidePageviews.date))
                   .limit(1)
                   .one()).date
    except NoResultFound:
        return datetime.datetime.min


@memoize("missing_traffic", time=60 * 10)
def get_missing_traffic(start, end):
    """Check for missing hourly traffic between start and end."""

    # NOTE: start, end must be UTC time without tzinfo
    time_points = get_time_points('hour', start, end)
    q = (Session.query(SitewidePageviews.date)
                .filter(SitewidePageviews.interval == "hour")
                .filter(SitewidePageviews.date.in_(time_points)))
    found = [t for (t,) in q]
    return [t for t in time_points if t not in found]


class SitewidePageviews(Base):
    """Pageviews across all areas of the site."""

    __tablename__ = "traffic_aggregate"

    date = Column(DateTime(), nullable=False, primary_key=True)
    interval = Column(String(), nullable=False, primary_key=True)
    unique_count = Column("unique", Integer())
    pageview_count = Column("total", BigInteger())

    @classmethod
    @memoize_traffic(time=3600)
    def history(cls, interval):
        time_points, q = make_history_query(cls, interval)
        return fill_gaps(time_points, q, "unique_count", "pageview_count")


class PageviewsBySubreddit(Base):
    """Pageviews within a subreddit (i.e. /r/something/...)."""

    __tablename__ = "traffic_subreddits"

    subreddit = Column(String(), nullable=False, primary_key=True)
    date = Column(DateTime(), nullable=False, primary_key=True)
    interval = Column(String(), nullable=False, primary_key=True)
    unique_count = Column("unique", Integer())
    pageview_count = Column("total", Integer())

    @classmethod
    @memoize_traffic(time=3600)
    def history(cls, interval, subreddit):
        time_points, q = make_history_query(cls, interval)
        q = q.filter(cls.subreddit == subreddit)
        return fill_gaps(time_points, q, "unique_count", "pageview_count")

    @classmethod
    @memoize_traffic(time=3600 * 6)
    def top_last_month(cls, srs=None):
        ids = [sr.name for sr in srs] if srs else None
        return top_last_month(cls, "subreddit", ids)


class PageviewsBySubredditAndPath(Base):
    """Pageviews within a subreddit with action included.

    `srpath` is the subreddit name, a dash, then the controller method called
    to render the page the user viewed. e.g. reddit.com-GET_listing. This is
    useful to determine how many pageviews in a subreddit are on listing pages,
    comment pages, or elsewhere.

    """

    __tablename__ = "traffic_srpaths"

    srpath = Column(String(), nullable=False, primary_key=True)
    date = Column(DateTime(), nullable=False, primary_key=True)
    interval = Column(String(), nullable=False, primary_key=True)
    unique_count = Column("unique", Integer())
    pageview_count = Column("total", Integer())


class PageviewsByLanguage(Base):
    """Sitewide pageviews correlated by user's interface language."""

    __tablename__ = "traffic_lang"

    lang = Column(String(), nullable=False, primary_key=True)
    date = Column(DateTime(), nullable=False, primary_key=True)
    interval = Column(String(), nullable=False, primary_key=True)
    unique_count = Column("unique", Integer())
    pageview_count = Column("total", BigInteger())

    @classmethod
    @memoize_traffic(time=3600)
    def history(cls, interval, lang):
        time_points, q = make_history_query(cls, interval)
        q = q.filter(cls.lang == lang)
        return fill_gaps(time_points, q, "unique_count", "pageview_count")

    @classmethod
    @memoize_traffic(time=3600 * 6)
    def top_last_month(cls):
        return top_last_month(cls, "lang")


class ClickthroughsByCodename(Base):
    """Clickthrough counts for ads."""

    __tablename__ = "traffic_click"

    codename = Column("fullname", String(), nullable=False, primary_key=True)
    date = Column(DateTime(), nullable=False, primary_key=True)
    interval = Column(String(), nullable=False, primary_key=True)
    unique_count = Column("unique", Integer())
    pageview_count = Column("total", Integer())

    @classmethod
    @memoize_traffic(time=3600)
    def history(cls, interval, codename):
        time_points, q = make_history_query(cls, interval)
        q = q.filter(cls.codename == codename)
        return fill_gaps(time_points, q, "unique_count", "pageview_count")

    @classmethod
    @memoize_traffic(time=3600)
    def promotion_history(cls, codename, start, stop):
        return promotion_history(cls, codename, start, stop)

    @classmethod
    @memoize_traffic(time=3600)
    def historical_totals(cls, interval):
        return totals(cls, interval)

    @classmethod
    @memoize_traffic(time=3600)
    def total_by_codename(cls, codenames):
        return total_by_codename(cls, codenames)


class TargetedClickthroughsByCodename(Base):
    """Clickthroughs for ads, correlated by ad campaign."""

    __tablename__ = "traffic_clicktarget"

    codename = Column("fullname", String(), nullable=False, primary_key=True)
    subreddit = Column(String(), nullable=False, primary_key=True)
    date = Column(DateTime(), nullable=False, primary_key=True)
    interval = Column(String(), nullable=False, primary_key=True)
    unique_count = Column("unique", Integer())
    pageview_count = Column("total", Integer())

    @classmethod
    @memoize_traffic(time=3600)
    def promotion_history(cls, codename, start, stop):
        return promotion_history(cls, codename, start, stop)

    @classmethod
    @memoize_traffic(time=3600)
    def total_by_codename(cls, codenames):
        return total_by_codename(cls, codenames)

    @classmethod
    def campaign_history(cls, codenames, start, stop):
        return campaign_history(cls, codenames, start, stop)


class AdImpressionsByCodename(Base):
    """Impressions for ads."""

    __tablename__ = "traffic_thing"

    codename = Column("fullname", String(), nullable=False, primary_key=True)
    date = Column(DateTime(), nullable=False, primary_key=True)
    interval = Column(String(), nullable=False, primary_key=True)
    unique_count = Column("unique", Integer())
    pageview_count = Column("total", BigInteger())

    @classmethod
    @memoize_traffic(time=3600)
    def history(cls, interval, codename):
        time_points, q = make_history_query(cls, interval)
        q = q.filter(cls.codename == codename)
        return fill_gaps(time_points, q, "unique_count", "pageview_count")

    @classmethod
    @memoize_traffic(time=3600)
    def promotion_history(cls, codename, start, stop):
        return promotion_history(cls, codename, start, stop)

    @classmethod
    @memoize_traffic(time=3600)
    def historical_totals(cls, interval):
        return totals(cls, interval)

    @classmethod
    @memoize_traffic(time=3600)
    def top_last_month(cls):
        return top_last_month(cls, "codename")

    @classmethod
    @memoize_traffic(time=3600)
    def recent_codenames(cls, fullname):
        """Get a list of recent codenames used for 300x100 ads.

        The 300x100 ads get a codename that looks like "fullname_campaign".
        This function gets a list of recent campaigns.

        """
        time_points = get_time_points('day')
        query = (Session.query(distinct(cls.codename).label("codename"))
                        .filter(cls.date.in_(time_points))
                        .filter(cls.codename.startswith(fullname)))
        return [row.codename for row in query]

    @classmethod
    @memoize_traffic(time=3600)
    def total_by_codename(cls, codename):
        return total_by_codename(cls, codename)


class TargetedImpressionsByCodename(Base):
    """Impressions for ads, correlated by ad campaign."""

    __tablename__ = "traffic_thingtarget"

    codename = Column("fullname", String(), nullable=False, primary_key=True)
    subreddit = Column(String(), nullable=False, primary_key=True)
    date = Column(DateTime(), nullable=False, primary_key=True)
    interval = Column(String(), nullable=False, primary_key=True)
    unique_count = Column("unique", Integer())
    pageview_count = Column("total", Integer())

    @classmethod
    @memoize_traffic(time=3600)
    def promotion_history(cls, codename, start, stop):
        return promotion_history(cls, codename, start, stop)

    @classmethod
    @memoize_traffic(time=3600)
    def total_by_codename(cls, codenames):
        return total_by_codename(cls, codenames)

    @classmethod
    def campaign_history(cls, codenames, start, stop):
        return campaign_history(cls, codenames, start, stop)


class SubscriptionsBySubreddit(Base):
    """Subscription statistics for subreddits.

    This table is different from the rest of the traffic ones.  It only
    contains data at a daily interval (hence no `interval` column) and is
    updated separately in the subscribers cron job (see
    reddit-job-subscribers).

    """

    __tablename__ = "traffic_subscriptions"

    subreddit = Column(String(), nullable=False, primary_key=True)
    date = Column(DateTime(), nullable=False, primary_key=True)
    subscriber_count = Column("unique", Integer())

    @classmethod
    @memoize_traffic(time=3600 * 6)
    def history(cls, interval, subreddit):
        time_points, q = make_history_query(cls, interval)
        q = q.filter(cls.subreddit == subreddit)
        return fill_gaps(time_points, q, "subscriber_count")


# create the tables if they don't exist
if g.db_create_tables:
    Base.metadata.create_all()

########NEW FILE########
__FILENAME__ = trylater
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import contextlib
import datetime

from pycassa.system_manager import TIME_UUID_TYPE

from r2.lib.db import tdb_cassandra


class TryLater(tdb_cassandra.View):
    _use_db = True
    _read_consistency_level = tdb_cassandra.CL.QUORUM
    _write_consistency_level = tdb_cassandra.CL.QUORUM
    _compare_with = TIME_UUID_TYPE

    @classmethod
    def multi_ready(cls, rowkeys, cutoff=None):
        if cutoff is None:
            cutoff = datetime.datetime.utcnow()
        return cls._cf.multiget(rowkeys,
                                column_finish=cutoff,
                                column_count=tdb_cassandra.max_column_count)

    @classmethod
    @contextlib.contextmanager
    def multi_handle(cls, rowkeys, cutoff=None):
        if cutoff is None:
            cutoff = datetime.datetime.utcnow()
        ready = cls.multi_ready(rowkeys, cutoff)
        yield ready
        for system, items in ready.iteritems():
            cls._remove(system, items.keys())

    @classmethod
    def schedule(cls, system, data, delay=None):
        if delay is None:
            delay = datetime.timedelta(minutes=60)
        key = datetime.datetime.utcnow() + delay
        cls._set_values(system, {key: data})

########NEW FILE########
__FILENAME__ = vote
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import json
import collections

from r2.lib.db.thing import MultiRelation, Relation
from r2.lib.db import tdb_cassandra
from r2.lib.db.tdb_cassandra import TdbException, ASCII_TYPE, UTF8_TYPE
from r2.lib.db.sorts import epoch_seconds
from r2.lib.utils import SimpleSillyStub, Storage

from account import Account
from link import Link, Comment

from pylons import g
from datetime import datetime, timedelta

__all__ = ['Vote', 'score_changes']

def score_changes(amount, old_amount):
    uc = dc = 0
    a, oa = amount, old_amount
    if oa == 0 and a > 0: uc = a
    elif oa == 0 and a < 0: dc = -a
    elif oa > 0 and a == 0: uc = -oa
    elif oa < 0 and a == 0: dc = oa
    elif oa > 0 and a < 0: uc = -oa; dc = -a
    elif oa < 0 and a > 0: dc = oa; uc = a
    return uc, dc


class VotesByAccount(tdb_cassandra.DenormalizedRelation):
    _use_db = False
    _thing1_cls = Account
    _read_consistency_level = tdb_cassandra.CL.ONE

    @classmethod
    def rel(cls, thing1_cls, thing2_cls):
        if (thing1_cls, thing2_cls) == (Account, Link):
            return LinkVotesByAccount
        elif (thing1_cls, thing2_cls) == (Account, Comment):
            return CommentVotesByAccount

        raise TdbException("Can't find relation for %r(%r,%r)"
                           % (cls, thing1_cls, thing2_cls))

    @classmethod
    def copy_from(cls, pgvote, vote_info):
        rel = cls.rel(Account, pgvote._thing2.__class__)
        rel.create(pgvote._thing1, pgvote._thing2, pgvote=pgvote,
                   vote_info=vote_info)

    @classmethod
    def value_for(cls, thing1, thing2, pgvote, vote_info):
        return pgvote._name


class LinkVotesByAccount(VotesByAccount):
    _use_db = True
    _thing2_cls = Link
    _views = []
    _last_modified_name = "LinkVote"


class CommentVotesByAccount(VotesByAccount):
    _use_db = True
    _thing2_cls = Comment
    _views = []
    _last_modified_name = "CommentVote"


class VoteDetailsByThing(tdb_cassandra.View):
    _use_db = False
    _ttl = timedelta(days=90)
    _fetch_all_columns = True
    _extra_schema_creation_args = dict(key_validation_class=ASCII_TYPE,
                                       default_validation_class=UTF8_TYPE)

    @classmethod
    def create(cls, thing1, thing2s, pgvote, vote_info):
        assert len(thing2s) == 1

        voter = pgvote._thing1
        votee = pgvote._thing2

        details = dict(
            direction=pgvote._name,
            date=epoch_seconds(pgvote._date),
            valid_user=pgvote.valid_user,
            valid_thing=pgvote.valid_thing,
            ip=getattr(pgvote, "ip", ""),
        )
        if vote_info and isinstance(vote_info, basestring):
            details['vote_info'] = vote_info
        cls._set_values(votee._id36, {voter._id36: json.dumps(details)})

    @classmethod
    def get_details(cls, thing):
        if isinstance(thing, Link):
            details_cls = VoteDetailsByLink
        elif isinstance(thing, Comment):
            details_cls = VoteDetailsByComment
        else:
            raise ValueError

        try:
            raw_details = details_cls._byID(thing._id36)
            return raw_details.decode_details()
        except tdb_cassandra.NotFound:
            return []

    def decode_details(self):
        raw_details = self._values()
        details = []
        for key, value in raw_details.iteritems():
            data = Storage(json.loads(value))
            data["_id"] = key + "_" + self._id
            data["voter_id"] = key
            details.append(data)
        details.sort(key=lambda d: d["date"])
        return details


@tdb_cassandra.view_of(LinkVotesByAccount)
class VoteDetailsByLink(VoteDetailsByThing):
    _use_db = True


@tdb_cassandra.view_of(CommentVotesByAccount)
class VoteDetailsByComment(VoteDetailsByThing):
    _use_db = True


class Vote(MultiRelation('vote',
                         Relation(Account, Link),
                         Relation(Account, Comment))):
    @classmethod
    def vote(cls, sub, obj, dir, ip, vote_info = None, cheater = False,
             timer=None, date=None):
        from admintools import valid_user, valid_thing, update_score
        from r2.lib.count import incr_sr_count
        from r2.lib.db import queries

        if timer is None:
            timer = SimpleSillyStub()

        sr = obj.subreddit_slow
        kind = obj.__class__.__name__.lower()
        karma = sub.karma(kind, sr)

        is_self_link = (kind == 'link'
                        and getattr(obj,'is_self',False))

        #check for old vote
        rel = cls.rel(sub, obj)
        oldvote = rel._fast_query(sub, obj, ['-1', '0', '1']).values()
        oldvote = filter(None, oldvote)

        timer.intermediate("pg_read_vote")

        amount = 1 if dir is True else 0 if dir is None else -1

        is_new = False
        #old vote
        if len(oldvote):
            v = oldvote[0]
            oldamount = int(v._name)
            v._name = str(amount)

            #these still need to be recalculated
            old_valid_thing = getattr(v, 'valid_thing', False)
            v.valid_thing = (valid_thing(v, karma, cheater = cheater)
                             and getattr(v,'valid_thing', False))
            v.valid_user = (getattr(v, 'valid_user', False)
                            and v.valid_thing
                            and valid_user(v, sr, karma))
        #new vote
        else:
            is_new = True
            oldamount = 0
            v = rel(sub, obj, str(amount), date=date)
            v.ip = ip
            old_valid_thing = v.valid_thing = valid_thing(v, karma, cheater = cheater)
            v.valid_user = (v.valid_thing and valid_user(v, sr, karma)
                            and not is_self_link)

        v._commit()

        timer.intermediate("pg_write_vote")

        up_change, down_change = score_changes(amount, oldamount)

        if not (is_new and obj.author_id == sub._id and amount == 1):
            # we don't do this if it's the author's initial automatic
            # vote, because we checked it in with _ups == 1
            update_score(obj, up_change, down_change,
                         v, old_valid_thing)
            timer.intermediate("pg_update_score")

        if v.valid_user:
            author = Account._byID(obj.author_id, data=True)
            author.incr_karma(kind, sr, up_change - down_change)
            timer.intermediate("pg_incr_karma")

        #update the sr's valid vote count
        if is_new and v.valid_thing and kind == 'link':
            if sub._id != obj.author_id:
                incr_sr_count(sr)
            timer.intermediate("incr_sr_counts")

        # now write it out to Cassandra. We'll write it out to both
        # this way for a while
        VotesByAccount.copy_from(v, vote_info)
        timer.intermediate("cassavotes")

        queries.changed(v._thing2, True)
        timer.intermediate("changed")

        return v

    @classmethod
    def likes(cls, sub, objs):
        if not sub or not objs:
            return {}

        from r2.models import Account
        assert isinstance(sub, Account)

        rels = {}
        for obj in objs:
            try:
                types = VotesByAccount.rel(sub.__class__, obj.__class__)
            except TdbException:
                # for types for which we don't have a vote rel, we'll
                # skip them
                continue

            rels.setdefault(types, []).append(obj)

        dirs_by_name = {"1": True, "0": None, "-1": False}

        ret = {}
        for relcls, items in rels.iteritems():
            votes = relcls.fast_query(sub, items)
            for cross, name in votes.iteritems():
                ret[cross] = dirs_by_name[name]
        return ret

########NEW FILE########
__FILENAME__ = wiki
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from datetime import datetime, timedelta
from r2.lib.db import tdb_cassandra
from r2.lib.db.thing import NotFound
from r2.lib.merge import *
from pycassa.system_manager import TIME_UUID_TYPE
from pylons import c, g
from pylons.controllers.util import abort
from r2.models.printable import Printable
from r2.models.account import Account
from collections import OrderedDict

import pycassa.types

# Used for the key/id for pages,
PAGE_ID_SEP = '\t'

# Number of days to keep recent revisions for
WIKI_RECENT_DAYS = g.wiki_keep_recent_days

# Max length of a single page in bytes
MAX_PAGE_LENGTH_BYTES = g.wiki_max_page_length_bytes

# Page names which should never be
impossible_namespaces = ('edit/', 'revisions/', 'settings/', 'discussions/', 
                         'revisions/', 'pages/', 'create/')

# Namespaces in which access is denied to do anything but view
restricted_namespaces = ('reddit/', 'config/', 'special/')

# Pages which may only be edited by mods, must be within restricted namespaces
special_pages = ('config/stylesheet', 'config/sidebar',
                 'config/submit_text', 'config/description')

# Pages which have a special length restrictions (In bytes)
special_length_restrictions_bytes = {
    'config/stylesheet': 128*1024,
    'config/submit_text': 1024,
    'config/sidebar': 5120,
    'config/description': 500
}

modactions = {'config/sidebar': "Updated subreddit sidebar",
              'config/submit_text': "Updated submission text",
              'config/description': "Updated subreddit description"}

# Page "index" in the subreddit "reddit.com" and a seperator of "\t" becomes:
#   "reddit.com\tindex"
def wiki_id(sr, page):
    return ('%s%s%s' % (sr, PAGE_ID_SEP, page)).lower()

class ContentLengthError(Exception):
    def __init__(self, max_length):
        Exception.__init__(self)
        self.max_length = max_length

class WikiPageExists(Exception):
    pass

class WikiBadRevision(Exception):
    pass

class WikiPageEditors(tdb_cassandra.View):
    _use_db = True
    _value_type = 'str'
    _connection_pool = 'main'

class WikiRevision(tdb_cassandra.UuidThing, Printable):
    """ Contains content (markdown), author of the edit, page the edit belongs to, and datetime of the edit """
    
    _use_db = True
    _connection_pool = 'main'
    
    _str_props = ('pageid', 'content', 'author', 'reason')
    _bool_props = ('hidden', 'admin_deleted')
    _defaults = {'admin_deleted': False}

    cache_ignore = set(list(_str_props)).union(Printable.cache_ignore).union(['wikipage'])
    
    def get_author(self):
        author = self._get('author')
        return Account._byID36(author, data=True) if author else None
    
    @classmethod
    def get_authors(cls, revisions):
        authors = [r._get('author') for r in revisions]
        authors = filter(None, authors)
        return Account._byID36(authors, data=True)
    
    @classmethod
    def get_printable_authors(cls, revisions):
        from r2.lib.pages import WrappedUser
        authors = cls.get_authors(revisions)
        return dict([(id36, WrappedUser(v))
                     for id36, v in authors.iteritems() if v])
    
    @classmethod
    def add_props(cls, user, wrapped):
        authors = cls.get_printable_authors(wrapped)
        pages = {r.page: None for r in wrapped}
        pages = WikiPage.get_multiple((c.site, page) for page in pages)
        for item in wrapped:
            item._hidden = item.is_hidden
            item._spam = False
            item.wikipage = pages[item.pageid]
            author = item._get('author')
            item.printable_author = authors.get(author, '[unknown]')
            item.reported = False
    
    @classmethod
    def get(cls, revid, pageid):
        wr = cls._byID(revid)
        if wr.pageid != pageid:
            raise WikiBadRevision('Revision is not for the expected page')
        return wr
    
    def toggle_hide(self):
        self.hidden = not self.is_hidden
        self._commit()
        return self.hidden
    
    @classmethod
    def create(cls, pageid, content, author=None, reason=None):
        kw = dict(pageid=pageid, content=content)
        if author:
            kw['author'] = author
        if reason:
            kw['reason'] = reason
        wr = cls(**kw)
        wr._commit()
        WikiRevisionsByPage.add_object(wr)
        WikiRevisionsRecentBySR.add_object(wr)
        return wr
    
    def _on_commit(self):
        WikiRevisionsByPage.add_object(self)
        WikiRevisionsRecentBySR.add_object(self)
    
    @classmethod
    def get_recent(cls, sr, count=100):
        return WikiRevisionsRecentBySR.query([sr._id36], count=count)
    
    @property
    def is_hidden(self):
        return bool(getattr(self, 'hidden', False))
    
    @property
    def info(self, sep=PAGE_ID_SEP):
        info = self.pageid.split(sep, 1)
        try:
            return {'sr': info[0], 'page': info[1]}
        except IndexError:
            g.log.error('Broken wiki page ID "%s" did PAGE_ID_SEP change?', self.pageid)
            return {'sr': 'broken', 'page': 'broken'}
    
    @property
    def page(self):
        return self.info['page']
    
    @property
    def sr(self):
        return self.info['sr']


class WikiPage(tdb_cassandra.Thing):
    """ Contains permissions, current content (markdown), subreddit, and current revision (ID)
        Key is subreddit-pagename """
    
    _use_db = True
    _connection_pool = 'main'
    
    _read_consistency_level = tdb_cassandra.CL.QUORUM
    _write_consistency_level = tdb_cassandra.CL.QUORUM
    
    _date_props = ('last_edit_date')
    _str_props = ('revision', 'name', 'last_edit_by', 'content', 'sr')
    _int_props = ('permlevel')
    _bool_props = ('listed')
    _defaults = {'listed': True}

    def get_author(self):
        if self._get('last_edit_by'):
            return Account._byID36(self.last_edit_by, data=True)
        return None
    
    @classmethod
    def id_for(cls, sr, name):
        id = getattr(sr, '_id36', None)
        if not id:
            raise tdb_cassandra.NotFound
        return wiki_id(id, name)
    
    @classmethod
    def get_multiple(cls, pages):
        """Get multiple wiki pages.
        
        Arguments:
        pages -- list of tuples in the form of [(sr, names),..]
        """
        return cls._byID([cls.id_for(sr, name) for sr, name in pages])
    
    @classmethod
    def get(cls, sr, name):
        return cls._byID(cls.id_for(sr, name))
    
    @classmethod
    def create(cls, sr, name):
        # Sanity check for a page name and subreddit
        if not name or not sr:
            raise ValueError
        name = name.lower()
        kw = dict(sr=sr._id36, name=name, permlevel=0, content='')
        page = cls(**kw)
        page._commit()
        return page
    
    @property
    def restricted(self):
        return WikiPage.is_restricted(self.name)

    @classmethod
    def is_impossible(cls, page):
        return ("%s/" % page) in impossible_namespaces or page.startswith(impossible_namespaces)
    
    @classmethod
    def is_restricted(cls, page):
        return ("%s/" % page) in restricted_namespaces or page.startswith(restricted_namespaces)
    
    @classmethod
    def is_special(cls, page):
        return page in special_pages
    
    @property
    def special(self):
        return WikiPage.is_special(self.name)
    
    def add_to_listing(self):
        WikiPagesBySR.add_object(self)
    
    def _on_create(self):
        self.add_to_listing()
    
    def _on_commit(self):
         self.add_to_listing()
    
    def remove_editor(self, user):
        WikiPageEditors._remove(self._id, [user])
    
    def add_editor(self, user):
        WikiPageEditors._set_values(self._id, {user: ''})
    
    @classmethod
    def get_pages(cls, sr, after=None, filter_check=None):
        NUM_AT_A_TIME = num = 1000
        pages = []
        while num >= NUM_AT_A_TIME:
            wikipages = WikiPagesBySR.query([sr._id36],
                                            after=after,
                                            count=NUM_AT_A_TIME)
            wikipages = list(wikipages)
            num = len(wikipages)
            pages += wikipages
            after = wikipages[-1] if num else None
        return filter(filter_check, pages)
    
    @classmethod
    def get_listing(cls, sr, filter_check=None):
        """
            Create a tree of pages from their path.
        """
        page_tree = OrderedDict()
        pages = cls.get_pages(sr, filter_check=filter_check)
        pages = sorted(pages, key=lambda page: page.name)
        for page in pages:
            p = page.name.split('/')
            cur_node = page_tree
            # Loop through all elements of the path except the page name portion
            for name in p[:-1]:
                next_node = cur_node.get(name)
                # If the element did not already exist in the tree, create it
                if not next_node:
                    new_node = OrderedDict()
                    cur_node[name] = [None, new_node]
                else:
                    # Otherwise, continue through
                    new_node = next_node[1]
                cur_node = new_node
            # Get the actual page name portion of the path
            pagename = p[-1]
            node = cur_node.get(pagename)
            # The node may already exist as a path name in the tree
            if node:
                node[0] = page
            else:
                cur_node[pagename] = [page, OrderedDict()]

        return page_tree, pages
    
    def get_editor_accounts(self):
        editors = self.get_editors()
        accounts = [Account._byID36(editor, data=True)
                    for editor in self.get_editors()]
        accounts = [account for account in accounts
                    if not account._deleted]
        return accounts
    
    def get_editors(self, properties=None):
        try:
            return WikiPageEditors._byID(self._id, properties=properties)._values().keys() or []
        except tdb_cassandra.NotFoundException:
            return []
    
    def has_editor(self, editor):
        return bool(self.get_editors(properties=[editor]))
    
    def revise(self, content, previous = None, author=None, force=False, reason=None):
        if self.content == content:
            return
        force = True if previous is None else force
        max_length = special_length_restrictions_bytes.get(self.name, MAX_PAGE_LENGTH_BYTES)
        if len(content) > max_length:
            raise ContentLengthError(max_length)
        
        revision = getattr(self, 'revision', None)
        
        if not force and (revision and previous != revision):
            if previous:
                origcontent = WikiRevision.get(previous, pageid=self._id).content
            else:
                origcontent = ''
            try:
                content = threewaymerge(origcontent, content, self.content)
            except ConflictException as e:
                e.new_id = revision
                raise e
        
        wr = WikiRevision.create(self._id, content, author, reason)
        self.content = content
        self.last_edit_by = author
        self.last_edit_date = wr.date
        self.revision = str(wr._id)
        self._commit()
        return wr
    
    def change_permlevel(self, permlevel, force=False):
        NUM_PERMLEVELS = 3
        if permlevel == self.permlevel:
            return
        if not force and int(permlevel) not in range(NUM_PERMLEVELS):
            raise ValueError('Permlevel not valid')
        self.permlevel = permlevel
        self._commit()
    
    def get_revisions(self, after=None, count=100):
        return WikiRevisionsByPage.query([self._id], after=after, count=count)
    
    def _commit(self, *a, **kw):
        if not self._id: # Creating a new page
            pageid = wiki_id(self.sr, self.name)
            try:
                WikiPage._byID(pageid)
                raise WikiPageExists()
            except tdb_cassandra.NotFound:
                self._id = pageid   
        return tdb_cassandra.Thing._commit(self, *a, **kw)

class WikiRevisionsByPage(tdb_cassandra.DenormalizedView):
    """ Associate revisions with pages """
    
    _use_db = True
    _connection_pool = 'main'
    _view_of = WikiRevision
    _compare_with = TIME_UUID_TYPE
    
    @classmethod
    def _rowkey(cls, wr):
        return wr.pageid

class WikiPagesBySR(tdb_cassandra.DenormalizedView):
    """ Associate revisions with subreddits, store only recent """
    _use_db = True
    _connection_pool = 'main'
    _view_of = WikiPage
    
    @classmethod
    def _rowkey(cls, wp):
        return wp.sr

class WikiRevisionsRecentBySR(tdb_cassandra.DenormalizedView):
    """ Associate revisions with subreddits, store only recent """
    _use_db = True
    _connection_pool = 'main'
    _view_of = WikiRevision
    _compare_with = TIME_UUID_TYPE
    _ttl = timedelta(days=WIKI_RECENT_DAYS)
    
    @classmethod
    def _rowkey(cls, wr):
        return wr.sr


class ImagesByWikiPage(tdb_cassandra.View):
    _use_db = True
    _read_consistency_level = tdb_cassandra.CL.QUORUM
    _write_consistency_level = tdb_cassandra.CL.QUORUM
    _extra_schema_creation_args = {
        "key_validation_class": pycassa.types.AsciiType(),
        "column_name_class": pycassa.types.UTF8Type(),
        "default_validation_class": pycassa.types.UTF8Type(),
    }

    @classmethod
    def add_image(cls, sr, page_name, image_name, url):
        rowkey = WikiPage.id_for(sr, page_name)
        cls._set_values(rowkey, {image_name: url})

    @classmethod
    def get_images(cls, sr, page_name):
        try:
            rowkey = WikiPage.id_for(sr, page_name)
            return cls._byID(rowkey)._values()
        except tdb_cassandra.NotFound:
            return {}

    @classmethod
    def get_image_count(cls, sr, page_name):
        rowkey = WikiPage.id_for(sr, page_name)
        return cls._cf.get_count(rowkey,
            read_consistency_level=cls._read_consistency_level)

    @classmethod
    def delete_image(cls, sr, page_name, image_name):
        rowkey = WikiPage.id_for(sr, page_name)
        cls._remove(rowkey, [image_name])

########NEW FILE########
__FILENAME__ = test_api
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from r2.tests import RedditTestCase


class AuthorizeNetExceptionTest(RedditTestCase):

    def test_exception_message(self):
        from r2.lib.authorize.api import AuthorizeNetException
        card_number = "<cardNumber>1111222233334444</cardNumber>"
        expected = "<cardNumber>...4444</cardNumber>"
        full_msg = "Wrong Card %s was given"

        exp = AuthorizeNetException(full_msg % (card_number))

        self.assertNotEqual(str(exp), (full_msg % card_number))
        self.assertEqual(str(exp), (full_msg % expected))

class SimpleXMLObjectTest(RedditTestCase):

    def setUp(self):
        from r2.lib.authorize.api import SimpleXMLObject
        self.basic_object = SimpleXMLObject(name="Test",
                                           test="123",
                                           )

    def test_to_xml(self):
        self.assertEqual(self.basic_object.toXML(),
                         "<test>123</test><name>Test</name>",
                         "Unexpected XML produced")

    def test_simple_tag(self):
        from r2.lib.authorize.api import SimpleXMLObject
        xml_output = SimpleXMLObject.simple_tag("cat", "Jini", breed="calico",
                                                               demenor="evil",
                                                               )
        self.assertEqual(xml_output,
                         '<cat breed="calico" demenor="evil">Jini</cat>')

    def test_from_xml(self):
        from r2.lib.authorize.api import SimpleXMLObject
        from BeautifulSoup import BeautifulStoneSoup
        class TestXML(SimpleXMLObject):
            _keys = ["color", "breed"]

        parsed = BeautifulStoneSoup("<dog>" +
                                    "<color>black</color>" +
                                    "<breed>mixed</breed>" +
                                    "<something>else</something>" +
                                    "</dog>")
        constructed = TestXML.fromXML(parsed)
        expected = SimpleXMLObject(color="black",
                                   breed="mixed",
                                   )
        self.assertEqual(constructed.toXML(), expected.toXML(), 
                         "Constructed does not match expected")

    def test_address(self):
        from r2.lib.authorize import Address
        address = Address(firstName="Bob",
                          lastName="Smith",
                          company="Reddit Inc.",
                          address="123 Main St.",
                          city="San Francisco",
                          state="California",
                          zip="12345",
                          country="USA",
                          phoneNumber="415-555-1234",
                          faxNumber="415-555-4321",
                          customerPaymentProfileId="1234567890",
                          customerAddressId="2233",
                          )
        expected = ("<firstName>Bob</firstName>" +
                   "<lastName>Smith</lastName>" +
                   "<company>Reddit Inc.</company>" +
                   "<address>123 Main St.</address>" +
                   "<city>San Francisco</city>" +
                   "<state>California</state>" +
                   "<zip>12345</zip>" +
                   "<country>USA</country>" +
                   "<phoneNumber>415-555-1234</phoneNumber>" +
                   "<faxNumber>415-555-4321</faxNumber>" +
                   "<customerPaymentProfileId>1234567890</customerPaymentProfileId>" +
                   "<customerAddressId>2233</customerAddressId>")

        self.assertEqual(address.toXML(), expected)

    def test_credit_card(self):
        from r2.lib.authorize import CreditCard
        card = CreditCard(cardNumber="1111222233334444",
                          expirationDate="11/22/33",
                          cardCode="123"
                          )
        expected = ("<cardNumber>1111222233334444</cardNumber>" +
                    "<expirationDate>11/22/33</expirationDate>" +
                    "<cardCode>123</cardCode>")
        self.assertEqual(card.toXML(), expected)

    def test_payment_profile(self):
        from r2.lib.authorize.api import PaymentProfile
        profile = PaymentProfile(billTo="Joe",
                                 paymentId="222",
                                 card="1111222233334444",
                                 validationMode="42",
                                 )
        expected = ("<billTo>Joe</billTo>" +
                    "<payment>" +
                        "<creditCard>1111222233334444</creditCard>" +
                    "</payment>" +
                    "<customerPaymentProfileId>222</customerPaymentProfileId>" +
                    "<validationMode>42</validationMode>")
        self.assertEqual(profile.toXML(), expected)

    def test_transation(self):
        from r2.lib.authorize.api import Transaction
        transaction = Transaction(amount="42.42",
                                  profile_id="112233",
                                  pay_id="1111",
                                  trans_id="2222", 
                                  order="42",
                                  )
     
        expected = ("<transaction>" +
                        "<amount>42.42</amount>" +
                        "<customerProfileId>112233</customerProfileId>" +
                        "<customerPaymentProfileId>1111</customerPaymentProfileId>" +
                        "<transId>2222</transId>" +
                        "<order>42</order>" +
                    "</transaction>")
        self.assertEqual(transaction.toXML(), expected)
    
class ImportTest(RedditTestCase):

    def test_importable(self):
        #validator
        from r2.lib.authorize import Address, CreditCard
        #promotecontroller
        from r2.lib.authorize import (
                                      get_account_info,
                                      edit_profile,
                                      PROFILE_LIMIT,
                                      )
        #promote.py
        from r2.lib.authorize import (
                                      auth_transaction,
                                      charge_transaction,
                                      is_charged_transaction,
                                      void_transaction,
                                      )

########NEW FILE########
__FILENAME__ = cssfilter_test
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import unittest

from r2.lib.cssfilter import validate_css


class TestCSSFilter(unittest.TestCase):
    def assertInvalid(self, css):
        serialized, errors = validate_css(css, {})
        self.assertNotEqual(errors, [])

    def test_offsite_url(self):
        testcase = u"*{background-image:url('http://foobar/')}"
        self.assertInvalid(testcase)

    def test_nested_url(self):
        testcase = u"*{background-image:calc(url('http://foobar/'))}"
        self.assertInvalid(testcase)

    def test_url_prelude(self):
        testcase = u"*[foo=url('http://foobar/')]{color:red;}"
        self.assertInvalid(testcase)

    def test_invalid_property(self):
        testcase = u"*{foo: red;}"
        self.assertInvalid(testcase)

    def test_import(self):
        testcase = u"@import 'foobar'; *{}"
        self.assertInvalid(testcase)

    def test_import_rule(self):
        testcase = u"*{ @import 'foobar'; }"
        self.assertInvalid(testcase)

    # IE<8 XSS
    def test_invalid_function(self):
        testcase = u"*{color:expression(alert(1));}"
        self.assertInvalid(testcase)

    def test_invalid_function_prelude(self):
        testcase = u"*[foo=expression(alert(1))]{color:red;}"
        self.assertInvalid(testcase)

    # Safari 5.x parser resynchronization issues
    def test_semicolon_function(self):
        testcase = u"*{color: calc(;color:red;);}"
        self.assertInvalid(testcase)

    def test_semicolon_block(self):
        testcase = u"*{color: [;color:red;];}"
        self.assertInvalid(testcase)

    # Safari 5.x prelude escape
    def test_escape_prelude(self):
        testcase = u"*[foo=bar{}*{color:blue}]{color:red;}"
        self.assertInvalid(testcase)

    # Multi-browser url() escape via spaces inside quotes
    def test_escape_url(self):
        testcase = u"*{background-image: url('foo bar');}"
        self.assertInvalid(testcase)

    # Control chars break out of quotes in multiple browsers
    def test_control_chars(self):
        testcase = u"*{font-family:'foobar\x03;color:red;';}"
        self.assertInvalid(testcase)

    def test_embedded_nulls(self):
        testcase = u"*{font-family:'foo\x00bar'}"
        self.assertInvalid(testcase)

    # Firefox allows backslashes in function names
    def test_escaped_url(self):
        testcase = u"*{background-image:\\u\\r\\l('http://foobar/')}"
        self.assertInvalid(testcase)

    # IE<8 allows backslash escapes in place of pretty much any char
    def test_escape_function_obfuscation(self):
        testcase = u"*{color: expression\\28 alert\\28 1 \\29 \\29 }"
        self.assertInvalid(testcase)

    # This is purely speculative, and may never affect actual browsers
    # https://developer.mozilla.org/en-US/docs/Web/CSS/attr
    def test_attr_url(self):
        testcase = u"*{background-image:attr(foobar url);}"
        self.assertInvalid(testcase)

########NEW FILE########
__FILENAME__ = permissions_test
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import unittest

from r2.tests import stage_for_paste

stage_for_paste()

from r2.lib.permissions import PermissionSet, ModeratorPermissionSet

class TestPermissionSet(PermissionSet):
    info = dict(x={}, y={})

class PermissionSetTest(unittest.TestCase):
    def test_dumps(self):
        self.assertEquals(
            '+all', PermissionSet(all=True).dumps())
        self.assertEquals(
            '+all', PermissionSet(all=True, other=True).dumps())
        self.assertEquals(
            '+a,-b', PermissionSet(a=True, b=False).dumps())

    def test_loads(self):
        self.assertEquals("", TestPermissionSet.loads(None).dumps())
        self.assertEquals("", TestPermissionSet.loads("").dumps())
        self.assertEquals("+x,+y", TestPermissionSet.loads("+x,+y").dumps())
        self.assertEquals("+x,-y", TestPermissionSet.loads("+x,-y").dumps())
        self.assertEquals("+all", TestPermissionSet.loads("+x,-y,+all").dumps())
        self.assertEquals("+x,-y,+z",
                          TestPermissionSet.loads("+x,-y,+z").dumps())
        self.assertRaises(ValueError,
                          TestPermissionSet.loads, "+x,-y,+z", validate=True)
        self.assertEquals(
            "+x,-y",
            TestPermissionSet.loads("-all,+x,-y", validate=True).dumps())

    def test_is_superuser(self):
        perm_set = PermissionSet()
        self.assertFalse(perm_set.is_superuser())
        perm_set[perm_set.ALL] = True
        self.assertTrue(perm_set.is_superuser())
        perm_set[perm_set.ALL] = False
        self.assertFalse(perm_set.is_superuser())

    def test_is_valid(self):
        perm_set = PermissionSet()
        self.assertFalse(perm_set.is_valid())

        perm_set = TestPermissionSet()
        self.assertTrue(perm_set.is_valid())
        perm_set['x'] = True
        self.assertTrue(perm_set.is_valid())
        perm_set[perm_set.ALL] = True
        self.assertTrue(perm_set.is_valid())
        perm_set['z'] = True
        self.assertFalse(perm_set.is_valid())

    def test_getitem(self):
        perm_set = PermissionSet()
        perm_set[perm_set.ALL] = True
        self.assertFalse(perm_set['x'])

        perm_set = TestPermissionSet()
        perm_set['x'] = True
        self.assertTrue(perm_set['x'])
        self.assertFalse(perm_set['y'])
        perm_set['x'] = False
        self.assertFalse(perm_set['x'])
        perm_set[perm_set.ALL] = True
        self.assertTrue(perm_set['x'])
        self.assertTrue(perm_set['y'])
        self.assertFalse(perm_set['z'])
        self.assertTrue(perm_set.get('x', False))
        self.assertFalse(perm_set.get('z', False))
        self.assertTrue(perm_set.get('z', True))


class ModeratorPermissionSetTest(unittest.TestCase):
    def test_loads(self):
        self.assertTrue(ModeratorPermissionSet.loads(None).is_superuser())
        self.assertFalse(ModeratorPermissionSet.loads('').is_superuser())


########NEW FILE########
__FILENAME__ = stats_test
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import unittest

from r2.lib import stats

class TimingStatBufferTest(unittest.TestCase):
    def test_tsb(self):
        tsb = stats.TimingStatBuffer()
        self.assertEquals([], list(tsb.flush()))

        for i in xrange(1, 4):
            for j in xrange(i):
                tsb.record(str(i), 0, 0.1 * (j + 1))
        self.assertEquals(
            set([('1', '1|c'),
                 ('1', '100.0|ms'),
                 ('2', '2|c'),
                 ('2', '150.0|ms'),  # (0.1 + 0.2) / 2
                 ('3', '3|c'),
                 ('3', '200.0|ms'),  # (0.1 + 0.2 + 0.3) / 3
                ]), set(tsb.flush()))

class CountingStatBufferTest(unittest.TestCase):
    def test_csb(self):
        csb = stats.CountingStatBuffer()
        self.assertEquals([], list(csb.flush()))

        for i in xrange(1, 4):
            for j in xrange(i):
                csb.record(str(i), j + 1)
        self.assertEquals(
            set([('1', '1|c'),
                 ('2', '3|c'),
                 ('3', '6|c')]),
            set(csb.flush()))

class StringCountBufferTest(unittest.TestCase):
    def test_encode_string(self):
        enc = stats.StringCountBuffer._encode_string
        self.assertEquals('test', enc('test'))
        self.assertEquals('\\n\\&\\\\&', enc('\n|\\&'))

    def test_scb(self):
        scb = stats.StringCountBuffer()
        self.assertEquals([], list(scb.flush()))

        for i in xrange(1, 4):
            for j in xrange(i):
                for k in xrange(j + 1):
                    scb.record(str(i), str(j))
        self.assertEquals(
            set([('1', '1|s|0'),
                 ('2', '1|s|0'),
                 ('2', '2|s|1'),
                 ('3', '1|s|0'),
                 ('3', '2|s|1'),
                 ('3', '3|s|2')]),
            set(scb.flush()))

class FakeUdpSocket:
    def __init__(self, *ignored_args):
        self.host = None
        self.port = None
        self.datagrams = []

    def sendto(self, datagram, host_port):
        self.datagrams.append(datagram)

class StatsdConnectionUnderTest(stats.StatsdConnection):
    _make_socket = FakeUdpSocket

class StatsdConnectionTest(unittest.TestCase):
    @staticmethod
    def connect(compress=False):
         return StatsdConnectionUnderTest('host:1000', compress=compress)

    def test_parse_addr(self):
        self.assertEquals(
            ('1:2', 3), stats.StatsdConnection._parse_addr('1:2:3'))

    def test_send(self):
        conn = self.connect()
        conn.send((i, i) for i in xrange(1, 6))
        self.assertEquals(
            ['1:1\n2:2\n3:3\n4:4\n5:5'],
            conn.sock.datagrams)

        # verify compression
        data = [('a.b.c.w', 1), ('a.b.c.x', 2), ('a.b.c.y', 3), ('a.b.z', 4),
                ('bbb', 5), ('bbc', 6)]
        conn = self.connect(compress=True)
        conn.send(reversed(data))
        self.assertEquals(
            ['a.b.c.w:1\n^06x:2\n^06y:3\n^04z:4\nbbb:5\nbbc:6'],
            conn.sock.datagrams)
        conn = self.connect(compress=False)
        conn.send(reversed(data))
        self.assertEquals(
            ['bbc:6\nbbb:5\na.b.z:4\na.b.c.y:3\na.b.c.x:2\na.b.c.w:1'],
            conn.sock.datagrams)

        # ensure send is a no-op when not connected
        conn.sock = None
        conn.send((i, i) for i in xrange(1, 6))

class StatsdClientUnderTest(stats.StatsdClient):
    @classmethod
    def _data_iterator(cls, x):
       return sorted(iter(x))

    @classmethod
    def _make_conn(cls, addr):
        return StatsdConnectionUnderTest(addr, compress=False)

class StatsdClientTest(unittest.TestCase):
    def test_flush(self):
        client = StatsdClientUnderTest('host:1000')
        client.timing_stats.record('t', 0, 1)
        client.counting_stats.record('c', 1)
        client.flush()
        self.assertEquals(
            ['c:1|c\nt:1000.0|ms\nt:1|c'],
            client.conn.sock.datagrams)

class CounterAndTimerTest(unittest.TestCase):
    @staticmethod
    def client():
        return StatsdClientUnderTest('host:1000')

    def test_get_stat_name(self):
        self.assertEquals(
            'a.b.c',
            stats._get_stat_name('a', '', u'b', None, 'c', 0))

    def test_counter(self):
        c = stats.Counter(self.client(), 'c')
        c.increment('a')
        c.increment('b', 2)
        c.decrement('c')
        c.decrement('d', 2)
        c += 1
        c -= 2
        self.assertEquals(
            set([('c.a', '1|c'),
                 ('c.b', '2|c'),
                 ('c.c', '-1|c'),
                 ('c.d', '-2|c'),
                 ('c', '-1|c')]),
            set(c.client.counting_stats.flush()))
        self.assertEquals(set(), set(c.client.counting_stats.flush()))

    def test_timer(self):
        t = stats.Timer(self.client(), 't')
        t._time = iter(i / 10.0 for i in xrange(10)).next
        self.assertRaises(AssertionError, t.intermediate, 'fail')
        self.assertRaises(AssertionError, t.stop)

        t.start()
        t.intermediate('a')
        t.intermediate('b')
        t.intermediate('c')
        t.stop(subname='t')

        self.assertRaises(AssertionError, t.intermediate, 'fail')
        self.assertRaises(AssertionError, t.stop)
        t.send('x', 0, 0.5)

        self.assertEquals(
            set([('t.a', '1|c'),
                 ('t.a', '100.0|ms'),
                 ('t.b', '1|c'),
                 ('t.b', '100.0|ms'),
                 ('t.c', '1|c'),
                 ('t.c', '100.0|ms'),
                 ('t.t', '1|c'),
                 ('t.t', '400.0|ms'),
                 ('t.x', '1|c'),
                 ('t.x', '500.0|ms')]),
            set(t.client.timing_stats.flush()))
        self.assertEquals(set(), set(t.client.timing_stats.flush()))

########NEW FILE########
__FILENAME__ = tracking_test
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import unittest


MESSAGE = "the quick brown fox jumped over..."
BLOCK_O_PADDING = ("\x10\x10\x10\x10\x10\x10\x10\x10"
                   "\x10\x10\x10\x10\x10\x10\x10\x10")
SECRET = "abcdefghijklmnopqrstuvwxyz"
ENCRYPTED = ("aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaIbzth1QTzJxzHbHGnJywG5V1uR3tWtSB"
             "8hTyIcfg6rUZC4Wo0pT8jkEt9o1c%2FkTn")


class TestPadding(unittest.TestCase):
    def test_pad_empty_string(self):
        from r2.lib.tracking import _pad_message
        padded = _pad_message("")
        self.assertEquals(padded, BLOCK_O_PADDING)

    def test_pad_round_string(self):
        from r2.lib.tracking import _pad_message, KEY_SIZE
        padded = _pad_message("x" * KEY_SIZE)
        self.assertEquals(len(padded), KEY_SIZE * 2)
        self.assertEquals(padded[KEY_SIZE:], BLOCK_O_PADDING)

    def test_unpad_empty_message(self):
        from r2.lib.tracking import _unpad_message
        unpadded = _unpad_message("")
        self.assertEquals(unpadded, "")

    def test_unpad_evil_message(self):
        from r2.lib.tracking import _unpad_message
        evil = ("a" * 88) + chr(57)
        result = _unpad_message(evil)
        self.assertEquals(result, "")

    def test_padding_roundtrip(self):
        from r2.lib.tracking import _unpad_message, _pad_message
        tested = _unpad_message(_pad_message(MESSAGE))
        self.assertEquals(MESSAGE, tested)


class TestEncryption(unittest.TestCase):
    def test_salt(self):
        from r2.lib.tracking import _make_salt, SALT_SIZE
        self.assertEquals(len(_make_salt()), SALT_SIZE)

    def test_encrypt(self):
        from r2.lib.tracking import _encrypt, SALT_SIZE
        encrypted = _encrypt(
            "a" * SALT_SIZE,
            MESSAGE,
            SECRET,
        )
        self.assertEquals(encrypted, ENCRYPTED)

    def test_decrypt(self):
        from r2.lib.tracking import _decrypt
        decrypted = _decrypt(ENCRYPTED, SECRET)
        self.assertEquals(MESSAGE, decrypted)

########NEW FILE########
__FILENAME__ = utils_test
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import collections
import unittest

from r2.lib import utils


class UtilsTest(unittest.TestCase):
    def test_weighted_lottery_errors(self):
        self.assertRaises(ValueError, utils.weighted_lottery, {})
        self.assertRaises(ValueError, utils.weighted_lottery, {'x': 0})
        self.assertRaises(
            ValueError, utils.weighted_lottery,
            collections.OrderedDict([('x', -1), ('y', 1)]))

    def test_weighted_lottery(self):
        weights = collections.OrderedDict(
            [('x', 2), (None, 0), (None, 0), ('y', 3), ('z', 1)])

        def expect(result, random_value):
            scaled_r = float(random_value) / sum(weights.itervalues())
            self.assertEquals(
                result,
                utils.weighted_lottery(weights, _random=lambda: scaled_r))

        expect('x', 0)
        expect('x', 1)
        expect('y', 2)
        expect('y', 3)
        expect('y', 4)
        expect('z', 5)
        self.assertRaises(ValueError, expect, None, 6)


class TestCanonicalizeEmail(unittest.TestCase):
    def test_empty_string(self):
        canonical = utils.canonicalize_email("")
        self.assertEquals(canonical, "")

    def test_unicode(self):
        canonical = utils.canonicalize_email(u"\u2713@example.com")
        self.assertEquals(canonical, "\xe2\x9c\x93@example.com")

    def test_localonly(self):
        canonical = utils.canonicalize_email("invalid")
        self.assertEquals(canonical, "")

    def test_multiple_ats(self):
        canonical = utils.canonicalize_email("invalid@invalid@invalid")
        self.assertEquals(canonical, "")

    def test_remove_dots(self):
        canonical = utils.canonicalize_email("d.o.t.s@example.com")
        self.assertEquals(canonical, "dots@example.com")

    def test_remove_plus_address(self):
        canonical = utils.canonicalize_email("fork+nork@example.com")
        self.assertEquals(canonical, "fork@example.com")

    def test_unicode_in_byte_str(self):
        # this shouldn't ever happen, but some entries in postgres appear
        # to be byte strings with non-ascii in 'em.
        canonical = utils.canonicalize_email("\xe2\x9c\x93@example.com")
        self.assertEquals(canonical, "\xe2\x9c\x93@example.com")

########NEW FILE########
__FILENAME__ = subreddit_test
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import unittest

from r2.lib.permissions import PermissionSet

from r2.models.account import Account
from r2.models.subreddit import SRMember, Subreddit

class TestPermissionSet(PermissionSet):
    info = dict(x={}, y={})


class SRMemberTest(unittest.TestCase):
    def setUp(self):
        a = Account()
        a._commit()
        sr = Subreddit()
        sr._commit()
        self.rel = SRMember(sr, a, 'test')

    def test_get_permissions(self):
        self.assertRaises(NotImplementedError, self.rel.get_permissions)
        self.rel._permission_class = TestPermissionSet
        self.assertEquals('', self.rel.get_permissions().dumps())
        self.rel.encoded_permissions = '+x,-y'
        self.assertEquals('+x,-y', self.rel.get_permissions().dumps())

    def test_has_permission(self):
        self.assertRaises(NotImplementedError, self.rel.has_permission, 'x')
        self.rel._permission_class = TestPermissionSet
        self.assertFalse(self.rel.has_permission('x'))
        self.rel.encoded_permissions = '+x,-y'
        self.assertTrue(self.rel.has_permission('x'))
        self.assertFalse(self.rel.has_permission('y'))
        self.rel.encoded_permissions = '+all'
        self.assertTrue(self.rel.has_permission('x'))
        self.assertTrue(self.rel.has_permission('y'))
        self.assertFalse(self.rel.has_permission('z'))

    def test_update_permissions(self):
        self.assertRaises(NotImplementedError,
                          self.rel.update_permissions, x=True)
        self.rel._permission_class = TestPermissionSet
        self.rel.update_permissions(x=True, y=False)
        self.assertEquals('+x,-y', self.rel.encoded_permissions)
        self.rel.update_permissions(x=None)
        self.assertEquals('-y', self.rel.encoded_permissions)
        self.rel.update_permissions(y=None, z=None)
        self.assertEquals('', self.rel.encoded_permissions)
        self.rel.update_permissions(x=True, y=False, all=True)
        self.assertEquals('+all', self.rel.encoded_permissions)

    def test_set_permissions(self):
        self.rel.set_permissions(PermissionSet(x=True, y=False))
        self.assertEquals('+x,-y', self.rel.encoded_permissions)

    def test_is_superuser(self):
        self.assertRaises(NotImplementedError, self.rel.is_superuser)
        self.rel._permission_class = TestPermissionSet
        self.assertFalse(self.rel.is_superuser())
        self.rel.encoded_permissions = '+all'
        self.assertTrue(self.rel.is_superuser())

if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = updateini
#!/usr/bin/env python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from ConfigParser import MissingSectionHeaderError
from StringIO import StringIO
import sys

from r2.lib.utils import parse_ini_file

HEADER = '''
# YOU DO NOT NEED TO EDIT THIS FILE
# This is a generated file. To update the configuration,
# edit the *.update file of the same name, and then
# run 'make ini'
# Configuration settings in the *.update file will override
# or be added to the base 'example.ini' file.
'''

def main(source_ini, update_ini):
    with open(source_ini) as source:
        parser = parse_ini_file(source)
    with open(update_ini) as f:
        updates = f.read()
    try:
        # Existing *.update files don't include section
        # headers; inject a [DEFAULT] header if the parsing
        # fails
        parser.readfp(StringIO(updates))
    except MissingSectionHeaderError:
        updates = "[DEFAULT]\n" + updates
        parser.readfp(StringIO(updates))
    print HEADER
    parser.write(sys.stdout)

if __name__ == '__main__':
    args = sys.argv
    if len(args) != 3:
        print 'usage: %s [source] [update]' % sys.argv[0]
        sys.exit(1)
    else:
        main(sys.argv[1], sys.argv[2])

########NEW FILE########
__FILENAME__ = clean_static_files
#!/usr/bin/python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Clean up the static files S3 bucket.

This script removes static files that are no longer used from the S3 bucket.

"""

import datetime
import itertools
import os
import subprocess

from pylons import g

from r2.lib.db.operators import desc
from r2.lib.plugin import PluginLoader
from r2.lib.utils import fetch_things2
from r2.lib.utils import read_static_file_config
from r2.models import Subreddit
import r2


def get_mature_files_on_s3(bucket):
    """Enumerate files currently on S3 that are older than one day."""

    minimum_age = datetime.timedelta(days=1)
    minimum_birthdate = datetime.datetime.utcnow() - minimum_age

    remote_files = {}
    for key in bucket.list():
        last_modified = datetime.datetime.strptime(key.last_modified,
                                                   "%Y-%m-%dT%H:%M:%S.%fZ")
        if last_modified < minimum_birthdate:
            remote_files[key.name] = key
    return remote_files


def _get_repo_source_static_files(package_root):
    static_file_root = os.path.join(package_root, "public", "static")
    old_root = os.getcwd()

    try:
        os.chdir(static_file_root)
    except OSError:
        # this repo has no static files!
        return

    try:
        git_files_string = subprocess.check_output([
            "git", "ls-tree", "-r", "--name-only", "HEAD", static_file_root])
        git_files = git_files_string.splitlines()
        prefix = os.path.commonprefix(git_files)
        for path in git_files:
            filename = path[len(prefix):]
            yield filename
    finally:
        os.chdir(old_root)


def get_source_static_files(plugins):
    """List all static files that are committed to the git repository."""

    package_root = os.path.dirname(r2.__file__)
    # oh "yield from", how i wish i had thee.
    for filename in _get_repo_source_static_files(package_root):
        yield filename

    for plugin in plugins:
        for filename in _get_repo_source_static_files(plugin.path):
            yield filename


def get_generated_static_files():
    """List all static files that are generated by the build process."""
    PluginLoader()  # ensure all the plugins put their statics in
    for filename, mangled in g.static_names.iteritems():
        yield filename
        yield mangled

        _, ext = os.path.splitext(filename)
        if ext in (".css", ".js"):
            yield filename + ".gzip"
            yield mangled + ".gzip"


def clean_static_files(config_file):
    bucket, config = read_static_file_config(config_file)
    ignored_prefixes = tuple(p.strip() for p in
                             config["ignored_prefixes"].split(","))

    plugins = PluginLoader()
    reachable_files = itertools.chain(
        get_source_static_files(plugins),
        get_generated_static_files(),
    )

    condemned_files = get_mature_files_on_s3(bucket)
    for reachable_file in reachable_files:
        if reachable_file in condemned_files:
            del condemned_files[reachable_file]

    for filename, key in condemned_files.iteritems():
        if not filename.startswith(ignored_prefixes):
            key.delete()

########NEW FILE########
__FILENAME__ = geoip_service
#!/usr/bin/python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
############################################################################### 
"""
This is a tiny Flask app used for geoip lookups against a maxmind database.

If you are using this service be sure to set `geoip_location` in your ini file.

"""

import json

import GeoIP
from flask import Flask, make_response

application = Flask(__name__)

# SET THESE PATHS TO YOUR MAXMIND GEOIP LEGACY DATABASES
# http://dev.maxmind.com/geoip/legacy/geolite/
COUNTRY_DB_PATH = '/usr/share/GeoIP/GeoIP.dat'
CITY_DB_PATH = '/usr/share/GeoIP/GeoIPCity.dat'
ORG_DB_PATH = '/usr/share/GeoIP/GeoIPOrg.dat'


try:
    gc = GeoIP.open(COUNTRY_DB_PATH, GeoIP.GEOIP_MEMORY_CACHE)
except:
    gc = None

try:
    gi = GeoIP.open(CITY_DB_PATH, GeoIP.GEOIP_MEMORY_CACHE)
except:
    gi = None

try:
    go = GeoIP.open(ORG_DB_PATH, GeoIP.GEOIP_MEMORY_CACHE)
except:
    go = None


def json_response(result):
    json_output = json.dumps(result, ensure_ascii=False, encoding='iso-8859-1')
    response = make_response(json_output.encode('utf-8'), 200)
    response.headers['Content-Type'] = 'application/json; charset=utf-8'
    return response


@application.route('/geoip/<ips>')
def get_record(ips):
    result = {}
    ips = ips.split('+')

    if gi:
        for ip in ips:
            result[ip] = gi.record_by_addr(ip)
    elif gc:
        for ip in ips:
            result[ip] = {
                'country_code': gc.country_code_by_addr(ip),
                'country_name': gc.country_name_by_addr(ip),
            }

    return json_response(result)


@application.route('/org/<ips>')
def get_organizations(ips):
    result = {}
    ips = ips.split('+')

    if go:
        for ip in ips:
             result[ip] = go.org_by_addr(ip)

    return json_response(result)


if __name__ == "__main__":
    application.run()

########NEW FILE########
__FILENAME__ = log_q
#! /usr/bin/python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################


from r2.lib import amqp, emailer
from pylons import g
from datetime import datetime
from md5 import md5
from random import shuffle, choice

import pickle

try:
    words = file(g.words_file).read().split("\n")
except IOError:
    words = []

shuffle(words)

def randword():
    try:
        return choice(words)
    except IndexError:
        return '???'

q = 'log_q'

def run(streamfile=None, verbose=False):
    if streamfile:
        stream_fp = open(streamfile, "a")
    else:
        stream_fp = None

    def streamlog(msg, important=False):
        if stream_fp:
            stream_fp.write(msg + "\n")
            stream_fp.flush()
        if important:
            print msg

    def add_timestamps (d):
        d['hms'] = d['time'].strftime("%H:%M:%S")

        d['occ'] = "<%s:%s, pid=%-5s, %s>" % (d['host'], d['port'], d['pid'],
                                      d['time'].strftime("%Y-%m-%d %H:%M:%S"))

    def limited_append(l, item):
        if len(l) >= 25:
            l.pop(12)
        l.append(item)

    def log_exception(d, daystring):
        exc_desc = d['exception_desc']
        exc_type = d['exception_type']

        exc_str = "%s: %s" % (exc_type, exc_desc)

        add_timestamps(d)

        tb = []

        key_material = exc_type
        pretty_lines = []

        make_lock_seen = False
        flaky_db_seen = False
        cassandra_seen = False

        for tpl in d['traceback']:
            tb.append(tpl)
            filename, lineno, funcname, text = tpl
            if text is None:
                pass
            elif (text.startswith("with g.make_lock(") or
                  text.startswith("with make_lock(")):
                make_lock_seen = True
            elif (text.startswith("(ProgrammingError) server closed the connection")):
                flaky_db_seen = True
            if '/cassandra/' in filename.lower():
                cassandra_seen = True
            if '/pycassa/' in filename.lower():
                cassandra_seen = True
            key_material += "%s %s " % (filename, funcname)
            pretty_lines.append ("%s:%s: %s()" % (filename, lineno, funcname))
            pretty_lines.append ("    %s" % text)

        if exc_desc.startswith("QueuePool limit of size"):
            fingerprint = "QueuePool_overflow"
        elif exc_desc.startswith("error 2 from memcached_get: HOSTNAME "):
            fingerprint = "memcache_suckitude"
        elif exc_type == "TimeoutExpired" and make_lock_seen:
            fingerprint = "make_lock_timeout"
        elif exc_desc.startswith("(OperationalError) FATAL: the database " +
                                 "system is in recovery mode"):
            fingerprint = "recovering_db"
        elif exc_desc.startswith("(OperationalError) could not connect " +
                                 "to server"):
            fingerprint = "unconnectable_db"
        elif exc_desc.startswith("(OperationalError) server closed the " +
                                 "connection unexpectedly"):
            fingerprint = "flaky_db_op"
        elif cassandra_seen:
            fingerprint = "something's wrong with cassandra"
        else:
            fingerprint = md5(key_material).hexdigest()

        nickname_key = "error_nickname-" + fingerprint
        status_key = "error_status-" + fingerprint

        nickname = g.hardcache.get(nickname_key)

        if nickname is None:
            nickname = '"%s" Exception' % randword().capitalize()
            news = ("A new kind of thing just happened! " +
                    "I'm going to call it a %s\n\n" % nickname)

            news += "Where and when: %s\n\n" % d['occ']
            news += "Traceback:\n"
            news += "\n".join(pretty_lines)
            news += exc_str
            news += "\n"

            emailer.nerds_email(news, "Exception Watcher")

            g.hardcache.set(nickname_key, nickname, 86400 * 365)
            g.hardcache.set(status_key, "new", 86400)

        if g.hardcache.get(status_key) == "fixed":
            g.hardcache.set(status_key, "new", 86400)
            news = "This was marked as fixed: %s\n" % nickname
            news += "But it just occurred, so I'm marking it new again."
            emailer.nerds_email(news, "Exception Watcher")

        err_key = "-".join(["error", daystring, fingerprint])

        existing = g.hardcache.get(err_key)

        if not existing:
            existing = dict(exception=exc_str, traceback=tb, occurrences=[])

        existing.setdefault('times_seen', 0)
        existing['times_seen'] += 1

        limited_append(existing['occurrences'], d['occ'])

        g.hardcache.set(err_key, existing, 7 * 86400)

        streamlog ("%s [X] %-70s" % (d['hms'], nickname), verbose)

    def log_text(d, daystring):
        add_timestamps(d)
        char = d['level'][0].upper()
        streamlog ("%s [%s] %r" % (d['hms'], char, d['text']), verbose)
        logclass_key = "logclass-" + d['classification']

        if not g.hardcache.get(logclass_key):
            g.hardcache.set(logclass_key, True, 86400 * 90)

            if d['level'] != 'debug':
                news = "The code just generated a [%s] message.\n" % \
                       d['classification']
                news += "I don't remember ever seeing one of those before.\n"
                news += "\n"
                news += "It happened on: %s\n" % d['occ']
                news += "The log level was: %s\n" % d['level']
                news += "The complete text was:\n"
                news += repr(d['text'])
                emailer.nerds_email (news, "reddit secretary")

        occ_key = "-".join(["logtext", daystring,
                            d['level'], d['classification']])

        occurrences = g.hardcache.get(occ_key)

        if occurrences is None:
            occurrences = []

        d2 = {}

        d2['occ'] = d['occ']
        d2['text'] = repr(d['text'])

        limited_append(occurrences, d2)
        g.hardcache.set(occ_key, occurrences, 86400 * 7)

    def myfunc(msg):
        daystring = datetime.now(g.display_tz).strftime("%Y/%m/%d")

        try:
            d = pickle.loads(msg.body)
        except TypeError:
            streamlog ("wtf is %r" % msg.body, True)
            return

        if not 'type' in d:
            streamlog ("wtf is %r" % d, True)
        elif d['type'] == 'exception':
            try:
                log_exception(d, daystring)
            except Exception as e:
                print "Error in log_exception(): %r" % e
        elif d['type'] == 'text':
            try:
                log_text(d, daystring)
            except Exception as e:
                print "Error in log_text(): %r" % e
        else:
            streamlog ("wtf is %r" % d['type'], True)

    amqp.consume_items(q, myfunc, verbose=verbose)


########NEW FILE########
__FILENAME__ = domainban_to_zookeeper
"""Move the 'banned domains' list from its current hardcache location to its
new location in Zookeeper
"""
from pylons import g

from r2.lib.zookeeper import LiveDict

zkbans = LiveDict(g.zookeeper, "/banned-domains", watch=False)

hcb = g.hardcache.backend
for domain in hcb.ids_by_category("domain", limit=5000):
    domain_info = g.hardcache.get("domain-" + domain)
    zkbans[domain] = domain_info

########NEW FILE########
__FILENAME__ = gilded_comments
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Fill in the gilded comment listing.

This listing is stored in get_gilded_comments and seen on /comments/gilded.

"""

import datetime

from pylons import g

from r2.lib.db.queries import get_gilded_comments, get_all_gilded_comments
from r2.lib.utils import Storage
from r2.models import GildingsByDay, Thing, Comment
from r2.models.query_cache import CachedQueryMutator


date = datetime.datetime.now(g.tz)
earliest_date = datetime.datetime(2012, 10, 01, tzinfo=g.tz)

already_seen = set()

with CachedQueryMutator() as m:
    while date > earliest_date:
        gildings = GildingsByDay.get_gildings(date)
        fullnames = [x["thing"] for x in gildings]
        things = Thing._by_fullname(fullnames, data=True, return_dict=False)
        comments = {t._fullname: t for t in things if isinstance(t, Comment)}

        for gilding in gildings:
            fullname = gilding["thing"]
            if fullname in comments and fullname not in already_seen:
                thing = gilding["thing"] = comments[fullname]
                gilding_object = Storage(gilding)
                m.insert(get_gilded_comments(thing.sr_id), [gilding_object])
                m.insert(get_all_gilded_comments(), [gilding_object])
                already_seen.add(fullname)
        date -= datetime.timedelta(days=1)

########NEW FILE########
__FILENAME__ = gilded_user_comments
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Fill in the gilded comment listing for users.

This listing is stored in get_gilded_user_comments and seen on
/user/<username>/gilded.

"""

import datetime

from pylons import g

from r2.lib.db.queries import get_gilded_user_comments
from r2.lib.utils import Storage
from r2.models import GildingsByDay, Thing, Comment
from r2.models.query_cache import CachedQueryMutator


date = datetime.datetime.now(g.tz)
earliest_date = datetime.datetime(2012, 10, 01, tzinfo=g.tz)

already_seen = set()

with CachedQueryMutator() as m:
    while date > earliest_date:
        gildings = GildingsByDay.get_gildings(date)
        fullnames = [x["thing"] for x in gildings]
        things = Thing._by_fullname(fullnames, data=True, return_dict=False)
        comments = {t._fullname: t for t in things if isinstance(t, Comment)}

        for gilding in gildings:
            fullname = gilding["thing"]
            if fullname in comments and fullname not in already_seen:
                thing = gilding["thing"] = comments[fullname]
                gilding_object = Storage(gilding)
                m.insert(get_gilded_user_comments(thing.author_id),
                         [gilding_object])
                already_seen.add(fullname)
        date -= datetime.timedelta(days=1)

########NEW FILE########
__FILENAME__ = modaction_by_srandmod
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
from r2.lib.db.operators import asc
from r2.lib.utils import fetch_things2
from r2.models import ModAction, ModActionBySRActionMod, Subreddit

def backfill(after=None):
    q = Subreddit._query(sort=asc('_date'))
    if after:
        sr = Subreddit._by_name(after)
        q = q._after(sr)

    for sr in fetch_things2(q):
        backfill_sr(sr)


def backfill_sr(sr):
    print "processing %s" % sr.name
    after = None
    count = 100
    q = ModAction.get_actions(sr, after=after, count=count)
    actions = list(q)
    while actions:
        for ma in actions:
            ModActionBySRActionMod.add_object(ma)
        q = ModAction.get_actions(sr, after=actions[-1], count=count)
        actions = list(q)

########NEW FILE########
__FILENAME__ = modmsgtime
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Ensure modmsgtime is properly set on all accounts.

See the comment in Account.is_moderator_somewhere for possible values of this
attribute now.

"""

from r2.lib.db.operators import desc
from r2.lib.utils import fetch_things2, progress
from r2.models import Account, Subreddit


all_accounts = Account._query(sort=desc("_date"))
for account in progress(fetch_things2(all_accounts)):
    is_moderator_somewhere = bool(Subreddit.reverse_moderator_ids(account))
    if is_moderator_somewhere:
        if not account.modmsgtime:
            account.modmsgtime = False
        else:
            # the account already has a date for modmsgtime meaning unread mail
            pass
    else:
        account.modmsgtime = None
    account._commit()

########NEW FILE########
__FILENAME__ = subreddit_images

# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

import urllib2

from pylons import g

from r2.lib.db.operators import desc
from r2.lib.utils import fetch_things2
from r2.lib.media import upload_media
from r2.models.subreddit import Subreddit
from r2.models.wiki import WikiPage, ImagesByWikiPage


all_subreddits = Subreddit._query(sort=desc("_date"))
for sr in fetch_things2(all_subreddits):
    images = sr.images.copy()
    images.pop("/empties/", None)

    if not images:
        continue

    print 'Processing /r/%s (id36: %s)' % (sr.name, sr._id36)

    # upgrade old-style image ids to urls
    for name, image_url in images.items():
        if not isinstance(image_url, int):
            continue

        print "  upgrading image %r" % image_url
        url = "http://%s/%s_%d.png" % (g.s3_old_thumb_bucket,
                                       sr._fullname, image_url)
        image_data = urllib2.urlopen(url).read()
        new_url = upload_media(image_data, file_type=".png")
        images[name] = new_url

    # use a timestamp of zero to make sure that we don't overwrite any changes
    # from live dual-writes.
    rowkey = WikiPage.id_for(sr, "config/stylesheet")
    ImagesByWikiPage._cf.insert(rowkey, images, timestamp=0)

########NEW FILE########
__FILENAME__ = user_gildings
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Fill in the gildings listing for users.

This listing is stored in get_user_gildings and seen on
/user/<username>/gildings.

"""

import datetime

from pylons import g

from r2.lib.db.queries import get_user_gildings
from r2.lib.utils import Storage
from r2.models import GildingsByDay, Thing, Comment
from r2.models.query_cache import CachedQueryMutator


date = datetime.datetime.now(g.tz)
earliest_date = datetime.datetime(2012, 10, 01, tzinfo=g.tz)

already_seen = set()

with CachedQueryMutator() as m:
    while date > earliest_date:
        gildings = GildingsByDay.get_gildings(date)
        fullnames = [x["thing"] for x in gildings]
        things = Thing._by_fullname(fullnames, data=True, return_dict=False)
        comments = {t._fullname: t for t in things if isinstance(t, Comment)}

        for gilding in gildings:
            fullname = gilding["thing"]
            if fullname in comments and fullname not in already_seen:
                thing = gilding["thing"] = comments[fullname]
                gilding_object = Storage(gilding)
                m.insert(get_user_gildings(gilding["user"]), [gilding_object])
                already_seen.add(fullname)
        date -= datetime.timedelta(days=1)

########NEW FILE########
__FILENAME__ = regenerate-query-cache
#!/usr/bin/python

from org.apache.pig.scripting import Pig


SCRIPT_ROOT = "udfs/dist/lib/"
INPUT_ROOT = "input/"
OUTPUT_ROOT = "output"

relations = {"savehide": ("UserQueryCache", "link"),
             "inbox_account_comment": ("UserQueryCache", "comment"),
             "inbox_account_message": ("UserQueryCache", "message"),
             "moderatorinbox": ("SubredditQueryCache", "message"),
             "vote_account_link": ("UserQueryCache", "link"),
            }

####### Pig script fragments
load_things = """
things =
LOAD '$THINGS'
    USING PigStorage()
    AS (id:long,
        ups:int,
        downs:int,
        deleted:chararray,
        spam:chararray,
        timestamp:double);
"""

make_things_items = """
items =
FOREACH things GENERATE *;
"""

load_rels = """
items =
LOAD '$RELS'
    USING PigStorage()
    AS (id:long,
        thing1_id:long,
        thing2_id:long,
        name:chararray,
        timestamp:double);
"""


load_and_map_data = """
data =
LOAD '$DATA'
    USING PigStorage()
    AS (id:long,
        key:chararray,
        value);

grouped_with_data =
COGROUP items BY id, data BY id;

items_with_data =
FOREACH grouped_with_data
    GENERATE FLATTEN(items),
             com.reddit.pig.MAKE_MAP(data.(key, value)) AS data;
"""

add_unread = """
SPLIT items_with_data
    INTO inbox IF 1 == 1,
         unread IF (chararray)data#'new' == 't';

inbox_with_relname =
FOREACH inbox GENERATE '$RELATION' AS relation, *;

unread_with_relname =
FOREACH unread GENERATE '$RELATION:unread' AS relation, *;

rels_with_relname =
UNION ONSCHEMA inbox_with_relname,
               unread_with_relname;
"""

add_relname = """
rels_with_relname =
FOREACH items GENERATE '$RELATION' AS relation, *;
"""

generate_rel_items = """
minimal_things =
FOREACH things GENERATE id, deleted;

joined =
JOIN rels_with_relname BY thing2_id LEFT OUTER,
     minimal_things BY id;

only_valid =
FILTER joined BY minimal_things::id IS NOT NULL AND
                 deleted == 'f';

potential_columns =
FOREACH only_valid
    GENERATE com.reddit.pig.MAKE_ROWKEY(relation, name, thing1_id) AS rowkey,
             com.reddit.pig.MAKE_THING2_FULLNAME(relation, thing2_id) AS colkey,
             timestamp AS value;
"""

store_top_1000_per_rowkey = """
non_null =
FILTER potential_columns BY rowkey IS NOT NULL AND colkey IS NOT NULL;

grouped =
GROUP non_null BY rowkey;

limited =
FOREACH grouped {
    sorted = ORDER non_null BY value DESC;
    limited = LIMIT sorted 1000;
    GENERATE group AS rowkey, FLATTEN(limited.(colkey, value));
};

jsonified =
FOREACH limited GENERATE rowkey,
                         colkey,
                         com.reddit.pig.TO_JSON(value);

STORE jsonified INTO '$OUTPUT' USING PigStorage();
"""

###### run the jobs
# register the reddit udfs
Pig.registerJar(SCRIPT_ROOT + "reddit-pig-udfs.jar")

# process rels
for rel, (cf, thing2_type) in relations.iteritems():
    # build source for a script
    script = "SET default_parallel 10;"
    script += load_rels
    if "inbox" in rel:
        script += load_and_map_data
        script += add_unread
    else:
        script += add_relname
    script += load_things
    script += generate_rel_items
    script += store_top_1000_per_rowkey

    # run it
    compiled = Pig.compile(script)
    bound = compiled.bind({
        "RELS": INPUT_ROOT + rel + ".dump",
        "DATA": INPUT_ROOT + rel + "-data.dump",
        "THINGS": INPUT_ROOT + thing2_type + ".dump",
        "RELATION": rel,
        "OUTPUT": "/".join((OUTPUT_ROOT, cf, rel)),
    })
    bound.runSingle()

# rebuild message-based queries (just get_sent right now)
if False:
    script = "SET default_parallel 10;"
    script += load_things
    script += make_things_items
    script += load_and_map_data
    script += """
    non_null =
    FILTER items_with_data BY data#'author_id' IS NOT NULL;

    potential_columns =
    FOREACH non_null
    GENERATE
        CONCAT('sent.', com.reddit.pig.TO_36(data#'author_id')) AS rowkey,
        com.reddit.pig.MAKE_FULLNAME('message', id) AS colkey,
        timestamp AS value;
    """
    script += store_top_1000_per_rowkey
    compiled = Pig.compile(script)
    bound = compiled.bind({
        "THINGS": INPUT_ROOT + "message.dump",
        "DATA": INPUT_ROOT + "message-data.dump",
        "OUTPUT": "/".join((OUTPUT_ROOT, "UserQueryCache", "sent")),
    })
    result = bound.runSingle()

# rebuild comment-based queries
if True:
    script = "SET default_parallel 10;"
    script += load_things
    script += make_things_items
    script += load_and_map_data
    script += """
    SPLIT items_with_data INTO
        spam_comments IF spam == 't',
        ham_comments IF spam == 'f';

    ham_comments_with_name =
    FOREACH ham_comments GENERATE 'sr_comments' AS name, *;

    reported_comments =
    FILTER ham_comments BY (int)data#'reported' > 0;

    reported_comments_with_name =
    FOREACH reported_comments GENERATE 'reported_comments' AS name, *;

    spam_comments_with_name =
    FOREACH spam_comments GENERATE 'spam_comments' AS name, *;

    comments_with_name =
    UNION ONSCHEMA ham_comments_with_name,
                   reported_comments_with_name,
                   spam_comments_with_name;

    potential_columns =
    FOREACH comments_with_name GENERATE
        CONCAT(name, CONCAT('.', com.reddit.pig.TO_36(data#'sr_id'))) AS rowkey,
        com.reddit.pig.MAKE_FULLNAME('comment', id) AS colkey,
        timestamp AS value;
    """
    script += store_top_1000_per_rowkey
    compiled = Pig.compile(script)
    bound = compiled.bind({
        "THINGS": INPUT_ROOT + "comment.dump",
        "DATA": INPUT_ROOT + "comment-data.dump",
        "OUTPUT": "/".join((OUTPUT_ROOT, "SubredditQueryCache", "comment")),
    })
    result = bound.runSingle()

########NEW FILE########
__FILENAME__ = tuples_to_sstables
#!/usr/bin/jython
"""A jython script that takes as input a set of tuples meant to be
bulk-loaded into Cassandra, and outputs a set of sstables usable
by Cassandra's sstableloader.

The Cassandra jars and configuration must be on the classpath for this
to function properly.
"""

from org.apache.cassandra.utils.ByteBufferUtil import bytes
from java.nio import ByteBuffer


def utf8(val):
    return bytes(val)

def datetime(val):
    milliseconds = long(float(val) * 1e3)
    return ByteBuffer.allocate(8).putLong(0, milliseconds)

COERCERS = dict(utf8=utf8,
                datetime=datetime)


def convert_to_sstables(input_files, column_family,
                        output_dir_name, keyspace, timestamp, buffer_size,
                        data_type):
    import fileinput
    from java.io import File
    from org.apache.cassandra.io.sstable import SSTableSimpleUnsortedWriter
    from org.apache.cassandra.db.marshal import AsciiType

    try:
        coercer = COERCERS[data_type]
    except KeyError:
        raise ValueError("invalid data type")

    output_dir = File(output_dir_name)

    if not output_dir.exists():
        output_dir.mkdir()

    writer = SSTableSimpleUnsortedWriter(output_dir,
                                         keyspace, column_family,
                                         AsciiType.instance, None,
                                         buffer_size)


    try:
        previous_rowkey = None
        for line in fileinput.input(input_files):
            rowkey, colkey, value = line.rstrip("\n").split("\t")

            if rowkey != previous_rowkey:
                writer.newRow(bytes(rowkey))

            coerced = coercer(value)
            writer.addColumn(bytes(colkey), coerced, timestamp)

            if fileinput.lineno() % 1000 == 0:
                print "%d items processed (%s)" % (fileinput.lineno(),
                                                   fileinput.filename())
    finally:
        writer.close()


def main():
    import os
    import optparse

    parser = optparse.OptionParser(
        usage="USAGE: tuples_to_sstables [options] COLUMN_FAMILY INPUT [...]")
    parser.add_option("--timestamp",
                      type="long",
                      nargs=1, dest="timestamp", default=0,
                      help="timestamp to use for each column")
    parser.add_option("--buffer-size",
                      type="int",
                      nargs=1, dest="buffer_size", default=128,
                      help="size in MB to buffer before writing SSTables")
    parser.add_option("--data-type",
                      nargs=1, dest="data_type", default="utf8",
                      help="type to coerce data into for column values")
    parser.add_option("-k", "--keyspace",
                      nargs=1, dest="keyspace", default="reddit",
                      help="the name of the keyspace the data is for")
    parser.add_option("-o", "--output-root",
                      nargs=1, dest="output_root", default=".",
                      help="the root directory to write the SSTables into")

    options, args = parser.parse_args()
    options = dict(options.__dict__) # in jython, __dict__ is a StringMap
    options["output_dir_name"] = os.path.join(options.pop("output_root"),
                                              options["keyspace"])
    options["column_family"], input_files = args[0], args[1:]
    return convert_to_sstables(input_files, **options)


if __name__ == "__main__":
    import sys
    sys.exit(main())

########NEW FILE########
__FILENAME__ = promoted_links
#!/usr/bin/python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
"""Tools for evaluating promoted link distribution."""

from collections import defaultdict
import datetime
from math import sqrt

from pylons import g
from sqlalchemy.sql.functions import sum as sa_sum

from r2.lib import promote
from r2.lib.db.operators import and_, or_
from r2.lib.utils import to36, weighted_lottery
from r2.models.traffic import (
    Session,
    TargetedImpressionsByCodename,
    PageviewsBySubredditAndPath,
)
from r2.models.bidding import PromotionWeights
from r2.models import (
    Link,
    PromoCampaign,
    DefaultSR,
)

LINK_PREFIX = Link._type_prefix + str(Link._type_id)
PC_PREFIX = PromoCampaign._type_prefix + str(PromoCampaign._type_id)


def error_statistics(errors):
    mean_error = sum(errors) / len(errors)
    min_error = min([abs(i) for i in errors])
    max_error = max([abs(i) for i in errors])
    stdev_error = sqrt(
        (sum([i ** 2 for i in errors]) / len(errors))
        - mean_error ** 2)
    return (mean_error, min_error, max_error, stdev_error)


def get_scheduled(date, sr_name=''):
    all_promotions = PromotionWeights.get_campaigns(date)
    fp_promotions = [p for p in all_promotions if p.sr_name == sr_name]
    campaigns = PromoCampaign._byID([i.promo_idx for i in fp_promotions],
                                    return_dict=False, data=True)
    links = Link._by_fullname([i.thing_name for i in fp_promotions],
                              return_dict=False, data=True)
    links = {l._id: l for l in links}
    kept = []
    for camp in campaigns:
        if camp.trans_id == 0:
            continue

        link = links[camp.link_id]
        if link._spam or not promote.is_accepted(link):
            continue

        kept.append(camp._id)

    return [('%s_%s' % (PC_PREFIX, to36(p.promo_idx)), p.thing_name, p.bid)
            for p in fp_promotions if p.promo_idx in kept]


def get_campaign_pageviews(date, sr_name=''):
    # ads go live at hour=5
    start = datetime.datetime(date.year, date.month, date.day, 5, 0)
    hours = [start + datetime.timedelta(hours=i) for i in xrange(24)]

    traffic_cls = TargetedImpressionsByCodename
    codename_string = PC_PREFIX + '_%'
    q = (Session.query(traffic_cls.codename,
                       sa_sum(traffic_cls.pageview_count).label('daily'))
            .filter(traffic_cls.subreddit == sr_name)
            .filter(traffic_cls.codename.like(codename_string))
            .filter(traffic_cls.interval == 'hour')
            .filter(traffic_cls.date.in_(hours))
            .group_by(traffic_cls.codename))

    pageviews = dict(q)
    return pageviews


def filter_campaigns(date, fullnames):
    campaigns = PromoCampaign._by_fullname(fullnames, data=True,
                                           return_dict=False)

    # filter out campaigns that shouldn't be live
    pc_date = datetime.datetime(date.year, date.month, date.day, 0, 0,
                                tzinfo=g.tz)

    campaigns = [camp for camp in campaigns
                 if camp.start_date <= pc_date <= camp.end_date]

    # check for links with targeted campaigns - we can't handle them now
    has_targeted = [camp.link_id for camp in campaigns if camp.sr_name != '']
    return [camp for camp in campaigns if camp.link_id not in has_targeted]


def get_frontpage_pageviews(date):
    sr_name = DefaultSR.name
    traffic_cls = PageviewsBySubredditAndPath
    q = (Session.query(traffic_cls.srpath, traffic_cls.pageview_count)
           .filter(traffic_cls.interval == 'day')
           .filter(traffic_cls.date == date)
           .filter(traffic_cls.srpath == '%s-GET_listing' % sr_name))
    r = list(q)
    return r[0][1]


def compare_pageviews(daysago=0, verbose=False):
    """Evaluate past delivery for promoted links.

    Check frontpage promoted links for their actual delivery compared to what
    would be expected based on their bids.

    """

    date = (datetime.datetime.now(g.tz) -
            datetime.timedelta(days=daysago)).date()

    scheduled = get_scheduled(date)
    pageviews_by_camp = get_campaign_pageviews(date)
    campaigns = filter_campaigns(date, pageviews_by_camp.keys())
    actual = []
    for camp in campaigns:
        link_fullname = '%s_%s' % (LINK_PREFIX, to36(camp.link_id))
        i = (camp._fullname, link_fullname, pageviews_by_camp[camp._fullname])
        actual.append(i)

    scheduled_links = {link for camp, link, pageviews in scheduled}
    actual_links = {link for camp, link, pageviews in actual}

    bid_by_link = defaultdict(int)
    total_bid = 0

    pageviews_by_link = defaultdict(int)
    total_pageviews = 0

    for camp, link, bid in scheduled:
        if link not in actual_links:
            if verbose:
                print '%s not found in actual, skipping' % link
            continue

        bid_by_link[link] += bid
        total_bid += bid

    for camp, link, pageviews in actual:
        # not ideal: links shouldn't be here
        if link not in scheduled_links:
            if verbose:
                print '%s not found in schedule, skipping' % link
            continue

        pageviews_by_link[link] += pageviews
        total_pageviews += pageviews

    errors = []
    for link, bid in sorted(bid_by_link.items(), key=lambda t: t[1]):
        pageviews = pageviews_by_link.get(link, 0)
        expected = bid / total_bid
        realized = float(pageviews) / total_pageviews
        difference = (realized - expected) / expected
        errors.append(difference)
        if verbose:
            print '%s - %s - %s - %s' % (link, expected, realized, difference)

    mean_error, min_error, max_error, stdev_error = error_statistics(errors)

    print '%s' % date
    print ('error %s max, %s min, %s +- %s' %
           (max_error, min_error, mean_error, stdev_error))
    print 'total bid %s' % total_bid
    print ('pageviews for promoted links targeted only to frontpage %s' %
           total_pageviews)
    print ('frontpage pageviews for all promoted links %s' %
           sum(pageviews_by_camp.values()))
    print 'promoted eligible pageviews %s' % get_frontpage_pageviews(date)


PROMOS = [('promo_%s' % i, i + 1) for i in xrange(100)]


def select_subset(n, weighted=False):
    promos = copy(PROMOS)
    selected = []

    if weighted:
        d = {(name, weight): weight for name, weight in promos}
        while len(selected) < n and d:
            i = weighted_lottery(d)
            del d[i]
            selected.append(i)
    else:
        # Sample without replacement
        if n > len(promos):
            return promos
        else:
            return random.sample(promos, n)
    return selected


def pick(subset, weighted=False):
    if weighted:
        d = {(name, weight): weight for name, weight in subset}
        picked = weighted_lottery(d)
    else:
        picked = random.choice(subset)
    return picked


def benchmark(subsets=1440, picks=6945, weighted_subset=False,
              weighted_pick=True, subset_size=10, verbose=False):
    """Test 2 stage randomization.

    First stage picks a subset of promoted links, second stage picks a single
    promoted link. This is to simulate the server side subset plus client side
    randomization of promoted link display.

    """

    counts = {(name, weight): 0 for name, weight in PROMOS}

    for i in xrange(subsets):
        subset = select_subset(subset_size, weighted=weighted_subset)

        for j in xrange(picks):
            name, weight = pick(subset, weighted=weighted_pick)
            counts[(name, weight)] += 1

    total_weight = sum(counts.values())
    errors = []
    for name, weight in sorted(counts.keys(), key=lambda t: t[1]):
        count = counts[(name, weight)]
        actual = float(count) / (subsets * picks)
        expected = float(weight) / total_weight
        error = (actual - expected) / expected
        errors.append(error)
        if verbose:
            print ('%s - expected: %s - actual: %s - error %s' %
                   (name, expected, actual, error))

    mean_error, min_error, max_error, stdev_error = error_statistics(errors)

    if verbose:
        print ('Error %s max, %s min, %s +- %s' %
               (max_error, min_error, mean_error, stdev_error))

    return (max_error, min_error, mean_error, stdev_error)

########NEW FILE########
__FILENAME__ = tracker
#!/usr/bin/python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################
"""
This is a tiny Flask app used for a couple of self-serve ad tracking
mechanisms. The URLs it provides are:

/fetch-trackers

    Given a list of Ad IDs, generate tracking hashes specific to the user's
    IP address. This must run outside the original request because the HTML
    may be cached by the CDN.

/click

    Promoted links have their URL replaced with a /click URL by the JS
    (after a call to /fetch-trackers). Redirect to the actual URL after logging
    the click. This must be run in a place whose logs are stored for traffic
    analysis.

For convenience, the script can compile itself into a Zip archive suitable for
use on Amazon Elastic Beanstalk (and possibly other systems).

"""


import cStringIO
import hashlib
import time

from ConfigParser import RawConfigParser
from wsgiref.handlers import format_date_time

from flask import Flask, request, json, make_response, abort, redirect


application = Flask(__name__)
MAX_FULLNAME_LENGTH = 128  # can include srname and codename, leave room
REQUIRED_PACKAGES = [
    "flask",
]


class ApplicationConfig(object):
    """A thin wrapper around ConfigParser that remembers what we read.

    The remembered settings can then be written out to a minimal config file
    when building the Elastic Beanstalk zipfile.

    """
    def __init__(self):
        self.input = RawConfigParser()
        with open("production.ini") as f:
            self.input.readfp(f)
        self.output = RawConfigParser()

    def get(self, section, key):
        value = self.input.get(section, key)

        # remember that we needed this configuration value
        if (section.upper() != "DEFAULT" and
            not self.output.has_section(section)):
            self.output.add_section(section)
        self.output.set(section, key, value)

        return value

    def to_config(self):
        io = cStringIO.StringIO()
        self.output.write(io)
        return io.getvalue()


config = ApplicationConfig()
tracking_secret = config.get('DEFAULT', 'tracking_secret')
adtracker_url = config.get('DEFAULT', 'adtracker_url')


def jsonpify(callback_name, data):
    data = callback_name + '(' + json.dumps(data) + ')'
    response = make_response(data)
    response.mimetype = 'text/javascript'
    return response


def get_client_ip():
    """Figure out the IP address of the remote client.

    If the remote address is on the 10.* network, we'll assume that it is a
    trusted load balancer and that the last component of X-Forwarded-For is
    trustworthy.

    """

    if request.remote_addr.startswith("10."):
        # it's a load balancer, use x-forwarded-for
        return request.access_route[-1]
    else:
        # direct connection to someone outside
        return request.remote_addr


@application.route("/")
def healthcheck():
    return "I am healthy."


@application.route('/fetch-trackers')
def fetch_trackers():
    ip = get_client_ip()
    jsonp_callback = request.args['callback']
    ids = request.args.getlist('ids[]')

    if len(ids) > 100:
        abort(400)

    hashed = {}
    for fullname in ids:
        if len(fullname) > MAX_FULLNAME_LENGTH:
            continue
        text = ''.join((ip, fullname, tracking_secret))
        hashed[fullname] = hashlib.sha1(text).hexdigest()
    return jsonpify(jsonp_callback, hashed)


@application.route('/click')
def click_redirect():
    ip = get_client_ip()
    destination = request.args['url'].encode('utf-8')
    fullname = request.args['id']
    observed_hash = request.args['hash']

    expected_hash_text = ''.join((ip, fullname, tracking_secret))
    expected_hash = hashlib.sha1(expected_hash_text).hexdigest()

    if expected_hash != observed_hash:
        abort(403)

    now = format_date_time(time.time())
    response = redirect(destination)
    response.headers['Cache-control'] = 'no-cache'
    response.headers['Pragma'] = 'no-cache'
    response.headers['Date'] = now
    response.headers['Expires'] = now
    return response


if __name__ == "__main__":
    # package up for elastic beanstalk
    import zipfile

    with zipfile.ZipFile("/tmp/tracker.zip", "w", zipfile.ZIP_DEFLATED) as zip:
        zip.write(__file__, "application.py")
        zip.writestr("production.ini", config.to_config())
        zip.writestr("requirements.txt", "\n".join(REQUIRED_PACKAGES) + "\n")

########NEW FILE########
__FILENAME__ = trylater
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################

from pylons import g

from r2.lib import amqp
from r2.lib.hooks import all_hooks, get_hook
from r2.models.trylater import TryLater

PREFIX = "trylater."


def register_core_hooks():
    # any trylater hooks implemented in r2 should be registered in this
    # function to ensure that they are available when trylater runs.
    from r2.models.account import trylater_hooks
    trylater_hooks.register_all()


## Entry point
def run_trylater():
    register_core_hooks()

    our_hooks = (key[len(PREFIX):] for key in all_hooks().keys()
                 if key.startswith(PREFIX))
    with TryLater.multi_handle(our_hooks) as handleable:
        for system, mature_items in handleable.iteritems():
            hook_name = "trylater.%s" % system
            g.log.info("Trying %s", system)

            get_hook(hook_name).call(mature_items=mature_items)

    amqp.worker.join()

########NEW FILE########
__FILENAME__ = upload_static_files_to_s3
#!/usr/bin/python
# The contents of this file are subject to the Common Public Attribution
# License Version 1.0. (the "License"); you may not use this file except in
# compliance with the License. You may obtain a copy of the License at
# http://code.reddit.com/LICENSE. The License is based on the Mozilla Public
# License Version 1.1, but Sections 14 and 15 have been added to cover use of
# software over a computer network and provide for limited attribution for the
# Original Developer. In addition, Exhibit A has been modified to be consistent
# with Exhibit B.
#
# Software distributed under the License is distributed on an "AS IS" basis,
# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License for
# the specific language governing rights and limitations under the License.
#
# The Original Code is reddit.
#
# The Original Developer is the Initial Developer.  The Initial Developer of
# the Original Code is reddit Inc.
#
# All portions of the code written by reddit are Copyright (c) 2006-2014 reddit
# Inc. All Rights Reserved.
###############################################################################


import os
import mimetypes

from r2.lib.utils import read_static_file_config


NEVER = 'Thu, 31 Dec 2037 23:59:59 GMT'

mimetypes.encodings_map['.gzip'] = 'gzip'

def upload(config_file):
    bucket, config = read_static_file_config(config_file)

    # build a list of files already in the bucket
    remote_files = {key.name : key.etag.strip('"') for key in bucket.list()}

    # upload local files not already in the bucket
    for root, dirs, files in os.walk(config["static_root"]):
        for file in files:
            absolute_path = os.path.join(root, file)

            key_name = os.path.relpath(absolute_path,
                                       start=config["static_root"])

            type, encoding = mimetypes.guess_type(file)
            if not type:
                continue
            headers = {}
            headers['Expires'] = NEVER
            headers['Content-Type'] = type
            if encoding:
                headers['Content-Encoding'] = encoding

            key = bucket.new_key(key_name)
            with open(absolute_path, 'rb') as f:
                etag, base64_tag = key.compute_md5(f)

                # don't upload the file if it already exists unmodified in the bucket
                if remote_files.get(key_name, None) == etag:
                    continue

                print "uploading", key_name, "to S3..."
                key.set_contents_from_file(
                    f,
                    headers=headers,
                    policy='public-read',
                    md5=(etag, base64_tag),
                )


if __name__ == "__main__":
    import sys

    if len(sys.argv) != 2:
        print >> sys.stderr, "USAGE: %s /path/to/config-file.ini" % sys.argv[0]
        sys.exit(1)

    upload(sys.argv[1])

########NEW FILE########
