Files in this directory from from Bob Ippolito's py2app.

License: Any components of the py2app suite may be distributed under
the MIT or PSF open source licenses.

This is version 1.0, SVN revision 789, from 2006/01/25.
The main repository is http://svn.red-bean.com/bob/macholib/trunk/macholib/
This directory contains the Distutils package.

There's a full documentation available at:

    http://docs.python.org/distutils/

The Distutils-SIG web page is also a good starting point:

    http://www.python.org/sigs/distutils-sig/

WARNING : Distutils must remain compatible with 2.3

$Id$

IDLE is Python's Tkinter-based Integrated DeveLopment Environment.

IDLE emphasizes a lightweight, clean design with a simple user interface.
Although it is suitable for beginners, even advanced users will find that
IDLE has everything they really need to develop pure Python code.

IDLE features a multi-window text editor with multiple undo, Python colorizing,
and many other capabilities, e.g. smart indent, call tips, and autocompletion.

The editor has comprehensive search functions, including searching through
multiple files.  Class browsers and path browsers provide fast access to
code objects from a top level viewpoint without dealing with code folding.

There is a Python Shell window which features colorizing and command recall.

IDLE executes Python code in a separate process, which is restarted for each
Run (F5) initiated from an editor window.  The environment can also be 
restarted from the Shell window without restarting IDLE.

This enhancement has often been requested, and is now finally available.  The
magic "reload/import *" incantations are no longer required when editing and
testing a module two or three steps down the import chain.

(Personal firewall software may warn about the connection IDLE makes to its
subprocess using this computer's internal loopback interface.  This connection
is not visible on any external interface and no data is sent to or received
from the Internet.)

It is possible to interrupt tightly looping user code, even on Windows.

Applications which cannot support subprocesses and/or sockets can still run
IDLE in a single process.

IDLE has an integrated debugger with stepping, persistent breakpoints, and call
stack visibility.

There is a GUI configuration manager which makes it easy to select fonts,
colors, keybindings, and startup options.  This facility includes a feature
which allows the user to specify additional help sources, either locally or on
the web.

IDLE is coded in 100% pure Python, using the Tkinter GUI toolkit (Tk/Tcl)
and is cross-platform, working on Unix, Mac, and Windows.

IDLE accepts command line arguments.  Try idle -h to see the options.


If you find bugs or have suggestions, let us know about them by using the
Python Bug Tracker:

http://sourceforge.net/projects/python

Patches are always appreciated at the Python Patch Tracker, and change
requests should be posted to the RFE Tracker.

For further details and links, read the Help files and check the IDLE home
page at

http://www.python.org/idle/

There is a mail list for IDLE: idle-dev@python.org.  You can join at

http://mail.python.org/mailman/listinfo/idle-dev

Writing new tests
=================

Precaution
----------

    New tests should always use only one Tk window at once, like all the
    current tests do. This means that you have to destroy the current window
    before creating another one, and clean up after the test. The motivation
    behind this is that some tests may depend on having its window focused
    while it is running to work properly, and it may be hard to force focus
    on your window across platforms (right now only test_traversal at
    test_ttk.test_widgets.NotebookTest depends on this).


In this directory:
- py2_test_grammar.py -- test file that exercises most/all of Python 2.x's grammar.
- py3_test_grammar.py -- test file that exercises most/all of Python 3.x's grammar.
- infinite_recursion.py -- test file that causes lib2to3's faster recursive pattern matching
  scheme to fail, but passes when lib2to3 falls back to iterative pattern matching.
- fixes/ -- for use by test_refactor.py

This directory exists so that 3rd party packages can be installed
here.  Read the source for site.py for more details.

This directory only contains tests for outstanding bugs that cause the
interpreter to segfault.  Ideally this directory should always be empty, but
sometimes it may not be easy to fix the underlying cause and the bug is deemed
too obscure to invest the effort.

Each test should fail when run from the command line:

	./python Lib/test/crashers/weakref_in_del.py

Put as much info into a docstring or comments to help determine the cause of the
failure, as well as a bugs.python.org issue number if it exists.  Particularly
note if the cause is system or environment dependent and what the variables are.

Once the crash is fixed, the test case should be moved into an appropriate test
(even if it was originally from the test suite).  This ensures the regression
doesn't happen again.  And if it does, it should be easier to track down.

This empty directory serves as destination for temporary files
created by some tests.

This directory contains test cases that are known to leak references.
The idea is that you can import these modules while in the interpreter
and call the leak function repeatedly.  This will only be helpful if
the interpreter was built in debug mode.  If the total ref count
doesn't increase, the bug has been fixed and the file should be removed
from the repository.

Note:  be careful to check for cyclic garbage.  Sometimes it may be helpful
to define the leak function like:

def leak():
    def inner_leak():
        # this is the function that leaks, but also creates cycles
    inner_leak()
    gc.collect() ; gc.collect() ; gc.collect()

Here's an example interpreter session for test_gestalt which still leaks:

>>> from test.leakers.test_gestalt import leak
[24275 refs]
>>> leak()
[28936 refs]
>>> leak()
[28938 refs]
>>> leak()
[28940 refs]
>>> 

Once the leak is fixed, the test case should be moved into an appropriate
test (even if it was originally from the test suite).  This ensures the
regression doesn't happen again.  And if it does, it should be easier
to track down.

Python Misc subdirectory
========================

This directory contains files that wouldn't fit in elsewhere.  Some
documents are only of historic importance.

Files found here
----------------

ACKS                    Acknowledgements
AIX-NOTES               Notes for building Python on AIX
BeOS-NOTES              Notes for building on BeOS
BeOS-setup.py           setup.py replacement for BeOS, see BeOS-NOTES
build.sh                Script to build and test latest Python from the repository
cheatsheet              Quick summary of Python by Ken Manheimer
developers.txt          A history of who got developer permissions, and why
gdbinit                 Handy stuff to put in your .gdbinit file, if you use gdb
HISTORY                 News from previous releases -- oldest last
indent.pro              GNU indent profile approximating my C style
maintainers.rst         A list of maintainers for library modules
NEWS                    News for this release (for some meaning of "this")
NEWS.help               How to edit NEWS
Porting                 Mini-FAQ on porting to new platforms
PURIFY.README           Information for Purify users
pymemcompat.h           Memory interface compatibility file.
python-config.in        Python script template for python-config
python.man              UNIX man page for the python interpreter
python-mode.el          Emacs mode for editing Python programs
python.pc.in            Package configuration info template for pkg-config
python-wing.wpr         Wing IDE project file
README                  The file you're reading now
README.coverity         Information about running Coverity's Prevent on Python
README.klocwork         Information about running Klocwork's K7 on Python
README.OpenBSD          Help for building problems on OpenBSD
README.valgrind         Information for Valgrind users, see valgrind-python.supp
RFD                     Request For Discussion about a Python newsgroup
RPM                     (Old) tools to build RPMs
setuid-prog.c           C helper program for set-uid Python scripts
SpecialBuilds.txt       Describes extra symbols you can set for debug builds
TextMate                A TextMate bundle for Python development
valgrind-python.supp    Valgrind suppression file, see README.valgrind
vgrindefs               Python configuration for vgrind (a generic pretty printer)
Vim                     Python development utilities for the Vim editor


This documentation tries to help people who intend to use Python on
AIX.

There used to be many issues with Python on AIX, but the major ones
have been corrected for version 3.2, so that Python should now work
rather well on this platform. The remaining known issues are listed in
this document.


======================================================================
			   Compiling Python
----------------------------------------------------------------------

You can compile Python with gcc or the native AIX compiler. The native
compiler used to give better performances on this system with older
versions of Python.  With Python 3.2 it may not be the case anymore,
as this compiler does not allow compiling Python with computed gotos.
Some benchmarks need to be done.

Compiling with gcc:

cd Python-3.2
CC=gcc OPT="-O2" ./configure --enable-shared
make

There are various aliases for the native compiler.  The recommended
alias for compiling Python is 'xlc_r', which provides a better level of
compatibility and handles thread initialization properly.

It is a good idea to add the '-qmaxmem=70000' option, otherwise the
compiler considers various files too complex to optimize.

Compiling with xlc:

cd Python-3.2
CC=xlc_r OPT="-O2 -qmaxmem=70000" ./configure --without-computed-gotos --enable-shared
make

Note:
On AIX 5.3 and earlier, you will also need to specify the
"--disable-ipv6" flag to configure. This has been corrected in AIX
6.1.


======================================================================
			  Memory Limitations
----------------------------------------------------------------------

Note: this section may not apply when compiling Python as a 64 bit
application.

By default on AIX each program gets one segment register for its data
segment. As each segment register covers 256 MB, a Python program that
would use more than 256MB will raise a MemoryError.  The standard
Python test suite is one such application.

To allocate more segment registers to Python, you must use the linker
option -bmaxdata or the ldedit tool to specify the number of bytes you
need in the data segment.

For example, if you want to allow 512MB of memory for Python (this is
enough for the test suite to run without MemoryErrors), you should run
the following command at the end of compilation:

ldedit -b maxdata:0x20000000 ./python

You can allow up to 2GB of memory for Python by using the value
0x80000000 for maxdata.

It is also possible to go beyond 2GB of memory by activating Large
Page Use. You should consult the IBM documentation if you need to use
this option. You can also follow the discussion of this problem
in issue 11212 at bugs.python.org.

http://publib.boulder.ibm.com/infocenter/aix/v6r1/index.jsp?topic=/com.ibm.aix.cmds/doc/aixcmds3/ldedit.htm


======================================================================
			     Known issues
----------------------------------------------------------------------

Those issues are currently affecting Python on AIX:

* Python has not been fully tested on AIX when compiled as a 64 bit
  application.

* issue 3526: the memory used by a Python process will never be
  released to the system. If you have a Python application on AIX that
  uses a lot of memory, you should read this issue and you may
  consider using the provided patch that implements a custom malloc
  implementation

* issue 11184: support for large files is currently broken

* issue 11185: os.wait4 does not behave correctly with option WNOHANG

* issue 1745108: there may be some problems with curses.panel

* issue 11192: test_socket fails

* issue 11190: test_locale fails

* issue 11193: test_subprocess fails

* issue 9920: minor arithmetic issues in cmath

* issue 11215: test_fileio fails

* issue 11188: test_time fails


======================================================================
		Implementation details for developers
----------------------------------------------------------------------

Python and python modules can now be built as shared libraries on AIX
as usual.

AIX shared libraries require that an "export" and "import" file be
provided at compile time to list all extern symbols which may be
shared between modules.  The "export" file (named python.exp) for the
modules and the libraries that belong to the Python core is created by
the "makexp_aix" script before performing the link of the python
binary. It lists all global symbols (exported during the link) of the
modules and the libraries that make up the python executable.

When shared library modules (.so files) are made, a second shell
script is invoked.  This script is named "ld_so_aix" and is also
provided with the distribution in the Modules subdirectory.  This
script acts as an "ld" wrapper which hides the explicit management of
"export" and "import" files; it adds the appropriate arguments (in the
appropriate order) to the link command that creates the shared module.
Among other things, it specifies that the "python.exp" file is an
"import" file for the shared module.

This mechanism should be transparent.


Coverity has a static analysis tool (Prevent) which is similar to Klocwork.
They run their tool on the Python source code (SVN head) on a daily basis.
The results are available at:

     http://scan.coverity.com/

About 20 people have access to the analysis reports.  Other
people can be added by request.

Prevent was first run on the Python 2.5 source code in March 2006.
There were originally about 100 defects reported.  Some of these
were false positives.  Over 70 issues were uncovered.

Each warning has a unique id and comments that can be made on it.
When checking in changes due to a warning, the unique id
as reported by the tool was added to the SVN commit message.

False positives were annotated so that the comments can
be reviewed and reversed if the analysis was incorrect.

Contact python-dev@python.org for more information.

=============
Emacs support
=============

If you want to edit Python code in Emacs, you should download python-mode.el
and install it somewhere on your load-path.  See the project page to download:

    https://launchpad.net/python-mode

While Emacs comes with a python.el file, it is not recommended.
python-mode.el is maintained by core Python developers and is generally
considered more Python programmer friendly.  For example, python-mode.el
includes a killer feature called `pdbtrack` which allows you to set a pdb
breakpoint in your code, run your program in an Emacs shell buffer, and do gud
style debugging when the breakpoint is hit.

python-mode.el is compatible with both GNU Emacs from the FSF, and XEmacs.

For more information and bug reporting, see the above project page.  For help,
development, or discussions, see the python-mode mailing list:

    http://mail.python.org/mailman/listinfo/python-mode


..
   Local Variables:
   mode: indented-text
   indent-tabs-mode: nil
   sentence-end-double-space: t
   fill-column: 78
   coding: utf-8
   End:


Klocwork has a static analysis tool (K7) which is similar to Coverity.
They will run their tool on the Python source code on demand.
The results are available at:

     https://opensource.klocwork.com/

Currently, only Neal Norwitz has access to the analysis reports.  Other
people can be added by request.

K7 was first run on the Python 2.5 source code in mid-July 2006.
This is after Coverity had been making their results available.
There were originally 175 defects reported.  Most of these
were false positives.  However, there were numerous real issues 
also uncovered.

Each warning has a unique id and comments that can be made on it.
When checking in changes due to a K7 report, the unique id
as reported by the tool was added to the SVN commit message.
A comment was added to the K7 warning indicating the SVN revision
in addition to any analysis.

False positives were also annotated so that the comments can
be reviewed and reversed if the analysis was incorrect.

A second run was performed on 10-Aug-2006.  The tool was tuned to remove
some false positives and perform some additional checks.  ~150 new
warnings were produced, primarily related to dereferencing NULL pointers.

Contact python-dev@python.org for more information.


2005-01-08

If you are have a problem building on OpenBSD and see output like this
while running configure:

checking curses.h presence... yes
configure: WARNING: curses.h: present but cannot be compiled
configure: WARNING: curses.h: check for missing prerequisite headers?
configure: WARNING: curses.h: see the Autoconf documentation
configure: WARNING: curses.h: section "Present But Cannot Be Compiled"
configure: WARNING: curses.h: proceeding with the preprocessor's result
configure: WARNING: curses.h: in the future, the compiler will take precedence

there is likely a problem that will prevent building python.  
If you see the messages above and are able to completely build python,
please tell python-dev@python.org indicating your version of OpenBSD
and any other relevant system configuration.

The build error that occurs while making may look something like this:

    /usr/include/sys/event.h:53: error: syntax error before "u_int"
    /usr/include/sys/event.h:55: error: syntax error before "u_short"

To fix this problem, you will probably need update Python's configure
script to disable certain options.  Search for a line that looks like:

    OpenBSD/2.* | OpenBSD/3.@<:@012345678@:>@)

If your version is not in that list, e.g., 3.9, add the version
number.  In this case, you would just need to add a 9 after the 8.
If you modify configure.in, you will need to regenerate configure
with autoconf.

If your version is already in the list, this is not a known problem.
Please submit a bug report here:

    http://sourceforge.net/tracker/?group_id=5470&atid=105470

This document describes some caveats about the use of Valgrind with
Python.  Valgrind is used periodically by Python developers to try
to ensure there are no memory leaks or invalid memory reads/writes.

If you don't want to read about the details of using Valgrind, there
are still two things you must do to suppress the warnings.  First,
you must use a suppressions file.  One is supplied in
Misc/valgrind-python.supp.  Second, you must do one of the following:

  * Uncomment Py_USING_MEMORY_DEBUGGER in Objects/obmalloc.c,
    then rebuild Python
  * Uncomment the lines in Misc/valgrind-python.supp that
    suppress the warnings for PyObject_Free and PyObject_Realloc

If you want to use Valgrind more effectively and catch even more
memory leaks, you will need to configure python --without-pymalloc.
PyMalloc allocates a few blocks in big chunks and most object
allocations don't call malloc, they use chunks doled about by PyMalloc
from the big blocks.  This means Valgrind can't detect
many allocations (and frees), except for those that are forwarded
to the system malloc.  Note: configuring python --without-pymalloc
makes Python run much slower, especially when running under Valgrind.
You may need to run the tests in batches under Valgrind to keep
the memory usage down to allow the tests to complete.  It seems to take
about 5 times longer to run --without-pymalloc.

Apr 15, 2006:
  test_ctypes causes Valgrind 3.1.1 to fail (crash).
  test_socket_ssl should be skipped when running valgrind.
	The reason is that it purposely uses uninitialized memory.
	This causes many spurious warnings, so it's easier to just skip it.


Details:
--------
Python uses its own small-object allocation scheme on top of malloc,
called PyMalloc.

Valgrind may show some unexpected results when PyMalloc is used.
Starting with Python 2.3, PyMalloc is used by default.  You can disable
PyMalloc when configuring python by adding the --without-pymalloc option.
If you disable PyMalloc, most of the information in this document and
the supplied suppressions file will not be useful.  As discussed above,
disabling PyMalloc can catch more problems.

If you use valgrind on a default build of Python,  you will see
many errors like:

        ==6399== Use of uninitialised value of size 4
        ==6399== at 0x4A9BDE7E: PyObject_Free (obmalloc.c:711)
        ==6399== by 0x4A9B8198: dictresize (dictobject.c:477)

These are expected and not a problem.  Tim Peters explains
the situation:

        PyMalloc needs to know whether an arbitrary address is one
	that's managed by it, or is managed by the system malloc.
	The current scheme allows this to be determined in constant
	time, regardless of how many memory areas are under pymalloc's
	control.

        The memory pymalloc manages itself is in one or more "arenas",
	each a large contiguous memory area obtained from malloc.
	The base address of each arena is saved by pymalloc
	in a vector.  Each arena is carved into "pools", and a field at
	the start of each pool contains the index of that pool's arena's
	base address in that vector.

        Given an arbitrary address, pymalloc computes the pool base
	address corresponding to it, then looks at "the index" stored
	near there.  If the index read up is out of bounds for the
	vector of arena base addresses pymalloc maintains, then
	pymalloc knows for certain that this address is not under
	pymalloc's control.  Otherwise the index is in bounds, and
	pymalloc compares

            the arena base address stored at that index in the vector

        to

            the arbitrary address pymalloc is investigating

        pymalloc controls this arbitrary address if and only if it lies
        in the arena the address's pool's index claims it lies in.

        It doesn't matter whether the memory pymalloc reads up ("the
	index") is initialized.  If it's not initialized, then
	whatever trash gets read up will lead pymalloc to conclude
	(correctly) that the address isn't controlled by it, either
	because the index is out of bounds, or the index is in bounds
	but the arena it represents doesn't contain the address.

        This determination has to be made on every call to one of
	pymalloc's free/realloc entry points, so its speed is critical
	(Python allocates and frees dynamic memory at a ferocious rate
	-- everything in Python, from integers to "stack frames",
	lives in the heap).

This directory contains support file used to build RPM releases of
Python.  Its contents are maintained by Sean Reifschneider
<jafo@tummy.com>.

It is recommended that RPM builders use the python*.src.rpm file
downloaded from the "ftp.python.org:/pub/python/<version>/rpms".  These
may be more up to date than the files included in the base Python
release tar-file.

If you wish to build RPMs from the base Python release tar-file, note
that you will have to download the
"doc/<version>/html-<version>.tar.bz2"
file from python.org and place it into your "SOURCES" directory for
the build to complete.  This is the same directory that you place the
Python-2.3.1 release tar-file in.  You can then use the ".spec" file in
this directory to build RPMs.

To generate or modify mapping headers
-------------------------------------
Mapping headers are imported from CJKCodecs as pre-generated form.
If you need to tweak or add something on it, please look at tools/
subdirectory of CJKCodecs' distribution.



Notes on implmentation characteristics of each codecs
-----------------------------------------------------

1) Big5 codec

  The big5 codec maps the following characters as cp950 does rather
  than conforming Unicode.org's that maps to 0xFFFD.

    BIG5        Unicode     Description

    0xA15A      0x2574      SPACING UNDERSCORE
    0xA1C3      0xFFE3      SPACING HEAVY OVERSCORE
    0xA1C5      0x02CD      SPACING HEAVY UNDERSCORE
    0xA1FE      0xFF0F      LT DIAG UP RIGHT TO LOW LEFT
    0xA240      0xFF3C      LT DIAG UP LEFT TO LOW RIGHT
    0xA2CC      0x5341      HANGZHOU NUMERAL TEN
    0xA2CE      0x5345      HANGZHOU NUMERAL THIRTY

  Because unicode 0x5341, 0x5345, 0xFF0F, 0xFF3C is mapped to another
  big5 codes already, a roundtrip compatibility is not guaranteed for
  them.


2) cp932 codec

  To conform to Windows's real mapping, cp932 codec maps the following
  codepoints in addition of the official cp932 mapping.

    CP932     Unicode     Description

    0x80      0x80        UNDEFINED
    0xA0      0xF8F0      UNDEFINED
    0xFD      0xF8F1      UNDEFINED
    0xFE      0xF8F2      UNDEFINED
    0xFF      0xF8F3      UNDEFINED


3) euc-jisx0213 codec

  The euc-jisx0213 codec maps JIS X 0213 Plane 1 code 0x2140 into
  unicode U+FF3C instead of U+005C as on unicode.org's mapping.
  Because euc-jisx0213 has REVERSE SOLIDUS on 0x5c already and A140
  is shown as a full width character, mapping to U+FF3C can make
  more sense.

  The euc-jisx0213 codec is enabled to decode JIS X 0212 codes on
  codeset 2. Because JIS X 0212 and JIS X 0213 Plane 2 don't have
  overlapped by each other, it doesn't bother standard conformations
  (and JIS X 0213 Plane 2 is intended to use so.) On encoding
  sessions, the codec will try to encode kanji characters in this
  order:

    JIS X 0213 Plane 1 -> JIS X 0213 Plane 2 -> JIS X 0212


4) euc-jp codec

  The euc-jp codec is a compatibility instance on these points:
   - U+FF3C FULLWIDTH REVERSE SOLIDUS is mapped to EUC-JP A1C0 (vice versa)
   - U+00A5 YEN SIGN is mapped to EUC-JP 0x5c. (one way)
   - U+203E OVERLINE is mapped to EUC-JP 0x7e. (one way)


5) shift-jis codec

  The shift-jis codec is mapping 0x20-0x7e area to U+20-U+7E directly
  instead of using JIS X 0201 for compatibility. The differences are:
   - U+005C REVERSE SOLIDUS is mapped to SHIFT-JIS 0x5c.
   - U+007E TILDE is mapped to SHIFT-JIS 0x7e.
   - U+FF3C FULL-WIDTH REVERSE SOLIDUS is mapped to SHIFT-JIS 815f.


ZLIB DATA COMPRESSION LIBRARY

zlib 1.2.3 is a general purpose data compression library.  All the code is
thread safe.  The data format used by the zlib library is described by RFCs
(Request for Comments) 1950 to 1952 in the files
http://www.ietf.org/rfc/rfc1950.txt (zlib format), rfc1951.txt (deflate format)
and rfc1952.txt (gzip format). These documents are also available in other
formats from ftp://ftp.uu.net/graphics/png/documents/zlib/zdoc-index.html

All functions of the compression library are documented in the file zlib.h
(volunteer to write man pages welcome, contact zlib@gzip.org). A usage example
of the library is given in the file example.c which also tests that the library
is working correctly. Another example is given in the file minigzip.c. The
compression library itself is composed of all source files except example.c and
minigzip.c.

To compile all files and run the test program, follow the instructions given at
the top of Makefile. In short "make test; make install" should work for most
machines. For Unix: "./configure; make test; make install". For MSDOS, use one
of the special makefiles such as Makefile.msc. For VMS, use make_vms.com.

Questions about zlib should be sent to <zlib@gzip.org>, or to Gilles Vollant
<info@winimage.com> for the Windows DLL version. The zlib home page is
http://www.zlib.org or http://www.gzip.org/zlib/ Before reporting a problem,
please check this site to verify that you have the latest version of zlib;
otherwise get the latest version and check whether the problem still exists or
not.

PLEASE read the zlib FAQ http://www.gzip.org/zlib/zlib_faq.html before asking
for help.

Mark Nelson <markn@ieee.org> wrote an article about zlib for the Jan. 1997
issue of  Dr. Dobb's Journal; a copy of the article is available in
http://dogma.net/markn/articles/zlibtool/zlibtool.htm

The changes made in version 1.2.3 are documented in the file ChangeLog.

Unsupported third party contributions are provided in directory "contrib".

A Java implementation of zlib is available in the Java Development Kit
http://java.sun.com/j2se/1.4.2/docs/api/java/util/zip/package-summary.html
See the zlib home page http://www.zlib.org for details.

A Perl interface to zlib written by Paul Marquess <pmqs@cpan.org> is in the
CPAN (Comprehensive Perl Archive Network) sites
http://www.cpan.org/modules/by-module/Compress/

A Python interface to zlib written by A.M. Kuchling <amk@amk.ca> is
available in Python 1.5 and later versions, see
http://www.python.org/doc/lib/module-zlib.html

A zlib binding for TCL written by Andreas Kupries <a.kupries@westend.com> is
availlable at http://www.oche.de/~akupries/soft/trf/trf_zip.html

An experimental package to read and write files in .zip format, written on top
of zlib by Gilles Vollant <info@winimage.com>, is available in the
contrib/minizip directory of zlib.


Notes for some targets:

- For Windows DLL versions, please see win32/DLL_FAQ.txt

- For 64-bit Irix, deflate.c must be compiled without any optimization. With
  -O, one libpng test fails. The test works in 32 bit mode (with the -n32
  compiler flag). The compiler bug has been reported to SGI.

- zlib doesn't work with gcc 2.6.3 on a DEC 3000/300LX under OSF/1 2.1 it works
  when compiled with cc.

- On Digital Unix 4.0D (formely OSF/1) on AlphaServer, the cc option -std1 is
  necessary to get gzprintf working correctly. This is done by configure.

- zlib doesn't work on HP-UX 9.05 with some versions of /bin/cc. It works with
  other compilers. Use "make test" to check your compiler.

- gzdopen is not supported on RISCOS, BEOS and by some Mac compilers.

- For PalmOs, see http://palmzlib.sourceforge.net/

- When building a shared, i.e. dynamic library on Mac OS X, the library must be
  installed before testing (do "make install" before "make test"), since the
  library location is specified in the library.


Acknowledgments:

  The deflate format used by zlib was defined by Phil Katz. The deflate
  and zlib specifications were written by L. Peter Deutsch. Thanks to all the
  people who reported problems and suggested various improvements in zlib;
  they are too numerous to cite here.

Copyright notice:

 (C) 1995-2004 Jean-loup Gailly and Mark Adler

  This software is provided 'as-is', without any express or implied
  warranty.  In no event will the authors be held liable for any damages
  arising from the use of this software.

  Permission is granted to anyone to use this software for any purpose,
  including commercial applications, and to alter it and redistribute it
  freely, subject to the following restrictions:

  1. The origin of this software must not be misrepresented; you must not
     claim that you wrote the original software. If you use this software
     in a product, an acknowledgment in the product documentation would be
     appreciated but is not required.
  2. Altered source versions must be plainly marked as such, and must not be
     misrepresented as being the original software.
  3. This notice may not be removed or altered from any source distribution.

  Jean-loup Gailly        Mark Adler
  jloup@gzip.org          madler@alumni.caltech.edu

If you use the zlib library in a product, we would appreciate *not*
receiving lengthy legal documents to sign. The sources are provided
for free but without warranty of any kind.  The library has been
entirely written by Jean-loup Gailly and Mark Adler; it does not
include third-party code.

If you redistribute modified sources, we would appreciate that you include
in the file ChangeLog history information documenting your changes. Please
read the FAQ for more information on the distribution of modified source
versions.

dlcompat for Darwin
=========================

This is dlcompat, a small library that emulates the dlopen()
interface on top of Darwin's dyld API.

dlcompat allows loading a ".dylib" library (as long as the RTLD_LOCAL 
flag isn't passed to dlopen()). It can be configured to yield a warning 
when trying to close it (dynamic libraries cannot currently be unloaded).

It automatically searches for modules in several directories when no 
absolute path is specified and the module is not found in the current 
directory.

The paths searched are those specified in the environment variables
LD_LIBRARY_PATH and DYLD_LIBRARY_PATH plus /lib, /usr/local/lib and 
/usr/lib or the path specified in the environment variable 
DYLD_FALLBACK_LIBRARY_PATH.

In the default install the behavior of dlsym is to automatically prepend
an underscore to passed in symbol names, this allows easier porting of
applications which were written specifically for ELF based lifeforms.

Installation
--------------
Type:
	./configure
	make
	sudo make install

This will compile the source file, generate both a static and shared
library called libdl and install it into /usr/local/lib. The header
file dlfcn.h will be installed in /usr/local/include.

If you want to place the files somewhere else, run

  make clean
  ./configure --prefix=<prefix>
  make
  sudo make install

where <prefix> is the hierarchy you want to install into, e.g. /usr
for /usr/lib and /usr/include (_NOT_ recommended!).

To enable debugging output (useful for me), run

  make clean
  ./configure --enable-debug
  make
  sudo make install
  
If you want old dlcompat style behavior of not prepending the underscore
on calls to dlsym then type:

  make clean
  ./configure --enable-fink
  make
  sudo make install

Usage
-------
Software that uses GNU autoconf will likely check for a library called
libdl, that's why I named it that way. For software that doesn't find
the library on its own, you must add a '-ldl' to the appropriate
Makefile (or environment) variable, usually LIBS.

If you installed dlcompat into a directory other than /usr/local/lib,
you must tell the compiler where to find it. Add '-L<prefix>/lib' to
LDFLAGS (or CFLAGS) and '-I<prefix>/include' to CPPFLAGS (or CFLAGS).

Notes
-----
If you are writing new software and plan to have Mac OX X compatibility you
should look at the dyld api's in /usr/include/mach-o/dyld.h, rather than
using dlcompat, using the native api's is the supported method of loading
dynamically on Mac OS X, if you want an small example, look at dlfcn_simple.c,
which should help get you started.

Also note that the functions in dlcompat are not thread safe, and while it is not
POSIX spec compliant, it is about as close to compliance as it is going to get though.

You can always get the latest version from opendarwin cvs:

  cvs -d :pserver:anonymous@anoncvs.opendarwin.org:/cvs/od login
  cvs -z3 -d :pserver:anonymous@anoncvs.opendarwin.org:/cvs/od \
               co -d dlcompat proj/dlcompat


It is hoped that this library will be useful, and as bug free as possible, if you find
any bugs please let us know about them so they can be fixed.

Please send bug reports to Peter O'Gorman <ogorman@users.sourceforge.net>

Thanks.


The files in this directory are taken from
http://www.opendarwin.org/cgi-bin/cvsweb.cgi/~checkout~/proj/dlcompat/

The LICENSE in this directory applies to these files.

Thomas Heller, Jan 2003

These files have been modified so they fall back to the system
dlfcn calls if available in libSystem.

Bob Ippolito, Feb 2006

Status
======

libffi-3.0.10 was released on XXXXXXXXXX, 2010. Check the libffi web
page for updates: <URL:http://sourceware.org/libffi/>.


What is libffi?
===============

Compilers for high level languages generate code that follow certain
conventions. These conventions are necessary, in part, for separate
compilation to work. One such convention is the "calling
convention". The "calling convention" is essentially a set of
assumptions made by the compiler about where function arguments will
be found on entry to a function. A "calling convention" also specifies
where the return value for a function is found.

Some programs may not know at the time of compilation what arguments
are to be passed to a function. For instance, an interpreter may be
told at run-time about the number and types of arguments used to call
a given function. Libffi can be used in such programs to provide a
bridge from the interpreter program to compiled code.

The libffi library provides a portable, high level programming
interface to various calling conventions. This allows a programmer to
call any function specified by a call interface description at run
time.  

FFI stands for Foreign Function Interface.  A foreign function
interface is the popular name for the interface that allows code
written in one language to call code written in another language. The
libffi library really only provides the lowest, machine dependent
layer of a fully featured foreign function interface. A layer must
exist above libffi that handles type conversions for values passed
between the two languages.


Supported Platforms
===================

Libffi has been ported to many different platforms.
For specific configuration details and testing status, please
refer to the wiki page here:

 http://www.moxielogic.org/wiki/index.php?title=Libffi_3.0.10

At the time of release, the following basic configurations have been
tested:

|--------------+------------------|
| Architecture | Operating System |
|--------------+------------------|
| Alpha        | Linux            |
| Alpha        | Tru64            |
| ARM          | Linux            |
| AVR32        | Linux            |
| HPPA         | HPUX             |
| IA-64        | Linux            |
| MIPS         | IRIX             |
| MIPS         | Linux            |
| MIPS64       | Linux            |
| PowerPC      | Linux            |
| PowerPC      | Mac OSX          |
| PowerPC      | FreeBSD          |
| PowerPC64    | Linux            |
| S390         | Linux            |
| S390X        | Linux            |
| SPARC        | Linux            |
| SPARC        | Solaris          |
| SPARC64      | Linux            |
| SPARC64      | FreeBSD          |
| X86          | FreeBSD          |
| X86          | kFreeBSD         |
| X86          | Linux            |
| X86          | Mac OSX          |
| X86          | OpenBSD          |
| X86          | Solaris          |
| X86          | Windows/Cygwin   |
| X86          | Windows/MingW    |
| X86-64       | FreeBSD          |
| X86-64       | Linux            |
| X86-64       | OpenBSD          |
| X86-64       | Windows/MingW    |
|--------------+------------------|

Please send additional platform test results to
libffi-discuss@sourceware.org and feel free to update the wiki page
above.

Installing libffi
=================

First you must configure the distribution for your particular
system. Go to the directory you wish to build libffi in and run the
"configure" program found in the root directory of the libffi source
distribution.

You may want to tell configure where to install the libffi library and
header files. To do that, use the --prefix configure switch.  Libffi
will install under /usr/local by default. 

If you want to enable extra run-time debugging checks use the the
--enable-debug configure switch. This is useful when your program dies
mysteriously while using libffi. 

Another useful configure switch is --enable-purify-safety. Using this
will add some extra code which will suppress certain warnings when you
are using Purify with libffi. Only use this switch when using 
Purify, as it will slow down the library.

It's also possible to build libffi on Windows platforms with
Microsoft's Visual C++ compiler.  In this case, use the msvcc.sh
wrapper script during configuration like so:

path/to/configure --enable-shared --enable-static \
	CC=path/to/msvcc.sh LD=link \
	CPP=\"cl -nologo -EP\"

Configure has many other options. Use "configure --help" to see them all.

Once configure has finished, type "make". Note that you must be using
GNU make.  You can ftp GNU make from prep.ai.mit.edu:/pub/gnu.

To ensure that libffi is working as advertised, type "make check".
This will require that you have DejaGNU installed.

To install the library and header files, type "make install".


History
=======

See the ChangeLog files for details.

3.0.10 ???-??-??
        Fix the N64 build on mips-sgi-irix6.5.
	Testsuite fixes for Tru64 Unix.
	Enable builds with Microsoft's compiler.
	Enable x86 builds with Sun's compiler.

3.0.9 Dec-31-09
        Add AVR32 and win64 ports.  Add ARM softfp support.
	Many fixes for AIX, Solaris, HP-UX, *BSD.
	Several PowerPC and x86-64 bug fixes.
	Build DLL for windows.

3.0.8 Dec-19-08
        Add *BSD, BeOS, and PA-Linux support.

3.0.7 Nov-11-08
        Fix for ppc FreeBSD.
	(thanks to Andreas Tobler)

3.0.6 Jul-17-08
        Fix for closures on sh.
	Mark the sh/sh64 stack as non-executable.
	(both thanks to Kaz Kojima)

3.0.5 Apr-3-08
        Fix libffi.pc file.
	Fix #define ARM for IcedTea users.
	Fix x86 closure bug.

3.0.4 Feb-24-08
        Fix x86 OpenBSD configury.

3.0.3 Feb-22-08
        Enable x86 OpenBSD thanks to Thomas Heller, and
	x86-64 FreeBSD thanks to Björn König and Andreas Tobler.
	Clean up test instruction in README.

3.0.2 Feb-21-08
        Improved x86 FreeBSD support.
	Thanks to Björn König.

3.0.1 Feb-15-08
        Fix instruction cache flushing bug on MIPS.
	Thanks to David Daney.

3.0.0 Feb-15-08
        Many changes, mostly thanks to the GCC project.
	Cygnus Solutions is now Red Hat.

  [10 years go by...]

1.20 Oct-5-98
	Raffaele Sena produces ARM port.

1.19 Oct-5-98
	Fixed x86 long double and long long return support.
	m68k bug fixes from Andreas Schwab.
	Patch for DU assembler compatibility for the Alpha from Richard
	Henderson.

1.18 Apr-17-98
	Bug fixes and MIPS configuration changes.

1.17 Feb-24-98
	Bug fixes and m68k port from Andreas Schwab. PowerPC port from
	Geoffrey Keating. Various bug x86, Sparc and MIPS bug fixes.

1.16 Feb-11-98
	Richard Henderson produces Alpha port.

1.15 Dec-4-97
	Fixed an n32 ABI bug. New libtool, auto* support.

1.14 May-13-97
	libtool is now used to generate shared and static libraries.
	Fixed a minor portability problem reported by Russ McManus
	<mcmanr@eq.gs.com>.

1.13 Dec-2-96
	Added --enable-purify-safety to keep Purify from complaining
	about certain low level code.
	Sparc fix for calling functions with < 6 args.
	Linux x86 a.out fix.

1.12 Nov-22-96
	Added missing ffi_type_void, needed for supporting void return 
	types. Fixed test case for non MIPS machines. Cygnus Support 
	is now Cygnus Solutions. 

1.11 Oct-30-96
	Added notes about GNU make.

1.10 Oct-29-96
	Added configuration fix for non GNU compilers.

1.09 Oct-29-96
	Added --enable-debug configure switch. Clean-ups based on LCLint 
	feedback. ffi_mips.h is always installed. Many configuration 
	fixes. Fixed ffitest.c for sparc builds.

1.08 Oct-15-96
	Fixed n32 problem. Many clean-ups.

1.07 Oct-14-96
	Gordon Irlam rewrites v8.S again. Bug fixes.

1.06 Oct-14-96
	Gordon Irlam improved the sparc port. 

1.05 Oct-14-96
	Interface changes based on feedback.

1.04 Oct-11-96
	Sparc port complete (modulo struct passing bug).

1.03 Oct-10-96
	Passing struct args, and returning struct values works for
	all architectures/calling conventions. Expanded tests.

1.02 Oct-9-96
	Added SGI n32 support. Fixed bugs in both o32 and Linux support.
	Added "make test".

1.01 Oct-8-96
	Fixed float passing bug in mips version. Restructured some
	of the code. Builds cleanly with SGI tools.

1.00 Oct-7-96
	First release. No public announcement.


Authors & Credits
=================

libffi was originally written by Anthony Green <green@redhat.com>.

The developers of the GNU Compiler Collection project have made
innumerable valuable contributions.  See the ChangeLog file for
details.

Some of the ideas behind libffi were inspired by Gianni Mariani's free
gencall library for Silicon Graphics machines.

The closure mechanism was designed and implemented by Kresten Krab
Thorup.

Major processor architecture ports were contributed by the following
developers:

alpha		Richard Henderson
arm		Raffaele Sena
cris		Simon Posnjak, Hans-Peter Nilsson
frv		Anthony Green
ia64		Hans Boehm
m32r		Kazuhiro Inaoka
m68k		Andreas Schwab
mips		Anthony Green, Casey Marshall
mips64		David Daney
pa		Randolph Chung, Dave Anglin, Andreas Tobler
powerpc		Geoffrey Keating, Andreas Tobler, 
			 David Edelsohn, John Hornkvist
powerpc64	Jakub Jelinek
s390		Gerhard Tonn, Ulrich Weigand
sh		Kaz Kojima
sh64		Kaz Kojima
sparc		Anthony Green, Gordon Irlam
x86		Anthony Green, Jon Beniston
x86-64		Bo Thorsen

Jesper Skov and Andrew Haley both did more than their fair share of
stepping through the code and tracking down bugs.

Thanks also to Tom Tromey for bug fixes, documentation and
configuration help.

Thanks to Jim Blandy, who provided some useful feedback on the libffi
interface.

Andreas Tobler has done a tremendous amount of work on the testsuite.

Alex Oliva solved the executable page problem for SElinux.

The list above is almost certainly incomplete and inaccurate.  I'm
happy to make corrections or additions upon request.

If you have a problem, or have found a bug, please send a note to
green@redhat.com.

This directory contains the libffi package, which is not part of GCC but
shipped with GCC as convenience.

Status
======

libffi-2.00 has not been released yet! This is a development snapshot!

libffi-1.20 was released on October 5, 1998. Check the libffi web
page for updates: <URL:http://sources.redhat.com/libffi/>.


What is libffi?
===============

Compilers for high level languages generate code that follow certain
conventions. These conventions are necessary, in part, for separate
compilation to work. One such convention is the "calling
convention". The "calling convention" is essentially a set of
assumptions made by the compiler about where function arguments will
be found on entry to a function. A "calling convention" also specifies
where the return value for a function is found.

Some programs may not know at the time of compilation what arguments
are to be passed to a function. For instance, an interpreter may be
told at run-time about the number and types of arguments used to call
a given function. Libffi can be used in such programs to provide a
bridge from the interpreter program to compiled code.

The libffi library provides a portable, high level programming
interface to various calling conventions. This allows a programmer to
call any function specified by a call interface description at run
time.  

Ffi stands for Foreign Function Interface. A foreign function
interface is the popular name for the interface that allows code
written in one language to call code written in another language. The
libffi library really only provides the lowest, machine dependent
layer of a fully featured foreign function interface. A layer must
exist above libffi that handles type conversions for values passed
between the two languages.


Supported Platforms and Prerequisites
=====================================

Libffi has been ported to:

	SunOS 4.1.3 & Solaris 2.x (SPARC-V8, SPARC-V9)

	Irix 5.3 & 6.2 (System V/o32 & n32)

	Intel x86 - Linux (System V ABI)

	Alpha - Linux and OSF/1

	m68k - Linux (System V ABI)

	PowerPC - Linux (System V ABI, Darwin, AIX)

	ARM - Linux (System V ABI)

Libffi has been tested with the egcs 1.0.2 gcc compiler. Chances are
that other versions will work.  Libffi has also been built and tested
with the SGI compiler tools.

On PowerPC, the tests failed (see the note below).

You must use GNU make to build libffi. SGI's make will not work.
Sun's probably won't either.
	
If you port libffi to another platform, please let me know! I assume
that some will be easy (x86 NetBSD), and others will be more difficult
(HP).


Installing libffi
=================

[Note: before actually performing any of these installation steps,
 you may wish to read the "Platform Specific Notes" below.]

First you must configure the distribution for your particular
system. Go to the directory you wish to build libffi in and run the
"configure" program found in the root directory of the libffi source
distribution.

You may want to tell configure where to install the libffi library and
header files. To do that, use the --prefix configure switch.  Libffi
will install under /usr/local by default. 

If you want to enable extra run-time debugging checks use the the
--enable-debug configure switch. This is useful when your program dies
mysteriously while using libffi. 

Another useful configure switch is --enable-purify-safety. Using this
will add some extra code which will suppress certain warnings when you
are using Purify with libffi. Only use this switch when using 
Purify, as it will slow down the library.

Configure has many other options. Use "configure --help" to see them all.

Once configure has finished, type "make". Note that you must be using
GNU make. SGI's make will not work.  Sun's probably won't either.
You can ftp GNU make from prep.ai.mit.edu:/pub/gnu.

To ensure that libffi is working as advertised, type "make test".

To install the library and header files, type "make install".


Using libffi
============

	The Basics
	----------

Libffi assumes that you have a pointer to the function you wish to
call and that you know the number and types of arguments to pass it,
as well as the return type of the function.

The first thing you must do is create an ffi_cif object that matches
the signature of the function you wish to call. The cif in ffi_cif
stands for Call InterFace. To prepare a call interface object, use the
following function:

ffi_status ffi_prep_cif(ffi_cif *cif, ffi_abi abi,
			unsigned int nargs, 
			ffi_type *rtype, ffi_type **atypes);

	CIF is a pointer to the call interface object you wish
		to initialize.

	ABI is an enum that specifies the calling convention 
		to use for the call. FFI_DEFAULT_ABI defaults
		to the system's native calling convention. Other
		ABI's may be used with care. They are system
		specific.

	NARGS is the number of arguments this function accepts.	
		libffi does not yet support vararg functions.

	RTYPE is a pointer to an ffi_type structure that represents
		the return type of the function. Ffi_type objects
		describe the types of values. libffi provides
		ffi_type objects for many of the native C types:
		signed int, unsigned int, signed char, unsigned char,
		etc. There is also a pointer ffi_type object and
		a void ffi_type. Use &ffi_type_void for functions that 
		don't return values.

	ATYPES is a vector of ffi_type pointers. ARGS must be NARGS long.
		If NARGS is 0, this is ignored.


ffi_prep_cif will return a status code that you are responsible 
for checking. It will be one of the following:

	FFI_OK - All is good.

	FFI_BAD_TYPEDEF - One of the ffi_type objects that ffi_prep_cif
		came across is bad.


Before making the call, the VALUES vector should be initialized 
with pointers to the appropriate argument values.

To call the the function using the initialized ffi_cif, use the
ffi_call function:

void ffi_call(ffi_cif *cif, void *fn, void *rvalue, void **avalues);

	CIF is a pointer to the ffi_cif initialized specifically
		for this function.

	FN is a pointer to the function you want to call.

	RVALUE is a pointer to a chunk of memory that is to hold the
		result of the function call. Currently, it must be
		at least one word in size (except for the n32 version
		under Irix 6.x, which must be a pointer to an 8 byte 
		aligned value (a long long). It must also be at least 
		word aligned (depending on the return type, and the
		system's alignment requirements). If RTYPE is 
		&ffi_type_void, this is ignored. If RVALUE is NULL, 
		the return value is discarded.

	AVALUES is a vector of void* that point to the memory locations
		holding the argument values for a call.
		If NARGS is 0, this is ignored.


If you are expecting a return value from FN it will have been stored
at RVALUE.



	An Example
	----------

Here is a trivial example that calls puts() a few times.

    #include <stdio.h>
    #include <ffi.h>
    
    int main()
    {
      ffi_cif cif;
      ffi_type *args[1];
      void *values[1];
      char *s;
      int rc;
      
      /* Initialize the argument info vectors */    
      args[0] = &ffi_type_uint;
      values[0] = &s;
      
      /* Initialize the cif */
      if (ffi_prep_cif(&cif, FFI_DEFAULT_ABI, 1, 
    		       &ffi_type_uint, args) == FFI_OK)
        {
          s = "Hello World!";
          ffi_call(&cif, puts, &rc, values);
          /* rc now holds the result of the call to puts */
          
          /* values holds a pointer to the function's arg, so to 
	     call puts() again all we need to do is change the 
             value of s */
          s = "This is cool!";
          ffi_call(&cif, puts, &rc, values);
        }
      
      return 0;
    }



	Aggregate Types
	---------------

Although libffi has no special support for unions or bit-fields, it is
perfectly happy passing structures back and forth. You must first
describe the structure to libffi by creating a new ffi_type object
for it. Here is the definition of ffi_type:

    typedef struct _ffi_type
    {
      unsigned size;
      short alignment;
      short type;
      struct _ffi_type **elements;
    } ffi_type;
    
All structures must have type set to FFI_TYPE_STRUCT.  You may set
size and alignment to 0. These will be calculated and reset to the
appropriate values by ffi_prep_cif().

elements is a NULL terminated array of pointers to ffi_type objects
that describe the type of the structure elements. These may, in turn,
be structure elements.

The following example initializes a ffi_type object representing the
tm struct from Linux's time.h:

				    struct tm {
					int tm_sec;
					int tm_min;
					int tm_hour;
					int tm_mday;
					int tm_mon;
					int tm_year;
					int tm_wday;
					int tm_yday;
					int tm_isdst;
					/* Those are for future use. */
					long int __tm_gmtoff__;
					__const char *__tm_zone__;
				    };

    {
      ffi_type tm_type;
      ffi_type *tm_type_elements[12];
      int i;

      tm_type.size = tm_type.alignment = 0;
      tm_type.elements = &tm_type_elements;
    
      for (i = 0; i < 9; i++)
          tm_type_elements[i] = &ffi_type_sint;

      tm_type_elements[9] = &ffi_type_slong;
      tm_type_elements[10] = &ffi_type_pointer;
      tm_type_elements[11] = NULL;

      /* tm_type can now be used to represent tm argument types and
	 return types for ffi_prep_cif() */
    }



Platform Specific Notes
=======================

	Intel x86
	---------

There are no known problems with the x86 port.

	Sun SPARC - SunOS 4.1.3 & Solaris 2.x
	-------------------------------------

You must use GNU Make to build libffi on Sun platforms.

	MIPS - Irix 5.3 & 6.x
	---------------------

Irix 6.2 and better supports three different calling conventions: o32,
n32 and n64. Currently, libffi only supports both o32 and n32 under
Irix 6.x, but only o32 under Irix 5.3. Libffi will automatically be
configured for whichever calling convention it was built for.

By default, the configure script will try to build libffi with the GNU
development tools. To build libffi with the SGI development tools, set
the environment variable CC to either "cc -32" or "cc -n32" before
running configure under Irix 6.x (depending on whether you want an o32
or n32 library), or just "cc" for Irix 5.3.

With the n32 calling convention, when returning structures smaller
than 16 bytes, be sure to provide an RVALUE that is 8 byte aligned.
Here's one way of forcing this:

	double struct_storage[2];
	my_small_struct *s = (my_small_struct *) struct_storage;  
	/* Use s for RVALUE */

If you don't do this you are liable to get spurious bus errors. 

"long long" values are not supported yet.

You must use GNU Make to build libffi on SGI platforms.

	ARM - System V ABI
	------------------

The ARM port was performed on a NetWinder running ARM Linux ELF
(2.0.31) and gcc 2.8.1.



	PowerPC System V ABI
	--------------------

There are two `System V ABI's which libffi implements for PowerPC.
They differ only in how small structures are returned from functions.

In the FFI_SYSV version, structures that are 8 bytes or smaller are
returned in registers.  This is what GCC does when it is configured
for solaris, and is what the System V ABI I have (dated September
1995) says.

In the FFI_GCC_SYSV version, all structures are returned the same way:
by passing a pointer as the first argument to the function.  This is
what GCC does when it is configured for linux or a generic sysv
target.

EGCS 1.0.1 (and probably other versions of EGCS/GCC) also has a
inconsistency with the SysV ABI: When a procedure is called with many
floating-point arguments, some of them get put on the stack.  They are
all supposed to be stored in double-precision format, even if they are
only single-precision, but EGCS stores single-precision arguments as
single-precision anyway.  This causes one test to fail (the `many
arguments' test).


What's With The Crazy Comments?
===============================

You might notice a number of cryptic comments in the code, delimited
by /*@ and @*/. These are annotations read by the program LCLint, a
tool for statically checking C programs. You can read all about it at
<http://larch-www.lcs.mit.edu:8001/larch/lclint/index.html>.


History
=======

1.20 Oct-5-98
	Raffaele Sena produces ARM port.

1.19 Oct-5-98
	Fixed x86 long double and long long return support.
	m68k bug fixes from Andreas Schwab.
	Patch for DU assembler compatibility for the Alpha from Richard
	Henderson.

1.18 Apr-17-98
	Bug fixes and MIPS configuration changes.

1.17 Feb-24-98
	Bug fixes and m68k port from Andreas Schwab. PowerPC port from
	Geoffrey Keating. Various bug x86, Sparc and MIPS bug fixes.

1.16 Feb-11-98
	Richard Henderson produces Alpha port.

1.15 Dec-4-97
	Fixed an n32 ABI bug. New libtool, auto* support.

1.14 May-13-97
	libtool is now used to generate shared and static libraries.
	Fixed a minor portability problem reported by Russ McManus
	<mcmanr@eq.gs.com>.

1.13 Dec-2-96
	Added --enable-purify-safety to keep Purify from complaining
	about certain low level code.
	Sparc fix for calling functions with < 6 args.
	Linux x86 a.out fix.

1.12 Nov-22-96
	Added missing ffi_type_void, needed for supporting void return 
	types. Fixed test case for non MIPS machines. Cygnus Support 
	is now Cygnus Solutions. 

1.11 Oct-30-96
	Added notes about GNU make.

1.10 Oct-29-96
	Added configuration fix for non GNU compilers.

1.09 Oct-29-96
	Added --enable-debug configure switch. Clean-ups based on LCLint 
	feedback. ffi_mips.h is always installed. Many configuration 
	fixes. Fixed ffitest.c for sparc builds.

1.08 Oct-15-96
	Fixed n32 problem. Many clean-ups.

1.07 Oct-14-96
	Gordon Irlam rewrites v8.S again. Bug fixes.

1.06 Oct-14-96
	Gordon Irlam improved the sparc port. 

1.05 Oct-14-96
	Interface changes based on feedback.

1.04 Oct-11-96
	Sparc port complete (modulo struct passing bug).

1.03 Oct-10-96
	Passing struct args, and returning struct values works for
	all architectures/calling conventions. Expanded tests.

1.02 Oct-9-96
	Added SGI n32 support. Fixed bugs in both o32 and Linux support.
	Added "make test".

1.01 Oct-8-96
	Fixed float passing bug in mips version. Restructured some
	of the code. Builds cleanly with SGI tools.

1.00 Oct-7-96
	First release. No public announcement.


Authors & Credits
=================

libffi was written by Anthony Green <green@cygnus.com>.

Portions of libffi were derived from Gianni Mariani's free gencall
library for Silicon Graphics machines.

The closure mechanism was designed and implemented by Kresten Krab
Thorup.

The Sparc port was derived from code contributed by the fine folks at
Visible Decisions Inc <http://www.vdi.com>. Further enhancements were
made by Gordon Irlam at Cygnus Solutions <http://www.cygnus.com>.

The Alpha port was written by Richard Henderson at Cygnus Solutions.

Andreas Schwab ported libffi to m68k Linux and provided a number of
bug fixes.

Geoffrey Keating ported libffi to the PowerPC.

Raffaele Sena ported libffi to the ARM.

Jesper Skov and Andrew Haley both did more than their fair share of
stepping through the code and tracking down bugs.

Thanks also to Tom Tromey for bug fixes and configuration help.

Thanks to Jim Blandy, who provided some useful feedback on the libffi
interface.

If you have a problem, or have found a bug, please send a note to
green@cygnus.com.

The purpose is to hack the libffi sources so that they can be compiled
with MSVC, and to extend them so that they have the features I need
for ctypes.

I retrieved the libffi sources from the gcc cvs repository on
2004-01-27.  Then I did 'configure' in a 'build' subdirectory on a x86
linux system, and copied the files I found useful.

This directory contains the libffi package, which is not part of GCC but
shipped with GCC as convenience.

Status
======

libffi-2.00 has not been released yet! This is a development snapshot!

libffi-1.20 was released on October 5, 1998. Check the libffi web
page for updates: <URL:http://sources.redhat.com/libffi/>.


What is libffi?
===============

Compilers for high level languages generate code that follow certain
conventions. These conventions are necessary, in part, for separate
compilation to work. One such convention is the "calling
convention". The "calling convention" is essentially a set of
assumptions made by the compiler about where function arguments will
be found on entry to a function. A "calling convention" also specifies
where the return value for a function is found.

Some programs may not know at the time of compilation what arguments
are to be passed to a function. For instance, an interpreter may be
told at run-time about the number and types of arguments used to call
a given function. Libffi can be used in such programs to provide a
bridge from the interpreter program to compiled code.

The libffi library provides a portable, high level programming
interface to various calling conventions. This allows a programmer to
call any function specified by a call interface description at run
time.  

Ffi stands for Foreign Function Interface. A foreign function
interface is the popular name for the interface that allows code
written in one language to call code written in another language. The
libffi library really only provides the lowest, machine dependent
layer of a fully featured foreign function interface. A layer must
exist above libffi that handles type conversions for values passed
between the two languages.


Supported Platforms and Prerequisites
=====================================

Libffi has been ported to:

	SunOS 4.1.3 & Solaris 2.x (SPARC-V8, SPARC-V9)

	Irix 5.3 & 6.2 (System V/o32 & n32)

	Intel x86 - Linux (System V ABI)

	Alpha - Linux and OSF/1

	m68k - Linux (System V ABI)

	PowerPC - Linux (System V ABI, Darwin, AIX)

	ARM - Linux (System V ABI)

Libffi has been tested with the egcs 1.0.2 gcc compiler. Chances are
that other versions will work.  Libffi has also been built and tested
with the SGI compiler tools.

On PowerPC, the tests failed (see the note below).

You must use GNU make to build libffi. SGI's make will not work.
Sun's probably won't either.
	
If you port libffi to another platform, please let me know! I assume
that some will be easy (x86 NetBSD), and others will be more difficult
(HP).


Installing libffi
=================

[Note: before actually performing any of these installation steps,
 you may wish to read the "Platform Specific Notes" below.]

First you must configure the distribution for your particular
system. Go to the directory you wish to build libffi in and run the
"configure" program found in the root directory of the libffi source
distribution.

You may want to tell configure where to install the libffi library and
header files. To do that, use the --prefix configure switch.  Libffi
will install under /usr/local by default. 

If you want to enable extra run-time debugging checks use the the
--enable-debug configure switch. This is useful when your program dies
mysteriously while using libffi. 

Another useful configure switch is --enable-purify-safety. Using this
will add some extra code which will suppress certain warnings when you
are using Purify with libffi. Only use this switch when using 
Purify, as it will slow down the library.

Configure has many other options. Use "configure --help" to see them all.

Once configure has finished, type "make". Note that you must be using
GNU make. SGI's make will not work.  Sun's probably won't either.
You can ftp GNU make from prep.ai.mit.edu:/pub/gnu.

To ensure that libffi is working as advertised, type "make test".

To install the library and header files, type "make install".


Using libffi
============

	The Basics
	----------

Libffi assumes that you have a pointer to the function you wish to
call and that you know the number and types of arguments to pass it,
as well as the return type of the function.

The first thing you must do is create an ffi_cif object that matches
the signature of the function you wish to call. The cif in ffi_cif
stands for Call InterFace. To prepare a call interface object, use the
following function:

ffi_status ffi_prep_cif(ffi_cif *cif, ffi_abi abi,
			unsigned int nargs, 
			ffi_type *rtype, ffi_type **atypes);

	CIF is a pointer to the call interface object you wish
		to initialize.

	ABI is an enum that specifies the calling convention 
		to use for the call. FFI_DEFAULT_ABI defaults
		to the system's native calling convention. Other
		ABI's may be used with care. They are system
		specific.

	NARGS is the number of arguments this function accepts.	
		libffi does not yet support vararg functions.

	RTYPE is a pointer to an ffi_type structure that represents
		the return type of the function. Ffi_type objects
		describe the types of values. libffi provides
		ffi_type objects for many of the native C types:
		signed int, unsigned int, signed char, unsigned char,
		etc. There is also a pointer ffi_type object and
		a void ffi_type. Use &ffi_type_void for functions that 
		don't return values.

	ATYPES is a vector of ffi_type pointers. ARGS must be NARGS long.
		If NARGS is 0, this is ignored.


ffi_prep_cif will return a status code that you are responsible 
for checking. It will be one of the following:

	FFI_OK - All is good.

	FFI_BAD_TYPEDEF - One of the ffi_type objects that ffi_prep_cif
		came across is bad.


Before making the call, the VALUES vector should be initialized 
with pointers to the appropriate argument values.

To call the the function using the initialized ffi_cif, use the
ffi_call function:

void ffi_call(ffi_cif *cif, void *fn, void *rvalue, void **avalues);

	CIF is a pointer to the ffi_cif initialized specifically
		for this function.

	FN is a pointer to the function you want to call.

	RVALUE is a pointer to a chunk of memory that is to hold the
		result of the function call. Currently, it must be
		at least one word in size (except for the n32 version
		under Irix 6.x, which must be a pointer to an 8 byte 
		aligned value (a long long). It must also be at least 
		word aligned (depending on the return type, and the
		system's alignment requirements). If RTYPE is 
		&ffi_type_void, this is ignored. If RVALUE is NULL, 
		the return value is discarded.

	AVALUES is a vector of void* that point to the memory locations
		holding the argument values for a call.
		If NARGS is 0, this is ignored.


If you are expecting a return value from FN it will have been stored
at RVALUE.



	An Example
	----------

Here is a trivial example that calls puts() a few times.

    #include <stdio.h>
    #include <ffi.h>
    
    int main()
    {
      ffi_cif cif;
      ffi_type *args[1];
      void *values[1];
      char *s;
      int rc;
      
      /* Initialize the argument info vectors */    
      args[0] = &ffi_type_uint;
      values[0] = &s;
      
      /* Initialize the cif */
      if (ffi_prep_cif(&cif, FFI_DEFAULT_ABI, 1, 
    		       &ffi_type_uint, args) == FFI_OK)
        {
          s = "Hello World!";
          ffi_call(&cif, puts, &rc, values);
          /* rc now holds the result of the call to puts */
          
          /* values holds a pointer to the function's arg, so to 
	     call puts() again all we need to do is change the 
             value of s */
          s = "This is cool!";
          ffi_call(&cif, puts, &rc, values);
        }
      
      return 0;
    }



	Aggregate Types
	---------------

Although libffi has no special support for unions or bit-fields, it is
perfectly happy passing structures back and forth. You must first
describe the structure to libffi by creating a new ffi_type object
for it. Here is the definition of ffi_type:

    typedef struct _ffi_type
    {
      unsigned size;
      short alignment;
      short type;
      struct _ffi_type **elements;
    } ffi_type;
    
All structures must have type set to FFI_TYPE_STRUCT.  You may set
size and alignment to 0. These will be calculated and reset to the
appropriate values by ffi_prep_cif().

elements is a NULL terminated array of pointers to ffi_type objects
that describe the type of the structure elements. These may, in turn,
be structure elements.

The following example initializes a ffi_type object representing the
tm struct from Linux's time.h:

				    struct tm {
					int tm_sec;
					int tm_min;
					int tm_hour;
					int tm_mday;
					int tm_mon;
					int tm_year;
					int tm_wday;
					int tm_yday;
					int tm_isdst;
					/* Those are for future use. */
					long int __tm_gmtoff__;
					__const char *__tm_zone__;
				    };

    {
      ffi_type tm_type;
      ffi_type *tm_type_elements[12];
      int i;

      tm_type.size = tm_type.alignment = 0;
      tm_type.elements = &tm_type_elements;
    
      for (i = 0; i < 9; i++)
          tm_type_elements[i] = &ffi_type_sint;

      tm_type_elements[9] = &ffi_type_slong;
      tm_type_elements[10] = &ffi_type_pointer;
      tm_type_elements[11] = NULL;

      /* tm_type can now be used to represent tm argument types and
	 return types for ffi_prep_cif() */
    }



Platform Specific Notes
=======================

	Intel x86
	---------

There are no known problems with the x86 port.

	Sun SPARC - SunOS 4.1.3 & Solaris 2.x
	-------------------------------------

You must use GNU Make to build libffi on Sun platforms.

	MIPS - Irix 5.3 & 6.x
	---------------------

Irix 6.2 and better supports three different calling conventions: o32,
n32 and n64. Currently, libffi only supports both o32 and n32 under
Irix 6.x, but only o32 under Irix 5.3. Libffi will automatically be
configured for whichever calling convention it was built for.

By default, the configure script will try to build libffi with the GNU
development tools. To build libffi with the SGI development tools, set
the environment variable CC to either "cc -32" or "cc -n32" before
running configure under Irix 6.x (depending on whether you want an o32
or n32 library), or just "cc" for Irix 5.3.

With the n32 calling convention, when returning structures smaller
than 16 bytes, be sure to provide an RVALUE that is 8 byte aligned.
Here's one way of forcing this:

	double struct_storage[2];
	my_small_struct *s = (my_small_struct *) struct_storage;  
	/* Use s for RVALUE */

If you don't do this you are liable to get spurious bus errors. 

"long long" values are not supported yet.

You must use GNU Make to build libffi on SGI platforms.

	ARM - System V ABI
	------------------

The ARM port was performed on a NetWinder running ARM Linux ELF
(2.0.31) and gcc 2.8.1.



	PowerPC System V ABI
	--------------------

There are two `System V ABI's which libffi implements for PowerPC.
They differ only in how small structures are returned from functions.

In the FFI_SYSV version, structures that are 8 bytes or smaller are
returned in registers.  This is what GCC does when it is configured
for solaris, and is what the System V ABI I have (dated September
1995) says.

In the FFI_GCC_SYSV version, all structures are returned the same way:
by passing a pointer as the first argument to the function.  This is
what GCC does when it is configured for linux or a generic sysv
target.

EGCS 1.0.1 (and probably other versions of EGCS/GCC) also has a
inconsistency with the SysV ABI: When a procedure is called with many
floating-point arguments, some of them get put on the stack.  They are
all supposed to be stored in double-precision format, even if they are
only single-precision, but EGCS stores single-precision arguments as
single-precision anyway.  This causes one test to fail (the `many
arguments' test).


What's With The Crazy Comments?
===============================

You might notice a number of cryptic comments in the code, delimited
by /*@ and @*/. These are annotations read by the program LCLint, a
tool for statically checking C programs. You can read all about it at
<http://larch-www.lcs.mit.edu:8001/larch/lclint/index.html>.


History
=======

1.20 Oct-5-98
	Raffaele Sena produces ARM port.

1.19 Oct-5-98
	Fixed x86 long double and long long return support.
	m68k bug fixes from Andreas Schwab.
	Patch for DU assembler compatibility for the Alpha from Richard
	Henderson.

1.18 Apr-17-98
	Bug fixes and MIPS configuration changes.

1.17 Feb-24-98
	Bug fixes and m68k port from Andreas Schwab. PowerPC port from
	Geoffrey Keating. Various bug x86, Sparc and MIPS bug fixes.

1.16 Feb-11-98
	Richard Henderson produces Alpha port.

1.15 Dec-4-97
	Fixed an n32 ABI bug. New libtool, auto* support.

1.14 May-13-97
	libtool is now used to generate shared and static libraries.
	Fixed a minor portability problem reported by Russ McManus
	<mcmanr@eq.gs.com>.

1.13 Dec-2-96
	Added --enable-purify-safety to keep Purify from complaining
	about certain low level code.
	Sparc fix for calling functions with < 6 args.
	Linux x86 a.out fix.

1.12 Nov-22-96
	Added missing ffi_type_void, needed for supporting void return 
	types. Fixed test case for non MIPS machines. Cygnus Support 
	is now Cygnus Solutions. 

1.11 Oct-30-96
	Added notes about GNU make.

1.10 Oct-29-96
	Added configuration fix for non GNU compilers.

1.09 Oct-29-96
	Added --enable-debug configure switch. Clean-ups based on LCLint 
	feedback. ffi_mips.h is always installed. Many configuration 
	fixes. Fixed ffitest.c for sparc builds.

1.08 Oct-15-96
	Fixed n32 problem. Many clean-ups.

1.07 Oct-14-96
	Gordon Irlam rewrites v8.S again. Bug fixes.

1.06 Oct-14-96
	Gordon Irlam improved the sparc port. 

1.05 Oct-14-96
	Interface changes based on feedback.

1.04 Oct-11-96
	Sparc port complete (modulo struct passing bug).

1.03 Oct-10-96
	Passing struct args, and returning struct values works for
	all architectures/calling conventions. Expanded tests.

1.02 Oct-9-96
	Added SGI n32 support. Fixed bugs in both o32 and Linux support.
	Added "make test".

1.01 Oct-8-96
	Fixed float passing bug in mips version. Restructured some
	of the code. Builds cleanly with SGI tools.

1.00 Oct-7-96
	First release. No public announcement.


Authors & Credits
=================

libffi was written by Anthony Green <green@cygnus.com>.

Portions of libffi were derived from Gianni Mariani's free gencall
library for Silicon Graphics machines.

The closure mechanism was designed and implemented by Kresten Krab
Thorup.

The Sparc port was derived from code contributed by the fine folks at
Visible Decisions Inc <http://www.vdi.com>. Further enhancements were
made by Gordon Irlam at Cygnus Solutions <http://www.cygnus.com>.

The Alpha port was written by Richard Henderson at Cygnus Solutions.

Andreas Schwab ported libffi to m68k Linux and provided a number of
bug fixes.

Geoffrey Keating ported libffi to the PowerPC.

Raffaele Sena ported libffi to the ARM.

Jesper Skov and Andrew Haley both did more than their fair share of
stepping through the code and tracking down bugs.

Thanks also to Tom Tromey for bug fixes and configuration help.

Thanks to Jim Blandy, who provided some useful feedback on the libffi
interface.

If you have a problem, or have found a bug, please send a note to
green@cygnus.com.

This directory contains a slightly modified version of libffi, extracted from
the GCC source-tree.

The only modifications are those that are necessary to compile libffi using 
the Apple provided compiler and outside of the GCC source tree.

bits shared by the stringobject and unicodeobject implementations (and
possibly other modules, in a not too distant future).

the stuff in here is included into relevant places; see the individual
source files for details.

--------------------------------------------------------------------
the following defines used by the different modules:

STRINGLIB_CHAR

    the type used to hold a character (char or Py_UNICODE)

STRINGLIB_EMPTY

    a PyObject representing the empty string, only to be used if
    STRINGLIB_MUTABLE is 0

Py_ssize_t STRINGLIB_LEN(PyObject*)

    returns the length of the given string object (which must be of the
    right type)

PyObject* STRINGLIB_NEW(STRINGLIB_CHAR*, Py_ssize_t)

    creates a new string object

STRINGLIB_CHAR* STRINGLIB_STR(PyObject*)

    returns the pointer to the character data for the given string
    object (which must be of the right type)

int STRINGLIB_CHECK_EXACT(PyObject *)

    returns true if the object is an instance of our type, not a subclass

STRINGLIB_MUTABLE

    must be 0 or 1 to tell the cpp macros in stringlib code if the object
    being operated on is mutable or not

This is Python version 2.7.2
============================

Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011
Python Software Foundation.  All rights reserved.

Copyright (c) 2000 BeOpen.com.
All rights reserved.

Copyright (c) 1995-2001 Corporation for National Research Initiatives.
All rights reserved.

Copyright (c) 1991-1995 Stichting Mathematisch Centrum.
All rights reserved.


License information
-------------------

See the file "LICENSE" for information on the history of this
software, terms & conditions for usage, and a DISCLAIMER OF ALL
WARRANTIES.

This Python distribution contains no GNU General Public Licensed
(GPLed) code so it may be used in proprietary projects just like prior
Python distributions.  There are interfaces to some GNU code but these
are entirely optional.

All trademarks referenced herein are property of their respective
holders.


What's new in this release?
---------------------------

See the file "Misc/NEWS".


If you don't read instructions
------------------------------

Congratulations on getting this far. :-)

To start building right away (on UNIX): type "./configure" in the
current directory and when it finishes, type "make".  This creates an
executable "./python"; to install in /usr/local, first do "su root"
and then "make install".

The section `Build instructions' below is still recommended reading.


What is Python anyway?
----------------------

Python is an interpreted, interactive object-oriented programming
language suitable (amongst other uses) for distributed application
development, scripting, numeric computing and system testing.  Python
is often compared to Tcl, Perl, Java, JavaScript, Visual Basic or
Scheme.  To find out more about what Python can do for you, point your
browser to http://www.python.org/.


How do I learn Python?
----------------------

The official tutorial is still a good place to start; see
http://docs.python.org/ for online and downloadable versions, as well
as a list of other introductions, and reference documentation.

There's a quickly growing set of books on Python.  See
http://wiki.python.org/moin/PythonBooks for a list.


Documentation
-------------

All documentation is provided online in a variety of formats.  In
order of importance for new users: Tutorial, Library Reference,
Language Reference, Extending & Embedding, and the Python/C API.  The
Library Reference is especially of immense value since much of
Python's power is described there, including the built-in data types
and functions!

All documentation is also available online at the Python web site
(http://docs.python.org/, see below).  It is available online for occasional
reference, or can be downloaded in many formats for faster access.  The
documentation is downloadable in HTML, PostScript, PDF, LaTeX, and
reStructuredText (2.6+) formats; the LaTeX and reStructuredText versions are
primarily for documentation authors, translators, and people with special
formatting requirements.


Web sites
---------

New Python releases and related technologies are published at
http://www.python.org/.  Come visit us!


Newsgroups and Mailing Lists
----------------------------

Read comp.lang.python, a high-volume discussion newsgroup about
Python, or comp.lang.python.announce, a low-volume moderated newsgroup
for Python-related announcements.  These are also accessible as
mailing lists: see http://www.python.org/community/lists/ for an
overview of these and many other Python-related mailing lists.

Archives are accessible via the Google Groups Usenet archive; see
http://groups.google.com/.  The mailing lists are also archived, see
http://www.python.org/community/lists/ for details.


Bug reports
-----------

To report or search for bugs, please use the Python Bug
Tracker at http://bugs.python.org/.


Patches and contributions
-------------------------

To submit a patch or other contribution, please use the Python Patch
Manager at http://bugs.python.org/.  Guidelines
for patch submission may be found at http://www.python.org/dev/patches/.

If you have a proposal to change Python, you may want to send an email to the
comp.lang.python or python-ideas mailing lists for inital feedback. A Python
Enhancement Proposal (PEP) may be submitted if your idea gains ground. All
current PEPs, as well as guidelines for submitting a new PEP, are listed at
http://www.python.org/dev/peps/.


Questions
---------

For help, if you can't find it in the manuals or on the web site, it's
best to post to the comp.lang.python or the Python mailing list (see
above).  If you specifically don't want to involve the newsgroup or
mailing list, send questions to help@python.org (a group of volunteers
who answer questions as they can).  The newsgroup is the most
efficient way to ask public questions.


Build instructions
==================

Before you can build Python, you must first configure it.
Fortunately, the configuration and build process has been automated
for Unix and Linux installations, so all you usually have to do is
type a few commands and sit back.  There are some platforms where
things are not quite as smooth; see the platform specific notes below.
If you want to build for multiple platforms sharing the same source
tree, see the section on VPATH below.

Start by running the script "./configure", which determines your
system configuration and creates the Makefile.  (It takes a minute or
two -- please be patient!)  You may want to pass options to the
configure script -- see the section below on configuration options and
variables.  When it's done, you are ready to run make.

To build Python, you normally type "make" in the toplevel directory.
If you have changed the configuration, the Makefile may have to be
rebuilt.  In this case, you may have to run make again to correctly
build your desired target.  The interpreter executable is built in the
top level directory.

Once you have built a Python interpreter, see the subsections below on
testing and installation.  If you run into trouble, see the next
section.

Previous versions of Python used a manual configuration process that
involved editing the file Modules/Setup.  While this file still exists
and manual configuration is still supported, it is rarely needed any
more: almost all modules are automatically built as appropriate under
guidance of the setup.py script, which is run by Make after the
interpreter has been built.


Troubleshooting
---------------

See also the platform specific notes in the next section.

If you run into other trouble, see the FAQ
(http://www.python.org/doc/faq/) for hints on what can go wrong, and
how to fix it.

If you rerun the configure script with different options, remove all
object files by running "make clean" before rebuilding.  Believe it or
not, "make clean" sometimes helps to clean up other inexplicable
problems as well.  Try it before sending in a bug report!

If the configure script fails or doesn't seem to find things that
should be there, inspect the config.log file.

If you get a warning for every file about the -Olimit option being no
longer supported, you can ignore it.  There's no foolproof way to know
whether this option is needed; all we can do is test whether it is
accepted without error.  On some systems, e.g. older SGI compilers, it
is essential for performance (specifically when compiling ceval.c,
which has more basic blocks than the default limit of 1000).  If the
warning bothers you, edit the Makefile to remove "-Olimit 1500" from
the OPT variable.

If you get failures in test_long, or sys.maxint gets set to -1, you
are probably experiencing compiler bugs, usually related to
optimization.  This is a common problem with some versions of gcc, and
some vendor-supplied compilers, which can sometimes be worked around
by turning off optimization.  Consider switching to stable versions
(gcc 2.95.2, gcc 3.x, or contact your vendor.)

From Python 2.0 onward, all Python C code is ANSI C.  Compiling using
old K&R-C-only compilers is no longer possible.  ANSI C compilers are
available for all modern systems, either in the form of updated
compilers from the vendor, or one of the free compilers (gcc).

If "make install" fails mysteriously during the "compiling the library"
step, make sure that you don't have any of the PYTHONPATH or PYTHONHOME
environment variables set, as they may interfere with the newly built
executable which is compiling the library.

Unsupported systems
-------------------

A number of systems are not supported in Python 2.7 anymore. Some
support code is still present, but will be removed in later versions.
If you still need to use current Python versions on these systems,
please send a message to python-dev@python.org indicating that you
volunteer to support this system. For a more detailed discussion 
regarding no-longer-supported and resupporting platforms, as well
as a list of platforms that became or will be unsupported, see PEP 11.

More specifically, the following systems are not supported any
longer:
- SunOS 4
- DYNIX
- dgux
- Minix
- NeXT
- Irix 4 and --with-sgi-dl
- Linux 1
- Systems defining __d6_pthread_create (configure.in)
- Systems defining PY_PTHREAD_D4, PY_PTHREAD_D6,
  or PY_PTHREAD_D7 in thread_pthread.h
- Systems using --with-dl-dld
- Systems using --without-universal-newlines
- MacOS 9
- Systems using --with-wctype-functions
- Win9x, WinME


Platform specific notes
-----------------------

(Some of these may no longer apply.  If you find you can build Python
on these platforms without the special directions mentioned here,
submit a documentation bug report to SourceForge (see Bug Reports
above) so we can remove them!)

Unix platforms: If your vendor still ships (and you still use) Berkeley DB
        1.85 you will need to edit Modules/Setup to build the bsddb185
        module and add a line to sitecustomize.py which makes it the
        default.  In Modules/Setup a line like

            bsddb185 bsddbmodule.c

        should work.  (You may need to add -I, -L or -l flags to direct the
        compiler and linker to your include files and libraries.)

XXX I think this next bit is out of date:

64-bit platforms: The modules audioop, and imageop don't work.
        The setup.py script disables them on 64-bit installations.
        Don't try to enable them in the Modules/Setup file.  They
        contain code that is quite wordsize sensitive.  (If you have a
        fix, let us know!)

Solaris: When using Sun's C compiler with threads, at least on Solaris
        2.5.1, you need to add the "-mt" compiler option (the simplest
        way is probably to specify the compiler with this option as
        the "CC" environment variable when running the configure
        script).

        When using GCC on Solaris, beware of binutils 2.13 or GCC
        versions built using it.  This mistakenly enables the
        -zcombreloc option which creates broken shared libraries on
        Solaris.  binutils 2.12 works, and the binutils maintainers
        are aware of the problem.  Binutils 2.13.1 only partially
        fixed things.  It appears that 2.13.2 solves the problem
        completely.  This problem is known to occur with Solaris 2.7
        and 2.8, but may also affect earlier and later versions of the
        OS.

        When the dynamic loader complains about errors finding shared
        libraries, such as

        ld.so.1: ./python: fatal: libstdc++.so.5: open failed:
        No such file or directory

        you need to first make sure that the library is available on
        your system. Then, you need to instruct the dynamic loader how
        to find it. You can choose any of the following strategies:

        1. When compiling Python, set LD_RUN_PATH to the directories
           containing missing libraries.
        2. When running Python, set LD_LIBRARY_PATH to these directories.
        3. Use crle(8) to extend the search path of the loader.
        4. Modify the installed GCC specs file, adding -R options into the
           *link: section.

        The complex object fails to compile on Solaris 10 with gcc 3.4 (at
        least up to 3.4.3).  To work around it, define Py_HUGE_VAL as
        HUGE_VAL(), e.g.:

          make CPPFLAGS='-D"Py_HUGE_VAL=HUGE_VAL()" -I. -I$(srcdir)/Include'
          ./python setup.py CPPFLAGS='-D"Py_HUGE_VAL=HUGE_VAL()"'

Linux:  A problem with threads and fork() was tracked down to a bug in
        the pthreads code in glibc version 2.0.5; glibc version 2.0.7
        solves the problem.  This causes the popen2 test to fail;
        problem and solution reported by Pablo Bleyer.

Red Hat Linux: Red Hat 9 built Python2.2 in UCS-4 mode and hacked
        Tcl to support it. To compile Python2.3 with Tkinter, you will
        need to pass --enable-unicode=ucs4 flag to ./configure.

        There's an executable /usr/bin/python which is Python
        1.5.2 on most older Red Hat installations; several key Red Hat tools
        require this version.  Python 2.1.x may be installed as
        /usr/bin/python2.  The Makefile installs Python as
        /usr/local/bin/python, which may or may not take precedence
        over /usr/bin/python, depending on how you have set up $PATH.

FreeBSD 3.x and probably platforms with NCurses that use libmytinfo or
        similar: When using cursesmodule, the linking is not done in
        the correct order with the defaults.  Remove "-ltermcap" from
        the readline entry in Setup, and use as curses entry: "curses
        cursesmodule.c -lmytinfo -lncurses -ltermcap" - "mytinfo" (so
        called on FreeBSD) should be the name of the auxiliary library
        required on your platform.  Normally, it would be linked
        automatically, but not necessarily in the correct order.

BSDI:   BSDI versions before 4.1 have known problems with threads,
        which can cause strange errors in a number of modules (for
        instance, the 'test_signal' test script will hang forever.)
        Turning off threads (with --with-threads=no) or upgrading to
        BSDI 4.1 solves this problem.

DEC Unix: Run configure with --with-dec-threads, or with
        --with-threads=no if no threads are desired (threads are on by
        default).  When using GCC, it is possible to get an internal
        compiler error if optimization is used.  This was reported for
        GCC 2.7.2.3 on selectmodule.c.  Manually compile the affected
        file without optimization to solve the problem.

DEC Ultrix: compile with GCC to avoid bugs in the native compiler,
        and pass SHELL=/bin/sh5 to Make when installing.

AIX:    A complete overhaul of the shared library support is now in
        place.  See Misc/AIX-NOTES for some notes on how it's done.
        (The optimizer bug reported at this place in previous releases
        has been worked around by a minimal code change.) If you get
        errors about pthread_* functions, during compile or during
        testing, try setting CC to a thread-safe (reentrant) compiler,
        like "cc_r".  For full C++ module support, set CC="xlC_r" (or
        CC="xlC" without thread support).

AIX 5.3: To build a 64-bit version with IBM's compiler, I used the
        following:

        export PATH=/usr/bin:/usr/vacpp/bin
        ./configure --with-gcc="xlc_r -q64" --with-cxx="xlC_r -q64" \
                    --disable-ipv6 AR="ar -X64"
        make

HP-UX:  When using threading, you may have to add -D_REENTRANT to the
        OPT variable in the top-level Makefile; reported by Pat Knight,
        this seems to make a difference (at least for HP-UX 10.20)
        even though pyconfig.h defines it. This seems unnecessary when
        using HP/UX 11 and later - threading seems to work "out of the
        box".

HP-UX ia64: When building on the ia64 (Itanium) platform using HP's
        compiler, some experience has shown that the compiler's
        optimiser produces a completely broken version of python
        (see http://bugs.python.org/814976). To work around this,
        edit the Makefile and remove -O from the OPT line.

        To build a 64-bit executable on an Itanium 2 system using HP's
        compiler, use these environment variables:

                CC=cc
                CXX=aCC
                BASECFLAGS="+DD64"
                LDFLAGS="+DD64 -lxnet"

        and call configure as:

                ./configure --without-gcc

        then *unset* the environment variables again before running
        make.  (At least one of these flags causes the build to fail
        if it remains set.)  You still have to edit the Makefile and
        remove -O from the OPT line.

HP PA-RISC 2.0: A recent bug report (http://bugs.python.org/546117)
        suggests that the C compiler in this 64-bit system has bugs
        in the optimizer that break Python.  Compiling without
        optimization solves the problems.

SCO:    The following apply to SCO 3 only; Python builds out of the box
        on SCO 5 (or so we've heard).

        1) Everything works much better if you add -U__STDC__ to the
        defs.  This is because all the SCO header files are broken.
        Anything that isn't mentioned in the C standard is
        conditionally excluded when __STDC__ is defined.

        2) Due to the U.S. export restrictions, SCO broke the crypt
        stuff out into a separate library, libcrypt_i.a so the LIBS
        needed be set to:

                LIBS=' -lsocket -lcrypt_i'

UnixWare: There are known bugs in the math library of the system, as well as
        problems in the handling of threads (calling fork in one
        thread may interrupt system calls in others). Therefore, test_math and
        tests involving threads will fail until those problems are fixed.

QNX:    Chris Herborth (chrish@qnx.com) writes:
        configure works best if you use GNU bash; a port is available on
        ftp.qnx.com in /usr/free.  I used the following process to build,
        test and install Python 1.5.x under QNX:

        1) CONFIG_SHELL=/usr/local/bin/bash CC=cc RANLIB=: \
            ./configure --verbose --without-gcc --with-libm=""

        2) edit Modules/Setup to activate everything that makes sense for
           your system... tested here at QNX with the following modules:

                array, audioop, binascii, cPickle, cStringIO, cmath,
                crypt, curses, errno, fcntl, gdbm, grp, imageop,
                _locale, math, md5, new, operator, parser, pcre,
                posix, pwd, readline, regex, reop,
                select, signal, socket, soundex, strop, struct,
                syslog, termios, time, timing, zlib, audioop, imageop

        3) make SHELL=/usr/local/bin/bash

           or, if you feel the need for speed:

           make SHELL=/usr/local/bin/bash OPT="-5 -Oil+nrt"

        4) make SHELL=/usr/local/bin/bash test

           Using GNU readline 2.2 seems to behave strangely, but I
           think that's a problem with my readline 2.2 port.  :-\

        5) make SHELL=/usr/local/bin/bash install

        If you get SIGSEGVs while running Python (I haven't yet, but
        I've only run small programs and the test cases), you're
        probably running out of stack; the default 32k could be a
        little tight.  To increase the stack size, edit the Makefile
        to read: LDFLAGS = -N 48k

BeOS:   See Misc/BeOS-NOTES for notes about compiling/installing
        Python on BeOS R3 or later.  Note that only the PowerPC
        platform is supported for R3; both PowerPC and x86 are
        supported for R4.

Cray T3E: Mark Hadfield (m.hadfield@niwa.co.nz) writes:
        Python can be built satisfactorily on a Cray T3E but based on
        my experience with the NIWA T3E (2002-05-22, version 2.2.1)
        there are a few bugs and gotchas. For more information see a
        thread on comp.lang.python in May 2002 entitled "Building
        Python on Cray T3E".

        1) Use Cray's cc and not gcc. The latter was reported not to
           work by Konrad Hinsen. It may work now, but it may not.

        2) To set sys.platform to something sensible, pass the
           following environment variable to the configure script:

             MACHDEP=unicosmk

        2) Run configure with option "--enable-unicode=ucs4".

        3) The Cray T3E does not support dynamic linking, so extension
           modules have to be built by adding (or uncommenting) lines
           in Modules/Setup. The minimum set of modules is

             posix, new, _sre, unicodedata

           On NIWA's vanilla T3E system the following have also been
           included successfully:

             _codecs, _locale, _socket, _symtable, _testcapi, _weakref
             array, binascii, cmath, cPickle, crypt, cStringIO, dbm
             errno, fcntl, grp, math, md5, operator, parser, pcre, pwd
             regex, rotor, select, struct, strop, syslog, termios
             time, timing, xreadlines

        4) Once the python executable and library have been built, make
           will execute setup.py, which will attempt to build remaining
           extensions and link them dynamically. Each of these attempts
           will fail but should not halt the make process. This is
           normal.

        5) Running "make test" uses a lot of resources and causes
           problems on our system. You might want to try running tests
           singly or in small groups.

SGI:    SGI's standard "make" utility (/bin/make or /usr/bin/make)
        does not check whether a command actually changed the file it
        is supposed to build.  This means that whenever you say "make"
        it will redo the link step.  The remedy is to use SGI's much
        smarter "smake" utility (/usr/sbin/smake), or GNU make.  If
        you set the first line of the Makefile to #!/usr/sbin/smake
        smake will be invoked by make (likewise for GNU make).

        WARNING: There are bugs in the optimizer of some versions of
        SGI's compilers that can cause bus errors or other strange
        behavior, especially on numerical operations.  To avoid this,
        try building with "make OPT=".

OS/2:   If you are running Warp3 or Warp4 and have IBM's VisualAge C/C++
        compiler installed, just change into the pc\os2vacpp directory
        and type NMAKE.  Threading and sockets are supported by default
        in the resulting binaries of PYTHON15.DLL and PYTHON.EXE.

Reliant UNIX: The thread support does not compile on Reliant UNIX, and
        there is a (minor) problem in the configure script for that
        platform as well.  This should be resolved in time for a
        future release.

MacOSX: The tests will crash on both 10.1 and 10.2 with SEGV in
        test_re and test_sre due to the small default stack size.  If
        you set the stack size to 2048 before doing a "make test" the
        failure can be avoided.  If you're using the tcsh or csh shells,
        use "limit stacksize 2048" and for the bash shell (the default
        as of OSX 10.3), use "ulimit -s 2048".

        On naked Darwin you may want to add the configure option
        "--disable-toolbox-glue" to disable the glue code for the Carbon
        interface modules. The modules themselves are currently only built
        if you add the --enable-framework option, see below.

        On a clean OSX /usr/local does not exist. Do a
        "sudo mkdir -m 775 /usr/local"
        before you do a make install. It is probably not a good idea to
        do "sudo make install" which installs everything as superuser,
        as this may later cause problems when installing distutils-based
        additions.

        Some people have reported problems building Python after using "fink"
        to install additional unix software. Disabling fink (remove all 
        references to /sw from your .profile or .login) should solve this.

        You may want to try the configure option "--enable-framework"
        which installs Python as a framework. The location can be set
        as argument to the --enable-framework option (default
        /Library/Frameworks). A framework install is probably needed if you
        want to use any Aqua-based GUI toolkit (whether Tkinter, wxPython,
        Carbon, Cocoa or anything else).

        You may also want to try the configure option "--enable-universalsdk"
        which builds Python as a universal binary with support for the 
        i386 and PPC architetures. This requires Xcode 2.1 or later to build.

        See Mac/README for more information on framework and 
        universal builds.

Cygwin: With recent (relative to the time of writing, 2001-12-19)
        Cygwin installations, there are problems with the interaction
        of dynamic linking and fork().  This manifests itself in build
        failures during the execution of setup.py.

        There are two workarounds that both enable Python (albeit
        without threading support) to build and pass all tests on
        NT/2000 (and most likely XP as well, though reports of testing
        on XP would be appreciated).

        The workarounds:

        (a) the band-aid fix is to link the _socket module statically
        rather than dynamically (which is the default).

        To do this, run "./configure --with-threads=no" including any
        other options you need (--prefix, etc.).  Then in Modules/Setup
        uncomment the lines:

        #SSL=/usr/local/ssl
        #_socket socketmodule.c \
        #       -DUSE_SSL -I$(SSL)/include -I$(SSL)/include/openssl \
        #       -L$(SSL)/lib -lssl -lcrypto

        and remove "local/" from the SSL variable.  Finally, just run
        "make"!

        (b) The "proper" fix is to rebase the Cygwin DLLs to prevent
        base address conflicts.  Details on how to do this can be
        found in the following mail:

           http://sources.redhat.com/ml/cygwin/2001-12/msg00894.html

        It is hoped that a version of this solution will be
        incorporated into the Cygwin distribution fairly soon.

        Two additional problems:

        (1) Threading support should still be disabled due to a known
        bug in Cygwin pthreads that causes test_threadedtempfile to
        hang.

        (2) The _curses module does not build.  This is a known
        Cygwin ncurses problem that should be resolved the next time
        that this package is released.

        On older versions of Cygwin, test_poll may hang and test_strftime
        may fail.

        The situation on 9X/Me is not accurately known at present.
        Some time ago, there were reports that the following
        regression tests failed:

            test_pwd
            test_select (hang)
            test_socket

        Due to the test_select hang on 9X/Me, one should run the
        regression test using the following:

            make TESTOPTS='-l -x test_select' test

        News regarding these platforms with more recent Cygwin
        versions would be appreciated!

Windows: When executing Python scripts on the command line using file type
        associations (i.e. starting "script.py" instead of "python script.py"),
        redirects may not work unless you set a specific registry key.  See
        the Knowledge Base article <http://support.microsoft.com/kb/321788>.


Configuring the bsddb and dbm modules
-------------------------------------

Beginning with Python version 2.3, the PyBsddb package
<http://pybsddb.sf.net/> was adopted into Python as the bsddb package,
exposing a set of package-level functions which provide
backwards-compatible behavior.  Only versions 3.3 through 4.4 of
Sleepycat's libraries provide the necessary API, so older versions
aren't supported through this interface.  The old bsddb module has
been retained as bsddb185, though it is not built by default.  Users
wishing to use it will have to tweak Modules/Setup to build it.  The
dbm module will still be built against the Sleepycat libraries if
other preferred alternatives (ndbm, gdbm) are not found.

Building the sqlite3 module
---------------------------

To build the sqlite3 module, you'll need the sqlite3 or libsqlite3
packages installed, including the header files. Many modern operating
systems distribute the headers in a separate package to the library -
often it will be the same name as the main package, but with a -dev or
-devel suffix. 

The version of pysqlite2 that's including in Python needs sqlite3 3.0.8
or later. setup.py attempts to check that it can find a correct version.

Configuring threads
-------------------

As of Python 2.0, threads are enabled by default.  If you wish to
compile without threads, or if your thread support is broken, pass the
--with-threads=no switch to configure.  Unfortunately, on some
platforms, additional compiler and/or linker options are required for
threads to work properly.  Below is a table of those options,
collected by Bill Janssen.  We would love to automate this process
more, but the information below is not enough to write a patch for the
configure.in file, so manual intervention is required.  If you patch
the configure.in file and are confident that the patch works, please
send in the patch.  (Don't bother patching the configure script itself
-- it is regenerated each time the configure.in file changes.)

Compiler switches for threads
.............................

The definition of _REENTRANT should be configured automatically, if
that does not work on your system, or if _REENTRANT is defined
incorrectly, please report that as a bug.

    OS/Compiler/threads                     Switches for use with threads
    (POSIX is draft 10, DCE is draft 4)     compile & link

    SunOS 5.{1-5}/{gcc,SunPro cc}/solaris   -mt
    SunOS 5.5/{gcc,SunPro cc}/POSIX         (nothing)
    DEC OSF/1 3.x/cc/DCE                    -threads
            (butenhof@zko.dec.com)
    Digital UNIX 4.x/cc/DCE                 -threads
            (butenhof@zko.dec.com)
    Digital UNIX 4.x/cc/POSIX               -pthread
            (butenhof@zko.dec.com)
    AIX 4.1.4/cc_r/d7                       (nothing)
            (buhrt@iquest.net)
    AIX 4.1.4/cc_r4/DCE                     (nothing)
            (buhrt@iquest.net)
    IRIX 6.2/cc/POSIX                       (nothing)
            (robertl@cwi.nl)


Linker (ld) libraries and flags for threads
...........................................

    OS/threads                          Libraries/switches for use with threads

    SunOS 5.{1-5}/solaris               -lthread
    SunOS 5.5/POSIX                     -lpthread
    DEC OSF/1 3.x/DCE                   -lpthreads -lmach -lc_r -lc
            (butenhof@zko.dec.com)
    Digital UNIX 4.x/DCE                -lpthreads -lpthread -lmach -lexc -lc
            (butenhof@zko.dec.com)
    Digital UNIX 4.x/POSIX              -lpthread -lmach -lexc -lc
            (butenhof@zko.dec.com)
    AIX 4.1.4/{draft7,DCE}              (nothing)
            (buhrt@iquest.net)
    IRIX 6.2/POSIX                      -lpthread
            (jph@emilia.engr.sgi.com)


Building a shared libpython
---------------------------

Starting with Python 2.3, the majority of the interpreter can be built
into a shared library, which can then be used by the interpreter
executable, and by applications embedding Python. To enable this feature,
configure with --enable-shared.

If you enable this feature, the same object files will be used to create
a static library.  In particular, the static library will contain object
files using position-independent code (PIC) on platforms where PIC flags
are needed for the shared library.


Configuring additional built-in modules
---------------------------------------

Starting with Python 2.1, the setup.py script at the top of the source
distribution attempts to detect which modules can be built and
automatically compiles them.  Autodetection doesn't always work, so
you can still customize the configuration by editing the Modules/Setup
file; but this should be considered a last resort.  The rest of this
section only applies if you decide to edit the Modules/Setup file.
You also need this to enable static linking of certain modules (which
is needed to enable profiling on some systems).

This file is initially copied from Setup.dist by the configure script;
if it does not exist yet, create it by copying Modules/Setup.dist
yourself (configure will never overwrite it).  Never edit Setup.dist
-- always edit Setup or Setup.local (see below).  Read the comments in
the file for information on what kind of edits are allowed.  When you
have edited Setup in the Modules directory, the interpreter will
automatically be rebuilt the next time you run make (in the toplevel
directory).

Many useful modules can be built on any Unix system, but some optional
modules can't be reliably autodetected.  Often the quickest way to
determine whether a particular module works or not is to see if it
will build: enable it in Setup, then if you get compilation or link
errors, disable it -- you're either missing support or need to adjust
the compilation and linking parameters for that module.

On SGI IRIX, there are modules that interface to many SGI specific
system libraries, e.g. the GL library and the audio hardware.  These
modules will not be built by the setup.py script.

In addition to the file Setup, you can also edit the file Setup.local.
(the makesetup script processes both).  You may find it more
convenient to edit Setup.local and leave Setup alone.  Then, when
installing a new Python version, you can copy your old Setup.local
file.


Setting the optimization/debugging options
------------------------------------------

If you want or need to change the optimization/debugging options for
the C compiler, assign to the OPT variable on the toplevel make
command; e.g. "make OPT=-g" will build a debugging version of Python
on most platforms.  The default is OPT=-O; a value for OPT in the
environment when the configure script is run overrides this default
(likewise for CC; and the initial value for LIBS is used as the base
set of libraries to link with).

When compiling with GCC, the default value of OPT will also include
the -Wall and -Wstrict-prototypes options.

Additional debugging code to help debug memory management problems can
be enabled by using the --with-pydebug option to the configure script.

For flags that change binary compatibility, use the EXTRA_CFLAGS
variable.


Profiling
---------

If you want C profiling turned on, the easiest way is to run configure
with the CC environment variable to the necessary compiler
invocation.  For example, on Linux, this works for profiling using
gprof(1):

    CC="gcc -pg" ./configure

Note that on Linux, gprof apparently does not work for shared
libraries.  The Makefile/Setup mechanism can be used to compile and
link most extension modules statically.


Coverage checking
-----------------

For C coverage checking using gcov, run "make coverage".  This will
build a Python binary with profiling activated, and a ".gcno" and
".gcda" file for every source file compiled with that option.  With
the built binary, now run the code whose coverage you want to check.
Then, you can see coverage statistics for each individual source file
by running gcov, e.g.

    gcov -o Modules zlibmodule

This will create a "zlibmodule.c.gcov" file in the current directory
containing coverage info for that source file.

This works only for source files statically compiled into the
executable; use the Makefile/Setup mechanism to compile and link
extension modules you want to coverage-check statically.


Testing
-------

To test the interpreter, type "make test" in the top-level directory.
This runs the test set twice (once with no compiled files, once with
the compiled files left by the previous test run).  The test set
produces some output.  You can generally ignore the messages about
skipped tests due to optional features which can't be imported.
If a message is printed about a failed test or a traceback or core
dump is produced, something is wrong.  On some Linux systems (those
that are not yet using glibc 6), test_strftime fails due to a
non-standard implementation of strftime() in the C library. Please
ignore this, or upgrade to glibc version 6.

By default, tests are prevented from overusing resources like disk space and
memory.  To enable these tests, run "make testall".

IMPORTANT: If the tests fail and you decide to mail a bug report,
*don't* include the output of "make test".  It is useless.  Run the
failing test manually, as follows:

        ./python Lib/test/regrtest.py -v test_whatever

(substituting the top of the source tree for '.' if you built in a
different directory).  This runs the test in verbose mode.


Installing
----------

To install the Python binary, library modules, shared library modules
(see below), include files, configuration files, and the manual page,
just type

        make install

This will install all platform-independent files in subdirectories of
the directory given with the --prefix option to configure or to the
`prefix' Make variable (default /usr/local).  All binary and other
platform-specific files will be installed in subdirectories if the
directory given by --exec-prefix or the `exec_prefix' Make variable
(defaults to the --prefix directory) is given.

If DESTDIR is set, it will be taken as the root directory of the
installation, and files will be installed into $(DESTDIR)$(prefix),
$(DESTDIR)$(exec_prefix), etc.

All subdirectories created will have Python's version number in their
name, e.g. the library modules are installed in
"/usr/local/lib/python<version>/" by default, where <version> is the
<major>.<minor> release number (e.g. "2.1").  The Python binary is
installed as "python<version>" and a hard link named "python" is
created.  The only file not installed with a version number in its
name is the manual page, installed as "/usr/local/man/man1/python.1"
by default.

If you want to install multiple versions of Python see the section below
entitled "Installing multiple versions".

The only thing you may have to install manually is the Python mode for
Emacs found in Misc/python-mode.el.  (But then again, more recent
versions of Emacs may already have it.)  Follow the instructions that
came with Emacs for installation of site-specific files.

On Mac OS X, if you have configured Python with --enable-framework, you
should use "make frameworkinstall" to do the installation. Note that this
installs the Python executable in a place that is not normally on your
PATH, you may want to set up a symlink in /usr/local/bin.


Installing multiple versions
----------------------------

On Unix and Mac systems if you intend to install multiple versions of Python
using the same installation prefix (--prefix argument to the configure
script) you must take care that your primary python executable is not
overwritten by the installation of a different version.  All files and
directories installed using "make altinstall" contain the major and minor
version and can thus live side-by-side.  "make install" also creates
${prefix}/bin/python which refers to ${prefix}/bin/pythonX.Y.  If you intend
to install multiple versions using the same prefix you must decide which
version (if any) is your "primary" version.  Install that version using
"make install".  Install all other versions using "make altinstall".

For example, if you want to install Python 2.5, 2.6 and 3.0 with 2.6 being
the primary version, you would execute "make install" in your 2.6 build
directory and "make altinstall" in the others.


Configuration options and variables
-----------------------------------

Some special cases are handled by passing options to the configure
script.

WARNING: if you rerun the configure script with different options, you
must run "make clean" before rebuilding.  Exceptions to this rule:
after changing --prefix or --exec-prefix, all you need to do is remove
Modules/getpath.o.

--with(out)-gcc: The configure script uses gcc (the GNU C compiler) if
        it finds it.  If you don't want this, or if this compiler is
        installed but broken on your platform, pass the option
        --without-gcc.  You can also pass "CC=cc" (or whatever the
        name of the proper C compiler is) in the environment, but the
        advantage of using --without-gcc is that this option is
        remembered by the config.status script for its --recheck
        option.

--prefix, --exec-prefix: If you want to install the binaries and the
        Python library somewhere else than in /usr/local/{bin,lib},
        you can pass the option --prefix=DIRECTORY; the interpreter
        binary will be installed as DIRECTORY/bin/python and the
        library files as DIRECTORY/lib/python/*.  If you pass
        --exec-prefix=DIRECTORY (as well) this overrides the
        installation prefix for architecture-dependent files (like the
        interpreter binary).  Note that --prefix=DIRECTORY also
        affects the default module search path (sys.path), when
        Modules/config.c is compiled.  Passing make the option
        prefix=DIRECTORY (and/or exec_prefix=DIRECTORY) overrides the
        prefix set at configuration time; this may be more convenient
        than re-running the configure script if you change your mind
        about the install prefix.

--with-readline: This option is no longer supported.  GNU
        readline is automatically enabled by setup.py when present.

--with-threads: On most Unix systems, you can now use multiple
        threads, and support for this is enabled by default.  To
        disable this, pass --with-threads=no.  If the library required
        for threads lives in a peculiar place, you can use
        --with-thread=DIRECTORY.  IMPORTANT: run "make clean" after
        changing (either enabling or disabling) this option, or you
        will get link errors!  Note: for DEC Unix use
        --with-dec-threads instead.

--with-sgi-dl: On SGI IRIX 4, dynamic loading of extension modules is
        supported by the "dl" library by Jack Jansen, which is
        ftp'able from ftp://ftp.cwi.nl/pub/dynload/dl-1.6.tar.Z.
        This is enabled (after you've ftp'ed and compiled the dl
        library) by passing --with-sgi-dl=DIRECTORY where DIRECTORY
        is the absolute pathname of the dl library.  (Don't bother on
        IRIX 5, it already has dynamic linking using SunOS style
        shared libraries.)  THIS OPTION IS UNSUPPORTED.

--with-dl-dld: Dynamic loading of modules is rumored to be supported
        on some other systems: VAX (Ultrix), Sun3 (SunOS 3.4), Sequent
        Symmetry (Dynix), and Atari ST.  This is done using a
        combination of the GNU dynamic loading package
        (ftp://ftp.cwi.nl/pub/dynload/dl-dld-1.1.tar.Z) and an
        emulation of the SGI dl library mentioned above (the emulation
        can be found at
        ftp://ftp.cwi.nl/pub/dynload/dld-3.2.3.tar.Z).  To
        enable this, ftp and compile both libraries, then call
        configure, passing it the option
        --with-dl-dld=DL_DIRECTORY,DLD_DIRECTORY where DL_DIRECTORY is
        the absolute pathname of the dl emulation library and
        DLD_DIRECTORY is the absolute pathname of the GNU dld library.
        (Don't bother on SunOS 4 or 5, they already have dynamic
        linking using shared libraries.)  THIS OPTION IS UNSUPPORTED.

--with-libm, --with-libc: It is possible to specify alternative
        versions for the Math library (default -lm) and the C library
        (default the empty string) using the options
        --with-libm=STRING and --with-libc=STRING, respectively.  For
        example, if your system requires that you pass -lc_s to the C
        compiler to use the shared C library, you can pass
        --with-libc=-lc_s. These libraries are passed after all other
        libraries, the C library last.

--with-libs='libs': Add 'libs' to the LIBS that the python interpreter
        is linked against.

--with-cxx-main=<compiler>: If you plan to use C++ extension modules,
        then -- on some platforms -- you need to compile python's main()
        function with the C++ compiler. With this option, make will use
        <compiler> to compile main() *and* to link the python executable.
        It is likely that the resulting executable depends on the C++
        runtime library of <compiler>. (The default is --without-cxx-main.)

        There are platforms that do not require you to build Python
        with a C++ compiler in order to use C++ extension modules.
        E.g., x86 Linux with ELF shared binaries and GCC 3.x, 4.x is such
        a platform. We recommend that you configure Python
        --without-cxx-main on those platforms because a mismatch
        between the C++ compiler version used to build Python and to
        build a C++ extension module is likely to cause a crash at
        runtime.

        The Python installation also stores the variable CXX that
        determines, e.g., the C++ compiler distutils calls by default
        to build C++ extensions. If you set CXX on the configure command
        line to any string of non-zero length, then configure won't
        change CXX. If you do not preset CXX but pass
        --with-cxx-main=<compiler>, then configure sets CXX=<compiler>.
        In all other cases, configure looks for a C++ compiler by
        some common names (c++, g++, gcc, CC, cxx, cc++, cl) and sets
        CXX to the first compiler it finds. If it does not find any
        C++ compiler, then it sets CXX="".

        Similarly, if you want to change the command used to link the
        python executable, then set LINKCC on the configure command line.


--with-pydebug:  Enable additional debugging code to help track down
        memory management problems.  This allows printing a list of all
        live objects when the interpreter terminates.

--with(out)-universal-newlines: enable reading of text files with
        foreign newline convention (default: enabled). In other words,
        any of \r, \n or \r\n is acceptable as end-of-line character.
        If enabled import and execfile will automatically accept any newline
        in files. Python code can open a file with open(file, 'U') to
        read it in universal newline mode. THIS OPTION IS UNSUPPORTED.

--with-tsc: Profile using the Pentium timestamping counter (TSC).

--with-system-ffi:  Build the _ctypes extension module using an ffi
        library installed on the system.

--with-dbmliborder=db1:db2:...:  Specify the order that backends for the
	dbm extension are checked. Valid value is a colon separated string
	with the backend names `ndbm', `gdbm' and `bdb'.

Building for multiple architectures (using the VPATH feature)
-------------------------------------------------------------

If your file system is shared between multiple architectures, it
usually is not necessary to make copies of the sources for each
architecture you want to support.  If the make program supports the
VPATH feature, you can create an empty build directory for each
architecture, and in each directory run the configure script (on the
appropriate machine with the appropriate options).  This creates the
necessary subdirectories and the Makefiles therein.  The Makefiles
contain a line VPATH=... which points to a directory containing the
actual sources.  (On SGI systems, use "smake -J1" instead of "make" if
you use VPATH -- don't try gnumake.)

For example, the following is all you need to build a minimal Python
in /usr/tmp/python (assuming ~guido/src/python is the toplevel
directory and you want to build in /usr/tmp/python):

        $ mkdir /usr/tmp/python
        $ cd /usr/tmp/python
        $ ~guido/src/python/configure
        [...]
        $ make
        [...]
        $

Note that configure copies the original Setup file to the build
directory if it finds no Setup file there.  This means that you can
edit the Setup file for each architecture independently.  For this
reason, subsequent changes to the original Setup file are not tracked
automatically, as they might overwrite local changes.  To force a copy
of a changed original Setup file, delete the target Setup file.  (The
makesetup script supports multiple input files, so if you want to be
fancy you can change the rules to create an empty Setup.local if it
doesn't exist and run it with arguments $(srcdir)/Setup Setup.local;
however this assumes that you only need to add modules.)

Also note that you can't use a workspace for VPATH and non VPATH builds. The
object files left behind by one version confuses the other.


Building on non-UNIX systems
----------------------------

For Windows (2000/NT/ME/98/95), assuming you have MS VC++ 7.1, the
project files are in PCbuild, the workspace is pcbuild.dsw.  See
PCbuild\readme.txt for detailed instructions.

For other non-Unix Windows compilers, in particular MS VC++ 6.0 and
for OS/2, enter the directory "PC" and read the file "readme.txt".

For the Mac, a separate source distribution will be made available,
for use with the CodeWarrior compiler.  If you are interested in Mac
development, join the PythonMac Special Interest Group
(http://www.python.org/sigs/pythonmac-sig/, or send email to
pythonmac-sig-request@python.org).

Of course, there are also binary distributions available for these
platforms -- see http://www.python.org/.

To port Python to a new non-UNIX system, you will have to fake the
effect of running the configure script manually (for Mac and PC, this
has already been done for you).  A good start is to copy the file
pyconfig.h.in to pyconfig.h and edit the latter to reflect the actual
configuration of your system.  Most symbols must simply be defined as
1 only if the corresponding feature is present and can be left alone
otherwise; however the *_t type symbols must be defined as some
variant of int if they need to be defined at all.

For all platforms, it's important that the build arrange to define the
preprocessor symbol NDEBUG on the compiler command line in a release
build of Python (else assert() calls remain in the code, hurting
release-build performance).  The Unix, Windows and Mac builds already
do this.


Miscellaneous issues
====================

Emacs mode
----------

There's an excellent Emacs editing mode for Python code; see the file
Misc/python-mode.el.  Originally written by the famous Tim Peters, it is now
maintained by the equally famous Barry Warsaw.  The latest version, along with
various other contributed Python-related Emacs goodies, is online at
http://launchpad.net/python-mode/.


Tkinter
-------

The setup.py script automatically configures this when it detects a
usable Tcl/Tk installation.  This requires Tcl/Tk version 8.0 or
higher.

For more Tkinter information, see the Tkinter Resource page:
http://www.python.org/topics/tkinter/

There are demos in the Demo/tkinter directory.

Note that there's a Python module called "Tkinter" (capital T) which
lives in Lib/lib-tk/Tkinter.py, and a C module called "_tkinter"
(lower case t and leading underscore) which lives in
Modules/_tkinter.c.  Demos and normal Tk applications import only the
Python Tkinter module -- only the latter imports the C _tkinter
module.  In order to find the C _tkinter module, it must be compiled
and linked into the Python interpreter -- the setup.py script does
this.  In order to find the Python Tkinter module, sys.path must be
set correctly -- normal installation takes care of this.


Distribution structure
----------------------

Most subdirectories have their own README files.  Most files have
comments.

Demo/           Demonstration scripts, modules and programs
Doc/            Documentation sources (reStructuredText)
Grammar/        Input for the parser generator
Include/        Public header files
LICENSE         Licensing information
Lib/            Python library modules
Mac/            Macintosh specific resources
Makefile.pre.in Source from which config.status creates the Makefile.pre
Misc/           Miscellaneous useful files
Modules/        Implementation of most built-in modules
Objects/        Implementation of most built-in object types
PC/             Files specific to PC ports (DOS, Windows, OS/2)
PCbuild/        Build directory for Microsoft Visual C++
Parser/         The parser and tokenizer and their input handling
Python/         The byte-compiler and interpreter
README          The file you're reading now
RISCOS/         Files specific to RISC OS port
Tools/          Some useful programs written in Python
pyconfig.h.in   Source from which pyconfig.h is created (GNU autoheader output)
configure       Configuration shell script (GNU autoconf output)
configure.in    Configuration specification (input for GNU autoconf)
install-sh      Shell script used to install files
setup.py        Python script used to build extension modules

The following files will (may) be created in the toplevel directory by
the configuration and build processes:

Makefile        Build rules
Makefile.pre    Build rules before running Modules/makesetup
buildno         Keeps track of the build number
config.cache    Cache of configuration variables
pyconfig.h      Configuration header
config.log      Log from last configure run
config.status   Status from last run of the configure script
getbuildinfo.o  Object file from Modules/getbuildinfo.c
libpython<version>.a    The library archive
python          The executable interpreter
reflog.txt      Output from running the regression suite with the -R flag 
tags, TAGS      Tags files for vi and Emacs


That's all, folks!
------------------


--Guido van Rossum (home page: http://www.python.org/~guido/)

audiopy - a program to control the Solaris audio device.

Contact: Barry Warsaw
Email:   bwarsaw@python.org
Version: 1.1

Introduction

    Audiopy is a program to control the Solaris audio device, allowing
    you to choose both the input and output devices, and to set the
    output volume.  It can be run either as a standalone command-line
    script, or as a Tkinter based GUI application.

    Note that your version of Python must have been built with the
    sunaudiodev module enabled.  It is not enabled by default however!
    You will need to edit your Modules/Setup file, uncomment the
    sunaudiodev module spec line and rebuild Python.

    Using audiopy, you can select one of three possible input devices:
    the microphone, the line-in jack, or the CD in.  These choices are
    mutually exclusive; you can only have one active input device at
    any one time (this is enforced by the underlying device).  Some
    input devices may not be supported on all Solaris machines.

    You can also choose to enable any of the three possible output
    devices: the headphone jack, the speakers, or the line-out jack.
    You can enable any combination of these three devices.

    You can also set the output gain (volume) level.

Running as a GUI

    Simply start audiopy with no arguments to start it as a Tkinter
    based GUI application.  It will pop up a window with two sections:
    the top portion contains three radio buttons indicating your
    selected input device; the middle portion contains three
    checkboxes indicating your selected output devices; the bottom
    portion contains a slider that changes the output gain.

    Note the underlined characters in the button labels.  These
    indicate keyboard accelerators so that pressing Alt+character you
    can select that device.  For example, Alt-s toggles the Speaker
    device.  The Alt accelerators are the same as those you'd use in
    as the short-form command line switches (see below).

    Alt-q is also an accelerator for selecting Quit from the File
    menu.

    Unsupported devices will appear dimmed out in the GUI.  When run
    as a GUI, audiopy monitors the audio device and automatically
    updates its display if the state of the device is changed by some
    other means.  With Python versions before 1.5.2 this is done by
    occasionally polling the device, but in Python 1.5.2 no polling is
    necessary (you don't really need to know this, but I thought I'd
    plug 1.5.2 :-).
    
Running as a Command Line Program

    You can run audiopy from the command line to select any
    combination of input or output device, by using the command line
    options.  Actually, any option forces audiopy to run as a command
    line program and not display its GUI.

    Options have the general form

        --device[={0,1}]
        -d[-{0,1}]

    meaning there is both a long-form and short-form of the switch,
    where `device' or `d' is one of the following:

        (input)
            microphone -- m
            linein     -- i
            cd         -- c

        (output)
            headphones -- p
            speaker    -- s
            lineout    -- o

    When no value is given, the switch just toggles the specified
    device.  With a value, 0 turns the device off and 1 turns the
    device on.  Any other value is an error.

    For example, to turn the speakers off, turn the headphones on, and 
    toggle the cd input device, run audiopy from the command line like 
    so:

    % ./audiopy -s=0 -p=1 -c

    Audiopy understands these other command line options:

    --gain volume
    -g volume
        Sets the output volume to the specified gain level.  This must 
        be an integer between MIN_GAIN and MAX_GAIN (usually [0..255], 
        but use the -h option to find the exact values).

    --version
    -v
        Print the version number and exit

    --help
    -h
        Print a help message and exit
        


Local Variables:
indent-tabs-mode: nil
End:

BGEN -- Automatic Generation of Extension Modules
=================================================

This directory contains BGEN -- a package that helps in generating
complete source code for Python extension module.  For examples of its
use, see the Mac Python source distribution (available separately
from the Python ftp archives).  Note that BGEN is not Mac specific!

WARNING: bgen has been removed in 3.0.

Comments on building tcl/tk for AMD64 with the MS SDK compiler
==============================================================

I did have to build tcl/tk manually.

First, I had to build the nmakehlp.exe helper utility manually by executing
   cl nmakehlp.c /link bufferoverflowU.lib
in both the tcl8.4.12\win and tk8.4.12\win directories.

Second, the AMD64 compiler refuses to compile the file
tcl8.4.12\generic\tclExecute.c because it insists on using intrinsics
for the 'ceil' and 'floor' functions:

  ..\generic\tclExecute.c(394) : error C2099: initializer is not a constant
  ..\generic\tclExecute.c(398) : error C2099: initializer is not a constant

I did comment out these lines; an alternative would have been to use
the /Oi- compiler flag to disable the intrinsic functions.
The commands then used were these:

   svn export http://svn.python.org/projects/external/tcl8.4.12
   cd tcl8.4.12\win
   REM
   echo patch the tcl8.4.12\generic\tclExecute.c file
   pause 
   REM
   cl nmakehlp.c /link bufferoverflowU.lib
   nmake -f makefile.vc MACHINE=AMD64
   nmake -f makefile.vc INSTALLDIR=..\..\tcltk install
   cd ..\..
   svn export http://svn.python.org/projects/external/tk8.4.12
   cd tk8.4.12\win
   cl nmakehlp.c /link bufferoverflowU.lib
   nmake -f makefile.vc TCLDIR=..\..\tcl8.4.12 MACHINE=AMD64
   nmake -f makefile.vc TCLDIR=..\..\tcl8.4.12 INSTALLDIR=..\..\tcltk install
   cd ..\..

This directory contains support tools for the Python compiler package,
which is now part of the standard library.

compile.py	Demo that compiles a Python module into a .pyc file
		using the pure-Python compiler code.

demo.py		Prints the names of all the methods defined in a module,
		as a demonstration of walking through the abstract syntax
		tree produced by the parser.

dumppyc.py	Dumps the contents of a .pyc file, printing 
		the attributes of the code object followed by a 
		code disassembly.

regrtest.py	Runs the Python test suite using bytecode generated 
		by the pure-Python compiler code instead of the
		builtin compiler.


FAQ Wizard
----------

Author: Guido van Rossum <guido@python.org>
Version: 1.0
Date:  6 April 1998


This is a CGI program that maintains a user-editable FAQ.  It uses RCS
to keep track of changes to individual FAQ entries.  It is fully
configurable; everything you might want to change when using this
program to maintain some other FAQ than the Python FAQ is contained in
the configuration module, faqconf.py.

Note that the bulk of the code is not an executable script; it's an
importable module.  The actual script in cgi-bin is minimal.

Files:

faqw.py		executable script to be edited and installed in cgi-bin
faqwiz.py	main module, lives in same directory as FAQ entry files
faqconf.py	main configuration module
faqcust.py	additional local customization module (optional)
move-faqwiz.sh  Script to move faqwiz entries.


What's New?
-----------

Version 1.0 corrects some minor bugs and uses tab-agnostic
indentation; it is otherwise unchanged from version 0.9.0.

Version 0.9.0 uses the re module (Perl style regular expressions) for
all its regular expression needs, instead of the regex and regsub
modules (Emacs style).  This affects the syntax for regular
expressions entered by the user as search strings (with "regular
expression" checked), hence the version number jump.


Setup Information
-----------------

This assumes you are familiar with Python, with your http server, and
with running CGI scripts under your http server.  You need Python 1.5
or better.

Select a place where the Python modules that constitute the FAQ wizard
will live (the directory where you unpacked it is an obvious choice).
This will be called the SRCDIR.  This directory should not be writable
by other users of your system (since they would be able to execute
arbitrary code by invoking the FAQ wizard's CGI script).

Create a dedicated working directory, preferably one that's not
directly reachable from your http server.  This will be called the
FAQDIR.  Create a subdirectory named RCS.  Make both the working
directory and the RCS subdirectory wrld-writable.  (This is essential,
since the FAQ wizard runs as use nobody, and needs to create
additional files here!)

Edit faqconf.py to reflect your setup.  You only need to edit the top
part, up till the line of all dashes.  The comments should guide you
in your edits.  (Actually, you can also choose to add your changes to
faqcust.py and leave faqconf.py alone.  This is essential if you are
maintaining multiple FAQs; see below.)

Don't forget to edit the SECTION_TITLES variables to reflect the set
of section titles for your FAQ!

Next, edit faqw.py to reflect the pathname of your Python interpreter
and the values for SRCDIR and FAQDIR that you just chose.  Then
install faqw.py in your cgi-bin directory.  Make sure that it is
world-executable.  You should now be able to connect to the FAQ wizard
by entering the following URL in your web client (subsituting the
appropriate host and port for "your.web.server", and perhaps
specifying a different directory for "cgi-bin" if local conventions so
dictate):

	http://your.web.server/cgi-bin/faqw.py

If you are unable to get this working, check your server's error_log
file.  The documentation for Python's cgi module in the Python Library
Reference Manual gives plentyu additional information about installing
and debugging CGI scripts, including setup debugging.  This
documentation is repeated in the doc string in the cgi module; try
``import cgi; print cgi.__doc__''.

Assuming this works, you should now be able to add the first entry to
your FAQ using the FAQ wizard interface.  This creates a file
faq01.001.htp in your working directory and an RCS revision history
file faq01.001.htp,v in the RCS subdirectory.  You can now exercise
the other FAQ wizard features (search, index, whole FAQ, what's new,
roulette, and so on).


Maintaining Multiple FAQs
-------------------------

If you have multiple FAQs, you need a separate FAQDIR per FAQ, and a
different customization file per FAQ.  The easiest thing to do would
be to have the faqcust.py for each FAQ live in the FAQDIR for that
FAQ, but that creates some security concerns, since the FAQDIR must be
world writable: *if* someone who breaks into your system (or a
legitimate user) manages to edit the faqcust.py file they can get
arbitrary code to execute through the FAQ wizard.  Therefore, you will
need a more complex setup.

The best way is probably to have a directory that is only writable by
you for each FAQ, where you place the copy of faqcust.py for that FAQ,
and have a world-writable subdirectory DATA for the data.  You then
set FAQDIR to point to the DATA directory and change the faqw.py
bootstrap script to add FAQDIR/.. to sys.path (in front of SRCDIR, so
the dummy faqcust.py from SRCDIR is ignored).

--Guido van Rossum (home page: http://www.python.org/~guido/)

framer is a tool to generate boilerplate code for C extension types.

The boilerplate is generated from a specification object written in
Python.  The specification uses the class statement to describe the
extension module and any extension types it contains.  From the
specification, framer can generate all the boilerplate C code,
including function definitions, argument handling code, and type
objects.

THE FREEZE SCRIPT
=================

(Directions for Windows are at the end of this file.)


What is Freeze?
---------------

Freeze make it possible to ship arbitrary Python programs to people
who don't have Python.  The shipped file (called a "frozen" version of
your Python program) is an executable, so this only works if your
platform is compatible with that on the receiving end (this is usually
a matter of having the same major operating system revision and CPU
type).

The shipped file contains a Python interpreter and large portions of
the Python run-time.  Some measures have been taken to avoid linking
unneeded modules, but the resulting binary is usually not small.

The Python source code of your program (and of the library modules
written in Python that it uses) is not included in the binary --
instead, the compiled byte-code (the instruction stream used
internally by the interpreter) is incorporated.  This gives some
protection of your Python source code, though not much -- a
disassembler for Python byte-code is available in the standard Python
library.  At least someone running "strings" on your binary won't see
the source.


How does Freeze know which modules to include?
----------------------------------------------

Previous versions of Freeze used a pretty simple-minded algorithm to
find the modules that your program uses, essentially searching for
lines starting with the word "import".  It was pretty easy to trick it
into making mistakes, either missing valid import statements, or
mistaking string literals (e.g. doc strings) for import statements.

This has been remedied: Freeze now uses the regular Python parser to
parse the program (and all its modules) and scans the generated byte
code for IMPORT instructions.  It may still be confused -- it will not
know about calls to the __import__ built-in function, or about import
statements constructed on the fly and executed using the 'exec'
statement, and it will consider import statements even when they are
unreachable (e.g. "if 0: import foobar").

This new version of Freeze also knows about Python's new package
import mechanism, and uses exactly the same rules to find imported
modules and packages.  One exception: if you write 'from package
import *', Python will look into the __all__ variable of the package
to determine which modules are to be imported, while Freeze will do a
directory listing.

One tricky issue: Freeze assumes that the Python interpreter and
environment you're using to run Freeze is the same one that would be
used to run your program, which should also be the same whose sources
and installed files you will learn about in the next section.  In
particular, your PYTHONPATH setting should be the same as for running
your program locally.  (Tip: if the program doesn't run when you type
"python hello.py" there's little chance of getting the frozen version
to run.)


How do I use Freeze?
--------------------

Normally, you should be able to use it as follows:

	python freeze.py hello.py

where hello.py is your program and freeze.py is the main file of
Freeze (in actuality, you'll probably specify an absolute pathname
such as /usr/joe/python/Tools/freeze/freeze.py).


What do I do next?
------------------

Freeze creates a number of files: frozen.c, config.c and Makefile,
plus one file for each Python module that gets included named
M_<module>.c.  To produce the frozen version of your program, you can
simply type "make".  This should produce a binary file.  If the
filename argument to Freeze was "hello.py", the binary will be called
"hello".

Note: you can use the -o option to freeze to specify an alternative
directory where these files are created. This makes it easier to
clean up after you've shipped the frozen binary.  You should invoke
"make" in the given directory.


Freezing Tkinter programs
-------------------------

Unfortunately, it is currently not possible to freeze programs that
use Tkinter without a Tcl/Tk installation. The best way to ship a
frozen Tkinter program is to decide in advance where you are going
to place the Tcl and Tk library files in the distributed setup, and
then declare these directories in your frozen Python program using
the TCL_LIBRARY, TK_LIBRARY and TIX_LIBRARY environment variables.

For example, assume you will ship your frozen program in the directory 
<root>/bin/windows-x86 and will place your Tcl library files 
in <root>/lib/tcl8.2 and your Tk library files in <root>/lib/tk8.2. Then
placing the following lines in your frozen Python script before importing
Tkinter or Tix would set the environment correctly for Tcl/Tk/Tix:

import os
import os.path
RootDir = os.path.dirname(os.path.dirname(os.getcwd()))

import sys
if sys.platform == "win32":
   sys.path = ['', '..\\..\\lib\\python-2.0']
   os.environ['TCL_LIBRARY'] = RootDir + '\\lib\\tcl8.2'
   os.environ['TK_LIBRARY'] = RootDir + '\\lib\\tk8.2'
   os.environ['TIX_LIBRARY'] = RootDir + '\\lib\\tix8.1'
elif sys.platform == "linux2":
   sys.path = ['', '../../lib/python-2.0']
   os.environ['TCL_LIBRARY'] = RootDir + '/lib/tcl8.2'
   os.environ['TK_LIBRARY'] = RootDir + '/lib/tk8.2'
   os.environ['TIX_LIBRARY'] = RootDir + '/lib/tix8.1'
elif sys.platform == "solaris":
   sys.path = ['', '../../lib/python-2.0']
   os.environ['TCL_LIBRARY'] = RootDir + '/lib/tcl8.2'
   os.environ['TK_LIBRARY'] = RootDir + '/lib/tk8.2'
   os.environ['TIX_LIBRARY'] = RootDir + '/lib/tix8.1'

This also adds <root>/lib/python-2.0 to your Python path
for any Python files such as _tkinter.pyd you may need.

Note that the dynamic libraries (such as tcl82.dll tk82.dll python20.dll
under Windows, or libtcl8.2.so and libtcl8.2.so under Unix) are required
at program load time, and are searched by the operating system loader
before Python can be started. Under Windows, the environment
variable PATH is consulted, and under Unix, it may be the
environment variable LD_LIBRARY_PATH and/or the system
shared library cache (ld.so). An additional preferred directory for
finding the dynamic libraries is built into the .dll or .so files at
compile time - see the LIB_RUNTIME_DIR variable in the Tcl makefile. 
The OS must find the dynamic libraries or your frozen program won't start. 
Usually I make sure that the .so or .dll files are in the same directory
as the executable, but this may not be foolproof.

A workaround to installing your Tcl library files with your frozen
executable would be possible, in which the Tcl/Tk library files are
incorporated in a frozen Python module as string literals and written
to a temporary location when the program runs; this is currently left
as an exercise for the reader.  An easier approach is to freeze the
Tcl/Tk/Tix code into the dynamic libraries using the Tcl ET code,
or the Tix Stand-Alone-Module code. Of course, you can also simply 
require that Tcl/Tk is required on the target installation, but be 
careful that the version corresponds.

There are some caveats using frozen Tkinter applications:
	Under Windows if you use the -s windows option, writing
to stdout or stderr is an error.
	The Tcl [info nameofexecutable] will be set to where the
program was frozen, not where it is run from.
	The global variables argc and argv do not exist.


A warning about shared library modules
--------------------------------------

When your Python installation uses shared library modules such as 
_tkinter.pyd, these will not be incorporated in the frozen program.
 Again, the frozen program will work when you test it, but it won't
 work when you ship it to a site without a Python installation.

Freeze prints a warning when this is the case at the end of the
freezing process:

	Warning: unknown modules remain: ...

When this occurs, the best thing to do is usually to rebuild Python
using static linking only. Or use the approach described in the previous
section to declare a library path using sys.path, and place the modules
such as _tkinter.pyd there.


Troubleshooting
---------------

If you have trouble using Freeze for a large program, it's probably
best to start playing with a really simple program first (like the file
hello.py).  If you can't get that to work there's something
fundamentally wrong -- perhaps you haven't installed Python.  To do a
proper install, you should do "make install" in the Python root
directory.


Usage under Windows 95 or NT
----------------------------

Under Windows 95 or NT, you *must* use the -p option and point it to
the top of the Python source tree.

WARNING: the resulting executable is not self-contained; it requires
the Python DLL, currently PYTHON20.DLL (it does not require the
standard library of .py files though).  It may also require one or
more extension modules loaded from .DLL or .PYD files; the module
names are printed in the warning message about remaining unknown
modules.

The driver script generates a Makefile that works with the Microsoft
command line C compiler (CL).  To compile, run "nmake"; this will
build a target "hello.exe" if the source was "hello.py".  Only the
files frozenmain.c and frozen.c are used; no config.c is generated or
used, since the standard DLL is used.

In order for this to work, you must have built Python using the VC++
(Developer Studio) 5.0 compiler.  The provided project builds
python20.lib in the subdirectory pcbuild\Release of thje Python source
tree, and this is where the generated Makefile expects it to be.  If
this is not the case, you can edit the Makefile or (probably better)
winmakemakefile.py (e.g., if you are using the 4.2 compiler, the
python20.lib file is generated in the subdirectory vc40 of the Python
source tree).

It is possible to create frozen programs that don't have a console
window, by specifying the option '-s windows'. See the Usage below.

Usage
-----

Here is a list of all of the options (taken from freeze.__doc__):

usage: freeze [options...] script [module]...

Options:
-p prefix:    This is the prefix used when you ran ``make install''
              in the Python build directory.
              (If you never ran this, freeze won't work.)
              The default is whatever sys.prefix evaluates to.
              It can also be the top directory of the Python source
              tree; then -P must point to the build tree.

-P exec_prefix: Like -p but this is the 'exec_prefix', used to
                install objects etc.  The default is whatever sys.exec_prefix
                evaluates to, or the -p argument if given.
                If -p points to the Python source tree, -P must point
                to the build tree, if different.

-e extension: A directory containing additional .o files that
              may be used to resolve modules.  This directory
              should also have a Setup file describing the .o files.
              On Windows, the name of a .INI file describing one
              or more extensions is passed.
              More than one -e option may be given.

-o dir:       Directory where the output files are created; default '.'.

-m:           Additional arguments are module names instead of filenames.

-a package=dir: Additional directories to be added to the package's
                __path__.  Used to simulate directories added by the
                package at runtime (eg, by OpenGL and win32com).
                More than one -a option may be given for each package.

-l file:      Pass the file to the linker (windows only)

-d:           Debugging mode for the module finder.

-q:           Make the module finder totally quiet.

-h:           Print this help message.

-x module     Exclude the specified module.

-i filename:  Include a file with additional command line options.  Used
              to prevent command lines growing beyond the capabilities of
              the shell/OS.  All arguments specified in filename
              are read and the -i option replaced with the parsed
              params (note - quoting args in this file is NOT supported)

-s subsystem: Specify the subsystem (For Windows only.); 
              'console' (default), 'windows', 'service' or 'com_dll'
              
-w:           Toggle Windows (NT or 95) behavior.
              (For debugging only -- on a win32 platform, win32 behavior
              is automatic.)

Arguments:

script:       The Python script to be executed by the resulting binary.

module ...:   Additional Python modules (referenced by pathname)
              that will be included in the resulting binary.  These
              may be .py or .pyc files.  If -m is specified, these are
              module names that are search in the path instead.



--Guido van Rossum (home page: http://www.python.org/~guido/)

Packaging Python as a Microsoft Installer Package (MSI)
=======================================================

Using this library, Python can be packaged as a MS-Windows
MSI file. To generate an installer package, you need
a build tree. By default, the build tree root directory
is assumed to be in "../..". This location can be changed
by adding a file config.py; see the beginning of msi.py
for additional customization options.

The packaging process assumes that binaries have been 
generated according to the instructions in PCBuild/README.txt,
and that you have either Visual Studio or the Platform SDK
installed. In addition, you need the Python COM extensions,
either from PythonWin, or from ActivePython.

To invoke the script, open a cmd.exe window which has 
cabarc.exe in its PATH (e.g. "Visual Studio .NET 2003
Command Prompt"). Then invoke

<path-to-python.exe> msi.py

If everything succeeds, pythonX.Y.Z.msi is generated
in the current directory.


________________________________________________________________________

PYBENCH - A Python Benchmark Suite
________________________________________________________________________

     Extendable suite of of low-level benchmarks for measuring
          the performance of the Python implementation 
                 (interpreter, compiler or VM).

pybench is a collection of tests that provides a standardized way to
measure the performance of Python implementations. It takes a very
close look at different aspects of Python programs and let's you
decide which factors are more important to you than others, rather
than wrapping everything up in one number, like the other performance
tests do (e.g. pystone which is included in the Python Standard
Library).

pybench has been used in the past by several Python developers to
track down performance bottlenecks or to demonstrate the impact of
optimizations and new features in Python.

The command line interface for pybench is the file pybench.py. Run
this script with option '--help' to get a listing of the possible
options. Without options, pybench will simply execute the benchmark
and then print out a report to stdout.


Micro-Manual
------------

Run 'pybench.py -h' to see the help screen.  Run 'pybench.py' to run
the benchmark suite using default settings and 'pybench.py -f <file>'
to have it store the results in a file too.

It is usually a good idea to run pybench.py multiple times to see
whether the environment, timers and benchmark run-times are suitable
for doing benchmark tests. 

You can use the comparison feature of pybench.py ('pybench.py -c
<file>') to check how well the system behaves in comparison to a
reference run. 

If the differences are well below 10% for each test, then you have a
system that is good for doing benchmark testings.  Of you get random
differences of more than 10% or significant differences between the
values for minimum and average time, then you likely have some
background processes running which cause the readings to become
inconsistent. Examples include: web-browsers, email clients, RSS
readers, music players, backup programs, etc.

If you are only interested in a few tests of the whole suite, you can
use the filtering option, e.g. 'pybench.py -t string' will only
run/show the tests that have 'string' in their name.

This is the current output of pybench.py --help:

"""
------------------------------------------------------------------------
PYBENCH - a benchmark test suite for Python interpreters/compilers.
------------------------------------------------------------------------

Synopsis:
 pybench.py [option] files...

Options and default settings:
  -n arg           number of rounds (10)
  -f arg           save benchmark to file arg ()
  -c arg           compare benchmark with the one in file arg ()
  -s arg           show benchmark in file arg, then exit ()
  -w arg           set warp factor to arg (10)
  -t arg           run only tests with names matching arg ()
  -C arg           set the number of calibration runs to arg (20)
  -d               hide noise in comparisons (0)
  -v               verbose output (not recommended) (0)
  --with-gc        enable garbage collection (0)
  --with-syscheck  use default sys check interval (0)
  --timer arg      use given timer (time.time)
  -h               show this help text
  --help           show this help text
  --debug          enable debugging
  --copyright      show copyright
  --examples       show examples of usage

Version:
 2.0

The normal operation is to run the suite and display the
results. Use -f to save them for later reuse or comparisons.

Available timers:

   time.time
   time.clock
   systimes.processtime

Examples:

python2.1 pybench.py -f p21.pybench
python2.5 pybench.py -f p25.pybench
python pybench.py -s p25.pybench -c p21.pybench
"""

License
-------

See LICENSE file.


Sample output
-------------

"""
-------------------------------------------------------------------------------
PYBENCH 2.0
-------------------------------------------------------------------------------
* using Python 2.4.2
* disabled garbage collection
* system check interval set to maximum: 2147483647
* using timer: time.time

Calibrating tests. Please wait...

Running 10 round(s) of the suite at warp factor 10:

* Round 1 done in 6.388 seconds.
* Round 2 done in 6.485 seconds.
* Round 3 done in 6.786 seconds.
...
* Round 10 done in 6.546 seconds.

-------------------------------------------------------------------------------
Benchmark: 2006-06-12 12:09:25
-------------------------------------------------------------------------------

    Rounds: 10
    Warp:   10
    Timer:  time.time

    Machine Details:
       Platform ID:  Linux-2.6.8-24.19-default-x86_64-with-SuSE-9.2-x86-64
       Processor:    x86_64

    Python:
       Executable:   /usr/local/bin/python
       Version:      2.4.2
       Compiler:     GCC 3.3.4 (pre 3.3.5 20040809)
       Bits:         64bit
       Build:        Oct  1 2005 15:24:35 (#1)
       Unicode:      UCS2


Test                             minimum  average  operation  overhead
-------------------------------------------------------------------------------
          BuiltinFunctionCalls:    126ms    145ms    0.28us    0.274ms
           BuiltinMethodLookup:    124ms    130ms    0.12us    0.316ms
                 CompareFloats:    109ms    110ms    0.09us    0.361ms
         CompareFloatsIntegers:    100ms    104ms    0.12us    0.271ms
               CompareIntegers:    137ms    138ms    0.08us    0.542ms
        CompareInternedStrings:    124ms    127ms    0.08us    1.367ms
                  CompareLongs:    100ms    104ms    0.10us    0.316ms
                CompareStrings:    111ms    115ms    0.12us    0.929ms
                CompareUnicode:    108ms    128ms    0.17us    0.693ms
                 ConcatStrings:    142ms    155ms    0.31us    0.562ms
                 ConcatUnicode:    119ms    127ms    0.42us    0.384ms
               CreateInstances:    123ms    128ms    1.14us    0.367ms
            CreateNewInstances:    121ms    126ms    1.49us    0.335ms
       CreateStringsWithConcat:    130ms    135ms    0.14us    0.916ms
       CreateUnicodeWithConcat:    130ms    135ms    0.34us    0.361ms
                  DictCreation:    108ms    109ms    0.27us    0.361ms
             DictWithFloatKeys:    149ms    153ms    0.17us    0.678ms
           DictWithIntegerKeys:    124ms    126ms    0.11us    0.915ms
            DictWithStringKeys:    114ms    117ms    0.10us    0.905ms
                      ForLoops:    110ms    111ms    4.46us    0.063ms
                    IfThenElse:    118ms    119ms    0.09us    0.685ms
                   ListSlicing:    116ms    120ms    8.59us    0.103ms
                NestedForLoops:    125ms    137ms    0.09us    0.019ms
          NormalClassAttribute:    124ms    136ms    0.11us    0.457ms
       NormalInstanceAttribute:    110ms    117ms    0.10us    0.454ms
           PythonFunctionCalls:    107ms    113ms    0.34us    0.271ms
             PythonMethodCalls:    140ms    149ms    0.66us    0.141ms
                     Recursion:    156ms    166ms    3.32us    0.452ms
                  SecondImport:    112ms    118ms    1.18us    0.180ms
           SecondPackageImport:    118ms    127ms    1.27us    0.180ms
         SecondSubmoduleImport:    140ms    151ms    1.51us    0.180ms
       SimpleComplexArithmetic:    128ms    139ms    0.16us    0.361ms
        SimpleDictManipulation:    134ms    136ms    0.11us    0.452ms
         SimpleFloatArithmetic:    110ms    113ms    0.09us    0.571ms
      SimpleIntFloatArithmetic:    106ms    111ms    0.08us    0.548ms
       SimpleIntegerArithmetic:    106ms    109ms    0.08us    0.544ms
        SimpleListManipulation:    103ms    113ms    0.10us    0.587ms
          SimpleLongArithmetic:    112ms    118ms    0.18us    0.271ms
                    SmallLists:    105ms    116ms    0.17us    0.366ms
                   SmallTuples:    108ms    128ms    0.24us    0.406ms
         SpecialClassAttribute:    119ms    136ms    0.11us    0.453ms
      SpecialInstanceAttribute:    143ms    155ms    0.13us    0.454ms
                StringMappings:    115ms    121ms    0.48us    0.405ms
              StringPredicates:    120ms    129ms    0.18us    2.064ms
                 StringSlicing:    111ms    127ms    0.23us    0.781ms
                     TryExcept:    125ms    126ms    0.06us    0.681ms
                TryRaiseExcept:    133ms    137ms    2.14us    0.361ms
                  TupleSlicing:    117ms    120ms    0.46us    0.066ms
               UnicodeMappings:    156ms    160ms    4.44us    0.429ms
             UnicodePredicates:    117ms    121ms    0.22us    2.487ms
             UnicodeProperties:    115ms    153ms    0.38us    2.070ms
                UnicodeSlicing:    126ms    129ms    0.26us    0.689ms
-------------------------------------------------------------------------------
Totals:                           6283ms   6673ms
"""
________________________________________________________________________

Writing New Tests
________________________________________________________________________

pybench tests are simple modules defining one or more pybench.Test
subclasses.

Writing a test essentially boils down to providing two methods:
.test() which runs .rounds number of .operations test operations each
and .calibrate() which does the same except that it doesn't actually
execute the operations.


Here's an example:
------------------

from pybench import Test

class IntegerCounting(Test):

    # Version number of the test as float (x.yy); this is important
    # for comparisons of benchmark runs - tests with unequal version
    # number will not get compared.
    version = 1.0
    
    # The number of abstract operations done in each round of the
    # test. An operation is the basic unit of what you want to
    # measure. The benchmark will output the amount of run-time per
    # operation. Note that in order to raise the measured timings
    # significantly above noise level, it is often required to repeat
    # sets of operations more than once per test round. The measured
    # overhead per test round should be less than 1 second.
    operations = 20

    # Number of rounds to execute per test run. This should be
    # adjusted to a figure that results in a test run-time of between
    # 1-2 seconds (at warp 1).
    rounds = 100000

    def test(self):

	""" Run the test.

	    The test needs to run self.rounds executing
	    self.operations number of operations each.

        """
        # Init the test
        a = 1

        # Run test rounds
	#
        # NOTE: Use xrange() for all test loops unless you want to face
	# a 20MB process !
	#
        for i in xrange(self.rounds):

            # Repeat the operations per round to raise the run-time
            # per operation significantly above the noise level of the
            # for-loop overhead. 

	    # Execute 20 operations (a += 1):
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1
            a += 1

    def calibrate(self):

	""" Calibrate the test.

	    This method should execute everything that is needed to
	    setup and run the test - except for the actual operations
	    that you intend to measure. pybench uses this method to
            measure the test implementation overhead.

        """
        # Init the test
        a = 1

        # Run test rounds (without actually doing any operation)
        for i in xrange(self.rounds):

	    # Skip the actual execution of the operations, since we
	    # only want to measure the test's administration overhead.
            pass

Registering a new test module
-----------------------------

To register a test module with pybench, the classes need to be
imported into the pybench.Setup module. pybench will then scan all the
symbols defined in that module for subclasses of pybench.Test and
automatically add them to the benchmark suite.


Breaking Comparability
----------------------

If a change is made to any individual test that means it is no
longer strictly comparable with previous runs, the '.version' class
variable should be updated. Therefafter, comparisons with previous
versions of the test will list as "n/a" to reflect the change.


Version History
---------------

  2.0: rewrote parts of pybench which resulted in more repeatable
       timings:
        - made timer a parameter
        - changed the platform default timer to use high-resolution
          timers rather than process timers (which have a much lower
          resolution)
        - added option to select timer
        - added process time timer (using systimes.py)
        - changed to use min() as timing estimator (average
          is still taken as well to provide an idea of the difference)
        - garbage collection is turned off per default
        - sys check interval is set to the highest possible value
        - calibration is now a separate step and done using
          a different strategy that allows measuring the test
          overhead more accurately
        - modified the tests to each give a run-time of between
          100-200ms using warp 10
        - changed default warp factor to 10 (from 20)
        - compared results with timeit.py and confirmed measurements
        - bumped all test versions to 2.0
        - updated platform.py to the latest version
        - changed the output format a bit to make it look
          nicer
        - refactored the APIs somewhat
  1.3+: Steve Holden added the NewInstances test and the filtering 
       option during the NeedForSpeed sprint; this also triggered a long 
       discussion on how to improve benchmark timing and finally
       resulted in the release of 2.0
  1.3: initial checkin into the Python SVN repository


Have fun,
--
Marc-Andre Lemburg
mal@lemburg.com

Pynche - The PYthonically Natural Color and Hue Editor

Contact: Barry A. Warsaw
Email:   bwarsaw@python.org
Version: 1.3

Introduction

    Pynche is a color editor based largely on a similar program that I
    originally wrote back in 1987 for the Sunview window system.  That
    editor was called ICE, the Interactive Color Editor.  I'd always
    wanted to port this program to X but didn't feel like hacking X
    and C code to do it.  Fast forward many years, to where Python +
    Tkinter provides such a nice programming environment, with enough
    power, that I finally buckled down and re-implemented it.  I
    changed the name because these days, too many other systems have
    the acronym `ICE'.

    Pynche should work with any variant of Python after 1.5.2
    (e.g. 2.0.1 and 2.1.1), using Tk 8.0.x.  It's been tested on
    Solaris 2.6, Windows NT 4, and various Linux distros.  You'll want
    to be sure to have at least Tk 8.0.3 for Windows.  Also, Pynche is
    very colormap intensive, so it doesn't work very well on 8-bit
    graphics cards; 24bit+ graphics cards are so cheap these days,
    I'll probably never "fix" that.

    Pynche must find a text database of colors names in order to
    provide `nearest' color matching.  Pynche is distributed with an
    rgb.txt file from the X11R6.4 distribution for this reason, along
    with other "Web related" database (see below).  You can use a
    different file with the -d option.  The file xlicense.txt contains
    the license only for rgb.txt and both files are in the X/
    subdirectory.

    Pynche is pronounced: Pin'-chee


Running Standalone

    On Unix, start it by running the `pynche' script.  On Windows, run
    pynche.pyw to inhibit the console window.  When run from the
    command line, the following options are recognized:

    --database file
    -d file
        Alternate location of the color database file.  Without this
        option, the first valid file found will be used (see below).

    --initfile file
    -i file
        Alternate location of the persistent initialization file.  See 
        the section on Persistency below.

    --ignore
    -X
        Ignore the persistent initialization file when starting up.
        Pynche will still write the current option settings to the
        persistent init file when it quits.

    --help
    -h
        Print the help message.

    initialcolor
        a Tk color name or #rrggbb color spec to be used as the
        initially selected color.  This overrides any color saved in
        the persistent init file.  Since `#' needs to be escaped in
        many shells, it is optional in the spec (e.g. #45dd1f is the
        same as 45dd1f).


Running as a Modal Dialog

    Pynche can be run as a modal dialog, inside another application,
    say as a general color chooser.  In fact, Grail 0.6 uses Pynche
    and a future version of IDLE may as well.  Pynche supports the API
    implemented by the Tkinter standard tkColorChooser module, with a
    few changes as described below.  By importing pyColorChooser from
    the Pynche package, you can run

        pyColorChooser.askcolor()

    which will popup Pynche as a modal dialog, and return the selected 
    color.

    There are some UI differences when running as a modal
    vs. standalone.  When running as a modal, there is no "Quit" menu
    item under the "File" menu.  Instead there are "Okay" and "Cancel"
    buttons.

    When "Okay" is hit, askcolor() returns the tuple

        ((r, g, b), "name")

    where r, g, and b are red, green, and blue color values
    respectively (in the range 0 to 255).  "name" will be a color name
    from the color database if there is an exact match, otherwise it
    will be an X11 color spec of the form "#rrggbb".  Note that this
    is different than tkColorChooser, which doesn't know anything
    about color names.

    askcolor() supports the following optional keyword arguments:

        color
            the color to set as the initial selected color

        master[*]
            the master window to use as the parent of the modal
            dialog.  Without this argument, pyColorChooser will create 
            its own Tkinter.Tk instance as the master.  This may not
            be what you want.

        databasefile
            similar to the --database option, the value must be a
            file name

        initfile[*]
            similar to the --initfile option, the value must be a
            file name

        ignore[*]
            similar to the --ignore flag, the value is a boolean

        wantspec
            When this is true, the "name" field in the return tuple
            will always be a color spec of the form "#rrggbb".  It
            will not return a color name even if there is a match;
            this is so pyColorChooser can exactly match the API of
            tkColorChooser.

        [*] these arguments must be specified the first time
        askcolor() is used and cannot be changed on subsequent calls.


The Colorstrip Window

    The top part of the main Pynche window contains the "variation
    strips".  Each strip contains a number of "color chips".  The
    strips always indicate the currently selected color by a highlight
    rectangle around the selected color chip, with an arrow pointing
    to the chip.  Each arrow has an associated number giving you the
    color value along the variation's axis.  Each variation strip
    shows you the colors that are reachable from the selected color by
    varying just one axis of the color solid.

    For example, when the selected color is (in Red/Green/Blue
    notation) 127/127/127, the Red Variations strip shows you every
    color in the range 0/127/127 to 255/127/127.  Similarly for the
    green and blue axes.  You can select any color by clicking on its
    chip.  This will update the highlight rectangle and the arrow, as
    well as other displays in Pynche.

    Click on "Update while dragging" if you want Pynche to update the
    selected color while you drag along any variation strip (this will
    be a bit slower).  Click on "Hexadecimal" to display the arrow
    numbers in hex.

    There are also two shortcut buttons in this window, which
    auto-select Black (0/0/0) and White (255/255/255).


The Proof Window

    In the lower left corner of the main window you see two larger
    color chips.  The Selected chip shows you a larger version of the
    color selected in the variation strips, along with its X11 color
    specification.  The Nearest chip shows you the closest color in
    the X11 database to the selected color, giving its X11 color
    specification, and below that, its X11 color name.  When the
    Selected chip color exactly matches the Nearest chip color, you
    will see the color name appear below the color specification for
    the Selected chip.
    
    Clicking on the Nearest color chip selects that color.  Color
    distance is calculated in the 3D space of the RGB color solid and
    if more than one color name is the same distance from the selected
    color, the first one found will be chosen.

    Note that there may be more than one X11 color name for the same
    RGB value.  In that case, the first one found in the text database
    is designated the "primary" name, and this is shown under the
    Nearest chip.  The other names are "aliases" and they are visible
    in the Color List Window (see below).

    Both the color specifications and color names are selectable for
    copying and pasting into another window.


The Type-in Window

    At the lower right of the main window are three entry fields.
    Here you can type numeric values for any of the three color axes.
    Legal values are between 0 and 255, and these fields do not allow
    you to enter illegal values.  You must hit Enter or Tab to select
    the new color.

    Click on "Update while typing" if you want Pynche to select the
    color on every keystroke (well, every one that produces a legal
    value!)  Click on "Hexadecimal" to display and enter color values
    in hex.


Other Views

    There are three secondary windows which are not displayed by
    default.  You can bring these up via the "View" menu on the main
    Pynche window.


The Text Window

    The "Text Window" allows you to see what effects various colors
    have on the standard Tk text widget elements.  In the upper part
    of the window is a plain Tk text widget and here you can edit the
    text, select a region of text, etc.  Below this is a button "Track
    color changes".  When this is turned on, any colors selected in
    the other windows will change the text widget element specified in
    the radio buttons below.  When this is turned off, text widget
    elements are not affected by color selection.

    You can choose which element gets changed by color selection by
    clicking on one of the radio buttons in the bottom part of this
    window.  Text foreground and background affect the text in the
    upper part of the window.  Selection foreground and background
    affect the colors of the primary selection which is what you see
    when you click the middle button (depending on window system) and
    drag it through some text.

    The Insertion is the insertion cursor in the text window, where
    new text will be inserted as you type.  The insertion cursor only
    has a background.


The Color List Window

    The "Color List" window shows every named color in the color name
    database (this window may take a while to come up).  In the upper
    part of the window you see a scrolling list of all the color names
    in the database, in alphabetical order.  Click on any color to
    select it.  In the bottom part of the window is displayed any
    aliases for the selected color (those color names that have the
    same RGB value, but were found later in the text database).  For
    example, find the color "Black" and you'll see that its aliases
    are "gray0" and "grey0".

    If the color has no aliases you'll see "<no aliases>" here.  If you
    just want to see if a color has an alias, and do not want to select a
    color when you click on it, turn off "Update on Click".

    Note that the color list is always updated when a color is selected
    from the main window.  There's no way to turn this feature off.  If
    the selected color has no matching color name you'll see
    "<no matching color>" in the Aliases window.


The Details Window

    The "Details" window gives you more control over color selection
    than just clicking on a color chip in the main window.  The row of
    buttons along the top apply the specified increment and decrement
    amounts to the selected color.  These delta amounts are applied to
    the variation strips specified by the check boxes labeled "Move
    Sliders".  Thus if just Red and Green are selected, hitting -10
    will subtract 10 from the color value along the red and green
    variation only.  Note the message under the checkboxes; this
    indicates the primary color level being changed when more than one
    slider is tied together.  For example, if Red and Green are
    selected, you will be changing the Yellow level of the selected
    color.

    The "At Boundary" behavior determines what happens when any color
    variation hits either the lower or upper boundaries (0 or 255) as
    a result of clicking on the top row buttons:

    Stop
        When the increment or decrement would send any of the tied
        variations out of bounds, the entire delta is discarded.

    Wrap Around
        When the increment or decrement would send any of the tied
        variations out of bounds, the out of bounds value is wrapped
        around to the other side.  Thus if red were at 238 and +25
        were clicked, red would have the value 7.

    Preserve Distance
        When the increment or decrement would send any of the tied
        variations out of bounds, all tied variations are wrapped as
        one, so as to preserve the distance between them.  Thus if
        green and blue were tied, and green was at 238 while blue was
        at 223, and +25 were clicked, green would be at 15 and blue
        would be at 0.

    Squash
        When the increment or decrement would send any of the tied
        variations out of bounds, the out of bounds variation is set
        to the ceiling of 255 or floor of 0, as appropriate.  In this
        way, all tied variations are squashed to one edge or the
        other.

    The top row buttons have the following keyboard accelerators:

    -25 == Shift Left Arrow
    -10 == Control Left Arrow
     -1 == Left Arrow
     +1 == Right Arrow
    +10 == Control Right Arrow
    +25 == Shift Right Arrow


Keyboard Accelerators

    Alt-w in any secondary window dismisses the window.  In the main
    window it exits Pynche (except when running as a modal).

    Alt-q in any window exits Pynche (except when running as a modal).


Persistency

    Pynche remembers various settings of options and colors between
    invocations, storing these values in a `persistent initialization
    file'.  The actual location of this file is specified by the
    --initfile option (see above), and defaults to ~/.pynche.

    When Pynche exits, it saves these values in the init file, and
    re-reads them when it starts up.  There is no locking on this
    file, so if you run multiple instances of Pynche at a time, you
    may clobber the init file.

    The actual options stored include

    - the currently selected color

    - all settings of checkbox and radio button options in all windows

    - the contents of the text window, the current text selection and
      insertion point, and all current text widget element color
      settings.

    - the name of the color database file (but not its contents)

    You can inhibit Pynche from reading the init file by supplying the
    --ignore option on the command line.  However, you cannot suppress
    the storing of the settings in the init file on Pynche exit.  If
    you really want to do this, use /dev/null as the init file, using
    --initfile.


Color Name Database Files

    Pynche uses a color name database file to calculate the nearest
    color to the selected color, and to display in the Color List
    view.  Several files are distributed with Pynche, described
    below.  By default, the X11 color name database file is selected.
    Other files:

    html40colors.txt -- the HTML 4.0 guaranteed color names

    websafe.txt -- the 216 "Web-safe" colors that Netscape and MSIE
    guarantee will not be dithered.  These are specified in #rrggbb
    format for both values and names

    webcolors.txt -- The 140 color names that Tim Peters and his
    sister say NS and MSIE both understand (with some controversy over 
    AliceBlue).

    namedcolors.txt -- an alternative set of Netscape colors.

    You can switch between files by choosing "Load palette..." from
    the "File" menu.  This brings up a standard Tk file dialog.
    Choose the file you want and then click "Ok".  If Pynche
    understands the format in this file, it will load the database and 
    update the appropriate windows.  If not, it will bring up an error 
    dialog.


To Do

    Here's a brief list of things I want to do (some mythical day):

    - Better support for resizing the top level windows

    - More output views, e.g. color solids

    - Have the notion of a `last color selected'; this may require a
      new output view

    - Support setting the font in the text view

    - Support distutils setup.py for installation

    I'm open to suggestions!



Local Variables:
indent-tabs-mode: nil
End:

This directory contains a number of Python programs that are useful
while building or extending Python.

audiopy		Audiopy is a program to control the Solaris audio
		device, allowing you to choose both the input and
		output devices, and to set the output volume, that can
		be run either as a command-line script, or as a
		Tkinter application.

bgen		Generate complete extension modules from a
		description.  Still under development!
		WARNING: bgen has been removed in 3.0.

compiler	Tools used to maintain the compiler package in the
		standard library.

faqwiz		FAQ Wizard.
		See http://www.python.org/cgi-bin/faqw.py
		for a live example.

freeze		Create a stand-alone executable from a Python program.

gdb             Python code to be run inside gdb, to make it easier to
                debug Python itself (by David Malcolm).

i18n		Tools for internationalization. pygettext.py 
		parses Python source code and generates .pot files,
		and msgfmt.py generates a binary message catalog 
		from a catalog in text format.

pynche		A Tkinter-based color editor.

scripts         A number of useful single-file programs, e.g. tabnanny.py
                by Tim Peters, which checks for inconsistent mixing of
                tabs and spaces, and 2to3, which converts Python 2 code
                to Python 3 code.

unicode		Tools used to generate unicode database files for
		Python 2.0 (by Fredrik Lundh).

versioncheck	A tool to automate checking whether you have the latest
		version of a package (by Jack Jansen).

webchecker	A link checker for web sites.

world		Script to take a list of Internet addresses and print
		out where in the world those addresses originate from,
		based on the top-level domain country code found in
		the address. 

This directory contains a collection of executable Python scripts that
are useful while building, extending or managing Python.  Some (e.g.,
dutree or lll) are also generally useful UNIX tools.

See also the Demo/scripts directory!

analyze_dxp.py		Analyzes the result of sys.getdxp()
byext.py		Print lines/words/chars stats of files by extension
byteyears.py		Print product of a file's size and age
checkappend.py		Search for multi-argument .append() calls
checkpyc.py		Check presence and validity of ".pyc" files
classfix.py		Convert old class syntax to new
cleanfuture.py		Fix reduntant Python __future__ statements
combinerefs.py		A helper for analyzing PYTHONDUMPREFS output.
copytime.py		Copy one file's atime and mtime to another
crlf.py			Change CRLF line endings to LF (Windows to Unix)
cvsfiles.py		Print a list of files that are under CVS
db2pickle.py		Dump a database file to a pickle
diff.py			Print file diffs in context, unified, or ndiff formats
dutree.py		Format du(1) output as a tree sorted by size
eptags.py		Create Emacs TAGS file for Python modules
find_recursionlimit.py  Find the maximum recursion limit on this machine 
finddiv.py		A grep-like tool that looks for division operators
findlinksto.py		Recursively find symbolic links to a given path prefix
findnocoding.py		Find source files which need an encoding declaration
fixcid.py		Massive identifier substitution on C source files
fixdiv.py		Tool to fix division operators.
fixheader.py		Add some cpp magic to a C include file
fixnotice.py		Fix the copyright notice in source files
fixps.py		Fix Python scripts' first line (if #!)
ftpmirror.py		FTP mirror script
google.py		Open a webbrowser with Google
gprof2html.py		Transform gprof(1) output into useful HTML
h2py.py			Translate #define's into Python assignments
hotshotmain.py		Main program to run script under control of hotshot
idle			Main program to start IDLE
ifdef.py		Remove #if(n)def groups from C sources
lfcr.py			Change LF line endings to CRLF (Unix to Windows)
linktree.py		Make a copy of a tree with links to original files
lll.py			Find and list symbolic links in current directory
logmerge.py		Consolidate CVS/RCS logs read from stdin
mailerdaemon.py		parse error messages from mailer daemons (Sjoerd&Jack)
md5sum.py		Print MD5 checksums of argument files.
methfix.py		Fix old method syntax def f(self, (a1, ..., aN)):
mkreal.py		Turn a symbolic link into a real file or directory
ndiff.py		Intelligent diff between text files (Tim Peters)
nm2def.py		Create a template for PC/python_nt.def (Marc Lemburg)
objgraph.py		Print object graph from nm output on a library
parseentities.py	Utility for parsing HTML entity definitions
pathfix.py		Change #!/usr/local/bin/python into something else
pdeps.py		Print dependencies between Python modules
pickle2db.py		Load a pickle generated by db2pickle.py to a database
pindent.py		Indent Python code, giving block-closing comments
ptags.py		Create vi tags file for Python modules
pydoc			Python documentation browser.
pysource.py		Find Python source files
redemo.py		Basic regular expression demonstration facility
reindent.py		Change .py files to use 4-space indents.
rgrep.py		Reverse grep through a file (useful for big logfiles)
serve.py		Small wsgiref-based web server, used in make serve in Doc
setup.py		Install all scripts listed here
suff.py			Sort a list of files by suffix
svneol.py		Sets svn:eol-style on all files in directory
texcheck.py             Validate Python LaTeX formatting (Raymond Hettinger)
texi2html.py		Convert GNU texinfo files into HTML
treesync.py		Synchronize source trees (very ideosyncratic)
untabify.py		Replace tabs with spaces in argument files
which.py		Find a program in $PATH
xxci.py			Wrapper for rcsdiff and ci

This is versioncheck 1.0, a first stab at automatic checking of versions of
Python extension packages installed on your system.

The basic idea is that each package contains a _checkversion.py
somewhere, probably at the root level of the package. In addition, each
package maintainer makes a file available on the net, through ftp or
http, which contains the version number of the most recent distribution
and some readable text explaining the differences with previous
versions, where to download the package, etc.

The checkversions.py script walks through the installed Python tree (or
through a tree of choice), and runs each _checkversion.py script. These
scripts retrieve the current-version file over the net, compares version
numbers and tells the user about new versions of packages available.

A boilerplate for the _checkversion.py file can be found here. Replace
package name, version and the URL of the version-check file and put it in
your distribution. In stead of a single URL you can also specify a list
of URLs. Each of these will be checked in order until one is available,
this is handy for distributions that live in multiple places. Put the
primary distribution site (the most up-to-date site) before others.
The script is executed with execfile(), not imported, and the current
directory is the checkversion directory, so be careful with globals,
importing, etc.

The version-check file consists of an rfc822-style header followed by
plaintext. The only header field checked currently is
'Current-Version:', which should contain te current version and is
matched against the string contained in the _checkversion.py script.
The rest of the file is human-readable text and presented to the user if
there is a version mismatch. It should contain at the very least a URL
of either the current distribution or a webpage describing it.

Pycheckversion.py is the module that does the actual checking of versions.
It should be fine where it is, it is imported by checkversion before anything
else is done, but if imports fail you may want to move it to somewhere
along sys.path.

	Jack Jansen, CWI, 23-Dec-97.
	<jack@cwi.nl>
	

Webchecker
----------

This is a simple web tree checker, useful to find bad links in a web
tree.  It currently checks links pointing within the same subweb for
validity.  The main program is "webchecker.py".  See its doc string
(or invoke it with the option "-?") for more defails.

History:

- Jan 1997.  First release.  The module robotparser.py was written by
Skip Montanaro; the rest is original work by Guido van Rossum.

- May 1999.  Sam Bayer contributed a new version, wcnew.py, which
supports checking internal links (#spam fragments in URLs) and some
other options.

- Nov 1999.  Sam Bayer contributed patches to reintegrate wcnew.py
into webchecker.py, and corresponding mods to wcgui.py and
websucker.py.

- Mar 2004.  Chris Herborth contributed a patch to let webchecker.py
handle XHTML's 'id' attribute.

world -- Print mappings between country names and DNS country codes.

Contact: Barry Warsaw
Email:   bwarsaw@python.org

This script will take a list of Internet addresses and print out where in the
world those addresses originate from, based on the top-level domain country
code found in the address.  Addresses can be in any of the following forms:

    xx                -- just the country code or top-level domain identifier
    host.domain.xx    -- any Internet host or network name
    somebody@where.xx -- an Internet email address

If no match is found, the address is interpreted as a regular expression [*]
and a reverse lookup is attempted.  This script will search the country names
and print a list of matching entries.  You can force reverse mappings with the
`-r' flag (see below).

For example:

    %% world tz us
    tz originated from Tanzania, United Republic of
    us originated from United States

    %% world united
    united matches 6 countries:
        ae: United Arab Emirates
        uk: United Kingdom (common practice)
        um: United States Minor Outlying Islands
        us: United States
        tz: Tanzania, United Republic of
        gb: United Kingdom


 [*] Note that regular expressions must conform to Python 1.5's re.py module
 syntax.  The comparison is done with the search() method.

Country codes are maintained by the RIPE Network Coordination Centre,
in coordination with the ISO 3166 Maintenance Agency at DIN Berlin.  The
authoritative source of counry code mappings is:

    <url:ftp://info.ripe.net/iso3166-countrycodes>

The latest known change to this information was:

    Thu Aug  7 17:59:51 MET DST 1997

This script also knows about non-geographic top-level domains.

Usage: world [-d] [-p file] [-o] [-h] addr [addr ...]

    --dump
    -d
        Print mapping of all top-level domains.

    --parse file
    -p file
        Parse an iso3166-countrycodes file extracting the two letter country
        code followed by the country name.  Note that the three letter country
        codes and numbers, which are also provided in the standard format
        file, are ignored.

    --outputdict
    -o
        When used in conjunction with the `-p' option, output is in the form
        of a Python dictionary, and country names are normalized
        w.r.t. capitalization.  This makes it appropriate for cutting and
        pasting back into this file.

    --reverse
    -r
        Force reverse lookup.  In this mode the address can be any Python
        regular expression; this is matched against all country names and a
        list of matching mappings is printed.  In normal mode (e.g. without
        this flag), reverse lookup is performed on addresses if no matching
        country code is found.

    -h
    --help
        Print this message.


Local Variables:
indent-tabs-mode: nil
End:

Files in this directory from from Bob Ippolito's py2app.

License: Any components of the py2app suite may be distributed under
the MIT or PSF open source licenses.

This is version 1.0, SVN revision 789, from 2006/01/25.
The main repository is http://svn.red-bean.com/bob/macholib/trunk/macholib/
This directory contains the Distutils package.

There's a full documentation available at:

    http://docs.python.org/distutils/

The Distutils-SIG web page is also a good starting point:

    http://www.python.org/sigs/distutils-sig/

WARNING : Distutils must remain compatible with 2.3

$Id$

In this directory:
- py2_test_grammar.py -- test file that exercises most/all of Python 2.x's grammar.
- py3_test_grammar.py -- test file that exercises most/all of Python 3.x's grammar.
- infinite_recursion.py -- test file that causes lib2to3's faster recursive pattern matching
  scheme to fail, but passes when lib2to3 falls back to iterative pattern matching.
- fixes/ -- for use by test_refactor.py

This directory exists so that 3rd party packages can be installed
here.  Read the source for site.py for more details.

This directory only contains tests for outstanding bugs that cause the
interpreter to segfault.  Ideally this directory should always be empty, but
sometimes it may not be easy to fix the underlying cause and the bug is deemed
too obscure to invest the effort.

Each test should fail when run from the command line:

	./python Lib/test/crashers/weakref_in_del.py

Put as much info into a docstring or comments to help determine the cause of the
failure, as well as a bugs.python.org issue number if it exists.  Particularly
note if the cause is system or environment dependent and what the variables are.

Once the crash is fixed, the test case should be moved into an appropriate test
(even if it was originally from the test suite).  This ensures the regression
doesn't happen again.  And if it does, it should be easier to track down.

This empty directory serves as destination for temporary files
created by some tests.

This directory contains test cases that are known to leak references.
The idea is that you can import these modules while in the interpreter
and call the leak function repeatedly.  This will only be helpful if
the interpreter was built in debug mode.  If the total ref count
doesn't increase, the bug has been fixed and the file should be removed
from the repository.

Note:  be careful to check for cyclic garbage.  Sometimes it may be helpful
to define the leak function like:

def leak():
    def inner_leak():
        # this is the function that leaks, but also creates cycles
    inner_leak()
    gc.collect() ; gc.collect() ; gc.collect()

Here's an example interpreter session for test_gestalt which still leaks:

>>> from test.leakers.test_gestalt import leak
[24275 refs]
>>> leak()
[28936 refs]
>>> leak()
[28938 refs]
>>> leak()
[28940 refs]
>>> 

Once the leak is fixed, the test case should be moved into an appropriate
test (even if it was originally from the test suite).  This ensures the
regression doesn't happen again.  And if it does, it should be easier
to track down.

Empythoned
==========

Empythoned is a build script that uses [Emscripten](https://github.com/kripken/emscripten)
to compile CPython for use in a browser. It attempts to compile the main
interpreter as a single small executable and the whole standard library as
dynamically loaded libraries.

The project is in its infancy. Right now the core interpreter works very well,
but many of the libraries either don't work at all or contain various bugs.

Empythoned is licensed under the [MIT license](http://www.opensource.org/licenses/mit-license.php).

