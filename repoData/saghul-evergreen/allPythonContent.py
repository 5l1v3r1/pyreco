__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# evergreen documentation build configuration file, created by
# sphinx-quickstart on Mon Mar  4 11:05:18 2013.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import re

def get_version():
    return re.search(r"""__version__\s+=\s+(?P<quote>['"])(?P<version>.+?)(?P=quote)""", open('../evergreen/__init__.py').read()).group('version')
_version = get_version()

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = []

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'Evergreen'
copyright = u'2013, Saúl Ibarra Corretgé'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = _version
# The full version, including alpha/beta/rc tags.
release = _version

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'nature'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'evergreendoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'evergreen.tex', u'evergreen Documentation',
   u'Saúl Ibarra Corretgé', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'evergreen', u'evergreen Documentation',
     [u'Saúl Ibarra Corretgé'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'evergreen', u'evergreen Documentation',
   u'Saúl Ibarra Corretgé', 'evergreen', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

########NEW FILE########
__FILENAME__ = channel
#
# This file is part of Evergreen. See the NOTICE for more information.
#

import six

from evergreen.event import Event
from evergreen.locks import Lock

__all__ = ['Channel']


class _Bomb(object):

    def __init__(self, exp_type, exp_value=None, exp_traceback=None):
        self.type = exp_type
        self.value = exp_value if exp_value is not None else exp_type()
        self.traceback = exp_traceback

    def raise_(self):
        six.reraise(self.type, self.value, self.traceback)


class Channel(object):

    def __init__(self):
        self._send_lock = Lock()
        self._recv_lock = Lock()
        self._new_data = Event()
        self._recv_data = Event()
        self._data = None

    def send(self, data):
        with self._send_lock:
            self._data = data
            self._new_data.set()
            self._recv_data.wait()
            self._recv_data.clear()

    def send_exception(self, exc_type, exc_value=None, exc_tb=None):
        self.send(_Bomb(exc_type, exc_value, exc_tb))

    def receive(self):
        with self._recv_lock:
            self._new_data.wait()
            data, self._data = self._data, None
            self._new_data.clear()
            self._recv_data.set()
        if isinstance(data, _Bomb):
            data.raise_()
        else:
            return data

    def __iter__(self):
        return self

    def next(self):
        return self.receive()

    if six.PY3:
        __next__ = next
        del next


########NEW FILE########
__FILENAME__ = loop
#
# This file is part of Evergreen. See the NOTICE for more information.
#

import os
import pyuv
import sys
import threading
import traceback

try:
    from time import monotonic as _time
except ImportError:
    from time import time as _time

try:
    import signal
except ImportError:
    signal= None

from collections import deque
from fibers import Fiber
from functools import partial

from evergreen.core.socketpair import SocketPair
from evergreen.core.threadpool import ThreadPool

__all__ = ['EventLoop']


_tls = threading.local()


class Handler(partial):

    def __new__(cls, func, *args, **kw):
        assert not isinstance(func, Handler)
        obj = partial.__new__(cls, func, *args, **kw)
        obj._cancelled = False
        return obj

    def cancel(self):
        self._cancelled = True


class Timer(Handler):

    def __new__(cls, handle, func, *args, **kw):
        obj = Handler.__new__(cls, func, *args, **kw)
        obj._timer_h = handle
        return obj

    def cancel(self):
        super(Timer, self).cancel()
        if self._timer_h and not self._timer_h.closed:
            loop = self._timer_h.loop.event_loop
            self._timer_h.close()
            loop._timers.remove(self._timer_h)
        self._timer_h = None


class SignalHandler(Handler):

    def __new__(cls, handle, func, *args, **kw):
        obj = Handler.__new__(cls, func, *args, **kw)
        obj._signal_h = handle
        return obj

    def cancel(self):
        super(SignalHandler, self).cancel()
        if self._signal_h and not self._signal_h.closed:
            loop = self._signal_h.loop.event_loop
            self._signal_h.close()
            loop._signals[self._signal_h.signum].discard(self._signal_h)
        self._signal_h = None


RUN_DEFAULT = 1
RUN_FOREVER = 2

class EventLoop(object):

    def __init__(self):
        if getattr(_tls, 'loop', None) is not None:
            raise RuntimeError('cannot instantiate more than one event loop per thread')
        _tls.loop = self
        self._loop = pyuv.Loop()
        self._loop.excepthook = self._handle_error
        self._loop.event_loop = self
        self._threadpool = ThreadPool(self)
        self.task = Fiber(self._run_loop)

        self._destroyed = False
        self._started = False
        self._running = False

        self._fd_map = dict()
        self._signals = dict()
        self._timers = set()
        self._ready = deque()

        self._ready_processor = pyuv.Idle(self._loop)
        self._waker = pyuv.Async(self._loop, self._async_cb)
        self._waker.unref()

        self._install_signal_checker()

    @classmethod
    def current(cls):
        """Get the current event loop singleton object.
        """
        try:
            return _tls.loop
        except AttributeError:
            # create loop only for main thread
            if threading.current_thread().name == 'MainThread':
                _tls.loop = cls()
                return _tls.loop
            raise RuntimeError('there is no event loop created in the current thread')

    @property
    def running(self):
        return self._running

    def call_soon(self, callback, *args, **kw):
        handler = Handler(callback, *args, **kw)
        self._add_callback(handler)
        return handler

    def call_from_thread(self, callback, *args, **kw):
        handler = Handler(callback, *args, **kw)
        # Here we don't call self._add_callback on purpose, because it's not thread
        # safe to start pyuv handles. We just append the callback to the queue and
        # wakeup the loop. This is thread safe because the queue is only processed
        # in a single place and in a thread safe manner.
        self._ready.append(handler)
        self._waker.send()
        return handler

    def call_later(self, delay, callback, *args, **kw):
        if delay <= 0:
            return self.call_soon(callback, *args, **kw)
        timer_h = pyuv.Timer(self._loop)
        handler = Timer(timer_h, callback, *args, **kw)
        timer_h.handler = handler
        timer_h.start(self._timer_cb, delay, 0)
        self._timers.add(timer_h)
        return handler

    def call_at(self, when, callback, *args, **kw):
        return self.call_later(when-self.time(), callback, *args, **kw)

    def time(self):
        return _time()

    def add_reader(self, fd, callback, *args, **kw):
        handler = Handler(callback, *args, **kw)
        try:
            poll_h = self._fd_map[fd]
        except KeyError:
            poll_h = self._create_poll_handle(fd)
            self._fd_map[fd] = poll_h
        else:
            if poll_h.read_handler:
                raise RuntimeError('another reader is already registered for fd {}'.format(fd))
        poll_h.pevents |= pyuv.UV_READABLE
        poll_h.read_handler = handler
        poll_h.start(poll_h.pevents, self._poll_cb)

    def remove_reader(self, fd):
        try:
            poll_h = self._fd_map[fd]
        except KeyError:
            return False
        else:
            handler = poll_h.read_handler
            poll_h.pevents &= ~pyuv.UV_READABLE
            poll_h.read_handler = None
            if poll_h.pevents == 0:
                poll_h.close()
                del self._fd_map[fd]
            else:
                poll_h.start(poll_h.pevents, self._poll_cb)
            if handler:
                handler.cancel()
                return True
            return False

    def add_writer(self, fd, callback, *args, **kw):
        handler = Handler(callback, *args, **kw)
        try:
            poll_h = self._fd_map[fd]
        except KeyError:
            poll_h = self._create_poll_handle(fd)
            self._fd_map[fd] = poll_h
        else:
            if poll_h.write_handler:
                raise RuntimeError('another writer is already registered for fd {}'.format(fd))
        poll_h.pevents |= pyuv.UV_WRITABLE
        poll_h.write_handler = handler
        poll_h.start(poll_h.pevents, self._poll_cb)

    def remove_writer(self, fd):
        try:
            poll_h = self._fd_map[fd]
        except KeyError:
            return False
        else:
            handler = poll_h.write_handler
            poll_h.pevents &= ~pyuv.UV_WRITABLE
            poll_h.write_handler = None
            if poll_h.pevents == 0:
                poll_h.close()
                del self._fd_map[fd]
            else:
                poll_h.start(poll_h.pevents, self._poll_cb)
            if handler:
                handler.cancel()
                return True
            return False

    def add_signal_handler(self, sig, callback, *args, **kwargs):
        self._validate_signal(sig)
        signal_h = pyuv.Signal(self._loop)
        handler = SignalHandler(signal_h, callback, *args, **kwargs)
        signal_h.handler = handler
        signal_h.signum = sig
        try:
            signal_h.start(self._signal_cb, sig)
            signal_h.unref()
        except Exception as e:
            signal_h.close()
            raise RuntimeError(str(e))
        else:
            self._signals.setdefault(sig, set()).add(signal_h)
        return handler

    def remove_signal_handler(self, sig):
        self._validate_signal(sig)
        try:
            handles = self._signals.pop(sig)
        except KeyError:
            return False
        for signal_h in handles:
            del signal_h.handler
            signal_h.close()
        return True

    def switch(self):
        if not self._started:
            self.run()
            return
        current = Fiber.current()
        assert current is not self.task, 'Cannot switch to MAIN from MAIN'
        try:
            if self.task.parent is not current:
                current.parent = self.task
        except ValueError:
            pass  # gets raised if there is a Fiber parent cycle
        return self.task.switch()

    def run(self, mode=RUN_DEFAULT):
        if Fiber.current() is not self.task.parent:
            raise RuntimeError('run() can only be called from MAIN fiber')
        if not self.task.is_alive():
            raise RuntimeError('event loop has already ended')
        if self._started:
            raise RuntimeError('event loop was already started')
        self._started = True
        self._running = True
        self._run_mode = mode
        try:
            self.task.switch()
        finally:
            self._running = False

    def run_forever(self):
        self.run(mode=RUN_FOREVER)

    def stop(self):
        if not self._started:
            raise RuntimeError('event loop has not been started yet')
        if self._loop:
            self._loop.stop()

    def destroy(self):
        if self._running:
            raise RuntimeError('destroy() cannot be called while the loop is running')

        if self._destroyed:
            raise RuntimeError('Event loop already destroyed')

        loop = getattr(_tls, 'loop', None)
        if loop is not self:
            raise RuntimeError('destroy() can only be called from the same thread were the event loop was created')
        del _tls.loop, loop

        self._destroyed = True
        self._uninstall_signal_checker()

        self._cleanup_loop()
        self._loop.event_loop = None
        self._loop.excepthook = None
        self._loop = None
        self._threadpool = None

        self._ready_processor = None
        self._waker = None

        self._fd_map.clear()
        self._signals.clear()
        self._timers.clear()
        self._ready.clear()

    # internal

    def _add_callback(self, cb):
        self._ready.append(cb)
        if not self._ready_processor.active:
            self._ready_processor.start(self._process_ready)

    def _handle_error(self, typ, value, tb):
        if not issubclass(typ, SystemExit):
            traceback.print_exception(typ, value, tb)
        if issubclass(typ, (KeyboardInterrupt, SystemExit, SystemError)):
            assert Fiber.current() is self.task
            self.task.parent.throw(typ, value, tb)

    def _run_loop(self):
        if self._run_mode == RUN_FOREVER:
            self._waker.ref()
        self._loop.run(pyuv.UV_RUN_DEFAULT)

    def _cleanup_loop(self):
        def cb(handle):
            if not handle.closed:
                handle.close()
        self._loop.walk(cb)
        # All handles are now closed, run will not block
        self._loop.run(pyuv.UV_RUN_NOWAIT)

    def _create_poll_handle(self, fd):
        poll_h = pyuv.Poll(self._loop, fd)
        poll_h.pevents = 0
        poll_h.read_handler = None
        poll_h.write_handler = None
        return poll_h

    def _process_ready(self, handle):
        # Run all queued callbacks
        ntodo = len(self._ready)
        for x in range(ntodo):
            handler = self._ready.popleft()
            if not handler._cancelled:
                # loop.excepthook takes care of exception handling
                handler()
        if not self._ready:
            self._ready_processor.stop()

    def _async_cb(self, handle):
        if not self._ready_processor.active:
            self._ready_processor.start(self._process_ready)

    def _timer_cb(self, timer):
        assert not timer.handler._cancelled
        assert not timer.repeat
        self._add_callback(timer.handler)
        timer.close()
        self._timers.remove(timer)
        del timer.handler

    def _signal_cb(self, signal_h, signum):
        self._add_callback(signal_h.handler)

    def _poll_cb(self, poll_h, events, error):
        fd = poll_h.fileno()
        if error is not None:
            # An error happened, signal both readability and writability and
            # let the error propagate
            if poll_h.read_handler is not None:
                if poll_h.read_handler._cancelled:
                    self.remove_reader(fd)
                else:
                    self._add_callback(poll_h.read_handler)
            if poll_h.write_handler is not None:
                if poll_h.write_handler._cancelled:
                    self.remove_writer(fd)
                else:
                    self._add_callback(poll_h.write_handler)
            return

        old_events = poll_h.pevents
        modified = False

        if events & pyuv.UV_READABLE:
            if poll_h.read_handler is not None:
                if poll_h.read_handler._cancelled:
                    self.remove_reader(fd)
                    modified = True
                else:
                    self._add_callback(poll_h.read_handler)
            else:
                poll_h.pevents &= ~pyuv.UV_READABLE
        if events & pyuv.UV_WRITABLE:
            if poll_h.write_handler is not None:
                if poll_h.write_handler._cancelled:
                    self.remove_writer(fd)
                    modified = True
                else:
                    self._add_callback(poll_h.write_handler)
            else:
                poll_h.pevents &= ~pyuv.UV_WRITABLE

        if not modified and old_events != poll_h.pevents:
            # Rearm the handle
            poll_h.start(poll_h.pevents, self._poll_cb)

    def _install_signal_checker(self):
        self._socketpair = SocketPair()
        self._signal_checker = None
        if hasattr(signal, 'set_wakeup_fd'):
            try:
                old_wakeup_fd = signal.set_wakeup_fd(self._socketpair.writer_fileno())
                if old_wakeup_fd != -1:
                    # Already set, restore it
                    signal.set_wakeup_fd(old_wakeup_fd)
                    self._socketpair.close()
                    self._socketpair = None
                else:
                    self._signal_checker = pyuv.util.SignalChecker(self._loop, self._socketpair.reader_fileno())
                    self._signal_checker.start()
                    self._signal_checker.unref()
            except ValueError:
                self._socketpair.close()
                self._socketpair = None

    def _uninstall_signal_checker(self):
        if self._signal_checker:
            self._signal_checker.close()
            self._signal_checker = None
        if self._socketpair:
            self._socketpair.close()
            self._socketpair = None

    def _validate_signal(self, sig):
        if not isinstance(sig, int):
            raise TypeError('sig must be an int, not {!r}'.format(sig))
        if signal is None:
            raise RuntimeError('Signals are not supported')
        if not (1 <= sig < signal.NSIG):
            raise ValueError('sig {} out of range(1, {})'.format(sig, signal.NSIG))


########NEW FILE########
__FILENAME__ = socketpair
#
# This file is part of Evergreen. See the NOTICE for more information.
#

from evergreen import patcher
socket = patcher.original('socket')

__all__ = ('SocketPair')


import errno

try:
    from socket import socketpair
except ImportError:
    def socketpair(family=socket.AF_INET, type=socket.SOCK_STREAM, proto=0):
        """Emulate the Unix socketpair() function on Windows."""
        # We create a connected TCP socket. Note the trick with setblocking(0)
        # that prevents us from having to create a thread.
        lsock = socket.socket(family, type, proto)
        lsock.bind(('localhost', 0))
        lsock.listen(1)
        addr, port = lsock.getsockname()
        csock = socket.socket(family, type, proto)
        csock.setblocking(False)
        try:
            csock.connect((addr, port))
        except socket.error as e:
            if e.errno != errno.WSAEWOULDBLOCK:
                lsock.close()
                csock.close()
                raise
        ssock, _ = lsock.accept()
        csock.setblocking(True)
        lsock.close()
        return (ssock, csock)


class SocketPair(object):

    def __init__(self):
        self._reader, self._writer = socketpair()
        self._reader.setblocking(False)
        self._writer.setblocking(False)

    def reader_fileno(self):
        return self._reader.fileno()

    def writer_fileno(self):
        return self._writer.fileno()

    def close(self):
        self._reader.close()
        self._writer.close()


########NEW FILE########
__FILENAME__ = threadpool
#
# This file is part of Evergreen. See the NOTICE for more information.
#

import pyuv
import sys

from evergreen.event import Event
from evergreen.futures import Future

__all__ = ('ThreadPool')

"""Internal thread pool which uses the pyuv work queuing capability. This module
is for internal use of Evergreen.
"""


class _Work(object):
    __slots__ = ('func', 'args', 'kwargs', 'result', 'exc')

    def __init__(self, func, *args, **kwargs):
        self.func = func
        self.args = args
        self.kwargs = kwargs
        self.result = None
        self.exc = None

    def __call__(self):
        try:
            self.result = self.func(*self.args, **self.kwargs)
        except BaseException:
            self.exc = sys.exc_info()
        self = None


class ThreadPool(object):

    def __init__(self, loop):
        self.loop = loop

    def spawn(self, func, *args, **kwargs):
        fut = Future()
        work = _Work(func, *args, **kwargs)
        def after(error):
            if error is not None:
                assert error == pyuv.errno.UV_ECANCELLED
                return
            if work.exc is not None:
                fut.set_exception(work.exc)
            else:
                fut.set_result(work.result)
        fut.set_running_or_notify_cancel()
        self.loop._loop.queue_work(work, after)
        return fut


########NEW FILE########
__FILENAME__ = utils
#
# This file is part of Evergreen. See the NOTICE for more information.
#

from evergreen.locks import Condition, Lock

__all__ = ['Result']


Null = object()

class Result(object):
    """
    Result is an internal object which is meant to be used like a Future, but being more
    lightweight and supporting a single waiter. Example:

    result = Result()

    def f1():
        with result:
            some_async_func()
            return result.get()

    def some_async_func():
        # this function runs in a different task
        ...
        result.set_value(42)
    """

    __slots__ = ['_lock', '_cond', '_locked', '_used', '_exc', '_value']

    def __init__(self):
        self._lock = Lock()
        self._cond = Condition(Lock())
        self._locked = False
        self._used = False
        self._exc = self._value = Null

    def acquire(self):
        self._lock.acquire()
        self._locked = True

    def release(self):
        self._lock.release()
        self._locked = False
        self._used = False
        self._exc = self._value = Null

    def get(self):
        assert self._locked
        assert not self._used
        try:
            with self._cond:
                if self._exc == self._value == Null:
                    self._cond.wait()
            if self._exc != Null:
                raise self._exc
            assert self._value != Null
            return self._value
        finally:
            self._used = True
            self._exc = self._value = Null

    def set_value(self, value):
        assert self._locked
        assert not self._used
        assert self._exc == self._value == Null
        with self._cond:
            self._value = value
            self._cond.notify_all()

    def set_exception(self, value):
        assert self._locked
        assert not self._used
        assert self._exc == self._value == Null
        with self._cond:
            self._exc = value
            self._cond.notify_all()

    def __enter__(self):
        self.acquire()

    def __exit__(self, typ, val, tb):
        self.release()


########NEW FILE########
__FILENAME__ = event
#
# This file is part of Evergreen. See the NOTICE for more information.
#

from evergreen.locks import Condition, Lock

__all__ = ['Event']


class Event(object):

    def __init__(self):
        self._cond = Condition(Lock())
        self._flag = False

    def is_set(self):
        return self._flag

    def set(self):
        self._cond.acquire()
        try:
            self._flag = True
            self._cond.notify_all()
        finally:
            self._cond.release()

    def clear(self):
        self._cond.acquire()
        try:
            self._flag = False
        finally:
            self._cond.release()

    def wait(self, timeout=None):
        self._cond.acquire()
        try:
            signaled = self._flag
            if not signaled:
                signaled = self._cond.wait(timeout)
            return signaled
        finally:
            self._cond.release()


########NEW FILE########
__FILENAME__ = _base
#
# This file is part of Evergreen. See the NOTICE for more information.
#

import evergreen

from evergreen.event import Event
from evergreen.locks import Condition, Lock
from evergreen.log import log
from evergreen.timeout import Timeout


FIRST_COMPLETED = 'FIRST_COMPLETED'
FIRST_EXCEPTION = 'FIRST_EXCEPTION'
ALL_COMPLETED = 'ALL_COMPLETED'
_AS_COMPLETED = '_AS_COMPLETED'

# Possible future states (for internal use by the futures package).
PENDING = 'PENDING'
RUNNING = 'RUNNING'
# The future was cancelled by the user...
CANCELLED = 'CANCELLED'
# ...and _Waiter.add_cancelled() was called by a worker.
CANCELLED_AND_NOTIFIED = 'CANCELLED_AND_NOTIFIED'
FINISHED = 'FINISHED'

_FUTURE_STATES = [
    PENDING,
    RUNNING,
    CANCELLED,
    CANCELLED_AND_NOTIFIED,
    FINISHED
]

_STATE_TO_DESCRIPTION_MAP = {
    PENDING: "pending",
    RUNNING: "running",
    CANCELLED: "cancelled",
    CANCELLED_AND_NOTIFIED: "cancelled",
    FINISHED: "finished"
}


class Error(Exception):
    """Base class for all future-related exceptions."""
    pass


class CancelledError(Error):
    """The Future was cancelled."""
    pass


class TimeoutError(Error):
    """The operation exceeded the given deadline."""
    pass


class _Waiter(object):
    """Provides the event that wait() and as_completed() block on."""
    def __init__(self):
        self.event = Event()
        self.finished_futures = []

    def add_result(self, future):
        self.finished_futures.append(future)

    def add_exception(self, future):
        self.finished_futures.append(future)

    def add_cancelled(self, future):
        self.finished_futures.append(future)


class _AsCompletedWaiter(_Waiter):
    """Used by as_completed()."""

    def __init__(self):
        super(_AsCompletedWaiter, self).__init__()
        self.lock = Lock()

    def add_result(self, future):
        with self.lock:
            super(_AsCompletedWaiter, self).add_result(future)
            self.event.set()

    def add_exception(self, future):
        with self.lock:
            super(_AsCompletedWaiter, self).add_exception(future)
            self.event.set()

    def add_cancelled(self, future):
        with self.lock:
            super(_AsCompletedWaiter, self).add_cancelled(future)
            self.event.set()


class _FirstCompletedWaiter(_Waiter):
    """Used by wait(return_when=FIRST_COMPLETED)."""

    def add_result(self, future):
        super(_FirstCompletedWaiter, self).add_result(future)
        self.event.set()

    def add_exception(self, future):
        super(_FirstCompletedWaiter, self).add_exception(future)
        self.event.set()

    def add_cancelled(self, future):
        super(_FirstCompletedWaiter, self).add_cancelled(future)
        self.event.set()


class _AllCompletedWaiter(_Waiter):
    """Used by wait(return_when=FIRST_EXCEPTION and ALL_COMPLETED)."""

    def __init__(self, num_pending_calls, stop_on_exception):
        self.num_pending_calls = num_pending_calls
        self.stop_on_exception = stop_on_exception
        self.lock = Lock()
        super(_AllCompletedWaiter, self).__init__()

    def _decrement_pending_calls(self):
        with self.lock:
            self.num_pending_calls -= 1
            if not self.num_pending_calls:
                self.event.set()

    def add_result(self, future):
        super(_AllCompletedWaiter, self).add_result(future)
        self._decrement_pending_calls()

    def add_exception(self, future):
        super(_AllCompletedWaiter, self).add_exception(future)
        if self.stop_on_exception:
            self.event.set()
        else:
            self._decrement_pending_calls()

    def add_cancelled(self, future):
        super(_AllCompletedWaiter, self).add_cancelled(future)
        self._decrement_pending_calls()


class _AcquireFutures(object):
    """A context manager that does an ordered acquire of Future conditions."""

    def __init__(self, futures):
        self.futures = sorted(futures, key=id)

    def __enter__(self):
        for future in self.futures:
            future._condition.acquire()

    def __exit__(self, *args):
        for future in self.futures:
            future._condition.release()


def _create_and_install_waiters(fs, return_when):
    if return_when == _AS_COMPLETED:
        waiter = _AsCompletedWaiter()
    elif return_when == FIRST_COMPLETED:
        waiter = _FirstCompletedWaiter()
    else:
        pending_count = sum(f._state not in [CANCELLED_AND_NOTIFIED, FINISHED] for f in fs)
        if return_when == FIRST_EXCEPTION:
            waiter = _AllCompletedWaiter(pending_count, stop_on_exception=True)
        elif return_when == ALL_COMPLETED:
            waiter = _AllCompletedWaiter(pending_count, stop_on_exception=False)
        else:
            raise ValueError("Invalid return condition: %r" % return_when)

    for f in fs:
        f._waiters.append(waiter)

    return waiter


def as_completed(fs, timeout=None):
    """An iterator over the given futures that yields each as it completes.

    Args:
        fs: The sequence of Futures (possibly created by different Executors) to
            iterate over.
        timeout: The maximum number of seconds to wait. If None, then there
            is no limit on the wait time.

    Returns:
        An iterator that yields the given Futures as they complete (finished or
        cancelled).

    Raises:
        TimeoutError: If the entire result iterator could not be generated
            before the given timeout.
    """
    with _AcquireFutures(fs):
        finished = set(f for f in fs if f._state in [CANCELLED_AND_NOTIFIED, FINISHED])
        pending = set(fs) - finished
        waiter = _create_and_install_waiters(fs, _AS_COMPLETED)

    timer = Timeout(timeout)
    timer.start()
    try:
        for future in finished:
            yield future
        while pending:
            waiter.event.wait()
            with waiter.lock:
                finished = waiter.finished_futures
                waiter.finished_futures = []
                waiter.event.clear()
            for future in finished:
                yield future
                pending.remove(future)
    except Timeout as e:
        if timer is not e:
            raise
        raise TimeoutError('%d (of %d) futures unfinished' % (len(pending), len(fs)))
    finally:
        timer.cancel()
        for f in fs:
            f._waiters.remove(waiter)


def wait(fs, timeout=None, return_when=ALL_COMPLETED):
    """Wait for the futures in the given sequence to complete.

    Args:
        fs: The sequence of Futures (possibly created by different Executors) to
            wait upon.
        timeout: The maximum number of seconds to wait. If None, then there
            is no limit on the wait time.
        return_when: Indicates when this function should return. The options
            are:

            FIRST_COMPLETED - Return when any future finishes or is
                              cancelled.
            FIRST_EXCEPTION - Return when any future finishes by raising an
                              exception. If no future raises an exception
                              then it is equivalent to ALL_COMPLETED.
            ALL_COMPLETED -   Return when all futures finish or are cancelled.

    Returns:
        A 2-tuple of sets. The first set, contains the
        futures that completed (is finished or cancelled) before the wait
        completed. The second set, contains uncompleted futures.
    """
    with _AcquireFutures(fs):
        done = set(f for f in fs if f._state in [CANCELLED_AND_NOTIFIED, FINISHED])
        not_done = set(fs) - done

        if (return_when == FIRST_COMPLETED) and done:
            return (done, not_done)
        elif (return_when == FIRST_EXCEPTION) and done:
            if any(f for f in done if not f.cancelled() and f.exception() is not None):
                return (done, not_done)

        if len(done) == len(fs):
            return (done, not_done)

        waiter = _create_and_install_waiters(fs, return_when)

    waiter.event.wait(timeout)
    for f in fs:
        f._waiters.remove(waiter)

    done.update(waiter.finished_futures)
    return (done, set(fs) - done)


class Future(object):

    def __init__(self):
        self._condition = Condition()
        self._state = PENDING
        self._result = None
        self._exception = None
        self._callbacks = []
        self._waiters = []

    def __repr__(self):
        with self._condition:
            if self._state == FINISHED:
                if self._exception:
                    text = 'raised %s' % self._exception.__class__.__name__
                else:
                    text = 'returned %s' % self._result.__class__.__name__
                return '<%s at %s state=%s %s>' % (
                    self.__class__.__name__,
                    hex(id(self)),
                    _STATE_TO_DESCRIPTION_MAP[self._state],
                    text)
            return '<%s at %s state=%s>' % (
                    self.__class__.__name__,
                    hex(id(self)),
                   _STATE_TO_DESCRIPTION_MAP[self._state])

    def cancel(self):
        with self._condition:
            if self._state in (RUNNING, FINISHED):
                return False
            elif self._state in (CANCELLED, CANCELLED_AND_NOTIFIED):
                return True
            self._state = CANCELLED
            self._condition.notify_all()
        self._run_callbacks()
        return True

    @property
    def cancelled(self):
        with self._condition:
            return self._state in (CANCELLED, CANCELLED_AND_NOTIFIED)

    @property
    def done(self):
        with self._condition:
            return self._state in (CANCELLED, CANCELLED_AND_NOTIFIED, FINISHED)

    def get(self, timeout=None, return_exception=False):
        with self._condition:
            if self._state in (CANCELLED, CANCELLED_AND_NOTIFIED):
                raise CancelledError()
            elif self._state == FINISHED:
                return self._get_result(return_exception)

            self._condition.wait(timeout)

            if self._state in (CANCELLED, CANCELLED_AND_NOTIFIED):
                raise CancelledError()
            elif self._state == FINISHED:
                return self._get_result(return_exception)
            else:
                raise TimeoutError()

    def add_done_callback(self, func):
        with self._condition:
            if self._state not in (CANCELLED, CANCELLED_AND_NOTIFIED, FINISHED):
                self._callbacks.append(func)
                return
        func(self)

    # Internal

    def _get_result(self, return_exception):
        if self._exception:
            if return_exception:
                return self._exception
            else:
                raise self._exception
        else:
            return self._result

    def _run_callbacks(self):
        for cb in self._callbacks:
            try:
                cb(self)
            except Exception:
                log.exception('exception calling callback for %r', self)
        self._callbacks = []

    def set_running_or_notify_cancel(self):
        with self._condition:
            if self._state == CANCELLED:
                self._state = CANCELLED_AND_NOTIFIED
                for waiter in self._waiters:
                    waiter.add_cancelled(self)
                # self._condition.notify_all() is not necessary because
                # self.cancel() triggers a notification.
                return False
            elif self._state == PENDING:
                self._state = RUNNING
                return True
            else:
                raise RuntimeError('Future in unexpected state: %s' % self._state)

    def set_result(self, result):
        with self._condition:
            self._result = result
            self._state = FINISHED
            for waiter in self._waiters:
                waiter.add_result(self)
            self._condition.notify_all()
        self._run_callbacks()

    def set_exception(self, exception):
        with self._condition:
            self._exception = exception
            self._state = FINISHED
            for waiter in self._waiters:
                waiter.add_exception(self)
            self._condition.notify_all()
        self._run_callbacks()


class Executor(object):

    def submit(self, fn, *args, **kwargs):
        """Submits a callable to be executed with the given arguments.

        Schedules the callable to be executed as fn(*args, **kwargs) and returns
        a Future instance representing the execution of the callable.

        Returns:
            A Future representing the given call.
        """
        raise NotImplementedError

    def map(self, fn, *iterables, **kwargs):
        """Returns a iterator equivalent to map(fn, iter).

        Args:
            fn: A callable that will take take as many arguments as there are
                passed iterables.
            timeout: The maximum number of seconds to wait. If None, then there
                is no limit on the wait time.

        Returns:
            An iterator equivalent to: map(func, *iterables) but the calls may
            be evaluated out-of-order.

        Raises:
            TimeoutError: If the entire result iterator could not be generated
                before the given timeout.
            Exception: If fn(*args) raises for any values.
        """
        loop = evergreen.current.loop

        timeout = kwargs.get('timeout')
        if timeout is not None:
            end_time = timeout + loop.time()

        fs = [self.submit(fn, *args) for args in zip(*iterables)]

        # Yield must be hidden in closure so that the futures are submitted
        # before the first iterator value is required.
        def result_iterator():
            try:
                for future in fs:
                    if timeout is None:
                        yield future.get()
                    else:
                        yield future.get(end_time - loop.time())
            finally:
                for future in fs:
                    future.cancel()
        return result_iterator()

    def shutdown(self, wait=True):
        """Clean-up the resources associated with the Executor.

        It is safe to call this method several times. Otherwise, no other
        methods can be called after this one.

        Args:
            wait: If True then shutdown will not return until all running
                futures have finished executing and the resources used by the
                executor have been reclaimed.
        """
        pass

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.shutdown(wait=True)
        return False


class InfiniteHandler(object):
    """Helper class to create a handler that keeps the loop alive until
    it's cancelled.
    """

    def __init__(self, loop):
        self.loop = loop
        self._cb()

    def _cb(self):
        self.handler = self.loop.call_later(24*3600, self._cb)

    def cancel(self):
        if self.handler:
            self.handler.cancel()
            self.handler = self.loop = None


########NEW FILE########
__FILENAME__ = _process
# Copyright 2009 Brian Quinlan. All Rights Reserved.
# Licensed to PSF under a Contributor Agreement.

"""Implements ProcessPoolExecutor.

The follow diagram and text describe the data-flow through the system:

|======================= In-process =====================|== Out-of-process ==|

+----------+     +----------+       +--------+     +-----------+    +---------+
|          |  => | Work Ids |    => |        |  => | Call Q    | => |         |
|          |     +----------+       |        |     +-----------+    |         |
|          |     | ...      |       |        |     | ...       |    |         |
|          |     | 6        |       |        |     | 5, call() |    |         |
|          |     | 7        |       |        |     | ...       |    |         |
| Process  |     | ...      |       | Local  |     +-----------+    | Process |
|  Pool    |     +----------+       | Worker |                      |  #1..n  |
| Executor |                        | Thread |                      |         |
|          |     +----------- +     |        |     +-----------+    |         |
|          | <=> | Work Items | <=> |        | <=  | Result Q  | <= |         |
|          |     +------------+     |        |     +-----------+    |         |
|          |     | 6: call()  |     |        |     | ...       |    |         |
|          |     |    future  |     |        |     | 4, result |    |         |
|          |     | ...        |     |        |     | 3, except |    |         |
+----------+     +------------+     +--------+     +-----------+    +---------+

Executor.submit() called:
- creates a uniquely numbered _WorkItem and adds it to the "Work Items" dict
- adds the id of the _WorkItem to the "Work Ids" queue

Local worker thread:
- reads work ids from the "Work Ids" queue and looks up the corresponding
  WorkItem from the "Work Items" dict: if the work item has been cancelled then
  it is simply removed from the dict, otherwise it is repackaged as a
  _CallItem and put in the "Call Q". New _CallItems are put in the "Call Q"
  until "Call Q" is full. NOTE: the size of the "Call Q" is kept small because
  calls placed in the "Call Q" can no longer be cancelled with Future.cancel().
- reads _ResultItems from "Result Q", updates the future stored in the
  "Work Items" dict and deletes the dict entry

Process #1..n:
- reads _CallItems from "Call Q", executes the calls, and puts the resulting
  _ResultItems in "Request Q"
"""

import atexit
import multiprocessing
import threading
import weakref

import evergreen
from evergreen.futures._base import Executor, Future, InfiniteHandler
from six.moves import queue


__author__ = 'Brian Quinlan (brian@sweetapp.com)'

# Workers are created as daemon threads and processes. This is done to allow the
# interpreter to exit when there are still idle processes in a
# ProcessPoolExecutor's process pool (i.e. shutdown() was not called). However,
# allowing workers to die with the interpreter has two undesirable properties:
#   - The workers would still be running during interpretor shutdown,
#     meaning that they would fail in unpredictable ways.
#   - The workers could be killed while evaluating a work item, which could
#     be bad if the callable being evaluated has external side-effects e.g.
#     writing to a file.
#
# To work around this problem, an exit handler is installed which tells the
# workers to exit when their work queues are empty and then waits until the
# threads/processes finish.

_thread_references = set()
_shutdown = False

def _python_exit():
    global _shutdown
    _shutdown = True
    for thread_reference in _thread_references:
        thread = thread_reference()
        if thread is not None:
            thread.join()

def _remove_dead_thread_references():
    """Remove inactive threads from _thread_references.

    Should be called periodically to prevent memory leaks in scenarios such as:
    >>> while True:
    >>> ...    t = ThreadPoolExecutor(max_workers=5)
    >>> ...    t.map(int, ['1', '2', '3', '4', '5'])
    """
    for thread_reference in set(_thread_references):
        if thread_reference() is None:
            _thread_references.discard(thread_reference)

# Controls how many more calls than processes will be queued in the call queue.
# A smaller number will mean that processes spend more time idle waiting for
# work while a larger number will make Future.cancel() succeed less frequently
# (Futures in the call queue cannot be cancelled).
EXTRA_QUEUED_CALLS = 1

class _WorkItem(object):
    def __init__(self, future, fn, args, kwargs):
        self.future = future
        self.fn = fn
        self.args = args
        self.kwargs = kwargs
        self.loop = evergreen.current.loop
        # Keep the loop alive while this work item is queued
        self.handler = InfiniteHandler(self.loop)
        self._event = threading.Event()
        self._cancelled = False

    def is_cancelled(self):
        self.loop.call_from_thread(self._set_running)
        self._event.wait()
        return self._cancelled

    def _set_running(self):
        if not self.future.set_running_or_notify_cancel():
            self.handler.cancel()
            self._cancelled = True
        self._event.set()

class _ResultItem(object):
    def __init__(self, work_id, exception=None, result=None):
        self.work_id = work_id
        self.exception = exception
        self.result = result

class _CallItem(object):
    __slots__ = ('work_id', 'fn', 'args', 'kwargs')

    def __init__(self, work_id, fn, args, kwargs):
        self.work_id = work_id
        self.fn = fn
        self.args = args
        self.kwargs = kwargs

    def __call__(self):
        return self.fn(*self.args, **self.kwargs)

def _process_worker(call_queue, result_queue, shutdown):
    """Evaluates calls from call_queue and places the results in result_queue.

    This worker is run in a seperate process.

    Args:
        call_queue: A multiprocessing.Queue of _CallItems that will be read and
            evaluated by the worker.
        result_queue: A multiprocessing.Queue of _ResultItems that will written
            to by the worker.
        shutdown: A multiprocessing.Event that will be set as a signal to the
            worker that it should exit when call_queue is empty.
    """
    while True:
        try:
            call_item = call_queue.get(block=True, timeout=0.1)
        except queue.Empty:
            if shutdown.is_set():
                return
        else:
            try:
                r = call_item()
            except BaseException as e:
                result_queue.put(_ResultItem(call_item.work_id, exception=e))
            else:
                result_queue.put(_ResultItem(call_item.work_id, result=r))

def _add_call_item_to_queue(pending_work_items,
                            work_ids,
                            call_queue):
    """Fills call_queue with _WorkItems from pending_work_items.

    This function never blocks.

    Args:
        pending_work_items: A dict mapping work ids to _WorkItems e.g.
            {5: <_WorkItem...>, 6: <_WorkItem...>, ...}
        work_ids: A queue.Queue of work ids e.g. Queue([5, 6, ...]). Work ids
            are consumed and the corresponding _WorkItems from
            pending_work_items are transformed into _CallItems and put in
            call_queue.
        call_queue: A multiprocessing.Queue that will be filled with _CallItems
            derived from _WorkItems.
    """
    while True:
        if call_queue.full():
            return
        try:
            work_id = work_ids.get(block=False)
        except queue.Empty:
            return
        else:
            work_item = pending_work_items[work_id]
            if not work_item.is_cancelled():
                call_queue.put(_CallItem(work_id,
                                         work_item.fn,
                                         work_item.args,
                                         work_item.kwargs),
                               block=True)
            else:
                del work_item, pending_work_items[work_id]
                continue

def _queue_manangement_worker(executor_reference,
                              processes,
                              pending_work_items,
                              work_ids_queue,
                              call_queue,
                              result_queue,
                              shutdown_process_event):
    """Manages the communication between this process and the worker processes.

    This function is run in a local thread.

    Args:
        executor_reference: A weakref.ref to the ProcessPoolExecutor that owns
            this thread. Used to determine if the ProcessPoolExecutor has been
            garbage collected and that this function can exit.
        process: A list of the multiprocessing.Process instances used as
            workers.
        pending_work_items: A dict mapping work ids to _WorkItems e.g.
            {5: <_WorkItem...>, 6: <_WorkItem...>, ...}
        work_ids_queue: A queue.Queue of work ids e.g. Queue([5, 6, ...]).
        call_queue: A multiprocessing.Queue that will be filled with _CallItems
            derived from _WorkItems for processing by the process workers.
        result_queue: A multiprocessing.Queue of _ResultItems generated by the
            process workers.
        shutdown_process_event: A multiprocessing.Event used to signal the
            process workers that they should exit when their work queue is
            empty.
    """
    while True:
        _add_call_item_to_queue(pending_work_items,
                                work_ids_queue,
                                call_queue)

        try:
            result_item = result_queue.get(block=True, timeout=0.1)
        except queue.Empty:
            executor = executor_reference()
            # No more work items can be added if:
            #   - The interpreter is shutting down OR
            #   - The executor that owns this worker has been collected OR
            #   - The executor that owns this worker has been shutdown.
            if _shutdown or executor is None or executor._shutdown_thread:
                # Since no new work items can be added, it is safe to shutdown
                # this thread if there are no pending work items.
                if not pending_work_items:
                    shutdown_process_event.set()

                    # If .join() is not called on the created processes then
                    # some multiprocessing.Queue methods may deadlock on Mac OSX.
                    for p in processes:
                        p.join()
                    return
            del executor
        else:
            work_item = pending_work_items.pop(result_item.work_id)
            loop = work_item.loop
            loop.call_from_thread(_set_work_result, work_item, result_item)
            del result_item, work_item, loop


def _set_work_result(work_item, result_item):
    loop = work_item.loop
    if result_item.exception:
        work_item.future.set_exception(result_item.exception)
    else:
        work_item.future.set_result(result_item.result)
    work_item.handler.cancel()


class ProcessPoolExecutor(Executor):
    def __init__(self, max_workers=None):
        """Initializes a new ProcessPoolExecutor instance.

        Args:
            max_workers: The maximum number of processes that can be used to
                execute the given calls. If None or not given then as many
                worker processes will be created as the machine has processors.
        """
        _remove_dead_thread_references()

        if max_workers is None:
            self._max_workers = multiprocessing.cpu_count()
        else:
            self._max_workers = max_workers

        # Make the call queue slightly larger than the number of processes to
        # prevent the worker processes from idling. But don't make it too big
        # because futures in the call queue cannot be cancelled.
        self._call_queue = multiprocessing.Queue(self._max_workers +
                                                 EXTRA_QUEUED_CALLS)
        self._result_queue = multiprocessing.Queue()
        self._work_ids = queue.Queue()
        self._queue_management_thread = None
        self._processes = set()

        # Shutdown is a two-step process.
        self._shutdown_thread = False
        self._shutdown_process_event = multiprocessing.Event()
        self._shutdown_lock = threading.Lock()
        self._queue_count = 0
        self._pending_work_items = {}

    def _start_queue_management_thread(self):
        if self._queue_management_thread is None:
            self._queue_management_thread = threading.Thread(
                    target=_queue_manangement_worker,
                    args=(weakref.ref(self),
                          self._processes,
                          self._pending_work_items,
                          self._work_ids,
                          self._call_queue,
                          self._result_queue,
                          self._shutdown_process_event))
            self._queue_management_thread.daemon = True
            self._queue_management_thread.start()
            _thread_references.add(weakref.ref(self._queue_management_thread))

    def _adjust_process_count(self):
        for _ in range(len(self._processes), self._max_workers):
            p = multiprocessing.Process(
                    target=_process_worker,
                    args=(self._call_queue,
                          self._result_queue,
                          self._shutdown_process_event))
            p.start()
            self._processes.add(p)

    def submit(self, fn, *args, **kwargs):
        with self._shutdown_lock:
            if self._shutdown_thread:
                raise RuntimeError('cannot schedule new futures after shutdown')

            f = Future()
            w = _WorkItem(f, fn, args, kwargs)

            self._pending_work_items[self._queue_count] = w
            self._work_ids.put(self._queue_count)
            self._queue_count += 1

            self._start_queue_management_thread()
            self._adjust_process_count()
            return f
    submit.__doc__ = Executor.submit.__doc__

    def shutdown(self, wait=True):
        with self._shutdown_lock:
            self._shutdown_thread = True
        if wait:
            if self._queue_management_thread:
                self._queue_management_thread.join()
        # To reduce the risk of openning too many files, remove references to
        # objects that use file descriptors.
        self._queue_management_thread = None
        self._call_queue = None
        self._result_queue = None
        self._shutdown_process_event = None
        self._processes = None
    shutdown.__doc__ = Executor.shutdown.__doc__

atexit.register(_python_exit)

########NEW FILE########
__FILENAME__ = _task
#
# This file is part of Evergreen. See the NOTICE for more information.
#

import evergreen

from evergreen.futures._base import Executor, Future
from evergreen.locks import Lock
from evergreen.queue import Queue


class _WorkItem(object):
    def __init__(self, future, fn, args, kwargs):
        self.future = future
        self.fn = fn
        self.args = args
        self.kwargs = kwargs

    def __call__(self):
        if not self.future.set_running_or_notify_cancel():
            return
        try:
            result = self.fn(*self.args, **self.kwargs)
        except BaseException as e:
            self.future.set_exception(e)
        else:
            self.future.set_result(result)


class TaskPoolExecutor(Executor):

    def __init__(self, max_workers):
        self._max_workers = max_workers
        self._tasks = set()
        self._work_queue = Queue()
        self._shutdown = False
        self._shutdown_lock = Lock()

    def submit(self, fn, *args, **kwargs):
        with self._shutdown_lock:
            if self._shutdown:
                raise RuntimeError('cannot schedule new futures after shutdown')
            f = Future()
            work = _WorkItem(f, fn, args, kwargs)
            self._work_queue.put(work)
            self._adjust_task_count()
            return f

    def shutdown(self, wait=True):
        with self._shutdown_lock:
            self._shutdown = True
            self._work_queue.put(None)
        if wait:
            for task in self._tasks:
                task.join()

    def _adjust_task_count(self):
        if len(self._tasks) < self._max_workers:
            t = evergreen.spawn(self._work)
            self._tasks.add(t)

    def _work(self):
        while True:
            work_item = self._work_queue.get(block=True)
            if work_item is not None:
                work_item()
                del work_item
                continue
            if self._shutdown:
                # Notice other workers
                self._work_queue.put(None)
                return


########NEW FILE########
__FILENAME__ = _thread
#
# This file is part of Evergreen. See the NOTICE for more information.
#

import atexit
import threading
import weakref

import evergreen
from evergreen.futures._base import Executor, Future, InfiniteHandler
from evergreen.log import log
from six.moves import queue


# Workers are created as daemon threads. This is done to allow the interpreter
# to exit when there are still idle threads in a ThreadPoolExecutor's thread
# pool (i.e. shutdown() was not called). However, allowing workers to die with
# the interpreter has two undesirable properties:
#   - The workers would still be running during interpretor shutdown,
#     meaning that they would fail in unpredictable ways.
#   - The workers could be killed while evaluating a work item, which could
#     be bad if the callable being evaluated has external side-effects e.g.
#     writing to a file.
#
# To work around this problem, an exit handler is installed which tells the
# workers to exit when their work queues are empty and then waits until the
# threads finish.

_thread_references = set()
_shutdown = False


def _python_exit():
    global _shutdown
    _shutdown = True
    for thread_reference in _thread_references:
        thread = thread_reference()
        if thread is not None:
            thread.join()


def _remove_dead_thread_references():
    """Remove inactive threads from _thread_references.

    Should be called periodically to prevent memory leaks in scenarios such as:
    >>> while True:
    ...    t = ThreadPoolExecutor(max_workers=5)
    ...    t.map(int, ['1', '2', '3', '4', '5'])
    """
    for thread_reference in set(_thread_references):
        if thread_reference() is None:
            _thread_references.discard(thread_reference)


atexit.register(_python_exit)


class _WorkItem(object):
    def __init__(self, future, fn, args, kwargs):
        self.future = future
        self.fn = fn
        self.args = args
        self.kwargs = kwargs
        self.loop = evergreen.current.loop
        # Keep the loop alive while this work item is queued
        self.handler = InfiniteHandler(self.loop)
        self._event = threading.Event()
        self._result = None
        self._exc = None
        self._cancelled = False

    def run(self):
        self.loop.call_from_thread(self._set_running)
        self._event.wait()
        if self._cancelled:
            return
        try:
            r = self.fn(*self.args, **self.kwargs)
            self.loop.call_from_thread(self.future.set_result, r)
        except BaseException as e:
            self.loop.call_from_thread(self.future.set_exception, e)
        finally:
            self.loop.call_from_thread(self.handler.cancel)
            self.loop = self.handler = None

    def _set_running(self):
        if not self.future.set_running_or_notify_cancel():
            self.handler.cancel()
            self._cancelled = True
        self._event.set()

def _worker(executor_reference, work_queue):
    try:
        while True:
            try:
                work_item = work_queue.get(block=True, timeout=0.1)
            except queue.Empty:
                executor = executor_reference()
                # Exit if:
                #   - The interpreter is shutting down OR
                #   - The executor that owns the worker has been collected OR
                #   - The executor that owns the worker has been shutdown.
                if _shutdown or executor is None or executor._shutdown:
                    return
                del executor
            else:
                work_item.run()
                del work_item
    except BaseException:
        log.critical('Exception in worker', exc_info=True)

class ThreadPoolExecutor(Executor):

    def __init__(self, max_workers):
        _remove_dead_thread_references()

        self._max_workers = max_workers
        self._work_queue = queue.Queue()
        self._threads = set()
        self._shutdown = False
        self._shutdown_lock = threading.Lock()

    def submit(self, fn, *args, **kwargs):
        with self._shutdown_lock:
            if self._shutdown:
                raise RuntimeError('cannot schedule new futures after shutdown')
            f = Future()
            w = _WorkItem(f, fn, args, kwargs)
            self._work_queue.put(w)
            self._adjust_thread_count()
            return f
    submit.__doc__ = Executor.submit.__doc__

    def _adjust_thread_count(self):
        if len(self._threads) < self._max_workers:
            t = threading.Thread(target=_worker, args=(weakref.ref(self), self._work_queue))
            t.daemon = True
            t.start()
            self._threads.add(t)
            _thread_references.add(weakref.ref(t))

    def shutdown(self, wait=True):
        with self._shutdown_lock:
            self._shutdown = True
        if wait:
            for t in self._threads:
                t.join()
    shutdown.__doc__ = Executor.shutdown.__doc__


########NEW FILE########
__FILENAME__ = errno
#
# This file is part of Evergreen. See the NOTICE for more information.
#

import pyuv


errorcode = {}
strerror = pyuv.errno.strerror

def _bootstrap():
    g = globals()
    for k, v in pyuv.errno.errorcode.items():
        e = v[3:]
        errorcode[k] = e
        g[e] = k
_bootstrap()
del _bootstrap, pyuv


########NEW FILE########
__FILENAME__ = pipe
#
# This file is part of Evergreen. See the NOTICE for more information.
#

import pyuv

import evergreen
from evergreen.core.utils import Result
from evergreen.io import errno
from evergreen.io.stream import BaseStream, StreamConnection, StreamServer
from evergreen.log import log

__all__ = ['PipeServer', 'PipeClient', 'PipeConnection', 'PipeStream', 'PipeError']


PipeError = pyuv.error.PipeError


class BasePipeStream(BaseStream):
    error_cls = PipeError

    def __init__(self, handle):
        super(BasePipeStream, self).__init__()
        self._handle = handle


class PipeStream(BasePipeStream):

    def __init__(self):
        loop = evergreen.current.loop
        handle = pyuv.Pipe(loop._loop)
        super(PipeStream, self).__init__(handle)

    def open(self, fd):
        self._handle.open(fd)
        self._set_connected()


class PipeConnection(BasePipeStream, StreamConnection):
    pass


class PipeClient(BasePipeStream):

    def __init__(self):
        loop = evergreen.current.loop
        handle = pyuv.Pipe(loop._loop)
        super(PipeClient, self).__init__(handle)
        self._connect_result = Result()

    def connect(self, target):
        if self._connected:
            raise PipeError('already connected')
        with self._connect_result:
            try:
                self._handle.connect(target, self.__connect_cb)
            except PipeError:
                self.close()
                raise
            try:
                self._connect_result.get()
            except PipeError:
                self.close()
                raise
        self._set_connected()

    def __connect_cb(self, handle, error):
        if error is not None:
            self._connect_result.set_exception(PipeError(error, errno.strerror(error)))
        else:
            self._connect_result.set_value(None)


class PipeServer(StreamServer):
    connection_cls = PipeConnection
    error_cls = PipeError

    def __init__(self):
        super(PipeServer, self).__init__()
        loop = evergreen.current.loop
        self._handle = pyuv.Pipe(loop._loop)
        self._name = None

    @property
    def pipename(self):
        self._check_closed()
        return self._name

    def _bind(self, name):
        self._handle.bind(name)
        self._name = name

    def _serve(self, backlog):
        self._handle.listen(self.__listen_cb, backlog)

    def _close(self):
        self._handle.close()

    def __listen_cb(self, handle, error):
        if error is not None:
            log.debug('listen failed: %d %s', error, errno.strerror(error))
            return
        pipe_handle = pyuv.Pipe(self._handle.loop)
        try:
            self._handle.accept(pipe_handle)
        except PipeError as e:
            log.debug('accept failed: %d %s', e.args[0], e.args[1])
            pipe_handle.close()
        else:
            conn = self.connection_cls(pipe_handle)
            conn._set_accepted(self)
            self.handle_connection(conn)


########NEW FILE########
__FILENAME__ = stream
#
# This file is part of Evergreen. See the NOTICE for more information.
#

import abc
import pyuv
import re
import six
import socket

import evergreen
from evergreen.core.utils import Result
from evergreen.event import Event
from evergreen.io import errno
from evergreen.io.util import StringBuffer
from evergreen.log import log

__all__ = ['AbstractBaseStream', 'BaseStream', 'StreamConnection', 'StreamServer', 'StreamError']


StreamError = pyuv.error.StreamError


class AbstractBaseStream(six.with_metaclass(abc.ABCMeta)):
    """Abstract base class for a stream-like object
    """

    error_cls = None  # to be defined by subclass

    MAX_BUFFER_SIZE = 100*1024*1024
    READ_CHUNK_SIZE = 4*1024

    def __init__(self):
        self._read_buffer = StringBuffer(self.MAX_BUFFER_SIZE)
        self._write_buffer = []

        self._connected = False
        self._closed = False

    def read_bytes(self, nbytes):
        assert isinstance(nbytes, six.integer_types)
        assert nbytes > 0
        return self._do_read(nbytes=nbytes)

    def read_until(self, delimiter):
        return self._do_read(delimiter=delimiter)

    def read_until_regex(self, regex):
        return self._do_read(regex=re.compile(regex))

    def write(self, data):
        self._check_closed()
        if not self._connected:
            self._write_buffer.append(data)
            return False
        else:
            return self._write(data)

    def shutdown(self):
        self._check_closed()
        self._shutdown()

    def close(self):
        if self._closed:
            return
        self._read_buffer.clear()
        self._write_buffer = []
        self._close()
        self._closed = True

    @property
    def closed(self):
        return self._closed

    def _set_connected(self):
        self._connected = True
        buf, self._write_buffer = self._write_buffer, []
        if buf:
            self._write(b''.join(buf))

    # internal

    def _do_read(self, delimiter=None, nbytes=None, regex=None):
        # See if we've already got the data from a previous read
        data = self._read_from_buffer(delimiter, nbytes, regex)
        if data is not None:
            return data
        self._check_closed()
        while not self.closed:
            self._read(self.READ_CHUNK_SIZE)
            data = self._read_from_buffer(delimiter, nbytes, regex)
            if data is not None:
                return data
        return b''

    def _read_from_buffer(self, delimiter=None, nbytes=None, regex=None):
        if nbytes is not None:
            return self._read_buffer.read(nbytes)
        elif delimiter is not None:
            return self._read_buffer.read_until(delimiter)
        elif regex is not None:
            return self._read_buffer.read_until_regex(regex)

    def _check_closed(self):
        if self._closed:
            raise self.error_cls('stream is closed')

    # internal, to be implemented by subclasses

    @abc.abstractmethod
    def _read(self, n):
        raise NotImplementedError

    @abc.abstractmethod
    def _write(self, data):
        raise NotImplementedError

    @abc.abstractmethod
    def _shutdown(self):
        raise NotImplementedError

    @abc.abstractmethod
    def _close(self):
        raise NotImplementedError


class BaseStream(AbstractBaseStream):
    """Base class for streams implemented using pyuv Stream objects as the underlying mechanism
    """

    def __init__(self):
        super(BaseStream, self).__init__()
        self._read_result = Result()
        self._shutdown_result = Result()
        self._pending_writes = 0
        self._flush_event = Event()
        self._flush_event.set()

    def flush(self):
        self._check_closed()
        self._flush_event.wait()

    def _read(self, n):
        with self._read_result:
            try:
                self._handle.start_read(self.__read_cb)
            except self.error_cls:
                self.close()
                raise
            try:
                data = self._read_result.get()
            except self.error_cls as e:
                self.close()
                if e.args[0] != errno.EOF:
                    raise
            else:
                self._read_buffer.feed(data)

    def _write(self, data):
        try:
            self._handle.write(data, self.__write_cb)
        except self.error_cls:
            self.close()
            raise
        if self._pending_writes == 0:
            self._flush_event.clear()
        self._pending_writes += 1
        return self._handle.write_queue_size == 0

    def _shutdown(self):
        with self._shutdown_result:
            self._handle.shutdown(self.__shutdown_cb)
            self._shutdown_result.get()

    def _close(self):
        self._handle.close()

    def __read_cb(self, handle, data, error):
        self._handle.stop_read()
        if error is not None:
            self._read_result.set_exception(self.error_cls(error, pyuv.errno.strerror(error)))
        else:
            self._read_result.set_value(data)

    def __write_cb(self, handle, error):
        self._pending_writes -= 1
        if self._pending_writes == 0:
            self._flush_event.set()
        if error is not None:
            log.debug('write failed: %d %s', error, pyuv.errno.strerror(error))
            evergreen.current.loop.call_soon(self.close)

    def __shutdown_cb(self, handle, error):
        if error is not None:
            self._shutdown_result.set_exception(self.error_cls(error, pyuv.errno.strerror(error)))
        else:
            self._shutdown_result.set_value(None)


class StreamConnection(BaseStream):

    @property
    def server(self):
        return self._server

    def close(self):
        super(StreamConnection, self).close()
        if self._server:
            self._server.connections.remove(self)
            self._server = None

    def _set_accepted(self, server):
        # To be called by the server
        self._server = server
        self._server.connections.append(self)
        self._set_connected()


class StreamServer(object):
    error_cls = None  # to be defined by subclass

    def __init__(self):
        self._end_event = Event()
        self._closed = False
        self.connections = []

    def handle_connection(self, connection):
        raise NotImplementedError

    def bind(self, address):
        self._check_closed()
        self._bind(address)

    def serve(self, backlog=None):
        self._check_closed()
        backlog = backlog or getattr(socket, 'SOMAXCONN', 128)
        self._serve(backlog)
        self._end_event.wait()

    def close(self):
        if not self._closed:
            self._close()
            self._closed = True
            for conn in self.connections[:]:
                conn.close()
            self._end_event.set()

    def _check_closed(self):
        if self._closed:
            raise self.error_cls('server is closed')

    def _bind(self, address):
        raise NotImplementedError

    def _serve(self, backlog):
        raise NotImplementedError

    def _close(self):
        raise NotImplementedError


########NEW FILE########
__FILENAME__ = tcp
#
# This file is part of Evergreen. See the NOTICE for more information.
#

import pyuv

import evergreen
from evergreen.core.utils import Result
from evergreen.io import errno
from evergreen.io.stream import BaseStream, StreamConnection, StreamServer
from evergreen.lib import socket
from evergreen.log import log

__all__ = ['TCPServer', 'TCPClient', 'TCPConnection', 'TCPError']


TCPError = pyuv.error.TCPError


class TCPStream(BaseStream):
    error_cls = TCPError

    def __init__(self, handle):
        super(TCPStream, self).__init__()
        self._handle = handle
        self._sockname = None
        self._peername = None

    @property
    def sockname(self):
        self._check_closed()
        if self._sockname is None:
            self._sockname = self._handle.getsockname()
        return self._sockname

    @property
    def peername(self):
        self._check_closed()
        if self._peername is None:
            self._peername = self._handle.getpeername()
        return self._peername


class TCPClient(TCPStream):

    def __init__(self):
        loop = evergreen.current.loop
        handle = pyuv.TCP(loop._loop)
        super(TCPClient, self).__init__(handle)
        self._connect_result = Result()

    def connect(self, target, source_address=None):
        if self._connected:
            raise TCPError('already connected')
        host, port = target
        try:
            r = socket.getaddrinfo(host, port, socket.AF_UNSPEC, socket.SOCK_STREAM)
        except socket.error as e:
            raise TCPError(e)
        if not r:
            raise TCPError('getaddrinfo returned no result')

        err = None
        loop = self._handle.loop
        for item in r:
            with self._connect_result:
                addr = item[-1]
                idx = addr[0].find('%')
                if idx != -1:
                    host, rest = addr[0], addr[1:]
                    addr = (host[:idx],) + rest
                handle = pyuv.TCP(loop)
                try:
                    if source_address:
                        handle.bind(source_address)
                    handle.connect(addr, self.__connect_cb)
                except TCPError as e:
                    err = e
                    handle.close()
                    continue
                try:
                    self._connect_result.get()
                except TCPError as e:
                    err = e
                    handle.close()
                    continue
                else:
                    self._handle.close()
                    self._handle = handle
                    break
        if err is not None:
            raise err
        self._set_connected()

    def __connect_cb(self, handle, error):
        if error is not None:
            self._connect_result.set_exception(TCPError(error, errno.strerror(error)))
        else:
            self._connect_result.set_value(None)


class TCPConnection(TCPStream, StreamConnection):
    pass


class TCPServer(StreamServer):
    connection_cls = TCPConnection
    error_cls = TCPError

    def __init__(self):
        super(TCPServer, self).__init__()
        loop = evergreen.current.loop
        self._handle = pyuv.TCP(loop._loop)

    @property
    def sockname(self):
        self._check_closed()
        return self._handle.getsockname()

    def _bind(self, address):
        self._handle.bind(address)

    def _serve(self, backlog):
        self._handle.listen(self.__listen_cb, backlog)

    def _close(self):
        self._handle.close()

    def __listen_cb(self, handle, error):
        if error is not None:
            log.debug('listen failed: %d %s', error, errno.strerror(error))
            return
        tcp_handle = pyuv.TCP(self._handle.loop)
        try:
            self._handle.accept(tcp_handle)
        except TCPError as e:
            log.debug('accept failed: %d %s', e.args[0], e.args[1])
            tcp_handle.close()
        else:
            conn = self.connection_cls(tcp_handle)
            conn._set_accepted(self)
            self.handle_connection(conn)


########NEW FILE########
__FILENAME__ = tty
#
# This file is part of Evergreen. See the NOTICE for more information.
#

import atexit
import os
import pyuv
import sys

import evergreen
from evergreen.io.stream import BaseStream

__all__ = ['TTYStream', 'TTYError', 'StdinStream', 'StdoutStream', 'StderrStream']


# Reset terminal settings on program exit
atexit.register(pyuv.TTY.reset_mode)


TTYError = pyuv.error.TTYError


class TTYStream(BaseStream):
    error_class = TTYError

    def __init__(self, fd, readable):
        super(TTYStream, self).__init__()
        loop = evergreen.current.loop
        self._handle = pyuv.TTY(loop._loop, fd, readable)
        self._set_connected()

    @property
    def winsize(self):
        return self._handle.get_winsize()

    def set_raw_mode(self, raw):
        self._handle.set_mode(raw)


def StdinStream(fd=None):
    if not fd:
        fd = os.dup(sys.stdin.fileno())
    return TTYStream(fd, True)


def StdoutStream(fd=None):
    if not fd:
        fd = os.dup(sys.stdout.fileno())
    return TTYStream(fd, False)


def StderrStream(fd=None):
    if not fd:
        fd = os.dup(sys.stderr.fileno())
    return TTYStream(fd, False)


########NEW FILE########
__FILENAME__ = udp
#
# This file is part of Evergreen. See the NOTICE for more information.
#

import pyuv

import evergreen
from evergreen.core.utils import Result
from evergreen.event import Event
from evergreen.io import errno
from evergreen.log import log

__all__ = ['UDPEndpoint', 'UDPError']


UDPError = pyuv.error.UDPError


class UDPEndpoint(object):

    def __init__(self):
        loop = evergreen.current.loop
        self._handle = pyuv.UDP(loop._loop)
        self._closed = False
        self._receive_result = Result()
        self._pending_writes = 0
        self._flush_event = Event()
        self._flush_event.set()
        self._sockname = None

    @property
    def sockname(self):
        self._check_closed()
        if self._sockname is None:
            self._sockname = self._handle.getsockname()
        return self._sockname

    def bind(self, addr):
        self._check_closed()
        self._handle.bind(addr)

    def send(self, data, addr):
        self._check_closed()
        self._handle.send(addr, data, self.__send_cb)
        if self._pending_writes == 0:
            self._flush_event.clear()
        self._pending_writes += 1

    def receive(self):
        self._check_closed()
        with self._receive_result:
            self._handle.start_recv(self.__receive_cb)
            return self._receive_result.get()

    def flush(self):
        self._check_closed()
        self._flush_event.wait()

    def close(self):
        if self._closed:
            return
        self._closed = True
        self._handle.close()

    def _check_closed(self):
        if self._closed:
            raise UDPError('endpoint is closed')

    def __send_cb(self, handle, error):
        self._pending_writes -= 1
        if self._pending_writes == 0:
            self._flush_event.set()
        if error is not None:
            log.debug('send failed: %d %s', error, pyuv.errno.strerror(error))
            evergreen.current.loop.call_soon(self.close)

    def __receive_cb(self, handle, addr, flags, data, error):
        self._handle.stop_recv()
        if error is not None:
            self._receive_result.set_exception(UDPError(error, errno.strerror(error)))
        else:
            self._receive_result.set_value((data, addr))


########NEW FILE########
__FILENAME__ = util
#
# This file is part of Evergreen. See the NOTICE for more information.
#

from collections import deque

__all__ = ['StringBuffer']


class StringBuffer(object):

    def __init__(self, max_size=100*1024*1024):
        self._max_size = max_size
        self._buf = deque()
        self._size = 0
        self._closed = False

    @property
    def closed(self):
        return self._closed

    def read(self, nbytes):
        self._check_closed()
        if self._size >= nbytes:
            return self._consume(nbytes)
        return None

    def read_until(self, delimiter):
        # Multi-byte delimiters (e.g. '\r\n') may straddle two
        # chunks in the read buffer, so we can't easily find them
        # without collapsing the buffer. However, since protocols
        # using delimited reads (as opposed to reads of a known
        # length) tend to be "line" oriented, the delimiter is likely
        # to be in the first few chunks. Merge the buffer gradually
        # since large merges are relatively expensive and get undone in
        # consume().
        self._check_closed()
        if self._buf:
            while True:
                loc = self._buf[0].find(delimiter)
                if loc != -1:
                    delimiter_len = len(delimiter)
                    return self._consume(loc + delimiter_len)
                if len(self._buf) == 1:
                    break
                self._double_prefix()
        return None

    def read_until_regex(self, regex):
        # regex must be a compiled re object
        self._check_closed()
        if self._buf:
            while True:
                m = regex.search(self._buf[0])
                if m is not None:
                    return self._consume(m.end())
                if len(self._buf) == 1:
                    break
                self._double_prefix()
        return None

    def feed(self, chunk):
        self._check_closed()
        self._buf.append(chunk)
        self._size += len(chunk)
        if self._size >= self._max_size:
            self.close()
            raise IOError('Maximum buffer size reached')

    def clear(self):
        self._check_closed()
        self._buf.clear()
        self._size = 0

    def close(self):
        if not self._closed:
            self._closed = True
            self._buf.clear()
            self._size = 0

    # internal

    def _check_closed(self):
        if self._closed:
            raise ValueError('I/O operation on closed buffer')

    def _consume(self, loc):
        if loc == 0:
            return b''
        self._merge_prefix(loc)
        self._size -= loc
        return self._buf.popleft()

    def _double_prefix(self):
        """Grow the given deque by doubling, but don't split the second chunk just
        because the first one is small.
        """
        new_len = max(len(self._buf[0]) * 2, (len(self._buf[0]) + len(self._buf[1])))
        self._merge_prefix(new_len)

    def _merge_prefix(self, size):
        """Replace the first entries in a deque of strings with a single
        string of up to size bytes.

        >>> d = collections.deque(['abc', 'de', 'fghi', 'j'])
        >>> _merge_prefix(d, 5); print(d)
        deque(['abcde', 'fghi', 'j'])

        Strings will be split as necessary to reach the desired size.
        >>> _merge_prefix(d, 7); print(d)
        deque(['abcdefg', 'hi', 'j'])

        >>> _merge_prefix(d, 3); print(d)
        deque(['abc', 'defg', 'hi', 'j'])

        >>> _merge_prefix(d, 100); print(d)
        deque(['abcdefghij'])
        """
        if len(self._buf) == 1 and len(self._buf[0]) <= size:
            return
        prefix = []
        remaining = size
        while self._buf and remaining > 0:
            chunk = self._buf.popleft()
            if len(chunk) > remaining:
                self._buf.appendleft(chunk[remaining:])
                chunk = chunk[:remaining]
            prefix.append(chunk)
            remaining -= len(chunk)
        if prefix:
            self._buf.appendleft(b''.join(prefix))
        if not self._buf:
            self._buf.appendleft(b'')



########NEW FILE########
__FILENAME__ = select
#
# This file is part of Evergreen. See the NOTICE for more information.
#

from __future__ import absolute_import

import six

import evergreen
from evergreen.event import Event

import select as __select__
__all__     = ['select', 'error']
__patched__ = ['select']

error = __select__.error


def get_fileno(obj):
    # The purpose of this function is to exactly replicate
    # the behavior of the select module when confronted with
    # abnormal filenos; the details are extensively tested in
    # the stdlib test/test_select.py.
    try:
        f = obj.fileno
    except AttributeError:
        if not isinstance(obj, six.integer_types):
            raise TypeError("Expected int or long, got " + type(obj))
        return obj
    else:
        rv = f()
        if not isinstance(rv, six.integer_types):
            raise TypeError("Expected int or long, got " + type(rv))
        return rv


class SelectHelper(object):

    def __init__(self):
        self.loop = evergreen.current.loop
        self._read_fds = []
        self._write_fds = []
        self._event = Event()
        self.rlist = []
        self.wlist = []

    def add_reader(self, fdobj):
        fd = get_fileno(fdobj)
        self._read_fds.append(fd)
        self.loop.add_reader(fd, self._on_read, fdobj)

    def add_writer(self, fdobj):
        fd = get_fileno(fdobj)
        self._write_fds.append(fd)
        self.loop.add_writer(fd, self._on_write, fdobj)

    def wait(self, timeout):
        self._event.wait(timeout)

    def close(self):
        for fd in self._read_fds:
            self.loop.remove_reader(fd)
        for fd in self._write_fds:
            self.loop.remove_writer(fd)

    def _on_read(self, fdobj):
        self.rlist.append(fdobj)
        self._event.set()

    def _on_write(self, fdobj):
        self.wlist.append(fdobj)
        self._event.set()


def select(read_list, write_list, error_list, timeout=None):
    if timeout is not None:
        # error checking like this is required by the stdlib unit tests
        try:
            timeout = float(timeout)
        except ValueError:
            raise TypeError("Expected number for timeout")

    helper = SelectHelper()
    for fdobj in read_list:
        helper.add_reader(fdobj)
    for fdobj in write_list:
        helper.add_writer(fdobj)

    try:
        helper.wait(timeout)
        return helper.rlist, helper.wlist, []
    finally:
        helper.close()


########NEW FILE########
__FILENAME__ = socket
#
# This file is part of Evergreen. See the NOTICE for more information.
#

from __future__ import absolute_import

import os
import _socket
import sys
import warnings

import six
import evergreen

from evergreen.event import Event
from evergreen.patcher import slurp_properties
from evergreen.timeout import Timeout

is_windows = sys.platform == 'win32'

if is_windows:
    from errno import WSAEINVAL as EINVAL
    from errno import WSAEWOULDBLOCK as EWOULDBLOCK
    from errno import WSAEINPROGRESS as EINPROGRESS
    from errno import WSAEALREADY as EALREADY
    from errno import WSAEISCONN as EISCONN
    EAGAIN = EWOULDBLOCK
else:
    from errno import EINVAL
    from errno import EWOULDBLOCK
    from errno import EINPROGRESS
    from errno import EALREADY
    from errno import EISCONN
    from errno import EAGAIN
from errno import EBADF

import socket as __socket__
__all__     = __socket__.__all__
__patched__ = ['fromfd', 'socketpair', 'ssl', 'socket', 'SocketType',
               'gethostbyname', 'gethostbyname_ex', 'getnameinfo', 'getaddrinfo',
               'create_connection',]

slurp_properties(__socket__, globals(), ignore=__patched__, srckeys=dir(__socket__))
del slurp_properties

if six.PY3:
    from socket import socket as __socket__socket__

    # for ssl.py to create weakref
    class _realsocket(_socket.socket):
        pass

    class _fileobject:
        def __init__(self, sock, mode='rwb', bufsize=-1, close=False):
            super().__init__()
            self._sock = sock
            self._close = close
            self._obj = __socket__socket__.makefile(sock, mode, bufsize)

        @property
        def closed(self):
            return self._obj.closed

        def __del__(self):
            try:
                self.close()
            except:
                pass

        def close(self):
            try:
                if self._obj is not None:
                    self._obj.close()
                if self._sock is not None and self._close:
                    self._sock.close()
            finally:
                self._sock = self._obj = None

        for _name in ['fileno', 'flush', 'isatty', 'readable', 'readline',
                      'readlines', 'seek', 'seekable', 'tell', 'truncate',
                      'writable', 'writelines', 'read', 'write', 'readinto',
                      'readall']:
            exec('''def %s(self, *args, **kwargs):
    return getattr(self._obj, '%s')(*args, **kwargs)
''' % (_name, _name))
        del _name
else:
    _fileobject = __socket__._fileobject
    _realsocket = _socket.socket


try:
    _GLOBAL_DEFAULT_TIMEOUT = __socket__._GLOBAL_DEFAULT_TIMEOUT
except AttributeError:
    _GLOBAL_DEFAULT_TIMEOUT = object()


def _get_memory(string, offset):
    try:
        return memoryview(string)[offset:]
    except TypeError:
        return buffer(string, offset)


class _closedsocket(object):
    __slots__ = []

    def _dummy(*args, **kwargs):
        raise error(EBADF, 'Bad file descriptor')
    # All _delegate_methods must also be initialized here.
    send = recv = recv_into = sendto = recvfrom = recvfrom_into = _dummy
    __getattr__ = _dummy


cancel_wait_ex = error(EBADF, 'File descriptor was closed by another task')


class IOHandler(object):

    def __init__(self, fd):
        self.fd = fd
        self._read_closed = False
        self._write_closed = False
        self._read_event = Event()
        self._write_event = Event()

    def wait_read(self, timeout=None, timeout_exc=None):
        if self._read_closed:
            raise cancel_wait_ex
        self._read_event.clear()
        loop = evergreen.current.loop
        loop.add_reader(self.fd, self._read_event.set)
        try:
            self._wait(self._read_event, timeout, timeout_exc)
            if self._read_closed:
                raise cancel_wait_ex
        finally:
            loop.remove_reader(self.fd)

    def wait_write(self, timeout=None, timeout_exc=None):
        if self._write_closed:
            raise cancel_wait_ex
        self._write_event.clear()
        loop = evergreen.current.loop
        loop.add_writer(self.fd, self._write_event.set)
        try:
            self._wait(self._write_event, timeout, timeout_exc)
            if self._write_closed:
                raise cancel_wait_ex
        finally:
            loop.remove_writer(self.fd)

    def close(self, read=True, write=True):
        if read:
            self._read_closed = True
            self._read_event.set()
        if write:
            self._write_closed = True
            self._write_event.set()

    def _wait(self, event, timeout, timeout_exc):
            r = event.wait(timeout)
            if not r and timeout_exc:
                raise timeout_exc

    def __repr__(self):
        return '<%s fd=%d>' % (self.__class__.__name__, self.fd)


class socket(object):

    def __init__(self, family=AF_INET, type=SOCK_STREAM, proto=0, _sock=None):
        if _sock is None:
            self._sock = _realsocket(family, type, proto)
            self.timeout = _socket.getdefaulttimeout()
        else:
            if hasattr(_sock, '_sock'):
                self._sock = _sock._sock
                self.timeout = getattr(_sock, 'timeout', False)
                if self.timeout is False:
                    self.timeout = _socket.getdefaulttimeout()
            else:
                self._sock = _sock
                self.timeout = _socket.getdefaulttimeout()
        self._io_refs = 0   # for Python 3
        self._sock.setblocking(0)
        self._io = IOHandler(self._sock.fileno())
        self._closed = False

    def _decref_socketios(self):
        if self._io_refs > 0:
            self._io_refs -= 1
        if self._closed:
            self.close()

    def __repr__(self):
        return '<%s at %s %s>' % (type(self).__name__, hex(id(self)), self._formatinfo())

    def __str__(self):
        return '<%s %s>' % (type(self).__name__, self._formatinfo())

    def _formatinfo(self):
        try:
            fileno = self.fileno()
        except Exception:
            fileno = str(sys.exc_info()[1])
        try:
            sockname = self.getsockname()
            sockname = '%s:%s' % sockname
        except Exception:
            sockname = None
        try:
            peername = self.getpeername()
            peername = '%s:%s' % peername
        except Exception:
            peername = None
        result = 'fileno=%s' % fileno
        if sockname is not None:
            result += ' sock=' + str(sockname)
        if peername is not None:
            result += ' peer=' + str(peername)
        if getattr(self, 'timeout', None) is not None:
            result += ' timeout=' + str(self.timeout)
        return result

    def accept(self):
        sock = self._sock
        while True:
            try:
                if six.PY3:
                    fd, address = sock._accept()
                    client_socket = _realsocket(self.family, self.type, self.proto, fileno=fd)
                else:
                    client_socket, address = sock.accept()
                break
            except error:
                ex = sys.exc_info()[1]
                if ex.args[0] != EWOULDBLOCK or self.timeout == 0.0:
                    raise
                del ex
                six.exc_clear()
            self._io.wait_read(timeout=self.timeout, timeout_exc=timeout('timed out'))
        return socket(_sock=client_socket), address

    def close(self):
        # This function should not reference any globals. See Python issue #808164.
        self._closed = True
        if self._io_refs <= 0:
            self._real_close()

    def _real_close(self, _closedsocket=_closedsocket):
        self._io.close()
        try:
            if six.PY3:
                self._sock.close()
        finally:
            self._sock = _closedsocket()

    @property
    def closed(self):
        return isinstance(self._sock, _closedsocket)

    def connect(self, address):
        if self.timeout == 0.0:
            return self._sock.connect(address)
        sock = self._sock
        if isinstance(address, tuple):
            r = getaddrinfo(address[0], address[1], sock.family, sock.type, sock.proto)
            address = r[0][-1]
        timer = Timeout(self.timeout, timeout('timed out'))
        timer.start()
        try:
            while True:
                err = sock.getsockopt(SOL_SOCKET, SO_ERROR)
                if err:
                    raise error(err, os.strerror(err))
                result = sock.connect_ex(address)
                if not result or result == EISCONN:
                    break
                elif (result in (EWOULDBLOCK, EINPROGRESS, EALREADY)) or (result == EINVAL and is_windows):
                    self._io.wait_write()
                else:
                    raise error(result, os.strerror(result))
        finally:
            timer.cancel()

    def connect_ex(self, address):
        try:
            return self.connect(address) or 0
        except timeout:
            return EAGAIN
        except error:
            ex = sys.exc_info()[1]
            if type(ex) is error:
                return ex.args[0]
            else:
                raise  # gaierror is not silented by connect_ex

    def dup(self):
        """dup() -> socket object

        Return a new socket object connected to the same system resource.
        Note, that the new socket does not inherit the timeout."""
        return socket(_sock=self._sock)

    def makefile(self, mode='rwb', bufsize=-1):
        # Two things to look out for:
        # 1) Closing the original socket object should not close the
        #    socket (hence creating a new instance)
        # 2) The resulting fileobject must keep the timeout in order
        #    to be compatible with the stdlib's socket.makefile.
        if six.PY3:
            sock = self
        else:
            sock = type(self)(_sock=self)
        return _fileobject(sock, mode, bufsize)

    def recv(self, *args):
        sock = self._sock  # keeping the reference so that fd is not closed during waiting
        while True:
            try:
                return sock.recv(*args)
            except error:
                ex = sys.exc_info()[1]
                if ex.args[0] != EWOULDBLOCK or self.timeout == 0.0:
                    raise
                del ex
                six.exc_clear()
            self._io.wait_read(timeout=self.timeout, timeout_exc=timeout('timed out'))

    def recvfrom(self, *args):
        sock = self._sock
        while True:
            try:
                return sock.recvfrom(*args)
            except error:
                ex = sys.exc_info()[1]
                if ex.args[0] != EWOULDBLOCK or self.timeout == 0.0:
                    raise
                del ex
                six.exc_clear()
            self._io.wait_read(timeout=self.timeout, timeout_exc=timeout('timed out'))

    def recvfrom_into(self, *args):
        sock = self._sock
        while True:
            try:
                return sock.recvfrom_into(*args)
            except error:
                ex = sys.exc_info()[1]
                if ex.args[0] != EWOULDBLOCK or self.timeout == 0.0:
                    raise
                del ex
                six.exc_clear()
            self._io.wait_read(timeout=self.timeout, timeout_exc=timeout('timed out'))

    def recv_into(self, *args):
        sock = self._sock
        while True:
            try:
                return sock.recv_into(*args)
            except error:
                ex = sys.exc_info()[1]
                if ex.args[0] != EWOULDBLOCK or self.timeout == 0.0:
                    raise
                del ex
                six.exc_clear()
            self._io.wait_read(timeout=self.timeout, timeout_exc=timeout('timed out'))

    def send(self, data, flags=0):
        sock = self._sock
        try:
            return sock.send(data, flags)
        except error:
            ex = sys.exc_info()[1]
            if ex.args[0] != EWOULDBLOCK or self.timeout == 0.0:
                raise
            del ex
            six.exc_clear()
            self._io.wait_write(timeout=self.timeout, timeout_exc=timeout('timed out'))
            try:
                return sock.send(data, flags)
            except error:
                ex = sys.exc_info()[1]
                if ex.args[0] == EWOULDBLOCK:
                    return 0
                raise

    def sendall(self, data, flags=0):
        if isinstance(data, six.text_type):
            data = data.encode()
        if self.timeout is None:
            data_sent = 0
            while data_sent < len(data):
                data_sent += self.send(_get_memory(data, data_sent), flags)
        else:
            timer = Timeout(self.timeout, timeout('timed out'))
            timer.start()
            try:
                data_sent = 0
                while True:
                    data_sent += self.send(_get_memory(data, data_sent), flags)
                    if data_sent >= len(data):
                        break
            finally:
                timer.cancel()

    def sendto(self, *args):
        sock = self._sock
        try:
            return sock.sendto(*args)
        except error:
            ex = sys.exc_info()[1]
            if ex.args[0] != EWOULDBLOCK or timeout == 0.0:
                raise
            del ex
            six.exc_clear()
            self._io.wait_write(timeout=self.timeout, timeout_exc=timeout('timed out'))
            try:
                return sock.sendto(*args)
            except error:
                ex = sys.exc_info()[1]
                if ex.args[0] == EWOULDBLOCK:
                    return 0
                raise

    def setblocking(self, flag):
        if flag:
            self.timeout = None
        else:
            self.timeout = 0.0

    def settimeout(self, howlong):
        if howlong is not None:
            try:
                f = howlong.__float__
            except AttributeError:
                raise TypeError('a float is required')
            howlong = f()
            if howlong < 0.0:
                raise ValueError('Timeout value out of range')
        self.timeout = howlong

    def gettimeout(self):
        return self.timeout

    def shutdown(self, how):
        if how == 0:  # SHUT_RD
            self._io.close(read=True, write=False)
        elif how == 1:  # SHUT_WR
            self._io.close(read=False, write=True)
        else:
            self._io.close()
        self._sock.shutdown(how)

    family = property(lambda self: self._sock.family, doc="the socket family")
    type = property(lambda self: self._sock.type, doc="the socket type")
    proto = property(lambda self: self._sock.proto, doc="the socket protocol")

    # delegate the functions that we haven't implemented to the real socket object

    _s = ("def %s(self, *args): return self._sock.%s(*args)\n\n"
          "%s.__doc__ = _realsocket.%s.__doc__\n")
    for _m in set(('bind',
                   'connect',
                   'connect_ex',
                   'fileno',
                   'listen',
                   'getpeername',
                   'getsockname',
                   'getsockopt',
                   'setsockopt',
                   'sendall',
                   'setblocking',
                   'settimeout',
                   'gettimeout',
                   'shutdown')) - set(locals()):
        exec (_s % (_m, _m, _m, _m))
    del _m, _s

SocketType = socket


def gethostbyname(*args, **kw):
    loop = evergreen.current.loop
    return loop._threadpool.spawn(__socket__.gethostbyname, *args, **kw).get()


def gethostbyname_ex(*args, **kw):
    loop = evergreen.current.loop
    return loop._threadpool.spawn(__socket__.gethostbyname_ex, *args, **kw).get()


def getnameinfo(*args, **kw):
    loop = evergreen.current.loop
    return loop._threadpool.spawn(__socket__.getnameinfo, *args, **kw).get()


def getaddrinfo(*args, **kw):
    loop = evergreen.current.loop
    return loop._threadpool.spawn(__socket__.getaddrinfo, *args, **kw).get()


def create_connection(address, timeout=_GLOBAL_DEFAULT_TIMEOUT, source_address=None):
    """Connect to *address* and return the socket object.

    Convenience function.  Connect to *address* (a 2-tuple ``(host,
    port)``) and return the socket object.  Passing the optional
    *timeout* parameter will set the timeout on the socket instance
    before attempting to connect.  If no *timeout* is supplied, the
    global default timeout setting returned by :func:`getdefaulttimeout`
    is used. If *source_address* is set it must be a tuple of (host, port)
    for the socket to bind as a source address before making the connection.
    An host of '' or port 0 tells the OS to use the default.
    """

    host, port = address
    err = None
    for res in getaddrinfo(host, port, 0 if has_ipv6 else AF_INET, SOCK_STREAM):
        af, socktype, proto, _canonname, sa = res
        sock = None
        try:
            sock = socket(af, socktype, proto)
            if timeout is not _GLOBAL_DEFAULT_TIMEOUT:
                sock.settimeout(timeout)
            if source_address:
                sock.bind(source_address)
            sock.connect(sa)
            return sock
        except error:
            err = sys.exc_info()[1]
            six.exc_clear()
            if sock is not None:
                sock.close()
    if err is not None:
        raise err
    else:
        raise error("getaddrinfo returns an empty list")


try:
    __original_fromfd__ = __socket__.fromfd
    def fromfd(*args):
        return socket(__original_fromfd__(*args))
except AttributeError:
    pass


try:
    __original_socketpair__ = __socket__.socketpair
    def socketpair(*args):
        one, two = __original_socketpair__(*args)
        return socket(one), socket(two)
except AttributeError:
    pass


try:
    from evergreen.lib import ssl as ssl_module
    sslerror = __socket__.sslerror
    __socket__.ssl
    def ssl(sock, certificate=None, private_key=None):
        warnings.warn("socket.ssl() is deprecated.  Use ssl.wrap_socket() instead.", DeprecationWarning, stacklevel=2)
        return ssl_module.sslwrap_simple(sock, private_key, certificate)
except Exception:
    # if the real socket module doesn't have the ssl method or sslerror
    # exception, we can't emulate them
    pass


########NEW FILE########
__FILENAME__ = ssl
#
# This file is part of Evergreen. See the NOTICE for more information.
#

from __future__ import absolute_import

import errno
import six
import sys

from evergreen.lib.socket import socket, _fileobject
from evergreen.lib.socket import error as socket_error, timeout as socket_timeout
from evergreen.patcher import slurp_properties

import ssl as __ssl__
__patched__ = ['SSLSocket', 'wrap_socket', 'socket', 'sslwrap_simple']

slurp_properties(__ssl__, globals(), ignore=__patched__, srckeys=dir(__ssl__))
del slurp_properties

_ssl = __ssl__._ssl


if sys.version_info >= (2, 7):
    ssl_timeout_exc = SSLError
else:
    ssl_timeout_exc = socket_timeout

_SSLErrorReadTimeout = ssl_timeout_exc('The read operation timed out')
_SSLErrorWriteTimeout = ssl_timeout_exc('The write operation timed out')
_SSLErrorHandshakeTimeout = ssl_timeout_exc('The handshake operation timed out')


class SSLSocket(socket):

    def __init__(self, sock, keyfile=None, certfile=None,
                 server_side=False, cert_reqs=CERT_NONE,
                 ssl_version=PROTOCOL_SSLv23, ca_certs=None,
                 do_handshake_on_connect=True,
                 suppress_ragged_eofs=True,
                 ciphers=None, server_hostname=None, _context=None):
        socket.__init__(self, _sock=sock)

        if certfile and not keyfile:
            keyfile = certfile
        # see if it's connected
        try:
            socket.getpeername(self)
        except socket_error as e:
            if e.args[0] != errno.ENOTCONN:
                raise
            # no, no connection yet
            self._sslobj = None
        else:
            # yes, create the SSL object
            if six.PY3:
                self._sslobj = None
                if _context:
                    self.context = _context
                else:
                    if server_side and not certfile:
                        raise ValueError("certfile must be specified for server-side operations")
                    if keyfile and not certfile:
                        raise ValueError("certfile must be specified")
                    if certfile and not keyfile:
                        keyfile = certfile
                    self.context = __ssl__._SSLContext(ssl_version)
                    self.context.verify_mode = cert_reqs
                    if ca_certs:
                        self.context.load_verify_locations(ca_certs)
                    if certfile:
                        self.context.load_cert_chain(certfile, keyfile)
                    if ciphers:
                        self.context.set_ciphers(ciphers)
                if server_side and server_hostname:
                    raise ValueError("server_hostname can only be specified in client mode")
                self.server_hostname = server_hostname
                try:
                    self._sslobj = self.context._wrap_socket(self._sock, server_side, server_hostname)
                except socket_error:
                    self.close()
                    raise
            else:
                if ciphers is None:
                    self._sslobj = _ssl.sslwrap(self._sock, server_side,
                                                keyfile, certfile,
                                                cert_reqs, ssl_version, ca_certs)
                else:
                    self._sslobj = _ssl.sslwrap(self._sock, server_side,
                                                keyfile, certfile,
                                                cert_reqs, ssl_version, ca_certs,
                                                ciphers)
            if do_handshake_on_connect:
                self.do_handshake()
        self.keyfile = keyfile
        self.certfile = certfile
        self.cert_reqs = cert_reqs
        self.ssl_version = ssl_version
        self.ca_certs = ca_certs
        self.ciphers = ciphers
        self.do_handshake_on_connect = do_handshake_on_connect
        self.suppress_ragged_eofs = suppress_ragged_eofs
        self._makefile_refs = 0

    def read(self, len=1024):
        """Read up to LEN bytes and return them.
        Return zero-length string on EOF."""
        while True:
            try:
                return self._sslobj.read(len)
            except SSLError:
                ex = sys.exc_info()[1]
                if ex.args[0] == SSL_ERROR_EOF and self.suppress_ragged_eofs:
                    return b''
                elif ex.args[0] == SSL_ERROR_WANT_READ:
                    if self.timeout == 0.0:
                        raise
                    six.exc_clear()
                    self._io.wait_read(timeout=self.timeout, timeout_exc=_SSLErrorReadTimeout)
                elif ex.args[0] == SSL_ERROR_WANT_WRITE:
                    if self.timeout == 0.0:
                        raise
                    six.exc_clear()
                    self._io.wait_write(timeout=self.timeout, timeout_exc=_SSLErrorReadTimeout)
                else:
                    raise

    def write(self, data):
        """Write DATA to the underlying SSL channel.  Returns
        number of bytes of DATA actually transmitted."""
        while True:
            try:
                return self._sslobj.write(data)
            except SSLError:
                ex = sys.exc_info()[1]
                if ex.args[0] == SSL_ERROR_WANT_READ:
                    if self.timeout == 0.0:
                        raise
                    six.exc_clear()
                    self._io.wait_read(timeout=self.timeout, timeout_exc=_SSLErrorWriteTimeout)
                elif ex.args[0] == SSL_ERROR_WANT_WRITE:
                    if self.timeout == 0.0:
                        raise
                    six.exc_clear()
                    self._io.wait_write(timeout=self.timeout, timeout_exc=_SSLErrorWriteTimeout)
                else:
                    raise

    def getpeercert(self, binary_form=False):
        """Returns a formatted version of the data in the
        certificate provided by the other end of the SSL channel.
        Return None if no certificate was provided, {} if a
        certificate was provided, but not validated."""
        return self._sslobj.peer_certificate(binary_form)

    def cipher(self):
        if not self._sslobj:
            return None
        else:
            return self._sslobj.cipher()

    def send(self, data, flags=0):
        if self._sslobj:
            if flags != 0:
                raise ValueError("non-zero flags not allowed in calls to send() on %s" % self.__class__)
            while True:
                try:
                    v = self._sslobj.write(data)
                except SSLError:
                    x = sys.exc_info()[1]
                    if x.args[0] == SSL_ERROR_WANT_READ:
                        if self.timeout == 0.0:
                            return 0
                        six.exc_clear()
                        self._io.wait_read(timeout=self.timeout, timeout_exc=socket_timeout('timed out'))
                    elif x.args[0] == SSL_ERROR_WANT_WRITE:
                        if self.timeout == 0.0:
                            return 0
                        six.exc_clear()
                        self._io.wait_write(timeout=self.timeout, timeout_exc=socket_timeout('timed out'))
                    else:
                        raise
                else:
                    return v
        else:
            return socket.send(self, data, flags)

    def sendto(self, *args):
        if self._sslobj:
            raise ValueError("sendto not allowed on instances of %s" % self.__class__)
        else:
            return socket.sendto(self, *args)

    def recv(self, buflen=1024, flags=0):
        if self._sslobj:
            if flags != 0:
                raise ValueError("non-zero flags not allowed in calls to recv() on %s" % self.__class__)
            # Shouldn't we wrap the SSL_WANT_READ errors as socket.timeout errors to match socket.recv's behavior?
            return self.read(buflen)
        else:
            return socket.recv(self, buflen, flags)

    def recv_into(self, buffer, nbytes=None, flags=0):
        if buffer and (nbytes is None):
            nbytes = len(buffer)
        elif nbytes is None:
            nbytes = 1024
        if self._sslobj:
            if flags != 0:
                raise ValueError("non-zero flags not allowed in calls to recv_into() on %s" % self.__class__)
            while True:
                try:
                    tmp_buffer = self.read(nbytes)
                    v = len(tmp_buffer)
                    buffer[:v] = tmp_buffer
                    return v
                except SSLError:
                    x = sys.exc_info()[1]
                    if x.args[0] == SSL_ERROR_WANT_READ:
                        if self.timeout == 0.0:
                            raise
                        six.exc_clear()
                        self._io.wait_read(timeout=self.timeout, timeout_exc=socket_timeout('timed out'))
                        continue
                    else:
                        raise
        else:
            return socket.recv_into(self, buffer, nbytes, flags)

    def recvfrom(self, *args):
        if self._sslobj:
            raise ValueError("recvfrom not allowed on instances of %s" % self.__class__)
        else:
            return socket.recvfrom(self, *args)

    def recvfrom_into(self, *args):
        if self._sslobj:
            raise ValueError("recvfrom_into not allowed on instances of %s" % self.__class__)
        else:
            return socket.recvfrom_into(self, *args)

    def pending(self):
        if self._sslobj:
            return self._sslobj.pending()
        else:
            return 0

    def _sslobj_shutdown(self):
        while True:
            try:
                return self._sslobj.shutdown()
            except SSLError:
                ex = sys.exc_info()[1]
                if ex.args[0] == SSL_ERROR_EOF and self.suppress_ragged_eofs:
                    return b''
                elif ex.args[0] == SSL_ERROR_WANT_READ:
                    if self.timeout == 0.0:
                        raise
                    six.exc_clear()
                    self._io.wait_read(timeout=self.timeout, timeout_exc=_SSLErrorReadTimeout)
                elif ex.args[0] == SSL_ERROR_WANT_WRITE:
                    if self.timeout == 0.0:
                        raise
                    six.exc_clear()
                    self._io.wait_write(timeout=self.timeout, timeout_exc=_SSLErrorWriteTimeout)
                else:
                    raise

    def unwrap(self):
        if self._sslobj:
            s = self._sslobj_shutdown()
            self._sslobj = None
            return socket(_sock=s)
        else:
            raise ValueError("No SSL wrapper around " + str(self))

    def shutdown(self, how):
        self._sslobj = None
        socket.shutdown(self, how)

    def close(self):
        if self._makefile_refs < 1:
            self._sslobj = None
            socket.close(self)
        else:
            self._makefile_refs -= 1

    def do_handshake(self):
        """Perform a TLS/SSL handshake."""
        while True:
            try:
                return self._sslobj.do_handshake()
            except SSLError:
                ex = sys.exc_info()[1]
                if ex.args[0] == SSL_ERROR_WANT_READ:
                    if self.timeout == 0.0:
                        raise
                    six.exc_clear()
                    self._io.wait_read(timeout=self.timeout, timeout_exc=_SSLErrorHandshakeTimeout)
                elif ex.args[0] == SSL_ERROR_WANT_WRITE:
                    if self.timeout == 0.0:
                        raise
                    six.exc_clear()
                    self._io.wait_write(timeout=self.timeout, timeout_exc=_SSLErrorHandshakeTimeout)
                else:
                    raise

    def connect(self, addr):
        """Connects to remote ADDR, and then wraps the connection in
        an SSL channel."""
        # Here we assume that the socket is client-side, and not
        # connected at the time of the call.  We connect it, then wrap it.
        if self._sslobj:
            raise ValueError("attempt to connect already-connected SSLSocket!")
        socket.connect(self, addr)
        if six.PY3:
            self._sslobj = self.context._wrap_socket(self._sock, False, self.server_hostname)
        else:
            if self.ciphers is None:
                self._sslobj = _ssl.sslwrap(self._sock, False, self.keyfile, self.certfile,
                                            self.cert_reqs, self.ssl_version,
                                            self.ca_certs)
            else:
                self._sslobj = _ssl.sslwrap(self._sock, False, self.keyfile, self.certfile,
                                            self.cert_reqs, self.ssl_version,
                                            self.ca_certs, self.ciphers)
        if self.do_handshake_on_connect:
            self.do_handshake()

    def accept(self):
        """Accepts a new connection from a remote client, and returns
        a tuple containing that new connection wrapped with a server-side
        SSL channel, and the address of the remote client."""
        newsock, addr = socket.accept(self)
        ssl_sock = SSLSocket(newsock._sock,
                             keyfile=self.keyfile,
                             certfile=self.certfile,
                             server_side=True,
                             cert_reqs=self.cert_reqs,
                             ssl_version=self.ssl_version,
                             ca_certs=self.ca_certs,
                             do_handshake_on_connect=self.do_handshake_on_connect,
                             suppress_ragged_eofs=self.suppress_ragged_eofs,
                             ciphers=self.ciphers)
        return ssl_sock, addr

    def makefile(self, mode='rwb', bufsize=-1):
        """Make and return a file-like object that
        works with the SSL connection.  Just use the code
        from the socket module."""
        self._makefile_refs += 1
        # close=True so as to decrement the reference count when done with
        # the file-like object.
        return _fileobject(self, mode, bufsize, close=True)


def wrap_socket(sock, *a, **kw):
    return SSLSocket(sock, *a, **kw)


if hasattr(__ssl__, 'sslwrap_simple'):
    def sslwrap_simple(sock, keyfile=None, certfile=None):
        """A replacement for the old socket.ssl function.  Designed
        for compability with Python 2.5 and earlier.  Will disappear in
        Python 3.0."""
        ssl_sock = SSLSocket(sock, keyfile=keyfile, certfile=certfile,
                             server_side=False,
                             cert_reqs=CERT_NONE,
                             ssl_version=PROTOCOL_SSLv23,
                             ca_certs=None)
        return ssl_sock
else:
    def sslwrap_simple(sock, keyfile=None, certfile=None):
        raise NotImplementedError


########NEW FILE########
__FILENAME__ = time
#
# This file is part of Evergreen. See the NOTICE for more information.
#

from __future__ import absolute_import

from evergreen import sleep
from evergreen.patcher import slurp_properties

import time as __time__
__patched__ = ['sleep']

slurp_properties(__time__, globals(), ignore=__patched__, srckeys=dir(__time__))
del slurp_properties


########NEW FILE########
__FILENAME__ = local
#
# This file is part of Evergreen. See the NOTICE for more information.
#

import fibers

__all__ = ['local']


def _get_local_dict():
    current = fibers.current()
    s = '_%s__local_dict__' % current.__class__.__name__
    if not hasattr(current, s):
        setattr(current, s, {})
    return getattr(current, s)


class local(object):

    def __getattribute__(self, attr):
        local_dict = _get_local_dict()
        try:
            return local_dict[attr]
        except KeyError:
            raise AttributeError("'local' object has no attribute '%s'" % attr)

    def __setattr__(self, attr, value):
        local_dict = _get_local_dict()
        local_dict[attr] = value

    def __delattr__(self, attr):
        local_dict = _get_local_dict()
        try:
            del local_dict[attr]
        except KeyError:
            raise AttributeError(attr)


########NEW FILE########
__FILENAME__ = locks
#
# This file is part of Evergreen. See the NOTICE for more information.
#

import evergreen
from evergreen.timeout import Timeout

__all__ = ['Semaphore', 'BoundedSemaphore', 'Lock', 'RLock', 'Condition', 'Barrier']


class Semaphore(object):

    def __init__(self, value=1):
        if value < 0:
            raise ValueError("Semaphore must be initialized with a positive number, got %s" % value)
        self._counter = value
        self._waiters = set()

    def acquire(self, blocking=True, timeout=None):
        if self._counter > 0:
            self._counter -= 1
            return True
        elif not blocking:
            return False
        else:
            current = evergreen.current.task
            self._waiters.add(current)
            timer = Timeout(timeout)
            timer.start()
            loop = evergreen.current.loop
            try:
                while self._counter <= 0:
                    loop.switch()
            except Timeout as e:
                if e is timer:
                    return False
                raise
            else:
                self._counter -= 1
                return True
            finally:
                timer.cancel()
                self._waiters.discard(current)

    def release(self):
        self._counter += 1
        if self._waiters:
            evergreen.current.loop.call_soon(self._notify_waiters)

    def _notify_waiters(self):
        if self._waiters and self._counter > 0:
            waiter = self._waiters.pop()
            waiter.switch()

    def __enter__(self):
        self.acquire()

    def __exit__(self, typ, val, tb):
        self.release()


class BoundedSemaphore(Semaphore):

    def __init__(self, value=1):
        super(BoundedSemaphore, self).__init__(value)
        self._initial_counter = value

    def release(self):
        if self._counter >= self._initial_counter:
            raise ValueError("Semaphore released too many times")
        return super(BoundedSemaphore, self).release()


class Lock(Semaphore):

    def __init__(self):
        super(Lock, self).__init__(value=1)


class RLock(object):

    def __init__(self):
        self._block = Semaphore()
        self._count = 0
        self._owner = None

    def acquire(self, blocking=True, timeout=None):
        me = evergreen.current.task
        if self._owner is me:
            self._count += 1
            return True
        r = self._block.acquire(blocking, timeout)
        if r:
            self._owner = me
            self._count = 1
        return r

    def release(self):
        if self._owner is not evergreen.current.task:
            raise RuntimeError('cannot release un-aquired lock')
        self._count = count = self._count - 1
        if not count:
            self._owner = None
            self._block.release()

    def __enter__(self):
        return self.acquire()

    def __exit__(self, typ, value, tb):
        self.release()

    # Needed by condition

    def _acquire_restore(self, state):
        self._block.acquire()
        self._count, self._owner = state

    def _release_save(self):
        state = (self._count, self._owner)
        self._count = 0
        self._owner = None
        self._block.release()
        return state

    def _is_owned(self):
        return self._owner is evergreen.current.task


class Condition(object):

    def __init__(self, lock=None):
        if lock is None:
            lock = RLock()
        self._lock = lock
        self._waiters = []

        # Export the lock's acquire() and release() methods
        self.acquire = lock.acquire
        self.release = lock.release
        # If the lock defines _release_save() and/or _acquire_restore(),
        # these override the default implementations (which just call
        # release() and acquire() on the lock).  Ditto for _is_owned().
        try:
            self._release_save = lock._release_save
        except AttributeError:
            pass
        try:
            self._acquire_restore = lock._acquire_restore
        except AttributeError:
            pass
        try:
            self._is_owned = lock._is_owned
        except AttributeError:
            pass

    def wait(self, timeout=None):
        if not self._is_owned():
            raise RuntimeError('cannot wait on un-acquired lock')
        waiter = Semaphore()
        waiter.acquire()
        self._waiters.append(waiter)
        saved_state = self._release_save()
        try:
            return waiter.acquire(timeout=timeout)
        finally:
            self._acquire_restore(saved_state)

    def wait_for(self, predicate, timeout=None):
        endtime = None
        waittime = timeout
        result = predicate()
        if not result:
            self.wait(waittime)
            result = predicate()
        return result

    def notify(self, n=1):
        if not self._is_owned():
            raise RuntimeError('cannot wait on un-acquired lock')
        _waiters = self._waiters
        waiters = _waiters[:n]
        for waiter in waiters:
            waiter.release()
            try:
                _waiters.remove(waiter)
            except ValueError:
                pass

    def notify_all(self):
        self.notify(len(self._waiters))

    def _acquire_restore(self, state):
        self._lock.acquire()

    def _release_save(self):
        self._lock.release()

    def _is_owned(self):
        # Return True if lock is owned by current_thread.
        # This method is called only if __lock doesn't have _is_owned().
        if self._lock.acquire(False):
            self._lock.release()
            return False
        else:
            return True

    def __enter__(self):
        return self._lock.__enter__()

    def __exit__(self, *args):
        return self._lock.__exit__(*args)


# A barrier class. Inspired in part by the pthread_barrier_* api and
# the CyclicBarrier class from Java. See
# http://sourceware.org/pthreads-win32/manual/pthread_barrier_init.html and
# http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/CyclicBarrier.html
# for information.
#
# We maintain two main states, 'filling' and 'draining' enabling the barrier
# to be cyclic.  Threads are not allowed into it until it has fully drained
# since the previous cycle.  In addition, a 'resetting' state exists which is
# similar to 'draining' except that threads leave with a BrokenBarrierError,
# and a 'broken' state in which all threads get the exception.

class Barrier(object):
    """Implements a Barrier.

    Useful for synchronizing a fixed number of threads at known synchronization
    points.  Threads block on 'wait()' and are simultaneously once they have all
    made that call.

    """

    def __init__(self, parties, action=None, timeout=None):
        """Create a barrier, initialised to 'parties' threads.

        'action' is a callable which, when supplied, will be called by one of
        the threads after they have all entered the barrier and just prior to
        releasing them all. If a 'timeout' is provided, it is uses as the
        default for all subsequent 'wait()' calls.

        """
        self._cond = Condition(Lock())
        self._action = action
        self._timeout = timeout
        self._parties = parties
        self._state = 0   # 0 filling, 1, draining, -1 resetting, -2 broken
        self._count = 0

    def wait(self, timeout=None):
        """Wait for the barrier.

        When the specified number of threads have started waiting, they are all
        simultaneously awoken. If an 'action' was provided for the barrier, one
        of the threads will have executed that callback prior to returning.
        Returns an individual index number from 0 to 'parties-1'.

        """
        if timeout is None:
            timeout = self._timeout
        with self._cond:
            self._enter()    # Block while the barrier drains.
            index = self._count
            self._count += 1
            try:
                if index + 1 == self._parties:
                    # We release the barrier
                    self._release()
                else:
                    # We wait until someone releases us
                    self._wait(timeout)
                return index
            finally:
                self._count -= 1
                # Wake up any threads waiting for barrier to drain.
                self._exit()

    # Block until the barrier is ready for us, or raise an exception
    # if it is broken.
    def _enter(self):
        while self._state in (-1, 1):
            # It is draining or resetting, wait until done
            self._cond.wait()
        #see if the barrier is in a broken state
        if self._state < 0:
            raise BrokenBarrierError
        assert self._state == 0

    # Optionally run the 'action' and release the threads waiting
    # in the barrier.
    def _release(self):
        try:
            if self._action:
                self._action()
            # enter draining state
            self._state = 1
            self._cond.notify_all()
        except:
            #an exception during the _action handler.  Break and reraise
            self._break()
            raise

    # Wait in the barrier until we are relased.  Raise an exception
    # if the barrier is reset or broken.
    def _wait(self, timeout):
        if not self._cond.wait_for(lambda: self._state != 0, timeout):
            #timed out.  Break the barrier
            self._break()
            raise BrokenBarrierError
        if self._state < 0:
            raise BrokenBarrierError
        assert self._state == 1

    # If we are the last thread to exit the barrier, signal any threads
    # waiting for the barrier to drain.
    def _exit(self):
        if self._count == 0:
            if self._state in (-1, 1):
                #resetting or draining
                self._state = 0
                self._cond.notify_all()

    def reset(self):
        """Reset the barrier to the initial state.

        Any threads currently waiting will get the BrokenBarrier exception
        raised.

        """
        with self._cond:
            if self._count > 0:
                if self._state == 0:
                    #reset the barrier, waking up threads
                    self._state = -1
                elif self._state == -2:
                    #was broken, set it to reset state
                    #which clears when the last thread exits
                    self._state = -1
            else:
                self._state = 0
            self._cond.notify_all()

    def abort(self):
        """Place the barrier into a 'broken' state.

        Useful in case of error.  Any currently waiting threads and threads
        attempting to 'wait()' will have BrokenBarrierError raised.

        """
        with self._cond:
            self._break()

    def _break(self):
        # An internal error was detected.  The barrier is set to
        # a broken state all parties awakened.
        self._state = -2
        self._cond.notify_all()

    @property
    def parties(self):
        """Return the number of threads required to trip the barrier."""
        return self._parties

    @property
    def n_waiting(self):
        """Return the number of threads currently waiting at the barrier."""
        # We don't need synchronization here since this is an ephemeral result
        # anyway.  It returns the correct value in the steady state.
        if self._state == 0:
            return self._count
        return 0

    @property
    def broken(self):
        """Return True if the barrier is in a broken state."""
        return self._state == -2


class BrokenBarrierError(RuntimeError):
    pass


########NEW FILE########
__FILENAME__ = log
#
# This file is part of Evergreen. See the NOTICE for more information.
#

import logging


log = logging.getLogger('evergreen')


########NEW FILE########
__FILENAME__ = patcher
#
# This file is part of Evergreen. See the NOTICE for more information.
#

import sys
import imp

__all__ = ['original', 'import_patched', 'patch', 'is_patched']

__exclude = set(('__builtins__', '__file__', '__name__'))


class SysModulesSaver(object):
    """Class that captures some subset of the current state of
    sys.modules.  Pass in an iterator of module names to the
    constructor."""
    def __init__(self, module_names=()):
        self._saved = {}
        imp.acquire_lock()
        self.save(*module_names)

    def save(self, *module_names):
        """Saves the named modules to the object."""
        for modname in module_names:
            self._saved[modname] = sys.modules.get(modname, None)

    def restore(self):
        """Restores the modules that the saver knows about into
        sys.modules.
        """
        try:
            for modname, mod in self._saved.items():
                if mod is not None:
                    sys.modules[modname] = mod
                else:
                    try:
                        del sys.modules[modname]
                    except KeyError:
                        pass
        finally:
            imp.release_lock()


def import_patched(module_name, **additional_modules):
    """Imports a module in a way that ensures that the module uses "green"
    versions of the standard library modules, so that everything works
    nonblockingly.

    The only required argument is the name of the module to be imported.
    """
    return inject(module_name, None, *tuple(additional_modules.items()))


def original(modname):
    """ This returns an unpatched version of a module."""
    # note that it's not necessary to temporarily install unpatched
    # versions of all patchable modules during the import of the
    # module; this is because none of them import each other, except
    # for threading which imports thread
    original_name = '__original_module_' + modname
    if original_name in sys.modules:
        return sys.modules.get(original_name)

    # re-import the "pure" module and store it in the global _originals
    # dict; be sure to restore whatever module had that name already
    saver = SysModulesSaver((modname,))
    sys.modules.pop(modname, None)
    try:
        real_mod = __import__(modname, {}, {}, modname.split('.')[:-1])
        # save a reference to the unpatched module so it doesn't get lost
        sys.modules[original_name] = real_mod
    finally:
        saver.restore()

    return sys.modules[original_name]


already_patched = {}

def patch(**on):
    """Globally patches certain system modules to be 'cooperaive'.

    The keyword arguments afford some control over which modules are patched.
    If no keyword arguments are supplied, all possible modules are patched.
    If keywords are set to True, only the specified modules are patched.  E.g.,
    ``monkey_patch(socket=True, select=True)`` patches only the select and
    socket modules.  Most arguments patch the single module of the same name
    (os, time, select).  The exception is socket, which also patches the ssl
    module if present.

    It's safe to call monkey_patch multiple times.
    """
    accepted_args = set(('select', 'socket', 'time'))
    default_on = on.pop("all", None)
    for k in on.keys():
        if k not in accepted_args:
            raise TypeError("patch() got an unexpected keyword argument %r" % k)
    if default_on is None:
        default_on = not (True in list(on.values()))
    for modname in accepted_args:
        on.setdefault(modname, default_on)

    modules_to_patch = []
    if on['select'] and not already_patched.get('select'):
        modules_to_patch += _select_modules()
        already_patched['select'] = True
    if on['socket'] and not already_patched.get('socket'):
        modules_to_patch += _socket_modules()
        already_patched['socket'] = True
    if on['time'] and not already_patched.get('time'):
        modules_to_patch += _time_modules()
        already_patched['time'] = True

    imp.acquire_lock()
    try:
        for name, mod in modules_to_patch:
            orig_mod = sys.modules.get(name)
            if orig_mod is None:
                orig_mod = __import__(name)
            for attr_name in mod.__patched__:
                patched_attr = getattr(mod, attr_name, None)
                if patched_attr is not None:
                    setattr(orig_mod, attr_name, patched_attr)
    finally:
        imp.release_lock()


def is_patched(module):
    """Returns True if the given module is monkeypatched currently, False if
    not.  *module* can be either the module itself or its name.

    Based entirely off the name of the module, so if you import a
    module some other way than with the import keyword (including
    import_patched), this might not be correct about that particular
    module."""
    return module in already_patched or getattr(module, '__name__', None) in already_patched


# internal

def _select_modules():
    from evergreen.lib import select
    return [('select', select)]

def _socket_modules():
    from evergreen.lib import socket
    try:
        from evergreen.lib import ssl
        return [('socket', socket), ('ssl', ssl)]
    except ImportError:
        return [('socket', socket)]

def _time_modules():
    from evergreen.lib import time
    return [('time', time)]


def inject(module_name, new_globals, *additional_modules):
    """Base method for "injecting" greened modules into an imported module.  It
    imports the module specified in *module_name*, arranging things so
    that the already-imported modules in *additional_modules* are used when
    *module_name* makes its imports.

    *new_globals* is either None or a globals dictionary that gets populated
    with the contents of the *module_name* module.  This is useful when creating
    a "green" version of some other module.

    *additional_modules* should be a collection of two-element tuples, of the
    form (<name>, <module>).  If it's not specified, a default selection of
    name/module pairs is used, which should cover all use cases but may be
    slower because there are inevitably redundant or unnecessary imports.
    """
    patched_name = '__patched_module_' + module_name
    if patched_name in sys.modules:
        # returning already-patched module so as not to destroy existing
        # references to patched modules
        return sys.modules[patched_name]

    if not additional_modules:
        # supply some defaults
        additional_modules = (_select_modules() +
                              _socket_modules() +
                              _time_modules())

    # after this we are gonna screw with sys.modules, so capture the
    # state of all the modules we're going to mess with, and lock
    saver = SysModulesSaver([name for name, m in additional_modules])
    saver.save(module_name)

    # Cover the target modules so that when you import the module it
    # sees only the patched versions
    for name, mod in additional_modules:
        sys.modules[name] = mod

    ## Remove the old module from sys.modules and reimport it while
    ## the specified modules are in place
    sys.modules.pop(module_name, None)
    try:
        module = __import__(module_name, {}, {}, module_name.split('.')[:-1])

        if new_globals is not None:
            ## Update the given globals dictionary with everything from this new module
            for name in dir(module):
                if name not in __exclude:
                    new_globals[name] = getattr(module, name)

        ## Keep a reference to the new module to prevent it from dying
        sys.modules[patched_name] = module
    finally:
        saver.restore()  # Put the original modules back

    return module


def slurp_properties(source, destination, ignore=[], srckeys=None):
    """Copy properties from *source* (assumed to be a module) to
    *destination* (assumed to be a dict).

    *ignore* lists properties that should not be thusly copied.
    *srckeys* is a list of keys to copy, if the source's __all__ is
    untrustworthy.
    """
    if srckeys is None:
        srckeys = source.__all__
    destination.update(dict([(name, getattr(source, name))
                              for name in srckeys
                                if not (name.startswith('__') or name in ignore)
                            ]))


########NEW FILE########
__FILENAME__ = queue
#
# This file is part of Evergreen. See the NOTICE for more information.
#

from __future__ import absolute_import

import heapq

from collections import deque
from six.moves import queue as __queue__

from evergreen.locks import Condition, Lock

__all__ = ['Empty', 'Full', 'Queue', 'PriorityQueue', 'LifoQueue']

Empty = __queue__.Empty
Full = __queue__.Full


class Queue(object):
    """Create a queue object with a given maximum size.

    If maxsize is <= 0, the queue size is infinite.
    """
    def __init__(self, maxsize=0):
        self.maxsize = maxsize
        self._init(maxsize)
        # mutex must be held whenever the queue is mutating.  All methods
        # that acquire mutex must release it before returning.  mutex
        # is shared between the three conditions, so acquiring and
        # releasing the conditions also acquires and releases mutex.
        self.mutex = Lock()
        # Notify not_empty whenever an item is added to the queue; a
        # task waiting to get is notified then.
        self.not_empty = Condition(self.mutex)
        # Notify not_full whenever an item is removed from the queue;
        # a task waiting to put is notified then.
        self.not_full = Condition(self.mutex)
        # Notify all_tasks_done whenever the number of unfinished tasks
        # drops to zero; task waiting to join() is notified to resume
        self.all_tasks_done = Condition(self.mutex)
        self.unfinished_tasks = 0

    def task_done(self):
        """Indicate that a formerly enqueued task is complete.

        Used by Queue consumer tasks.  For each get() used to fetch a task,
        a subsequent call to task_done() tells the queue that the processing
        on the task is complete.

        If a join() is currently blocking, it will resume when all items
        have been processed (meaning that a task_done() call was received
        for every item that had been put() into the queue).

        Raises a ValueError if called more times than there were items
        placed in the queue.
        """
        self.all_tasks_done.acquire()
        try:
            unfinished = self.unfinished_tasks - 1
            if unfinished <= 0:
                if unfinished < 0:
                    raise ValueError('task_done() called too many times')
                self.all_tasks_done.notify_all()
            self.unfinished_tasks = unfinished
        finally:
            self.all_tasks_done.release()

    def join(self):
        """Blocks until all items in the Queue have been gotten and processed.

        The count of unfinished tasks goes up whenever an item is added to the
        queue. The count goes down whenever a consumer task calls task_done()
        to indicate the item was retrieved and all work on it is complete.

        When the count of unfinished tasks drops to zero, join() unblocks.
        """
        self.all_tasks_done.acquire()
        try:
            while self.unfinished_tasks:
                self.all_tasks_done.wait()
        finally:
            self.all_tasks_done.release()

    def qsize(self):
        """Return the approximate size of the queue (not reliable!)."""
        self.mutex.acquire()
        n = self._qsize()
        self.mutex.release()
        return n

    def empty(self):
        """Return True if the queue is empty, False otherwise (not reliable!).

        This method is likely to be removed at some point.  Use qsize() == 0
        as a direct substitute, but be aware that either approach risks a race
        condition where a queue can grow before the result of empty() or
        qsize() can be used.

        To create code that needs to wait for all queued tasks to be
        completed, the preferred technique is to use the join() method.

        """
        self.mutex.acquire()
        n = not self._qsize()
        self.mutex.release()
        return n

    def full(self):
        """Return True if the queue is full, False otherwise (not reliable!).

        This method is likely to be removed at some point.  Use qsize() >= n
        as a direct substitute, but be aware that either approach risks a race
        condition where a queue can shrink before the result of full() or
        qsize() can be used.

        """
        self.mutex.acquire()
        n = 0 < self.maxsize <= self._qsize()
        self.mutex.release()
        return n

    def put(self, item, block=True, timeout=None):
        """Put an item into the queue.

        If optional args 'block' is true and 'timeout' is None (the default),
        block if necessary until a free slot is available. If 'timeout' is
        a positive number, it blocks at most 'timeout' seconds and raises
        the Full exception if no free slot was available within that time.
        Otherwise ('block' is false), put an item on the queue if a free slot
        is immediately available, else raise the Full exception ('timeout'
        is ignored in that case).
        """
        self.not_full.acquire()
        try:
            if self.maxsize > 0:
                if not block:
                    if self._qsize() >= self.maxsize:
                        raise Full
                elif timeout is None:
                    while self._qsize() >= self.maxsize:
                        self.not_full.wait()
                elif timeout < 0:
                    raise ValueError("'timeout' must be a positive number")
                else:
                    if self._qsize() >= self.maxsize:
                        self.not_full.wait(timeout)
                        if self._qsize() >= self.maxsize:
                            raise Full
            self._put(item)
            self.unfinished_tasks += 1
            self.not_empty.notify()
        finally:
            self.not_full.release()

    def put_nowait(self, item):
        """Put an item into the queue without blocking.

        Only enqueue the item if a free slot is immediately available.
        Otherwise raise the Full exception.
        """
        return self.put(item, False)

    def get(self, block=True, timeout=None):
        """Remove and return an item from the queue.

        If optional args 'block' is true and 'timeout' is None (the default),
        block if necessary until an item is available. If 'timeout' is
        a positive number, it blocks at most 'timeout' seconds and raises
        the Empty exception if no item was available within that time.
        Otherwise ('block' is false), return an item if one is immediately
        available, else raise the Empty exception ('timeout' is ignored
        in that case).
        """
        self.not_empty.acquire()
        try:
            if not block:
                if not self._qsize():
                    raise Empty
            elif timeout is None:
                while not self._qsize():
                    self.not_empty.wait()
            elif timeout < 0:
                raise ValueError("'timeout' must be a positive number")
            else:
                if not self._qsize():
                    self.not_empty.wait(timeout)
                    if not self._qsize():
                        raise Empty
            item = self._get()
            self.not_full.notify()
            return item
        finally:
            self.not_empty.release()

    def get_nowait(self):
        """Remove and return an item from the queue without blocking.

        Only get an item if one is immediately available. Otherwise
        raise the Empty exception.
        """
        return self.get(False)

    # Override these methods to implement other queue organizations
    # (e.g. stack or priority queue).
    # These will only be called with appropriate locks held

    # Initialize the queue representation
    def _init(self, maxsize):
        self.queue = deque()

    def _qsize(self):
        return len(self.queue)

    # Put a new item in the queue
    def _put(self, item):
        self.queue.append(item)

    # Get an item from the queue
    def _get(self):
        return self.queue.popleft()


class PriorityQueue(Queue):
    '''Variant of Queue that retrieves open entries in priority order (lowest first).

    Entries are typically tuples of the form:  (priority number, data).
    '''

    def _init(self, maxsize):
        self.queue = []

    def _qsize(self):
        return len(self.queue)

    def _put(self, item):
        heapq.heappush(self.queue, item)

    def _get(self):
        return heapq.heappop(self.queue)


class LifoQueue(Queue):
    '''Variant of Queue that retrieves most recently added entries first.'''

    def _init(self, maxsize):
        self.queue = []

    def _qsize(self):
        return len(self.queue)

    def _put(self, item):
        self.queue.append(item)

    def _get(self):
        return self.queue.pop()


########NEW FILE########
__FILENAME__ = tasks
#
# This file is part of Evergreen. See the NOTICE for more information.
#

import six

import evergreen
from evergreen.event import Event

from fibers import Fiber


__all__ = ['Task', 'TaskExit', 'spawn', 'sleep', 'task']


def sleep(seconds=0):
    """Yield control to another eligible coroutine until at least *seconds* have
    elapsed.

    *seconds* may be specified as an integer, or a float if fractional seconds
    are desired.
    """
    loop = evergreen.current.loop
    current = Fiber.current()
    assert loop.task is not current
    timer = loop.call_later(seconds, current.switch)
    try:
        loop.switch()
    finally:
        timer.cancel()


def spawn(func, *args, **kwargs):
    """Create a task to run ``func(*args, **kwargs)``.  Returns a
    :class:`Task` objec.

    Execution control returns immediately to the caller; the created task
    is merely scheduled to be run at the next available opportunity.
    Use :func:`spawn_later` to  arrange for tasks to be spawned
    after a finite delay.
    """
    t = Task(target=func, args=args, kwargs=kwargs)
    t.start()
    return t


def task(func):
    """Decorator to run the decorated function as a Task
    """
    def task_wrapper(*args, **kwargs):
        return spawn(func, *args, **kwargs)
    return task_wrapper


class TaskExit(BaseException):
    pass


# Helper to generate new task names
_counter = 0
def _newname(template="Task-%d"):
    global _counter
    _counter = _counter + 1
    return template % _counter


class Task(Fiber):

    def __init__(self, target=None, name=None, args=(), kwargs={}):
        super(Task, self).__init__(target=self.__run, parent=evergreen.current.loop.task)
        self._name = str(name or _newname())
        self._target = target
        self._args = args
        self._kwargs = kwargs
        self._started = False
        self._running = False
        self._exit_event = Event()

    def start(self):
        if self._started:
            raise RuntimeError('tasks can only be started once')
        self._started = True
        evergreen.current.loop.call_soon(self.switch)

    def run(self):
        if self._target:
            self._target(*self._args, **self._kwargs)

    def join(self, timeout=None):
        """Wait for this Task to end. If a timeout is given, after the time expires the function
        will return anyway."""
        if not self._started:
            raise RuntimeError('cannot join task before it is started')
        return self._exit_event.wait(timeout)

    def kill(self, typ=TaskExit, value=None, tb=None):
        """Terminates the current task by raising an exception into it.
        Whatever that task might be doing; be it waiting for I/O or another
        primitive, it sees an exception as soon as it yields control.

        By default, this exception is TaskExit, but a specific exception
        may be specified.
        """
        if not self.is_alive():
            return
        if not value:
            value = typ()
        if not self._running:
            # task hasn't started yet and therefore throw won't work
            def just_raise():
                six.reraise(typ, value, tb)
            self.run = just_raise
            return
        evergreen.current.loop.call_soon(self.throw, typ, value, tb)

    def __repr__(self):
        status = "initial"
        if self._started:
            status = "started"
        if self._running:
            status = "running"
        if self._exit_event.is_set():
            status = "ended"
        return "<%s(%s, %s)>" % (self.__class__.__name__, self._name, status)

    @property
    def name(self):
        return self._name

    # internal

    def __run(self):
        try:
            self._running = True
            self.run()
        except TaskExit:
            pass
        finally:
            self._running = False
            del self._target, self._args, self._kwargs
            self._exit_event.set()


########NEW FILE########
__FILENAME__ = timeout
#
# This file is part of Evergreen. See the NOTICE for more information.
#

import evergreen

__all__ = ['Timeout']


class Timeout(BaseException):
    """Raises *exception* in the current task after *timeout* seconds.

    When *exception* is omitted or ``None``, the :class:`Timeout` instance
    itself is raised. If *seconds* is None, the timer is not scheduled, and is
    only useful if you're planning to raise it directly.

    Timeout objects are context managers, and so can be used in with statements.
    When used in a with statement, if *exception* is ``False``, the timeout is
    still raised, but the context manager suppresses it, so the code outside the
    with-block won't see it.
    """

    def __init__(self, seconds=None, exception=None):
        self.seconds = seconds
        self.exception = exception
        self._timer = None

    def start(self):
        """Schedule the timeout.  This is called on construction, so
        it should not be called explicitly, unless the timer has been
        canceled."""
        assert not self._timer, '%r is already started; to restart it, cancel it first' % self
        loop = evergreen.current.loop
        current = evergreen.current.task
        if self.seconds is None or self.seconds < 0:
            # "fake" timeout (never expires)
            self._timer = None
        elif self.exception is None or isinstance(self.exception, bool):
            # timeout that raises self
            self._timer = loop.call_later(self.seconds, self._timer_cb, current.throw, self)
        else:
            # regular timeout with user-provided exception
            self._timer = loop.call_later(self.seconds, self._timer_cb, current.throw, self.exception)

    def _timer_cb(self, func, arg):
        self._timer = None
        func(arg)

    def cancel(self):
        """If the timeout is pending, cancel it.  If not using
        Timeouts in ``with`` statements, always call cancel() in a
        ``finally`` after the block of code that is getting timed out.
        If not canceled, the timeout will be raised later on, in some
        unexpected section of the application."""
        if self._timer is not None:
            self._timer.cancel()
            self._timer = None

    def __repr__(self):
        if self.exception is None:
            exception = ''
        else:
            exception = ' exception=%r' % self.exception
        return '<%s at %s seconds=%s%s>' % (
            self.__class__.__name__, hex(id(self)), self.seconds, exception)

    def __str__(self):
        if self.seconds is None:
            return ''
        if self.seconds == 1:
            suffix = ''
        else:
            suffix = 's'
        if self.exception is None or self.exception is True:
            return '%s second%s' % (self.seconds, suffix)
        elif self.exception is False:
            return '%s second%s (silent)' % (self.seconds, suffix)
        else:
            return '%s second%s (%s)' % (self.seconds, suffix, self.exception)

    def __enter__(self):
        if not self._timer:
            self.start()
        return self

    def __exit__(self, typ, value, tb):
        self.cancel()
        if value is self and self.exception is False:
            return True


########NEW FILE########
__FILENAME__ = crawler

from evergreen import futures, patcher
urllib2 = patcher.import_patched('urllib2')

urls = ["http://google.com",
        "http://yahoo.com",
        "http://bing.com"]


def fetch(url):
    return urllib2.urlopen(url).read()

executor = futures.TaskPoolExecutor(100)
for body in executor.map(fetch, urls):
    print("got body {}".format(len(body)))


########NEW FILE########
__FILENAME__ = echo-server

import sys

import evergreen
from evergreen.io import tcp

loop = evergreen.EventLoop()


class EchoServer(tcp.TCPServer):

    @evergreen.task
    def handle_connection(self, connection):
        print('client connected from {}'.format(connection.peername))
        while True:
            data = connection.read_until('\n')
            if not data:
                break
            connection.write(data)
        print('connection closed')


def main():
    server = EchoServer()
    port = int(sys.argv[1] if len(sys.argv) > 1 else 1234)
    server.bind(('0.0.0.0', port))
    print ('listening on {}'.format(server.sockname))
    server.serve()


evergreen.spawn(main)
loop.run()


########NEW FILE########
__FILENAME__ = common

import os
import sys
sys.path.insert(0, '../')


if sys.version_info < (2, 7) or (0x03000000 <= sys.hexversion < 0x03010000):
    # py26 or py30
    import unittest2 as unittest
else:
    import unittest

import evergreen


class dummy(object):
    pass


class EvergreenTestCase(unittest.TestCase):

    def setUp(self):
        self.loop = evergreen.EventLoop()

    def tearDown(self):
        self.loop.destroy()
        self.loop = None


########NEW FILE########
__FILENAME__ = test_channel

from common import unittest, EvergreenTestCase

import evergreen


class ChannelTests(EvergreenTestCase):

    def test_channel_simple(self):
        ch = evergreen.Channel()
        def sender():
            ch.send('test')
        def receiver():
            self.assertEqual(ch.receive(), 'test')
        evergreen.spawn(sender)
        evergreen.spawn(receiver)
        self.loop.run()

    def test_channel_exception(self):
        ch = evergreen.Channel()
        def sender():
            ch.send_exception(RuntimeError)
        def receiver():
            self.assertRaises(RuntimeError, ch.receive)
        evergreen.spawn(sender)
        evergreen.spawn(receiver)
        self.loop.run()

    def test_channel_iter(self):
        ch = evergreen.Channel()
        def sender():
            ch.send('hello')
            evergreen.sleep(0)
            ch.send('world')
            evergreen.sleep(0)
            ch.send_exception(StopIteration)
        def receiver():
            items = []
            for item in ch:
                items.append(item)
            self.assertEqual(items, ['hello', 'world'])
        evergreen.spawn(sender)
        evergreen.spawn(receiver)
        self.loop.run()

    def test_channel_multiple_waiters(self):
        ch = evergreen.Channel()
        def sender():
            ch.send('hello')
            ch.send('world')
        def receiver1():
            self.assertEqual(ch.receive(), 'hello')
        def receiver2():
            self.assertEqual(ch.receive(), 'world')
        evergreen.spawn(sender)
        evergreen.spawn(receiver1)
        evergreen.spawn(receiver2)
        self.loop.run()

    def test_channel_iter(self):
        ch = evergreen.Channel()
        def sender1():
            ch.send('hello')
        def sender2():
            ch.send('world')
            ch.send_exception(StopIteration)
        def receiver():
            items = []
            for item in ch:
                items.append(item)
            self.assertEqual(items, ['hello', 'world'])
        evergreen.spawn(sender1)
        evergreen.spawn(sender2)
        evergreen.spawn(receiver)
        self.loop.run()


if __name__ == '__main__':
    unittest.main(verbosity=2)


########NEW FILE########
__FILENAME__ = test_event

from common import unittest, EvergreenTestCase

import evergreen
from evergreen.event import Event


class EventTests(EvergreenTestCase):

    def test_event_simple(self):
        ev = Event()
        def waiter():
            self.assertTrue(ev.wait())
        evergreen.spawn(waiter)
        evergreen.spawn(ev.set)
        self.loop.run()

    def test_event_timeout(self):
        ev = Event()
        def waiter():
            self.assertFalse(ev.wait(0.001))
        evergreen.spawn(waiter)
        self.loop.call_later(0.1, ev.set)
        self.loop.run()

    def test_event_kill_waiter(self):
        ev = Event()
        def waiter():
            ev.wait()
        t1 = evergreen.spawn(waiter)
        evergreen.spawn(t1.kill)
        evergreen.spawn(ev.set)
        self.loop.run()
        self.assertTrue(ev.is_set())

    def test_event_clear(self):
        ev = Event()
        def waiter():
            self.assertTrue(ev.wait())
            ev.clear()
        evergreen.spawn(waiter)
        evergreen.spawn(ev.set)
        self.loop.run()
        self.assertFalse(ev.is_set())


if __name__ == '__main__':
    unittest.main(verbosity=2)


########NEW FILE########
__FILENAME__ = test_futures

from common import dummy, unittest, EvergreenTestCase

import os
import sys

import evergreen
from evergreen import futures


def dummy():
    return 42


class FuturesTests(EvergreenTestCase):

    def test_taskpool_executor(self):
        executor = futures.TaskPoolExecutor(10)
        def func():
            return 42
        def waiter():
            f = executor.submit(func)
            self.assertEqual(f.get(), 42)
        evergreen.spawn(waiter)
        self.loop.run()

    def test_taskpool_executor_raises(self):
        executor = futures.TaskPoolExecutor(10)
        def func():
            1/0
        def waiter():
            f = executor.submit(func)
            self.assertRaises(ZeroDivisionError, f.get)
        evergreen.spawn(waiter)
        self.loop.run()

    def test_taskpool_executor_return_exception(self):
        executor = futures.TaskPoolExecutor(10)
        def func():
            1/0
        def waiter():
            f = executor.submit(func)
            e = f.get(return_exception=True)
            self.assertTrue(isinstance(e, ZeroDivisionError))
        evergreen.spawn(waiter)
        self.loop.run()

    def test_threadpool_executor(self):
        executor = futures.ThreadPoolExecutor(5)
        def func():
            import time
            time.sleep(0.01)
            return 42
        def waiter():
            f = executor.submit(func)
            self.assertEqual(f.get(), 42)
        evergreen.spawn(waiter)
        self.loop.run()

    def test_processpool_executor(self):
        if 'TRAVIS' in os.environ:
            self.skipTest('Disabled on Travis')
            return
        if sys.platform == 'win32':
            self.skipTest('Temporarily disabled on Windows')
            return
        executor = futures.ProcessPoolExecutor(5)
        def waiter():
            f = executor.submit(dummy)
            self.assertEqual(f.get(), 42)
        evergreen.spawn(waiter)
        self.loop.run()

    def test_executor_with(self):
        def func():
            return 42
        def waiter():
            with futures.TaskPoolExecutor(5) as e:
                f = e.submit(func)
                self.assertEqual(f.get(), 42)
        evergreen.spawn(waiter)
        self.loop.run()

    def test_future_wait(self):
        executor = futures.TaskPoolExecutor(10)
        def func():
            evergreen.sleep(0.001)
            return 42
        def waiter():
            f = executor.submit(func)
            done, not_done = futures.wait([f])
            self.assertTrue(f in done)
            self.assertEqual(f.get(), 42)
        evergreen.spawn(waiter)
        self.loop.run()

    def test_future_wait_multiple(self):
        executor = futures.TaskPoolExecutor(10)
        def func():
            evergreen.sleep(0.001)
            return 42
        def waiter():
            f1 = executor.submit(func)
            f2 = executor.submit(func)
            done, not_done = futures.wait([f1, f2])
            self.assertTrue(f1 in done and f2 in done)
            self.assertEqual(f1.get(), 42)
            self.assertEqual(f2.get(), 42)
        evergreen.spawn(waiter)
        self.loop.run()

    def test_future_wait_multiple_wait_first(self):
        executor = futures.TaskPoolExecutor(10)
        def func(x):
            evergreen.sleep(x)
            return 42
        def waiter():
            f = executor.submit(func, 0.01)
            l = [f]
            for x in range(100):
                l.append(executor.submit(func, 100))
            done, not_done = futures.wait(l, return_when=futures.FIRST_COMPLETED)
            self.assertTrue(f in done)
            self.assertEqual(len(not_done), 100)
            self.assertEqual(f.get(), 42)
            self.loop.stop()
        evergreen.spawn(waiter)
        self.loop.run()

    def test_future_wait_multiple_exception(self):
        executor = futures.TaskPoolExecutor(10)
        def func():
            evergreen.sleep(0.001)
            return 42
        def raiser():
            1/0
        def waiter():
            f1 = executor.submit(raiser)
            f2 = executor.submit(func)
            f3 = executor.submit(func)
            done, not_done = futures.wait([f1, f2, f3], return_when=futures.FIRST_EXCEPTION)
            self.assertTrue(f1 in done)
            self.assertTrue(f2 in not_done and f3 in not_done)
            self.assertRaises(ZeroDivisionError, f1.get)
            self.loop.stop()
        evergreen.spawn(waiter)
        self.loop.run()

    def test_future_as_completed(self):
        executor = futures.TaskPoolExecutor(10)
        def func(x):
            evergreen.sleep(x)
            return 42
        def waiter():
            l = [executor.submit(func, 0.001) for x in range(10)]
            for f in futures.as_completed(l):
                self.assertEqual(f.get(), 42)
        evergreen.spawn(waiter)
        self.loop.run()

    def test_future_as_completed_timeout(self):
        executor = futures.TaskPoolExecutor(10)
        def func():
            evergreen.sleep(0.1)
            return 42
        def get_results(lst):
            return list(futures.as_completed(lst, timeout=0.01))
        def waiter():
            self.assertRaises(futures.TimeoutError, get_results, [executor.submit(func)])
        evergreen.spawn(waiter)
        self.loop.run()

    def test_map(self):
        executor = futures.TaskPoolExecutor(10)
        def func(x):
            return x*x
        def waiter():
            l1 = [2, 4, 6]
            l2 = [4, 16, 36]
            r = list(executor.map(func, l1))
            self.assertEqual(r, l2)
        evergreen.spawn(waiter)
        self.loop.run()


if __name__ == '__main__':
    unittest.main(verbosity=2)


########NEW FILE########
__FILENAME__ = test_io

from common import dummy, unittest, EvergreenTestCase

import re
import sys

import evergreen
from evergreen.io import tcp, pipe, udp
from evergreen.io.util import StringBuffer


if sys.platform == 'win32':
    TEST_PIPE = '\\\\.\\pipe\\test-pipe'
else:
    TEST_PIPE = 'test-pipe'
TEST_PORT = 1234
TEST_SERVER = ('0.0.0.0', TEST_PORT)
TEST_CLIENT = ('127.0.0.1', TEST_PORT)
TEST_UDP_ENDPOINT = TEST_CLIENT


class EchoMixin(object):

    @evergreen.task
    def handle_connection(self, connection):
        while True:
            data = connection.read_until(b'\n')
            if not data:
                break
            connection.write(data)
            connection.flush()
            if connection.closed:
                # Connection might have been closed after we flushed
                break


class EchoTCPServer(EchoMixin, tcp.TCPServer):
    pass


class EchoPipeServer(EchoMixin, pipe.PipeServer):
    pass


class BufferTest(unittest.TestCase):

    def test_read(self):
        buf = StringBuffer()
        buf.feed(b'hello world')
        data = buf.read(5)
        self.assertEqual(data, b'hello')
        data = buf.read(6)
        self.assertEqual(data, b' world')
        data = buf.read(100)
        self.assertEqual(data, None)

    def test_read_until(self):
        buf = StringBuffer()
        buf.feed(b'hello\nworld\n')
        data = buf.read_until(b'\n')
        self.assertEqual(data, b'hello\n')
        data = buf.read_until(b'\n')
        self.assertEqual(data, b'world\n')
        data = buf.read_until(b'\n')
        self.assertEqual(data, None)

    def test_read_until_regex(self):
        regex = re.compile(b'~~')
        buf = StringBuffer()
        buf.feed(b'hello~~world~~')
        data = buf.read_until_regex(regex)
        self.assertEqual(data, b'hello~~')
        data = buf.read_until_regex(regex)
        self.assertEqual(data, b'world~~')
        data = buf.read_until_regex(regex)
        self.assertEqual(data, None)

    def test_clear(self):
        buf = StringBuffer()
        buf.feed(b'hello world')
        buf.clear()
        data = buf.read(5)
        self.assertEqual(data, None)

    def test_close(self):
        buf = StringBuffer()
        buf.feed(b'hello world')
        buf.close()
        self.assertTrue(buf.closed)
        self.assertRaises(ValueError, buf.read, 5)


class IOTests(EvergreenTestCase):

    def _start_tcp_echo_server(self):
        self.server = EchoTCPServer()
        self.server.bind(TEST_SERVER)
        self.server.serve()

    def test_tcp_read(self):
        def connect():
            client = tcp.TCPClient()
            client.connect(TEST_CLIENT)
            client.sockname    # for coverage
            client.peername    # for coverage
            r = client.write(b'PING\n')
            self.assertTrue(r)
            data = client.read_until(b'\n')
            self.assertEqual(data, b'PING\n')
            client.close()
            self.server.close()
        evergreen.spawn(self._start_tcp_echo_server)
        evergreen.spawn(connect)
        self.loop.run()

    def test_tcp_read_error(self):
        def connect():
            client = tcp.TCPClient()
            client.connect(TEST_CLIENT)
            evergreen.sleep(0.1)  # give it some time to stabilize
            client.close()
            self.assertRaises(tcp.TCPError, client.read_until, b'\n')
            self.server.close()
        evergreen.spawn(self._start_tcp_echo_server)
        evergreen.spawn(connect)
        self.loop.run()

    def test_tcp_shutdown(self):
        def connect():
            client = tcp.TCPClient()
            client.connect(TEST_CLIENT)
            while True:
                if not client.write(b'PING\n'):
                    break
            client.shutdown()
            client.close()
            self.server.close()
        evergreen.spawn(self._start_tcp_echo_server)
        evergreen.spawn(connect)
        self.loop.run()

    def _start_pipe_echo_server(self):
        self.server = EchoPipeServer()
        self.server.bind(TEST_PIPE)
        self.server.serve()

    def test_pipe_read(self):
        def connect():
            client = pipe.PipeClient()
            client.connect(TEST_PIPE)
            client.write(b'PING\n')
            data = client.read_until(b'\n')
            self.assertEqual(data, b'PING\n')
            client.close()
            self.server.close()
        evergreen.spawn(self._start_pipe_echo_server)
        evergreen.spawn(connect)
        self.loop.run()

    def test_pipe_read_error(self):
        def connect():
            client = pipe.PipeClient()
            client.connect(TEST_PIPE)
            evergreen.sleep(0.1)  # give it some time to stabilize
            client.close()
            self.assertRaises(pipe.PipeError, client.read_until, b'\n')
            self.server.close()
        evergreen.spawn(self._start_pipe_echo_server)
        evergreen.spawn(connect)
        self.loop.run()

    def _start_udp_echo_server(self):
        self.server = udp.UDPEndpoint()
        self.server.bind(TEST_UDP_ENDPOINT)
        self.server.sockname    # for coverage
        try:
            while True:
                data, addr = self.server.receive()
                self.server.send(data, addr)
                self.server.flush()
        except udp.UDPError:
            pass

    def test_udp_ping_pong(self):
        def connect():
            client = udp.UDPEndpoint()
            client.send(b'PING', TEST_UDP_ENDPOINT)
            client.flush()
            data, addr = client.receive()
            self.assertEqual(data, b'PING')
            client.close()
            self.assertRaises(udp.UDPError, client.receive)
            self.server.close()
        evergreen.spawn(self._start_udp_echo_server)
        evergreen.spawn(connect)
        self.loop.run()


if __name__ == '__main__':
    unittest.main(verbosity=2)


########NEW FILE########
__FILENAME__ = test_local

from common import unittest, EvergreenTestCase

import evergreen
from evergreen.local import local


class LocalTests(EvergreenTestCase):

    def test_local(self):
        tls = local()
        tls.foo = 42
        def func(x):
            self.assertRaises(AttributeError, lambda: tls.foo)
            tls.foo = x
            self.assertEqual(tls.foo, x)
        evergreen.spawn(func, 1)
        evergreen.spawn(func, 2)
        evergreen.spawn(func, 3)
        self.loop.run()


if __name__ == '__main__':
    unittest.main(verbosity=2)


########NEW FILE########
__FILENAME__ = test_locks

from common import dummy, unittest, EvergreenTestCase

import evergreen
from evergreen import locks


class LocksTests(EvergreenTestCase):

    def test_semaphore(self):
        def func():
            lock = locks.Semaphore()
            lock.acquire()
            self.assertFalse(lock.acquire(blocking=False))
        evergreen.spawn(func)
        self.loop.run()

    def test_bounded_semaphore(self):
        def func():
            lock = locks.BoundedSemaphore()
            lock.acquire()
            lock.release()
            self.assertRaises(ValueError, lock.release)
        evergreen.spawn(func)
        self.loop.run()

    def test_rlock(self):
        lock = locks.RLock()
        def func1():
            lock.acquire()
            self.assertTrue(lock.acquire())
        def func2():
            self.assertFalse(lock.acquire(blocking=False))
        evergreen.spawn(func1)
        evergreen.spawn(func2)
        self.loop.run()

    def test_condition(self):
        d = dummy()
        d.value = 0
        cond = locks.Condition()
        def func1():
            with cond:
                self.assertEqual(d.value, 0)
                cond.wait()
                self.assertEqual(d.value, 42)
        def func2():
            with cond:
                d.value = 42
                cond.notify_all()
        evergreen.spawn(func1)
        evergreen.spawn(func2)
        self.loop.run()

    def test_barrier(self):
        num_tasks = 10
        d = dummy()
        d.count = 0
        barrier = locks.Barrier(num_tasks+1, timeout=1)
        def func():
            evergreen.sleep(0.01)
            d.count += 1
            barrier.wait()
        def runner():
            for x in range(num_tasks):
                evergreen.spawn(func)
            barrier.wait()
        evergreen.spawn(runner)
        self.loop.run()
        self.assertEqual(d.count, num_tasks)
        self.assertEqual(barrier.parties, num_tasks+1)


if __name__ == '__main__':
    unittest.main(verbosity=2)


########NEW FILE########
__FILENAME__ = test_loop

from common import dummy, unittest, EvergreenTestCase

import evergreen
import signal
import threading

from six.moves import queue


class TLSTest(unittest.TestCase):

    def test_no_noop(self):
        loop = evergreen.current.loop
        self.assertTrue(loop)
        loop.destroy()
        self.assertRaises(RuntimeError, loop.destroy)

    def test_make_loop(self):
        loop = evergreen.EventLoop()
        self.assertTrue(evergreen.current.loop is loop)
        loop.destroy()

    def test_destroy_create(self):
        loop1 = evergreen.current.loop
        self.assertTrue(loop1)
        loop1.destroy()
        loop2 = evergreen.current.loop
        self.assertFalse(loop1 is loop2)
        loop2.destroy()

    def _start_loop(self, q):
        l = evergreen.EventLoop()
        q.put(l)
        l.run_forever()
        l.destroy()

    def test_destroy(self):
        q = queue.Queue()
        thread = threading.Thread(target=self._start_loop, args=(q,))
        thread.start()

        loop = q.get()
        self.assertRaises(RuntimeError, loop.destroy)
        loop.call_from_thread(loop.stop)

        thread.join()


class LoopTests(EvergreenTestCase):

    def test_run(self):
        self.loop.run()
        self.assertFalse(self.loop.task.is_alive())

    def test_call_soon(self):
        d = dummy()
        d.called = False
        def func():
            d.called = True
        self.loop.call_soon(func)
        self.loop.run()
        self.assertTrue(d.called)

    def test_call_soon_cancel(self):
        d = dummy()
        d.called = False
        def func():
            d.called = True
        h =  self.loop.call_soon(func)
        h.cancel()
        self.loop.run()
        self.assertFalse(d.called)

    def test_call_later(self):
        d = dummy()
        d.called = False
        def func():
            d.called = True
        self.loop.call_later(0.1, func)
        self.loop.run()
        self.assertTrue(d.called)

    def test_call_later_cancel(self):
        d = dummy()
        d.called = False
        def func():
            d.called = True
        h =  self.loop.call_later(0.1, func)
        h.cancel()
        self.loop.run()
        self.assertFalse(d.called)

    def test_call_later_cancel2(self):
        d = dummy()
        d.called = False
        def func1():
            h2.cancel()
        def func2():
            d.called = True
        h1 =  self.loop.call_later(0.01, func1)
        h2 =  self.loop.call_later(0.01, func2)
        self.loop.run()
        self.assertFalse(d.called)

    def test_call_at(self):
        d = dummy()
        d.called = False
        def func():
            d.called = True
        self.loop.call_at(self.loop.time()+0.1, func)
        self.loop.run()
        self.assertTrue(d.called)

    def test_stop(self):
        self.assertRaises(RuntimeError, self.loop.stop)

    def test_stop2(self):
        self.loop.call_later(100, lambda: None)
        self.loop.call_later(0.01, self.loop.stop)
        t0 = self.loop.time()
        self.loop.run()
        t1 = self.loop.time()
        self.assertTrue(0 <= t1-t0 < 0.1)

    def test_internal_threadpool(self):
        tid = threading.current_thread().ident
        def runner():
            import time
            time.sleep(0.001)
            return threading.current_thread().ident
        def func():
            r = self.loop._threadpool.spawn(runner)
            self.assertNotEqual(r.get(), tid)
        evergreen.spawn(func)
        self.loop.run()

    def test_run_forever(self):
        d = dummy()
        d.called = False
        def stop_loop():
            d.called = True
            self.loop.stop()
        def func():
            import time
            time.sleep(0.2)
            self.loop.call_from_thread(stop_loop)
        t = threading.Thread(target=func)
        t.start()
        self.loop.run_forever()
        t.join()
        self.assertTrue(d.called)

    def test_signal(self):
        if not hasattr(signal, 'SIGALRM'):
            self.skipTest('No signal support')
            return
        d = dummy()
        d.called = False
        def signal_cb():
            d.called = True
        h = self.loop.add_signal_handler(signal.SIGALRM, signal_cb)
        signal.setitimer(signal.ITIMER_REAL, 0.1, 0)  # Send SIGALRM once
        self.loop.call_later(0.15, self.loop.stop)
        self.loop.run_forever()
        self.assertTrue(d.called)

    def test_signal_multi(self):
        if not hasattr(signal, 'SIGALRM'):
            self.skipTest('No signal support')
            return
        d = dummy()
        d.called1 = False
        d.called2 = False
        def signal_cb1():
            d.called1 = True
        def signal_cb2():
            d.called2 = True
        h1 = self.loop.add_signal_handler(signal.SIGALRM, signal_cb1)
        h2 = self.loop.add_signal_handler(signal.SIGALRM, signal_cb2)
        signal.setitimer(signal.ITIMER_REAL, 0.1, 0)  # Send SIGALRM once
        self.loop.call_later(0.15, self.loop.stop)
        self.loop.run_forever()
        self.assertTrue(d.called1)
        self.assertTrue(d.called2)

    def test_signal_cancel(self):
        if not hasattr(signal, 'SIGALRM'):
            self.skipTest('No signal support')
            return
        d = dummy()
        d.called1 = False
        d.called2 = False
        def signal_cb1():
            d.called1 = True
        def signal_cb2():
            d.called2 = True
        h1 = self.loop.add_signal_handler(signal.SIGALRM, signal_cb1)
        h2 = self.loop.add_signal_handler(signal.SIGALRM, signal_cb2)
        h2.cancel()
        signal.setitimer(signal.ITIMER_REAL, 0.1, 0)  # Send SIGALRM once
        self.loop.call_later(0.15, self.loop.stop)
        self.loop.run_forever()
        self.assertTrue(d.called1)
        self.assertFalse(d.called2)

    def test_signal_remove(self):
        if not hasattr(signal, 'SIGALRM'):
            self.skipTest('No signal support')
            return
        d = dummy()
        d.called1 = False
        d.called2 = False
        def signal_cb1():
            d.called1 = True
        def signal_cb2():
            d.called2 = True
        h1 = self.loop.add_signal_handler(signal.SIGALRM, signal_cb1)
        self.loop.remove_signal_handler(signal.SIGALRM)
        h2 = self.loop.add_signal_handler(signal.SIGALRM, signal_cb2)
        signal.setitimer(signal.ITIMER_REAL, 0.1, 0)  # Send SIGALRM once
        self.loop.call_later(0.15, self.loop.stop)
        self.loop.run_forever()
        self.assertFalse(d.called1)
        self.assertTrue(d.called2)

    def test_destroy_while_running(self):
        def func():
            evergreen.sleep(0.01)
            self.assertRaises(RuntimeError, self.loop.destroy)
        evergreen.spawn(func)
        self.loop.run()

    def test_reuse_handler(self):
        handler = self.loop.call_later(1, lambda: None)
        self.assertRaises(AssertionError, self.loop.call_later, 1, handler)


if __name__ == '__main__':
    unittest.main(verbosity=2)


########NEW FILE########
__FILENAME__ = test_tasks

from common import dummy, unittest, EvergreenTestCase

import evergreen
import time


class MyTask(evergreen.Task):
    called = False

    def run(self):
        self.called = True


class TasksTests(EvergreenTestCase):

    def test_simple_task(self):
        d = dummy()
        d.called = False
        def func():
            d.called = True
        task = evergreen.Task(target=func)
        task.start()
        self.loop.run()
        self.assertTrue(d.called)

    def test_spawn(self):
        d = dummy()
        d.called = False
        def func():
            d.called = True
        evergreen.spawn(func)
        self.loop.run()
        self.assertTrue(d.called)

    def test_spawn_kill(self):
        d = dummy()
        d.called = False
        def func():
            d.called = True
        task = evergreen.spawn(func)
        task.kill()
        self.loop.run()
        self.assertFalse(d.called)

    def test_spawn_kill_join(self):
        d = dummy()
        d.called = False
        def func1():
            d.called = True
        def func2():
            self.assertTrue(t1.join())
        t1 = evergreen.spawn(func1)
        t1.kill()
        t2 = evergreen.spawn(func2)
        self.loop.run()
        self.assertFalse(d.called)

    def test_task_decorator(self):
        d = dummy()
        d.called = False
        @evergreen.task
        def func():
            d.called = True
        func()
        self.loop.run()
        self.assertTrue(d.called)

    def test_task_sleep(self):
        called = []
        def func():
            called.append(None)
            evergreen.sleep(0.001)
            called.append(None)
        evergreen.spawn(func)
        t0 = time.time()
        self.loop.run()
        t1 = time.time()
        self.assertEqual(len(called), 2)
        self.assertTrue(0 <= t1-t0 < 0.1)

    def test_spawn_order(self):
        called = []
        def func(x):
            called.append(x)
        for i in range(5):
            evergreen.spawn(func, i)
        self.loop.run()
        self.assertEqual(called, [0, 1, 2, 3, 4])

    def test_custom_task(self):
        task = MyTask()
        task.start()
        self.loop.run()
        self.assertTrue(task.called)

    def test_kill_running(self):
        called = []
        def func():
            evergreen.sleep(0)
            called.append(None)
            evergreen.sleep(0)
            called.append(None)
        task1 = evergreen.spawn(func)
        task2 = evergreen.spawn(task1.kill)
        self.loop.run()
        self.assertEqual(len(called), 1)

    def test_run_order(self):
        called = []
        def func(x):
            called.append(x)
            evergreen.sleep(0)
            called.append(x)
        evergreen.spawn(func, 1)
        evergreen.spawn(func, 2)
        evergreen.spawn(func, 3)
        self.loop.run()
        self.assertEqual(called, [1, 2, 3, 1, 2, 3])

    def test_task_join(self):
        d = dummy()
        d.called = False
        def func1():
            evergreen.sleep(0.01)
        def func2():
            d.called = t1.join()
        t1 = evergreen.spawn(func1)
        t2 = evergreen.spawn(func2)
        self.loop.run()
        self.assertTrue(d.called)

    def test_task_join_timeout(self):
        d = dummy()
        d.called = False
        def func1():
            evergreen.sleep(10)
        def func2():
            d.called = t1.join(0.01)
        def func3():
            evergreen.sleep(0.1)
            t1.kill()
        t1 = evergreen.spawn(func1)
        t2 = evergreen.spawn(func2)
        t3 = evergreen.spawn(func3)
        self.loop.run()
        self.assertTrue(not d.called)

    def test_task_bogus_switch(self):
        def func1():
            evergreen.sleep(0)
            evergreen.sleep(0)
        def func2():
            self.assertRaises(RuntimeError, t1.switch)
            self.assertRaises(RuntimeError, t1.throw)
        t1 = evergreen.spawn(func1)
        t2 = evergreen.spawn(func2)
        self.loop.run()

#    def test_task_exception(self):
#        def func():
#            1/0
#        evergreen.spawn(func)
#        self.loop.run()

    def test_join_before_start(self):
        t = evergreen.Task()
        self.assertRaises(RuntimeError, t.join)
        self.loop.run()


if __name__ == '__main__':
    unittest.main(verbosity=2)


########NEW FILE########
__FILENAME__ = test_timeout

from common import unittest, EvergreenTestCase

import evergreen
from evergreen.timeout import Timeout


class FooTimeout(Exception):
    pass


class TimeoutTests(EvergreenTestCase):

    def test_with_timeout(self):
        def sleep():
            with Timeout(0.01):
                evergreen.sleep(10)
        def func():
            self.assertRaises(Timeout, sleep)
        evergreen.spawn(func)
        self.loop.run()

    def test_with_none_timeout(self):
        def sleep():
            with Timeout(None):
                evergreen.sleep(0.01)
        def func():
            sleep()
        evergreen.spawn(func)
        self.loop.run()

    def test_with_negative_timeout(self):
        def sleep():
            with Timeout(-1):
                evergreen.sleep(0.01)
        def func():
            sleep()
        evergreen.spawn(func)
        self.loop.run()

    def test_timeout_custom_exception(self):
        def sleep():
            with Timeout(0.01, FooTimeout):
                evergreen.sleep(10)
        def func():
            self.assertRaises(FooTimeout, sleep)
        evergreen.spawn(func)
        self.loop.run()

    def test_timeout(self):
        def func():
            t = Timeout(0.01)
            t.start()
            try:
                evergreen.sleep(10)
            except Timeout as e:
                self.assertTrue(t is e)
        evergreen.spawn(func)
        self.loop.run()


if __name__ == '__main__':
    unittest.main(verbosity=2)


########NEW FILE########
